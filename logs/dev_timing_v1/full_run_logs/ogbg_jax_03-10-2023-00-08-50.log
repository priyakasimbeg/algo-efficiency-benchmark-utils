I0310 00:09:04.007666 140522088007488 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/ogbg_jax.
I0310 00:09:04.051895 140522088007488 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0310 00:09:04.979210 140522088007488 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0310 00:09:04.979760 140522088007488 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0310 00:09:04.982501 140522088007488 submission_runner.py:485] Using RNG seed 3801688354
I0310 00:09:06.310598 140522088007488 submission_runner.py:494] --- Tuning run 1/1 ---
I0310 00:09:06.310781 140522088007488 submission_runner.py:499] Creating tuning directory at /experiment_runs/timing/ogbg_jax/trial_1.
I0310 00:09:06.310955 140522088007488 logger_utils.py:84] Saving hparams to /experiment_runs/timing/ogbg_jax/trial_1/hparams.json.
I0310 00:09:06.433935 140522088007488 submission_runner.py:230] Initializing dataset.
I0310 00:09:06.653780 140522088007488 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:09:06.658489 140522088007488 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0310 00:09:06.814785 140522088007488 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:09:06.844076 140522088007488 submission_runner.py:237] Initializing model.
I0310 00:09:14.162058 140522088007488 submission_runner.py:247] Initializing optimizer.
I0310 00:09:14.499037 140522088007488 submission_runner.py:254] Initializing metrics bundle.
I0310 00:09:14.499246 140522088007488 submission_runner.py:269] Initializing checkpoint and logger.
I0310 00:09:14.500179 140522088007488 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/ogbg_jax/trial_1 with prefix checkpoint_
I0310 00:09:14.500429 140522088007488 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0310 00:09:14.500494 140522088007488 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0310 00:09:15.262993 140522088007488 submission_runner.py:290] Saving meta data to /experiment_runs/timing/ogbg_jax/trial_1/meta_data_0.json.
I0310 00:09:15.263967 140522088007488 submission_runner.py:293] Saving flags to /experiment_runs/timing/ogbg_jax/trial_1/flags_0.json.
I0310 00:09:15.266846 140522088007488 submission_runner.py:303] Starting training loop.
I0310 00:09:33.617691 140345874577152 logging_writer.py:48] [0] global_step=0, grad_norm=2.0182607173919678, loss=0.7542535066604614
I0310 00:09:33.625313 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:09:33.633394 140522088007488 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:09:33.637400 140522088007488 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0310 00:09:33.696097 140522088007488 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0310 00:09:48.675176 140522088007488 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0310 00:11:02.111541 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:11:02.114400 140522088007488 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:11:02.118052 140522088007488 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0310 00:11:02.171452 140522088007488 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:12:02.640327 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:12:02.642871 140522088007488 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:12:02.646374 140522088007488 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0310 00:12:02.695295 140522088007488 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0310 00:13:03.546825 140522088007488 submission_runner.py:363] Time since start: 18.36s, 	Step: 1, 	{'train/accuracy': 0.4633101522922516, 'train/loss': 0.7533521056175232, 'train/mean_average_precision': 0.020762612813650893, 'validation/accuracy': 0.47774508595466614, 'validation/loss': 0.7442635893821716, 'validation/mean_average_precision': 0.02582417221414534, 'validation/num_examples': 43793, 'test/accuracy': 0.48069873452186584, 'test/loss': 0.7438132762908936, 'test/mean_average_precision': 0.02632104148489224, 'test/num_examples': 43793}
I0310 00:13:03.553679 140336302728960 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=18.280787, test/accuracy=0.480699, test/loss=0.743813, test/mean_average_precision=0.026321, test/num_examples=43793, total_duration=18.358446, train/accuracy=0.463310, train/loss=0.753352, train/mean_average_precision=0.020763, validation/accuracy=0.477745, validation/loss=0.744264, validation/mean_average_precision=0.025824, validation/num_examples=43793
I0310 00:13:03.577430 140522088007488 checkpoints.py:356] Saving checkpoint at step: 1
I0310 00:13:03.638293 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_1
I0310 00:13:03.638483 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_1.
I0310 00:13:25.174084 140336311121664 logging_writer.py:48] [100] global_step=100, grad_norm=0.013009591028094292, loss=0.05298753082752228
I0310 00:13:46.660898 140337510835968 logging_writer.py:48] [200] global_step=200, grad_norm=0.011857216246426105, loss=0.06526839733123779
I0310 00:14:08.317980 140336311121664 logging_writer.py:48] [300] global_step=300, grad_norm=0.007778529077768326, loss=0.051119618117809296
I0310 00:14:29.908212 140337510835968 logging_writer.py:48] [400] global_step=400, grad_norm=0.0076730442233383656, loss=0.055713389068841934
I0310 00:14:51.512821 140336311121664 logging_writer.py:48] [500] global_step=500, grad_norm=0.009380084462463856, loss=0.05766948685050011
I0310 00:15:13.087087 140337510835968 logging_writer.py:48] [600] global_step=600, grad_norm=0.008260740898549557, loss=0.05938651040196419
I0310 00:15:34.737989 140336311121664 logging_writer.py:48] [700] global_step=700, grad_norm=0.010916285216808319, loss=0.054921358823776245
I0310 00:15:56.357232 140337510835968 logging_writer.py:48] [800] global_step=800, grad_norm=0.011812758632004261, loss=0.059375375509262085
I0310 00:16:17.894957 140336311121664 logging_writer.py:48] [900] global_step=900, grad_norm=0.008678505197167397, loss=0.056019872426986694
I0310 00:16:39.478417 140337510835968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.008906246162950993, loss=0.056760337203741074
I0310 00:17:01.108269 140336311121664 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.010120803490281105, loss=0.05195944383740425
I0310 00:17:03.764363 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:18:12.294933 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:18:14.757061 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:18:17.162298 140522088007488 submission_runner.py:363] Time since start: 468.50s, 	Step: 1113, 	{'train/accuracy': 0.9866687059402466, 'train/loss': 0.05564166232943535, 'train/mean_average_precision': 0.030714027414759357, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06562034040689468, 'validation/mean_average_precision': 0.030828926366209247, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06906113773584366, 'test/mean_average_precision': 0.03229104538075053, 'test/num_examples': 43793}
I0310 00:18:17.169133 140337510835968 logging_writer.py:48] [1113] global_step=1113, preemption_count=0, score=257.488099, test/accuracy=0.983142, test/loss=0.069061, test/mean_average_precision=0.032291, test/num_examples=43793, total_duration=468.497490, train/accuracy=0.986669, train/loss=0.055642, train/mean_average_precision=0.030714, validation/accuracy=0.984118, validation/loss=0.065620, validation/mean_average_precision=0.030829, validation/num_examples=43793
I0310 00:18:17.191463 140522088007488 checkpoints.py:356] Saving checkpoint at step: 1113
I0310 00:18:17.251289 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_1113
I0310 00:18:17.251468 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_1113.
I0310 00:18:36.549514 140336311121664 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.006869434844702482, loss=0.05398198962211609
I0310 00:18:58.368318 140337410467584 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.008391201496124268, loss=0.05846966430544853
I0310 00:19:20.535816 140336311121664 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.009068561717867851, loss=0.049675531685352325
I0310 00:19:42.830831 140337410467584 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.010220658965408802, loss=0.053269706666469574
I0310 00:20:05.189131 140336311121664 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.019157400354743004, loss=0.05329659581184387
I0310 00:20:27.709479 140337410467584 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.016476277261972427, loss=0.055848378688097
I0310 00:20:49.867274 140336311121664 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.022694237530231476, loss=0.04927736520767212
I0310 00:21:12.129898 140337410467584 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.008992452174425125, loss=0.050303176045417786
I0310 00:21:34.282331 140336311121664 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.025124557316303253, loss=0.05799391120672226
I0310 00:21:56.008764 140337410467584 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.014318033121526241, loss=0.05079389363527298
I0310 00:22:17.390747 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:23:27.118796 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:23:29.566980 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:23:31.960879 140522088007488 submission_runner.py:363] Time since start: 782.12s, 	Step: 2199, 	{'train/accuracy': 0.9868823885917664, 'train/loss': 0.05244118720293045, 'train/mean_average_precision': 0.04272859091010647, 'validation/accuracy': 0.9841358065605164, 'validation/loss': 0.0627637505531311, 'validation/mean_average_precision': 0.045199726204732824, 'validation/num_examples': 43793, 'test/accuracy': 0.9831463098526001, 'test/loss': 0.06622272729873657, 'test/mean_average_precision': 0.046553446653382013, 'test/num_examples': 43793}
I0310 00:23:31.968044 140336311121664 logging_writer.py:48] [2199] global_step=2199, preemption_count=0, score=496.714777, test/accuracy=0.983146, test/loss=0.066223, test/mean_average_precision=0.046553, test/num_examples=43793, total_duration=782.123857, train/accuracy=0.986882, train/loss=0.052441, train/mean_average_precision=0.042729, validation/accuracy=0.984136, validation/loss=0.062764, validation/mean_average_precision=0.045200, validation/num_examples=43793
I0310 00:23:31.990968 140522088007488 checkpoints.py:356] Saving checkpoint at step: 2199
I0310 00:23:32.049504 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_2199
I0310 00:23:32.049680 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_2199.
I0310 00:23:32.506316 140337410467584 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.015424687415361404, loss=0.0440746508538723
I0310 00:23:54.660769 140337393682176 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.013694451190531254, loss=0.046290334314107895
I0310 00:24:16.922106 140337410467584 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.019370829686522484, loss=0.05477205291390419
I0310 00:24:38.878143 140337393682176 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.020830299705266953, loss=0.04828374460339546
I0310 00:25:00.715804 140337410467584 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.009411191567778587, loss=0.048632584512233734
I0310 00:25:22.510219 140337393682176 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.014189508743584156, loss=0.055476728826761246
I0310 00:25:44.418894 140337410467584 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.016296664252877235, loss=0.048956435173749924
I0310 00:26:06.230460 140337393682176 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.012194340117275715, loss=0.05335168167948723
I0310 00:26:28.148591 140337410467584 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01709730736911297, loss=0.04618304222822189
I0310 00:26:50.255329 140337393682176 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.04536105692386627, loss=0.05688660219311714
I0310 00:27:12.160141 140337410467584 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.013802694156765938, loss=0.047826122492551804
I0310 00:27:32.101163 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:28:42.380554 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:28:44.810800 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:28:47.197680 140522088007488 submission_runner.py:363] Time since start: 1096.83s, 	Step: 3293, 	{'train/accuracy': 0.987008810043335, 'train/loss': 0.04882008209824562, 'train/mean_average_precision': 0.07940899341472557, 'validation/accuracy': 0.9843103289604187, 'validation/loss': 0.05884339287877083, 'validation/mean_average_precision': 0.0756762533598784, 'validation/num_examples': 43793, 'test/accuracy': 0.9833114147186279, 'test/loss': 0.06225157156586647, 'test/mean_average_precision': 0.07848469650188986, 'test/num_examples': 43793}
I0310 00:28:47.204640 140337393682176 logging_writer.py:48] [3293] global_step=3293, preemption_count=0, score=735.875115, test/accuracy=0.983311, test/loss=0.062252, test/mean_average_precision=0.078485, test/num_examples=43793, total_duration=1096.834289, train/accuracy=0.987009, train/loss=0.048820, train/mean_average_precision=0.079409, validation/accuracy=0.984310, validation/loss=0.058843, validation/mean_average_precision=0.075676, validation/num_examples=43793
I0310 00:28:47.227277 140522088007488 checkpoints.py:356] Saving checkpoint at step: 3293
I0310 00:28:47.282882 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_3293
I0310 00:28:47.283047 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_3293.
I0310 00:28:48.976048 140337410467584 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.03053169697523117, loss=0.05693619325757027
I0310 00:29:10.536778 140337376896768 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.009803458116948605, loss=0.04838850349187851
I0310 00:29:32.056597 140337410467584 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.02577120251953602, loss=0.04627018794417381
I0310 00:29:53.561672 140337376896768 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.016640905290842056, loss=0.04502558708190918
I0310 00:30:15.196828 140337410467584 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.01594093069434166, loss=0.04177376255393028
I0310 00:30:36.954134 140337376896768 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.02862418256700039, loss=0.04763529449701309
I0310 00:30:58.444552 140337410467584 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.03979102894663811, loss=0.05178895220160484
I0310 00:31:19.963852 140337376896768 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.04818735271692276, loss=0.04757478088140488
I0310 00:31:41.924097 140337410467584 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.020688386633992195, loss=0.04178410768508911
I0310 00:32:04.139787 140337376896768 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.013112726621329784, loss=0.04555642604827881
I0310 00:32:25.952529 140337410467584 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.027538560330867767, loss=0.03960876539349556
I0310 00:32:47.373506 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:33:58.288428 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:34:00.812212 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:34:03.298954 140522088007488 submission_runner.py:363] Time since start: 1412.11s, 	Step: 4399, 	{'train/accuracy': 0.9873459935188293, 'train/loss': 0.045277658849954605, 'train/mean_average_precision': 0.11703214719667794, 'validation/accuracy': 0.984790563583374, 'validation/loss': 0.05424490198493004, 'validation/mean_average_precision': 0.11637773352645944, 'validation/num_examples': 43793, 'test/accuracy': 0.9838037490844727, 'test/loss': 0.05727805942296982, 'test/mean_average_precision': 0.1159356758880539, 'test/num_examples': 43793}
I0310 00:34:03.306272 140337376896768 logging_writer.py:48] [4399] global_step=4399, preemption_count=0, score=975.061988, test/accuracy=0.983804, test/loss=0.057278, test/mean_average_precision=0.115936, test/num_examples=43793, total_duration=1412.106632, train/accuracy=0.987346, train/loss=0.045278, train/mean_average_precision=0.117032, validation/accuracy=0.984791, validation/loss=0.054245, validation/mean_average_precision=0.116378, validation/num_examples=43793
I0310 00:34:03.327863 140522088007488 checkpoints.py:356] Saving checkpoint at step: 4399
I0310 00:34:03.381956 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_4399
I0310 00:34:03.382122 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_4399.
I0310 00:34:03.838269 140337410467584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.02547469362616539, loss=0.04329005628824234
I0310 00:34:26.027978 140337368504064 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.017786582931876183, loss=0.04138307273387909
I0310 00:34:48.083030 140337410467584 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.03102138265967369, loss=0.04106287658214569
I0310 00:35:10.289134 140337368504064 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.026390478014945984, loss=0.04953417181968689
I0310 00:35:32.515223 140337410467584 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01633215695619583, loss=0.05016472935676575
I0310 00:35:54.112291 140337368504064 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.015214065089821815, loss=0.04492194578051567
I0310 00:36:15.929181 140337410467584 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.027669841423630714, loss=0.04596378281712532
I0310 00:36:37.713093 140337368504064 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.01603875681757927, loss=0.04075102135539055
I0310 00:36:59.160672 140337410467584 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.019559714943170547, loss=0.04735662415623665
I0310 00:37:20.842161 140337368504064 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0275648832321167, loss=0.0481342077255249
I0310 00:37:42.584738 140337410467584 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.015409354120492935, loss=0.04131445661187172
I0310 00:38:03.389256 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:39:14.337985 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:39:16.894595 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:39:19.393196 140522088007488 submission_runner.py:363] Time since start: 1728.12s, 	Step: 5497, 	{'train/accuracy': 0.987989068031311, 'train/loss': 0.0418923944234848, 'train/mean_average_precision': 0.15518756730899458, 'validation/accuracy': 0.9852683544158936, 'validation/loss': 0.051105041056871414, 'validation/mean_average_precision': 0.14437138067149355, 'validation/num_examples': 43793, 'test/accuracy': 0.9843412041664124, 'test/loss': 0.053774476051330566, 'test/mean_average_precision': 0.14319260514220633, 'test/num_examples': 43793}
I0310 00:39:19.400572 140337368504064 logging_writer.py:48] [5497] global_step=5497, preemption_count=0, score=1214.149690, test/accuracy=0.984341, test/loss=0.053774, test/mean_average_precision=0.143193, test/num_examples=43793, total_duration=1728.122383, train/accuracy=0.987989, train/loss=0.041892, train/mean_average_precision=0.155188, validation/accuracy=0.985268, validation/loss=0.051105, validation/mean_average_precision=0.144371, validation/num_examples=43793
I0310 00:39:19.423430 140522088007488 checkpoints.py:356] Saving checkpoint at step: 5497
I0310 00:39:19.479643 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_5497
I0310 00:39:19.479831 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_5497.
I0310 00:39:20.352026 140337410467584 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.028674496337771416, loss=0.04107142239809036
I0310 00:39:41.866978 140337360111360 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.017552388831973076, loss=0.040998172014951706
I0310 00:40:03.268002 140337410467584 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01965251937508583, loss=0.04615933820605278
I0310 00:40:24.884500 140337360111360 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01529401633888483, loss=0.042039982974529266
I0310 00:40:46.329861 140337410467584 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.018338225781917572, loss=0.04683694988489151
I0310 00:41:07.763200 140337360111360 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.017747538164258003, loss=0.04306900128722191
I0310 00:41:29.402824 140337410467584 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.03257763758301735, loss=0.04008494317531586
I0310 00:41:50.984301 140337360111360 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.020864536985754967, loss=0.04652348905801773
I0310 00:42:13.597211 140337410467584 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.022783244028687477, loss=0.04275559261441231
I0310 00:42:35.335103 140337360111360 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.023900113999843597, loss=0.04336679354310036
I0310 00:42:57.006998 140337410467584 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.020534805953502655, loss=0.03812326863408089
I0310 00:43:18.143560 140337360111360 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.022191019728779793, loss=0.04189533740282059
I0310 00:43:19.619707 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:44:29.114246 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:44:31.646025 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:44:34.119751 140522088007488 submission_runner.py:363] Time since start: 2044.35s, 	Step: 6608, 	{'train/accuracy': 0.9882456660270691, 'train/loss': 0.04050499200820923, 'train/mean_average_precision': 0.1727856297363402, 'validation/accuracy': 0.9855602383613586, 'validation/loss': 0.04969675466418266, 'validation/mean_average_precision': 0.16902526123352843, 'validation/num_examples': 43793, 'test/accuracy': 0.9845610857009888, 'test/loss': 0.05244671553373337, 'test/mean_average_precision': 0.1668487929943095, 'test/num_examples': 43793}
I0310 00:44:34.127042 140337410467584 logging_writer.py:48] [6608] global_step=6608, preemption_count=0, score=1453.382835, test/accuracy=0.984561, test/loss=0.052447, test/mean_average_precision=0.166849, test/num_examples=43793, total_duration=2044.352840, train/accuracy=0.988246, train/loss=0.040505, train/mean_average_precision=0.172786, validation/accuracy=0.985560, validation/loss=0.049697, validation/mean_average_precision=0.169025, validation/num_examples=43793
I0310 00:44:34.148975 140522088007488 checkpoints.py:356] Saving checkpoint at step: 6608
I0310 00:44:34.203186 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_6608
I0310 00:44:34.203356 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_6608.
I0310 00:44:54.086881 140337360111360 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.017027143388986588, loss=0.034247200936079025
I0310 00:45:15.577430 140337351718656 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.032943129539489746, loss=0.04251601919531822
I0310 00:45:36.996240 140337360111360 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.02059713564813137, loss=0.037733711302280426
I0310 00:45:58.105390 140337351718656 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.01534741185605526, loss=0.041867826133966446
I0310 00:46:19.507113 140337360111360 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.021928314119577408, loss=0.04148290678858757
I0310 00:46:40.744984 140337351718656 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.017740663141012192, loss=0.04206709563732147
I0310 00:47:01.963569 140337360111360 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.018814019858837128, loss=0.03908628597855568
I0310 00:47:23.272271 140337351718656 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.026383545249700546, loss=0.0387270413339138
I0310 00:47:44.549915 140337360111360 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.017340542748570442, loss=0.03727773204445839
I0310 00:48:06.059870 140337351718656 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.022354215383529663, loss=0.039509207010269165
I0310 00:48:27.530173 140337360111360 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01925870217382908, loss=0.04093608260154724
I0310 00:48:34.368242 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:49:45.773553 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:49:48.296344 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:49:50.769043 140522088007488 submission_runner.py:363] Time since start: 2359.10s, 	Step: 7733, 	{'train/accuracy': 0.9886783361434937, 'train/loss': 0.03920215368270874, 'train/mean_average_precision': 0.20398363084449, 'validation/accuracy': 0.9857465624809265, 'validation/loss': 0.04850979894399643, 'validation/mean_average_precision': 0.18766381678493252, 'validation/num_examples': 43793, 'test/accuracy': 0.9848302006721497, 'test/loss': 0.05114562809467316, 'test/mean_average_precision': 0.1864157352959772, 'test/num_examples': 43793}
I0310 00:49:50.776611 140337351718656 logging_writer.py:48] [7733] global_step=7733, preemption_count=0, score=1692.623529, test/accuracy=0.984830, test/loss=0.051146, test/mean_average_precision=0.186416, test/num_examples=43793, total_duration=2359.101372, train/accuracy=0.988678, train/loss=0.039202, train/mean_average_precision=0.203984, validation/accuracy=0.985747, validation/loss=0.048510, validation/mean_average_precision=0.187664, validation/num_examples=43793
I0310 00:49:50.800909 140522088007488 checkpoints.py:356] Saving checkpoint at step: 7733
I0310 00:49:50.856935 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_7733
I0310 00:49:50.857128 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_7733.
I0310 00:50:05.431407 140337360111360 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.028549544513225555, loss=0.04242086410522461
I0310 00:50:26.771781 140337343325952 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.01602715440094471, loss=0.04270851984620094
I0310 00:50:48.282564 140337360111360 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.019754035398364067, loss=0.039912149310112
I0310 00:51:10.083016 140337343325952 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.022372400388121605, loss=0.04316562041640282
I0310 00:51:31.885967 140337360111360 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.019916091114282608, loss=0.041765015572309494
I0310 00:51:53.692198 140337343325952 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.022056110203266144, loss=0.03914366662502289
I0310 00:52:15.629312 140337360111360 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.018933076411485672, loss=0.03686906024813652
I0310 00:52:37.211076 140337343325952 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.023307330906391144, loss=0.037516288459300995
I0310 00:52:58.771086 140337360111360 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.01776748336851597, loss=0.041247669607400894
I0310 00:53:20.266491 140337343325952 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.015078642405569553, loss=0.041520439088344574
I0310 00:53:41.800898 140337360111360 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.031667426228523254, loss=0.03729432076215744
I0310 00:53:50.938552 140522088007488 spec.py:298] Evaluating on the training split.
I0310 00:55:01.841827 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 00:55:04.371142 140522088007488 spec.py:326] Evaluating on the test split.
I0310 00:55:06.846212 140522088007488 submission_runner.py:363] Time since start: 2675.67s, 	Step: 8844, 	{'train/accuracy': 0.9889577627182007, 'train/loss': 0.03760764002799988, 'train/mean_average_precision': 0.22992603326127095, 'validation/accuracy': 0.9860250353813171, 'validation/loss': 0.04749814420938492, 'validation/mean_average_precision': 0.20023700551958615, 'validation/num_examples': 43793, 'test/accuracy': 0.9850892424583435, 'test/loss': 0.05006244406104088, 'test/mean_average_precision': 0.19719134094988466, 'test/num_examples': 43793}
I0310 00:55:06.853648 140337343325952 logging_writer.py:48] [8844] global_step=8844, preemption_count=0, score=1931.783727, test/accuracy=0.985089, test/loss=0.050062, test/mean_average_precision=0.197191, test/num_examples=43793, total_duration=2675.671675, train/accuracy=0.988958, train/loss=0.037608, train/mean_average_precision=0.229926, validation/accuracy=0.986025, validation/loss=0.047498, validation/mean_average_precision=0.200237, validation/num_examples=43793
I0310 00:55:06.875854 140522088007488 checkpoints.py:356] Saving checkpoint at step: 8844
I0310 00:55:06.934261 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_8844
I0310 00:55:06.934447 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_8844.
I0310 00:55:19.036755 140337360111360 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01489945873618126, loss=0.03880078345537186
I0310 00:55:40.278775 140337334933248 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.026828983798623085, loss=0.0400269441306591
I0310 00:56:01.657406 140337360111360 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.020826248452067375, loss=0.03873074799776077
I0310 00:56:23.324658 140337334933248 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.018778329715132713, loss=0.03927472233772278
I0310 00:56:44.608072 140337360111360 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.018146978691220284, loss=0.03751141205430031
I0310 00:57:05.963476 140337334933248 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.02606678195297718, loss=0.03703022003173828
I0310 00:57:27.857378 140337360111360 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.01666424050927162, loss=0.03910267353057861
I0310 00:57:49.712912 140337334933248 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01879487931728363, loss=0.03858844190835953
I0310 00:58:11.284407 140337360111360 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.018093938007950783, loss=0.04059593379497528
I0310 00:58:32.734648 140337334933248 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.013537776656448841, loss=0.03388241305947304
I0310 00:58:54.370111 140337360111360 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.02003963105380535, loss=0.03897700086236
I0310 00:59:07.034187 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:00:16.930355 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:00:19.398679 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:00:22.049378 140522088007488 submission_runner.py:363] Time since start: 2991.77s, 	Step: 9960, 	{'train/accuracy': 0.9891706705093384, 'train/loss': 0.03690830245614052, 'train/mean_average_precision': 0.23260336242919455, 'validation/accuracy': 0.9861687421798706, 'validation/loss': 0.046687301248311996, 'validation/mean_average_precision': 0.21369782680286115, 'validation/num_examples': 43793, 'test/accuracy': 0.9852396249771118, 'test/loss': 0.04930362105369568, 'test/mean_average_precision': 0.21115270765009342, 'test/num_examples': 43793}
I0310 01:00:22.057134 140337334933248 logging_writer.py:48] [9960] global_step=9960, preemption_count=0, score=2170.975360, test/accuracy=0.985240, test/loss=0.049304, test/mean_average_precision=0.211153, test/num_examples=43793, total_duration=2991.767316, train/accuracy=0.989171, train/loss=0.036908, train/mean_average_precision=0.232603, validation/accuracy=0.986169, validation/loss=0.046687, validation/mean_average_precision=0.213698, validation/num_examples=43793
I0310 01:00:22.080352 140522088007488 checkpoints.py:356] Saving checkpoint at step: 9960
I0310 01:00:22.132172 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_9960
I0310 01:00:22.132328 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_9960.
I0310 01:00:30.934771 140337360111360 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.021562976762652397, loss=0.03598495200276375
I0310 01:00:52.410203 140337326540544 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.019876811653375626, loss=0.03387129679322243
I0310 01:01:13.890582 140337360111360 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.016623275354504585, loss=0.03655144199728966
I0310 01:01:35.494458 140337326540544 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.026244068518280983, loss=0.03853195533156395
I0310 01:01:56.966745 140337360111360 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.01791905239224434, loss=0.03688918799161911
I0310 01:02:18.140109 140337326540544 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.015931887552142143, loss=0.034266483038663864
I0310 01:02:39.619733 140337360111360 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.017260093241930008, loss=0.036347631365060806
I0310 01:03:01.005457 140337326540544 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.01965290866792202, loss=0.04319724068045616
I0310 01:03:22.475908 140337360111360 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0165115799754858, loss=0.03509027883410454
I0310 01:03:43.735347 140337326540544 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.021601025015115738, loss=0.0404839850962162
I0310 01:04:04.853857 140337360111360 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.016900023445487022, loss=0.035036858171224594
I0310 01:04:22.264081 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:05:31.863629 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:05:34.286540 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:05:36.679910 140522088007488 submission_runner.py:363] Time since start: 3307.00s, 	Step: 11083, 	{'train/accuracy': 0.9894261360168457, 'train/loss': 0.03590495139360428, 'train/mean_average_precision': 0.27090100534404726, 'validation/accuracy': 0.9861236810684204, 'validation/loss': 0.04637667164206505, 'validation/mean_average_precision': 0.2228510590263648, 'validation/num_examples': 43793, 'test/accuracy': 0.9852451086044312, 'test/loss': 0.048730336129665375, 'test/mean_average_precision': 0.21855736703451714, 'test/num_examples': 43793}
I0310 01:05:36.687918 140337326540544 logging_writer.py:48] [11083] global_step=11083, preemption_count=0, score=2410.202110, test/accuracy=0.985245, test/loss=0.048730, test/mean_average_precision=0.218557, test/num_examples=43793, total_duration=3306.997213, train/accuracy=0.989426, train/loss=0.035905, train/mean_average_precision=0.270901, validation/accuracy=0.986124, validation/loss=0.046377, validation/mean_average_precision=0.222851, validation/num_examples=43793
I0310 01:05:36.711175 140522088007488 checkpoints.py:356] Saving checkpoint at step: 11083
I0310 01:05:36.767042 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_11083
I0310 01:05:36.767211 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_11083.
I0310 01:05:40.576288 140337360111360 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.023538755252957344, loss=0.03530923277139664
I0310 01:06:01.747022 140337318147840 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.01778222806751728, loss=0.0331367664039135
I0310 01:06:22.624583 140337360111360 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0197744220495224, loss=0.03525464981794357
I0310 01:06:43.558898 140337318147840 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.018579337745904922, loss=0.03947262838482857
I0310 01:07:04.546750 140337360111360 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.018362684175372124, loss=0.03669661656022072
I0310 01:07:25.554027 140337318147840 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.020465344190597534, loss=0.03584242984652519
I0310 01:07:46.722132 140337360111360 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.017078613862395287, loss=0.03473687916994095
I0310 01:08:07.828476 140337318147840 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.019746307283639908, loss=0.03487013280391693
I0310 01:08:28.899650 140337360111360 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.019700000062584877, loss=0.036795031279325485
I0310 01:08:49.966846 140337318147840 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02087956853210926, loss=0.04198877513408661
I0310 01:09:10.912190 140337360111360 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.022828098386526108, loss=0.041938889771699905
I0310 01:09:31.961209 140337318147840 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.01830373890697956, loss=0.036165352910757065
I0310 01:09:36.788351 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:10:44.018607 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:10:46.446852 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:10:48.802346 140522088007488 submission_runner.py:363] Time since start: 3621.52s, 	Step: 12224, 	{'train/accuracy': 0.9896247982978821, 'train/loss': 0.035026419907808304, 'train/mean_average_precision': 0.2776431724503076, 'validation/accuracy': 0.9863846898078918, 'validation/loss': 0.04553086683154106, 'validation/mean_average_precision': 0.22985622081844234, 'validation/num_examples': 43793, 'test/accuracy': 0.9855808019638062, 'test/loss': 0.04802538827061653, 'test/mean_average_precision': 0.2270139557640769, 'test/num_examples': 43793}
I0310 01:10:48.809844 140337360111360 logging_writer.py:48] [12224] global_step=12224, preemption_count=0, score=2649.328296, test/accuracy=0.985581, test/loss=0.048025, test/mean_average_precision=0.227014, test/num_examples=43793, total_duration=3621.521480, train/accuracy=0.989625, train/loss=0.035026, train/mean_average_precision=0.277643, validation/accuracy=0.986385, validation/loss=0.045531, validation/mean_average_precision=0.229856, validation/num_examples=43793
I0310 01:10:48.832425 140522088007488 checkpoints.py:356] Saving checkpoint at step: 12224
I0310 01:10:48.889120 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_12224
I0310 01:10:48.889321 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_12224.
I0310 01:11:05.238089 140337318147840 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.019888602197170258, loss=0.03554164990782738
I0310 01:11:26.489454 140337309755136 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.023353584110736847, loss=0.03568635135889053
I0310 01:11:47.583918 140337318147840 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.019181249663233757, loss=0.03294023498892784
I0310 01:12:08.582674 140337309755136 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.022404370829463005, loss=0.03961541876196861
I0310 01:12:30.034149 140337318147840 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03352553769946098, loss=0.04143868759274483
I0310 01:12:51.327886 140337309755136 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.017582077533006668, loss=0.03289273381233215
I0310 01:13:12.387701 140337318147840 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.01716727390885353, loss=0.03447898104786873
I0310 01:13:33.759881 140337309755136 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.015606090426445007, loss=0.0382966585457325
I0310 01:13:55.025896 140337318147840 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.01724512316286564, loss=0.03714621067047119
I0310 01:14:16.290831 140337309755136 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.01593254879117012, loss=0.037041161209344864
I0310 01:14:37.710962 140337318147840 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.01989428699016571, loss=0.034660037606954575
I0310 01:14:48.950476 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:15:56.502404 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:15:59.318895 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:16:02.069421 140522088007488 submission_runner.py:363] Time since start: 3933.68s, 	Step: 13353, 	{'train/accuracy': 0.9896613359451294, 'train/loss': 0.03479495644569397, 'train/mean_average_precision': 0.3005225618116755, 'validation/accuracy': 0.9863631725311279, 'validation/loss': 0.04552923142910004, 'validation/mean_average_precision': 0.23627502472612452, 'validation/num_examples': 43793, 'test/accuracy': 0.9854978322982788, 'test/loss': 0.04812765121459961, 'test/mean_average_precision': 0.2325977923699956, 'test/num_examples': 43793}
I0310 01:16:02.078145 140337309755136 logging_writer.py:48] [13353] global_step=13353, preemption_count=0, score=2888.466889, test/accuracy=0.985498, test/loss=0.048128, test/mean_average_precision=0.232598, test/num_examples=43793, total_duration=3933.683593, train/accuracy=0.989661, train/loss=0.034795, train/mean_average_precision=0.300523, validation/accuracy=0.986363, validation/loss=0.045529, validation/mean_average_precision=0.236275, validation/num_examples=43793
I0310 01:16:02.099956 140522088007488 checkpoints.py:356] Saving checkpoint at step: 13353
I0310 01:16:02.157421 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_13353
I0310 01:16:02.157633 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_13353.
I0310 01:16:12.529781 140337318147840 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02670644409954548, loss=0.03622044622898102
I0310 01:16:34.245030 140337301362432 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.017809484153985977, loss=0.03985930606722832
I0310 01:16:55.663341 140337318147840 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.020256048068404198, loss=0.03856627643108368
I0310 01:17:17.152992 140337301362432 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.01731305755674839, loss=0.03424902632832527
I0310 01:17:38.637145 140337318147840 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.015320448204874992, loss=0.03476104885339737
I0310 01:18:00.181743 140337301362432 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.017040878534317017, loss=0.0341767854988575
I0310 01:18:22.220211 140337318147840 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.02305474318563938, loss=0.038218531757593155
I0310 01:18:44.312565 140337301362432 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0189256202429533, loss=0.03473787382245064
I0310 01:19:06.220044 140337318147840 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.014423450455069542, loss=0.03477556258440018
I0310 01:19:28.160758 140337301362432 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.01610136218369007, loss=0.03808995336294174
I0310 01:19:49.601108 140337318147840 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.017770996317267418, loss=0.03446091711521149
I0310 01:20:02.169930 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:21:08.655200 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:21:11.315801 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:21:13.973063 140522088007488 submission_runner.py:363] Time since start: 4246.90s, 	Step: 14460, 	{'train/accuracy': 0.9898945093154907, 'train/loss': 0.03391683101654053, 'train/mean_average_precision': 0.3174757700792308, 'validation/accuracy': 0.9865389466285706, 'validation/loss': 0.04531919211149216, 'validation/mean_average_precision': 0.2443098595078268, 'validation/num_examples': 43793, 'test/accuracy': 0.9856911301612854, 'test/loss': 0.04803220555186272, 'test/mean_average_precision': 0.24092631547557586, 'test/num_examples': 43793}
I0310 01:21:13.982125 140337301362432 logging_writer.py:48] [14460] global_step=14460, preemption_count=0, score=3127.453481, test/accuracy=0.985691, test/loss=0.048032, test/mean_average_precision=0.240926, test/num_examples=43793, total_duration=4246.903049, train/accuracy=0.989895, train/loss=0.033917, train/mean_average_precision=0.317476, validation/accuracy=0.986539, validation/loss=0.045319, validation/mean_average_precision=0.244310, validation/num_examples=43793
I0310 01:21:14.003458 140522088007488 checkpoints.py:356] Saving checkpoint at step: 14460
I0310 01:21:14.060968 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_14460
I0310 01:21:14.061199 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_14460.
I0310 01:21:22.976625 140337318147840 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.021987419575452805, loss=0.03257052227854729
I0310 01:21:44.551662 140337217533696 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.01803877018392086, loss=0.03449666500091553
I0310 01:22:06.194942 140337318147840 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.018482742831110954, loss=0.03443540260195732
I0310 01:22:27.512156 140337217533696 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.016707085072994232, loss=0.0334918349981308
I0310 01:22:48.793834 140337318147840 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.020615270361304283, loss=0.035867899656295776
I0310 01:23:09.975772 140337217533696 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.018057988956570625, loss=0.034232307225465775
I0310 01:23:31.347000 140337318147840 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.022166401147842407, loss=0.03714454546570778
I0310 01:23:53.195234 140337217533696 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.020072955638170242, loss=0.036523934453725815
I0310 01:24:14.994071 140337318147840 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.019083445891737938, loss=0.037083860486745834
I0310 01:24:36.869010 140337217533696 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.018502607941627502, loss=0.03279046341776848
I0310 01:24:58.452613 140337318147840 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.019061168655753136, loss=0.0317847765982151
I0310 01:25:14.118795 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:26:22.817573 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:26:25.288446 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:26:27.735741 140522088007488 submission_runner.py:363] Time since start: 4558.85s, 	Step: 15573, 	{'train/accuracy': 0.9902538061141968, 'train/loss': 0.03294676914811134, 'train/mean_average_precision': 0.32236856335580866, 'validation/accuracy': 0.9866043329238892, 'validation/loss': 0.04456211254000664, 'validation/mean_average_precision': 0.24657216638144497, 'validation/num_examples': 43793, 'test/accuracy': 0.9857879877090454, 'test/loss': 0.04702179133892059, 'test/mean_average_precision': 0.24542297378205571, 'test/num_examples': 43793}
I0310 01:26:27.743457 140337217533696 logging_writer.py:48] [15573] global_step=15573, preemption_count=0, score=3366.541899, test/accuracy=0.985788, test/loss=0.047022, test/mean_average_precision=0.245423, test/num_examples=43793, total_duration=4558.851926, train/accuracy=0.990254, train/loss=0.032947, train/mean_average_precision=0.322369, validation/accuracy=0.986604, validation/loss=0.044562, validation/mean_average_precision=0.246572, validation/num_examples=43793
I0310 01:26:27.767670 140522088007488 checkpoints.py:356] Saving checkpoint at step: 15573
I0310 01:26:27.820974 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_15573
I0310 01:26:27.821135 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_15573.
I0310 01:26:33.848593 140337318147840 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.01827106438577175, loss=0.03568582236766815
I0310 01:26:55.015279 140337209140992 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.022500833496451378, loss=0.035828642547130585
I0310 01:27:16.293328 140337318147840 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.01631413772702217, loss=0.035829029977321625
I0310 01:27:37.676171 140337209140992 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.017123617231845856, loss=0.03777707368135452
I0310 01:27:58.515281 140337318147840 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.01708834432065487, loss=0.03404112160205841
I0310 01:28:19.991384 140337209140992 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.021473461762070656, loss=0.03775889798998833
I0310 01:28:41.269027 140337318147840 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.019479900598526, loss=0.03779080882668495
I0310 01:29:02.337606 140337209140992 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.01903994008898735, loss=0.0368461012840271
I0310 01:29:23.259977 140337318147840 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.01950349099934101, loss=0.0359046496450901
I0310 01:29:44.238780 140337209140992 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.019834432750940323, loss=0.03704411908984184
I0310 01:30:05.520199 140337318147840 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.020182393491268158, loss=0.03708880394697189
I0310 01:30:26.943877 140337209140992 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.023310568183660507, loss=0.03457847982645035
I0310 01:30:28.021047 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:31:33.815592 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:31:36.263894 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:31:38.655022 140522088007488 submission_runner.py:363] Time since start: 4872.75s, 	Step: 16706, 	{'train/accuracy': 0.9902787804603577, 'train/loss': 0.032544948160648346, 'train/mean_average_precision': 0.35065750071325663, 'validation/accuracy': 0.9866745471954346, 'validation/loss': 0.044370539486408234, 'validation/mean_average_precision': 0.2510461765582846, 'validation/num_examples': 43793, 'test/accuracy': 0.9857863187789917, 'test/loss': 0.047004345804452896, 'test/mean_average_precision': 0.24414154676585528, 'test/num_examples': 43793}
I0310 01:31:38.663258 140337318147840 logging_writer.py:48] [16706] global_step=16706, preemption_count=0, score=3605.860064, test/accuracy=0.985786, test/loss=0.047004, test/mean_average_precision=0.244142, test/num_examples=43793, total_duration=4872.754177, train/accuracy=0.990279, train/loss=0.032545, train/mean_average_precision=0.350658, validation/accuracy=0.986675, validation/loss=0.044371, validation/mean_average_precision=0.251046, validation/num_examples=43793
I0310 01:31:38.686594 140522088007488 checkpoints.py:356] Saving checkpoint at step: 16706
I0310 01:31:38.744522 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_16706
I0310 01:31:38.744705 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_16706.
I0310 01:31:59.252054 140337209140992 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.021080555394291878, loss=0.03545285761356354
I0310 01:32:21.074020 140337200748288 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.021601907908916473, loss=0.03534494712948799
I0310 01:32:42.740285 140337209140992 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.015816466882824898, loss=0.03346411883831024
I0310 01:33:04.084773 140337200748288 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.01947387494146824, loss=0.03472544625401497
I0310 01:33:25.290496 140337209140992 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.018182650208473206, loss=0.03227728605270386
I0310 01:33:46.615279 140337200748288 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0176959540694952, loss=0.03367682173848152
I0310 01:34:07.981691 140337209140992 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.016119956970214844, loss=0.031083110719919205
I0310 01:34:29.104935 140337200748288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.01847033016383648, loss=0.03612559661269188
I0310 01:34:50.125848 140337209140992 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0172792449593544, loss=0.030228234827518463
I0310 01:35:11.185006 140337200748288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.021159732714295387, loss=0.04052910581231117
I0310 01:35:32.273858 140337209140992 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.019870508462190628, loss=0.033173590898513794
I0310 01:35:38.844857 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:36:46.194723 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:36:48.642624 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:36:51.129118 140522088007488 submission_runner.py:363] Time since start: 5183.58s, 	Step: 17832, 	{'train/accuracy': 0.9902923107147217, 'train/loss': 0.032254334539175034, 'train/mean_average_precision': 0.3603714976815377, 'validation/accuracy': 0.9867256879806519, 'validation/loss': 0.04407187178730965, 'validation/mean_average_precision': 0.2582388030718322, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.046489518135786057, 'test/mean_average_precision': 0.2527791950513228, 'test/num_examples': 43793}
I0310 01:36:51.137305 140337200748288 logging_writer.py:48] [17832] global_step=17832, preemption_count=0, score=3845.100883, test/accuracy=0.985933, test/loss=0.046490, test/mean_average_precision=0.252779, test/num_examples=43793, total_duration=5183.577986, train/accuracy=0.990292, train/loss=0.032254, train/mean_average_precision=0.360371, validation/accuracy=0.986726, validation/loss=0.044072, validation/mean_average_precision=0.258239, validation/num_examples=43793
I0310 01:36:51.159277 140522088007488 checkpoints.py:356] Saving checkpoint at step: 17832
I0310 01:36:51.218814 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_17832
I0310 01:36:51.219010 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_17832.
I0310 01:37:05.921636 140337209140992 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.01748163439333439, loss=0.035088278353214264
I0310 01:37:27.185535 140337116870400 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.018588494509458542, loss=0.03446030244231224
I0310 01:37:48.424696 140337209140992 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.018666543066501617, loss=0.03401588276028633
I0310 01:38:09.924351 140337116870400 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.019861815497279167, loss=0.033579353243112564
I0310 01:38:31.197708 140337209140992 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.020714428275823593, loss=0.034941621124744415
I0310 01:38:52.514386 140337116870400 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0198978278785944, loss=0.03260492905974388
I0310 01:39:13.914485 140337209140992 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.01912545971572399, loss=0.03374350816011429
I0310 01:39:35.086775 140337116870400 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.017329037189483643, loss=0.03435264527797699
I0310 01:39:56.839995 140337209140992 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.020328931510448456, loss=0.032987166196107864
I0310 01:40:18.633093 140337116870400 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.01883060298860073, loss=0.033327583223581314
I0310 01:40:39.880009 140337209140992 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.01872451975941658, loss=0.03161552920937538
I0310 01:40:51.393581 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:41:56.735331 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:41:59.395654 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:42:02.009998 140522088007488 submission_runner.py:363] Time since start: 5496.13s, 	Step: 18955, 	{'train/accuracy': 0.9905962347984314, 'train/loss': 0.031303875148296356, 'train/mean_average_precision': 0.35930217710334533, 'validation/accuracy': 0.9868072867393494, 'validation/loss': 0.044214390218257904, 'validation/mean_average_precision': 0.2631376017358639, 'validation/num_examples': 43793, 'test/accuracy': 0.9859544038772583, 'test/loss': 0.0467858612537384, 'test/mean_average_precision': 0.2571986786837273, 'test/num_examples': 43793}
I0310 01:42:02.018823 140337116870400 logging_writer.py:48] [18955] global_step=18955, preemption_count=0, score=4084.399681, test/accuracy=0.985954, test/loss=0.046786, test/mean_average_precision=0.257199, test/num_examples=43793, total_duration=5496.126699, train/accuracy=0.990596, train/loss=0.031304, train/mean_average_precision=0.359302, validation/accuracy=0.986807, validation/loss=0.044214, validation/mean_average_precision=0.263138, validation/num_examples=43793
I0310 01:42:02.039783 140522088007488 checkpoints.py:356] Saving checkpoint at step: 18955
I0310 01:42:02.097339 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_18955
I0310 01:42:02.097527 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_18955.
I0310 01:42:11.963533 140337209140992 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.022203318774700165, loss=0.034597694873809814
I0310 01:42:33.296006 140337108477696 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.019252795726060867, loss=0.02860647812485695
I0310 01:42:54.336695 140337209140992 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.019195212051272392, loss=0.03273649513721466
I0310 01:43:15.218952 140337108477696 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.01895315945148468, loss=0.031108304858207703
I0310 01:43:36.280357 140337209140992 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.020082848146557808, loss=0.030558742582798004
I0310 01:43:57.175950 140337108477696 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.021283674985170364, loss=0.035428065806627274
I0310 01:44:18.244366 140337209140992 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.022876974195241928, loss=0.03327232971787453
I0310 01:44:39.445816 140337108477696 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.01868702657520771, loss=0.03126506507396698
I0310 01:45:00.418626 140337209140992 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.019841093569993973, loss=0.03460760787129402
I0310 01:45:21.315994 140337108477696 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.02133515477180481, loss=0.03477037325501442
I0310 01:45:42.480787 140337209140992 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.01891213469207287, loss=0.03578438237309456
I0310 01:46:02.280424 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:47:06.711890 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:47:09.415832 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:47:12.106636 140522088007488 submission_runner.py:363] Time since start: 5807.01s, 	Step: 20094, 	{'train/accuracy': 0.9906952381134033, 'train/loss': 0.031167756766080856, 'train/mean_average_precision': 0.370850600396595, 'validation/accuracy': 0.9867196083068848, 'validation/loss': 0.043960459530353546, 'validation/mean_average_precision': 0.2676115239743034, 'validation/num_examples': 43793, 'test/accuracy': 0.9859552383422852, 'test/loss': 0.04631413146853447, 'test/mean_average_precision': 0.25820553173124156, 'test/num_examples': 43793}
I0310 01:47:12.115611 140337108477696 logging_writer.py:48] [20094] global_step=20094, preemption_count=0, score=4323.591435, test/accuracy=0.985955, test/loss=0.046314, test/mean_average_precision=0.258206, test/num_examples=43793, total_duration=5807.013544, train/accuracy=0.990695, train/loss=0.031168, train/mean_average_precision=0.370851, validation/accuracy=0.986720, validation/loss=0.043960, validation/mean_average_precision=0.267612, validation/num_examples=43793
I0310 01:47:12.136218 140522088007488 checkpoints.py:356] Saving checkpoint at step: 20094
I0310 01:47:12.195507 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_20094
I0310 01:47:12.195710 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_20094.
I0310 01:47:13.684637 140337209140992 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.017487218603491783, loss=0.028043732047080994
I0310 01:47:34.779848 140337100084992 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.02494116872549057, loss=0.03333349898457527
I0310 01:47:55.791748 140337209140992 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.019838524982333183, loss=0.031891483813524246
I0310 01:48:16.812551 140337100084992 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.02039111964404583, loss=0.036829348653554916
I0310 01:48:37.830296 140337209140992 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.01929907500743866, loss=0.03182271495461464
I0310 01:48:58.704336 140337100084992 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.02038242295384407, loss=0.03470045328140259
I0310 01:49:19.810359 140337209140992 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.02006777562201023, loss=0.03145618736743927
I0310 01:49:41.352490 140337100084992 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.025583835318684578, loss=0.03332659229636192
I0310 01:50:02.955836 140337209140992 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.019482538104057312, loss=0.031294021755456924
I0310 01:50:24.626661 140337100084992 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.018163898959755898, loss=0.03021092154085636
I0310 01:50:45.972785 140337209140992 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.01981026493012905, loss=0.03391236439347267
I0310 01:51:07.023151 140337100084992 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.019615016877651215, loss=0.028763914480805397
I0310 01:51:12.294903 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:52:18.759561 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:52:21.413148 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:52:24.019166 140522088007488 submission_runner.py:363] Time since start: 6117.03s, 	Step: 21226, 	{'train/accuracy': 0.9909365177154541, 'train/loss': 0.03006860613822937, 'train/mean_average_precision': 0.3912124208161957, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.04380332678556442, 'validation/mean_average_precision': 0.2652267498329103, 'validation/num_examples': 43793, 'test/accuracy': 0.9860377907752991, 'test/loss': 0.04619389772415161, 'test/mean_average_precision': 0.25721364302041605, 'test/num_examples': 43793}
I0310 01:52:24.028523 140337209140992 logging_writer.py:48] [21226] global_step=21226, preemption_count=0, score=4562.695594, test/accuracy=0.986038, test/loss=0.046194, test/mean_average_precision=0.257214, test/num_examples=43793, total_duration=6117.028023, train/accuracy=0.990937, train/loss=0.030069, train/mean_average_precision=0.391212, validation/accuracy=0.986821, validation/loss=0.043803, validation/mean_average_precision=0.265227, validation/num_examples=43793
I0310 01:52:24.049098 140522088007488 checkpoints.py:356] Saving checkpoint at step: 21226
I0310 01:52:24.107548 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_21226
I0310 01:52:24.107750 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_21226.
I0310 01:52:40.187044 140337100084992 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.01825055480003357, loss=0.030923834070563316
I0310 01:53:01.339281 140337091692288 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.019781898707151413, loss=0.03390127047896385
I0310 01:53:22.830522 140337100084992 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.02205538935959339, loss=0.033783603459596634
I0310 01:53:44.272294 140337091692288 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.02562035247683525, loss=0.035098832100629807
I0310 01:54:05.877168 140337100084992 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.02263844758272171, loss=0.03669503703713417
I0310 01:54:27.518817 140337091692288 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.020394058898091316, loss=0.03259915113449097
I0310 01:54:48.806487 140337100084992 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.021442675963044167, loss=0.03396649286150932
I0310 01:55:10.112062 140337091692288 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.02278333343565464, loss=0.03329915925860405
I0310 01:55:31.519538 140337100084992 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.01943589746952057, loss=0.03109443187713623
I0310 01:55:53.383649 140337091692288 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.020144391804933548, loss=0.03289933130145073
I0310 01:56:15.252059 140337100084992 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.018242962658405304, loss=0.029273120686411858
I0310 01:56:24.200724 140522088007488 spec.py:298] Evaluating on the training split.
I0310 01:57:31.583901 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 01:57:34.087686 140522088007488 spec.py:326] Evaluating on the test split.
I0310 01:57:36.493883 140522088007488 submission_runner.py:363] Time since start: 6428.93s, 	Step: 22342, 	{'train/accuracy': 0.9909611940383911, 'train/loss': 0.030134335160255432, 'train/mean_average_precision': 0.3989052899207177, 'validation/accuracy': 0.9869120121002197, 'validation/loss': 0.04364078491926193, 'validation/mean_average_precision': 0.27210255099594965, 'validation/num_examples': 43793, 'test/accuracy': 0.9860946536064148, 'test/loss': 0.04601095989346504, 'test/mean_average_precision': 0.2599206296922674, 'test/num_examples': 43793}
I0310 01:57:36.502280 140337091692288 logging_writer.py:48] [22342] global_step=22342, preemption_count=0, score=4801.843662, test/accuracy=0.986095, test/loss=0.046011, test/mean_average_precision=0.259921, test/num_examples=43793, total_duration=6428.933857, train/accuracy=0.990961, train/loss=0.030134, train/mean_average_precision=0.398905, validation/accuracy=0.986912, validation/loss=0.043641, validation/mean_average_precision=0.272103, validation/num_examples=43793
I0310 01:57:36.524150 140522088007488 checkpoints.py:356] Saving checkpoint at step: 22342
I0310 01:57:36.579480 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_22342
I0310 01:57:36.579697 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_22342.
I0310 01:57:49.365546 140337100084992 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.018474625423550606, loss=0.03179347142577171
I0310 01:58:10.943652 140337083299584 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.02433081902563572, loss=0.031966373324394226
I0310 01:58:32.484983 140337100084992 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.022162027657032013, loss=0.030041350051760674
I0310 01:58:53.874534 140337083299584 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.01952524483203888, loss=0.03350767493247986
I0310 01:59:15.409440 140337100084992 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.021090753376483917, loss=0.032815080136060715
I0310 01:59:36.734547 140337083299584 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.0206650011241436, loss=0.03369517624378204
I0310 01:59:58.130213 140337100084992 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.018809961155056953, loss=0.029200388118624687
I0310 02:00:19.639326 140337083299584 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.02055283822119236, loss=0.030972037464380264
I0310 02:00:41.422644 140337100084992 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.020882152020931244, loss=0.03177546709775925
I0310 02:01:03.093463 140337083299584 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.02038446255028248, loss=0.0328483022749424
I0310 02:01:24.790002 140337100084992 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.018293045461177826, loss=0.030825886875391006
I0310 02:01:36.625695 140522088007488 spec.py:298] Evaluating on the training split.
I0310 02:02:41.820725 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 02:02:44.255013 140522088007488 spec.py:326] Evaluating on the test split.
I0310 02:02:46.675664 140522088007488 submission_runner.py:363] Time since start: 6741.36s, 	Step: 23456, 	{'train/accuracy': 0.9911778569221497, 'train/loss': 0.029499530792236328, 'train/mean_average_precision': 0.41706115056901044, 'validation/accuracy': 0.9868897199630737, 'validation/loss': 0.04353258013725281, 'validation/mean_average_precision': 0.276975914283694, 'validation/num_examples': 43793, 'test/accuracy': 0.9860786199569702, 'test/loss': 0.046179041266441345, 'test/mean_average_precision': 0.2605527562204906, 'test/num_examples': 43793}
I0310 02:02:46.684197 140337083299584 logging_writer.py:48] [23456] global_step=23456, preemption_count=0, score=5041.016554, test/accuracy=0.986079, test/loss=0.046179, test/mean_average_precision=0.260553, test/num_examples=43793, total_duration=6741.358829, train/accuracy=0.991178, train/loss=0.029500, train/mean_average_precision=0.417061, validation/accuracy=0.986890, validation/loss=0.043533, validation/mean_average_precision=0.276976, validation/num_examples=43793
I0310 02:02:46.706021 140522088007488 checkpoints.py:356] Saving checkpoint at step: 23456
I0310 02:02:46.767893 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_23456
I0310 02:02:46.768109 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_23456.
I0310 02:02:56.369233 140337100084992 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.020231852307915688, loss=0.029104916378855705
I0310 02:03:17.663323 140337074906880 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.020194442942738533, loss=0.027050232514739037
I0310 02:03:39.051526 140337100084992 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.01988462544977665, loss=0.031752970069646835
I0310 02:04:00.445125 140337074906880 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.02629285864531994, loss=0.03677191957831383
I0310 02:04:22.271865 140337100084992 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.021468091756105423, loss=0.029996007680892944
I0310 02:04:43.706970 140337074906880 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.021613137796521187, loss=0.027698548510670662
I0310 02:05:05.262452 140337100084992 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.018694916740059853, loss=0.029089348390698433
I0310 02:05:26.636950 140337074906880 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.020801598206162453, loss=0.03191109001636505
I0310 02:05:47.708063 140337100084992 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.019917819648981094, loss=0.03255883976817131
I0310 02:06:08.770679 140337074906880 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.02075582928955555, loss=0.031172916293144226
I0310 02:06:29.912603 140337100084992 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.023717431351542473, loss=0.033885929733514786
I0310 02:06:46.852714 140522088007488 spec.py:298] Evaluating on the training split.
I0310 02:07:50.058018 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 02:07:52.535897 140522088007488 spec.py:326] Evaluating on the test split.
I0310 02:07:54.956269 140522088007488 submission_runner.py:363] Time since start: 7051.59s, 	Step: 24582, 	{'train/accuracy': 0.9911675453186035, 'train/loss': 0.02918705716729164, 'train/mean_average_precision': 0.41569746505184557, 'validation/accuracy': 0.9870402812957764, 'validation/loss': 0.04383406788110733, 'validation/mean_average_precision': 0.28026512409292204, 'validation/num_examples': 43793, 'test/accuracy': 0.986156165599823, 'test/loss': 0.046506091952323914, 'test/mean_average_precision': 0.26547195893494, 'test/num_examples': 43793}
I0310 02:07:54.964889 140337074906880 logging_writer.py:48] [24582] global_step=24582, preemption_count=0, score=5280.216640, test/accuracy=0.986156, test/loss=0.046506, test/mean_average_precision=0.265472, test/num_examples=43793, total_duration=7051.585840, train/accuracy=0.991168, train/loss=0.029187, train/mean_average_precision=0.415697, validation/accuracy=0.987040, validation/loss=0.043834, validation/mean_average_precision=0.280265, validation/num_examples=43793
I0310 02:07:54.986913 140522088007488 checkpoints.py:356] Saving checkpoint at step: 24582
I0310 02:07:55.048091 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_24582
I0310 02:07:55.048275 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_24582.
I0310 02:07:59.023425 140337100084992 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.018548274412751198, loss=0.030078502371907234
I0310 02:08:19.883616 140337066514176 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.01978086307644844, loss=0.030065929517149925
I0310 02:08:40.976839 140337100084992 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.022532057017087936, loss=0.03334549069404602
I0310 02:09:02.347143 140337066514176 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.021229781210422516, loss=0.031471509486436844
I0310 02:09:23.992483 140337100084992 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.02376522682607174, loss=0.02989346534013748
I0310 02:09:45.351240 140337066514176 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.021365569904446602, loss=0.030508125200867653
I0310 02:10:06.691917 140337100084992 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.020511861890554428, loss=0.029176250100135803
I0310 02:10:28.293601 140337066514176 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.01998787932097912, loss=0.030658502131700516
I0310 02:10:49.690565 140337100084992 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.019214550033211708, loss=0.031450144946575165
I0310 02:11:10.427037 140337066514176 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.025500264018774033, loss=0.03352981433272362
I0310 02:11:31.523339 140337100084992 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.017682483419775963, loss=0.02850515767931938
I0310 02:11:52.519544 140337066514176 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.02194385416805744, loss=0.030608298256993294
I0310 02:11:55.067625 140522088007488 spec.py:298] Evaluating on the training split.
I0310 02:13:01.504671 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 02:13:04.019197 140522088007488 spec.py:326] Evaluating on the test split.
I0310 02:13:06.415617 140522088007488 submission_runner.py:363] Time since start: 7359.80s, 	Step: 25713, 	{'train/accuracy': 0.9912753105163574, 'train/loss': 0.028984393924474716, 'train/mean_average_precision': 0.4172110831502971, 'validation/accuracy': 0.9869668483734131, 'validation/loss': 0.04378751665353775, 'validation/mean_average_precision': 0.27607520994616963, 'validation/num_examples': 43793, 'test/accuracy': 0.9860453605651855, 'test/loss': 0.046627502888441086, 'test/mean_average_precision': 0.2661244551847657, 'test/num_examples': 43793}
I0310 02:13:06.424155 140337100084992 logging_writer.py:48] [25713] global_step=25713, preemption_count=0, score=5519.340153, test/accuracy=0.986045, test/loss=0.046628, test/mean_average_precision=0.266124, test/num_examples=43793, total_duration=7359.800756, train/accuracy=0.991275, train/loss=0.028984, train/mean_average_precision=0.417211, validation/accuracy=0.986967, validation/loss=0.043788, validation/mean_average_precision=0.276075, validation/num_examples=43793
I0310 02:13:06.448374 140522088007488 checkpoints.py:356] Saving checkpoint at step: 25713
I0310 02:13:06.509608 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_25713
I0310 02:13:06.509791 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_25713.
I0310 02:13:25.241961 140337066514176 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.02366003394126892, loss=0.03310706093907356
I0310 02:13:46.692003 140336982587136 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.02184179238975048, loss=0.030728718265891075
I0310 02:14:08.332391 140337066514176 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.017247969284653664, loss=0.026898643001914024
I0310 02:14:29.706041 140336982587136 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.01877228170633316, loss=0.02653980813920498
I0310 02:14:51.085604 140337066514176 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.02270808257162571, loss=0.030408402904868126
I0310 02:15:12.560652 140336982587136 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.01966872252523899, loss=0.03223844990134239
I0310 02:15:33.844730 140337066514176 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.024836964905261993, loss=0.03696781024336815
I0310 02:15:55.141245 140336982587136 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0200409647077322, loss=0.026511110365390778
I0310 02:16:16.284399 140337066514176 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.024702271446585655, loss=0.03310934826731682
I0310 02:16:37.167250 140336982587136 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.018548250198364258, loss=0.02707047574222088
I0310 02:16:58.231884 140337066514176 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.02583424746990204, loss=0.03326738625764847
I0310 02:17:06.515431 140522088007488 spec.py:298] Evaluating on the training split.
I0310 02:18:11.579697 140522088007488 spec.py:310] Evaluating on the validation split.
I0310 02:18:14.043695 140522088007488 spec.py:326] Evaluating on the test split.
I0310 02:18:16.435143 140522088007488 submission_runner.py:363] Time since start: 7671.25s, 	Step: 26840, 	{'train/accuracy': 0.9916506409645081, 'train/loss': 0.02761257439851761, 'train/mean_average_precision': 0.4510568778540773, 'validation/accuracy': 0.9869716763496399, 'validation/loss': 0.04368821159005165, 'validation/mean_average_precision': 0.282706351350225, 'validation/num_examples': 43793, 'test/accuracy': 0.9861493706703186, 'test/loss': 0.04636182636022568, 'test/mean_average_precision': 0.2709392056527052, 'test/num_examples': 43793}
I0310 02:18:16.444343 140336982587136 logging_writer.py:48] [26840] global_step=26840, preemption_count=0, score=5758.457790, test/accuracy=0.986149, test/loss=0.046362, test/mean_average_precision=0.270939, test/num_examples=43793, total_duration=7671.248559, train/accuracy=0.991651, train/loss=0.027613, train/mean_average_precision=0.451057, validation/accuracy=0.986972, validation/loss=0.043688, validation/mean_average_precision=0.282706, validation/num_examples=43793
I0310 02:18:16.466349 140522088007488 checkpoints.py:356] Saving checkpoint at step: 26840
I0310 02:18:16.525155 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_26840
I0310 02:18:16.525340 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_26840.
I0310 02:18:16.533197 140337066514176 logging_writer.py:48] [26840] global_step=26840, preemption_count=0, score=5758.457790
I0310 02:18:16.550768 140522088007488 checkpoints.py:356] Saving checkpoint at step: 26840
I0310 02:18:16.651856 140522088007488 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_26840
I0310 02:18:16.652027 140522088007488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/ogbg_jax/trial_1/checkpoint_26840.
I0310 02:18:16.769581 140522088007488 submission_runner.py:524] Tuning trial 1/1
I0310 02:18:16.769814 140522088007488 submission_runner.py:525] Hyperparameters: Hyperparameters(learning_rate=2.4917728606918423, beta1=0.9449369031171744, warmup_steps=3000, decay_steps_factor=0.861509027839639, end_factor=0.001, weight_decay=1.2859640541025928e-07)
I0310 02:18:16.770885 140522088007488 submission_runner.py:526] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4633101522922516, 'train/loss': 0.7533521056175232, 'train/mean_average_precision': 0.020762612813650893, 'validation/accuracy': 0.47774508595466614, 'validation/loss': 0.7442635893821716, 'validation/mean_average_precision': 0.02582417221414534, 'validation/num_examples': 43793, 'test/accuracy': 0.48069873452186584, 'test/loss': 0.7438132762908936, 'test/mean_average_precision': 0.02632104148489224, 'test/num_examples': 43793, 'score': 18.280787467956543, 'total_duration': 18.3584463596344, 'global_step': 1, 'preemption_count': 0}), (1113, {'train/accuracy': 0.9866687059402466, 'train/loss': 0.05564166232943535, 'train/mean_average_precision': 0.030714027414759357, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06562034040689468, 'validation/mean_average_precision': 0.030828926366209247, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06906113773584366, 'test/mean_average_precision': 0.03229104538075053, 'test/num_examples': 43793, 'score': 257.488098859787, 'total_duration': 468.49748969078064, 'global_step': 1113, 'preemption_count': 0}), (2199, {'train/accuracy': 0.9868823885917664, 'train/loss': 0.05244118720293045, 'train/mean_average_precision': 0.04272859091010647, 'validation/accuracy': 0.9841358065605164, 'validation/loss': 0.0627637505531311, 'validation/mean_average_precision': 0.045199726204732824, 'validation/num_examples': 43793, 'test/accuracy': 0.9831463098526001, 'test/loss': 0.06622272729873657, 'test/mean_average_precision': 0.046553446653382013, 'test/num_examples': 43793, 'score': 496.71477723121643, 'total_duration': 782.1238567829132, 'global_step': 2199, 'preemption_count': 0}), (3293, {'train/accuracy': 0.987008810043335, 'train/loss': 0.04882008209824562, 'train/mean_average_precision': 0.07940899341472557, 'validation/accuracy': 0.9843103289604187, 'validation/loss': 0.05884339287877083, 'validation/mean_average_precision': 0.0756762533598784, 'validation/num_examples': 43793, 'test/accuracy': 0.9833114147186279, 'test/loss': 0.06225157156586647, 'test/mean_average_precision': 0.07848469650188986, 'test/num_examples': 43793, 'score': 735.8751149177551, 'total_duration': 1096.834289073944, 'global_step': 3293, 'preemption_count': 0}), (4399, {'train/accuracy': 0.9873459935188293, 'train/loss': 0.045277658849954605, 'train/mean_average_precision': 0.11703214719667794, 'validation/accuracy': 0.984790563583374, 'validation/loss': 0.05424490198493004, 'validation/mean_average_precision': 0.11637773352645944, 'validation/num_examples': 43793, 'test/accuracy': 0.9838037490844727, 'test/loss': 0.05727805942296982, 'test/mean_average_precision': 0.1159356758880539, 'test/num_examples': 43793, 'score': 975.0619878768921, 'total_duration': 1412.1066315174103, 'global_step': 4399, 'preemption_count': 0}), (5497, {'train/accuracy': 0.987989068031311, 'train/loss': 0.0418923944234848, 'train/mean_average_precision': 0.15518756730899458, 'validation/accuracy': 0.9852683544158936, 'validation/loss': 0.051105041056871414, 'validation/mean_average_precision': 0.14437138067149355, 'validation/num_examples': 43793, 'test/accuracy': 0.9843412041664124, 'test/loss': 0.053774476051330566, 'test/mean_average_precision': 0.14319260514220633, 'test/num_examples': 43793, 'score': 1214.1496896743774, 'total_duration': 1728.1223833560944, 'global_step': 5497, 'preemption_count': 0}), (6608, {'train/accuracy': 0.9882456660270691, 'train/loss': 0.04050499200820923, 'train/mean_average_precision': 0.1727856297363402, 'validation/accuracy': 0.9855602383613586, 'validation/loss': 0.04969675466418266, 'validation/mean_average_precision': 0.16902526123352843, 'validation/num_examples': 43793, 'test/accuracy': 0.9845610857009888, 'test/loss': 0.05244671553373337, 'test/mean_average_precision': 0.1668487929943095, 'test/num_examples': 43793, 'score': 1453.3828353881836, 'total_duration': 2044.3528397083282, 'global_step': 6608, 'preemption_count': 0}), (7733, {'train/accuracy': 0.9886783361434937, 'train/loss': 0.03920215368270874, 'train/mean_average_precision': 0.20398363084449, 'validation/accuracy': 0.9857465624809265, 'validation/loss': 0.04850979894399643, 'validation/mean_average_precision': 0.18766381678493252, 'validation/num_examples': 43793, 'test/accuracy': 0.9848302006721497, 'test/loss': 0.05114562809467316, 'test/mean_average_precision': 0.1864157352959772, 'test/num_examples': 43793, 'score': 1692.6235291957855, 'total_duration': 2359.101371526718, 'global_step': 7733, 'preemption_count': 0}), (8844, {'train/accuracy': 0.9889577627182007, 'train/loss': 0.03760764002799988, 'train/mean_average_precision': 0.22992603326127095, 'validation/accuracy': 0.9860250353813171, 'validation/loss': 0.04749814420938492, 'validation/mean_average_precision': 0.20023700551958615, 'validation/num_examples': 43793, 'test/accuracy': 0.9850892424583435, 'test/loss': 0.05006244406104088, 'test/mean_average_precision': 0.19719134094988466, 'test/num_examples': 43793, 'score': 1931.7837274074554, 'total_duration': 2675.6716747283936, 'global_step': 8844, 'preemption_count': 0}), (9960, {'train/accuracy': 0.9891706705093384, 'train/loss': 0.03690830245614052, 'train/mean_average_precision': 0.23260336242919455, 'validation/accuracy': 0.9861687421798706, 'validation/loss': 0.046687301248311996, 'validation/mean_average_precision': 0.21369782680286115, 'validation/num_examples': 43793, 'test/accuracy': 0.9852396249771118, 'test/loss': 0.04930362105369568, 'test/mean_average_precision': 0.21115270765009342, 'test/num_examples': 43793, 'score': 2170.9753596782684, 'total_duration': 2991.7673156261444, 'global_step': 9960, 'preemption_count': 0}), (11083, {'train/accuracy': 0.9894261360168457, 'train/loss': 0.03590495139360428, 'train/mean_average_precision': 0.27090100534404726, 'validation/accuracy': 0.9861236810684204, 'validation/loss': 0.04637667164206505, 'validation/mean_average_precision': 0.2228510590263648, 'validation/num_examples': 43793, 'test/accuracy': 0.9852451086044312, 'test/loss': 0.048730336129665375, 'test/mean_average_precision': 0.21855736703451714, 'test/num_examples': 43793, 'score': 2410.2021102905273, 'total_duration': 3306.9972126483917, 'global_step': 11083, 'preemption_count': 0}), (12224, {'train/accuracy': 0.9896247982978821, 'train/loss': 0.035026419907808304, 'train/mean_average_precision': 0.2776431724503076, 'validation/accuracy': 0.9863846898078918, 'validation/loss': 0.04553086683154106, 'validation/mean_average_precision': 0.22985622081844234, 'validation/num_examples': 43793, 'test/accuracy': 0.9855808019638062, 'test/loss': 0.04802538827061653, 'test/mean_average_precision': 0.2270139557640769, 'test/num_examples': 43793, 'score': 2649.3282957077026, 'total_duration': 3621.521479845047, 'global_step': 12224, 'preemption_count': 0}), (13353, {'train/accuracy': 0.9896613359451294, 'train/loss': 0.03479495644569397, 'train/mean_average_precision': 0.3005225618116755, 'validation/accuracy': 0.9863631725311279, 'validation/loss': 0.04552923142910004, 'validation/mean_average_precision': 0.23627502472612452, 'validation/num_examples': 43793, 'test/accuracy': 0.9854978322982788, 'test/loss': 0.04812765121459961, 'test/mean_average_precision': 0.2325977923699956, 'test/num_examples': 43793, 'score': 2888.466888666153, 'total_duration': 3933.6835927963257, 'global_step': 13353, 'preemption_count': 0}), (14460, {'train/accuracy': 0.9898945093154907, 'train/loss': 0.03391683101654053, 'train/mean_average_precision': 0.3174757700792308, 'validation/accuracy': 0.9865389466285706, 'validation/loss': 0.04531919211149216, 'validation/mean_average_precision': 0.2443098595078268, 'validation/num_examples': 43793, 'test/accuracy': 0.9856911301612854, 'test/loss': 0.04803220555186272, 'test/mean_average_precision': 0.24092631547557586, 'test/num_examples': 43793, 'score': 3127.45348072052, 'total_duration': 4246.903049230576, 'global_step': 14460, 'preemption_count': 0}), (15573, {'train/accuracy': 0.9902538061141968, 'train/loss': 0.03294676914811134, 'train/mean_average_precision': 0.32236856335580866, 'validation/accuracy': 0.9866043329238892, 'validation/loss': 0.04456211254000664, 'validation/mean_average_precision': 0.24657216638144497, 'validation/num_examples': 43793, 'test/accuracy': 0.9857879877090454, 'test/loss': 0.04702179133892059, 'test/mean_average_precision': 0.24542297378205571, 'test/num_examples': 43793, 'score': 3366.5418994426727, 'total_duration': 4558.851925611496, 'global_step': 15573, 'preemption_count': 0}), (16706, {'train/accuracy': 0.9902787804603577, 'train/loss': 0.032544948160648346, 'train/mean_average_precision': 0.35065750071325663, 'validation/accuracy': 0.9866745471954346, 'validation/loss': 0.044370539486408234, 'validation/mean_average_precision': 0.2510461765582846, 'validation/num_examples': 43793, 'test/accuracy': 0.9857863187789917, 'test/loss': 0.047004345804452896, 'test/mean_average_precision': 0.24414154676585528, 'test/num_examples': 43793, 'score': 3605.860063791275, 'total_duration': 4872.754176855087, 'global_step': 16706, 'preemption_count': 0}), (17832, {'train/accuracy': 0.9902923107147217, 'train/loss': 0.032254334539175034, 'train/mean_average_precision': 0.3603714976815377, 'validation/accuracy': 0.9867256879806519, 'validation/loss': 0.04407187178730965, 'validation/mean_average_precision': 0.2582388030718322, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.046489518135786057, 'test/mean_average_precision': 0.2527791950513228, 'test/num_examples': 43793, 'score': 3845.1008825302124, 'total_duration': 5183.577986001968, 'global_step': 17832, 'preemption_count': 0}), (18955, {'train/accuracy': 0.9905962347984314, 'train/loss': 0.031303875148296356, 'train/mean_average_precision': 0.35930217710334533, 'validation/accuracy': 0.9868072867393494, 'validation/loss': 0.044214390218257904, 'validation/mean_average_precision': 0.2631376017358639, 'validation/num_examples': 43793, 'test/accuracy': 0.9859544038772583, 'test/loss': 0.0467858612537384, 'test/mean_average_precision': 0.2571986786837273, 'test/num_examples': 43793, 'score': 4084.3996810913086, 'total_duration': 5496.126699209213, 'global_step': 18955, 'preemption_count': 0}), (20094, {'train/accuracy': 0.9906952381134033, 'train/loss': 0.031167756766080856, 'train/mean_average_precision': 0.370850600396595, 'validation/accuracy': 0.9867196083068848, 'validation/loss': 0.043960459530353546, 'validation/mean_average_precision': 0.2676115239743034, 'validation/num_examples': 43793, 'test/accuracy': 0.9859552383422852, 'test/loss': 0.04631413146853447, 'test/mean_average_precision': 0.25820553173124156, 'test/num_examples': 43793, 'score': 4323.591435432434, 'total_duration': 5807.013543605804, 'global_step': 20094, 'preemption_count': 0}), (21226, {'train/accuracy': 0.9909365177154541, 'train/loss': 0.03006860613822937, 'train/mean_average_precision': 0.3912124208161957, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.04380332678556442, 'validation/mean_average_precision': 0.2652267498329103, 'validation/num_examples': 43793, 'test/accuracy': 0.9860377907752991, 'test/loss': 0.04619389772415161, 'test/mean_average_precision': 0.25721364302041605, 'test/num_examples': 43793, 'score': 4562.695594072342, 'total_duration': 6117.028023481369, 'global_step': 21226, 'preemption_count': 0}), (22342, {'train/accuracy': 0.9909611940383911, 'train/loss': 0.030134335160255432, 'train/mean_average_precision': 0.3989052899207177, 'validation/accuracy': 0.9869120121002197, 'validation/loss': 0.04364078491926193, 'validation/mean_average_precision': 0.27210255099594965, 'validation/num_examples': 43793, 'test/accuracy': 0.9860946536064148, 'test/loss': 0.04601095989346504, 'test/mean_average_precision': 0.2599206296922674, 'test/num_examples': 43793, 'score': 4801.843661785126, 'total_duration': 6428.933856725693, 'global_step': 22342, 'preemption_count': 0}), (23456, {'train/accuracy': 0.9911778569221497, 'train/loss': 0.029499530792236328, 'train/mean_average_precision': 0.41706115056901044, 'validation/accuracy': 0.9868897199630737, 'validation/loss': 0.04353258013725281, 'validation/mean_average_precision': 0.276975914283694, 'validation/num_examples': 43793, 'test/accuracy': 0.9860786199569702, 'test/loss': 0.046179041266441345, 'test/mean_average_precision': 0.2605527562204906, 'test/num_examples': 43793, 'score': 5041.016553878784, 'total_duration': 6741.358828544617, 'global_step': 23456, 'preemption_count': 0}), (24582, {'train/accuracy': 0.9911675453186035, 'train/loss': 0.02918705716729164, 'train/mean_average_precision': 0.41569746505184557, 'validation/accuracy': 0.9870402812957764, 'validation/loss': 0.04383406788110733, 'validation/mean_average_precision': 0.28026512409292204, 'validation/num_examples': 43793, 'test/accuracy': 0.986156165599823, 'test/loss': 0.046506091952323914, 'test/mean_average_precision': 0.26547195893494, 'test/num_examples': 43793, 'score': 5280.216639757156, 'total_duration': 7051.58584022522, 'global_step': 24582, 'preemption_count': 0}), (25713, {'train/accuracy': 0.9912753105163574, 'train/loss': 0.028984393924474716, 'train/mean_average_precision': 0.4172110831502971, 'validation/accuracy': 0.9869668483734131, 'validation/loss': 0.04378751665353775, 'validation/mean_average_precision': 0.27607520994616963, 'validation/num_examples': 43793, 'test/accuracy': 0.9860453605651855, 'test/loss': 0.046627502888441086, 'test/mean_average_precision': 0.2661244551847657, 'test/num_examples': 43793, 'score': 5519.340153455734, 'total_duration': 7359.800756216049, 'global_step': 25713, 'preemption_count': 0}), (26840, {'train/accuracy': 0.9916506409645081, 'train/loss': 0.02761257439851761, 'train/mean_average_precision': 0.4510568778540773, 'validation/accuracy': 0.9869716763496399, 'validation/loss': 0.04368821159005165, 'validation/mean_average_precision': 0.282706351350225, 'validation/num_examples': 43793, 'test/accuracy': 0.9861493706703186, 'test/loss': 0.04636182636022568, 'test/mean_average_precision': 0.2709392056527052, 'test/num_examples': 43793, 'score': 5758.4577896595, 'total_duration': 7671.248559474945, 'global_step': 26840, 'preemption_count': 0})], 'global_step': 26840}
I0310 02:18:16.771018 140522088007488 submission_runner.py:527] Timing: 5758.4577896595
I0310 02:18:16.771075 140522088007488 submission_runner.py:528] ====================
I0310 02:18:16.771204 140522088007488 submission_runner.py:587] Finalogbg score: 5758.4577896595
