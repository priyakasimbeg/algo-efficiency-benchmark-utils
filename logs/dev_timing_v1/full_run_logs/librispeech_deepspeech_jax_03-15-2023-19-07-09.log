I0315 19:07:28.197858 139695971202880 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/librispeech_deepspeech_jax.
I0315 19:07:28.269124 139695971202880 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0315 19:07:29.200175 139695971202880 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0315 19:07:29.200847 139695971202880 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0315 19:07:29.205047 139695971202880 submission_runner.py:484] Using RNG seed 2067100496
I0315 19:07:31.838582 139695971202880 submission_runner.py:493] --- Tuning run 1/1 ---
I0315 19:07:31.838782 139695971202880 submission_runner.py:498] Creating tuning directory at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1.
I0315 19:07:31.838942 139695971202880 logger_utils.py:84] Saving hparams to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/hparams.json.
I0315 19:07:31.962683 139695971202880 submission_runner.py:230] Initializing dataset.
I0315 19:07:31.962870 139695971202880 submission_runner.py:237] Initializing model.
I0315 19:07:48.143496 139695971202880 submission_runner.py:247] Initializing optimizer.
I0315 19:07:48.798565 139695971202880 submission_runner.py:254] Initializing metrics bundle.
I0315 19:07:48.798768 139695971202880 submission_runner.py:268] Initializing checkpoint and logger.
I0315 19:07:48.799715 139695971202880 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0315 19:07:48.799968 139695971202880 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0315 19:07:48.800028 139695971202880 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:07:49.954860 139695971202880 submission_runner.py:289] Saving meta data to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0315 19:07:49.955757 139695971202880 submission_runner.py:292] Saving flags to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0315 19:07:49.963194 139695971202880 submission_runner.py:302] Starting training loop.
I0315 19:07:50.165833 139695971202880 input_pipeline.py:20] Loading split = train-clean-100
I0315 19:07:50.204574 139695971202880 input_pipeline.py:20] Loading split = train-clean-360
I0315 19:07:50.526204 139695971202880 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0315 19:08:46.634402 139519637735168 logging_writer.py:48] [0] global_step=0, grad_norm=25.01940155029297, loss=32.92803955078125
I0315 19:08:46.660218 139695971202880 spec.py:298] Evaluating on the training split.
I0315 19:08:46.781054 139695971202880 input_pipeline.py:20] Loading split = train-clean-100
I0315 19:08:46.807084 139695971202880 input_pipeline.py:20] Loading split = train-clean-360
I0315 19:08:47.083615 139695971202880 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0315 19:10:19.015485 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 19:10:19.102942 139695971202880 input_pipeline.py:20] Loading split = dev-clean
I0315 19:10:19.107933 139695971202880 input_pipeline.py:20] Loading split = dev-other
I0315 19:11:14.725054 139695971202880 spec.py:326] Evaluating on the test split.
I0315 19:11:14.811522 139695971202880 input_pipeline.py:20] Loading split = test-clean
I0315 19:11:50.981688 139695971202880 submission_runner.py:362] Time since start: 56.70s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.193344, dtype=float32), 'train/wer': 4.242477562834475, 'validation/ctc_loss': DeviceArray(30.926653, dtype=float32), 'validation/wer': 3.8057675423786046, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.063675, dtype=float32), 'test/wer': 4.079215160563037, 'test/num_examples': 2472}
I0315 19:11:51.003227 139517197539072 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=56.494421, test/ctc_loss=31.063674926757812, test/num_examples=2472, test/wer=4.079215, total_duration=56.696979, train/ctc_loss=32.19334411621094, train/wer=4.242478, validation/ctc_loss=30.926652908325195, validation/num_examples=5348, validation/wer=3.805768
I0315 19:11:51.141905 139695971202880 checkpoints.py:356] Saving checkpoint at step: 1
I0315 19:11:51.740232 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0315 19:11:51.751230 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0315 19:14:06.049349 139520142026496 logging_writer.py:48] [100] global_step=100, grad_norm=3.2148375511169434, loss=5.857160568237305
I0315 19:16:03.753951 139520150419200 logging_writer.py:48] [200] global_step=200, grad_norm=2.3333840370178223, loss=4.969588279724121
I0315 19:17:59.871632 139520142026496 logging_writer.py:48] [300] global_step=300, grad_norm=4.104928493499756, loss=3.9835805892944336
I0315 19:19:55.993338 139520150419200 logging_writer.py:48] [400] global_step=400, grad_norm=1.715502381324768, loss=3.4238247871398926
I0315 19:21:52.209831 139520142026496 logging_writer.py:48] [500] global_step=500, grad_norm=2.074615478515625, loss=3.1364223957061768
I0315 19:23:48.964362 139520150419200 logging_writer.py:48] [600] global_step=600, grad_norm=2.1699891090393066, loss=2.999274730682373
I0315 19:25:45.287527 139520142026496 logging_writer.py:48] [700] global_step=700, grad_norm=2.91167950630188, loss=2.8239846229553223
I0315 19:27:41.611766 139520150419200 logging_writer.py:48] [800] global_step=800, grad_norm=2.86948561668396, loss=2.75160551071167
I0315 19:29:36.964626 139520142026496 logging_writer.py:48] [900] global_step=900, grad_norm=2.2253615856170654, loss=2.6741394996643066
I0315 19:31:28.993158 139520150419200 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.4106621742248535, loss=2.5888140201568604
I0315 19:33:25.855216 139520301487872 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.110445737838745, loss=2.5442745685577393
I0315 19:35:18.342615 139520293095168 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.1236813068389893, loss=2.476769208908081
I0315 19:37:11.492162 139520301487872 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.649139404296875, loss=2.476832866668701
I0315 19:39:05.296448 139520293095168 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.57016658782959, loss=2.4441847801208496
I0315 19:40:57.372551 139520301487872 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.3679749965667725, loss=2.310607671737671
I0315 19:42:52.754469 139520293095168 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.9486942291259766, loss=2.312483549118042
I0315 19:44:47.467210 139520301487872 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.2963545322418213, loss=2.2534916400909424
I0315 19:46:39.358808 139520293095168 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.4819717407226562, loss=2.2806789875030518
I0315 19:48:31.190546 139520301487872 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.9356741905212402, loss=2.304575204849243
I0315 19:50:23.744838 139520293095168 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.3272199630737305, loss=2.261934280395508
I0315 19:51:52.882586 139695971202880 spec.py:298] Evaluating on the training split.
I0315 19:52:31.833374 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 19:53:10.823753 139695971202880 spec.py:326] Evaluating on the test split.
I0315 19:53:31.236578 139695971202880 submission_runner.py:362] Time since start: 2642.92s, 	Step: 2077, 	{'train/ctc_loss': DeviceArray(0.81215405, dtype=float32), 'train/wer': 0.25568474387757206, 'validation/ctc_loss': DeviceArray(1.2603388, dtype=float32), 'validation/wer': 0.34052426940925623, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8674421, dtype=float32), 'test/wer': 0.26736132269006563, 'test/num_examples': 2472}
I0315 19:53:31.254260 139519510431488 logging_writer.py:48] [2077] global_step=2077, preemption_count=0, score=2453.403180, test/ctc_loss=0.8674420714378357, test/num_examples=2472, test/wer=0.267361, total_duration=2642.919138, train/ctc_loss=0.8121540546417236, train/wer=0.255685, validation/ctc_loss=1.2603387832641602, validation/num_examples=5348, validation/wer=0.340524
I0315 19:53:31.420297 139695971202880 checkpoints.py:356] Saving checkpoint at step: 2077
I0315 19:53:32.104931 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_2077
I0315 19:53:32.120266 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_2077.
I0315 19:53:59.088925 139518824937216 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.9104373455047607, loss=2.2008161544799805
I0315 19:55:53.978257 139517952521984 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.04768967628479, loss=2.1891071796417236
I0315 19:57:49.488589 139518824937216 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.113842725753784, loss=2.1662747859954834
I0315 19:59:46.643501 139517952521984 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.2732298374176025, loss=2.191580295562744
I0315 20:01:41.000340 139518824937216 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.5570223331451416, loss=2.222768783569336
I0315 20:03:35.022030 139517952521984 logging_writer.py:48] [2600] global_step=2600, grad_norm=8.180794715881348, loss=2.1140120029449463
I0315 20:05:27.671292 139518824937216 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.078606605529785, loss=2.0349934101104736
I0315 20:07:20.727963 139517952521984 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.522939443588257, loss=2.111966609954834
I0315 20:09:14.084972 139518824937216 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.685937881469727, loss=2.035839080810547
I0315 20:11:05.944416 139517952521984 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.621274471282959, loss=2.0698070526123047
I0315 20:13:01.712344 139521612207872 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.6419591903686523, loss=2.0959410667419434
I0315 20:14:54.523097 139521603815168 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.667435646057129, loss=2.034574508666992
I0315 20:16:48.731192 139521612207872 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.4119105339050293, loss=2.099669933319092
I0315 20:18:40.875019 139521603815168 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.7756175994873047, loss=2.0046231746673584
I0315 20:20:32.857197 139521612207872 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.1587462425231934, loss=2.038423776626587
I0315 20:22:25.393514 139521603815168 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.8011977672576904, loss=2.0848214626312256
I0315 20:24:17.307196 139521612207872 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.903682231903076, loss=1.9708669185638428
I0315 20:26:10.669558 139521603815168 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.3107171058654785, loss=2.1104722023010254
I0315 20:28:06.669994 139521612207872 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.826162099838257, loss=2.003741502761841
I0315 20:30:02.544761 139521603815168 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.744401454925537, loss=2.0054454803466797
I0315 20:31:56.479929 139521612207872 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.034714698791504, loss=2.022416114807129
I0315 20:33:33.080054 139695971202880 spec.py:298] Evaluating on the training split.
I0315 20:34:11.708575 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 20:34:51.879157 139695971202880 spec.py:326] Evaluating on the test split.
I0315 20:35:12.394666 139695971202880 submission_runner.py:362] Time since start: 5143.12s, 	Step: 4182, 	{'train/ctc_loss': DeviceArray(0.6369123, dtype=float32), 'train/wer': 0.2090829182537357, 'validation/ctc_loss': DeviceArray(1.0139186, dtype=float32), 'validation/wer': 0.28464336366004495, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.66698223, dtype=float32), 'test/wer': 0.21302784717567486, 'test/num_examples': 2472}
I0315 20:35:12.415704 139519717807872 logging_writer.py:48] [4182] global_step=4182, preemption_count=0, score=4849.879686, test/ctc_loss=0.6669822335243225, test/num_examples=2472, test/wer=0.213028, total_duration=5143.116613, train/ctc_loss=0.6369122862815857, train/wer=0.209083, validation/ctc_loss=1.0139186382293701, validation/num_examples=5348, validation/wer=0.284643
I0315 20:35:12.574760 139695971202880 checkpoints.py:356] Saving checkpoint at step: 4182
I0315 20:35:13.237172 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_4182
I0315 20:35:13.252930 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_4182.
I0315 20:35:34.529778 139519709415168 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.529280662536621, loss=1.9504809379577637
I0315 20:37:30.831121 139519659058944 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.473074197769165, loss=1.994526743888855
I0315 20:39:22.556479 139519709415168 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.98078989982605, loss=1.9376400709152222
I0315 20:41:14.533233 139519659058944 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.4908998012542725, loss=2.0729522705078125
I0315 20:43:09.828360 139519709415168 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.4556567668914795, loss=2.0379645824432373
I0315 20:45:06.788475 139519659058944 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.334232807159424, loss=1.9358675479888916
I0315 20:47:02.414422 139519709415168 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.1999351978302, loss=2.0206470489501953
I0315 20:48:57.642808 139519659058944 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.569286584854126, loss=1.9648094177246094
I0315 20:50:50.153326 139519709415168 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.9698314666748047, loss=1.9014708995819092
I0315 20:52:42.146744 139519659058944 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.756136655807495, loss=1.9387571811676025
I0315 20:54:38.487957 139519717807872 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.1785943508148193, loss=1.9857527017593384
I0315 20:56:30.337373 139519709415168 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.1134562492370605, loss=1.97249174118042
I0315 20:58:23.181099 139519717807872 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.1334168910980225, loss=1.9736336469650269
I0315 21:00:17.085686 139519709415168 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.6667821407318115, loss=1.9869955778121948
I0315 21:02:10.883663 139519717807872 logging_writer.py:48] [5600] global_step=5600, grad_norm=5.102224349975586, loss=2.017463445663452
I0315 21:04:08.243075 139519709415168 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.901534080505371, loss=1.987351417541504
I0315 21:06:05.456588 139519717807872 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.6782617568969727, loss=1.9829862117767334
I0315 21:08:00.971454 139519709415168 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.761830806732178, loss=1.960654854774475
I0315 21:09:52.923181 139519717807872 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.319674015045166, loss=1.9549940824508667
I0315 21:11:45.287261 139519709415168 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.5801825523376465, loss=1.9783285856246948
I0315 21:13:40.820325 139521612207872 logging_writer.py:48] [6200] global_step=6200, grad_norm=4.713772296905518, loss=1.989220142364502
I0315 21:15:13.535499 139695971202880 spec.py:298] Evaluating on the training split.
I0315 21:15:52.826625 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 21:16:32.831896 139695971202880 spec.py:326] Evaluating on the test split.
I0315 21:16:53.974994 139695971202880 submission_runner.py:362] Time since start: 7643.57s, 	Step: 6284, 	{'train/ctc_loss': DeviceArray(0.5552566, dtype=float32), 'train/wer': 0.1862833467748312, 'validation/ctc_loss': DeviceArray(0.9586521, dtype=float32), 'validation/wer': 0.26954432748989376, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6149973, dtype=float32), 'test/wer': 0.1948286718258079, 'test/num_examples': 2472}
I0315 21:16:53.996541 139521612207872 logging_writer.py:48] [6284] global_step=6284, preemption_count=0, score=7245.587885, test/ctc_loss=0.6149973273277283, test/num_examples=2472, test/wer=0.194829, total_duration=7643.572221, train/ctc_loss=0.5552566051483154, train/wer=0.186283, validation/ctc_loss=0.9586520791053772, validation/num_examples=5348, validation/wer=0.269544
I0315 21:16:54.161469 139695971202880 checkpoints.py:356] Saving checkpoint at step: 6284
I0315 21:16:54.834256 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_6284
I0315 21:16:54.849856 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_6284.
I0315 21:17:13.883245 139521603815168 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.30643630027771, loss=1.9646457433700562
I0315 21:19:06.697539 139520203626240 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.5732548236846924, loss=1.9873148202896118
I0315 21:21:00.383826 139521603815168 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.646967649459839, loss=1.9264518022537231
I0315 21:22:54.644531 139520203626240 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.3887741565704346, loss=2.05088210105896
I0315 21:24:46.679995 139521603815168 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.8531229496002197, loss=1.989022970199585
I0315 21:26:38.443484 139520203626240 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.5230705738067627, loss=1.964202880859375
I0315 21:28:31.392464 139521603815168 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.0770671367645264, loss=1.967187523841858
I0315 21:30:25.829385 139520203626240 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.651437282562256, loss=1.9672820568084717
I0315 21:32:22.975556 139521603815168 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.271916151046753, loss=2.0212435722351074
I0315 21:34:19.696946 139520203626240 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.5674657821655273, loss=2.0160763263702393
I0315 21:36:18.163883 139521612207872 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.994525671005249, loss=1.9298795461654663
I0315 21:38:10.036561 139521603815168 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.5204498767852783, loss=1.9432895183563232
I0315 21:40:01.890272 139521612207872 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.5482747554779053, loss=1.9277050495147705
I0315 21:41:53.776101 139521603815168 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.269690990447998, loss=2.0358335971832275
I0315 21:43:47.689338 139521612207872 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.438957691192627, loss=2.0225682258605957
I0315 21:45:44.896307 139521603815168 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.809718370437622, loss=1.905245304107666
I0315 21:47:42.034625 139521612207872 logging_writer.py:48] [7900] global_step=7900, grad_norm=4.264460563659668, loss=1.9319171905517578
I0315 21:49:39.097131 139521603815168 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.5972723960876465, loss=1.9795526266098022
I0315 21:51:35.040227 139521612207872 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.4487528800964355, loss=1.8793251514434814
I0315 21:53:31.304688 139521603815168 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.743182897567749, loss=1.9195857048034668
I0315 21:55:28.269306 139521612207872 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.262584686279297, loss=2.108166456222534
I0315 21:56:55.247919 139695971202880 spec.py:298] Evaluating on the training split.
I0315 21:57:34.604888 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 21:58:15.280430 139695971202880 spec.py:326] Evaluating on the test split.
I0315 21:58:35.807269 139695971202880 submission_runner.py:362] Time since start: 10145.28s, 	Step: 8378, 	{'train/ctc_loss': DeviceArray(0.59614384, dtype=float32), 'train/wer': 0.19433652043690897, 'validation/ctc_loss': DeviceArray(0.965135, dtype=float32), 'validation/wer': 0.2709239838300418, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.630256, dtype=float32), 'test/wer': 0.2011861962504824, 'test/num_examples': 2472}
I0315 21:58:35.827035 139521612207872 logging_writer.py:48] [8378] global_step=8378, preemption_count=0, score=9641.359872, test/ctc_loss=0.6302559971809387, test/num_examples=2472, test/wer=0.201186, total_duration=10145.284652, train/ctc_loss=0.5961438417434692, train/wer=0.194337, validation/ctc_loss=0.9651349782943726, validation/num_examples=5348, validation/wer=0.270924
I0315 21:58:35.983072 139695971202880 checkpoints.py:356] Saving checkpoint at step: 8378
I0315 21:58:36.674460 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_8378
I0315 21:58:36.689986 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_8378.
I0315 21:59:02.433494 139521603815168 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.3803722858428955, loss=1.9664020538330078
I0315 22:00:54.341575 139520625313536 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.4474499225616455, loss=2.0054073333740234
I0315 22:02:48.193028 139521603815168 logging_writer.py:48] [8600] global_step=8600, grad_norm=4.178215980529785, loss=2.0912275314331055
I0315 22:04:40.223283 139520625313536 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.3468892574310303, loss=1.9532667398452759
I0315 22:06:32.244842 139521603815168 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.004176139831543, loss=1.8960545063018799
I0315 22:08:24.498447 139520625313536 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.619767904281616, loss=1.9360512495040894
I0315 22:10:17.526077 139521603815168 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.937412738800049, loss=1.917493224143982
I0315 22:12:09.383129 139520625313536 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.509192705154419, loss=1.915484070777893
I0315 22:14:04.448464 139521603815168 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.725459575653076, loss=1.9446598291397095
I0315 22:16:04.703039 139521612207872 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.891737222671509, loss=1.8769586086273193
I0315 22:17:59.794217 139521603815168 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.9493978023529053, loss=1.8721190690994263
I0315 22:19:51.893909 139521612207872 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.9194111824035645, loss=1.949855089187622
I0315 22:21:44.782320 139521603815168 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.4344053268432617, loss=1.862413763999939
I0315 22:23:39.978428 139521612207872 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.643648624420166, loss=1.8942837715148926
I0315 22:25:34.080892 139521603815168 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.8846418857574463, loss=1.936081886291504
I0315 22:27:30.561494 139521612207872 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.3048694133758545, loss=1.8491227626800537
I0315 22:29:22.826197 139521603815168 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.0117363929748535, loss=1.8387162685394287
I0315 22:31:15.780324 139521612207872 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.6256120204925537, loss=1.9079476594924927
I0315 22:33:08.233368 139521603815168 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.3843283653259277, loss=1.8948556184768677
I0315 22:35:06.672975 139521612207872 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.731574058532715, loss=1.8849267959594727
I0315 22:37:00.722938 139521603815168 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.135265588760376, loss=1.889047384262085
I0315 22:38:36.754231 139695971202880 spec.py:298] Evaluating on the training split.
I0315 22:39:15.549989 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 22:39:55.141387 139695971202880 spec.py:326] Evaluating on the test split.
I0315 22:40:15.926050 139695971202880 submission_runner.py:362] Time since start: 12646.79s, 	Step: 10487, 	{'train/ctc_loss': DeviceArray(0.545344, dtype=float32), 'train/wer': 0.1800702341864359, 'validation/ctc_loss': DeviceArray(0.8962344, dtype=float32), 'validation/wer': 0.25514959140946847, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5680764, dtype=float32), 'test/wer': 0.18111835557451303, 'test/num_examples': 2472}
I0315 22:40:15.947434 139521612207872 logging_writer.py:48] [10487] global_step=10487, preemption_count=0, score=12036.813678, test/ctc_loss=0.5680763721466064, test/num_examples=2472, test/wer=0.181118, total_duration=12646.790967, train/ctc_loss=0.5453439950942993, train/wer=0.180070, validation/ctc_loss=0.896234393119812, validation/num_examples=5348, validation/wer=0.255150
I0315 22:40:16.105731 139695971202880 checkpoints.py:356] Saving checkpoint at step: 10487
I0315 22:40:16.771537 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_10487
I0315 22:40:16.787286 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_10487.
I0315 22:40:32.460309 139521603815168 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.709686756134033, loss=1.890203833580017
I0315 22:42:27.564179 139521528280832 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.0407187938690186, loss=1.9182486534118652
I0315 22:44:22.831621 139521603815168 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.3463690280914307, loss=1.8855088949203491
I0315 22:46:17.801550 139521528280832 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.5382180213928223, loss=1.8699918985366821
I0315 22:48:09.646723 139521603815168 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.679511547088623, loss=1.8666168451309204
I0315 22:50:01.511370 139521528280832 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.7902779579162598, loss=1.808965802192688
I0315 22:51:53.465853 139521603815168 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.2891623973846436, loss=1.932713508605957
I0315 22:53:45.532986 139521528280832 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.700091600418091, loss=1.8552049398422241
I0315 22:55:37.582484 139521603815168 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.815450429916382, loss=1.930388331413269
I0315 22:57:33.467033 139520741807872 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.5446553230285645, loss=1.871638298034668
I0315 22:59:25.503930 139520733415168 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.8671836853027344, loss=1.8559752702713013
I0315 23:01:17.753425 139520741807872 logging_writer.py:48] [11600] global_step=11600, grad_norm=7.085200786590576, loss=1.8886961936950684
I0315 23:03:10.139476 139520733415168 logging_writer.py:48] [11700] global_step=11700, grad_norm=6.005854606628418, loss=1.863923192024231
I0315 23:05:02.528448 139520741807872 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.3072309494018555, loss=1.9021522998809814
I0315 23:06:55.556097 139520733415168 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.603038787841797, loss=1.869531273841858
I0315 23:08:47.745987 139520741807872 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.421676158905029, loss=1.8114616870880127
I0315 23:10:40.228885 139520733415168 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.3602375984191895, loss=1.8745437860488892
I0315 23:12:33.517178 139520741807872 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.244157791137695, loss=1.8482496738433838
I0315 23:14:25.614021 139520733415168 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.1499691009521484, loss=1.8211958408355713
I0315 23:16:21.243959 139521612207872 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.508021831512451, loss=1.8626296520233154
I0315 23:18:13.527105 139521603815168 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.4743361473083496, loss=1.850799322128296
I0315 23:20:05.764338 139521612207872 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.135400772094727, loss=1.8478989601135254
I0315 23:20:16.816920 139695971202880 spec.py:298] Evaluating on the training split.
I0315 23:20:55.876152 139695971202880 spec.py:310] Evaluating on the validation split.
I0315 23:21:35.738775 139695971202880 spec.py:326] Evaluating on the test split.
I0315 23:21:56.766791 139695971202880 submission_runner.py:362] Time since start: 15146.85s, 	Step: 12611, 	{'train/ctc_loss': DeviceArray(0.5447605, dtype=float32), 'train/wer': 0.17707027436503406, 'validation/ctc_loss': DeviceArray(0.8583534, dtype=float32), 'validation/wer': 0.2454630531891287, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.54895985, dtype=float32), 'test/wer': 0.17579672171104746, 'test/num_examples': 2472}
I0315 23:21:56.786791 139521612207872 logging_writer.py:48] [12611] global_step=12611, preemption_count=0, score=14432.291597, test/ctc_loss=0.5489598512649536, test/num_examples=2472, test/wer=0.175797, total_duration=15146.853664, train/ctc_loss=0.544760525226593, train/wer=0.177070, validation/ctc_loss=0.8583533763885498, validation/num_examples=5348, validation/wer=0.245463
I0315 23:21:56.949144 139695971202880 checkpoints.py:356] Saving checkpoint at step: 12611
I0315 23:21:57.642986 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_12611
I0315 23:21:57.658590 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_12611.
I0315 23:23:38.747417 139521603815168 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.7211716175079346, loss=1.854472041130066
I0315 23:25:30.747774 139520900368128 logging_writer.py:48] [12800] global_step=12800, grad_norm=6.075159072875977, loss=1.8766471147537231
I0315 23:27:22.763515 139521603815168 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.978228807449341, loss=1.8985857963562012
I0315 23:29:17.967529 139520900368128 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.290564775466919, loss=1.9098929166793823
I0315 23:31:13.400942 139521603815168 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.4675228595733643, loss=1.8684263229370117
I0315 23:33:10.024012 139520900368128 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.4865458011627197, loss=1.8391923904418945
I0315 23:35:06.541233 139521603815168 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.5982868671417236, loss=1.9299383163452148
I0315 23:37:02.336333 139521612207872 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.252089023590088, loss=1.8513811826705933
I0315 23:38:54.333138 139521603815168 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.184467315673828, loss=1.9287211894989014
I0315 23:40:46.342792 139521612207872 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.451488494873047, loss=1.9051105976104736
I0315 23:42:38.350517 139521603815168 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.656111717224121, loss=1.876556396484375
I0315 23:44:30.275735 139521612207872 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.4982619285583496, loss=1.8283157348632812
I0315 23:46:22.193294 139521603815168 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.174816131591797, loss=1.9064452648162842
I0315 23:48:14.118637 139521612207872 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.4476962089538574, loss=1.877859115600586
I0315 23:50:05.934897 139521603815168 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.141507863998413, loss=1.9705209732055664
I0315 23:51:57.813747 139521612207872 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.339870452880859, loss=1.8130565881729126
I0315 23:53:49.729305 139521603815168 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.584608554840088, loss=1.859351634979248
I0315 23:55:41.971683 139521612207872 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.9077157974243164, loss=1.9001961946487427
I0315 23:57:38.135390 139521612207872 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.388664722442627, loss=1.849679708480835
I0315 23:59:34.653038 139521603815168 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.11922287940979, loss=1.8340991735458374
I0316 00:01:31.519945 139521612207872 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.5407469272613525, loss=1.8531562089920044
I0316 00:01:58.302349 139695971202880 spec.py:298] Evaluating on the training split.
I0316 00:02:37.930408 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 00:03:17.801298 139695971202880 spec.py:326] Evaluating on the test split.
I0316 00:03:38.550536 139695971202880 submission_runner.py:362] Time since start: 17648.34s, 	Step: 14724, 	{'train/ctc_loss': DeviceArray(0.52372175, dtype=float32), 'train/wer': 0.17187267351583604, 'validation/ctc_loss': DeviceArray(0.85709625, dtype=float32), 'validation/wer': 0.24445966675993014, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.55053353, dtype=float32), 'test/wer': 0.17384681006641886, 'test/num_examples': 2472}
I0316 00:03:38.570053 139521612207872 logging_writer.py:48] [14724] global_step=14724, preemption_count=0, score=16828.270415, test/ctc_loss=0.5505335330963135, test/num_examples=2472, test/wer=0.173847, total_duration=17648.339059, train/ctc_loss=0.5237217545509338, train/wer=0.171873, validation/ctc_loss=0.857096254825592, validation/num_examples=5348, validation/wer=0.244460
I0316 00:03:38.713347 139695971202880 checkpoints.py:356] Saving checkpoint at step: 14724
I0316 00:03:39.399617 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_14724
I0316 00:03:39.415100 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_14724.
I0316 00:05:10.449833 139521603815168 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.469841003417969, loss=1.8308849334716797
I0316 00:07:04.498756 139519727466240 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.575368881225586, loss=1.7779335975646973
I0316 00:08:58.555824 139521603815168 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.806159019470215, loss=1.926754117012024
I0316 00:10:50.340272 139519727466240 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.40608549118042, loss=1.8813761472702026
I0316 00:12:42.108567 139521603815168 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.5659313201904297, loss=1.870365023612976
I0316 00:14:34.065625 139519727466240 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.015103340148926, loss=1.9223790168762207
I0316 00:16:25.952703 139521603815168 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.7410340309143066, loss=1.93254816532135
I0316 00:18:21.618656 139521612207872 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.4782941341400146, loss=1.8050236701965332
I0316 00:20:16.210850 139521603815168 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.097686529159546, loss=1.9046812057495117
I0316 00:22:11.913761 139521612207872 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.362431764602661, loss=1.8077023029327393
I0316 00:24:07.866765 139521603815168 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.083868503570557, loss=1.8537771701812744
I0316 00:26:03.598897 139521612207872 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.3253071308135986, loss=1.8397170305252075
I0316 00:27:55.670377 139521603815168 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.9641988277435303, loss=1.980178713798523
I0316 00:29:50.075123 139521612207872 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.5300300121307373, loss=1.8535943031311035
I0316 00:31:42.371697 139521603815168 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.249606609344482, loss=1.8560329675674438
I0316 00:33:34.695745 139521612207872 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.2980010509490967, loss=1.8667734861373901
I0316 00:35:26.561164 139521603815168 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.8736457824707031, loss=1.8452861309051514
I0316 00:37:22.011398 139521612207872 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.778887748718262, loss=1.8291690349578857
I0316 00:39:14.112705 139521603815168 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.8933095932006836, loss=1.9220647811889648
I0316 00:41:06.313566 139521612207872 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.395950794219971, loss=1.8701097965240479
I0316 00:42:59.251690 139521603815168 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.624929428100586, loss=1.8812958002090454
I0316 00:43:40.452995 139695971202880 spec.py:298] Evaluating on the training split.
I0316 00:44:19.826551 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 00:44:59.905899 139695971202880 spec.py:326] Evaluating on the test split.
I0316 00:45:20.056433 139695971202880 submission_runner.py:362] Time since start: 20150.49s, 	Step: 16838, 	{'train/ctc_loss': DeviceArray(0.48307234, dtype=float32), 'train/wer': 0.16286954939025974, 'validation/ctc_loss': DeviceArray(0.85676265, dtype=float32), 'validation/wer': 0.24295458711613233, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.54435754, dtype=float32), 'test/wer': 0.17238437633294743, 'test/num_examples': 2472}
I0316 00:45:20.077074 139521182127872 logging_writer.py:48] [16838] global_step=16838, preemption_count=0, score=19224.702161, test/ctc_loss=0.5443575382232666, test/num_examples=2472, test/wer=0.172384, total_duration=20150.489717, train/ctc_loss=0.48307234048843384, train/wer=0.162870, validation/ctc_loss=0.8567626476287842, validation/num_examples=5348, validation/wer=0.242955
I0316 00:45:20.223489 139695971202880 checkpoints.py:356] Saving checkpoint at step: 16838
I0316 00:45:20.929244 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_16838
I0316 00:45:20.944759 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_16838.
I0316 00:46:33.630247 139521173735168 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.464733839035034, loss=1.9149982929229736
I0316 00:48:25.485208 139521073022720 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.176067352294922, loss=1.838696837425232
I0316 00:50:18.188756 139521173735168 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.1484949588775635, loss=1.96700119972229
I0316 00:52:11.057197 139521073022720 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.2479586601257324, loss=1.874242901802063
I0316 00:54:04.649554 139521173735168 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.165848255157471, loss=1.856729507446289
I0316 00:55:58.257042 139521073022720 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.855013847351074, loss=1.832788109779358
I0316 00:57:51.271375 139521173735168 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.400075435638428, loss=1.7982066869735718
I0316 00:59:51.111480 139521182127872 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.712494134902954, loss=1.8158056735992432
I0316 01:01:47.835641 139521173735168 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.540956974029541, loss=1.8915389776229858
I0316 01:03:41.056343 139521182127872 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.8681561946868896, loss=1.8360906839370728
I0316 01:05:34.089971 139521173735168 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.549785614013672, loss=1.8509727716445923
I0316 01:07:30.279175 139521182127872 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.728950023651123, loss=1.8439667224884033
I0316 01:09:23.948698 139521173735168 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.9489028453826904, loss=1.9376542568206787
I0316 01:11:19.949047 139521182127872 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.081254005432129, loss=1.8199888467788696
I0316 01:13:15.370613 139521173735168 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.304218292236328, loss=1.8483341932296753
I0316 01:15:08.394882 139521182127872 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.5481319427490234, loss=1.8316502571105957
I0316 01:17:00.765488 139521173735168 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.523320198059082, loss=1.8524419069290161
I0316 01:18:58.255532 139521182127872 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.536485195159912, loss=1.783932089805603
I0316 01:20:52.194451 139521173735168 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.125453472137451, loss=1.8524471521377563
I0316 01:22:44.010524 139521182127872 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.436239719390869, loss=1.8693199157714844
I0316 01:24:36.814626 139521173735168 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.8951871395111084, loss=1.8435999155044556
I0316 01:25:21.321682 139695971202880 spec.py:298] Evaluating on the training split.
I0316 01:26:01.049516 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 01:26:39.994024 139695971202880 spec.py:326] Evaluating on the test split.
I0316 01:27:00.184247 139695971202880 submission_runner.py:362] Time since start: 22651.36s, 	Step: 18941, 	{'train/ctc_loss': DeviceArray(0.45573974, dtype=float32), 'train/wer': 0.14906713341029637, 'validation/ctc_loss': DeviceArray(0.82808894, dtype=float32), 'validation/wer': 0.2351301025576706, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.52317816, dtype=float32), 'test/wer': 0.1670830540491134, 'test/num_examples': 2472}
I0316 01:27:00.205508 139521612207872 logging_writer.py:48] [18941] global_step=18941, preemption_count=0, score=21620.542850, test/ctc_loss=0.5231781601905823, test/num_examples=2472, test/wer=0.167083, total_duration=22651.358413, train/ctc_loss=0.45573973655700684, train/wer=0.149067, validation/ctc_loss=0.8280889391899109, validation/num_examples=5348, validation/wer=0.235130
I0316 01:27:00.357016 139695971202880 checkpoints.py:356] Saving checkpoint at step: 18941
I0316 01:27:01.056586 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_18941
I0316 01:27:01.072282 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_18941.
I0316 01:28:08.948217 139521603815168 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.45974588394165, loss=1.8471068143844604
I0316 01:30:03.249480 139521494710016 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.485661268234253, loss=1.8576234579086304
I0316 01:31:56.763661 139521603815168 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.3931357860565186, loss=1.8237615823745728
I0316 01:33:49.968282 139521494710016 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.4833321571350098, loss=1.911413550376892
I0316 01:35:42.292583 139521603815168 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.5459251403808594, loss=1.8167343139648438
I0316 01:37:36.566709 139521494710016 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.7374916076660156, loss=1.8031429052352905
I0316 01:39:33.120830 139521612207872 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.782848834991455, loss=1.7935285568237305
I0316 01:41:24.919700 139521603815168 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.347602605819702, loss=1.836677074432373
I0316 01:43:16.830098 139521612207872 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.271834135055542, loss=1.8557621240615845
I0316 01:45:08.792464 139521603815168 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.906264305114746, loss=1.7822535037994385
I0316 01:47:00.569746 139521612207872 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.8237087726593018, loss=1.8003393411636353
I0316 01:48:54.009720 139521603815168 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.411482334136963, loss=1.8311777114868164
I0316 01:50:49.212127 139521612207872 logging_writer.py:48] [20200] global_step=20200, grad_norm=5.91371488571167, loss=1.8550522327423096
I0316 01:52:42.198709 139521603815168 logging_writer.py:48] [20300] global_step=20300, grad_norm=4.141627311706543, loss=1.812604308128357
I0316 01:54:34.926616 139521612207872 logging_writer.py:48] [20400] global_step=20400, grad_norm=7.328909873962402, loss=1.8534129858016968
I0316 01:56:29.829818 139521603815168 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.1531155109405518, loss=1.8669419288635254
I0316 01:58:25.399090 139521612207872 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.0933873653411865, loss=1.805421233177185
I0316 02:00:17.458818 139521603815168 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.5265321731567383, loss=1.838283896446228
I0316 02:02:10.174289 139521612207872 logging_writer.py:48] [20800] global_step=20800, grad_norm=6.603464126586914, loss=1.8701412677764893
I0316 02:04:02.545500 139521603815168 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.410682678222656, loss=1.829957127571106
I0316 02:05:55.397157 139521612207872 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.186396837234497, loss=1.830877423286438
I0316 02:07:01.158895 139695971202880 spec.py:298] Evaluating on the training split.
I0316 02:07:40.672113 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 02:08:20.579218 139695971202880 spec.py:326] Evaluating on the test split.
I0316 02:08:40.933581 139695971202880 submission_runner.py:362] Time since start: 25151.20s, 	Step: 21060, 	{'train/ctc_loss': DeviceArray(0.48157805, dtype=float32), 'train/wer': 0.16443123984404748, 'validation/ctc_loss': DeviceArray(0.82588047, dtype=float32), 'validation/wer': 0.2345608737180291, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5237341, dtype=float32), 'test/wer': 0.16822050250848009, 'test/num_examples': 2472}
I0316 02:08:40.958209 139521612207872 logging_writer.py:48] [21060] global_step=21060, preemption_count=0, score=24016.081519, test/ctc_loss=0.5237340927124023, test/num_examples=2472, test/wer=0.168221, total_duration=25151.195614, train/ctc_loss=0.4815780520439148, train/wer=0.164431, validation/ctc_loss=0.8258804678916931, validation/num_examples=5348, validation/wer=0.234561
I0316 02:08:41.110544 139695971202880 checkpoints.py:356] Saving checkpoint at step: 21060
I0316 02:08:41.869893 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_21060
I0316 02:08:41.885743 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_21060.
I0316 02:09:27.688794 139521603815168 logging_writer.py:48] [21100] global_step=21100, grad_norm=9.808208465576172, loss=1.8592824935913086
I0316 02:11:19.514058 139521158637312 logging_writer.py:48] [21200] global_step=21200, grad_norm=8.140361785888672, loss=1.9162229299545288
I0316 02:13:11.333359 139521603815168 logging_writer.py:48] [21300] global_step=21300, grad_norm=5.179472923278809, loss=1.84080970287323
I0316 02:15:03.197049 139521158637312 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.8020641803741455, loss=1.8092795610427856
I0316 02:16:56.348695 139521603815168 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.6501786708831787, loss=1.8651251792907715
I0316 02:18:50.779206 139521158637312 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.259171485900879, loss=1.7599543333053589
I0316 02:20:51.560307 139521612207872 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.6448564529418945, loss=1.7984148263931274
I0316 02:22:48.604253 139521603815168 logging_writer.py:48] [21800] global_step=21800, grad_norm=6.950938701629639, loss=1.7237662076950073
I0316 02:24:45.445639 139521612207872 logging_writer.py:48] [21900] global_step=21900, grad_norm=4.457364559173584, loss=1.905045747756958
I0316 02:26:42.458679 139521603815168 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.4942593574523926, loss=1.831242561340332
I0316 02:28:39.448565 139521612207872 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.353060483932495, loss=1.9198064804077148
I0316 02:30:36.657802 139521603815168 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.791843295097351, loss=1.8956729173660278
I0316 02:32:33.836608 139521612207872 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.520413637161255, loss=1.8506473302841187
I0316 02:34:30.924949 139521603815168 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.7530174255371094, loss=1.8377618789672852
I0316 02:36:27.730525 139521612207872 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.508676290512085, loss=1.8694063425064087
I0316 02:38:25.040465 139521603815168 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.081341505050659, loss=1.8552742004394531
I0316 02:40:25.083517 139521612207872 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.415898084640503, loss=1.838912844657898
I0316 02:42:21.073956 139521603815168 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.1685104370117188, loss=1.8027963638305664
I0316 02:44:16.683704 139521612207872 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.168362855911255, loss=1.8090033531188965
I0316 02:46:12.597805 139521603815168 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.7009308338165283, loss=1.7770929336547852
I0316 02:48:04.675602 139521612207872 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.6288979053497314, loss=1.8179482221603394
I0316 02:48:42.479379 139695971202880 spec.py:298] Evaluating on the training split.
I0316 02:49:21.589993 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 02:50:00.802826 139695971202880 spec.py:326] Evaluating on the test split.
I0316 02:50:21.321102 139695971202880 submission_runner.py:362] Time since start: 27652.52s, 	Step: 23135, 	{'train/ctc_loss': DeviceArray(0.4736947, dtype=float32), 'train/wer': 0.15400195668843703, 'validation/ctc_loss': DeviceArray(0.80814207, dtype=float32), 'validation/wer': 0.2295632374649056, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.50509924, dtype=float32), 'test/wer': 0.16058334856701806, 'test/num_examples': 2472}
I0316 02:50:21.341733 139521612207872 logging_writer.py:48] [23135] global_step=23135, preemption_count=0, score=26412.238770, test/ctc_loss=0.5050992369651794, test/num_examples=2472, test/wer=0.160583, total_duration=27652.516103, train/ctc_loss=0.47369471192359924, train/wer=0.154002, validation/ctc_loss=0.8081420660018921, validation/num_examples=5348, validation/wer=0.229563
I0316 02:50:21.488528 139695971202880 checkpoints.py:356] Saving checkpoint at step: 23135
I0316 02:50:22.187350 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_23135
I0316 02:50:22.203028 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_23135.
I0316 02:51:36.315138 139521603815168 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.677753448486328, loss=1.8007534742355347
I0316 02:53:28.071761 139519806535424 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.9434220790863037, loss=1.8075443506240845
I0316 02:55:23.671627 139521603815168 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.650446653366089, loss=1.8220374584197998
I0316 02:57:16.174664 139519806535424 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.2903764247894287, loss=1.8453166484832764
I0316 02:59:08.303941 139521603815168 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.551968812942505, loss=1.7946667671203613
I0316 03:01:05.528544 139521612207872 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.2056925296783447, loss=1.8122256994247437
I0316 03:02:59.942178 139521603815168 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.9975461959838867, loss=1.8466790914535522
I0316 03:04:55.152836 139521612207872 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.6318068504333496, loss=1.9309617280960083
I0316 03:06:50.269572 139521603815168 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.030818223953247, loss=1.8391844034194946
I0316 03:08:46.126324 139521612207872 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.44053053855896, loss=1.8437366485595703
I0316 03:10:41.527911 139521603815168 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.205110788345337, loss=1.7831647396087646
I0316 03:12:33.379607 139521612207872 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.473959445953369, loss=1.8493099212646484
I0316 03:14:25.272927 139521603815168 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.4959278106689453, loss=1.8127025365829468
I0316 03:16:17.347910 139521612207872 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.3349719047546387, loss=1.7583965063095093
I0316 03:18:11.212998 139521603815168 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.243403673171997, loss=1.7723687887191772
I0316 03:20:05.468132 139521612207872 logging_writer.py:48] [24700] global_step=24700, grad_norm=4.02794885635376, loss=1.8012281656265259
I0316 03:22:05.666342 139521612207872 logging_writer.py:48] [24800] global_step=24800, grad_norm=4.085594654083252, loss=1.8109086751937866
I0316 03:24:02.536767 139521603815168 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.0007760524749756, loss=1.7922064065933228
I0316 03:25:59.340339 139521612207872 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.4628713130950928, loss=1.7720924615859985
I0316 03:27:51.382796 139521603815168 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.4717442989349365, loss=1.751909852027893
I0316 03:29:43.141635 139521612207872 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.9394948482513428, loss=1.8136295080184937
I0316 03:30:23.203827 139695971202880 spec.py:298] Evaluating on the training split.
I0316 03:31:02.714594 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 03:31:41.581057 139695971202880 spec.py:326] Evaluating on the test split.
I0316 03:32:02.703987 139695971202880 submission_runner.py:362] Time since start: 30153.24s, 	Step: 25237, 	{'train/ctc_loss': DeviceArray(0.50195736, dtype=float32), 'train/wer': 0.16301474661671986, 'validation/ctc_loss': DeviceArray(0.8191411, dtype=float32), 'validation/wer': 0.23173402541269092, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5072574, dtype=float32), 'test/wer': 0.1615379928097008, 'test/num_examples': 2472}
I0316 03:32:02.725111 139520270763776 logging_writer.py:48] [25237] global_step=25237, preemption_count=0, score=28808.717686, test/ctc_loss=0.5072574019432068, test/num_examples=2472, test/wer=0.161538, total_duration=30153.240557, train/ctc_loss=0.501957356929779, train/wer=0.163015, validation/ctc_loss=0.8191410899162292, validation/num_examples=5348, validation/wer=0.231734
I0316 03:32:02.867150 139695971202880 checkpoints.py:356] Saving checkpoint at step: 25237
I0316 03:32:03.558090 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_25237
I0316 03:32:03.573824 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_25237.
I0316 03:33:15.039993 139520262371072 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.4383385181427, loss=1.818191409111023
I0316 03:35:06.948037 139520128087808 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.505758047103882, loss=1.8413276672363281
I0316 03:37:00.016602 139520262371072 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.9060800075531006, loss=1.7321248054504395
I0316 03:38:52.320523 139520128087808 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.8845864534378052, loss=1.8207050561904907
I0316 03:40:44.359451 139520262371072 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.0104331970214844, loss=1.7913283109664917
I0316 03:42:41.358082 139520270763776 logging_writer.py:48] [25800] global_step=25800, grad_norm=5.271204948425293, loss=1.8468281030654907
I0316 03:44:34.970307 139520262371072 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.7572758197784424, loss=1.7701750993728638
I0316 03:46:28.663603 139520270763776 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.7245287895202637, loss=1.7950384616851807
I0316 03:48:21.548072 139520262371072 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.356827735900879, loss=1.7849311828613281
I0316 03:50:13.852128 139520270763776 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7139065265655518, loss=1.7401537895202637
I0316 03:52:06.214572 139520262371072 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.0357038974761963, loss=1.845978856086731
I0316 03:53:58.827744 139520270763776 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.2273213863372803, loss=1.8035832643508911
I0316 03:55:53.406023 139520262371072 logging_writer.py:48] [26500] global_step=26500, grad_norm=7.110411167144775, loss=1.7775455713272095
I0316 03:57:49.097613 139520270763776 logging_writer.py:48] [26600] global_step=26600, grad_norm=6.415351867675781, loss=1.8857957124710083
I0316 03:59:45.210990 139520262371072 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.74153733253479, loss=1.7819557189941406
I0316 04:01:44.553927 139519943083776 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.229635000228882, loss=1.7807668447494507
I0316 04:03:38.684068 139519934691072 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.7858550548553467, loss=1.8041014671325684
I0316 04:05:32.316520 139519943083776 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.4936540126800537, loss=1.776597499847412
I0316 04:07:28.985054 139519934691072 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.4655377864837646, loss=1.7180813550949097
I0316 04:09:23.986257 139519943083776 logging_writer.py:48] [27200] global_step=27200, grad_norm=5.297647953033447, loss=1.8370553255081177
I0316 04:11:21.269207 139519934691072 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.655555248260498, loss=1.8189754486083984
I0316 04:12:04.553364 139695971202880 spec.py:298] Evaluating on the training split.
I0316 04:12:44.332731 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 04:13:24.581255 139695971202880 spec.py:326] Evaluating on the test split.
I0316 04:13:45.441897 139695971202880 submission_runner.py:362] Time since start: 32654.59s, 	Step: 27338, 	{'train/ctc_loss': DeviceArray(0.48064682, dtype=float32), 'train/wer': 0.15680111122543472, 'validation/ctc_loss': DeviceArray(0.80080104, dtype=float32), 'validation/wer': 0.22917731960752155, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.49994656, dtype=float32), 'test/wer': 0.16038023277070257, 'test/num_examples': 2472}
I0316 04:13:45.463520 139519943083776 logging_writer.py:48] [27338] global_step=27338, preemption_count=0, score=31205.187256, test/ctc_loss=0.49994656443595886, test/num_examples=2472, test/wer=0.160380, total_duration=32654.590096, train/ctc_loss=0.4806468188762665, train/wer=0.156801, validation/ctc_loss=0.8008010387420654, validation/num_examples=5348, validation/wer=0.229177
I0316 04:13:45.605747 139695971202880 checkpoints.py:356] Saving checkpoint at step: 27338
I0316 04:13:46.287315 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_27338
I0316 04:13:46.302748 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_27338.
I0316 04:14:57.292799 139519934691072 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.982224225997925, loss=1.8291212320327759
I0316 04:16:50.359437 139519792015104 logging_writer.py:48] [27500] global_step=27500, grad_norm=4.584046840667725, loss=1.8069326877593994
I0316 04:18:43.681666 139519934691072 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.3905460834503174, loss=1.8037618398666382
I0316 04:20:35.766174 139519792015104 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.8320446014404297, loss=1.8168538808822632
I0316 04:22:27.786934 139519934691072 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.4604198932647705, loss=1.775291919708252
I0316 04:24:23.719012 139520270763776 logging_writer.py:48] [27900] global_step=27900, grad_norm=6.087330341339111, loss=1.7661687135696411
I0316 04:26:18.731724 139520262371072 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.167181968688965, loss=1.780025601387024
I0316 04:28:11.994285 139520270763776 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.289924144744873, loss=1.8152987957000732
I0316 04:30:05.546792 139520262371072 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.9574413299560547, loss=1.8289414644241333
I0316 04:31:59.055634 139520270763776 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.5173089504241943, loss=1.816096305847168
I0316 04:33:51.108592 139520262371072 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.964370012283325, loss=1.7841955423355103
I0316 04:35:44.209285 139520270763776 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.3085856437683105, loss=1.7348337173461914
I0316 04:37:37.720015 139520262371072 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.628638505935669, loss=1.8425650596618652
I0316 04:39:30.766400 139520270763776 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.172936201095581, loss=1.7912441492080688
I0316 04:41:22.917854 139520262371072 logging_writer.py:48] [28800] global_step=28800, grad_norm=4.154904842376709, loss=1.8488659858703613
I0316 04:43:18.746376 139519943083776 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.0174949169158936, loss=1.840104579925537
I0316 04:45:10.641964 139519934691072 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.8046035766601562, loss=1.8177716732025146
I0316 04:47:05.315361 139519943083776 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.5318565368652344, loss=1.8395520448684692
I0316 04:49:02.650609 139519934691072 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.7715117931365967, loss=1.7227681875228882
I0316 04:50:59.486379 139519943083776 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.8123598098754883, loss=1.7704510688781738
I0316 04:52:55.105472 139519934691072 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.521278142929077, loss=1.8405920267105103
I0316 04:53:46.873139 139695971202880 spec.py:298] Evaluating on the training split.
I0316 04:54:26.214123 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 04:55:05.700703 139695971202880 spec.py:326] Evaluating on the test split.
I0316 04:55:25.810796 139695971202880 submission_runner.py:362] Time since start: 35156.91s, 	Step: 29446, 	{'train/ctc_loss': DeviceArray(0.46080554, dtype=float32), 'train/wer': 0.15309214137916474, 'validation/ctc_loss': DeviceArray(0.80033123, dtype=float32), 'validation/wer': 0.22792308657102336, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4957157, dtype=float32), 'test/wer': 0.1604817906688603, 'test/num_examples': 2472}
I0316 04:55:25.833011 139520700843776 logging_writer.py:48] [29446] global_step=29446, preemption_count=0, score=33601.094840, test/ctc_loss=0.4957157075405121, test/num_examples=2472, test/wer=0.160482, total_duration=35156.909881, train/ctc_loss=0.4608055353164673, train/wer=0.153092, validation/ctc_loss=0.8003312349319458, validation/num_examples=5348, validation/wer=0.227923
I0316 04:55:25.974946 139695971202880 checkpoints.py:356] Saving checkpoint at step: 29446
I0316 04:55:26.652679 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_29446
I0316 04:55:26.668179 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_29446.
I0316 04:56:28.729292 139520692451072 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.889984607696533, loss=1.7693051099777222
I0316 04:58:22.535699 139520541382400 logging_writer.py:48] [29600] global_step=29600, grad_norm=4.523079872131348, loss=1.7570208311080933
I0316 05:00:16.062415 139520692451072 logging_writer.py:48] [29700] global_step=29700, grad_norm=5.049701690673828, loss=1.8023406267166138
I0316 05:02:11.675940 139520541382400 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.2362818717956543, loss=1.7417725324630737
I0316 05:04:09.876602 139520700843776 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.563678026199341, loss=1.7505567073822021
I0316 05:06:01.748687 139520692451072 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.426290273666382, loss=1.769386887550354
I0316 05:07:55.015501 139520700843776 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.5440287590026855, loss=1.8474162817001343
I0316 05:09:51.279125 139520692451072 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.876757860183716, loss=1.8346298933029175
I0316 05:11:44.752171 139520700843776 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.683893918991089, loss=1.7358708381652832
I0316 05:13:37.609411 139520692451072 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.391125440597534, loss=1.7827022075653076
I0316 05:15:29.515914 139520700843776 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.736088752746582, loss=1.7657203674316406
I0316 05:17:22.116389 139520692451072 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.5328116416931152, loss=1.8542146682739258
I0316 05:19:17.929013 139520700843776 logging_writer.py:48] [30700] global_step=30700, grad_norm=4.321664333343506, loss=1.785086989402771
I0316 05:21:09.819302 139520692451072 logging_writer.py:48] [30800] global_step=30800, grad_norm=4.395884037017822, loss=1.7865959405899048
I0316 05:23:05.728867 139520373163776 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.141265869140625, loss=1.780109167098999
I0316 05:24:59.477665 139520364771072 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.077200174331665, loss=1.7529550790786743
I0316 05:26:53.483991 139520373163776 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.6884779930114746, loss=1.8181031942367554
I0316 05:28:48.538256 139520364771072 logging_writer.py:48] [31200] global_step=31200, grad_norm=6.716761589050293, loss=1.8114091157913208
I0316 05:30:40.405179 139520373163776 logging_writer.py:48] [31300] global_step=31300, grad_norm=8.826228141784668, loss=1.7620583772659302
I0316 05:32:34.148024 139520364771072 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.1347622871398926, loss=1.7421176433563232
I0316 05:34:31.637050 139520373163776 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.949517250061035, loss=1.7836785316467285
I0316 05:35:27.780965 139695971202880 spec.py:298] Evaluating on the training split.
I0316 05:36:07.537365 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 05:36:47.162545 139695971202880 spec.py:326] Evaluating on the test split.
I0316 05:37:08.112630 139695971202880 submission_runner.py:362] Time since start: 37657.82s, 	Step: 31549, 	{'train/ctc_loss': DeviceArray(0.43244648, dtype=float32), 'train/wer': 0.14583061634367095, 'validation/ctc_loss': DeviceArray(0.7749419, dtype=float32), 'validation/wer': 0.22092832540593735, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47858682, dtype=float32), 'test/wer': 0.15339304937745007, 'test/num_examples': 2472}
I0316 05:37:08.135393 139519943083776 logging_writer.py:48] [31549] global_step=31549, preemption_count=0, score=35997.585949, test/ctc_loss=0.4785868227481842, test/num_examples=2472, test/wer=0.153393, total_duration=37657.817707, train/ctc_loss=0.4324464797973633, train/wer=0.145831, validation/ctc_loss=0.7749419212341309, validation/num_examples=5348, validation/wer=0.220928
I0316 05:37:08.286769 139695971202880 checkpoints.py:356] Saving checkpoint at step: 31549
I0316 05:37:08.997077 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_31549
I0316 05:37:09.012480 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_31549.
I0316 05:38:08.001595 139519934691072 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.0386972427368164, loss=1.7324695587158203
I0316 05:40:01.182166 139519775229696 logging_writer.py:48] [31700] global_step=31700, grad_norm=5.180337905883789, loss=1.8266711235046387
I0316 05:41:53.001391 139519934691072 logging_writer.py:48] [31800] global_step=31800, grad_norm=4.229611873626709, loss=1.764541506767273
I0316 05:43:44.971784 139519775229696 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.2394051551818848, loss=1.7088921070098877
I0316 05:45:40.574153 139520700843776 logging_writer.py:48] [32000] global_step=32000, grad_norm=5.339293003082275, loss=1.7592191696166992
I0316 05:47:32.259797 139520692451072 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.7782809734344482, loss=1.7284044027328491
I0316 05:49:24.979515 139520700843776 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.1765072345733643, loss=1.7193975448608398
I0316 05:51:21.182306 139520692451072 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.491767168045044, loss=1.7890316247940063
I0316 05:53:17.097317 139520700843776 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.632741928100586, loss=1.7652842998504639
I0316 05:55:10.508343 139520692451072 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.5700716972351074, loss=1.703682541847229
I0316 05:57:07.702751 139520700843776 logging_writer.py:48] [32600] global_step=32600, grad_norm=4.773692607879639, loss=1.756115436553955
I0316 05:59:02.948833 139520692451072 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.4937148094177246, loss=1.7879163026809692
I0316 06:00:56.468965 139520700843776 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.166654109954834, loss=1.803857445716858
I0316 06:02:48.236102 139520692451072 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.00471568107605, loss=1.7373719215393066
I0316 06:04:43.840381 139519943083776 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.8983824253082275, loss=1.696723461151123
I0316 06:06:39.721746 139519934691072 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.467337131500244, loss=1.7185544967651367
I0316 06:08:33.575492 139519943083776 logging_writer.py:48] [33200] global_step=33200, grad_norm=4.094070911407471, loss=1.7545744180679321
I0316 06:10:25.604164 139519934691072 logging_writer.py:48] [33300] global_step=33300, grad_norm=4.437389850616455, loss=1.7667505741119385
I0316 06:12:18.473838 139519943083776 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.527615785598755, loss=1.6662002801895142
I0316 06:14:10.524632 139519934691072 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.9938478469848633, loss=1.7419500350952148
I0316 06:16:03.708052 139519943083776 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.700544595718384, loss=1.7796801328659058
I0316 06:17:09.706411 139695971202880 spec.py:298] Evaluating on the training split.
I0316 06:17:49.606923 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 06:18:29.610579 139695971202880 spec.py:326] Evaluating on the test split.
I0316 06:18:49.919394 139695971202880 submission_runner.py:362] Time since start: 40159.74s, 	Step: 33659, 	{'train/ctc_loss': DeviceArray(0.41294086, dtype=float32), 'train/wer': 0.13861472215687679, 'validation/ctc_loss': DeviceArray(0.76251364, dtype=float32), 'validation/wer': 0.21762872772530367, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47610524, dtype=float32), 'test/wer': 0.152093108281031, 'test/num_examples': 2472}
I0316 06:18:49.944202 139519943083776 logging_writer.py:48] [33659] global_step=33659, preemption_count=0, score=38393.682708, test/ctc_loss=0.4761052429676056, test/num_examples=2472, test/wer=0.152093, total_duration=40159.743139, train/ctc_loss=0.4129408597946167, train/wer=0.138615, validation/ctc_loss=0.7625136375427246, validation/num_examples=5348, validation/wer=0.217629
I0316 06:18:50.087849 139695971202880 checkpoints.py:356] Saving checkpoint at step: 33659
I0316 06:18:50.779301 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_33659
I0316 06:18:50.794796 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_33659.
I0316 06:19:38.848885 139519934691072 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.808002471923828, loss=1.7420861721038818
I0316 06:21:32.854114 139519766836992 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.530488967895508, loss=1.7398205995559692
I0316 06:23:26.560906 139519934691072 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.650470018386841, loss=1.7159614562988281
I0316 06:25:24.655035 139519943083776 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.0277230739593506, loss=1.6756997108459473
I0316 06:27:19.078078 139519934691072 logging_writer.py:48] [34100] global_step=34100, grad_norm=6.156667232513428, loss=1.793492078781128
I0316 06:29:14.929158 139519943083776 logging_writer.py:48] [34200] global_step=34200, grad_norm=6.168881416320801, loss=1.7459098100662231
I0316 06:31:09.955032 139519934691072 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.6540842056274414, loss=1.6879054307937622
I0316 06:33:05.052598 139519943083776 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.8470959663391113, loss=1.7749269008636475
I0316 06:35:00.269999 139519934691072 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.983717441558838, loss=1.7000151872634888
I0316 06:36:57.725014 139519943083776 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.90183687210083, loss=1.7253649234771729
I0316 06:38:54.974538 139519934691072 logging_writer.py:48] [34700] global_step=34700, grad_norm=4.9251532554626465, loss=1.7835091352462769
I0316 06:40:51.956906 139519943083776 logging_writer.py:48] [34800] global_step=34800, grad_norm=4.055557727813721, loss=1.7695411443710327
I0316 06:42:48.867501 139519934691072 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.9075775146484375, loss=1.6734846830368042
I0316 06:44:43.369582 139519943083776 logging_writer.py:48] [35000] global_step=35000, grad_norm=4.076466083526611, loss=1.701429009437561
I0316 06:46:40.410405 139519943083776 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.505323648452759, loss=1.6510136127471924
I0316 06:48:33.502213 139519934691072 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.71335506439209, loss=1.737532138824463
I0316 06:50:28.576020 139519943083776 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.435676336288452, loss=1.743491530418396
I0316 06:52:21.650097 139519934691072 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.9378740787506104, loss=1.7799162864685059
I0316 06:54:15.204501 139519943083776 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.406316041946411, loss=1.7652024030685425
I0316 06:56:09.385131 139519934691072 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.286003589630127, loss=1.7034599781036377
I0316 06:58:02.462163 139519943083776 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.6250052452087402, loss=1.7073038816452026
I0316 06:58:50.939385 139695971202880 spec.py:298] Evaluating on the training split.
I0316 06:59:29.785077 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 07:00:09.361644 139695971202880 spec.py:326] Evaluating on the test split.
I0316 07:00:29.455703 139695971202880 submission_runner.py:362] Time since start: 42660.98s, 	Step: 35744, 	{'train/ctc_loss': DeviceArray(0.44583458, dtype=float32), 'train/wer': 0.14954931530594556, 'validation/ctc_loss': DeviceArray(0.750568, dtype=float32), 'validation/wer': 0.21619118370654805, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45987254, dtype=float32), 'test/wer': 0.14841671236772083, 'test/num_examples': 2472}
I0316 07:00:29.477328 139520598447872 logging_writer.py:48] [35744] global_step=35744, preemption_count=0, score=40789.263860, test/ctc_loss=0.4598725438117981, test/num_examples=2472, test/wer=0.148417, total_duration=42660.976123, train/ctc_loss=0.44583457708358765, train/wer=0.149549, validation/ctc_loss=0.7505679726600647, validation/num_examples=5348, validation/wer=0.216191
I0316 07:00:29.624456 139695971202880 checkpoints.py:356] Saving checkpoint at step: 35744
I0316 07:00:30.313484 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_35744
I0316 07:00:30.328871 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_35744.
I0316 07:01:34.110129 139520590055168 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.1424572467803955, loss=1.7248728275299072
I0316 07:03:25.926091 139520413808384 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.331416368484497, loss=1.7346237897872925
I0316 07:05:20.045723 139520590055168 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.035233497619629, loss=1.7636469602584839
I0316 07:07:19.706732 139519943087872 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.816660165786743, loss=1.7256089448928833
I0316 07:09:15.682973 139519934695168 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.523451328277588, loss=1.824747085571289
I0316 07:11:07.983919 139519943087872 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.9856956005096436, loss=1.6629024744033813
I0316 07:12:59.763704 139519934695168 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.9013962745666504, loss=1.6738979816436768
I0316 07:14:51.767722 139519943087872 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.9722349643707275, loss=1.6958116292953491
I0316 07:16:43.668822 139519934695168 logging_writer.py:48] [36600] global_step=36600, grad_norm=4.414146900177002, loss=1.7102073431015015
I0316 07:18:35.579605 139519943087872 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.0865421295166016, loss=1.7272834777832031
I0316 07:20:27.389641 139519934695168 logging_writer.py:48] [36800] global_step=36800, grad_norm=4.958127021789551, loss=1.7237777709960938
I0316 07:22:19.261225 139519943087872 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.213653802871704, loss=1.7773874998092651
I0316 07:24:12.224959 139519934695168 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.908585548400879, loss=1.7362512350082397
I0316 07:26:07.893406 139520598447872 logging_writer.py:48] [37100] global_step=37100, grad_norm=4.80374002456665, loss=1.7163749933242798
I0316 07:28:01.150711 139520590055168 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.5266337394714355, loss=1.6775106191635132
I0316 07:29:53.041536 139520598447872 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.4897961616516113, loss=1.73428213596344
I0316 07:31:46.384760 139520590055168 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.7299206256866455, loss=1.7447683811187744
I0316 07:33:38.684822 139520598447872 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.516549825668335, loss=1.7378028631210327
I0316 07:35:31.312057 139520590055168 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.0028154850006104, loss=1.7606532573699951
I0316 07:37:25.986514 139520598447872 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.681349754333496, loss=1.7122483253479004
I0316 07:39:23.324265 139520590055168 logging_writer.py:48] [37800] global_step=37800, grad_norm=4.724123477935791, loss=1.6288690567016602
I0316 07:40:30.547880 139695971202880 spec.py:298] Evaluating on the training split.
I0316 07:41:09.354439 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 07:41:48.281370 139695971202880 spec.py:326] Evaluating on the test split.
I0316 07:42:09.039073 139695971202880 submission_runner.py:362] Time since start: 45160.58s, 	Step: 37859, 	{'train/ctc_loss': DeviceArray(0.41303307, dtype=float32), 'train/wer': 0.13608863857101774, 'validation/ctc_loss': DeviceArray(0.7354816, dtype=float32), 'validation/wer': 0.21175312834663143, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45660117, dtype=float32), 'test/wer': 0.14774643023987977, 'test/num_examples': 2472}
I0316 07:42:09.063279 139521612207872 logging_writer.py:48] [37859] global_step=37859, preemption_count=0, score=43184.830417, test/ctc_loss=0.45660117268562317, test/num_examples=2472, test/wer=0.147746, total_duration=45160.584617, train/ctc_loss=0.41303306818008423, train/wer=0.136089, validation/ctc_loss=0.7354816198348999, validation/num_examples=5348, validation/wer=0.211753
I0316 07:42:09.220767 139695971202880 checkpoints.py:356] Saving checkpoint at step: 37859
I0316 07:42:09.933583 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_37859
I0316 07:42:09.948897 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_37859.
I0316 07:42:56.893133 139521603815168 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.358262300491333, loss=1.628637671470642
I0316 07:44:48.675124 139521419175680 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.63936710357666, loss=1.727625846862793
I0316 07:46:44.762344 139521603815168 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.4219160079956055, loss=1.7141398191452026
I0316 07:48:44.305183 139520526767872 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.2257320880889893, loss=1.7169519662857056
I0316 07:50:40.153560 139520518375168 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.033233404159546, loss=1.793799638748169
I0316 07:52:33.648593 139520526767872 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.708922863006592, loss=1.7185450792312622
I0316 07:54:25.643488 139520518375168 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.2990658283233643, loss=1.7147873640060425
I0316 07:56:18.456412 139520526767872 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.9963715076446533, loss=1.6670174598693848
I0316 07:58:12.088059 139520518375168 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.2163047790527344, loss=1.7324087619781494
I0316 08:00:05.937799 139520526767872 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.7893404960632324, loss=1.7097536325454712
I0316 08:01:58.461424 139520518375168 logging_writer.py:48] [38900] global_step=38900, grad_norm=4.44162130355835, loss=1.6707218885421753
I0316 08:03:50.375812 139520526767872 logging_writer.py:48] [39000] global_step=39000, grad_norm=4.698143482208252, loss=1.7199907302856445
I0316 08:05:42.271109 139520518375168 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.4430766105651855, loss=1.7035150527954102
I0316 08:07:38.922532 139521612207872 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.568129777908325, loss=1.725386142730713
I0316 08:09:31.950203 139521603815168 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.465139150619507, loss=1.696216344833374
I0316 08:11:25.287700 139521612207872 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.2518227100372314, loss=1.6608318090438843
I0316 08:13:17.547499 139521603815168 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.573518991470337, loss=1.7543418407440186
I0316 08:15:09.622130 139521612207872 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.849855661392212, loss=1.6777567863464355
I0316 08:17:01.734797 139521603815168 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.58539080619812, loss=1.718969464302063
I0316 08:18:53.812380 139521612207872 logging_writer.py:48] [39800] global_step=39800, grad_norm=4.554687023162842, loss=1.7718485593795776
I0316 08:20:45.855548 139521603815168 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.2756900787353516, loss=1.693995475769043
I0316 08:22:10.785294 139695971202880 spec.py:298] Evaluating on the training split.
I0316 08:22:50.585895 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 08:23:30.271595 139695971202880 spec.py:326] Evaluating on the test split.
I0316 08:23:51.127474 139695971202880 submission_runner.py:362] Time since start: 47660.82s, 	Step: 39977, 	{'train/ctc_loss': DeviceArray(0.43436962, dtype=float32), 'train/wer': 0.14176884866106057, 'validation/ctc_loss': DeviceArray(0.7282851, dtype=float32), 'validation/wer': 0.2097174116489305, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44979516, dtype=float32), 'test/wer': 0.1445778238173583, 'test/num_examples': 2472}
I0316 08:23:51.149910 139521320367872 logging_writer.py:48] [39977] global_step=39977, preemption_count=0, score=45581.040444, test/ctc_loss=0.4497951567173004, test/num_examples=2472, test/wer=0.144578, total_duration=47660.822032, train/ctc_loss=0.43436962366104126, train/wer=0.141769, validation/ctc_loss=0.7282850742340088, validation/num_examples=5348, validation/wer=0.209717
I0316 08:23:51.302148 139695971202880 checkpoints.py:356] Saving checkpoint at step: 39977
I0316 08:23:51.993963 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_39977
I0316 08:23:52.009488 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_39977.
I0316 08:24:19.091073 139521311975168 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.6392362117767334, loss=1.759560227394104
I0316 08:26:10.805311 139521118942976 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.7398886680603027, loss=1.6706403493881226
I0316 08:28:06.670949 139520992687872 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.6729068756103516, loss=1.66428804397583
I0316 08:29:58.490771 139520984295168 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.238184928894043, loss=1.6205955743789673
I0316 08:31:52.675322 139520992687872 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.0400032997131348, loss=1.788407325744629
I0316 08:33:46.248102 139520984295168 logging_writer.py:48] [40500] global_step=40500, grad_norm=4.8609161376953125, loss=1.7367727756500244
I0316 08:35:39.225224 139520992687872 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.813603639602661, loss=1.7339149713516235
I0316 08:37:31.094894 139520984295168 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.0342042446136475, loss=1.6652730703353882
I0316 08:39:26.719699 139520992687872 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.2681052684783936, loss=1.641453742980957
I0316 08:41:21.833006 139520984295168 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.442986011505127, loss=1.6561522483825684
I0316 08:43:17.363320 139520992687872 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.2527003288269043, loss=1.7126140594482422
I0316 08:45:14.880136 139520984295168 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.486058473587036, loss=1.6914807558059692
I0316 08:47:14.358510 139521320367872 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.719602108001709, loss=1.6520133018493652
I0316 08:49:08.496866 139521311975168 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.9509124755859375, loss=1.6663910150527954
I0316 08:51:05.332668 139521320367872 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.7163918018341064, loss=1.7115579843521118
I0316 08:52:58.414790 139521311975168 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.2434909343719482, loss=1.668219804763794
I0316 08:54:55.856055 139521320367872 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.608522653579712, loss=1.6576498746871948
I0316 08:56:48.575906 139521311975168 logging_writer.py:48] [41700] global_step=41700, grad_norm=4.110572338104248, loss=1.7183738946914673
I0316 08:58:44.324761 139521320367872 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.6350138187408447, loss=1.7340490818023682
I0316 09:00:37.836138 139521311975168 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.5817103385925293, loss=1.7162121534347534
I0316 09:02:30.279534 139521320367872 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.3933756351470947, loss=1.693357229232788
I0316 09:03:52.703977 139695971202880 spec.py:298] Evaluating on the training split.
I0316 09:04:31.960876 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 09:05:11.621514 139695971202880 spec.py:326] Evaluating on the test split.
I0316 09:05:32.087840 139695971202880 submission_runner.py:362] Time since start: 50162.74s, 	Step: 42074, 	{'train/ctc_loss': DeviceArray(0.41445267, dtype=float32), 'train/wer': 0.13429670599017446, 'validation/ctc_loss': DeviceArray(0.7203168, dtype=float32), 'validation/wer': 0.2095148047738039, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4378079, dtype=float32), 'test/wer': 0.14151077529299452, 'test/num_examples': 2472}
I0316 09:05:32.110790 139521320367872 logging_writer.py:48] [42074] global_step=42074, preemption_count=0, score=47977.306325, test/ctc_loss=0.4378078877925873, test/num_examples=2472, test/wer=0.141511, total_duration=50162.740718, train/ctc_loss=0.4144526720046997, train/wer=0.134297, validation/ctc_loss=0.7203168272972107, validation/num_examples=5348, validation/wer=0.209515
I0316 09:05:32.251741 139695971202880 checkpoints.py:356] Saving checkpoint at step: 42074
I0316 09:05:32.911895 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_42074
I0316 09:05:32.927322 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_42074.
I0316 09:06:03.138803 139521311975168 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.7244770526885986, loss=1.740668773651123
I0316 09:07:55.488843 139519833401088 logging_writer.py:48] [42200] global_step=42200, grad_norm=5.709472179412842, loss=1.6417236328125
I0316 09:09:52.864349 139521320367872 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.1010024547576904, loss=1.6335532665252686
I0316 09:11:49.315855 139521311975168 logging_writer.py:48] [42400] global_step=42400, grad_norm=4.863668441772461, loss=1.627977728843689
I0316 09:13:44.313922 139521320367872 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.1491920948028564, loss=1.6861013174057007
I0316 09:15:39.344083 139521311975168 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.0440781116485596, loss=1.6226599216461182
I0316 09:17:34.536511 139521320367872 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.4235575199127197, loss=1.6429710388183594
I0316 09:19:26.645455 139521311975168 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.8484156131744385, loss=1.7125060558319092
I0316 09:21:21.294632 139521320367872 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.804886817932129, loss=1.7037235498428345
I0316 09:23:14.056035 139521311975168 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.575382709503174, loss=1.6529664993286133
I0316 09:25:05.855978 139521320367872 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.4597513675689697, loss=1.6833442449569702
I0316 09:26:58.841395 139521311975168 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.0884621143341064, loss=1.6908923387527466
I0316 09:28:58.213087 139521320367872 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.200878858566284, loss=1.7142679691314697
I0316 09:30:55.266808 139521311975168 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.6065499782562256, loss=1.6672073602676392
I0316 09:32:52.743654 139521320367872 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.214796543121338, loss=1.6174943447113037
I0316 09:34:50.490355 139521311975168 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.2776408195495605, loss=1.6233516931533813
I0316 09:36:44.926938 139521320367872 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.4160163402557373, loss=1.63485848903656
I0316 09:38:37.007815 139521311975168 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.0386505126953125, loss=1.6479560136795044
I0316 09:40:29.372218 139521320367872 logging_writer.py:48] [43900] global_step=43900, grad_norm=7.82228946685791, loss=1.739266037940979
I0316 09:42:22.359719 139521311975168 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.9652115106582642, loss=1.7206346988677979
I0316 09:44:17.064052 139521320367872 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.1246821880340576, loss=1.729151725769043
I0316 09:45:33.335763 139695971202880 spec.py:298] Evaluating on the training split.
I0316 09:46:13.058514 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 09:46:52.154038 139695971202880 spec.py:326] Evaluating on the test split.
I0316 09:47:12.256542 139695971202880 submission_runner.py:362] Time since start: 52663.37s, 	Step: 44167, 	{'train/ctc_loss': DeviceArray(0.35354257, dtype=float32), 'train/wer': 0.11913327798448183, 'validation/ctc_loss': DeviceArray(0.71076226, dtype=float32), 'validation/wer': 0.20510569325319106, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.43437228, dtype=float32), 'test/wer': 0.13736721304815874, 'test/num_examples': 2472}
I0316 09:47:12.279767 139521320367872 logging_writer.py:48] [44167] global_step=44167, preemption_count=0, score=50373.302373, test/ctc_loss=0.43437227606773376, test/num_examples=2472, test/wer=0.137367, total_duration=52663.372497, train/ctc_loss=0.3535425662994385, train/wer=0.119133, validation/ctc_loss=0.7107622623443604, validation/num_examples=5348, validation/wer=0.205106
I0316 09:47:12.425617 139695971202880 checkpoints.py:356] Saving checkpoint at step: 44167
I0316 09:47:13.124492 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_44167
I0316 09:47:13.139838 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_44167.
I0316 09:47:52.565252 139521311975168 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.6520912647247314, loss=1.7517811059951782
I0316 09:49:51.574399 139521320367872 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.098740577697754, loss=1.6290549039840698
I0316 09:51:43.553537 139521311975168 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.2048635482788086, loss=1.6863820552825928
I0316 09:53:35.644385 139521320367872 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.531909942626953, loss=1.723394513130188
I0316 09:55:30.965192 139521311975168 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.17600154876709, loss=1.6606086492538452
I0316 09:57:26.865594 139521320367872 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.331556797027588, loss=1.7020131349563599
I0316 09:59:20.567254 139521311975168 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.8672633171081543, loss=1.7268035411834717
I0316 10:01:12.855991 139521320367872 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.183986186981201, loss=1.6630131006240845
I0316 10:03:04.969119 139521311975168 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.0547776222229004, loss=1.6450655460357666
I0316 10:04:57.105830 139521320367872 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.2649827003479004, loss=1.6918931007385254
I0316 10:06:49.177443 139521311975168 logging_writer.py:48] [45200] global_step=45200, grad_norm=6.668619155883789, loss=1.6548094749450684
I0316 10:08:41.187504 139521320367872 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.3993659019470215, loss=1.6442058086395264
I0316 10:10:37.134971 139521320367872 logging_writer.py:48] [45400] global_step=45400, grad_norm=4.146655082702637, loss=1.646066665649414
I0316 10:12:30.115139 139521311975168 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.6533801555633545, loss=1.6361439228057861
I0316 10:14:23.987683 139521320367872 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.699474334716797, loss=1.6769261360168457
I0316 10:16:16.127744 139521311975168 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.6092073917388916, loss=1.690787434577942
I0316 10:18:08.919407 139521320367872 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.6579079627990723, loss=1.5206708908081055
I0316 10:20:01.180309 139521311975168 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.040980577468872, loss=1.6172575950622559
I0316 10:21:55.082907 139521320367872 logging_writer.py:48] [46000] global_step=46000, grad_norm=4.939325332641602, loss=1.6323553323745728
I0316 10:23:48.178637 139521311975168 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.2981083393096924, loss=1.6261143684387207
I0316 10:25:40.271228 139521320367872 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.4685258865356445, loss=1.5808829069137573
I0316 10:27:14.262804 139695971202880 spec.py:298] Evaluating on the training split.
I0316 10:27:54.028653 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 10:28:33.833950 139695971202880 spec.py:326] Evaluating on the test split.
I0316 10:28:54.488964 139695971202880 submission_runner.py:362] Time since start: 55164.30s, 	Step: 46284, 	{'train/ctc_loss': DeviceArray(0.39207852, dtype=float32), 'train/wer': 0.12940064623767186, 'validation/ctc_loss': DeviceArray(0.6987011, dtype=float32), 'validation/wer': 0.20069658173257823, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.42200115, dtype=float32), 'test/wer': 0.13478764243495217, 'test/num_examples': 2472}
I0316 10:28:54.511094 139521028527872 logging_writer.py:48] [46284] global_step=46284, preemption_count=0, score=52769.788003, test/ctc_loss=0.4220011532306671, test/num_examples=2472, test/wer=0.134788, total_duration=55164.299538, train/ctc_loss=0.3920785188674927, train/wer=0.129401, validation/ctc_loss=0.6987010836601257, validation/num_examples=5348, validation/wer=0.200697
I0316 10:28:54.656737 139695971202880 checkpoints.py:356] Saving checkpoint at step: 46284
I0316 10:28:55.361146 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_46284
I0316 10:28:55.376748 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_46284.
I0316 10:29:14.910024 139521020135168 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.253225564956665, loss=1.7052229642868042
I0316 10:31:13.076253 139521028527872 logging_writer.py:48] [46400] global_step=46400, grad_norm=4.430185794830322, loss=1.656416893005371
I0316 10:33:07.165691 139521020135168 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.140105962753296, loss=1.6100486516952515
I0316 10:35:02.360643 139521028527872 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.0650882720947266, loss=1.569329023361206
I0316 10:36:55.838163 139521020135168 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.055567979812622, loss=1.6425584554672241
I0316 10:38:51.490380 139521028527872 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.0685129165649414, loss=1.6227096319198608
I0316 10:40:46.957828 139521020135168 logging_writer.py:48] [46900] global_step=46900, grad_norm=4.622615337371826, loss=1.655799150466919
I0316 10:42:41.533041 139521028527872 logging_writer.py:48] [47000] global_step=47000, grad_norm=4.266366958618164, loss=1.701109528541565
I0316 10:44:37.044169 139521020135168 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.758815050125122, loss=1.679570198059082
I0316 10:46:32.490608 139521028527872 logging_writer.py:48] [47200] global_step=47200, grad_norm=6.129894256591797, loss=1.6833045482635498
I0316 10:48:27.789361 139521020135168 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8059765100479126, loss=1.6364444494247437
I0316 10:50:26.302739 139521028527872 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.6238059997558594, loss=1.58799409866333
I0316 10:52:22.010818 139521020135168 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.6372482776641846, loss=1.6593395471572876
I0316 10:54:18.652987 139521028527872 logging_writer.py:48] [47600] global_step=47600, grad_norm=4.317587852478027, loss=1.6716408729553223
I0316 10:56:15.200700 139521020135168 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.8147311210632324, loss=1.644463300704956
I0316 10:58:11.473939 139521028527872 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.9080424308776855, loss=1.5661084651947021
I0316 11:00:08.774742 139521020135168 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.1981353759765625, loss=1.7082691192626953
I0316 11:02:04.852549 139521028527872 logging_writer.py:48] [48000] global_step=48000, grad_norm=2.7926065921783447, loss=1.637799859046936
I0316 11:03:56.919320 139521020135168 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.178635597229004, loss=1.6687166690826416
I0316 11:05:49.081950 139521028527872 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.5622432231903076, loss=1.6003642082214355
I0316 11:07:41.175622 139521020135168 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.2254111766815186, loss=1.6439831256866455
I0316 11:08:56.013166 139695971202880 spec.py:298] Evaluating on the training split.
I0316 11:09:34.709131 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 11:10:14.811285 139695971202880 spec.py:326] Evaluating on the test split.
I0316 11:10:36.012450 139695971202880 submission_runner.py:362] Time since start: 57666.05s, 	Step: 48368, 	{'train/ctc_loss': DeviceArray(0.5084408, dtype=float32), 'train/wer': 0.16248379555550735, 'validation/ctc_loss': DeviceArray(0.68937474, dtype=float32), 'validation/wer': 0.1984100184275777, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4183758, dtype=float32), 'test/wer': 0.1356407287794772, 'test/num_examples': 2472}
I0316 11:10:36.036598 139521028527872 logging_writer.py:48] [48368] global_step=48368, preemption_count=0, score=55165.873659, test/ctc_loss=0.41837579011917114, test/num_examples=2472, test/wer=0.135641, total_duration=57666.049900, train/ctc_loss=0.5084407925605774, train/wer=0.162484, validation/ctc_loss=0.6893747448921204, validation/num_examples=5348, validation/wer=0.198410
I0316 11:10:36.177438 139695971202880 checkpoints.py:356] Saving checkpoint at step: 48368
I0316 11:10:36.866197 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_48368
I0316 11:10:36.881496 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_48368.
I0316 11:11:13.791759 139521020135168 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.958146095275879, loss=1.6225709915161133
I0316 11:13:10.574746 139521028527872 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.5880002975463867, loss=1.6152048110961914
I0316 11:15:02.828828 139521020135168 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.204501152038574, loss=1.5776718854904175
I0316 11:16:57.356592 139521028527872 logging_writer.py:48] [48700] global_step=48700, grad_norm=2.388127326965332, loss=1.6164554357528687
I0316 11:18:50.322776 139521020135168 logging_writer.py:48] [48800] global_step=48800, grad_norm=4.173440933227539, loss=1.6733157634735107
I0316 11:20:42.574415 139521028527872 logging_writer.py:48] [48900] global_step=48900, grad_norm=2.3449087142944336, loss=1.6630711555480957
I0316 11:22:34.578644 139521020135168 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.9801857471466064, loss=1.6427559852600098
I0316 11:24:26.622096 139521028527872 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.2459657192230225, loss=1.5942257642745972
I0316 11:26:18.774414 139521020135168 logging_writer.py:48] [49200] global_step=49200, grad_norm=2.474540948867798, loss=1.633323311805725
I0316 11:28:12.626719 139521028527872 logging_writer.py:48] [49300] global_step=49300, grad_norm=2.887392044067383, loss=1.6653082370758057
I0316 11:30:09.072216 139521020135168 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.493117094039917, loss=1.6279656887054443
I0316 11:32:08.630879 139521028527872 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.909412145614624, loss=1.6387386322021484
I0316 11:34:01.802736 139521020135168 logging_writer.py:48] [49600] global_step=49600, grad_norm=4.642892837524414, loss=1.600234031677246
I0316 11:35:54.483846 139521028527872 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.087064504623413, loss=1.5737277269363403
I0316 11:37:46.662024 139521020135168 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.889782190322876, loss=1.6053271293640137
I0316 11:39:38.800463 139521028527872 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.864804267883301, loss=1.6199889183044434
I0316 11:41:35.398404 139521020135168 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.9803173542022705, loss=1.6099011898040771
I0316 11:43:29.457041 139521028527872 logging_writer.py:48] [50100] global_step=50100, grad_norm=2.0868194103240967, loss=1.629631519317627
I0316 11:45:21.511852 139521020135168 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.384594678878784, loss=1.600865125656128
I0316 11:47:13.561477 139521028527872 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.122408866882324, loss=1.6569201946258545
I0316 11:49:05.668501 139521020135168 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.510800361633301, loss=1.6984021663665771
I0316 11:50:37.738719 139695971202880 spec.py:298] Evaluating on the training split.
I0316 11:51:16.709862 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 11:51:56.400322 139695971202880 spec.py:326] Evaluating on the test split.
I0316 11:52:16.818545 139695971202880 submission_runner.py:362] Time since start: 60167.78s, 	Step: 50480, 	{'train/ctc_loss': DeviceArray(0.5224714, dtype=float32), 'train/wer': 0.16567390375103394, 'validation/ctc_loss': DeviceArray(0.6699646, dtype=float32), 'validation/wer': 0.19279491360263967, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.40971416, dtype=float32), 'test/wer': 0.13159872443279913, 'test/num_examples': 2472}
I0316 11:52:16.840661 139520373159680 logging_writer.py:48] [50480] global_step=50480, preemption_count=0, score=57562.159517, test/ctc_loss=0.4097141623497009, test/num_examples=2472, test/wer=0.131599, total_duration=60167.775367, train/ctc_loss=0.5224714279174805, train/wer=0.165674, validation/ctc_loss=0.669964611530304, validation/num_examples=5348, validation/wer=0.192795
I0316 11:52:16.983014 139695971202880 checkpoints.py:356] Saving checkpoint at step: 50480
I0316 11:52:17.654704 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_50480
I0316 11:52:17.670235 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_50480.
I0316 11:52:42.379189 139520364766976 logging_writer.py:48] [50500] global_step=50500, grad_norm=2.678173542022705, loss=1.67898690700531
I0316 11:54:39.411422 139520129771264 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.6056830883026123, loss=1.6354858875274658
I0316 11:56:34.920326 139520364766976 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.1475000381469727, loss=1.643917202949524
I0316 11:58:27.953917 139520129771264 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.2781834602355957, loss=1.6035677194595337
I0316 12:00:19.846115 139520364766976 logging_writer.py:48] [50900] global_step=50900, grad_norm=2.5075321197509766, loss=1.641935110092163
I0316 12:02:11.966864 139520129771264 logging_writer.py:48] [51000] global_step=51000, grad_norm=5.317043304443359, loss=1.6444973945617676
I0316 12:04:07.382024 139520364766976 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.5127718448638916, loss=1.65225350856781
I0316 12:05:59.838171 139520129771264 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.951373815536499, loss=1.573191523551941
I0316 12:07:52.411155 139520364766976 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.500933885574341, loss=1.5623548030853271
I0316 12:09:44.700870 139520129771264 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.756244659423828, loss=1.6205404996871948
I0316 12:11:40.190323 139520373159680 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8375334739685059, loss=1.6743351221084595
I0316 12:13:32.161751 139520364766976 logging_writer.py:48] [51600] global_step=51600, grad_norm=4.309136867523193, loss=1.6181222200393677
I0316 12:15:24.273816 139520373159680 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8613274097442627, loss=1.6753098964691162
I0316 12:17:17.389843 139520364766976 logging_writer.py:48] [51800] global_step=51800, grad_norm=2.6662182807922363, loss=1.644429326057434
I0316 12:19:12.614765 139520373159680 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.9574031829833984, loss=1.6534488201141357
I0316 12:21:04.479995 139520364766976 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.9118452072143555, loss=1.606492280960083
I0316 12:22:57.010160 139520373159680 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.703555107116699, loss=1.625679612159729
I0316 12:24:52.250276 139520364766976 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.287599802017212, loss=1.6619888544082642
I0316 12:26:49.482465 139520373159680 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.0353214740753174, loss=1.5968670845031738
I0316 12:28:47.113842 139520364766976 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.907393455505371, loss=1.6259448528289795
I0316 12:30:39.526706 139520373159680 logging_writer.py:48] [52500] global_step=52500, grad_norm=2.8551292419433594, loss=1.566841721534729
I0316 12:32:17.841936 139695971202880 spec.py:298] Evaluating on the training split.
I0316 12:32:55.437247 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 12:33:34.706234 139695971202880 spec.py:326] Evaluating on the test split.
I0316 12:33:54.928941 139695971202880 submission_runner.py:362] Time since start: 62667.88s, 	Step: 52586, 	{'train/ctc_loss': DeviceArray(0.59174687, dtype=float32), 'train/wer': 0.1890951208240958, 'validation/ctc_loss': DeviceArray(0.66507673, dtype=float32), 'validation/wer': 0.19214850119152138, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3961193, dtype=float32), 'test/wer': 0.1268458147990169, 'test/num_examples': 2472}
I0316 12:33:54.951922 139521182127872 logging_writer.py:48] [52586] global_step=52586, preemption_count=0, score=59957.824938, test/ctc_loss=0.39611929655075073, test/num_examples=2472, test/wer=0.126846, total_duration=62667.878462, train/ctc_loss=0.5917468667030334, train/wer=0.189095, validation/ctc_loss=0.665076732635498, validation/num_examples=5348, validation/wer=0.192149
I0316 12:33:55.091086 139695971202880 checkpoints.py:356] Saving checkpoint at step: 52586
I0316 12:33:55.740528 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_52586
I0316 12:33:55.755855 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_52586.
I0316 12:34:12.573579 139521173735168 logging_writer.py:48] [52600] global_step=52600, grad_norm=2.3988680839538574, loss=1.6476197242736816
I0316 12:36:04.717505 139519684228864 logging_writer.py:48] [52700] global_step=52700, grad_norm=2.2478644847869873, loss=1.5927700996398926
I0316 12:37:58.914748 139521173735168 logging_writer.py:48] [52800] global_step=52800, grad_norm=2.3664119243621826, loss=1.5813225507736206
I0316 12:39:50.703670 139519684228864 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.9816701412200928, loss=1.6306711435317993
I0316 12:41:42.401282 139521173735168 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.1265342235565186, loss=1.5865063667297363
I0316 12:43:34.202422 139519684228864 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.570934534072876, loss=1.678829550743103
I0316 12:45:26.692472 139521173735168 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.268353223800659, loss=1.7010517120361328
I0316 12:47:20.334403 139519684228864 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.6579928398132324, loss=1.5782175064086914
I0316 12:49:17.424346 139521173735168 logging_writer.py:48] [53400] global_step=53400, grad_norm=2.731184482574463, loss=1.6743483543395996
I0316 12:51:14.359041 139519684228864 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.443718671798706, loss=1.6381752490997314
I0316 12:53:14.840303 139521182127872 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.4655675888061523, loss=1.6067899465560913
I0316 12:55:08.879509 139521173735168 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.906886577606201, loss=1.6104319095611572
I0316 12:57:00.828541 139521182127872 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.9227768182754517, loss=1.5980224609375
I0316 12:58:52.970093 139521173735168 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.544365167617798, loss=1.539329171180725
I0316 13:00:46.657963 139521182127872 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.430511474609375, loss=1.598759412765503
I0316 13:02:41.080812 139521173735168 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.7911062240600586, loss=1.6349561214447021
I0316 13:04:33.628291 139521182127872 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.562873363494873, loss=1.5740747451782227
I0316 13:06:29.813378 139521173735168 logging_writer.py:48] [54300] global_step=54300, grad_norm=2.330676555633545, loss=1.623167634010315
I0316 13:08:22.991352 139521182127872 logging_writer.py:48] [54400] global_step=54400, grad_norm=4.0268754959106445, loss=1.5869557857513428
I0316 13:10:18.256951 139521173735168 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.2357795238494873, loss=1.5261493921279907
I0316 13:12:13.704367 139521182127872 logging_writer.py:48] [54600] global_step=54600, grad_norm=4.944948196411133, loss=1.5819902420043945
I0316 13:13:56.503378 139695971202880 spec.py:298] Evaluating on the training split.
I0316 13:14:33.909187 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 13:15:13.450599 139695971202880 spec.py:326] Evaluating on the test split.
I0316 13:15:33.615835 139695971202880 submission_runner.py:362] Time since start: 65166.54s, 	Step: 54693, 	{'train/ctc_loss': DeviceArray(0.5352647, dtype=float32), 'train/wer': 0.16660092199541945, 'validation/ctc_loss': DeviceArray(0.6480917, dtype=float32), 'validation/wer': 0.18710262520622487, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38718212, dtype=float32), 'test/wer': 0.12377876627465317, 'test/num_examples': 2472}
I0316 13:15:33.642382 139521612207872 logging_writer.py:48] [54693] global_step=54693, preemption_count=0, score=62354.072063, test/ctc_loss=0.3871821165084839, test/num_examples=2472, test/wer=0.123779, total_duration=65166.540109, train/ctc_loss=0.5352646708488464, train/wer=0.166601, validation/ctc_loss=0.6480916738510132, validation/num_examples=5348, validation/wer=0.187103
I0316 13:15:33.786191 139695971202880 checkpoints.py:356] Saving checkpoint at step: 54693
I0316 13:15:34.417507 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_54693
I0316 13:15:34.432997 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_54693.
I0316 13:15:43.447861 139521603815168 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.4653635025024414, loss=1.5777562856674194
I0316 13:17:35.521366 139521352034048 logging_writer.py:48] [54800] global_step=54800, grad_norm=4.257105827331543, loss=1.666148066520691
I0316 13:19:27.553882 139521603815168 logging_writer.py:48] [54900] global_step=54900, grad_norm=6.07472038269043, loss=1.637237310409546
I0316 13:21:20.917091 139521352034048 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.566492795944214, loss=1.5919781923294067
I0316 13:23:13.956313 139521603815168 logging_writer.py:48] [55100] global_step=55100, grad_norm=2.499812126159668, loss=1.623157262802124
I0316 13:25:06.016097 139521352034048 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.406684160232544, loss=1.5889593362808228
I0316 13:26:59.537306 139521603815168 logging_writer.py:48] [55300] global_step=55300, grad_norm=4.044795036315918, loss=1.6143567562103271
I0316 13:28:54.050247 139521352034048 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.0181968212127686, loss=1.6051639318466187
I0316 13:30:46.540863 139521603815168 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.0638608932495117, loss=1.5610551834106445
I0316 13:32:41.844740 139521352034048 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.513139247894287, loss=1.594994068145752
I0316 13:34:39.557955 139521284527872 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.449971914291382, loss=1.6389635801315308
I0316 13:36:35.463420 139521276135168 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.4193997383117676, loss=1.5807334184646606
I0316 13:38:28.442867 139521284527872 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.2003183364868164, loss=1.5944944620132446
I0316 13:40:20.494744 139521276135168 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.8906644582748413, loss=1.6135523319244385
I0316 13:42:12.757854 139521284527872 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.1082091331481934, loss=1.6269900798797607
I0316 13:44:05.377460 139521276135168 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.7171111106872559, loss=1.5342594385147095
I0316 13:46:00.100838 139521284527872 logging_writer.py:48] [56300] global_step=56300, grad_norm=2.1814093589782715, loss=1.6117464303970337
I0316 13:47:52.860181 139521276135168 logging_writer.py:48] [56400] global_step=56400, grad_norm=5.52250337600708, loss=1.581796407699585
I0316 13:49:45.902830 139521284527872 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.457603931427002, loss=1.5840901136398315
I0316 13:51:38.261857 139521276135168 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.467597007751465, loss=1.5430572032928467
I0316 13:53:34.125545 139521612207872 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.006110668182373, loss=1.6011689901351929
I0316 13:55:28.177767 139521603815168 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9356144666671753, loss=1.579789161682129
I0316 13:55:35.061003 139695971202880 spec.py:298] Evaluating on the training split.
I0316 13:56:14.042681 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 13:56:54.364394 139695971202880 spec.py:326] Evaluating on the test split.
I0316 13:57:14.958926 139695971202880 submission_runner.py:362] Time since start: 67665.10s, 	Step: 56807, 	{'train/ctc_loss': DeviceArray(0.50082695, dtype=float32), 'train/wer': 0.16171378308262724, 'validation/ctc_loss': DeviceArray(0.65364826, dtype=float32), 'validation/wer': 0.18780692529595075, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38501444, dtype=float32), 'test/wer': 0.12481465683586213, 'test/num_examples': 2472}
I0316 13:57:14.985898 139521612207872 logging_writer.py:48] [56807] global_step=56807, preemption_count=0, score=64750.040594, test/ctc_loss=0.38501444458961487, test/num_examples=2472, test/wer=0.124815, total_duration=67665.097726, train/ctc_loss=0.5008269548416138, train/wer=0.161714, validation/ctc_loss=0.6536482572555542, validation/num_examples=5348, validation/wer=0.187807
I0316 13:57:15.128677 139695971202880 checkpoints.py:356] Saving checkpoint at step: 56807
I0316 13:57:15.794133 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_56807
I0316 13:57:15.809808 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_56807.
I0316 13:59:00.851561 139521603815168 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.5013082027435303, loss=1.5397486686706543
I0316 14:00:54.653319 139519995131648 logging_writer.py:48] [57000] global_step=57000, grad_norm=4.122648239135742, loss=1.5650131702423096
I0316 14:02:49.342491 139521603815168 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.2553272247314453, loss=1.5551789999008179
I0316 14:04:41.153902 139519995131648 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.147754430770874, loss=1.5617363452911377
I0316 14:06:32.998663 139521603815168 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.991326332092285, loss=1.5624922513961792
I0316 14:08:26.999248 139519995131648 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.617450714111328, loss=1.5341757535934448
I0316 14:10:23.358679 139521603815168 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.3746490478515625, loss=1.583845853805542
I0316 14:12:19.119962 139519995131648 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.429065227508545, loss=1.5934704542160034
I0316 14:14:18.529218 139521612207872 logging_writer.py:48] [57700] global_step=57700, grad_norm=2.4946396350860596, loss=1.5453383922576904
I0316 14:16:12.732893 139521603815168 logging_writer.py:48] [57800] global_step=57800, grad_norm=2.2560107707977295, loss=1.5335761308670044
I0316 14:18:04.442507 139521612207872 logging_writer.py:48] [57900] global_step=57900, grad_norm=2.6709213256835938, loss=1.5225225687026978
I0316 14:19:56.475545 139521603815168 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.8839781284332275, loss=1.5295054912567139
I0316 14:21:50.378706 139521612207872 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.7975451946258545, loss=1.5535658597946167
I0316 14:23:43.323566 139521603815168 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.377136707305908, loss=1.5008893013000488
I0316 14:25:40.135828 139521612207872 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.87565279006958, loss=1.5319396257400513
I0316 14:27:33.463443 139521603815168 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.104037284851074, loss=1.4924904108047485
I0316 14:29:25.218385 139521612207872 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.151491641998291, loss=1.5489850044250488
I0316 14:31:17.015635 139521603815168 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.856172800064087, loss=1.5556977987289429
I0316 14:33:09.093956 139521612207872 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.9639127254486084, loss=1.6183196306228638
I0316 14:35:05.640206 139521612207872 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.1926181316375732, loss=1.5466985702514648
I0316 14:36:59.446862 139521603815168 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.516364097595215, loss=1.5286649465560913
I0316 14:37:16.659563 139695971202880 spec.py:298] Evaluating on the training split.
I0316 14:37:55.522057 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 14:38:34.614391 139695971202880 spec.py:326] Evaluating on the test split.
I0316 14:38:54.986754 139695971202880 submission_runner.py:362] Time since start: 70166.70s, 	Step: 58916, 	{'train/ctc_loss': DeviceArray(0.44363508, dtype=float32), 'train/wer': 0.14610816865871298, 'validation/ctc_loss': DeviceArray(0.6260144, dtype=float32), 'validation/wer': 0.18001138457679283, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.37110373, dtype=float32), 'test/wer': 0.11863993662787155, 'test/num_examples': 2472}
I0316 14:38:55.007156 139521612207872 logging_writer.py:48] [58916] global_step=58916, preemption_count=0, score=67146.279909, test/ctc_loss=0.3711037337779999, test/num_examples=2472, test/wer=0.118640, total_duration=70166.696290, train/ctc_loss=0.44363507628440857, train/wer=0.146108, validation/ctc_loss=0.6260144114494324, validation/num_examples=5348, validation/wer=0.180011
I0316 14:38:55.423460 139695971202880 checkpoints.py:356] Saving checkpoint at step: 58916
I0316 14:38:56.137192 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_58916
I0316 14:38:56.152480 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_58916.
I0316 14:40:32.974852 139521603815168 logging_writer.py:48] [59000] global_step=59000, grad_norm=2.547626256942749, loss=1.497666597366333
I0316 14:42:26.717015 139521007568640 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.7499616146087646, loss=1.4915897846221924
I0316 14:44:19.185355 139521603815168 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.110774517059326, loss=1.4925458431243896
I0316 14:46:10.871694 139521007568640 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8656275272369385, loss=1.4930111169815063
I0316 14:48:02.599884 139521603815168 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.081714630126953, loss=1.5943374633789062
I0316 14:49:54.403333 139521007568640 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.1877551078796387, loss=1.5460485219955444
I0316 14:51:47.887788 139521603815168 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.1766648292541504, loss=1.5264190435409546
I0316 14:53:41.037001 139521007568640 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.395707368850708, loss=1.4819761514663696
I0316 14:55:38.873331 139521612207872 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.3417718410491943, loss=1.5792052745819092
I0316 14:57:36.264033 139521603815168 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.6825737953186035, loss=1.5437424182891846
I0316 14:59:33.676758 139521612207872 logging_writer.py:48] [60000] global_step=60000, grad_norm=4.555166244506836, loss=1.530791163444519
I0316 15:01:26.542619 139521603815168 logging_writer.py:48] [60100] global_step=60100, grad_norm=4.142651081085205, loss=1.6359281539916992
I0316 15:03:18.827081 139521612207872 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.800706148147583, loss=1.5647733211517334
I0316 15:05:10.660168 139521603815168 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.879462242126465, loss=1.5303494930267334
I0316 15:07:02.721467 139521612207872 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8635295629501343, loss=1.5180174112319946
I0316 15:08:54.605656 139521603815168 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.7438271045684814, loss=1.5628728866577148
I0316 15:10:46.481348 139521612207872 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.7428964376449585, loss=1.5274540185928345
I0316 15:12:38.279693 139521603815168 logging_writer.py:48] [60700] global_step=60700, grad_norm=2.1624038219451904, loss=1.538038730621338
I0316 15:14:33.818334 139521612207872 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.675302505493164, loss=1.4688690900802612
I0316 15:16:26.600207 139521603815168 logging_writer.py:48] [60900] global_step=60900, grad_norm=4.303534507751465, loss=1.5037513971328735
I0316 15:18:22.002613 139521612207872 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.8759915828704834, loss=1.5557752847671509
I0316 15:18:57.030232 139695971202880 spec.py:298] Evaluating on the training split.
I0316 15:19:35.874307 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 15:20:14.698211 139695971202880 spec.py:326] Evaluating on the test split.
I0316 15:20:35.422107 139695971202880 submission_runner.py:362] Time since start: 72667.07s, 	Step: 61031, 	{'train/ctc_loss': DeviceArray(0.48412248, dtype=float32), 'train/wer': 0.15694602095292923, 'validation/ctc_loss': DeviceArray(0.622409, dtype=float32), 'validation/wer': 0.1797701859159278, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.36532286, dtype=float32), 'test/wer': 0.11849775557045071, 'test/num_examples': 2472}
I0316 15:20:35.445947 139520598447872 logging_writer.py:48] [61031] global_step=61031, preemption_count=0, score=69542.489112, test/ctc_loss=0.36532285809516907, test/num_examples=2472, test/wer=0.118498, total_duration=72667.066968, train/ctc_loss=0.48412248492240906, train/wer=0.156946, validation/ctc_loss=0.6224089860916138, validation/num_examples=5348, validation/wer=0.179770
I0316 15:20:35.592342 139695971202880 checkpoints.py:356] Saving checkpoint at step: 61031
I0316 15:20:36.244930 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_61031
I0316 15:20:36.260437 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_61031.
I0316 15:21:54.692423 139520590055168 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.293647289276123, loss=1.5494496822357178
I0316 15:23:50.230268 139520313095936 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.5751476287841797, loss=1.5354950428009033
I0316 15:25:43.784561 139520590055168 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.8692870140075684, loss=1.50931715965271
I0316 15:27:35.610425 139520313095936 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.3201119899749756, loss=1.5330172777175903
I0316 15:29:27.628422 139520590055168 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.4106335639953613, loss=1.507470726966858
I0316 15:31:19.805126 139520313095936 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.468489646911621, loss=1.542707085609436
I0316 15:33:11.895257 139520590055168 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.6443088054656982, loss=1.4715769290924072
I0316 15:35:07.735298 139520270767872 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.7194011211395264, loss=1.4972859621047974
I0316 15:36:59.814959 139520262375168 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.0469579696655273, loss=1.500547170639038
I0316 15:38:51.840494 139520270767872 logging_writer.py:48] [62000] global_step=62000, grad_norm=4.217136859893799, loss=1.509342074394226
I0316 15:40:45.033473 139520262375168 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.416309118270874, loss=1.4789708852767944
I0316 15:42:37.301189 139520270767872 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.397829532623291, loss=1.511318325996399
I0316 15:44:29.794739 139520262375168 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.1000144481658936, loss=1.5256825685501099
I0316 15:46:22.005199 139520270767872 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.49362850189209, loss=1.5443568229675293
I0316 15:48:14.476517 139520262375168 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.0066678524017334, loss=1.4642544984817505
I0316 15:50:06.954248 139520270767872 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.8360838890075684, loss=1.5121257305145264
I0316 15:51:58.958154 139520262375168 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.5306999683380127, loss=1.4499784708023071
I0316 15:53:51.997934 139520270767872 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.7456893920898438, loss=1.4736706018447876
I0316 15:55:48.261566 139520270767872 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.657067060470581, loss=1.5421375036239624
I0316 15:57:40.872638 139520262375168 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.0851378440856934, loss=1.532137393951416
I0316 15:59:33.801516 139520270767872 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.3865561485290527, loss=1.4944088459014893
I0316 16:00:37.409719 139695971202880 spec.py:298] Evaluating on the training split.
I0316 16:01:16.105005 139695971202880 spec.py:310] Evaluating on the validation split.
I0316 16:01:56.307492 139695971202880 spec.py:326] Evaluating on the test split.
I0316 16:02:16.583437 139695971202880 submission_runner.py:362] Time since start: 75167.45s, 	Step: 63157, 	{'train/ctc_loss': DeviceArray(0.43825582, dtype=float32), 'train/wer': 0.14569017350970623, 'validation/ctc_loss': DeviceArray(0.6049114, dtype=float32), 'validation/wer': 0.17438663180542022, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.35514513, dtype=float32), 'test/wer': 0.11486198281640363, 'test/num_examples': 2472}
I0316 16:02:16.613541 139521397167872 logging_writer.py:48] [63157] global_step=63157, preemption_count=0, score=71939.121822, test/ctc_loss=0.355145126581192, test/num_examples=2472, test/wer=0.114862, total_duration=75167.446455, train/ctc_loss=0.43825581669807434, train/wer=0.145690, validation/ctc_loss=0.6049113869667053, validation/num_examples=5348, validation/wer=0.174387
I0316 16:02:16.766823 139695971202880 checkpoints.py:356] Saving checkpoint at step: 63157
I0316 16:02:17.534498 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_63157
I0316 16:02:17.549230 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_63157.
I0316 16:03:08.389316 139521388775168 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.6123058795928955, loss=1.5351070165634155
I0316 16:03:19.460942 139521103423232 logging_writer.py:48] [63211] global_step=63211, preemption_count=0, score=72000.875514
I0316 16:03:19.811145 139695971202880 checkpoints.py:356] Saving checkpoint at step: 63211
I0316 16:03:20.560134 139695971202880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_63211
I0316 16:03:20.575268 139695971202880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_jax/trial_1/checkpoint_63211.
I0316 16:03:22.749123 139695971202880 submission_runner.py:523] Tuning trial 1/1
I0316 16:03:22.749403 139695971202880 submission_runner.py:524] Hyperparameters: Hyperparameters(learning_rate=0.004958460849689891, beta1=0.863744242567442, beta2=0.6291854735396584, warmup_steps=1200, weight_decay=0.1147386261512052)
I0316 16:03:22.763050 139695971202880 submission_runner.py:525] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.193344, dtype=float32), 'train/wer': 4.242477562834475, 'validation/ctc_loss': DeviceArray(30.926653, dtype=float32), 'validation/wer': 3.8057675423786046, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.063675, dtype=float32), 'test/wer': 4.079215160563037, 'test/num_examples': 2472, 'score': 56.49442148208618, 'total_duration': 56.69697904586792, 'global_step': 1, 'preemption_count': 0}), (2077, {'train/ctc_loss': DeviceArray(0.81215405, dtype=float32), 'train/wer': 0.25568474387757206, 'validation/ctc_loss': DeviceArray(1.2603388, dtype=float32), 'validation/wer': 0.34052426940925623, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8674421, dtype=float32), 'test/wer': 0.26736132269006563, 'test/num_examples': 2472, 'score': 2453.4031801223755, 'total_duration': 2642.919138431549, 'global_step': 2077, 'preemption_count': 0}), (4182, {'train/ctc_loss': DeviceArray(0.6369123, dtype=float32), 'train/wer': 0.2090829182537357, 'validation/ctc_loss': DeviceArray(1.0139186, dtype=float32), 'validation/wer': 0.28464336366004495, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.66698223, dtype=float32), 'test/wer': 0.21302784717567486, 'test/num_examples': 2472, 'score': 4849.879686355591, 'total_duration': 5143.116612672806, 'global_step': 4182, 'preemption_count': 0}), (6284, {'train/ctc_loss': DeviceArray(0.5552566, dtype=float32), 'train/wer': 0.1862833467748312, 'validation/ctc_loss': DeviceArray(0.9586521, dtype=float32), 'validation/wer': 0.26954432748989376, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6149973, dtype=float32), 'test/wer': 0.1948286718258079, 'test/num_examples': 2472, 'score': 7245.5878846645355, 'total_duration': 7643.572221040726, 'global_step': 6284, 'preemption_count': 0}), (8378, {'train/ctc_loss': DeviceArray(0.59614384, dtype=float32), 'train/wer': 0.19433652043690897, 'validation/ctc_loss': DeviceArray(0.965135, dtype=float32), 'validation/wer': 0.2709239838300418, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.630256, dtype=float32), 'test/wer': 0.2011861962504824, 'test/num_examples': 2472, 'score': 9641.359872341156, 'total_duration': 10145.284651517868, 'global_step': 8378, 'preemption_count': 0}), (10487, {'train/ctc_loss': DeviceArray(0.545344, dtype=float32), 'train/wer': 0.1800702341864359, 'validation/ctc_loss': DeviceArray(0.8962344, dtype=float32), 'validation/wer': 0.25514959140946847, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5680764, dtype=float32), 'test/wer': 0.18111835557451303, 'test/num_examples': 2472, 'score': 12036.8136780262, 'total_duration': 12646.79096698761, 'global_step': 10487, 'preemption_count': 0}), (12611, {'train/ctc_loss': DeviceArray(0.5447605, dtype=float32), 'train/wer': 0.17707027436503406, 'validation/ctc_loss': DeviceArray(0.8583534, dtype=float32), 'validation/wer': 0.2454630531891287, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.54895985, dtype=float32), 'test/wer': 0.17579672171104746, 'test/num_examples': 2472, 'score': 14432.291597127914, 'total_duration': 15146.853663682938, 'global_step': 12611, 'preemption_count': 0}), (14724, {'train/ctc_loss': DeviceArray(0.52372175, dtype=float32), 'train/wer': 0.17187267351583604, 'validation/ctc_loss': DeviceArray(0.85709625, dtype=float32), 'validation/wer': 0.24445966675993014, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.55053353, dtype=float32), 'test/wer': 0.17384681006641886, 'test/num_examples': 2472, 'score': 16828.270414590836, 'total_duration': 17648.339059114456, 'global_step': 14724, 'preemption_count': 0}), (16838, {'train/ctc_loss': DeviceArray(0.48307234, dtype=float32), 'train/wer': 0.16286954939025974, 'validation/ctc_loss': DeviceArray(0.85676265, dtype=float32), 'validation/wer': 0.24295458711613233, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.54435754, dtype=float32), 'test/wer': 0.17238437633294743, 'test/num_examples': 2472, 'score': 19224.702160596848, 'total_duration': 20150.489716768265, 'global_step': 16838, 'preemption_count': 0}), (18941, {'train/ctc_loss': DeviceArray(0.45573974, dtype=float32), 'train/wer': 0.14906713341029637, 'validation/ctc_loss': DeviceArray(0.82808894, dtype=float32), 'validation/wer': 0.2351301025576706, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.52317816, dtype=float32), 'test/wer': 0.1670830540491134, 'test/num_examples': 2472, 'score': 21620.54284977913, 'total_duration': 22651.358412504196, 'global_step': 18941, 'preemption_count': 0}), (21060, {'train/ctc_loss': DeviceArray(0.48157805, dtype=float32), 'train/wer': 0.16443123984404748, 'validation/ctc_loss': DeviceArray(0.82588047, dtype=float32), 'validation/wer': 0.2345608737180291, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5237341, dtype=float32), 'test/wer': 0.16822050250848009, 'test/num_examples': 2472, 'score': 24016.081519126892, 'total_duration': 25151.195613622665, 'global_step': 21060, 'preemption_count': 0}), (23135, {'train/ctc_loss': DeviceArray(0.4736947, dtype=float32), 'train/wer': 0.15400195668843703, 'validation/ctc_loss': DeviceArray(0.80814207, dtype=float32), 'validation/wer': 0.2295632374649056, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.50509924, dtype=float32), 'test/wer': 0.16058334856701806, 'test/num_examples': 2472, 'score': 26412.23876953125, 'total_duration': 27652.51610302925, 'global_step': 23135, 'preemption_count': 0}), (25237, {'train/ctc_loss': DeviceArray(0.50195736, dtype=float32), 'train/wer': 0.16301474661671986, 'validation/ctc_loss': DeviceArray(0.8191411, dtype=float32), 'validation/wer': 0.23173402541269092, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5072574, dtype=float32), 'test/wer': 0.1615379928097008, 'test/num_examples': 2472, 'score': 28808.717685699463, 'total_duration': 30153.240557432175, 'global_step': 25237, 'preemption_count': 0}), (27338, {'train/ctc_loss': DeviceArray(0.48064682, dtype=float32), 'train/wer': 0.15680111122543472, 'validation/ctc_loss': DeviceArray(0.80080104, dtype=float32), 'validation/wer': 0.22917731960752155, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.49994656, dtype=float32), 'test/wer': 0.16038023277070257, 'test/num_examples': 2472, 'score': 31205.187256336212, 'total_duration': 32654.590095758438, 'global_step': 27338, 'preemption_count': 0}), (29446, {'train/ctc_loss': DeviceArray(0.46080554, dtype=float32), 'train/wer': 0.15309214137916474, 'validation/ctc_loss': DeviceArray(0.80033123, dtype=float32), 'validation/wer': 0.22792308657102336, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4957157, dtype=float32), 'test/wer': 0.1604817906688603, 'test/num_examples': 2472, 'score': 33601.09483957291, 'total_duration': 35156.90988063812, 'global_step': 29446, 'preemption_count': 0}), (31549, {'train/ctc_loss': DeviceArray(0.43244648, dtype=float32), 'train/wer': 0.14583061634367095, 'validation/ctc_loss': DeviceArray(0.7749419, dtype=float32), 'validation/wer': 0.22092832540593735, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47858682, dtype=float32), 'test/wer': 0.15339304937745007, 'test/num_examples': 2472, 'score': 35997.58594942093, 'total_duration': 37657.81770682335, 'global_step': 31549, 'preemption_count': 0}), (33659, {'train/ctc_loss': DeviceArray(0.41294086, dtype=float32), 'train/wer': 0.13861472215687679, 'validation/ctc_loss': DeviceArray(0.76251364, dtype=float32), 'validation/wer': 0.21762872772530367, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.47610524, dtype=float32), 'test/wer': 0.152093108281031, 'test/num_examples': 2472, 'score': 38393.68270778656, 'total_duration': 40159.74313926697, 'global_step': 33659, 'preemption_count': 0}), (35744, {'train/ctc_loss': DeviceArray(0.44583458, dtype=float32), 'train/wer': 0.14954931530594556, 'validation/ctc_loss': DeviceArray(0.750568, dtype=float32), 'validation/wer': 0.21619118370654805, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45987254, dtype=float32), 'test/wer': 0.14841671236772083, 'test/num_examples': 2472, 'score': 40789.26385951042, 'total_duration': 42660.97612309456, 'global_step': 35744, 'preemption_count': 0}), (37859, {'train/ctc_loss': DeviceArray(0.41303307, dtype=float32), 'train/wer': 0.13608863857101774, 'validation/ctc_loss': DeviceArray(0.7354816, dtype=float32), 'validation/wer': 0.21175312834663143, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45660117, dtype=float32), 'test/wer': 0.14774643023987977, 'test/num_examples': 2472, 'score': 43184.8304169178, 'total_duration': 45160.58461713791, 'global_step': 37859, 'preemption_count': 0}), (39977, {'train/ctc_loss': DeviceArray(0.43436962, dtype=float32), 'train/wer': 0.14176884866106057, 'validation/ctc_loss': DeviceArray(0.7282851, dtype=float32), 'validation/wer': 0.2097174116489305, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44979516, dtype=float32), 'test/wer': 0.1445778238173583, 'test/num_examples': 2472, 'score': 45581.040444374084, 'total_duration': 47660.82203197479, 'global_step': 39977, 'preemption_count': 0}), (42074, {'train/ctc_loss': DeviceArray(0.41445267, dtype=float32), 'train/wer': 0.13429670599017446, 'validation/ctc_loss': DeviceArray(0.7203168, dtype=float32), 'validation/wer': 0.2095148047738039, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4378079, dtype=float32), 'test/wer': 0.14151077529299452, 'test/num_examples': 2472, 'score': 47977.30632472038, 'total_duration': 50162.74071764946, 'global_step': 42074, 'preemption_count': 0}), (44167, {'train/ctc_loss': DeviceArray(0.35354257, dtype=float32), 'train/wer': 0.11913327798448183, 'validation/ctc_loss': DeviceArray(0.71076226, dtype=float32), 'validation/wer': 0.20510569325319106, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.43437228, dtype=float32), 'test/wer': 0.13736721304815874, 'test/num_examples': 2472, 'score': 50373.30237317085, 'total_duration': 52663.37249660492, 'global_step': 44167, 'preemption_count': 0}), (46284, {'train/ctc_loss': DeviceArray(0.39207852, dtype=float32), 'train/wer': 0.12940064623767186, 'validation/ctc_loss': DeviceArray(0.6987011, dtype=float32), 'validation/wer': 0.20069658173257823, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.42200115, dtype=float32), 'test/wer': 0.13478764243495217, 'test/num_examples': 2472, 'score': 52769.78800344467, 'total_duration': 55164.29953765869, 'global_step': 46284, 'preemption_count': 0}), (48368, {'train/ctc_loss': DeviceArray(0.5084408, dtype=float32), 'train/wer': 0.16248379555550735, 'validation/ctc_loss': DeviceArray(0.68937474, dtype=float32), 'validation/wer': 0.1984100184275777, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4183758, dtype=float32), 'test/wer': 0.1356407287794772, 'test/num_examples': 2472, 'score': 55165.87365937233, 'total_duration': 57666.04990029335, 'global_step': 48368, 'preemption_count': 0}), (50480, {'train/ctc_loss': DeviceArray(0.5224714, dtype=float32), 'train/wer': 0.16567390375103394, 'validation/ctc_loss': DeviceArray(0.6699646, dtype=float32), 'validation/wer': 0.19279491360263967, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.40971416, dtype=float32), 'test/wer': 0.13159872443279913, 'test/num_examples': 2472, 'score': 57562.15951657295, 'total_duration': 60167.77536702156, 'global_step': 50480, 'preemption_count': 0}), (52586, {'train/ctc_loss': DeviceArray(0.59174687, dtype=float32), 'train/wer': 0.1890951208240958, 'validation/ctc_loss': DeviceArray(0.66507673, dtype=float32), 'validation/wer': 0.19214850119152138, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3961193, dtype=float32), 'test/wer': 0.1268458147990169, 'test/num_examples': 2472, 'score': 59957.82493805885, 'total_duration': 62667.87846183777, 'global_step': 52586, 'preemption_count': 0}), (54693, {'train/ctc_loss': DeviceArray(0.5352647, dtype=float32), 'train/wer': 0.16660092199541945, 'validation/ctc_loss': DeviceArray(0.6480917, dtype=float32), 'validation/wer': 0.18710262520622487, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38718212, dtype=float32), 'test/wer': 0.12377876627465317, 'test/num_examples': 2472, 'score': 62354.07206273079, 'total_duration': 65166.540108919144, 'global_step': 54693, 'preemption_count': 0}), (56807, {'train/ctc_loss': DeviceArray(0.50082695, dtype=float32), 'train/wer': 0.16171378308262724, 'validation/ctc_loss': DeviceArray(0.65364826, dtype=float32), 'validation/wer': 0.18780692529595075, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38501444, dtype=float32), 'test/wer': 0.12481465683586213, 'test/num_examples': 2472, 'score': 64750.04059433937, 'total_duration': 67665.0977256298, 'global_step': 56807, 'preemption_count': 0}), (58916, {'train/ctc_loss': DeviceArray(0.44363508, dtype=float32), 'train/wer': 0.14610816865871298, 'validation/ctc_loss': DeviceArray(0.6260144, dtype=float32), 'validation/wer': 0.18001138457679283, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.37110373, dtype=float32), 'test/wer': 0.11863993662787155, 'test/num_examples': 2472, 'score': 67146.27990889549, 'total_duration': 70166.69629001617, 'global_step': 58916, 'preemption_count': 0}), (61031, {'train/ctc_loss': DeviceArray(0.48412248, dtype=float32), 'train/wer': 0.15694602095292923, 'validation/ctc_loss': DeviceArray(0.622409, dtype=float32), 'validation/wer': 0.1797701859159278, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.36532286, dtype=float32), 'test/wer': 0.11849775557045071, 'test/num_examples': 2472, 'score': 69542.48911190033, 'total_duration': 72667.06696772575, 'global_step': 61031, 'preemption_count': 0}), (63157, {'train/ctc_loss': DeviceArray(0.43825582, dtype=float32), 'train/wer': 0.14569017350970623, 'validation/ctc_loss': DeviceArray(0.6049114, dtype=float32), 'validation/wer': 0.17438663180542022, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.35514513, dtype=float32), 'test/wer': 0.11486198281640363, 'test/num_examples': 2472, 'score': 71939.12182211876, 'total_duration': 75167.44645500183, 'global_step': 63157, 'preemption_count': 0})], 'global_step': 63211}
I0316 16:03:22.763292 139695971202880 submission_runner.py:526] Timing: 72000.87551403046
I0316 16:03:22.763354 139695971202880 submission_runner.py:527] ====================
I0316 16:03:22.766174 139695971202880 submission_runner.py:586] Final librispeech_deepspeech score: 72000.87551403046
