I0313 20:26:45.453193 140362249443136 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/wmt_jax.
I0313 20:26:45.499634 140362249443136 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0313 20:26:46.387284 140362249443136 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0313 20:26:46.387863 140362249443136 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0313 20:26:46.390921 140362249443136 submission_runner.py:484] Using RNG seed 2326104725
I0313 20:26:47.637169 140362249443136 submission_runner.py:493] --- Tuning run 1/1 ---
I0313 20:26:47.637364 140362249443136 submission_runner.py:498] Creating tuning directory at /experiment_runs/timing/wmt_jax/trial_1.
I0313 20:26:47.637535 140362249443136 logger_utils.py:84] Saving hparams to /experiment_runs/timing/wmt_jax/trial_1/hparams.json.
I0313 20:26:47.757495 140362249443136 submission_runner.py:230] Initializing dataset.
I0313 20:26:47.764716 140362249443136 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0313 20:26:47.767821 140362249443136 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0313 20:26:47.767929 140362249443136 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0313 20:26:47.838793 140362249443136 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0313 20:26:49.583331 140362249443136 submission_runner.py:237] Initializing model.
I0313 20:27:01.072208 140362249443136 submission_runner.py:247] Initializing optimizer.
I0313 20:27:01.895124 140362249443136 submission_runner.py:254] Initializing metrics bundle.
I0313 20:27:01.895329 140362249443136 submission_runner.py:268] Initializing checkpoint and logger.
I0313 20:27:01.896128 140362249443136 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/wmt_jax/trial_1 with prefix checkpoint_
I0313 20:27:01.896375 140362249443136 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0313 20:27:01.896436 140362249443136 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0313 20:27:02.784008 140362249443136 submission_runner.py:289] Saving meta data to /experiment_runs/timing/wmt_jax/trial_1/meta_data_0.json.
I0313 20:27:02.784872 140362249443136 submission_runner.py:292] Saving flags to /experiment_runs/timing/wmt_jax/trial_1/flags_0.json.
I0313 20:27:02.787482 140362249443136 submission_runner.py:302] Starting training loop.
I0313 20:27:37.998376 140186079979264 logging_writer.py:48] [0] global_step=0, grad_norm=4.881795406341553, loss=11.146684646606445
I0313 20:27:38.008709 140362249443136 spec.py:298] Evaluating on the training split.
I0313 20:27:38.011226 140362249443136 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0313 20:27:38.013739 140362249443136 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0313 20:27:38.013862 140362249443136 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0313 20:27:38.044116 140362249443136 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0313 20:27:46.082756 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 20:32:51.268246 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 20:32:51.271376 140362249443136 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0313 20:32:51.274246 140362249443136 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0313 20:32:51.274358 140362249443136 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0313 20:32:51.305955 140362249443136 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0313 20:32:58.441201 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 20:37:55.207185 140362249443136 spec.py:326] Evaluating on the test split.
I0313 20:37:55.209548 140362249443136 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0313 20:37:55.211986 140362249443136 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0313 20:37:55.212086 140362249443136 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0313 20:37:55.238815 140362249443136 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0313 20:38:01.903887 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 20:42:53.141595 140362249443136 submission_runner.py:362] Time since start: 35.22s, 	Step: 1, 	{'train/accuracy': 0.0006027110503055155, 'train/loss': 11.159331321716309, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.150862693786621, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.170835494995117, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0313 20:42:53.149626 140175047849728 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=35.030888, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.170835, test/num_examples=3003, total_duration=35.221196, train/accuracy=0.000603, train/bleu=0.000000, train/loss=11.159331, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.150863, validation/num_examples=3000
I0313 20:42:54.091454 140362249443136 checkpoints.py:356] Saving checkpoint at step: 1
I0313 20:42:57.900490 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_1
I0313 20:42:57.904099 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_1.
I0313 20:43:35.485072 140175056242432 logging_writer.py:48] [100] global_step=100, grad_norm=0.15888197720050812, loss=8.209412574768066
I0313 20:44:13.109871 140175165347584 logging_writer.py:48] [200] global_step=200, grad_norm=0.6062433123588562, loss=7.304101467132568
I0313 20:44:50.808717 140175056242432 logging_writer.py:48] [300] global_step=300, grad_norm=0.735378086566925, loss=6.674458026885986
I0313 20:45:28.517499 140175165347584 logging_writer.py:48] [400] global_step=400, grad_norm=0.4557032287120819, loss=6.138596057891846
I0313 20:46:06.228611 140175056242432 logging_writer.py:48] [500] global_step=500, grad_norm=0.5617221593856812, loss=5.72279691696167
I0313 20:46:43.920601 140175165347584 logging_writer.py:48] [600] global_step=600, grad_norm=0.4262290596961975, loss=5.400308132171631
I0313 20:47:21.688604 140175056242432 logging_writer.py:48] [700] global_step=700, grad_norm=0.6456434726715088, loss=5.110318660736084
I0313 20:47:59.405894 140175165347584 logging_writer.py:48] [800] global_step=800, grad_norm=0.6516770720481873, loss=4.848538875579834
I0313 20:48:37.100513 140175056242432 logging_writer.py:48] [900] global_step=900, grad_norm=0.6015300750732422, loss=4.589355945587158
I0313 20:49:14.798967 140175165347584 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6004951596260071, loss=4.321408748626709
I0313 20:49:52.545353 140175056242432 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6348222494125366, loss=4.11568021774292
I0313 20:50:30.271346 140175165347584 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5210659503936768, loss=3.818687677383423
I0313 20:51:08.024870 140175056242432 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5422611236572266, loss=3.9127790927886963
I0313 20:51:45.726570 140175165347584 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5414677858352661, loss=3.676448106765747
I0313 20:52:23.422281 140175056242432 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.46361491084098816, loss=3.5810651779174805
I0313 20:53:01.109499 140175165347584 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.40050873160362244, loss=3.423025131225586
I0313 20:53:38.813505 140175056242432 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.3632589280605316, loss=3.315335988998413
I0313 20:54:16.538364 140175165347584 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5516777634620667, loss=3.2579946517944336
I0313 20:54:54.228113 140175056242432 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.34058067202568054, loss=3.1836180686950684
I0313 20:55:31.945544 140175165347584 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.34774935245513916, loss=3.1710119247436523
I0313 20:56:09.560932 140175056242432 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.30877912044525146, loss=2.9651424884796143
I0313 20:56:47.272111 140175165347584 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3608945906162262, loss=2.9696319103240967
I0313 20:56:57.940767 140362249443136 spec.py:298] Evaluating on the training split.
I0313 20:57:00.933393 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 20:59:57.714244 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 21:00:00.359838 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:02:45.031247 140362249443136 spec.py:326] Evaluating on the test split.
I0313 21:02:47.715005 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:05:16.134697 140362249443136 submission_runner.py:362] Time since start: 1795.15s, 	Step: 2230, 	{'train/accuracy': 0.5143572092056274, 'train/loss': 2.816361427307129, 'train/bleu': 22.219906944273397, 'validation/accuracy': 0.5151950716972351, 'validation/loss': 2.8118090629577637, 'validation/bleu': 18.070424568182716, 'validation/num_examples': 3000, 'test/accuracy': 0.5126140117645264, 'test/loss': 2.859344720840454, 'test/bleu': 16.560917835093097, 'test/num_examples': 3003}
I0313 21:05:16.142155 140175056242432 logging_writer.py:48] [2230] global_step=2230, preemption_count=0, score=871.670559, test/accuracy=0.512614, test/bleu=16.560918, test/loss=2.859345, test/num_examples=3003, total_duration=1795.153236, train/accuracy=0.514357, train/bleu=22.219907, train/loss=2.816361, validation/accuracy=0.515195, validation/bleu=18.070425, validation/loss=2.811809, validation/num_examples=3000
I0313 21:05:17.244520 140362249443136 checkpoints.py:356] Saving checkpoint at step: 2230
I0313 21:05:21.436210 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_2230
I0313 21:05:21.439763 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_2230.
I0313 21:05:48.100059 140175165347584 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.3590879440307617, loss=2.8808701038360596
I0313 21:06:25.649744 140175099287296 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2537786066532135, loss=2.906966209411621
I0313 21:07:03.247783 140175165347584 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.2637215852737427, loss=2.8377668857574463
I0313 21:07:40.947006 140175099287296 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.24661190807819366, loss=2.7345926761627197
I0313 21:08:18.650548 140175165347584 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.22579769790172577, loss=2.7663614749908447
I0313 21:08:56.364401 140175099287296 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.27099350094795227, loss=2.6977739334106445
I0313 21:09:34.017775 140175165347584 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.21441400051116943, loss=2.6507019996643066
I0313 21:10:11.698743 140175099287296 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2017003744840622, loss=2.5371458530426025
I0313 21:10:49.344589 140175165347584 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2019428312778473, loss=2.5455574989318848
I0313 21:11:27.023267 140175099287296 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.19092974066734314, loss=2.52254056930542
I0313 21:12:04.746508 140175165347584 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.2510214149951935, loss=2.6446714401245117
I0313 21:12:42.432314 140175099287296 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.18424946069717407, loss=2.4585273265838623
I0313 21:13:20.089339 140175165347584 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.1853240579366684, loss=2.493236780166626
I0313 21:13:57.768826 140175099287296 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.19072078168392181, loss=2.510324001312256
I0313 21:14:35.476847 140175165347584 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.1973865032196045, loss=2.423109769821167
I0313 21:15:13.139005 140175099287296 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.17346306145191193, loss=2.3596138954162598
I0313 21:15:50.788407 140175165347584 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.24670426547527313, loss=2.322183132171631
I0313 21:16:28.472850 140175099287296 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.1858239471912384, loss=2.445971727371216
I0313 21:17:06.173552 140175165347584 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.1638992726802826, loss=2.461796760559082
I0313 21:17:43.907086 140175099287296 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.17270521819591522, loss=2.320173978805542
I0313 21:18:21.553796 140175165347584 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.1767423301935196, loss=2.3419394493103027
I0313 21:18:59.232932 140175099287296 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.24557240307331085, loss=2.322675943374634
I0313 21:19:21.535871 140362249443136 spec.py:298] Evaluating on the training split.
I0313 21:19:24.519887 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:22:00.101713 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 21:22:02.755561 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:24:24.681850 140362249443136 spec.py:326] Evaluating on the test split.
I0313 21:24:27.379141 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:26:39.118200 140362249443136 submission_runner.py:362] Time since start: 3138.75s, 	Step: 4461, 	{'train/accuracy': 0.5813961625099182, 'train/loss': 2.2187016010284424, 'train/bleu': 27.086892822799186, 'validation/accuracy': 0.5903212428092957, 'validation/loss': 2.145246982574463, 'validation/bleu': 23.256466716545344, 'validation/num_examples': 3000, 'test/accuracy': 0.5921794176101685, 'test/loss': 2.125871419906616, 'test/bleu': 21.74518902577842, 'test/num_examples': 3003}
I0313 21:26:39.125826 140175165347584 logging_writer.py:48] [4461] global_step=4461, preemption_count=0, score=1707.885342, test/accuracy=0.592179, test/bleu=21.745189, test/loss=2.125871, test/num_examples=3003, total_duration=3138.748324, train/accuracy=0.581396, train/bleu=27.086893, train/loss=2.218702, validation/accuracy=0.590321, validation/bleu=23.256467, validation/loss=2.145247, validation/num_examples=3000
I0313 21:26:40.325007 140362249443136 checkpoints.py:356] Saving checkpoint at step: 4461
I0313 21:26:44.986584 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_4461
I0313 21:26:44.990534 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_4461.
I0313 21:27:00.032856 140175099287296 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.19515472650527954, loss=2.3173232078552246
I0313 21:27:37.619497 140174680880896 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.16193599998950958, loss=2.344059944152832
I0313 21:28:15.262081 140175099287296 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15843719244003296, loss=2.1771585941314697
I0313 21:28:52.973067 140174680880896 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.15512149035930634, loss=2.25895357131958
I0313 21:29:30.642242 140175099287296 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.18377850949764252, loss=2.2719414234161377
I0313 21:30:08.386228 140174680880896 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.14725978672504425, loss=2.2115557193756104
I0313 21:30:46.068367 140175099287296 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.14652852714061737, loss=2.247849941253662
I0313 21:31:23.764544 140174680880896 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.14641357958316803, loss=2.2994863986968994
I0313 21:32:01.407167 140175099287296 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.16438959538936615, loss=2.2051215171813965
I0313 21:32:39.016112 140174680880896 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1507437378168106, loss=2.259948253631592
I0313 21:33:16.678173 140175099287296 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.19432322680950165, loss=2.2066943645477295
I0313 21:33:54.354552 140174680880896 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.15712304413318634, loss=2.214998722076416
I0313 21:34:32.021316 140175099287296 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.16664473712444305, loss=2.2320597171783447
I0313 21:35:09.665827 140174680880896 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.15865609049797058, loss=2.1846320629119873
I0313 21:35:47.367166 140175099287296 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.15222235023975372, loss=2.18593430519104
I0313 21:36:25.041380 140174680880896 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.1601722091436386, loss=2.184309959411621
I0313 21:37:02.707122 140175099287296 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.15054386854171753, loss=2.2389473915100098
I0313 21:37:40.374241 140174680880896 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.14317619800567627, loss=2.1746115684509277
I0313 21:38:18.050044 140175099287296 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.15157195925712585, loss=2.1736931800842285
I0313 21:38:55.823467 140174680880896 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.13706117868423462, loss=2.2292768955230713
I0313 21:39:33.488864 140175099287296 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1631365716457367, loss=2.189438581466675
I0313 21:40:11.179018 140174680880896 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.1860778033733368, loss=2.1003870964050293
I0313 21:40:45.181377 140362249443136 spec.py:298] Evaluating on the training split.
I0313 21:40:48.155792 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:43:24.898345 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 21:43:27.534645 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:45:41.201145 140362249443136 spec.py:326] Evaluating on the test split.
I0313 21:45:43.886938 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 21:48:02.209121 140362249443136 submission_runner.py:362] Time since start: 4422.39s, 	Step: 6692, 	{'train/accuracy': 0.6101222634315491, 'train/loss': 1.9686866998672485, 'train/bleu': 29.085300689586337, 'validation/accuracy': 0.6167561411857605, 'validation/loss': 1.929276466369629, 'validation/bleu': 24.833213725935916, 'validation/num_examples': 3000, 'test/accuracy': 0.6195340156555176, 'test/loss': 1.8934417963027954, 'test/bleu': 23.621627750290173, 'test/num_examples': 3003}
I0313 21:48:02.216642 140175099287296 logging_writer.py:48] [6692] global_step=6692, preemption_count=0, score=2544.730269, test/accuracy=0.619534, test/bleu=23.621628, test/loss=1.893442, test/num_examples=3003, total_duration=4422.393855, train/accuracy=0.610122, train/bleu=29.085301, train/loss=1.968687, validation/accuracy=0.616756, validation/bleu=24.833214, validation/loss=1.929276, validation/num_examples=3000
I0313 21:48:03.437233 140362249443136 checkpoints.py:356] Saving checkpoint at step: 6692
I0313 21:48:07.822143 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_6692
I0313 21:48:07.825865 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_6692.
I0313 21:48:11.220381 140174680880896 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.15448813140392303, loss=2.10605525970459
I0313 21:48:48.787628 140174672488192 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.19453662633895874, loss=2.1469693183898926
I0313 21:49:26.408933 140174680880896 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.20426258444786072, loss=2.1567769050598145
I0313 21:50:04.007305 140174672488192 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.13664254546165466, loss=2.1889383792877197
I0313 21:50:41.663095 140174680880896 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.20323102176189423, loss=2.1470932960510254
I0313 21:51:19.319427 140174672488192 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.14720092713832855, loss=2.088502883911133
I0313 21:51:57.079676 140174680880896 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1673700511455536, loss=2.0427868366241455
I0313 21:52:34.785589 140174672488192 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.1403290331363678, loss=2.1010475158691406
I0313 21:53:12.515563 140174680880896 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.1304054856300354, loss=1.9828261137008667
I0313 21:53:50.198269 140174672488192 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1376659870147705, loss=2.1627912521362305
I0313 21:54:27.894752 140174680880896 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.14822790026664734, loss=2.0628163814544678
I0313 21:55:05.592190 140174672488192 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.13868406414985657, loss=2.022987127304077
I0313 21:55:43.269741 140174680880896 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.14713254570960999, loss=2.0915963649749756
I0313 21:56:20.938013 140174672488192 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.2583160996437073, loss=2.1146934032440186
I0313 21:56:58.611946 140174680880896 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.16095365583896637, loss=2.043400764465332
I0313 21:57:36.233711 140174672488192 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1937609314918518, loss=2.0251293182373047
I0313 21:58:13.874343 140174680880896 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.19392704963684082, loss=2.1207361221313477
I0313 21:58:51.529333 140174672488192 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.1461467295885086, loss=2.0357584953308105
I0313 21:59:29.171915 140174680880896 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.15882569551467896, loss=2.0428547859191895
I0313 22:00:06.848247 140174672488192 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.17948758602142334, loss=2.1218173503875732
I0313 22:00:44.545703 140174680880896 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.15667162835597992, loss=1.9729866981506348
I0313 22:01:22.282241 140174672488192 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.15847919881343842, loss=2.068758487701416
I0313 22:01:59.946309 140174680880896 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.22566676139831543, loss=1.9977401494979858
I0313 22:02:07.942226 140362249443136 spec.py:298] Evaluating on the training split.
I0313 22:02:10.919715 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:05:21.426936 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 22:05:24.056563 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:07:36.504214 140362249443136 spec.py:326] Evaluating on the test split.
I0313 22:07:39.226717 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:09:51.773266 140362249443136 submission_runner.py:362] Time since start: 5705.15s, 	Step: 8923, 	{'train/accuracy': 0.61446213722229, 'train/loss': 1.9436389207839966, 'train/bleu': 29.205921910986337, 'validation/accuracy': 0.6297379732131958, 'validation/loss': 1.8282923698425293, 'validation/bleu': 26.115767538692335, 'validation/num_examples': 3000, 'test/accuracy': 0.6348266005516052, 'test/loss': 1.7782090902328491, 'test/bleu': 24.86373861912968, 'test/num_examples': 3003}
I0313 22:09:51.780943 140174672488192 logging_writer.py:48] [8923] global_step=8923, preemption_count=0, score=3381.046310, test/accuracy=0.634827, test/bleu=24.863739, test/loss=1.778209, test/num_examples=3003, total_duration=5705.154695, train/accuracy=0.614462, train/bleu=29.205922, train/loss=1.943639, validation/accuracy=0.629738, validation/bleu=26.115768, validation/loss=1.828292, validation/num_examples=3000
I0313 22:09:52.987699 140362249443136 checkpoints.py:356] Saving checkpoint at step: 8923
I0313 22:09:56.531094 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_8923
I0313 22:09:56.536949 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_8923.
I0313 22:10:25.826356 140174680880896 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.14315184950828552, loss=2.056262254714966
I0313 22:11:03.383821 140174664095488 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.17151810228824615, loss=2.0099830627441406
I0313 22:11:41.091994 140174680880896 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1850336343050003, loss=2.168788433074951
I0313 22:12:18.844324 140174664095488 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.25563859939575195, loss=1.9929550886154175
I0313 22:12:56.487540 140174680880896 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.14982107281684875, loss=1.9804257154464722
I0313 22:13:34.136552 140174664095488 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.1564214527606964, loss=2.007652997970581
I0313 22:14:11.766556 140174680880896 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.24053683876991272, loss=2.0725045204162598
I0313 22:14:49.415409 140174664095488 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15248647332191467, loss=1.9347387552261353
I0313 22:15:27.064912 140174680880896 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.20711340010166168, loss=2.0994932651519775
I0313 22:16:04.719972 140174664095488 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16053931415081024, loss=1.938376545906067
I0313 22:16:42.461976 140174680880896 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.18591322004795074, loss=1.951192021369934
I0313 22:17:20.092699 140174664095488 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.24091589450836182, loss=2.017733097076416
I0313 22:17:57.739128 140174680880896 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.21545690298080444, loss=2.089524745941162
I0313 22:18:35.446864 140174664095488 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.19305579364299774, loss=2.010091543197632
I0313 22:19:13.129480 140174680880896 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18729674816131592, loss=1.9765437841415405
I0313 22:19:50.790467 140174664095488 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.1524965763092041, loss=2.0303707122802734
I0313 22:20:28.465847 140174680880896 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.17572180926799774, loss=1.935079574584961
I0313 22:21:06.137450 140174664095488 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.19042940437793732, loss=2.0323100090026855
I0313 22:21:43.793567 140174680880896 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1691409796476364, loss=2.037097692489624
I0313 22:22:21.481941 140174664095488 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.15799671411514282, loss=1.923065185546875
I0313 22:22:59.123820 140174680880896 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.19771116971969604, loss=2.028768301010132
I0313 22:23:36.773304 140174664095488 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.22029174864292145, loss=1.9095146656036377
I0313 22:23:56.832049 140362249443136 spec.py:298] Evaluating on the training split.
I0313 22:23:59.825677 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:27:18.612348 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 22:27:21.261808 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:29:50.881092 140362249443136 spec.py:326] Evaluating on the test split.
I0313 22:29:53.563117 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:32:11.646214 140362249443136 submission_runner.py:362] Time since start: 7014.04s, 	Step: 11155, 	{'train/accuracy': 0.6194417476654053, 'train/loss': 1.9060981273651123, 'train/bleu': 29.758638333649177, 'validation/accuracy': 0.6378098130226135, 'validation/loss': 1.7630480527877808, 'validation/bleu': 26.723611272957527, 'validation/num_examples': 3000, 'test/accuracy': 0.6444134712219238, 'test/loss': 1.7045421600341797, 'test/bleu': 25.5441652507552, 'test/num_examples': 3003}
I0313 22:32:11.653859 140174680880896 logging_writer.py:48] [11155] global_step=11155, preemption_count=0, score=4218.005665, test/accuracy=0.644413, test/bleu=25.544165, test/loss=1.704542, test/num_examples=3003, total_duration=7014.044519, train/accuracy=0.619442, train/bleu=29.758638, train/loss=1.906098, validation/accuracy=0.637810, validation/bleu=26.723611, validation/loss=1.763048, validation/num_examples=3000
I0313 22:32:12.795862 140362249443136 checkpoints.py:356] Saving checkpoint at step: 11155
I0313 22:32:16.936493 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_11155
I0313 22:32:16.996244 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_11155.
I0313 22:32:34.310542 140174664095488 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.17541946470737457, loss=2.002896308898926
I0313 22:33:11.838033 140174655702784 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1825658231973648, loss=1.9492188692092896
I0313 22:33:49.483291 140174664095488 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.19909797608852386, loss=1.9891458749771118
I0313 22:34:27.143517 140174655702784 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.16087134182453156, loss=2.037834882736206
I0313 22:35:04.808064 140174664095488 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1643565595149994, loss=1.9567769765853882
I0313 22:35:42.422666 140174655702784 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.17555847764015198, loss=2.0571374893188477
I0313 22:36:20.089405 140174664095488 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.24817365407943726, loss=1.8993891477584839
I0313 22:36:57.750426 140174655702784 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.15301413834095, loss=1.9825961589813232
I0313 22:37:35.439488 140174664095488 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1674058586359024, loss=1.963351845741272
I0313 22:38:13.102054 140174655702784 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.2375616580247879, loss=1.9700682163238525
I0313 22:38:50.797937 140174664095488 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.15488171577453613, loss=1.9351016283035278
I0313 22:39:28.401879 140174655702784 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1779845803976059, loss=1.947782278060913
I0313 22:40:06.067094 140174664095488 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3449040353298187, loss=1.8778822422027588
I0313 22:40:43.734395 140174655702784 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.28143996000289917, loss=1.973374366760254
I0313 22:41:21.432101 140174664095488 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.20236457884311676, loss=1.9387331008911133
I0313 22:41:59.140031 140174655702784 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3880571722984314, loss=1.9589473009109497
I0313 22:42:36.810683 140174664095488 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.24976500868797302, loss=1.9249072074890137
I0313 22:43:14.446836 140174655702784 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.17036737501621246, loss=1.9073801040649414
I0313 22:43:52.092976 140174664095488 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.18571323156356812, loss=1.9086730480194092
I0313 22:44:29.770663 140174655702784 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.16223424673080444, loss=1.9497106075286865
I0313 22:45:07.456358 140174664095488 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.18501721322536469, loss=1.8907074928283691
I0313 22:45:45.115812 140174655702784 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.28124314546585083, loss=1.9713757038116455
I0313 22:46:17.243446 140362249443136 spec.py:298] Evaluating on the training split.
I0313 22:46:20.233015 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:48:57.744091 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 22:49:00.381553 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:51:18.084685 140362249443136 spec.py:326] Evaluating on the test split.
I0313 22:51:20.768745 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 22:53:28.014582 140362249443136 submission_runner.py:362] Time since start: 8354.46s, 	Step: 13387, 	{'train/accuracy': 0.6339147686958313, 'train/loss': 1.7766928672790527, 'train/bleu': 30.946284645795878, 'validation/accuracy': 0.6437737941741943, 'validation/loss': 1.7152987718582153, 'validation/bleu': 27.193930608612, 'validation/num_examples': 3000, 'test/accuracy': 0.6501423716545105, 'test/loss': 1.6509428024291992, 'test/bleu': 26.02213064066428, 'test/num_examples': 3003}
I0313 22:53:28.022835 140174664095488 logging_writer.py:48] [13387] global_step=13387, preemption_count=0, score=5054.657431, test/accuracy=0.650142, test/bleu=26.022131, test/loss=1.650943, test/num_examples=3003, total_duration=8354.455899, train/accuracy=0.633915, train/bleu=30.946285, train/loss=1.776693, validation/accuracy=0.643774, validation/bleu=27.193931, validation/loss=1.715299, validation/num_examples=3000
I0313 22:53:29.108020 140362249443136 checkpoints.py:356] Saving checkpoint at step: 13387
I0313 22:53:34.803499 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_13387
I0313 22:53:34.865967 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_13387.
I0313 22:53:40.149087 140174655702784 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.25049954652786255, loss=1.9772138595581055
I0313 22:54:17.727521 140174647310080 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.18563030660152435, loss=2.029970645904541
I0313 22:54:55.322007 140174655702784 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.17748740315437317, loss=1.9088869094848633
I0313 22:55:33.009792 140174647310080 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.15962813794612885, loss=1.878253698348999
I0313 22:56:10.735136 140174655702784 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.18980205059051514, loss=1.9674395322799683
I0313 22:56:48.464218 140174647310080 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.19166453182697296, loss=1.938185691833496
I0313 22:57:26.157204 140174655702784 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.15457291901111603, loss=1.9873095750808716
I0313 22:58:03.818753 140174647310080 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.17198246717453003, loss=1.923408031463623
I0313 22:58:41.479965 140174655702784 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2402147501707077, loss=1.874569058418274
I0313 22:59:19.135437 140174647310080 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.19242897629737854, loss=1.943719744682312
I0313 22:59:56.784564 140174655702784 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.19752204418182373, loss=1.9119118452072144
I0313 23:00:34.454949 140174647310080 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1825728565454483, loss=1.9304046630859375
I0313 23:01:12.164569 140174655702784 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.20911440253257751, loss=1.9685206413269043
I0313 23:01:49.871385 140174647310080 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2427179515361786, loss=1.9598368406295776
I0313 23:02:27.569270 140174655702784 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.18435777723789215, loss=1.9973742961883545
I0313 23:03:05.283114 140174647310080 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.18449755012989044, loss=1.8560595512390137
I0313 23:03:43.032236 140174655702784 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2589658796787262, loss=1.8985813856124878
I0313 23:04:20.730264 140174647310080 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.21169745922088623, loss=1.864131212234497
I0313 23:04:58.434925 140174655702784 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.17106705904006958, loss=2.0263822078704834
I0313 23:05:36.127815 140174647310080 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.17494867742061615, loss=1.90030038356781
I0313 23:06:13.877707 140174655702784 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1665671318769455, loss=1.865687608718872
I0313 23:06:51.540963 140174647310080 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.20066747069358826, loss=1.8720569610595703
I0313 23:07:29.182451 140174655702784 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.15493372082710266, loss=1.9183615446090698
I0313 23:07:34.937893 140362249443136 spec.py:298] Evaluating on the training split.
I0313 23:07:37.909385 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:10:48.758884 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 23:10:51.392724 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:13:22.410950 140362249443136 spec.py:326] Evaluating on the test split.
I0313 23:13:25.102764 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:16:32.699994 140362249443136 submission_runner.py:362] Time since start: 9632.15s, 	Step: 15617, 	{'train/accuracy': 0.6290231943130493, 'train/loss': 1.8172813653945923, 'train/bleu': 31.038015604363164, 'validation/accuracy': 0.6465263962745667, 'validation/loss': 1.6824228763580322, 'validation/bleu': 27.092466678717624, 'validation/num_examples': 3000, 'test/accuracy': 0.6564174294471741, 'test/loss': 1.6197476387023926, 'test/bleu': 26.50804508547904, 'test/num_examples': 3003}
I0313 23:16:32.708122 140174647310080 logging_writer.py:48] [15617] global_step=15617, preemption_count=0, score=5890.777984, test/accuracy=0.656417, test/bleu=26.508045, test/loss=1.619748, test/num_examples=3003, total_duration=9632.150339, train/accuracy=0.629023, train/bleu=31.038016, train/loss=1.817281, validation/accuracy=0.646526, validation/bleu=27.092467, validation/loss=1.682423, validation/num_examples=3000
I0313 23:16:33.715592 140362249443136 checkpoints.py:356] Saving checkpoint at step: 15617
I0313 23:16:39.012613 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_15617
I0313 23:16:39.043682 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_15617.
I0313 23:17:10.639488 140174655702784 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.18477119505405426, loss=1.9355803728103638
I0313 23:17:48.290618 140174638917376 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.21492192149162292, loss=1.8767664432525635
I0313 23:18:25.942784 140174655702784 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.15604454278945923, loss=1.8562729358673096
I0313 23:19:03.617968 140174638917376 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.15635700523853302, loss=1.8912086486816406
I0313 23:19:41.314499 140174655702784 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2460344284772873, loss=1.909720778465271
I0313 23:20:18.973211 140174638917376 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.16837584972381592, loss=1.8467615842819214
I0313 23:20:56.625240 140174655702784 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1879725456237793, loss=1.9063535928726196
I0313 23:21:34.285408 140174638917376 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2612445652484894, loss=1.7755857706069946
I0313 23:22:11.970916 140174655702784 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.16614285111427307, loss=1.8276753425598145
I0313 23:22:49.693681 140174638917376 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.231308713555336, loss=1.8360077142715454
I0313 23:23:27.345761 140174655702784 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3727934658527374, loss=1.8393336534500122
I0313 23:24:04.990324 140174638917376 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.17567633092403412, loss=1.9220225811004639
I0313 23:24:42.689609 140174655702784 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.22912418842315674, loss=1.893220067024231
I0313 23:25:20.407723 140174638917376 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.18669630587100983, loss=1.9515093564987183
I0313 23:25:58.096504 140174655702784 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2035096287727356, loss=1.9732364416122437
I0313 23:26:35.816829 140174638917376 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.18320320546627045, loss=1.8559383153915405
I0313 23:27:13.469914 140174655702784 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.18813231587409973, loss=1.8908674716949463
I0313 23:27:51.157937 140174638917376 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.17879824340343475, loss=1.977705955505371
I0313 23:28:28.801298 140174655702784 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.20961971580982208, loss=1.8622239828109741
I0313 23:29:06.467756 140174638917376 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3068079352378845, loss=1.9360663890838623
I0313 23:29:44.154210 140174655702784 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1779283583164215, loss=1.8948439359664917
I0313 23:30:21.817203 140174638917376 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2584368884563446, loss=1.9218997955322266
I0313 23:30:39.282852 140362249443136 spec.py:298] Evaluating on the training split.
I0313 23:30:42.264033 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:34:02.213177 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 23:34:04.860299 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:36:18.263958 140362249443136 spec.py:326] Evaluating on the test split.
I0313 23:36:20.949230 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:38:40.373092 140362249443136 submission_runner.py:362] Time since start: 11016.50s, 	Step: 17848, 	{'train/accuracy': 0.6307936906814575, 'train/loss': 1.814158320426941, 'train/bleu': 30.63108390629473, 'validation/accuracy': 0.6499113440513611, 'validation/loss': 1.6637382507324219, 'validation/bleu': 27.58651886264353, 'validation/num_examples': 3000, 'test/accuracy': 0.6602173447608948, 'test/loss': 1.5958359241485596, 'test/bleu': 26.650819217820796, 'test/num_examples': 3003}
I0313 23:38:40.381511 140174655702784 logging_writer.py:48] [17848] global_step=17848, preemption_count=0, score=6727.686891, test/accuracy=0.660217, test/bleu=26.650819, test/loss=1.595836, test/num_examples=3003, total_duration=11016.495325, train/accuracy=0.630794, train/bleu=30.631084, train/loss=1.814158, validation/accuracy=0.649911, validation/bleu=27.586519, validation/loss=1.663738, validation/num_examples=3000
I0313 23:38:41.419351 140362249443136 checkpoints.py:356] Saving checkpoint at step: 17848
I0313 23:38:46.014559 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_17848
I0313 23:38:46.059971 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_17848.
I0313 23:39:05.995872 140174638917376 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.18776041269302368, loss=1.8582912683486938
I0313 23:39:43.547233 140174630524672 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.2418556660413742, loss=1.8699970245361328
I0313 23:40:21.103975 140174638917376 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.20377276837825775, loss=1.8296868801116943
I0313 23:40:58.753721 140174630524672 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.17478422820568085, loss=2.0374741554260254
I0313 23:41:36.374668 140174638917376 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.15349091589450836, loss=1.848816156387329
I0313 23:42:14.043105 140174630524672 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.21521539986133575, loss=1.816091537475586
I0313 23:42:51.738568 140174638917376 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.1920662671327591, loss=1.913237452507019
I0313 23:43:29.381918 140174630524672 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.19043031334877014, loss=1.8369078636169434
I0313 23:44:07.068753 140174638917376 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.18002180755138397, loss=1.8493329286575317
I0313 23:44:44.722887 140174630524672 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.2581762671470642, loss=1.8860784769058228
I0313 23:45:22.402651 140174638917376 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.16370464861392975, loss=1.784562587738037
I0313 23:46:00.110009 140174630524672 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.17007575929164886, loss=1.794092059135437
I0313 23:46:37.728407 140174638917376 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.1667110174894333, loss=1.844070315361023
I0313 23:47:15.369927 140174630524672 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.19006618857383728, loss=1.8593488931655884
I0313 23:47:53.082074 140174638917376 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.16125859320163727, loss=1.7924034595489502
I0313 23:48:30.779583 140174630524672 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.17000240087509155, loss=1.8924144506454468
I0313 23:49:08.535083 140174638917376 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.18710249662399292, loss=1.8403030633926392
I0313 23:49:46.225152 140174630524672 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.16202719509601593, loss=1.909165382385254
I0313 23:50:23.860950 140174638917376 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.19571471214294434, loss=1.7984507083892822
I0313 23:51:01.518301 140174630524672 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17329338192939758, loss=1.8072237968444824
I0313 23:51:39.205673 140174638917376 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16137707233428955, loss=1.8623287677764893
I0313 23:52:16.881891 140174630524672 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.1696256548166275, loss=1.839923620223999
I0313 23:52:46.327643 140362249443136 spec.py:298] Evaluating on the training split.
I0313 23:52:49.313006 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:56:41.646436 140362249443136 spec.py:310] Evaluating on the validation split.
I0313 23:56:44.279175 140362249443136 workload.py:179] Translating evaluation dataset.
I0313 23:59:26.738633 140362249443136 spec.py:326] Evaluating on the test split.
I0313 23:59:29.442014 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:02:14.039909 140362249443136 submission_runner.py:362] Time since start: 12343.54s, 	Step: 20080, 	{'train/accuracy': 0.6370040774345398, 'train/loss': 1.748664140701294, 'train/bleu': 31.081748435670853, 'validation/accuracy': 0.6506552696228027, 'validation/loss': 1.645272135734558, 'validation/bleu': 27.4452514532358, 'validation/num_examples': 3000, 'test/accuracy': 0.6628435254096985, 'test/loss': 1.5758757591247559, 'test/bleu': 26.967201040200088, 'test/num_examples': 3003}
I0314 00:02:14.048389 140174638917376 logging_writer.py:48] [20080] global_step=20080, preemption_count=0, score=7564.623552, test/accuracy=0.662844, test/bleu=26.967201, test/loss=1.575876, test/num_examples=3003, total_duration=12343.540098, train/accuracy=0.637004, train/bleu=31.081748, train/loss=1.748664, validation/accuracy=0.650655, validation/bleu=27.445251, validation/loss=1.645272, validation/num_examples=3000
I0314 00:02:15.121371 140362249443136 checkpoints.py:356] Saving checkpoint at step: 20080
I0314 00:02:19.979255 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_20080
I0314 00:02:20.031024 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_20080.
I0314 00:02:27.928221 140174630524672 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.2318977564573288, loss=1.8230454921722412
I0314 00:03:05.473522 140174622131968 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.19457879662513733, loss=1.8427473306655884
I0314 00:03:43.075186 140174630524672 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.1679149717092514, loss=1.8532214164733887
I0314 00:04:20.708240 140174622131968 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.20058560371398926, loss=1.86163330078125
I0314 00:04:58.373225 140174630524672 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.1812414973974228, loss=1.8197476863861084
I0314 00:05:36.047300 140174622131968 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.20642803609371185, loss=1.7960025072097778
I0314 00:06:13.663372 140174630524672 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1677173376083374, loss=1.8236982822418213
I0314 00:06:51.388903 140174622131968 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.198200985789299, loss=1.8810621500015259
I0314 00:07:29.099114 140174630524672 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.1717664897441864, loss=1.8212367296218872
I0314 00:08:06.768639 140174622131968 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.19966572523117065, loss=1.8188470602035522
I0314 00:08:44.469301 140174630524672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.187763050198555, loss=1.8412760496139526
I0314 00:09:22.181308 140174622131968 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.17912431061267853, loss=1.8957327604293823
I0314 00:09:59.838769 140174630524672 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2869225740432739, loss=1.871641993522644
I0314 00:10:37.494270 140174622131968 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.1884891390800476, loss=1.9347848892211914
I0314 00:11:15.185545 140174630524672 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.21272695064544678, loss=1.8746604919433594
I0314 00:11:52.831621 140174622131968 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.1975294053554535, loss=1.8417246341705322
I0314 00:12:30.550037 140174630524672 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.18340666592121124, loss=1.8219314813613892
I0314 00:13:08.239593 140174622131968 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.16723045706748962, loss=1.8015997409820557
I0314 00:13:45.927353 140174630524672 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.21403217315673828, loss=1.8114060163497925
I0314 00:14:23.560195 140174622131968 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.18149690330028534, loss=1.892320990562439
I0314 00:15:01.252549 140174630524672 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.21886368095874786, loss=1.852709174156189
I0314 00:15:38.959690 140174622131968 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.20000571012496948, loss=1.7783693075180054
I0314 00:16:16.627781 140174630524672 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18591417372226715, loss=1.7802895307540894
I0314 00:16:20.122104 140362249443136 spec.py:298] Evaluating on the training split.
I0314 00:16:23.102160 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:20:36.449874 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 00:20:39.086673 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:24:26.249516 140362249443136 spec.py:326] Evaluating on the test split.
I0314 00:24:28.927108 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:28:13.018607 140362249443136 submission_runner.py:362] Time since start: 13757.33s, 	Step: 22311, 	{'train/accuracy': 0.6368141770362854, 'train/loss': 1.7602704763412476, 'train/bleu': 30.84683704303534, 'validation/accuracy': 0.6548337936401367, 'validation/loss': 1.6273638010025024, 'validation/bleu': 27.703256750213143, 'validation/num_examples': 3000, 'test/accuracy': 0.6681657433509827, 'test/loss': 1.553288221359253, 'test/bleu': 27.054677927028752, 'test/num_examples': 3003}
I0314 00:28:13.027907 140174622131968 logging_writer.py:48] [22311] global_step=22311, preemption_count=0, score=8401.100093, test/accuracy=0.668166, test/bleu=27.054678, test/loss=1.553288, test/num_examples=3003, total_duration=13757.334580, train/accuracy=0.636814, train/bleu=30.846837, train/loss=1.760270, validation/accuracy=0.654834, validation/bleu=27.703257, validation/loss=1.627364, validation/num_examples=3000
I0314 00:28:14.079002 140362249443136 checkpoints.py:356] Saving checkpoint at step: 22311
I0314 00:28:18.741281 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_22311
I0314 00:28:18.796622 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_22311.
I0314 00:28:52.511944 140174630524672 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.16720837354660034, loss=1.8768566846847534
I0314 00:29:30.062207 140174613739264 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.24497777223587036, loss=1.8238928318023682
I0314 00:30:07.649736 140174630524672 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.25162023305892944, loss=1.836113691329956
I0314 00:30:45.313731 140174613739264 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.18984685838222504, loss=1.8317710161209106
I0314 00:31:22.957894 140174630524672 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.19223499298095703, loss=1.791099190711975
I0314 00:32:00.614871 140174613739264 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.27094268798828125, loss=1.7762258052825928
I0314 00:32:38.289139 140174630524672 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.22598683834075928, loss=1.8872387409210205
I0314 00:33:15.945304 140174613739264 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.17143763601779938, loss=1.8248412609100342
I0314 00:33:53.691488 140174630524672 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.20777292549610138, loss=1.8089932203292847
I0314 00:34:31.389237 140174613739264 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.21785132586956024, loss=1.8273118734359741
I0314 00:35:09.035652 140174630524672 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.2053653448820114, loss=1.9059048891067505
I0314 00:35:46.741392 140174613739264 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.18409281969070435, loss=1.8250948190689087
I0314 00:36:24.411869 140174630524672 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.20784518122673035, loss=1.8758422136306763
I0314 00:37:02.165955 140174613739264 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.19975459575653076, loss=1.8459974527359009
I0314 00:37:39.869458 140174630524672 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.1966540813446045, loss=1.8675792217254639
I0314 00:38:17.570689 140174613739264 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.21063293516635895, loss=1.8200963735580444
I0314 00:38:55.210593 140174630524672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.19380004703998566, loss=1.8958027362823486
I0314 00:39:32.913337 140174613739264 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.19697919487953186, loss=1.8213614225387573
I0314 00:40:10.579786 140174630524672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.19130051136016846, loss=1.8243989944458008
I0314 00:40:48.232023 140174613739264 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2825906276702881, loss=1.8664382696151733
I0314 00:41:25.902182 140174630524672 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.21875378489494324, loss=1.9198129177093506
I0314 00:42:03.569005 140174613739264 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.1947849839925766, loss=1.8372905254364014
I0314 00:42:19.152913 140362249443136 spec.py:298] Evaluating on the training split.
I0314 00:42:22.131538 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:45:48.474647 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 00:45:51.123862 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:48:11.527603 140362249443136 spec.py:326] Evaluating on the test split.
I0314 00:48:14.230076 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 00:50:38.902424 140362249443136 submission_runner.py:362] Time since start: 15316.37s, 	Step: 24543, 	{'train/accuracy': 0.6358434557914734, 'train/loss': 1.7613184452056885, 'train/bleu': 30.93244260895864, 'validation/accuracy': 0.6564456820487976, 'validation/loss': 1.6108226776123047, 'validation/bleu': 28.009714369537495, 'validation/num_examples': 3000, 'test/accuracy': 0.6662250757217407, 'test/loss': 1.5462158918380737, 'test/bleu': 27.05864208360408, 'test/num_examples': 3003}
I0314 00:50:38.910862 140174630524672 logging_writer.py:48] [24543] global_step=24543, preemption_count=0, score=9237.966645, test/accuracy=0.666225, test/bleu=27.058642, test/loss=1.546216, test/num_examples=3003, total_duration=15316.365354, train/accuracy=0.635843, train/bleu=30.932443, train/loss=1.761318, validation/accuracy=0.656446, validation/bleu=28.009714, validation/loss=1.610823, validation/num_examples=3000
I0314 00:50:39.954246 140362249443136 checkpoints.py:356] Saving checkpoint at step: 24543
I0314 00:50:44.288336 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_24543
I0314 00:50:44.316668 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_24543.
I0314 00:51:06.124476 140174613739264 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.18223954737186432, loss=1.8524599075317383
I0314 00:51:43.655132 140174605346560 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.18506117165088654, loss=1.8117077350616455
I0314 00:52:21.243547 140174613739264 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.19189293682575226, loss=1.7479970455169678
I0314 00:52:58.859979 140174605346560 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1962544023990631, loss=1.8539323806762695
I0314 00:53:36.516860 140174613739264 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.19565880298614502, loss=1.8537802696228027
I0314 00:54:14.227888 140174605346560 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.173704132437706, loss=1.8219096660614014
I0314 00:54:51.915179 140174613739264 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.19023779034614563, loss=1.858681559562683
I0314 00:55:29.561951 140174605346560 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1896294206380844, loss=1.8316317796707153
I0314 00:56:07.269377 140174613739264 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2406361699104309, loss=1.770438551902771
I0314 00:56:44.903705 140174605346560 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.180536150932312, loss=1.7985756397247314
I0314 00:57:22.580885 140174613739264 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.18837642669677734, loss=1.814009666442871
I0314 00:58:00.248450 140174605346560 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.2093581259250641, loss=1.7761515378952026
I0314 00:58:37.921827 140174613739264 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.19513607025146484, loss=1.8834846019744873
I0314 00:59:15.566240 140174605346560 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.1835436373949051, loss=1.7673965692520142
I0314 00:59:53.218619 140174613739264 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.18323968350887299, loss=1.71089768409729
I0314 01:00:30.918476 140174605346560 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.17772039771080017, loss=1.8368892669677734
I0314 01:01:08.595752 140174613739264 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.19439509510993958, loss=1.8576679229736328
I0314 01:01:46.237615 140174605346560 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.20989499986171722, loss=1.8079184293746948
I0314 01:02:23.881083 140174613739264 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.19433872401714325, loss=1.8658477067947388
I0314 01:03:01.545724 140174605346560 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1997779756784439, loss=1.8697880506515503
I0314 01:03:39.247019 140174613739264 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.17365261912345886, loss=1.8134270906448364
I0314 01:04:16.912112 140174605346560 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.18116547167301178, loss=1.8542275428771973
I0314 01:04:44.490957 140362249443136 spec.py:298] Evaluating on the training split.
I0314 01:04:47.457916 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:08:40.239679 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 01:08:42.876305 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:11:09.635253 140362249443136 spec.py:326] Evaluating on the test split.
I0314 01:11:12.324383 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:13:24.609668 140362249443136 submission_runner.py:362] Time since start: 16661.70s, 	Step: 26775, 	{'train/accuracy': 0.6415444016456604, 'train/loss': 1.7208589315414429, 'train/bleu': 30.971115904086908, 'validation/accuracy': 0.6567928194999695, 'validation/loss': 1.6031814813613892, 'validation/bleu': 28.025297010655358, 'validation/num_examples': 3000, 'test/accuracy': 0.6676543951034546, 'test/loss': 1.5357131958007812, 'test/bleu': 27.354715059831896, 'test/num_examples': 3003}
I0314 01:13:24.618923 140174613739264 logging_writer.py:48] [26775] global_step=26775, preemption_count=0, score=10074.697485, test/accuracy=0.667654, test/bleu=27.354715, test/loss=1.535713, test/num_examples=3003, total_duration=16661.703431, train/accuracy=0.641544, train/bleu=30.971116, train/loss=1.720859, validation/accuracy=0.656793, validation/bleu=28.025297, validation/loss=1.603181, validation/num_examples=3000
I0314 01:13:25.678403 140362249443136 checkpoints.py:356] Saving checkpoint at step: 26775
I0314 01:13:30.005609 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_26775
I0314 01:13:30.060455 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_26775.
I0314 01:13:39.813372 140174605346560 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.1948750764131546, loss=1.9349416494369507
I0314 01:14:17.400867 140174596953856 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1860983818769455, loss=1.7813849449157715
I0314 01:14:54.976408 140174605346560 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.1820908933877945, loss=1.7921867370605469
I0314 01:15:32.615426 140174596953856 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.16613909602165222, loss=1.7637526988983154
I0314 01:16:10.335726 140174605346560 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.18008601665496826, loss=1.8415532112121582
I0314 01:16:47.972462 140174596953856 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.19494016468524933, loss=1.7747787237167358
I0314 01:17:25.724997 140174605346560 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.19439248740673065, loss=1.754008173942566
I0314 01:18:03.433238 140174596953856 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.20668040215969086, loss=1.8494455814361572
I0314 01:18:41.093493 140174605346560 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.19739745557308197, loss=1.801718831062317
I0314 01:19:18.729954 140174596953856 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.23017433285713196, loss=1.7735570669174194
I0314 01:19:56.359204 140174605346560 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.20522616803646088, loss=1.7565864324569702
I0314 01:20:33.956422 140174596953856 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.18642866611480713, loss=1.7418010234832764
I0314 01:21:11.588115 140174605346560 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.3069041669368744, loss=1.8566573858261108
I0314 01:21:49.310656 140174596953856 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.27061983942985535, loss=1.722486138343811
I0314 01:22:26.977924 140174605346560 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.2036866843700409, loss=1.7251192331314087
I0314 01:23:04.687453 140174596953856 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.17870746552944183, loss=1.8194689750671387
I0314 01:23:42.394368 140174605346560 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.20262707769870758, loss=1.8357771635055542
I0314 01:24:20.099131 140174596953856 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.18905648589134216, loss=1.8855743408203125
I0314 01:24:57.801347 140174605346560 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.21819894015789032, loss=1.8134021759033203
I0314 01:25:35.456609 140174596953856 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.2186245173215866, loss=1.7749323844909668
I0314 01:26:13.166097 140174605346560 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.19025146961212158, loss=1.8020458221435547
I0314 01:26:50.864388 140174596953856 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.20691390335559845, loss=1.8251408338546753
I0314 01:27:28.572438 140174605346560 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.20070216059684753, loss=1.752632737159729
I0314 01:27:30.183019 140362249443136 spec.py:298] Evaluating on the training split.
I0314 01:27:33.169690 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:31:43.260933 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 01:31:45.901709 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:34:27.937599 140362249443136 spec.py:326] Evaluating on the test split.
I0314 01:34:30.625813 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:37:21.654116 140362249443136 submission_runner.py:362] Time since start: 18027.40s, 	Step: 29006, 	{'train/accuracy': 0.6368036270141602, 'train/loss': 1.7542250156402588, 'train/bleu': 31.17904782082637, 'validation/accuracy': 0.6587395071983337, 'validation/loss': 1.593796968460083, 'validation/bleu': 28.123257556698714, 'validation/num_examples': 3000, 'test/accuracy': 0.6702457666397095, 'test/loss': 1.5255166292190552, 'test/bleu': 27.509523812908828, 'test/num_examples': 3003}
I0314 01:37:21.663293 140174596953856 logging_writer.py:48] [29006] global_step=29006, preemption_count=0, score=10911.387860, test/accuracy=0.670246, test/bleu=27.509524, test/loss=1.525517, test/num_examples=3003, total_duration=18027.395493, train/accuracy=0.636804, train/bleu=31.179048, train/loss=1.754225, validation/accuracy=0.658740, validation/bleu=28.123258, validation/loss=1.593797, validation/num_examples=3000
I0314 01:37:22.678619 140362249443136 checkpoints.py:356] Saving checkpoint at step: 29006
I0314 01:37:27.314189 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_29006
I0314 01:37:27.370523 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_29006.
I0314 01:38:03.056955 140174605346560 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.17085368931293488, loss=1.7918267250061035
I0314 01:38:40.670053 140174588561152 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.18383406102657318, loss=1.8299304246902466
I0314 01:39:18.295966 140174605346560 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.19396427273750305, loss=1.8656120300292969
I0314 01:39:55.949515 140174588561152 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.1823328733444214, loss=1.8379249572753906
I0314 01:40:33.627126 140174605346560 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.19488245248794556, loss=1.796354055404663
I0314 01:41:11.297154 140174588561152 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.2554391920566559, loss=1.724545955657959
I0314 01:41:49.036577 140174605346560 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2134803980588913, loss=1.7975926399230957
I0314 01:42:26.769315 140174588561152 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.21104760468006134, loss=1.8418911695480347
I0314 01:43:04.504488 140174605346560 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.19424471259117126, loss=1.7910399436950684
I0314 01:43:42.163754 140174588561152 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.17825236916542053, loss=1.7643519639968872
I0314 01:44:19.880833 140174605346560 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.21365998685359955, loss=1.745745062828064
I0314 01:44:57.576241 140174588561152 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.19641683995723724, loss=1.8178116083145142
I0314 01:45:35.258247 140174605346560 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.21557432413101196, loss=1.6964308023452759
I0314 01:46:13.016699 140174588561152 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.19631226360797882, loss=1.7674444913864136
I0314 01:46:50.685195 140174605346560 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.24275054037570953, loss=1.775430679321289
I0314 01:47:28.427931 140174588561152 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.18652985990047455, loss=1.7818087339401245
I0314 01:48:06.130442 140174605346560 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.18958716094493866, loss=1.805874228477478
I0314 01:48:43.840478 140174588561152 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.1774510145187378, loss=1.730010747909546
I0314 01:49:21.504143 140174605346560 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.18264897167682648, loss=1.759918451309204
I0314 01:49:59.193149 140174588561152 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.26255810260772705, loss=1.8628478050231934
I0314 01:50:36.909491 140174605346560 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.18669019639492035, loss=1.7570252418518066
I0314 01:51:14.620139 140174588561152 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2081296443939209, loss=1.7817988395690918
I0314 01:51:27.532088 140362249443136 spec.py:298] Evaluating on the training split.
I0314 01:51:30.516791 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:55:33.722079 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 01:55:36.360446 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 01:57:55.316444 140362249443136 spec.py:326] Evaluating on the test split.
I0314 01:57:58.004327 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:00:02.581749 140362249443136 submission_runner.py:362] Time since start: 19464.74s, 	Step: 31236, 	{'train/accuracy': 0.6428775191307068, 'train/loss': 1.7256025075912476, 'train/bleu': 31.34213016240891, 'validation/accuracy': 0.6609093546867371, 'validation/loss': 1.5861154794692993, 'validation/bleu': 28.29135554322365, 'validation/num_examples': 3000, 'test/accuracy': 0.6745453476905823, 'test/loss': 1.511854887008667, 'test/bleu': 27.72874425253369, 'test/num_examples': 3003}
I0314 02:00:02.592028 140174605346560 logging_writer.py:48] [31236] global_step=31236, preemption_count=0, score=11747.613956, test/accuracy=0.674545, test/bleu=27.728744, test/loss=1.511855, test/num_examples=3003, total_duration=19464.744548, train/accuracy=0.642878, train/bleu=31.342130, train/loss=1.725603, validation/accuracy=0.660909, validation/bleu=28.291356, validation/loss=1.586115, validation/num_examples=3000
I0314 02:00:03.593022 140362249443136 checkpoints.py:356] Saving checkpoint at step: 31236
I0314 02:00:08.345967 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_31236
I0314 02:00:08.412654 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_31236.
I0314 02:00:32.826046 140174588561152 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.22480596601963043, loss=1.809615969657898
I0314 02:01:10.398351 140174580168448 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.17713797092437744, loss=1.8208122253417969
I0314 02:01:47.992431 140174588561152 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.18008799850940704, loss=1.8350036144256592
I0314 02:02:25.644247 140174580168448 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.24138157069683075, loss=1.8330036401748657
I0314 02:03:03.286438 140174588561152 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.21147608757019043, loss=1.8592529296875
I0314 02:03:40.914036 140174580168448 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.18906907737255096, loss=1.8167974948883057
I0314 02:04:18.548917 140174588561152 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.18167167901992798, loss=1.8109750747680664
I0314 02:04:56.206044 140174580168448 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.20626458525657654, loss=1.7420073747634888
I0314 02:05:33.940251 140174588561152 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2043130248785019, loss=1.8034452199935913
I0314 02:06:11.561939 140174580168448 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.22019033133983612, loss=1.8775237798690796
I0314 02:06:49.179430 140174588561152 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.1838817149400711, loss=1.6603493690490723
I0314 02:07:26.898828 140174580168448 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2319410741329193, loss=1.733960509300232
I0314 02:08:04.546713 140174588561152 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.19665269553661346, loss=1.7167390584945679
I0314 02:08:42.223041 140174580168448 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.19519510865211487, loss=1.7956594228744507
I0314 02:09:19.915913 140174588561152 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.1964617669582367, loss=1.7808754444122314
I0314 02:09:57.613588 140174580168448 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.23654820024967194, loss=1.8117228746414185
I0314 02:10:35.299121 140174588561152 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.20730391144752502, loss=1.7799503803253174
I0314 02:11:12.930610 140174580168448 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.19377854466438293, loss=1.7422945499420166
I0314 02:11:50.609865 140174588561152 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.17755074799060822, loss=1.6863751411437988
I0314 02:12:28.311059 140174580168448 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.17163066565990448, loss=1.739669680595398
I0314 02:13:06.030940 140174588561152 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.21396705508232117, loss=1.7671085596084595
I0314 02:13:43.689014 140174580168448 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.19085094332695007, loss=1.820370078086853
I0314 02:14:08.629997 140362249443136 spec.py:298] Evaluating on the training split.
I0314 02:14:11.604447 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:18:37.903181 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 02:18:40.548511 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:21:43.589236 140362249443136 spec.py:326] Evaluating on the test split.
I0314 02:21:46.282644 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:24:48.787472 140362249443136 submission_runner.py:362] Time since start: 20825.84s, 	Step: 33468, 	{'train/accuracy': 0.6477102041244507, 'train/loss': 1.679289698600769, 'train/bleu': 31.7570253547598, 'validation/accuracy': 0.6622980237007141, 'validation/loss': 1.5731664896011353, 'validation/bleu': 28.533663182301904, 'validation/num_examples': 3000, 'test/accuracy': 0.6735343933105469, 'test/loss': 1.5009454488754272, 'test/bleu': 27.56764231633376, 'test/num_examples': 3003}
I0314 02:24:48.796869 140174588561152 logging_writer.py:48] [33468] global_step=33468, preemption_count=0, score=12584.526011, test/accuracy=0.673534, test/bleu=27.567642, test/loss=1.500945, test/num_examples=3003, total_duration=20825.842468, train/accuracy=0.647710, train/bleu=31.757025, train/loss=1.679290, validation/accuracy=0.662298, validation/bleu=28.533663, validation/loss=1.573166, validation/num_examples=3000
I0314 02:24:49.788067 140362249443136 checkpoints.py:356] Saving checkpoint at step: 33468
I0314 02:24:54.217001 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_33468
I0314 02:24:54.251606 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_33468.
I0314 02:25:06.663052 140174580168448 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.20924162864685059, loss=1.8475943803787231
I0314 02:25:44.228972 140174571775744 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.1922733187675476, loss=1.700588345527649
I0314 02:26:21.834178 140174580168448 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.18822848796844482, loss=1.773048996925354
I0314 02:26:59.488428 140174571775744 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.17632949352264404, loss=1.7715507745742798
I0314 02:27:37.185976 140174580168448 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.20705251395702362, loss=1.7415753602981567
I0314 02:28:14.810358 140174571775744 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2128543257713318, loss=1.7984846830368042
I0314 02:28:52.533171 140174580168448 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.19516119360923767, loss=1.7185640335083008
I0314 02:29:30.201855 140174571775744 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.21129165589809418, loss=1.79261314868927
I0314 02:30:07.876716 140174580168448 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.21670496463775635, loss=1.7358174324035645
I0314 02:30:45.502822 140174571775744 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.19439032673835754, loss=1.7967016696929932
I0314 02:31:23.157926 140174580168448 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.2203659564256668, loss=1.7774028778076172
I0314 02:32:00.860564 140174571775744 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2075868844985962, loss=1.8232088088989258
I0314 02:32:38.508917 140174580168448 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.231221541762352, loss=1.772437572479248
I0314 02:33:16.204889 140174571775744 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.23140233755111694, loss=1.7548664808273315
I0314 02:33:53.862728 140174580168448 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.21655228734016418, loss=1.7768129110336304
I0314 02:34:31.511625 140174571775744 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.265446275472641, loss=1.7210278511047363
I0314 02:35:09.170196 140174580168448 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1799965500831604, loss=1.7330446243286133
I0314 02:35:46.867984 140174571775744 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.1864812821149826, loss=1.7342761754989624
I0314 02:36:24.524821 140174580168448 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.19701047241687775, loss=1.8155999183654785
I0314 02:37:02.178186 140174571775744 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.1762194186449051, loss=1.7820180654525757
I0314 02:37:39.851148 140174580168448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.22665809094905853, loss=1.7893072366714478
I0314 02:38:17.564242 140174571775744 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.20206592977046967, loss=1.6680654287338257
I0314 02:38:54.583099 140362249443136 spec.py:298] Evaluating on the training split.
I0314 02:38:57.558263 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:43:31.632579 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 02:43:34.276744 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:47:19.161703 140362249443136 spec.py:326] Evaluating on the test split.
I0314 02:47:21.863872 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 02:51:07.338564 140362249443136 submission_runner.py:362] Time since start: 22311.80s, 	Step: 35700, 	{'train/accuracy': 0.6457854509353638, 'train/loss': 1.696174144744873, 'train/bleu': 31.491807952689236, 'validation/accuracy': 0.6630296111106873, 'validation/loss': 1.568543791770935, 'validation/bleu': 28.530390041821494, 'validation/num_examples': 3000, 'test/accuracy': 0.6756028532981873, 'test/loss': 1.4859144687652588, 'test/bleu': 28.23277314478723, 'test/num_examples': 3003}
I0314 02:51:07.349113 140174580168448 logging_writer.py:48] [35700] global_step=35700, preemption_count=0, score=13421.435861, test/accuracy=0.675603, test/bleu=28.232773, test/loss=1.485914, test/num_examples=3003, total_duration=22311.795570, train/accuracy=0.645785, train/bleu=31.491808, train/loss=1.696174, validation/accuracy=0.663030, validation/bleu=28.530390, validation/loss=1.568544, validation/num_examples=3000
I0314 02:51:08.351622 140362249443136 checkpoints.py:356] Saving checkpoint at step: 35700
I0314 02:51:12.587518 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_35700
I0314 02:51:12.621812 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_35700.
I0314 02:51:13.019329 140174571775744 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.20756009221076965, loss=1.7854204177856445
I0314 02:51:50.572769 140174563383040 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.2074090987443924, loss=1.717384696006775
I0314 02:52:28.208305 140174571775744 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.18887096643447876, loss=1.7163984775543213
I0314 02:53:05.809466 140174563383040 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.21167844533920288, loss=1.7656196355819702
I0314 02:53:43.508329 140174571775744 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.22175578773021698, loss=1.7405860424041748
I0314 02:54:21.183040 140174563383040 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.2600395381450653, loss=1.775144100189209
I0314 02:54:58.823606 140174571775744 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.20194140076637268, loss=1.7802081108093262
I0314 02:55:36.490148 140174563383040 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.28189048171043396, loss=1.8482868671417236
I0314 02:56:14.225329 140174571775744 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.19384627044200897, loss=1.7090715169906616
I0314 02:56:51.919734 140174563383040 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.2544640898704529, loss=1.7459899187088013
I0314 02:57:29.571253 140174571775744 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.18706348538398743, loss=1.8040403127670288
I0314 02:58:07.244526 140174563383040 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.20441851019859314, loss=1.7934374809265137
I0314 02:58:44.935932 140174571775744 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.17499226331710815, loss=1.762412667274475
I0314 02:59:22.623085 140174563383040 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.24043431878089905, loss=1.7652486562728882
I0314 03:00:00.317701 140174571775744 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.20440642535686493, loss=1.78432035446167
I0314 03:00:37.978372 140174563383040 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.21062077581882477, loss=1.7529709339141846
I0314 03:01:15.707791 140174571775744 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2131437063217163, loss=1.7942248582839966
I0314 03:01:53.396622 140174563383040 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.2321673035621643, loss=1.7581535577774048
I0314 03:02:31.068669 140174571775744 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.1835675835609436, loss=1.8187050819396973
I0314 03:03:08.761263 140174563383040 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.1807127147912979, loss=1.6878221035003662
I0314 03:03:46.456014 140174571775744 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.20560790598392487, loss=1.7314426898956299
I0314 03:04:24.134854 140174563383040 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.20733799040317535, loss=1.7928264141082764
I0314 03:05:01.819294 140174571775744 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.18509729206562042, loss=1.7404521703720093
I0314 03:05:12.845925 140362249443136 spec.py:298] Evaluating on the training split.
I0314 03:05:15.827095 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:09:36.562783 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 03:09:39.207357 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:12:10.673696 140362249443136 spec.py:326] Evaluating on the test split.
I0314 03:12:13.365967 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:14:30.516383 140362249443136 submission_runner.py:362] Time since start: 23890.06s, 	Step: 37931, 	{'train/accuracy': 0.6595262289047241, 'train/loss': 1.5863367319107056, 'train/bleu': 32.4212970149453, 'validation/accuracy': 0.6662905812263489, 'validation/loss': 1.5519459247589111, 'validation/bleu': 28.74829559657764, 'validation/num_examples': 3000, 'test/accuracy': 0.6785195469856262, 'test/loss': 1.4766432046890259, 'test/bleu': 28.00383939571852, 'test/num_examples': 3003}
I0314 03:14:30.526223 140174563383040 logging_writer.py:48] [37931] global_step=37931, preemption_count=0, score=14257.872126, test/accuracy=0.678520, test/bleu=28.003839, test/loss=1.476643, test/num_examples=3003, total_duration=23890.058379, train/accuracy=0.659526, train/bleu=32.421297, train/loss=1.586337, validation/accuracy=0.666291, validation/bleu=28.748296, validation/loss=1.551946, validation/num_examples=3000
I0314 03:14:31.523169 140362249443136 checkpoints.py:356] Saving checkpoint at step: 37931
I0314 03:14:35.850482 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_37931
I0314 03:14:35.912992 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_37931.
I0314 03:15:02.244936 140174571775744 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.19917602837085724, loss=1.710721492767334
I0314 03:15:39.787256 140174554990336 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.18917520344257355, loss=1.7226638793945312
I0314 03:16:17.357342 140174571775744 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.18537598848342896, loss=1.7738220691680908
I0314 03:16:55.022870 140174554990336 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.18412375450134277, loss=1.7785018682479858
I0314 03:17:32.669921 140174571775744 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.2096633017063141, loss=1.7580076456069946
I0314 03:18:10.332364 140174554990336 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2065899819135666, loss=1.893738865852356
I0314 03:18:47.988502 140174571775744 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.22299082577228546, loss=1.722633719444275
I0314 03:19:25.641750 140174554990336 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.271767258644104, loss=1.6879621744155884
I0314 03:20:03.314042 140174571775744 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.18119114637374878, loss=1.7543054819107056
I0314 03:20:40.917252 140174554990336 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1840587556362152, loss=1.7242859601974487
I0314 03:21:18.594854 140174571775744 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.19837211072444916, loss=1.7984029054641724
I0314 03:21:56.260882 140174554990336 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.17972835898399353, loss=1.7090204954147339
I0314 03:22:33.967061 140174571775744 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.18253669142723083, loss=1.7678183317184448
I0314 03:23:11.653274 140174554990336 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.22914153337478638, loss=1.805789589881897
I0314 03:23:49.323781 140174571775744 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1926046758890152, loss=1.7696802616119385
I0314 03:24:26.956347 140174554990336 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.18321770429611206, loss=1.7450191974639893
I0314 03:25:04.653002 140174571775744 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.18239325284957886, loss=1.708695888519287
I0314 03:25:42.319854 140174554990336 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.19862763583660126, loss=1.7308145761489868
I0314 03:26:20.029289 140174571775744 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.19722843170166016, loss=1.7948421239852905
I0314 03:26:57.688158 140174554990336 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.22895261645317078, loss=1.6945992708206177
I0314 03:27:35.331271 140174571775744 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.23899208009243011, loss=1.6980006694793701
I0314 03:28:13.030366 140174554990336 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.20690396428108215, loss=1.8042397499084473
I0314 03:28:36.095345 140362249443136 spec.py:298] Evaluating on the training split.
I0314 03:28:39.064209 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:32:48.421726 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 03:32:51.056580 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:35:38.759118 140362249443136 spec.py:326] Evaluating on the test split.
I0314 03:35:41.454180 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:38:26.051296 140362249443136 submission_runner.py:362] Time since start: 25293.31s, 	Step: 40163, 	{'train/accuracy': 0.6504812836647034, 'train/loss': 1.6621646881103516, 'train/bleu': 32.10532827916792, 'validation/accuracy': 0.6668112874031067, 'validation/loss': 1.551001787185669, 'validation/bleu': 28.823667011296276, 'validation/num_examples': 3000, 'test/accuracy': 0.6789495348930359, 'test/loss': 1.4692944288253784, 'test/bleu': 28.12709210080108, 'test/num_examples': 3003}
I0314 03:38:26.061960 140174571775744 logging_writer.py:48] [40163] global_step=40163, preemption_count=0, score=15094.631681, test/accuracy=0.678950, test/bleu=28.127092, test/loss=1.469294, test/num_examples=3003, total_duration=25293.307805, train/accuracy=0.650481, train/bleu=32.105328, train/loss=1.662165, validation/accuracy=0.666811, validation/bleu=28.823667, validation/loss=1.551002, validation/num_examples=3000
I0314 03:38:27.054047 140362249443136 checkpoints.py:356] Saving checkpoint at step: 40163
I0314 03:38:31.353029 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_40163
I0314 03:38:31.374804 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_40163.
I0314 03:38:45.646369 140174554990336 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19301632046699524, loss=1.6798369884490967
I0314 03:39:23.205450 140174546597632 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.19710023701190948, loss=1.7699843645095825
I0314 03:40:00.826880 140174554990336 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.18374446034431458, loss=1.7225432395935059
I0314 03:40:38.473842 140174546597632 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1933266669511795, loss=1.7073761224746704
I0314 03:41:16.093044 140174554990336 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.18948376178741455, loss=1.7375918626785278
I0314 03:41:53.777706 140174546597632 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.17807646095752716, loss=1.7032761573791504
I0314 03:42:31.470924 140174554990336 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.21224316954612732, loss=1.6394037008285522
I0314 03:43:09.161923 140174546597632 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.21271538734436035, loss=1.7492982149124146
I0314 03:43:46.844282 140174554990336 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.19558805227279663, loss=1.6891772747039795
I0314 03:44:24.525069 140174546597632 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.20334477722644806, loss=1.7337554693222046
I0314 03:45:02.237466 140174554990336 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.18723735213279724, loss=1.7609062194824219
I0314 03:45:39.936701 140174546597632 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2051873505115509, loss=1.7485363483428955
I0314 03:46:17.607055 140174554990336 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.20471473038196564, loss=1.655569076538086
I0314 03:46:55.271258 140174546597632 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.21727460622787476, loss=1.7765158414840698
I0314 03:47:32.930484 140174554990336 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2563243806362152, loss=1.8401597738265991
I0314 03:48:10.608561 140174546597632 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.20498007535934448, loss=1.7640899419784546
I0314 03:48:48.254629 140174554990336 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.18752913177013397, loss=1.721809983253479
I0314 03:49:25.964185 140174546597632 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.19567398726940155, loss=1.79860520362854
I0314 03:50:03.616048 140174554990336 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.19538277387619019, loss=1.7349635362625122
I0314 03:50:41.311765 140174546597632 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1877562254667282, loss=1.6475830078125
I0314 03:51:19.020931 140174554990336 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1984051913022995, loss=1.805543065071106
I0314 03:51:56.676450 140174546597632 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1851276457309723, loss=1.7351529598236084
I0314 03:52:31.496823 140362249443136 spec.py:298] Evaluating on the training split.
I0314 03:52:34.468637 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 03:57:10.421848 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 03:57:13.071861 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:00:04.621734 140362249443136 spec.py:326] Evaluating on the test split.
I0314 04:00:07.320808 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:02:40.283360 140362249443136 submission_runner.py:362] Time since start: 26728.71s, 	Step: 42394, 	{'train/accuracy': 0.6495952606201172, 'train/loss': 1.6698956489562988, 'train/bleu': 31.842459514819787, 'validation/accuracy': 0.6689687371253967, 'validation/loss': 1.537227749824524, 'validation/bleu': 28.90239046261636, 'validation/num_examples': 3000, 'test/accuracy': 0.6820521950721741, 'test/loss': 1.455510139465332, 'test/bleu': 28.497689775181783, 'test/num_examples': 3003}
I0314 04:02:40.293041 140174554990336 logging_writer.py:48] [42394] global_step=42394, preemption_count=0, score=15931.476198, test/accuracy=0.682052, test/bleu=28.497690, test/loss=1.455510, test/num_examples=3003, total_duration=26728.709293, train/accuracy=0.649595, train/bleu=31.842460, train/loss=1.669896, validation/accuracy=0.668969, validation/bleu=28.902390, validation/loss=1.537228, validation/num_examples=3000
I0314 04:02:41.292296 140362249443136 checkpoints.py:356] Saving checkpoint at step: 42394
I0314 04:02:45.311681 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_42394
I0314 04:02:45.339028 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_42394.
I0314 04:02:47.973922 140174546597632 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.1920519471168518, loss=1.6804633140563965
I0314 04:03:25.524425 140174538204928 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2033866047859192, loss=1.7739399671554565
I0314 04:04:03.111430 140174546597632 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2069949358701706, loss=1.827113389968872
I0314 04:04:40.708099 140174538204928 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2097930908203125, loss=1.733925461769104
I0314 04:05:18.412134 140174546597632 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.21176846325397491, loss=1.7752447128295898
I0314 04:05:56.099498 140174538204928 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.19069577753543854, loss=1.701789379119873
I0314 04:06:33.751740 140174546597632 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.1912672519683838, loss=1.7510849237442017
I0314 04:07:11.439215 140174538204928 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.19775010645389557, loss=1.7929531335830688
I0314 04:07:49.092634 140174546597632 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.18852663040161133, loss=1.796293020248413
I0314 04:08:26.776502 140174538204928 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.1853630393743515, loss=1.659928798675537
I0314 04:09:04.482138 140174546597632 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.18841888010501862, loss=1.7711396217346191
I0314 04:09:42.167112 140174538204928 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.18465453386306763, loss=1.7275478839874268
I0314 04:10:19.793124 140174546597632 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.18582592904567719, loss=1.7331949472427368
I0314 04:10:57.440251 140174538204928 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.18998301029205322, loss=1.7605890035629272
I0314 04:11:35.107197 140174546597632 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.18386580049991608, loss=1.75095796585083
I0314 04:12:12.766897 140174538204928 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19943565130233765, loss=1.7559220790863037
I0314 04:12:50.431651 140174546597632 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.18083816766738892, loss=1.7577013969421387
I0314 04:13:28.069145 140174538204928 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1997596174478531, loss=1.716512680053711
I0314 04:14:05.770257 140174546597632 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.194991335272789, loss=1.6991859674453735
I0314 04:14:43.426740 140174538204928 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.20990735292434692, loss=1.7600147724151611
I0314 04:15:21.138260 140174546597632 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.1982780396938324, loss=1.7078205347061157
I0314 04:15:58.833851 140174538204928 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.18979549407958984, loss=1.782137155532837
I0314 04:16:36.482744 140174546597632 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.19655859470367432, loss=1.8124346733093262
I0314 04:16:45.597308 140362249443136 spec.py:298] Evaluating on the training split.
I0314 04:16:48.577731 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:21:10.049119 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 04:21:12.694657 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:23:32.828828 140362249443136 spec.py:326] Evaluating on the test split.
I0314 04:23:35.517282 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:25:50.235724 140362249443136 submission_runner.py:362] Time since start: 28182.81s, 	Step: 44626, 	{'train/accuracy': 0.6603747606277466, 'train/loss': 1.5871729850769043, 'train/bleu': 32.634918828783306, 'validation/accuracy': 0.6694895029067993, 'validation/loss': 1.5236845016479492, 'validation/bleu': 28.692436833359817, 'validation/num_examples': 3000, 'test/accuracy': 0.6816454529762268, 'test/loss': 1.446325421333313, 'test/bleu': 28.566711219938966, 'test/num_examples': 3003}
I0314 04:25:50.247229 140174538204928 logging_writer.py:48] [44626] global_step=44626, preemption_count=0, score=16768.198625, test/accuracy=0.681645, test/bleu=28.566711, test/loss=1.446325, test/num_examples=3003, total_duration=28182.809782, train/accuracy=0.660375, train/bleu=32.634919, train/loss=1.587173, validation/accuracy=0.669490, validation/bleu=28.692437, validation/loss=1.523685, validation/num_examples=3000
I0314 04:25:51.253267 140362249443136 checkpoints.py:356] Saving checkpoint at step: 44626
I0314 04:25:55.674865 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_44626
I0314 04:25:55.731995 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_44626.
I0314 04:26:23.921720 140174546597632 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.18749408423900604, loss=1.6996475458145142
I0314 04:27:01.480018 140174529812224 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.2015831172466278, loss=1.7095730304718018
I0314 04:27:39.129064 140174546597632 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.21954001486301422, loss=1.796745777130127
I0314 04:28:16.851874 140174529812224 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.19675080478191376, loss=1.7120684385299683
I0314 04:28:54.546797 140174546597632 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.1973462700843811, loss=1.6849071979522705
I0314 04:29:32.215307 140174529812224 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.22273209691047668, loss=1.7610881328582764
I0314 04:30:09.868510 140174546597632 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.22797417640686035, loss=1.743147850036621
I0314 04:30:47.551415 140174529812224 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.20593704283237457, loss=1.735335350036621
I0314 04:31:25.189836 140174546597632 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.27199241518974304, loss=1.7415817975997925
I0314 04:32:02.854844 140174529812224 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.19480429589748383, loss=1.7924199104309082
I0314 04:32:40.549859 140174546597632 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.19488753378391266, loss=1.7884130477905273
I0314 04:33:18.226957 140174529812224 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.20298902690410614, loss=1.7398970127105713
I0314 04:33:55.855290 140174546597632 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.19283752143383026, loss=1.7140429019927979
I0314 04:34:33.522069 140174529812224 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.19503402709960938, loss=1.693603754043579
I0314 04:35:11.160299 140174546597632 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2442890852689743, loss=1.7112236022949219
I0314 04:35:48.820942 140174529812224 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.20658527314662933, loss=1.7173707485198975
I0314 04:36:26.491727 140174546597632 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19425567984580994, loss=1.7463511228561401
I0314 04:37:04.163686 140174529812224 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.18793755769729614, loss=1.728153109550476
I0314 04:37:41.835279 140174546597632 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.18873824179172516, loss=1.7289501428604126
I0314 04:38:19.468494 140174529812224 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2567645311355591, loss=1.69223952293396
I0314 04:38:57.099025 140174546597632 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.20652170479297638, loss=1.7285064458847046
I0314 04:39:34.768269 140174529812224 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.18854184448719025, loss=1.67466402053833
I0314 04:39:55.955613 140362249443136 spec.py:298] Evaluating on the training split.
I0314 04:39:58.929570 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:44:13.010355 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 04:44:15.645768 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:46:43.889806 140362249443136 spec.py:326] Evaluating on the test split.
I0314 04:46:46.587738 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 04:49:25.359122 140362249443136 submission_runner.py:362] Time since start: 29573.17s, 	Step: 46858, 	{'train/accuracy': 0.6571536064147949, 'train/loss': 1.6222567558288574, 'train/bleu': 32.46321439260969, 'validation/accuracy': 0.6699234843254089, 'validation/loss': 1.5176522731781006, 'validation/bleu': 28.782965250092456, 'validation/num_examples': 3000, 'test/accuracy': 0.6819940805435181, 'test/loss': 1.4372467994689941, 'test/bleu': 28.311786705428617, 'test/num_examples': 3003}
I0314 04:49:25.368942 140174546597632 logging_writer.py:48] [46858] global_step=46858, preemption_count=0, score=17604.970880, test/accuracy=0.681994, test/bleu=28.311787, test/loss=1.437247, test/num_examples=3003, total_duration=29573.168073, train/accuracy=0.657154, train/bleu=32.463214, train/loss=1.622257, validation/accuracy=0.669923, validation/bleu=28.782965, validation/loss=1.517652, validation/num_examples=3000
I0314 04:49:26.356215 140362249443136 checkpoints.py:356] Saving checkpoint at step: 46858
I0314 04:49:30.752646 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_46858
I0314 04:49:30.771706 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_46858.
I0314 04:49:46.931281 140174529812224 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.19611237943172455, loss=1.7533111572265625
I0314 04:50:24.459917 140174521419520 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.18578609824180603, loss=1.7308628559112549
I0314 04:51:02.066871 140174529812224 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2152549922466278, loss=1.801398515701294
I0314 04:51:39.679464 140174521419520 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.22554215788841248, loss=1.789556622505188
I0314 04:52:17.359119 140174529812224 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.17821459472179413, loss=1.6487053632736206
I0314 04:52:55.009772 140174521419520 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.19232334196567535, loss=1.6603608131408691
I0314 04:53:32.669596 140174529812224 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.17977197468280792, loss=1.727498173713684
I0314 04:54:10.311390 140174521419520 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.22831852734088898, loss=1.6872482299804688
I0314 04:54:47.996368 140174529812224 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19120335578918457, loss=1.682837724685669
I0314 04:55:25.705232 140174521419520 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2217569351196289, loss=1.7158697843551636
I0314 04:56:03.382008 140174529812224 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.22132761776447296, loss=1.6643916368484497
I0314 04:56:41.110708 140174521419520 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.1910421997308731, loss=1.706119179725647
I0314 04:57:18.792288 140174529812224 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.20524653792381287, loss=1.6868767738342285
I0314 04:57:56.491402 140174521419520 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.195743590593338, loss=1.6237553358078003
I0314 04:58:34.097053 140174529812224 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.20600982010364532, loss=1.7324788570404053
I0314 04:59:11.743865 140174521419520 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.19159390032291412, loss=1.7184765338897705
I0314 04:59:49.377071 140174529812224 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.22499573230743408, loss=1.6743539571762085
I0314 05:00:26.996768 140174521419520 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.20091930031776428, loss=1.677152395248413
I0314 05:01:04.644883 140174529812224 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.1839137226343155, loss=1.6280219554901123
I0314 05:01:42.262759 140174521419520 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.20241332054138184, loss=1.6888175010681152
I0314 05:02:19.980791 140174529812224 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.1962558925151825, loss=1.688226342201233
I0314 05:02:57.607244 140174521419520 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.19756567478179932, loss=1.692926287651062
I0314 05:03:30.852165 140362249443136 spec.py:298] Evaluating on the training split.
I0314 05:03:33.829515 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:06:51.765033 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 05:06:54.404430 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:09:35.120032 140362249443136 spec.py:326] Evaluating on the test split.
I0314 05:09:37.823166 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:11:58.556394 140362249443136 submission_runner.py:362] Time since start: 30988.06s, 	Step: 49090, 	{'train/accuracy': 0.6543256640434265, 'train/loss': 1.6302969455718994, 'train/bleu': 31.571055193856335, 'validation/accuracy': 0.6719693541526794, 'validation/loss': 1.5077913999557495, 'validation/bleu': 29.030486197011683, 'validation/num_examples': 3000, 'test/accuracy': 0.6837836503982544, 'test/loss': 1.4256820678710938, 'test/bleu': 28.832387092424568, 'test/num_examples': 3003}
I0314 05:11:58.569552 140174529812224 logging_writer.py:48] [49090] global_step=49090, preemption_count=0, score=18441.416205, test/accuracy=0.683784, test/bleu=28.832387, test/loss=1.425682, test/num_examples=3003, total_duration=30988.064636, train/accuracy=0.654326, train/bleu=31.571055, train/loss=1.630297, validation/accuracy=0.671969, validation/bleu=29.030486, validation/loss=1.507791, validation/num_examples=3000
I0314 05:11:59.872373 140362249443136 checkpoints.py:356] Saving checkpoint at step: 49090
I0314 05:12:04.982541 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_49090
I0314 05:12:05.036378 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_49090.
I0314 05:12:09.173523 140174521419520 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.19132252037525177, loss=1.7080882787704468
I0314 05:12:46.765849 140174513026816 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19534221291542053, loss=1.7614737749099731
I0314 05:13:24.389700 140174521419520 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.1946052759885788, loss=1.7298498153686523
I0314 05:14:02.058621 140174513026816 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2010258436203003, loss=1.6124104261398315
I0314 05:14:39.701639 140174521419520 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.19551832973957062, loss=1.6679188013076782
I0314 05:15:17.361820 140174513026816 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.21869589388370514, loss=1.7433600425720215
I0314 05:15:55.027706 140174521419520 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.20832161605358124, loss=1.7206968069076538
I0314 05:16:32.648928 140174513026816 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.191759392619133, loss=1.7030614614486694
I0314 05:17:10.348969 140174521419520 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.21414603292942047, loss=1.7761101722717285
I0314 05:17:48.016514 140174513026816 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.1909596472978592, loss=1.7328977584838867
I0314 05:18:25.722197 140174521419520 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.20003990828990936, loss=1.7065726518630981
I0314 05:19:03.407103 140174513026816 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.22777551412582397, loss=1.7307202816009521
I0314 05:19:41.061334 140174521419520 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.20134514570236206, loss=1.7106633186340332
I0314 05:20:18.749944 140174513026816 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.20915450155735016, loss=1.7639130353927612
I0314 05:20:56.401616 140174521419520 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21489205956459045, loss=1.6739914417266846
I0314 05:21:34.038960 140174513026816 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.19497913122177124, loss=1.6983321905136108
I0314 05:22:11.733305 140174521419520 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.20081371068954468, loss=1.737563967704773
I0314 05:22:49.399192 140174513026816 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.19182367622852325, loss=1.6397303342819214
I0314 05:23:26.995354 140174521419520 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.20627380907535553, loss=1.7787929773330688
I0314 05:24:04.657442 140174513026816 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.19145455956459045, loss=1.6192432641983032
I0314 05:24:42.327713 140174521419520 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.19091039896011353, loss=1.6604077816009521
I0314 05:25:20.027714 140174513026816 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1863844096660614, loss=1.6968820095062256
I0314 05:25:57.712199 140174521419520 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.21173588931560516, loss=1.7074940204620361
I0314 05:26:05.342332 140362249443136 spec.py:298] Evaluating on the training split.
I0314 05:26:08.321592 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:30:13.342895 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 05:30:15.969907 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:33:17.916988 140362249443136 spec.py:326] Evaluating on the test split.
I0314 05:33:20.607598 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:36:13.952783 140362249443136 submission_runner.py:362] Time since start: 32342.55s, 	Step: 51322, 	{'train/accuracy': 0.6635710000991821, 'train/loss': 1.570998191833496, 'train/bleu': 32.44542809145213, 'validation/accuracy': 0.6733704209327698, 'validation/loss': 1.5007225275039673, 'validation/bleu': 29.3283344804116, 'validation/num_examples': 3000, 'test/accuracy': 0.6865493059158325, 'test/loss': 1.4169565439224243, 'test/bleu': 29.025726556626203, 'test/num_examples': 3003}
I0314 05:36:13.962701 140174513026816 logging_writer.py:48] [51322] global_step=51322, preemption_count=0, score=19278.192417, test/accuracy=0.686549, test/bleu=29.025727, test/loss=1.416957, test/num_examples=3003, total_duration=32342.554805, train/accuracy=0.663571, train/bleu=32.445428, train/loss=1.570998, validation/accuracy=0.673370, validation/bleu=29.328334, validation/loss=1.500723, validation/num_examples=3000
I0314 05:36:14.970135 140362249443136 checkpoints.py:356] Saving checkpoint at step: 51322
I0314 05:36:19.881094 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_51322
I0314 05:36:19.942499 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_51322.
I0314 05:36:49.627868 140174521419520 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.19908013939857483, loss=1.638997197151184
I0314 05:37:27.234741 140174504634112 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.20899340510368347, loss=1.7302097082138062
I0314 05:38:04.904007 140174521419520 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.18823401629924774, loss=1.7108025550842285
I0314 05:38:42.571853 140174504634112 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.1896255612373352, loss=1.6479562520980835
I0314 05:39:20.234972 140174521419520 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.19557365775108337, loss=1.7079641819000244
I0314 05:39:57.895556 140174504634112 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.1942184418439865, loss=1.7138330936431885
I0314 05:40:35.633238 140174521419520 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.18777820467948914, loss=1.638349175453186
I0314 05:41:13.327804 140174504634112 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.21054136753082275, loss=1.7397487163543701
I0314 05:41:51.027916 140174521419520 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20003105700016022, loss=1.6897143125534058
I0314 05:42:28.720324 140174504634112 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.18198078870773315, loss=1.6992528438568115
I0314 05:43:06.388541 140174521419520 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1948218196630478, loss=1.6742171049118042
I0314 05:43:44.108184 140174504634112 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19295303523540497, loss=1.715208649635315
I0314 05:44:21.758767 140174521419520 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.18215784430503845, loss=1.608257532119751
I0314 05:44:59.397703 140174504634112 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.19148071110248566, loss=1.714095950126648
I0314 05:45:37.095420 140174521419520 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.19979442656040192, loss=1.6861835718154907
I0314 05:46:14.754334 140174504634112 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2200927585363388, loss=1.6757827997207642
I0314 05:46:52.432863 140174521419520 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.19207587838172913, loss=1.6690644025802612
I0314 05:47:30.103192 140174504634112 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2505309581756592, loss=1.7104504108428955
I0314 05:48:07.779168 140174521419520 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.1894519031047821, loss=1.713026762008667
I0314 05:48:45.420003 140174504634112 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.22113245725631714, loss=1.7010658979415894
I0314 05:49:23.049248 140174521419520 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.5856702327728271, loss=1.646785020828247
I0314 05:50:00.745567 140174504634112 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20586533844470978, loss=1.6958895921707153
I0314 05:50:20.064386 140362249443136 spec.py:298] Evaluating on the training split.
I0314 05:50:23.048373 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:54:31.524956 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 05:54:34.180366 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:56:59.967657 140362249443136 spec.py:326] Evaluating on the test split.
I0314 05:57:02.689651 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 05:59:33.351597 140362249443136 submission_runner.py:362] Time since start: 33797.28s, 	Step: 53553, 	{'train/accuracy': 0.6601815819740295, 'train/loss': 1.6014275550842285, 'train/bleu': 32.31214223579732, 'validation/accuracy': 0.6755898594856262, 'validation/loss': 1.4959287643432617, 'validation/bleu': 29.16912043726757, 'validation/num_examples': 3000, 'test/accuracy': 0.6896635890007019, 'test/loss': 1.4046108722686768, 'test/bleu': 29.387408864400843, 'test/num_examples': 3003}
I0314 05:59:33.365102 140174521419520 logging_writer.py:48] [53553] global_step=53553, preemption_count=0, score=20114.469725, test/accuracy=0.689664, test/bleu=29.387409, test/loss=1.404611, test/num_examples=3003, total_duration=33797.276859, train/accuracy=0.660182, train/bleu=32.312142, train/loss=1.601428, validation/accuracy=0.675590, validation/bleu=29.169120, validation/loss=1.495929, validation/num_examples=3000
I0314 05:59:34.670764 140362249443136 checkpoints.py:356] Saving checkpoint at step: 53553
I0314 05:59:40.297702 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_53553
I0314 05:59:40.322349 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_53553.
I0314 05:59:58.361857 140174504634112 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.19640131294727325, loss=1.5836896896362305
I0314 06:00:35.908194 140174496241408 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.19807353615760803, loss=1.6888362169265747
I0314 06:01:13.458366 140174504634112 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.20869199931621552, loss=1.7369097471237183
I0314 06:01:51.118664 140174496241408 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.2310430258512497, loss=1.6975022554397583
I0314 06:02:28.798340 140174504634112 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21796491742134094, loss=1.6842108964920044
I0314 06:03:06.392650 140174496241408 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.20061488449573517, loss=1.6363766193389893
I0314 06:03:44.044664 140174504634112 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.1967916041612625, loss=1.6458977460861206
I0314 06:04:21.680166 140174496241408 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.22390961647033691, loss=1.6908892393112183
I0314 06:04:59.342052 140174504634112 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.20339041948318481, loss=1.7081787586212158
I0314 06:05:37.007070 140174496241408 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.1992691308259964, loss=1.7623704671859741
I0314 06:06:14.686558 140174504634112 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.20480023324489594, loss=1.6500589847564697
I0314 06:06:52.307253 140174496241408 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.21086981892585754, loss=1.737729787826538
I0314 06:07:29.975255 140174504634112 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.19505071640014648, loss=1.6088190078735352
I0314 06:08:07.738480 140174496241408 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.1977606564760208, loss=1.8071457147598267
I0314 06:08:45.416935 140174504634112 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.18345965445041656, loss=1.6323330402374268
I0314 06:09:23.087120 140174496241408 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.23761625587940216, loss=1.7070754766464233
I0314 06:10:00.791987 140174504634112 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.20283719897270203, loss=1.6069612503051758
I0314 06:10:38.457780 140174496241408 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19147616624832153, loss=1.6530332565307617
I0314 06:11:16.122949 140174504634112 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.2256801873445511, loss=1.6274091005325317
I0314 06:11:53.776573 140174496241408 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.19287991523742676, loss=1.6617176532745361
I0314 06:12:31.466349 140174504634112 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.19229409098625183, loss=1.6485981941223145
I0314 06:13:09.104571 140174496241408 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.2104400098323822, loss=1.6599527597427368
I0314 06:13:40.483896 140362249443136 spec.py:298] Evaluating on the training split.
I0314 06:13:43.464097 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 06:17:31.661098 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 06:17:34.307141 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 06:19:55.568313 140362249443136 spec.py:326] Evaluating on the test split.
I0314 06:19:58.267063 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 06:22:17.187163 140362249443136 submission_runner.py:362] Time since start: 35197.70s, 	Step: 55785, 	{'train/accuracy': 0.65819251537323, 'train/loss': 1.6151007413864136, 'train/bleu': 32.755434773289586, 'validation/accuracy': 0.6769289970397949, 'validation/loss': 1.479386329650879, 'validation/bleu': 29.329343182014053, 'validation/num_examples': 3000, 'test/accuracy': 0.691278874874115, 'test/loss': 1.3896815776824951, 'test/bleu': 29.13925278526704, 'test/num_examples': 3003}
I0314 06:22:17.198791 140174504634112 logging_writer.py:48] [55785] global_step=55785, preemption_count=0, score=20951.210311, test/accuracy=0.691279, test/bleu=29.139253, test/loss=1.389682, test/num_examples=3003, total_duration=35197.696357, train/accuracy=0.658193, train/bleu=32.755435, train/loss=1.615101, validation/accuracy=0.676929, validation/bleu=29.329343, validation/loss=1.479386, validation/num_examples=3000
I0314 06:22:18.203813 140362249443136 checkpoints.py:356] Saving checkpoint at step: 55785
I0314 06:22:22.687390 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_55785
I0314 06:22:22.737576 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_55785.
I0314 06:22:28.752312 140174496241408 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.20651012659072876, loss=1.6870545148849487
I0314 06:23:06.357385 140174487848704 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.1890595406293869, loss=1.633892297744751
I0314 06:23:44.029899 140174496241408 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.17654703557491302, loss=1.586430311203003
I0314 06:24:21.592838 140174487848704 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.19916647672653198, loss=1.6631680727005005
I0314 06:24:59.270635 140174496241408 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19084003567695618, loss=1.5837618112564087
I0314 06:25:36.918802 140174487848704 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.19373735785484314, loss=1.658624529838562
I0314 06:26:14.604026 140174496241408 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.20737570524215698, loss=1.661108374595642
I0314 06:26:52.271005 140174487848704 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1953485757112503, loss=1.753004789352417
I0314 06:27:29.938827 140174496241408 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.19811098277568817, loss=1.7244479656219482
I0314 06:28:07.608587 140174487848704 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.18967337906360626, loss=1.7203121185302734
I0314 06:28:45.247476 140174496241408 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.223837211728096, loss=1.6358715295791626
I0314 06:29:22.954497 140174487848704 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.20299206674098969, loss=1.6648849248886108
I0314 06:30:00.622070 140174496241408 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2003474235534668, loss=1.6566410064697266
I0314 06:30:38.377977 140174487848704 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.18713165819644928, loss=1.6569255590438843
I0314 06:31:16.010518 140174496241408 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.25117728114128113, loss=1.6814019680023193
I0314 06:31:53.718611 140174487848704 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.2165510058403015, loss=1.6731321811676025
I0314 06:32:31.387160 140174496241408 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.18210329115390778, loss=1.6077537536621094
I0314 06:33:09.060912 140174487848704 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1892784833908081, loss=1.6510775089263916
I0314 06:33:46.720020 140174496241408 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19916673004627228, loss=1.6202744245529175
I0314 06:34:24.369699 140174487848704 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2010062336921692, loss=1.6935535669326782
I0314 06:35:01.983766 140174496241408 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.19505438208580017, loss=1.6573545932769775
I0314 06:35:39.688262 140174487848704 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.19262060523033142, loss=1.6909446716308594
I0314 06:36:17.408765 140174496241408 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.20928242802619934, loss=1.6604342460632324
I0314 06:36:22.800162 140362249443136 spec.py:298] Evaluating on the training split.
I0314 06:36:25.781248 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 06:40:28.050146 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 06:40:30.691514 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 06:43:01.545218 140362249443136 spec.py:326] Evaluating on the test split.
I0314 06:43:04.229215 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 06:45:47.236023 140362249443136 submission_runner.py:362] Time since start: 36560.01s, 	Step: 58016, 	{'train/accuracy': 0.6656844615936279, 'train/loss': 1.5551341772079468, 'train/bleu': 33.26694420429745, 'validation/accuracy': 0.679222822189331, 'validation/loss': 1.4697920083999634, 'validation/bleu': 29.382844667726182, 'validation/num_examples': 3000, 'test/accuracy': 0.6908140182495117, 'test/loss': 1.3853192329406738, 'test/bleu': 29.09565726119244, 'test/num_examples': 3003}
I0314 06:45:47.247911 140174487848704 logging_writer.py:48] [58016] global_step=58016, preemption_count=0, score=21787.203674, test/accuracy=0.690814, test/bleu=29.095657, test/loss=1.385319, test/num_examples=3003, total_duration=36560.012635, train/accuracy=0.665684, train/bleu=33.266944, train/loss=1.555134, validation/accuracy=0.679223, validation/bleu=29.382845, validation/loss=1.469792, validation/num_examples=3000
I0314 06:45:48.234081 140362249443136 checkpoints.py:356] Saving checkpoint at step: 58016
I0314 06:45:52.724123 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_58016
I0314 06:45:52.757631 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_58016.
I0314 06:46:24.684660 140174496241408 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.19104310870170593, loss=1.7447314262390137
I0314 06:47:02.266622 140174479456000 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19305071234703064, loss=1.6157821416854858
I0314 06:47:39.978829 140174496241408 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.21461960673332214, loss=1.6668251752853394
I0314 06:48:17.639571 140174479456000 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.21810489892959595, loss=1.6435861587524414
I0314 06:48:55.337692 140174496241408 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.20005753636360168, loss=1.6684367656707764
I0314 06:49:33.040925 140174479456000 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3758155405521393, loss=1.6875696182250977
I0314 06:50:10.761381 140174496241408 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.18883366882801056, loss=1.6022318601608276
I0314 06:50:48.456713 140174479456000 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.1899814009666443, loss=1.6275771856307983
I0314 06:51:26.154732 140174496241408 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.1969793140888214, loss=1.7024097442626953
I0314 06:52:03.793100 140174479456000 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.21205748617649078, loss=1.7207711935043335
I0314 06:52:41.479545 140174496241408 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19098742306232452, loss=1.6811866760253906
I0314 06:53:19.164552 140174479456000 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.26660487055778503, loss=1.6560426950454712
I0314 06:53:56.872446 140174496241408 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.20145229995250702, loss=1.643973708152771
I0314 06:54:34.545419 140174479456000 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.19890256226062775, loss=1.56238853931427
I0314 06:55:12.224892 140174496241408 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.20496919751167297, loss=1.6297775506973267
I0314 06:55:49.868900 140174479456000 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.19797936081886292, loss=1.5902833938598633
I0314 06:56:27.516329 140174496241408 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.21216757595539093, loss=1.633569359779358
I0314 06:57:05.236338 140174479456000 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.1981610804796219, loss=1.5837360620498657
I0314 06:57:42.950502 140174496241408 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.2141486406326294, loss=1.6833760738372803
I0314 06:58:20.613731 140174479456000 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2042982280254364, loss=1.6375823020935059
I0314 06:58:58.276147 140174496241408 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.18554528057575226, loss=1.5742253065109253
I0314 06:59:35.973278 140174479456000 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.20172905921936035, loss=1.6231000423431396
I0314 06:59:53.051454 140362249443136 spec.py:298] Evaluating on the training split.
I0314 06:59:56.025543 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:03:41.370442 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 07:03:44.017107 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:06:39.594796 140362249443136 spec.py:326] Evaluating on the test split.
I0314 07:06:42.277331 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:09:48.979579 140362249443136 submission_runner.py:362] Time since start: 37970.26s, 	Step: 60247, 	{'train/accuracy': 0.6657096743583679, 'train/loss': 1.5562946796417236, 'train/bleu': 33.08732463210115, 'validation/accuracy': 0.679396390914917, 'validation/loss': 1.4645025730133057, 'validation/bleu': 29.69073636905002, 'validation/num_examples': 3000, 'test/accuracy': 0.692661702632904, 'test/loss': 1.373465895652771, 'test/bleu': 29.352713988778685, 'test/num_examples': 3003}
I0314 07:09:48.993041 140174496241408 logging_writer.py:48] [60247] global_step=60247, preemption_count=0, score=22623.990482, test/accuracy=0.692662, test/bleu=29.352714, test/loss=1.373466, test/num_examples=3003, total_duration=37970.263926, train/accuracy=0.665710, train/bleu=33.087325, train/loss=1.556295, validation/accuracy=0.679396, validation/bleu=29.690736, validation/loss=1.464503, validation/num_examples=3000
I0314 07:09:50.300243 140362249443136 checkpoints.py:356] Saving checkpoint at step: 60247
I0314 07:09:55.401622 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_60247
I0314 07:09:55.429847 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_60247.
I0314 07:10:15.697310 140174479456000 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.21660558879375458, loss=1.6617828607559204
I0314 07:10:53.276107 140174471063296 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.21057584881782532, loss=1.5867995023727417
I0314 07:11:30.900396 140174479456000 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2105858474969864, loss=1.6896593570709229
I0314 07:12:08.604276 140174471063296 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2069282978773117, loss=1.7784323692321777
I0314 07:12:46.226101 140174479456000 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.18488994240760803, loss=1.5824148654937744
I0314 07:13:23.909854 140174471063296 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2039916068315506, loss=1.651064157485962
I0314 07:14:01.604332 140174479456000 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.19601866602897644, loss=1.6745144128799438
I0314 07:14:39.245079 140174471063296 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2008226215839386, loss=1.6890195608139038
I0314 07:15:16.933120 140174479456000 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.21270020306110382, loss=1.642696738243103
I0314 07:15:54.611779 140174471063296 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2044234424829483, loss=1.7153558731079102
I0314 07:16:32.288733 140174479456000 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2015267163515091, loss=1.6760855913162231
I0314 07:17:09.922428 140174471063296 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.19491271674633026, loss=1.6325500011444092
I0314 07:17:47.593216 140174479456000 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19867564737796783, loss=1.6277254819869995
I0314 07:18:25.279435 140174471063296 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.19343648850917816, loss=1.6450166702270508
I0314 07:19:02.915863 140174479456000 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.19120000302791595, loss=1.6042133569717407
I0314 07:19:40.649820 140174471063296 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.20010216534137726, loss=1.6888062953948975
I0314 07:20:18.287880 140174479456000 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.20257237553596497, loss=1.6840134859085083
I0314 07:20:55.975846 140174471063296 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.20306310057640076, loss=1.6962099075317383
I0314 07:21:33.665329 140174479456000 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.19178538024425507, loss=1.6215929985046387
I0314 07:22:11.394128 140174471063296 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.5776458382606506, loss=1.6564456224441528
I0314 07:22:49.089647 140174479456000 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.21419787406921387, loss=1.6335811614990234
I0314 07:23:26.790683 140174471063296 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.20970764756202698, loss=1.6418559551239014
I0314 07:23:55.507978 140362249443136 spec.py:298] Evaluating on the training split.
I0314 07:23:58.486586 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:27:45.082167 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 07:27:47.734975 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:30:36.482930 140362249443136 spec.py:326] Evaluating on the test split.
I0314 07:30:39.170698 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:33:31.945610 140362249443136 submission_runner.py:362] Time since start: 39412.72s, 	Step: 62478, 	{'train/accuracy': 0.6648634672164917, 'train/loss': 1.5673481225967407, 'train/bleu': 33.100659059809296, 'validation/accuracy': 0.6807107329368591, 'validation/loss': 1.457166314125061, 'validation/bleu': 29.706534131390086, 'validation/num_examples': 3000, 'test/accuracy': 0.6943234205245972, 'test/loss': 1.3682466745376587, 'test/bleu': 29.394509592756187, 'test/num_examples': 3003}
I0314 07:33:31.957906 140174479456000 logging_writer.py:48] [62478] global_step=62478, preemption_count=0, score=23460.752337, test/accuracy=0.694323, test/bleu=29.394510, test/loss=1.368247, test/num_examples=3003, total_duration=39412.720452, train/accuracy=0.664863, train/bleu=33.100659, train/loss=1.567348, validation/accuracy=0.680711, validation/bleu=29.706534, validation/loss=1.457166, validation/num_examples=3000
I0314 07:33:32.960223 140362249443136 checkpoints.py:356] Saving checkpoint at step: 62478
I0314 07:33:36.848926 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_62478
I0314 07:33:36.871603 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_62478.
I0314 07:33:45.518463 140174471063296 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20024773478507996, loss=1.6009129285812378
I0314 07:34:23.008116 140174462670592 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.23735050857067108, loss=1.68192458152771
I0314 07:35:00.596193 140174471063296 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.20354537665843964, loss=1.6679983139038086
I0314 07:35:38.287333 140174462670592 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.20678356289863586, loss=1.6342402696609497
I0314 07:36:15.917271 140174471063296 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.21375121176242828, loss=1.5927280187606812
I0314 07:36:53.604289 140174462670592 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.19168618321418762, loss=1.57080078125
I0314 07:37:31.295113 140174471063296 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.21165063977241516, loss=1.6318610906600952
I0314 07:38:08.993401 140174462670592 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.20067098736763, loss=1.6839489936828613
I0314 07:38:46.687742 140174471063296 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.21778035163879395, loss=1.562787652015686
I0314 07:39:24.387752 140174462670592 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.20090825855731964, loss=1.59957754611969
I0314 07:40:02.091897 140174471063296 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.21841779351234436, loss=1.6459321975708008
I0314 07:40:39.758379 140174462670592 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.20198498666286469, loss=1.5906407833099365
I0314 07:41:17.509119 140174471063296 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20052418112754822, loss=1.6652041673660278
I0314 07:41:55.206151 140174462670592 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.20254121720790863, loss=1.638785719871521
I0314 07:42:32.915874 140174471063296 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.21173883974552155, loss=1.612649917602539
I0314 07:43:10.633816 140174462670592 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.20029661059379578, loss=1.619020938873291
I0314 07:43:48.303983 140174471063296 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.2279009073972702, loss=1.5571539402008057
I0314 07:44:25.992633 140174462670592 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.20058473944664001, loss=1.624572515487671
I0314 07:45:03.712141 140174471063296 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19272024929523468, loss=1.6024519205093384
I0314 07:45:41.402650 140174462670592 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.18840453028678894, loss=1.6148552894592285
I0314 07:46:19.045077 140174471063296 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.19652609527111053, loss=1.6351677179336548
I0314 07:46:56.720219 140174462670592 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.1905808299779892, loss=1.5670486688613892
I0314 07:47:34.400113 140174471063296 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.2203931212425232, loss=1.6438969373703003
I0314 07:47:37.131075 140362249443136 spec.py:298] Evaluating on the training split.
I0314 07:47:40.112713 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:51:55.250881 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 07:51:57.895097 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:54:42.946220 140362249443136 spec.py:326] Evaluating on the test split.
I0314 07:54:45.632693 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 07:57:47.958852 140362249443136 submission_runner.py:362] Time since start: 40834.34s, 	Step: 64709, 	{'train/accuracy': 0.6688649654388428, 'train/loss': 1.539658784866333, 'train/bleu': 34.075775404244766, 'validation/accuracy': 0.6819258332252502, 'validation/loss': 1.4514163732528687, 'validation/bleu': 29.856367639101986, 'validation/num_examples': 3000, 'test/accuracy': 0.6962756514549255, 'test/loss': 1.3569159507751465, 'test/bleu': 29.80527906017588, 'test/num_examples': 3003}
I0314 07:57:47.970504 140174462670592 logging_writer.py:48] [64709] global_step=64709, preemption_count=0, score=24297.520780, test/accuracy=0.696276, test/bleu=29.805279, test/loss=1.356916, test/num_examples=3003, total_duration=40834.343548, train/accuracy=0.668865, train/bleu=34.075775, train/loss=1.539659, validation/accuracy=0.681926, validation/bleu=29.856368, validation/loss=1.451416, validation/num_examples=3000
I0314 07:57:48.949164 140362249443136 checkpoints.py:356] Saving checkpoint at step: 64709
I0314 07:57:52.859363 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_64709
I0314 07:57:52.878414 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_64709.
I0314 07:58:27.380934 140174471063296 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.19956324994564056, loss=1.5999261140823364
I0314 07:59:04.973475 140174454277888 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.21418412029743195, loss=1.6946505308151245
I0314 07:59:42.583908 140174471063296 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2172885537147522, loss=1.6074392795562744
I0314 08:00:20.294516 140174454277888 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.20008742809295654, loss=1.5848907232284546
I0314 08:00:57.943352 140174471063296 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.21520860493183136, loss=1.5988397598266602
I0314 08:01:35.589735 140174454277888 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2037390172481537, loss=1.690975546836853
I0314 08:02:13.219059 140174471063296 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.19966644048690796, loss=1.6268341541290283
I0314 08:02:50.929964 140174454277888 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.21848231554031372, loss=1.6664175987243652
I0314 08:03:28.601884 140174471063296 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.1960233896970749, loss=1.5684638023376465
I0314 08:04:06.290788 140174454277888 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.21495960652828217, loss=1.6416102647781372
I0314 08:04:43.965773 140174471063296 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.19614312052726746, loss=1.595781922340393
I0314 08:05:21.680488 140174454277888 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.19717250764369965, loss=1.6073919534683228
I0314 08:05:59.346130 140174471063296 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.21489201486110687, loss=1.61043381690979
I0314 08:06:37.028229 140174454277888 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.19859683513641357, loss=1.5267118215560913
I0314 08:07:14.663562 140174471063296 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.20900127291679382, loss=1.6287506818771362
I0314 08:07:52.309024 140174454277888 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.19788523018360138, loss=1.5651308298110962
I0314 08:08:29.950213 140174471063296 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2089996635913849, loss=1.6274733543395996
I0314 08:09:07.639639 140174454277888 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.20989692211151123, loss=1.5282557010650635
I0314 08:09:45.297772 140174471063296 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.20013323426246643, loss=1.6609472036361694
I0314 08:10:22.985548 140174454277888 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.20367003977298737, loss=1.5636333227157593
I0314 08:11:00.678983 140174471063296 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2083720564842224, loss=1.5219660997390747
I0314 08:11:38.364886 140174454277888 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.21231673657894135, loss=1.5887069702148438
I0314 08:11:53.132514 140362249443136 spec.py:298] Evaluating on the training split.
I0314 08:11:56.130297 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 08:16:18.159730 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 08:16:20.803966 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 08:19:07.016649 140362249443136 spec.py:326] Evaluating on the test split.
I0314 08:19:09.702186 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 08:21:56.098620 140362249443136 submission_runner.py:362] Time since start: 42290.34s, 	Step: 66941, 	{'train/accuracy': 0.6656746864318848, 'train/loss': 1.550531029701233, 'train/bleu': 33.21185821451705, 'validation/accuracy': 0.6841948628425598, 'validation/loss': 1.4397732019424438, 'validation/bleu': 29.929811644462767, 'validation/num_examples': 3000, 'test/accuracy': 0.6980768442153931, 'test/loss': 1.3459830284118652, 'test/bleu': 29.831121429786897, 'test/num_examples': 3003}
I0314 08:21:56.111275 140174471063296 logging_writer.py:48] [66941] global_step=66941, preemption_count=0, score=25134.194210, test/accuracy=0.698077, test/bleu=29.831121, test/loss=1.345983, test/num_examples=3003, total_duration=42290.344971, train/accuracy=0.665675, train/bleu=33.211858, train/loss=1.550531, validation/accuracy=0.684195, validation/bleu=29.929812, validation/loss=1.439773, validation/num_examples=3000
I0314 08:21:57.098268 140362249443136 checkpoints.py:356] Saving checkpoint at step: 66941
I0314 08:22:01.102225 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_66941
I0314 08:22:01.143221 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_66941.
I0314 08:22:23.659576 140174454277888 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.20714056491851807, loss=1.5478509664535522
I0314 08:23:01.249273 140174445885184 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.19381482899188995, loss=1.5843008756637573
I0314 08:23:38.836112 140174454277888 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2144608348608017, loss=1.6237916946411133
I0314 08:24:16.494307 140174445885184 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3473348319530487, loss=1.6511799097061157
I0314 08:24:54.192064 140174454277888 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.19668447971343994, loss=1.6001603603363037
I0314 08:25:31.878622 140174445885184 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.21943046152591705, loss=1.6539796590805054
I0314 08:26:09.537119 140174454277888 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.22726517915725708, loss=1.6156097650527954
I0314 08:26:47.178942 140174445885184 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.19425347447395325, loss=1.4982197284698486
I0314 08:27:24.818616 140174454277888 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.23937179148197174, loss=1.6505590677261353
I0314 08:28:02.455323 140174445885184 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.22694000601768494, loss=1.6673952341079712
I0314 08:28:40.129539 140174454277888 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.20239880681037903, loss=1.5355448722839355
I0314 08:29:17.821003 140174445885184 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.21472783386707306, loss=1.6481186151504517
I0314 08:29:55.513242 140174454277888 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.1931021362543106, loss=1.6487901210784912
I0314 08:30:33.186182 140174445885184 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.19613176584243774, loss=1.6110337972640991
I0314 08:31:10.908037 140174454277888 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.21998371183872223, loss=1.7330845594406128
I0314 08:31:48.605824 140174445885184 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.20961321890354156, loss=1.6177290678024292
I0314 08:32:26.261784 140174454277888 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2087569385766983, loss=1.5840003490447998
I0314 08:33:03.956315 140174445885184 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.22358685731887817, loss=1.6752631664276123
I0314 08:33:41.634166 140174454277888 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.23242294788360596, loss=1.6144559383392334
I0314 08:34:19.287641 140174445885184 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.21809540688991547, loss=1.5856245756149292
I0314 08:34:56.952665 140174454277888 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.21012914180755615, loss=1.599766492843628
I0314 08:35:34.637775 140174445885184 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2141941487789154, loss=1.6215795278549194
I0314 08:36:01.501390 140362249443136 spec.py:298] Evaluating on the training split.
I0314 08:36:04.472795 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 08:40:15.986438 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 08:40:18.611078 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 08:43:13.048652 140362249443136 spec.py:326] Evaluating on the test split.
I0314 08:43:15.748269 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 08:45:57.958133 140362249443136 submission_runner.py:362] Time since start: 43738.71s, 	Step: 69173, 	{'train/accuracy': 0.683720588684082, 'train/loss': 1.438248872756958, 'train/bleu': 34.86608844240907, 'validation/accuracy': 0.685732364654541, 'validation/loss': 1.4290837049484253, 'validation/bleu': 30.075729879807703, 'validation/num_examples': 3000, 'test/accuracy': 0.7005054950714111, 'test/loss': 1.3343896865844727, 'test/bleu': 29.839132366150555, 'test/num_examples': 3003}
I0314 08:45:57.970426 140174454277888 logging_writer.py:48] [69173] global_step=69173, preemption_count=0, score=25971.048472, test/accuracy=0.700505, test/bleu=29.839132, test/loss=1.334390, test/num_examples=3003, total_duration=43738.713867, train/accuracy=0.683721, train/bleu=34.866088, train/loss=1.438249, validation/accuracy=0.685732, validation/bleu=30.075730, validation/loss=1.429084, validation/num_examples=3000
I0314 08:45:58.962889 140362249443136 checkpoints.py:356] Saving checkpoint at step: 69173
I0314 08:46:02.920597 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_69173
I0314 08:46:02.945205 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_69173.
I0314 08:46:13.490860 140174445885184 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.19921685755252838, loss=1.5284868478775024
I0314 08:46:51.021979 140174437492480 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.20044772326946259, loss=1.6029672622680664
I0314 08:47:28.645437 140174445885184 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2026962786912918, loss=1.5424275398254395
I0314 08:48:06.336395 140174437492480 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.21507611870765686, loss=1.550940990447998
I0314 08:48:43.971385 140174445885184 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.20800109207630157, loss=1.5745680332183838
I0314 08:49:21.671499 140174437492480 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2125847488641739, loss=1.5878149271011353
I0314 08:49:59.341968 140174445885184 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.1986432820558548, loss=1.5354244709014893
I0314 08:50:37.084283 140174437492480 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.19735676050186157, loss=1.5636723041534424
I0314 08:51:14.772060 140174445885184 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.1956123262643814, loss=1.5722905397415161
I0314 08:51:52.437924 140174437492480 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.21057380735874176, loss=1.6487022638320923
I0314 08:52:30.063784 140174445885184 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.21322128176689148, loss=1.6503891944885254
I0314 08:53:07.714805 140174437492480 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.24716432392597198, loss=1.5855026245117188
I0314 08:53:45.354496 140174445885184 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.20062299072742462, loss=1.5991731882095337
I0314 08:54:23.055969 140174437492480 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.19485239684581757, loss=1.5406850576400757
I0314 08:55:00.744455 140174445885184 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.22178038954734802, loss=1.6955708265304565
I0314 08:55:38.370850 140174437492480 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.19450543820858002, loss=1.6148873567581177
I0314 08:56:16.036808 140174445885184 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.20339679718017578, loss=1.5844446420669556
I0314 08:56:53.738847 140174437492480 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.20355622470378876, loss=1.5433299541473389
I0314 08:57:31.445530 140174445885184 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20075862109661102, loss=1.577284574508667
I0314 08:58:09.149304 140174437492480 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.20120376348495483, loss=1.556562900543213
I0314 08:58:46.803623 140174445885184 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.215351864695549, loss=1.5359784364700317
I0314 08:59:24.493289 140174437492480 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.20769408345222473, loss=1.5778076648712158
I0314 09:00:02.183516 140174445885184 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2913617491722107, loss=1.5276635885238647
I0314 09:00:03.032881 140362249443136 spec.py:298] Evaluating on the training split.
I0314 09:00:06.010879 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:04:29.332294 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 09:04:31.995269 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:07:48.466470 140362249443136 spec.py:326] Evaluating on the test split.
I0314 09:07:51.174540 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:11:14.415941 140362249443136 submission_runner.py:362] Time since start: 45180.25s, 	Step: 71404, 	{'train/accuracy': 0.674594521522522, 'train/loss': 1.4916661977767944, 'train/bleu': 33.85651910761374, 'validation/accuracy': 0.6862035393714905, 'validation/loss': 1.4226713180541992, 'validation/bleu': 30.284451896905654, 'validation/num_examples': 3000, 'test/accuracy': 0.7018534541130066, 'test/loss': 1.3239104747772217, 'test/bleu': 30.34487870493568, 'test/num_examples': 3003}
I0314 09:11:14.428858 140174437492480 logging_writer.py:48] [71404] global_step=71404, preemption_count=0, score=26807.771700, test/accuracy=0.701853, test/bleu=30.344879, test/loss=1.323910, test/num_examples=3003, total_duration=45180.245358, train/accuracy=0.674595, train/bleu=33.856519, train/loss=1.491666, validation/accuracy=0.686204, validation/bleu=30.284452, validation/loss=1.422671, validation/num_examples=3000
I0314 09:11:15.427654 140362249443136 checkpoints.py:356] Saving checkpoint at step: 71404
I0314 09:11:19.376991 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_71404
I0314 09:11:19.395725 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_71404.
I0314 09:11:55.800461 140174445885184 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.1987461894750595, loss=1.569455862045288
I0314 09:12:33.346014 140174429099776 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.21055102348327637, loss=1.593660593032837
I0314 09:13:10.992154 140174445885184 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.39202389121055603, loss=1.5536025762557983
I0314 09:13:48.675333 140174429099776 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.21071551740169525, loss=1.652167558670044
I0314 09:14:26.333631 140174445885184 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.20582349598407745, loss=1.6344642639160156
I0314 09:15:03.994861 140174429099776 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.20149704813957214, loss=1.615879774093628
I0314 09:15:41.665313 140174445885184 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.21689587831497192, loss=1.6476181745529175
I0314 09:16:19.332279 140174429099776 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2026749849319458, loss=1.5564110279083252
I0314 09:16:57.005531 140174445885184 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.21061986684799194, loss=1.6490979194641113
I0314 09:17:34.649884 140174429099776 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.20094028115272522, loss=1.6567891836166382
I0314 09:18:12.356934 140174445885184 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.20064638555049896, loss=1.5275444984436035
I0314 09:18:50.011883 140174429099776 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.19755171239376068, loss=1.5988999605178833
I0314 09:19:27.736722 140174445885184 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2070535272359848, loss=1.5994298458099365
I0314 09:20:05.380751 140174429099776 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2037799060344696, loss=1.5336718559265137
I0314 09:20:43.039031 140174445885184 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.20389588177204132, loss=1.5610870122909546
I0314 09:21:20.756245 140174429099776 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.22461272776126862, loss=1.603386640548706
I0314 09:21:58.432721 140174445885184 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.20263753831386566, loss=1.561544418334961
I0314 09:22:36.077651 140174429099776 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.5754399299621582, loss=1.5592572689056396
I0314 09:23:13.833347 140174445885184 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.21553798019886017, loss=1.558774709701538
I0314 09:23:51.482752 140174429099776 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2062433362007141, loss=1.6519800424575806
I0314 09:24:29.143272 140174445885184 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.21366557478904724, loss=1.576279640197754
I0314 09:25:06.911967 140174429099776 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.20441663265228271, loss=1.53103768825531
I0314 09:25:19.439192 140362249443136 spec.py:298] Evaluating on the training split.
I0314 09:25:22.421468 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:29:26.695106 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 09:29:29.323512 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:31:58.531898 140362249443136 spec.py:326] Evaluating on the test split.
I0314 09:32:01.229270 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:34:22.922137 140362249443136 submission_runner.py:362] Time since start: 46696.65s, 	Step: 73635, 	{'train/accuracy': 0.6756576299667358, 'train/loss': 1.4886127710342407, 'train/bleu': 33.85141670897177, 'validation/accuracy': 0.6877533793449402, 'validation/loss': 1.4158799648284912, 'validation/bleu': 30.272028257036162, 'validation/num_examples': 3000, 'test/accuracy': 0.7029574513435364, 'test/loss': 1.316908597946167, 'test/bleu': 30.412541661395533, 'test/num_examples': 3003}
I0314 09:34:22.934098 140174445885184 logging_writer.py:48] [73635] global_step=73635, preemption_count=0, score=27644.342528, test/accuracy=0.702957, test/bleu=30.412542, test/loss=1.316909, test/num_examples=3003, total_duration=46696.651671, train/accuracy=0.675658, train/bleu=33.851417, train/loss=1.488613, validation/accuracy=0.687753, validation/bleu=30.272028, validation/loss=1.415880, validation/num_examples=3000
I0314 09:34:23.926553 140362249443136 checkpoints.py:356] Saving checkpoint at step: 73635
I0314 09:34:28.178269 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_73635
I0314 09:34:28.207978 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_73635.
I0314 09:34:53.046195 140174429099776 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.21093930304050446, loss=1.6623090505599976
I0314 09:35:30.634815 140174420707072 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.19720859825611115, loss=1.581423044204712
I0314 09:36:08.301964 140174429099776 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.20638099312782288, loss=1.5824003219604492
I0314 09:36:45.920777 140174420707072 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.21654252707958221, loss=1.5665814876556396
I0314 09:37:23.577461 140174429099776 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.21361711621284485, loss=1.516999363899231
I0314 09:38:01.229138 140174420707072 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.21470627188682556, loss=1.5490878820419312
I0314 09:38:38.902577 140174429099776 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.19810372591018677, loss=1.524007797241211
I0314 09:39:16.584215 140174420707072 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.19838175177574158, loss=1.5239460468292236
I0314 09:39:54.212154 140174429099776 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.21800105273723602, loss=1.5686924457550049
I0314 09:40:31.882408 140174420707072 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2190479338169098, loss=1.6143672466278076
I0314 09:41:09.557118 140174429099776 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.2205270677804947, loss=1.6190208196640015
I0314 09:41:47.291964 140174420707072 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2142808586359024, loss=1.6749591827392578
I0314 09:42:24.925148 140174429099776 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.19727723300457, loss=1.4674898386001587
I0314 09:43:02.570010 140174420707072 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.19280292093753815, loss=1.5618144273757935
I0314 09:43:40.296100 140174429099776 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2069820910692215, loss=1.5673589706420898
I0314 09:44:17.927544 140174420707072 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4037526845932007, loss=1.5267988443374634
I0314 09:44:55.639530 140174429099776 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.21011394262313843, loss=1.5082842111587524
I0314 09:45:33.343343 140174420707072 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20585332810878754, loss=1.5687847137451172
I0314 09:46:11.032134 140174429099776 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.21475769579410553, loss=1.5113208293914795
I0314 09:46:48.727678 140174420707072 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20344533026218414, loss=1.5825849771499634
I0314 09:47:26.405370 140174429099776 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.20243960618972778, loss=1.5086407661437988
I0314 09:48:04.050159 140174420707072 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.20286700129508972, loss=1.5499207973480225
I0314 09:48:28.270096 140362249443136 spec.py:298] Evaluating on the training split.
I0314 09:48:31.251585 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:52:13.360447 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 09:52:16.009522 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:54:44.216180 140362249443136 spec.py:326] Evaluating on the test split.
I0314 09:54:46.919140 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 09:57:01.758759 140362249443136 submission_runner.py:362] Time since start: 48085.48s, 	Step: 75866, 	{'train/accuracy': 0.683646559715271, 'train/loss': 1.4423679113388062, 'train/bleu': 34.934186979304044, 'validation/accuracy': 0.6895264983177185, 'validation/loss': 1.4075807332992554, 'validation/bleu': 30.563442889154814, 'validation/num_examples': 3000, 'test/accuracy': 0.7027714848518372, 'test/loss': 1.310380458831787, 'test/bleu': 30.446460403167094, 'test/num_examples': 3003}
I0314 09:57:01.773835 140174429099776 logging_writer.py:48] [75866] global_step=75866, preemption_count=0, score=28480.846055, test/accuracy=0.702771, test/bleu=30.446460, test/loss=1.310380, test/num_examples=3003, total_duration=48085.482558, train/accuracy=0.683647, train/bleu=34.934187, train/loss=1.442368, validation/accuracy=0.689526, validation/bleu=30.563443, validation/loss=1.407581, validation/num_examples=3000
I0314 09:57:03.046115 140362249443136 checkpoints.py:356] Saving checkpoint at step: 75866
I0314 09:57:07.584112 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_75866
I0314 09:57:07.613353 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_75866.
I0314 09:57:20.771543 140174420707072 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.21056173741817474, loss=1.5288044214248657
I0314 09:57:58.286868 140174412314368 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.21060548722743988, loss=1.5414291620254517
I0314 09:58:35.889071 140174420707072 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.2161472737789154, loss=1.5819761753082275
I0314 09:59:13.531355 140174412314368 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21723665297031403, loss=1.619938611984253
I0314 09:59:51.224213 140174420707072 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.20994246006011963, loss=1.6571177244186401
I0314 10:00:28.917663 140174412314368 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2136342078447342, loss=1.5641452074050903
I0314 10:01:06.629771 140174420707072 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.21950030326843262, loss=1.6126056909561157
I0314 10:01:44.281359 140174412314368 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.20458455383777618, loss=1.5619268417358398
I0314 10:02:21.890025 140174420707072 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2077920287847519, loss=1.569632649421692
I0314 10:02:59.527439 140174412314368 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.22079870104789734, loss=1.5543118715286255
I0314 10:03:37.195588 140174420707072 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.21921555697917938, loss=1.6326546669006348
I0314 10:04:14.911273 140174412314368 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.19708028435707092, loss=1.5278058052062988
I0314 10:04:52.579206 140174420707072 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.22035469114780426, loss=1.5381340980529785
I0314 10:05:30.280363 140174412314368 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.26831331849098206, loss=1.6348700523376465
I0314 10:06:07.897248 140174420707072 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.20111630856990814, loss=1.51820969581604
I0314 10:06:45.562400 140174412314368 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.21256543695926666, loss=1.5421549081802368
I0314 10:07:23.235639 140174420707072 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.21500539779663086, loss=1.523569107055664
I0314 10:08:00.937106 140174412314368 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.2113458514213562, loss=1.5946247577667236
I0314 10:08:38.607871 140174420707072 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.21410737931728363, loss=1.492232084274292
I0314 10:09:16.338136 140174412314368 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.2225949615240097, loss=1.5281846523284912
I0314 10:09:54.026432 140174420707072 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.21626009047031403, loss=1.5993379354476929
I0314 10:10:31.759547 140174412314368 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.2119511216878891, loss=1.505939245223999
I0314 10:11:07.638388 140362249443136 spec.py:298] Evaluating on the training split.
I0314 10:11:10.626456 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 10:15:57.590747 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 10:16:00.234029 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 10:19:10.327517 140362249443136 spec.py:326] Evaluating on the test split.
I0314 10:19:13.015853 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 10:22:33.705753 140362249443136 submission_runner.py:362] Time since start: 49444.85s, 	Step: 78097, 	{'train/accuracy': 0.6807436347007751, 'train/loss': 1.457224726676941, 'train/bleu': 34.121504070574105, 'validation/accuracy': 0.6886337399482727, 'validation/loss': 1.4049874544143677, 'validation/bleu': 30.38149107629208, 'validation/num_examples': 3000, 'test/accuracy': 0.7060484886169434, 'test/loss': 1.3031991720199585, 'test/bleu': 30.48310500204027, 'test/num_examples': 3003}
I0314 10:22:33.718275 140174420707072 logging_writer.py:48] [78097] global_step=78097, preemption_count=0, score=29316.900856, test/accuracy=0.706048, test/bleu=30.483105, test/loss=1.303199, test/num_examples=3003, total_duration=49444.850864, train/accuracy=0.680744, train/bleu=34.121504, train/loss=1.457225, validation/accuracy=0.688634, validation/bleu=30.381491, validation/loss=1.404987, validation/num_examples=3000
I0314 10:22:34.743231 140362249443136 checkpoints.py:356] Saving checkpoint at step: 78097
I0314 10:22:39.236870 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_78097
I0314 10:22:39.303462 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_78097.
I0314 10:22:40.822044 140174412314368 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.20965521037578583, loss=1.5151934623718262
I0314 10:23:18.358944 140174403921664 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.20776136219501495, loss=1.560158133506775
I0314 10:23:55.965274 140174412314368 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.21679261326789856, loss=1.5006029605865479
I0314 10:24:33.584397 140174403921664 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2057739943265915, loss=1.5283926725387573
I0314 10:25:11.243173 140174412314368 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.22421935200691223, loss=1.543000340461731
I0314 10:25:48.948527 140174403921664 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.21213671565055847, loss=1.5550752878189087
I0314 10:26:26.588246 140174412314368 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.20888552069664001, loss=1.506644368171692
I0314 10:27:04.268917 140174403921664 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.20269793272018433, loss=1.4007397890090942
I0314 10:27:41.906711 140174412314368 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.22373425960540771, loss=1.5054420232772827
I0314 10:28:19.572655 140174403921664 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2024407833814621, loss=1.4911829233169556
I0314 10:28:57.227942 140174412314368 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.19704373180866241, loss=1.4743764400482178
I0314 10:29:34.855343 140174403921664 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.21750879287719727, loss=1.4967325925827026
I0314 10:30:12.502901 140174412314368 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20676568150520325, loss=1.5301750898361206
I0314 10:30:50.248374 140174403921664 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.21215713024139404, loss=1.5939210653305054
I0314 10:31:27.892982 140174412314368 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.21090085804462433, loss=1.536891222000122
I0314 10:32:05.573827 140174403921664 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2171400934457779, loss=1.5574166774749756
I0314 10:32:43.267807 140174412314368 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.22577250003814697, loss=1.5481832027435303
I0314 10:33:20.934922 140174403921664 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2133619487285614, loss=1.5777924060821533
I0314 10:33:58.634910 140174412314368 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2233930230140686, loss=1.5578148365020752
I0314 10:34:36.337842 140174403921664 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.21761775016784668, loss=1.5479516983032227
I0314 10:35:14.041433 140174412314368 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.20482641458511353, loss=1.4796028137207031
I0314 10:35:51.788950 140174403921664 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.2133987993001938, loss=1.531786322593689
I0314 10:36:29.427856 140174412314368 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.21104654669761658, loss=1.5955318212509155
I0314 10:36:39.308022 140362249443136 spec.py:298] Evaluating on the training split.
I0314 10:36:42.282237 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 10:40:52.675087 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 10:40:55.313040 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 10:43:39.051045 140362249443136 spec.py:326] Evaluating on the test split.
I0314 10:43:41.750015 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 10:46:12.539759 140362249443136 submission_runner.py:362] Time since start: 50976.52s, 	Step: 80328, 	{'train/accuracy': 0.680019736289978, 'train/loss': 1.4670724868774414, 'train/bleu': 34.457180700613506, 'validation/accuracy': 0.6901588439941406, 'validation/loss': 1.3966951370239258, 'validation/bleu': 30.515326431563537, 'validation/num_examples': 3000, 'test/accuracy': 0.706129789352417, 'test/loss': 1.298000454902649, 'test/bleu': 30.595205816177007, 'test/num_examples': 3003}
I0314 10:46:12.552417 140174403921664 logging_writer.py:48] [80328] global_step=80328, preemption_count=0, score=30153.474783, test/accuracy=0.706130, test/bleu=30.595206, test/loss=1.298000, test/num_examples=3003, total_duration=50976.520502, train/accuracy=0.680020, train/bleu=34.457181, train/loss=1.467072, validation/accuracy=0.690159, validation/bleu=30.515326, validation/loss=1.396695, validation/num_examples=3000
I0314 10:46:13.559511 140362249443136 checkpoints.py:356] Saving checkpoint at step: 80328
I0314 10:46:18.493779 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_80328
I0314 10:46:18.558073 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_80328.
I0314 10:46:45.954329 140174412314368 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.21652989089488983, loss=1.5259374380111694
I0314 10:47:23.493940 140174395528960 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.21358293294906616, loss=1.5521103143692017
I0314 10:48:01.160031 140174412314368 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.20448210835456848, loss=1.4974238872528076
I0314 10:48:38.769862 140174395528960 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20509380102157593, loss=1.4803392887115479
I0314 10:49:16.465312 140174412314368 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.21720059216022491, loss=1.51072096824646
I0314 10:49:54.212128 140174395528960 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2159305065870285, loss=1.5432074069976807
I0314 10:50:31.950084 140174412314368 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.21194668114185333, loss=1.5279754400253296
I0314 10:51:09.619318 140174395528960 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.2267780601978302, loss=1.5721241235733032
I0314 10:51:47.298008 140174412314368 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.22249996662139893, loss=1.5457799434661865
I0314 10:52:24.949915 140174395528960 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.20627783238887787, loss=1.5260010957717896
I0314 10:53:02.665587 140174412314368 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.22834034264087677, loss=1.5178312063217163
I0314 10:53:40.411803 140174395528960 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.21750058233737946, loss=1.4946668148040771
I0314 10:54:18.073378 140174412314368 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.22679367661476135, loss=1.4847261905670166
I0314 10:54:55.778069 140174395528960 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.22485202550888062, loss=1.5285253524780273
I0314 10:55:33.452069 140174412314368 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.21673515439033508, loss=1.5644311904907227
I0314 10:56:11.120972 140174395528960 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.21668167412281036, loss=1.5405364036560059
I0314 10:56:48.798607 140174412314368 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2064744234085083, loss=1.5206613540649414
I0314 10:57:26.465704 140174395528960 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.22109463810920715, loss=1.5223146677017212
I0314 10:58:04.143409 140174412314368 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.21529720723628998, loss=1.5461870431900024
I0314 10:58:41.801587 140174395528960 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.23153704404830933, loss=1.4890605211257935
I0314 10:59:19.450763 140174412314368 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.21605022251605988, loss=1.5270037651062012
I0314 10:59:57.147486 140174395528960 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.21193189918994904, loss=1.5745184421539307
I0314 11:00:18.755175 140362249443136 spec.py:298] Evaluating on the training split.
I0314 11:00:21.728183 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:04:21.945637 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 11:04:24.575933 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:07:38.603205 140362249443136 spec.py:326] Evaluating on the test split.
I0314 11:07:41.281809 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:10:50.496716 140362249443136 submission_runner.py:362] Time since start: 52395.97s, 	Step: 82559, 	{'train/accuracy': 0.6884565353393555, 'train/loss': 1.4128315448760986, 'train/bleu': 34.590137194395766, 'validation/accuracy': 0.6907044053077698, 'validation/loss': 1.391269564628601, 'validation/bleu': 30.48122446316172, 'validation/num_examples': 3000, 'test/accuracy': 0.7069897055625916, 'test/loss': 1.2902978658676147, 'test/bleu': 30.506023187292076, 'test/num_examples': 3003}
I0314 11:10:50.509532 140174412314368 logging_writer.py:48] [82559] global_step=82559, preemption_count=0, score=30989.849897, test/accuracy=0.706990, test/bleu=30.506023, test/loss=1.290298, test/num_examples=3003, total_duration=52395.967652, train/accuracy=0.688457, train/bleu=34.590137, train/loss=1.412832, validation/accuracy=0.690704, validation/bleu=30.481224, validation/loss=1.391270, validation/num_examples=3000
I0314 11:10:51.516761 140362249443136 checkpoints.py:356] Saving checkpoint at step: 82559
I0314 11:10:56.232411 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_82559
I0314 11:10:56.279240 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_82559.
I0314 11:11:12.075158 140174395528960 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.21552711725234985, loss=1.5792185068130493
I0314 11:11:49.598093 140174387136256 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2268967181444168, loss=1.5719269514083862
I0314 11:12:27.198519 140174395528960 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.21816745400428772, loss=1.531129002571106
I0314 11:13:04.864681 140174387136256 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.22399777173995972, loss=1.4911551475524902
I0314 11:13:42.583941 140174395528960 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.22624480724334717, loss=1.458597183227539
I0314 11:14:20.282404 140174387136256 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.20204363763332367, loss=1.5129657983779907
I0314 11:14:57.993848 140174395528960 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.23481369018554688, loss=1.5250364542007446
I0314 11:15:35.699737 140174387136256 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2140350490808487, loss=1.5335906744003296
I0314 11:16:13.402801 140174395528960 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.23014752566814423, loss=1.5191766023635864
I0314 11:16:51.134079 140174387136256 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.22812268137931824, loss=1.4582403898239136
I0314 11:17:28.858778 140174395528960 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.2119791954755783, loss=1.585405707359314
I0314 11:18:06.568415 140174387136256 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.23551785945892334, loss=1.5545589923858643
I0314 11:18:44.255527 140174395528960 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.21129803359508514, loss=1.4636589288711548
I0314 11:19:21.976242 140174387136256 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.20763862133026123, loss=1.494194507598877
I0314 11:19:59.681656 140174395528960 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21860316395759583, loss=1.552807331085205
I0314 11:20:37.354612 140174387136256 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.20556488633155823, loss=1.4863358736038208
I0314 11:21:15.036073 140174395528960 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.20450752973556519, loss=1.4459969997406006
I0314 11:21:52.764112 140174387136256 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.2204698771238327, loss=1.5362063646316528
I0314 11:22:30.492863 140174395528960 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.22051608562469482, loss=1.543404221534729
I0314 11:23:08.193993 140174387136256 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.2247319221496582, loss=1.5355983972549438
I0314 11:23:45.908992 140174395528960 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.21916444599628448, loss=1.4843029975891113
I0314 11:24:23.594233 140174387136256 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.22334598004817963, loss=1.55925714969635
I0314 11:24:56.498834 140362249443136 spec.py:298] Evaluating on the training split.
I0314 11:24:59.483996 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:29:03.438453 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 11:29:06.092227 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:32:08.184255 140362249443136 spec.py:326] Evaluating on the test split.
I0314 11:32:10.867069 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:35:21.265749 140362249443136 submission_runner.py:362] Time since start: 53873.71s, 	Step: 84789, 	{'train/accuracy': 0.685408353805542, 'train/loss': 1.425815463066101, 'train/bleu': 34.84863068854933, 'validation/accuracy': 0.6917831301689148, 'validation/loss': 1.3856123685836792, 'validation/bleu': 30.657043733021364, 'validation/num_examples': 3000, 'test/accuracy': 0.7086631059646606, 'test/loss': 1.2799674272537231, 'test/bleu': 30.805698902724128, 'test/num_examples': 3003}
I0314 11:35:21.279285 140174395528960 logging_writer.py:48] [84789] global_step=84789, preemption_count=0, score=31826.298433, test/accuracy=0.708663, test/bleu=30.805699, test/loss=1.279967, test/num_examples=3003, total_duration=53873.711287, train/accuracy=0.685408, train/bleu=34.848631, train/loss=1.425815, validation/accuracy=0.691783, validation/bleu=30.657044, validation/loss=1.385612, validation/num_examples=3000
I0314 11:35:22.290595 140362249443136 checkpoints.py:356] Saving checkpoint at step: 84789
I0314 11:35:26.378508 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_84789
I0314 11:35:26.424942 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_84789.
I0314 11:35:30.940315 140174387136256 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.23653733730316162, loss=1.513135552406311
I0314 11:36:08.513550 140174378743552 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.21062174439430237, loss=1.5095785856246948
I0314 11:36:46.085239 140174387136256 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.21484073996543884, loss=1.4884060621261597
I0314 11:37:23.697130 140174378743552 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.20635788142681122, loss=1.5279808044433594
I0314 11:38:01.375025 140174387136256 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2184382975101471, loss=1.5034724473953247
I0314 11:38:39.065769 140174378743552 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.21188578009605408, loss=1.502509593963623
I0314 11:39:16.780352 140174387136256 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.22223304212093353, loss=1.5159474611282349
I0314 11:39:54.469484 140174378743552 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.2348499447107315, loss=1.5899007320404053
I0314 11:40:32.102194 140174387136256 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.21074235439300537, loss=1.4150809049606323
I0314 11:41:09.795917 140174378743552 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.20854920148849487, loss=1.4487091302871704
I0314 11:41:47.495527 140174387136256 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.22762033343315125, loss=1.52574622631073
I0314 11:42:25.152135 140174378743552 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.21023274958133698, loss=1.3878321647644043
I0314 11:43:02.878856 140174387136256 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.21088454127311707, loss=1.4867783784866333
I0314 11:43:40.537175 140174378743552 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.22032764554023743, loss=1.4322336912155151
I0314 11:44:18.215437 140174387136256 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.22318555414676666, loss=1.4323290586471558
I0314 11:44:55.860790 140174378743552 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.22437125444412231, loss=1.540373682975769
I0314 11:45:33.479680 140174387136256 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21059979498386383, loss=1.504769206047058
I0314 11:46:11.144560 140174378743552 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.21917882561683655, loss=1.5206173658370972
I0314 11:46:48.795935 140174387136256 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.22098757326602936, loss=1.5082448720932007
I0314 11:47:26.464531 140174378743552 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.2178688943386078, loss=1.473180890083313
I0314 11:48:04.152690 140174387136256 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2237616330385208, loss=1.644402027130127
I0314 11:48:41.786475 140174378743552 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.22598765790462494, loss=1.4631038904190063
I0314 11:49:19.451168 140174387136256 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2180616855621338, loss=1.4739927053451538
I0314 11:49:26.708454 140362249443136 spec.py:298] Evaluating on the training split.
I0314 11:49:29.689007 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:53:36.375164 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 11:53:39.015766 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:56:15.840839 140362249443136 spec.py:326] Evaluating on the test split.
I0314 11:56:18.530816 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 11:59:17.593553 140362249443136 submission_runner.py:362] Time since start: 55343.92s, 	Step: 87021, 	{'train/accuracy': 0.6836018562316895, 'train/loss': 1.4459940195083618, 'train/bleu': 35.087354079933775, 'validation/accuracy': 0.6922294497489929, 'validation/loss': 1.3842662572860718, 'validation/bleu': 30.75781520407087, 'validation/num_examples': 3000, 'test/accuracy': 0.7090232968330383, 'test/loss': 1.278700590133667, 'test/bleu': 30.881470009883994, 'test/num_examples': 3003}
I0314 11:59:17.606231 140174378743552 logging_writer.py:48] [87021] global_step=87021, preemption_count=0, score=32662.752252, test/accuracy=0.709023, test/bleu=30.881470, test/loss=1.278701, test/num_examples=3003, total_duration=55343.920930, train/accuracy=0.683602, train/bleu=35.087354, train/loss=1.445994, validation/accuracy=0.692229, validation/bleu=30.757815, validation/loss=1.384266, validation/num_examples=3000
I0314 11:59:18.606231 140362249443136 checkpoints.py:356] Saving checkpoint at step: 87021
I0314 11:59:23.110058 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_87021
I0314 11:59:23.164823 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_87021.
I0314 11:59:53.203841 140174387136256 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.25598421692848206, loss=1.4230183362960815
I0314 12:00:30.717992 140174370350848 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2068532556295395, loss=1.4929510354995728
I0314 12:01:08.314239 140174387136256 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.22354310750961304, loss=1.4241688251495361
I0314 12:01:45.963711 140174370350848 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.21997785568237305, loss=1.4858508110046387
I0314 12:02:23.618097 140174387136256 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.21579064428806305, loss=1.4057927131652832
I0314 12:03:01.303685 140174370350848 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.21103280782699585, loss=1.3466788530349731
I0314 12:03:38.977210 140174387136256 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.22281281650066376, loss=1.4971867799758911
I0314 12:04:16.668799 140174370350848 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.23282837867736816, loss=1.4676915407180786
I0314 12:04:54.376881 140174387136256 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.21777577698230743, loss=1.441971778869629
I0314 12:05:32.109536 140174370350848 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.21311481297016144, loss=1.4721925258636475
I0314 12:06:09.744655 140174387136256 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.21820341050624847, loss=1.4946681261062622
I0314 12:06:47.420017 140174370350848 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21812623739242554, loss=1.4152095317840576
I0314 12:07:25.114134 140174387136256 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.22098548710346222, loss=1.5282169580459595
I0314 12:08:02.816064 140174370350848 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.21723656356334686, loss=1.4999597072601318
I0314 12:08:40.523180 140174387136256 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21932651102542877, loss=1.4005876779556274
I0314 12:09:18.185137 140174370350848 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.2253873199224472, loss=1.5112751722335815
I0314 12:09:55.874855 140174387136256 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21242506802082062, loss=1.4734692573547363
I0314 12:10:33.552819 140174370350848 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.21302646398544312, loss=1.5302783250808716
I0314 12:11:11.159310 140174387136256 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2281809002161026, loss=1.521953821182251
I0314 12:11:48.898442 140174370350848 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.2109798789024353, loss=1.4827708005905151
I0314 12:12:26.572533 140174387136256 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.2221240997314453, loss=1.518314242362976
I0314 12:13:04.265572 140174370350848 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2179899662733078, loss=1.5039554834365845
I0314 12:13:23.190163 140362249443136 spec.py:298] Evaluating on the training split.
I0314 12:13:26.166209 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 12:17:33.752504 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 12:17:36.386014 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 12:20:39.944769 140362249443136 spec.py:326] Evaluating on the test split.
I0314 12:20:42.624801 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 12:24:12.493613 140362249443136 submission_runner.py:362] Time since start: 56780.40s, 	Step: 89252, 	{'train/accuracy': 0.6896376609802246, 'train/loss': 1.4097293615341187, 'train/bleu': 35.52343475030811, 'validation/accuracy': 0.6925890445709229, 'validation/loss': 1.3816791772842407, 'validation/bleu': 30.657612762972896, 'validation/num_examples': 3000, 'test/accuracy': 0.7099297046661377, 'test/loss': 1.275104284286499, 'test/bleu': 30.841054561399737, 'test/num_examples': 3003}
I0314 12:24:12.507318 140174387136256 logging_writer.py:48] [89252] global_step=89252, preemption_count=0, score=33499.484035, test/accuracy=0.709930, test/bleu=30.841055, test/loss=1.275104, test/num_examples=3003, total_duration=56780.402623, train/accuracy=0.689638, train/bleu=35.523435, train/loss=1.409729, validation/accuracy=0.692589, validation/bleu=30.657613, validation/loss=1.381679, validation/num_examples=3000
I0314 12:24:13.506160 140362249443136 checkpoints.py:356] Saving checkpoint at step: 89252
I0314 12:24:18.017324 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_89252
I0314 12:24:18.061505 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_89252.
I0314 12:24:36.445871 140174370350848 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.222805455327034, loss=1.5029006004333496
I0314 12:25:13.988143 140174361958144 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.2147168219089508, loss=1.5064374208450317
I0314 12:25:51.584996 140174370350848 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.21464668214321136, loss=1.5082309246063232
I0314 12:26:29.249050 140174361958144 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.22125162184238434, loss=1.3759514093399048
I0314 12:27:06.927278 140174370350848 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.22016285359859467, loss=1.4445699453353882
I0314 12:27:44.595775 140174361958144 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.22593754529953003, loss=1.454430103302002
I0314 12:28:22.297579 140174370350848 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.22422434389591217, loss=1.4230676889419556
I0314 12:29:00.002500 140174361958144 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.21649177372455597, loss=1.5241447687149048
I0314 12:29:37.645206 140174370350848 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.22181083261966705, loss=1.5928609371185303
I0314 12:30:15.313414 140174361958144 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2227873057126999, loss=1.5252516269683838
I0314 12:30:53.001780 140174370350848 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.22373399138450623, loss=1.4402966499328613
I0314 12:31:30.706285 140174361958144 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.2270418256521225, loss=1.461943507194519
I0314 12:32:08.404448 140174370350848 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.21469205617904663, loss=1.5187724828720093
I0314 12:32:46.062945 140174361958144 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.22970087826251984, loss=1.4747530221939087
I0314 12:33:23.740272 140174370350848 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.22704365849494934, loss=1.4374239444732666
I0314 12:34:01.370570 140174361958144 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.23781771957874298, loss=1.5933423042297363
I0314 12:34:39.033048 140174370350848 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.21615010499954224, loss=1.4547293186187744
I0314 12:35:16.695480 140174361958144 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21244390308856964, loss=1.4593366384506226
I0314 12:35:54.360193 140174370350848 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.23519396781921387, loss=1.5170984268188477
I0314 12:36:32.034247 140174361958144 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.2136501669883728, loss=1.4578208923339844
I0314 12:37:09.689957 140174370350848 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.21368195116519928, loss=1.524170994758606
I0314 12:37:47.348004 140174361958144 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.23651081323623657, loss=1.6373838186264038
I0314 12:38:18.298920 140362249443136 spec.py:298] Evaluating on the training split.
I0314 12:38:21.278708 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 12:42:27.248865 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 12:42:29.879375 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 12:45:29.979332 140362249443136 spec.py:326] Evaluating on the test split.
I0314 12:45:32.664750 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 12:49:02.432858 140362249443136 submission_runner.py:362] Time since start: 58275.51s, 	Step: 91484, 	{'train/accuracy': 0.6925252079963684, 'train/loss': 1.3906869888305664, 'train/bleu': 35.35231408765702, 'validation/accuracy': 0.6932461857795715, 'validation/loss': 1.3762766122817993, 'validation/bleu': 30.612884425960974, 'validation/num_examples': 3000, 'test/accuracy': 0.710440993309021, 'test/loss': 1.2723934650421143, 'test/bleu': 30.98221843462107, 'test/num_examples': 3003}
I0314 12:49:02.446388 140174370350848 logging_writer.py:48] [91484] global_step=91484, preemption_count=0, score=34336.368538, test/accuracy=0.710441, test/bleu=30.982218, test/loss=1.272393, test/num_examples=3003, total_duration=58275.511391, train/accuracy=0.692525, train/bleu=35.352314, train/loss=1.390687, validation/accuracy=0.693246, validation/bleu=30.612884, validation/loss=1.376277, validation/num_examples=3000
I0314 12:49:03.459531 140362249443136 checkpoints.py:356] Saving checkpoint at step: 91484
I0314 12:49:07.888376 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_91484
I0314 12:49:07.928524 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_91484.
I0314 12:49:14.303295 140174361958144 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.21903227269649506, loss=1.5254393815994263
I0314 12:49:51.882941 140174353565440 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.22251920402050018, loss=1.546234369277954
I0314 12:50:29.470283 140174361958144 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.2157604992389679, loss=1.474157691001892
I0314 12:51:07.093083 140174353565440 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.21277178823947906, loss=1.4061558246612549
I0314 12:51:44.716264 140174361958144 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.21595491468906403, loss=1.4605252742767334
I0314 12:52:22.373092 140174353565440 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2326141893863678, loss=1.4768342971801758
I0314 12:53:00.028141 140174361958144 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.2415660321712494, loss=1.5143433809280396
I0314 12:53:37.712573 140174353565440 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.21583673357963562, loss=1.491786241531372
I0314 12:54:15.360575 140174361958144 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21647033095359802, loss=1.4420340061187744
I0314 12:54:53.033989 140174353565440 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.22383126616477966, loss=1.4739223718643188
I0314 12:55:30.690629 140174361958144 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22850975394248962, loss=1.4675549268722534
I0314 12:56:08.380747 140174353565440 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.2174069732427597, loss=1.5067598819732666
I0314 12:56:46.075400 140174361958144 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.22076523303985596, loss=1.4541338682174683
I0314 12:57:23.751781 140174353565440 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21592234075069427, loss=1.4722225666046143
I0314 12:58:01.431805 140174361958144 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.22110100090503693, loss=1.48581862449646
I0314 12:58:39.135739 140174353565440 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.2437192052602768, loss=1.432861089706421
I0314 12:59:16.769993 140174361958144 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.22850079834461212, loss=1.548792839050293
I0314 12:59:54.425312 140174353565440 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.2206680327653885, loss=1.4551966190338135
I0314 13:00:32.103757 140174361958144 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.2140122503042221, loss=1.493537425994873
I0314 13:01:09.763407 140174353565440 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21823614835739136, loss=1.4657546281814575
I0314 13:01:47.421931 140174361958144 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.21819092333316803, loss=1.51878023147583
I0314 13:02:25.102752 140174353565440 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.22170087695121765, loss=1.494175910949707
I0314 13:03:02.781206 140174361958144 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.21497297286987305, loss=1.469596028327942
I0314 13:03:08.142260 140362249443136 spec.py:298] Evaluating on the training split.
I0314 13:03:11.130917 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:07:29.362826 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 13:07:32.002430 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:10:48.603670 140362249443136 spec.py:326] Evaluating on the test split.
I0314 13:10:51.306025 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:14:16.867888 140362249443136 submission_runner.py:362] Time since start: 59765.35s, 	Step: 93716, 	{'train/accuracy': 0.689274787902832, 'train/loss': 1.4142224788665771, 'train/bleu': 35.30038883795664, 'validation/accuracy': 0.6940645575523376, 'validation/loss': 1.3738274574279785, 'validation/bleu': 30.71579415849708, 'validation/num_examples': 3000, 'test/accuracy': 0.710557222366333, 'test/loss': 1.2690550088882446, 'test/bleu': 31.07544673061049, 'test/num_examples': 3003}
I0314 13:14:16.881810 140174353565440 logging_writer.py:48] [93716] global_step=93716, preemption_count=0, score=35173.065176, test/accuracy=0.710557, test/bleu=31.075447, test/loss=1.269055, test/num_examples=3003, total_duration=59765.354714, train/accuracy=0.689275, train/bleu=35.300389, train/loss=1.414222, validation/accuracy=0.694065, validation/bleu=30.715794, validation/loss=1.373827, validation/num_examples=3000
I0314 13:14:17.883125 140362249443136 checkpoints.py:356] Saving checkpoint at step: 93716
I0314 13:14:22.153536 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_93716
I0314 13:14:22.199876 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_93716.
I0314 13:14:54.146900 140174361958144 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2099287509918213, loss=1.3815979957580566
I0314 13:15:31.767286 140174345172736 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.21947181224822998, loss=1.4566713571548462
I0314 13:16:09.416546 140174361958144 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21378135681152344, loss=1.4760172367095947
I0314 13:16:47.118468 140174345172736 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.22043050825595856, loss=1.4641987085342407
I0314 13:17:24.861279 140174361958144 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.21062523126602173, loss=1.4444897174835205
I0314 13:18:02.499352 140174345172736 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.21876835823059082, loss=1.5148991346359253
I0314 13:18:40.208142 140174361958144 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21928027272224426, loss=1.5144144296646118
I0314 13:19:17.892193 140174345172736 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.22613592445850372, loss=1.492863655090332
I0314 13:19:55.545905 140174361958144 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.2262142300605774, loss=1.5690748691558838
I0314 13:20:33.212610 140174345172736 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2194421887397766, loss=1.4252601861953735
I0314 13:21:10.861761 140174361958144 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.22509773075580597, loss=1.4550687074661255
I0314 13:21:48.559631 140174345172736 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21618112921714783, loss=1.4280681610107422
I0314 13:22:26.273871 140174361958144 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.2194450944662094, loss=1.3882322311401367
I0314 13:23:03.939800 140174345172736 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.23296098411083221, loss=1.5984026193618774
I0314 13:23:41.595878 140174361958144 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.2181224226951599, loss=1.4882993698120117
I0314 13:24:19.233150 140174345172736 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.217367023229599, loss=1.4857949018478394
I0314 13:24:56.910260 140174361958144 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.21879127621650696, loss=1.473171353340149
I0314 13:25:34.664267 140174345172736 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.21529346704483032, loss=1.389055848121643
I0314 13:26:12.319445 140174361958144 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22237829864025116, loss=1.4592512845993042
I0314 13:26:49.988201 140174345172736 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.22348862886428833, loss=1.5489059686660767
I0314 13:27:27.638280 140174361958144 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.22456523776054382, loss=1.5081136226654053
I0314 13:28:05.335721 140174345172736 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.22550305724143982, loss=1.4824681282043457
I0314 13:28:22.395183 140362249443136 spec.py:298] Evaluating on the training split.
I0314 13:28:25.373176 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:32:38.759833 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 13:32:41.400111 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:35:49.523862 140362249443136 spec.py:326] Evaluating on the test split.
I0314 13:35:52.217230 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:38:57.281777 140362249443136 submission_runner.py:362] Time since start: 61279.61s, 	Step: 95947, 	{'train/accuracy': 0.6895872354507446, 'train/loss': 1.4059841632843018, 'train/bleu': 35.13083048430764, 'validation/accuracy': 0.6939157247543335, 'validation/loss': 1.3741854429244995, 'validation/bleu': 30.88732485030228, 'validation/num_examples': 3000, 'test/accuracy': 0.7105920910835266, 'test/loss': 1.2683740854263306, 'test/bleu': 30.91373736133046, 'test/num_examples': 3003}
I0314 13:38:57.295966 140174361958144 logging_writer.py:48] [95947] global_step=95947, preemption_count=0, score=36009.744793, test/accuracy=0.710592, test/bleu=30.913737, test/loss=1.268374, test/num_examples=3003, total_duration=61279.607652, train/accuracy=0.689587, train/bleu=35.130830, train/loss=1.405984, validation/accuracy=0.693916, validation/bleu=30.887325, validation/loss=1.374185, validation/num_examples=3000
I0314 13:38:58.284980 140362249443136 checkpoints.py:356] Saving checkpoint at step: 95947
I0314 13:39:02.653111 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_95947
I0314 13:39:02.715224 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_95947.
I0314 13:39:22.992192 140174345172736 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.22038906812667847, loss=1.4476886987686157
I0314 13:40:00.578925 140174336780032 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.22147370874881744, loss=1.4884201288223267
I0314 13:40:38.242318 140174345172736 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.22415967285633087, loss=1.5250192880630493
I0314 13:41:15.898397 140174336780032 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.22896112501621246, loss=1.4956183433532715
I0314 13:41:53.561457 140174345172736 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.23210623860359192, loss=1.5230683088302612
I0314 13:42:31.211350 140174336780032 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22326722741127014, loss=1.518048644065857
I0314 13:43:08.882079 140174345172736 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.21160677075386047, loss=1.4352167844772339
I0314 13:43:46.564418 140174336780032 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.23014646768569946, loss=1.5068628787994385
I0314 13:44:24.218795 140174345172736 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.2160794585943222, loss=1.4933024644851685
I0314 13:45:01.914102 140174336780032 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.22010774910449982, loss=1.4818506240844727
I0314 13:45:39.620155 140174345172736 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.22246330976486206, loss=1.4805115461349487
I0314 13:46:17.285366 140174336780032 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.21914076805114746, loss=1.4507642984390259
I0314 13:46:54.954254 140174345172736 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22447611391544342, loss=1.5181705951690674
I0314 13:47:32.615991 140174336780032 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.20608320832252502, loss=1.3967527151107788
I0314 13:48:10.298762 140174345172736 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.20441077649593353, loss=1.4148494005203247
I0314 13:48:48.006599 140174336780032 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21524018049240112, loss=1.4729410409927368
I0314 13:49:25.611665 140174345172736 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21776263415813446, loss=1.4106591939926147
I0314 13:50:03.365516 140174336780032 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.207906574010849, loss=1.4775842428207397
I0314 13:50:41.054268 140174345172736 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.2149335891008377, loss=1.5072221755981445
I0314 13:51:18.705718 140174336780032 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21074029803276062, loss=1.4861602783203125
I0314 13:51:56.356043 140174345172736 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.31877410411834717, loss=1.4662467241287231
I0314 13:52:34.021068 140174336780032 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.22263845801353455, loss=1.469216227531433
I0314 13:53:02.722320 140362249443136 spec.py:298] Evaluating on the training split.
I0314 13:53:05.696823 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 13:57:05.420950 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 13:57:08.066932 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:00:06.263428 140362249443136 spec.py:326] Evaluating on the test split.
I0314 14:00:08.979688 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:03:15.563846 140362249443136 submission_runner.py:362] Time since start: 62759.93s, 	Step: 98178, 	{'train/accuracy': 0.6894282102584839, 'train/loss': 1.4078638553619385, 'train/bleu': 35.557609758214646, 'validation/accuracy': 0.6938413381576538, 'validation/loss': 1.3735398054122925, 'validation/bleu': 30.80878583402332, 'validation/num_examples': 3000, 'test/accuracy': 0.7108244895935059, 'test/loss': 1.267401933670044, 'test/bleu': 31.202344620051388, 'test/num_examples': 3003}
I0314 14:03:15.577755 140174345172736 logging_writer.py:48] [98178] global_step=98178, preemption_count=0, score=36846.324922, test/accuracy=0.710824, test/bleu=31.202345, test/loss=1.267402, test/num_examples=3003, total_duration=62759.934798, train/accuracy=0.689428, train/bleu=35.557610, train/loss=1.407864, validation/accuracy=0.693841, validation/bleu=30.808786, validation/loss=1.373540, validation/num_examples=3000
I0314 14:03:16.576413 140362249443136 checkpoints.py:356] Saving checkpoint at step: 98178
I0314 14:03:21.454449 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_98178
I0314 14:03:21.515676 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_98178.
I0314 14:03:30.169738 140174336780032 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21531453728675842, loss=1.4791699647903442
I0314 14:04:07.746831 140174328387328 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.22422167658805847, loss=1.5548877716064453
I0314 14:04:45.352023 140174336780032 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21979756653308868, loss=1.4057886600494385
I0314 14:05:22.996128 140174328387328 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21653150022029877, loss=1.4671099185943604
I0314 14:06:00.705925 140174336780032 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.22572217881679535, loss=1.500697135925293
I0314 14:06:38.406398 140174328387328 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.21861965954303741, loss=1.5019111633300781
I0314 14:07:16.072520 140174336780032 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.2035091370344162, loss=1.3797699213027954
I0314 14:07:53.741547 140174328387328 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.21074749529361725, loss=1.4765565395355225
I0314 14:08:31.433389 140174336780032 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2129361480474472, loss=1.4548860788345337
I0314 14:09:09.119856 140174328387328 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.20804868638515472, loss=1.3926609754562378
I0314 14:09:46.802478 140174336780032 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.2076965570449829, loss=1.4119244813919067
I0314 14:10:24.473522 140174328387328 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21460489928722382, loss=1.463467001914978
I0314 14:11:02.151495 140174336780032 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.21871024370193481, loss=1.5232443809509277
I0314 14:11:39.806069 140174328387328 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.21383491158485413, loss=1.4457201957702637
I0314 14:12:17.530329 140174336780032 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.22189773619174957, loss=1.5624393224716187
I0314 14:12:55.256680 140174328387328 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.216303750872612, loss=1.4425206184387207
I0314 14:13:32.985116 140174336780032 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2220495492219925, loss=1.4653244018554688
I0314 14:14:10.671503 140174328387328 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.22782807052135468, loss=1.4240232706069946
I0314 14:14:48.389806 140174336780032 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.23234553635120392, loss=1.4711836576461792
I0314 14:15:26.065281 140174328387328 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.21511241793632507, loss=1.4531831741333008
I0314 14:16:03.750440 140174336780032 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.23118671774864197, loss=1.5505181550979614
I0314 14:16:41.429114 140174328387328 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.20720882713794708, loss=1.4748138189315796
I0314 14:17:19.098886 140174336780032 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.21020397543907166, loss=1.445509910583496
I0314 14:17:21.832416 140362249443136 spec.py:298] Evaluating on the training split.
I0314 14:17:24.833240 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:21:26.745026 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 14:21:29.376305 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:24:22.482898 140362249443136 spec.py:326] Evaluating on the test split.
I0314 14:24:25.173429 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:27:23.555166 140362249443136 submission_runner.py:362] Time since start: 64219.04s, 	Step: 100409, 	{'train/accuracy': 0.6887820363044739, 'train/loss': 1.412752389907837, 'train/bleu': 35.41154375646261, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 14:27:23.570357 140174328387328 logging_writer.py:48] [100409] global_step=100409, preemption_count=0, score=37682.971592, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=64219.044890, train/accuracy=0.688782, train/bleu=35.411544, train/loss=1.412752, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 14:27:24.562397 140362249443136 checkpoints.py:356] Saving checkpoint at step: 100409
I0314 14:27:29.208339 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_100409
I0314 14:27:29.262886 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_100409.
I0314 14:28:03.763461 140174336780032 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.22032807767391205, loss=1.5295381546020508
I0314 14:28:41.344125 140174319994624 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21704141795635223, loss=1.4680306911468506
I0314 14:29:18.974420 140174336780032 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.2106335163116455, loss=1.3787122964859009
I0314 14:29:56.615279 140174319994624 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.211588516831398, loss=1.453863263130188
I0314 14:30:34.272569 140174336780032 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.21549634635448456, loss=1.5217684507369995
I0314 14:31:11.961875 140174319994624 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22207435965538025, loss=1.5412884950637817
I0314 14:31:49.666575 140174336780032 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.21668899059295654, loss=1.425337314605713
I0314 14:32:27.333095 140174319994624 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.21494553983211517, loss=1.5179471969604492
I0314 14:33:04.996278 140174336780032 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.21982017159461975, loss=1.4510703086853027
I0314 14:33:42.694203 140174319994624 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.2142261266708374, loss=1.438876748085022
I0314 14:34:20.341369 140174336780032 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.21481510996818542, loss=1.4312260150909424
I0314 14:34:57.980517 140174319994624 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.2107905149459839, loss=1.4637489318847656
I0314 14:35:35.693010 140174336780032 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.21345673501491547, loss=1.4897583723068237
I0314 14:36:13.369712 140174319994624 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.20656540989875793, loss=1.432458758354187
I0314 14:36:51.027993 140174336780032 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22461558878421783, loss=1.5180091857910156
I0314 14:37:28.712195 140174319994624 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.2087962031364441, loss=1.5150818824768066
I0314 14:38:06.398037 140174336780032 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.22398340702056885, loss=1.5044397115707397
I0314 14:38:44.047954 140174319994624 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.21736420691013336, loss=1.493606686592102
I0314 14:39:21.754601 140174336780032 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.2268444150686264, loss=1.4419673681259155
I0314 14:39:59.470887 140174319994624 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.20684029161930084, loss=1.4221842288970947
I0314 14:40:37.178760 140174336780032 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.225121408700943, loss=1.3860691785812378
I0314 14:41:14.808865 140174319994624 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.22286798059940338, loss=1.4289181232452393
I0314 14:41:29.592995 140362249443136 spec.py:298] Evaluating on the training split.
I0314 14:41:32.585620 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:45:33.653242 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 14:45:36.289858 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:48:29.376639 140362249443136 spec.py:326] Evaluating on the test split.
I0314 14:48:32.076875 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 14:51:30.416772 140362249443136 submission_runner.py:362] Time since start: 65666.81s, 	Step: 102641, 	{'train/accuracy': 0.6892935037612915, 'train/loss': 1.4130560159683228, 'train/bleu': 35.62646510213669, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 14:51:30.431717 140174336780032 logging_writer.py:48] [102641] global_step=102641, preemption_count=0, score=38519.980839, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=65666.805465, train/accuracy=0.689294, train/bleu=35.626465, train/loss=1.413056, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 14:51:31.434863 140362249443136 checkpoints.py:356] Saving checkpoint at step: 102641
I0314 14:51:35.884595 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_102641
I0314 14:51:35.940560 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_102641.
I0314 14:51:58.429382 140174319994624 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23805095255374908, loss=1.4178975820541382
I0314 14:52:36.003362 140174311601920 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.2148771435022354, loss=1.4718222618103027
I0314 14:53:13.648005 140174319994624 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.21050967276096344, loss=1.4566420316696167
I0314 14:53:51.259931 140174311601920 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.21838895976543427, loss=1.3945292234420776
I0314 14:54:28.964538 140174319994624 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.20595772564411163, loss=1.4250085353851318
I0314 14:55:06.614472 140174311601920 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.22052226960659027, loss=1.4439619779586792
I0314 14:55:44.325561 140174319994624 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22111208736896515, loss=1.4945883750915527
I0314 14:56:21.998487 140174311601920 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.23040097951889038, loss=1.4514610767364502
I0314 14:56:59.690535 140174319994624 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.2250920683145523, loss=1.475079894065857
I0314 14:57:37.362779 140174311601920 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21565723419189453, loss=1.469709873199463
I0314 14:58:15.011623 140174319994624 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.22755908966064453, loss=1.4804052114486694
I0314 14:58:52.708652 140174311601920 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22610050439834595, loss=1.4628359079360962
I0314 14:59:30.372496 140174319994624 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.2325865477323532, loss=1.5226067304611206
I0314 15:00:07.972081 140174311601920 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.21112160384655, loss=1.4157886505126953
I0314 15:00:45.650755 140174319994624 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.2166881561279297, loss=1.4521244764328003
I0314 15:01:23.318270 140174311601920 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.2157227247953415, loss=1.446456789970398
I0314 15:02:01.002003 140174319994624 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.2110908180475235, loss=1.4808050394058228
I0314 15:02:38.697493 140174311601920 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.20252478122711182, loss=1.4093153476715088
I0314 15:03:16.375479 140174319994624 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.23022986948490143, loss=1.426862359046936
I0314 15:03:54.058726 140174311601920 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.21808959543704987, loss=1.4945656061172485
I0314 15:04:31.743499 140174319994624 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.2226538509130478, loss=1.5084723234176636
I0314 15:05:09.460692 140174311601920 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.2099122256040573, loss=1.4181658029556274
I0314 15:05:36.268875 140362249443136 spec.py:298] Evaluating on the training split.
I0314 15:05:39.248585 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:09:41.619705 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 15:09:44.247731 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:12:37.366014 140362249443136 spec.py:326] Evaluating on the test split.
I0314 15:12:40.061425 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:15:38.172203 140362249443136 submission_runner.py:362] Time since start: 67113.48s, 	Step: 104873, 	{'train/accuracy': 0.691219687461853, 'train/loss': 1.3957493305206299, 'train/bleu': 35.39732856373887, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 15:15:38.189436 140174319994624 logging_writer.py:48] [104873] global_step=104873, preemption_count=0, score=39356.813399, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=67113.481351, train/accuracy=0.691220, train/bleu=35.397329, train/loss=1.395749, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 15:15:39.189906 140362249443136 checkpoints.py:356] Saving checkpoint at step: 104873
I0314 15:15:43.724009 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_104873
I0314 15:15:43.779411 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_104873.
I0314 15:15:54.272026 140174311601920 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.23069187998771667, loss=1.532450795173645
I0314 15:16:31.812791 140174303209216 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22202447056770325, loss=1.4956493377685547
I0314 15:17:09.407125 140174311601920 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.2078952044248581, loss=1.4777964353561401
I0314 15:17:47.075291 140174303209216 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.21760360896587372, loss=1.4396910667419434
I0314 15:18:24.757597 140174311601920 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.20708034932613373, loss=1.4305928945541382
I0314 15:19:02.420808 140174303209216 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.21118929982185364, loss=1.414916753768921
I0314 15:19:40.134238 140174311601920 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.22672006487846375, loss=1.495368242263794
I0314 15:20:17.781986 140174303209216 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.22858555614948273, loss=1.4561606645584106
I0314 15:20:55.544944 140174311601920 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.2220732867717743, loss=1.4835166931152344
I0314 15:21:33.196910 140174303209216 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.21970142424106598, loss=1.435854434967041
I0314 15:22:10.838073 140174311601920 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.21756836771965027, loss=1.4921425580978394
I0314 15:22:48.552675 140174303209216 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.22019743919372559, loss=1.4599673748016357
I0314 15:23:26.290733 140174311601920 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.23607510328292847, loss=1.385827660560608
I0314 15:24:04.013465 140174303209216 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.2183849811553955, loss=1.5016952753067017
I0314 15:24:41.773317 140174311601920 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.21665765345096588, loss=1.51518976688385
I0314 15:25:19.464088 140174303209216 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.21392209827899933, loss=1.4602316617965698
I0314 15:25:57.139394 140174311601920 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.22763386368751526, loss=1.487358570098877
I0314 15:26:34.798864 140174303209216 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.21553945541381836, loss=1.4908822774887085
I0314 15:27:12.477669 140174311601920 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.21642538905143738, loss=1.4312636852264404
I0314 15:27:50.176991 140174303209216 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.2181469053030014, loss=1.4226094484329224
I0314 15:28:27.830127 140174311601920 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.20814034342765808, loss=1.4752459526062012
I0314 15:29:05.511035 140174303209216 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.2166261225938797, loss=1.4446791410446167
I0314 15:29:43.153601 140174311601920 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.23323826491832733, loss=1.5131407976150513
I0314 15:29:44.003152 140362249443136 spec.py:298] Evaluating on the training split.
I0314 15:29:46.991153 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:33:39.302046 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 15:33:41.933904 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:36:35.007745 140362249443136 spec.py:326] Evaluating on the test split.
I0314 15:36:37.713032 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:39:36.022666 140362249443136 submission_runner.py:362] Time since start: 68561.22s, 	Step: 107104, 	{'train/accuracy': 0.6907973289489746, 'train/loss': 1.3969403505325317, 'train/bleu': 35.39891174047552, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 15:39:36.037292 140174303209216 logging_writer.py:48] [107104] global_step=107104, preemption_count=0, score=40193.716478, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=68561.215621, train/accuracy=0.690797, train/bleu=35.398912, train/loss=1.396940, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 15:39:37.041172 140362249443136 checkpoints.py:356] Saving checkpoint at step: 107104
I0314 15:39:41.557961 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_107104
I0314 15:39:41.618499 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_107104.
I0314 15:40:18.005038 140174311601920 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.21660281717777252, loss=1.454507827758789
I0314 15:40:55.576221 140174294816512 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.21630673110485077, loss=1.500596046447754
I0314 15:41:33.226812 140174311601920 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.20872105658054352, loss=1.429260492324829
I0314 15:42:10.892155 140174294816512 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.21576601266860962, loss=1.4109469652175903
I0314 15:42:48.540735 140174311601920 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.2191336750984192, loss=1.409500241279602
I0314 15:43:26.215193 140174294816512 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.21525134146213531, loss=1.5001671314239502
I0314 15:44:03.872838 140174311601920 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.20726323127746582, loss=1.458013653755188
I0314 15:44:41.528302 140174294816512 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.21591562032699585, loss=1.4926868677139282
I0314 15:45:19.200026 140174311601920 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.2216544896364212, loss=1.4419888257980347
I0314 15:45:56.886456 140174294816512 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.21060922741889954, loss=1.4882692098617554
I0314 15:46:34.589158 140174311601920 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.22501325607299805, loss=1.45846426486969
I0314 15:47:12.344168 140174294816512 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.22695375978946686, loss=1.5456756353378296
I0314 15:47:50.004831 140174311601920 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.22062739729881287, loss=1.4579097032546997
I0314 15:48:27.725476 140174294816512 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.21779483556747437, loss=1.4843559265136719
I0314 15:49:05.461525 140174311601920 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.2136242687702179, loss=1.480954647064209
I0314 15:49:43.131657 140174294816512 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.22921325266361237, loss=1.4962280988693237
I0314 15:50:20.824448 140174311601920 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.23605120182037354, loss=1.5182749032974243
I0314 15:50:58.481268 140174294816512 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.22365455329418182, loss=1.4869710206985474
I0314 15:51:36.174867 140174311601920 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.21814435720443726, loss=1.5080500841140747
I0314 15:52:13.880398 140174294816512 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.21568970382213593, loss=1.4550538063049316
I0314 15:52:51.607543 140174311601920 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.21861355006694794, loss=1.5383206605911255
I0314 15:53:29.248516 140174294816512 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.22116981446743011, loss=1.4058927297592163
I0314 15:53:41.773518 140362249443136 spec.py:298] Evaluating on the training split.
I0314 15:53:44.759277 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 15:57:31.703046 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 15:57:34.337943 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:00:27.430615 140362249443136 spec.py:326] Evaluating on the test split.
I0314 16:00:30.123278 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:03:28.368486 140362249443136 submission_runner.py:362] Time since start: 69998.99s, 	Step: 109335, 	{'train/accuracy': 0.6931254863739014, 'train/loss': 1.388649344444275, 'train/bleu': 35.28562686450435, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 16:03:28.383234 140174311601920 logging_writer.py:48] [109335] global_step=109335, preemption_count=0, score=41030.518193, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=69998.985992, train/accuracy=0.693125, train/bleu=35.285627, train/loss=1.388649, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 16:03:29.388369 140362249443136 checkpoints.py:356] Saving checkpoint at step: 109335
I0314 16:03:34.118896 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_109335
I0314 16:03:34.167907 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_109335.
I0314 16:03:58.953744 140174294816512 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.22407786548137665, loss=1.5492048263549805
I0314 16:04:36.524706 140174286423808 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22032713890075684, loss=1.4087492227554321
I0314 16:05:14.108908 140174294816512 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.20697186887264252, loss=1.418407678604126
I0314 16:05:51.812825 140174286423808 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.22248423099517822, loss=1.4083393812179565
I0314 16:06:29.517909 140174294816512 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.21271447837352753, loss=1.370774507522583
I0314 16:07:07.208497 140174286423808 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.21620909869670868, loss=1.509295105934143
I0314 16:07:44.922782 140174294816512 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.2299720048904419, loss=1.5255258083343506
I0314 16:08:22.652085 140174286423808 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.2094220370054245, loss=1.4817928075790405
I0314 16:09:00.387501 140174294816512 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.2157829999923706, loss=1.4545942544937134
I0314 16:09:38.092778 140174286423808 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.21723879873752594, loss=1.4559071063995361
I0314 16:10:15.803506 140174294816512 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.20849119126796722, loss=1.402334451675415
I0314 16:10:53.531884 140174286423808 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.2144874632358551, loss=1.4952291250228882
I0314 16:11:31.269737 140174294816512 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.21332642436027527, loss=1.4841638803482056
I0314 16:12:08.981704 140174286423808 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.2109152227640152, loss=1.4610815048217773
I0314 16:12:46.672527 140174294816512 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.21728426218032837, loss=1.4590728282928467
I0314 16:13:24.356719 140174286423808 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.21448390185832977, loss=1.434264898300171
I0314 16:14:02.049873 140174294816512 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.21787497401237488, loss=1.397987723350525
I0314 16:14:39.765790 140174286423808 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.22960714995861053, loss=1.5311306715011597
I0314 16:15:17.444491 140174294816512 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.21783447265625, loss=1.3836863040924072
I0314 16:15:55.110954 140174286423808 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.2177411913871765, loss=1.5539158582687378
I0314 16:16:32.790237 140174294816512 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.22407247126102448, loss=1.4615581035614014
I0314 16:17:10.496768 140174286423808 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.22564706206321716, loss=1.4992262125015259
I0314 16:17:34.334038 140362249443136 spec.py:298] Evaluating on the training split.
I0314 16:17:37.310311 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:21:27.774584 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 16:21:30.425465 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:24:23.884374 140362249443136 spec.py:326] Evaluating on the test split.
I0314 16:24:26.583620 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:27:25.194045 140362249443136 submission_runner.py:362] Time since start: 71431.55s, 	Step: 111565, 	{'train/accuracy': 0.691834568977356, 'train/loss': 1.3927584886550903, 'train/bleu': 35.26646620182535, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 16:27:25.212304 140174294816512 logging_writer.py:48] [111565] global_step=111565, preemption_count=0, score=41867.385419, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=71431.546515, train/accuracy=0.691835, train/bleu=35.266466, train/loss=1.392758, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 16:27:26.518427 140362249443136 checkpoints.py:356] Saving checkpoint at step: 111565
I0314 16:27:31.823682 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_111565
I0314 16:27:31.862546 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_111565.
I0314 16:27:45.369544 140174286423808 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.21282470226287842, loss=1.4162216186523438
I0314 16:28:22.889322 140174278031104 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.2057572454214096, loss=1.454685091972351
I0314 16:29:00.534030 140174286423808 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.21965448558330536, loss=1.3958696126937866
I0314 16:29:38.155142 140174278031104 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.21457859873771667, loss=1.5283647775650024
I0314 16:30:15.844672 140174286423808 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.21620532870292664, loss=1.444648027420044
I0314 16:30:53.562913 140174278031104 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.22567486763000488, loss=1.4612751007080078
I0314 16:31:31.236030 140174286423808 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.22047267854213715, loss=1.4436999559402466
I0314 16:32:08.877387 140174278031104 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.21411657333374023, loss=1.4208450317382812
I0314 16:32:46.622282 140174286423808 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.22223737835884094, loss=1.4165797233581543
I0314 16:33:24.305460 140174278031104 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.2209586650133133, loss=1.4601515531539917
I0314 16:34:01.988958 140174286423808 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.22252359986305237, loss=1.5135945081710815
I0314 16:34:39.703371 140174278031104 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.21420925855636597, loss=1.5036156177520752
I0314 16:35:17.387704 140174286423808 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.21800273656845093, loss=1.484255075454712
I0314 16:35:55.042723 140174278031104 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.21285639703273773, loss=1.436916708946228
I0314 16:36:32.691185 140174286423808 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.21117430925369263, loss=1.461921215057373
I0314 16:37:10.373313 140174278031104 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.21202467381954193, loss=1.4346923828125
I0314 16:37:48.054274 140174286423808 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.22404150664806366, loss=1.515516996383667
I0314 16:38:25.690333 140174278031104 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.2724056541919708, loss=1.5319609642028809
I0314 16:39:03.340536 140174286423808 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.21468131244182587, loss=1.462814450263977
I0314 16:39:41.079993 140174278031104 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.22727704048156738, loss=1.4086018800735474
I0314 16:40:18.746044 140174286423808 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.23205727338790894, loss=1.4633187055587769
I0314 16:40:56.468136 140174278031104 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.22708925604820251, loss=1.5411804914474487
I0314 16:41:31.976483 140362249443136 spec.py:298] Evaluating on the training split.
I0314 16:41:34.958000 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:45:34.725943 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 16:45:37.362192 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:48:30.498083 140362249443136 spec.py:326] Evaluating on the test split.
I0314 16:48:33.178539 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 16:51:31.529356 140362249443136 submission_runner.py:362] Time since start: 72869.19s, 	Step: 113796, 	{'train/accuracy': 0.6906685829162598, 'train/loss': 1.4012737274169922, 'train/bleu': 35.455053689878476, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 16:51:31.544680 140174286423808 logging_writer.py:48] [113796] global_step=113796, preemption_count=0, score=42703.880705, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=72869.188956, train/accuracy=0.690669, train/bleu=35.455054, train/loss=1.401274, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 16:51:32.559634 140362249443136 checkpoints.py:356] Saving checkpoint at step: 113796
I0314 16:51:37.213695 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_113796
I0314 16:51:37.249693 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_113796.
I0314 16:51:39.139274 140174278031104 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.21643443405628204, loss=1.4719178676605225
I0314 16:52:16.657655 140174269638400 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.22483018040657043, loss=1.5425710678100586
I0314 16:52:54.269334 140174278031104 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.21885190904140472, loss=1.4652235507965088
I0314 16:53:31.884085 140174269638400 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.2216249406337738, loss=1.4823611974716187
I0314 16:54:09.592719 140174278031104 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.22391995787620544, loss=1.4539239406585693
I0314 16:54:47.265166 140174269638400 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.22351782023906708, loss=1.4444622993469238
I0314 16:55:24.997124 140174278031104 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.21084249019622803, loss=1.4900497198104858
I0314 16:56:02.709627 140174269638400 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.21848978102207184, loss=1.5223482847213745
I0314 16:56:40.393781 140174278031104 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.2263580709695816, loss=1.4497593641281128
I0314 16:57:18.078150 140174269638400 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.21520254015922546, loss=1.4902573823928833
I0314 16:57:55.702883 140174278031104 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.22565385699272156, loss=1.497211217880249
I0314 16:58:33.467148 140174269638400 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.21517445147037506, loss=1.5039137601852417
I0314 16:59:11.172994 140174278031104 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.21641938388347626, loss=1.4451684951782227
I0314 16:59:48.840784 140174269638400 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.21993207931518555, loss=1.3520491123199463
I0314 17:00:26.494072 140174278031104 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.21251161396503448, loss=1.4789305925369263
I0314 17:01:04.169213 140174269638400 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.20491769909858704, loss=1.4412813186645508
I0314 17:01:41.910535 140174278031104 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.22605100274085999, loss=1.5058914422988892
I0314 17:02:19.569631 140174269638400 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.22543923556804657, loss=1.4921623468399048
I0314 17:02:57.261054 140174278031104 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.22051909565925598, loss=1.4298903942108154
I0314 17:03:34.916611 140174269638400 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.21512660384178162, loss=1.4244341850280762
I0314 17:04:12.586471 140174278031104 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.21531327068805695, loss=1.4719709157943726
I0314 17:04:50.271250 140174269638400 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.21274308860301971, loss=1.4519379138946533
I0314 17:05:27.949016 140174278031104 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.21831542253494263, loss=1.4978283643722534
I0314 17:05:37.477266 140362249443136 spec.py:298] Evaluating on the training split.
I0314 17:05:40.457449 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:09:18.510106 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 17:09:21.153797 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:12:14.293859 140362249443136 spec.py:326] Evaluating on the test split.
I0314 17:12:16.996205 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:15:15.263003 140362249443136 submission_runner.py:362] Time since start: 74314.69s, 	Step: 116027, 	{'train/accuracy': 0.690453052520752, 'train/loss': 1.4024473428726196, 'train/bleu': 35.16958077809361, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 17:15:15.278913 140174269638400 logging_writer.py:48] [116027] global_step=116027, preemption_count=0, score=43540.707411, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=74314.689742, train/accuracy=0.690453, train/bleu=35.169581, train/loss=1.402447, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 17:15:16.278095 140362249443136 checkpoints.py:356] Saving checkpoint at step: 116027
I0314 17:15:20.451964 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_116027
I0314 17:15:20.487069 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_116027.
I0314 17:15:48.249622 140174278031104 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.20697346329689026, loss=1.4494671821594238
I0314 17:16:25.859028 140174261245696 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.24793903529644012, loss=1.535995602607727
I0314 17:17:03.496197 140174278031104 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.22949571907520294, loss=1.4060161113739014
I0314 17:17:41.222589 140174261245696 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.2185164988040924, loss=1.4799355268478394
I0314 17:18:18.915259 140174278031104 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.21027514338493347, loss=1.419037938117981
I0314 17:18:56.584515 140174261245696 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.22162015736103058, loss=1.4355061054229736
I0314 17:19:34.245729 140174278031104 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.22374960780143738, loss=1.5023623704910278
I0314 17:20:11.952645 140174261245696 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.21758848428726196, loss=1.4855246543884277
I0314 17:20:49.669922 140174278031104 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.21846331655979156, loss=1.5067476034164429
I0314 17:21:27.344314 140174261245696 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.21633334457874298, loss=1.4251925945281982
I0314 17:22:05.000566 140174278031104 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.2089899778366089, loss=1.4179890155792236
I0314 17:22:42.648262 140174261245696 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.21032942831516266, loss=1.5414822101593018
I0314 17:23:20.329233 140174278031104 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.2192983776330948, loss=1.4533041715621948
I0314 17:23:57.997502 140174261245696 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.22737058997154236, loss=1.4814823865890503
I0314 17:24:35.666922 140174278031104 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.22274699807167053, loss=1.4582581520080566
I0314 17:25:13.357643 140174261245696 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.22661800682544708, loss=1.5522829294204712
I0314 17:25:51.032077 140174278031104 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.20879711210727692, loss=1.429107666015625
I0314 17:26:28.663409 140174261245696 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.20966170728206635, loss=1.390378713607788
I0314 17:27:06.346257 140174278031104 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.2199040651321411, loss=1.4953985214233398
I0314 17:27:44.040332 140174261245696 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.21872353553771973, loss=1.3882116079330444
I0314 17:28:21.764174 140174278031104 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.21686087548732758, loss=1.4463318586349487
I0314 17:28:59.426544 140174261245696 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.21471181511878967, loss=1.505638837814331
I0314 17:29:20.608654 140362249443136 spec.py:298] Evaluating on the training split.
I0314 17:29:23.581020 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:33:24.762964 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 17:33:27.404399 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:36:20.372930 140362249443136 spec.py:326] Evaluating on the test split.
I0314 17:36:23.086261 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:39:21.344530 140362249443136 submission_runner.py:362] Time since start: 75737.82s, 	Step: 118258, 	{'train/accuracy': 0.6903471350669861, 'train/loss': 1.4064098596572876, 'train/bleu': 35.53923727365814, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 17:39:21.360594 140174278031104 logging_writer.py:48] [118258] global_step=118258, preemption_count=0, score=44377.444216, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=75737.821126, train/accuracy=0.690347, train/bleu=35.539237, train/loss=1.406410, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 17:39:22.371233 140362249443136 checkpoints.py:356] Saving checkpoint at step: 118258
I0314 17:39:26.711689 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_118258
I0314 17:39:26.767394 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_118258.
I0314 17:39:42.866954 140174261245696 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.22716836631298065, loss=1.4439605474472046
I0314 17:40:20.458374 140174252852992 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.21355696022510529, loss=1.5233405828475952
I0314 17:40:58.104089 140174261245696 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.2277006059885025, loss=1.4874663352966309
I0314 17:41:35.733077 140174252852992 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.21767637133598328, loss=1.5454198122024536
I0314 17:42:13.410018 140174261245696 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.22336028516292572, loss=1.5308935642242432
I0314 17:42:51.080149 140174252852992 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.2068900167942047, loss=1.4257744550704956
I0314 17:43:28.784676 140174261245696 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.22974111139774323, loss=1.5264426469802856
I0314 17:44:06.441257 140174252852992 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.22096025943756104, loss=1.46538245677948
I0314 17:44:44.136624 140174261245696 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.2193058282136917, loss=1.4156066179275513
I0314 17:45:21.750973 140174252852992 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.2180265486240387, loss=1.4934228658676147
I0314 17:45:59.414927 140174261245696 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.22962385416030884, loss=1.452189326286316
I0314 17:46:37.095546 140174252852992 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.22603648900985718, loss=1.4839656352996826
I0314 17:47:14.750076 140174261245696 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23831555247306824, loss=1.4623411893844604
I0314 17:47:52.472587 140174252852992 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.22149483859539032, loss=1.4314476251602173
I0314 17:48:30.132933 140174261245696 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.22498004138469696, loss=1.4774407148361206
I0314 17:49:07.805978 140174252852992 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.217633455991745, loss=1.4686198234558105
I0314 17:49:45.539987 140174261245696 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.2163447141647339, loss=1.4000780582427979
I0314 17:50:23.236992 140174252852992 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.22016893327236176, loss=1.3891568183898926
I0314 17:51:00.892044 140174261245696 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.2158573716878891, loss=1.4220589399337769
I0314 17:51:38.567213 140174252852992 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.22323845326900482, loss=1.4735510349273682
I0314 17:52:16.267787 140174261245696 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.2160644829273224, loss=1.4629323482513428
I0314 17:52:53.925123 140174252852992 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.21422213315963745, loss=1.4463982582092285
I0314 17:53:26.847454 140362249443136 spec.py:298] Evaluating on the training split.
I0314 17:53:29.833087 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 17:57:27.728575 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 17:57:30.354513 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:00:23.447228 140362249443136 spec.py:326] Evaluating on the test split.
I0314 18:00:26.139719 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:03:24.419122 140362249443136 submission_runner.py:362] Time since start: 77184.06s, 	Step: 120489, 	{'train/accuracy': 0.6912932991981506, 'train/loss': 1.4013986587524414, 'train/bleu': 35.395819280536834, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 18:03:24.435712 140174261245696 logging_writer.py:48] [120489] global_step=120489, preemption_count=0, score=45214.221200, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=77184.059902, train/accuracy=0.691293, train/bleu=35.395819, train/loss=1.401399, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 18:03:25.436511 140362249443136 checkpoints.py:356] Saving checkpoint at step: 120489
I0314 18:03:30.064405 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_120489
I0314 18:03:30.111723 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_120489.
I0314 18:03:34.627418 140174252852992 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.2177332639694214, loss=1.4712631702423096
I0314 18:04:12.162833 140174244460288 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.21202452480793, loss=1.45810067653656
I0314 18:04:49.754297 140174252852992 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.2197769582271576, loss=1.479718565940857
I0314 18:05:27.358784 140174244460288 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.2165350764989853, loss=1.512800931930542
I0314 18:06:04.978424 140174252852992 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.21723145246505737, loss=1.4996181726455688
I0314 18:06:42.657979 140174244460288 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.2223798930644989, loss=1.4817838668823242
I0314 18:07:20.340358 140174252852992 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.2065826952457428, loss=1.409873127937317
I0314 18:07:58.012109 140174244460288 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.2191823571920395, loss=1.4431049823760986
I0314 18:08:35.693188 140174252852992 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.21998834609985352, loss=1.3904072046279907
I0314 18:09:13.318801 140174244460288 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.21249647438526154, loss=1.4356331825256348
I0314 18:09:50.980789 140174252852992 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.21201340854167938, loss=1.472433090209961
I0314 18:10:28.625650 140174244460288 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.21838702261447906, loss=1.4951118230819702
I0314 18:11:06.352830 140174252852992 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.2134036272764206, loss=1.4595645666122437
I0314 18:11:44.115553 140174244460288 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23239494860172272, loss=1.5437955856323242
I0314 18:12:21.779683 140174252852992 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.22055929899215698, loss=1.40216064453125
I0314 18:12:59.450364 140174244460288 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.21325527131557465, loss=1.4455386400222778
I0314 18:13:37.108857 140174252852992 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.22071412205696106, loss=1.5038052797317505
I0314 18:14:14.797337 140174244460288 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.2156985104084015, loss=1.4870513677597046
I0314 18:14:52.458777 140174252852992 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.20764213800430298, loss=1.4148656129837036
I0314 18:15:30.179923 140174244460288 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.21773652732372284, loss=1.3999888896942139
I0314 18:16:07.900018 140174252852992 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.2322935312986374, loss=1.4710628986358643
I0314 18:16:45.587160 140174244460288 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.20743879675865173, loss=1.4003127813339233
I0314 18:17:23.263103 140174252852992 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.21772819757461548, loss=1.4774041175842285
I0314 18:17:30.491858 140362249443136 spec.py:298] Evaluating on the training split.
I0314 18:17:33.463982 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:21:33.046641 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 18:21:35.676534 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:24:28.689586 140362249443136 spec.py:326] Evaluating on the test split.
I0314 18:24:31.375401 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:27:29.637082 140362249443136 submission_runner.py:362] Time since start: 78627.70s, 	Step: 122721, 	{'train/accuracy': 0.6924155950546265, 'train/loss': 1.3925426006317139, 'train/bleu': 35.37975292567648, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 18:27:29.652536 140174244460288 logging_writer.py:48] [122721] global_step=122721, preemption_count=0, score=46051.093280, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=78627.704336, train/accuracy=0.692416, train/bleu=35.379753, train/loss=1.392543, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 18:27:30.648481 140362249443136 checkpoints.py:356] Saving checkpoint at step: 122721
I0314 18:27:35.237674 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_122721
I0314 18:27:35.276492 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_122721.
I0314 18:28:05.320706 140174252852992 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.21229074895381927, loss=1.4998905658721924
I0314 18:28:42.910861 140174236067584 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.21430182456970215, loss=1.4425239562988281
I0314 18:29:20.558006 140174252852992 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3217020332813263, loss=1.4024370908737183
I0314 18:29:58.227104 140174236067584 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.2282210886478424, loss=1.4291486740112305
I0314 18:30:35.898154 140174252852992 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.23018430173397064, loss=1.4446663856506348
I0314 18:31:13.569708 140174236067584 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.21710945665836334, loss=1.5206743478775024
I0314 18:31:51.295883 140174252852992 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.2234964221715927, loss=1.4356582164764404
I0314 18:32:28.989230 140174236067584 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.22045345604419708, loss=1.4701318740844727
I0314 18:33:06.667610 140174252852992 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.2172035425901413, loss=1.4677793979644775
I0314 18:33:44.353204 140174236067584 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.23586268723011017, loss=1.47059166431427
I0314 18:34:22.003556 140174252852992 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.2344110608100891, loss=1.5726165771484375
I0314 18:34:59.734695 140174236067584 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.22212165594100952, loss=1.3997715711593628
I0314 18:35:37.479411 140174252852992 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.22026197612285614, loss=1.4295275211334229
I0314 18:36:15.212149 140174236067584 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.21795235574245453, loss=1.4477698802947998
I0314 18:36:52.945088 140174252852992 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.21883784234523773, loss=1.4997272491455078
I0314 18:37:30.636446 140174236067584 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.21148434281349182, loss=1.4848861694335938
I0314 18:38:08.334203 140174252852992 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.22744140028953552, loss=1.545758843421936
I0314 18:38:46.033292 140174236067584 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.22208137810230255, loss=1.467230200767517
I0314 18:39:23.769069 140174252852992 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.21847642958164215, loss=1.4617371559143066
I0314 18:40:01.467134 140174236067584 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.21996760368347168, loss=1.461159110069275
I0314 18:40:39.171654 140174252852992 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.20732776820659637, loss=1.498069405555725
I0314 18:41:16.878018 140174236067584 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.21985271573066711, loss=1.5267835855484009
I0314 18:41:35.467125 140362249443136 spec.py:298] Evaluating on the training split.
I0314 18:41:38.460079 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:45:40.640320 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 18:45:43.273844 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:48:36.473129 140362249443136 spec.py:326] Evaluating on the test split.
I0314 18:48:39.176449 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 18:51:37.453292 140362249443136 submission_runner.py:362] Time since start: 80072.68s, 	Step: 124951, 	{'train/accuracy': 0.6925859451293945, 'train/loss': 1.3929697275161743, 'train/bleu': 34.95425328831614, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 18:51:37.470679 140174252852992 logging_writer.py:48] [124951] global_step=124951, preemption_count=0, score=46887.465125, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=80072.679579, train/accuracy=0.692586, train/bleu=34.954253, train/loss=1.392970, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 18:51:38.478506 140362249443136 checkpoints.py:356] Saving checkpoint at step: 124951
I0314 18:51:43.005011 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_124951
I0314 18:51:43.065016 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_124951.
I0314 18:52:01.800964 140174236067584 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.21991494297981262, loss=1.5093499422073364
I0314 18:52:39.300623 140174227674880 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.22458846867084503, loss=1.5567741394042969
I0314 18:53:16.949305 140174236067584 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.22333721816539764, loss=1.4577102661132812
I0314 18:53:54.606613 140174227674880 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.21910710632801056, loss=1.5184108018875122
I0314 18:54:32.311932 140174236067584 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.21868164837360382, loss=1.473418951034546
I0314 18:55:09.962228 140174227674880 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.20928560197353363, loss=1.4591755867004395
I0314 18:55:47.693679 140174236067584 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.2298336923122406, loss=1.441964864730835
I0314 18:56:25.323318 140174227674880 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.2051115483045578, loss=1.455099105834961
I0314 18:57:03.063557 140174236067584 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.20384350419044495, loss=1.41166090965271
I0314 18:57:40.779718 140174227674880 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.20557214319705963, loss=1.404420018196106
I0314 18:58:18.456922 140174236067584 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.2226189225912094, loss=1.480175495147705
I0314 18:58:56.138165 140174227674880 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.21324458718299866, loss=1.4548903703689575
I0314 18:59:33.826415 140174236067584 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.22194252908229828, loss=1.5165224075317383
I0314 19:00:11.547127 140174227674880 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.2150932401418686, loss=1.4857910871505737
I0314 19:00:49.161958 140174236067584 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.21693269908428192, loss=1.4552719593048096
I0314 19:01:26.869733 140174227674880 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.21977733075618744, loss=1.4754384756088257
I0314 19:02:04.585342 140174236067584 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.21796512603759766, loss=1.4493353366851807
I0314 19:02:42.281680 140174227674880 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.2252149134874344, loss=1.5202478170394897
I0314 19:03:20.026336 140174236067584 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.24137763679027557, loss=1.4809409379959106
I0314 19:03:57.733710 140174227674880 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.2179817259311676, loss=1.4879571199417114
I0314 19:04:35.398565 140174236067584 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.2214367389678955, loss=1.5072457790374756
I0314 19:05:13.058391 140174227674880 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.20724302530288696, loss=1.4420228004455566
I0314 19:05:43.310582 140362249443136 spec.py:298] Evaluating on the training split.
I0314 19:05:46.306343 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:10:02.490560 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 19:10:05.139336 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:12:58.536905 140362249443136 spec.py:326] Evaluating on the test split.
I0314 19:13:01.241820 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:15:59.719136 140362249443136 submission_runner.py:362] Time since start: 81520.52s, 	Step: 127182, 	{'train/accuracy': 0.6919482350349426, 'train/loss': 1.3941630125045776, 'train/bleu': 35.47721451220266, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 19:15:59.736056 140174236067584 logging_writer.py:48] [127182] global_step=127182, preemption_count=0, score=47724.043553, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=81520.523011, train/accuracy=0.691948, train/bleu=35.477215, train/loss=1.394163, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 19:16:00.746214 140362249443136 checkpoints.py:356] Saving checkpoint at step: 127182
I0314 19:16:05.461090 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_127182
I0314 19:16:05.509015 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_127182.
I0314 19:16:12.649538 140174227674880 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.2329234480857849, loss=1.4296183586120605
I0314 19:16:50.196019 140174219282176 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.20881053805351257, loss=1.527138113975525
I0314 19:17:27.841796 140174227674880 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.2297043353319168, loss=1.551993489265442
I0314 19:18:05.522680 140174219282176 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.21252155303955078, loss=1.4867051839828491
I0314 19:18:43.176073 140174227674880 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.22302007675170898, loss=1.4527584314346313
I0314 19:19:20.851382 140174219282176 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.20991049706935883, loss=1.4433848857879639
I0314 19:19:58.583185 140174227674880 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.21778817474842072, loss=1.4124387502670288
I0314 19:20:36.276421 140174219282176 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.22609303891658783, loss=1.5449109077453613
I0314 19:21:13.980280 140174227674880 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.23521456122398376, loss=1.5484578609466553
I0314 19:21:51.703821 140174219282176 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.21425491571426392, loss=1.4614505767822266
I0314 19:22:29.412276 140174227674880 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.22298133373260498, loss=1.390744686126709
I0314 19:23:07.100268 140174219282176 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.20846474170684814, loss=1.4458589553833008
I0314 19:23:44.811770 140174227674880 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.22423787415027618, loss=1.4795622825622559
I0314 19:24:22.490508 140174219282176 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.2160528302192688, loss=1.4996265172958374
I0314 19:25:00.233384 140174227674880 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.21657229959964752, loss=1.4177285432815552
I0314 19:25:37.911315 140174219282176 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.21335919201374054, loss=1.408950924873352
I0314 19:26:15.617464 140174227674880 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.22490297257900238, loss=1.5461080074310303
I0314 19:26:53.320392 140174219282176 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.22118477523326874, loss=1.4225940704345703
I0314 19:27:30.989059 140174227674880 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.22310909628868103, loss=1.4576174020767212
I0314 19:28:08.709013 140174219282176 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.20339763164520264, loss=1.4674659967422485
I0314 19:28:46.401062 140174227674880 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.2234787791967392, loss=1.4542372226715088
I0314 19:29:24.085594 140174219282176 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.2203858494758606, loss=1.4295134544372559
I0314 19:30:01.791477 140174227674880 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.22858940064907074, loss=1.5394577980041504
I0314 19:30:05.675123 140362249443136 spec.py:298] Evaluating on the training split.
I0314 19:30:08.677533 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:34:11.790142 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 19:34:14.437436 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:37:07.773705 140362249443136 spec.py:326] Evaluating on the test split.
I0314 19:37:10.463521 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:40:08.818453 140362249443136 submission_runner.py:362] Time since start: 82982.89s, 	Step: 129412, 	{'train/accuracy': 0.6909036040306091, 'train/loss': 1.4049686193466187, 'train/bleu': 35.39050138809128, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 19:40:08.835448 140174219282176 logging_writer.py:48] [129412] global_step=129412, preemption_count=0, score=48560.933728, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=82982.887587, train/accuracy=0.690904, train/bleu=35.390501, train/loss=1.404969, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 19:40:09.827904 140362249443136 checkpoints.py:356] Saving checkpoint at step: 129412
I0314 19:40:13.903772 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_129412
I0314 19:40:13.921074 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_129412.
I0314 19:40:47.294829 140174227674880 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.21762296557426453, loss=1.502995252609253
I0314 19:41:24.869513 140174210889472 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.2142610102891922, loss=1.4877958297729492
I0314 19:42:02.531394 140174227674880 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.220690056681633, loss=1.449415683746338
I0314 19:42:40.201427 140174210889472 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.22427308559417725, loss=1.4352165460586548
I0314 19:43:17.842784 140174227674880 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.23226356506347656, loss=1.5425945520401
I0314 19:43:55.494550 140174210889472 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.22301605343818665, loss=1.4785261154174805
I0314 19:44:33.142220 140174227674880 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.21963180601596832, loss=1.543393611907959
I0314 19:45:10.832932 140174210889472 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.21991166472434998, loss=1.3849414587020874
I0314 19:45:48.491521 140174227674880 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.2159581035375595, loss=1.4535393714904785
I0314 19:46:26.226578 140174210889472 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.21184620261192322, loss=1.433184266090393
I0314 19:47:03.956732 140174227674880 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.2141656130552292, loss=1.4775371551513672
I0314 19:47:41.634002 140174210889472 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.21460573375225067, loss=1.557979702949524
I0314 19:48:19.335198 140174227674880 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.20919564366340637, loss=1.4476120471954346
I0314 19:48:57.066773 140174210889472 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.2136525809764862, loss=1.4855272769927979
I0314 19:49:34.804621 140174227674880 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.22631730139255524, loss=1.525303602218628
I0314 19:50:12.542387 140174210889472 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.2143915891647339, loss=1.4705191850662231
I0314 19:50:50.221243 140174227674880 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.22703322768211365, loss=1.4940178394317627
I0314 19:51:27.936174 140174210889472 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.21469804644584656, loss=1.443547010421753
I0314 19:52:05.675403 140174227674880 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.2208838164806366, loss=1.3939357995986938
I0314 19:52:43.372263 140174210889472 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.21795426309108734, loss=1.4659372568130493
I0314 19:53:21.043859 140174227674880 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.22044780850410461, loss=1.5415931940078735
I0314 19:53:58.748928 140174210889472 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.2379811555147171, loss=1.5128551721572876
I0314 19:54:13.947060 140362249443136 spec.py:298] Evaluating on the training split.
I0314 19:54:16.925800 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 19:58:13.582172 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 19:58:16.218876 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 20:01:09.600253 140362249443136 spec.py:326] Evaluating on the test split.
I0314 20:01:12.305011 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 20:04:10.841315 140362249443136 submission_runner.py:362] Time since start: 84431.16s, 	Step: 131642, 	{'train/accuracy': 0.6910332441329956, 'train/loss': 1.398209810256958, 'train/bleu': 35.53157046216318, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 20:04:10.857579 140174227674880 logging_writer.py:48] [131642] global_step=131642, preemption_count=0, score=49397.701250, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=84431.159535, train/accuracy=0.691033, train/bleu=35.531570, train/loss=1.398210, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 20:04:11.857038 140362249443136 checkpoints.py:356] Saving checkpoint at step: 131642
I0314 20:04:16.146449 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_131642
I0314 20:04:16.195132 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_131642.
I0314 20:04:38.345300 140174210889472 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.21397539973258972, loss=1.464534044265747
I0314 20:05:15.913924 140174202496768 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.2206297218799591, loss=1.4627734422683716
I0314 20:05:53.543456 140174210889472 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.22303776443004608, loss=1.501680612564087
I0314 20:06:31.200799 140174202496768 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.22276872396469116, loss=1.4718109369277954
I0314 20:07:08.877586 140174210889472 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.21976818144321442, loss=1.4463374614715576
I0314 20:07:46.516996 140174202496768 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.21423515677452087, loss=1.477436900138855
I0314 20:08:24.203740 140174210889472 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.2157304733991623, loss=1.4209227561950684
I0314 20:09:01.947917 140174202496768 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.22144614160060883, loss=1.4651625156402588
I0314 20:09:39.676675 140174210889472 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.21895770728588104, loss=1.5474419593811035
I0314 20:10:17.324372 140174202496768 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.21699203550815582, loss=1.4709340333938599
I0314 20:10:54.967241 140174210889472 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.2144073098897934, loss=1.4644439220428467
I0314 20:11:32.648586 140174202496768 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.23054558038711548, loss=1.512566328048706
I0314 20:12:10.319438 140174210889472 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2204906940460205, loss=1.4563089609146118
I0314 20:12:48.014674 140174202496768 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.21186195313930511, loss=1.4877208471298218
I0314 20:13:25.706553 140174210889472 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.21173635125160217, loss=1.4109188318252563
I0314 20:14:03.418653 140174202496768 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.21256722509860992, loss=1.4894458055496216
I0314 20:14:41.078074 140174210889472 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.21406002342700958, loss=1.4798566102981567
I0314 20:14:52.882605 140362249443136 spec.py:298] Evaluating on the training split.
I0314 20:14:55.860771 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 20:18:45.096200 140362249443136 spec.py:310] Evaluating on the validation split.
I0314 20:18:47.722571 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 20:21:40.836040 140362249443136 spec.py:326] Evaluating on the test split.
I0314 20:21:43.525389 140362249443136 workload.py:179] Translating evaluation dataset.
I0314 20:24:41.875578 140362249443136 submission_runner.py:362] Time since start: 85670.10s, 	Step: 133333, 	{'train/accuracy': 0.6889711618423462, 'train/loss': 1.4083534479141235, 'train/bleu': 35.62008001688247, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003}
I0314 20:24:41.892677 140174202496768 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=50031.858003, test/accuracy=0.710976, test/bleu=31.130084, test/loss=1.267545, test/num_examples=3003, total_duration=85670.095080, train/accuracy=0.688971, train/bleu=35.620080, train/loss=1.408353, validation/accuracy=0.693866, validation/bleu=30.816523, validation/loss=1.373521, validation/num_examples=3000
I0314 20:24:42.898056 140362249443136 checkpoints.py:356] Saving checkpoint at step: 133333
I0314 20:24:47.420116 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333
I0314 20:24:47.470419 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333.
I0314 20:24:47.489560 140174210889472 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=50031.858003
I0314 20:24:48.053834 140362249443136 checkpoints.py:356] Saving checkpoint at step: 133333
I0314 20:24:54.940830 140362249443136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333
I0314 20:24:55.000174 140362249443136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/wmt_jax/trial_1/checkpoint_133333.
I0314 20:24:55.083713 140362249443136 submission_runner.py:523] Tuning trial 1/1
I0314 20:24:55.083901 140362249443136 submission_runner.py:524] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, beta1=0.9326607383586145, beta2=0.9955159689799007, warmup_steps=1999, weight_decay=0.08121616522670176, label_smoothing=0.0)
I0314 20:24:55.086209 140362249443136 submission_runner.py:525] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006027110503055155, 'train/loss': 11.159331321716309, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.150862693786621, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.170835494995117, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 35.030888080596924, 'total_duration': 35.22119641304016, 'global_step': 1, 'preemption_count': 0}), (2230, {'train/accuracy': 0.5143572092056274, 'train/loss': 2.816361427307129, 'train/bleu': 22.219906944273397, 'validation/accuracy': 0.5151950716972351, 'validation/loss': 2.8118090629577637, 'validation/bleu': 18.070424568182716, 'validation/num_examples': 3000, 'test/accuracy': 0.5126140117645264, 'test/loss': 2.859344720840454, 'test/bleu': 16.560917835093097, 'test/num_examples': 3003, 'score': 871.6705594062805, 'total_duration': 1795.1532361507416, 'global_step': 2230, 'preemption_count': 0}), (4461, {'train/accuracy': 0.5813961625099182, 'train/loss': 2.2187016010284424, 'train/bleu': 27.086892822799186, 'validation/accuracy': 0.5903212428092957, 'validation/loss': 2.145246982574463, 'validation/bleu': 23.256466716545344, 'validation/num_examples': 3000, 'test/accuracy': 0.5921794176101685, 'test/loss': 2.125871419906616, 'test/bleu': 21.74518902577842, 'test/num_examples': 3003, 'score': 1707.8853421211243, 'total_duration': 3138.748323917389, 'global_step': 4461, 'preemption_count': 0}), (6692, {'train/accuracy': 0.6101222634315491, 'train/loss': 1.9686866998672485, 'train/bleu': 29.085300689586337, 'validation/accuracy': 0.6167561411857605, 'validation/loss': 1.929276466369629, 'validation/bleu': 24.833213725935916, 'validation/num_examples': 3000, 'test/accuracy': 0.6195340156555176, 'test/loss': 1.8934417963027954, 'test/bleu': 23.621627750290173, 'test/num_examples': 3003, 'score': 2544.7302691936493, 'total_duration': 4422.393854856491, 'global_step': 6692, 'preemption_count': 0}), (8923, {'train/accuracy': 0.61446213722229, 'train/loss': 1.9436389207839966, 'train/bleu': 29.205921910986337, 'validation/accuracy': 0.6297379732131958, 'validation/loss': 1.8282923698425293, 'validation/bleu': 26.115767538692335, 'validation/num_examples': 3000, 'test/accuracy': 0.6348266005516052, 'test/loss': 1.7782090902328491, 'test/bleu': 24.86373861912968, 'test/num_examples': 3003, 'score': 3381.0463104248047, 'total_duration': 5705.15469455719, 'global_step': 8923, 'preemption_count': 0}), (11155, {'train/accuracy': 0.6194417476654053, 'train/loss': 1.9060981273651123, 'train/bleu': 29.758638333649177, 'validation/accuracy': 0.6378098130226135, 'validation/loss': 1.7630480527877808, 'validation/bleu': 26.723611272957527, 'validation/num_examples': 3000, 'test/accuracy': 0.6444134712219238, 'test/loss': 1.7045421600341797, 'test/bleu': 25.5441652507552, 'test/num_examples': 3003, 'score': 4218.005664825439, 'total_duration': 7014.0445194244385, 'global_step': 11155, 'preemption_count': 0}), (13387, {'train/accuracy': 0.6339147686958313, 'train/loss': 1.7766928672790527, 'train/bleu': 30.946284645795878, 'validation/accuracy': 0.6437737941741943, 'validation/loss': 1.7152987718582153, 'validation/bleu': 27.193930608612, 'validation/num_examples': 3000, 'test/accuracy': 0.6501423716545105, 'test/loss': 1.6509428024291992, 'test/bleu': 26.02213064066428, 'test/num_examples': 3003, 'score': 5054.657431364059, 'total_duration': 8354.45589852333, 'global_step': 13387, 'preemption_count': 0}), (15617, {'train/accuracy': 0.6290231943130493, 'train/loss': 1.8172813653945923, 'train/bleu': 31.038015604363164, 'validation/accuracy': 0.6465263962745667, 'validation/loss': 1.6824228763580322, 'validation/bleu': 27.092466678717624, 'validation/num_examples': 3000, 'test/accuracy': 0.6564174294471741, 'test/loss': 1.6197476387023926, 'test/bleu': 26.50804508547904, 'test/num_examples': 3003, 'score': 5890.777983903885, 'total_duration': 9632.150338888168, 'global_step': 15617, 'preemption_count': 0}), (17848, {'train/accuracy': 0.6307936906814575, 'train/loss': 1.814158320426941, 'train/bleu': 30.63108390629473, 'validation/accuracy': 0.6499113440513611, 'validation/loss': 1.6637382507324219, 'validation/bleu': 27.58651886264353, 'validation/num_examples': 3000, 'test/accuracy': 0.6602173447608948, 'test/loss': 1.5958359241485596, 'test/bleu': 26.650819217820796, 'test/num_examples': 3003, 'score': 6727.686890602112, 'total_duration': 11016.49532532692, 'global_step': 17848, 'preemption_count': 0}), (20080, {'train/accuracy': 0.6370040774345398, 'train/loss': 1.748664140701294, 'train/bleu': 31.081748435670853, 'validation/accuracy': 0.6506552696228027, 'validation/loss': 1.645272135734558, 'validation/bleu': 27.4452514532358, 'validation/num_examples': 3000, 'test/accuracy': 0.6628435254096985, 'test/loss': 1.5758757591247559, 'test/bleu': 26.967201040200088, 'test/num_examples': 3003, 'score': 7564.6235518455505, 'total_duration': 12343.540097951889, 'global_step': 20080, 'preemption_count': 0}), (22311, {'train/accuracy': 0.6368141770362854, 'train/loss': 1.7602704763412476, 'train/bleu': 30.84683704303534, 'validation/accuracy': 0.6548337936401367, 'validation/loss': 1.6273638010025024, 'validation/bleu': 27.703256750213143, 'validation/num_examples': 3000, 'test/accuracy': 0.6681657433509827, 'test/loss': 1.553288221359253, 'test/bleu': 27.054677927028752, 'test/num_examples': 3003, 'score': 8401.100092887878, 'total_duration': 13757.33457994461, 'global_step': 22311, 'preemption_count': 0}), (24543, {'train/accuracy': 0.6358434557914734, 'train/loss': 1.7613184452056885, 'train/bleu': 30.93244260895864, 'validation/accuracy': 0.6564456820487976, 'validation/loss': 1.6108226776123047, 'validation/bleu': 28.009714369537495, 'validation/num_examples': 3000, 'test/accuracy': 0.6662250757217407, 'test/loss': 1.5462158918380737, 'test/bleu': 27.05864208360408, 'test/num_examples': 3003, 'score': 9237.966644525528, 'total_duration': 15316.365353822708, 'global_step': 24543, 'preemption_count': 0}), (26775, {'train/accuracy': 0.6415444016456604, 'train/loss': 1.7208589315414429, 'train/bleu': 30.971115904086908, 'validation/accuracy': 0.6567928194999695, 'validation/loss': 1.6031814813613892, 'validation/bleu': 28.025297010655358, 'validation/num_examples': 3000, 'test/accuracy': 0.6676543951034546, 'test/loss': 1.5357131958007812, 'test/bleu': 27.354715059831896, 'test/num_examples': 3003, 'score': 10074.69748544693, 'total_duration': 16661.70343065262, 'global_step': 26775, 'preemption_count': 0}), (29006, {'train/accuracy': 0.6368036270141602, 'train/loss': 1.7542250156402588, 'train/bleu': 31.17904782082637, 'validation/accuracy': 0.6587395071983337, 'validation/loss': 1.593796968460083, 'validation/bleu': 28.123257556698714, 'validation/num_examples': 3000, 'test/accuracy': 0.6702457666397095, 'test/loss': 1.5255166292190552, 'test/bleu': 27.509523812908828, 'test/num_examples': 3003, 'score': 10911.387860059738, 'total_duration': 18027.39549255371, 'global_step': 29006, 'preemption_count': 0}), (31236, {'train/accuracy': 0.6428775191307068, 'train/loss': 1.7256025075912476, 'train/bleu': 31.34213016240891, 'validation/accuracy': 0.6609093546867371, 'validation/loss': 1.5861154794692993, 'validation/bleu': 28.29135554322365, 'validation/num_examples': 3000, 'test/accuracy': 0.6745453476905823, 'test/loss': 1.511854887008667, 'test/bleu': 27.72874425253369, 'test/num_examples': 3003, 'score': 11747.613956212997, 'total_duration': 19464.744547605515, 'global_step': 31236, 'preemption_count': 0}), (33468, {'train/accuracy': 0.6477102041244507, 'train/loss': 1.679289698600769, 'train/bleu': 31.7570253547598, 'validation/accuracy': 0.6622980237007141, 'validation/loss': 1.5731664896011353, 'validation/bleu': 28.533663182301904, 'validation/num_examples': 3000, 'test/accuracy': 0.6735343933105469, 'test/loss': 1.5009454488754272, 'test/bleu': 27.56764231633376, 'test/num_examples': 3003, 'score': 12584.526010751724, 'total_duration': 20825.842467546463, 'global_step': 33468, 'preemption_count': 0}), (35700, {'train/accuracy': 0.6457854509353638, 'train/loss': 1.696174144744873, 'train/bleu': 31.491807952689236, 'validation/accuracy': 0.6630296111106873, 'validation/loss': 1.568543791770935, 'validation/bleu': 28.530390041821494, 'validation/num_examples': 3000, 'test/accuracy': 0.6756028532981873, 'test/loss': 1.4859144687652588, 'test/bleu': 28.23277314478723, 'test/num_examples': 3003, 'score': 13421.43586063385, 'total_duration': 22311.795570135117, 'global_step': 35700, 'preemption_count': 0}), (37931, {'train/accuracy': 0.6595262289047241, 'train/loss': 1.5863367319107056, 'train/bleu': 32.4212970149453, 'validation/accuracy': 0.6662905812263489, 'validation/loss': 1.5519459247589111, 'validation/bleu': 28.74829559657764, 'validation/num_examples': 3000, 'test/accuracy': 0.6785195469856262, 'test/loss': 1.4766432046890259, 'test/bleu': 28.00383939571852, 'test/num_examples': 3003, 'score': 14257.872126340866, 'total_duration': 23890.058379411697, 'global_step': 37931, 'preemption_count': 0}), (40163, {'train/accuracy': 0.6504812836647034, 'train/loss': 1.6621646881103516, 'train/bleu': 32.10532827916792, 'validation/accuracy': 0.6668112874031067, 'validation/loss': 1.551001787185669, 'validation/bleu': 28.823667011296276, 'validation/num_examples': 3000, 'test/accuracy': 0.6789495348930359, 'test/loss': 1.4692944288253784, 'test/bleu': 28.12709210080108, 'test/num_examples': 3003, 'score': 15094.63168144226, 'total_duration': 25293.30780506134, 'global_step': 40163, 'preemption_count': 0}), (42394, {'train/accuracy': 0.6495952606201172, 'train/loss': 1.6698956489562988, 'train/bleu': 31.842459514819787, 'validation/accuracy': 0.6689687371253967, 'validation/loss': 1.537227749824524, 'validation/bleu': 28.90239046261636, 'validation/num_examples': 3000, 'test/accuracy': 0.6820521950721741, 'test/loss': 1.455510139465332, 'test/bleu': 28.497689775181783, 'test/num_examples': 3003, 'score': 15931.476197719574, 'total_duration': 26728.709292650223, 'global_step': 42394, 'preemption_count': 0}), (44626, {'train/accuracy': 0.6603747606277466, 'train/loss': 1.5871729850769043, 'train/bleu': 32.634918828783306, 'validation/accuracy': 0.6694895029067993, 'validation/loss': 1.5236845016479492, 'validation/bleu': 28.692436833359817, 'validation/num_examples': 3000, 'test/accuracy': 0.6816454529762268, 'test/loss': 1.446325421333313, 'test/bleu': 28.566711219938966, 'test/num_examples': 3003, 'score': 16768.19862484932, 'total_duration': 28182.80978178978, 'global_step': 44626, 'preemption_count': 0}), (46858, {'train/accuracy': 0.6571536064147949, 'train/loss': 1.6222567558288574, 'train/bleu': 32.46321439260969, 'validation/accuracy': 0.6699234843254089, 'validation/loss': 1.5176522731781006, 'validation/bleu': 28.782965250092456, 'validation/num_examples': 3000, 'test/accuracy': 0.6819940805435181, 'test/loss': 1.4372467994689941, 'test/bleu': 28.311786705428617, 'test/num_examples': 3003, 'score': 17604.970880270004, 'total_duration': 29573.168073415756, 'global_step': 46858, 'preemption_count': 0}), (49090, {'train/accuracy': 0.6543256640434265, 'train/loss': 1.6302969455718994, 'train/bleu': 31.571055193856335, 'validation/accuracy': 0.6719693541526794, 'validation/loss': 1.5077913999557495, 'validation/bleu': 29.030486197011683, 'validation/num_examples': 3000, 'test/accuracy': 0.6837836503982544, 'test/loss': 1.4256820678710938, 'test/bleu': 28.832387092424568, 'test/num_examples': 3003, 'score': 18441.416204690933, 'total_duration': 30988.06463575363, 'global_step': 49090, 'preemption_count': 0}), (51322, {'train/accuracy': 0.6635710000991821, 'train/loss': 1.570998191833496, 'train/bleu': 32.44542809145213, 'validation/accuracy': 0.6733704209327698, 'validation/loss': 1.5007225275039673, 'validation/bleu': 29.3283344804116, 'validation/num_examples': 3000, 'test/accuracy': 0.6865493059158325, 'test/loss': 1.4169565439224243, 'test/bleu': 29.025726556626203, 'test/num_examples': 3003, 'score': 19278.192416906357, 'total_duration': 32342.554804563522, 'global_step': 51322, 'preemption_count': 0}), (53553, {'train/accuracy': 0.6601815819740295, 'train/loss': 1.6014275550842285, 'train/bleu': 32.31214223579732, 'validation/accuracy': 0.6755898594856262, 'validation/loss': 1.4959287643432617, 'validation/bleu': 29.16912043726757, 'validation/num_examples': 3000, 'test/accuracy': 0.6896635890007019, 'test/loss': 1.4046108722686768, 'test/bleu': 29.387408864400843, 'test/num_examples': 3003, 'score': 20114.469725370407, 'total_duration': 33797.27685856819, 'global_step': 53553, 'preemption_count': 0}), (55785, {'train/accuracy': 0.65819251537323, 'train/loss': 1.6151007413864136, 'train/bleu': 32.755434773289586, 'validation/accuracy': 0.6769289970397949, 'validation/loss': 1.479386329650879, 'validation/bleu': 29.329343182014053, 'validation/num_examples': 3000, 'test/accuracy': 0.691278874874115, 'test/loss': 1.3896815776824951, 'test/bleu': 29.13925278526704, 'test/num_examples': 3003, 'score': 20951.210310697556, 'total_duration': 35197.69635677338, 'global_step': 55785, 'preemption_count': 0}), (58016, {'train/accuracy': 0.6656844615936279, 'train/loss': 1.5551341772079468, 'train/bleu': 33.26694420429745, 'validation/accuracy': 0.679222822189331, 'validation/loss': 1.4697920083999634, 'validation/bleu': 29.382844667726182, 'validation/num_examples': 3000, 'test/accuracy': 0.6908140182495117, 'test/loss': 1.3853192329406738, 'test/bleu': 29.09565726119244, 'test/num_examples': 3003, 'score': 21787.203674077988, 'total_duration': 36560.01263451576, 'global_step': 58016, 'preemption_count': 0}), (60247, {'train/accuracy': 0.6657096743583679, 'train/loss': 1.5562946796417236, 'train/bleu': 33.08732463210115, 'validation/accuracy': 0.679396390914917, 'validation/loss': 1.4645025730133057, 'validation/bleu': 29.69073636905002, 'validation/num_examples': 3000, 'test/accuracy': 0.692661702632904, 'test/loss': 1.373465895652771, 'test/bleu': 29.352713988778685, 'test/num_examples': 3003, 'score': 22623.990482091904, 'total_duration': 37970.263926267624, 'global_step': 60247, 'preemption_count': 0}), (62478, {'train/accuracy': 0.6648634672164917, 'train/loss': 1.5673481225967407, 'train/bleu': 33.100659059809296, 'validation/accuracy': 0.6807107329368591, 'validation/loss': 1.457166314125061, 'validation/bleu': 29.706534131390086, 'validation/num_examples': 3000, 'test/accuracy': 0.6943234205245972, 'test/loss': 1.3682466745376587, 'test/bleu': 29.394509592756187, 'test/num_examples': 3003, 'score': 23460.752336502075, 'total_duration': 39412.720452308655, 'global_step': 62478, 'preemption_count': 0}), (64709, {'train/accuracy': 0.6688649654388428, 'train/loss': 1.539658784866333, 'train/bleu': 34.075775404244766, 'validation/accuracy': 0.6819258332252502, 'validation/loss': 1.4514163732528687, 'validation/bleu': 29.856367639101986, 'validation/num_examples': 3000, 'test/accuracy': 0.6962756514549255, 'test/loss': 1.3569159507751465, 'test/bleu': 29.80527906017588, 'test/num_examples': 3003, 'score': 24297.520780086517, 'total_duration': 40834.343547821045, 'global_step': 64709, 'preemption_count': 0}), (66941, {'train/accuracy': 0.6656746864318848, 'train/loss': 1.550531029701233, 'train/bleu': 33.21185821451705, 'validation/accuracy': 0.6841948628425598, 'validation/loss': 1.4397732019424438, 'validation/bleu': 29.929811644462767, 'validation/num_examples': 3000, 'test/accuracy': 0.6980768442153931, 'test/loss': 1.3459830284118652, 'test/bleu': 29.831121429786897, 'test/num_examples': 3003, 'score': 25134.19421005249, 'total_duration': 42290.344970703125, 'global_step': 66941, 'preemption_count': 0}), (69173, {'train/accuracy': 0.683720588684082, 'train/loss': 1.438248872756958, 'train/bleu': 34.86608844240907, 'validation/accuracy': 0.685732364654541, 'validation/loss': 1.4290837049484253, 'validation/bleu': 30.075729879807703, 'validation/num_examples': 3000, 'test/accuracy': 0.7005054950714111, 'test/loss': 1.3343896865844727, 'test/bleu': 29.839132366150555, 'test/num_examples': 3003, 'score': 25971.048471927643, 'total_duration': 43738.71386671066, 'global_step': 69173, 'preemption_count': 0}), (71404, {'train/accuracy': 0.674594521522522, 'train/loss': 1.4916661977767944, 'train/bleu': 33.85651910761374, 'validation/accuracy': 0.6862035393714905, 'validation/loss': 1.4226713180541992, 'validation/bleu': 30.284451896905654, 'validation/num_examples': 3000, 'test/accuracy': 0.7018534541130066, 'test/loss': 1.3239104747772217, 'test/bleu': 30.34487870493568, 'test/num_examples': 3003, 'score': 26807.771699666977, 'total_duration': 45180.24535822868, 'global_step': 71404, 'preemption_count': 0}), (73635, {'train/accuracy': 0.6756576299667358, 'train/loss': 1.4886127710342407, 'train/bleu': 33.85141670897177, 'validation/accuracy': 0.6877533793449402, 'validation/loss': 1.4158799648284912, 'validation/bleu': 30.272028257036162, 'validation/num_examples': 3000, 'test/accuracy': 0.7029574513435364, 'test/loss': 1.316908597946167, 'test/bleu': 30.412541661395533, 'test/num_examples': 3003, 'score': 27644.342527866364, 'total_duration': 46696.65167093277, 'global_step': 73635, 'preemption_count': 0}), (75866, {'train/accuracy': 0.683646559715271, 'train/loss': 1.4423679113388062, 'train/bleu': 34.934186979304044, 'validation/accuracy': 0.6895264983177185, 'validation/loss': 1.4075807332992554, 'validation/bleu': 30.563442889154814, 'validation/num_examples': 3000, 'test/accuracy': 0.7027714848518372, 'test/loss': 1.310380458831787, 'test/bleu': 30.446460403167094, 'test/num_examples': 3003, 'score': 28480.846055030823, 'total_duration': 48085.48255777359, 'global_step': 75866, 'preemption_count': 0}), (78097, {'train/accuracy': 0.6807436347007751, 'train/loss': 1.457224726676941, 'train/bleu': 34.121504070574105, 'validation/accuracy': 0.6886337399482727, 'validation/loss': 1.4049874544143677, 'validation/bleu': 30.38149107629208, 'validation/num_examples': 3000, 'test/accuracy': 0.7060484886169434, 'test/loss': 1.3031991720199585, 'test/bleu': 30.48310500204027, 'test/num_examples': 3003, 'score': 29316.900856256485, 'total_duration': 49444.85086417198, 'global_step': 78097, 'preemption_count': 0}), (80328, {'train/accuracy': 0.680019736289978, 'train/loss': 1.4670724868774414, 'train/bleu': 34.457180700613506, 'validation/accuracy': 0.6901588439941406, 'validation/loss': 1.3966951370239258, 'validation/bleu': 30.515326431563537, 'validation/num_examples': 3000, 'test/accuracy': 0.706129789352417, 'test/loss': 1.298000454902649, 'test/bleu': 30.595205816177007, 'test/num_examples': 3003, 'score': 30153.474783420563, 'total_duration': 50976.52050161362, 'global_step': 80328, 'preemption_count': 0}), (82559, {'train/accuracy': 0.6884565353393555, 'train/loss': 1.4128315448760986, 'train/bleu': 34.590137194395766, 'validation/accuracy': 0.6907044053077698, 'validation/loss': 1.391269564628601, 'validation/bleu': 30.48122446316172, 'validation/num_examples': 3000, 'test/accuracy': 0.7069897055625916, 'test/loss': 1.2902978658676147, 'test/bleu': 30.506023187292076, 'test/num_examples': 3003, 'score': 30989.849897146225, 'total_duration': 52395.967651844025, 'global_step': 82559, 'preemption_count': 0}), (84789, {'train/accuracy': 0.685408353805542, 'train/loss': 1.425815463066101, 'train/bleu': 34.84863068854933, 'validation/accuracy': 0.6917831301689148, 'validation/loss': 1.3856123685836792, 'validation/bleu': 30.657043733021364, 'validation/num_examples': 3000, 'test/accuracy': 0.7086631059646606, 'test/loss': 1.2799674272537231, 'test/bleu': 30.805698902724128, 'test/num_examples': 3003, 'score': 31826.298433303833, 'total_duration': 53873.711287260056, 'global_step': 84789, 'preemption_count': 0}), (87021, {'train/accuracy': 0.6836018562316895, 'train/loss': 1.4459940195083618, 'train/bleu': 35.087354079933775, 'validation/accuracy': 0.6922294497489929, 'validation/loss': 1.3842662572860718, 'validation/bleu': 30.75781520407087, 'validation/num_examples': 3000, 'test/accuracy': 0.7090232968330383, 'test/loss': 1.278700590133667, 'test/bleu': 30.881470009883994, 'test/num_examples': 3003, 'score': 32662.75225162506, 'total_duration': 55343.92093038559, 'global_step': 87021, 'preemption_count': 0}), (89252, {'train/accuracy': 0.6896376609802246, 'train/loss': 1.4097293615341187, 'train/bleu': 35.52343475030811, 'validation/accuracy': 0.6925890445709229, 'validation/loss': 1.3816791772842407, 'validation/bleu': 30.657612762972896, 'validation/num_examples': 3000, 'test/accuracy': 0.7099297046661377, 'test/loss': 1.275104284286499, 'test/bleu': 30.841054561399737, 'test/num_examples': 3003, 'score': 33499.48403477669, 'total_duration': 56780.402622938156, 'global_step': 89252, 'preemption_count': 0}), (91484, {'train/accuracy': 0.6925252079963684, 'train/loss': 1.3906869888305664, 'train/bleu': 35.35231408765702, 'validation/accuracy': 0.6932461857795715, 'validation/loss': 1.3762766122817993, 'validation/bleu': 30.612884425960974, 'validation/num_examples': 3000, 'test/accuracy': 0.710440993309021, 'test/loss': 1.2723934650421143, 'test/bleu': 30.98221843462107, 'test/num_examples': 3003, 'score': 34336.36853814125, 'total_duration': 58275.511390924454, 'global_step': 91484, 'preemption_count': 0}), (93716, {'train/accuracy': 0.689274787902832, 'train/loss': 1.4142224788665771, 'train/bleu': 35.30038883795664, 'validation/accuracy': 0.6940645575523376, 'validation/loss': 1.3738274574279785, 'validation/bleu': 30.71579415849708, 'validation/num_examples': 3000, 'test/accuracy': 0.710557222366333, 'test/loss': 1.2690550088882446, 'test/bleu': 31.07544673061049, 'test/num_examples': 3003, 'score': 35173.065175533295, 'total_duration': 59765.354714393616, 'global_step': 93716, 'preemption_count': 0}), (95947, {'train/accuracy': 0.6895872354507446, 'train/loss': 1.4059841632843018, 'train/bleu': 35.13083048430764, 'validation/accuracy': 0.6939157247543335, 'validation/loss': 1.3741854429244995, 'validation/bleu': 30.88732485030228, 'validation/num_examples': 3000, 'test/accuracy': 0.7105920910835266, 'test/loss': 1.2683740854263306, 'test/bleu': 30.91373736133046, 'test/num_examples': 3003, 'score': 36009.74479293823, 'total_duration': 61279.60765171051, 'global_step': 95947, 'preemption_count': 0}), (98178, {'train/accuracy': 0.6894282102584839, 'train/loss': 1.4078638553619385, 'train/bleu': 35.557609758214646, 'validation/accuracy': 0.6938413381576538, 'validation/loss': 1.3735398054122925, 'validation/bleu': 30.80878583402332, 'validation/num_examples': 3000, 'test/accuracy': 0.7108244895935059, 'test/loss': 1.267401933670044, 'test/bleu': 31.202344620051388, 'test/num_examples': 3003, 'score': 36846.32492208481, 'total_duration': 62759.934797525406, 'global_step': 98178, 'preemption_count': 0}), (100409, {'train/accuracy': 0.6887820363044739, 'train/loss': 1.412752389907837, 'train/bleu': 35.41154375646261, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 37682.9715924263, 'total_duration': 64219.04488968849, 'global_step': 100409, 'preemption_count': 0}), (102641, {'train/accuracy': 0.6892935037612915, 'train/loss': 1.4130560159683228, 'train/bleu': 35.62646510213669, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 38519.98083901405, 'total_duration': 65666.80546545982, 'global_step': 102641, 'preemption_count': 0}), (104873, {'train/accuracy': 0.691219687461853, 'train/loss': 1.3957493305206299, 'train/bleu': 35.39732856373887, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 39356.81339907646, 'total_duration': 67113.48135066032, 'global_step': 104873, 'preemption_count': 0}), (107104, {'train/accuracy': 0.6907973289489746, 'train/loss': 1.3969403505325317, 'train/bleu': 35.39891174047552, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 40193.71647763252, 'total_duration': 68561.21562051773, 'global_step': 107104, 'preemption_count': 0}), (109335, {'train/accuracy': 0.6931254863739014, 'train/loss': 1.388649344444275, 'train/bleu': 35.28562686450435, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 41030.5181927681, 'total_duration': 69998.98599243164, 'global_step': 109335, 'preemption_count': 0}), (111565, {'train/accuracy': 0.691834568977356, 'train/loss': 1.3927584886550903, 'train/bleu': 35.26646620182535, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 41867.385419130325, 'total_duration': 71431.54651522636, 'global_step': 111565, 'preemption_count': 0}), (113796, {'train/accuracy': 0.6906685829162598, 'train/loss': 1.4012737274169922, 'train/bleu': 35.455053689878476, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 42703.8807053566, 'total_duration': 72869.18895626068, 'global_step': 113796, 'preemption_count': 0}), (116027, {'train/accuracy': 0.690453052520752, 'train/loss': 1.4024473428726196, 'train/bleu': 35.16958077809361, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 43540.707411289215, 'total_duration': 74314.68974232674, 'global_step': 116027, 'preemption_count': 0}), (118258, {'train/accuracy': 0.6903471350669861, 'train/loss': 1.4064098596572876, 'train/bleu': 35.53923727365814, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 44377.44421553612, 'total_duration': 75737.82112598419, 'global_step': 118258, 'preemption_count': 0}), (120489, {'train/accuracy': 0.6912932991981506, 'train/loss': 1.4013986587524414, 'train/bleu': 35.395819280536834, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 45214.2211997509, 'total_duration': 77184.05990242958, 'global_step': 120489, 'preemption_count': 0}), (122721, {'train/accuracy': 0.6924155950546265, 'train/loss': 1.3925426006317139, 'train/bleu': 35.37975292567648, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 46051.09327983856, 'total_duration': 78627.70433568954, 'global_step': 122721, 'preemption_count': 0}), (124951, {'train/accuracy': 0.6925859451293945, 'train/loss': 1.3929697275161743, 'train/bleu': 34.95425328831614, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 46887.46512532234, 'total_duration': 80072.67957854271, 'global_step': 124951, 'preemption_count': 0}), (127182, {'train/accuracy': 0.6919482350349426, 'train/loss': 1.3941630125045776, 'train/bleu': 35.47721451220266, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 47724.04355287552, 'total_duration': 81520.52301120758, 'global_step': 127182, 'preemption_count': 0}), (129412, {'train/accuracy': 0.6909036040306091, 'train/loss': 1.4049686193466187, 'train/bleu': 35.39050138809128, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 48560.93372750282, 'total_duration': 82982.88758707047, 'global_step': 129412, 'preemption_count': 0}), (131642, {'train/accuracy': 0.6910332441329956, 'train/loss': 1.398209810256958, 'train/bleu': 35.53157046216318, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 49397.701249837875, 'total_duration': 84431.15953469276, 'global_step': 131642, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6889711618423462, 'train/loss': 1.4083534479141235, 'train/bleu': 35.62008001688247, 'validation/accuracy': 0.6938661336898804, 'validation/loss': 1.3735207319259644, 'validation/bleu': 30.816522688899795, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2675447463989258, 'test/bleu': 31.130083945814377, 'test/num_examples': 3003, 'score': 50031.85800266266, 'total_duration': 85670.09508013725, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0314 20:24:55.086351 140362249443136 submission_runner.py:526] Timing: 50031.85800266266
I0314 20:24:55.086392 140362249443136 submission_runner.py:527] ====================
I0314 20:24:55.086555 140362249443136 submission_runner.py:586] Final wmt score: 50031.85800266266
