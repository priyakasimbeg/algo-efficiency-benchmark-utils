I0322 02:32:38.252990 139837973382976 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/criteo1tb_jax.
I0322 02:32:38.572787 139837973382976 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0322 02:32:39.528573 139837973382976 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0322 02:32:39.529981 139837973382976 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0322 02:32:39.532625 139837973382976 submission_runner.py:504] Using RNG seed 2362857085
I0322 02:32:40.846255 139837973382976 submission_runner.py:513] --- Tuning run 1/1 ---
I0322 02:32:40.846440 139837973382976 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing/criteo1tb_jax/trial_1.
I0322 02:32:40.846649 139837973382976 logger_utils.py:84] Saving hparams to /experiment_runs/timing/criteo1tb_jax/trial_1/hparams.json.
I0322 02:32:40.969962 139837973382976 submission_runner.py:230] Starting train once: RAM USED (GB) 4.060205056
I0322 02:32:40.970122 139837973382976 submission_runner.py:231] Initializing dataset.
I0322 02:32:40.970291 139837973382976 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.060205056
I0322 02:32:40.970351 139837973382976 submission_runner.py:240] Initializing model.
I0322 02:32:46.474160 139837973382976 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.850827776
I0322 02:32:46.474346 139837973382976 submission_runner.py:252] Initializing optimizer.
I0322 02:32:49.024304 139837973382976 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.852609536
I0322 02:32:49.024477 139837973382976 submission_runner.py:261] Initializing metrics bundle.
I0322 02:32:49.024523 139837973382976 submission_runner.py:275] Initializing checkpoint and logger.
I0322 02:32:49.025551 139837973382976 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing/criteo1tb_jax/trial_1 with prefix checkpoint_
I0322 02:32:49.025817 139837973382976 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0322 02:32:49.025888 139837973382976 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0322 02:32:49.665679 139837973382976 submission_runner.py:296] Saving meta data to /experiment_runs/timing/criteo1tb_jax/trial_1/meta_data_0.json.
I0322 02:32:49.667416 139837973382976 submission_runner.py:299] Saving flags to /experiment_runs/timing/criteo1tb_jax/trial_1/flags_0.json.
I0322 02:32:49.695898 139837973382976 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 7.894081536
I0322 02:32:49.696155 139837973382976 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.894081536
I0322 02:32:49.696237 139837973382976 submission_runner.py:312] Starting training loop.
I0322 02:35:19.775119 139837973382976 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 45.932429312
I0322 02:35:38.978723 139632423458560 logging_writer.py:48] [0] global_step=0, grad_norm=9.177837371826172, loss=1.0212737321853638
I0322 02:35:39.025870 139837973382976 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 53.216608256
I0322 02:35:39.026162 139837973382976 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 53.216608256
I0322 02:35:39.026247 139837973382976 spec.py:298] Evaluating on the training split.
I0322 02:44:24.736914 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 02:48:38.318567 139837973382976 spec.py:326] Evaluating on the test split.
I0322 02:53:15.645044 139837973382976 submission_runner.py:380] Time since start: 169.33s, 	Step: 1, 	{'train/loss': 1.020911390067375, 'validation/loss': 1.015960629213483, 'validation/num_examples': 89000000, 'test/loss': 1.0197544460472014, 'test/num_examples': 89274637}
I0322 02:53:15.645591 139837973382976 submission_runner.py:390] After eval at step 1: RAM USED (GB) 96.287686656
I0322 02:53:15.653075 139574339917568 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=169.137648, test/loss=1.019754, test/num_examples=89274637, total_duration=169.330006, train/loss=1.020911, validation/loss=1.015961, validation/num_examples=89000000
I0322 02:53:18.739709 139837973382976 checkpoints.py:356] Saving checkpoint at step: 1
I0322 02:53:36.660078 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_1
I0322 02:53:36.672659 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_1.
I0322 02:53:36.685411 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 96.364855296
I0322 02:53:36.688947 139837973382976 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 96.323751936
I0322 02:53:36.728478 139837973382976 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 96.32344064
I0322 02:55:01.024875 139574331524864 logging_writer.py:48] [100] global_step=100, grad_norm=0.13790205121040344, loss=0.13027825951576233
I0322 02:57:12.551065 139574223304448 logging_writer.py:48] [200] global_step=200, grad_norm=0.008696035481989384, loss=0.12585647404193878
I0322 02:59:14.345229 139574331524864 logging_writer.py:48] [300] global_step=300, grad_norm=0.03502601012587547, loss=0.1273435801267624
I0322 03:01:36.177850 139574223304448 logging_writer.py:48] [400] global_step=400, grad_norm=0.07127103954553604, loss=0.1260787844657898
I0322 03:02:37.014351 139837973382976 submission_runner.py:371] Before eval at step 456: RAM USED (GB) 103.774347264
I0322 03:02:37.014542 139837973382976 spec.py:298] Evaluating on the training split.
I0322 03:12:34.165185 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 03:16:17.530855 139837973382976 spec.py:326] Evaluating on the test split.
I0322 03:20:22.039689 139837973382976 submission_runner.py:380] Time since start: 1787.32s, 	Step: 456, 	{'train/loss': 0.12476036178442904, 'validation/loss': 0.12637974157303372, 'validation/num_examples': 89000000, 'test/loss': 0.12867388080222605, 'test/num_examples': 89274637}
I0322 03:20:22.040448 139837973382976 submission_runner.py:390] After eval at step 456: RAM USED (GB) 110.174449664
I0322 03:20:22.050534 139573585753856 logging_writer.py:48] [456] global_step=456, preemption_count=0, score=707.119798, test/loss=0.128674, test/num_examples=89274637, total_duration=1787.317940, train/loss=0.124760, validation/loss=0.126380, validation/num_examples=89000000
I0322 03:20:27.548255 139837973382976 checkpoints.py:356] Saving checkpoint at step: 456
I0322 03:20:51.669437 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_456
I0322 03:20:51.852671 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_456.
I0322 03:20:52.107059 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 456: RAM USED (GB) 110.23271936
I0322 03:21:14.123456 139573577361152 logging_writer.py:48] [500] global_step=500, grad_norm=0.005918936338275671, loss=0.12679558992385864
I0322 03:23:01.331334 139573459928832 logging_writer.py:48] [600] global_step=600, grad_norm=0.029433341696858406, loss=0.12641847133636475
I0322 03:24:48.740336 139573577361152 logging_writer.py:48] [700] global_step=700, grad_norm=0.007424770388752222, loss=0.12641641497612
I0322 03:27:10.666972 139573459928832 logging_writer.py:48] [800] global_step=800, grad_norm=0.009425590746104717, loss=0.12357401847839355
I0322 03:29:04.156792 139573577361152 logging_writer.py:48] [900] global_step=900, grad_norm=0.015585598535835743, loss=0.1239011138677597
I0322 03:29:52.912432 139837973382976 submission_runner.py:371] Before eval at step 945: RAM USED (GB) 110.651891712
I0322 03:29:52.912618 139837973382976 spec.py:298] Evaluating on the training split.
I0322 03:39:35.514165 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 03:43:33.212884 139837973382976 spec.py:326] Evaluating on the test split.
I0322 03:48:23.244832 139837973382976 submission_runner.py:380] Time since start: 3423.22s, 	Step: 945, 	{'train/loss': 0.12418469337087243, 'validation/loss': 0.12561313483146067, 'validation/num_examples': 89000000, 'test/loss': 0.1279254039419953, 'test/num_examples': 89274637}
I0322 03:48:23.245457 139837973382976 submission_runner.py:390] After eval at step 945: RAM USED (GB) 114.028863488
I0322 03:48:23.252743 139572629444352 logging_writer.py:48] [945] global_step=945, preemption_count=0, score=1245.393692, test/loss=0.127925, test/num_examples=89274637, total_duration=3423.215986, train/loss=0.124185, validation/loss=0.125613, validation/num_examples=89000000
I0322 03:48:29.475177 139837973382976 checkpoints.py:356] Saving checkpoint at step: 945
I0322 03:49:03.144577 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_945
I0322 03:49:03.342894 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_945.
I0322 03:49:03.594742 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 945: RAM USED (GB) 114.095034368
I0322 03:49:38.296999 139572411365120 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03470369055867195, loss=0.12418490648269653
I0322 03:51:40.915462 139572361008896 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.020177708938717842, loss=0.12455998361110687
I0322 03:53:30.262513 139572411365120 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.00502440519630909, loss=0.12501148879528046
I0322 03:55:22.027963 139572361008896 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.00823240913450718, loss=0.1233716756105423
I0322 03:57:10.184027 139572411365120 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.007357880473136902, loss=0.12275806069374084
I0322 03:58:04.478272 139837973382976 submission_runner.py:371] Before eval at step 1452: RAM USED (GB) 114.513719296
I0322 03:58:04.478468 139837973382976 spec.py:298] Evaluating on the training split.
I0322 04:07:47.011116 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 04:11:36.417705 139837973382976 spec.py:326] Evaluating on the test split.
I0322 04:16:27.720865 139837973382976 submission_runner.py:380] Time since start: 5114.78s, 	Step: 1452, 	{'train/loss': 0.12348678622551049, 'validation/loss': 0.12519216853932585, 'validation/num_examples': 89000000, 'test/loss': 0.12749894463306527, 'test/num_examples': 89274637}
I0322 04:16:27.721623 139837973382976 submission_runner.py:390] After eval at step 1452: RAM USED (GB) 117.744521216
I0322 04:16:27.729862 139579214472960 logging_writer.py:48] [1452] global_step=1452, preemption_count=0, score=1783.936321, test/loss=0.127499, test/num_examples=89274637, total_duration=5114.781864, train/loss=0.123487, validation/loss=0.125192, validation/num_examples=89000000
I0322 04:16:33.087722 139837973382976 checkpoints.py:356] Saving checkpoint at step: 1452
I0322 04:17:04.741705 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_1452
I0322 04:17:04.958312 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_1452.
I0322 04:17:05.197910 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 1452: RAM USED (GB) 117.779795968
I0322 04:17:41.268178 139579206080256 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.008332553319633007, loss=0.12375684827566147
I0322 04:19:55.426774 139573199886080 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.006922196596860886, loss=0.12391921877861023
I0322 04:21:47.367862 139579206080256 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.018548250198364258, loss=0.12459002435207367
I0322 04:23:46.570672 139573199886080 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.009115142747759819, loss=0.12424694001674652
I0322 04:26:05.451887 139837973382976 submission_runner.py:371] Before eval at step 1897: RAM USED (GB) 117.218430976
I0322 04:26:05.452166 139837973382976 spec.py:298] Evaluating on the training split.
I0322 04:35:49.877303 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 04:39:46.684131 139837973382976 spec.py:326] Evaluating on the test split.
I0322 04:44:31.157650 139837973382976 submission_runner.py:380] Time since start: 6795.76s, 	Step: 1897, 	{'train/loss': 0.12402606484116308, 'validation/loss': 0.12499457303370787, 'validation/num_examples': 89000000, 'test/loss': 0.12723609282219764, 'test/num_examples': 89274637}
I0322 04:44:31.158337 139837973382976 submission_runner.py:390] After eval at step 1897: RAM USED (GB) 119.854350336
I0322 04:44:31.166549 139566908438272 logging_writer.py:48] [1897] global_step=1897, preemption_count=0, score=2321.776710, test/loss=0.127236, test/num_examples=89274637, total_duration=6795.755421, train/loss=0.124026, validation/loss=0.124995, validation/num_examples=89000000
I0322 04:44:36.834740 139837973382976 checkpoints.py:356] Saving checkpoint at step: 1897
I0322 04:45:07.335560 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_1897
I0322 04:45:07.518666 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_1897.
I0322 04:45:07.799958 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 1897: RAM USED (GB) 119.915741184
I0322 04:45:08.247660 139566900045568 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.021530788391828537, loss=0.12280848622322083
I0322 04:46:35.077239 139566757435136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.011815644800662994, loss=0.12481842190027237
I0322 04:48:26.644058 139566900045568 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01701429672539234, loss=0.12061533331871033
I0322 04:50:16.668586 139566757435136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.017326075583696365, loss=0.12268748879432678
I0322 04:54:18.014767 139837973382976 submission_runner.py:371] Before eval at step 2232: RAM USED (GB) 119.158284288
I0322 04:54:18.015021 139837973382976 spec.py:298] Evaluating on the training split.
I0322 05:04:02.070027 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 05:08:00.371943 139837973382976 spec.py:326] Evaluating on the test split.
I0322 05:12:28.965564 139837973382976 submission_runner.py:380] Time since start: 8488.32s, 	Step: 2232, 	{'train/loss': 0.12359939137305119, 'validation/loss': 0.12484747191011236, 'validation/num_examples': 89000000, 'test/loss': 0.1270926590269978, 'test/num_examples': 89274637}
I0322 05:12:28.966084 139837973382976 submission_runner.py:390] After eval at step 2232: RAM USED (GB) 122.725761024
I0322 05:12:28.974236 139566900045568 logging_writer.py:48] [2232] global_step=2232, preemption_count=0, score=2870.098403, test/loss=0.127093, test/num_examples=89274637, total_duration=8488.318291, train/loss=0.123599, validation/loss=0.124847, validation/num_examples=89000000
I0322 05:12:35.992928 139837973382976 checkpoints.py:356] Saving checkpoint at step: 2232
I0322 05:13:10.554498 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_2232
I0322 05:13:10.777557 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_2232.
I0322 05:13:11.057218 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 2232: RAM USED (GB) 122.634665984
I0322 05:14:08.644778 139566757435136 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.009443026967346668, loss=0.12299028038978577
I0322 05:16:28.428803 139566631610112 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.020194293931126595, loss=0.123735710978508
I0322 05:18:38.780204 139566757435136 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.008709755726158619, loss=0.12500418722629547
I0322 05:20:28.541493 139566631610112 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.026571709662675858, loss=0.12468764930963516
I0322 05:22:11.060175 139837973382976 submission_runner.py:371] Before eval at step 2681: RAM USED (GB) 122.60026368
I0322 05:22:11.060415 139837973382976 spec.py:298] Evaluating on the training split.
I0322 05:32:33.107272 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 05:36:46.179595 139837973382976 spec.py:326] Evaluating on the test split.
I0322 05:41:21.591803 139837973382976 submission_runner.py:380] Time since start: 10161.36s, 	Step: 2681, 	{'train/loss': 0.12346651976910297, 'validation/loss': 0.12473349438202247, 'validation/num_examples': 89000000, 'test/loss': 0.12697861767838944, 'test/num_examples': 89274637}
I0322 05:41:21.592457 139837973382976 submission_runner.py:390] After eval at step 2681: RAM USED (GB) 124.99687424
I0322 05:41:21.602340 139566757435136 logging_writer.py:48] [2681] global_step=2681, preemption_count=0, score=3407.701980, test/loss=0.126979, test/num_examples=89274637, total_duration=10161.363698, train/loss=0.123467, validation/loss=0.124733, validation/num_examples=89000000
I0322 05:41:27.199212 139837973382976 checkpoints.py:356] Saving checkpoint at step: 2681
I0322 05:41:59.872082 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_2681
I0322 05:42:00.173739 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_2681.
I0322 05:42:00.427740 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 2681: RAM USED (GB) 125.006020608
I0322 05:42:02.578047 139566631610112 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.010176092386245728, loss=0.12430061399936676
I0322 05:43:45.900371 139566582318848 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03492765501141548, loss=0.12386950105428696
I0322 05:45:39.210702 139566631610112 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.013863838277757168, loss=0.12434842437505722
I0322 05:47:35.790133 139566582318848 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.005480155348777771, loss=0.12298454344272614
I0322 05:49:54.366120 139566631610112 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.004697529133409262, loss=0.1245088130235672
I0322 05:51:01.849774 139837973382976 submission_runner.py:371] Before eval at step 3146: RAM USED (GB) 124.451954688
I0322 05:51:01.849954 139837973382976 spec.py:298] Evaluating on the training split.
I0322 06:00:56.765573 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 06:05:07.407780 139837973382976 spec.py:326] Evaluating on the test split.
I0322 06:09:28.667531 139837973382976 submission_runner.py:380] Time since start: 11892.15s, 	Step: 3146, 	{'train/loss': 0.12275181560502323, 'validation/loss': 0.12462255056179776, 'validation/num_examples': 89000000, 'test/loss': 0.1268901603038722, 'test/num_examples': 89274637}
I0322 06:09:28.668030 139837973382976 submission_runner.py:390] After eval at step 3146: RAM USED (GB) 125.750657024
I0322 06:09:28.674633 139566582318848 logging_writer.py:48] [3146] global_step=3146, preemption_count=0, score=3947.314368, test/loss=0.126890, test/num_examples=89274637, total_duration=11892.153425, train/loss=0.122752, validation/loss=0.124623, validation/num_examples=89000000
I0322 06:09:33.973075 139837973382976 checkpoints.py:356] Saving checkpoint at step: 3146
I0322 06:10:06.557677 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_3146
I0322 06:10:06.885247 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_3146.
I0322 06:10:07.112800 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 3146: RAM USED (GB) 125.771542528
I0322 06:10:49.161047 139566631610112 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.013492969796061516, loss=0.12296216189861298
I0322 06:12:45.617646 139569476392704 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.020105190575122833, loss=0.124534472823143
I0322 06:15:01.147130 139566631610112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.008214631117880344, loss=0.1242440864443779
I0322 06:17:19.829600 139569476392704 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0043456414714455605, loss=0.12403387576341629
I0322 06:19:07.753647 139837973382976 submission_runner.py:371] Before eval at step 3592: RAM USED (GB) 125.167628288
I0322 06:19:07.753834 139837973382976 spec.py:298] Evaluating on the training split.
I0322 06:29:45.960670 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 06:33:52.358278 139837973382976 spec.py:326] Evaluating on the test split.
I0322 06:38:51.419373 139837973382976 submission_runner.py:380] Time since start: 13578.06s, 	Step: 3592, 	{'train/loss': 0.12320403266777617, 'validation/loss': 0.12445521348314607, 'validation/num_examples': 89000000, 'test/loss': 0.12673623080651675, 'test/num_examples': 89274637}
I0322 06:38:51.420062 139837973382976 submission_runner.py:390] After eval at step 3592: RAM USED (GB) 126.938427392
I0322 06:38:51.427215 139566631610112 logging_writer.py:48] [3592] global_step=3592, preemption_count=0, score=4485.581653, test/loss=0.126736, test/num_examples=89274637, total_duration=13578.057069, train/loss=0.123204, validation/loss=0.124455, validation/num_examples=89000000
I0322 06:38:56.875833 139837973382976 checkpoints.py:356] Saving checkpoint at step: 3592
I0322 06:39:29.987721 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_3592
I0322 06:39:30.318696 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_3592.
I0322 06:39:30.626829 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 3592: RAM USED (GB) 127.05093632
I0322 06:39:31.608028 139569476392704 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.012856489978730679, loss=0.12465189397335052
I0322 06:41:31.566231 139566740649728 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.008462878875434399, loss=0.12325484305620193
I0322 06:43:27.742659 139569476392704 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.005658051464706659, loss=0.1236596331000328
I0322 06:45:34.430140 139566740649728 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.005971010774374008, loss=0.12315378338098526
I0322 06:47:46.524161 139569476392704 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.019361862912774086, loss=0.12193246185779572
I0322 06:48:31.879496 139837973382976 submission_runner.py:371] Before eval at step 4031: RAM USED (GB) 126.68782592
I0322 06:48:31.879678 139837973382976 spec.py:298] Evaluating on the training split.
I0322 06:58:34.758500 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 07:02:40.437392 139837973382976 spec.py:326] Evaluating on the test split.
I0322 07:07:25.658840 139837973382976 submission_runner.py:380] Time since start: 15342.18s, 	Step: 4031, 	{'train/loss': 0.12316748567852051, 'validation/loss': 0.12441125842696629, 'validation/num_examples': 89000000, 'test/loss': 0.12674220114723064, 'test/num_examples': 89274637}
I0322 07:07:25.659300 139837973382976 submission_runner.py:390] After eval at step 4031: RAM USED (GB) 128.035495936
I0322 07:07:25.666433 139566740649728 logging_writer.py:48] [4031] global_step=4031, preemption_count=0, score=5025.090418, test/loss=0.126742, test/num_examples=89274637, total_duration=15342.183005, train/loss=0.123167, validation/loss=0.124411, validation/num_examples=89000000
I0322 07:07:31.405034 139837973382976 checkpoints.py:356] Saving checkpoint at step: 4031
I0322 07:08:03.198425 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_4031
I0322 07:08:03.503020 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_4031.
I0322 07:08:03.798972 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 4031: RAM USED (GB) 128.06832128
I0322 07:09:03.364635 139569476392704 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.012258954346179962, loss=0.12110399454832077
I0322 07:11:08.571096 139566732257024 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.02240866981446743, loss=0.12424732744693756
I0322 07:13:14.370135 139569476392704 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.004645449575036764, loss=0.1234668642282486
I0322 07:15:09.896501 139566732257024 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.005007649771869183, loss=0.12228918820619583
I0322 07:17:01.732011 139569476392704 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.005862588994204998, loss=0.12262291461229324
I0322 07:17:03.978858 139837973382976 submission_runner.py:371] Before eval at step 4503: RAM USED (GB) 127.506182144
I0322 07:17:03.979030 139837973382976 spec.py:298] Evaluating on the training split.
I0322 07:26:51.559631 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 07:31:12.538994 139837973382976 spec.py:326] Evaluating on the test split.
I0322 07:35:34.953118 139837973382976 submission_runner.py:380] Time since start: 17054.28s, 	Step: 4503, 	{'train/loss': 0.12230831143433246, 'validation/loss': 0.12423997752808989, 'validation/num_examples': 89000000, 'test/loss': 0.1265204472351985, 'test/num_examples': 89274637}
I0322 07:35:34.953668 139837973382976 submission_runner.py:390] After eval at step 4503: RAM USED (GB) 129.301864448
I0322 07:35:34.961242 139566732257024 logging_writer.py:48] [4503] global_step=4503, preemption_count=0, score=5562.866342, test/loss=0.126520, test/num_examples=89274637, total_duration=17054.282449, train/loss=0.122308, validation/loss=0.124240, validation/num_examples=89000000
I0322 07:35:40.688018 139837973382976 checkpoints.py:356] Saving checkpoint at step: 4503
I0322 07:36:12.778587 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_4503
I0322 07:36:13.123353 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_4503.
I0322 07:36:13.419090 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 4503: RAM USED (GB) 129.310478336
I0322 07:37:37.939260 139569476392704 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.006902465596795082, loss=0.1226407065987587
I0322 07:45:14.199953 139837973382976 submission_runner.py:371] Before eval at step 4687: RAM USED (GB) 128.85788672
I0322 07:45:14.200476 139837973382976 spec.py:298] Evaluating on the training split.
I0322 07:55:10.567839 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 07:59:20.155122 139837973382976 spec.py:326] Evaluating on the test split.
I0322 08:04:17.030693 139837973382976 submission_runner.py:380] Time since start: 18744.50s, 	Step: 4687, 	{'train/loss': 0.12167955725573722, 'validation/loss': 0.12419113483146067, 'validation/num_examples': 89000000, 'test/loss': 0.1264805590864514, 'test/num_examples': 89274637}
I0322 08:04:17.031243 139837973382976 submission_runner.py:390] After eval at step 4687: RAM USED (GB) 130.830958592
I0322 08:04:17.039914 139566723864320 logging_writer.py:48] [4687] global_step=4687, preemption_count=0, score=6101.500720, test/loss=0.126481, test/num_examples=89274637, total_duration=18744.503527, train/loss=0.121680, validation/loss=0.124191, validation/num_examples=89000000
I0322 08:04:22.794092 139837973382976 checkpoints.py:356] Saving checkpoint at step: 4687
I0322 08:04:55.801714 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_4687
I0322 08:04:56.121818 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_4687.
I0322 08:04:56.461291 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 4687: RAM USED (GB) 130.954899456
I0322 08:04:57.973937 139569476392704 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.02005159854888916, loss=0.12188950926065445
I0322 08:06:48.212037 139566599104256 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0064963530749082565, loss=0.1238352581858635
I0322 08:08:41.116313 139569476392704 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.005084678530693054, loss=0.12263085693120956
I0322 08:10:34.712838 139566599104256 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00413218978792429, loss=0.12255216389894485
I0322 08:12:43.790729 139569476392704 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.006259659770876169, loss=0.12327541410923004
I0322 08:13:57.195353 139837973382976 submission_runner.py:371] Before eval at step 5164: RAM USED (GB) 130.248699904
I0322 08:13:57.195544 139837973382976 spec.py:298] Evaluating on the training split.
I0322 08:24:07.280781 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 08:28:20.596052 139837973382976 spec.py:326] Evaluating on the test split.
I0322 08:32:41.142033 139837973382976 submission_runner.py:380] Time since start: 20467.50s, 	Step: 5164, 	{'train/loss': 0.1229080076687822, 'validation/loss': 0.12403183146067416, 'validation/num_examples': 89000000, 'test/loss': 0.1263157530396903, 'test/num_examples': 89274637}
I0322 08:32:41.142475 139837973382976 submission_runner.py:390] After eval at step 5164: RAM USED (GB) 131.689914368
I0322 08:32:41.149503 139566599104256 logging_writer.py:48] [5164] global_step=5164, preemption_count=0, score=6640.417433, test/loss=0.126316, test/num_examples=89274637, total_duration=20467.498997, train/loss=0.122908, validation/loss=0.124032, validation/num_examples=89000000
I0322 08:32:46.715567 139837973382976 checkpoints.py:356] Saving checkpoint at step: 5164
I0322 08:33:19.658686 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_5164
I0322 08:33:19.998366 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_5164.
I0322 08:33:20.274228 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 5164: RAM USED (GB) 131.819978752
I0322 08:33:35.238369 139569476392704 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.006796081084758043, loss=0.12286825478076935
I0322 08:35:36.021658 139566590711552 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.007838612422347069, loss=0.12362135201692581
I0322 08:37:57.857473 139569476392704 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.006661976687610149, loss=0.12212816625833511
I0322 08:39:54.959901 139566590711552 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.004855179227888584, loss=0.1240551620721817
I0322 08:41:55.384030 139569476392704 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.00522528076544404, loss=0.12233155965805054
I0322 08:42:20.801288 139837973382976 submission_runner.py:371] Before eval at step 5618: RAM USED (GB) 131.17454336
I0322 08:42:20.801494 139837973382976 spec.py:298] Evaluating on the training split.
I0322 08:52:47.967676 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 08:56:49.370098 139837973382976 spec.py:326] Evaluating on the test split.
I0322 09:01:31.653831 139837973382976 submission_runner.py:380] Time since start: 22171.10s, 	Step: 5618, 	{'train/loss': 0.12217508053586622, 'validation/loss': 0.12393314606741573, 'validation/num_examples': 89000000, 'test/loss': 0.1262224678662093, 'test/num_examples': 89274637}
I0322 09:01:31.654780 139837973382976 submission_runner.py:390] After eval at step 5618: RAM USED (GB) 132.40817664
I0322 09:01:31.662443 139566590711552 logging_writer.py:48] [5618] global_step=5618, preemption_count=0, score=7178.574098, test/loss=0.126222, test/num_examples=89274637, total_duration=22171.104754, train/loss=0.122175, validation/loss=0.123933, validation/num_examples=89000000
I0322 09:01:37.447858 139837973382976 checkpoints.py:356] Saving checkpoint at step: 5618
I0322 09:02:11.101764 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_5618
I0322 09:02:11.431120 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_5618.
I0322 09:02:11.786962 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 5618: RAM USED (GB) 132.409245696
I0322 09:03:21.252969 139569476392704 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.006673934869468212, loss=0.12293724715709686
I0322 09:05:24.969288 139566582318848 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.004026478156447411, loss=0.1227794960141182
I0322 09:07:26.819997 139569476392704 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0058992174454033375, loss=0.120355024933815
I0322 09:09:31.913732 139566582318848 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005014033522456884, loss=0.12317845225334167
I0322 09:11:11.981814 139837973382976 submission_runner.py:371] Before eval at step 6088: RAM USED (GB) 131.856560128
I0322 09:11:11.982044 139837973382976 spec.py:298] Evaluating on the training split.
I0322 09:21:15.158083 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 09:25:14.962193 139837973382976 spec.py:326] Evaluating on the test split.
I0322 09:29:44.416698 139837973382976 submission_runner.py:380] Time since start: 23902.29s, 	Step: 6088, 	{'train/loss': 0.12206860585103936, 'validation/loss': 0.12379508988764044, 'validation/num_examples': 89000000, 'test/loss': 0.12611520335837378, 'test/num_examples': 89274637}
I0322 09:29:44.417134 139837973382976 submission_runner.py:390] After eval at step 6088: RAM USED (GB) 133.557620736
I0322 09:29:44.426736 139569476392704 logging_writer.py:48] [6088] global_step=6088, preemption_count=0, score=7716.364302, test/loss=0.126115, test/num_examples=89274637, total_duration=23902.285365, train/loss=0.122069, validation/loss=0.123795, validation/num_examples=89000000
I0322 09:29:50.097106 139837973382976 checkpoints.py:356] Saving checkpoint at step: 6088
I0322 09:30:23.397896 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_6088
I0322 09:30:23.744700 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_6088.
I0322 09:30:24.094378 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 6088: RAM USED (GB) 133.556932608
I0322 09:30:25.509327 139566582318848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.004249297082424164, loss=0.12287615239620209
I0322 09:32:40.791451 139566749042432 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.004937834106385708, loss=0.12449413537979126
I0322 09:34:41.445175 139566582318848 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0043340278789401054, loss=0.12378828227519989
I0322 09:36:47.955918 139566749042432 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.006060699000954628, loss=0.1225634440779686
I0322 09:38:56.417059 139566582318848 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.012660548090934753, loss=0.12446343153715134
I0322 09:39:25.879829 139837973382976 submission_runner.py:371] Before eval at step 6525: RAM USED (GB) 133.204201472
I0322 09:39:25.880045 139837973382976 spec.py:298] Evaluating on the training split.
I0322 09:49:22.485425 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 09:53:13.296825 139837973382976 spec.py:326] Evaluating on the test split.
I0322 09:57:42.910422 139837973382976 submission_runner.py:380] Time since start: 25596.18s, 	Step: 6525, 	{'train/loss': 0.12166291742837104, 'validation/loss': 0.12377671910112359, 'validation/num_examples': 89000000, 'test/loss': 0.12607261567470726, 'test/num_examples': 89274637}
I0322 09:57:42.911000 139837973382976 submission_runner.py:390] After eval at step 6525: RAM USED (GB) 134.578937856
I0322 09:57:42.918552 139566749042432 logging_writer.py:48] [6525] global_step=6525, preemption_count=0, score=8256.382668, test/loss=0.126073, test/num_examples=89274637, total_duration=25596.183496, train/loss=0.121663, validation/loss=0.123777, validation/num_examples=89000000
I0322 09:57:48.429402 139837973382976 checkpoints.py:356] Saving checkpoint at step: 6525
I0322 09:58:21.461919 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_6525
I0322 09:58:21.787450 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_6525.
I0322 09:58:22.058175 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 6525: RAM USED (GB) 134.609362944
I0322 09:59:21.791977 139566582318848 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.004191842395812273, loss=0.12241556495428085
I0322 10:01:19.760091 139569517291264 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.004437258001416922, loss=0.12352506816387177
I0322 10:03:17.911067 139566582318848 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0043893055990338326, loss=0.12297282367944717
I0322 10:05:12.468342 139569517291264 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.004713199567049742, loss=0.12301675230264664
I0322 10:07:20.212601 139566582318848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.005379258189350367, loss=0.12197445333003998
I0322 10:07:22.555077 139837973382976 submission_runner.py:371] Before eval at step 7003: RAM USED (GB) 134.39356928
I0322 10:07:22.555279 139837973382976 spec.py:298] Evaluating on the training split.
I0322 10:17:33.008094 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 10:21:39.530752 139837973382976 spec.py:326] Evaluating on the test split.
I0322 10:26:34.147164 139837973382976 submission_runner.py:380] Time since start: 27272.86s, 	Step: 7003, 	{'train/loss': 0.12157210573893948, 'validation/loss': 0.12373316853932584, 'validation/num_examples': 89000000, 'test/loss': 0.12601982352501753, 'test/num_examples': 89274637}
I0322 10:26:34.147770 139837973382976 submission_runner.py:390] After eval at step 7003: RAM USED (GB) 135.847022592
I0322 10:26:34.155280 139569517291264 logging_writer.py:48] [7003] global_step=7003, preemption_count=0, score=8794.454400, test/loss=0.126020, test/num_examples=89274637, total_duration=27272.858673, train/loss=0.121572, validation/loss=0.123733, validation/num_examples=89000000
I0322 10:26:41.129487 139837973382976 checkpoints.py:356] Saving checkpoint at step: 7003
I0322 10:27:21.464796 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_7003
I0322 10:27:21.797888 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_7003.
I0322 10:27:22.119030 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 7003: RAM USED (GB) 135.835164672
I0322 10:29:38.921668 139566582318848 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.004555875435471535, loss=0.12210635840892792
I0322 10:36:22.734104 139837973382976 submission_runner.py:371] Before eval at step 7181: RAM USED (GB) 134.981259264
I0322 10:36:22.734300 139837973382976 spec.py:298] Evaluating on the training split.
I0322 10:46:10.409312 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 10:50:39.922843 139837973382976 spec.py:326] Evaluating on the test split.
I0322 10:55:13.345358 139837973382976 submission_runner.py:380] Time since start: 29013.04s, 	Step: 7181, 	{'train/loss': 0.12156992751592394, 'validation/loss': 0.12372912359550561, 'validation/num_examples': 89000000, 'test/loss': 0.1260020357181626, 'test/num_examples': 89274637}
I0322 10:55:13.346128 139837973382976 submission_runner.py:390] After eval at step 7181: RAM USED (GB) 136.45193216
I0322 10:55:13.354012 139566313883392 logging_writer.py:48] [7181] global_step=7181, preemption_count=0, score=9332.935638, test/loss=0.126002, test/num_examples=89274637, total_duration=29013.037544, train/loss=0.121570, validation/loss=0.123729, validation/num_examples=89000000
I0322 10:55:20.395566 139837973382976 checkpoints.py:356] Saving checkpoint at step: 7181
I0322 10:56:00.019070 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_7181
I0322 10:56:00.336881 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_7181.
I0322 10:56:00.678147 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 7181: RAM USED (GB) 136.511602688
I0322 10:56:02.835971 139566582318848 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.008244394324719906, loss=0.12108143419027328
I0322 10:57:56.078188 139565852514048 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.005452272482216358, loss=0.12348087131977081
I0322 10:59:53.722047 139566582318848 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.009570385329425335, loss=0.1254068911075592
I0322 11:01:50.529278 139565852514048 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0042214118875563145, loss=0.12311209738254547
I0322 11:03:48.758834 139566582318848 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.006529286503791809, loss=0.12464575469493866
I0322 11:05:00.902841 139837973382976 submission_runner.py:371] Before eval at step 7661: RAM USED (GB) 135.799472128
I0322 11:05:00.903017 139837973382976 spec.py:298] Evaluating on the training split.
I0322 11:14:56.575835 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 11:19:11.274438 139837973382976 spec.py:326] Evaluating on the test split.
I0322 11:23:33.297647 139837973382976 submission_runner.py:380] Time since start: 30731.21s, 	Step: 7661, 	{'train/loss': 0.12255382186968244, 'validation/loss': 0.12371042696629213, 'validation/num_examples': 89000000, 'test/loss': 0.12600044512082417, 'test/num_examples': 89274637}
I0322 11:23:33.298321 139837973382976 submission_runner.py:390] After eval at step 7661: RAM USED (GB) 137.229565952
I0322 11:23:33.306541 139565852514048 logging_writer.py:48] [7661] global_step=7661, preemption_count=0, score=9871.342013, test/loss=0.126000, test/num_examples=89274637, total_duration=30731.206448, train/loss=0.122554, validation/loss=0.123710, validation/num_examples=89000000
I0322 11:23:38.873578 139837973382976 checkpoints.py:356] Saving checkpoint at step: 7661
I0322 11:24:12.464649 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_7661
I0322 11:24:12.824353 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_7661.
I0322 11:24:13.106029 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 7661: RAM USED (GB) 137.21634816
I0322 11:24:35.431180 139566582318848 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.004368035588413477, loss=0.12255082279443741
I0322 11:26:53.517345 139565844121344 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.004331993404775858, loss=0.12274843454360962
I0322 11:29:05.427146 139566582318848 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0046165077947080135, loss=0.12390294671058655
I0322 11:31:03.381116 139565844121344 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.005477479193359613, loss=0.12433801591396332
I0322 11:32:58.938753 139566582318848 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.004235142841935158, loss=0.1232379898428917
I0322 11:33:13.226685 139837973382976 submission_runner.py:371] Before eval at step 8112: RAM USED (GB) 136.548532224
I0322 11:33:13.226870 139837973382976 spec.py:298] Evaluating on the training split.
I0322 11:43:39.919921 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 11:47:48.165416 139837973382976 spec.py:326] Evaluating on the test split.
I0322 11:52:15.705170 139837973382976 submission_runner.py:380] Time since start: 32423.53s, 	Step: 8112, 	{'train/loss': 0.12165932448319076, 'validation/loss': 0.12374215730337079, 'validation/num_examples': 89000000, 'test/loss': 0.12600479126003056, 'test/num_examples': 89274637}
I0322 11:52:15.705747 139837973382976 submission_runner.py:390] After eval at step 8112: RAM USED (GB) 138.056155136
I0322 11:52:15.713319 139565844121344 logging_writer.py:48] [8112] global_step=8112, preemption_count=0, score=10409.100373, test/loss=0.126005, test/num_examples=89274637, total_duration=32423.530340, train/loss=0.121659, validation/loss=0.123742, validation/num_examples=89000000
I0322 11:52:21.574984 139837973382976 checkpoints.py:356] Saving checkpoint at step: 8112
I0322 11:52:55.708112 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_8112
I0322 11:52:56.065261 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_8112.
I0322 11:52:56.466700 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 8112: RAM USED (GB) 138.076889088
I0322 11:54:13.384789 139566582318848 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.006365899927914143, loss=0.125009685754776
I0322 11:56:41.134882 139565633369856 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.00527219707146287, loss=0.12268140912055969
I0322 11:58:56.225274 139566582318848 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.004369920585304499, loss=0.12388477474451065
I0322 12:00:51.970703 139565633369856 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.006696266122162342, loss=0.12176559865474701
I0322 12:01:56.782295 139837973382976 submission_runner.py:371] Before eval at step 8558: RAM USED (GB) 137.5510528
I0322 12:01:56.782483 139837973382976 spec.py:298] Evaluating on the training split.
I0322 12:11:44.100070 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 12:15:48.177147 139837973382976 spec.py:326] Evaluating on the test split.
I0322 12:20:24.193112 139837973382976 submission_runner.py:380] Time since start: 34147.09s, 	Step: 8558, 	{'train/loss': 0.12152419606055119, 'validation/loss': 0.1237353595505618, 'validation/num_examples': 89000000, 'test/loss': 0.12600257338486853, 'test/num_examples': 89274637}
I0322 12:20:24.193589 139837973382976 submission_runner.py:390] After eval at step 8558: RAM USED (GB) 139.475705856
I0322 12:20:24.200774 139566582318848 logging_writer.py:48] [8558] global_step=8558, preemption_count=0, score=10947.048584, test/loss=0.126003, test/num_examples=89274637, total_duration=34147.085923, train/loss=0.121524, validation/loss=0.123735, validation/num_examples=89000000
I0322 12:20:30.086419 139837973382976 checkpoints.py:356] Saving checkpoint at step: 8558
I0322 12:21:02.989279 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_8558
I0322 12:21:03.302030 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_8558.
I0322 12:21:03.651071 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 8558: RAM USED (GB) 139.488129024
I0322 12:21:24.517314 139565633369856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0056646219454705715, loss=0.12515893578529358
I0322 12:23:20.543835 139565584078592 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.006028159987181425, loss=0.1248280256986618
I0322 12:25:20.621133 139565633369856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.005385139957070351, loss=0.12216757237911224
I0322 12:27:29.993184 139565584078592 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.005818227771669626, loss=0.12238773703575134
I0322 12:29:28.201883 139565633369856 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.00419613067060709, loss=0.12343604862689972
I0322 12:30:03.783973 139837973382976 submission_runner.py:371] Before eval at step 9032: RAM USED (GB) 138.322395136
I0322 12:30:03.784146 139837973382976 spec.py:298] Evaluating on the training split.
I0322 12:40:13.305637 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 12:44:18.523351 139837973382976 spec.py:326] Evaluating on the test split.
I0322 12:48:39.350925 139837973382976 submission_runner.py:380] Time since start: 35834.09s, 	Step: 9032, 	{'train/loss': 0.12238248329639785, 'validation/loss': 0.12375129213483146, 'validation/num_examples': 89000000, 'test/loss': 0.12600437680861137, 'test/num_examples': 89274637}
I0322 12:48:39.351408 139837973382976 submission_runner.py:390] After eval at step 9032: RAM USED (GB) 140.063567872
I0322 12:48:39.358871 139565584078592 logging_writer.py:48] [9032] global_step=9032, preemption_count=0, score=11484.797163, test/loss=0.126004, test/num_examples=89274637, total_duration=35834.087583, train/loss=0.122382, validation/loss=0.123751, validation/num_examples=89000000
I0322 12:48:45.181917 139837973382976 checkpoints.py:356] Saving checkpoint at step: 9032
I0322 12:49:18.745616 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_9032
I0322 12:49:19.118961 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_9032.
I0322 12:49:19.520624 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 9032: RAM USED (GB) 140.080975872
I0322 12:50:11.423567 139565633369856 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.00408894894644618, loss=0.12345696985721588
I0322 12:52:11.019447 139565624977152 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.004946838598698378, loss=0.12281998246908188
I0322 12:54:07.651658 139565633369856 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0038487364072352648, loss=0.12374139577150345
I0322 12:56:08.752919 139565624977152 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.005650064907968044, loss=0.12488089501857758
I0322 12:58:05.037877 139565633369856 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.004322042688727379, loss=0.1242288276553154
I0322 12:58:19.894059 139837973382976 submission_runner.py:371] Before eval at step 9514: RAM USED (GB) 139.313414144
I0322 12:58:19.894248 139837973382976 spec.py:298] Evaluating on the training split.
I0322 13:08:28.639176 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 13:12:24.616658 139837973382976 spec.py:326] Evaluating on the test split.
I0322 13:17:17.666285 139837973382976 submission_runner.py:380] Time since start: 37530.20s, 	Step: 9514, 	{'train/loss': 0.12222868278677451, 'validation/loss': 0.12375278651685394, 'validation/num_examples': 89000000, 'test/loss': 0.12600340228770687, 'test/num_examples': 89274637}
I0322 13:17:17.667033 139837973382976 submission_runner.py:390] After eval at step 9514: RAM USED (GB) 140.435857408
I0322 13:17:17.674638 139565624977152 logging_writer.py:48] [9514] global_step=9514, preemption_count=0, score=12022.789176, test/loss=0.126003, test/num_examples=89274637, total_duration=37530.197715, train/loss=0.122229, validation/loss=0.123753, validation/num_examples=89000000
I0322 13:17:23.523885 139837973382976 checkpoints.py:356] Saving checkpoint at step: 9514
I0322 13:17:56.730444 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_9514
I0322 13:17:57.070228 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_9514.
I0322 13:17:57.435979 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 9514: RAM USED (GB) 140.423327744
I0322 13:24:16.999021 139565633369856 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.007853444665670395, loss=0.12172351032495499
I0322 13:26:15.409447 139566337996544 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.004283891059458256, loss=0.12400598078966141
I0322 13:26:57.705540 139837973382976 submission_runner.py:371] Before eval at step 9736: RAM USED (GB) 139.763924992
I0322 13:26:57.705737 139837973382976 spec.py:298] Evaluating on the training split.
I0322 13:37:07.081263 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 13:41:11.495850 139837973382976 spec.py:326] Evaluating on the test split.
I0322 13:45:32.029116 139837973382976 submission_runner.py:380] Time since start: 39248.01s, 	Step: 9736, 	{'train/loss': 0.12155877815791138, 'validation/loss': 0.12373820224719101, 'validation/num_examples': 89000000, 'test/loss': 0.12600351430160395, 'test/num_examples': 89274637}
I0322 13:45:32.029583 139837973382976 submission_runner.py:390] After eval at step 9736: RAM USED (GB) 141.915004928
I0322 13:45:32.037668 139565189818112 logging_writer.py:48] [9736] global_step=9736, preemption_count=0, score=12560.865938, test/loss=0.126004, test/num_examples=89274637, total_duration=39248.009168, train/loss=0.121559, validation/loss=0.123738, validation/num_examples=89000000
I0322 13:45:37.938141 139837973382976 checkpoints.py:356] Saving checkpoint at step: 9736
I0322 13:46:11.215094 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_9736
I0322 13:46:11.579134 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_9736.
I0322 13:46:11.988772 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 9736: RAM USED (GB) 141.949423616
I0322 13:46:59.039100 139565181425408 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.006766937207430601, loss=0.12202450633049011
I0322 13:48:57.669850 139564182120192 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0056455498561263084, loss=0.12235911190509796
I0322 13:51:12.978751 139565181425408 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.005203257314860821, loss=0.12390074878931046
I0322 13:53:10.695484 139564182120192 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.003654075786471367, loss=0.12381912767887115
I0322 13:55:08.070459 139565181425408 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.004196112509816885, loss=0.12308210134506226
I0322 13:55:12.864604 139837973382976 submission_runner.py:371] Before eval at step 10205: RAM USED (GB) 140.93959168
I0322 13:55:12.864783 139837973382976 spec.py:298] Evaluating on the training split.
I0322 14:05:24.605017 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 14:09:17.772188 139837973382976 spec.py:326] Evaluating on the test split.
I0322 14:13:45.424510 139837973382976 submission_runner.py:380] Time since start: 40943.17s, 	Step: 10205, 	{'train/loss': 0.12172730974277386, 'validation/loss': 0.12370751685393258, 'validation/num_examples': 89000000, 'test/loss': 0.12600353670438336, 'test/num_examples': 89274637}
I0322 14:13:45.425166 139837973382976 submission_runner.py:390] After eval at step 10205: RAM USED (GB) 142.862696448
I0322 14:13:45.432765 139564182120192 logging_writer.py:48] [10205] global_step=10205, preemption_count=0, score=13099.341674, test/loss=0.126004, test/num_examples=89274637, total_duration=40943.168183, train/loss=0.121727, validation/loss=0.123708, validation/num_examples=89000000
I0322 14:13:51.339661 139837973382976 checkpoints.py:356] Saving checkpoint at step: 10205
I0322 14:14:24.744172 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10205
I0322 14:14:25.095977 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10205.
I0322 14:14:25.514809 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 10205: RAM USED (GB) 142.918176768
I0322 14:15:50.586138 139565181425408 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.00813355389982462, loss=0.12129474431276321
I0322 14:17:48.782018 139564341516032 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0060797487385571, loss=0.12281905114650726
I0322 14:20:17.933165 139565181425408 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.004491668194532394, loss=0.12450844794511795
I0322 14:22:21.099767 139564341516032 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.004587219096720219, loss=0.12300845235586166
I0322 14:23:25.523747 139837973382976 submission_runner.py:371] Before eval at step 10653: RAM USED (GB) 142.11293184
I0322 14:23:25.523964 139837973382976 spec.py:298] Evaluating on the training split.
I0322 14:33:43.497808 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 14:38:08.836998 139837973382976 spec.py:326] Evaluating on the test split.
I0322 14:42:57.205760 139837973382976 submission_runner.py:380] Time since start: 42635.83s, 	Step: 10653, 	{'train/loss': 0.1213267861788022, 'validation/loss': 0.123716, 'validation/num_examples': 89000000, 'test/loss': 0.1260028646210009, 'test/num_examples': 89274637}
I0322 14:42:57.206307 139837973382976 submission_runner.py:390] After eval at step 10653: RAM USED (GB) 143.402655744
I0322 14:42:57.214875 139565181425408 logging_writer.py:48] [10653] global_step=10653, preemption_count=0, score=13636.991622, test/loss=0.126003, test/num_examples=89274637, total_duration=42635.827364, train/loss=0.121327, validation/loss=0.123716, validation/num_examples=89000000
I0322 14:43:03.053070 139837973382976 checkpoints.py:356] Saving checkpoint at step: 10653
I0322 14:43:36.149546 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10653
I0322 14:43:36.472920 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10653.
I0322 14:43:36.856789 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 10653: RAM USED (GB) 143.33310976
I0322 14:43:37.637261 139837973382976 submission_runner.py:371] Before eval at step 10666: RAM USED (GB) 143.150833664
I0322 14:43:37.637423 139837973382976 spec.py:298] Evaluating on the training split.
I0322 14:53:43.985642 139837973382976 spec.py:310] Evaluating on the validation split.
I0322 14:57:51.883859 139837973382976 spec.py:326] Evaluating on the test split.
I0322 15:01:56.688225 139837973382976 submission_runner.py:380] Time since start: 43847.94s, 	Step: 10666, 	{'train/loss': 0.12161926314443065, 'validation/loss': 0.12371066292134832, 'validation/num_examples': 89000000, 'test/loss': 0.12600447762111874, 'test/num_examples': 89274637}
I0322 15:01:56.688870 139837973382976 submission_runner.py:390] After eval at step 10666: RAM USED (GB) 143.742857216
I0322 15:01:56.696899 139563318097664 logging_writer.py:48] [10666] global_step=10666, preemption_count=0, score=13637.103796, test/loss=0.126004, test/num_examples=89274637, total_duration=43847.940955, train/loss=0.121619, validation/loss=0.123711, validation/num_examples=89000000
I0322 15:02:02.460717 139837973382976 checkpoints.py:356] Saving checkpoint at step: 10666
I0322 15:02:35.824803 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10666
I0322 15:02:36.211493 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10666.
I0322 15:02:36.589812 139837973382976 submission_runner.py:409] After logging and checkpointing eval at step 10666: RAM USED (GB) 143.686074368
I0322 15:02:36.604873 139563309704960 logging_writer.py:48] [10666] global_step=10666, preemption_count=0, score=13637.103796
I0322 15:02:42.239524 139837973382976 checkpoints.py:356] Saving checkpoint at step: 10666
I0322 15:03:22.779999 139837973382976 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10666
I0322 15:03:23.090889 139837973382976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/criteo1tb_jax/trial_1/checkpoint_10666.
I0322 15:04:54.043697 139837973382976 submission_runner.py:543] Tuning trial 1/1
I0322 15:04:54.043940 139837973382976 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0033313215673016375, beta1=0.948000082541717, beta2=0.9987934318891598, warmup_steps=159, weight_decay=0.0035784380304876183)
I0322 15:04:54.048204 139837973382976 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/loss': 1.020911390067375, 'validation/loss': 1.015960629213483, 'validation/num_examples': 89000000, 'test/loss': 1.0197544460472014, 'test/num_examples': 89274637, 'score': 169.13764762878418, 'total_duration': 169.3300061225891, 'global_step': 1, 'preemption_count': 0}), (456, {'train/loss': 0.12476036178442904, 'validation/loss': 0.12637974157303372, 'validation/num_examples': 89000000, 'test/loss': 0.12867388080222605, 'test/num_examples': 89274637, 'score': 707.1197981834412, 'total_duration': 1787.3179397583008, 'global_step': 456, 'preemption_count': 0}), (945, {'train/loss': 0.12418469337087243, 'validation/loss': 0.12561313483146067, 'validation/num_examples': 89000000, 'test/loss': 0.1279254039419953, 'test/num_examples': 89274637, 'score': 1245.3936922550201, 'total_duration': 3423.2159855365753, 'global_step': 945, 'preemption_count': 0}), (1452, {'train/loss': 0.12348678622551049, 'validation/loss': 0.12519216853932585, 'validation/num_examples': 89000000, 'test/loss': 0.12749894463306527, 'test/num_examples': 89274637, 'score': 1783.9363214969635, 'total_duration': 5114.781864404678, 'global_step': 1452, 'preemption_count': 0}), (1897, {'train/loss': 0.12402606484116308, 'validation/loss': 0.12499457303370787, 'validation/num_examples': 89000000, 'test/loss': 0.12723609282219764, 'test/num_examples': 89274637, 'score': 2321.7767095565796, 'total_duration': 6795.755421161652, 'global_step': 1897, 'preemption_count': 0}), (2232, {'train/loss': 0.12359939137305119, 'validation/loss': 0.12484747191011236, 'validation/num_examples': 89000000, 'test/loss': 0.1270926590269978, 'test/num_examples': 89274637, 'score': 2870.0984029769897, 'total_duration': 8488.31829071045, 'global_step': 2232, 'preemption_count': 0}), (2681, {'train/loss': 0.12346651976910297, 'validation/loss': 0.12473349438202247, 'validation/num_examples': 89000000, 'test/loss': 0.12697861767838944, 'test/num_examples': 89274637, 'score': 3407.7019803524017, 'total_duration': 10161.363698244095, 'global_step': 2681, 'preemption_count': 0}), (3146, {'train/loss': 0.12275181560502323, 'validation/loss': 0.12462255056179776, 'validation/num_examples': 89000000, 'test/loss': 0.1268901603038722, 'test/num_examples': 89274637, 'score': 3947.3143677711487, 'total_duration': 11892.153424739838, 'global_step': 3146, 'preemption_count': 0}), (3592, {'train/loss': 0.12320403266777617, 'validation/loss': 0.12445521348314607, 'validation/num_examples': 89000000, 'test/loss': 0.12673623080651675, 'test/num_examples': 89274637, 'score': 4485.581652641296, 'total_duration': 13578.057069063187, 'global_step': 3592, 'preemption_count': 0}), (4031, {'train/loss': 0.12316748567852051, 'validation/loss': 0.12441125842696629, 'validation/num_examples': 89000000, 'test/loss': 0.12674220114723064, 'test/num_examples': 89274637, 'score': 5025.090418100357, 'total_duration': 15342.18300485611, 'global_step': 4031, 'preemption_count': 0}), (4503, {'train/loss': 0.12230831143433246, 'validation/loss': 0.12423997752808989, 'validation/num_examples': 89000000, 'test/loss': 0.1265204472351985, 'test/num_examples': 89274637, 'score': 5562.8663418293, 'total_duration': 17054.282448530197, 'global_step': 4503, 'preemption_count': 0}), (4687, {'train/loss': 0.12167955725573722, 'validation/loss': 0.12419113483146067, 'validation/num_examples': 89000000, 'test/loss': 0.1264805590864514, 'test/num_examples': 89274637, 'score': 6101.5007202625275, 'total_duration': 18744.50352692604, 'global_step': 4687, 'preemption_count': 0}), (5164, {'train/loss': 0.1229080076687822, 'validation/loss': 0.12403183146067416, 'validation/num_examples': 89000000, 'test/loss': 0.1263157530396903, 'test/num_examples': 89274637, 'score': 6640.417432785034, 'total_duration': 20467.498997449875, 'global_step': 5164, 'preemption_count': 0}), (5618, {'train/loss': 0.12217508053586622, 'validation/loss': 0.12393314606741573, 'validation/num_examples': 89000000, 'test/loss': 0.1262224678662093, 'test/num_examples': 89274637, 'score': 7178.574098110199, 'total_duration': 22171.10475420952, 'global_step': 5618, 'preemption_count': 0}), (6088, {'train/loss': 0.12206860585103936, 'validation/loss': 0.12379508988764044, 'validation/num_examples': 89000000, 'test/loss': 0.12611520335837378, 'test/num_examples': 89274637, 'score': 7716.364301681519, 'total_duration': 23902.285364866257, 'global_step': 6088, 'preemption_count': 0}), (6525, {'train/loss': 0.12166291742837104, 'validation/loss': 0.12377671910112359, 'validation/num_examples': 89000000, 'test/loss': 0.12607261567470726, 'test/num_examples': 89274637, 'score': 8256.382667779922, 'total_duration': 25596.183495521545, 'global_step': 6525, 'preemption_count': 0}), (7003, {'train/loss': 0.12157210573893948, 'validation/loss': 0.12373316853932584, 'validation/num_examples': 89000000, 'test/loss': 0.12601982352501753, 'test/num_examples': 89274637, 'score': 8794.454399585724, 'total_duration': 27272.858672857285, 'global_step': 7003, 'preemption_count': 0}), (7181, {'train/loss': 0.12156992751592394, 'validation/loss': 0.12372912359550561, 'validation/num_examples': 89000000, 'test/loss': 0.1260020357181626, 'test/num_examples': 89274637, 'score': 9332.935637712479, 'total_duration': 29013.037543535233, 'global_step': 7181, 'preemption_count': 0}), (7661, {'train/loss': 0.12255382186968244, 'validation/loss': 0.12371042696629213, 'validation/num_examples': 89000000, 'test/loss': 0.12600044512082417, 'test/num_examples': 89274637, 'score': 9871.342013120651, 'total_duration': 30731.206448316574, 'global_step': 7661, 'preemption_count': 0}), (8112, {'train/loss': 0.12165932448319076, 'validation/loss': 0.12374215730337079, 'validation/num_examples': 89000000, 'test/loss': 0.12600479126003056, 'test/num_examples': 89274637, 'score': 10409.10037279129, 'total_duration': 32423.530339717865, 'global_step': 8112, 'preemption_count': 0}), (8558, {'train/loss': 0.12152419606055119, 'validation/loss': 0.1237353595505618, 'validation/num_examples': 89000000, 'test/loss': 0.12600257338486853, 'test/num_examples': 89274637, 'score': 10947.048583745956, 'total_duration': 34147.085923194885, 'global_step': 8558, 'preemption_count': 0}), (9032, {'train/loss': 0.12238248329639785, 'validation/loss': 0.12375129213483146, 'validation/num_examples': 89000000, 'test/loss': 0.12600437680861137, 'test/num_examples': 89274637, 'score': 11484.797162771225, 'total_duration': 35834.08758330345, 'global_step': 9032, 'preemption_count': 0}), (9514, {'train/loss': 0.12222868278677451, 'validation/loss': 0.12375278651685394, 'validation/num_examples': 89000000, 'test/loss': 0.12600340228770687, 'test/num_examples': 89274637, 'score': 12022.789176225662, 'total_duration': 37530.19771528244, 'global_step': 9514, 'preemption_count': 0}), (9736, {'train/loss': 0.12155877815791138, 'validation/loss': 0.12373820224719101, 'validation/num_examples': 89000000, 'test/loss': 0.12600351430160395, 'test/num_examples': 89274637, 'score': 12560.865938186646, 'total_duration': 39248.0091676712, 'global_step': 9736, 'preemption_count': 0}), (10205, {'train/loss': 0.12172730974277386, 'validation/loss': 0.12370751685393258, 'validation/num_examples': 89000000, 'test/loss': 0.12600353670438336, 'test/num_examples': 89274637, 'score': 13099.34167432785, 'total_duration': 40943.16818332672, 'global_step': 10205, 'preemption_count': 0}), (10653, {'train/loss': 0.1213267861788022, 'validation/loss': 0.123716, 'validation/num_examples': 89000000, 'test/loss': 0.1260028646210009, 'test/num_examples': 89274637, 'score': 13636.991622209549, 'total_duration': 42635.827363967896, 'global_step': 10653, 'preemption_count': 0}), (10666, {'train/loss': 0.12161926314443065, 'validation/loss': 0.12371066292134832, 'validation/num_examples': 89000000, 'test/loss': 0.12600447762111874, 'test/num_examples': 89274637, 'score': 13637.103796243668, 'total_duration': 43847.94095468521, 'global_step': 10666, 'preemption_count': 0})], 'global_step': 10666}
I0322 15:04:54.048387 139837973382976 submission_runner.py:546] Timing: 13637.103796243668
I0322 15:04:54.048443 139837973382976 submission_runner.py:547] ====================
I0322 15:04:54.048552 139837973382976 submission_runner.py:606] Final criteo1tb score: 13637.103796243668
