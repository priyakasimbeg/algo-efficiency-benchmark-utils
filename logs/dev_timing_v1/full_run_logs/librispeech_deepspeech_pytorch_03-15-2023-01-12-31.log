WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0315 01:12:50.630546 140376610289472 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0315 01:12:50.630588 140118354061120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0315 01:12:50.631408 140716637374272 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0315 01:12:50.631435 140520407676736 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0315 01:12:50.631616 140254368069440 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0315 01:12:50.631751 139723621988160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0315 01:12:50.632412 139925058475840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0315 01:12:50.641768 140125394065216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0315 01:12:50.641972 140716637374272 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.642019 140520407676736 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.642071 140125394065216 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.642223 140254368069440 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.642337 139723621988160 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.643067 139925058475840 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.651516 140376610289472 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.651546 140118354061120 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0315 01:12:50.846423 140125394065216 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing/librispeech_deepspeech_pytorch.
W0315 01:12:51.125768 140376610289472 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 01:12:51.126043 140520407676736 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 01:12:51.127801 139723621988160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 01:12:51.129323 140125394065216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 01:12:51.131739 140716637374272 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0315 01:12:51.132119 140125394065216 submission_runner.py:484] Using RNG seed 2993425857
I0315 01:12:51.133106 140125394065216 submission_runner.py:493] --- Tuning run 1/1 ---
I0315 01:12:51.133216 140125394065216 submission_runner.py:498] Creating tuning directory at /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1.
I0315 01:12:51.133396 140125394065216 logger_utils.py:84] Saving hparams to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0315 01:12:51.134120 140125394065216 submission_runner.py:230] Initializing dataset.
I0315 01:12:51.134248 140125394065216 input_pipeline.py:20] Loading split = train-clean-100
W0315 01:12:51.138098 139925058475840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 01:12:51.142853 140254368069440 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0315 01:12:51.144949 140118354061120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0315 01:12:51.169413 140125394065216 input_pipeline.py:20] Loading split = train-clean-360
I0315 01:12:51.498927 140125394065216 input_pipeline.py:20] Loading split = train-other-500
I0315 01:12:51.893972 140125394065216 submission_runner.py:237] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0315 01:12:59.734743 140125394065216 submission_runner.py:247] Initializing optimizer.
I0315 01:12:59.735601 140125394065216 submission_runner.py:254] Initializing metrics bundle.
I0315 01:12:59.735721 140125394065216 submission_runner.py:268] Initializing checkpoint and logger.
I0315 01:12:59.737220 140125394065216 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0315 01:12:59.737329 140125394065216 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0315 01:13:00.449705 140125394065216 submission_runner.py:289] Saving meta data to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0315 01:13:00.450655 140125394065216 submission_runner.py:292] Saving flags to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0315 01:13:00.459026 140125394065216 submission_runner.py:302] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0315 01:13:08.772563 140099148838656 logging_writer.py:48] [0] global_step=0, grad_norm=18.219406, loss=33.871288
I0315 01:13:08.793831 140125394065216 pytorch_submission_base.py:86] 0) loss = 33.871, grad_norm = 18.219
I0315 01:13:08.794724 140125394065216 spec.py:298] Evaluating on the training split.
I0315 01:13:08.795741 140125394065216 input_pipeline.py:20] Loading split = train-clean-100
I0315 01:13:08.823122 140125394065216 input_pipeline.py:20] Loading split = train-clean-360
I0315 01:13:09.210022 140125394065216 input_pipeline.py:20] Loading split = train-other-500
I0315 01:13:25.619412 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 01:13:25.620547 140125394065216 input_pipeline.py:20] Loading split = dev-clean
I0315 01:13:25.624575 140125394065216 input_pipeline.py:20] Loading split = dev-other
I0315 01:13:37.297627 140125394065216 spec.py:326] Evaluating on the test split.
I0315 01:13:37.298895 140125394065216 input_pipeline.py:20] Loading split = test-clean
I0315 01:13:44.374128 140125394065216 submission_runner.py:362] Time since start: 8.34s, 	Step: 1, 	{'train/ctc_loss': 31.93918941811849, 'train/wer': 4.302303821569428, 'validation/ctc_loss': 31.023385699216394, 'validation/wer': 3.8533771061652105, 'validation/num_examples': 5348, 'test/ctc_loss': 31.23632815020324, 'test/wer': 4.29628501208539, 'test/num_examples': 2472}
I0315 01:13:44.398393 140096422536960 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=8.334447, test/ctc_loss=31.236328, test/num_examples=2472, test/wer=4.296285, total_duration=8.336068, train/ctc_loss=31.939189, train/wer=4.302304, validation/ctc_loss=31.023386, validation/num_examples=5348, validation/wer=3.853377
I0315 01:13:44.680076 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_1.
I0315 01:13:44.712065 140125394065216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712142 140254368069440 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712148 139723621988160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712134 140118354061120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712139 140520407676736 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712153 139925058475840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712146 140376610289472 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:44.712949 140716637374272 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0315 01:13:45.808838 140096414144256 logging_writer.py:48] [1] global_step=1, grad_norm=17.797096, loss=33.248032
I0315 01:13:45.812323 140125394065216 pytorch_submission_base.py:86] 1) loss = 33.248, grad_norm = 17.797
I0315 01:13:46.752076 140096422536960 logging_writer.py:48] [2] global_step=2, grad_norm=21.790949, loss=33.609764
I0315 01:13:46.755550 140125394065216 pytorch_submission_base.py:86] 2) loss = 33.610, grad_norm = 21.791
I0315 01:13:47.552619 140096414144256 logging_writer.py:48] [3] global_step=3, grad_norm=35.030651, loss=33.215370
I0315 01:13:47.556231 140125394065216 pytorch_submission_base.py:86] 3) loss = 33.215, grad_norm = 35.031
I0315 01:13:48.370886 140096422536960 logging_writer.py:48] [4] global_step=4, grad_norm=47.349617, loss=31.914116
I0315 01:13:48.374710 140125394065216 pytorch_submission_base.py:86] 4) loss = 31.914, grad_norm = 47.350
I0315 01:13:49.173310 140096414144256 logging_writer.py:48] [5] global_step=5, grad_norm=49.599564, loss=31.013340
I0315 01:13:49.176467 140125394065216 pytorch_submission_base.py:86] 5) loss = 31.013, grad_norm = 49.600
I0315 01:13:49.986370 140096422536960 logging_writer.py:48] [6] global_step=6, grad_norm=43.261818, loss=29.933657
I0315 01:13:49.989694 140125394065216 pytorch_submission_base.py:86] 6) loss = 29.934, grad_norm = 43.262
I0315 01:13:50.798151 140096414144256 logging_writer.py:48] [7] global_step=7, grad_norm=38.047146, loss=27.928747
I0315 01:13:50.801383 140125394065216 pytorch_submission_base.py:86] 7) loss = 27.929, grad_norm = 38.047
I0315 01:13:51.603056 140096422536960 logging_writer.py:48] [8] global_step=8, grad_norm=31.942160, loss=27.247381
I0315 01:13:51.606322 140125394065216 pytorch_submission_base.py:86] 8) loss = 27.247, grad_norm = 31.942
I0315 01:13:52.404960 140096414144256 logging_writer.py:48] [9] global_step=9, grad_norm=27.735424, loss=26.137394
I0315 01:13:52.408028 140125394065216 pytorch_submission_base.py:86] 9) loss = 26.137, grad_norm = 27.735
I0315 01:13:53.215764 140096422536960 logging_writer.py:48] [10] global_step=10, grad_norm=25.313868, loss=25.364735
I0315 01:13:53.219233 140125394065216 pytorch_submission_base.py:86] 10) loss = 25.365, grad_norm = 25.314
I0315 01:13:54.026918 140096414144256 logging_writer.py:48] [11] global_step=11, grad_norm=25.575321, loss=24.221771
I0315 01:13:54.030086 140125394065216 pytorch_submission_base.py:86] 11) loss = 24.222, grad_norm = 25.575
I0315 01:13:54.827320 140096422536960 logging_writer.py:48] [12] global_step=12, grad_norm=24.562029, loss=23.535852
I0315 01:13:54.830780 140125394065216 pytorch_submission_base.py:86] 12) loss = 23.536, grad_norm = 24.562
I0315 01:13:55.630713 140096414144256 logging_writer.py:48] [13] global_step=13, grad_norm=24.181667, loss=22.100477
I0315 01:13:55.633845 140125394065216 pytorch_submission_base.py:86] 13) loss = 22.100, grad_norm = 24.182
I0315 01:13:56.431530 140096422536960 logging_writer.py:48] [14] global_step=14, grad_norm=23.766560, loss=21.183395
I0315 01:13:56.434963 140125394065216 pytorch_submission_base.py:86] 14) loss = 21.183, grad_norm = 23.767
I0315 01:13:57.251531 140096414144256 logging_writer.py:48] [15] global_step=15, grad_norm=22.798174, loss=19.197887
I0315 01:13:57.254773 140125394065216 pytorch_submission_base.py:86] 15) loss = 19.198, grad_norm = 22.798
I0315 01:13:58.051981 140096422536960 logging_writer.py:48] [16] global_step=16, grad_norm=21.462782, loss=18.590839
I0315 01:13:58.054954 140125394065216 pytorch_submission_base.py:86] 16) loss = 18.591, grad_norm = 21.463
I0315 01:13:58.851308 140096414144256 logging_writer.py:48] [17] global_step=17, grad_norm=20.174967, loss=16.999262
I0315 01:13:58.854632 140125394065216 pytorch_submission_base.py:86] 17) loss = 16.999, grad_norm = 20.175
I0315 01:13:59.654692 140096422536960 logging_writer.py:48] [18] global_step=18, grad_norm=18.106529, loss=16.070395
I0315 01:13:59.658083 140125394065216 pytorch_submission_base.py:86] 18) loss = 16.070, grad_norm = 18.107
I0315 01:14:00.452877 140096414144256 logging_writer.py:48] [19] global_step=19, grad_norm=15.915075, loss=14.860853
I0315 01:14:00.456315 140125394065216 pytorch_submission_base.py:86] 19) loss = 14.861, grad_norm = 15.915
I0315 01:14:01.271251 140096422536960 logging_writer.py:48] [20] global_step=20, grad_norm=13.531850, loss=13.663094
I0315 01:14:01.274274 140125394065216 pytorch_submission_base.py:86] 20) loss = 13.663, grad_norm = 13.532
I0315 01:14:02.100358 140096414144256 logging_writer.py:48] [21] global_step=21, grad_norm=12.204435, loss=13.002902
I0315 01:14:02.103696 140125394065216 pytorch_submission_base.py:86] 21) loss = 13.003, grad_norm = 12.204
I0315 01:14:02.915528 140096422536960 logging_writer.py:48] [22] global_step=22, grad_norm=10.841134, loss=12.222771
I0315 01:14:02.918903 140125394065216 pytorch_submission_base.py:86] 22) loss = 12.223, grad_norm = 10.841
I0315 01:14:03.729690 140096414144256 logging_writer.py:48] [23] global_step=23, grad_norm=9.102490, loss=11.248044
I0315 01:14:03.733133 140125394065216 pytorch_submission_base.py:86] 23) loss = 11.248, grad_norm = 9.102
I0315 01:14:04.550880 140096422536960 logging_writer.py:48] [24] global_step=24, grad_norm=8.528153, loss=10.845543
I0315 01:14:04.554240 140125394065216 pytorch_submission_base.py:86] 24) loss = 10.846, grad_norm = 8.528
I0315 01:14:05.357427 140096414144256 logging_writer.py:48] [25] global_step=25, grad_norm=7.027720, loss=10.159667
I0315 01:14:05.360598 140125394065216 pytorch_submission_base.py:86] 25) loss = 10.160, grad_norm = 7.028
I0315 01:14:06.189938 140096422536960 logging_writer.py:48] [26] global_step=26, grad_norm=6.084587, loss=9.650002
I0315 01:14:06.193062 140125394065216 pytorch_submission_base.py:86] 26) loss = 9.650, grad_norm = 6.085
I0315 01:14:06.996882 140096414144256 logging_writer.py:48] [27] global_step=27, grad_norm=5.115822, loss=9.174296
I0315 01:14:06.999983 140125394065216 pytorch_submission_base.py:86] 27) loss = 9.174, grad_norm = 5.116
I0315 01:14:07.834508 140096422536960 logging_writer.py:48] [28] global_step=28, grad_norm=4.350632, loss=8.635910
I0315 01:14:07.837790 140125394065216 pytorch_submission_base.py:86] 28) loss = 8.636, grad_norm = 4.351
I0315 01:14:08.646646 140096414144256 logging_writer.py:48] [29] global_step=29, grad_norm=3.312687, loss=8.276455
I0315 01:14:08.649945 140125394065216 pytorch_submission_base.py:86] 29) loss = 8.276, grad_norm = 3.313
I0315 01:14:09.477474 140096422536960 logging_writer.py:48] [30] global_step=30, grad_norm=3.437017, loss=8.008050
I0315 01:14:09.480811 140125394065216 pytorch_submission_base.py:86] 30) loss = 8.008, grad_norm = 3.437
I0315 01:14:10.289761 140096414144256 logging_writer.py:48] [31] global_step=31, grad_norm=2.519700, loss=7.714114
I0315 01:14:10.293592 140125394065216 pytorch_submission_base.py:86] 31) loss = 7.714, grad_norm = 2.520
I0315 01:14:11.096065 140096422536960 logging_writer.py:48] [32] global_step=32, grad_norm=2.278352, loss=7.537746
I0315 01:14:11.099259 140125394065216 pytorch_submission_base.py:86] 32) loss = 7.538, grad_norm = 2.278
I0315 01:14:11.900076 140096414144256 logging_writer.py:48] [33] global_step=33, grad_norm=1.958376, loss=7.396663
I0315 01:14:11.903174 140125394065216 pytorch_submission_base.py:86] 33) loss = 7.397, grad_norm = 1.958
I0315 01:14:12.706243 140096422536960 logging_writer.py:48] [34] global_step=34, grad_norm=2.357498, loss=7.303977
I0315 01:14:12.709526 140125394065216 pytorch_submission_base.py:86] 34) loss = 7.304, grad_norm = 2.357
I0315 01:14:13.512058 140096414144256 logging_writer.py:48] [35] global_step=35, grad_norm=3.437955, loss=7.218143
I0315 01:14:13.515283 140125394065216 pytorch_submission_base.py:86] 35) loss = 7.218, grad_norm = 3.438
I0315 01:14:14.313800 140096422536960 logging_writer.py:48] [36] global_step=36, grad_norm=4.334325, loss=7.128063
I0315 01:14:14.317123 140125394065216 pytorch_submission_base.py:86] 36) loss = 7.128, grad_norm = 4.334
I0315 01:14:15.115410 140096414144256 logging_writer.py:48] [37] global_step=37, grad_norm=2.764788, loss=6.944394
I0315 01:14:15.118921 140125394065216 pytorch_submission_base.py:86] 37) loss = 6.944, grad_norm = 2.765
I0315 01:14:15.923232 140096422536960 logging_writer.py:48] [38] global_step=38, grad_norm=2.240283, loss=6.878753
I0315 01:14:15.926350 140125394065216 pytorch_submission_base.py:86] 38) loss = 6.879, grad_norm = 2.240
I0315 01:14:16.723369 140096414144256 logging_writer.py:48] [39] global_step=39, grad_norm=3.044445, loss=6.749580
I0315 01:14:16.726399 140125394065216 pytorch_submission_base.py:86] 39) loss = 6.750, grad_norm = 3.044
I0315 01:14:17.536009 140096422536960 logging_writer.py:48] [40] global_step=40, grad_norm=7.654461, loss=6.834176
I0315 01:14:17.539618 140125394065216 pytorch_submission_base.py:86] 40) loss = 6.834, grad_norm = 7.654
I0315 01:14:18.350314 140096414144256 logging_writer.py:48] [41] global_step=41, grad_norm=7.740752, loss=6.712652
I0315 01:14:18.353533 140125394065216 pytorch_submission_base.py:86] 41) loss = 6.713, grad_norm = 7.741
I0315 01:14:19.165237 140096422536960 logging_writer.py:48] [42] global_step=42, grad_norm=4.053868, loss=6.574253
I0315 01:14:19.168462 140125394065216 pytorch_submission_base.py:86] 42) loss = 6.574, grad_norm = 4.054
I0315 01:14:19.964063 140096414144256 logging_writer.py:48] [43] global_step=43, grad_norm=3.379810, loss=6.485219
I0315 01:14:19.967274 140125394065216 pytorch_submission_base.py:86] 43) loss = 6.485, grad_norm = 3.380
I0315 01:14:20.780315 140096422536960 logging_writer.py:48] [44] global_step=44, grad_norm=5.528207, loss=6.438491
I0315 01:14:20.783588 140125394065216 pytorch_submission_base.py:86] 44) loss = 6.438, grad_norm = 5.528
I0315 01:14:21.594546 140096414144256 logging_writer.py:48] [45] global_step=45, grad_norm=7.194870, loss=6.464435
I0315 01:14:21.598046 140125394065216 pytorch_submission_base.py:86] 45) loss = 6.464, grad_norm = 7.195
I0315 01:14:22.397797 140096422536960 logging_writer.py:48] [46] global_step=46, grad_norm=4.843254, loss=6.331238
I0315 01:14:22.401061 140125394065216 pytorch_submission_base.py:86] 46) loss = 6.331, grad_norm = 4.843
I0315 01:14:23.202546 140096414144256 logging_writer.py:48] [47] global_step=47, grad_norm=2.400989, loss=6.238756
I0315 01:14:23.205602 140125394065216 pytorch_submission_base.py:86] 47) loss = 6.239, grad_norm = 2.401
I0315 01:14:24.006113 140096422536960 logging_writer.py:48] [48] global_step=48, grad_norm=1.100258, loss=6.194049
I0315 01:14:24.009486 140125394065216 pytorch_submission_base.py:86] 48) loss = 6.194, grad_norm = 1.100
I0315 01:14:24.817478 140096414144256 logging_writer.py:48] [49] global_step=49, grad_norm=1.757973, loss=6.137370
I0315 01:14:24.820782 140125394065216 pytorch_submission_base.py:86] 49) loss = 6.137, grad_norm = 1.758
I0315 01:14:25.627467 140096422536960 logging_writer.py:48] [50] global_step=50, grad_norm=3.841086, loss=6.133753
I0315 01:14:25.630781 140125394065216 pytorch_submission_base.py:86] 50) loss = 6.134, grad_norm = 3.841
I0315 01:14:26.432028 140096414144256 logging_writer.py:48] [51] global_step=51, grad_norm=6.918488, loss=6.189673
I0315 01:14:26.435116 140125394065216 pytorch_submission_base.py:86] 51) loss = 6.190, grad_norm = 6.918
I0315 01:14:27.251535 140096422536960 logging_writer.py:48] [52] global_step=52, grad_norm=4.896626, loss=6.110586
I0315 01:14:27.254828 140125394065216 pytorch_submission_base.py:86] 52) loss = 6.111, grad_norm = 4.897
I0315 01:14:28.065299 140096414144256 logging_writer.py:48] [53] global_step=53, grad_norm=1.389293, loss=6.036798
I0315 01:14:28.068706 140125394065216 pytorch_submission_base.py:86] 53) loss = 6.037, grad_norm = 1.389
I0315 01:14:28.872291 140096422536960 logging_writer.py:48] [54] global_step=54, grad_norm=0.517606, loss=6.020911
I0315 01:14:28.875715 140125394065216 pytorch_submission_base.py:86] 54) loss = 6.021, grad_norm = 0.518
I0315 01:14:29.681951 140096414144256 logging_writer.py:48] [55] global_step=55, grad_norm=1.458735, loss=6.010859
I0315 01:14:29.685078 140125394065216 pytorch_submission_base.py:86] 55) loss = 6.011, grad_norm = 1.459
I0315 01:14:30.509794 140096422536960 logging_writer.py:48] [56] global_step=56, grad_norm=2.297355, loss=5.958370
I0315 01:14:30.512951 140125394065216 pytorch_submission_base.py:86] 56) loss = 5.958, grad_norm = 2.297
I0315 01:14:31.326504 140096414144256 logging_writer.py:48] [57] global_step=57, grad_norm=4.099341, loss=6.006132
I0315 01:14:31.329782 140125394065216 pytorch_submission_base.py:86] 57) loss = 6.006, grad_norm = 4.099
I0315 01:14:32.139982 140096422536960 logging_writer.py:48] [58] global_step=58, grad_norm=5.426837, loss=6.044708
I0315 01:14:32.143127 140125394065216 pytorch_submission_base.py:86] 58) loss = 6.045, grad_norm = 5.427
I0315 01:14:32.961573 140096414144256 logging_writer.py:48] [59] global_step=59, grad_norm=3.660534, loss=5.983131
I0315 01:14:32.964806 140125394065216 pytorch_submission_base.py:86] 59) loss = 5.983, grad_norm = 3.661
I0315 01:14:33.780511 140096422536960 logging_writer.py:48] [60] global_step=60, grad_norm=1.795294, loss=5.957044
I0315 01:14:33.783499 140125394065216 pytorch_submission_base.py:86] 60) loss = 5.957, grad_norm = 1.795
I0315 01:14:34.585201 140096414144256 logging_writer.py:48] [61] global_step=61, grad_norm=2.134502, loss=5.938278
I0315 01:14:34.588420 140125394065216 pytorch_submission_base.py:86] 61) loss = 5.938, grad_norm = 2.135
I0315 01:14:35.393557 140096422536960 logging_writer.py:48] [62] global_step=62, grad_norm=2.833752, loss=5.940887
I0315 01:14:35.396803 140125394065216 pytorch_submission_base.py:86] 62) loss = 5.941, grad_norm = 2.834
I0315 01:14:36.196469 140096414144256 logging_writer.py:48] [63] global_step=63, grad_norm=3.505296, loss=5.945550
I0315 01:14:36.199825 140125394065216 pytorch_submission_base.py:86] 63) loss = 5.946, grad_norm = 3.505
I0315 01:14:37.001876 140096422536960 logging_writer.py:48] [64] global_step=64, grad_norm=3.521317, loss=5.941583
I0315 01:14:37.005062 140125394065216 pytorch_submission_base.py:86] 64) loss = 5.942, grad_norm = 3.521
I0315 01:14:37.808141 140096414144256 logging_writer.py:48] [65] global_step=65, grad_norm=3.112001, loss=5.958754
I0315 01:14:37.811240 140125394065216 pytorch_submission_base.py:86] 65) loss = 5.959, grad_norm = 3.112
I0315 01:14:38.608286 140096422536960 logging_writer.py:48] [66] global_step=66, grad_norm=2.760786, loss=5.915364
I0315 01:14:38.611461 140125394065216 pytorch_submission_base.py:86] 66) loss = 5.915, grad_norm = 2.761
I0315 01:14:39.416906 140096414144256 logging_writer.py:48] [67] global_step=67, grad_norm=2.948798, loss=5.919329
I0315 01:14:39.420352 140125394065216 pytorch_submission_base.py:86] 67) loss = 5.919, grad_norm = 2.949
I0315 01:14:40.226587 140096422536960 logging_writer.py:48] [68] global_step=68, grad_norm=2.793335, loss=5.915032
I0315 01:14:40.229860 140125394065216 pytorch_submission_base.py:86] 68) loss = 5.915, grad_norm = 2.793
I0315 01:14:41.038901 140096414144256 logging_writer.py:48] [69] global_step=69, grad_norm=2.367888, loss=5.928731
I0315 01:14:41.042454 140125394065216 pytorch_submission_base.py:86] 69) loss = 5.929, grad_norm = 2.368
I0315 01:14:41.842549 140096422536960 logging_writer.py:48] [70] global_step=70, grad_norm=2.493388, loss=5.892203
I0315 01:14:41.845764 140125394065216 pytorch_submission_base.py:86] 70) loss = 5.892, grad_norm = 2.493
I0315 01:14:42.694849 140096414144256 logging_writer.py:48] [71] global_step=71, grad_norm=3.743062, loss=5.912996
I0315 01:14:42.697838 140125394065216 pytorch_submission_base.py:86] 71) loss = 5.913, grad_norm = 3.743
I0315 01:14:43.518587 140096422536960 logging_writer.py:48] [72] global_step=72, grad_norm=3.870708, loss=5.898035
I0315 01:14:43.521865 140125394065216 pytorch_submission_base.py:86] 72) loss = 5.898, grad_norm = 3.871
I0315 01:14:44.320306 140096414144256 logging_writer.py:48] [73] global_step=73, grad_norm=2.626790, loss=5.883808
I0315 01:14:44.323404 140125394065216 pytorch_submission_base.py:86] 73) loss = 5.884, grad_norm = 2.627
I0315 01:14:45.125933 140096422536960 logging_writer.py:48] [74] global_step=74, grad_norm=1.975419, loss=5.888802
I0315 01:14:45.129163 140125394065216 pytorch_submission_base.py:86] 74) loss = 5.889, grad_norm = 1.975
I0315 01:14:45.969134 140096414144256 logging_writer.py:48] [75] global_step=75, grad_norm=2.251300, loss=5.885711
I0315 01:14:45.972372 140125394065216 pytorch_submission_base.py:86] 75) loss = 5.886, grad_norm = 2.251
I0315 01:14:46.776009 140096422536960 logging_writer.py:48] [76] global_step=76, grad_norm=2.755926, loss=5.879438
I0315 01:14:46.779195 140125394065216 pytorch_submission_base.py:86] 76) loss = 5.879, grad_norm = 2.756
I0315 01:14:47.594240 140096414144256 logging_writer.py:48] [77] global_step=77, grad_norm=3.459794, loss=5.892991
I0315 01:14:47.597494 140125394065216 pytorch_submission_base.py:86] 77) loss = 5.893, grad_norm = 3.460
I0315 01:14:48.399898 140096422536960 logging_writer.py:48] [78] global_step=78, grad_norm=3.900757, loss=5.910529
I0315 01:14:48.403169 140125394065216 pytorch_submission_base.py:86] 78) loss = 5.911, grad_norm = 3.901
I0315 01:14:49.228223 140096414144256 logging_writer.py:48] [79] global_step=79, grad_norm=2.978610, loss=5.889291
I0315 01:14:49.231457 140125394065216 pytorch_submission_base.py:86] 79) loss = 5.889, grad_norm = 2.979
I0315 01:14:50.040102 140096422536960 logging_writer.py:48] [80] global_step=80, grad_norm=2.164897, loss=5.872174
I0315 01:14:50.043222 140125394065216 pytorch_submission_base.py:86] 80) loss = 5.872, grad_norm = 2.165
I0315 01:14:50.856975 140096414144256 logging_writer.py:48] [81] global_step=81, grad_norm=2.566572, loss=5.862578
I0315 01:14:50.860157 140125394065216 pytorch_submission_base.py:86] 81) loss = 5.863, grad_norm = 2.567
I0315 01:14:51.666893 140096422536960 logging_writer.py:48] [82] global_step=82, grad_norm=3.279654, loss=5.871302
I0315 01:14:51.670251 140125394065216 pytorch_submission_base.py:86] 82) loss = 5.871, grad_norm = 3.280
I0315 01:14:52.494810 140096414144256 logging_writer.py:48] [83] global_step=83, grad_norm=3.414833, loss=5.886189
I0315 01:14:52.497867 140125394065216 pytorch_submission_base.py:86] 83) loss = 5.886, grad_norm = 3.415
I0315 01:14:53.310554 140096422536960 logging_writer.py:48] [84] global_step=84, grad_norm=2.974976, loss=5.881999
I0315 01:14:53.313688 140125394065216 pytorch_submission_base.py:86] 84) loss = 5.882, grad_norm = 2.975
I0315 01:14:54.121736 140096414144256 logging_writer.py:48] [85] global_step=85, grad_norm=2.518738, loss=5.846506
I0315 01:14:54.124979 140125394065216 pytorch_submission_base.py:86] 85) loss = 5.847, grad_norm = 2.519
I0315 01:14:54.929720 140096422536960 logging_writer.py:48] [86] global_step=86, grad_norm=2.444491, loss=5.843436
I0315 01:14:54.933063 140125394065216 pytorch_submission_base.py:86] 86) loss = 5.843, grad_norm = 2.444
I0315 01:14:55.742134 140096414144256 logging_writer.py:48] [87] global_step=87, grad_norm=3.218467, loss=5.849173
I0315 01:14:55.745328 140125394065216 pytorch_submission_base.py:86] 87) loss = 5.849, grad_norm = 3.218
I0315 01:14:56.547911 140096422536960 logging_writer.py:48] [88] global_step=88, grad_norm=4.070077, loss=5.875806
I0315 01:14:56.551163 140125394065216 pytorch_submission_base.py:86] 88) loss = 5.876, grad_norm = 4.070
I0315 01:14:57.356312 140096414144256 logging_writer.py:48] [89] global_step=89, grad_norm=3.482149, loss=5.887347
I0315 01:14:57.359370 140125394065216 pytorch_submission_base.py:86] 89) loss = 5.887, grad_norm = 3.482
I0315 01:14:58.160208 140096422536960 logging_writer.py:48] [90] global_step=90, grad_norm=1.938993, loss=5.830247
I0315 01:14:58.163270 140125394065216 pytorch_submission_base.py:86] 90) loss = 5.830, grad_norm = 1.939
I0315 01:14:58.966064 140096414144256 logging_writer.py:48] [91] global_step=91, grad_norm=1.183713, loss=5.825585
I0315 01:14:58.969413 140125394065216 pytorch_submission_base.py:86] 91) loss = 5.826, grad_norm = 1.184
I0315 01:14:59.776613 140096422536960 logging_writer.py:48] [92] global_step=92, grad_norm=1.560068, loss=5.828075
I0315 01:14:59.779787 140125394065216 pytorch_submission_base.py:86] 92) loss = 5.828, grad_norm = 1.560
I0315 01:15:00.579572 140096414144256 logging_writer.py:48] [93] global_step=93, grad_norm=2.727552, loss=5.843864
I0315 01:15:00.583002 140125394065216 pytorch_submission_base.py:86] 93) loss = 5.844, grad_norm = 2.728
I0315 01:15:01.399774 140096422536960 logging_writer.py:48] [94] global_step=94, grad_norm=4.214000, loss=5.870817
I0315 01:15:01.403890 140125394065216 pytorch_submission_base.py:86] 94) loss = 5.871, grad_norm = 4.214
I0315 01:15:02.225005 140096414144256 logging_writer.py:48] [95] global_step=95, grad_norm=3.677713, loss=5.862681
I0315 01:15:02.228372 140125394065216 pytorch_submission_base.py:86] 95) loss = 5.863, grad_norm = 3.678
I0315 01:15:03.035456 140096422536960 logging_writer.py:48] [96] global_step=96, grad_norm=3.091944, loss=5.856286
I0315 01:15:03.038842 140125394065216 pytorch_submission_base.py:86] 96) loss = 5.856, grad_norm = 3.092
I0315 01:15:03.835831 140096414144256 logging_writer.py:48] [97] global_step=97, grad_norm=3.701885, loss=5.844363
I0315 01:15:03.838956 140125394065216 pytorch_submission_base.py:86] 97) loss = 5.844, grad_norm = 3.702
I0315 01:15:04.644315 140096422536960 logging_writer.py:48] [98] global_step=98, grad_norm=2.703204, loss=5.840292
I0315 01:15:04.647577 140125394065216 pytorch_submission_base.py:86] 98) loss = 5.840, grad_norm = 2.703
I0315 01:15:05.444035 140096414144256 logging_writer.py:48] [99] global_step=99, grad_norm=1.519481, loss=5.824684
I0315 01:15:05.447183 140125394065216 pytorch_submission_base.py:86] 99) loss = 5.825, grad_norm = 1.519
I0315 01:15:06.248294 140096422536960 logging_writer.py:48] [100] global_step=100, grad_norm=1.430300, loss=5.815164
I0315 01:15:06.251542 140125394065216 pytorch_submission_base.py:86] 100) loss = 5.815, grad_norm = 1.430
I0315 01:20:23.521580 140096414144256 logging_writer.py:48] [500] global_step=500, grad_norm=2.171568, loss=3.143173
I0315 01:20:23.528761 140125394065216 pytorch_submission_base.py:86] 500) loss = 3.143, grad_norm = 2.172
I0315 01:26:58.510337 140096422536960 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.668808, loss=2.586877
I0315 01:26:58.518076 140125394065216 pytorch_submission_base.py:86] 1000) loss = 2.587, grad_norm = 3.669
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 01:33:36.529009 140096422536960 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.842196, loss=2.371786
I0315 01:33:36.536693 140125394065216 pytorch_submission_base.py:86] 1500) loss = 2.372, grad_norm = 6.842
I0315 01:40:10.955524 140096414144256 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.410066, loss=2.185978
I0315 01:40:10.985336 140125394065216 pytorch_submission_base.py:86] 2000) loss = 2.186, grad_norm = 2.410
I0315 01:46:48.980072 140096422536960 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.330304, loss=2.150717
I0315 01:46:48.989387 140125394065216 pytorch_submission_base.py:86] 2500) loss = 2.151, grad_norm = 3.330
I0315 01:53:23.573748 140096414144256 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.279820, loss=2.165542
I0315 01:53:23.579169 140125394065216 pytorch_submission_base.py:86] 3000) loss = 2.166, grad_norm = 2.280
I0315 01:53:45.694721 140125394065216 spec.py:298] Evaluating on the training split.
I0315 01:53:57.009228 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 01:54:06.492669 140125394065216 spec.py:326] Evaluating on the test split.
I0315 01:54:11.831286 140125394065216 submission_runner.py:362] Time since start: 2444.92s, 	Step: 3029, 	{'train/ctc_loss': 0.8536634729083233, 'train/wer': 0.2609438324493404, 'validation/ctc_loss': 1.0798566962715994, 'validation/wer': 0.29901993916863806, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7207238672656301, 'test/wer': 0.22426015071192087, 'test/num_examples': 2472}
I0315 01:54:11.848383 140096422536960 logging_writer.py:48] [3029] global_step=3029, preemption_count=0, score=1479.519024, test/ctc_loss=0.720724, test/num_examples=2472, test/wer=0.224260, total_duration=2444.917552, train/ctc_loss=0.853663, train/wer=0.260944, validation/ctc_loss=1.079857, validation/num_examples=5348, validation/wer=0.299020
I0315 01:54:12.111097 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_3029.
I0315 02:00:27.895179 140096422536960 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.600696, loss=2.130844
I0315 02:00:27.907485 140125394065216 pytorch_submission_base.py:86] 3500) loss = 2.131, grad_norm = 4.601
I0315 02:07:02.117404 140096414144256 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.864800, loss=2.071570
I0315 02:07:02.124883 140125394065216 pytorch_submission_base.py:86] 4000) loss = 2.072, grad_norm = 4.865
I0315 02:13:40.147837 140096422536960 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.773937, loss=2.062156
I0315 02:13:40.153778 140125394065216 pytorch_submission_base.py:86] 4500) loss = 2.062, grad_norm = 3.774
I0315 02:20:14.725276 140096414144256 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.206173, loss=2.036833
I0315 02:20:14.730416 140125394065216 pytorch_submission_base.py:86] 5000) loss = 2.037, grad_norm = 3.206
I0315 02:26:53.304502 140096422536960 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.083575, loss=2.009565
I0315 02:26:53.310989 140125394065216 pytorch_submission_base.py:86] 5500) loss = 2.010, grad_norm = 3.084
I0315 02:33:27.547375 140096414144256 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.091318, loss=2.025964
I0315 02:33:27.552457 140125394065216 pytorch_submission_base.py:86] 6000) loss = 2.026, grad_norm = 5.091
I0315 02:34:12.505992 140125394065216 spec.py:298] Evaluating on the training split.
I0315 02:34:23.642720 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 02:34:33.226080 140125394065216 spec.py:326] Evaluating on the test split.
I0315 02:34:38.683376 140125394065216 submission_runner.py:362] Time since start: 4871.73s, 	Step: 6058, 	{'train/ctc_loss': 0.7318948715212132, 'train/wer': 0.22858697130422956, 'validation/ctc_loss': 0.9625944383225337, 'validation/wer': 0.2694539661082412, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6220618064068649, 'test/wer': 0.1974894887575407, 'test/num_examples': 2472}
I0315 02:34:38.701384 140096422536960 logging_writer.py:48] [6058] global_step=6058, preemption_count=0, score=2918.492120, test/ctc_loss=0.622062, test/num_examples=2472, test/wer=0.197489, total_duration=4871.731914, train/ctc_loss=0.731895, train/wer=0.228587, validation/ctc_loss=0.962594, validation/num_examples=5348, validation/wer=0.269454
I0315 02:34:38.967614 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_6058.
I0315 02:40:31.608766 140096422536960 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.169954, loss=2.582678
I0315 02:40:31.617778 140125394065216 pytorch_submission_base.py:86] 6500) loss = 2.583, grad_norm = 3.170
I0315 02:47:05.665551 140096414144256 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.575635, loss=2.053428
I0315 02:47:05.671563 140125394065216 pytorch_submission_base.py:86] 7000) loss = 2.053, grad_norm = 2.576
I0315 02:53:43.189215 140096422536960 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.821390, loss=2.086177
I0315 02:53:43.195363 140125394065216 pytorch_submission_base.py:86] 7500) loss = 2.086, grad_norm = 2.821
I0315 03:00:17.802436 140096414144256 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.377069, loss=1.985206
I0315 03:00:17.806950 140125394065216 pytorch_submission_base.py:86] 8000) loss = 1.985, grad_norm = 2.377
I0315 03:06:54.896544 140096422536960 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.476572, loss=2.002884
I0315 03:06:54.903097 140125394065216 pytorch_submission_base.py:86] 8500) loss = 2.003, grad_norm = 4.477
I0315 03:13:28.827472 140096414144256 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.715276, loss=2.004413
I0315 03:13:28.832883 140125394065216 pytorch_submission_base.py:86] 9000) loss = 2.004, grad_norm = 2.715
I0315 03:14:39.721046 140125394065216 spec.py:298] Evaluating on the training split.
I0315 03:14:51.078524 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 03:15:00.437362 140125394065216 spec.py:326] Evaluating on the test split.
I0315 03:15:05.863358 140125394065216 submission_runner.py:362] Time since start: 7298.95s, 	Step: 9091, 	{'train/ctc_loss': 0.7632255519302311, 'train/wer': 0.2327158982728138, 'validation/ctc_loss': 0.9753175152576854, 'validation/wer': 0.271655482064404, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6540050971030389, 'test/wer': 0.20354233948774197, 'test/num_examples': 2472}
I0315 03:15:05.880146 140096422536960 logging_writer.py:48] [9091] global_step=9091, preemption_count=0, score=4358.576508, test/ctc_loss=0.654005, test/num_examples=2472, test/wer=0.203542, total_duration=7298.948317, train/ctc_loss=0.763226, train/wer=0.232716, validation/ctc_loss=0.975318, validation/num_examples=5348, validation/wer=0.271655
I0315 03:15:06.157487 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_9091.
I0315 03:20:31.815386 140096422536960 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.958854, loss=2.044179
I0315 03:20:31.824799 140125394065216 pytorch_submission_base.py:86] 9500) loss = 2.044, grad_norm = 2.959
I0315 03:27:05.667950 140096414144256 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.263807, loss=1.979707
I0315 03:27:05.673776 140125394065216 pytorch_submission_base.py:86] 10000) loss = 1.980, grad_norm = 2.264
I0315 03:33:42.707314 140096422536960 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.659196, loss=1.952403
I0315 03:33:42.713804 140125394065216 pytorch_submission_base.py:86] 10500) loss = 1.952, grad_norm = 2.659
I0315 03:40:16.512775 140096414144256 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.999647, loss=1.911119
I0315 03:40:16.517039 140125394065216 pytorch_submission_base.py:86] 11000) loss = 1.911, grad_norm = 3.000
I0315 03:46:52.756282 140096422536960 logging_writer.py:48] [11500] global_step=11500, grad_norm=5.041554, loss=1.965054
I0315 03:46:52.762158 140125394065216 pytorch_submission_base.py:86] 11500) loss = 1.965, grad_norm = 5.042
I0315 03:53:27.360065 140096414144256 logging_writer.py:48] [12000] global_step=12000, grad_norm=8.059411, loss=1.956892
I0315 03:53:27.365077 140125394065216 pytorch_submission_base.py:86] 12000) loss = 1.957, grad_norm = 8.059
I0315 03:55:06.544445 140125394065216 spec.py:298] Evaluating on the training split.
I0315 03:55:17.874224 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 03:55:27.295501 140125394065216 spec.py:326] Evaluating on the test split.
I0315 03:55:32.772730 140125394065216 submission_runner.py:362] Time since start: 9725.77s, 	Step: 12127, 	{'train/ctc_loss': 0.6818992231810697, 'train/wer': 0.21349653202774377, 'validation/ctc_loss': 0.9075155873267028, 'validation/wer': 0.2549992758171197, 'validation/num_examples': 5348, 'test/ctc_loss': 0.581432773888638, 'test/wer': 0.18449007779335, 'test/num_examples': 2472}
I0315 03:55:32.791570 140096422536960 logging_writer.py:48] [12127] global_step=12127, preemption_count=0, score=5798.079732, test/ctc_loss=0.581433, test/num_examples=2472, test/wer=0.184490, total_duration=9725.770421, train/ctc_loss=0.681899, train/wer=0.213497, validation/ctc_loss=0.907516, validation/num_examples=5348, validation/wer=0.254999
I0315 03:55:33.084322 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_12127.
I0315 04:00:30.571684 140096422536960 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.847779, loss=1.912078
I0315 04:00:30.580775 140125394065216 pytorch_submission_base.py:86] 12500) loss = 1.912, grad_norm = 1.848
I0315 04:07:05.591211 140096414144256 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.455139, loss=1.932711
I0315 04:07:05.595381 140125394065216 pytorch_submission_base.py:86] 13000) loss = 1.933, grad_norm = 2.455
I0315 04:13:41.628671 140096422536960 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.565612, loss=1.874194
I0315 04:13:41.636646 140125394065216 pytorch_submission_base.py:86] 13500) loss = 1.874, grad_norm = 2.566
I0315 04:20:16.225119 140096414144256 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.726718, loss=1.893654
I0315 04:20:16.261422 140125394065216 pytorch_submission_base.py:86] 14000) loss = 1.894, grad_norm = 3.727
I0315 04:26:52.246377 140096422536960 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.521617, loss=1.860861
I0315 04:26:52.253300 140125394065216 pytorch_submission_base.py:86] 14500) loss = 1.861, grad_norm = 2.522
I0315 04:33:27.094210 140096414144256 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.770797, loss=1.905147
I0315 04:33:27.134285 140125394065216 pytorch_submission_base.py:86] 15000) loss = 1.905, grad_norm = 1.771
I0315 04:35:33.963726 140125394065216 spec.py:298] Evaluating on the training split.
I0315 04:35:45.528926 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 04:35:55.657694 140125394065216 spec.py:326] Evaluating on the test split.
I0315 04:36:01.125053 140125394065216 submission_runner.py:362] Time since start: 12153.19s, 	Step: 15162, 	{'train/ctc_loss': 0.6765091471678424, 'train/wer': 0.21382292941656467, 'validation/ctc_loss': 0.9095822227559273, 'validation/wer': 0.2548158161541061, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5684614733208594, 'test/wer': 0.18154489874677554, 'test/num_examples': 2472}
I0315 04:36:01.140567 140096422536960 logging_writer.py:48] [15162] global_step=15162, preemption_count=0, score=7238.046769, test/ctc_loss=0.568461, test/num_examples=2472, test/wer=0.181545, total_duration=12153.186944, train/ctc_loss=0.676509, train/wer=0.213823, validation/ctc_loss=0.909582, validation/num_examples=5348, validation/wer=0.254816
I0315 04:36:01.446963 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_15162.
I0315 04:40:30.834917 140096422536960 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.799949, loss=1.851817
I0315 04:40:30.847854 140125394065216 pytorch_submission_base.py:86] 15500) loss = 1.852, grad_norm = 2.800
I0315 04:47:06.586486 140096414144256 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.892567, loss=1.920145
I0315 04:47:06.612994 140125394065216 pytorch_submission_base.py:86] 16000) loss = 1.920, grad_norm = 2.893
I0315 04:53:42.414083 140096422536960 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.510490, loss=1.857600
I0315 04:53:42.422585 140125394065216 pytorch_submission_base.py:86] 16500) loss = 1.858, grad_norm = 2.510
I0315 05:00:17.224984 140096414144256 logging_writer.py:48] [17000] global_step=17000, grad_norm=5.454269, loss=1.877332
I0315 05:00:17.230124 140125394065216 pytorch_submission_base.py:86] 17000) loss = 1.877, grad_norm = 5.454
I0315 05:06:51.089388 140096422536960 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.107736, loss=1.892378
I0315 05:06:51.124418 140125394065216 pytorch_submission_base.py:86] 17500) loss = 1.892, grad_norm = 2.108
I0315 05:13:28.055482 140096422536960 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.096065, loss=1.866309
I0315 05:13:28.065250 140125394065216 pytorch_submission_base.py:86] 18000) loss = 1.866, grad_norm = 3.096
I0315 05:16:02.356151 140125394065216 spec.py:298] Evaluating on the training split.
I0315 05:16:13.709383 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 05:16:23.312834 140125394065216 spec.py:326] Evaluating on the test split.
I0315 05:16:28.779967 140125394065216 submission_runner.py:362] Time since start: 14581.58s, 	Step: 18197, 	{'train/ctc_loss': 0.637365543239617, 'train/wer': 0.20052223582211343, 'validation/ctc_loss': 0.8570463193188668, 'validation/wer': 0.243528218992903, 'validation/num_examples': 5348, 'test/ctc_loss': 0.541136726563004, 'test/wer': 0.17142973209026466, 'test/num_examples': 2472}
I0315 05:16:28.796491 140096422536960 logging_writer.py:48] [18197] global_step=18197, preemption_count=0, score=8677.668782, test/ctc_loss=0.541137, test/num_examples=2472, test/wer=0.171430, total_duration=14581.583610, train/ctc_loss=0.637366, train/wer=0.200522, validation/ctc_loss=0.857046, validation/num_examples=5348, validation/wer=0.243528
I0315 05:16:29.075807 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_18197.
I0315 05:20:28.379622 140096414144256 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.686538, loss=1.862002
I0315 05:20:28.383386 140125394065216 pytorch_submission_base.py:86] 18500) loss = 1.862, grad_norm = 2.687
I0315 05:27:05.858269 140096422536960 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.103616, loss=1.855411
I0315 05:27:05.866308 140125394065216 pytorch_submission_base.py:86] 19000) loss = 1.855, grad_norm = 3.104
I0315 05:33:39.390545 140096414144256 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.044924, loss=1.893848
I0315 05:33:39.397624 140125394065216 pytorch_submission_base.py:86] 19500) loss = 1.894, grad_norm = 3.045
I0315 05:40:16.648728 140096422536960 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.349097, loss=1.915147
I0315 05:40:16.656133 140125394065216 pytorch_submission_base.py:86] 20000) loss = 1.915, grad_norm = 2.349
I0315 05:46:50.761447 140096414144256 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.285623, loss=1.862747
I0315 05:46:50.766245 140125394065216 pytorch_submission_base.py:86] 20500) loss = 1.863, grad_norm = 2.286
I0315 05:53:27.769283 140096422536960 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.868219, loss=1.875848
I0315 05:53:27.778663 140125394065216 pytorch_submission_base.py:86] 21000) loss = 1.876, grad_norm = 2.868
I0315 05:56:29.573923 140125394065216 spec.py:298] Evaluating on the training split.
I0315 05:56:40.883877 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 05:56:50.257935 140125394065216 spec.py:326] Evaluating on the test split.
I0315 05:56:55.746999 140125394065216 submission_runner.py:362] Time since start: 17008.80s, 	Step: 21232, 	{'train/ctc_loss': 0.6446513441652606, 'train/wer': 0.20171902624779, 'validation/ctc_loss': 0.8697257383966245, 'validation/wer': 0.2446289769709844, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5452130480192271, 'test/wer': 0.17082038470131822, 'test/num_examples': 2472}
I0315 05:56:55.766310 140096422536960 logging_writer.py:48] [21232] global_step=21232, preemption_count=0, score=10117.276619, test/ctc_loss=0.545213, test/num_examples=2472, test/wer=0.170820, total_duration=17008.799179, train/ctc_loss=0.644651, train/wer=0.201719, validation/ctc_loss=0.869726, validation/num_examples=5348, validation/wer=0.244629
I0315 05:56:56.048164 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_21232.
I0315 06:00:27.906288 140096414144256 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.882951, loss=1.859809
I0315 06:00:27.910528 140125394065216 pytorch_submission_base.py:86] 21500) loss = 1.860, grad_norm = 3.883
I0315 06:07:05.830137 140096422536960 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.917754, loss=1.906848
I0315 06:07:05.836456 140125394065216 pytorch_submission_base.py:86] 22000) loss = 1.907, grad_norm = 2.918
I0315 06:13:39.424097 140096414144256 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.120503, loss=1.854279
I0315 06:13:39.431212 140125394065216 pytorch_submission_base.py:86] 22500) loss = 1.854, grad_norm = 2.121
I0315 06:20:16.524710 140096422536960 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.154011, loss=1.798339
I0315 06:20:16.530938 140125394065216 pytorch_submission_base.py:86] 23000) loss = 1.798, grad_norm = 3.154
I0315 06:26:50.771316 140096414144256 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.248499, loss=1.827686
I0315 06:26:50.778713 140125394065216 pytorch_submission_base.py:86] 23500) loss = 1.828, grad_norm = 2.248
I0315 06:33:27.693221 140096422536960 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.324735, loss=1.860380
I0315 06:33:27.699588 140125394065216 pytorch_submission_base.py:86] 24000) loss = 1.860, grad_norm = 2.325
I0315 06:36:57.096703 140125394065216 spec.py:298] Evaluating on the training split.
I0315 06:37:08.198378 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 06:37:17.582641 140125394065216 spec.py:326] Evaluating on the test split.
I0315 06:37:23.023241 140125394065216 submission_runner.py:362] Time since start: 19436.32s, 	Step: 24267, 	{'train/ctc_loss': 0.6293176724400805, 'train/wer': 0.19786209710322317, 'validation/ctc_loss': 0.8471805265157223, 'validation/wer': 0.23900931781972673, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5373631968191496, 'test/wer': 0.1695813783437938, 'test/num_examples': 2472}
I0315 06:37:23.043509 140096422536960 logging_writer.py:48] [24267] global_step=24267, preemption_count=0, score=11557.462610, test/ctc_loss=0.537363, test/num_examples=2472, test/wer=0.169581, total_duration=19436.320679, train/ctc_loss=0.629318, train/wer=0.197862, validation/ctc_loss=0.847181, validation/num_examples=5348, validation/wer=0.239009
I0315 06:37:23.322256 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_24267.
I0315 06:40:27.700221 140096414144256 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.108312, loss=1.854011
I0315 06:40:27.703690 140125394065216 pytorch_submission_base.py:86] 24500) loss = 1.854, grad_norm = 3.108
I0315 06:47:04.508665 140096422536960 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.971205, loss=1.877068
I0315 06:47:04.515412 140125394065216 pytorch_submission_base.py:86] 25000) loss = 1.877, grad_norm = 3.971
I0315 06:53:38.416469 140096414144256 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.730867, loss=1.783351
I0315 06:53:38.422171 140125394065216 pytorch_submission_base.py:86] 25500) loss = 1.783, grad_norm = 1.731
I0315 07:00:14.683132 140096422536960 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.595684, loss=1.886864
I0315 07:00:14.690628 140125394065216 pytorch_submission_base.py:86] 26000) loss = 1.887, grad_norm = 1.596
I0315 07:06:48.436615 140096414144256 logging_writer.py:48] [26500] global_step=26500, grad_norm=4.696715, loss=1.892618
I0315 07:06:48.441362 140125394065216 pytorch_submission_base.py:86] 26500) loss = 1.893, grad_norm = 4.697
I0315 07:13:25.174097 140096422536960 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.548560, loss=1.802853
I0315 07:13:25.183166 140125394065216 pytorch_submission_base.py:86] 27000) loss = 1.803, grad_norm = 2.549
I0315 07:17:23.668850 140125394065216 spec.py:298] Evaluating on the training split.
I0315 07:17:35.151893 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 07:17:44.659043 140125394065216 spec.py:326] Evaluating on the test split.
I0315 07:17:50.168999 140125394065216 submission_runner.py:362] Time since start: 21862.89s, 	Step: 27303, 	{'train/ctc_loss': 0.6027879528267366, 'train/wer': 0.19220998232014144, 'validation/ctc_loss': 0.8270796641111613, 'validation/wer': 0.23604499589629702, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5146555826182334, 'test/wer': 0.1643206792192229, 'test/num_examples': 2472}
I0315 07:17:50.187045 140096422536960 logging_writer.py:48] [27303] global_step=27303, preemption_count=0, score=12996.649068, test/ctc_loss=0.514656, test/num_examples=2472, test/wer=0.164321, total_duration=21862.893896, train/ctc_loss=0.602788, train/wer=0.192210, validation/ctc_loss=0.827080, validation/num_examples=5348, validation/wer=0.236045
I0315 07:17:50.462504 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_27303.
I0315 07:20:26.369620 140096414144256 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.173370, loss=1.799004
I0315 07:20:26.373748 140125394065216 pytorch_submission_base.py:86] 27500) loss = 1.799, grad_norm = 2.173
I0315 07:27:02.925915 140096422536960 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.746476, loss=1.784105
I0315 07:27:02.932374 140125394065216 pytorch_submission_base.py:86] 28000) loss = 1.784, grad_norm = 2.746
I0315 07:33:37.174983 140096414144256 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.174197, loss=1.774693
I0315 07:33:37.181200 140125394065216 pytorch_submission_base.py:86] 28500) loss = 1.775, grad_norm = 2.174
I0315 07:40:13.756638 140096422536960 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.338607, loss=1.859916
I0315 07:40:13.762588 140125394065216 pytorch_submission_base.py:86] 29000) loss = 1.860, grad_norm = 2.339
I0315 07:46:48.542772 140096414144256 logging_writer.py:48] [29500] global_step=29500, grad_norm=5.079910, loss=1.833077
I0315 07:46:48.549073 140125394065216 pytorch_submission_base.py:86] 29500) loss = 1.833, grad_norm = 5.080
I0315 07:53:24.798263 140096422536960 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.639791, loss=1.849070
I0315 07:53:24.807927 140125394065216 pytorch_submission_base.py:86] 30000) loss = 1.849, grad_norm = 1.640
I0315 07:57:50.783800 140125394065216 spec.py:298] Evaluating on the training split.
I0315 07:58:02.147591 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 07:58:11.501068 140125394065216 spec.py:326] Evaluating on the test split.
I0315 07:58:16.939044 140125394065216 submission_runner.py:362] Time since start: 24290.01s, 	Step: 30338, 	{'train/ctc_loss': 0.5939442894364145, 'train/wer': 0.19060519515843874, 'validation/ctc_loss': 0.8216068818910739, 'validation/wer': 0.23378554530970888, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5084121858668301, 'test/wer': 0.16257388337090975, 'test/num_examples': 2472}
I0315 07:58:16.955118 140096422536960 logging_writer.py:48] [30338] global_step=30338, preemption_count=0, score=14435.969843, test/ctc_loss=0.508412, test/num_examples=2472, test/wer=0.162574, total_duration=24290.007303, train/ctc_loss=0.593944, train/wer=0.190605, validation/ctc_loss=0.821607, validation/num_examples=5348, validation/wer=0.233786
I0315 07:58:17.232404 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_30338.
I0315 08:00:25.529292 140096414144256 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.850318, loss=1.757017
I0315 08:00:25.532867 140125394065216 pytorch_submission_base.py:86] 30500) loss = 1.757, grad_norm = 1.850
I0315 08:07:01.577870 140096422536960 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.563883, loss=1.857597
I0315 08:07:01.585957 140125394065216 pytorch_submission_base.py:86] 31000) loss = 1.858, grad_norm = 3.564
I0315 08:13:36.452935 140096414144256 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.317327, loss=1.814282
I0315 08:13:36.486768 140125394065216 pytorch_submission_base.py:86] 31500) loss = 1.814, grad_norm = 2.317
I0315 08:20:12.556752 140096422536960 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.836387, loss=1.787587
I0315 08:20:12.564384 140125394065216 pytorch_submission_base.py:86] 32000) loss = 1.788, grad_norm = 2.836
I0315 08:26:47.107418 140096414144256 logging_writer.py:48] [32500] global_step=32500, grad_norm=4.600787, loss=1.840130
I0315 08:26:47.143366 140125394065216 pytorch_submission_base.py:86] 32500) loss = 1.840, grad_norm = 4.601
I0315 08:33:23.349474 140096422536960 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.504911, loss=1.787397
I0315 08:33:23.356921 140125394065216 pytorch_submission_base.py:86] 33000) loss = 1.787, grad_norm = 1.505
I0315 08:38:17.900820 140125394065216 spec.py:298] Evaluating on the training split.
I0315 08:38:29.477803 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 08:38:39.052426 140125394065216 spec.py:326] Evaluating on the test split.
I0315 08:38:44.592160 140125394065216 submission_runner.py:362] Time since start: 26717.13s, 	Step: 33373, 	{'train/ctc_loss': 0.5825310291347333, 'train/wer': 0.1858561131510948, 'validation/ctc_loss': 0.8100194488396625, 'validation/wer': 0.23088881378844203, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5036864777566294, 'test/wer': 0.16121300753559603, 'test/num_examples': 2472}
I0315 08:38:44.608770 140096422536960 logging_writer.py:48] [33373] global_step=33373, preemption_count=0, score=15875.212516, test/ctc_loss=0.503686, test/num_examples=2472, test/wer=0.161213, total_duration=26717.126010, train/ctc_loss=0.582531, train/wer=0.185856, validation/ctc_loss=0.810019, validation/num_examples=5348, validation/wer=0.230889
I0315 08:38:44.890742 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_33373.
I0315 08:40:25.651301 140096414144256 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.290116, loss=1.736729
I0315 08:40:25.655945 140125394065216 pytorch_submission_base.py:86] 33500) loss = 1.737, grad_norm = 2.290
I0315 08:47:01.035317 140096422536960 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.060584, loss=1.798212
I0315 08:47:01.050614 140125394065216 pytorch_submission_base.py:86] 34000) loss = 1.798, grad_norm = 3.061
I0315 08:53:35.717486 140096414144256 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.029347, loss=1.760734
I0315 08:53:35.724800 140125394065216 pytorch_submission_base.py:86] 34500) loss = 1.761, grad_norm = 4.029
I0315 09:00:09.361460 140096422536960 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.892599, loss=1.812185
I0315 09:00:09.367491 140125394065216 pytorch_submission_base.py:86] 35000) loss = 1.812, grad_norm = 3.893
I0315 09:06:46.735838 140096422536960 logging_writer.py:48] [35500] global_step=35500, grad_norm=4.430534, loss=1.702130
I0315 09:06:46.742368 140125394065216 pytorch_submission_base.py:86] 35500) loss = 1.702, grad_norm = 4.431
I0315 09:13:20.548659 140096414144256 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.769305, loss=1.751474
I0315 09:13:20.553791 140125394065216 pytorch_submission_base.py:86] 36000) loss = 1.751, grad_norm = 2.769
I0315 09:18:45.382891 140125394065216 spec.py:298] Evaluating on the training split.
I0315 09:18:57.134756 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 09:19:06.609256 140125394065216 spec.py:326] Evaluating on the test split.
I0315 09:19:12.067917 140125394065216 submission_runner.py:362] Time since start: 29144.61s, 	Step: 36408, 	{'train/ctc_loss': 0.558619981720911, 'train/wer': 0.17851761185910514, 'validation/ctc_loss': 0.7766549369286216, 'validation/wer': 0.22133925553999903, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4827727796954642, 'test/wer': 0.1535352304348709, 'test/num_examples': 2472}
I0315 09:19:12.085773 140096422536960 logging_writer.py:48] [36408] global_step=36408, preemption_count=0, score=17314.363725, test/ctc_loss=0.482773, test/num_examples=2472, test/wer=0.153535, total_duration=29144.606675, train/ctc_loss=0.558620, train/wer=0.178518, validation/ctc_loss=0.776655, validation/num_examples=5348, validation/wer=0.221339
I0315 09:19:12.364646 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_36408.
I0315 09:20:25.582835 140096414144256 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.807499, loss=1.786386
I0315 09:20:25.587322 140125394065216 pytorch_submission_base.py:86] 36500) loss = 1.786, grad_norm = 2.807
I0315 09:26:59.749923 140096422536960 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.141772, loss=1.793671
I0315 09:26:59.754913 140125394065216 pytorch_submission_base.py:86] 37000) loss = 1.794, grad_norm = 4.142
I0315 09:33:37.662504 140096422536960 logging_writer.py:48] [37500] global_step=37500, grad_norm=4.534467, loss=1.789496
I0315 09:33:37.670116 140125394065216 pytorch_submission_base.py:86] 37500) loss = 1.789, grad_norm = 4.534
I0315 09:40:11.940485 140096414144256 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.518822, loss=1.729565
I0315 09:40:11.946812 140125394065216 pytorch_submission_base.py:86] 38000) loss = 1.730, grad_norm = 2.519
I0315 09:46:48.395638 140096422536960 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.185256, loss=1.736829
I0315 09:46:48.402236 140125394065216 pytorch_submission_base.py:86] 38500) loss = 1.737, grad_norm = 3.185
I0315 09:53:22.277165 140096414144256 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.376701, loss=1.741392
I0315 09:53:22.282654 140125394065216 pytorch_submission_base.py:86] 39000) loss = 1.741, grad_norm = 2.377
I0315 09:59:13.010100 140125394065216 spec.py:298] Evaluating on the training split.
I0315 09:59:24.386742 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 09:59:34.409824 140125394065216 spec.py:326] Evaluating on the test split.
I0315 09:59:39.886677 140125394065216 submission_runner.py:362] Time since start: 31572.23s, 	Step: 39441, 	{'train/ctc_loss': 0.5439371001978682, 'train/wer': 0.1756017951856385, 'validation/ctc_loss': 0.7631817297066505, 'validation/wer': 0.21771834113841548, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4663417297083683, 'test/wer': 0.15067129770682267, 'test/num_examples': 2472}
I0315 09:59:39.904050 140096422536960 logging_writer.py:48] [39441] global_step=39441, preemption_count=0, score=18753.205456, test/ctc_loss=0.466342, test/num_examples=2472, test/wer=0.150671, total_duration=31572.232677, train/ctc_loss=0.543937, train/wer=0.175602, validation/ctc_loss=0.763182, validation/num_examples=5348, validation/wer=0.217718
I0315 09:59:40.186820 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_39441.
I0315 10:00:27.437092 140096414144256 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.503831, loss=1.737148
I0315 10:00:27.440916 140125394065216 pytorch_submission_base.py:86] 39500) loss = 1.737, grad_norm = 3.504
I0315 10:07:01.488009 140096422536960 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.258425, loss=1.843257
I0315 10:07:01.492312 140125394065216 pytorch_submission_base.py:86] 40000) loss = 1.843, grad_norm = 2.258
I0315 10:13:39.824626 140096422536960 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.268607, loss=1.715972
I0315 10:13:39.831138 140125394065216 pytorch_submission_base.py:86] 40500) loss = 1.716, grad_norm = 2.269
I0315 10:20:14.052868 140096414144256 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.350392, loss=1.681593
I0315 10:20:14.059649 140125394065216 pytorch_submission_base.py:86] 41000) loss = 1.682, grad_norm = 2.350
I0315 10:26:51.851205 140096422536960 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.572712, loss=1.762262
I0315 10:26:51.857724 140125394065216 pytorch_submission_base.py:86] 41500) loss = 1.762, grad_norm = 2.573
I0315 10:33:26.047039 140096414144256 logging_writer.py:48] [42000] global_step=42000, grad_norm=7.354533, loss=1.801851
I0315 10:33:26.052088 140125394065216 pytorch_submission_base.py:86] 42000) loss = 1.802, grad_norm = 7.355
I0315 10:39:41.137587 140125394065216 spec.py:298] Evaluating on the training split.
I0315 10:39:52.549661 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 10:40:02.001764 140125394065216 spec.py:326] Evaluating on the test split.
I0315 10:40:07.698465 140125394065216 submission_runner.py:362] Time since start: 34000.37s, 	Step: 42473, 	{'train/ctc_loss': 0.5299292024116877, 'train/wer': 0.17026519787841699, 'validation/ctc_loss': 0.7587249001657626, 'validation/wer': 0.21590305605175494, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4633404776114588, 'test/wer': 0.14711677127130177, 'test/num_examples': 2472}
I0315 10:40:07.716364 140096422536960 logging_writer.py:48] [42473] global_step=42473, preemption_count=0, score=20193.167012, test/ctc_loss=0.463340, test/num_examples=2472, test/wer=0.147117, total_duration=34000.366332, train/ctc_loss=0.529929, train/wer=0.170265, validation/ctc_loss=0.758725, validation/num_examples=5348, validation/wer=0.215903
I0315 10:40:08.000651 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_42473.
I0315 10:40:30.118323 140096414144256 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.806184, loss=1.647834
I0315 10:40:30.121599 140125394065216 pytorch_submission_base.py:86] 42500) loss = 1.648, grad_norm = 1.806
I0315 10:47:04.608293 140096422536960 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.205312, loss=1.715511
I0315 10:47:04.612566 140125394065216 pytorch_submission_base.py:86] 43000) loss = 1.716, grad_norm = 3.205
I0315 10:53:42.655436 140096422536960 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.633917, loss=1.700740
I0315 10:53:42.661932 140125394065216 pytorch_submission_base.py:86] 43500) loss = 1.701, grad_norm = 3.634
I0315 11:00:16.362023 140096414144256 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.490844, loss=1.771622
I0315 11:00:16.367960 140125394065216 pytorch_submission_base.py:86] 44000) loss = 1.772, grad_norm = 2.491
I0315 11:06:53.381057 140096422536960 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.855048, loss=1.723690
I0315 11:06:53.387880 140125394065216 pytorch_submission_base.py:86] 44500) loss = 1.724, grad_norm = 2.855
I0315 11:13:27.615204 140096414144256 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.933838, loss=1.711077
I0315 11:13:27.620620 140125394065216 pytorch_submission_base.py:86] 45000) loss = 1.711, grad_norm = 2.934
I0315 11:20:04.586337 140096422536960 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.399415, loss=1.733615
I0315 11:20:04.595721 140125394065216 pytorch_submission_base.py:86] 45500) loss = 1.734, grad_norm = 3.399
I0315 11:20:08.556668 140125394065216 spec.py:298] Evaluating on the training split.
I0315 11:20:20.146450 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 11:20:30.212593 140125394065216 spec.py:326] Evaluating on the test split.
I0315 11:20:35.988682 140125394065216 submission_runner.py:362] Time since start: 36427.78s, 	Step: 45506, 	{'train/ctc_loss': 0.5078471756351094, 'train/wer': 0.1632530939752482, 'validation/ctc_loss': 0.7319133345420182, 'validation/wer': 0.20800463477043402, 'validation/num_examples': 5348, 'test/ctc_loss': 0.44068145525517777, 'test/wer': 0.1422623037393618, 'test/num_examples': 2472}
I0315 11:20:36.007417 140096422536960 logging_writer.py:48] [45506] global_step=45506, preemption_count=0, score=21633.578391, test/ctc_loss=0.440681, test/num_examples=2472, test/wer=0.142262, total_duration=36427.782433, train/ctc_loss=0.507847, train/wer=0.163253, validation/ctc_loss=0.731913, validation/num_examples=5348, validation/wer=0.208005
I0315 11:20:36.294039 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_45506.
I0315 11:27:06.616376 140096414144256 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.024695, loss=1.672114
I0315 11:27:06.620815 140125394065216 pytorch_submission_base.py:86] 46000) loss = 1.672, grad_norm = 3.025
I0315 11:33:43.183512 140096422536960 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.082778, loss=1.734643
I0315 11:33:43.190194 140125394065216 pytorch_submission_base.py:86] 46500) loss = 1.735, grad_norm = 3.083
I0315 11:40:17.692197 140096414144256 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.644012, loss=1.705812
I0315 11:40:17.697110 140125394065216 pytorch_submission_base.py:86] 47000) loss = 1.706, grad_norm = 2.644
I0315 11:46:53.828189 140096422536960 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.050293, loss=1.673591
I0315 11:46:53.836611 140125394065216 pytorch_submission_base.py:86] 47500) loss = 1.674, grad_norm = 3.050
I0315 11:53:28.412971 140096414144256 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.807544, loss=1.697258
I0315 11:53:28.445008 140125394065216 pytorch_submission_base.py:86] 48000) loss = 1.697, grad_norm = 1.808
I0315 12:00:04.571648 140096422536960 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.521651, loss=1.670755
I0315 12:00:04.580317 140125394065216 pytorch_submission_base.py:86] 48500) loss = 1.671, grad_norm = 2.522
I0315 12:00:37.031652 140125394065216 spec.py:298] Evaluating on the training split.
I0315 12:00:48.770687 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 12:00:58.306730 140125394065216 spec.py:326] Evaluating on the test split.
I0315 12:01:03.664659 140125394065216 submission_runner.py:362] Time since start: 38856.26s, 	Step: 48542, 	{'train/ctc_loss': 0.487724931051073, 'train/wer': 0.1580361757105943, 'validation/ctc_loss': 0.7048034729285966, 'validation/wer': 0.20176700622797278, 'validation/num_examples': 5348, 'test/ctc_loss': 0.42862452819536745, 'test/wer': 0.1388296467816302, 'test/num_examples': 2472}
I0315 12:01:03.682279 140096422536960 logging_writer.py:48] [48542] global_step=48542, preemption_count=0, score=23072.637334, test/ctc_loss=0.428625, test/num_examples=2472, test/wer=0.138830, total_duration=38856.257021, train/ctc_loss=0.487725, train/wer=0.158036, validation/ctc_loss=0.704803, validation/num_examples=5348, validation/wer=0.201767
I0315 12:01:03.961421 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_48542.
I0315 12:07:06.323702 140096414144256 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.650969, loss=1.723448
I0315 12:07:06.327831 140125394065216 pytorch_submission_base.py:86] 49000) loss = 1.723, grad_norm = 1.651
I0315 12:13:42.147886 140096422536960 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.893931, loss=1.611443
I0315 12:13:42.154731 140125394065216 pytorch_submission_base.py:86] 49500) loss = 1.611, grad_norm = 1.894
I0315 12:20:17.168021 140096414144256 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.096478, loss=1.691689
I0315 12:20:17.202663 140125394065216 pytorch_submission_base.py:86] 50000) loss = 1.692, grad_norm = 3.096
I0315 12:26:53.003255 140096422536960 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.183252, loss=1.620144
I0315 12:26:53.012587 140125394065216 pytorch_submission_base.py:86] 50500) loss = 1.620, grad_norm = 3.183
I0315 12:33:27.794072 140096414144256 logging_writer.py:48] [51000] global_step=51000, grad_norm=2.703752, loss=1.645517
I0315 12:33:27.798314 140125394065216 pytorch_submission_base.py:86] 51000) loss = 1.646, grad_norm = 2.704
I0315 12:40:03.891802 140096422536960 logging_writer.py:48] [51500] global_step=51500, grad_norm=2.402792, loss=1.698634
I0315 12:40:03.981117 140125394065216 pytorch_submission_base.py:86] 51500) loss = 1.699, grad_norm = 2.403
I0315 12:41:04.411486 140125394065216 spec.py:298] Evaluating on the training split.
I0315 12:41:15.878424 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 12:41:25.371208 140125394065216 spec.py:326] Evaluating on the test split.
I0315 12:41:30.895837 140125394065216 submission_runner.py:362] Time since start: 41283.63s, 	Step: 51577, 	{'train/ctc_loss': 0.4768412285783985, 'train/wer': 0.15413028695770434, 'validation/ctc_loss': 0.6987997817303345, 'validation/wer': 0.19901511128276927, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4167082604119621, 'test/wer': 0.13405642556821645, 'test/num_examples': 2472}
I0315 12:41:30.911975 140096422536960 logging_writer.py:48] [51577] global_step=51577, preemption_count=0, score=24512.323668, test/ctc_loss=0.416708, test/num_examples=2472, test/wer=0.134056, total_duration=41283.634703, train/ctc_loss=0.476841, train/wer=0.154130, validation/ctc_loss=0.698800, validation/num_examples=5348, validation/wer=0.199015
I0315 12:41:31.189543 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_51577.
I0315 12:47:06.119269 140096414144256 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.346961, loss=1.659570
I0315 12:47:06.124608 140125394065216 pytorch_submission_base.py:86] 52000) loss = 1.660, grad_norm = 3.347
I0315 12:53:39.659463 140096422536960 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.488473, loss=1.579443
I0315 12:53:39.664003 140125394065216 pytorch_submission_base.py:86] 52500) loss = 1.579, grad_norm = 1.488
I0315 13:00:17.791754 140096422536960 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.716947, loss=1.697119
I0315 13:00:17.799344 140125394065216 pytorch_submission_base.py:86] 53000) loss = 1.697, grad_norm = 2.717
I0315 13:06:51.461616 140096414144256 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.862983, loss=1.619798
I0315 13:06:51.466099 140125394065216 pytorch_submission_base.py:86] 53500) loss = 1.620, grad_norm = 1.863
I0315 13:13:28.596624 140096422536960 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.710508, loss=1.591754
I0315 13:13:28.605328 140125394065216 pytorch_submission_base.py:86] 54000) loss = 1.592, grad_norm = 2.711
I0315 13:20:02.104324 140096414144256 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.898150, loss=1.568943
I0315 13:20:02.113929 140125394065216 pytorch_submission_base.py:86] 54500) loss = 1.569, grad_norm = 2.898
I0315 13:21:31.691138 140125394065216 spec.py:298] Evaluating on the training split.
I0315 13:21:42.934573 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 13:21:52.317076 140125394065216 spec.py:326] Evaluating on the test split.
I0315 13:21:57.759162 140125394065216 submission_runner.py:362] Time since start: 43710.92s, 	Step: 54612, 	{'train/ctc_loss': 0.46292978729171325, 'train/wer': 0.15136134910920712, 'validation/ctc_loss': 0.6821459769627788, 'validation/wer': 0.19615700284845267, 'validation/num_examples': 5348, 'test/ctc_loss': 0.40861560120814244, 'test/wer': 0.12936445067332886, 'test/num_examples': 2472}
I0315 13:21:57.776483 140096422536960 logging_writer.py:48] [54612] global_step=54612, preemption_count=0, score=25951.820009, test/ctc_loss=0.408616, test/num_examples=2472, test/wer=0.129364, total_duration=43710.916138, train/ctc_loss=0.462930, train/wer=0.151361, validation/ctc_loss=0.682146, validation/num_examples=5348, validation/wer=0.196157
I0315 13:21:58.057170 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_54612.
I0315 13:27:06.088785 140096414144256 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.688089, loss=1.650956
I0315 13:27:06.094108 140125394065216 pytorch_submission_base.py:86] 55000) loss = 1.651, grad_norm = 2.688
I0315 13:33:39.872724 140096422536960 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.118903, loss=1.646918
I0315 13:33:39.878619 140125394065216 pytorch_submission_base.py:86] 55500) loss = 1.647, grad_norm = 3.119
I0315 13:40:17.883250 140096422536960 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.675380, loss=1.619704
I0315 13:40:17.890767 140125394065216 pytorch_submission_base.py:86] 56000) loss = 1.620, grad_norm = 2.675
I0315 13:46:51.555847 140096414144256 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.762711, loss=1.639343
I0315 13:46:51.561204 140125394065216 pytorch_submission_base.py:86] 56500) loss = 1.639, grad_norm = 2.763
I0315 13:53:29.000104 140096422536960 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.800912, loss=1.600403
I0315 13:53:29.008313 140125394065216 pytorch_submission_base.py:86] 57000) loss = 1.600, grad_norm = 2.801
I0315 14:00:02.723315 140096414144256 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.433638, loss=1.614684
I0315 14:00:02.731400 140125394065216 pytorch_submission_base.py:86] 57500) loss = 1.615, grad_norm = 3.434
I0315 14:01:59.127900 140125394065216 spec.py:298] Evaluating on the training split.
I0315 14:02:10.597985 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 14:02:20.075630 140125394065216 spec.py:326] Evaluating on the test split.
I0315 14:02:25.563923 140125394065216 submission_runner.py:362] Time since start: 46138.35s, 	Step: 57649, 	{'train/ctc_loss': 0.45433552377467423, 'train/wer': 0.1489188086495308, 'validation/ctc_loss': 0.6763871621961021, 'validation/wer': 0.19435137353352966, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4038598760403897, 'test/wer': 0.13007535596043304, 'test/num_examples': 2472}
I0315 14:02:25.582245 140096422536960 logging_writer.py:48] [57649] global_step=57649, preemption_count=0, score=27391.152173, test/ctc_loss=0.403860, test/num_examples=2472, test/wer=0.130075, total_duration=46138.351587, train/ctc_loss=0.454336, train/wer=0.148919, validation/ctc_loss=0.676387, validation/num_examples=5348, validation/wer=0.194351
I0315 14:02:25.869850 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_57649.
I0315 14:07:07.338017 140096422536960 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.950815, loss=1.558491
I0315 14:07:07.345969 140125394065216 pytorch_submission_base.py:86] 58000) loss = 1.558, grad_norm = 1.951
I0315 14:13:40.972568 140096414144256 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.029541, loss=1.616690
I0315 14:13:40.978611 140125394065216 pytorch_submission_base.py:86] 58500) loss = 1.617, grad_norm = 3.030
I0315 14:20:18.447154 140096422536960 logging_writer.py:48] [59000] global_step=59000, grad_norm=2.105657, loss=1.594777
I0315 14:20:18.454231 140125394065216 pytorch_submission_base.py:86] 59000) loss = 1.595, grad_norm = 2.106
I0315 14:26:52.416311 140096414144256 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.515342, loss=1.593520
I0315 14:26:52.421082 140125394065216 pytorch_submission_base.py:86] 59500) loss = 1.594, grad_norm = 1.515
I0315 14:33:29.513916 140096422536960 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.396349, loss=1.624240
I0315 14:33:29.521966 140125394065216 pytorch_submission_base.py:86] 60000) loss = 1.624, grad_norm = 3.396
I0315 14:40:03.549744 140096414144256 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.773099, loss=1.576327
I0315 14:40:03.556292 140125394065216 pytorch_submission_base.py:86] 60500) loss = 1.576, grad_norm = 1.773
I0315 14:42:26.948465 140125394065216 spec.py:298] Evaluating on the training split.
I0315 14:42:38.457931 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 14:42:47.989507 140125394065216 spec.py:326] Evaluating on the test split.
I0315 14:42:53.289267 140125394065216 submission_runner.py:362] Time since start: 48566.17s, 	Step: 60683, 	{'train/ctc_loss': 0.4241881030913514, 'train/wer': 0.1388548891608867, 'validation/ctc_loss': 0.6411154396159835, 'validation/wer': 0.18568049051320426, 'validation/num_examples': 5348, 'test/ctc_loss': 0.37723472078843795, 'test/wer': 0.12195072410781387, 'test/num_examples': 2472}
I0315 14:42:53.307373 140096422536960 logging_writer.py:48] [60683] global_step=60683, preemption_count=0, score=28831.753898, test/ctc_loss=0.377235, test/num_examples=2472, test/wer=0.121951, total_duration=48566.173623, train/ctc_loss=0.424188, train/wer=0.138855, validation/ctc_loss=0.641115, validation/num_examples=5348, validation/wer=0.185680
I0315 14:42:53.595942 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_60683.
I0315 14:47:07.367088 140096422536960 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.368947, loss=1.590071
I0315 14:47:07.373652 140125394065216 pytorch_submission_base.py:86] 61000) loss = 1.590, grad_norm = 2.369
I0315 14:53:40.903465 140096414144256 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.089565, loss=1.578028
I0315 14:53:40.907787 140125394065216 pytorch_submission_base.py:86] 61500) loss = 1.578, grad_norm = 2.090
I0315 15:00:17.918330 140096422536960 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.775753, loss=1.544853
I0315 15:00:17.924672 140125394065216 pytorch_submission_base.py:86] 62000) loss = 1.545, grad_norm = 2.776
I0315 15:06:51.936978 140096414144256 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.540830, loss=1.557404
I0315 15:06:51.942913 140125394065216 pytorch_submission_base.py:86] 62500) loss = 1.557, grad_norm = 2.541
I0315 15:13:27.927460 140096422536960 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.154906, loss=1.519105
I0315 15:13:27.934175 140125394065216 pytorch_submission_base.py:86] 63000) loss = 1.519, grad_norm = 2.155
I0315 15:20:02.246705 140096414144256 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.937680, loss=1.604858
I0315 15:20:02.253347 140125394065216 pytorch_submission_base.py:86] 63500) loss = 1.605, grad_norm = 1.938
I0315 15:22:54.161016 140125394065216 spec.py:298] Evaluating on the training split.
I0315 15:23:05.589421 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 15:23:15.674277 140125394065216 spec.py:326] Evaluating on the test split.
I0315 15:23:21.155449 140125394065216 submission_runner.py:362] Time since start: 50993.39s, 	Step: 63719, 	{'train/ctc_loss': 0.4074829967626192, 'train/wer': 0.13338229294165646, 'validation/ctc_loss': 0.6254113649349508, 'validation/wer': 0.179529763916381, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3700274765710691, 'test/wer': 0.11839619767229298, 'test/num_examples': 2472}
I0315 15:23:21.174259 140096422536960 logging_writer.py:48] [63719] global_step=63719, preemption_count=0, score=30271.745222, test/ctc_loss=0.370027, test/num_examples=2472, test/wer=0.118396, total_duration=50993.386365, train/ctc_loss=0.407483, train/wer=0.133382, validation/ctc_loss=0.625411, validation/num_examples=5348, validation/wer=0.179530
I0315 15:23:21.455458 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_63719.
I0315 15:27:05.888207 140096422536960 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.863292, loss=1.543775
I0315 15:27:05.896748 140125394065216 pytorch_submission_base.py:86] 64000) loss = 1.544, grad_norm = 2.863
I0315 15:33:40.108154 140096414144256 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.465634, loss=1.568388
I0315 15:33:40.112496 140125394065216 pytorch_submission_base.py:86] 64500) loss = 1.568, grad_norm = 2.466
I0315 15:40:16.197330 140096422536960 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.736649, loss=1.479081
I0315 15:40:16.203420 140125394065216 pytorch_submission_base.py:86] 65000) loss = 1.479, grad_norm = 2.737
I0315 15:46:52.358892 140096414144256 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.100237, loss=1.533295
I0315 15:46:52.393193 140125394065216 pytorch_submission_base.py:86] 65500) loss = 1.533, grad_norm = 3.100
I0315 15:53:29.156209 140096422536960 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.806619, loss=1.469384
I0315 15:53:29.165281 140125394065216 pytorch_submission_base.py:86] 66000) loss = 1.469, grad_norm = 2.807
I0315 16:00:04.221797 140096414144256 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.961111, loss=1.580304
I0315 16:00:04.257672 140125394065216 pytorch_submission_base.py:86] 66500) loss = 1.580, grad_norm = 2.961
I0315 16:03:22.478620 140125394065216 spec.py:298] Evaluating on the training split.
I0315 16:03:33.840959 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 16:03:43.245100 140125394065216 spec.py:326] Evaluating on the test split.
I0315 16:03:48.592751 140125394065216 submission_runner.py:362] Time since start: 53421.70s, 	Step: 66753, 	{'train/ctc_loss': 0.387560203255338, 'train/wer': 0.12758873929008568, 'validation/ctc_loss': 0.6150465971594334, 'validation/wer': 0.17609230917781105, 'validation/num_examples': 5348, 'test/ctc_loss': 0.35035581932705334, 'test/wer': 0.11283082485324884, 'test/num_examples': 2472}
I0315 16:03:48.609521 140096422536960 logging_writer.py:48] [66753] global_step=66753, preemption_count=0, score=31712.431437, test/ctc_loss=0.350356, test/num_examples=2472, test/wer=0.112831, total_duration=53421.703681, train/ctc_loss=0.387560, train/wer=0.127589, validation/ctc_loss=0.615047, validation/num_examples=5348, validation/wer=0.176092
I0315 16:03:48.889727 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_66753.
I0315 16:07:06.405094 140096422536960 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.723665, loss=1.525988
I0315 16:07:06.414732 140125394065216 pytorch_submission_base.py:86] 67000) loss = 1.526, grad_norm = 1.724
I0315 16:13:40.637032 140096414144256 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.194222, loss=1.531114
I0315 16:13:40.663333 140125394065216 pytorch_submission_base.py:86] 67500) loss = 1.531, grad_norm = 2.194
I0315 16:20:16.383647 140096422536960 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.153937, loss=1.496049
I0315 16:20:16.391492 140125394065216 pytorch_submission_base.py:86] 68000) loss = 1.496, grad_norm = 2.154
I0315 16:26:52.367824 140096414144256 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.249297, loss=1.476082
I0315 16:26:52.371947 140125394065216 pytorch_submission_base.py:86] 68500) loss = 1.476, grad_norm = 2.249
I0315 16:33:26.261707 140096422536960 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.235028, loss=1.501730
I0315 16:33:26.296476 140125394065216 pytorch_submission_base.py:86] 69000) loss = 1.502, grad_norm = 2.235
I0315 16:40:03.494834 140096422536960 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.004306, loss=1.435885
I0315 16:40:03.504319 140125394065216 pytorch_submission_base.py:86] 69500) loss = 1.436, grad_norm = 2.004
I0315 16:43:49.291088 140125394065216 spec.py:298] Evaluating on the training split.
I0315 16:44:00.588646 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 16:44:10.223700 140125394065216 spec.py:326] Evaluating on the test split.
I0315 16:44:15.482040 140125394065216 submission_runner.py:362] Time since start: 55848.52s, 	Step: 69788, 	{'train/ctc_loss': 0.368376556644847, 'train/wer': 0.12348157214742282, 'validation/ctc_loss': 0.5953523414739049, 'validation/wer': 0.1713706367981461, 'validation/num_examples': 5348, 'test/ctc_loss': 0.33779593643138267, 'test/wer': 0.11000751528446367, 'test/num_examples': 2472}
I0315 16:44:15.499912 140096422536960 logging_writer.py:48] [69788] global_step=69788, preemption_count=0, score=33152.590007, test/ctc_loss=0.337796, test/num_examples=2472, test/wer=0.110008, total_duration=55848.518532, train/ctc_loss=0.368377, train/wer=0.123482, validation/ctc_loss=0.595352, validation/num_examples=5348, validation/wer=0.171371
I0315 16:44:15.777621 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_69788.
I0315 16:47:03.208972 140096414144256 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.018507, loss=1.542163
I0315 16:47:03.213054 140125394065216 pytorch_submission_base.py:86] 70000) loss = 1.542, grad_norm = 2.019
I0315 16:53:41.021408 140096422536960 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.238578, loss=1.509658
I0315 16:53:41.031322 140125394065216 pytorch_submission_base.py:86] 70500) loss = 1.510, grad_norm = 2.239
I0315 17:00:14.469386 140096414144256 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.912930, loss=1.499670
I0315 17:00:14.474994 140125394065216 pytorch_submission_base.py:86] 71000) loss = 1.500, grad_norm = 2.913
I0315 17:06:51.639888 140096422536960 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.705622, loss=1.458137
I0315 17:06:51.647416 140125394065216 pytorch_submission_base.py:86] 71500) loss = 1.458, grad_norm = 1.706
I0315 17:13:25.101456 140096414144256 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.646877, loss=1.512445
I0315 17:13:25.108807 140125394065216 pytorch_submission_base.py:86] 72000) loss = 1.512, grad_norm = 3.647
I0315 17:20:01.746704 140096422536960 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.983688, loss=1.392295
I0315 17:20:01.757073 140125394065216 pytorch_submission_base.py:86] 72500) loss = 1.392, grad_norm = 1.984
I0315 17:24:16.321374 140125394065216 spec.py:298] Evaluating on the training split.
I0315 17:24:27.723657 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 17:24:37.333142 140125394065216 spec.py:326] Evaluating on the test split.
I0315 17:24:42.779311 140125394065216 submission_runner.py:362] Time since start: 58275.55s, 	Step: 72824, 	{'train/ctc_loss': 0.3497983042714453, 'train/wer': 0.11638242894056848, 'validation/ctc_loss': 0.5706440053935604, 'validation/wer': 0.16649447207068024, 'validation/num_examples': 5348, 'test/ctc_loss': 0.32754323363604104, 'test/wer': 0.10543740986736538, 'test/num_examples': 2472}
I0315 17:24:42.797850 140096422536960 logging_writer.py:48] [72824] global_step=72824, preemption_count=0, score=34592.600947, test/ctc_loss=0.327543, test/num_examples=2472, test/wer=0.105437, total_duration=58275.548144, train/ctc_loss=0.349798, train/wer=0.116382, validation/ctc_loss=0.570644, validation/num_examples=5348, validation/wer=0.166494
I0315 17:24:43.078381 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_72824.
I0315 17:27:02.671379 140096414144256 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.439699, loss=1.505416
I0315 17:27:02.677278 140125394065216 pytorch_submission_base.py:86] 73000) loss = 1.505, grad_norm = 2.440
I0315 17:33:40.283306 140096422536960 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.404652, loss=1.459132
I0315 17:33:40.294027 140125394065216 pytorch_submission_base.py:86] 73500) loss = 1.459, grad_norm = 1.405
I0315 17:40:13.980890 140096414144256 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.288653, loss=1.468502
I0315 17:40:13.985367 140125394065216 pytorch_submission_base.py:86] 74000) loss = 1.469, grad_norm = 2.289
I0315 17:46:51.998413 140096422536960 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.627802, loss=1.502487
I0315 17:46:52.004437 140125394065216 pytorch_submission_base.py:86] 74500) loss = 1.502, grad_norm = 2.628
I0315 17:53:25.734245 140096414144256 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.007827, loss=1.478801
I0315 17:53:25.739497 140125394065216 pytorch_submission_base.py:86] 75000) loss = 1.479, grad_norm = 2.008
I0315 18:00:02.548967 140096422536960 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.703595, loss=1.427808
I0315 18:00:02.557537 140125394065216 pytorch_submission_base.py:86] 75500) loss = 1.428, grad_norm = 1.704
I0315 18:04:43.573568 140125394065216 spec.py:298] Evaluating on the training split.
I0315 18:04:55.316153 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 18:05:05.008627 140125394065216 spec.py:326] Evaluating on the test split.
I0315 18:05:10.440670 140125394065216 submission_runner.py:362] Time since start: 60702.80s, 	Step: 75858, 	{'train/ctc_loss': 0.33354638996462765, 'train/wer': 0.11156806745546036, 'validation/ctc_loss': 0.5579576206801286, 'validation/wer': 0.1613093226476126, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3148453882105297, 'test/wer': 0.10139540552068735, 'test/num_examples': 2472}
I0315 18:05:10.461059 140096422536960 logging_writer.py:48] [75858] global_step=75858, preemption_count=0, score=36032.993739, test/ctc_loss=0.314845, test/num_examples=2472, test/wer=0.101395, total_duration=60702.796386, train/ctc_loss=0.333546, train/wer=0.111568, validation/ctc_loss=0.557958, validation/num_examples=5348, validation/wer=0.161309
I0315 18:05:10.752226 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_75858.
I0315 18:07:03.236461 140096414144256 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.567395, loss=1.485709
I0315 18:07:03.240526 140125394065216 pytorch_submission_base.py:86] 76000) loss = 1.486, grad_norm = 3.567
I0315 18:13:40.325755 140096422536960 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.743029, loss=1.409061
I0315 18:13:40.332381 140125394065216 pytorch_submission_base.py:86] 76500) loss = 1.409, grad_norm = 1.743
I0315 18:20:13.626292 140096414144256 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.175135, loss=1.432169
I0315 18:20:13.630757 140125394065216 pytorch_submission_base.py:86] 77000) loss = 1.432, grad_norm = 2.175
I0315 18:26:50.665658 140096422536960 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.072401, loss=1.389637
I0315 18:26:50.671786 140125394065216 pytorch_submission_base.py:86] 77500) loss = 1.390, grad_norm = 3.072
I0315 18:33:24.275029 140096414144256 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.916100, loss=1.382359
I0315 18:33:24.280141 140125394065216 pytorch_submission_base.py:86] 78000) loss = 1.382, grad_norm = 2.916
I0315 18:40:01.240190 140096422536960 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.386767, loss=1.402114
I0315 18:40:01.247114 140125394065216 pytorch_submission_base.py:86] 78500) loss = 1.402, grad_norm = 2.387
I0315 18:45:11.257640 140125394065216 spec.py:298] Evaluating on the training split.
I0315 18:45:22.587762 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 18:45:32.174602 140125394065216 spec.py:326] Evaluating on the test split.
I0315 18:45:37.594135 140125394065216 submission_runner.py:362] Time since start: 63130.48s, 	Step: 78894, 	{'train/ctc_loss': 0.3119584248134821, 'train/wer': 0.1061281109751122, 'validation/ctc_loss': 0.5377285807684097, 'validation/wer': 0.156645584898373, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3012144684737725, 'test/wer': 0.09712997379806228, 'test/num_examples': 2472}
I0315 18:45:37.614476 140096422536960 logging_writer.py:48] [78894] global_step=78894, preemption_count=0, score=37472.940475, test/ctc_loss=0.301214, test/num_examples=2472, test/wer=0.097130, total_duration=63130.484990, train/ctc_loss=0.311958, train/wer=0.106128, validation/ctc_loss=0.537729, validation/num_examples=5348, validation/wer=0.156646
I0315 18:45:37.896112 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_78894.
I0315 18:47:02.180007 140096414144256 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.274056, loss=1.448207
I0315 18:47:02.184530 140125394065216 pytorch_submission_base.py:86] 79000) loss = 1.448, grad_norm = 2.274
I0315 18:53:38.585851 140096422536960 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.767649, loss=1.381126
I0315 18:53:38.595539 140125394065216 pytorch_submission_base.py:86] 79500) loss = 1.381, grad_norm = 2.768
I0315 19:00:12.425789 140125394065216 spec.py:298] Evaluating on the training split.
I0315 19:00:23.582598 140125394065216 spec.py:310] Evaluating on the validation split.
I0315 19:00:33.100479 140125394065216 spec.py:326] Evaluating on the test split.
I0315 19:00:38.406672 140125394065216 submission_runner.py:362] Time since start: 64031.65s, 	Step: 80000, 	{'train/ctc_loss': 0.30590943232332474, 'train/wer': 0.1034407724738202, 'validation/ctc_loss': 0.5324165477603224, 'validation/wer': 0.15304398204026456, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2977673962836312, 'test/wer': 0.09457071476448724, 'test/num_examples': 2472}
I0315 19:00:38.425676 140096422536960 logging_writer.py:48] [80000] global_step=80000, preemption_count=0, score=37997.330496, test/ctc_loss=0.297767, test/num_examples=2472, test/wer=0.094571, total_duration=64031.649178, train/ctc_loss=0.305909, train/wer=0.103441, validation/ctc_loss=0.532417, validation/num_examples=5348, validation/wer=0.153044
I0315 19:00:38.705951 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_80000.
I0315 19:00:38.717077 140096414144256 logging_writer.py:48] [80000] global_step=80000, preemption_count=0, score=37997.330496
I0315 19:00:39.282444 140125394065216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing/librispeech_deepspeech_pytorch/trial_1/checkpoint_80000.
I0315 19:00:39.430901 140125394065216 submission_runner.py:523] Tuning trial 1/1
I0315 19:00:39.431194 140125394065216 submission_runner.py:524] Hyperparameters: Hyperparameters(learning_rate=0.004958460849689891, beta1=0.863744242567442, beta2=0.6291854735396584, warmup_steps=1200, weight_decay=0.1147386261512052)
I0315 19:00:39.432048 140125394065216 submission_runner.py:525] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.93918941811849, 'train/wer': 4.302303821569428, 'validation/ctc_loss': 31.023385699216394, 'validation/wer': 3.8533771061652105, 'validation/num_examples': 5348, 'test/ctc_loss': 31.23632815020324, 'test/wer': 4.29628501208539, 'test/num_examples': 2472, 'score': 8.334447383880615, 'total_duration': 8.336068153381348, 'global_step': 1, 'preemption_count': 0}), (3029, {'train/ctc_loss': 0.8536634729083233, 'train/wer': 0.2609438324493404, 'validation/ctc_loss': 1.0798566962715994, 'validation/wer': 0.29901993916863806, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7207238672656301, 'test/wer': 0.22426015071192087, 'test/num_examples': 2472, 'score': 1479.519023656845, 'total_duration': 2444.9175519943237, 'global_step': 3029, 'preemption_count': 0}), (6058, {'train/ctc_loss': 0.7318948715212132, 'train/wer': 0.22858697130422956, 'validation/ctc_loss': 0.9625944383225337, 'validation/wer': 0.2694539661082412, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6220618064068649, 'test/wer': 0.1974894887575407, 'test/num_examples': 2472, 'score': 2918.4921197891235, 'total_duration': 4871.731914281845, 'global_step': 6058, 'preemption_count': 0}), (9091, {'train/ctc_loss': 0.7632255519302311, 'train/wer': 0.2327158982728138, 'validation/ctc_loss': 0.9753175152576854, 'validation/wer': 0.271655482064404, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6540050971030389, 'test/wer': 0.20354233948774197, 'test/num_examples': 2472, 'score': 4358.576508283615, 'total_duration': 7298.948316574097, 'global_step': 9091, 'preemption_count': 0}), (12127, {'train/ctc_loss': 0.6818992231810697, 'train/wer': 0.21349653202774377, 'validation/ctc_loss': 0.9075155873267028, 'validation/wer': 0.2549992758171197, 'validation/num_examples': 5348, 'test/ctc_loss': 0.581432773888638, 'test/wer': 0.18449007779335, 'test/num_examples': 2472, 'score': 5798.079731702805, 'total_duration': 9725.770420789719, 'global_step': 12127, 'preemption_count': 0}), (15162, {'train/ctc_loss': 0.6765091471678424, 'train/wer': 0.21382292941656467, 'validation/ctc_loss': 0.9095822227559273, 'validation/wer': 0.2548158161541061, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5684614733208594, 'test/wer': 0.18154489874677554, 'test/num_examples': 2472, 'score': 7238.046768665314, 'total_duration': 12153.186943769455, 'global_step': 15162, 'preemption_count': 0}), (18197, {'train/ctc_loss': 0.637365543239617, 'train/wer': 0.20052223582211343, 'validation/ctc_loss': 0.8570463193188668, 'validation/wer': 0.243528218992903, 'validation/num_examples': 5348, 'test/ctc_loss': 0.541136726563004, 'test/wer': 0.17142973209026466, 'test/num_examples': 2472, 'score': 8677.668782234192, 'total_duration': 14581.583609819412, 'global_step': 18197, 'preemption_count': 0}), (21232, {'train/ctc_loss': 0.6446513441652606, 'train/wer': 0.20171902624779, 'validation/ctc_loss': 0.8697257383966245, 'validation/wer': 0.2446289769709844, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5452130480192271, 'test/wer': 0.17082038470131822, 'test/num_examples': 2472, 'score': 10117.276618719101, 'total_duration': 17008.79917860031, 'global_step': 21232, 'preemption_count': 0}), (24267, {'train/ctc_loss': 0.6293176724400805, 'train/wer': 0.19786209710322317, 'validation/ctc_loss': 0.8471805265157223, 'validation/wer': 0.23900931781972673, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5373631968191496, 'test/wer': 0.1695813783437938, 'test/num_examples': 2472, 'score': 11557.462610244751, 'total_duration': 19436.320678949356, 'global_step': 24267, 'preemption_count': 0}), (27303, {'train/ctc_loss': 0.6027879528267366, 'train/wer': 0.19220998232014144, 'validation/ctc_loss': 0.8270796641111613, 'validation/wer': 0.23604499589629702, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5146555826182334, 'test/wer': 0.1643206792192229, 'test/num_examples': 2472, 'score': 12996.649067878723, 'total_duration': 21862.893896102905, 'global_step': 27303, 'preemption_count': 0}), (30338, {'train/ctc_loss': 0.5939442894364145, 'train/wer': 0.19060519515843874, 'validation/ctc_loss': 0.8216068818910739, 'validation/wer': 0.23378554530970888, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5084121858668301, 'test/wer': 0.16257388337090975, 'test/num_examples': 2472, 'score': 14435.969842910767, 'total_duration': 24290.007303237915, 'global_step': 30338, 'preemption_count': 0}), (33373, {'train/ctc_loss': 0.5825310291347333, 'train/wer': 0.1858561131510948, 'validation/ctc_loss': 0.8100194488396625, 'validation/wer': 0.23088881378844203, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5036864777566294, 'test/wer': 0.16121300753559603, 'test/num_examples': 2472, 'score': 15875.212516069412, 'total_duration': 26717.12601041794, 'global_step': 33373, 'preemption_count': 0}), (36408, {'train/ctc_loss': 0.558619981720911, 'train/wer': 0.17851761185910514, 'validation/ctc_loss': 0.7766549369286216, 'validation/wer': 0.22133925553999903, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4827727796954642, 'test/wer': 0.1535352304348709, 'test/num_examples': 2472, 'score': 17314.363724946976, 'total_duration': 29144.60667538643, 'global_step': 36408, 'preemption_count': 0}), (39441, {'train/ctc_loss': 0.5439371001978682, 'train/wer': 0.1756017951856385, 'validation/ctc_loss': 0.7631817297066505, 'validation/wer': 0.21771834113841548, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4663417297083683, 'test/wer': 0.15067129770682267, 'test/num_examples': 2472, 'score': 18753.205456495285, 'total_duration': 31572.23267674446, 'global_step': 39441, 'preemption_count': 0}), (42473, {'train/ctc_loss': 0.5299292024116877, 'train/wer': 0.17026519787841699, 'validation/ctc_loss': 0.7587249001657626, 'validation/wer': 0.21590305605175494, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4633404776114588, 'test/wer': 0.14711677127130177, 'test/num_examples': 2472, 'score': 20193.167011737823, 'total_duration': 34000.3663315773, 'global_step': 42473, 'preemption_count': 0}), (45506, {'train/ctc_loss': 0.5078471756351094, 'train/wer': 0.1632530939752482, 'validation/ctc_loss': 0.7319133345420182, 'validation/wer': 0.20800463477043402, 'validation/num_examples': 5348, 'test/ctc_loss': 0.44068145525517777, 'test/wer': 0.1422623037393618, 'test/num_examples': 2472, 'score': 21633.578390598297, 'total_duration': 36427.78243303299, 'global_step': 45506, 'preemption_count': 0}), (48542, {'train/ctc_loss': 0.487724931051073, 'train/wer': 0.1580361757105943, 'validation/ctc_loss': 0.7048034729285966, 'validation/wer': 0.20176700622797278, 'validation/num_examples': 5348, 'test/ctc_loss': 0.42862452819536745, 'test/wer': 0.1388296467816302, 'test/num_examples': 2472, 'score': 23072.63733434677, 'total_duration': 38856.25702095032, 'global_step': 48542, 'preemption_count': 0}), (51577, {'train/ctc_loss': 0.4768412285783985, 'train/wer': 0.15413028695770434, 'validation/ctc_loss': 0.6987997817303345, 'validation/wer': 0.19901511128276927, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4167082604119621, 'test/wer': 0.13405642556821645, 'test/num_examples': 2472, 'score': 24512.323668003082, 'total_duration': 41283.634702682495, 'global_step': 51577, 'preemption_count': 0}), (54612, {'train/ctc_loss': 0.46292978729171325, 'train/wer': 0.15136134910920712, 'validation/ctc_loss': 0.6821459769627788, 'validation/wer': 0.19615700284845267, 'validation/num_examples': 5348, 'test/ctc_loss': 0.40861560120814244, 'test/wer': 0.12936445067332886, 'test/num_examples': 2472, 'score': 25951.82000899315, 'total_duration': 43710.91613817215, 'global_step': 54612, 'preemption_count': 0}), (57649, {'train/ctc_loss': 0.45433552377467423, 'train/wer': 0.1489188086495308, 'validation/ctc_loss': 0.6763871621961021, 'validation/wer': 0.19435137353352966, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4038598760403897, 'test/wer': 0.13007535596043304, 'test/num_examples': 2472, 'score': 27391.15217256546, 'total_duration': 46138.351586818695, 'global_step': 57649, 'preemption_count': 0}), (60683, {'train/ctc_loss': 0.4241881030913514, 'train/wer': 0.1388548891608867, 'validation/ctc_loss': 0.6411154396159835, 'validation/wer': 0.18568049051320426, 'validation/num_examples': 5348, 'test/ctc_loss': 0.37723472078843795, 'test/wer': 0.12195072410781387, 'test/num_examples': 2472, 'score': 28831.753898382187, 'total_duration': 48566.1736228466, 'global_step': 60683, 'preemption_count': 0}), (63719, {'train/ctc_loss': 0.4074829967626192, 'train/wer': 0.13338229294165646, 'validation/ctc_loss': 0.6254113649349508, 'validation/wer': 0.179529763916381, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3700274765710691, 'test/wer': 0.11839619767229298, 'test/num_examples': 2472, 'score': 30271.745221614838, 'total_duration': 50993.38636493683, 'global_step': 63719, 'preemption_count': 0}), (66753, {'train/ctc_loss': 0.387560203255338, 'train/wer': 0.12758873929008568, 'validation/ctc_loss': 0.6150465971594334, 'validation/wer': 0.17609230917781105, 'validation/num_examples': 5348, 'test/ctc_loss': 0.35035581932705334, 'test/wer': 0.11283082485324884, 'test/num_examples': 2472, 'score': 31712.43143749237, 'total_duration': 53421.703681230545, 'global_step': 66753, 'preemption_count': 0}), (69788, {'train/ctc_loss': 0.368376556644847, 'train/wer': 0.12348157214742282, 'validation/ctc_loss': 0.5953523414739049, 'validation/wer': 0.1713706367981461, 'validation/num_examples': 5348, 'test/ctc_loss': 0.33779593643138267, 'test/wer': 0.11000751528446367, 'test/num_examples': 2472, 'score': 33152.59000658989, 'total_duration': 55848.518532037735, 'global_step': 69788, 'preemption_count': 0}), (72824, {'train/ctc_loss': 0.3497983042714453, 'train/wer': 0.11638242894056848, 'validation/ctc_loss': 0.5706440053935604, 'validation/wer': 0.16649447207068024, 'validation/num_examples': 5348, 'test/ctc_loss': 0.32754323363604104, 'test/wer': 0.10543740986736538, 'test/num_examples': 2472, 'score': 34592.60094666481, 'total_duration': 58275.54814362526, 'global_step': 72824, 'preemption_count': 0}), (75858, {'train/ctc_loss': 0.33354638996462765, 'train/wer': 0.11156806745546036, 'validation/ctc_loss': 0.5579576206801286, 'validation/wer': 0.1613093226476126, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3148453882105297, 'test/wer': 0.10139540552068735, 'test/num_examples': 2472, 'score': 36032.99373936653, 'total_duration': 60702.79638648033, 'global_step': 75858, 'preemption_count': 0}), (78894, {'train/ctc_loss': 0.3119584248134821, 'train/wer': 0.1061281109751122, 'validation/ctc_loss': 0.5377285807684097, 'validation/wer': 0.156645584898373, 'validation/num_examples': 5348, 'test/ctc_loss': 0.3012144684737725, 'test/wer': 0.09712997379806228, 'test/num_examples': 2472, 'score': 37472.94047498703, 'total_duration': 63130.484989881516, 'global_step': 78894, 'preemption_count': 0}), (80000, {'train/ctc_loss': 0.30590943232332474, 'train/wer': 0.1034407724738202, 'validation/ctc_loss': 0.5324165477603224, 'validation/wer': 0.15304398204026456, 'validation/num_examples': 5348, 'test/ctc_loss': 0.2977673962836312, 'test/wer': 0.09457071476448724, 'test/num_examples': 2472, 'score': 37997.33049631119, 'total_duration': 64031.64917778969, 'global_step': 80000, 'preemption_count': 0})], 'global_step': 80000}
I0315 19:00:39.432163 140125394065216 submission_runner.py:526] Timing: 37997.33049631119
I0315 19:00:39.432216 140125394065216 submission_runner.py:527] ====================
I0315 19:00:39.432449 140125394065216 submission_runner.py:586] Final librispeech_deepspeech score: 37997.33049631119
