I0329 18:15:42.808611 139832824284992 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/imagenet_resnet_jax.
I0329 18:15:42.850425 139832824284992 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 18:15:44.086195 139832824284992 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0329 18:15:44.086841 139832824284992 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 18:15:44.091910 139832824284992 submission_runner.py:504] Using RNG seed 838776662
I0329 18:15:45.577929 139832824284992 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 18:15:45.578113 139832824284992 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1.
I0329 18:15:45.578298 139832824284992 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/hparams.json.
I0329 18:15:45.714668 139832824284992 submission_runner.py:230] Starting train once: RAM USED (GB) 4.217798656
I0329 18:15:45.714827 139832824284992 submission_runner.py:231] Initializing dataset.
I0329 18:15:45.729557 139832824284992 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:15:45.738618 139832824284992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 18:15:45.738727 139832824284992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 18:15:45.973231 139832824284992 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:15:47.152027 139832824284992 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.24292352
I0329 18:15:47.152190 139832824284992 submission_runner.py:240] Initializing model.
I0329 18:16:00.328971 139832824284992 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.297787392
I0329 18:16:00.329163 139832824284992 submission_runner.py:252] Initializing optimizer.
I0329 18:16:01.349324 139832824284992 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.298053632
I0329 18:16:01.349495 139832824284992 submission_runner.py:261] Initializing metrics bundle.
I0329 18:16:01.349545 139832824284992 submission_runner.py:275] Initializing checkpoint and logger.
I0329 18:16:01.350470 139832824284992 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0329 18:16:02.421119 139832824284992 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0329 18:16:02.422035 139832824284992 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/flags_0.json.
I0329 18:16:02.424678 139832824284992 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.29421568
I0329 18:16:02.424867 139832824284992 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.29421568
I0329 18:16:02.424929 139832824284992 submission_runner.py:312] Starting training loop.
2023-03-29 18:16:04.849381: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-03-29 18:16:05.007828: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-03-29 18:16:05.358808: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0329 18:16:05.796086 139832824284992 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 12.509380608
2023-03-29 18:16:05.831323: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-03-29 18:16:06.012245: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0329 18:16:45.750304 139653168490240 logging_writer.py:48] [0] global_step=0, grad_norm=0.5316120982170105, loss=6.934168338775635
I0329 18:16:45.762778 139832824284992 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 23.771660288
I0329 18:16:45.763122 139832824284992 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 23.771660288
I0329 18:16:45.763209 139832824284992 spec.py:298] Evaluating on the training split.
I0329 18:16:46.295041 139832824284992 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:16:46.302998 139832824284992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 18:16:46.303123 139832824284992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 18:16:46.378489 139832824284992 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:16:59.013133 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 18:16:59.807288 139832824284992 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:16:59.822272 139832824284992 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 18:16:59.822566 139832824284992 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 18:16:59.880797 139832824284992 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:17:18.782824 139832824284992 spec.py:326] Evaluating on the test split.
I0329 18:17:19.244516 139832824284992 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 18:17:19.248979 139832824284992 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0329 18:17:19.281387 139832824284992 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 18:17:29.078943 139832824284992 submission_runner.py:380] Time since start: 43.34s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.911753177642822, 'validation/accuracy': 0.000859999970998615, 'validation/loss': 6.912240028381348, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.9117817878723145, 'test/num_examples': 10000}
I0329 18:17:29.079607 139832824284992 submission_runner.py:390] After eval at step 1: RAM USED (GB) 65.191190528
I0329 18:17:29.087862 139627524495104 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=43.256062, test/accuracy=0.000800, test/loss=6.911782, test/num_examples=10000, total_duration=43.338196, train/accuracy=0.000977, train/loss=6.911753, validation/accuracy=0.000860, validation/loss=6.912240, validation/num_examples=50000
I0329 18:17:29.205819 139832824284992 checkpoints.py:356] Saving checkpoint at step: 1
I0329 18:17:29.798205 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1
I0329 18:17:29.804859 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1.
I0329 18:17:29.810609 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 65.160945664
I0329 18:17:29.815514 139832824284992 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 65.159254016
I0329 18:17:29.892658 139832824284992 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 65.409789952
I0329 18:18:03.827886 139627532887808 logging_writer.py:48] [100] global_step=100, grad_norm=0.5278679728507996, loss=6.878458023071289
I0329 18:18:37.762768 139627692283648 logging_writer.py:48] [200] global_step=200, grad_norm=0.5705872178077698, loss=6.809778690338135
I0329 18:19:12.018052 139627532887808 logging_writer.py:48] [300] global_step=300, grad_norm=0.6103903651237488, loss=6.708987712860107
I0329 18:19:46.054438 139627692283648 logging_writer.py:48] [400] global_step=400, grad_norm=0.6218869090080261, loss=6.595348834991455
I0329 18:20:20.094918 139627532887808 logging_writer.py:48] [500] global_step=500, grad_norm=0.6305283904075623, loss=6.543095588684082
I0329 18:20:54.060444 139627692283648 logging_writer.py:48] [600] global_step=600, grad_norm=0.6416551470756531, loss=6.482855319976807
I0329 18:21:28.214567 139627532887808 logging_writer.py:48] [700] global_step=700, grad_norm=0.6617724299430847, loss=6.450142860412598
I0329 18:22:02.451178 139627692283648 logging_writer.py:48] [800] global_step=800, grad_norm=0.7414891719818115, loss=6.38899564743042
I0329 18:22:36.263270 139627532887808 logging_writer.py:48] [900] global_step=900, grad_norm=0.7269521951675415, loss=6.3568925857543945
I0329 18:23:10.253795 139627692283648 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8007981181144714, loss=6.313401222229004
I0329 18:23:43.971241 139627532887808 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8310251235961914, loss=6.217652320861816
I0329 18:24:18.005533 139627692283648 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7439513206481934, loss=6.1930952072143555
I0329 18:24:51.974335 139627532887808 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7601506114006042, loss=6.168683052062988
I0329 18:25:26.014532 139627692283648 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7781232595443726, loss=6.151856422424316
I0329 18:26:00.044856 139627532887808 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9685295224189758, loss=6.093145370483398
I0329 18:26:00.052198 139832824284992 submission_runner.py:371] Before eval at step 1501: RAM USED (GB) 66.787127296
I0329 18:26:00.052315 139832824284992 spec.py:298] Evaluating on the training split.
I0329 18:26:07.512053 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 18:26:17.011142 139832824284992 spec.py:326] Evaluating on the test split.
I0329 18:26:19.295254 139832824284992 submission_runner.py:380] Time since start: 597.63s, 	Step: 1501, 	{'train/accuracy': 0.06825972348451614, 'train/loss': 5.452796459197998, 'validation/accuracy': 0.06317999958992004, 'validation/loss': 5.526564121246338, 'validation/num_examples': 50000, 'test/accuracy': 0.04740000143647194, 'test/loss': 5.737519264221191, 'test/num_examples': 10000}
I0329 18:26:19.295917 139832824284992 submission_runner.py:390] After eval at step 1501: RAM USED (GB) 72.834023424
I0329 18:26:19.303723 139627709069056 logging_writer.py:48] [1501] global_step=1501, preemption_count=0, score=544.068803, test/accuracy=0.047400, test/loss=5.737519, test/num_examples=10000, total_duration=597.626470, train/accuracy=0.068260, train/loss=5.452796, validation/accuracy=0.063180, validation/loss=5.526564, validation/num_examples=50000
I0329 18:26:19.435586 139832824284992 checkpoints.py:356] Saving checkpoint at step: 1501
I0329 18:26:20.034274 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1501
I0329 18:26:20.042140 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_1501.
I0329 18:26:20.049559 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 1501: RAM USED (GB) 72.803115008
I0329 18:26:53.730235 139627792930560 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8420623540878296, loss=5.983792304992676
I0329 18:27:27.527407 139655542462208 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7959184050559998, loss=5.981877326965332
I0329 18:28:01.162792 139627792930560 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9358140826225281, loss=5.8874430656433105
I0329 18:28:34.927527 139655542462208 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8038989305496216, loss=5.792877197265625
I0329 18:29:08.554662 139627792930560 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8433862328529358, loss=5.7363128662109375
I0329 18:29:42.300899 139655542462208 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9173464179039001, loss=5.7092485427856445
I0329 18:30:15.985294 139627792930560 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9431909322738647, loss=5.637131214141846
I0329 18:30:49.734107 139655542462208 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8454386591911316, loss=5.569127559661865
I0329 18:31:23.328769 139627792930560 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0850101709365845, loss=5.578046798706055
I0329 18:31:57.244248 139655542462208 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8241603374481201, loss=5.513674736022949
I0329 18:32:30.963827 139627792930560 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8056536912918091, loss=5.478293418884277
I0329 18:33:04.531781 139655542462208 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9043282866477966, loss=5.42121696472168
I0329 18:33:38.261608 139627792930560 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0256417989730835, loss=5.369769096374512
I0329 18:34:12.012423 139655542462208 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8732993006706238, loss=5.324679374694824
I0329 18:34:45.653378 139627792930560 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.859994113445282, loss=5.259326934814453
I0329 18:34:50.119096 139832824284992 submission_runner.py:371] Before eval at step 3015: RAM USED (GB) 73.994788864
I0329 18:34:50.119289 139832824284992 spec.py:298] Evaluating on the training split.
I0329 18:34:57.337888 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 18:35:06.639714 139832824284992 spec.py:326] Evaluating on the test split.
I0329 18:35:08.849420 139832824284992 submission_runner.py:380] Time since start: 1127.69s, 	Step: 3015, 	{'train/accuracy': 0.19114716351032257, 'train/loss': 4.206458568572998, 'validation/accuracy': 0.1718599945306778, 'validation/loss': 4.324267387390137, 'validation/num_examples': 50000, 'test/accuracy': 0.12410000711679459, 'test/loss': 4.771923542022705, 'test/num_examples': 10000}
I0329 18:35:08.850046 139832824284992 submission_runner.py:390] After eval at step 3015: RAM USED (GB) 79.25241856
I0329 18:35:08.857696 139655542462208 logging_writer.py:48] [3015] global_step=3015, preemption_count=0, score=1045.275890, test/accuracy=0.124100, test/loss=4.771924, test/num_examples=10000, total_duration=1127.693144, train/accuracy=0.191147, train/loss=4.206459, validation/accuracy=0.171860, validation/loss=4.324267, validation/num_examples=50000
I0329 18:35:09.042135 139832824284992 checkpoints.py:356] Saving checkpoint at step: 3015
I0329 18:35:09.717869 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_3015
I0329 18:35:09.725741 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_3015.
I0329 18:35:09.735063 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 3015: RAM USED (GB) 79.225147392
I0329 18:35:38.745471 139627792930560 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8139931559562683, loss=5.168464660644531
I0329 18:36:12.459371 139655508891392 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9106995463371277, loss=5.168396949768066
I0329 18:36:46.095600 139627792930560 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8923950791358948, loss=5.1388630867004395
I0329 18:37:19.770322 139655508891392 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8752208948135376, loss=5.00614070892334
I0329 18:37:53.500609 139627792930560 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8742679357528687, loss=4.968346118927002
I0329 18:38:27.223140 139655508891392 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9100492000579834, loss=5.064280033111572
I0329 18:39:00.771338 139627792930560 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8546586036682129, loss=5.019477844238281
I0329 18:39:34.351898 139655508891392 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8346374034881592, loss=4.903254985809326
I0329 18:40:07.990567 139627792930560 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7472800016403198, loss=4.803447723388672
I0329 18:40:41.814940 139655508891392 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8312804102897644, loss=4.812128067016602
I0329 18:41:15.610914 139627792930560 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7666227221488953, loss=4.733266353607178
I0329 18:41:49.298338 139655508891392 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7958336472511292, loss=4.758650302886963
I0329 18:42:22.934231 139627792930560 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7357472777366638, loss=4.7182183265686035
I0329 18:42:56.469825 139655508891392 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7667452096939087, loss=4.682703018188477
I0329 18:43:30.105894 139627792930560 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7803382277488708, loss=4.674850940704346
I0329 18:43:39.937277 139832824284992 submission_runner.py:371] Before eval at step 4531: RAM USED (GB) 80.0676864
I0329 18:43:39.937498 139832824284992 spec.py:298] Evaluating on the training split.
I0329 18:43:47.096513 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 18:43:56.821955 139832824284992 spec.py:326] Evaluating on the test split.
I0329 18:43:58.815301 139832824284992 submission_runner.py:380] Time since start: 1657.51s, 	Step: 4531, 	{'train/accuracy': 0.2748126685619354, 'train/loss': 3.6349735260009766, 'validation/accuracy': 0.25415998697280884, 'validation/loss': 3.7673704624176025, 'validation/num_examples': 50000, 'test/accuracy': 0.19290000200271606, 'test/loss': 4.284948348999023, 'test/num_examples': 10000}
I0329 18:43:58.816083 139832824284992 submission_runner.py:390] After eval at step 4531: RAM USED (GB) 85.29074176
I0329 18:43:58.824578 139655508891392 logging_writer.py:48] [4531] global_step=4531, preemption_count=0, score=1543.311230, test/accuracy=0.192900, test/loss=4.284948, test/num_examples=10000, total_duration=1657.511117, train/accuracy=0.274813, train/loss=3.634974, validation/accuracy=0.254160, validation/loss=3.767370, validation/num_examples=50000
I0329 18:43:58.982986 139832824284992 checkpoints.py:356] Saving checkpoint at step: 4531
I0329 18:43:59.513695 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_4531
I0329 18:43:59.515812 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_4531.
I0329 18:43:59.527005 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 4531: RAM USED (GB) 85.274763264
I0329 18:44:23.010688 139627792930560 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7497934103012085, loss=4.683391571044922
I0329 18:44:56.748803 139655089485568 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7633737325668335, loss=4.586374759674072
I0329 18:45:30.487125 139627792930560 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6932916045188904, loss=4.63407564163208
I0329 18:46:04.171588 139655089485568 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.693699836730957, loss=4.635164737701416
I0329 18:46:37.988344 139627792930560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7276332974433899, loss=4.570244789123535
I0329 18:47:11.672496 139655089485568 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.678168535232544, loss=4.471522331237793
I0329 18:47:45.338536 139627792930560 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6903553605079651, loss=4.435304641723633
I0329 18:48:18.955427 139655089485568 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.700462281703949, loss=4.364830017089844
I0329 18:48:52.555818 139627792930560 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6852327585220337, loss=4.452510356903076
I0329 18:49:26.067871 139655089485568 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6518128514289856, loss=4.40518045425415
I0329 18:49:59.646773 139627792930560 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6632905006408691, loss=4.252253532409668
I0329 18:50:33.590754 139655089485568 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6425666809082031, loss=4.235039234161377
I0329 18:51:07.411204 139627792930560 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6515317559242249, loss=4.395898818969727
I0329 18:51:41.183172 139655089485568 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6231456995010376, loss=4.276796817779541
I0329 18:52:14.825149 139627792930560 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6847019195556641, loss=4.240082263946533
I0329 18:52:29.717181 139832824284992 submission_runner.py:371] Before eval at step 6046: RAM USED (GB) 86.130520064
I0329 18:52:29.717402 139832824284992 spec.py:298] Evaluating on the training split.
I0329 18:52:36.697525 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 18:52:45.500649 139832824284992 spec.py:326] Evaluating on the test split.
I0329 18:52:47.492783 139832824284992 submission_runner.py:380] Time since start: 2187.29s, 	Step: 6046, 	{'train/accuracy': 0.41519850492477417, 'train/loss': 2.8192198276519775, 'validation/accuracy': 0.3875199854373932, 'validation/loss': 2.9463870525360107, 'validation/num_examples': 50000, 'test/accuracy': 0.2815999984741211, 'test/loss': 3.60437273979187, 'test/num_examples': 10000}
I0329 18:52:47.493409 139832824284992 submission_runner.py:390] After eval at step 6046: RAM USED (GB) 91.411935232
I0329 18:52:47.501764 139655089485568 logging_writer.py:48] [6046] global_step=6046, preemption_count=0, score=2048.131116, test/accuracy=0.281600, test/loss=3.604373, test/num_examples=10000, total_duration=2187.291267, train/accuracy=0.415199, train/loss=2.819220, validation/accuracy=0.387520, validation/loss=2.946387, validation/num_examples=50000
I0329 18:52:47.648388 139832824284992 checkpoints.py:356] Saving checkpoint at step: 6046
I0329 18:52:48.251533 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_6046
I0329 18:52:48.257892 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_6046.
I0329 18:52:48.265777 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 6046: RAM USED (GB) 91.38169856
I0329 18:53:06.879111 139627792930560 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6376442313194275, loss=4.284441947937012
I0329 18:53:40.504022 139654896535296 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6288883686065674, loss=4.187514781951904
I0329 18:54:14.242011 139627792930560 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6475461721420288, loss=4.223621845245361
I0329 18:54:47.843801 139654896535296 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6459147334098816, loss=4.260767459869385
I0329 18:55:21.499155 139627792930560 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6254564523696899, loss=4.240848064422607
I0329 18:55:55.142534 139654896535296 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6089239716529846, loss=4.183787822723389
I0329 18:56:28.884596 139627792930560 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6010154485702515, loss=4.204913139343262
I0329 18:57:02.459809 139654896535296 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6095848679542542, loss=4.1038055419921875
I0329 18:57:36.260237 139627792930560 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6015558838844299, loss=4.176156997680664
I0329 18:58:09.953621 139654896535296 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6108818054199219, loss=4.17301607131958
I0329 18:58:43.782310 139627792930560 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6008217930793762, loss=4.08756685256958
I0329 18:59:17.434396 139654896535296 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5800815224647522, loss=4.093181133270264
I0329 18:59:51.134524 139627792930560 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5692076086997986, loss=4.073847770690918
I0329 19:00:24.748681 139654896535296 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5760317444801331, loss=4.06367301940918
I0329 19:00:58.422286 139627792930560 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5729274749755859, loss=4.048686981201172
I0329 19:01:18.393039 139832824284992 submission_runner.py:371] Before eval at step 7561: RAM USED (GB) 92.183298048
I0329 19:01:18.393243 139832824284992 spec.py:298] Evaluating on the training split.
I0329 19:01:25.253158 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 19:01:34.121390 139832824284992 spec.py:326] Evaluating on the test split.
I0329 19:01:36.100577 139832824284992 submission_runner.py:380] Time since start: 2715.97s, 	Step: 7561, 	{'train/accuracy': 0.4656808078289032, 'train/loss': 2.616759777069092, 'validation/accuracy': 0.43741998076438904, 'validation/loss': 2.752345561981201, 'validation/num_examples': 50000, 'test/accuracy': 0.3255000114440918, 'test/loss': 3.4183709621429443, 'test/num_examples': 10000}
I0329 19:01:36.101346 139832824284992 submission_runner.py:390] After eval at step 7561: RAM USED (GB) 97.35149568
I0329 19:01:36.110597 139654896535296 logging_writer.py:48] [7561] global_step=7561, preemption_count=0, score=2549.613059, test/accuracy=0.325500, test/loss=3.418371, test/num_examples=10000, total_duration=2715.967172, train/accuracy=0.465681, train/loss=2.616760, validation/accuracy=0.437420, validation/loss=2.752346, validation/num_examples=50000
I0329 19:01:36.220691 139832824284992 checkpoints.py:356] Saving checkpoint at step: 7561
I0329 19:01:36.931891 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_7561
I0329 19:01:36.938485 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_7561.
I0329 19:01:36.942567 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 7561: RAM USED (GB) 97.397485568
I0329 19:01:50.387133 139627792930560 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5539708733558655, loss=3.946720838546753
I0329 19:02:23.989439 139654888142592 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5733346939086914, loss=4.006258010864258
I0329 19:02:57.772780 139627792930560 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5906421542167664, loss=3.9434168338775635
I0329 19:03:31.419566 139654888142592 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5752330422401428, loss=4.04510498046875
I0329 19:04:05.016565 139627792930560 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5482181906700134, loss=4.068768501281738
I0329 19:04:38.549498 139654888142592 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5498916506767273, loss=3.9416987895965576
I0329 19:05:12.214879 139627792930560 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5565090775489807, loss=3.893054485321045
I0329 19:05:45.830840 139654888142592 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.549959659576416, loss=4.0446600914001465
I0329 19:06:19.421607 139627792930560 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5283054113388062, loss=3.991333246231079
I0329 19:06:52.920504 139654888142592 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5572646260261536, loss=3.8819570541381836
I0329 19:07:26.512889 139627792930560 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5575234889984131, loss=3.892789125442505
I0329 19:08:00.174216 139654888142592 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5343417525291443, loss=3.9013009071350098
I0329 19:08:33.922459 139627792930560 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5328963398933411, loss=3.8414711952209473
I0329 19:09:07.374717 139654888142592 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5444594621658325, loss=3.883798599243164
I0329 19:09:40.983552 139627792930560 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5321751832962036, loss=3.9393720626831055
I0329 19:10:07.179920 139832824284992 submission_runner.py:371] Before eval at step 9080: RAM USED (GB) 98.030637056
I0329 19:10:07.180110 139832824284992 spec.py:298] Evaluating on the training split.
I0329 19:10:14.137742 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 19:10:23.753007 139832824284992 spec.py:326] Evaluating on the test split.
I0329 19:10:25.739480 139832824284992 submission_runner.py:380] Time since start: 3244.75s, 	Step: 9080, 	{'train/accuracy': 0.5672233700752258, 'train/loss': 2.14668607711792, 'validation/accuracy': 0.5097399950027466, 'validation/loss': 2.407517671585083, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 3.059067964553833, 'test/num_examples': 10000}
I0329 19:10:25.740159 139832824284992 submission_runner.py:390] After eval at step 9080: RAM USED (GB) 103.130652672
I0329 19:10:25.749552 139654888142592 logging_writer.py:48] [9080] global_step=9080, preemption_count=0, score=3047.051747, test/accuracy=0.393600, test/loss=3.059068, test/num_examples=10000, total_duration=3244.754087, train/accuracy=0.567223, train/loss=2.146686, validation/accuracy=0.509740, validation/loss=2.407518, validation/num_examples=50000
I0329 19:10:25.873963 139832824284992 checkpoints.py:356] Saving checkpoint at step: 9080
I0329 19:10:26.440000 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_9080
I0329 19:10:26.447718 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_9080.
I0329 19:10:26.453263 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 9080: RAM USED (GB) 103.136866304
I0329 19:10:33.473337 139627792930560 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5080926418304443, loss=3.729621648788452
I0329 19:11:07.192128 139654879749888 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5186523199081421, loss=3.8987977504730225
I0329 19:11:40.811497 139627792930560 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5214206576347351, loss=3.850207567214966
I0329 19:12:14.339185 139654879749888 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5043861269950867, loss=3.8331804275512695
I0329 19:12:48.045633 139627792930560 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4985389709472656, loss=3.772653102874756
I0329 19:13:21.591471 139654879749888 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.523857593536377, loss=3.804028034210205
I0329 19:13:55.141649 139627792930560 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5303831100463867, loss=3.858220338821411
I0329 19:14:28.692059 139654879749888 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5087981820106506, loss=3.8173258304595947
I0329 19:15:02.278968 139627792930560 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.48863205313682556, loss=3.676814079284668
I0329 19:15:35.930100 139654879749888 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4818077087402344, loss=3.729036331176758
I0329 19:16:09.484414 139627792930560 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5278958678245544, loss=3.772770643234253
I0329 19:16:43.047698 139654879749888 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4976692497730255, loss=3.741162061691284
I0329 19:17:16.522614 139627792930560 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5024451613426208, loss=3.769313335418701
I0329 19:17:49.961020 139654879749888 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4867405891418457, loss=3.6921520233154297
I0329 19:18:23.548816 139627792930560 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4858579933643341, loss=3.6657497882843018
I0329 19:18:56.462981 139832824284992 submission_runner.py:371] Before eval at step 10600: RAM USED (GB) 104.528166912
I0329 19:18:56.463247 139832824284992 spec.py:298] Evaluating on the training split.
I0329 19:19:03.489716 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 19:19:12.914512 139832824284992 spec.py:326] Evaluating on the test split.
I0329 19:19:15.106246 139832824284992 submission_runner.py:380] Time since start: 3774.04s, 	Step: 10600, 	{'train/accuracy': 0.5964604616165161, 'train/loss': 2.0152413845062256, 'validation/accuracy': 0.5387399792671204, 'validation/loss': 2.270502805709839, 'validation/num_examples': 50000, 'test/accuracy': 0.4142000079154968, 'test/loss': 2.913388252258301, 'test/num_examples': 10000}
I0329 19:19:15.106923 139832824284992 submission_runner.py:390] After eval at step 10600: RAM USED (GB) 109.749673984
I0329 19:19:15.115211 139654879749888 logging_writer.py:48] [10600] global_step=10600, preemption_count=0, score=3543.924931, test/accuracy=0.414200, test/loss=2.913388, test/num_examples=10000, total_duration=3774.036642, train/accuracy=0.596460, train/loss=2.015241, validation/accuracy=0.538740, validation/loss=2.270503, validation/num_examples=50000
I0329 19:19:15.247250 139832824284992 checkpoints.py:356] Saving checkpoint at step: 10600
I0329 19:19:15.811321 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_10600
I0329 19:19:15.817495 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_10600.
I0329 19:19:15.821625 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 10600: RAM USED (GB) 109.77421312
I0329 19:19:16.174450 139627792930560 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5201496481895447, loss=3.7336807250976562
I0329 19:19:49.755177 139654871357184 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4855300784111023, loss=3.692288398742676
I0329 19:20:23.362089 139627792930560 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.470488965511322, loss=3.6059353351593018
I0329 19:20:56.847286 139654871357184 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5057390928268433, loss=3.7257142066955566
I0329 19:21:30.616574 139627792930560 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5007863640785217, loss=3.759366273880005
I0329 19:22:04.227695 139654871357184 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4704541563987732, loss=3.6620559692382812
I0329 19:22:37.799267 139627792930560 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.494076669216156, loss=3.5850441455841064
I0329 19:23:11.512943 139654871357184 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5041286945343018, loss=3.69578218460083
I0329 19:23:44.993083 139627792930560 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.478945791721344, loss=3.6460108757019043
I0329 19:24:18.612429 139654871357184 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.48971259593963623, loss=3.6515591144561768
I0329 19:24:52.170737 139627792930560 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4954542815685272, loss=3.5644218921661377
I0329 19:25:25.688869 139654871357184 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.4903794229030609, loss=3.6529431343078613
I0329 19:25:59.328964 139627792930560 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.49118316173553467, loss=3.6163735389709473
I0329 19:26:33.100584 139654871357184 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4986698031425476, loss=3.554779291152954
I0329 19:27:06.653773 139627792930560 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.47374969720840454, loss=3.560230016708374
I0329 19:27:40.220638 139654871357184 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.4743771553039551, loss=3.5769143104553223
I0329 19:27:46.016395 139832824284992 submission_runner.py:371] Before eval at step 12119: RAM USED (GB) 110.37784064
I0329 19:27:46.016592 139832824284992 spec.py:298] Evaluating on the training split.
I0329 19:27:52.978379 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 19:28:02.719269 139832824284992 spec.py:326] Evaluating on the test split.
I0329 19:28:04.822200 139832824284992 submission_runner.py:380] Time since start: 4303.59s, 	Step: 12119, 	{'train/accuracy': 0.629902720451355, 'train/loss': 1.738381028175354, 'validation/accuracy': 0.5724200010299683, 'validation/loss': 1.98341965675354, 'validation/num_examples': 50000, 'test/accuracy': 0.45090001821517944, 'test/loss': 2.649935007095337, 'test/num_examples': 10000}
I0329 19:28:04.822858 139832824284992 submission_runner.py:390] After eval at step 12119: RAM USED (GB) 115.658317824
I0329 19:28:04.832540 139627792930560 logging_writer.py:48] [12119] global_step=12119, preemption_count=0, score=4048.737250, test/accuracy=0.450900, test/loss=2.649935, test/num_examples=10000, total_duration=4303.590207, train/accuracy=0.629903, train/loss=1.738381, validation/accuracy=0.572420, validation/loss=1.983420, validation/num_examples=50000
I0329 19:28:04.958345 139832824284992 checkpoints.py:356] Saving checkpoint at step: 12119
I0329 19:28:05.569638 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_12119
I0329 19:28:05.577503 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_12119.
I0329 19:28:05.581242 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 12119: RAM USED (GB) 115.741995008
I0329 19:28:33.295512 139654871357184 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4831560552120209, loss=3.6234779357910156
I0329 19:29:06.996415 139650609948416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.47713273763656616, loss=3.5294086933135986
I0329 19:29:40.563431 139654871357184 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4826112687587738, loss=3.6224591732025146
I0329 19:30:14.120743 139650609948416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.47035378217697144, loss=3.532677412033081
I0329 19:30:47.735268 139654871357184 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.48259586095809937, loss=3.494351863861084
I0329 19:31:21.441897 139650609948416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4611908495426178, loss=3.485837459564209
I0329 19:31:55.181813 139654871357184 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.46212586760520935, loss=3.4262897968292236
I0329 19:32:28.827279 139650609948416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.46757230162620544, loss=3.503868579864502
I0329 19:33:02.608793 139654871357184 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.46018803119659424, loss=3.520824909210205
I0329 19:33:36.300330 139650609948416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4655488133430481, loss=3.482511281967163
I0329 19:34:10.001035 139654871357184 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4603491425514221, loss=3.496922731399536
I0329 19:34:43.600468 139650609948416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4638761281967163, loss=3.5011324882507324
I0329 19:35:17.477517 139654871357184 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.46941033005714417, loss=3.527859926223755
I0329 19:35:51.163076 139650609948416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.47896629571914673, loss=3.5099332332611084
I0329 19:36:24.712290 139654871357184 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4511856734752655, loss=3.414576530456543
I0329 19:36:35.870929 139832824284992 submission_runner.py:371] Before eval at step 13635: RAM USED (GB) 116.39558144
I0329 19:36:35.871222 139832824284992 spec.py:298] Evaluating on the training split.
I0329 19:36:42.857805 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 19:36:52.598511 139832824284992 spec.py:326] Evaluating on the test split.
I0329 19:36:54.715634 139832824284992 submission_runner.py:380] Time since start: 4833.44s, 	Step: 13635, 	{'train/accuracy': 0.6545161008834839, 'train/loss': 1.6564663648605347, 'validation/accuracy': 0.5937600135803223, 'validation/loss': 1.9143271446228027, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.5886404514312744, 'test/num_examples': 10000}
I0329 19:36:54.716283 139832824284992 submission_runner.py:390] After eval at step 13635: RAM USED (GB) 121.543692288
I0329 19:36:54.725705 139650609948416 logging_writer.py:48] [13635] global_step=13635, preemption_count=0, score=4553.736001, test/accuracy=0.465300, test/loss=2.588640, test/num_examples=10000, total_duration=4833.444566, train/accuracy=0.654516, train/loss=1.656466, validation/accuracy=0.593760, validation/loss=1.914327, validation/num_examples=50000
I0329 19:36:54.870351 139832824284992 checkpoints.py:356] Saving checkpoint at step: 13635
I0329 19:36:55.500345 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_13635
I0329 19:36:55.510255 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_13635.
I0329 19:36:55.514627 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 13635: RAM USED (GB) 121.64007936
I0329 19:37:17.834126 139654871357184 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.4695884585380554, loss=3.4737329483032227
I0329 19:37:51.406733 139649712383744 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4845237135887146, loss=3.531248092651367
I0329 19:38:25.020875 139654871357184 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.4746449291706085, loss=3.4640090465545654
I0329 19:38:57.963076 139832824284992 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 122.18898432
I0329 19:38:57.963294 139832824284992 spec.py:298] Evaluating on the training split.
I0329 19:39:04.852506 139832824284992 spec.py:310] Evaluating on the validation split.
I0329 19:39:14.518773 139832824284992 spec.py:326] Evaluating on the test split.
I0329 19:39:16.728939 139832824284992 submission_runner.py:380] Time since start: 4975.54s, 	Step: 14000, 	{'train/accuracy': 0.6503706574440002, 'train/loss': 1.6921987533569336, 'validation/accuracy': 0.5972399711608887, 'validation/loss': 1.9247254133224487, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.621046304702759, 'test/num_examples': 10000}
I0329 19:39:16.729583 139832824284992 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 127.38842624
I0329 19:39:16.738673 139649712383744 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4674.633164, test/accuracy=0.463900, test/loss=2.621046, test/num_examples=10000, total_duration=4975.537283, train/accuracy=0.650371, train/loss=1.692199, validation/accuracy=0.597240, validation/loss=1.924725, validation/num_examples=50000
I0329 19:39:16.890402 139832824284992 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 19:39:17.514905 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_14000
I0329 19:39:17.522454 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0329 19:39:17.526264 139832824284992 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 127.499157504
I0329 19:39:17.536429 139654871357184 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4674.633164
I0329 19:39:17.662472 139832824284992 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 19:39:18.509549 139832824284992 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_14000
I0329 19:39:18.518120 139832824284992 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0329 19:39:18.678977 139832824284992 submission_runner.py:543] Tuning trial 1/1
I0329 19:39:18.680745 139832824284992 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0329 19:39:18.683491 139832824284992 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.911753177642822, 'validation/accuracy': 0.000859999970998615, 'validation/loss': 6.912240028381348, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.9117817878723145, 'test/num_examples': 10000, 'score': 43.256062269210815, 'total_duration': 43.33819603919983, 'global_step': 1, 'preemption_count': 0}), (1501, {'train/accuracy': 0.06825972348451614, 'train/loss': 5.452796459197998, 'validation/accuracy': 0.06317999958992004, 'validation/loss': 5.526564121246338, 'validation/num_examples': 50000, 'test/accuracy': 0.04740000143647194, 'test/loss': 5.737519264221191, 'test/num_examples': 10000, 'score': 544.0688033103943, 'total_duration': 597.6264700889587, 'global_step': 1501, 'preemption_count': 0}), (3015, {'train/accuracy': 0.19114716351032257, 'train/loss': 4.206458568572998, 'validation/accuracy': 0.1718599945306778, 'validation/loss': 4.324267387390137, 'validation/num_examples': 50000, 'test/accuracy': 0.12410000711679459, 'test/loss': 4.771923542022705, 'test/num_examples': 10000, 'score': 1045.2758903503418, 'total_duration': 1127.693143606186, 'global_step': 3015, 'preemption_count': 0}), (4531, {'train/accuracy': 0.2748126685619354, 'train/loss': 3.6349735260009766, 'validation/accuracy': 0.25415998697280884, 'validation/loss': 3.7673704624176025, 'validation/num_examples': 50000, 'test/accuracy': 0.19290000200271606, 'test/loss': 4.284948348999023, 'test/num_examples': 10000, 'score': 1543.3112304210663, 'total_duration': 1657.5111167430878, 'global_step': 4531, 'preemption_count': 0}), (6046, {'train/accuracy': 0.41519850492477417, 'train/loss': 2.8192198276519775, 'validation/accuracy': 0.3875199854373932, 'validation/loss': 2.9463870525360107, 'validation/num_examples': 50000, 'test/accuracy': 0.2815999984741211, 'test/loss': 3.60437273979187, 'test/num_examples': 10000, 'score': 2048.1311161518097, 'total_duration': 2187.2912669181824, 'global_step': 6046, 'preemption_count': 0}), (7561, {'train/accuracy': 0.4656808078289032, 'train/loss': 2.616759777069092, 'validation/accuracy': 0.43741998076438904, 'validation/loss': 2.752345561981201, 'validation/num_examples': 50000, 'test/accuracy': 0.3255000114440918, 'test/loss': 3.4183709621429443, 'test/num_examples': 10000, 'score': 2549.6130590438843, 'total_duration': 2715.967172384262, 'global_step': 7561, 'preemption_count': 0}), (9080, {'train/accuracy': 0.5672233700752258, 'train/loss': 2.14668607711792, 'validation/accuracy': 0.5097399950027466, 'validation/loss': 2.407517671585083, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 3.059067964553833, 'test/num_examples': 10000, 'score': 3047.0517473220825, 'total_duration': 3244.754086971283, 'global_step': 9080, 'preemption_count': 0}), (10600, {'train/accuracy': 0.5964604616165161, 'train/loss': 2.0152413845062256, 'validation/accuracy': 0.5387399792671204, 'validation/loss': 2.270502805709839, 'validation/num_examples': 50000, 'test/accuracy': 0.4142000079154968, 'test/loss': 2.913388252258301, 'test/num_examples': 10000, 'score': 3543.9249312877655, 'total_duration': 3774.036642074585, 'global_step': 10600, 'preemption_count': 0}), (12119, {'train/accuracy': 0.629902720451355, 'train/loss': 1.738381028175354, 'validation/accuracy': 0.5724200010299683, 'validation/loss': 1.98341965675354, 'validation/num_examples': 50000, 'test/accuracy': 0.45090001821517944, 'test/loss': 2.649935007095337, 'test/num_examples': 10000, 'score': 4048.7372496128082, 'total_duration': 4303.590206861496, 'global_step': 12119, 'preemption_count': 0}), (13635, {'train/accuracy': 0.6545161008834839, 'train/loss': 1.6564663648605347, 'validation/accuracy': 0.5937600135803223, 'validation/loss': 1.9143271446228027, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.5886404514312744, 'test/num_examples': 10000, 'score': 4553.736000537872, 'total_duration': 4833.44456577301, 'global_step': 13635, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6503706574440002, 'train/loss': 1.6921987533569336, 'validation/accuracy': 0.5972399711608887, 'validation/loss': 1.9247254133224487, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.621046304702759, 'test/num_examples': 10000, 'score': 4674.633164167404, 'total_duration': 4975.537282705307, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0329 19:39:18.683594 139832824284992 submission_runner.py:546] Timing: 4674.633164167404
I0329 19:39:18.683641 139832824284992 submission_runner.py:547] ====================
I0329 19:39:18.683740 139832824284992 submission_runner.py:606] Final imagenet_resnet score: 4674.633164167404
