I0330 05:12:19.237556 140421956904768 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/librispeech_conformer_jax.
I0330 05:12:19.290395 140421956904768 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0330 05:12:20.150333 140421956904768 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0330 05:12:20.151027 140421956904768 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0330 05:12:20.158154 140421956904768 submission_runner.py:504] Using RNG seed 3969520757
I0330 05:12:22.607133 140421956904768 submission_runner.py:513] --- Tuning run 1/1 ---
I0330 05:12:22.607347 140421956904768 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1.
I0330 05:12:22.607519 140421956904768 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/hparams.json.
I0330 05:12:22.746186 140421956904768 submission_runner.py:230] Starting train once: RAM USED (GB) 4.486823936
I0330 05:12:22.746358 140421956904768 submission_runner.py:231] Initializing dataset.
I0330 05:12:22.746532 140421956904768 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.486823936
I0330 05:12:22.746592 140421956904768 submission_runner.py:240] Initializing model.
I0330 05:12:29.070514 140421956904768 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.91943168
I0330 05:12:29.070713 140421956904768 submission_runner.py:252] Initializing optimizer.
I0330 05:12:29.823663 140421956904768 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.91969792
I0330 05:12:29.823848 140421956904768 submission_runner.py:261] Initializing metrics bundle.
I0330 05:12:29.823900 140421956904768 submission_runner.py:275] Initializing checkpoint and logger.
I0330 05:12:29.824835 140421956904768 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0330 05:12:29.825101 140421956904768 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0330 05:12:29.825211 140421956904768 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0330 05:12:30.534181 140421956904768 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0330 05:12:30.535047 140421956904768 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/flags_0.json.
I0330 05:12:30.540852 140421956904768 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 7.916232704
I0330 05:12:30.541079 140421956904768 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.916232704
I0330 05:12:30.541160 140421956904768 submission_runner.py:312] Starting training loop.
I0330 05:12:30.754836 140421956904768 input_pipeline.py:20] Loading split = train-clean-100
I0330 05:12:30.792093 140421956904768 input_pipeline.py:20] Loading split = train-clean-360
I0330 05:12:31.138248 140421956904768 input_pipeline.py:20] Loading split = train-other-500
I0330 05:12:34.498134 140421956904768 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 8.396955648
2023-03-30 05:13:28.090920: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-03-30 05:13:28.356274: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0330 05:13:30.087152 140246821893888 logging_writer.py:48] [0] global_step=0, grad_norm=55.33430099487305, loss=32.040348052978516
I0330 05:13:30.103579 140421956904768 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 13.370474496
I0330 05:13:30.103838 140421956904768 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 13.370474496
I0330 05:13:30.103935 140421956904768 spec.py:298] Evaluating on the training split.
I0330 05:13:30.218659 140421956904768 input_pipeline.py:20] Loading split = train-clean-100
I0330 05:13:30.250748 140421956904768 input_pipeline.py:20] Loading split = train-clean-360
I0330 05:13:30.571508 140421956904768 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0330 05:14:39.162769 140421956904768 spec.py:310] Evaluating on the validation split.
I0330 05:14:39.231513 140421956904768 input_pipeline.py:20] Loading split = dev-clean
I0330 05:14:39.237347 140421956904768 input_pipeline.py:20] Loading split = dev-other
I0330 05:15:21.350795 140421956904768 spec.py:326] Evaluating on the test split.
I0330 05:15:21.422935 140421956904768 input_pipeline.py:20] Loading split = test-clean
I0330 05:15:50.916481 140421956904768 submission_runner.py:380] Time since start: 59.56s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.950188, dtype=float32), 'train/wer': 1.4629500447702142, 'validation/ctc_loss': DeviceArray(30.86757, dtype=float32), 'validation/wer': 1.0566045017318064, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.897873, dtype=float32), 'test/wer': 1.1219304125281824, 'test/num_examples': 2472}
I0330 05:15:50.917764 140421956904768 submission_runner.py:390] After eval at step 1: RAM USED (GB) 20.886044672
I0330 05:15:50.930491 140243198015232 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=59.348967, test/ctc_loss=30.897872924804688, test/num_examples=2472, test/wer=1.121930, total_duration=59.562744, train/ctc_loss=31.95018768310547, train/wer=1.462950, validation/ctc_loss=30.867570877075195, validation/num_examples=5348, validation/wer=1.056605
I0330 05:15:51.146054 140421956904768 checkpoints.py:356] Saving checkpoint at step: 1
I0330 05:15:52.030436 140421956904768 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_1
I0330 05:15:52.047784 140421956904768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_1.
I0330 05:15:52.061874 140421956904768 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 20.897366016
I0330 05:15:52.110062 140421956904768 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 20.894834688
I0330 05:16:07.359719 140421956904768 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 21.226549248
I0330 05:17:22.465246 140247575910144 logging_writer.py:48] [100] global_step=100, grad_norm=3.4439563751220703, loss=5.935351371765137
I0330 05:18:38.873970 140247584302848 logging_writer.py:48] [200] global_step=200, grad_norm=2.9336743354797363, loss=9.231048583984375
I0330 05:19:55.384743 140247575910144 logging_writer.py:48] [300] global_step=300, grad_norm=0.14725950360298157, loss=8.423348426818848
I0330 05:21:11.577054 140247584302848 logging_writer.py:48] [400] global_step=400, grad_norm=3.043269395828247, loss=8.383310317993164
I0330 05:22:27.450428 140247575910144 logging_writer.py:48] [500] global_step=500, grad_norm=12.287283897399902, loss=7.773655891418457
I0330 05:23:42.140496 140247584302848 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1774.0621337890625
I0330 05:24:56.431101 140247575910144 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1824.5111083984375
I0330 05:26:14.777385 140247584302848 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1825.031494140625
I0330 05:27:36.740597 140247575910144 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1697.931640625
I0330 05:29:01.699311 140247584302848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1844.7587890625
I0330 05:30:21.743268 140248365553408 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1878.3267822265625
I0330 05:31:36.088976 140248357160704 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1806.7413330078125
I0330 05:32:50.467016 140248365553408 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1747.77880859375
I0330 05:34:04.775240 140248357160704 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1869.8206787109375
I0330 05:35:20.473787 140248365553408 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1858.28369140625
I0330 05:36:42.185647 140248357160704 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1915.572998046875
I0330 05:38:04.968203 140248365553408 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1824.5111083984375
I0330 05:39:29.705698 140248357160704 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1772.5880126953125
I0330 05:40:55.155487 140248365553408 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1814.296142578125
I0330 05:42:20.601948 140248357160704 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1872.2823486328125
I0330 05:43:44.698095 140248365553408 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1798.4906005859375
I0330 05:44:59.013594 140248357160704 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1964.2227783203125
I0330 05:46:13.164413 140248365553408 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1889.9754638671875
I0330 05:47:27.156102 140248357160704 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1800.008056640625
I0330 05:48:41.040208 140248365553408 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1802.035400390625
I0330 05:49:59.092734 140248357160704 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1787.5648193359375
I0330 05:51:21.388871 140248365553408 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1784.6986083984375
I0330 05:52:47.890816 140248357160704 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1833.0028076171875
I0330 05:54:12.916663 140248365553408 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1747.898193359375
I0330 05:55:38.949357 140248357160704 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1786.4422607421875
I0330 05:55:52.844245 140421956904768 submission_runner.py:371] Before eval at step 3018: RAM USED (GB) 22.976098304
I0330 05:55:52.844463 140421956904768 spec.py:298] Evaluating on the training split.
I0330 05:56:21.845150 140421956904768 spec.py:310] Evaluating on the validation split.
I0330 05:56:59.307488 140421956904768 spec.py:326] Evaluating on the test split.
I0330 05:57:18.844788 140421956904768 submission_runner.py:380] Time since start: 2602.30s, 	Step: 3018, 	{'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 05:57:18.846056 140421956904768 submission_runner.py:390] After eval at step 3018: RAM USED (GB) 20.662935552
I0330 05:57:18.866350 140248365553408 logging_writer.py:48] [3018] global_step=3018, preemption_count=0, score=2454.080177, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2602.301237, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0330 05:57:19.056018 140421956904768 checkpoints.py:356] Saving checkpoint at step: 3018
I0330 05:57:19.959422 140421956904768 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_3018
I0330 05:57:19.977318 140421956904768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_3018.
I0330 05:57:19.987875 140421956904768 submission_runner.py:409] After logging and checkpointing eval at step 3018: RAM USED (GB) 20.630904832
I0330 05:58:25.178551 140249020913408 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1854.648681640625
I0330 05:59:39.404308 140249012520704 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1870.093994140625
I0330 06:00:53.666537 140249020913408 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1853.97705078125
I0330 06:02:07.975617 140249012520704 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1866.004638671875
I0330 06:03:22.441554 140249020913408 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1783.206787109375
I0330 06:04:43.883301 140249012520704 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1796.7235107421875
I0330 06:06:05.905418 140249020913408 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1889.556884765625
I0330 06:07:31.214936 140249012520704 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1815.582763671875
I0330 06:08:53.315979 140249020913408 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1769.52490234375
I0330 06:10:15.384591 140249012520704 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1792.195068359375
I0330 06:11:40.106075 140249020913408 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1849.423095703125
I0330 06:12:59.363606 140249676273408 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1825.421875
I0330 06:14:13.663056 140249667880704 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1786.8162841796875
I0330 06:15:27.912218 140249676273408 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1870.50390625
I0330 06:16:42.180841 140249667880704 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1828.159423828125
I0330 06:18:05.137839 140249676273408 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1819.8419189453125
I0330 06:19:30.743824 140249667880704 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1852.6353759765625
I0330 06:20:55.371121 140249676273408 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1952.5372314453125
I0330 06:22:21.307846 140249667880704 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1852.9036865234375
I0330 06:23:46.820163 140249676273408 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1808.91162109375
I0330 06:25:09.009589 140249667880704 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1785.0718994140625
I0330 06:26:29.800495 140248365553408 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1795.085693359375
I0330 06:27:44.019912 140248357160704 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1808.272705078125
I0330 06:28:58.301821 140248365553408 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1798.4906005859375
I0330 06:30:12.531178 140248357160704 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1771.3614501953125
I0330 06:31:31.054642 140248365553408 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1813.2679443359375
I0330 06:32:53.592320 140248357160704 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1824.9013671875
I0330 06:34:19.770944 140248365553408 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1911.99609375
I0330 06:35:43.889985 140248357160704 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1825.421875
I0330 06:37:09.478953 140248365553408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1841.9713134765625
I0330 06:37:20.252418 140421956904768 submission_runner.py:371] Before eval at step 6014: RAM USED (GB) 21.89811712
I0330 06:37:20.252645 140421956904768 spec.py:298] Evaluating on the training split.
I0330 06:37:49.178642 140421956904768 spec.py:310] Evaluating on the validation split.
I0330 06:38:26.814764 140421956904768 spec.py:326] Evaluating on the test split.
I0330 06:38:46.172526 140421956904768 submission_runner.py:380] Time since start: 5089.71s, 	Step: 6014, 	{'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 06:38:46.173838 140421956904768 submission_runner.py:390] After eval at step 6014: RAM USED (GB) 20.471451648
I0330 06:38:46.193679 140248365553408 logging_writer.py:48] [6014] global_step=6014, preemption_count=0, score=4848.438244, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5089.708801, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0330 06:38:46.383109 140421956904768 checkpoints.py:356] Saving checkpoint at step: 6014
I0330 06:38:47.286475 140421956904768 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_6014
I0330 06:38:47.304373 140421956904768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_6014.
I0330 06:38:47.314615 140421956904768 submission_runner.py:409] After logging and checkpointing eval at step 6014: RAM USED (GB) 20.439035904
I0330 06:39:51.892002 140248357160704 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1869.1380615234375
I0330 06:41:09.706757 140249020913408 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1839.720947265625
I0330 06:42:23.993056 140249012520704 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1810.318603515625
I0330 06:43:38.254537 140249020913408 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1842.8995361328125
I0330 06:44:52.535725 140249012520704 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1762.4588623046875
I0330 06:46:09.023725 140249020913408 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1833.79052734375
I0330 06:47:32.502774 140249012520704 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1811.343505859375
I0330 06:48:55.015118 140249020913408 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1759.0679931640625
I0330 06:50:18.599011 140249012520704 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1844.3599853515625
I0330 06:51:37.898946 140249020913408 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1748.73388671875
I0330 06:53:00.762340 140249012520704 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1859.7686767578125
I0330 06:54:24.916536 140249020913408 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1762.7015380859375
I0330 06:55:43.566159 140249020913408 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1824.9013671875
I0330 06:56:58.008155 140249012520704 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1856.1280517578125
I0330 06:58:12.294436 140249020913408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1823.9912109375
I0330 06:59:26.513752 140249012520704 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1848.621826171875
I0330 07:00:45.854682 140249020913408 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1810.0625
I0330 07:02:10.690699 140249012520704 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1823.8612060546875
I0330 07:03:35.112759 140249020913408 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1765.497802734375
I0330 07:05:01.655884 140249012520704 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1910.4266357421875
I0330 07:06:28.161581 140249020913408 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1745.5145263671875
I0330 07:07:55.196334 140249012520704 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1765.1324462890625
I0330 07:09:18.021892 140249676273408 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1813.139404296875
I0330 07:10:32.170095 140249667880704 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1764.037841796875
I0330 07:11:46.362882 140249676273408 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1814.296142578125
I0330 07:13:00.725643 140249667880704 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1829.5965576171875
I0330 07:14:24.169791 140249676273408 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1789.6893310546875
I0330 07:15:48.959519 140249667880704 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1875.7117919921875
I0330 07:17:10.013044 140249676273408 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1848.75537109375
I0330 07:18:30.801158 140249667880704 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1917.1510009765625
I0330 07:18:48.019136 140421956904768 submission_runner.py:371] Before eval at step 9022: RAM USED (GB) 22.258655232
I0330 07:18:48.019355 140421956904768 spec.py:298] Evaluating on the training split.
I0330 07:19:17.316223 140421956904768 spec.py:310] Evaluating on the validation split.
I0330 07:19:53.586002 140421956904768 spec.py:326] Evaluating on the test split.
I0330 07:20:13.634182 140421956904768 submission_runner.py:380] Time since start: 7577.47s, 	Step: 9022, 	{'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 07:20:13.635509 140421956904768 submission_runner.py:390] After eval at step 9022: RAM USED (GB) 20.616753152
I0330 07:20:13.656298 140249676273408 logging_writer.py:48] [9022] global_step=9022, preemption_count=0, score=7243.246060, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7577.474138, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0330 07:20:13.830965 140421956904768 checkpoints.py:356] Saving checkpoint at step: 9022
I0330 07:20:14.735859 140421956904768 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_9022
I0330 07:20:14.753408 140421956904768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_9022.
I0330 07:20:14.760451 140421956904768 submission_runner.py:409] After logging and checkpointing eval at step 9022: RAM USED (GB) 20.638199808
I0330 07:21:13.379728 140249667880704 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1825.9427490234375
I0330 07:22:27.754140 140248370091776 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1807.7618408203125
I0330 07:23:45.849180 140249676273408 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1905.1661376953125
I0330 07:25:00.078126 140249667880704 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1847.1546630859375
I0330 07:26:14.285460 140249676273408 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1764.645751953125
I0330 07:27:28.510391 140249667880704 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1856.397216796875
I0330 07:28:42.799167 140249676273408 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1738.16650390625
I0330 07:29:57.056805 140249667880704 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1803.9403076171875
I0330 07:31:21.034492 140249676273408 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1765.863037109375
I0330 07:32:45.111768 140421956904768 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 22.268850176
I0330 07:32:45.111975 140421956904768 spec.py:298] Evaluating on the training split.
I0330 07:33:14.085444 140421956904768 spec.py:310] Evaluating on the validation split.
I0330 07:33:51.948578 140421956904768 spec.py:326] Evaluating on the test split.
I0330 07:34:11.245209 140421956904768 submission_runner.py:380] Time since start: 8414.57s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0330 07:34:11.246634 140421956904768 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 21.628100608
I0330 07:34:11.264385 140249676273408 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7991.616704, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=8414.569513, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0330 07:34:11.461991 140421956904768 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 07:34:12.368623 140421956904768 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_10000
I0330 07:34:12.386285 140421956904768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0330 07:34:12.397699 140421956904768 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.644582912
I0330 07:34:12.405754 140249667880704 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7991.616704
I0330 07:34:12.570116 140421956904768 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 07:34:13.798476 140421956904768 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_10000
I0330 07:34:13.816519 140421956904768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0330 07:34:14.944561 140421956904768 submission_runner.py:543] Tuning trial 1/1
I0330 07:34:14.944790 140421956904768 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0330 07:34:14.949151 140421956904768 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.950188, dtype=float32), 'train/wer': 1.4629500447702142, 'validation/ctc_loss': DeviceArray(30.86757, dtype=float32), 'validation/wer': 1.0566045017318064, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.897873, dtype=float32), 'test/wer': 1.1219304125281824, 'test/num_examples': 2472, 'score': 59.3489670753479, 'total_duration': 59.56274366378784, 'global_step': 1, 'preemption_count': 0}), (3018, {'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2454.0801768302917, 'total_duration': 2602.301237344742, 'global_step': 3018, 'preemption_count': 0}), (6014, {'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4848.438243865967, 'total_duration': 5089.708801269531, 'global_step': 6014, 'preemption_count': 0}), (9022, {'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7243.246059656143, 'total_duration': 7577.474138021469, 'global_step': 9022, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7991.616704463959, 'total_duration': 8414.569513320923, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0330 07:34:14.949323 140421956904768 submission_runner.py:546] Timing: 7991.616704463959
I0330 07:34:14.949378 140421956904768 submission_runner.py:547] ====================
I0330 07:34:14.949739 140421956904768 submission_runner.py:606] Final librispeech_conformer score: 7991.616704463959
