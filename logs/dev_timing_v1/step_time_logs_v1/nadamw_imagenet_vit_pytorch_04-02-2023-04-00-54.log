WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 04:01:16.746739 140558682449728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 04:01:16.746775 140491372799808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 04:01:16.746794 140604614244160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 04:01:16.747565 140648370685760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 04:01:16.747735 140250002519872 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 04:01:16.747777 140636033976128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 04:01:16.747869 139749048850240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 04:01:16.757827 140529317082944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 04:01:16.758047 140648370685760 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.758131 140529317082944 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.758267 140250002519872 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.758280 140636033976128 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.758460 139749048850240 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.767612 140491372799808 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.767641 140558682449728 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:16.767763 140604614244160 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 04:01:18.852779 140529317082944 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/imagenet_vit_pytorch.
W0402 04:01:18.890314 139749048850240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.891459 140648370685760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.891780 140558682449728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.892044 140529317082944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.892410 140250002519872 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.893280 140604614244160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.893967 140491372799808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 04:01:18.896402 140636033976128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 04:01:18.896859 140529317082944 submission_runner.py:511] Using RNG seed 3586983848
I0402 04:01:18.897856 140529317082944 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 04:01:18.897972 140529317082944 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1.
I0402 04:01:18.898176 140529317082944 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0402 04:01:18.899523 140529317082944 submission_runner.py:230] Starting train once: RAM USED (GB) 5.945126912
I0402 04:01:18.899623 140529317082944 submission_runner.py:231] Initializing dataset.
I0402 04:01:23.277634 140529317082944 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 8.00923648
I0402 04:01:23.277842 140529317082944 submission_runner.py:240] Initializing model.
I0402 04:01:27.700791 140529317082944 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.188890112
I0402 04:01:27.700989 140529317082944 submission_runner.py:252] Initializing optimizer.
I0402 04:01:27.702200 140529317082944 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.188890112
I0402 04:01:27.702327 140529317082944 submission_runner.py:261] Initializing metrics bundle.
I0402 04:01:27.702390 140529317082944 submission_runner.py:276] Initializing checkpoint and logger.
I0402 04:01:28.405782 140529317082944 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0402 04:01:28.406665 140529317082944 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0402 04:01:28.451912 140529317082944 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.242330624
I0402 04:01:28.452994 140529317082944 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.242330624
I0402 04:01:28.453113 140529317082944 submission_runner.py:313] Starting training loop.
I0402 04:01:30.955378 140529317082944 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.49602816
I0402 04:01:35.060882 140500610832128 logging_writer.py:48] [0] global_step=0, grad_norm=0.344756, loss=6.907756
I0402 04:01:35.075650 140529317082944 submission.py:296] 0) loss = 6.908, grad_norm = 0.345
I0402 04:01:35.076448 140529317082944 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.104828928
I0402 04:01:35.077047 140529317082944 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.105377792
I0402 04:01:35.077177 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:02:25.509307 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:03:10.002006 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:03:10.020752 140529317082944 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0402 04:03:10.027240 140529317082944 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0402 04:03:10.112385 140529317082944 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0402 04:03:23.315001 140529317082944 submission_runner.py:382] Time since start: 6.62s, 	Step: 1, 	{'train/accuracy': 0.00216796875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00178, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0016, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0402 04:03:23.316101 140529317082944 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.637384704
I0402 04:03:23.328109 140495460235008 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.622586, test/accuracy=0.001600, test/loss=6.907755, test/num_examples=10000, total_duration=6.624485, train/accuracy=0.002168, train/loss=6.907756, validation/accuracy=0.001780, validation/loss=6.907756, validation/num_examples=50000
I0402 04:03:23.760183 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0402 04:03:23.760893 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.639916032
I0402 04:03:23.776313 140529317082944 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.640923648
I0402 04:03:23.783148 140529317082944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783166 140604614244160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783154 140636033976128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783182 140558682449728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783182 140491372799808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783176 140648370685760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783190 139749048850240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:23.783763 140250002519872 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 04:03:24.366568 140495451842304 logging_writer.py:48] [1] global_step=1, grad_norm=0.355298, loss=6.907756
I0402 04:03:24.370222 140529317082944 submission.py:296] 1) loss = 6.908, grad_norm = 0.355
I0402 04:03:24.371315 140529317082944 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.660260864
I0402 04:03:24.786751 140495460235008 logging_writer.py:48] [2] global_step=2, grad_norm=0.353702, loss=6.907754
I0402 04:03:24.790384 140529317082944 submission.py:296] 2) loss = 6.908, grad_norm = 0.354
I0402 04:03:25.196028 140495451842304 logging_writer.py:48] [3] global_step=3, grad_norm=0.348545, loss=6.907751
I0402 04:03:25.199998 140529317082944 submission.py:296] 3) loss = 6.908, grad_norm = 0.349
I0402 04:03:25.603287 140495460235008 logging_writer.py:48] [4] global_step=4, grad_norm=0.347361, loss=6.907753
I0402 04:03:25.606796 140529317082944 submission.py:296] 4) loss = 6.908, grad_norm = 0.347
I0402 04:03:26.034055 140495451842304 logging_writer.py:48] [5] global_step=5, grad_norm=0.349244, loss=6.907754
I0402 04:03:26.037852 140529317082944 submission.py:296] 5) loss = 6.908, grad_norm = 0.349
I0402 04:03:26.443130 140495460235008 logging_writer.py:48] [6] global_step=6, grad_norm=0.353929, loss=6.907739
I0402 04:03:26.446485 140529317082944 submission.py:296] 6) loss = 6.908, grad_norm = 0.354
I0402 04:03:26.868911 140495451842304 logging_writer.py:48] [7] global_step=7, grad_norm=0.357985, loss=6.907744
I0402 04:03:26.873217 140529317082944 submission.py:296] 7) loss = 6.908, grad_norm = 0.358
I0402 04:03:27.298080 140495460235008 logging_writer.py:48] [8] global_step=8, grad_norm=0.355515, loss=6.907731
I0402 04:03:27.302194 140529317082944 submission.py:296] 8) loss = 6.908, grad_norm = 0.356
I0402 04:03:27.723452 140495451842304 logging_writer.py:48] [9] global_step=9, grad_norm=0.356768, loss=6.907744
I0402 04:03:27.727456 140529317082944 submission.py:296] 9) loss = 6.908, grad_norm = 0.357
I0402 04:03:28.135264 140495460235008 logging_writer.py:48] [10] global_step=10, grad_norm=0.347796, loss=6.907719
I0402 04:03:28.139383 140529317082944 submission.py:296] 10) loss = 6.908, grad_norm = 0.348
I0402 04:03:28.564068 140495451842304 logging_writer.py:48] [11] global_step=11, grad_norm=0.346557, loss=6.907711
I0402 04:03:28.567614 140529317082944 submission.py:296] 11) loss = 6.908, grad_norm = 0.347
I0402 04:03:29.000825 140495460235008 logging_writer.py:48] [12] global_step=12, grad_norm=0.348659, loss=6.907714
I0402 04:03:29.005044 140529317082944 submission.py:296] 12) loss = 6.908, grad_norm = 0.349
I0402 04:03:29.409878 140495451842304 logging_writer.py:48] [13] global_step=13, grad_norm=0.341066, loss=6.907698
I0402 04:03:29.414102 140529317082944 submission.py:296] 13) loss = 6.908, grad_norm = 0.341
I0402 04:03:29.821193 140495460235008 logging_writer.py:48] [14] global_step=14, grad_norm=0.351196, loss=6.907727
I0402 04:03:29.824897 140529317082944 submission.py:296] 14) loss = 6.908, grad_norm = 0.351
I0402 04:03:30.231336 140495451842304 logging_writer.py:48] [15] global_step=15, grad_norm=0.343498, loss=6.907702
I0402 04:03:30.235301 140529317082944 submission.py:296] 15) loss = 6.908, grad_norm = 0.343
I0402 04:03:30.673788 140495460235008 logging_writer.py:48] [16] global_step=16, grad_norm=0.336375, loss=6.907703
I0402 04:03:30.677538 140529317082944 submission.py:296] 16) loss = 6.908, grad_norm = 0.336
I0402 04:03:31.115564 140495451842304 logging_writer.py:48] [17] global_step=17, grad_norm=0.343095, loss=6.907659
I0402 04:03:31.119707 140529317082944 submission.py:296] 17) loss = 6.908, grad_norm = 0.343
I0402 04:03:31.526860 140495460235008 logging_writer.py:48] [18] global_step=18, grad_norm=0.350560, loss=6.907672
I0402 04:03:31.530287 140529317082944 submission.py:296] 18) loss = 6.908, grad_norm = 0.351
I0402 04:03:31.942185 140495451842304 logging_writer.py:48] [19] global_step=19, grad_norm=0.346350, loss=6.907672
I0402 04:03:31.946027 140529317082944 submission.py:296] 19) loss = 6.908, grad_norm = 0.346
I0402 04:03:32.354836 140495460235008 logging_writer.py:48] [20] global_step=20, grad_norm=0.344609, loss=6.907591
I0402 04:03:32.358498 140529317082944 submission.py:296] 20) loss = 6.908, grad_norm = 0.345
I0402 04:03:32.769515 140495451842304 logging_writer.py:48] [21] global_step=21, grad_norm=0.346240, loss=6.907574
I0402 04:03:32.773445 140529317082944 submission.py:296] 21) loss = 6.908, grad_norm = 0.346
I0402 04:03:33.185349 140495460235008 logging_writer.py:48] [22] global_step=22, grad_norm=0.344835, loss=6.907597
I0402 04:03:33.189373 140529317082944 submission.py:296] 22) loss = 6.908, grad_norm = 0.345
I0402 04:03:33.598835 140495451842304 logging_writer.py:48] [23] global_step=23, grad_norm=0.338022, loss=6.907573
I0402 04:03:33.602967 140529317082944 submission.py:296] 23) loss = 6.908, grad_norm = 0.338
I0402 04:03:34.026449 140495460235008 logging_writer.py:48] [24] global_step=24, grad_norm=0.355644, loss=6.907417
I0402 04:03:34.033534 140529317082944 submission.py:296] 24) loss = 6.907, grad_norm = 0.356
I0402 04:03:34.445547 140495451842304 logging_writer.py:48] [25] global_step=25, grad_norm=0.342621, loss=6.907490
I0402 04:03:34.451329 140529317082944 submission.py:296] 25) loss = 6.907, grad_norm = 0.343
I0402 04:03:34.876307 140495460235008 logging_writer.py:48] [26] global_step=26, grad_norm=0.344405, loss=6.907426
I0402 04:03:34.879890 140529317082944 submission.py:296] 26) loss = 6.907, grad_norm = 0.344
I0402 04:03:35.300530 140495451842304 logging_writer.py:48] [27] global_step=27, grad_norm=0.342988, loss=6.907538
I0402 04:03:35.303958 140529317082944 submission.py:296] 27) loss = 6.908, grad_norm = 0.343
I0402 04:03:35.734797 140495460235008 logging_writer.py:48] [28] global_step=28, grad_norm=0.350526, loss=6.907431
I0402 04:03:35.738378 140529317082944 submission.py:296] 28) loss = 6.907, grad_norm = 0.351
I0402 04:03:36.147456 140495451842304 logging_writer.py:48] [29] global_step=29, grad_norm=0.348594, loss=6.907548
I0402 04:03:36.150951 140529317082944 submission.py:296] 29) loss = 6.908, grad_norm = 0.349
I0402 04:03:36.569234 140495460235008 logging_writer.py:48] [30] global_step=30, grad_norm=0.358294, loss=6.907242
I0402 04:03:36.573242 140529317082944 submission.py:296] 30) loss = 6.907, grad_norm = 0.358
I0402 04:03:36.988011 140495451842304 logging_writer.py:48] [31] global_step=31, grad_norm=0.359197, loss=6.907347
I0402 04:03:36.991952 140529317082944 submission.py:296] 31) loss = 6.907, grad_norm = 0.359
I0402 04:03:37.399140 140495460235008 logging_writer.py:48] [32] global_step=32, grad_norm=0.334273, loss=6.907404
I0402 04:03:37.407078 140529317082944 submission.py:296] 32) loss = 6.907, grad_norm = 0.334
I0402 04:03:37.813455 140495451842304 logging_writer.py:48] [33] global_step=33, grad_norm=0.357746, loss=6.907232
I0402 04:03:37.820301 140529317082944 submission.py:296] 33) loss = 6.907, grad_norm = 0.358
I0402 04:03:38.246339 140495460235008 logging_writer.py:48] [34] global_step=34, grad_norm=0.350916, loss=6.907211
I0402 04:03:38.252869 140529317082944 submission.py:296] 34) loss = 6.907, grad_norm = 0.351
I0402 04:03:38.671909 140495451842304 logging_writer.py:48] [35] global_step=35, grad_norm=0.343670, loss=6.907231
I0402 04:03:38.675470 140529317082944 submission.py:296] 35) loss = 6.907, grad_norm = 0.344
I0402 04:03:39.104154 140495460235008 logging_writer.py:48] [36] global_step=36, grad_norm=0.364259, loss=6.907214
I0402 04:03:39.107928 140529317082944 submission.py:296] 36) loss = 6.907, grad_norm = 0.364
I0402 04:03:39.520300 140495451842304 logging_writer.py:48] [37] global_step=37, grad_norm=0.345416, loss=6.907278
I0402 04:03:39.523831 140529317082944 submission.py:296] 37) loss = 6.907, grad_norm = 0.345
I0402 04:03:39.932865 140495460235008 logging_writer.py:48] [38] global_step=38, grad_norm=0.354059, loss=6.906997
I0402 04:03:39.937230 140529317082944 submission.py:296] 38) loss = 6.907, grad_norm = 0.354
I0402 04:03:40.346635 140495451842304 logging_writer.py:48] [39] global_step=39, grad_norm=0.355536, loss=6.906951
I0402 04:03:40.352648 140529317082944 submission.py:296] 39) loss = 6.907, grad_norm = 0.356
I0402 04:03:40.792145 140495460235008 logging_writer.py:48] [40] global_step=40, grad_norm=0.351881, loss=6.906965
I0402 04:03:40.795730 140529317082944 submission.py:296] 40) loss = 6.907, grad_norm = 0.352
I0402 04:03:41.203549 140495451842304 logging_writer.py:48] [41] global_step=41, grad_norm=0.344009, loss=6.906961
I0402 04:03:41.208430 140529317082944 submission.py:296] 41) loss = 6.907, grad_norm = 0.344
I0402 04:03:41.617708 140495460235008 logging_writer.py:48] [42] global_step=42, grad_norm=0.361528, loss=6.906735
I0402 04:03:41.621399 140529317082944 submission.py:296] 42) loss = 6.907, grad_norm = 0.362
I0402 04:03:42.038175 140495451842304 logging_writer.py:48] [43] global_step=43, grad_norm=0.364247, loss=6.906473
I0402 04:03:42.041992 140529317082944 submission.py:296] 43) loss = 6.906, grad_norm = 0.364
I0402 04:03:42.455870 140495460235008 logging_writer.py:48] [44] global_step=44, grad_norm=0.346527, loss=6.906638
I0402 04:03:42.460959 140529317082944 submission.py:296] 44) loss = 6.907, grad_norm = 0.347
I0402 04:03:42.870392 140495451842304 logging_writer.py:48] [45] global_step=45, grad_norm=0.355741, loss=6.906497
I0402 04:03:42.877197 140529317082944 submission.py:296] 45) loss = 6.906, grad_norm = 0.356
I0402 04:03:43.320344 140495460235008 logging_writer.py:48] [46] global_step=46, grad_norm=0.349519, loss=6.906579
I0402 04:03:43.324929 140529317082944 submission.py:296] 46) loss = 6.907, grad_norm = 0.350
I0402 04:03:43.734401 140495451842304 logging_writer.py:48] [47] global_step=47, grad_norm=0.365720, loss=6.906085
I0402 04:03:43.738017 140529317082944 submission.py:296] 47) loss = 6.906, grad_norm = 0.366
I0402 04:03:44.159464 140495460235008 logging_writer.py:48] [48] global_step=48, grad_norm=0.362370, loss=6.906091
I0402 04:03:44.163199 140529317082944 submission.py:296] 48) loss = 6.906, grad_norm = 0.362
I0402 04:03:44.578505 140495451842304 logging_writer.py:48] [49] global_step=49, grad_norm=0.384668, loss=6.906398
I0402 04:03:44.582751 140529317082944 submission.py:296] 49) loss = 6.906, grad_norm = 0.385
I0402 04:03:44.992472 140495460235008 logging_writer.py:48] [50] global_step=50, grad_norm=0.374339, loss=6.906270
I0402 04:03:45.000551 140529317082944 submission.py:296] 50) loss = 6.906, grad_norm = 0.374
I0402 04:03:45.408170 140495451842304 logging_writer.py:48] [51] global_step=51, grad_norm=0.386886, loss=6.905376
I0402 04:03:45.412406 140529317082944 submission.py:296] 51) loss = 6.905, grad_norm = 0.387
I0402 04:03:45.840766 140495460235008 logging_writer.py:48] [52] global_step=52, grad_norm=0.360572, loss=6.906023
I0402 04:03:45.844679 140529317082944 submission.py:296] 52) loss = 6.906, grad_norm = 0.361
I0402 04:03:46.260111 140495451842304 logging_writer.py:48] [53] global_step=53, grad_norm=0.374834, loss=6.905557
I0402 04:03:46.263582 140529317082944 submission.py:296] 53) loss = 6.906, grad_norm = 0.375
I0402 04:03:46.682969 140495460235008 logging_writer.py:48] [54] global_step=54, grad_norm=0.368428, loss=6.905274
I0402 04:03:46.686398 140529317082944 submission.py:296] 54) loss = 6.905, grad_norm = 0.368
I0402 04:03:47.092256 140495451842304 logging_writer.py:48] [55] global_step=55, grad_norm=0.400762, loss=6.905262
I0402 04:03:47.095857 140529317082944 submission.py:296] 55) loss = 6.905, grad_norm = 0.401
I0402 04:03:47.533270 140495460235008 logging_writer.py:48] [56] global_step=56, grad_norm=0.392424, loss=6.905124
I0402 04:03:47.537346 140529317082944 submission.py:296] 56) loss = 6.905, grad_norm = 0.392
I0402 04:03:47.950978 140495451842304 logging_writer.py:48] [57] global_step=57, grad_norm=0.398777, loss=6.904926
I0402 04:03:47.957536 140529317082944 submission.py:296] 57) loss = 6.905, grad_norm = 0.399
I0402 04:03:48.369614 140495460235008 logging_writer.py:48] [58] global_step=58, grad_norm=0.401868, loss=6.905025
I0402 04:03:48.373618 140529317082944 submission.py:296] 58) loss = 6.905, grad_norm = 0.402
I0402 04:03:48.780080 140495451842304 logging_writer.py:48] [59] global_step=59, grad_norm=0.367494, loss=6.905173
I0402 04:03:48.783800 140529317082944 submission.py:296] 59) loss = 6.905, grad_norm = 0.367
I0402 04:03:49.197614 140495460235008 logging_writer.py:48] [60] global_step=60, grad_norm=0.391354, loss=6.904215
I0402 04:03:49.201627 140529317082944 submission.py:296] 60) loss = 6.904, grad_norm = 0.391
I0402 04:03:49.633429 140495451842304 logging_writer.py:48] [61] global_step=61, grad_norm=0.408400, loss=6.904236
I0402 04:03:49.637305 140529317082944 submission.py:296] 61) loss = 6.904, grad_norm = 0.408
I0402 04:03:50.045074 140495460235008 logging_writer.py:48] [62] global_step=62, grad_norm=0.382137, loss=6.904672
I0402 04:03:50.048500 140529317082944 submission.py:296] 62) loss = 6.905, grad_norm = 0.382
I0402 04:03:50.460761 140495451842304 logging_writer.py:48] [63] global_step=63, grad_norm=0.406173, loss=6.903471
I0402 04:03:50.464749 140529317082944 submission.py:296] 63) loss = 6.903, grad_norm = 0.406
I0402 04:03:50.873182 140495460235008 logging_writer.py:48] [64] global_step=64, grad_norm=0.409232, loss=6.903733
I0402 04:03:50.877113 140529317082944 submission.py:296] 64) loss = 6.904, grad_norm = 0.409
I0402 04:03:51.286539 140495451842304 logging_writer.py:48] [65] global_step=65, grad_norm=0.389070, loss=6.905115
I0402 04:03:51.290538 140529317082944 submission.py:296] 65) loss = 6.905, grad_norm = 0.389
I0402 04:03:51.727398 140495460235008 logging_writer.py:48] [66] global_step=66, grad_norm=0.399881, loss=6.903179
I0402 04:03:51.731251 140529317082944 submission.py:296] 66) loss = 6.903, grad_norm = 0.400
I0402 04:03:52.138526 140495451842304 logging_writer.py:48] [67] global_step=67, grad_norm=0.402061, loss=6.903952
I0402 04:03:52.142013 140529317082944 submission.py:296] 67) loss = 6.904, grad_norm = 0.402
I0402 04:03:52.550668 140495460235008 logging_writer.py:48] [68] global_step=68, grad_norm=0.405390, loss=6.902677
I0402 04:03:52.554703 140529317082944 submission.py:296] 68) loss = 6.903, grad_norm = 0.405
I0402 04:03:52.965779 140495451842304 logging_writer.py:48] [69] global_step=69, grad_norm=0.389941, loss=6.904617
I0402 04:03:52.969610 140529317082944 submission.py:296] 69) loss = 6.905, grad_norm = 0.390
I0402 04:03:53.380342 140495460235008 logging_writer.py:48] [70] global_step=70, grad_norm=0.420232, loss=6.902768
I0402 04:03:53.384342 140529317082944 submission.py:296] 70) loss = 6.903, grad_norm = 0.420
I0402 04:03:53.797587 140495451842304 logging_writer.py:48] [71] global_step=71, grad_norm=0.434891, loss=6.902490
I0402 04:03:53.801186 140529317082944 submission.py:296] 71) loss = 6.902, grad_norm = 0.435
I0402 04:03:54.218675 140495460235008 logging_writer.py:48] [72] global_step=72, grad_norm=0.429560, loss=6.901951
I0402 04:03:54.222105 140529317082944 submission.py:296] 72) loss = 6.902, grad_norm = 0.430
I0402 04:03:54.631830 140495451842304 logging_writer.py:48] [73] global_step=73, grad_norm=0.403455, loss=6.901618
I0402 04:03:54.638694 140529317082944 submission.py:296] 73) loss = 6.902, grad_norm = 0.403
I0402 04:03:55.048961 140495460235008 logging_writer.py:48] [74] global_step=74, grad_norm=0.418980, loss=6.901921
I0402 04:03:55.052803 140529317082944 submission.py:296] 74) loss = 6.902, grad_norm = 0.419
I0402 04:03:55.463083 140495451842304 logging_writer.py:48] [75] global_step=75, grad_norm=0.407321, loss=6.900504
I0402 04:03:55.467088 140529317082944 submission.py:296] 75) loss = 6.901, grad_norm = 0.407
I0402 04:03:55.892833 140495460235008 logging_writer.py:48] [76] global_step=76, grad_norm=0.424293, loss=6.901751
I0402 04:03:55.896479 140529317082944 submission.py:296] 76) loss = 6.902, grad_norm = 0.424
I0402 04:03:56.305266 140495451842304 logging_writer.py:48] [77] global_step=77, grad_norm=0.417514, loss=6.900416
I0402 04:03:56.309069 140529317082944 submission.py:296] 77) loss = 6.900, grad_norm = 0.418
I0402 04:03:56.718685 140495460235008 logging_writer.py:48] [78] global_step=78, grad_norm=0.430406, loss=6.900889
I0402 04:03:56.722319 140529317082944 submission.py:296] 78) loss = 6.901, grad_norm = 0.430
I0402 04:03:57.131567 140495451842304 logging_writer.py:48] [79] global_step=79, grad_norm=0.425115, loss=6.900299
I0402 04:03:57.135676 140529317082944 submission.py:296] 79) loss = 6.900, grad_norm = 0.425
I0402 04:03:57.558124 140495460235008 logging_writer.py:48] [80] global_step=80, grad_norm=0.428903, loss=6.899507
I0402 04:03:57.562196 140529317082944 submission.py:296] 80) loss = 6.900, grad_norm = 0.429
I0402 04:03:57.981661 140495451842304 logging_writer.py:48] [81] global_step=81, grad_norm=0.425902, loss=6.899107
I0402 04:03:57.985495 140529317082944 submission.py:296] 81) loss = 6.899, grad_norm = 0.426
I0402 04:03:58.395281 140495460235008 logging_writer.py:48] [82] global_step=82, grad_norm=0.411953, loss=6.899789
I0402 04:03:58.398799 140529317082944 submission.py:296] 82) loss = 6.900, grad_norm = 0.412
I0402 04:03:58.818732 140495451842304 logging_writer.py:48] [83] global_step=83, grad_norm=0.408807, loss=6.900183
I0402 04:03:58.822757 140529317082944 submission.py:296] 83) loss = 6.900, grad_norm = 0.409
I0402 04:03:59.254976 140495460235008 logging_writer.py:48] [84] global_step=84, grad_norm=0.443951, loss=6.899144
I0402 04:03:59.259194 140529317082944 submission.py:296] 84) loss = 6.899, grad_norm = 0.444
I0402 04:03:59.668596 140495451842304 logging_writer.py:48] [85] global_step=85, grad_norm=0.454171, loss=6.897887
I0402 04:03:59.672551 140529317082944 submission.py:296] 85) loss = 6.898, grad_norm = 0.454
I0402 04:04:00.106527 140495460235008 logging_writer.py:48] [86] global_step=86, grad_norm=0.416499, loss=6.898444
I0402 04:04:00.110269 140529317082944 submission.py:296] 86) loss = 6.898, grad_norm = 0.416
I0402 04:04:00.528302 140495451842304 logging_writer.py:48] [87] global_step=87, grad_norm=0.418445, loss=6.897890
I0402 04:04:00.532174 140529317082944 submission.py:296] 87) loss = 6.898, grad_norm = 0.418
I0402 04:04:00.942575 140495460235008 logging_writer.py:48] [88] global_step=88, grad_norm=0.425302, loss=6.898222
I0402 04:04:00.946381 140529317082944 submission.py:296] 88) loss = 6.898, grad_norm = 0.425
I0402 04:04:01.378791 140495451842304 logging_writer.py:48] [89] global_step=89, grad_norm=0.446130, loss=6.896722
I0402 04:04:01.382535 140529317082944 submission.py:296] 89) loss = 6.897, grad_norm = 0.446
I0402 04:04:01.796692 140495460235008 logging_writer.py:48] [90] global_step=90, grad_norm=0.465350, loss=6.897055
I0402 04:04:01.800654 140529317082944 submission.py:296] 90) loss = 6.897, grad_norm = 0.465
I0402 04:04:02.212887 140495451842304 logging_writer.py:48] [91] global_step=91, grad_norm=0.450296, loss=6.897391
I0402 04:04:02.216326 140529317082944 submission.py:296] 91) loss = 6.897, grad_norm = 0.450
I0402 04:04:02.631673 140495460235008 logging_writer.py:48] [92] global_step=92, grad_norm=0.458184, loss=6.895683
I0402 04:04:02.636697 140529317082944 submission.py:296] 92) loss = 6.896, grad_norm = 0.458
I0402 04:04:03.044834 140495451842304 logging_writer.py:48] [93] global_step=93, grad_norm=0.429986, loss=6.897089
I0402 04:04:03.053101 140529317082944 submission.py:296] 93) loss = 6.897, grad_norm = 0.430
I0402 04:04:03.478601 140495460235008 logging_writer.py:48] [94] global_step=94, grad_norm=0.425103, loss=6.897452
I0402 04:04:03.482408 140529317082944 submission.py:296] 94) loss = 6.897, grad_norm = 0.425
I0402 04:04:03.889583 140495451842304 logging_writer.py:48] [95] global_step=95, grad_norm=0.454900, loss=6.895141
I0402 04:04:03.893704 140529317082944 submission.py:296] 95) loss = 6.895, grad_norm = 0.455
I0402 04:04:04.310926 140495460235008 logging_writer.py:48] [96] global_step=96, grad_norm=0.444614, loss=6.894202
I0402 04:04:04.314329 140529317082944 submission.py:296] 96) loss = 6.894, grad_norm = 0.445
I0402 04:04:04.728266 140495451842304 logging_writer.py:48] [97] global_step=97, grad_norm=0.438314, loss=6.895472
I0402 04:04:04.732187 140529317082944 submission.py:296] 97) loss = 6.895, grad_norm = 0.438
I0402 04:04:05.149388 140495460235008 logging_writer.py:48] [98] global_step=98, grad_norm=0.454019, loss=6.892481
I0402 04:04:05.153758 140529317082944 submission.py:296] 98) loss = 6.892, grad_norm = 0.454
I0402 04:04:05.572442 140495451842304 logging_writer.py:48] [99] global_step=99, grad_norm=0.426722, loss=6.895313
I0402 04:04:05.576443 140529317082944 submission.py:296] 99) loss = 6.895, grad_norm = 0.427
I0402 04:04:05.984088 140495460235008 logging_writer.py:48] [100] global_step=100, grad_norm=0.451154, loss=6.894553
I0402 04:04:05.988245 140529317082944 submission.py:296] 100) loss = 6.895, grad_norm = 0.451
I0402 04:06:50.033624 140495451842304 logging_writer.py:48] [500] global_step=500, grad_norm=1.127360, loss=6.666419
I0402 04:06:50.037869 140529317082944 submission.py:296] 500) loss = 6.666, grad_norm = 1.127
I0402 04:10:14.662806 140495460235008 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.928871, loss=6.333000
I0402 04:10:14.668392 140529317082944 submission.py:296] 1000) loss = 6.333, grad_norm = 0.929
I0402 04:10:24.127444 140529317082944 submission_runner.py:373] Before eval at step 1024: RAM USED (GB) 98.715373568
I0402 04:10:24.127695 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:11:08.521188 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:11:52.924637 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:11:54.350807 140529317082944 submission_runner.py:382] Time since start: 535.67s, 	Step: 1024, 	{'train/accuracy': 0.04453125, 'train/loss': 5.8090283203125, 'validation/accuracy': 0.04154, 'validation/loss': 5.8348875, 'validation/num_examples': 50000, 'test/accuracy': 0.0326, 'test/loss': 5.946262109375, 'test/num_examples': 10000}
I0402 04:11:54.351162 140529317082944 submission_runner.py:396] After eval at step 1024: RAM USED (GB) 98.839416832
I0402 04:11:54.359230 140484345325312 logging_writer.py:48] [1024] global_step=1024, preemption_count=0, score=424.673873, test/accuracy=0.032600, test/loss=5.946262, test/num_examples=10000, total_duration=535.673910, train/accuracy=0.044531, train/loss=5.809028, validation/accuracy=0.041540, validation/loss=5.834887, validation/num_examples=50000
I0402 04:11:54.788655 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_1024.
I0402 04:11:54.789429 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 1024: RAM USED (GB) 98.839015424
I0402 04:15:12.104337 140484353718016 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.233479, loss=6.224576
I0402 04:15:12.117082 140529317082944 submission.py:296] 1500) loss = 6.225, grad_norm = 1.233
I0402 04:18:37.220898 140484345325312 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.120776, loss=6.172032
I0402 04:18:37.225772 140529317082944 submission.py:296] 2000) loss = 6.172, grad_norm = 1.121
I0402 04:18:54.833054 140529317082944 submission_runner.py:373] Before eval at step 2044: RAM USED (GB) 100.171259904
I0402 04:18:54.833271 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:19:38.653076 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:20:23.530587 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:20:24.940260 140529317082944 submission_runner.py:382] Time since start: 1046.38s, 	Step: 2044, 	{'train/accuracy': 0.09740234375, 'train/loss': 5.091727294921875, 'validation/accuracy': 0.0883, 'validation/loss': 5.143273125, 'validation/num_examples': 50000, 'test/accuracy': 0.0715, 'test/loss': 5.34801328125, 'test/num_examples': 10000}
I0402 04:20:24.940608 140529317082944 submission_runner.py:396] After eval at step 2044: RAM USED (GB) 100.27040768
I0402 04:20:24.948458 140484353718016 logging_writer.py:48] [2044] global_step=2044, preemption_count=0, score=842.275666, test/accuracy=0.071500, test/loss=5.348013, test/num_examples=10000, total_duration=1046.379716, train/accuracy=0.097402, train/loss=5.091727, validation/accuracy=0.088300, validation/loss=5.143273, validation/num_examples=50000
I0402 04:20:25.375389 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_2044.
I0402 04:20:25.376145 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 2044: RAM USED (GB) 100.269498368
I0402 04:23:32.073047 140484345325312 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.212285, loss=5.716625
I0402 04:23:32.076502 140529317082944 submission.py:296] 2500) loss = 5.717, grad_norm = 1.212
I0402 04:26:59.178831 140484353718016 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.709163, loss=5.863216
I0402 04:26:59.184640 140529317082944 submission.py:296] 3000) loss = 5.863, grad_norm = 1.709
I0402 04:27:25.463724 140529317082944 submission_runner.py:373] Before eval at step 3065: RAM USED (GB) 100.619784192
I0402 04:27:25.464022 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:28:08.842771 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:28:53.330152 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:28:54.739344 140529317082944 submission_runner.py:382] Time since start: 1557.01s, 	Step: 3065, 	{'train/accuracy': 0.16751953125, 'train/loss': 4.467237854003907, 'validation/accuracy': 0.15388, 'validation/loss': 4.5403503125, 'validation/num_examples': 50000, 'test/accuracy': 0.1162, 'test/loss': 4.85844140625, 'test/num_examples': 10000}
I0402 04:28:54.739691 140529317082944 submission_runner.py:396] After eval at step 3065: RAM USED (GB) 100.643033088
I0402 04:28:54.747337 140484345325312 logging_writer.py:48] [3065] global_step=3065, preemption_count=0, score=1259.937800, test/accuracy=0.116200, test/loss=4.858441, test/num_examples=10000, total_duration=1557.009630, train/accuracy=0.167520, train/loss=4.467238, validation/accuracy=0.153880, validation/loss=4.540350, validation/num_examples=50000
I0402 04:28:55.174415 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_3065.
I0402 04:28:55.175171 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 3065: RAM USED (GB) 100.642123776
I0402 04:31:54.121849 140484353718016 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.372908, loss=5.204397
I0402 04:31:54.126841 140529317082944 submission.py:296] 3500) loss = 5.204, grad_norm = 1.373
I0402 04:35:20.891079 140484345325312 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.066763, loss=5.350813
I0402 04:35:20.897081 140529317082944 submission.py:296] 4000) loss = 5.351, grad_norm = 1.067
I0402 04:35:55.377108 140529317082944 submission_runner.py:373] Before eval at step 4085: RAM USED (GB) 100.78937088
I0402 04:35:55.377379 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:36:39.421663 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:37:24.749331 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:37:26.159733 140529317082944 submission_runner.py:382] Time since start: 2066.92s, 	Step: 4085, 	{'train/accuracy': 0.23513671875, 'train/loss': 3.9353451538085937, 'validation/accuracy': 0.21558, 'validation/loss': 4.042288125, 'validation/num_examples': 50000, 'test/accuracy': 0.1663, 'test/loss': 4.42484765625, 'test/num_examples': 10000}
I0402 04:37:26.160143 140529317082944 submission_runner.py:396] After eval at step 4085: RAM USED (GB) 100.726386688
I0402 04:37:26.168061 140484353718016 logging_writer.py:48] [4085] global_step=4085, preemption_count=0, score=1677.731514, test/accuracy=0.166300, test/loss=4.424848, test/num_examples=10000, total_duration=2066.923481, train/accuracy=0.235137, train/loss=3.935345, validation/accuracy=0.215580, validation/loss=4.042288, validation/num_examples=50000
I0402 04:37:26.601581 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_4085.
I0402 04:37:26.602365 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 4085: RAM USED (GB) 100.725477376
I0402 04:40:17.571999 140484345325312 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.270925, loss=4.941376
I0402 04:40:17.577120 140529317082944 submission.py:296] 4500) loss = 4.941, grad_norm = 1.271
I0402 04:43:42.052250 140484353718016 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.138335, loss=5.239939
I0402 04:43:42.056656 140529317082944 submission.py:296] 5000) loss = 5.240, grad_norm = 1.138
I0402 04:44:26.906655 140529317082944 submission_runner.py:373] Before eval at step 5105: RAM USED (GB) 100.83145728
I0402 04:44:26.906857 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:45:10.905932 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:45:55.776774 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:45:57.190104 140529317082944 submission_runner.py:382] Time since start: 2578.45s, 	Step: 5105, 	{'train/accuracy': 0.289765625, 'train/loss': 3.508529052734375, 'validation/accuracy': 0.26488, 'validation/loss': 3.6356446875, 'validation/num_examples': 50000, 'test/accuracy': 0.2049, 'test/loss': 4.0719703125, 'test/num_examples': 10000}
I0402 04:45:57.190469 140529317082944 submission_runner.py:396] After eval at step 5105: RAM USED (GB) 100.814434304
I0402 04:45:57.199008 140484345325312 logging_writer.py:48] [5105] global_step=5105, preemption_count=0, score=2095.675435, test/accuracy=0.204900, test/loss=4.071970, test/num_examples=10000, total_duration=2578.453086, train/accuracy=0.289766, train/loss=3.508529, validation/accuracy=0.264880, validation/loss=3.635645, validation/num_examples=50000
I0402 04:45:57.630098 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_5105.
I0402 04:45:57.630860 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 5105: RAM USED (GB) 100.813774848
I0402 04:48:40.339144 140484353718016 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.036158, loss=4.858585
I0402 04:48:40.342990 140529317082944 submission.py:296] 5500) loss = 4.859, grad_norm = 1.036
I0402 04:52:05.717418 140484345325312 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.201254, loss=4.730804
I0402 04:52:05.721553 140529317082944 submission.py:296] 6000) loss = 4.731, grad_norm = 1.201
I0402 04:52:57.700252 140529317082944 submission_runner.py:373] Before eval at step 6128: RAM USED (GB) 101.105762304
I0402 04:52:57.700617 140529317082944 spec.py:298] Evaluating on the training split.
I0402 04:53:41.490696 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 04:54:26.044340 140529317082944 spec.py:326] Evaluating on the test split.
I0402 04:54:27.458261 140529317082944 submission_runner.py:382] Time since start: 3089.25s, 	Step: 6128, 	{'train/accuracy': 0.3385546875, 'train/loss': 3.226836853027344, 'validation/accuracy': 0.3163, 'validation/loss': 3.3673534375, 'validation/num_examples': 50000, 'test/accuracy': 0.2405, 'test/loss': 3.854529296875, 'test/num_examples': 10000}
I0402 04:54:27.458619 140529317082944 submission_runner.py:396] After eval at step 6128: RAM USED (GB) 101.000384512
I0402 04:54:27.466684 140484353718016 logging_writer.py:48] [6128] global_step=6128, preemption_count=0, score=2513.328995, test/accuracy=0.240500, test/loss=3.854529, test/num_examples=10000, total_duration=3089.246101, train/accuracy=0.338555, train/loss=3.226837, validation/accuracy=0.316300, validation/loss=3.367353, validation/num_examples=50000
I0402 04:54:27.895148 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_6128.
I0402 04:54:27.895867 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 6128: RAM USED (GB) 101.001269248
I0402 04:57:02.459581 140484345325312 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.177159, loss=4.629920
I0402 04:57:02.463382 140529317082944 submission.py:296] 6500) loss = 4.630, grad_norm = 1.177
I0402 05:00:27.593183 140484353718016 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.932152, loss=4.902489
I0402 05:00:27.597678 140529317082944 submission.py:296] 7000) loss = 4.902, grad_norm = 0.932
I0402 05:01:28.088419 140529317082944 submission_runner.py:373] Before eval at step 7148: RAM USED (GB) 100.971454464
I0402 05:01:28.088672 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:02:11.646997 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:02:55.975150 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:02:57.382517 140529317082944 submission_runner.py:382] Time since start: 3599.63s, 	Step: 7148, 	{'train/accuracy': 0.3862109375, 'train/loss': 2.879422607421875, 'validation/accuracy': 0.35908, 'validation/loss': 3.04332, 'validation/num_examples': 50000, 'test/accuracy': 0.2752, 'test/loss': 3.577373828125, 'test/num_examples': 10000}
I0402 05:02:57.382865 140529317082944 submission_runner.py:396] After eval at step 7148: RAM USED (GB) 100.919914496
I0402 05:02:57.391510 140484345325312 logging_writer.py:48] [7148] global_step=7148, preemption_count=0, score=2931.133197, test/accuracy=0.275200, test/loss=3.577374, test/num_examples=10000, total_duration=3599.634580, train/accuracy=0.386211, train/loss=2.879423, validation/accuracy=0.359080, validation/loss=3.043320, validation/num_examples=50000
I0402 05:02:57.818091 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_7148.
I0402 05:02:57.818903 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 7148: RAM USED (GB) 100.919005184
I0402 05:05:22.118977 140484353718016 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.022005, loss=4.797717
I0402 05:05:22.122755 140529317082944 submission.py:296] 7500) loss = 4.798, grad_norm = 1.022
I0402 05:08:49.655204 140484345325312 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.964111, loss=4.686460
I0402 05:08:49.659903 140529317082944 submission.py:296] 8000) loss = 4.686, grad_norm = 0.964
I0402 05:09:58.164590 140529317082944 submission_runner.py:373] Before eval at step 8168: RAM USED (GB) 101.050724352
I0402 05:09:58.164844 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:10:41.981267 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:11:26.540558 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:11:27.955097 140529317082944 submission_runner.py:382] Time since start: 4109.71s, 	Step: 8168, 	{'train/accuracy': 0.4228515625, 'train/loss': 2.75065673828125, 'validation/accuracy': 0.38682, 'validation/loss': 2.922659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3003, 'test/loss': 3.464353125, 'test/num_examples': 10000}
I0402 05:11:27.955449 140529317082944 submission_runner.py:396] After eval at step 8168: RAM USED (GB) 101.16466688
I0402 05:11:27.964310 140484353718016 logging_writer.py:48] [8168] global_step=8168, preemption_count=0, score=3349.114241, test/accuracy=0.300300, test/loss=3.464353, test/num_examples=10000, total_duration=4109.711232, train/accuracy=0.422852, train/loss=2.750657, validation/accuracy=0.386820, validation/loss=2.922659, validation/num_examples=50000
I0402 05:11:28.386319 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_8168.
I0402 05:11:28.387103 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 8168: RAM USED (GB) 101.163757568
I0402 05:13:44.743337 140484345325312 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.887911, loss=4.663234
I0402 05:13:44.747200 140529317082944 submission.py:296] 8500) loss = 4.663, grad_norm = 0.888
I0402 05:17:11.865756 140484353718016 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.904051, loss=4.324605
I0402 05:17:11.870046 140529317082944 submission.py:296] 9000) loss = 4.325, grad_norm = 0.904
I0402 05:18:28.698911 140529317082944 submission_runner.py:373] Before eval at step 9188: RAM USED (GB) 101.02683648
I0402 05:18:28.699211 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:19:12.333641 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:19:56.433968 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:19:57.844509 140529317082944 submission_runner.py:382] Time since start: 4620.25s, 	Step: 9188, 	{'train/accuracy': 0.46013671875, 'train/loss': 2.498959655761719, 'validation/accuracy': 0.42114, 'validation/loss': 2.6824925, 'validation/num_examples': 50000, 'test/accuracy': 0.3279, 'test/loss': 3.2574310546875, 'test/num_examples': 10000}
I0402 05:19:57.844857 140529317082944 submission_runner.py:396] After eval at step 9188: RAM USED (GB) 101.001846784
I0402 05:19:57.852687 140484345325312 logging_writer.py:48] [9188] global_step=9188, preemption_count=0, score=3767.095798, test/accuracy=0.327900, test/loss=3.257431, test/num_examples=10000, total_duration=4620.245406, train/accuracy=0.460137, train/loss=2.498960, validation/accuracy=0.421140, validation/loss=2.682492, validation/num_examples=50000
I0402 05:19:58.274963 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_9188.
I0402 05:19:58.275694 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 9188: RAM USED (GB) 101.000937472
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 05:22:06.624559 140484353718016 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.822389, loss=4.580528
I0402 05:22:06.629121 140529317082944 submission.py:296] 9500) loss = 4.581, grad_norm = 0.822
I0402 05:25:31.502365 140484345325312 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.959174, loss=3.909626
I0402 05:25:31.506420 140529317082944 submission.py:296] 10000) loss = 3.910, grad_norm = 0.959
I0402 05:26:58.641203 140529317082944 submission_runner.py:373] Before eval at step 10209: RAM USED (GB) 101.036199936
I0402 05:26:58.641430 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:27:42.454873 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:28:28.226759 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:28:29.638359 140529317082944 submission_runner.py:382] Time since start: 5130.19s, 	Step: 10209, 	{'train/accuracy': 0.4911328125, 'train/loss': 2.336753234863281, 'validation/accuracy': 0.4541, 'validation/loss': 2.53005015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3521, 'test/loss': 3.121391015625, 'test/num_examples': 10000}
I0402 05:28:29.638743 140529317082944 submission_runner.py:396] After eval at step 10209: RAM USED (GB) 101.064368128
I0402 05:28:29.646719 140484353718016 logging_writer.py:48] [10209] global_step=10209, preemption_count=0, score=4185.083998, test/accuracy=0.352100, test/loss=3.121391, test/num_examples=10000, total_duration=5130.187776, train/accuracy=0.491133, train/loss=2.336753, validation/accuracy=0.454100, validation/loss=2.530050, validation/num_examples=50000
I0402 05:28:30.071915 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_10209.
I0402 05:28:30.072729 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 10209: RAM USED (GB) 101.06345472
I0402 05:30:29.906579 140484345325312 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.873142, loss=4.377519
I0402 05:30:29.910518 140529317082944 submission.py:296] 10500) loss = 4.378, grad_norm = 0.873
I0402 05:33:54.909340 140484353718016 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.715349, loss=4.312356
I0402 05:33:54.915823 140529317082944 submission.py:296] 11000) loss = 4.312, grad_norm = 0.715
I0402 05:35:30.459332 140529317082944 submission_runner.py:373] Before eval at step 11235: RAM USED (GB) 101.064486912
I0402 05:35:30.459598 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:36:14.242110 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:37:01.949234 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:37:03.357148 140529317082944 submission_runner.py:382] Time since start: 5642.01s, 	Step: 11235, 	{'train/accuracy': 0.52154296875, 'train/loss': 2.170928497314453, 'validation/accuracy': 0.47858, 'validation/loss': 2.37556796875, 'validation/num_examples': 50000, 'test/accuracy': 0.3674, 'test/loss': 2.9961392578125, 'test/num_examples': 10000}
I0402 05:37:03.357521 140529317082944 submission_runner.py:396] After eval at step 11235: RAM USED (GB) 100.966580224
I0402 05:37:03.365278 140484345325312 logging_writer.py:48] [11235] global_step=11235, preemption_count=0, score=4603.111119, test/accuracy=0.367400, test/loss=2.996139, test/num_examples=10000, total_duration=5642.005788, train/accuracy=0.521543, train/loss=2.170928, validation/accuracy=0.478580, validation/loss=2.375568, validation/num_examples=50000
I0402 05:37:03.791631 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_11235.
I0402 05:37:03.792363 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 11235: RAM USED (GB) 100.966432768
I0402 05:38:54.818011 140484353718016 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.724795, loss=4.276764
I0402 05:38:54.822345 140529317082944 submission.py:296] 11500) loss = 4.277, grad_norm = 0.725
I0402 05:42:20.103141 140484345325312 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.711528, loss=4.116926
I0402 05:42:20.108268 140529317082944 submission.py:296] 12000) loss = 4.117, grad_norm = 0.712
I0402 05:44:03.925493 140529317082944 submission_runner.py:373] Before eval at step 12255: RAM USED (GB) 101.1405824
I0402 05:44:03.925716 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:44:47.781528 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:45:33.173035 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:45:34.590611 140529317082944 submission_runner.py:382] Time since start: 6155.47s, 	Step: 12255, 	{'train/accuracy': 0.54201171875, 'train/loss': 2.1016802978515625, 'validation/accuracy': 0.49788, 'validation/loss': 2.3054509375, 'validation/num_examples': 50000, 'test/accuracy': 0.3894, 'test/loss': 2.90998828125, 'test/num_examples': 10000}
I0402 05:45:34.590957 140529317082944 submission_runner.py:396] After eval at step 12255: RAM USED (GB) 101.123321856
I0402 05:45:34.599512 140484353718016 logging_writer.py:48] [12255] global_step=12255, preemption_count=0, score=5020.866484, test/accuracy=0.389400, test/loss=2.909988, test/num_examples=10000, total_duration=6155.471428, train/accuracy=0.542012, train/loss=2.101680, validation/accuracy=0.497880, validation/loss=2.305451, validation/num_examples=50000
I0402 05:45:35.029783 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_12255.
I0402 05:45:35.030562 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 12255: RAM USED (GB) 101.122666496
I0402 05:47:15.666592 140484345325312 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.799504, loss=4.110533
I0402 05:47:15.672591 140529317082944 submission.py:296] 12500) loss = 4.111, grad_norm = 0.800
I0402 05:50:42.780238 140484353718016 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.711622, loss=4.061302
I0402 05:50:42.786276 140529317082944 submission.py:296] 13000) loss = 4.061, grad_norm = 0.712
I0402 05:52:35.225748 140529317082944 submission_runner.py:373] Before eval at step 13275: RAM USED (GB) 100.95884288
I0402 05:52:35.225967 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:53:18.858415 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 05:54:03.188771 140529317082944 spec.py:326] Evaluating on the test split.
I0402 05:54:04.600189 140529317082944 submission_runner.py:382] Time since start: 6666.77s, 	Step: 13275, 	{'train/accuracy': 0.56408203125, 'train/loss': 1.981978302001953, 'validation/accuracy': 0.51718, 'validation/loss': 2.1985875, 'validation/num_examples': 50000, 'test/accuracy': 0.4119, 'test/loss': 2.7951654296875, 'test/num_examples': 10000}
I0402 05:54:04.600558 140529317082944 submission_runner.py:396] After eval at step 13275: RAM USED (GB) 101.052432384
I0402 05:54:04.608400 140484345325312 logging_writer.py:48] [13275] global_step=13275, preemption_count=0, score=5438.725362, test/accuracy=0.411900, test/loss=2.795165, test/num_examples=10000, total_duration=6666.772371, train/accuracy=0.564082, train/loss=1.981978, validation/accuracy=0.517180, validation/loss=2.198587, validation/num_examples=50000
I0402 05:54:05.032202 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_13275.
I0402 05:54:05.032915 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 13275: RAM USED (GB) 101.05307136
I0402 05:55:37.638690 140484353718016 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.696097, loss=3.987952
I0402 05:55:37.643094 140529317082944 submission.py:296] 13500) loss = 3.988, grad_norm = 0.696
I0402 05:59:04.042201 140529317082944 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.967415808
I0402 05:59:04.042481 140529317082944 spec.py:298] Evaluating on the training split.
I0402 05:59:47.598013 140529317082944 spec.py:310] Evaluating on the validation split.
I0402 06:00:32.496310 140529317082944 spec.py:326] Evaluating on the test split.
I0402 06:00:33.908665 140529317082944 submission_runner.py:382] Time since start: 7055.59s, 	Step: 14000, 	{'train/accuracy': 0.57568359375, 'train/loss': 1.920372314453125, 'validation/accuracy': 0.52564, 'validation/loss': 2.14087984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4141, 'test/loss': 2.77226953125, 'test/num_examples': 10000}
I0402 06:00:33.909026 140529317082944 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.925046784
I0402 06:00:33.917498 140484345325312 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5736.065009, test/accuracy=0.414100, test/loss=2.772270, test/num_examples=10000, total_duration=7055.588725, train/accuracy=0.575684, train/loss=1.920372, validation/accuracy=0.525640, validation/loss=2.140880, validation/num_examples=50000
I0402 06:00:34.356910 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0402 06:00:34.357700 140529317082944 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.924137472
I0402 06:00:34.366154 140484353718016 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5736.065009
I0402 06:00:35.399165 140529317082944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0402 06:00:35.775328 140529317082944 submission_runner.py:550] Tuning trial 1/1
I0402 06:00:35.775555 140529317082944 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 06:00:35.776218 140529317082944 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00216796875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00178, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0016, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.622585773468018, 'total_duration': 6.624485015869141, 'global_step': 1, 'preemption_count': 0}), (1024, {'train/accuracy': 0.04453125, 'train/loss': 5.8090283203125, 'validation/accuracy': 0.04154, 'validation/loss': 5.8348875, 'validation/num_examples': 50000, 'test/accuracy': 0.0326, 'test/loss': 5.946262109375, 'test/num_examples': 10000, 'score': 424.67387318611145, 'total_duration': 535.6739103794098, 'global_step': 1024, 'preemption_count': 0}), (2044, {'train/accuracy': 0.09740234375, 'train/loss': 5.091727294921875, 'validation/accuracy': 0.0883, 'validation/loss': 5.143273125, 'validation/num_examples': 50000, 'test/accuracy': 0.0715, 'test/loss': 5.34801328125, 'test/num_examples': 10000, 'score': 842.2756657600403, 'total_duration': 1046.3797163963318, 'global_step': 2044, 'preemption_count': 0}), (3065, {'train/accuracy': 0.16751953125, 'train/loss': 4.467237854003907, 'validation/accuracy': 0.15388, 'validation/loss': 4.5403503125, 'validation/num_examples': 50000, 'test/accuracy': 0.1162, 'test/loss': 4.85844140625, 'test/num_examples': 10000, 'score': 1259.9377999305725, 'total_duration': 1557.0096299648285, 'global_step': 3065, 'preemption_count': 0}), (4085, {'train/accuracy': 0.23513671875, 'train/loss': 3.9353451538085937, 'validation/accuracy': 0.21558, 'validation/loss': 4.042288125, 'validation/num_examples': 50000, 'test/accuracy': 0.1663, 'test/loss': 4.42484765625, 'test/num_examples': 10000, 'score': 1677.7315139770508, 'total_duration': 2066.9234807491302, 'global_step': 4085, 'preemption_count': 0}), (5105, {'train/accuracy': 0.289765625, 'train/loss': 3.508529052734375, 'validation/accuracy': 0.26488, 'validation/loss': 3.6356446875, 'validation/num_examples': 50000, 'test/accuracy': 0.2049, 'test/loss': 4.0719703125, 'test/num_examples': 10000, 'score': 2095.6754353046417, 'total_duration': 2578.4530856609344, 'global_step': 5105, 'preemption_count': 0}), (6128, {'train/accuracy': 0.3385546875, 'train/loss': 3.226836853027344, 'validation/accuracy': 0.3163, 'validation/loss': 3.3673534375, 'validation/num_examples': 50000, 'test/accuracy': 0.2405, 'test/loss': 3.854529296875, 'test/num_examples': 10000, 'score': 2513.3289952278137, 'total_duration': 3089.246100664139, 'global_step': 6128, 'preemption_count': 0}), (7148, {'train/accuracy': 0.3862109375, 'train/loss': 2.879422607421875, 'validation/accuracy': 0.35908, 'validation/loss': 3.04332, 'validation/num_examples': 50000, 'test/accuracy': 0.2752, 'test/loss': 3.577373828125, 'test/num_examples': 10000, 'score': 2931.1331973075867, 'total_duration': 3599.634579896927, 'global_step': 7148, 'preemption_count': 0}), (8168, {'train/accuracy': 0.4228515625, 'train/loss': 2.75065673828125, 'validation/accuracy': 0.38682, 'validation/loss': 2.922659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3003, 'test/loss': 3.464353125, 'test/num_examples': 10000, 'score': 3349.114241361618, 'total_duration': 4109.711231946945, 'global_step': 8168, 'preemption_count': 0}), (9188, {'train/accuracy': 0.46013671875, 'train/loss': 2.498959655761719, 'validation/accuracy': 0.42114, 'validation/loss': 2.6824925, 'validation/num_examples': 50000, 'test/accuracy': 0.3279, 'test/loss': 3.2574310546875, 'test/num_examples': 10000, 'score': 3767.095798254013, 'total_duration': 4620.245405673981, 'global_step': 9188, 'preemption_count': 0}), (10209, {'train/accuracy': 0.4911328125, 'train/loss': 2.336753234863281, 'validation/accuracy': 0.4541, 'validation/loss': 2.53005015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3521, 'test/loss': 3.121391015625, 'test/num_examples': 10000, 'score': 4185.083998203278, 'total_duration': 5130.187775611877, 'global_step': 10209, 'preemption_count': 0}), (11235, {'train/accuracy': 0.52154296875, 'train/loss': 2.170928497314453, 'validation/accuracy': 0.47858, 'validation/loss': 2.37556796875, 'validation/num_examples': 50000, 'test/accuracy': 0.3674, 'test/loss': 2.9961392578125, 'test/num_examples': 10000, 'score': 4603.111118555069, 'total_duration': 5642.005788326263, 'global_step': 11235, 'preemption_count': 0}), (12255, {'train/accuracy': 0.54201171875, 'train/loss': 2.1016802978515625, 'validation/accuracy': 0.49788, 'validation/loss': 2.3054509375, 'validation/num_examples': 50000, 'test/accuracy': 0.3894, 'test/loss': 2.90998828125, 'test/num_examples': 10000, 'score': 5020.8664836883545, 'total_duration': 6155.471427679062, 'global_step': 12255, 'preemption_count': 0}), (13275, {'train/accuracy': 0.56408203125, 'train/loss': 1.981978302001953, 'validation/accuracy': 0.51718, 'validation/loss': 2.1985875, 'validation/num_examples': 50000, 'test/accuracy': 0.4119, 'test/loss': 2.7951654296875, 'test/num_examples': 10000, 'score': 5438.725361824036, 'total_duration': 6666.772371053696, 'global_step': 13275, 'preemption_count': 0}), (14000, {'train/accuracy': 0.57568359375, 'train/loss': 1.920372314453125, 'validation/accuracy': 0.52564, 'validation/loss': 2.14087984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4141, 'test/loss': 2.77226953125, 'test/num_examples': 10000, 'score': 5736.065008878708, 'total_duration': 7055.588724851608, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0402 06:00:35.776333 140529317082944 submission_runner.py:553] Timing: 5736.065008878708
I0402 06:00:35.776434 140529317082944 submission_runner.py:554] ====================
I0402 06:00:35.776595 140529317082944 submission_runner.py:613] Final imagenet_vit score: 5736.065008878708
