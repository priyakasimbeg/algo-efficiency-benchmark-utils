I0329 22:47:59.027147 139954562164544 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/wmt_jax.
I0329 22:47:59.070473 139954562164544 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 22:48:00.026486 139954562164544 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0329 22:48:00.027095 139954562164544 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 22:48:00.031843 139954562164544 submission_runner.py:504] Using RNG seed 2552417831
I0329 22:48:01.324648 139954562164544 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 22:48:01.324852 139954562164544 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/wmt_jax/trial_1.
I0329 22:48:01.325044 139954562164544 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/wmt_jax/trial_1/hparams.json.
I0329 22:48:01.457693 139954562164544 submission_runner.py:230] Starting train once: RAM USED (GB) 4.478603264
I0329 22:48:01.457868 139954562164544 submission_runner.py:231] Initializing dataset.
I0329 22:48:01.467604 139954562164544 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:48:01.470868 139954562164544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:48:01.470974 139954562164544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:48:01.547821 139954562164544 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:48:03.752109 139954562164544 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.57961472
I0329 22:48:03.752325 139954562164544 submission_runner.py:240] Initializing model.
I0329 22:48:16.542842 139954562164544 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.653930496
I0329 22:48:16.543038 139954562164544 submission_runner.py:252] Initializing optimizer.
I0329 22:48:17.603976 139954562164544 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.65400832
I0329 22:48:17.604160 139954562164544 submission_runner.py:261] Initializing metrics bundle.
I0329 22:48:17.604212 139954562164544 submission_runner.py:275] Initializing checkpoint and logger.
I0329 22:48:17.605188 139954562164544 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/wmt_jax/trial_1 with prefix checkpoint_
I0329 22:48:17.605470 139954562164544 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0329 22:48:17.605569 139954562164544 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0329 22:48:18.603712 139954562164544 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/wmt_jax/trial_1/meta_data_0.json.
I0329 22:48:18.604661 139954562164544 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/wmt_jax/trial_1/flags_0.json.
I0329 22:48:18.607567 139954562164544 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.648515584
I0329 22:48:18.607788 139954562164544 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.648515584
I0329 22:48:18.607856 139954562164544 submission_runner.py:312] Starting training loop.
I0329 22:48:19.298692 139954562164544 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 8.786550784
I0329 22:48:53.050001 139778393630464 logging_writer.py:48] [0] global_step=0, grad_norm=5.970484256744385, loss=11.030695915222168
I0329 22:48:53.063606 139954562164544 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 11.443101696
I0329 22:48:53.063847 139954562164544 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 11.443101696
I0329 22:48:53.063925 139954562164544 spec.py:298] Evaluating on the training split.
I0329 22:48:53.066505 139954562164544 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:48:53.069135 139954562164544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:48:53.069271 139954562164544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:48:53.103454 139954562164544 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0329 22:49:01.777518 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 22:54:11.963669 139954562164544 spec.py:310] Evaluating on the validation split.
I0329 22:54:11.966853 139954562164544 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:54:11.969849 139954562164544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:54:11.969960 139954562164544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:54:12.001919 139954562164544 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:54:19.989005 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 22:59:23.996175 139954562164544 spec.py:326] Evaluating on the test split.
I0329 22:59:23.999744 139954562164544 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:59:24.002822 139954562164544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 22:59:24.002935 139954562164544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 22:59:24.037142 139954562164544 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0329 22:59:31.537916 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:04:27.614408 139954562164544 submission_runner.py:380] Time since start: 34.46s, 	Step: 1, 	{'train/accuracy': 0.0005706720403395593, 'train/loss': 11.026650428771973, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.061946868896484, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.06695556640625, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0329 23:04:27.615083 139954562164544 submission_runner.py:390] After eval at step 1: RAM USED (GB) 11.898769408
I0329 23:04:27.624654 139767194302208 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=34.233872, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.066956, test/num_examples=3003, total_duration=34.456034, train/accuracy=0.000571, train/bleu=0.000000, train/loss=11.026650, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.061947, validation/num_examples=3000
I0329 23:04:28.909507 139954562164544 checkpoints.py:356] Saving checkpoint at step: 1
I0329 23:04:33.441965 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_1
I0329 23:04:33.446465 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_1.
I0329 23:04:33.452755 139954562164544 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 13.02949888
I0329 23:04:33.455895 139954562164544 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 13.02949888
I0329 23:04:33.582378 139954562164544 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 11.982852096
I0329 23:05:10.002061 139767202694912 logging_writer.py:48] [100] global_step=100, grad_norm=0.22843821346759796, loss=8.575865745544434
I0329 23:05:46.542496 139767336978176 logging_writer.py:48] [200] global_step=200, grad_norm=0.7697895765304565, loss=7.984509468078613
I0329 23:06:23.128977 139767202694912 logging_writer.py:48] [300] global_step=300, grad_norm=0.8577802777290344, loss=7.592460632324219
I0329 23:06:59.717663 139767336978176 logging_writer.py:48] [400] global_step=400, grad_norm=0.3970482051372528, loss=7.225470066070557
I0329 23:07:36.300860 139767202694912 logging_writer.py:48] [500] global_step=500, grad_norm=0.9717095494270325, loss=6.915647506713867
I0329 23:08:12.890002 139767336978176 logging_writer.py:48] [600] global_step=600, grad_norm=0.7977677583694458, loss=6.660990238189697
I0329 23:08:49.475623 139767202694912 logging_writer.py:48] [700] global_step=700, grad_norm=0.8039340972900391, loss=6.449056625366211
I0329 23:09:26.094351 139767336978176 logging_writer.py:48] [800] global_step=800, grad_norm=1.025307536125183, loss=6.250672817230225
I0329 23:10:02.637346 139767202694912 logging_writer.py:48] [900] global_step=900, grad_norm=0.5760135054588318, loss=6.234160900115967
I0329 23:10:39.241059 139767336978176 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4194079637527466, loss=5.960325241088867
I0329 23:11:15.814578 139767202694912 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4603603184223175, loss=5.747600555419922
I0329 23:11:52.425377 139767336978176 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5592382550239563, loss=5.611481189727783
I0329 23:12:29.020986 139767202694912 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.503471314907074, loss=5.5363450050354
I0329 23:13:05.626843 139767336978176 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7509190440177917, loss=5.447434425354004
I0329 23:13:42.168648 139767202694912 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.32488542795181274, loss=7.036375045776367
I0329 23:14:18.666131 139767336978176 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.3694206774234772, loss=6.401381492614746
I0329 23:14:55.119818 139767202694912 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5200986862182617, loss=6.1098408699035645
I0329 23:15:31.619655 139767336978176 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.776203989982605, loss=6.7991533279418945
I0329 23:16:08.155979 139767202694912 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.45385420322418213, loss=5.925224781036377
I0329 23:16:44.648225 139767336978176 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3346172273159027, loss=5.902170658111572
I0329 23:17:21.159367 139767202694912 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.43829360604286194, loss=5.68997859954834
I0329 23:17:57.673452 139767336978176 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6694256663322449, loss=5.7125959396362305
I0329 23:18:33.568701 139954562164544 submission_runner.py:371] Before eval at step 2300: RAM USED (GB) 12.274581504
I0329 23:18:33.568940 139954562164544 spec.py:298] Evaluating on the training split.
I0329 23:18:36.617728 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:23:23.449933 139954562164544 spec.py:310] Evaluating on the validation split.
I0329 23:23:26.157418 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:27:50.965918 139954562164544 spec.py:326] Evaluating on the test split.
I0329 23:27:53.718083 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:32:21.694921 139954562164544 submission_runner.py:380] Time since start: 1814.96s, 	Step: 2300, 	{'train/accuracy': 0.3556661009788513, 'train/loss': 4.303223609924316, 'train/bleu': 7.828649764444679, 'validation/accuracy': 0.33369705080986023, 'validation/loss': 4.518587112426758, 'validation/bleu': 4.732924734426034, 'validation/num_examples': 3000, 'test/accuracy': 0.31485679745674133, 'test/loss': 4.767424583435059, 'test/bleu': 3.4027620633669504, 'test/num_examples': 3003}
I0329 23:32:21.695426 139954562164544 submission_runner.py:390] After eval at step 2300: RAM USED (GB) 12.490604544
I0329 23:32:21.703971 139767202694912 logging_writer.py:48] [2300] global_step=2300, preemption_count=0, score=869.075858, test/accuracy=0.314857, test/bleu=3.402762, test/loss=4.767425, test/num_examples=3003, total_duration=1814.959629, train/accuracy=0.355666, train/bleu=7.828650, train/loss=4.303224, validation/accuracy=0.333697, validation/bleu=4.732925, validation/loss=4.518587, validation/num_examples=3000
I0329 23:32:22.763821 139954562164544 checkpoints.py:356] Saving checkpoint at step: 2300
I0329 23:32:26.515568 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_2300
I0329 23:32:26.519501 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_2300.
I0329 23:32:26.524290 139954562164544 submission_runner.py:409] After logging and checkpointing eval at step 2300: RAM USED (GB) 13.64672512
I0329 23:32:26.907539 139767336978176 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6776881217956543, loss=5.130284786224365
I0329 23:33:03.495025 139767311800064 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.41082561016082764, loss=4.8447136878967285
I0329 23:33:40.071698 139767336978176 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.41608405113220215, loss=4.716355323791504
I0329 23:34:16.585321 139767311800064 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4567960798740387, loss=4.540832996368408
I0329 23:34:53.118223 139767336978176 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4755273759365082, loss=4.450342178344727
I0329 23:35:29.674132 139767311800064 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.43563520908355713, loss=4.420722484588623
I0329 23:36:06.235702 139767336978176 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.46188151836395264, loss=4.374910354614258
I0329 23:36:42.750945 139767311800064 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.4483295679092407, loss=4.152380466461182
I0329 23:37:19.250552 139767336978176 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.45134347677230835, loss=4.180248260498047
I0329 23:37:55.806499 139767311800064 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3911025822162628, loss=4.118400573730469
I0329 23:38:32.352092 139767336978176 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3790195882320404, loss=4.030243396759033
I0329 23:39:08.879727 139767311800064 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.386728435754776, loss=4.057496547698975
I0329 23:39:45.470631 139767336978176 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.4478994905948639, loss=3.928361415863037
I0329 23:40:22.028702 139767311800064 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.3890593945980072, loss=3.9513847827911377
I0329 23:40:58.558876 139767336978176 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4007937014102936, loss=3.9223947525024414
I0329 23:41:35.053362 139767311800064 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.3526986241340637, loss=3.8788726329803467
I0329 23:42:11.576573 139767336978176 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.3204907476902008, loss=3.7799479961395264
I0329 23:42:48.088832 139767311800064 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3571145236492157, loss=3.8598875999450684
I0329 23:43:24.603248 139767336978176 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.33419495820999146, loss=3.8655588626861572
I0329 23:44:01.183468 139767311800064 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3096238970756531, loss=3.8075602054595947
I0329 23:44:37.703770 139767336978176 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.4233671724796295, loss=3.7699522972106934
I0329 23:45:14.219011 139767311800064 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.28258001804351807, loss=3.7186474800109863
I0329 23:45:50.737940 139767336978176 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.288076788187027, loss=3.665637731552124
I0329 23:46:26.584480 139954562164544 submission_runner.py:371] Before eval at step 4600: RAM USED (GB) 12.596711424
I0329 23:46:26.584681 139954562164544 spec.py:298] Evaluating on the training split.
I0329 23:46:29.625823 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:49:25.706725 139954562164544 spec.py:310] Evaluating on the validation split.
I0329 23:49:28.402607 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:52:11.351187 139954562164544 spec.py:326] Evaluating on the test split.
I0329 23:52:14.118526 139954562164544 workload.py:179] Translating evaluation dataset.
I0329 23:54:55.421078 139954562164544 submission_runner.py:380] Time since start: 3487.98s, 	Step: 4600, 	{'train/accuracy': 0.5466380715370178, 'train/loss': 2.6193461418151855, 'train/bleu': 25.127151814231137, 'validation/accuracy': 0.5559013485908508, 'validation/loss': 2.553778886795044, 'validation/bleu': 21.186889420001098, 'validation/num_examples': 3000, 'test/accuracy': 0.5541340112686157, 'test/loss': 2.571117877960205, 'test/bleu': 19.64724870080513, 'test/num_examples': 3003}
I0329 23:54:55.421667 139954562164544 submission_runner.py:390] After eval at step 4600: RAM USED (GB) 12.79827968
I0329 23:54:55.431308 139767311800064 logging_writer.py:48] [4600] global_step=4600, preemption_count=0, score=1705.315555, test/accuracy=0.554134, test/bleu=19.647249, test/loss=2.571118, test/num_examples=3003, total_duration=3487.975916, train/accuracy=0.546638, train/bleu=25.127152, train/loss=2.619346, validation/accuracy=0.555901, validation/bleu=21.186889, validation/loss=2.553779, validation/num_examples=3000
I0329 23:54:56.713817 139954562164544 checkpoints.py:356] Saving checkpoint at step: 4600
I0329 23:55:01.078222 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_4600
I0329 23:55:01.082709 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_4600.
I0329 23:55:01.088143 139954562164544 submission_runner.py:409] After logging and checkpointing eval at step 4600: RAM USED (GB) 13.962645504
I0329 23:55:01.468642 139767336978176 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.2615000307559967, loss=3.590505361557007
I0329 23:55:37.931909 139766893393664 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.2587600350379944, loss=3.64349365234375
I0329 23:56:14.402477 139767336978176 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.2800219655036926, loss=3.642319440841675
I0329 23:56:50.926346 139766893393664 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.26623767614364624, loss=3.5732619762420654
I0329 23:57:27.423017 139767336978176 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.27248960733413696, loss=3.553136110305786
I0329 23:58:03.920092 139766893393664 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.2107093185186386, loss=3.552612781524658
I0329 23:58:40.408182 139767336978176 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.22572456300258636, loss=3.5024802684783936
I0329 23:59:16.940369 139766893393664 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.21794337034225464, loss=3.610792398452759
I0329 23:59:53.454648 139767336978176 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.21817022562026978, loss=3.566500663757324
I0330 00:00:29.914122 139766893393664 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.26572662591934204, loss=3.5424914360046387
I0330 00:01:06.394133 139767336978176 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.19994011521339417, loss=3.4743716716766357
I0330 00:01:42.904819 139766893393664 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.3494710922241211, loss=3.4382247924804688
I0330 00:02:19.399420 139767336978176 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2440938502550125, loss=3.5464913845062256
I0330 00:02:55.883923 139766893393664 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.19997401535511017, loss=3.4900062084198
I0330 00:03:32.367038 139767336978176 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.21188299357891083, loss=3.4198050498962402
I0330 00:04:08.846379 139766893393664 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.24935691058635712, loss=3.4153213500976562
I0330 00:04:45.291864 139767336978176 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.18065772950649261, loss=3.426285982131958
I0330 00:05:21.739327 139766893393664 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.21565522253513336, loss=3.452298879623413
I0330 00:05:58.210671 139767336978176 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.20101679861545563, loss=3.4339983463287354
I0330 00:06:34.641721 139766893393664 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.17035385966300964, loss=3.3971893787384033
I0330 00:07:11.107498 139767336978176 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.17444977164268494, loss=3.4044225215911865
I0330 00:07:47.669373 139766893393664 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.17701824009418488, loss=3.381131172180176
I0330 00:08:24.160180 139767336978176 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.172498419880867, loss=3.435027837753296
I0330 00:09:00.689418 139766893393664 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.25668081641197205, loss=3.431191921234131
I0330 00:09:01.139396 139954562164544 submission_runner.py:371] Before eval at step 6903: RAM USED (GB) 13.041770496
I0330 00:09:01.139577 139954562164544 spec.py:298] Evaluating on the training split.
I0330 00:09:04.172317 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:11:42.511083 139954562164544 spec.py:310] Evaluating on the validation split.
I0330 00:11:45.216903 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:14:18.332387 139954562164544 spec.py:326] Evaluating on the test split.
I0330 00:14:21.083922 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:16:50.385229 139954562164544 submission_runner.py:380] Time since start: 4842.53s, 	Step: 6903, 	{'train/accuracy': 0.5915433168411255, 'train/loss': 2.2383978366851807, 'train/bleu': 27.84416829831101, 'validation/accuracy': 0.5976614952087402, 'validation/loss': 2.1956751346588135, 'validation/bleu': 23.86892874328269, 'validation/num_examples': 3000, 'test/accuracy': 0.6039858460426331, 'test/loss': 2.1670989990234375, 'test/bleu': 23.157942290519433, 'test/num_examples': 3003}
I0330 00:16:50.385689 139954562164544 submission_runner.py:390] After eval at step 6903: RAM USED (GB) 13.148393472
I0330 00:16:50.394467 139767336978176 logging_writer.py:48] [6903] global_step=6903, preemption_count=0, score=2541.563166, test/accuracy=0.603986, test/bleu=23.157942, test/loss=2.167099, test/num_examples=3003, total_duration=4842.530806, train/accuracy=0.591543, train/bleu=27.844168, train/loss=2.238398, validation/accuracy=0.597661, validation/bleu=23.868929, validation/loss=2.195675, validation/num_examples=3000
I0330 00:16:51.441376 139954562164544 checkpoints.py:356] Saving checkpoint at step: 6903
I0330 00:16:55.122184 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_6903
I0330 00:16:55.126193 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_6903.
I0330 00:16:55.131295 139954562164544 submission_runner.py:409] After logging and checkpointing eval at step 6903: RAM USED (GB) 14.303993856
I0330 00:17:30.883847 139766893393664 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.1826561689376831, loss=3.4280002117156982
I0330 00:18:07.375530 139766885000960 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.17951133847236633, loss=3.4522578716278076
I0330 00:18:43.881619 139766893393664 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.14928074181079865, loss=3.3164279460906982
I0330 00:19:20.393561 139766885000960 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.18463148176670074, loss=3.248570442199707
I0330 00:19:56.877480 139766893393664 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.18107055127620697, loss=3.2808501720428467
I0330 00:20:33.345470 139766885000960 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.16981525719165802, loss=3.2536468505859375
I0330 00:21:09.826964 139766893393664 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.2150501012802124, loss=3.283505916595459
I0330 00:21:46.321016 139766885000960 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.15836934745311737, loss=3.306882858276367
I0330 00:22:22.801621 139766893393664 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.26216769218444824, loss=3.311828374862671
I0330 00:22:59.276847 139766885000960 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.14356529712677002, loss=3.2813262939453125
I0330 00:23:35.738831 139766893393664 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.16213874518871307, loss=3.271120309829712
I0330 00:24:12.223730 139766885000960 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.17080377042293549, loss=3.319770574569702
I0330 00:24:48.701383 139766893393664 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17210720479488373, loss=3.2666378021240234
I0330 00:25:25.174470 139766885000960 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.15558837354183197, loss=3.2917191982269287
I0330 00:26:01.689086 139766893393664 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.16274064779281616, loss=3.223910331726074
I0330 00:26:38.161991 139766885000960 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.21484221518039703, loss=3.235142707824707
I0330 00:27:14.634603 139766893393664 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.15251554548740387, loss=3.253978729248047
I0330 00:27:51.082440 139766885000960 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.19123993813991547, loss=3.280768394470215
I0330 00:28:27.554804 139766893393664 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1507328599691391, loss=3.318337917327881
I0330 00:29:04.059579 139766885000960 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.16299666464328766, loss=3.3047425746917725
I0330 00:29:40.560271 139766893393664 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15176615118980408, loss=3.336265802383423
I0330 00:30:17.040564 139766885000960 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.15240967273712158, loss=3.349480390548706
I0330 00:30:53.525909 139766893393664 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.186748668551445, loss=3.2440524101257324
I0330 00:30:55.433623 139954562164544 submission_runner.py:371] Before eval at step 9207: RAM USED (GB) 13.417381888
I0330 00:30:55.433849 139954562164544 spec.py:298] Evaluating on the training split.
I0330 00:30:58.474380 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:33:42.850098 139954562164544 spec.py:310] Evaluating on the validation split.
I0330 00:33:45.555104 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:36:12.956483 139954562164544 spec.py:326] Evaluating on the test split.
I0330 00:36:15.723737 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:38:38.185109 139954562164544 submission_runner.py:380] Time since start: 6156.82s, 	Step: 9207, 	{'train/accuracy': 0.6097925305366516, 'train/loss': 2.1041805744171143, 'train/bleu': 28.526843928845224, 'validation/accuracy': 0.620054304599762, 'validation/loss': 2.009960889816284, 'validation/bleu': 25.554567201788664, 'validation/num_examples': 3000, 'test/accuracy': 0.6275405287742615, 'test/loss': 1.9752119779586792, 'test/bleu': 24.20949670302629, 'test/num_examples': 3003}
I0330 00:38:38.185587 139954562164544 submission_runner.py:390] After eval at step 9207: RAM USED (GB) 13.467185152
I0330 00:38:38.194619 139766885000960 logging_writer.py:48] [9207] global_step=9207, preemption_count=0, score=3378.148472, test/accuracy=0.627541, test/bleu=24.209497, test/loss=1.975212, test/num_examples=3003, total_duration=6156.824954, train/accuracy=0.609793, train/bleu=28.526844, train/loss=2.104181, validation/accuracy=0.620054, validation/bleu=25.554567, validation/loss=2.009961, validation/num_examples=3000
I0330 00:38:39.202407 139954562164544 checkpoints.py:356] Saving checkpoint at step: 9207
I0330 00:38:42.832068 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_9207
I0330 00:38:42.835960 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_9207.
I0330 00:38:42.840647 139954562164544 submission_runner.py:409] After logging and checkpointing eval at step 9207: RAM USED (GB) 14.623404032
I0330 00:39:17.121911 139766893393664 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.17459243535995483, loss=3.258631467819214
I0330 00:39:53.600723 139766876608256 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.14480385184288025, loss=3.227907419204712
I0330 00:40:30.096268 139766893393664 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.1471583992242813, loss=3.159271240234375
I0330 00:41:06.578477 139766876608256 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.19596832990646362, loss=3.2854480743408203
I0330 00:41:43.063774 139766893393664 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.16241703927516937, loss=3.1433768272399902
I0330 00:42:19.546576 139766876608256 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.18608573079109192, loss=3.2281863689422607
I0330 00:42:55.999248 139766893393664 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.14088216423988342, loss=3.1549954414367676
I0330 00:43:31.853366 139954562164544 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 13.478551552
I0330 00:43:31.853584 139954562164544 spec.py:298] Evaluating on the training split.
I0330 00:43:34.881400 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:46:12.697372 139954562164544 spec.py:310] Evaluating on the validation split.
I0330 00:46:15.400337 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:48:47.593794 139954562164544 spec.py:326] Evaluating on the test split.
I0330 00:48:50.352311 139954562164544 workload.py:179] Translating evaluation dataset.
I0330 00:51:15.915887 139954562164544 submission_runner.py:380] Time since start: 6913.24s, 	Step: 10000, 	{'train/accuracy': 0.6113137602806091, 'train/loss': 2.0626540184020996, 'train/bleu': 28.964171111130568, 'validation/accuracy': 0.6257454752922058, 'validation/loss': 1.9571536779403687, 'validation/bleu': 25.894332768463237, 'validation/num_examples': 3000, 'test/accuracy': 0.6321306228637695, 'test/loss': 1.9107729196548462, 'test/bleu': 24.891481648271146, 'test/num_examples': 3003}
I0330 00:51:15.916377 139954562164544 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 13.561499648
I0330 00:51:15.925976 139766876608256 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3665.876084, test/accuracy=0.632131, test/bleu=24.891482, test/loss=1.910773, test/num_examples=3003, total_duration=6913.244749, train/accuracy=0.611314, train/bleu=28.964171, train/loss=2.062654, validation/accuracy=0.625745, validation/bleu=25.894333, validation/loss=1.957154, validation/num_examples=3000
I0330 00:51:16.934355 139954562164544 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 00:51:20.525486 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_10000
I0330 00:51:20.529411 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_10000.
I0330 00:51:20.534184 139954562164544 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.717620224
I0330 00:51:20.541201 139766893393664 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3665.876084
I0330 00:51:21.106338 139954562164544 checkpoints.py:356] Saving checkpoint at step: 10000
I0330 00:51:26.609179 139954562164544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_10000
I0330 00:51:26.613212 139954562164544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/wmt_jax/trial_1/checkpoint_10000.
I0330 00:51:26.696170 139954562164544 submission_runner.py:543] Tuning trial 1/1
I0330 00:51:26.696365 139954562164544 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0330 00:51:26.697572 139954562164544 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005706720403395593, 'train/loss': 11.026650428771973, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.061946868896484, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.06695556640625, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 34.233872175216675, 'total_duration': 34.45603370666504, 'global_step': 1, 'preemption_count': 0}), (2300, {'train/accuracy': 0.3556661009788513, 'train/loss': 4.303223609924316, 'train/bleu': 7.828649764444679, 'validation/accuracy': 0.33369705080986023, 'validation/loss': 4.518587112426758, 'validation/bleu': 4.732924734426034, 'validation/num_examples': 3000, 'test/accuracy': 0.31485679745674133, 'test/loss': 4.767424583435059, 'test/bleu': 3.4027620633669504, 'test/num_examples': 3003, 'score': 869.0758578777313, 'total_duration': 1814.9596292972565, 'global_step': 2300, 'preemption_count': 0}), (4600, {'train/accuracy': 0.5466380715370178, 'train/loss': 2.6193461418151855, 'train/bleu': 25.127151814231137, 'validation/accuracy': 0.5559013485908508, 'validation/loss': 2.553778886795044, 'validation/bleu': 21.186889420001098, 'validation/num_examples': 3000, 'test/accuracy': 0.5541340112686157, 'test/loss': 2.571117877960205, 'test/bleu': 19.64724870080513, 'test/num_examples': 3003, 'score': 1705.315554857254, 'total_duration': 3487.975916147232, 'global_step': 4600, 'preemption_count': 0}), (6903, {'train/accuracy': 0.5915433168411255, 'train/loss': 2.2383978366851807, 'train/bleu': 27.84416829831101, 'validation/accuracy': 0.5976614952087402, 'validation/loss': 2.1956751346588135, 'validation/bleu': 23.86892874328269, 'validation/num_examples': 3000, 'test/accuracy': 0.6039858460426331, 'test/loss': 2.1670989990234375, 'test/bleu': 23.157942290519433, 'test/num_examples': 3003, 'score': 2541.5631663799286, 'total_duration': 4842.530805587769, 'global_step': 6903, 'preemption_count': 0}), (9207, {'train/accuracy': 0.6097925305366516, 'train/loss': 2.1041805744171143, 'train/bleu': 28.526843928845224, 'validation/accuracy': 0.620054304599762, 'validation/loss': 2.009960889816284, 'validation/bleu': 25.554567201788664, 'validation/num_examples': 3000, 'test/accuracy': 0.6275405287742615, 'test/loss': 1.9752119779586792, 'test/bleu': 24.20949670302629, 'test/num_examples': 3003, 'score': 3378.1484718322754, 'total_duration': 6156.824953794479, 'global_step': 9207, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6113137602806091, 'train/loss': 2.0626540184020996, 'train/bleu': 28.964171111130568, 'validation/accuracy': 0.6257454752922058, 'validation/loss': 1.9571536779403687, 'validation/bleu': 25.894332768463237, 'validation/num_examples': 3000, 'test/accuracy': 0.6321306228637695, 'test/loss': 1.9107729196548462, 'test/bleu': 24.891481648271146, 'test/num_examples': 3003, 'score': 3665.876083612442, 'total_duration': 6913.244749069214, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0330 00:51:26.697698 139954562164544 submission_runner.py:546] Timing: 3665.876083612442
I0330 00:51:26.697747 139954562164544 submission_runner.py:547] ====================
I0330 00:51:26.697839 139954562164544 submission_runner.py:606] Final wmt score: 3665.876083612442
