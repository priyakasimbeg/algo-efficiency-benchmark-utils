WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 06:46:22.349198 140089380288320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 06:46:22.372035 140541276436288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 06:46:22.372780 140334516062016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 06:46:23.336651 140123991738176 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 06:46:23.336774 140025585682240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 06:46:23.337538 139780418320192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 06:46:23.339064 139628739430208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 06:46:23.346621 140115014068032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 06:46:23.346986 140115014068032 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.347344 140025585682240 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.347374 140123991738176 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.348118 139780418320192 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.349777 139628739430208 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.353125 140089380288320 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.355604 140541276436288 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:23.355643 140334516062016 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 06:46:27.197880 140115014068032 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/wmt_pytorch.
W0402 06:46:27.265535 139628739430208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.267327 140115014068032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.267890 140123991738176 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.268115 140025585682240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.268367 140334516062016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.268325 139780418320192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.268546 140089380288320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 06:46:27.269715 140541276436288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 06:46:27.271206 140115014068032 submission_runner.py:511] Using RNG seed 1098915336
I0402 06:46:27.272248 140115014068032 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 06:46:27.272359 140115014068032 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/wmt_pytorch/trial_1.
I0402 06:46:27.272590 140115014068032 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/hparams.json.
I0402 06:46:27.273777 140115014068032 submission_runner.py:230] Starting train once: RAM USED (GB) 15.236681728
I0402 06:46:27.273868 140115014068032 submission_runner.py:231] Initializing dataset.
I0402 06:46:27.274029 140115014068032 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.236681728
I0402 06:46:27.274086 140115014068032 submission_runner.py:240] Initializing model.
I0402 06:46:30.751418 140115014068032 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.582447616
I0402 06:46:30.751612 140115014068032 submission_runner.py:252] Initializing optimizer.
I0402 06:46:30.866038 140115014068032 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.585462272
I0402 06:46:30.866220 140115014068032 submission_runner.py:261] Initializing metrics bundle.
I0402 06:46:30.866275 140115014068032 submission_runner.py:276] Initializing checkpoint and logger.
I0402 06:46:30.868068 140115014068032 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 06:46:30.868192 140115014068032 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 06:46:31.501308 140115014068032 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/meta_data_0.json.
I0402 06:46:31.502289 140115014068032 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/flags_0.json.
I0402 06:46:31.537458 140115014068032 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.638788096
I0402 06:46:31.538754 140115014068032 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.638788096
I0402 06:46:31.538891 140115014068032 submission_runner.py:313] Starting training loop.
I0402 06:46:31.549001 140115014068032 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 06:46:31.552452 140115014068032 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 06:46:31.552572 140115014068032 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 06:46:31.607228 140115014068032 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 06:46:33.751974 140115014068032 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 19.938480128
I0402 06:46:35.262116 140074897360640 logging_writer.py:48] [0] global_step=0, grad_norm=4.940122, loss=11.188684
I0402 06:46:35.266563 140115014068032 submission.py:139] 0) loss = 11.189, grad_norm = 4.940
I0402 06:46:35.267576 140115014068032 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 20.03513344
I0402 06:46:35.268099 140115014068032 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 20.03513344
I0402 06:46:35.268212 140115014068032 spec.py:298] Evaluating on the training split.
I0402 06:46:35.270033 140115014068032 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 06:46:35.272727 140115014068032 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 06:46:35.272833 140115014068032 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 06:46:35.300789 140115014068032 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 06:46:39.465324 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 06:51:11.513896 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 06:51:11.517192 140115014068032 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 06:51:11.520569 140115014068032 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 06:51:11.520683 140115014068032 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 06:51:11.548816 140115014068032 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 06:51:15.375763 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 06:55:42.189705 140115014068032 spec.py:326] Evaluating on the test split.
I0402 06:55:42.192139 140115014068032 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 06:55:42.194716 140115014068032 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 06:55:42.194837 140115014068032 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 06:55:42.222260 140115014068032 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 06:55:46.113506 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:00:18.327147 140115014068032 submission_runner.py:382] Time since start: 3.73s, 	Step: 1, 	{'train/accuracy': 0.000628140703517588, 'train/loss': 11.260627712425766, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.264276791360306, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.25242940561269, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0402 07:00:18.327601 140115014068032 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.438499328
I0402 07:00:18.335515 140057296840448 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.727843, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.252429, test/num_examples=3003, total_duration=3.730097, train/accuracy=0.000628, train/bleu=0.000000, train/loss=11.260628, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.264277, validation/num_examples=3000
I0402 07:00:19.858089 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_1.
I0402 07:00:19.858694 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.438245376
I0402 07:00:19.862656 140115014068032 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.438237184
I0402 07:00:19.866178 140115014068032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866309 140334516062016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866321 140089380288320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866341 140123991738176 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866562 140541276436288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866560 140025585682240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866622 139628739430208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:19.866614 139780418320192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:00:20.288599 140057288447744 logging_writer.py:48] [1] global_step=1, grad_norm=5.011067, loss=11.189450
I0402 07:00:20.292434 140115014068032 submission.py:139] 1) loss = 11.189, grad_norm = 5.011
I0402 07:00:20.293266 140115014068032 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.438745088
I0402 07:00:20.722909 140057296840448 logging_writer.py:48] [2] global_step=2, grad_norm=4.946415, loss=11.192192
I0402 07:00:20.726615 140115014068032 submission.py:139] 2) loss = 11.192, grad_norm = 4.946
I0402 07:00:21.156832 140057288447744 logging_writer.py:48] [3] global_step=3, grad_norm=4.908471, loss=11.171624
I0402 07:00:21.160254 140115014068032 submission.py:139] 3) loss = 11.172, grad_norm = 4.908
I0402 07:00:21.589344 140057296840448 logging_writer.py:48] [4] global_step=4, grad_norm=4.880799, loss=11.141500
I0402 07:00:21.593108 140115014068032 submission.py:139] 4) loss = 11.142, grad_norm = 4.881
I0402 07:00:22.024437 140057288447744 logging_writer.py:48] [5] global_step=5, grad_norm=4.794644, loss=11.121627
I0402 07:00:22.028017 140115014068032 submission.py:139] 5) loss = 11.122, grad_norm = 4.795
I0402 07:00:22.458080 140057296840448 logging_writer.py:48] [6] global_step=6, grad_norm=4.764018, loss=11.086349
I0402 07:00:22.461688 140115014068032 submission.py:139] 6) loss = 11.086, grad_norm = 4.764
I0402 07:00:22.894512 140057288447744 logging_writer.py:48] [7] global_step=7, grad_norm=4.507087, loss=11.028490
I0402 07:00:22.898037 140115014068032 submission.py:139] 7) loss = 11.028, grad_norm = 4.507
I0402 07:00:23.330004 140057296840448 logging_writer.py:48] [8] global_step=8, grad_norm=4.419732, loss=10.955599
I0402 07:00:23.333680 140115014068032 submission.py:139] 8) loss = 10.956, grad_norm = 4.420
I0402 07:00:23.765628 140057288447744 logging_writer.py:48] [9] global_step=9, grad_norm=4.164151, loss=10.879025
I0402 07:00:23.769370 140115014068032 submission.py:139] 9) loss = 10.879, grad_norm = 4.164
I0402 07:00:24.203544 140057296840448 logging_writer.py:48] [10] global_step=10, grad_norm=3.881604, loss=10.792656
I0402 07:00:24.206604 140115014068032 submission.py:139] 10) loss = 10.793, grad_norm = 3.882
I0402 07:00:24.636634 140057288447744 logging_writer.py:48] [11] global_step=11, grad_norm=3.571393, loss=10.707948
I0402 07:00:24.639680 140115014068032 submission.py:139] 11) loss = 10.708, grad_norm = 3.571
I0402 07:00:25.074863 140057296840448 logging_writer.py:48] [12] global_step=12, grad_norm=3.266760, loss=10.616905
I0402 07:00:25.078749 140115014068032 submission.py:139] 12) loss = 10.617, grad_norm = 3.267
I0402 07:00:25.510780 140057288447744 logging_writer.py:48] [13] global_step=13, grad_norm=3.046233, loss=10.533624
I0402 07:00:25.514583 140115014068032 submission.py:139] 13) loss = 10.534, grad_norm = 3.046
I0402 07:00:25.947840 140057296840448 logging_writer.py:48] [14] global_step=14, grad_norm=2.758542, loss=10.449708
I0402 07:00:25.951083 140115014068032 submission.py:139] 14) loss = 10.450, grad_norm = 2.759
I0402 07:00:26.384143 140057288447744 logging_writer.py:48] [15] global_step=15, grad_norm=2.525533, loss=10.373267
I0402 07:00:26.387916 140115014068032 submission.py:139] 15) loss = 10.373, grad_norm = 2.526
I0402 07:00:26.817061 140057296840448 logging_writer.py:48] [16] global_step=16, grad_norm=2.325908, loss=10.298089
I0402 07:00:26.820237 140115014068032 submission.py:139] 16) loss = 10.298, grad_norm = 2.326
I0402 07:00:27.252554 140057288447744 logging_writer.py:48] [17] global_step=17, grad_norm=2.154415, loss=10.224160
I0402 07:00:27.255587 140115014068032 submission.py:139] 17) loss = 10.224, grad_norm = 2.154
I0402 07:00:27.687889 140057296840448 logging_writer.py:48] [18] global_step=18, grad_norm=2.014018, loss=10.159229
I0402 07:00:27.690876 140115014068032 submission.py:139] 18) loss = 10.159, grad_norm = 2.014
I0402 07:00:28.124466 140057288447744 logging_writer.py:48] [19] global_step=19, grad_norm=1.903068, loss=10.089406
I0402 07:00:28.127563 140115014068032 submission.py:139] 19) loss = 10.089, grad_norm = 1.903
I0402 07:00:28.562680 140057296840448 logging_writer.py:48] [20] global_step=20, grad_norm=1.764910, loss=10.050296
I0402 07:00:28.565831 140115014068032 submission.py:139] 20) loss = 10.050, grad_norm = 1.765
I0402 07:00:28.997572 140057288447744 logging_writer.py:48] [21] global_step=21, grad_norm=1.672039, loss=10.000529
I0402 07:00:29.000751 140115014068032 submission.py:139] 21) loss = 10.001, grad_norm = 1.672
I0402 07:00:29.436831 140057296840448 logging_writer.py:48] [22] global_step=22, grad_norm=1.598234, loss=9.948501
I0402 07:00:29.440020 140115014068032 submission.py:139] 22) loss = 9.949, grad_norm = 1.598
I0402 07:00:29.873839 140057288447744 logging_writer.py:48] [23] global_step=23, grad_norm=1.516408, loss=9.881967
I0402 07:00:29.876951 140115014068032 submission.py:139] 23) loss = 9.882, grad_norm = 1.516
I0402 07:00:30.311069 140057296840448 logging_writer.py:48] [24] global_step=24, grad_norm=1.433838, loss=9.842173
I0402 07:00:30.314110 140115014068032 submission.py:139] 24) loss = 9.842, grad_norm = 1.434
I0402 07:00:30.747497 140057288447744 logging_writer.py:48] [25] global_step=25, grad_norm=1.338740, loss=9.808988
I0402 07:00:30.750589 140115014068032 submission.py:139] 25) loss = 9.809, grad_norm = 1.339
I0402 07:00:31.181663 140057296840448 logging_writer.py:48] [26] global_step=26, grad_norm=1.255117, loss=9.773181
I0402 07:00:31.184853 140115014068032 submission.py:139] 26) loss = 9.773, grad_norm = 1.255
I0402 07:00:31.619819 140057288447744 logging_writer.py:48] [27] global_step=27, grad_norm=1.181574, loss=9.752974
I0402 07:00:31.623215 140115014068032 submission.py:139] 27) loss = 9.753, grad_norm = 1.182
I0402 07:00:32.057154 140057296840448 logging_writer.py:48] [28] global_step=28, grad_norm=1.096155, loss=9.703568
I0402 07:00:32.060415 140115014068032 submission.py:139] 28) loss = 9.704, grad_norm = 1.096
I0402 07:00:32.489938 140057288447744 logging_writer.py:48] [29] global_step=29, grad_norm=1.026919, loss=9.673993
I0402 07:00:32.493320 140115014068032 submission.py:139] 29) loss = 9.674, grad_norm = 1.027
I0402 07:00:32.925871 140057296840448 logging_writer.py:48] [30] global_step=30, grad_norm=0.983660, loss=9.624943
I0402 07:00:32.928805 140115014068032 submission.py:139] 30) loss = 9.625, grad_norm = 0.984
I0402 07:00:33.360378 140057288447744 logging_writer.py:48] [31] global_step=31, grad_norm=0.925309, loss=9.606653
I0402 07:00:33.363781 140115014068032 submission.py:139] 31) loss = 9.607, grad_norm = 0.925
I0402 07:00:33.797808 140057296840448 logging_writer.py:48] [32] global_step=32, grad_norm=0.877541, loss=9.574303
I0402 07:00:33.800863 140115014068032 submission.py:139] 32) loss = 9.574, grad_norm = 0.878
I0402 07:00:34.230290 140057288447744 logging_writer.py:48] [33] global_step=33, grad_norm=0.830223, loss=9.572264
I0402 07:00:34.233785 140115014068032 submission.py:139] 33) loss = 9.572, grad_norm = 0.830
I0402 07:00:34.667065 140057296840448 logging_writer.py:48] [34] global_step=34, grad_norm=0.800244, loss=9.532280
I0402 07:00:34.670329 140115014068032 submission.py:139] 34) loss = 9.532, grad_norm = 0.800
I0402 07:00:35.102457 140057288447744 logging_writer.py:48] [35] global_step=35, grad_norm=0.762718, loss=9.536066
I0402 07:00:35.105708 140115014068032 submission.py:139] 35) loss = 9.536, grad_norm = 0.763
I0402 07:00:35.535719 140057296840448 logging_writer.py:48] [36] global_step=36, grad_norm=0.742525, loss=9.506104
I0402 07:00:35.539523 140115014068032 submission.py:139] 36) loss = 9.506, grad_norm = 0.743
I0402 07:00:35.969828 140057288447744 logging_writer.py:48] [37] global_step=37, grad_norm=0.695845, loss=9.513742
I0402 07:00:35.973672 140115014068032 submission.py:139] 37) loss = 9.514, grad_norm = 0.696
I0402 07:00:36.405320 140057296840448 logging_writer.py:48] [38] global_step=38, grad_norm=0.681352, loss=9.474570
I0402 07:00:36.409108 140115014068032 submission.py:139] 38) loss = 9.475, grad_norm = 0.681
I0402 07:00:36.841868 140057288447744 logging_writer.py:48] [39] global_step=39, grad_norm=0.659074, loss=9.456850
I0402 07:00:36.845379 140115014068032 submission.py:139] 39) loss = 9.457, grad_norm = 0.659
I0402 07:00:37.275323 140057296840448 logging_writer.py:48] [40] global_step=40, grad_norm=0.644740, loss=9.456060
I0402 07:00:37.278608 140115014068032 submission.py:139] 40) loss = 9.456, grad_norm = 0.645
I0402 07:00:37.706939 140057288447744 logging_writer.py:48] [41] global_step=41, grad_norm=0.625188, loss=9.414395
I0402 07:00:37.710340 140115014068032 submission.py:139] 41) loss = 9.414, grad_norm = 0.625
I0402 07:00:38.142621 140057296840448 logging_writer.py:48] [42] global_step=42, grad_norm=0.580117, loss=9.431786
I0402 07:00:38.145833 140115014068032 submission.py:139] 42) loss = 9.432, grad_norm = 0.580
I0402 07:00:38.572703 140057288447744 logging_writer.py:48] [43] global_step=43, grad_norm=0.586646, loss=9.380589
I0402 07:00:38.575786 140115014068032 submission.py:139] 43) loss = 9.381, grad_norm = 0.587
I0402 07:00:39.009711 140057296840448 logging_writer.py:48] [44] global_step=44, grad_norm=0.550075, loss=9.385633
I0402 07:00:39.012831 140115014068032 submission.py:139] 44) loss = 9.386, grad_norm = 0.550
I0402 07:00:39.444417 140057288447744 logging_writer.py:48] [45] global_step=45, grad_norm=0.521293, loss=9.360721
I0402 07:00:39.447571 140115014068032 submission.py:139] 45) loss = 9.361, grad_norm = 0.521
I0402 07:00:39.879663 140057296840448 logging_writer.py:48] [46] global_step=46, grad_norm=0.510016, loss=9.351683
I0402 07:00:39.882837 140115014068032 submission.py:139] 46) loss = 9.352, grad_norm = 0.510
I0402 07:00:40.314141 140057288447744 logging_writer.py:48] [47] global_step=47, grad_norm=0.489462, loss=9.353663
I0402 07:00:40.317280 140115014068032 submission.py:139] 47) loss = 9.354, grad_norm = 0.489
I0402 07:00:40.749366 140057296840448 logging_writer.py:48] [48] global_step=48, grad_norm=0.472454, loss=9.333545
I0402 07:00:40.752395 140115014068032 submission.py:139] 48) loss = 9.334, grad_norm = 0.472
I0402 07:00:41.187774 140057288447744 logging_writer.py:48] [49] global_step=49, grad_norm=0.463445, loss=9.308665
I0402 07:00:41.191155 140115014068032 submission.py:139] 49) loss = 9.309, grad_norm = 0.463
I0402 07:00:41.622984 140057296840448 logging_writer.py:48] [50] global_step=50, grad_norm=0.450670, loss=9.306493
I0402 07:00:41.625942 140115014068032 submission.py:139] 50) loss = 9.306, grad_norm = 0.451
I0402 07:00:42.058340 140057288447744 logging_writer.py:48] [51] global_step=51, grad_norm=0.432183, loss=9.307425
I0402 07:00:42.061643 140115014068032 submission.py:139] 51) loss = 9.307, grad_norm = 0.432
I0402 07:00:42.495619 140057296840448 logging_writer.py:48] [52] global_step=52, grad_norm=0.418937, loss=9.287910
I0402 07:00:42.498625 140115014068032 submission.py:139] 52) loss = 9.288, grad_norm = 0.419
I0402 07:00:42.926923 140057288447744 logging_writer.py:48] [53] global_step=53, grad_norm=0.398009, loss=9.300776
I0402 07:00:42.930051 140115014068032 submission.py:139] 53) loss = 9.301, grad_norm = 0.398
I0402 07:00:43.362848 140057296840448 logging_writer.py:48] [54] global_step=54, grad_norm=0.410739, loss=9.301068
I0402 07:00:43.366040 140115014068032 submission.py:139] 54) loss = 9.301, grad_norm = 0.411
I0402 07:00:43.799027 140057288447744 logging_writer.py:48] [55] global_step=55, grad_norm=0.389739, loss=9.286142
I0402 07:00:43.802003 140115014068032 submission.py:139] 55) loss = 9.286, grad_norm = 0.390
I0402 07:00:44.233926 140057296840448 logging_writer.py:48] [56] global_step=56, grad_norm=0.386957, loss=9.286592
I0402 07:00:44.236845 140115014068032 submission.py:139] 56) loss = 9.287, grad_norm = 0.387
I0402 07:00:44.667471 140057288447744 logging_writer.py:48] [57] global_step=57, grad_norm=0.374659, loss=9.264541
I0402 07:00:44.670356 140115014068032 submission.py:139] 57) loss = 9.265, grad_norm = 0.375
I0402 07:00:45.103078 140057296840448 logging_writer.py:48] [58] global_step=58, grad_norm=0.357096, loss=9.264334
I0402 07:00:45.106167 140115014068032 submission.py:139] 58) loss = 9.264, grad_norm = 0.357
I0402 07:00:45.540192 140057288447744 logging_writer.py:48] [59] global_step=59, grad_norm=0.352856, loss=9.255024
I0402 07:00:45.543580 140115014068032 submission.py:139] 59) loss = 9.255, grad_norm = 0.353
I0402 07:00:45.972328 140057296840448 logging_writer.py:48] [60] global_step=60, grad_norm=0.343964, loss=9.245098
I0402 07:00:45.975720 140115014068032 submission.py:139] 60) loss = 9.245, grad_norm = 0.344
I0402 07:00:46.408451 140057288447744 logging_writer.py:48] [61] global_step=61, grad_norm=0.336123, loss=9.244417
I0402 07:00:46.412068 140115014068032 submission.py:139] 61) loss = 9.244, grad_norm = 0.336
I0402 07:00:46.846361 140057296840448 logging_writer.py:48] [62] global_step=62, grad_norm=0.336169, loss=9.217715
I0402 07:00:46.849390 140115014068032 submission.py:139] 62) loss = 9.218, grad_norm = 0.336
I0402 07:00:47.281869 140057288447744 logging_writer.py:48] [63] global_step=63, grad_norm=0.313140, loss=9.230317
I0402 07:00:47.284949 140115014068032 submission.py:139] 63) loss = 9.230, grad_norm = 0.313
I0402 07:00:47.717868 140057296840448 logging_writer.py:48] [64] global_step=64, grad_norm=0.315197, loss=9.224865
I0402 07:00:47.721027 140115014068032 submission.py:139] 64) loss = 9.225, grad_norm = 0.315
I0402 07:00:48.152171 140057288447744 logging_writer.py:48] [65] global_step=65, grad_norm=0.308075, loss=9.226703
I0402 07:00:48.155205 140115014068032 submission.py:139] 65) loss = 9.227, grad_norm = 0.308
I0402 07:00:48.584938 140057296840448 logging_writer.py:48] [66] global_step=66, grad_norm=0.302037, loss=9.175369
I0402 07:00:48.588015 140115014068032 submission.py:139] 66) loss = 9.175, grad_norm = 0.302
I0402 07:00:49.017663 140057288447744 logging_writer.py:48] [67] global_step=67, grad_norm=0.290834, loss=9.194121
I0402 07:00:49.020719 140115014068032 submission.py:139] 67) loss = 9.194, grad_norm = 0.291
I0402 07:00:49.452437 140057296840448 logging_writer.py:48] [68] global_step=68, grad_norm=0.282346, loss=9.176794
I0402 07:00:49.455533 140115014068032 submission.py:139] 68) loss = 9.177, grad_norm = 0.282
I0402 07:00:49.887702 140057288447744 logging_writer.py:48] [69] global_step=69, grad_norm=0.273668, loss=9.189848
I0402 07:00:49.890839 140115014068032 submission.py:139] 69) loss = 9.190, grad_norm = 0.274
I0402 07:00:50.328430 140057296840448 logging_writer.py:48] [70] global_step=70, grad_norm=0.267220, loss=9.179094
I0402 07:00:50.331570 140115014068032 submission.py:139] 70) loss = 9.179, grad_norm = 0.267
I0402 07:00:50.766255 140057288447744 logging_writer.py:48] [71] global_step=71, grad_norm=0.264399, loss=9.182738
I0402 07:00:50.769131 140115014068032 submission.py:139] 71) loss = 9.183, grad_norm = 0.264
I0402 07:00:51.199543 140057296840448 logging_writer.py:48] [72] global_step=72, grad_norm=0.256325, loss=9.163692
I0402 07:00:51.202636 140115014068032 submission.py:139] 72) loss = 9.164, grad_norm = 0.256
I0402 07:00:51.638549 140057288447744 logging_writer.py:48] [73] global_step=73, grad_norm=0.261837, loss=9.137284
I0402 07:00:51.642348 140115014068032 submission.py:139] 73) loss = 9.137, grad_norm = 0.262
I0402 07:00:52.075089 140057296840448 logging_writer.py:48] [74] global_step=74, grad_norm=0.251740, loss=9.153648
I0402 07:00:52.078572 140115014068032 submission.py:139] 74) loss = 9.154, grad_norm = 0.252
I0402 07:00:52.517747 140057288447744 logging_writer.py:48] [75] global_step=75, grad_norm=0.243500, loss=9.159735
I0402 07:00:52.521562 140115014068032 submission.py:139] 75) loss = 9.160, grad_norm = 0.243
I0402 07:00:52.954895 140057296840448 logging_writer.py:48] [76] global_step=76, grad_norm=0.249041, loss=9.150764
I0402 07:00:52.958117 140115014068032 submission.py:139] 76) loss = 9.151, grad_norm = 0.249
I0402 07:00:53.389844 140057288447744 logging_writer.py:48] [77] global_step=77, grad_norm=0.242693, loss=9.136765
I0402 07:00:53.392959 140115014068032 submission.py:139] 77) loss = 9.137, grad_norm = 0.243
I0402 07:00:53.827242 140057296840448 logging_writer.py:48] [78] global_step=78, grad_norm=0.236196, loss=9.148952
I0402 07:00:53.830866 140115014068032 submission.py:139] 78) loss = 9.149, grad_norm = 0.236
I0402 07:00:54.265645 140057288447744 logging_writer.py:48] [79] global_step=79, grad_norm=0.229470, loss=9.148453
I0402 07:00:54.268886 140115014068032 submission.py:139] 79) loss = 9.148, grad_norm = 0.229
I0402 07:00:54.701609 140057296840448 logging_writer.py:48] [80] global_step=80, grad_norm=0.221006, loss=9.143649
I0402 07:00:54.705539 140115014068032 submission.py:139] 80) loss = 9.144, grad_norm = 0.221
I0402 07:00:55.136724 140057288447744 logging_writer.py:48] [81] global_step=81, grad_norm=0.220616, loss=9.132977
I0402 07:00:55.140390 140115014068032 submission.py:139] 81) loss = 9.133, grad_norm = 0.221
I0402 07:00:55.576308 140057296840448 logging_writer.py:48] [82] global_step=82, grad_norm=0.222451, loss=9.106747
I0402 07:00:55.580040 140115014068032 submission.py:139] 82) loss = 9.107, grad_norm = 0.222
I0402 07:00:56.013650 140057288447744 logging_writer.py:48] [83] global_step=83, grad_norm=0.224238, loss=9.137080
I0402 07:00:56.016723 140115014068032 submission.py:139] 83) loss = 9.137, grad_norm = 0.224
I0402 07:00:56.450006 140057296840448 logging_writer.py:48] [84] global_step=84, grad_norm=0.210442, loss=9.126216
I0402 07:00:56.452944 140115014068032 submission.py:139] 84) loss = 9.126, grad_norm = 0.210
I0402 07:00:56.884351 140057288447744 logging_writer.py:48] [85] global_step=85, grad_norm=0.202272, loss=9.141617
I0402 07:00:56.887397 140115014068032 submission.py:139] 85) loss = 9.142, grad_norm = 0.202
I0402 07:00:57.319308 140057296840448 logging_writer.py:48] [86] global_step=86, grad_norm=0.205877, loss=9.123802
I0402 07:00:57.322616 140115014068032 submission.py:139] 86) loss = 9.124, grad_norm = 0.206
I0402 07:00:57.755700 140057288447744 logging_writer.py:48] [87] global_step=87, grad_norm=0.206393, loss=9.106762
I0402 07:00:57.758760 140115014068032 submission.py:139] 87) loss = 9.107, grad_norm = 0.206
I0402 07:00:58.192199 140057296840448 logging_writer.py:48] [88] global_step=88, grad_norm=0.205551, loss=9.065490
I0402 07:00:58.195177 140115014068032 submission.py:139] 88) loss = 9.065, grad_norm = 0.206
I0402 07:00:58.628542 140057288447744 logging_writer.py:48] [89] global_step=89, grad_norm=0.196036, loss=9.094029
I0402 07:00:58.631508 140115014068032 submission.py:139] 89) loss = 9.094, grad_norm = 0.196
I0402 07:00:59.060822 140057296840448 logging_writer.py:48] [90] global_step=90, grad_norm=0.187561, loss=9.111266
I0402 07:00:59.063622 140115014068032 submission.py:139] 90) loss = 9.111, grad_norm = 0.188
I0402 07:00:59.496005 140057288447744 logging_writer.py:48] [91] global_step=91, grad_norm=0.187764, loss=9.090498
I0402 07:00:59.499045 140115014068032 submission.py:139] 91) loss = 9.090, grad_norm = 0.188
I0402 07:00:59.929394 140057296840448 logging_writer.py:48] [92] global_step=92, grad_norm=0.184706, loss=9.075055
I0402 07:00:59.933095 140115014068032 submission.py:139] 92) loss = 9.075, grad_norm = 0.185
I0402 07:01:00.364308 140057288447744 logging_writer.py:48] [93] global_step=93, grad_norm=0.189852, loss=9.094550
I0402 07:01:00.367362 140115014068032 submission.py:139] 93) loss = 9.095, grad_norm = 0.190
I0402 07:01:00.800096 140057296840448 logging_writer.py:48] [94] global_step=94, grad_norm=0.188423, loss=9.112453
I0402 07:01:00.803192 140115014068032 submission.py:139] 94) loss = 9.112, grad_norm = 0.188
I0402 07:01:01.233639 140057288447744 logging_writer.py:48] [95] global_step=95, grad_norm=0.181027, loss=9.088586
I0402 07:01:01.236594 140115014068032 submission.py:139] 95) loss = 9.089, grad_norm = 0.181
I0402 07:01:01.669284 140057296840448 logging_writer.py:48] [96] global_step=96, grad_norm=0.178247, loss=9.077992
I0402 07:01:01.672681 140115014068032 submission.py:139] 96) loss = 9.078, grad_norm = 0.178
I0402 07:01:02.104430 140057288447744 logging_writer.py:48] [97] global_step=97, grad_norm=0.174637, loss=9.082399
I0402 07:01:02.107546 140115014068032 submission.py:139] 97) loss = 9.082, grad_norm = 0.175
I0402 07:01:02.540307 140057296840448 logging_writer.py:48] [98] global_step=98, grad_norm=0.172281, loss=9.082414
I0402 07:01:02.543337 140115014068032 submission.py:139] 98) loss = 9.082, grad_norm = 0.172
I0402 07:01:02.975717 140057288447744 logging_writer.py:48] [99] global_step=99, grad_norm=0.168817, loss=9.089719
I0402 07:01:02.978680 140115014068032 submission.py:139] 99) loss = 9.090, grad_norm = 0.169
I0402 07:01:03.408170 140057296840448 logging_writer.py:48] [100] global_step=100, grad_norm=0.170047, loss=9.080819
I0402 07:01:03.411205 140115014068032 submission.py:139] 100) loss = 9.081, grad_norm = 0.170
I0402 07:03:53.299391 140057288447744 logging_writer.py:48] [500] global_step=500, grad_norm=0.444945, loss=8.458024
I0402 07:03:53.302814 140115014068032 submission.py:139] 500) loss = 8.458, grad_norm = 0.445
I0402 07:07:25.935518 140057296840448 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.811759, loss=7.854583
I0402 07:07:25.939314 140115014068032 submission.py:139] 1000) loss = 7.855, grad_norm = 0.812
I0402 07:10:58.619595 140057288447744 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.754793, loss=7.449971
I0402 07:10:58.623022 140115014068032 submission.py:139] 1500) loss = 7.450, grad_norm = 0.755
I0402 07:14:20.163785 140115014068032 submission_runner.py:373] Before eval at step 1975: RAM USED (GB) 20.850536448
I0402 07:14:20.163997 140115014068032 spec.py:298] Evaluating on the training split.
I0402 07:14:24.028667 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:18:54.657159 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 07:18:58.395901 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:23:23.708802 140115014068032 spec.py:326] Evaluating on the test split.
I0402 07:23:27.507557 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:27:58.839886 140115014068032 submission_runner.py:382] Time since start: 1668.62s, 	Step: 1975, 	{'train/accuracy': 0.2819024725274725, 'train/loss': 5.8628891941391945, 'train/bleu': 5.424462670933116, 'validation/accuracy': 0.26023235917719556, 'validation/loss': 6.129470651324844, 'validation/bleu': 2.7044553783130656, 'validation/num_examples': 3000, 'test/accuracy': 0.237336587066411, 'test/loss': 6.430974376852014, 'test/bleu': 1.739978561672262, 'test/num_examples': 3003}
I0402 07:27:58.840289 140115014068032 submission_runner.py:396] After eval at step 1975: RAM USED (GB) 21.065629696
I0402 07:27:58.848333 140057296840448 logging_writer.py:48] [1975] global_step=1975, preemption_count=0, score=838.890090, test/accuracy=0.237337, test/bleu=1.739979, test/loss=6.430974, test/num_examples=3003, total_duration=1668.623434, train/accuracy=0.281902, train/bleu=5.424463, train/loss=5.862889, validation/accuracy=0.260232, validation/bleu=2.704455, validation/loss=6.129471, validation/num_examples=3000
I0402 07:28:00.335620 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_1975.
I0402 07:28:00.336282 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 1975: RAM USED (GB) 21.064998912
I0402 07:28:11.366134 140057288447744 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.524332, loss=7.099174
I0402 07:28:11.369810 140115014068032 submission.py:139] 2000) loss = 7.099, grad_norm = 0.524
I0402 07:31:44.003525 140057296840448 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.811350, loss=6.753804
I0402 07:31:44.007230 140115014068032 submission.py:139] 2500) loss = 6.754, grad_norm = 0.811
I0402 07:35:16.477114 140057288447744 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.679318, loss=6.489244
I0402 07:35:16.480694 140115014068032 submission.py:139] 3000) loss = 6.489, grad_norm = 0.679
I0402 07:38:48.824223 140057296840448 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.802669, loss=6.099679
I0402 07:38:48.827319 140115014068032 submission.py:139] 3500) loss = 6.100, grad_norm = 0.803
I0402 07:42:00.456154 140115014068032 submission_runner.py:373] Before eval at step 3952: RAM USED (GB) 21.174259712
I0402 07:42:00.456359 140115014068032 spec.py:298] Evaluating on the training split.
I0402 07:42:04.350004 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:45:49.236348 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 07:45:52.982928 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:49:26.267560 140115014068032 spec.py:326] Evaluating on the test split.
I0402 07:49:30.053157 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 07:52:37.562744 140115014068032 submission_runner.py:382] Time since start: 3328.92s, 	Step: 3952, 	{'train/accuracy': 0.4099175612628527, 'train/loss': 4.3396181546405765, 'train/bleu': 13.757641374131435, 'validation/accuracy': 0.39631250697449505, 'validation/loss': 4.449085798688175, 'validation/bleu': 9.249500073994247, 'validation/num_examples': 3000, 'test/accuracy': 0.38024519202835394, 'test/loss': 4.664600400906397, 'test/bleu': 7.849940560244745, 'test/num_examples': 3003}
I0402 07:52:37.563186 140115014068032 submission_runner.py:396] After eval at step 3952: RAM USED (GB) 21.279264768
I0402 07:52:37.571279 140057288447744 logging_writer.py:48] [3952] global_step=3952, preemption_count=0, score=1673.622093, test/accuracy=0.380245, test/bleu=7.849941, test/loss=4.664600, test/num_examples=3003, total_duration=3328.917758, train/accuracy=0.409918, train/bleu=13.757641, train/loss=4.339618, validation/accuracy=0.396313, validation/bleu=9.249500, validation/loss=4.449086, validation/num_examples=3000
I0402 07:52:39.044229 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_3952.
I0402 07:52:39.044834 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 3952: RAM USED (GB) 21.27978496
I0402 07:52:59.866218 140057296840448 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.581950, loss=5.884239
I0402 07:52:59.869775 140115014068032 submission.py:139] 4000) loss = 5.884, grad_norm = 0.582
I0402 07:56:32.484451 140057288447744 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.561523, loss=5.635168
I0402 07:56:32.487636 140115014068032 submission.py:139] 4500) loss = 5.635, grad_norm = 0.562
I0402 08:00:04.904095 140057296840448 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.504392, loss=5.344978
I0402 08:00:04.907886 140115014068032 submission.py:139] 5000) loss = 5.345, grad_norm = 0.504
I0402 08:03:37.334238 140057288447744 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.565136, loss=5.339783
I0402 08:03:37.337920 140115014068032 submission.py:139] 5500) loss = 5.340, grad_norm = 0.565
I0402 08:06:39.086443 140115014068032 submission_runner.py:373] Before eval at step 5929: RAM USED (GB) 21.344370688
I0402 08:06:39.086647 140115014068032 spec.py:298] Evaluating on the training split.
I0402 08:06:42.964234 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:09:03.545060 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 08:09:07.274699 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:11:23.466337 140115014068032 spec.py:326] Evaluating on the test split.
I0402 08:11:27.264341 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:13:42.842074 140115014068032 submission_runner.py:382] Time since start: 4807.55s, 	Step: 5929, 	{'train/accuracy': 0.5060472787245739, 'train/loss': 3.438287406083929, 'train/bleu': 21.535695679445798, 'validation/accuracy': 0.5061561542944291, 'validation/loss': 3.4319014178373486, 'validation/bleu': 17.553519757205073, 'validation/num_examples': 3000, 'test/accuracy': 0.49777467898437044, 'test/loss': 3.53861193422811, 'test/bleu': 15.890591379247667, 'test/num_examples': 3003}
I0402 08:13:42.842480 140115014068032 submission_runner.py:396] After eval at step 5929: RAM USED (GB) 21.395984384
I0402 08:13:42.851150 140057296840448 logging_writer.py:48] [5929] global_step=5929, preemption_count=0, score=2508.170262, test/accuracy=0.497775, test/bleu=15.890591, test/loss=3.538612, test/num_examples=3003, total_duration=4807.548042, train/accuracy=0.506047, train/bleu=21.535696, train/loss=3.438287, validation/accuracy=0.506156, validation/bleu=17.553520, validation/loss=3.431901, validation/num_examples=3000
I0402 08:13:44.318965 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_5929.
I0402 08:13:44.319597 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 5929: RAM USED (GB) 21.395296256
I0402 08:14:14.937443 140057288447744 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.565045, loss=5.189980
I0402 08:14:14.940984 140115014068032 submission.py:139] 6000) loss = 5.190, grad_norm = 0.565
I0402 08:17:47.356031 140057296840448 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.447950, loss=5.094644
I0402 08:17:47.359398 140115014068032 submission.py:139] 6500) loss = 5.095, grad_norm = 0.448
I0402 08:21:19.901997 140057288447744 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.439620, loss=5.119563
I0402 08:21:19.905272 140115014068032 submission.py:139] 7000) loss = 5.120, grad_norm = 0.440
I0402 08:24:52.275058 140057296840448 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.400853, loss=4.912022
I0402 08:24:52.278766 140115014068032 submission.py:139] 7500) loss = 4.912, grad_norm = 0.401
I0402 08:27:44.693968 140115014068032 submission_runner.py:373] Before eval at step 7907: RAM USED (GB) 21.76782336
I0402 08:27:44.694188 140115014068032 spec.py:298] Evaluating on the training split.
I0402 08:27:48.553112 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:30:14.485130 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 08:30:18.219379 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:32:32.613594 140115014068032 spec.py:326] Evaluating on the test split.
I0402 08:32:36.418813 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:34:46.326066 140115014068032 submission_runner.py:382] Time since start: 6073.15s, 	Step: 7907, 	{'train/accuracy': 0.5498872411672248, 'train/loss': 3.0165073521036927, 'train/bleu': 25.220030316059063, 'validation/accuracy': 0.5499869809425798, 'validation/loss': 2.9781686680884305, 'validation/bleu': 21.09208598029194, 'validation/num_examples': 3000, 'test/accuracy': 0.5494393120678636, 'test/loss': 3.025727369124397, 'test/bleu': 19.348194924967082, 'test/num_examples': 3003}
I0402 08:34:46.326453 140115014068032 submission_runner.py:396] After eval at step 7907: RAM USED (GB) 21.804814336
I0402 08:34:46.334778 140057288447744 logging_writer.py:48] [7907] global_step=7907, preemption_count=0, score=3342.985245, test/accuracy=0.549439, test/bleu=19.348195, test/loss=3.025727, test/num_examples=3003, total_duration=6073.154531, train/accuracy=0.549887, train/bleu=25.220030, train/loss=3.016507, validation/accuracy=0.549987, validation/bleu=21.092086, validation/loss=2.978169, validation/num_examples=3000
I0402 08:34:47.823074 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_7907.
I0402 08:34:47.823725 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 7907: RAM USED (GB) 21.802827776
I0402 08:35:27.833214 140057296840448 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.393551, loss=4.943712
I0402 08:35:27.836255 140115014068032 submission.py:139] 8000) loss = 4.944, grad_norm = 0.394
I0402 08:39:00.381469 140057288447744 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.461037, loss=4.893533
I0402 08:39:00.384812 140115014068032 submission.py:139] 8500) loss = 4.894, grad_norm = 0.461
I0402 08:42:32.815228 140057296840448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.403108, loss=4.762156
I0402 08:42:32.818896 140115014068032 submission.py:139] 9000) loss = 4.762, grad_norm = 0.403
I0402 08:46:05.206091 140057288447744 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.395063, loss=4.736830
I0402 08:46:05.209113 140115014068032 submission.py:139] 9500) loss = 4.737, grad_norm = 0.395
I0402 08:48:48.165766 140115014068032 submission_runner.py:373] Before eval at step 9885: RAM USED (GB) 21.959770112
I0402 08:48:48.165967 140115014068032 spec.py:298] Evaluating on the training split.
I0402 08:48:52.029388 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:51:26.327040 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 08:51:30.037270 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:53:47.778687 140115014068032 spec.py:326] Evaluating on the test split.
I0402 08:53:51.576922 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:55:59.706931 140115014068032 submission_runner.py:382] Time since start: 7336.63s, 	Step: 9885, 	{'train/accuracy': 0.5700870752945194, 'train/loss': 2.8110770232200784, 'train/bleu': 26.32275698861567, 'validation/accuracy': 0.5736196699358966, 'validation/loss': 2.748675620885048, 'validation/bleu': 22.681632627821017, 'validation/num_examples': 3000, 'test/accuracy': 0.5736099006449363, 'test/loss': 2.7710417756086225, 'test/bleu': 21.020805532483642, 'test/num_examples': 3003}
I0402 08:55:59.707315 140115014068032 submission_runner.py:396] After eval at step 9885: RAM USED (GB) 21.99353344
I0402 08:55:59.715310 140057296840448 logging_writer.py:48] [9885] global_step=9885, preemption_count=0, score=4177.913882, test/accuracy=0.573610, test/bleu=21.020806, test/loss=2.771042, test/num_examples=3003, total_duration=7336.626063, train/accuracy=0.570087, train/bleu=26.322757, train/loss=2.811077, validation/accuracy=0.573620, validation/bleu=22.681633, validation/loss=2.748676, validation/num_examples=3000
I0402 08:56:01.135254 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_9885.
I0402 08:56:01.135888 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 9885: RAM USED (GB) 21.993549824
I0402 08:56:50.043071 140115014068032 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 21.992108032
I0402 08:56:50.043258 140115014068032 spec.py:298] Evaluating on the training split.
I0402 08:56:53.937251 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 08:59:16.538385 140115014068032 spec.py:310] Evaluating on the validation split.
I0402 08:59:20.261790 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 09:01:31.948312 140115014068032 spec.py:326] Evaluating on the test split.
I0402 09:01:35.737532 140115014068032 workload.py:130] Translating evaluation dataset.
I0402 09:03:41.993696 140115014068032 submission_runner.py:382] Time since start: 7818.50s, 	Step: 10000, 	{'train/accuracy': 0.5683687846559403, 'train/loss': 2.8054606896532133, 'train/bleu': 26.647696929879746, 'validation/accuracy': 0.5734832798105417, 'validation/loss': 2.712325908854199, 'validation/bleu': 22.72693487733308, 'validation/num_examples': 3000, 'test/accuracy': 0.5755272790657138, 'test/loss': 2.7270359435825924, 'test/bleu': 21.22539528064128, 'test/num_examples': 3003}
I0402 09:03:41.994080 140115014068032 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 22.06169088
I0402 09:03:42.002745 140057288447744 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4226.500387, test/accuracy=0.575527, test/bleu=21.225395, test/loss=2.727036, test/num_examples=3003, total_duration=7818.501089, train/accuracy=0.568369, train/bleu=26.647697, train/loss=2.805461, validation/accuracy=0.573483, validation/bleu=22.726935, validation/loss=2.712326, validation/num_examples=3000
I0402 09:03:43.434661 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_10000.
I0402 09:03:43.435251 140115014068032 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.062022656
I0402 09:03:43.442490 140057296840448 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4226.500387
I0402 09:03:46.243069 140115014068032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/wmt_pytorch/trial_1/checkpoint_10000.
I0402 09:03:46.265416 140115014068032 submission_runner.py:550] Tuning trial 1/1
I0402 09:03:46.265589 140115014068032 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 09:03:46.266412 140115014068032 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000628140703517588, 'train/loss': 11.260627712425766, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.264276791360306, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.25242940561269, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.7278432846069336, 'total_duration': 3.7300968170166016, 'global_step': 1, 'preemption_count': 0}), (1975, {'train/accuracy': 0.2819024725274725, 'train/loss': 5.8628891941391945, 'train/bleu': 5.424462670933116, 'validation/accuracy': 0.26023235917719556, 'validation/loss': 6.129470651324844, 'validation/bleu': 2.7044553783130656, 'validation/num_examples': 3000, 'test/accuracy': 0.237336587066411, 'test/loss': 6.430974376852014, 'test/bleu': 1.739978561672262, 'test/num_examples': 3003, 'score': 838.8900899887085, 'total_duration': 1668.6234340667725, 'global_step': 1975, 'preemption_count': 0}), (3952, {'train/accuracy': 0.4099175612628527, 'train/loss': 4.3396181546405765, 'train/bleu': 13.757641374131435, 'validation/accuracy': 0.39631250697449505, 'validation/loss': 4.449085798688175, 'validation/bleu': 9.249500073994247, 'validation/num_examples': 3000, 'test/accuracy': 0.38024519202835394, 'test/loss': 4.664600400906397, 'test/bleu': 7.849940560244745, 'test/num_examples': 3003, 'score': 1673.6220932006836, 'total_duration': 3328.9177582263947, 'global_step': 3952, 'preemption_count': 0}), (5929, {'train/accuracy': 0.5060472787245739, 'train/loss': 3.438287406083929, 'train/bleu': 21.535695679445798, 'validation/accuracy': 0.5061561542944291, 'validation/loss': 3.4319014178373486, 'validation/bleu': 17.553519757205073, 'validation/num_examples': 3000, 'test/accuracy': 0.49777467898437044, 'test/loss': 3.53861193422811, 'test/bleu': 15.890591379247667, 'test/num_examples': 3003, 'score': 2508.170261621475, 'total_duration': 4807.548041820526, 'global_step': 5929, 'preemption_count': 0}), (7907, {'train/accuracy': 0.5498872411672248, 'train/loss': 3.0165073521036927, 'train/bleu': 25.220030316059063, 'validation/accuracy': 0.5499869809425798, 'validation/loss': 2.9781686680884305, 'validation/bleu': 21.09208598029194, 'validation/num_examples': 3000, 'test/accuracy': 0.5494393120678636, 'test/loss': 3.025727369124397, 'test/bleu': 19.348194924967082, 'test/num_examples': 3003, 'score': 3342.9852454662323, 'total_duration': 6073.154530763626, 'global_step': 7907, 'preemption_count': 0}), (9885, {'train/accuracy': 0.5700870752945194, 'train/loss': 2.8110770232200784, 'train/bleu': 26.32275698861567, 'validation/accuracy': 0.5736196699358966, 'validation/loss': 2.748675620885048, 'validation/bleu': 22.681632627821017, 'validation/num_examples': 3000, 'test/accuracy': 0.5736099006449363, 'test/loss': 2.7710417756086225, 'test/bleu': 21.020805532483642, 'test/num_examples': 3003, 'score': 4177.913882493973, 'total_duration': 7336.626063108444, 'global_step': 9885, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5683687846559403, 'train/loss': 2.8054606896532133, 'train/bleu': 26.647696929879746, 'validation/accuracy': 0.5734832798105417, 'validation/loss': 2.712325908854199, 'validation/bleu': 22.72693487733308, 'validation/num_examples': 3000, 'test/accuracy': 0.5755272790657138, 'test/loss': 2.7270359435825924, 'test/bleu': 21.22539528064128, 'test/num_examples': 3003, 'score': 4226.500387430191, 'total_duration': 7818.501089096069, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0402 09:03:46.266499 140115014068032 submission_runner.py:553] Timing: 4226.500387430191
I0402 09:03:46.266544 140115014068032 submission_runner.py:554] ====================
I0402 09:03:46.266614 140115014068032 submission_runner.py:613] Final wmt score: 4226.500387430191
