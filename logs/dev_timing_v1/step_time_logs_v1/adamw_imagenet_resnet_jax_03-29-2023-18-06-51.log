I0329 18:07:06.034437 140428736329536 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/imagenet_resnet_jax.
I0329 18:07:06.077318 140428736329536 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 18:07:07.285858 140428736329536 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0329 18:07:07.286411 140428736329536 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 18:07:07.291427 140428736329536 submission_runner.py:504] Using RNG seed 1246476127
I0329 18:07:08.825743 140428736329536 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 18:07:08.825927 140428736329536 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1.
I0329 18:07:08.826104 140428736329536 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/hparams.json.
I0329 18:07:08.963210 140428736329536 submission_runner.py:230] Starting train once: RAM USED (GB) 4.452683776
I0329 18:07:08.963422 140428736329536 submission_runner.py:231] Initializing dataset.
I0329 18:07:08.977813 140428736329536 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:07:08.986242 140428736329536 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 18:07:08.986348 140428736329536 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 18:07:09.231231 140428736329536 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:07:10.422163 140428736329536 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.524896256
I0329 18:07:10.422350 140428736329536 submission_runner.py:240] Initializing model.
I0329 18:07:24.179136 140428736329536 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.586305536
I0329 18:07:24.179328 140428736329536 submission_runner.py:252] Initializing optimizer.
I0329 18:07:25.354452 140428736329536 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.588111872
I0329 18:07:25.354639 140428736329536 submission_runner.py:261] Initializing metrics bundle.
I0329 18:07:25.354699 140428736329536 submission_runner.py:275] Initializing checkpoint and logger.
I0329 18:07:25.355503 140428736329536 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0329 18:07:26.750436 140428736329536 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0329 18:07:26.752155 140428736329536 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0329 18:07:26.754971 140428736329536 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.58417152
I0329 18:07:26.755199 140428736329536 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.58417152
I0329 18:07:26.755286 140428736329536 submission_runner.py:312] Starting training loop.
2023-03-29 18:07:29.160893: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-03-29 18:07:29.317119: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-03-29 18:07:29.672457: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-03-29 18:07:30.073899: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0329 18:07:30.177143 140428736329536 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 13.578969088
2023-03-29 18:07:30.250455: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0329 18:08:14.628388 140250227660544 logging_writer.py:48] [0] global_step=0, grad_norm=0.6090543270111084, loss=6.938074111938477
I0329 18:08:14.641040 140428736329536 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 24.0212992
I0329 18:08:14.641273 140428736329536 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 24.0212992
I0329 18:08:14.641360 140428736329536 spec.py:298] Evaluating on the training split.
I0329 18:08:15.165528 140428736329536 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:08:15.173882 140428736329536 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 18:08:15.174002 140428736329536 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 18:08:15.248910 140428736329536 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:08:27.907927 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 18:08:28.799501 140428736329536 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:08:28.815191 140428736329536 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 18:08:28.815471 140428736329536 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 18:08:28.882644 140428736329536 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 18:08:47.439321 140428736329536 spec.py:326] Evaluating on the test split.
I0329 18:08:47.891039 140428736329536 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 18:08:47.895269 140428736329536 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0329 18:08:47.926394 140428736329536 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 18:08:57.793219 140428736329536 submission_runner.py:380] Time since start: 47.89s, 	Step: 1, 	{'train/accuracy': 0.0012555803405120969, 'train/loss': 6.911462306976318, 'validation/accuracy': 0.0009800000116229057, 'validation/loss': 6.91173791885376, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.911153316497803, 'test/num_examples': 10000}
I0329 18:08:57.793877 140428736329536 submission_runner.py:390] After eval at step 1: RAM USED (GB) 65.386962944
I0329 18:08:57.800968 140223954540288 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=47.801194, test/accuracy=0.000800, test/loss=6.911153, test/num_examples=10000, total_duration=47.886071, train/accuracy=0.001256, train/loss=6.911462, validation/accuracy=0.000980, validation/loss=6.911738, validation/num_examples=50000
I0329 18:08:58.031447 140428736329536 checkpoints.py:356] Saving checkpoint at step: 1
I0329 18:08:58.953689 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1
I0329 18:08:58.966309 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1.
I0329 18:08:58.972728 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 65.45975296
I0329 18:08:58.977866 140428736329536 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 65.458499584
I0329 18:08:59.056330 140428736329536 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 65.733697536
I0329 18:09:33.153104 140223962932992 logging_writer.py:48] [100] global_step=100, grad_norm=0.6320098042488098, loss=6.86870002746582
I0329 18:10:07.446448 140224323622656 logging_writer.py:48] [200] global_step=200, grad_norm=0.6632875204086304, loss=6.760900974273682
I0329 18:10:41.643622 140223962932992 logging_writer.py:48] [300] global_step=300, grad_norm=0.7380589842796326, loss=6.559402942657471
I0329 18:11:15.737183 140224323622656 logging_writer.py:48] [400] global_step=400, grad_norm=0.8823413848876953, loss=6.396724700927734
I0329 18:11:49.905656 140223962932992 logging_writer.py:48] [500] global_step=500, grad_norm=0.9557089805603027, loss=6.248590469360352
I0329 18:12:24.292176 140224323622656 logging_writer.py:48] [600] global_step=600, grad_norm=1.489349126815796, loss=6.126271724700928
I0329 18:12:58.486702 140223962932992 logging_writer.py:48] [700] global_step=700, grad_norm=1.3682094812393188, loss=6.018496513366699
I0329 18:13:32.665793 140224323622656 logging_writer.py:48] [800] global_step=800, grad_norm=3.0184621810913086, loss=5.880611419677734
I0329 18:14:06.818625 140223962932992 logging_writer.py:48] [900] global_step=900, grad_norm=2.049403190612793, loss=5.85282564163208
I0329 18:14:41.030954 140224323622656 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.2490649223327637, loss=5.746396541595459
I0329 18:15:15.312956 140223962932992 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.251668453216553, loss=5.618252754211426
I0329 18:15:49.463523 140224323622656 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.2880589962005615, loss=5.513856410980225
I0329 18:16:23.599183 140223962932992 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.5937297344207764, loss=5.541645050048828
I0329 18:16:57.856556 140224323622656 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.7363498210906982, loss=5.362023830413818
I0329 18:17:29.081887 140428736329536 submission_runner.py:371] Before eval at step 1493: RAM USED (GB) 67.502186496
I0329 18:17:29.082086 140428736329536 spec.py:298] Evaluating on the training split.
I0329 18:17:36.491678 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 18:17:45.776918 140428736329536 spec.py:326] Evaluating on the test split.
I0329 18:17:48.028020 140428736329536 submission_runner.py:380] Time since start: 602.33s, 	Step: 1493, 	{'train/accuracy': 0.12783002853393555, 'train/loss': 4.688037872314453, 'validation/accuracy': 0.11208000034093857, 'validation/loss': 4.803345680236816, 'validation/num_examples': 50000, 'test/accuracy': 0.08150000125169754, 'test/loss': 5.126199245452881, 'test/num_examples': 10000}
I0329 18:17:48.028757 140428736329536 submission_runner.py:390] After eval at step 1493: RAM USED (GB) 72.926097408
I0329 18:17:48.037529 140224340408064 logging_writer.py:48] [1493] global_step=1493, preemption_count=0, score=548.989168, test/accuracy=0.081500, test/loss=5.126199, test/num_examples=10000, total_duration=602.325490, train/accuracy=0.127830, train/loss=4.688038, validation/accuracy=0.112080, validation/loss=4.803346, validation/num_examples=50000
I0329 18:17:48.273840 140428736329536 checkpoints.py:356] Saving checkpoint at step: 1493
I0329 18:17:49.281382 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1493
I0329 18:17:49.291735 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1493.
I0329 18:17:49.299522 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 1493: RAM USED (GB) 73.0112
I0329 18:17:52.029831 140224348800768 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.322171688079834, loss=5.359825134277344
I0329 18:18:26.199982 140251800508160 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.502200603485107, loss=5.274631500244141
I0329 18:19:00.324601 140224348800768 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.9521470069885254, loss=5.274633407592773
I0329 18:19:34.526500 140251800508160 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.745943546295166, loss=5.104865550994873
I0329 18:20:08.526846 140224348800768 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.287121057510376, loss=5.089626312255859
I0329 18:20:42.785893 140251800508160 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.043918609619141, loss=5.023056507110596
I0329 18:21:16.885574 140224348800768 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.314201593399048, loss=4.825213432312012
I0329 18:21:51.118538 140251800508160 logging_writer.py:48] [2200] global_step=2200, grad_norm=7.894761085510254, loss=4.899656295776367
I0329 18:22:25.050755 140224348800768 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.49501895904541, loss=4.755967140197754
I0329 18:22:59.158959 140251800508160 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.120399475097656, loss=4.722766399383545
I0329 18:23:33.354678 140224348800768 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.788165807723999, loss=4.646622180938721
I0329 18:24:07.613709 140251800508160 logging_writer.py:48] [2600] global_step=2600, grad_norm=6.036935806274414, loss=4.517467498779297
I0329 18:24:41.879312 140224348800768 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.4560091495513916, loss=4.462771892547607
I0329 18:25:16.094050 140251800508160 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.1214282512664795, loss=4.49061393737793
I0329 18:25:50.374372 140224348800768 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.5187642574310303, loss=4.502484321594238
I0329 18:26:19.455820 140428736329536 submission_runner.py:371] Before eval at step 2987: RAM USED (GB) 74.08898048
I0329 18:26:19.456043 140428736329536 spec.py:298] Evaluating on the training split.
I0329 18:26:26.480246 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 18:26:35.977739 140428736329536 spec.py:326] Evaluating on the test split.
I0329 18:26:38.082523 140428736329536 submission_runner.py:380] Time since start: 1132.70s, 	Step: 2987, 	{'train/accuracy': 0.26357221603393555, 'train/loss': 3.5914604663848877, 'validation/accuracy': 0.2400599867105484, 'validation/loss': 3.7537214756011963, 'validation/num_examples': 50000, 'test/accuracy': 0.1769000142812729, 'test/loss': 4.283113956451416, 'test/num_examples': 10000}
I0329 18:26:38.083153 140428736329536 submission_runner.py:390] After eval at step 2987: RAM USED (GB) 79.301111808
I0329 18:26:38.091275 140251800508160 logging_writer.py:48] [2987] global_step=2987, preemption_count=0, score=1048.422747, test/accuracy=0.176900, test/loss=4.283114, test/num_examples=10000, total_duration=1132.699231, train/accuracy=0.263572, train/loss=3.591460, validation/accuracy=0.240060, validation/loss=3.753721, validation/num_examples=50000
I0329 18:26:38.314431 140428736329536 checkpoints.py:356] Saving checkpoint at step: 2987
I0329 18:26:39.201792 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_2987
I0329 18:26:39.211925 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_2987.
I0329 18:26:39.219138 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 2987: RAM USED (GB) 79.387377664
I0329 18:26:43.976409 140224348800768 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.2187881469726562, loss=4.544591903686523
I0329 18:27:18.057790 140251766937344 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.56860876083374, loss=4.350040912628174
I0329 18:27:52.144214 140224348800768 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.6100165843963623, loss=4.291993141174316
I0329 18:28:26.002314 140251766937344 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.9874684810638428, loss=4.205705165863037
I0329 18:29:00.022624 140224348800768 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.1082584857940674, loss=4.12088680267334
I0329 18:29:34.171764 140251766937344 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.797894239425659, loss=4.218507289886475
I0329 18:30:08.259847 140224348800768 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.4968643188476562, loss=4.096564769744873
I0329 18:30:42.280252 140251766937344 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.063978433609009, loss=4.055739402770996
I0329 18:31:16.347799 140224348800768 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.9512250423431396, loss=4.089513778686523
I0329 18:31:50.306089 140251766937344 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.679243564605713, loss=3.9444069862365723
I0329 18:32:24.443157 140224348800768 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.696416139602661, loss=4.024844646453857
I0329 18:32:58.593028 140251766937344 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.8659473657608032, loss=3.9067916870117188
I0329 18:33:32.640520 140224348800768 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.932868480682373, loss=3.8295888900756836
I0329 18:34:06.594039 140251766937344 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.93694806098938, loss=3.8587841987609863
I0329 18:34:40.838654 140224348800768 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.3369879722595215, loss=3.8491714000701904
I0329 18:35:09.313118 140428736329536 submission_runner.py:371] Before eval at step 4485: RAM USED (GB) 80.11393024
I0329 18:35:09.313315 140428736329536 spec.py:298] Evaluating on the training split.
I0329 18:35:16.309537 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 18:35:25.881951 140428736329536 spec.py:326] Evaluating on the test split.
I0329 18:35:27.967310 140428736329536 submission_runner.py:380] Time since start: 1662.56s, 	Step: 4485, 	{'train/accuracy': 0.37579718232154846, 'train/loss': 2.888429641723633, 'validation/accuracy': 0.3510400056838989, 'validation/loss': 3.0506608486175537, 'validation/num_examples': 50000, 'test/accuracy': 0.2711000144481659, 'test/loss': 3.664693593978882, 'test/num_examples': 10000}
I0329 18:35:27.968027 140428736329536 submission_runner.py:390] After eval at step 4485: RAM USED (GB) 85.372125184
I0329 18:35:27.976699 140251766937344 logging_writer.py:48] [4485] global_step=4485, preemption_count=0, score=1541.699998, test/accuracy=0.271100, test/loss=3.664694, test/num_examples=10000, total_duration=1662.556631, train/accuracy=0.375797, train/loss=2.888430, validation/accuracy=0.351040, validation/loss=3.050661, validation/num_examples=50000
I0329 18:35:28.258453 140428736329536 checkpoints.py:356] Saving checkpoint at step: 4485
I0329 18:35:29.292720 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_4485
I0329 18:35:29.303023 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_4485.
I0329 18:35:29.310561 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 4485: RAM USED (GB) 85.473611776
I0329 18:35:34.776410 140224348800768 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.17899227142334, loss=3.8853278160095215
I0329 18:36:08.936073 140252190586624 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.1078312397003174, loss=3.800278663635254
I0329 18:36:42.716540 140224348800768 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.6637067794799805, loss=3.862514019012451
I0329 18:37:16.878237 140252190586624 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.268472671508789, loss=3.7318081855773926
I0329 18:37:51.032974 140224348800768 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.866647481918335, loss=3.746817111968994
I0329 18:38:25.034010 140252190586624 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.7497471570968628, loss=3.7187092304229736
I0329 18:38:59.258736 140224348800768 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.363499641418457, loss=3.591886520385742
I0329 18:39:33.270087 140252190586624 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.7851427793502808, loss=3.611616849899292
I0329 18:40:07.382070 140224348800768 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6614394187927246, loss=3.6558139324188232
I0329 18:40:41.470348 140252190586624 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.606331706047058, loss=3.6236417293548584
I0329 18:41:15.494160 140224348800768 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.797445297241211, loss=3.5145792961120605
I0329 18:41:49.483433 140252190586624 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.393859624862671, loss=3.584200143814087
I0329 18:42:23.557289 140224348800768 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.8584980964660645, loss=3.4807050228118896
I0329 18:42:57.685528 140252190586624 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.049119234085083, loss=3.519580602645874
I0329 18:43:31.674162 140224348800768 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.261778712272644, loss=3.508664608001709
I0329 18:43:59.532261 140428736329536 submission_runner.py:371] Before eval at step 5984: RAM USED (GB) 86.159007744
I0329 18:43:59.532501 140428736329536 spec.py:298] Evaluating on the training split.
I0329 18:44:06.627323 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 18:44:16.259427 140428736329536 spec.py:326] Evaluating on the test split.
I0329 18:44:18.452473 140428736329536 submission_runner.py:380] Time since start: 2192.78s, 	Step: 5984, 	{'train/accuracy': 0.4654217064380646, 'train/loss': 2.465484142303467, 'validation/accuracy': 0.426179975271225, 'validation/loss': 2.6499319076538086, 'validation/num_examples': 50000, 'test/accuracy': 0.32770001888275146, 'test/loss': 3.2530672550201416, 'test/num_examples': 10000}
I0329 18:44:18.453039 140428736329536 submission_runner.py:390] After eval at step 5984: RAM USED (GB) 91.438096384
I0329 18:44:18.460834 140252190586624 logging_writer.py:48] [5984] global_step=5984, preemption_count=0, score=2030.348713, test/accuracy=0.327700, test/loss=3.253067, test/num_examples=10000, total_duration=2192.775568, train/accuracy=0.465422, train/loss=2.465484, validation/accuracy=0.426180, validation/loss=2.649932, validation/num_examples=50000
I0329 18:44:18.716869 140428736329536 checkpoints.py:356] Saving checkpoint at step: 5984
I0329 18:44:19.638543 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_5984
I0329 18:44:19.648961 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_5984.
I0329 18:44:19.656123 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 5984: RAM USED (GB) 91.56048896
I0329 18:44:25.484902 140224348800768 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.1190335750579834, loss=3.471123218536377
I0329 18:44:59.676879 140251947329280 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.6332588195800781, loss=3.5137012004852295
I0329 18:45:33.793765 140224348800768 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.6357003450393677, loss=3.5058302879333496
I0329 18:46:07.935831 140251947329280 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.3363174200057983, loss=3.3678860664367676
I0329 18:46:42.056792 140224348800768 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.8395256996154785, loss=3.5134999752044678
I0329 18:47:16.118203 140251947329280 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.429317831993103, loss=3.3762736320495605
I0329 18:47:50.240317 140224348800768 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.4042073488235474, loss=3.368429183959961
I0329 18:48:24.157154 140251947329280 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.1598385572433472, loss=3.3806169033050537
I0329 18:48:58.298414 140224348800768 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.5310405492782593, loss=3.3740925788879395
I0329 18:49:32.219559 140251947329280 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.052007794380188, loss=3.263298749923706
I0329 18:50:06.269040 140224348800768 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.7037792205810547, loss=3.2909483909606934
I0329 18:50:40.631872 140251947329280 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.1812033653259277, loss=3.231872081756592
I0329 18:51:14.657759 140224348800768 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.3193010091781616, loss=3.235827684402466
I0329 18:51:48.614322 140251947329280 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.1011122465133667, loss=3.1963744163513184
I0329 18:52:22.834526 140224348800768 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.2734452486038208, loss=3.2630767822265625
I0329 18:52:49.679605 140428736329536 submission_runner.py:371] Before eval at step 7481: RAM USED (GB) 92.088799232
I0329 18:52:49.679797 140428736329536 spec.py:298] Evaluating on the training split.
I0329 18:52:56.767230 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 18:53:06.404657 140428736329536 spec.py:326] Evaluating on the test split.
I0329 18:53:08.306286 140428736329536 submission_runner.py:380] Time since start: 2722.92s, 	Step: 7481, 	{'train/accuracy': 0.5115593075752258, 'train/loss': 2.168877601623535, 'validation/accuracy': 0.47863999009132385, 'validation/loss': 2.3353493213653564, 'validation/num_examples': 50000, 'test/accuracy': 0.3702000081539154, 'test/loss': 3.0277156829833984, 'test/num_examples': 10000}
I0329 18:53:08.307052 140428736329536 submission_runner.py:390] After eval at step 7481: RAM USED (GB) 97.363685376
I0329 18:53:08.316985 140251947329280 logging_writer.py:48] [7481] global_step=7481, preemption_count=0, score=2516.027231, test/accuracy=0.370200, test/loss=3.027716, test/num_examples=10000, total_duration=2722.923214, train/accuracy=0.511559, train/loss=2.168878, validation/accuracy=0.478640, validation/loss=2.335349, validation/num_examples=50000
I0329 18:53:08.535257 140428736329536 checkpoints.py:356] Saving checkpoint at step: 7481
I0329 18:53:09.412521 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_7481
I0329 18:53:09.425833 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_7481.
I0329 18:53:09.435578 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 7481: RAM USED (GB) 97.498185728
I0329 18:53:16.225975 140224348800768 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.1173756122589111, loss=3.225243330001831
I0329 18:53:50.430969 140251938936576 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9922792315483093, loss=3.244856357574463
I0329 18:54:24.589171 140224348800768 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.32308828830719, loss=3.314157485961914
I0329 18:54:58.668710 140251938936576 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.333348274230957, loss=3.219555139541626
I0329 18:55:32.534382 140224348800768 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0963021516799927, loss=3.2808070182800293
I0329 18:56:06.572016 140251938936576 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.0899473428726196, loss=3.1767067909240723
I0329 18:56:40.828386 140224348800768 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.2914561033248901, loss=3.2332212924957275
I0329 18:57:15.038248 140251938936576 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.841762125492096, loss=3.0919575691223145
I0329 18:57:48.998568 140224348800768 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.062123417854309, loss=3.0546717643737793
I0329 18:58:23.124315 140251938936576 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.875382125377655, loss=3.1637957096099854
I0329 18:58:57.222773 140224348800768 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0446562767028809, loss=3.191035270690918
I0329 18:59:31.473370 140251938936576 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9347127676010132, loss=3.0448405742645264
I0329 19:00:05.485051 140224348800768 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9855583310127258, loss=3.1510934829711914
I0329 19:00:39.611356 140251938936576 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0225698947906494, loss=3.2272133827209473
I0329 19:01:13.718060 140224348800768 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0996030569076538, loss=3.0465569496154785
I0329 19:01:39.620122 140428736329536 submission_runner.py:371] Before eval at step 8978: RAM USED (GB) 98.101211136
I0329 19:01:39.620314 140428736329536 spec.py:298] Evaluating on the training split.
I0329 19:01:46.856450 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 19:01:56.502579 140428736329536 spec.py:326] Evaluating on the test split.
I0329 19:01:58.685564 140428736329536 submission_runner.py:380] Time since start: 3252.86s, 	Step: 8978, 	{'train/accuracy': 0.5593112111091614, 'train/loss': 1.9267630577087402, 'validation/accuracy': 0.5211399793624878, 'validation/loss': 2.128671884536743, 'validation/num_examples': 50000, 'test/accuracy': 0.4036000072956085, 'test/loss': 2.8252134323120117, 'test/num_examples': 10000}
I0329 19:01:58.686180 140428736329536 submission_runner.py:390] After eval at step 8978: RAM USED (GB) 103.325212672
I0329 19:01:58.695261 140251938936576 logging_writer.py:48] [8978] global_step=8978, preemption_count=0, score=3000.678460, test/accuracy=0.403600, test/loss=2.825213, test/num_examples=10000, total_duration=3252.863705, train/accuracy=0.559311, train/loss=1.926763, validation/accuracy=0.521140, validation/loss=2.128672, validation/num_examples=50000
I0329 19:01:58.912225 140428736329536 checkpoints.py:356] Saving checkpoint at step: 8978
I0329 19:01:59.815418 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_8978
I0329 19:01:59.831487 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_8978.
I0329 19:01:59.840741 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 8978: RAM USED (GB) 103.43911424
I0329 19:02:07.728515 140224348800768 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9035235047340393, loss=3.088155746459961
I0329 19:02:41.694923 140251930543872 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.8208599090576172, loss=3.0483052730560303
I0329 19:03:15.878683 140224348800768 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7540883421897888, loss=3.069434881210327
I0329 19:03:50.048535 140251930543872 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8835095763206482, loss=3.11791729927063
I0329 19:04:23.941674 140224348800768 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.063842535018921, loss=3.054281711578369
I0329 19:04:57.982725 140251930543872 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9123392105102539, loss=2.9471564292907715
I0329 19:05:32.036743 140224348800768 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.1928085088729858, loss=3.084622621536255
I0329 19:06:06.154689 140251930543872 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7143846154212952, loss=3.034008741378784
I0329 19:06:40.346609 140224348800768 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9958513975143433, loss=3.0404715538024902
I0329 19:07:14.220430 140251930543872 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7674669623374939, loss=2.9538474082946777
I0329 19:07:48.212865 140224348800768 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.910801351070404, loss=2.9489030838012695
I0329 19:08:22.439692 140251930543872 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7617524862289429, loss=2.9702308177948
I0329 19:08:56.587180 140224348800768 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0448055267333984, loss=2.943356513977051
I0329 19:09:30.486621 140251930543872 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7142500281333923, loss=3.00699782371521
I0329 19:10:04.498832 140224348800768 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7192314863204956, loss=2.8971452713012695
I0329 19:10:29.945023 140428736329536 submission_runner.py:371] Before eval at step 10476: RAM USED (GB) 104.019038208
I0329 19:10:29.945219 140428736329536 spec.py:298] Evaluating on the training split.
I0329 19:10:37.159134 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 19:10:46.828090 140428736329536 spec.py:326] Evaluating on the test split.
I0329 19:10:49.013512 140428736329536 submission_runner.py:380] Time since start: 3783.19s, 	Step: 10476, 	{'train/accuracy': 0.5933912396430969, 'train/loss': 1.8159579038619995, 'validation/accuracy': 0.5174399614334106, 'validation/loss': 2.1919045448303223, 'validation/num_examples': 50000, 'test/accuracy': 0.4001000225543976, 'test/loss': 2.8725318908691406, 'test/num_examples': 10000}
I0329 19:10:49.014274 140428736329536 submission_runner.py:390] After eval at step 10476: RAM USED (GB) 109.19016448
I0329 19:10:49.024090 140251930543872 logging_writer.py:48] [10476] global_step=10476, preemption_count=0, score=3486.451815, test/accuracy=0.400100, test/loss=2.872532, test/num_examples=10000, total_duration=3783.188319, train/accuracy=0.593391, train/loss=1.815958, validation/accuracy=0.517440, validation/loss=2.191905, validation/num_examples=50000
I0329 19:10:49.263690 140428736329536 checkpoints.py:356] Saving checkpoint at step: 10476
I0329 19:10:50.277157 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_10476
I0329 19:10:50.287606 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_10476.
I0329 19:10:50.297866 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 10476: RAM USED (GB) 109.178212352
I0329 19:10:58.752807 140224348800768 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0267590284347534, loss=2.9254162311553955
I0329 19:11:32.901026 140248411526912 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7109261155128479, loss=2.9719953536987305
I0329 19:12:07.127907 140224348800768 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6070385575294495, loss=2.9209039211273193
I0329 19:12:41.281877 140248411526912 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8228880167007446, loss=2.875293016433716
I0329 19:13:15.365609 140224348800768 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8799564838409424, loss=2.92311954498291
I0329 19:13:49.392065 140248411526912 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7941084504127502, loss=2.918550968170166
I0329 19:14:23.434637 140224348800768 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7384918332099915, loss=2.786504030227661
I0329 19:14:57.185680 140248411526912 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6409260034561157, loss=2.801360607147217
I0329 19:15:31.317968 140224348800768 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7083436846733093, loss=2.9634745121002197
I0329 19:16:05.303317 140248411526912 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7473198175430298, loss=2.8330917358398438
I0329 19:16:39.318086 140224348800768 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7427796125411987, loss=2.89589786529541
I0329 19:17:13.268969 140248411526912 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7833279371261597, loss=2.810025691986084
I0329 19:17:47.305140 140224348800768 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7450089454650879, loss=2.888282299041748
I0329 19:18:21.475473 140248411526912 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6151862740516663, loss=2.725104808807373
I0329 19:18:55.452283 140224348800768 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7406327724456787, loss=2.835355520248413
I0329 19:19:20.604962 140428736329536 submission_runner.py:371] Before eval at step 11976: RAM USED (GB) 109.771485184
I0329 19:19:20.605164 140428736329536 spec.py:298] Evaluating on the training split.
I0329 19:19:27.659701 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 19:19:37.165848 140428736329536 spec.py:326] Evaluating on the test split.
I0329 19:19:39.374810 140428736329536 submission_runner.py:380] Time since start: 4313.85s, 	Step: 11976, 	{'train/accuracy': 0.645527720451355, 'train/loss': 1.5438076257705688, 'validation/accuracy': 0.5817999839782715, 'validation/loss': 1.84953773021698, 'validation/num_examples': 50000, 'test/accuracy': 0.45510002970695496, 'test/loss': 2.5620789527893066, 'test/num_examples': 10000}
I0329 19:19:39.375444 140428736329536 submission_runner.py:390] After eval at step 11976: RAM USED (GB) 115.085586432
I0329 19:19:39.384073 140248411526912 logging_writer.py:48] [11976] global_step=11976, preemption_count=0, score=3990.205957, test/accuracy=0.455100, test/loss=2.562079, test/num_examples=10000, total_duration=4313.848574, train/accuracy=0.645528, train/loss=1.543808, validation/accuracy=0.581800, validation/loss=1.849538, validation/num_examples=50000
I0329 19:19:39.660598 140428736329536 checkpoints.py:356] Saving checkpoint at step: 11976
I0329 19:19:40.590373 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_11976
I0329 19:19:40.605150 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_11976.
I0329 19:19:40.613420 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 11976: RAM USED (GB) 115.213131776
I0329 19:19:49.146883 140224348800768 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6474554538726807, loss=2.8625431060791016
I0329 19:20:23.158552 140248197625600 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.627867579460144, loss=2.8888967037200928
I0329 19:20:57.298424 140224348800768 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5638328194618225, loss=2.9004998207092285
I0329 19:21:31.188973 140248197625600 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6919615268707275, loss=2.7884774208068848
I0329 19:22:05.177833 140224348800768 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6473285555839539, loss=2.7084171772003174
I0329 19:22:39.147273 140248197625600 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5691019892692566, loss=2.752953290939331
I0329 19:23:13.052732 140224348800768 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5741051435470581, loss=2.7891409397125244
I0329 19:23:46.886259 140248197625600 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6763395071029663, loss=2.77482271194458
I0329 19:24:20.602764 140224348800768 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6876769661903381, loss=2.798753499984741
I0329 19:24:54.473420 140248197625600 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6540660262107849, loss=2.7299959659576416
I0329 19:25:28.361502 140224348800768 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5957130193710327, loss=2.802924871444702
I0329 19:26:02.230377 140248197625600 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6435222625732422, loss=2.694680690765381
I0329 19:26:36.430648 140224348800768 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5462861657142639, loss=2.7974767684936523
I0329 19:27:10.539759 140248197625600 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5363117456436157, loss=2.7432942390441895
I0329 19:27:44.456931 140224348800768 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6236684918403625, loss=2.6564931869506836
I0329 19:28:10.715453 140428736329536 submission_runner.py:371] Before eval at step 13479: RAM USED (GB) 115.87026944
I0329 19:28:10.715671 140428736329536 spec.py:298] Evaluating on the training split.
I0329 19:28:17.896237 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 19:28:27.649627 140428736329536 spec.py:326] Evaluating on the test split.
I0329 19:28:29.725237 140428736329536 submission_runner.py:380] Time since start: 4843.96s, 	Step: 13479, 	{'train/accuracy': 0.6769770383834839, 'train/loss': 1.3930621147155762, 'validation/accuracy': 0.6094399690628052, 'validation/loss': 1.7137682437896729, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.4153172969818115, 'test/num_examples': 10000}
I0329 19:28:29.726025 140428736329536 submission_runner.py:390] After eval at step 13479: RAM USED (GB) 121.013874688
I0329 19:28:29.735520 140248197625600 logging_writer.py:48] [13479] global_step=13479, preemption_count=0, score=4492.422730, test/accuracy=0.482000, test/loss=2.415317, test/num_examples=10000, total_duration=4843.959024, train/accuracy=0.676977, train/loss=1.393062, validation/accuracy=0.609440, validation/loss=1.713768, validation/num_examples=50000
I0329 19:28:30.057174 140428736329536 checkpoints.py:356] Saving checkpoint at step: 13479
I0329 19:28:31.171869 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_13479
I0329 19:28:31.190163 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_13479.
I0329 19:28:31.199826 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 13479: RAM USED (GB) 121.16924416
I0329 19:28:38.721740 140224348800768 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6953541040420532, loss=2.7730860710144043
I0329 19:29:12.808266 140248751249152 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6705889105796814, loss=2.7730016708374023
I0329 19:29:46.619563 140224348800768 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7468998432159424, loss=2.758730411529541
I0329 19:30:20.587229 140248751249152 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5108532905578613, loss=2.693319797515869
I0329 19:30:54.573476 140224348800768 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6625787019729614, loss=2.6706504821777344
I0329 19:31:28.017153 140428736329536 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 121.646329856
I0329 19:31:28.017348 140428736329536 spec.py:298] Evaluating on the training split.
I0329 19:31:35.100410 140428736329536 spec.py:310] Evaluating on the validation split.
I0329 19:31:44.995202 140428736329536 spec.py:326] Evaluating on the test split.
I0329 19:31:46.945401 140428736329536 submission_runner.py:380] Time since start: 5041.26s, 	Step: 14000, 	{'train/accuracy': 0.645906388759613, 'train/loss': 1.568077802658081, 'validation/accuracy': 0.587440013885498, 'validation/loss': 1.8502565622329712, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.52903413772583, 'test/num_examples': 10000}
I0329 19:31:46.945969 140428736329536 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 126.881251328
I0329 19:31:46.954888 140248751249152 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4665.635682, test/accuracy=0.460100, test/loss=2.529034, test/num_examples=10000, total_duration=5041.260751, train/accuracy=0.645906, train/loss=1.568078, validation/accuracy=0.587440, validation/loss=1.850257, validation/num_examples=50000
I0329 19:31:47.206541 140428736329536 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 19:31:48.146834 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_14000
I0329 19:31:48.164965 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0329 19:31:48.170869 140428736329536 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 127.087276032
I0329 19:31:48.183392 140224348800768 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4665.635682
I0329 19:31:48.355001 140428736329536 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 19:31:49.616285 140428736329536 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_14000
I0329 19:31:49.633868 140428736329536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0329 19:31:49.834703 140428736329536 submission_runner.py:543] Tuning trial 1/1
I0329 19:31:49.835768 140428736329536 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0329 19:31:49.838858 140428736329536 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012555803405120969, 'train/loss': 6.911462306976318, 'validation/accuracy': 0.0009800000116229057, 'validation/loss': 6.91173791885376, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.911153316497803, 'test/num_examples': 10000, 'score': 47.801193714141846, 'total_duration': 47.88607144355774, 'global_step': 1, 'preemption_count': 0}), (1493, {'train/accuracy': 0.12783002853393555, 'train/loss': 4.688037872314453, 'validation/accuracy': 0.11208000034093857, 'validation/loss': 4.803345680236816, 'validation/num_examples': 50000, 'test/accuracy': 0.08150000125169754, 'test/loss': 5.126199245452881, 'test/num_examples': 10000, 'score': 548.9891681671143, 'total_duration': 602.3254895210266, 'global_step': 1493, 'preemption_count': 0}), (2987, {'train/accuracy': 0.26357221603393555, 'train/loss': 3.5914604663848877, 'validation/accuracy': 0.2400599867105484, 'validation/loss': 3.7537214756011963, 'validation/num_examples': 50000, 'test/accuracy': 0.1769000142812729, 'test/loss': 4.283113956451416, 'test/num_examples': 10000, 'score': 1048.422747373581, 'total_duration': 1132.6992313861847, 'global_step': 2987, 'preemption_count': 0}), (4485, {'train/accuracy': 0.37579718232154846, 'train/loss': 2.888429641723633, 'validation/accuracy': 0.3510400056838989, 'validation/loss': 3.0506608486175537, 'validation/num_examples': 50000, 'test/accuracy': 0.2711000144481659, 'test/loss': 3.664693593978882, 'test/num_examples': 10000, 'score': 1541.699997663498, 'total_duration': 1662.5566310882568, 'global_step': 4485, 'preemption_count': 0}), (5984, {'train/accuracy': 0.4654217064380646, 'train/loss': 2.465484142303467, 'validation/accuracy': 0.426179975271225, 'validation/loss': 2.6499319076538086, 'validation/num_examples': 50000, 'test/accuracy': 0.32770001888275146, 'test/loss': 3.2530672550201416, 'test/num_examples': 10000, 'score': 2030.3487131595612, 'total_duration': 2192.77556848526, 'global_step': 5984, 'preemption_count': 0}), (7481, {'train/accuracy': 0.5115593075752258, 'train/loss': 2.168877601623535, 'validation/accuracy': 0.47863999009132385, 'validation/loss': 2.3353493213653564, 'validation/num_examples': 50000, 'test/accuracy': 0.3702000081539154, 'test/loss': 3.0277156829833984, 'test/num_examples': 10000, 'score': 2516.027230501175, 'total_duration': 2722.923214197159, 'global_step': 7481, 'preemption_count': 0}), (8978, {'train/accuracy': 0.5593112111091614, 'train/loss': 1.9267630577087402, 'validation/accuracy': 0.5211399793624878, 'validation/loss': 2.128671884536743, 'validation/num_examples': 50000, 'test/accuracy': 0.4036000072956085, 'test/loss': 2.8252134323120117, 'test/num_examples': 10000, 'score': 3000.678459882736, 'total_duration': 3252.8637046813965, 'global_step': 8978, 'preemption_count': 0}), (10476, {'train/accuracy': 0.5933912396430969, 'train/loss': 1.8159579038619995, 'validation/accuracy': 0.5174399614334106, 'validation/loss': 2.1919045448303223, 'validation/num_examples': 50000, 'test/accuracy': 0.4001000225543976, 'test/loss': 2.8725318908691406, 'test/num_examples': 10000, 'score': 3486.451814889908, 'total_duration': 3783.188319206238, 'global_step': 10476, 'preemption_count': 0}), (11976, {'train/accuracy': 0.645527720451355, 'train/loss': 1.5438076257705688, 'validation/accuracy': 0.5817999839782715, 'validation/loss': 1.84953773021698, 'validation/num_examples': 50000, 'test/accuracy': 0.45510002970695496, 'test/loss': 2.5620789527893066, 'test/num_examples': 10000, 'score': 3990.2059569358826, 'total_duration': 4313.8485741615295, 'global_step': 11976, 'preemption_count': 0}), (13479, {'train/accuracy': 0.6769770383834839, 'train/loss': 1.3930621147155762, 'validation/accuracy': 0.6094399690628052, 'validation/loss': 1.7137682437896729, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.4153172969818115, 'test/num_examples': 10000, 'score': 4492.422730207443, 'total_duration': 4843.959024429321, 'global_step': 13479, 'preemption_count': 0}), (14000, {'train/accuracy': 0.645906388759613, 'train/loss': 1.568077802658081, 'validation/accuracy': 0.587440013885498, 'validation/loss': 1.8502565622329712, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.52903413772583, 'test/num_examples': 10000, 'score': 4665.635681629181, 'total_duration': 5041.260751008987, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0329 19:31:49.838961 140428736329536 submission_runner.py:546] Timing: 4665.635681629181
I0329 19:31:49.839009 140428736329536 submission_runner.py:547] ====================
I0329 19:31:49.839101 140428736329536 submission_runner.py:606] Final imagenet_resnet score: 4665.635681629181
