WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 07:06:40.022728 140492011624256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 07:06:40.023743 139961488463680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 07:06:40.023761 140262307743552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 07:06:40.023787 140446425601856 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 07:06:40.023804 140673729976128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 07:06:40.023841 139927914723136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 07:06:40.024246 140275522778944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 07:06:40.034168 140193614219072 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 07:06:40.034317 139961488463680 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.034368 140446425601856 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.034460 140193614219072 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.034445 140262307743552 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.034520 140673729976128 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.034549 139927914723136 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.034816 140275522778944 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:40.043612 140492011624256 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 07:06:43.851094 140193614219072 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/wmt_pytorch.
W0402 07:06:43.881919 139961488463680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.883947 140262307743552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.883953 140275522778944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.884720 139927914723136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.884737 140673729976128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.885006 140193614219072 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.888026 140492011624256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 07:06:43.888525 140446425601856 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 07:06:43.888447 140193614219072 submission_runner.py:511] Using RNG seed 450479944
I0402 07:06:43.889456 140193614219072 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 07:06:43.889569 140193614219072 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/wmt_pytorch/trial_1.
I0402 07:06:43.889771 140193614219072 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/hparams.json.
I0402 07:06:43.890902 140193614219072 submission_runner.py:230] Starting train once: RAM USED (GB) 15.349469184
I0402 07:06:43.891000 140193614219072 submission_runner.py:231] Initializing dataset.
I0402 07:06:43.891183 140193614219072 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.349469184
I0402 07:06:43.891246 140193614219072 submission_runner.py:240] Initializing model.
I0402 07:06:47.440651 140193614219072 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.659677696
I0402 07:06:47.440848 140193614219072 submission_runner.py:252] Initializing optimizer.
I0402 07:06:47.442040 140193614219072 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.6596736
I0402 07:06:47.442147 140193614219072 submission_runner.py:261] Initializing metrics bundle.
I0402 07:06:47.442204 140193614219072 submission_runner.py:276] Initializing checkpoint and logger.
I0402 07:06:47.443882 140193614219072 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 07:06:47.443993 140193614219072 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 07:06:48.093219 140193614219072 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/meta_data_0.json.
I0402 07:06:48.094008 140193614219072 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/flags_0.json.
I0402 07:06:48.125448 140193614219072 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.713048576
I0402 07:06:48.126522 140193614219072 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.713048576
I0402 07:06:48.126645 140193614219072 submission_runner.py:313] Starting training loop.
I0402 07:06:48.137084 140193614219072 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 07:06:48.140509 140193614219072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 07:06:48.140624 140193614219072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 07:06:48.192655 140193614219072 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 07:06:50.340641 140193614219072 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 20.000141312
I0402 07:06:51.889600 140146192135936 logging_writer.py:48] [0] global_step=0, grad_norm=5.995220, loss=11.128612
I0402 07:06:51.894737 140193614219072 submission.py:296] 0) loss = 11.129, grad_norm = 5.995
I0402 07:06:51.895722 140193614219072 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 20.100083712
I0402 07:06:51.902688 140193614219072 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 20.100083712
I0402 07:06:51.902797 140193614219072 spec.py:298] Evaluating on the training split.
I0402 07:06:51.904786 140193614219072 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 07:06:51.907475 140193614219072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 07:06:51.907591 140193614219072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 07:06:51.935992 140193614219072 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0402 07:06:56.100380 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:11:27.918552 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 07:11:27.921749 140193614219072 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 07:11:27.925532 140193614219072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 07:11:27.925666 140193614219072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 07:11:27.954704 140193614219072 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 07:11:31.763146 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:15:57.925064 140193614219072 spec.py:326] Evaluating on the test split.
I0402 07:15:57.927542 140193614219072 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 07:15:57.930411 140193614219072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0402 07:15:57.930551 140193614219072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0402 07:15:57.957829 140193614219072 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0402 07:16:01.826385 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:20:33.415433 140193614219072 submission_runner.py:382] Time since start: 3.78s, 	Step: 1, 	{'train/accuracy': 0.0006631830499559784, 'train/loss': 11.138438032404496, 'train/bleu': 3.976327322716544e-10, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.113079038077643, 'validation/bleu': 1.8014638110548825e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.130345418627622, 'test/bleu': 6.55137722313088e-10, 'test/num_examples': 3003}
I0402 07:20:33.415897 140193614219072 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.447076352
I0402 07:20:33.424060 140135898105600 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.774654, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.130345, test/num_examples=3003, total_duration=3.776654, train/accuracy=0.000663, train/bleu=0.000000, train/loss=11.138438, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.113079, validation/num_examples=3000
I0402 07:20:35.716706 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_1.
I0402 07:20:35.717374 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.447174656
I0402 07:20:35.721681 140193614219072 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.447174656
I0402 07:20:35.725367 140193614219072 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.725418 140673729976128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.725588 140262307743552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.725566 140275522778944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.725609 140446425601856 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.725840 139927914723136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.726119 139961488463680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:35.726136 140492011624256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 07:20:36.158182 140135889712896 logging_writer.py:48] [1] global_step=1, grad_norm=5.822178, loss=11.127107
I0402 07:20:36.162035 140193614219072 submission.py:296] 1) loss = 11.127, grad_norm = 5.822
I0402 07:20:36.162879 140193614219072 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.446490624
I0402 07:20:36.603100 140135898105600 logging_writer.py:48] [2] global_step=2, grad_norm=5.969080, loss=11.104819
I0402 07:20:36.606396 140193614219072 submission.py:296] 2) loss = 11.105, grad_norm = 5.969
I0402 07:20:37.050318 140135889712896 logging_writer.py:48] [3] global_step=3, grad_norm=5.814947, loss=11.086202
I0402 07:20:37.053552 140193614219072 submission.py:296] 3) loss = 11.086, grad_norm = 5.815
I0402 07:20:37.498492 140135898105600 logging_writer.py:48] [4] global_step=4, grad_norm=5.731169, loss=11.057940
I0402 07:20:37.502257 140193614219072 submission.py:296] 4) loss = 11.058, grad_norm = 5.731
I0402 07:20:37.944259 140135889712896 logging_writer.py:48] [5] global_step=5, grad_norm=5.650579, loss=11.019119
I0402 07:20:37.947571 140193614219072 submission.py:296] 5) loss = 11.019, grad_norm = 5.651
I0402 07:20:38.392712 140135898105600 logging_writer.py:48] [6] global_step=6, grad_norm=5.595784, loss=10.966438
I0402 07:20:38.396571 140193614219072 submission.py:296] 6) loss = 10.966, grad_norm = 5.596
I0402 07:20:38.839927 140135889712896 logging_writer.py:48] [7] global_step=7, grad_norm=5.313181, loss=10.927786
I0402 07:20:38.843657 140193614219072 submission.py:296] 7) loss = 10.928, grad_norm = 5.313
I0402 07:20:39.286958 140135898105600 logging_writer.py:48] [8] global_step=8, grad_norm=5.169757, loss=10.870034
I0402 07:20:39.290604 140193614219072 submission.py:296] 8) loss = 10.870, grad_norm = 5.170
I0402 07:20:39.736909 140135889712896 logging_writer.py:48] [9] global_step=9, grad_norm=5.017597, loss=10.809497
I0402 07:20:39.740587 140193614219072 submission.py:296] 9) loss = 10.809, grad_norm = 5.018
I0402 07:20:40.183369 140135898105600 logging_writer.py:48] [10] global_step=10, grad_norm=4.823300, loss=10.736226
I0402 07:20:40.186790 140193614219072 submission.py:296] 10) loss = 10.736, grad_norm = 4.823
I0402 07:20:40.631553 140135889712896 logging_writer.py:48] [11] global_step=11, grad_norm=4.535727, loss=10.665752
I0402 07:20:40.634742 140193614219072 submission.py:296] 11) loss = 10.666, grad_norm = 4.536
I0402 07:20:41.079653 140135898105600 logging_writer.py:48] [12] global_step=12, grad_norm=4.267149, loss=10.596062
I0402 07:20:41.083194 140193614219072 submission.py:296] 12) loss = 10.596, grad_norm = 4.267
I0402 07:20:41.530035 140135889712896 logging_writer.py:48] [13] global_step=13, grad_norm=4.105379, loss=10.507324
I0402 07:20:41.533721 140193614219072 submission.py:296] 13) loss = 10.507, grad_norm = 4.105
I0402 07:20:41.976727 140135898105600 logging_writer.py:48] [14] global_step=14, grad_norm=3.785238, loss=10.445227
I0402 07:20:41.980520 140193614219072 submission.py:296] 14) loss = 10.445, grad_norm = 3.785
I0402 07:20:42.426564 140135889712896 logging_writer.py:48] [15] global_step=15, grad_norm=3.564142, loss=10.369090
I0402 07:20:42.430217 140193614219072 submission.py:296] 15) loss = 10.369, grad_norm = 3.564
I0402 07:20:42.877409 140135898105600 logging_writer.py:48] [16] global_step=16, grad_norm=3.305898, loss=10.303845
I0402 07:20:42.880581 140193614219072 submission.py:296] 16) loss = 10.304, grad_norm = 3.306
I0402 07:20:43.326059 140135889712896 logging_writer.py:48] [17] global_step=17, grad_norm=3.115276, loss=10.224128
I0402 07:20:43.329222 140193614219072 submission.py:296] 17) loss = 10.224, grad_norm = 3.115
I0402 07:20:43.770865 140135898105600 logging_writer.py:48] [18] global_step=18, grad_norm=2.842439, loss=10.182407
I0402 07:20:43.773815 140193614219072 submission.py:296] 18) loss = 10.182, grad_norm = 2.842
I0402 07:20:44.220906 140135889712896 logging_writer.py:48] [19] global_step=19, grad_norm=2.681469, loss=10.104752
I0402 07:20:44.223807 140193614219072 submission.py:296] 19) loss = 10.105, grad_norm = 2.681
I0402 07:20:44.665559 140135898105600 logging_writer.py:48] [20] global_step=20, grad_norm=2.499613, loss=10.050119
I0402 07:20:44.668710 140193614219072 submission.py:296] 20) loss = 10.050, grad_norm = 2.500
I0402 07:20:45.114037 140135889712896 logging_writer.py:48] [21] global_step=21, grad_norm=2.318761, loss=9.992237
I0402 07:20:45.117190 140193614219072 submission.py:296] 21) loss = 9.992, grad_norm = 2.319
I0402 07:20:45.563096 140135898105600 logging_writer.py:48] [22] global_step=22, grad_norm=2.205685, loss=9.924296
I0402 07:20:45.566837 140193614219072 submission.py:296] 22) loss = 9.924, grad_norm = 2.206
I0402 07:20:46.021824 140135889712896 logging_writer.py:48] [23] global_step=23, grad_norm=2.063993, loss=9.876390
I0402 07:20:46.025403 140193614219072 submission.py:296] 23) loss = 9.876, grad_norm = 2.064
I0402 07:20:46.466552 140135898105600 logging_writer.py:48] [24] global_step=24, grad_norm=1.930425, loss=9.815023
I0402 07:20:46.470366 140193614219072 submission.py:296] 24) loss = 9.815, grad_norm = 1.930
I0402 07:20:46.921294 140135889712896 logging_writer.py:48] [25] global_step=25, grad_norm=1.836305, loss=9.755892
I0402 07:20:46.925351 140193614219072 submission.py:296] 25) loss = 9.756, grad_norm = 1.836
I0402 07:20:47.368223 140135898105600 logging_writer.py:48] [26] global_step=26, grad_norm=1.691732, loss=9.727434
I0402 07:20:47.372114 140193614219072 submission.py:296] 26) loss = 9.727, grad_norm = 1.692
I0402 07:20:47.812891 140135889712896 logging_writer.py:48] [27] global_step=27, grad_norm=1.596990, loss=9.687813
I0402 07:20:47.817066 140193614219072 submission.py:296] 27) loss = 9.688, grad_norm = 1.597
I0402 07:20:48.263391 140135898105600 logging_writer.py:48] [28] global_step=28, grad_norm=1.496821, loss=9.645630
I0402 07:20:48.266510 140193614219072 submission.py:296] 28) loss = 9.646, grad_norm = 1.497
I0402 07:20:48.708394 140135889712896 logging_writer.py:48] [29] global_step=29, grad_norm=1.443498, loss=9.592066
I0402 07:20:48.711416 140193614219072 submission.py:296] 29) loss = 9.592, grad_norm = 1.443
I0402 07:20:49.153019 140135898105600 logging_writer.py:48] [30] global_step=30, grad_norm=1.369041, loss=9.543249
I0402 07:20:49.156012 140193614219072 submission.py:296] 30) loss = 9.543, grad_norm = 1.369
I0402 07:20:49.600094 140135889712896 logging_writer.py:48] [31] global_step=31, grad_norm=1.296655, loss=9.501070
I0402 07:20:49.603668 140193614219072 submission.py:296] 31) loss = 9.501, grad_norm = 1.297
I0402 07:20:50.045830 140135898105600 logging_writer.py:48] [32] global_step=32, grad_norm=1.233604, loss=9.476709
I0402 07:20:50.049499 140193614219072 submission.py:296] 32) loss = 9.477, grad_norm = 1.234
I0402 07:20:50.490031 140135889712896 logging_writer.py:48] [33] global_step=33, grad_norm=1.154158, loss=9.449831
I0402 07:20:50.493911 140193614219072 submission.py:296] 33) loss = 9.450, grad_norm = 1.154
I0402 07:20:50.934404 140135898105600 logging_writer.py:48] [34] global_step=34, grad_norm=1.089442, loss=9.412265
I0402 07:20:50.937898 140193614219072 submission.py:296] 34) loss = 9.412, grad_norm = 1.089
I0402 07:20:51.379227 140135889712896 logging_writer.py:48] [35] global_step=35, grad_norm=1.027329, loss=9.392792
I0402 07:20:51.382750 140193614219072 submission.py:296] 35) loss = 9.393, grad_norm = 1.027
I0402 07:20:51.824656 140135898105600 logging_writer.py:48] [36] global_step=36, grad_norm=0.988657, loss=9.344872
I0402 07:20:51.828377 140193614219072 submission.py:296] 36) loss = 9.345, grad_norm = 0.989
I0402 07:20:52.273300 140135889712896 logging_writer.py:48] [37] global_step=37, grad_norm=0.930642, loss=9.367900
I0402 07:20:52.276900 140193614219072 submission.py:296] 37) loss = 9.368, grad_norm = 0.931
I0402 07:20:52.723397 140135898105600 logging_writer.py:48] [38] global_step=38, grad_norm=0.880919, loss=9.331934
I0402 07:20:52.726938 140193614219072 submission.py:296] 38) loss = 9.332, grad_norm = 0.881
I0402 07:20:53.173166 140135889712896 logging_writer.py:48] [39] global_step=39, grad_norm=0.857779, loss=9.292577
I0402 07:20:53.176233 140193614219072 submission.py:296] 39) loss = 9.293, grad_norm = 0.858
I0402 07:20:53.621233 140135898105600 logging_writer.py:48] [40] global_step=40, grad_norm=0.824515, loss=9.235657
I0402 07:20:53.624477 140193614219072 submission.py:296] 40) loss = 9.236, grad_norm = 0.825
I0402 07:20:54.065996 140135889712896 logging_writer.py:48] [41] global_step=41, grad_norm=0.789854, loss=9.231544
I0402 07:20:54.069334 140193614219072 submission.py:296] 41) loss = 9.232, grad_norm = 0.790
I0402 07:20:54.516067 140135898105600 logging_writer.py:48] [42] global_step=42, grad_norm=0.745610, loss=9.213606
I0402 07:20:54.519370 140193614219072 submission.py:296] 42) loss = 9.214, grad_norm = 0.746
I0402 07:20:54.966880 140135889712896 logging_writer.py:48] [43] global_step=43, grad_norm=0.734757, loss=9.176924
I0402 07:20:54.969887 140193614219072 submission.py:296] 43) loss = 9.177, grad_norm = 0.735
I0402 07:20:55.412490 140135898105600 logging_writer.py:48] [44] global_step=44, grad_norm=0.705960, loss=9.158667
I0402 07:20:55.415529 140193614219072 submission.py:296] 44) loss = 9.159, grad_norm = 0.706
I0402 07:20:55.863684 140135889712896 logging_writer.py:48] [45] global_step=45, grad_norm=0.669012, loss=9.150596
I0402 07:20:55.866858 140193614219072 submission.py:296] 45) loss = 9.151, grad_norm = 0.669
I0402 07:20:56.311478 140135898105600 logging_writer.py:48] [46] global_step=46, grad_norm=0.639245, loss=9.108658
I0402 07:20:56.314693 140193614219072 submission.py:296] 46) loss = 9.109, grad_norm = 0.639
I0402 07:20:56.764122 140135889712896 logging_writer.py:48] [47] global_step=47, grad_norm=0.601952, loss=9.098732
I0402 07:20:56.767818 140193614219072 submission.py:296] 47) loss = 9.099, grad_norm = 0.602
I0402 07:20:57.214038 140135898105600 logging_writer.py:48] [48] global_step=48, grad_norm=0.592408, loss=9.090335
I0402 07:20:57.217574 140193614219072 submission.py:296] 48) loss = 9.090, grad_norm = 0.592
I0402 07:20:57.662698 140135889712896 logging_writer.py:48] [49] global_step=49, grad_norm=0.568839, loss=9.070776
I0402 07:20:57.666001 140193614219072 submission.py:296] 49) loss = 9.071, grad_norm = 0.569
I0402 07:20:58.110572 140135898105600 logging_writer.py:48] [50] global_step=50, grad_norm=0.554136, loss=9.044496
I0402 07:20:58.113589 140193614219072 submission.py:296] 50) loss = 9.044, grad_norm = 0.554
I0402 07:20:58.559589 140135889712896 logging_writer.py:48] [51] global_step=51, grad_norm=0.521995, loss=9.017495
I0402 07:20:58.562568 140193614219072 submission.py:296] 51) loss = 9.017, grad_norm = 0.522
I0402 07:20:59.008871 140135898105600 logging_writer.py:48] [52] global_step=52, grad_norm=0.501921, loss=9.025102
I0402 07:20:59.012878 140193614219072 submission.py:296] 52) loss = 9.025, grad_norm = 0.502
I0402 07:20:59.456804 140135889712896 logging_writer.py:48] [53] global_step=53, grad_norm=0.474313, loss=8.997789
I0402 07:20:59.460396 140193614219072 submission.py:296] 53) loss = 8.998, grad_norm = 0.474
I0402 07:20:59.906934 140135898105600 logging_writer.py:48] [54] global_step=54, grad_norm=0.460721, loss=9.011258
I0402 07:20:59.910468 140193614219072 submission.py:296] 54) loss = 9.011, grad_norm = 0.461
I0402 07:21:00.356587 140135889712896 logging_writer.py:48] [55] global_step=55, grad_norm=0.457030, loss=9.000051
I0402 07:21:00.359997 140193614219072 submission.py:296] 55) loss = 9.000, grad_norm = 0.457
I0402 07:21:00.803221 140135898105600 logging_writer.py:48] [56] global_step=56, grad_norm=0.433106, loss=8.994472
I0402 07:21:00.806988 140193614219072 submission.py:296] 56) loss = 8.994, grad_norm = 0.433
I0402 07:21:01.252573 140135889712896 logging_writer.py:48] [57] global_step=57, grad_norm=0.416581, loss=8.952842
I0402 07:21:01.255931 140193614219072 submission.py:296] 57) loss = 8.953, grad_norm = 0.417
I0402 07:21:01.703748 140135898105600 logging_writer.py:48] [58] global_step=58, grad_norm=0.405552, loss=8.948386
I0402 07:21:01.707386 140193614219072 submission.py:296] 58) loss = 8.948, grad_norm = 0.406
I0402 07:21:02.152416 140135889712896 logging_writer.py:48] [59] global_step=59, grad_norm=0.391209, loss=8.899409
I0402 07:21:02.155703 140193614219072 submission.py:296] 59) loss = 8.899, grad_norm = 0.391
I0402 07:21:02.603027 140135898105600 logging_writer.py:48] [60] global_step=60, grad_norm=0.374262, loss=8.946603
I0402 07:21:02.606776 140193614219072 submission.py:296] 60) loss = 8.947, grad_norm = 0.374
I0402 07:21:03.053276 140135889712896 logging_writer.py:48] [61] global_step=61, grad_norm=0.365636, loss=8.905971
I0402 07:21:03.056519 140193614219072 submission.py:296] 61) loss = 8.906, grad_norm = 0.366
I0402 07:21:03.498006 140135898105600 logging_writer.py:48] [62] global_step=62, grad_norm=0.348382, loss=8.931870
I0402 07:21:03.501490 140193614219072 submission.py:296] 62) loss = 8.932, grad_norm = 0.348
I0402 07:21:03.945048 140135889712896 logging_writer.py:48] [63] global_step=63, grad_norm=0.337588, loss=8.912746
I0402 07:21:03.948655 140193614219072 submission.py:296] 63) loss = 8.913, grad_norm = 0.338
I0402 07:21:04.398328 140135898105600 logging_writer.py:48] [64] global_step=64, grad_norm=0.324434, loss=8.873502
I0402 07:21:04.401825 140193614219072 submission.py:296] 64) loss = 8.874, grad_norm = 0.324
I0402 07:21:04.842199 140135889712896 logging_writer.py:48] [65] global_step=65, grad_norm=0.313591, loss=8.893239
I0402 07:21:04.846356 140193614219072 submission.py:296] 65) loss = 8.893, grad_norm = 0.314
I0402 07:21:05.294115 140135898105600 logging_writer.py:48] [66] global_step=66, grad_norm=0.304641, loss=8.860070
I0402 07:21:05.297917 140193614219072 submission.py:296] 66) loss = 8.860, grad_norm = 0.305
I0402 07:21:05.743077 140135889712896 logging_writer.py:48] [67] global_step=67, grad_norm=0.288270, loss=8.841806
I0402 07:21:05.746604 140193614219072 submission.py:296] 67) loss = 8.842, grad_norm = 0.288
I0402 07:21:06.185245 140135898105600 logging_writer.py:48] [68] global_step=68, grad_norm=0.293612, loss=8.836833
I0402 07:21:06.188517 140193614219072 submission.py:296] 68) loss = 8.837, grad_norm = 0.294
I0402 07:21:06.632014 140135889712896 logging_writer.py:48] [69] global_step=69, grad_norm=0.286619, loss=8.826820
I0402 07:21:06.635143 140193614219072 submission.py:296] 69) loss = 8.827, grad_norm = 0.287
I0402 07:21:07.077208 140135898105600 logging_writer.py:48] [70] global_step=70, grad_norm=0.283093, loss=8.826323
I0402 07:21:07.080407 140193614219072 submission.py:296] 70) loss = 8.826, grad_norm = 0.283
I0402 07:21:07.518646 140135889712896 logging_writer.py:48] [71] global_step=71, grad_norm=0.267764, loss=8.797288
I0402 07:21:07.521791 140193614219072 submission.py:296] 71) loss = 8.797, grad_norm = 0.268
I0402 07:21:07.991173 140135898105600 logging_writer.py:48] [72] global_step=72, grad_norm=0.265256, loss=8.835299
I0402 07:21:07.995079 140193614219072 submission.py:296] 72) loss = 8.835, grad_norm = 0.265
I0402 07:21:08.444439 140135889712896 logging_writer.py:48] [73] global_step=73, grad_norm=0.266420, loss=8.791451
I0402 07:21:08.448817 140193614219072 submission.py:296] 73) loss = 8.791, grad_norm = 0.266
I0402 07:21:08.908848 140135898105600 logging_writer.py:48] [74] global_step=74, grad_norm=0.244441, loss=8.811999
I0402 07:21:08.912986 140193614219072 submission.py:296] 74) loss = 8.812, grad_norm = 0.244
I0402 07:21:09.356817 140135889712896 logging_writer.py:48] [75] global_step=75, grad_norm=0.245749, loss=8.806419
I0402 07:21:09.360011 140193614219072 submission.py:296] 75) loss = 8.806, grad_norm = 0.246
I0402 07:21:09.803121 140135898105600 logging_writer.py:48] [76] global_step=76, grad_norm=0.239976, loss=8.762489
I0402 07:21:09.806073 140193614219072 submission.py:296] 76) loss = 8.762, grad_norm = 0.240
I0402 07:21:10.266090 140135889712896 logging_writer.py:48] [77] global_step=77, grad_norm=0.220731, loss=8.780179
I0402 07:21:10.269978 140193614219072 submission.py:296] 77) loss = 8.780, grad_norm = 0.221
I0402 07:21:10.710044 140135898105600 logging_writer.py:48] [78] global_step=78, grad_norm=0.221774, loss=8.774912
I0402 07:21:10.713859 140193614219072 submission.py:296] 78) loss = 8.775, grad_norm = 0.222
I0402 07:21:11.157800 140135889712896 logging_writer.py:48] [79] global_step=79, grad_norm=0.228381, loss=8.757618
I0402 07:21:11.161406 140193614219072 submission.py:296] 79) loss = 8.758, grad_norm = 0.228
I0402 07:21:11.604825 140135898105600 logging_writer.py:48] [80] global_step=80, grad_norm=0.203515, loss=8.789020
I0402 07:21:11.608579 140193614219072 submission.py:296] 80) loss = 8.789, grad_norm = 0.204
I0402 07:21:12.052770 140135889712896 logging_writer.py:48] [81] global_step=81, grad_norm=0.205000, loss=8.744395
I0402 07:21:12.056192 140193614219072 submission.py:296] 81) loss = 8.744, grad_norm = 0.205
I0402 07:21:12.503253 140135898105600 logging_writer.py:48] [82] global_step=82, grad_norm=0.200398, loss=8.733511
I0402 07:21:12.506649 140193614219072 submission.py:296] 82) loss = 8.734, grad_norm = 0.200
I0402 07:21:12.951698 140135889712896 logging_writer.py:48] [83] global_step=83, grad_norm=0.195822, loss=8.764463
I0402 07:21:12.954969 140193614219072 submission.py:296] 83) loss = 8.764, grad_norm = 0.196
I0402 07:21:13.412405 140135898105600 logging_writer.py:48] [84] global_step=84, grad_norm=0.195344, loss=8.751587
I0402 07:21:13.415514 140193614219072 submission.py:296] 84) loss = 8.752, grad_norm = 0.195
I0402 07:21:13.859857 140135889712896 logging_writer.py:48] [85] global_step=85, grad_norm=0.199003, loss=8.741065
I0402 07:21:13.863226 140193614219072 submission.py:296] 85) loss = 8.741, grad_norm = 0.199
I0402 07:21:14.307301 140135898105600 logging_writer.py:48] [86] global_step=86, grad_norm=0.184678, loss=8.744467
I0402 07:21:14.311011 140193614219072 submission.py:296] 86) loss = 8.744, grad_norm = 0.185
I0402 07:21:14.752567 140135889712896 logging_writer.py:48] [87] global_step=87, grad_norm=0.203299, loss=8.702384
I0402 07:21:14.756077 140193614219072 submission.py:296] 87) loss = 8.702, grad_norm = 0.203
I0402 07:21:15.203347 140135898105600 logging_writer.py:48] [88] global_step=88, grad_norm=0.182740, loss=8.693567
I0402 07:21:15.207029 140193614219072 submission.py:296] 88) loss = 8.694, grad_norm = 0.183
I0402 07:21:15.653186 140135889712896 logging_writer.py:48] [89] global_step=89, grad_norm=0.184439, loss=8.722337
I0402 07:21:15.656574 140193614219072 submission.py:296] 89) loss = 8.722, grad_norm = 0.184
I0402 07:21:16.098861 140135898105600 logging_writer.py:48] [90] global_step=90, grad_norm=0.188211, loss=8.741451
I0402 07:21:16.102384 140193614219072 submission.py:296] 90) loss = 8.741, grad_norm = 0.188
I0402 07:21:16.545124 140135889712896 logging_writer.py:48] [91] global_step=91, grad_norm=0.188755, loss=8.722393
I0402 07:21:16.548385 140193614219072 submission.py:296] 91) loss = 8.722, grad_norm = 0.189
I0402 07:21:16.988367 140135898105600 logging_writer.py:48] [92] global_step=92, grad_norm=0.182767, loss=8.694053
I0402 07:21:16.991389 140193614219072 submission.py:296] 92) loss = 8.694, grad_norm = 0.183
I0402 07:21:17.434298 140135889712896 logging_writer.py:48] [93] global_step=93, grad_norm=0.183623, loss=8.705218
I0402 07:21:17.437472 140193614219072 submission.py:296] 93) loss = 8.705, grad_norm = 0.184
I0402 07:21:17.882682 140135898105600 logging_writer.py:48] [94] global_step=94, grad_norm=0.182422, loss=8.736161
I0402 07:21:17.885634 140193614219072 submission.py:296] 94) loss = 8.736, grad_norm = 0.182
I0402 07:21:18.330890 140135889712896 logging_writer.py:48] [95] global_step=95, grad_norm=0.176936, loss=8.688895
I0402 07:21:18.334402 140193614219072 submission.py:296] 95) loss = 8.689, grad_norm = 0.177
I0402 07:21:18.776334 140135898105600 logging_writer.py:48] [96] global_step=96, grad_norm=0.176928, loss=8.676244
I0402 07:21:18.780017 140193614219072 submission.py:296] 96) loss = 8.676, grad_norm = 0.177
I0402 07:21:19.226657 140135889712896 logging_writer.py:48] [97] global_step=97, grad_norm=0.188478, loss=8.666263
I0402 07:21:19.230227 140193614219072 submission.py:296] 97) loss = 8.666, grad_norm = 0.188
I0402 07:21:19.680064 140135898105600 logging_writer.py:48] [98] global_step=98, grad_norm=0.184320, loss=8.715057
I0402 07:21:19.683763 140193614219072 submission.py:296] 98) loss = 8.715, grad_norm = 0.184
I0402 07:21:20.128432 140135889712896 logging_writer.py:48] [99] global_step=99, grad_norm=0.177753, loss=8.725427
I0402 07:21:20.131991 140193614219072 submission.py:296] 99) loss = 8.725, grad_norm = 0.178
I0402 07:21:20.586950 140135898105600 logging_writer.py:48] [100] global_step=100, grad_norm=0.172286, loss=8.663142
I0402 07:21:20.590607 140193614219072 submission.py:296] 100) loss = 8.663, grad_norm = 0.172
I0402 07:24:16.458488 140135889712896 logging_writer.py:48] [500] global_step=500, grad_norm=0.832254, loss=6.899701
I0402 07:24:16.461920 140193614219072 submission.py:296] 500) loss = 6.900, grad_norm = 0.832
I0402 07:27:56.750886 140135898105600 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.753858, loss=5.721015
I0402 07:27:56.754907 140193614219072 submission.py:296] 1000) loss = 5.721, grad_norm = 0.754
I0402 07:31:36.703709 140135889712896 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.838374, loss=4.882226
I0402 07:31:36.707083 140193614219072 submission.py:296] 1500) loss = 4.882, grad_norm = 0.838
I0402 07:34:36.011411 140193614219072 submission_runner.py:373] Before eval at step 1908: RAM USED (GB) 20.798439424
I0402 07:34:36.011625 140193614219072 spec.py:298] Evaluating on the training split.
I0402 07:34:39.883737 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:37:21.891234 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 07:37:25.622449 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:39:46.900709 140193614219072 spec.py:326] Evaluating on the test split.
I0402 07:39:50.697926 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:42:04.395872 140193614219072 submission_runner.py:382] Time since start: 1667.88s, 	Step: 1908, 	{'train/accuracy': 0.46130979498861047, 'train/loss': 3.488741813781321, 'train/bleu': 17.344515493738466, 'validation/accuracy': 0.45463788421718265, 'validation/loss': 3.561776202402946, 'validation/bleu': 13.363891523963982, 'validation/num_examples': 3000, 'test/accuracy': 0.4464586601592005, 'test/loss': 3.697563331590262, 'test/bleu': 11.632306743200662, 'test/num_examples': 3003}
I0402 07:42:04.396309 140193614219072 submission_runner.py:396] After eval at step 1908: RAM USED (GB) 21.0651136
I0402 07:42:04.404603 140135898105600 logging_writer.py:48] [1908] global_step=1908, preemption_count=0, score=840.989740, test/accuracy=0.446459, test/bleu=11.632307, test/loss=3.697563, test/num_examples=3003, total_duration=1667.884962, train/accuracy=0.461310, train/bleu=17.344515, train/loss=3.488742, validation/accuracy=0.454638, validation/bleu=13.363892, validation/loss=3.561776, validation/num_examples=3000
I0402 07:42:06.631768 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_1908.
I0402 07:42:06.632428 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 1908: RAM USED (GB) 21.064802304
I0402 07:42:47.596196 140135889712896 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.662491, loss=4.384846
I0402 07:42:47.599480 140193614219072 submission.py:296] 2000) loss = 4.385, grad_norm = 0.662
I0402 07:46:27.779065 140135898105600 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.579196, loss=4.101249
I0402 07:46:27.783362 140193614219072 submission.py:296] 2500) loss = 4.101, grad_norm = 0.579
I0402 07:50:07.782062 140135889712896 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.458744, loss=3.871043
I0402 07:50:07.786010 140193614219072 submission.py:296] 3000) loss = 3.871, grad_norm = 0.459
I0402 07:53:47.827966 140135898105600 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.421911, loss=3.720019
I0402 07:53:47.831856 140193614219072 submission.py:296] 3500) loss = 3.720, grad_norm = 0.422
I0402 07:56:06.641059 140193614219072 submission_runner.py:373] Before eval at step 3816: RAM USED (GB) 21.163851776
I0402 07:56:06.641331 140193614219072 spec.py:298] Evaluating on the training split.
I0402 07:56:10.493685 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 07:58:29.783031 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 07:58:33.507804 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:00:45.087429 140193614219072 spec.py:326] Evaluating on the test split.
I0402 08:00:48.865035 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:02:57.773448 140193614219072 submission_runner.py:382] Time since start: 2958.51s, 	Step: 3816, 	{'train/accuracy': 0.5501937785094488, 'train/loss': 2.6208575556470146, 'train/bleu': 24.77224775731282, 'validation/accuracy': 0.5551450075014569, 'validation/loss': 2.5680673906709153, 'validation/bleu': 21.072200499822156, 'validation/num_examples': 3000, 'test/accuracy': 0.5547498692696531, 'test/loss': 2.58918968973331, 'test/bleu': 19.19976853385424, 'test/num_examples': 3003}
I0402 08:02:57.773853 140193614219072 submission_runner.py:396] After eval at step 3816: RAM USED (GB) 21.209153536
I0402 08:02:57.782615 140135889712896 logging_writer.py:48] [3816] global_step=3816, preemption_count=0, score=1677.703082, test/accuracy=0.554750, test/bleu=19.199769, test/loss=2.589190, test/num_examples=3003, total_duration=2958.514372, train/accuracy=0.550194, train/bleu=24.772248, train/loss=2.620858, validation/accuracy=0.555145, validation/bleu=21.072200, validation/loss=2.568067, validation/num_examples=3000
I0402 08:02:59.967789 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_3816.
I0402 08:02:59.968454 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 3816: RAM USED (GB) 21.208240128
I0402 08:04:21.321367 140135898105600 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.377406, loss=3.673245
I0402 08:04:21.324743 140193614219072 submission.py:296] 4000) loss = 3.673, grad_norm = 0.377
I0402 08:08:01.585568 140135889712896 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.381623, loss=3.570796
I0402 08:08:01.589284 140193614219072 submission.py:296] 4500) loss = 3.571, grad_norm = 0.382
I0402 08:11:41.665447 140135898105600 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.293291, loss=3.482000
I0402 08:11:41.668808 140193614219072 submission.py:296] 5000) loss = 3.482, grad_norm = 0.293
I0402 08:15:21.804772 140135889712896 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.277882, loss=3.454671
I0402 08:15:21.807861 140193614219072 submission.py:296] 5500) loss = 3.455, grad_norm = 0.278
I0402 08:17:00.264613 140193614219072 submission_runner.py:373] Before eval at step 5725: RAM USED (GB) 21.282054144
I0402 08:17:00.264814 140193614219072 spec.py:298] Evaluating on the training split.
I0402 08:17:04.125395 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:19:23.661659 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 08:19:27.381715 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:21:50.969515 140193614219072 spec.py:326] Evaluating on the test split.
I0402 08:21:54.768498 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:24:23.199327 140193614219072 submission_runner.py:382] Time since start: 4212.14s, 	Step: 5725, 	{'train/accuracy': 0.5737643619552655, 'train/loss': 2.390111036043692, 'train/bleu': 27.043347497511395, 'validation/accuracy': 0.5880646241212136, 'validation/loss': 2.2797890370237193, 'validation/bleu': 23.391484088507543, 'validation/num_examples': 3000, 'test/accuracy': 0.5901458369647319, 'test/loss': 2.261974536633548, 'test/bleu': 22.08815847187788, 'test/num_examples': 3003}
I0402 08:24:23.199770 140193614219072 submission_runner.py:396] After eval at step 5725: RAM USED (GB) 21.368307712
I0402 08:24:23.208534 140135898105600 logging_writer.py:48] [5725] global_step=5725, preemption_count=0, score=2514.601297, test/accuracy=0.590146, test/bleu=22.088158, test/loss=2.261975, test/num_examples=3003, total_duration=4212.136315, train/accuracy=0.573764, train/bleu=27.043347, train/loss=2.390111, validation/accuracy=0.588065, validation/bleu=23.391484, validation/loss=2.279789, validation/num_examples=3000
I0402 08:24:25.642654 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_5725.
I0402 08:24:25.643316 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 5725: RAM USED (GB) 21.3677056
I0402 08:26:26.678390 140135889712896 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.258073, loss=3.384700
I0402 08:26:26.682287 140193614219072 submission.py:296] 6000) loss = 3.385, grad_norm = 0.258
I0402 08:30:06.483465 140135898105600 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.218150, loss=3.376246
I0402 08:30:06.486622 140193614219072 submission.py:296] 6500) loss = 3.376, grad_norm = 0.218
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0402 08:33:46.402786 140135889712896 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.209888, loss=3.492070
I0402 08:33:46.406214 140193614219072 submission.py:296] 7000) loss = 3.492, grad_norm = 0.210
I0402 08:37:26.439563 140135898105600 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.185073, loss=3.226861
I0402 08:37:26.443039 140193614219072 submission.py:296] 7500) loss = 3.227, grad_norm = 0.185
I0402 08:38:25.877944 140193614219072 submission_runner.py:373] Before eval at step 7636: RAM USED (GB) 21.725933568
I0402 08:38:25.878145 140193614219072 spec.py:298] Evaluating on the training split.
I0402 08:38:29.740289 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:41:00.074124 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 08:41:03.773956 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:43:19.869044 140193614219072 spec.py:326] Evaluating on the test split.
I0402 08:43:23.647581 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 08:45:29.311465 140193614219072 submission_runner.py:382] Time since start: 5497.75s, 	Step: 7636, 	{'train/accuracy': 0.6013047914970705, 'train/loss': 2.1622721559443687, 'train/bleu': 28.50287940313989, 'validation/accuracy': 0.6122304745136452, 'validation/loss': 2.069508011369977, 'validation/bleu': 25.109565998345797, 'validation/num_examples': 3000, 'test/accuracy': 0.6164894544186857, 'test/loss': 2.048385843355993, 'test/bleu': 23.98738730134721, 'test/num_examples': 3003}
I0402 08:45:29.311839 140193614219072 submission_runner.py:396] After eval at step 7636: RAM USED (GB) 21.786136576
I0402 08:45:29.320561 140135889712896 logging_writer.py:48] [7636] global_step=7636, preemption_count=0, score=3351.413808, test/accuracy=0.616489, test/bleu=23.987387, test/loss=2.048386, test/num_examples=3003, total_duration=5497.751392, train/accuracy=0.601305, train/bleu=28.502879, train/loss=2.162272, validation/accuracy=0.612230, validation/bleu=25.109566, validation/loss=2.069508, validation/num_examples=3000
I0402 08:45:31.760293 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_7636.
I0402 08:45:31.760968 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 7636: RAM USED (GB) 21.78504704
I0402 08:48:12.214574 140135898105600 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.155234, loss=3.278992
I0402 08:48:12.217887 140193614219072 submission.py:296] 8000) loss = 3.279, grad_norm = 0.155
I0402 08:51:52.101419 140135889712896 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.165479, loss=3.333936
I0402 08:51:52.104758 140193614219072 submission.py:296] 8500) loss = 3.334, grad_norm = 0.165
I0402 08:55:31.943209 140135898105600 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.164548, loss=3.245467
I0402 08:55:31.946818 140193614219072 submission.py:296] 9000) loss = 3.245, grad_norm = 0.165
I0402 08:59:11.736816 140135889712896 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.160948, loss=3.208287
I0402 08:59:11.739989 140193614219072 submission.py:296] 9500) loss = 3.208, grad_norm = 0.161
I0402 08:59:31.998035 140193614219072 submission_runner.py:373] Before eval at step 9547: RAM USED (GB) 21.935517696
I0402 08:59:31.998237 140193614219072 spec.py:298] Evaluating on the training split.
I0402 08:59:35.850927 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 09:01:57.955218 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 09:02:01.676049 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 09:04:06.345439 140193614219072 spec.py:326] Evaluating on the test split.
I0402 09:04:10.126312 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 09:06:03.942639 140193614219072 submission_runner.py:382] Time since start: 6763.87s, 	Step: 9547, 	{'train/accuracy': 0.6109869197876902, 'train/loss': 2.0735643721842005, 'train/bleu': 29.10630832616709, 'validation/accuracy': 0.6262414601182874, 'validation/loss': 1.9515450831359809, 'validation/bleu': 25.80665562062824, 'validation/num_examples': 3000, 'test/accuracy': 0.634582534425658, 'test/loss': 1.9075351519377142, 'test/bleu': 24.95212957598967, 'test/num_examples': 3003}
I0402 09:06:03.943032 140193614219072 submission_runner.py:396] After eval at step 9547: RAM USED (GB) 22.0013568
I0402 09:06:03.951968 140135898105600 logging_writer.py:48] [9547] global_step=9547, preemption_count=0, score=4188.305603, test/accuracy=0.634583, test/bleu=24.952130, test/loss=1.907535, test/num_examples=3003, total_duration=6763.871636, train/accuracy=0.610987, train/bleu=29.106308, train/loss=2.073564, validation/accuracy=0.626241, validation/bleu=25.806656, validation/loss=1.951545, validation/num_examples=3000
I0402 09:06:06.414909 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_9547.
I0402 09:06:06.415560 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 9547: RAM USED (GB) 21.999837184
I0402 09:09:25.459744 140193614219072 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.01647104
I0402 09:09:25.460020 140193614219072 spec.py:298] Evaluating on the training split.
I0402 09:09:29.324078 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 09:11:47.407062 140193614219072 spec.py:310] Evaluating on the validation split.
I0402 09:11:51.122524 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 09:13:53.817366 140193614219072 spec.py:326] Evaluating on the test split.
I0402 09:13:57.609823 140193614219072 workload.py:130] Translating evaluation dataset.
I0402 09:15:51.697453 140193614219072 submission_runner.py:382] Time since start: 7357.33s, 	Step: 10000, 	{'train/accuracy': 0.6107791352188536, 'train/loss': 2.0745231899929495, 'train/bleu': 29.011791137986435, 'validation/accuracy': 0.6274813703487867, 'validation/loss': 1.9379339685806747, 'validation/bleu': 25.96851084368893, 'validation/num_examples': 3000, 'test/accuracy': 0.6340015106617861, 'test/loss': 1.8974986926965314, 'test/bleu': 25.015875643745748, 'test/num_examples': 3003}
I0402 09:15:51.697832 140193614219072 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 22.034735104
I0402 09:15:51.707521 140135889712896 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4386.557447, test/accuracy=0.634002, test/bleu=25.015876, test/loss=1.897499, test/num_examples=3003, total_duration=7357.331806, train/accuracy=0.610779, train/bleu=29.011791, train/loss=2.074523, validation/accuracy=0.627481, validation/bleu=25.968511, validation/loss=1.937934, validation/num_examples=3000
I0402 09:15:54.134316 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_10000.
I0402 09:15:54.134949 140193614219072 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.033154048
I0402 09:15:54.143625 140135898105600 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4386.557447
I0402 09:15:58.301930 140193614219072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/wmt_pytorch/trial_1/checkpoint_10000.
I0402 09:15:58.324915 140193614219072 submission_runner.py:550] Tuning trial 1/1
I0402 09:15:58.325089 140193614219072 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0402 09:15:58.326095 140193614219072 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006631830499559784, 'train/loss': 11.138438032404496, 'train/bleu': 3.976327322716544e-10, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.113079038077643, 'validation/bleu': 1.8014638110548825e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.130345418627622, 'test/bleu': 6.55137722313088e-10, 'test/num_examples': 3003, 'score': 3.774653911590576, 'total_duration': 3.7766542434692383, 'global_step': 1, 'preemption_count': 0}), (1908, {'train/accuracy': 0.46130979498861047, 'train/loss': 3.488741813781321, 'train/bleu': 17.344515493738466, 'validation/accuracy': 0.45463788421718265, 'validation/loss': 3.561776202402946, 'validation/bleu': 13.363891523963982, 'validation/num_examples': 3000, 'test/accuracy': 0.4464586601592005, 'test/loss': 3.697563331590262, 'test/bleu': 11.632306743200662, 'test/num_examples': 3003, 'score': 840.9897401332855, 'total_duration': 1667.884961605072, 'global_step': 1908, 'preemption_count': 0}), (3816, {'train/accuracy': 0.5501937785094488, 'train/loss': 2.6208575556470146, 'train/bleu': 24.77224775731282, 'validation/accuracy': 0.5551450075014569, 'validation/loss': 2.5680673906709153, 'validation/bleu': 21.072200499822156, 'validation/num_examples': 3000, 'test/accuracy': 0.5547498692696531, 'test/loss': 2.58918968973331, 'test/bleu': 19.19976853385424, 'test/num_examples': 3003, 'score': 1677.7030820846558, 'total_duration': 2958.5143723487854, 'global_step': 3816, 'preemption_count': 0}), (5725, {'train/accuracy': 0.5737643619552655, 'train/loss': 2.390111036043692, 'train/bleu': 27.043347497511395, 'validation/accuracy': 0.5880646241212136, 'validation/loss': 2.2797890370237193, 'validation/bleu': 23.391484088507543, 'validation/num_examples': 3000, 'test/accuracy': 0.5901458369647319, 'test/loss': 2.261974536633548, 'test/bleu': 22.08815847187788, 'test/num_examples': 3003, 'score': 2514.60129737854, 'total_duration': 4212.136314630508, 'global_step': 5725, 'preemption_count': 0}), (7636, {'train/accuracy': 0.6013047914970705, 'train/loss': 2.1622721559443687, 'train/bleu': 28.50287940313989, 'validation/accuracy': 0.6122304745136452, 'validation/loss': 2.069508011369977, 'validation/bleu': 25.109565998345797, 'validation/num_examples': 3000, 'test/accuracy': 0.6164894544186857, 'test/loss': 2.048385843355993, 'test/bleu': 23.98738730134721, 'test/num_examples': 3003, 'score': 3351.4138083457947, 'total_duration': 5497.751391887665, 'global_step': 7636, 'preemption_count': 0}), (9547, {'train/accuracy': 0.6109869197876902, 'train/loss': 2.0735643721842005, 'train/bleu': 29.10630832616709, 'validation/accuracy': 0.6262414601182874, 'validation/loss': 1.9515450831359809, 'validation/bleu': 25.80665562062824, 'validation/num_examples': 3000, 'test/accuracy': 0.634582534425658, 'test/loss': 1.9075351519377142, 'test/bleu': 24.95212957598967, 'test/num_examples': 3003, 'score': 4188.305602550507, 'total_duration': 6763.871636390686, 'global_step': 9547, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6107791352188536, 'train/loss': 2.0745231899929495, 'train/bleu': 29.011791137986435, 'validation/accuracy': 0.6274813703487867, 'validation/loss': 1.9379339685806747, 'validation/bleu': 25.96851084368893, 'validation/num_examples': 3000, 'test/accuracy': 0.6340015106617861, 'test/loss': 1.8974986926965314, 'test/bleu': 25.015875643745748, 'test/num_examples': 3003, 'score': 4386.5574469566345, 'total_duration': 7357.331805944443, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0402 09:15:58.326216 140193614219072 submission_runner.py:553] Timing: 4386.5574469566345
I0402 09:15:58.326291 140193614219072 submission_runner.py:554] ====================
I0402 09:15:58.326411 140193614219072 submission_runner.py:613] Final wmt score: 4386.5574469566345
