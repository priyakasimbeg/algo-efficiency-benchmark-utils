WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 09:23:27.413356 139756690147136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 09:23:27.413399 139763906225984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 09:23:27.413483 139687949977408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 09:23:27.414354 140627406800704 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 09:23:27.414404 139971922630464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 09:23:27.414425 140675140405056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 09:23:27.414433 140350361093952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 09:23:27.414685 140627406800704 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.414708 140675140405056 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.414732 139971922630464 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.414616 140227183036224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 09:23:27.414817 140350361093952 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.414927 140227183036224 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.423958 139756690147136 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.424152 139763906225984 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.424258 139687949977408 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:23:27.430798 140675140405056 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/criteo1tb_pytorch.
W0401 09:23:27.467110 139756690147136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.467663 140350361093952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.468120 140227183036224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.468192 139687949977408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.469279 140627406800704 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.469850 139971922630464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.471484 139763906225984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:23:27.471980 140675140405056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 09:23:27.475405 140675140405056 submission_runner.py:504] Using RNG seed 481388399
I0401 09:23:27.476761 140675140405056 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 09:23:27.476877 140675140405056 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1.
I0401 09:23:27.477072 140675140405056 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/hparams.json.
I0401 09:23:27.478050 140675140405056 submission_runner.py:230] Starting train once: RAM USED (GB) 5.57703168
I0401 09:23:27.478145 140675140405056 submission_runner.py:231] Initializing dataset.
I0401 09:23:27.478327 140675140405056 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.57703168
I0401 09:23:27.478405 140675140405056 submission_runner.py:240] Initializing model.
I0401 09:23:42.348129 140675140405056 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.157915648
I0401 09:23:42.348301 140675140405056 submission_runner.py:252] Initializing optimizer.
I0401 09:23:42.348935 140675140405056 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.157915648
I0401 09:23:42.349043 140675140405056 submission_runner.py:261] Initializing metrics bundle.
I0401 09:23:42.349113 140675140405056 submission_runner.py:275] Initializing checkpoint and logger.
I0401 09:23:42.352382 140675140405056 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 09:23:42.352488 140675140405056 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 09:23:42.942686 140675140405056 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0401 09:23:42.943587 140675140405056 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/flags_0.json.
I0401 09:23:42.986575 140675140405056 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 15.206408192
I0401 09:23:42.987611 140675140405056 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.206408192
I0401 09:23:42.987738 140675140405056 submission_runner.py:312] Starting training loop.
I0401 09:26:15.771077 140675140405056 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 53.988990976
I0401 09:26:19.155310 140592492308224 logging_writer.py:48] [0] global_step=0, grad_norm=7.306894, loss=0.564142
I0401 09:26:19.161548 140675140405056 submission.py:119] 0) loss = 0.564, grad_norm = 7.307
I0401 09:26:19.162008 140675140405056 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 58.326523904
I0401 09:26:19.162582 140675140405056 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 58.32704
I0401 09:26:19.162688 140675140405056 spec.py:298] Evaluating on the training split.
I0401 09:36:02.719760 140675140405056 spec.py:310] Evaluating on the validation split.
I0401 09:41:24.815736 140675140405056 spec.py:326] Evaluating on the test split.
I0401 09:45:49.119723 140675140405056 submission_runner.py:380] Time since start: 156.18s, 	Step: 1, 	{'train/loss': 0.5640724524581494, 'validation/loss': 0.5652694831460674, 'validation/num_examples': 89000000, 'test/loss': 0.5644447481763494, 'test/num_examples': 89274637}
I0401 09:45:49.120209 140675140405056 submission_runner.py:390] After eval at step 1: RAM USED (GB) 106.041368576
I0401 09:45:49.135402 140536103012096 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=156.173466, test/loss=0.564445, test/num_examples=89274637, total_duration=156.175391, train/loss=0.564072, validation/loss=0.565269, validation/num_examples=89000000
I0401 09:46:03.362718 140675140405056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_1.
I0401 09:46:03.363186 140675140405056 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 106.082922496
I0401 09:46:03.380459 140675140405056 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 106.042085376
I0401 09:46:03.383717 140675140405056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383716 139971922630464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383720 140627406800704 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383720 139763906225984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383720 139756690147136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383736 140227183036224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383723 140350361093952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:03.383733 139687949977408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:46:05.132449 140536094619392 logging_writer.py:48] [1] global_step=1, grad_norm=7.299121, loss=0.564004
I0401 09:46:05.144189 140675140405056 submission.py:119] 1) loss = 0.564, grad_norm = 7.299
I0401 09:46:05.144609 140675140405056 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 106.118152192
I0401 09:46:06.880957 140536103012096 logging_writer.py:48] [2] global_step=2, grad_norm=7.201021, loss=0.553421
I0401 09:46:06.887654 140675140405056 submission.py:119] 2) loss = 0.553, grad_norm = 7.201
I0401 09:46:08.702037 140536094619392 logging_writer.py:48] [3] global_step=3, grad_norm=6.969150, loss=0.533118
I0401 09:46:08.711131 140675140405056 submission.py:119] 3) loss = 0.533, grad_norm = 6.969
I0401 09:46:10.466659 140536103012096 logging_writer.py:48] [4] global_step=4, grad_norm=6.590170, loss=0.504240
I0401 09:46:10.477098 140675140405056 submission.py:119] 4) loss = 0.504, grad_norm = 6.590
I0401 09:46:12.222621 140536094619392 logging_writer.py:48] [5] global_step=5, grad_norm=6.110482, loss=0.468062
I0401 09:46:12.238916 140675140405056 submission.py:119] 5) loss = 0.468, grad_norm = 6.110
I0401 09:46:13.980248 140536103012096 logging_writer.py:48] [6] global_step=6, grad_norm=5.515859, loss=0.427161
I0401 09:46:13.987185 140675140405056 submission.py:119] 6) loss = 0.427, grad_norm = 5.516
I0401 09:46:15.799368 140536094619392 logging_writer.py:48] [7] global_step=7, grad_norm=4.844301, loss=0.383488
I0401 09:46:15.812565 140675140405056 submission.py:119] 7) loss = 0.383, grad_norm = 4.844
I0401 09:46:17.566123 140536103012096 logging_writer.py:48] [8] global_step=8, grad_norm=4.126771, loss=0.341370
I0401 09:46:17.595116 140675140405056 submission.py:119] 8) loss = 0.341, grad_norm = 4.127
I0401 09:46:19.341570 140536094619392 logging_writer.py:48] [9] global_step=9, grad_norm=3.445601, loss=0.300457
I0401 09:46:19.355176 140675140405056 submission.py:119] 9) loss = 0.300, grad_norm = 3.446
I0401 09:46:21.201162 140536103012096 logging_writer.py:48] [10] global_step=10, grad_norm=2.862216, loss=0.264050
I0401 09:46:21.227253 140675140405056 submission.py:119] 10) loss = 0.264, grad_norm = 2.862
I0401 09:46:23.087842 140536094619392 logging_writer.py:48] [11] global_step=11, grad_norm=2.288421, loss=0.232334
I0401 09:46:23.109621 140675140405056 submission.py:119] 11) loss = 0.232, grad_norm = 2.288
I0401 09:46:24.972868 140536103012096 logging_writer.py:48] [12] global_step=12, grad_norm=1.748549, loss=0.206659
I0401 09:46:24.981984 140675140405056 submission.py:119] 12) loss = 0.207, grad_norm = 1.749
I0401 09:46:26.764607 140536094619392 logging_writer.py:48] [13] global_step=13, grad_norm=1.262829, loss=0.186324
I0401 09:46:26.807716 140675140405056 submission.py:119] 13) loss = 0.186, grad_norm = 1.263
I0401 09:46:28.573561 140536103012096 logging_writer.py:48] [14] global_step=14, grad_norm=0.818396, loss=0.173303
I0401 09:46:28.591476 140675140405056 submission.py:119] 14) loss = 0.173, grad_norm = 0.818
I0401 09:46:30.337767 140536094619392 logging_writer.py:48] [15] global_step=15, grad_norm=0.460905, loss=0.164507
I0401 09:46:30.342131 140675140405056 submission.py:119] 15) loss = 0.165, grad_norm = 0.461
I0401 09:46:32.127480 140536103012096 logging_writer.py:48] [16] global_step=16, grad_norm=0.189061, loss=0.162036
I0401 09:46:32.137634 140675140405056 submission.py:119] 16) loss = 0.162, grad_norm = 0.189
I0401 09:46:33.872220 140536094619392 logging_writer.py:48] [17] global_step=17, grad_norm=0.193238, loss=0.162201
I0401 09:46:33.892471 140675140405056 submission.py:119] 17) loss = 0.162, grad_norm = 0.193
I0401 09:46:35.635017 140536103012096 logging_writer.py:48] [18] global_step=18, grad_norm=0.312239, loss=0.161182
I0401 09:46:35.648362 140675140405056 submission.py:119] 18) loss = 0.161, grad_norm = 0.312
I0401 09:46:37.405267 140536094619392 logging_writer.py:48] [19] global_step=19, grad_norm=0.401847, loss=0.160509
I0401 09:46:37.426910 140675140405056 submission.py:119] 19) loss = 0.161, grad_norm = 0.402
I0401 09:46:39.279271 140536103012096 logging_writer.py:48] [20] global_step=20, grad_norm=0.475299, loss=0.163009
I0401 09:46:39.300435 140675140405056 submission.py:119] 20) loss = 0.163, grad_norm = 0.475
I0401 09:46:41.136714 140536094619392 logging_writer.py:48] [21] global_step=21, grad_norm=0.496859, loss=0.163526
I0401 09:46:41.166260 140675140405056 submission.py:119] 21) loss = 0.164, grad_norm = 0.497
I0401 09:46:42.900652 140536103012096 logging_writer.py:48] [22] global_step=22, grad_norm=0.455338, loss=0.159036
I0401 09:46:42.911578 140675140405056 submission.py:119] 22) loss = 0.159, grad_norm = 0.455
I0401 09:46:44.713009 140536094619392 logging_writer.py:48] [23] global_step=23, grad_norm=0.423185, loss=0.158536
I0401 09:46:44.719860 140675140405056 submission.py:119] 23) loss = 0.159, grad_norm = 0.423
I0401 09:46:46.523583 140536103012096 logging_writer.py:48] [24] global_step=24, grad_norm=0.333154, loss=0.153897
I0401 09:46:46.530802 140675140405056 submission.py:119] 24) loss = 0.154, grad_norm = 0.333
I0401 09:46:48.310837 140536094619392 logging_writer.py:48] [25] global_step=25, grad_norm=0.272023, loss=0.154608
I0401 09:46:48.326685 140675140405056 submission.py:119] 25) loss = 0.155, grad_norm = 0.272
I0401 09:46:50.072080 140536103012096 logging_writer.py:48] [26] global_step=26, grad_norm=0.211887, loss=0.156122
I0401 09:46:50.080415 140675140405056 submission.py:119] 26) loss = 0.156, grad_norm = 0.212
I0401 09:46:51.833754 140536094619392 logging_writer.py:48] [27] global_step=27, grad_norm=0.122911, loss=0.151957
I0401 09:46:51.844990 140675140405056 submission.py:119] 27) loss = 0.152, grad_norm = 0.123
I0401 09:46:53.684730 140536103012096 logging_writer.py:48] [28] global_step=28, grad_norm=0.099108, loss=0.150997
I0401 09:46:53.713675 140675140405056 submission.py:119] 28) loss = 0.151, grad_norm = 0.099
I0401 09:46:55.466530 140536094619392 logging_writer.py:48] [29] global_step=29, grad_norm=0.089111, loss=0.149494
I0401 09:46:55.483573 140675140405056 submission.py:119] 29) loss = 0.149, grad_norm = 0.089
I0401 09:46:57.232520 140536103012096 logging_writer.py:48] [30] global_step=30, grad_norm=0.081376, loss=0.148902
I0401 09:46:57.235707 140675140405056 submission.py:119] 30) loss = 0.149, grad_norm = 0.081
I0401 09:46:58.958647 140536094619392 logging_writer.py:48] [31] global_step=31, grad_norm=0.097033, loss=0.147889
I0401 09:46:58.962119 140675140405056 submission.py:119] 31) loss = 0.148, grad_norm = 0.097
I0401 09:47:00.745370 140536103012096 logging_writer.py:48] [32] global_step=32, grad_norm=0.115500, loss=0.148735
I0401 09:47:00.748794 140675140405056 submission.py:119] 32) loss = 0.149, grad_norm = 0.116
I0401 09:47:02.505363 140536094619392 logging_writer.py:48] [33] global_step=33, grad_norm=0.072418, loss=0.147229
I0401 09:47:02.509490 140675140405056 submission.py:119] 33) loss = 0.147, grad_norm = 0.072
I0401 09:47:04.239018 140536103012096 logging_writer.py:48] [34] global_step=34, grad_norm=0.077513, loss=0.144177
I0401 09:47:04.245214 140675140405056 submission.py:119] 34) loss = 0.144, grad_norm = 0.078
I0401 09:47:06.075963 140536094619392 logging_writer.py:48] [35] global_step=35, grad_norm=0.058097, loss=0.143945
I0401 09:47:06.079887 140675140405056 submission.py:119] 35) loss = 0.144, grad_norm = 0.058
I0401 09:47:07.823981 140536103012096 logging_writer.py:48] [36] global_step=36, grad_norm=0.090348, loss=0.144387
I0401 09:47:07.827858 140675140405056 submission.py:119] 36) loss = 0.144, grad_norm = 0.090
I0401 09:47:09.565019 140536094619392 logging_writer.py:48] [37] global_step=37, grad_norm=0.051803, loss=0.141642
I0401 09:47:09.568604 140675140405056 submission.py:119] 37) loss = 0.142, grad_norm = 0.052
I0401 09:47:11.323650 140536103012096 logging_writer.py:48] [38] global_step=38, grad_norm=0.043429, loss=0.142372
I0401 09:47:11.328095 140675140405056 submission.py:119] 38) loss = 0.142, grad_norm = 0.043
I0401 09:47:13.205859 140536094619392 logging_writer.py:48] [39] global_step=39, grad_norm=0.043201, loss=0.141681
I0401 09:47:13.209445 140675140405056 submission.py:119] 39) loss = 0.142, grad_norm = 0.043
I0401 09:47:14.941625 140536103012096 logging_writer.py:48] [40] global_step=40, grad_norm=0.034337, loss=0.140059
I0401 09:47:14.944920 140675140405056 submission.py:119] 40) loss = 0.140, grad_norm = 0.034
I0401 09:47:16.695444 140536094619392 logging_writer.py:48] [41] global_step=41, grad_norm=0.051459, loss=0.141255
I0401 09:47:16.698895 140675140405056 submission.py:119] 41) loss = 0.141, grad_norm = 0.051
I0401 09:47:18.455106 140536103012096 logging_writer.py:48] [42] global_step=42, grad_norm=0.065665, loss=0.138623
I0401 09:47:18.458421 140675140405056 submission.py:119] 42) loss = 0.139, grad_norm = 0.066
I0401 09:47:20.275450 140536094619392 logging_writer.py:48] [43] global_step=43, grad_norm=0.109994, loss=0.139253
I0401 09:47:20.278965 140675140405056 submission.py:119] 43) loss = 0.139, grad_norm = 0.110
I0401 09:47:22.015659 140536103012096 logging_writer.py:48] [44] global_step=44, grad_norm=0.138071, loss=0.137272
I0401 09:47:22.019433 140675140405056 submission.py:119] 44) loss = 0.137, grad_norm = 0.138
I0401 09:47:23.832886 140536094619392 logging_writer.py:48] [45] global_step=45, grad_norm=0.160436, loss=0.139700
I0401 09:47:23.836581 140675140405056 submission.py:119] 45) loss = 0.140, grad_norm = 0.160
I0401 09:47:25.618490 140536103012096 logging_writer.py:48] [46] global_step=46, grad_norm=0.110558, loss=0.138507
I0401 09:47:25.622175 140675140405056 submission.py:119] 46) loss = 0.139, grad_norm = 0.111
I0401 09:47:27.527102 140536094619392 logging_writer.py:48] [47] global_step=47, grad_norm=0.020197, loss=0.137878
I0401 09:47:27.530737 140675140405056 submission.py:119] 47) loss = 0.138, grad_norm = 0.020
I0401 09:47:29.363262 140536103012096 logging_writer.py:48] [48] global_step=48, grad_norm=0.091587, loss=0.137656
I0401 09:47:29.366854 140675140405056 submission.py:119] 48) loss = 0.138, grad_norm = 0.092
I0401 09:47:31.234880 140536094619392 logging_writer.py:48] [49] global_step=49, grad_norm=0.128266, loss=0.137295
I0401 09:47:31.238183 140675140405056 submission.py:119] 49) loss = 0.137, grad_norm = 0.128
I0401 09:47:32.975462 140536103012096 logging_writer.py:48] [50] global_step=50, grad_norm=0.125882, loss=0.138449
I0401 09:47:32.979274 140675140405056 submission.py:119] 50) loss = 0.138, grad_norm = 0.126
I0401 09:47:34.780061 140536094619392 logging_writer.py:48] [51] global_step=51, grad_norm=0.078275, loss=0.137109
I0401 09:47:34.783610 140675140405056 submission.py:119] 51) loss = 0.137, grad_norm = 0.078
I0401 09:47:36.679764 140536103012096 logging_writer.py:48] [52] global_step=52, grad_norm=0.019172, loss=0.136521
I0401 09:47:36.683410 140675140405056 submission.py:119] 52) loss = 0.137, grad_norm = 0.019
I0401 09:47:38.494332 140536094619392 logging_writer.py:48] [53] global_step=53, grad_norm=0.076167, loss=0.136825
I0401 09:47:38.499740 140675140405056 submission.py:119] 53) loss = 0.137, grad_norm = 0.076
I0401 09:47:40.253005 140536103012096 logging_writer.py:48] [54] global_step=54, grad_norm=0.145722, loss=0.136815
I0401 09:47:40.256640 140675140405056 submission.py:119] 54) loss = 0.137, grad_norm = 0.146
I0401 09:47:42.061294 140536094619392 logging_writer.py:48] [55] global_step=55, grad_norm=0.213799, loss=0.136895
I0401 09:47:42.064915 140675140405056 submission.py:119] 55) loss = 0.137, grad_norm = 0.214
I0401 09:47:43.877150 140536103012096 logging_writer.py:48] [56] global_step=56, grad_norm=0.226510, loss=0.138750
I0401 09:47:43.880257 140675140405056 submission.py:119] 56) loss = 0.139, grad_norm = 0.227
I0401 09:47:45.625256 140536094619392 logging_writer.py:48] [57] global_step=57, grad_norm=0.149428, loss=0.137086
I0401 09:47:45.628294 140675140405056 submission.py:119] 57) loss = 0.137, grad_norm = 0.149
I0401 09:47:47.406140 140536103012096 logging_writer.py:48] [58] global_step=58, grad_norm=0.029118, loss=0.137135
I0401 09:47:47.409596 140675140405056 submission.py:119] 58) loss = 0.137, grad_norm = 0.029
I0401 09:47:49.169889 140536094619392 logging_writer.py:48] [59] global_step=59, grad_norm=0.105784, loss=0.134583
I0401 09:47:49.173146 140675140405056 submission.py:119] 59) loss = 0.135, grad_norm = 0.106
I0401 09:47:50.945077 140536103012096 logging_writer.py:48] [60] global_step=60, grad_norm=0.220909, loss=0.134863
I0401 09:47:50.948234 140675140405056 submission.py:119] 60) loss = 0.135, grad_norm = 0.221
I0401 09:47:52.688184 140536094619392 logging_writer.py:48] [61] global_step=61, grad_norm=0.275541, loss=0.135762
I0401 09:47:52.691525 140675140405056 submission.py:119] 61) loss = 0.136, grad_norm = 0.276
I0401 09:47:54.436762 140536103012096 logging_writer.py:48] [62] global_step=62, grad_norm=0.246616, loss=0.134911
I0401 09:47:54.439956 140675140405056 submission.py:119] 62) loss = 0.135, grad_norm = 0.247
I0401 09:47:56.252417 140536094619392 logging_writer.py:48] [63] global_step=63, grad_norm=0.108133, loss=0.135869
I0401 09:47:56.255715 140675140405056 submission.py:119] 63) loss = 0.136, grad_norm = 0.108
I0401 09:47:58.018730 140536103012096 logging_writer.py:48] [64] global_step=64, grad_norm=0.076751, loss=0.132605
I0401 09:47:58.022096 140675140405056 submission.py:119] 64) loss = 0.133, grad_norm = 0.077
I0401 09:47:59.839596 140536094619392 logging_writer.py:48] [65] global_step=65, grad_norm=0.315143, loss=0.136611
I0401 09:47:59.843865 140675140405056 submission.py:119] 65) loss = 0.137, grad_norm = 0.315
I0401 09:48:01.597366 140536103012096 logging_writer.py:48] [66] global_step=66, grad_norm=0.543450, loss=0.136832
I0401 09:48:01.600453 140675140405056 submission.py:119] 66) loss = 0.137, grad_norm = 0.543
I0401 09:48:03.386842 140536094619392 logging_writer.py:48] [67] global_step=67, grad_norm=0.446420, loss=0.135185
I0401 09:48:03.390085 140675140405056 submission.py:119] 67) loss = 0.135, grad_norm = 0.446
I0401 09:48:05.114992 140536103012096 logging_writer.py:48] [68] global_step=68, grad_norm=0.024998, loss=0.133546
I0401 09:48:05.118329 140675140405056 submission.py:119] 68) loss = 0.134, grad_norm = 0.025
I0401 09:48:06.843587 140536094619392 logging_writer.py:48] [69] global_step=69, grad_norm=0.345747, loss=0.135064
I0401 09:48:06.846962 140675140405056 submission.py:119] 69) loss = 0.135, grad_norm = 0.346
I0401 09:48:08.577484 140536103012096 logging_writer.py:48] [70] global_step=70, grad_norm=0.297763, loss=0.134149
I0401 09:48:08.580621 140675140405056 submission.py:119] 70) loss = 0.134, grad_norm = 0.298
I0401 09:48:10.423376 140536094619392 logging_writer.py:48] [71] global_step=71, grad_norm=0.058363, loss=0.132405
I0401 09:48:10.426865 140675140405056 submission.py:119] 71) loss = 0.132, grad_norm = 0.058
I0401 09:48:12.152799 140536103012096 logging_writer.py:48] [72] global_step=72, grad_norm=0.275072, loss=0.132904
I0401 09:48:12.156382 140675140405056 submission.py:119] 72) loss = 0.133, grad_norm = 0.275
I0401 09:48:13.890547 140536094619392 logging_writer.py:48] [73] global_step=73, grad_norm=0.198823, loss=0.131213
I0401 09:48:13.893702 140675140405056 submission.py:119] 73) loss = 0.131, grad_norm = 0.199
I0401 09:48:15.662237 140536103012096 logging_writer.py:48] [74] global_step=74, grad_norm=0.070147, loss=0.131073
I0401 09:48:15.665421 140675140405056 submission.py:119] 74) loss = 0.131, grad_norm = 0.070
I0401 09:48:17.448740 140536094619392 logging_writer.py:48] [75] global_step=75, grad_norm=0.243453, loss=0.132165
I0401 09:48:17.451973 140675140405056 submission.py:119] 75) loss = 0.132, grad_norm = 0.243
I0401 09:48:19.201351 140536103012096 logging_writer.py:48] [76] global_step=76, grad_norm=0.223793, loss=0.130369
I0401 09:48:19.204498 140675140405056 submission.py:119] 76) loss = 0.130, grad_norm = 0.224
I0401 09:48:20.935082 140536094619392 logging_writer.py:48] [77] global_step=77, grad_norm=0.092646, loss=0.129697
I0401 09:48:20.938386 140675140405056 submission.py:119] 77) loss = 0.130, grad_norm = 0.093
I0401 09:48:22.762522 140536103012096 logging_writer.py:48] [78] global_step=78, grad_norm=0.029895, loss=0.131404
I0401 09:48:22.765856 140675140405056 submission.py:119] 78) loss = 0.131, grad_norm = 0.030
I0401 09:48:24.502899 140536094619392 logging_writer.py:48] [79] global_step=79, grad_norm=0.114718, loss=0.131768
I0401 09:48:24.506787 140675140405056 submission.py:119] 79) loss = 0.132, grad_norm = 0.115
I0401 09:48:26.257917 140536103012096 logging_writer.py:48] [80] global_step=80, grad_norm=0.186743, loss=0.132968
I0401 09:48:26.261487 140675140405056 submission.py:119] 80) loss = 0.133, grad_norm = 0.187
I0401 09:48:28.009397 140536094619392 logging_writer.py:48] [81] global_step=81, grad_norm=0.260919, loss=0.132288
I0401 09:48:28.012728 140675140405056 submission.py:119] 81) loss = 0.132, grad_norm = 0.261
I0401 09:48:29.753745 140536103012096 logging_writer.py:48] [82] global_step=82, grad_norm=0.414022, loss=0.132155
I0401 09:48:29.756851 140675140405056 submission.py:119] 82) loss = 0.132, grad_norm = 0.414
I0401 09:48:31.509358 140536094619392 logging_writer.py:48] [83] global_step=83, grad_norm=0.530280, loss=0.132390
I0401 09:48:31.512365 140675140405056 submission.py:119] 83) loss = 0.132, grad_norm = 0.530
I0401 09:48:33.255479 140536103012096 logging_writer.py:48] [84] global_step=84, grad_norm=0.450128, loss=0.130915
I0401 09:48:33.258949 140675140405056 submission.py:119] 84) loss = 0.131, grad_norm = 0.450
I0401 09:48:35.002245 140536094619392 logging_writer.py:48] [85] global_step=85, grad_norm=0.191246, loss=0.129416
I0401 09:48:35.005526 140675140405056 submission.py:119] 85) loss = 0.129, grad_norm = 0.191
I0401 09:48:36.729820 140536103012096 logging_writer.py:48] [86] global_step=86, grad_norm=0.098010, loss=0.128407
I0401 09:48:36.732934 140675140405056 submission.py:119] 86) loss = 0.128, grad_norm = 0.098
I0401 09:48:38.464580 140536094619392 logging_writer.py:48] [87] global_step=87, grad_norm=0.224700, loss=0.129480
I0401 09:48:38.468390 140675140405056 submission.py:119] 87) loss = 0.129, grad_norm = 0.225
I0401 09:48:40.275593 140536103012096 logging_writer.py:48] [88] global_step=88, grad_norm=0.176062, loss=0.128064
I0401 09:48:40.278866 140675140405056 submission.py:119] 88) loss = 0.128, grad_norm = 0.176
I0401 09:48:42.014305 140536094619392 logging_writer.py:48] [89] global_step=89, grad_norm=0.021134, loss=0.129145
I0401 09:48:42.018139 140675140405056 submission.py:119] 89) loss = 0.129, grad_norm = 0.021
I0401 09:48:43.777916 140536103012096 logging_writer.py:48] [90] global_step=90, grad_norm=0.219111, loss=0.127362
I0401 09:48:43.781695 140675140405056 submission.py:119] 90) loss = 0.127, grad_norm = 0.219
I0401 09:48:45.519620 140536094619392 logging_writer.py:48] [91] global_step=91, grad_norm=0.342570, loss=0.129419
I0401 09:48:45.523689 140675140405056 submission.py:119] 91) loss = 0.129, grad_norm = 0.343
I0401 09:48:47.264054 140536103012096 logging_writer.py:48] [92] global_step=92, grad_norm=0.312329, loss=0.130618
I0401 09:48:47.267249 140675140405056 submission.py:119] 92) loss = 0.131, grad_norm = 0.312
I0401 09:48:49.001813 140536094619392 logging_writer.py:48] [93] global_step=93, grad_norm=0.147964, loss=0.129692
I0401 09:48:49.005845 140675140405056 submission.py:119] 93) loss = 0.130, grad_norm = 0.148
I0401 09:48:50.737982 140536103012096 logging_writer.py:48] [94] global_step=94, grad_norm=0.049242, loss=0.128421
I0401 09:48:50.741199 140675140405056 submission.py:119] 94) loss = 0.128, grad_norm = 0.049
I0401 09:48:52.531128 140536094619392 logging_writer.py:48] [95] global_step=95, grad_norm=0.179560, loss=0.129538
I0401 09:48:52.534250 140675140405056 submission.py:119] 95) loss = 0.130, grad_norm = 0.180
I0401 09:48:54.262784 140536103012096 logging_writer.py:48] [96] global_step=96, grad_norm=0.219980, loss=0.129002
I0401 09:48:54.265791 140675140405056 submission.py:119] 96) loss = 0.129, grad_norm = 0.220
I0401 09:48:55.994165 140536094619392 logging_writer.py:48] [97] global_step=97, grad_norm=0.114088, loss=0.130536
I0401 09:48:55.997301 140675140405056 submission.py:119] 97) loss = 0.131, grad_norm = 0.114
I0401 09:48:57.782432 140536103012096 logging_writer.py:48] [98] global_step=98, grad_norm=0.074258, loss=0.127602
I0401 09:48:57.785562 140675140405056 submission.py:119] 98) loss = 0.128, grad_norm = 0.074
I0401 09:48:59.555854 140536094619392 logging_writer.py:48] [99] global_step=99, grad_norm=0.244122, loss=0.129559
I0401 09:48:59.559165 140675140405056 submission.py:119] 99) loss = 0.130, grad_norm = 0.244
I0401 09:49:01.304968 140536103012096 logging_writer.py:48] [100] global_step=100, grad_norm=0.359184, loss=0.129965
I0401 09:49:01.311292 140675140405056 submission.py:119] 100) loss = 0.130, grad_norm = 0.359
I0401 09:55:04.350332 140675140405056 submission_runner.py:371] Before eval at step 308: RAM USED (GB) 114.427232256
I0401 09:55:04.350553 140675140405056 spec.py:298] Evaluating on the training split.
I0401 10:05:09.973163 140675140405056 spec.py:310] Evaluating on the validation split.
I0401 10:10:29.007176 140675140405056 spec.py:326] Evaluating on the test split.
I0401 10:15:31.354465 140675140405056 submission_runner.py:380] Time since start: 1881.25s, 	Step: 308, 	{'train/loss': 0.12633218547717187, 'validation/loss': 0.1273255393258427, 'validation/num_examples': 89000000, 'test/loss': 0.129657687658814, 'test/num_examples': 89274637}
I0401 10:15:31.354968 140675140405056 submission_runner.py:390] After eval at step 308: RAM USED (GB) 117.15627008
I0401 10:15:31.364260 140536094619392 logging_writer.py:48] [308] global_step=308, preemption_count=0, score=674.433709, test/loss=0.129658, test/num_examples=89274637, total_duration=1881.254051, train/loss=0.126332, validation/loss=0.127326, validation/num_examples=89000000
I0401 10:15:45.462007 140675140405056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_308.
I0401 10:15:45.462535 140675140405056 submission_runner.py:409] After logging and checkpointing eval at step 308: RAM USED (GB) 117.166370816
I0401 10:21:21.587438 140536103012096 logging_writer.py:48] [500] global_step=500, grad_norm=0.042311, loss=0.124919
I0401 10:21:21.591285 140675140405056 submission.py:119] 500) loss = 0.125, grad_norm = 0.042
I0401 10:24:47.141644 140675140405056 submission_runner.py:371] Before eval at step 619: RAM USED (GB) 120.035147776
I0401 10:24:47.141843 140675140405056 spec.py:298] Evaluating on the training split.
I0401 10:34:55.473088 140675140405056 spec.py:310] Evaluating on the validation split.
I0401 10:40:04.557026 140675140405056 spec.py:326] Evaluating on the test split.
I0401 10:44:52.780937 140675140405056 submission_runner.py:380] Time since start: 3664.05s, 	Step: 619, 	{'train/loss': 0.12560799369222544, 'validation/loss': 0.126621797752809, 'validation/num_examples': 89000000, 'test/loss': 0.12897358518523017, 'test/num_examples': 89274637}
I0401 10:44:52.781333 140675140405056 submission_runner.py:390] After eval at step 619: RAM USED (GB) 122.201792512
I0401 10:44:52.789107 140531514464000 logging_writer.py:48] [619] global_step=619, preemption_count=0, score=1182.255796, test/loss=0.128974, test/num_examples=89274637, total_duration=3664.045063, train/loss=0.125608, validation/loss=0.126622, validation/num_examples=89000000
I0401 10:45:06.834924 140675140405056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_619.
I0401 10:45:06.835442 140675140405056 submission_runner.py:409] After logging and checkpointing eval at step 619: RAM USED (GB) 122.2254592
I0401 10:50:21.472043 140675140405056 submission_runner.py:371] Before eval at step 800: RAM USED (GB) 123.706298368
I0401 10:50:21.472231 140675140405056 spec.py:298] Evaluating on the training split.
I0401 11:00:26.256827 140675140405056 spec.py:310] Evaluating on the validation split.
I0401 11:05:21.442897 140675140405056 spec.py:326] Evaluating on the test split.
I0401 11:09:56.737792 140675140405056 submission_runner.py:380] Time since start: 5198.38s, 	Step: 800, 	{'train/loss': 0.1238393551992791, 'validation/loss': 0.12592280898876404, 'validation/num_examples': 89000000, 'test/loss': 0.12822655330427163, 'test/num_examples': 89274637}
I0401 11:09:56.738165 140675140405056 submission_runner.py:390] After eval at step 800: RAM USED (GB) 124.988076032
I0401 11:09:56.747943 140531506071296 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1477.115729, test/loss=0.128227, test/num_examples=89274637, total_duration=5198.375281, train/loss=0.123839, validation/loss=0.125923, validation/num_examples=89000000
I0401 11:10:10.696195 140675140405056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0401 11:10:10.696641 140675140405056 submission_runner.py:409] After logging and checkpointing eval at step 800: RAM USED (GB) 125.004296192
I0401 11:10:10.727256 140531514464000 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1477.115729
I0401 11:10:28.459820 140675140405056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0401 11:12:07.750426 140675140405056 submission_runner.py:543] Tuning trial 1/1
I0401 11:12:07.750678 140675140405056 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 11:12:07.756431 140675140405056 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/loss': 0.5640724524581494, 'validation/loss': 0.5652694831460674, 'validation/num_examples': 89000000, 'test/loss': 0.5644447481763494, 'test/num_examples': 89274637, 'score': 156.17346596717834, 'total_duration': 156.17539143562317, 'global_step': 1, 'preemption_count': 0}), (308, {'train/loss': 0.12633218547717187, 'validation/loss': 0.1273255393258427, 'validation/num_examples': 89000000, 'test/loss': 0.129657687658814, 'test/num_examples': 89274637, 'score': 674.4337093830109, 'total_duration': 1881.254051208496, 'global_step': 308, 'preemption_count': 0}), (619, {'train/loss': 0.12560799369222544, 'validation/loss': 0.126621797752809, 'validation/num_examples': 89000000, 'test/loss': 0.12897358518523017, 'test/num_examples': 89274637, 'score': 1182.2557961940765, 'total_duration': 3664.0450632572174, 'global_step': 619, 'preemption_count': 0}), (800, {'train/loss': 0.1238393551992791, 'validation/loss': 0.12592280898876404, 'validation/num_examples': 89000000, 'test/loss': 0.12822655330427163, 'test/num_examples': 89274637, 'score': 1477.115728855133, 'total_duration': 5198.375280857086, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0401 11:12:07.757060 140675140405056 submission_runner.py:546] Timing: 1477.115728855133
I0401 11:12:07.757174 140675140405056 submission_runner.py:547] ====================
I0401 11:12:07.757279 140675140405056 submission_runner.py:606] Final criteo1tb score: 1477.115728855133
