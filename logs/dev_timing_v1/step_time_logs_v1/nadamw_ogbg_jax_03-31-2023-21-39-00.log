I0331 21:39:14.202843 139716486924096 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/ogbg_jax.
I0331 21:39:14.248762 139716486924096 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 21:39:15.080099 139716486924096 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0331 21:39:15.081151 139716486924096 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 21:39:15.084318 139716486924096 submission_runner.py:511] Using RNG seed 2299927235
I0331 21:39:16.331237 139716486924096 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 21:39:16.331421 139716486924096 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/ogbg_jax/trial_1.
I0331 21:39:16.331589 139716486924096 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/hparams.json.
I0331 21:39:16.455169 139716486924096 submission_runner.py:230] Starting train once: RAM USED (GB) 4.305354752
I0331 21:39:16.455326 139716486924096 submission_runner.py:231] Initializing dataset.
I0331 21:39:16.686020 139716486924096 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:39:16.690162 139716486924096 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:39:16.849244 139716486924096 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:39:16.877633 139716486924096 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.381335552
I0331 21:39:16.877781 139716486924096 submission_runner.py:240] Initializing model.
I0331 21:39:24.048308 139716486924096 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.197230592
I0331 21:39:24.048502 139716486924096 submission_runner.py:252] Initializing optimizer.
I0331 21:39:24.444365 139716486924096 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.197255168
I0331 21:39:24.444535 139716486924096 submission_runner.py:261] Initializing metrics bundle.
I0331 21:39:24.444582 139716486924096 submission_runner.py:276] Initializing checkpoint and logger.
I0331 21:39:24.445438 139716486924096 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw/ogbg_jax/trial_1 with prefix checkpoint_
I0331 21:39:24.445657 139716486924096 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0331 21:39:24.445716 139716486924096 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0331 21:39:25.322093 139716486924096 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/meta_data_0.json.
I0331 21:39:25.323026 139716486924096 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/flags_0.json.
I0331 21:39:25.325936 139716486924096 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.199462912
I0331 21:39:25.326117 139716486924096 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.199462912
I0331 21:39:25.326178 139716486924096 submission_runner.py:313] Starting training loop.
I0331 21:39:26.911976 139716486924096 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.356171776
I0331 21:39:45.044341 139540551423744 logging_writer.py:48] [0] global_step=0, grad_norm=2.7116246223449707, loss=0.7381288409233093
I0331 21:39:45.053816 139716486924096 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 10.91393536
I0331 21:39:45.054096 139716486924096 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 10.91393536
I0331 21:39:45.054185 139716486924096 spec.py:298] Evaluating on the training split.
I0331 21:39:45.061717 139716486924096 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:39:45.065228 139716486924096 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:39:45.119991 139716486924096 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0331 21:40:00.414646 139716486924096 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0331 21:41:13.095066 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 21:41:13.097683 139716486924096 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:41:13.101261 139716486924096 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:41:13.152761 139716486924096 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:42:16.740016 139716486924096 spec.py:326] Evaluating on the test split.
I0331 21:42:16.742708 139716486924096 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:42:16.746173 139716486924096 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0331 21:42:16.798544 139716486924096 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0331 21:43:20.812810 139716486924096 submission_runner.py:382] Time since start: 19.73s, 	Step: 1, 	{'train/accuracy': 0.48976242542266846, 'train/loss': 0.7381417155265808, 'train/mean_average_precision': 0.022152494235098484, 'validation/accuracy': 0.49025821685791016, 'validation/loss': 0.7420746088027954, 'validation/mean_average_precision': 0.025762619447083083, 'validation/num_examples': 43793, 'test/accuracy': 0.4909301996231079, 'test/loss': 0.7422943115234375, 'test/mean_average_precision': 0.028363941542385883, 'test/num_examples': 43793}
I0331 21:43:20.813251 139716486924096 submission_runner.py:396] After eval at step 1: RAM USED (GB) 12.292407296
I0331 21:43:20.820107 139530702747392 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=19.652745, test/accuracy=0.490930, test/loss=0.742294, test/mean_average_precision=0.028364, test/num_examples=43793, total_duration=19.727904, train/accuracy=0.489762, train/loss=0.738142, train/mean_average_precision=0.022152, validation/accuracy=0.490258, validation/loss=0.742075, validation/mean_average_precision=0.025763, validation/num_examples=43793
I0331 21:43:20.855063 139716486924096 checkpoints.py:356] Saving checkpoint at step: 1
I0331 21:43:20.950641 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_1
I0331 21:43:20.950863 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_1.
I0331 21:43:20.951643 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.29195264
I0331 21:43:21.189048 139716486924096 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.340314112
I0331 21:43:21.202014 139716486924096 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.341080064
I0331 21:43:44.154459 139530711140096 logging_writer.py:48] [100] global_step=100, grad_norm=0.3997207283973694, loss=0.360208123922348
I0331 21:44:07.034163 139531977979648 logging_writer.py:48] [200] global_step=200, grad_norm=0.22223247587680817, loss=0.20136967301368713
I0331 21:44:29.853874 139530711140096 logging_writer.py:48] [300] global_step=300, grad_norm=0.1012468934059143, loss=0.10846558213233948
I0331 21:44:52.615381 139531977979648 logging_writer.py:48] [400] global_step=400, grad_norm=0.06155453994870186, loss=0.07493945956230164
I0331 21:45:15.379658 139530711140096 logging_writer.py:48] [500] global_step=500, grad_norm=0.04071374982595444, loss=0.0663185715675354
I0331 21:45:38.353588 139531977979648 logging_writer.py:48] [600] global_step=600, grad_norm=0.04918524622917175, loss=0.05546218529343605
I0331 21:46:01.442368 139530711140096 logging_writer.py:48] [700] global_step=700, grad_norm=0.051263805478811264, loss=0.057363152503967285
I0331 21:46:24.235134 139531977979648 logging_writer.py:48] [800] global_step=800, grad_norm=0.04469092935323715, loss=0.05254581198096275
I0331 21:46:47.142753 139530711140096 logging_writer.py:48] [900] global_step=900, grad_norm=0.11396518349647522, loss=0.05316471308469772
I0331 21:47:10.060110 139531977979648 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.1326010376214981, loss=0.04961496964097023
I0331 21:47:21.079794 139716486924096 submission_runner.py:373] Before eval at step 1049: RAM USED (GB) 13.216063488
I0331 21:47:21.080025 139716486924096 spec.py:298] Evaluating on the training split.
I0331 21:48:34.747236 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 21:48:37.338150 139716486924096 spec.py:326] Evaluating on the test split.
I0331 21:48:39.880432 139716486924096 submission_runner.py:382] Time since start: 475.75s, 	Step: 1049, 	{'train/accuracy': 0.9871981143951416, 'train/loss': 0.048134058713912964, 'train/mean_average_precision': 0.08459621566620609, 'validation/accuracy': 0.9845709800720215, 'validation/loss': 0.05754664167761803, 'validation/mean_average_precision': 0.08589219149408239, 'validation/num_examples': 43793, 'test/accuracy': 0.9835876822471619, 'test/loss': 0.06077642738819122, 'test/mean_average_precision': 0.08549572468655789, 'test/num_examples': 43793}
I0331 21:48:39.880896 139716486924096 submission_runner.py:396] After eval at step 1049: RAM USED (GB) 13.708746752
I0331 21:48:39.889174 139530711140096 logging_writer.py:48] [1049] global_step=1049, preemption_count=0, score=258.695879, test/accuracy=0.983588, test/loss=0.060776, test/mean_average_precision=0.085496, test/num_examples=43793, total_duration=475.753189, train/accuracy=0.987198, train/loss=0.048134, train/mean_average_precision=0.084596, validation/accuracy=0.984571, validation/loss=0.057547, validation/mean_average_precision=0.085892, validation/num_examples=43793
I0331 21:48:39.924670 139716486924096 checkpoints.py:356] Saving checkpoint at step: 1049
I0331 21:48:40.035881 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_1049
I0331 21:48:40.036468 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_1049.
I0331 21:48:40.037325 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 1049: RAM USED (GB) 13.711679488
I0331 21:48:52.117308 139531977979648 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.06370705366134644, loss=0.04642036557197571
I0331 21:49:14.880795 139531810502400 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02748515084385872, loss=0.044505760073661804
I0331 21:49:37.761827 139531977979648 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.049233339726924896, loss=0.04540671035647392
I0331 21:50:00.623021 139531810502400 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.05869561433792114, loss=0.04762112721800804
I0331 21:50:23.509036 139531977979648 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.05709724873304367, loss=0.05084734037518501
I0331 21:50:46.272069 139531810502400 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0845503956079483, loss=0.04606453701853752
I0331 21:51:09.050675 139531977979648 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.07685057073831558, loss=0.04831906780600548
I0331 21:51:31.452634 139531810502400 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.031097695231437683, loss=0.045608486980199814
I0331 21:51:53.726241 139531977979648 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.04291816055774689, loss=0.042943406850099564
I0331 21:52:16.427139 139531810502400 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.05626877024769783, loss=0.04612831026315689
I0331 21:52:38.917066 139531977979648 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.042001836001873016, loss=0.04444580152630806
I0331 21:52:40.056387 139716486924096 submission_runner.py:373] Before eval at step 2106: RAM USED (GB) 14.060048384
I0331 21:52:40.056565 139716486924096 spec.py:298] Evaluating on the training split.
I0331 21:53:51.868503 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 21:53:54.381437 139716486924096 spec.py:326] Evaluating on the test split.
I0331 21:53:57.105202 139716486924096 submission_runner.py:382] Time since start: 794.73s, 	Step: 2106, 	{'train/accuracy': 0.9879052639007568, 'train/loss': 0.043121349066495895, 'train/mean_average_precision': 0.1493209229895821, 'validation/accuracy': 0.9850824475288391, 'validation/loss': 0.05205149948596954, 'validation/mean_average_precision': 0.14348516007764509, 'validation/num_examples': 43793, 'test/accuracy': 0.9841268062591553, 'test/loss': 0.05482203885912895, 'test/mean_average_precision': 0.14235315835492543, 'test/num_examples': 43793}
I0331 21:53:57.105628 139716486924096 submission_runner.py:396] After eval at step 2106: RAM USED (GB) 14.377283584
I0331 21:53:57.112602 139531810502400 logging_writer.py:48] [2106] global_step=2106, preemption_count=0, score=497.609314, test/accuracy=0.984127, test/loss=0.054822, test/mean_average_precision=0.142353, test/num_examples=43793, total_duration=794.729809, train/accuracy=0.987905, train/loss=0.043121, train/mean_average_precision=0.149321, validation/accuracy=0.985082, validation/loss=0.052051, validation/mean_average_precision=0.143485, validation/num_examples=43793
I0331 21:53:57.145040 139716486924096 checkpoints.py:356] Saving checkpoint at step: 2106
I0331 21:53:57.240655 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_2106
I0331 21:53:57.240869 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_2106.
I0331 21:53:57.241673 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 2106: RAM USED (GB) 14.374477824
I0331 21:54:18.820423 139531977979648 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.029952513054013252, loss=0.04512854665517807
I0331 21:54:41.363240 139531785324288 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.050623293966054916, loss=0.04799623042345047
I0331 21:55:03.644859 139531977979648 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.024234985932707787, loss=0.041542936116456985
I0331 21:55:25.853523 139531785324288 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.06065388768911362, loss=0.04555129632353783
I0331 21:55:48.122891 139531977979648 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.058475762605667114, loss=0.04275764897465706
I0331 21:56:10.704543 139531785324288 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.034198712557554245, loss=0.03967878967523575
I0331 21:56:33.087616 139531977979648 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.024768443778157234, loss=0.04658610746264458
I0331 21:56:55.218590 139531785324288 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.05131501704454422, loss=0.0444788821041584
I0331 21:57:17.393065 139531977979648 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.03228147700428963, loss=0.04467712715268135
I0331 21:57:39.387192 139531785324288 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.04026683047413826, loss=0.04572813957929611
I0331 21:57:57.379823 139716486924096 submission_runner.py:373] Before eval at step 3182: RAM USED (GB) 14.658863104
I0331 21:57:57.380030 139716486924096 spec.py:298] Evaluating on the training split.
I0331 21:59:09.963715 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 21:59:12.531532 139716486924096 spec.py:326] Evaluating on the test split.
I0331 21:59:15.010734 139716486924096 submission_runner.py:382] Time since start: 1112.05s, 	Step: 3182, 	{'train/accuracy': 0.9883849024772644, 'train/loss': 0.04051045700907707, 'train/mean_average_precision': 0.1975992880810856, 'validation/accuracy': 0.9854884147644043, 'validation/loss': 0.0498342402279377, 'validation/mean_average_precision': 0.17233641055689627, 'validation/num_examples': 43793, 'test/accuracy': 0.9845888614654541, 'test/loss': 0.052604321390390396, 'test/mean_average_precision': 0.1751485490763336, 'test/num_examples': 43793}
I0331 21:59:15.011156 139716486924096 submission_runner.py:396] After eval at step 3182: RAM USED (GB) 14.940667904
I0331 21:59:15.018063 139531977979648 logging_writer.py:48] [3182] global_step=3182, preemption_count=0, score=736.664228, test/accuracy=0.984589, test/loss=0.052604, test/mean_average_precision=0.175149, test/num_examples=43793, total_duration=1112.053213, train/accuracy=0.988385, train/loss=0.040510, train/mean_average_precision=0.197599, validation/accuracy=0.985488, validation/loss=0.049834, validation/mean_average_precision=0.172336, validation/num_examples=43793
I0331 21:59:15.051043 139716486924096 checkpoints.py:356] Saving checkpoint at step: 3182
I0331 21:59:15.140941 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_3182
I0331 21:59:15.141546 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_3182.
I0331 21:59:15.142355 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 3182: RAM USED (GB) 14.943080448
I0331 21:59:19.393065 139531785324288 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.02528155781328678, loss=0.038381319493055344
I0331 21:59:41.717896 139531776931584 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.022984934970736504, loss=0.04195215180516243
I0331 22:00:04.137613 139531785324288 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.03706297278404236, loss=0.04309677332639694
I0331 22:00:26.526958 139531776931584 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.03289760649204254, loss=0.044225819408893585
I0331 22:00:49.105167 139531785324288 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.019262012094259262, loss=0.04409123212099075
I0331 22:01:11.719709 139531776931584 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.024349134415388107, loss=0.03860466182231903
I0331 22:01:33.846128 139531785324288 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.03349009156227112, loss=0.041727762669324875
I0331 22:01:56.017591 139531776931584 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.024969423189759254, loss=0.04599464684724808
I0331 22:02:18.323435 139531785324288 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.019603760913014412, loss=0.043307021260261536
I0331 22:02:40.704365 139531776931584 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.015298774465918541, loss=0.040949758142232895
I0331 22:03:03.206771 139531785324288 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.022837236523628235, loss=0.04182426631450653
I0331 22:03:15.198918 139716486924096 submission_runner.py:373] Before eval at step 4255: RAM USED (GB) 15.116255232
I0331 22:03:15.199102 139716486924096 spec.py:298] Evaluating on the training split.
I0331 22:04:26.069709 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 22:04:28.620069 139716486924096 spec.py:326] Evaluating on the test split.
I0331 22:04:31.137728 139716486924096 submission_runner.py:382] Time since start: 1429.87s, 	Step: 4255, 	{'train/accuracy': 0.9888395071029663, 'train/loss': 0.03836547210812569, 'train/mean_average_precision': 0.23219140018637066, 'validation/accuracy': 0.9857725501060486, 'validation/loss': 0.04823737591505051, 'validation/mean_average_precision': 0.19540093222157418, 'validation/num_examples': 43793, 'test/accuracy': 0.9848672747612, 'test/loss': 0.05087009444832802, 'test/mean_average_precision': 0.1948618684646546, 'test/num_examples': 43793}
I0331 22:04:31.138230 139716486924096 submission_runner.py:396] After eval at step 4255: RAM USED (GB) 15.480119296
I0331 22:04:31.145715 139531776931584 logging_writer.py:48] [4255] global_step=4255, preemption_count=0, score=975.624212, test/accuracy=0.984867, test/loss=0.050870, test/mean_average_precision=0.194862, test/num_examples=43793, total_duration=1429.872315, train/accuracy=0.988840, train/loss=0.038365, train/mean_average_precision=0.232191, validation/accuracy=0.985773, validation/loss=0.048237, validation/mean_average_precision=0.195401, validation/num_examples=43793
I0331 22:04:31.179979 139716486924096 checkpoints.py:356] Saving checkpoint at step: 4255
I0331 22:04:31.275667 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_4255
I0331 22:04:31.276287 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_4255.
I0331 22:04:31.277189 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 4255: RAM USED (GB) 15.478603776
I0331 22:04:41.543478 139531785324288 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.026041576638817787, loss=0.04143192246556282
I0331 22:05:04.114879 139531768538880 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.020445551723241806, loss=0.044716689735651016
I0331 22:05:26.829719 139531785324288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.02004295401275158, loss=0.039084721356630325
I0331 22:05:49.356450 139531768538880 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.022278008982539177, loss=0.04004187509417534
I0331 22:06:12.016901 139531785324288 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.02997184917330742, loss=0.04020099714398384
I0331 22:06:34.624684 139531768538880 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.023722155019640923, loss=0.04109635204076767
I0331 22:06:57.714937 139531785324288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.014192190021276474, loss=0.03922140970826149
I0331 22:07:20.670278 139531768538880 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.017090443521738052, loss=0.04294143617153168
I0331 22:07:43.035697 139531785324288 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.027942895889282227, loss=0.041181229054927826
I0331 22:08:05.584722 139531768538880 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.017881082370877266, loss=0.038268983364105225
I0331 22:08:28.239259 139531785324288 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.021394724026322365, loss=0.03917606920003891
I0331 22:08:31.391552 139716486924096 submission_runner.py:373] Before eval at step 5315: RAM USED (GB) 15.638142976
I0331 22:08:31.391729 139716486924096 spec.py:298] Evaluating on the training split.
I0331 22:09:45.218652 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 22:09:47.802700 139716486924096 spec.py:326] Evaluating on the test split.
I0331 22:09:50.311863 139716486924096 submission_runner.py:382] Time since start: 1746.06s, 	Step: 5315, 	{'train/accuracy': 0.9891887903213501, 'train/loss': 0.03715222328901291, 'train/mean_average_precision': 0.2717542542800047, 'validation/accuracy': 0.986095666885376, 'validation/loss': 0.0469900406897068, 'validation/mean_average_precision': 0.2118971047685548, 'validation/num_examples': 43793, 'test/accuracy': 0.9851414561271667, 'test/loss': 0.04967585951089859, 'test/mean_average_precision': 0.21525378082854046, 'test/num_examples': 43793}
I0331 22:09:50.312336 139716486924096 submission_runner.py:396] After eval at step 5315: RAM USED (GB) 15.917142016
I0331 22:09:50.319645 139531768538880 logging_writer.py:48] [5315] global_step=5315, preemption_count=0, score=1214.631396, test/accuracy=0.985141, test/loss=0.049676, test/mean_average_precision=0.215254, test/num_examples=43793, total_duration=1746.064896, train/accuracy=0.989189, train/loss=0.037152, train/mean_average_precision=0.271754, validation/accuracy=0.986096, validation/loss=0.046990, validation/mean_average_precision=0.211897, validation/num_examples=43793
I0331 22:09:50.354416 139716486924096 checkpoints.py:356] Saving checkpoint at step: 5315
I0331 22:09:50.470709 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_5315
I0331 22:09:50.471116 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_5315.
I0331 22:09:50.471969 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 5315: RAM USED (GB) 15.939198976
I0331 22:10:09.818007 139531785324288 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.013609351590275764, loss=0.03904873877763748
I0331 22:10:32.128073 139531760146176 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.02186296321451664, loss=0.04030323401093483
I0331 22:10:54.502208 139531785324288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.013145921751856804, loss=0.04282781854271889
I0331 22:11:16.816843 139531760146176 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01758223958313465, loss=0.04030821844935417
I0331 22:11:39.591159 139531785324288 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.012028095312416553, loss=0.03952190279960632
I0331 22:12:02.004460 139531760146176 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.011000473983585835, loss=0.03895248472690582
I0331 22:12:23.890669 139716486924096 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 16.0379904
I0331 22:12:23.890874 139716486924096 spec.py:298] Evaluating on the training split.
I0331 22:13:35.232176 139716486924096 spec.py:310] Evaluating on the validation split.
I0331 22:13:37.849169 139716486924096 spec.py:326] Evaluating on the test split.
I0331 22:13:40.423973 139716486924096 submission_runner.py:382] Time since start: 1978.56s, 	Step: 6000, 	{'train/accuracy': 0.9892237186431885, 'train/loss': 0.036369506269693375, 'train/mean_average_precision': 0.2756437146024425, 'validation/accuracy': 0.9862101674079895, 'validation/loss': 0.04645436257123947, 'validation/mean_average_precision': 0.2268335733576351, 'validation/num_examples': 43793, 'test/accuracy': 0.9852926731109619, 'test/loss': 0.049229178577661514, 'test/mean_average_precision': 0.22044970210526899, 'test/num_examples': 43793}
I0331 22:13:40.424406 139716486924096 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 16.47589376
I0331 22:13:40.431812 139531785324288 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1367.350144, test/accuracy=0.985293, test/loss=0.049229, test/mean_average_precision=0.220450, test/num_examples=43793, total_duration=1978.564042, train/accuracy=0.989224, train/loss=0.036370, train/mean_average_precision=0.275644, validation/accuracy=0.986210, validation/loss=0.046454, validation/mean_average_precision=0.226834, validation/num_examples=43793
I0331 22:13:40.465723 139716486924096 checkpoints.py:356] Saving checkpoint at step: 6000
I0331 22:13:40.557239 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_6000
I0331 22:13:40.557646 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_6000.
I0331 22:13:40.558578 139716486924096 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 16.474226688
I0331 22:13:40.564747 139531760146176 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1367.350144
I0331 22:13:40.593872 139716486924096 checkpoints.py:356] Saving checkpoint at step: 6000
I0331 22:13:40.752793 139716486924096 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_6000
I0331 22:13:40.753221 139716486924096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/ogbg_jax/trial_1/checkpoint_6000.
I0331 22:13:40.910211 139716486924096 submission_runner.py:550] Tuning trial 1/1
I0331 22:13:40.910440 139716486924096 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0331 22:13:40.911547 139716486924096 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.48976242542266846, 'train/loss': 0.7381417155265808, 'train/mean_average_precision': 0.022152494235098484, 'validation/accuracy': 0.49025821685791016, 'validation/loss': 0.7420746088027954, 'validation/mean_average_precision': 0.025762619447083083, 'validation/num_examples': 43793, 'test/accuracy': 0.4909301996231079, 'test/loss': 0.7422943115234375, 'test/mean_average_precision': 0.028363941542385883, 'test/num_examples': 43793, 'score': 19.652745246887207, 'total_duration': 19.727904081344604, 'global_step': 1, 'preemption_count': 0}), (1049, {'train/accuracy': 0.9871981143951416, 'train/loss': 0.048134058713912964, 'train/mean_average_precision': 0.08459621566620609, 'validation/accuracy': 0.9845709800720215, 'validation/loss': 0.05754664167761803, 'validation/mean_average_precision': 0.08589219149408239, 'validation/num_examples': 43793, 'test/accuracy': 0.9835876822471619, 'test/loss': 0.06077642738819122, 'test/mean_average_precision': 0.08549572468655789, 'test/num_examples': 43793, 'score': 258.6958792209625, 'total_duration': 475.75318932533264, 'global_step': 1049, 'preemption_count': 0}), (2106, {'train/accuracy': 0.9879052639007568, 'train/loss': 0.043121349066495895, 'train/mean_average_precision': 0.1493209229895821, 'validation/accuracy': 0.9850824475288391, 'validation/loss': 0.05205149948596954, 'validation/mean_average_precision': 0.14348516007764509, 'validation/num_examples': 43793, 'test/accuracy': 0.9841268062591553, 'test/loss': 0.05482203885912895, 'test/mean_average_precision': 0.14235315835492543, 'test/num_examples': 43793, 'score': 497.60931420326233, 'total_duration': 794.7298092842102, 'global_step': 2106, 'preemption_count': 0}), (3182, {'train/accuracy': 0.9883849024772644, 'train/loss': 0.04051045700907707, 'train/mean_average_precision': 0.1975992880810856, 'validation/accuracy': 0.9854884147644043, 'validation/loss': 0.0498342402279377, 'validation/mean_average_precision': 0.17233641055689627, 'validation/num_examples': 43793, 'test/accuracy': 0.9845888614654541, 'test/loss': 0.052604321390390396, 'test/mean_average_precision': 0.1751485490763336, 'test/num_examples': 43793, 'score': 736.6642277240753, 'total_duration': 1112.0532128810883, 'global_step': 3182, 'preemption_count': 0}), (4255, {'train/accuracy': 0.9888395071029663, 'train/loss': 0.03836547210812569, 'train/mean_average_precision': 0.23219140018637066, 'validation/accuracy': 0.9857725501060486, 'validation/loss': 0.04823737591505051, 'validation/mean_average_precision': 0.19540093222157418, 'validation/num_examples': 43793, 'test/accuracy': 0.9848672747612, 'test/loss': 0.05087009444832802, 'test/mean_average_precision': 0.1948618684646546, 'test/num_examples': 43793, 'score': 975.6242115497589, 'total_duration': 1429.8723154067993, 'global_step': 4255, 'preemption_count': 0}), (5315, {'train/accuracy': 0.9891887903213501, 'train/loss': 0.03715222328901291, 'train/mean_average_precision': 0.2717542542800047, 'validation/accuracy': 0.986095666885376, 'validation/loss': 0.0469900406897068, 'validation/mean_average_precision': 0.2118971047685548, 'validation/num_examples': 43793, 'test/accuracy': 0.9851414561271667, 'test/loss': 0.04967585951089859, 'test/mean_average_precision': 0.21525378082854046, 'test/num_examples': 43793, 'score': 1214.6313955783844, 'total_duration': 1746.0648956298828, 'global_step': 5315, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9892237186431885, 'train/loss': 0.036369506269693375, 'train/mean_average_precision': 0.2756437146024425, 'validation/accuracy': 0.9862101674079895, 'validation/loss': 0.04645436257123947, 'validation/mean_average_precision': 0.2268335733576351, 'validation/num_examples': 43793, 'test/accuracy': 0.9852926731109619, 'test/loss': 0.049229178577661514, 'test/mean_average_precision': 0.22044970210526899, 'test/num_examples': 43793, 'score': 1367.350144147873, 'total_duration': 1978.5640420913696, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0331 22:13:40.911670 139716486924096 submission_runner.py:553] Timing: 1367.350144147873
I0331 22:13:40.911719 139716486924096 submission_runner.py:554] ====================
I0331 22:13:40.911836 139716486924096 submission_runner.py:613] Final ogbg score: 1367.350144147873
