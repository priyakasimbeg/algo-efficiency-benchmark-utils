I0329 22:02:38.504275 139926464792384 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/ogbg_jax.
I0329 22:02:38.555081 139926464792384 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 22:02:39.406301 139926464792384 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0329 22:02:39.406936 139926464792384 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 22:02:39.411832 139926464792384 submission_runner.py:504] Using RNG seed 1216849328
I0329 22:02:40.670295 139926464792384 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 22:02:40.670490 139926464792384 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/ogbg_jax/trial_1.
I0329 22:02:40.670688 139926464792384 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/ogbg_jax/trial_1/hparams.json.
I0329 22:02:40.806808 139926464792384 submission_runner.py:230] Starting train once: RAM USED (GB) 4.345430016
I0329 22:02:40.806982 139926464792384 submission_runner.py:231] Initializing dataset.
I0329 22:02:41.061766 139926464792384 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:02:41.066909 139926464792384 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 22:02:41.235909 139926464792384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:02:41.268720 139926464792384 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.41288704
I0329 22:02:41.268900 139926464792384 submission_runner.py:240] Initializing model.
I0329 22:02:48.912030 139926464792384 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.228868096
I0329 22:02:48.912226 139926464792384 submission_runner.py:252] Initializing optimizer.
I0329 22:02:49.345104 139926464792384 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.229957632
I0329 22:02:49.345298 139926464792384 submission_runner.py:261] Initializing metrics bundle.
I0329 22:02:49.345375 139926464792384 submission_runner.py:275] Initializing checkpoint and logger.
I0329 22:02:49.346559 139926464792384 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/ogbg_jax/trial_1 with prefix checkpoint_
I0329 22:02:49.346830 139926464792384 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0329 22:02:49.346916 139926464792384 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0329 22:02:50.242695 139926464792384 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/ogbg_jax/trial_1/meta_data_0.json.
I0329 22:02:50.243733 139926464792384 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/ogbg_jax/trial_1/flags_0.json.
I0329 22:02:50.246505 139926464792384 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.229658624
I0329 22:02:50.246731 139926464792384 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.229658624
I0329 22:02:50.246814 139926464792384 submission_runner.py:312] Starting training loop.
I0329 22:02:51.892760 139926464792384 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 8.3830784
I0329 22:03:11.250938 139750207907584 logging_writer.py:48] [0] global_step=0, grad_norm=2.8750267028808594, loss=0.7045334577560425
I0329 22:03:11.260762 139926464792384 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 11.043708928
I0329 22:03:11.261067 139926464792384 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 11.043708928
I0329 22:03:11.261148 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:03:11.270196 139926464792384 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:03:11.275274 139926464792384 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 22:03:11.339577 139926464792384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0329 22:03:29.227399 139926464792384 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0329 22:04:50.565319 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:04:50.568751 139926464792384 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:04:50.573260 139926464792384 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 22:04:50.633201 139926464792384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:06:02.716506 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:06:02.720159 139926464792384 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:06:02.724700 139926464792384 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0329 22:06:02.783983 139926464792384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0329 22:07:15.165425 139926464792384 submission_runner.py:380] Time since start: 21.01s, 	Step: 1, 	{'train/accuracy': 0.5896993279457092, 'train/loss': 0.7032931447029114, 'train/mean_average_precision': 0.022585955060818078, 'validation/accuracy': 0.587599515914917, 'validation/loss': 0.7057192921638489, 'validation/mean_average_precision': 0.0254660818910648, 'validation/num_examples': 43793, 'test/accuracy': 0.587836742401123, 'test/loss': 0.7061472535133362, 'test/mean_average_precision': 0.027733035515473833, 'test/num_examples': 43793}
I0329 22:07:15.165872 139926464792384 submission_runner.py:390] After eval at step 1: RAM USED (GB) 12.394491904
I0329 22:07:15.173455 139740577343232 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=20.933069, test/accuracy=0.587837, test/loss=0.706147, test/mean_average_precision=0.027733, test/num_examples=43793, total_duration=21.014334, train/accuracy=0.589699, train/loss=0.703293, train/mean_average_precision=0.022586, validation/accuracy=0.587600, validation/loss=0.705719, validation/mean_average_precision=0.025466, validation/num_examples=43793
I0329 22:07:15.209785 139926464792384 checkpoints.py:356] Saving checkpoint at step: 1
I0329 22:07:15.312079 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_1
I0329 22:07:15.312490 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_1.
I0329 22:07:15.313325 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 12.395061248
I0329 22:07:15.564817 139926464792384 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 12.442058752
I0329 22:07:15.579073 139926464792384 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 12.442025984
I0329 22:07:41.048086 139740585735936 logging_writer.py:48] [100] global_step=100, grad_norm=0.37087368965148926, loss=0.3149564564228058
I0329 22:08:06.859890 139741785433856 logging_writer.py:48] [200] global_step=200, grad_norm=0.2499230057001114, loss=0.1875600963830948
I0329 22:08:32.876709 139740585735936 logging_writer.py:48] [300] global_step=300, grad_norm=0.12978766858577728, loss=0.11189357936382294
I0329 22:08:58.814947 139741785433856 logging_writer.py:48] [400] global_step=400, grad_norm=0.06797196716070175, loss=0.07734702527523041
I0329 22:09:24.581263 139740585735936 logging_writer.py:48] [500] global_step=500, grad_norm=0.12744581699371338, loss=0.06255804002285004
I0329 22:09:50.517974 139741785433856 logging_writer.py:48] [600] global_step=600, grad_norm=0.022392721846699715, loss=0.06395281106233597
I0329 22:10:16.362161 139740585735936 logging_writer.py:48] [700] global_step=700, grad_norm=0.022192100062966347, loss=0.05857584625482559
I0329 22:10:42.168550 139741785433856 logging_writer.py:48] [800] global_step=800, grad_norm=0.021539179608225822, loss=0.054060690104961395
I0329 22:11:08.234941 139740585735936 logging_writer.py:48] [900] global_step=900, grad_norm=0.06898476928472519, loss=0.053873974829912186
I0329 22:11:15.334398 139926464792384 submission_runner.py:371] Before eval at step 928: RAM USED (GB) 13.193367552
I0329 22:11:15.334579 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:12:37.143484 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:12:39.797694 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:12:42.392853 139926464792384 submission_runner.py:380] Time since start: 505.09s, 	Step: 928, 	{'train/accuracy': 0.9867643117904663, 'train/loss': 0.05130469426512718, 'train/mean_average_precision': 0.056781959335915316, 'validation/accuracy': 0.9841402769088745, 'validation/loss': 0.06080959364771843, 'validation/mean_average_precision': 0.056217195213032915, 'validation/num_examples': 43793, 'test/accuracy': 0.9831715822219849, 'test/loss': 0.06409144401550293, 'test/mean_average_precision': 0.05644631758445601, 'test/num_examples': 43793}
I0329 22:12:42.393279 139926464792384 submission_runner.py:390] After eval at step 928: RAM USED (GB) 13.670862848
I0329 22:12:42.401302 139741785433856 logging_writer.py:48] [928] global_step=928, preemption_count=0, score=259.937367, test/accuracy=0.983172, test/loss=0.064091, test/mean_average_precision=0.056446, test/num_examples=43793, total_duration=505.087195, train/accuracy=0.986764, train/loss=0.051305, train/mean_average_precision=0.056782, validation/accuracy=0.984140, validation/loss=0.060810, validation/mean_average_precision=0.056217, validation/num_examples=43793
I0329 22:12:42.435935 139926464792384 checkpoints.py:356] Saving checkpoint at step: 928
I0329 22:12:42.540699 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_928
I0329 22:12:42.541289 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_928.
I0329 22:12:42.542207 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 928: RAM USED (GB) 13.673541632
I0329 22:13:01.524585 139740585735936 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.039203789085149765, loss=0.0497191846370697
I0329 22:13:27.484542 139741685081856 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.029173236340284348, loss=0.049611516296863556
I0329 22:13:53.690136 139740585735936 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02762172557413578, loss=0.04631371051073074
I0329 22:14:19.801060 139741685081856 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.02102086693048477, loss=0.05291534960269928
I0329 22:14:45.949240 139740585735936 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.026716459542512894, loss=0.04694491624832153
I0329 22:15:11.933490 139741685081856 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.021701106801629066, loss=0.04787946864962578
I0329 22:15:37.643826 139740585735936 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02541256695985794, loss=0.05421413853764534
I0329 22:16:03.887095 139741685081856 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.025311820209026337, loss=0.042251165956258774
I0329 22:16:30.011295 139740585735936 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.026964295655488968, loss=0.05428900197148323
I0329 22:16:42.793168 139926464792384 submission_runner.py:371] Before eval at step 1850: RAM USED (GB) 14.016815104
I0329 22:16:42.793343 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:18:04.110985 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:18:06.730011 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:18:09.286545 139926464792384 submission_runner.py:380] Time since start: 832.55s, 	Step: 1850, 	{'train/accuracy': 0.9870824217796326, 'train/loss': 0.048014700412750244, 'train/mean_average_precision': 0.09593947050303112, 'validation/accuracy': 0.9844337701797485, 'validation/loss': 0.05759074538946152, 'validation/mean_average_precision': 0.09330843481639035, 'validation/num_examples': 43793, 'test/accuracy': 0.9834406971931458, 'test/loss': 0.06116626039147377, 'test/mean_average_precision': 0.09210515573616007, 'test/num_examples': 43793}
I0329 22:18:09.287002 139926464792384 submission_runner.py:390] After eval at step 1850: RAM USED (GB) 14.395379712
I0329 22:18:09.296340 139741685081856 logging_writer.py:48] [1850] global_step=1850, preemption_count=0, score=499.156338, test/accuracy=0.983441, test/loss=0.061166, test/mean_average_precision=0.092105, test/num_examples=43793, total_duration=832.545965, train/accuracy=0.987082, train/loss=0.048015, train/mean_average_precision=0.095939, validation/accuracy=0.984434, validation/loss=0.057591, validation/mean_average_precision=0.093308, validation/num_examples=43793
I0329 22:18:09.331516 139926464792384 checkpoints.py:356] Saving checkpoint at step: 1850
I0329 22:18:09.436121 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_1850
I0329 22:18:09.436348 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_1850.
I0329 22:18:09.437200 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 1850: RAM USED (GB) 14.392315904
I0329 22:18:22.466807 139740585735936 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.02273215726017952, loss=0.04792974144220352
I0329 22:18:47.997521 139741668296448 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.019821856170892715, loss=0.04795714467763901
I0329 22:19:13.893832 139740585735936 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.046547651290893555, loss=0.04807717725634575
I0329 22:19:39.875259 139741668296448 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.019538259133696556, loss=0.04630117490887642
I0329 22:20:06.003985 139740585735936 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.02441173419356346, loss=0.05035914480686188
I0329 22:20:32.016003 139741668296448 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.047770190984010696, loss=0.047870103269815445
I0329 22:20:58.016436 139740585735936 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.027135340496897697, loss=0.04674392566084862
I0329 22:21:23.648697 139741668296448 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.01842621900141239, loss=0.04537472128868103
I0329 22:21:49.084255 139740585735936 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.025025920942425728, loss=0.043621815741062164
I0329 22:22:09.685758 139926464792384 submission_runner.py:371] Before eval at step 2781: RAM USED (GB) 14.628265984
I0329 22:22:09.685944 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:23:30.713254 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:23:33.385567 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:23:35.951690 139926464792384 submission_runner.py:380] Time since start: 1159.44s, 	Step: 2781, 	{'train/accuracy': 0.9877657294273376, 'train/loss': 0.043688155710697174, 'train/mean_average_precision': 0.14669934079485614, 'validation/accuracy': 0.9849773049354553, 'validation/loss': 0.05297050625085831, 'validation/mean_average_precision': 0.1358451147114, 'validation/num_examples': 43793, 'test/accuracy': 0.9839979410171509, 'test/loss': 0.05594930425286293, 'test/mean_average_precision': 0.13199371071907537, 'test/num_examples': 43793}
I0329 22:23:35.952142 139926464792384 submission_runner.py:390] After eval at step 2781: RAM USED (GB) 14.847311872
I0329 22:23:35.961622 139741668296448 logging_writer.py:48] [2781] global_step=2781, preemption_count=0, score=738.378957, test/accuracy=0.983998, test/loss=0.055949, test/mean_average_precision=0.131994, test/num_examples=43793, total_duration=1159.438575, train/accuracy=0.987766, train/loss=0.043688, train/mean_average_precision=0.146699, validation/accuracy=0.984977, validation/loss=0.052971, validation/mean_average_precision=0.135845, validation/num_examples=43793
I0329 22:23:35.996608 139926464792384 checkpoints.py:356] Saving checkpoint at step: 2781
I0329 22:23:36.113656 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_2781
I0329 22:23:36.114200 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_2781.
I0329 22:23:36.115044 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 2781: RAM USED (GB) 14.87067136
I0329 22:23:41.197338 139740585735936 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03263838589191437, loss=0.045824456959962845
I0329 22:24:06.744946 139741659903744 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.032414790242910385, loss=0.04475947842001915
I0329 22:24:32.490989 139740585735936 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.025773601606488228, loss=0.045402221381664276
I0329 22:24:58.576179 139741659903744 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.013018299825489521, loss=0.042848777025938034
I0329 22:25:24.134418 139740585735936 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.01311250776052475, loss=0.04582882300019264
I0329 22:25:49.655602 139741659903744 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.020640943199396133, loss=0.04879184067249298
I0329 22:26:15.540156 139740585735936 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.012616841122508049, loss=0.044446300715208054
I0329 22:26:41.105561 139741659903744 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.012692107819020748, loss=0.044332753866910934
I0329 22:27:06.874724 139740585735936 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.014209496788680553, loss=0.040954720228910446
I0329 22:27:32.667902 139741659903744 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.011027130298316479, loss=0.03959443047642708
I0329 22:27:36.255789 139926464792384 submission_runner.py:371] Before eval at step 3715: RAM USED (GB) 15.115608064
I0329 22:27:36.255979 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:28:57.482378 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:29:00.111601 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:29:02.695819 139926464792384 submission_runner.py:380] Time since start: 1486.01s, 	Step: 3715, 	{'train/accuracy': 0.9876130223274231, 'train/loss': 0.04355683922767639, 'train/mean_average_precision': 0.1671618282028592, 'validation/accuracy': 0.9848977327346802, 'validation/loss': 0.05337638780474663, 'validation/mean_average_precision': 0.1593370712752889, 'validation/num_examples': 43793, 'test/accuracy': 0.9839145541191101, 'test/loss': 0.05651284009218216, 'test/mean_average_precision': 0.15114956285303616, 'test/num_examples': 43793}
I0329 22:29:02.696240 139926464792384 submission_runner.py:390] After eval at step 3715: RAM USED (GB) 15.557619712
I0329 22:29:02.705396 139740585735936 logging_writer.py:48] [3715] global_step=3715, preemption_count=0, score=977.486569, test/accuracy=0.983915, test/loss=0.056513, test/mean_average_precision=0.151150, test/num_examples=43793, total_duration=1486.008513, train/accuracy=0.987613, train/loss=0.043557, train/mean_average_precision=0.167162, validation/accuracy=0.984898, validation/loss=0.053376, validation/mean_average_precision=0.159337, validation/num_examples=43793
I0329 22:29:02.740629 139926464792384 checkpoints.py:356] Saving checkpoint at step: 3715
I0329 22:29:02.834907 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_3715
I0329 22:29:02.835533 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_3715.
I0329 22:29:02.836431 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 3715: RAM USED (GB) 15.55636224
I0329 22:29:25.021487 139741659903744 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.03284629061818123, loss=0.04568300023674965
I0329 22:29:51.020136 139741651511040 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.014406716451048851, loss=0.044160693883895874
I0329 22:30:17.230428 139741659903744 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015487085096538067, loss=0.04205269739031792
I0329 22:30:43.299145 139741651511040 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.014690963551402092, loss=0.041918519884347916
I0329 22:31:09.327913 139741659903744 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.010844009928405285, loss=0.04411161690950394
I0329 22:31:35.338217 139741651511040 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.031561609357595444, loss=0.04782559350132942
I0329 22:32:01.412163 139741659903744 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.017528487369418144, loss=0.04021848738193512
I0329 22:32:27.537280 139741651511040 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.010786606930196285, loss=0.040480293333530426
I0329 22:32:53.657807 139741659903744 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.011443093419075012, loss=0.04726437106728554
I0329 22:33:02.945729 139926464792384 submission_runner.py:371] Before eval at step 4637: RAM USED (GB) 15.75829504
I0329 22:33:02.945901 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:34:23.545940 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:34:26.123070 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:34:28.674525 139926464792384 submission_runner.py:380] Time since start: 1812.70s, 	Step: 4637, 	{'train/accuracy': 0.9883093237876892, 'train/loss': 0.04057011753320694, 'train/mean_average_precision': 0.2030987848011287, 'validation/accuracy': 0.9854128956794739, 'validation/loss': 0.0500301718711853, 'validation/mean_average_precision': 0.17348212072133162, 'validation/num_examples': 43793, 'test/accuracy': 0.9845522046089172, 'test/loss': 0.052670590579509735, 'test/mean_average_precision': 0.17384140029345962, 'test/num_examples': 43793}
I0329 22:34:28.674968 139926464792384 submission_runner.py:390] After eval at step 4637: RAM USED (GB) 16.197804032
I0329 22:34:28.684144 139741651511040 logging_writer.py:48] [4637] global_step=4637, preemption_count=0, score=1216.551370, test/accuracy=0.984552, test/loss=0.052671, test/mean_average_precision=0.173841, test/num_examples=43793, total_duration=1812.698534, train/accuracy=0.988309, train/loss=0.040570, train/mean_average_precision=0.203099, validation/accuracy=0.985413, validation/loss=0.050030, validation/mean_average_precision=0.173482, validation/num_examples=43793
I0329 22:34:28.719895 139926464792384 checkpoints.py:356] Saving checkpoint at step: 4637
I0329 22:34:28.812974 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_4637
I0329 22:34:28.813565 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_4637.
I0329 22:34:28.814452 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 4637: RAM USED (GB) 16.198385664
I0329 22:34:45.520331 139741659903744 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.014768100343644619, loss=0.04546356201171875
I0329 22:35:11.586745 139741634725632 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.012035312131047249, loss=0.04453786090016365
I0329 22:35:37.597493 139741659903744 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.013269017450511456, loss=0.04271003603935242
I0329 22:36:03.921958 139741634725632 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.02293817698955536, loss=0.042739931493997574
I0329 22:36:30.175967 139741659903744 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.011365101672708988, loss=0.04180237650871277
I0329 22:36:56.893056 139741634725632 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.011121554300189018, loss=0.04391421005129814
I0329 22:37:23.289610 139741659903744 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.012088045477867126, loss=0.043001290410757065
I0329 22:37:48.943840 139741634725632 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.009536300785839558, loss=0.04284437373280525
I0329 22:38:14.710243 139741659903744 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014688082039356232, loss=0.04249018430709839
I0329 22:38:28.852358 139926464792384 submission_runner.py:371] Before eval at step 5555: RAM USED (GB) 16.40296448
I0329 22:38:28.852540 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:39:48.661575 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:39:51.271345 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:39:53.808931 139926464792384 submission_runner.py:380] Time since start: 2138.61s, 	Step: 5555, 	{'train/accuracy': 0.9886943697929382, 'train/loss': 0.03891220688819885, 'train/mean_average_precision': 0.21823091108761622, 'validation/accuracy': 0.985671877861023, 'validation/loss': 0.04863852635025978, 'validation/mean_average_precision': 0.1876559631414037, 'validation/num_examples': 43793, 'test/accuracy': 0.9848407506942749, 'test/loss': 0.05141185224056244, 'test/mean_average_precision': 0.18953209816513975, 'test/num_examples': 43793}
I0329 22:39:53.809382 139926464792384 submission_runner.py:390] After eval at step 5555: RAM USED (GB) 16.741302272
I0329 22:39:53.817751 139741634725632 logging_writer.py:48] [5555] global_step=5555, preemption_count=0, score=1455.567818, test/accuracy=0.984841, test/loss=0.051412, test/mean_average_precision=0.189532, test/num_examples=43793, total_duration=2138.605206, train/accuracy=0.988694, train/loss=0.038912, train/mean_average_precision=0.218231, validation/accuracy=0.985672, validation/loss=0.048639, validation/mean_average_precision=0.187656, validation/num_examples=43793
I0329 22:39:53.853210 139926464792384 checkpoints.py:356] Saving checkpoint at step: 5555
I0329 22:39:53.947031 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_5555
I0329 22:39:53.947661 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_5555.
I0329 22:39:53.948586 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 5555: RAM USED (GB) 16.74192896
I0329 22:40:05.477434 139741659903744 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.01303401030600071, loss=0.04513796791434288
I0329 22:40:30.555436 139741626332928 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.008867770433425903, loss=0.0412771999835968
I0329 22:40:55.876603 139741659903744 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.011045808903872967, loss=0.03987853601574898
I0329 22:41:21.285175 139741626332928 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.014746511355042458, loss=0.042361125349998474
I0329 22:41:46.484052 139926464792384 submission_runner.py:371] Before eval at step 6000: RAM USED (GB) 16.77563904
I0329 22:41:46.484250 139926464792384 spec.py:298] Evaluating on the training split.
I0329 22:43:05.712900 139926464792384 spec.py:310] Evaluating on the validation split.
I0329 22:43:08.301595 139926464792384 spec.py:326] Evaluating on the test split.
I0329 22:43:10.809355 139926464792384 submission_runner.py:380] Time since start: 2336.24s, 	Step: 6000, 	{'train/accuracy': 0.9888890981674194, 'train/loss': 0.0383332334458828, 'train/mean_average_precision': 0.25940115526258656, 'validation/accuracy': 0.9858484864234924, 'validation/loss': 0.0481545627117157, 'validation/mean_average_precision': 0.20237831814766927, 'validation/num_examples': 43793, 'test/accuracy': 0.9849528074264526, 'test/loss': 0.05082980543375015, 'test/mean_average_precision': 0.20345178525362712, 'test/num_examples': 43793}
I0329 22:43:10.809801 139926464792384 submission_runner.py:390] After eval at step 6000: RAM USED (GB) 17.11171584
I0329 22:43:10.818050 139741659903744 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1567.606980, test/accuracy=0.984953, test/loss=0.050830, test/mean_average_precision=0.203452, test/num_examples=43793, total_duration=2336.236842, train/accuracy=0.988889, train/loss=0.038333, train/mean_average_precision=0.259401, validation/accuracy=0.985848, validation/loss=0.048155, validation/mean_average_precision=0.202378, validation/num_examples=43793
I0329 22:43:10.853596 139926464792384 checkpoints.py:356] Saving checkpoint at step: 6000
I0329 22:43:10.948146 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_6000
I0329 22:43:10.948355 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_6000.
I0329 22:43:10.949172 139926464792384 submission_runner.py:409] After logging and checkpointing eval at step 6000: RAM USED (GB) 17.11210496
I0329 22:43:10.956257 139741626332928 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1567.606980
I0329 22:43:10.986104 139926464792384 checkpoints.py:356] Saving checkpoint at step: 6000
I0329 22:43:11.136496 139926464792384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_6000
I0329 22:43:11.136768 139926464792384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/ogbg_jax/trial_1/checkpoint_6000.
I0329 22:43:11.299611 139926464792384 submission_runner.py:543] Tuning trial 1/1
I0329 22:43:11.299882 139926464792384 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0329 22:43:11.301160 139926464792384 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5896993279457092, 'train/loss': 0.7032931447029114, 'train/mean_average_precision': 0.022585955060818078, 'validation/accuracy': 0.587599515914917, 'validation/loss': 0.7057192921638489, 'validation/mean_average_precision': 0.0254660818910648, 'validation/num_examples': 43793, 'test/accuracy': 0.587836742401123, 'test/loss': 0.7061472535133362, 'test/mean_average_precision': 0.027733035515473833, 'test/num_examples': 43793, 'score': 20.93306851387024, 'total_duration': 21.014333963394165, 'global_step': 1, 'preemption_count': 0}), (928, {'train/accuracy': 0.9867643117904663, 'train/loss': 0.05130469426512718, 'train/mean_average_precision': 0.056781959335915316, 'validation/accuracy': 0.9841402769088745, 'validation/loss': 0.06080959364771843, 'validation/mean_average_precision': 0.056217195213032915, 'validation/num_examples': 43793, 'test/accuracy': 0.9831715822219849, 'test/loss': 0.06409144401550293, 'test/mean_average_precision': 0.05644631758445601, 'test/num_examples': 43793, 'score': 259.9373667240143, 'total_duration': 505.0871949195862, 'global_step': 928, 'preemption_count': 0}), (1850, {'train/accuracy': 0.9870824217796326, 'train/loss': 0.048014700412750244, 'train/mean_average_precision': 0.09593947050303112, 'validation/accuracy': 0.9844337701797485, 'validation/loss': 0.05759074538946152, 'validation/mean_average_precision': 0.09330843481639035, 'validation/num_examples': 43793, 'test/accuracy': 0.9834406971931458, 'test/loss': 0.06116626039147377, 'test/mean_average_precision': 0.09210515573616007, 'test/num_examples': 43793, 'score': 499.1563377380371, 'total_duration': 832.5459651947021, 'global_step': 1850, 'preemption_count': 0}), (2781, {'train/accuracy': 0.9877657294273376, 'train/loss': 0.043688155710697174, 'train/mean_average_precision': 0.14669934079485614, 'validation/accuracy': 0.9849773049354553, 'validation/loss': 0.05297050625085831, 'validation/mean_average_precision': 0.1358451147114, 'validation/num_examples': 43793, 'test/accuracy': 0.9839979410171509, 'test/loss': 0.05594930425286293, 'test/mean_average_precision': 0.13199371071907537, 'test/num_examples': 43793, 'score': 738.3789565563202, 'total_duration': 1159.4385750293732, 'global_step': 2781, 'preemption_count': 0}), (3715, {'train/accuracy': 0.9876130223274231, 'train/loss': 0.04355683922767639, 'train/mean_average_precision': 0.1671618282028592, 'validation/accuracy': 0.9848977327346802, 'validation/loss': 0.05337638780474663, 'validation/mean_average_precision': 0.1593370712752889, 'validation/num_examples': 43793, 'test/accuracy': 0.9839145541191101, 'test/loss': 0.05651284009218216, 'test/mean_average_precision': 0.15114956285303616, 'test/num_examples': 43793, 'score': 977.4865691661835, 'total_duration': 1486.0085129737854, 'global_step': 3715, 'preemption_count': 0}), (4637, {'train/accuracy': 0.9883093237876892, 'train/loss': 0.04057011753320694, 'train/mean_average_precision': 0.2030987848011287, 'validation/accuracy': 0.9854128956794739, 'validation/loss': 0.0500301718711853, 'validation/mean_average_precision': 0.17348212072133162, 'validation/num_examples': 43793, 'test/accuracy': 0.9845522046089172, 'test/loss': 0.052670590579509735, 'test/mean_average_precision': 0.17384140029345962, 'test/num_examples': 43793, 'score': 1216.5513696670532, 'total_duration': 1812.698534488678, 'global_step': 4637, 'preemption_count': 0}), (5555, {'train/accuracy': 0.9886943697929382, 'train/loss': 0.03891220688819885, 'train/mean_average_precision': 0.21823091108761622, 'validation/accuracy': 0.985671877861023, 'validation/loss': 0.04863852635025978, 'validation/mean_average_precision': 0.1876559631414037, 'validation/num_examples': 43793, 'test/accuracy': 0.9848407506942749, 'test/loss': 0.05141185224056244, 'test/mean_average_precision': 0.18953209816513975, 'test/num_examples': 43793, 'score': 1455.5678181648254, 'total_duration': 2138.6052055358887, 'global_step': 5555, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9888890981674194, 'train/loss': 0.0383332334458828, 'train/mean_average_precision': 0.25940115526258656, 'validation/accuracy': 0.9858484864234924, 'validation/loss': 0.0481545627117157, 'validation/mean_average_precision': 0.20237831814766927, 'validation/num_examples': 43793, 'test/accuracy': 0.9849528074264526, 'test/loss': 0.05082980543375015, 'test/mean_average_precision': 0.20345178525362712, 'test/num_examples': 43793, 'score': 1567.6069803237915, 'total_duration': 2336.2368416786194, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0329 22:43:11.301279 139926464792384 submission_runner.py:546] Timing: 1567.6069803237915
I0329 22:43:11.301326 139926464792384 submission_runner.py:547] ====================
I0329 22:43:11.301430 139926464792384 submission_runner.py:606] Final ogbg score: 1567.6069803237915
