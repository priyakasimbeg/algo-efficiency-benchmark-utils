WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 09:18:15.688686 140376925730624 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 09:18:15.688812 140250019940160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 09:18:15.688815 139682086098752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 09:18:15.689726 140178237503296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 09:18:15.689764 140012301289280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 09:18:15.690168 140075654059840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 09:18:15.690190 139846161741632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 09:18:15.690863 139784838604608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 09:18:15.691235 139784838604608 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.699400 140376925730624 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.699560 139682086098752 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.699582 140250019940160 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.700323 140178237503296 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.700544 140012301289280 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.700773 139846161741632 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.700803 140075654059840 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 09:18:15.709711 139846161741632 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/criteo1tb_pytorch.
W0401 09:18:16.045770 140376925730624 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.045969 139846161741632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.046450 139784838604608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.046525 140178237503296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.046659 140250019940160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.046810 140012301289280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.047035 140075654059840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 09:18:16.048431 139682086098752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 09:18:16.051317 139846161741632 submission_runner.py:504] Using RNG seed 1288752398
I0401 09:18:16.052284 139846161741632 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 09:18:16.052414 139846161741632 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1.
I0401 09:18:16.052613 139846161741632 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/hparams.json.
I0401 09:18:16.053482 139846161741632 submission_runner.py:230] Starting train once: RAM USED (GB) 5.611433984
I0401 09:18:16.053580 139846161741632 submission_runner.py:231] Initializing dataset.
I0401 09:18:16.053745 139846161741632 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.611433984
I0401 09:18:16.053802 139846161741632 submission_runner.py:240] Initializing model.
I0401 09:18:30.758622 139846161741632 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.192223744
I0401 09:18:30.758799 139846161741632 submission_runner.py:252] Initializing optimizer.
I0401 09:18:31.289408 139846161741632 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.19777792
I0401 09:18:31.289589 139846161741632 submission_runner.py:261] Initializing metrics bundle.
I0401 09:18:31.289636 139846161741632 submission_runner.py:275] Initializing checkpoint and logger.
I0401 09:18:31.293361 139846161741632 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 09:18:31.293503 139846161741632 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 09:18:31.962883 139846161741632 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0401 09:18:31.963782 139846161741632 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/flags_0.json.
I0401 09:18:32.004005 139846161741632 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 15.250059264
I0401 09:18:32.005141 139846161741632 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.250059264
I0401 09:18:32.005265 139846161741632 submission_runner.py:312] Starting training loop.
I0401 09:20:59.173058 139846161741632 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 54.03561984
I0401 09:21:02.341331 139763168311040 logging_writer.py:48] [0] global_step=0, grad_norm=6.941428, loss=0.847025
I0401 09:21:02.352040 139846161741632 submission.py:139] 0) loss = 0.847, grad_norm = 6.941
I0401 09:21:02.352491 139846161741632 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 58.1144576
I0401 09:21:02.353061 139846161741632 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 58.1144576
I0401 09:21:02.353185 139846161741632 spec.py:298] Evaluating on the training split.
I0401 09:30:53.746406 139846161741632 spec.py:310] Evaluating on the validation split.
I0401 09:36:01.434045 139846161741632 spec.py:326] Evaluating on the test split.
I0401 09:39:41.191144 139846161741632 submission_runner.py:380] Time since start: 150.35s, 	Step: 1, 	{'train/loss': 0.8470995231443157, 'validation/loss': 0.8461043595505618, 'validation/num_examples': 89000000, 'test/loss': 0.847177233551787, 'test/num_examples': 89274637}
I0401 09:39:41.191598 139846161741632 submission_runner.py:390] After eval at step 1: RAM USED (GB) 106.204639232
I0401 09:39:41.206657 139707845379840 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=150.346351, test/loss=0.847177, test/num_examples=89274637, total_duration=150.348445, train/loss=0.847100, validation/loss=0.846104, validation/num_examples=89000000
I0401 09:39:50.492713 139846161741632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_1.
I0401 09:39:50.493239 139846161741632 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 106.394877952
I0401 09:39:50.512850 139846161741632 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 106.35485184
I0401 09:39:50.516143 139846161741632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516147 140376925730624 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516154 140012301289280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516156 139682086098752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516149 140250019940160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516156 140075654059840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516165 139784838604608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:50.516159 140178237503296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 09:39:52.231125 139707836987136 logging_writer.py:48] [1] global_step=1, grad_norm=6.935261, loss=0.847039
I0401 09:39:52.241255 139846161741632 submission.py:139] 1) loss = 0.847, grad_norm = 6.935
I0401 09:39:52.241740 139846161741632 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 106.446094336
I0401 09:39:53.963541 139707845379840 logging_writer.py:48] [2] global_step=2, grad_norm=6.472318, loss=0.773108
I0401 09:39:53.966933 139846161741632 submission.py:139] 2) loss = 0.773, grad_norm = 6.472
I0401 09:39:55.650230 139707836987136 logging_writer.py:48] [3] global_step=3, grad_norm=5.210631, loss=0.597655
I0401 09:39:55.653685 139846161741632 submission.py:139] 3) loss = 0.598, grad_norm = 5.211
I0401 09:39:57.355573 139707845379840 logging_writer.py:48] [4] global_step=4, grad_norm=3.451173, loss=0.396282
I0401 09:39:57.358935 139846161741632 submission.py:139] 4) loss = 0.396, grad_norm = 3.451
I0401 09:39:59.093441 139707836987136 logging_writer.py:48] [5] global_step=5, grad_norm=2.004943, loss=0.251504
I0401 09:39:59.096714 139846161741632 submission.py:139] 5) loss = 0.252, grad_norm = 2.005
I0401 09:40:00.796521 139707845379840 logging_writer.py:48] [6] global_step=6, grad_norm=0.700297, loss=0.177814
I0401 09:40:00.799717 139846161741632 submission.py:139] 6) loss = 0.178, grad_norm = 0.700
I0401 09:40:02.529680 139707836987136 logging_writer.py:48] [7] global_step=7, grad_norm=0.324261, loss=0.166572
I0401 09:40:02.533198 139846161741632 submission.py:139] 7) loss = 0.167, grad_norm = 0.324
I0401 09:40:04.253714 139707845379840 logging_writer.py:48] [8] global_step=8, grad_norm=0.964147, loss=0.196371
I0401 09:40:04.257460 139846161741632 submission.py:139] 8) loss = 0.196, grad_norm = 0.964
I0401 09:40:05.961842 139707836987136 logging_writer.py:48] [9] global_step=9, grad_norm=1.459389, loss=0.249605
I0401 09:40:05.965198 139846161741632 submission.py:139] 9) loss = 0.250, grad_norm = 1.459
I0401 09:40:07.643992 139707845379840 logging_writer.py:48] [10] global_step=10, grad_norm=1.827510, loss=0.302404
I0401 09:40:07.647488 139846161741632 submission.py:139] 10) loss = 0.302, grad_norm = 1.828
I0401 09:40:09.364704 139707836987136 logging_writer.py:48] [11] global_step=11, grad_norm=2.006459, loss=0.328274
I0401 09:40:09.370342 139846161741632 submission.py:139] 11) loss = 0.328, grad_norm = 2.006
I0401 09:40:11.148040 139707845379840 logging_writer.py:48] [12] global_step=12, grad_norm=2.029692, loss=0.328129
I0401 09:40:11.151287 139846161741632 submission.py:139] 12) loss = 0.328, grad_norm = 2.030
I0401 09:40:12.919067 139707836987136 logging_writer.py:48] [13] global_step=13, grad_norm=1.777179, loss=0.286293
I0401 09:40:12.926076 139846161741632 submission.py:139] 13) loss = 0.286, grad_norm = 1.777
I0401 09:40:14.699688 139707845379840 logging_writer.py:48] [14] global_step=14, grad_norm=1.322952, loss=0.221674
I0401 09:40:14.703165 139846161741632 submission.py:139] 14) loss = 0.222, grad_norm = 1.323
I0401 09:40:16.491324 139707836987136 logging_writer.py:48] [15] global_step=15, grad_norm=0.652071, loss=0.164231
I0401 09:40:16.497936 139846161741632 submission.py:139] 15) loss = 0.164, grad_norm = 0.652
I0401 09:40:18.188989 139707845379840 logging_writer.py:48] [16] global_step=16, grad_norm=0.292842, loss=0.148980
I0401 09:40:18.193988 139846161741632 submission.py:139] 16) loss = 0.149, grad_norm = 0.293
I0401 09:40:19.891351 139707845379840 logging_writer.py:48] [17] global_step=17, grad_norm=0.994494, loss=0.180108
I0401 09:40:19.907369 139846161741632 submission.py:139] 17) loss = 0.180, grad_norm = 0.994
I0401 09:40:21.630458 139707836987136 logging_writer.py:48] [18] global_step=18, grad_norm=1.229134, loss=0.198688
I0401 09:40:21.633514 139846161741632 submission.py:139] 18) loss = 0.199, grad_norm = 1.229
I0401 09:40:23.335609 139707845379840 logging_writer.py:48] [19] global_step=19, grad_norm=1.104653, loss=0.183422
I0401 09:40:23.338953 139846161741632 submission.py:139] 19) loss = 0.183, grad_norm = 1.105
I0401 09:40:25.047749 139707836987136 logging_writer.py:48] [20] global_step=20, grad_norm=0.538737, loss=0.153064
I0401 09:40:25.053886 139846161741632 submission.py:139] 20) loss = 0.153, grad_norm = 0.539
I0401 09:40:26.798521 139707845379840 logging_writer.py:48] [21] global_step=21, grad_norm=0.257212, loss=0.146785
I0401 09:40:26.801964 139846161741632 submission.py:139] 21) loss = 0.147, grad_norm = 0.257
I0401 09:40:28.539949 139707836987136 logging_writer.py:48] [22] global_step=22, grad_norm=0.810849, loss=0.169089
I0401 09:40:28.543137 139846161741632 submission.py:139] 22) loss = 0.169, grad_norm = 0.811
I0401 09:40:30.237618 139707845379840 logging_writer.py:48] [23] global_step=23, grad_norm=1.048965, loss=0.186652
I0401 09:40:30.245755 139846161741632 submission.py:139] 23) loss = 0.187, grad_norm = 1.049
I0401 09:40:31.961209 139707836987136 logging_writer.py:48] [24] global_step=24, grad_norm=1.074284, loss=0.191789
I0401 09:40:31.982047 139846161741632 submission.py:139] 24) loss = 0.192, grad_norm = 1.074
I0401 09:40:33.658122 139707845379840 logging_writer.py:48] [25] global_step=25, grad_norm=0.820199, loss=0.170052
I0401 09:40:33.669707 139846161741632 submission.py:139] 25) loss = 0.170, grad_norm = 0.820
I0401 09:40:35.408659 139707836987136 logging_writer.py:48] [26] global_step=26, grad_norm=0.350850, loss=0.146866
I0401 09:40:35.419410 139846161741632 submission.py:139] 26) loss = 0.147, grad_norm = 0.351
I0401 09:40:37.197153 139707845379840 logging_writer.py:48] [27] global_step=27, grad_norm=0.212606, loss=0.143508
I0401 09:40:37.210406 139846161741632 submission.py:139] 27) loss = 0.144, grad_norm = 0.213
I0401 09:40:38.938770 139707836987136 logging_writer.py:48] [28] global_step=28, grad_norm=0.600311, loss=0.156751
I0401 09:40:38.962657 139846161741632 submission.py:139] 28) loss = 0.157, grad_norm = 0.600
I0401 09:40:40.649701 139707845379840 logging_writer.py:48] [29] global_step=29, grad_norm=0.728377, loss=0.166019
I0401 09:40:40.693073 139846161741632 submission.py:139] 29) loss = 0.166, grad_norm = 0.728
I0401 09:40:42.389323 139707836987136 logging_writer.py:48] [30] global_step=30, grad_norm=0.659824, loss=0.164128
I0401 09:40:42.403917 139846161741632 submission.py:139] 30) loss = 0.164, grad_norm = 0.660
I0401 09:40:44.139553 139707845379840 logging_writer.py:48] [31] global_step=31, grad_norm=0.448089, loss=0.153672
I0401 09:40:44.153234 139846161741632 submission.py:139] 31) loss = 0.154, grad_norm = 0.448
I0401 09:40:45.856292 139707836987136 logging_writer.py:48] [32] global_step=32, grad_norm=0.141921, loss=0.142801
I0401 09:40:45.871886 139846161741632 submission.py:139] 32) loss = 0.143, grad_norm = 0.142
I0401 09:40:47.566361 139707845379840 logging_writer.py:48] [33] global_step=33, grad_norm=0.209537, loss=0.143548
I0401 09:40:47.576730 139846161741632 submission.py:139] 33) loss = 0.144, grad_norm = 0.210
I0401 09:40:49.293512 139707836987136 logging_writer.py:48] [34] global_step=34, grad_norm=0.464910, loss=0.153275
I0401 09:40:49.305312 139846161741632 submission.py:139] 34) loss = 0.153, grad_norm = 0.465
I0401 09:40:50.989563 139707845379840 logging_writer.py:48] [35] global_step=35, grad_norm=0.537927, loss=0.154036
I0401 09:40:51.007229 139846161741632 submission.py:139] 35) loss = 0.154, grad_norm = 0.538
I0401 09:40:52.704236 139707836987136 logging_writer.py:48] [36] global_step=36, grad_norm=0.503220, loss=0.153932
I0401 09:40:52.746746 139846161741632 submission.py:139] 36) loss = 0.154, grad_norm = 0.503
I0401 09:40:54.509801 139707845379840 logging_writer.py:48] [37] global_step=37, grad_norm=0.350423, loss=0.148075
I0401 09:40:54.524311 139846161741632 submission.py:139] 37) loss = 0.148, grad_norm = 0.350
I0401 09:40:56.204299 139707845379840 logging_writer.py:48] [38] global_step=38, grad_norm=0.114825, loss=0.140079
I0401 09:40:56.227637 139846161741632 submission.py:139] 38) loss = 0.140, grad_norm = 0.115
I0401 09:40:57.927936 139707836987136 logging_writer.py:48] [39] global_step=39, grad_norm=0.098387, loss=0.141932
I0401 09:40:57.939816 139846161741632 submission.py:139] 39) loss = 0.142, grad_norm = 0.098
I0401 09:40:59.686776 139707845379840 logging_writer.py:48] [40] global_step=40, grad_norm=0.232128, loss=0.145298
I0401 09:40:59.705651 139846161741632 submission.py:139] 40) loss = 0.145, grad_norm = 0.232
I0401 09:41:01.488974 139707836987136 logging_writer.py:48] [41] global_step=41, grad_norm=0.300966, loss=0.146305
I0401 09:41:01.502042 139846161741632 submission.py:139] 41) loss = 0.146, grad_norm = 0.301
I0401 09:41:03.197836 139707845379840 logging_writer.py:48] [42] global_step=42, grad_norm=0.290781, loss=0.146990
I0401 09:41:03.206403 139846161741632 submission.py:139] 42) loss = 0.147, grad_norm = 0.291
I0401 09:41:04.907569 139707836987136 logging_writer.py:48] [43] global_step=43, grad_norm=0.236261, loss=0.142905
I0401 09:41:04.935417 139846161741632 submission.py:139] 43) loss = 0.143, grad_norm = 0.236
I0401 09:41:06.723632 139707845379840 logging_writer.py:48] [44] global_step=44, grad_norm=0.136097, loss=0.139744
I0401 09:41:06.730417 139846161741632 submission.py:139] 44) loss = 0.140, grad_norm = 0.136
I0401 09:41:08.439644 139707836987136 logging_writer.py:48] [45] global_step=45, grad_norm=0.027313, loss=0.140078
I0401 09:41:08.453758 139846161741632 submission.py:139] 45) loss = 0.140, grad_norm = 0.027
I0401 09:41:10.157499 139707845379840 logging_writer.py:48] [46] global_step=46, grad_norm=0.117185, loss=0.141358
I0401 09:41:10.177621 139846161741632 submission.py:139] 46) loss = 0.141, grad_norm = 0.117
I0401 09:41:11.914003 139707836987136 logging_writer.py:48] [47] global_step=47, grad_norm=0.199291, loss=0.142698
I0401 09:41:11.924357 139846161741632 submission.py:139] 47) loss = 0.143, grad_norm = 0.199
I0401 09:41:13.633562 139707845379840 logging_writer.py:48] [48] global_step=48, grad_norm=0.245464, loss=0.145906
I0401 09:41:13.650857 139846161741632 submission.py:139] 48) loss = 0.146, grad_norm = 0.245
I0401 09:41:15.357824 139707836987136 logging_writer.py:48] [49] global_step=49, grad_norm=0.227811, loss=0.144299
I0401 09:41:15.374887 139846161741632 submission.py:139] 49) loss = 0.144, grad_norm = 0.228
I0401 09:41:17.110132 139707845379840 logging_writer.py:48] [50] global_step=50, grad_norm=0.166021, loss=0.140957
I0401 09:41:17.123744 139846161741632 submission.py:139] 50) loss = 0.141, grad_norm = 0.166
I0401 09:41:18.795518 139707836987136 logging_writer.py:48] [51] global_step=51, grad_norm=0.087577, loss=0.140212
I0401 09:41:18.806087 139846161741632 submission.py:139] 51) loss = 0.140, grad_norm = 0.088
I0401 09:41:20.492546 139707845379840 logging_writer.py:48] [52] global_step=52, grad_norm=0.028978, loss=0.136472
I0401 09:41:20.501211 139846161741632 submission.py:139] 52) loss = 0.136, grad_norm = 0.029
I0401 09:41:22.230454 139707836987136 logging_writer.py:48] [53] global_step=53, grad_norm=0.096481, loss=0.137349
I0401 09:41:22.240565 139846161741632 submission.py:139] 53) loss = 0.137, grad_norm = 0.096
I0401 09:41:23.981493 139707845379840 logging_writer.py:48] [54] global_step=54, grad_norm=0.138710, loss=0.140421
I0401 09:41:23.991098 139846161741632 submission.py:139] 54) loss = 0.140, grad_norm = 0.139
I0401 09:41:25.670608 139707836987136 logging_writer.py:48] [55] global_step=55, grad_norm=0.157581, loss=0.141382
I0401 09:41:25.683374 139846161741632 submission.py:139] 55) loss = 0.141, grad_norm = 0.158
I0401 09:41:27.384012 139707845379840 logging_writer.py:48] [56] global_step=56, grad_norm=0.153040, loss=0.138971
I0401 09:41:27.390787 139846161741632 submission.py:139] 56) loss = 0.139, grad_norm = 0.153
I0401 09:41:29.114384 139707836987136 logging_writer.py:48] [57] global_step=57, grad_norm=0.104890, loss=0.140269
I0401 09:41:29.137752 139846161741632 submission.py:139] 57) loss = 0.140, grad_norm = 0.105
I0401 09:41:30.834850 139707845379840 logging_writer.py:48] [58] global_step=58, grad_norm=0.052168, loss=0.138126
I0401 09:41:30.848541 139846161741632 submission.py:139] 58) loss = 0.138, grad_norm = 0.052
I0401 09:41:32.558821 139707836987136 logging_writer.py:48] [59] global_step=59, grad_norm=0.033874, loss=0.140117
I0401 09:41:32.582246 139846161741632 submission.py:139] 59) loss = 0.140, grad_norm = 0.034
I0401 09:41:34.331783 139707845379840 logging_writer.py:48] [60] global_step=60, grad_norm=0.090909, loss=0.140762
I0401 09:41:34.346090 139846161741632 submission.py:139] 60) loss = 0.141, grad_norm = 0.091
I0401 09:41:36.050309 139707836987136 logging_writer.py:48] [61] global_step=61, grad_norm=0.130737, loss=0.141507
I0401 09:41:36.063777 139846161741632 submission.py:139] 61) loss = 0.142, grad_norm = 0.131
I0401 09:41:37.797843 139707845379840 logging_writer.py:48] [62] global_step=62, grad_norm=0.131750, loss=0.139416
I0401 09:41:37.802995 139846161741632 submission.py:139] 62) loss = 0.139, grad_norm = 0.132
I0401 09:41:39.586464 139707836987136 logging_writer.py:48] [63] global_step=63, grad_norm=0.109120, loss=0.139858
I0401 09:41:39.595761 139846161741632 submission.py:139] 63) loss = 0.140, grad_norm = 0.109
I0401 09:41:41.288691 139707845379840 logging_writer.py:48] [64] global_step=64, grad_norm=0.057812, loss=0.137850
I0401 09:41:41.295442 139846161741632 submission.py:139] 64) loss = 0.138, grad_norm = 0.058
I0401 09:41:42.984889 139707836987136 logging_writer.py:48] [65] global_step=65, grad_norm=0.012358, loss=0.137120
I0401 09:41:42.989995 139846161741632 submission.py:139] 65) loss = 0.137, grad_norm = 0.012
I0401 09:41:44.705187 139707845379840 logging_writer.py:48] [66] global_step=66, grad_norm=0.058424, loss=0.137337
I0401 09:41:44.726218 139846161741632 submission.py:139] 66) loss = 0.137, grad_norm = 0.058
I0401 09:41:46.420168 139707836987136 logging_writer.py:48] [67] global_step=67, grad_norm=0.094588, loss=0.138303
I0401 09:41:46.430860 139846161741632 submission.py:139] 67) loss = 0.138, grad_norm = 0.095
I0401 09:41:48.117186 139707845379840 logging_writer.py:48] [68] global_step=68, grad_norm=0.106931, loss=0.137605
I0401 09:41:48.133606 139846161741632 submission.py:139] 68) loss = 0.138, grad_norm = 0.107
I0401 09:41:49.895686 139707836987136 logging_writer.py:48] [69] global_step=69, grad_norm=0.087038, loss=0.139112
I0401 09:41:49.912835 139846161741632 submission.py:139] 69) loss = 0.139, grad_norm = 0.087
I0401 09:41:51.707568 139707845379840 logging_writer.py:48] [70] global_step=70, grad_norm=0.043857, loss=0.139302
I0401 09:41:51.737049 139846161741632 submission.py:139] 70) loss = 0.139, grad_norm = 0.044
I0401 09:41:53.471906 139707836987136 logging_writer.py:48] [71] global_step=71, grad_norm=0.010041, loss=0.137661
I0401 09:41:53.477360 139846161741632 submission.py:139] 71) loss = 0.138, grad_norm = 0.010
I0401 09:41:55.172383 139707845379840 logging_writer.py:48] [72] global_step=72, grad_norm=0.052238, loss=0.137549
I0401 09:41:55.186635 139846161741632 submission.py:139] 72) loss = 0.138, grad_norm = 0.052
I0401 09:41:56.878119 139707836987136 logging_writer.py:48] [73] global_step=73, grad_norm=0.076178, loss=0.136123
I0401 09:41:56.910076 139846161741632 submission.py:139] 73) loss = 0.136, grad_norm = 0.076
I0401 09:41:58.635539 139707845379840 logging_writer.py:48] [74] global_step=74, grad_norm=0.095741, loss=0.138376
I0401 09:41:58.647866 139846161741632 submission.py:139] 74) loss = 0.138, grad_norm = 0.096
I0401 09:42:00.359168 139707836987136 logging_writer.py:48] [75] global_step=75, grad_norm=0.067513, loss=0.137577
I0401 09:42:00.371049 139846161741632 submission.py:139] 75) loss = 0.138, grad_norm = 0.068
I0401 09:42:02.041107 139707845379840 logging_writer.py:48] [76] global_step=76, grad_norm=0.019680, loss=0.136065
I0401 09:42:02.048439 139846161741632 submission.py:139] 76) loss = 0.136, grad_norm = 0.020
I0401 09:42:03.800006 139707836987136 logging_writer.py:48] [77] global_step=77, grad_norm=0.019313, loss=0.138430
I0401 09:42:03.810851 139846161741632 submission.py:139] 77) loss = 0.138, grad_norm = 0.019
I0401 09:42:05.481979 139707845379840 logging_writer.py:48] [78] global_step=78, grad_norm=0.060365, loss=0.137931
I0401 09:42:05.490754 139846161741632 submission.py:139] 78) loss = 0.138, grad_norm = 0.060
I0401 09:42:07.176697 139707836987136 logging_writer.py:48] [79] global_step=79, grad_norm=0.075814, loss=0.138838
I0401 09:42:07.192965 139846161741632 submission.py:139] 79) loss = 0.139, grad_norm = 0.076
I0401 09:42:08.931828 139707845379840 logging_writer.py:48] [80] global_step=80, grad_norm=0.070651, loss=0.136875
I0401 09:42:08.961153 139846161741632 submission.py:139] 80) loss = 0.137, grad_norm = 0.071
I0401 09:42:10.652144 139707836987136 logging_writer.py:48] [81] global_step=81, grad_norm=0.033754, loss=0.137586
I0401 09:42:10.678557 139846161741632 submission.py:139] 81) loss = 0.138, grad_norm = 0.034
I0401 09:42:12.370696 139707845379840 logging_writer.py:48] [82] global_step=82, grad_norm=0.018773, loss=0.138777
I0401 09:42:12.386762 139846161741632 submission.py:139] 82) loss = 0.139, grad_norm = 0.019
I0401 09:42:14.156973 139707836987136 logging_writer.py:48] [83] global_step=83, grad_norm=0.050036, loss=0.138275
I0401 09:42:14.160068 139846161741632 submission.py:139] 83) loss = 0.138, grad_norm = 0.050
I0401 09:42:15.867383 139707845379840 logging_writer.py:48] [84] global_step=84, grad_norm=0.063357, loss=0.137556
I0401 09:42:15.871475 139846161741632 submission.py:139] 84) loss = 0.138, grad_norm = 0.063
I0401 09:42:17.577766 139707836987136 logging_writer.py:48] [85] global_step=85, grad_norm=0.056289, loss=0.137578
I0401 09:42:17.581592 139846161741632 submission.py:139] 85) loss = 0.138, grad_norm = 0.056
I0401 09:42:19.273949 139707845379840 logging_writer.py:48] [86] global_step=86, grad_norm=0.019123, loss=0.135678
I0401 09:42:19.277810 139846161741632 submission.py:139] 86) loss = 0.136, grad_norm = 0.019
I0401 09:42:20.976930 139707836987136 logging_writer.py:48] [87] global_step=87, grad_norm=0.021286, loss=0.134227
I0401 09:42:20.980388 139846161741632 submission.py:139] 87) loss = 0.134, grad_norm = 0.021
I0401 09:42:22.682970 139707845379840 logging_writer.py:48] [88] global_step=88, grad_norm=0.033673, loss=0.137595
I0401 09:42:22.686465 139846161741632 submission.py:139] 88) loss = 0.138, grad_norm = 0.034
I0401 09:42:24.384841 139707836987136 logging_writer.py:48] [89] global_step=89, grad_norm=0.047773, loss=0.135936
I0401 09:42:24.388709 139846161741632 submission.py:139] 89) loss = 0.136, grad_norm = 0.048
I0401 09:42:26.104712 139707845379840 logging_writer.py:48] [90] global_step=90, grad_norm=0.042319, loss=0.134650
I0401 09:42:26.107927 139846161741632 submission.py:139] 90) loss = 0.135, grad_norm = 0.042
I0401 09:42:27.814596 139707836987136 logging_writer.py:48] [91] global_step=91, grad_norm=0.012839, loss=0.135863
I0401 09:42:27.818460 139846161741632 submission.py:139] 91) loss = 0.136, grad_norm = 0.013
I0401 09:42:29.556422 139707845379840 logging_writer.py:48] [92] global_step=92, grad_norm=0.019370, loss=0.136243
I0401 09:42:29.559916 139846161741632 submission.py:139] 92) loss = 0.136, grad_norm = 0.019
I0401 09:42:31.258529 139707836987136 logging_writer.py:48] [93] global_step=93, grad_norm=0.043461, loss=0.137344
I0401 09:42:31.261561 139846161741632 submission.py:139] 93) loss = 0.137, grad_norm = 0.043
I0401 09:42:32.944454 139707845379840 logging_writer.py:48] [94] global_step=94, grad_norm=0.036170, loss=0.136077
I0401 09:42:32.948521 139846161741632 submission.py:139] 94) loss = 0.136, grad_norm = 0.036
I0401 09:42:34.711956 139707836987136 logging_writer.py:48] [95] global_step=95, grad_norm=0.031037, loss=0.137955
I0401 09:42:34.715526 139846161741632 submission.py:139] 95) loss = 0.138, grad_norm = 0.031
I0401 09:42:36.429215 139707845379840 logging_writer.py:48] [96] global_step=96, grad_norm=0.004512, loss=0.136710
I0401 09:42:36.432427 139846161741632 submission.py:139] 96) loss = 0.137, grad_norm = 0.005
I0401 09:42:38.117279 139707836987136 logging_writer.py:48] [97] global_step=97, grad_norm=0.023622, loss=0.135994
I0401 09:42:38.120340 139846161741632 submission.py:139] 97) loss = 0.136, grad_norm = 0.024
I0401 09:42:39.883001 139707845379840 logging_writer.py:48] [98] global_step=98, grad_norm=0.028167, loss=0.137831
I0401 09:42:39.886085 139846161741632 submission.py:139] 98) loss = 0.138, grad_norm = 0.028
I0401 09:42:41.656691 139707836987136 logging_writer.py:48] [99] global_step=99, grad_norm=0.034381, loss=0.136087
I0401 09:42:41.660645 139846161741632 submission.py:139] 99) loss = 0.136, grad_norm = 0.034
I0401 09:42:43.362264 139707845379840 logging_writer.py:48] [100] global_step=100, grad_norm=0.021947, loss=0.135667
I0401 09:42:43.365448 139846161741632 submission.py:139] 100) loss = 0.136, grad_norm = 0.022
I0401 09:48:51.601349 139846161741632 submission_runner.py:371] Before eval at step 317: RAM USED (GB) 114.536398848
I0401 09:48:51.601550 139846161741632 spec.py:298] Evaluating on the training split.
I0401 09:59:03.539686 139846161741632 spec.py:310] Evaluating on the validation split.
I0401 10:04:10.001762 139846161741632 spec.py:326] Evaluating on the test split.
I0401 10:09:12.816586 139846161741632 submission_runner.py:380] Time since start: 1819.52s, 	Step: 317, 	{'train/loss': 0.13747886000879553, 'validation/loss': 0.13697408988764045, 'validation/num_examples': 89000000, 'test/loss': 0.14039228185268343, 'test/num_examples': 89274637}
I0401 10:09:12.816998 139846161741632 submission_runner.py:390] After eval at step 317: RAM USED (GB) 117.520252928
I0401 10:09:12.825165 139746092750592 logging_writer.py:48] [317] global_step=317, preemption_count=0, score=673.900944, test/loss=0.140392, test/num_examples=89274637, total_duration=1819.521053, train/loss=0.137479, validation/loss=0.136974, validation/num_examples=89000000
I0401 10:09:22.122923 139846161741632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_317.
I0401 10:09:22.123364 139846161741632 submission_runner.py:409] After logging and checkpointing eval at step 317: RAM USED (GB) 117.533560832
I0401 10:14:37.935155 139746084357888 logging_writer.py:48] [500] global_step=500, grad_norm=0.021254, loss=0.135956
I0401 10:14:37.938982 139846161741632 submission.py:139] 500) loss = 0.136, grad_norm = 0.021
I0401 10:18:22.437958 139846161741632 submission_runner.py:371] Before eval at step 633: RAM USED (GB) 119.927967744
I0401 10:18:22.438162 139846161741632 spec.py:298] Evaluating on the training split.
I0401 10:28:25.657049 139846161741632 spec.py:310] Evaluating on the validation split.
I0401 10:33:23.855292 139846161741632 spec.py:326] Evaluating on the test split.
I0401 10:37:21.883943 139846161741632 submission_runner.py:380] Time since start: 3590.35s, 	Step: 633, 	{'train/loss': 0.13505708022184282, 'validation/loss': 0.13526696629213483, 'validation/num_examples': 89000000, 'test/loss': 0.138772359276017, 'test/num_examples': 89274637}
I0401 10:37:21.884406 139846161741632 submission_runner.py:390] After eval at step 633: RAM USED (GB) 121.81399552
I0401 10:37:21.894560 139746092750592 logging_writer.py:48] [633] global_step=633, preemption_count=0, score=1188.790069, test/loss=0.138772, test/num_examples=89274637, total_duration=3590.354023, train/loss=0.135057, validation/loss=0.135267, validation/num_examples=89000000
I0401 10:37:31.148426 139846161741632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_633.
I0401 10:37:31.148886 139846161741632 submission_runner.py:409] After logging and checkpointing eval at step 633: RAM USED (GB) 121.792253952
I0401 10:42:14.208145 139846161741632 submission_runner.py:371] Before eval at step 800: RAM USED (GB) 122.608529408
I0401 10:42:14.208359 139846161741632 spec.py:298] Evaluating on the training split.
I0401 10:52:03.090743 139846161741632 spec.py:310] Evaluating on the validation split.
I0401 10:57:05.161026 139846161741632 spec.py:326] Evaluating on the test split.
I0401 11:01:53.237789 139846161741632 submission_runner.py:380] Time since start: 5022.13s, 	Step: 800, 	{'train/loss': 0.13445863029025248, 'validation/loss': 0.13364047191011236, 'validation/num_examples': 89000000, 'test/loss': 0.13692380513403823, 'test/num_examples': 89274637}
I0401 11:01:53.238451 139846161741632 submission_runner.py:390] After eval at step 800: RAM USED (GB) 124.008677376
I0401 11:01:53.248354 139707560191744 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1458.329308, test/loss=0.136924, test/num_examples=89274637, total_duration=5022.127659, train/loss=0.134459, validation/loss=0.133640, validation/num_examples=89000000
I0401 11:02:02.369473 139846161741632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_800.
I0401 11:02:02.369908 139846161741632 submission_runner.py:409] After logging and checkpointing eval at step 800: RAM USED (GB) 124.071010304
I0401 11:02:02.377940 139707551799040 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1458.329308
I0401 11:02:14.188457 139846161741632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_800.
I0401 11:03:53.263944 139846161741632 submission_runner.py:543] Tuning trial 1/1
I0401 11:03:53.264219 139846161741632 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 11:03:53.269466 139846161741632 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/loss': 0.8470995231443157, 'validation/loss': 0.8461043595505618, 'validation/num_examples': 89000000, 'test/loss': 0.847177233551787, 'test/num_examples': 89274637, 'score': 150.34635138511658, 'total_duration': 150.34844493865967, 'global_step': 1, 'preemption_count': 0}), (317, {'train/loss': 0.13747886000879553, 'validation/loss': 0.13697408988764045, 'validation/num_examples': 89000000, 'test/loss': 0.14039228185268343, 'test/num_examples': 89274637, 'score': 673.9009442329407, 'total_duration': 1819.5210528373718, 'global_step': 317, 'preemption_count': 0}), (633, {'train/loss': 0.13505708022184282, 'validation/loss': 0.13526696629213483, 'validation/num_examples': 89000000, 'test/loss': 0.138772359276017, 'test/num_examples': 89274637, 'score': 1188.790069103241, 'total_duration': 3590.3540234565735, 'global_step': 633, 'preemption_count': 0}), (800, {'train/loss': 0.13445863029025248, 'validation/loss': 0.13364047191011236, 'validation/num_examples': 89000000, 'test/loss': 0.13692380513403823, 'test/num_examples': 89274637, 'score': 1458.3293080329895, 'total_duration': 5022.127658605576, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0401 11:03:53.270105 139846161741632 submission_runner.py:546] Timing: 1458.3293080329895
I0401 11:03:53.270170 139846161741632 submission_runner.py:547] ====================
I0401 11:03:53.270251 139846161741632 submission_runner.py:606] Final criteo1tb score: 1458.3293080329895
