I0329 19:41:47.141197 140501021517632 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/imagenet_vit_jax.
I0329 19:41:47.189057 140501021517632 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0329 19:41:48.048992 140501021517632 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0329 19:41:48.049992 140501021517632 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0329 19:41:48.056934 140501021517632 submission_runner.py:504] Using RNG seed 3155114003
I0329 19:41:50.335450 140501021517632 submission_runner.py:513] --- Tuning run 1/1 ---
I0329 19:41:50.335667 140501021517632 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1.
I0329 19:41:50.335873 140501021517632 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/hparams.json.
I0329 19:41:50.472772 140501021517632 submission_runner.py:230] Starting train once: RAM USED (GB) 4.360749056
I0329 19:41:50.472941 140501021517632 submission_runner.py:231] Initializing dataset.
I0329 19:41:50.487936 140501021517632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:41:50.497660 140501021517632 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 19:41:50.497782 140501021517632 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 19:41:50.762461 140501021517632 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:41:58.903332 140501021517632 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.7541248
I0329 19:41:58.903558 140501021517632 submission_runner.py:240] Initializing model.
I0329 19:42:10.679608 140501021517632 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.54081536
I0329 19:42:10.679821 140501021517632 submission_runner.py:252] Initializing optimizer.
I0329 19:42:11.200752 140501021517632 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.54157312
I0329 19:42:11.200923 140501021517632 submission_runner.py:261] Initializing metrics bundle.
I0329 19:42:11.200973 140501021517632 submission_runner.py:275] Initializing checkpoint and logger.
I0329 19:42:11.201889 140501021517632 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0329 19:42:12.036642 140501021517632 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/meta_data_0.json.
I0329 19:42:12.037631 140501021517632 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/flags_0.json.
I0329 19:42:12.041620 140501021517632 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.541220864
I0329 19:42:12.041820 140501021517632 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.541220864
I0329 19:42:12.041884 140501021517632 submission_runner.py:312] Starting training loop.
I0329 19:42:15.142693 140501021517632 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 14.179790848
I0329 19:42:58.669691 140323921585920 logging_writer.py:48] [0] global_step=0, grad_norm=0.29275575280189514, loss=6.9077534675598145
I0329 19:42:58.682961 140501021517632 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 46.540816384
I0329 19:42:58.683303 140501021517632 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 46.540816384
I0329 19:42:58.683415 140501021517632 spec.py:298] Evaluating on the training split.
I0329 19:42:58.691430 140501021517632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:42:58.699437 140501021517632 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 19:42:58.699556 140501021517632 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 19:42:58.775236 140501021517632 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:43:17.531468 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 19:43:17.538789 140501021517632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:43:17.550113 140501021517632 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0329 19:43:17.550503 140501021517632 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0329 19:43:17.620216 140501021517632 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0329 19:43:36.230844 140501021517632 spec.py:326] Evaluating on the test split.
I0329 19:43:36.237875 140501021517632 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 19:43:36.243794 140501021517632 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0329 19:43:36.275724 140501021517632 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0329 19:43:47.658657 140501021517632 submission_runner.py:380] Time since start: 46.64s, 	Step: 1, 	{'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0329 19:43:47.659133 140501021517632 submission_runner.py:390] After eval at step 1: RAM USED (GB) 106.047537152
I0329 19:43:47.668187 140263204833024 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=46.557778, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=46.641414, train/accuracy=0.000898, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0329 19:43:47.793140 140501021517632 checkpoints.py:356] Saving checkpoint at step: 1
I0329 19:43:48.314760 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_1
I0329 19:43:48.320574 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_1.
I0329 19:43:48.322346 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 106.148913152
I0329 19:43:48.326571 140501021517632 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 106.148913152
I0329 19:44:02.571524 140501021517632 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 105.556393984
I0329 19:44:41.309628 140319874041600 logging_writer.py:48] [100] global_step=100, grad_norm=0.2938940227031708, loss=6.905335426330566
I0329 19:45:20.431297 140319882434304 logging_writer.py:48] [200] global_step=200, grad_norm=0.29218676686286926, loss=6.898250102996826
I0329 19:45:59.573124 140319874041600 logging_writer.py:48] [300] global_step=300, grad_norm=0.315937340259552, loss=6.871192455291748
I0329 19:46:38.763382 140319882434304 logging_writer.py:48] [400] global_step=400, grad_norm=0.5151145458221436, loss=6.814515590667725
I0329 19:47:18.366698 140319874041600 logging_writer.py:48] [500] global_step=500, grad_norm=0.6884403228759766, loss=6.764859199523926
I0329 19:47:58.020319 140319882434304 logging_writer.py:48] [600] global_step=600, grad_norm=0.9591318964958191, loss=6.6791791915893555
I0329 19:48:37.743311 140319874041600 logging_writer.py:48] [700] global_step=700, grad_norm=0.7867224812507629, loss=6.657776832580566
I0329 19:49:17.370280 140319882434304 logging_writer.py:48] [800] global_step=800, grad_norm=0.8930429816246033, loss=6.612214088439941
I0329 19:49:57.394685 140319874041600 logging_writer.py:48] [900] global_step=900, grad_norm=0.8326623439788818, loss=6.5450286865234375
I0329 19:50:37.177650 140319882434304 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3154131174087524, loss=6.54464054107666
I0329 19:50:48.480554 140501021517632 submission_runner.py:371] Before eval at step 1030: RAM USED (GB) 79.739281408
I0329 19:50:48.480841 140501021517632 spec.py:298] Evaluating on the training split.
I0329 19:50:59.519030 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 19:51:06.193744 140501021517632 spec.py:326] Evaluating on the test split.
I0329 19:51:08.195584 140501021517632 submission_runner.py:380] Time since start: 516.44s, 	Step: 1030, 	{'train/accuracy': 0.03267578035593033, 'train/loss': 6.112421035766602, 'validation/accuracy': 0.032600000500679016, 'validation/loss': 6.134460926055908, 'validation/num_examples': 50000, 'test/accuracy': 0.027400001883506775, 'test/loss': 6.2289137840271, 'test/num_examples': 10000}
I0329 19:51:08.196206 140501021517632 submission_runner.py:390] After eval at step 1030: RAM USED (GB) 84.242599936
I0329 19:51:08.215223 140263540377344 logging_writer.py:48] [1030] global_step=1030, preemption_count=0, score=459.852943, test/accuracy=0.027400, test/loss=6.228914, test/num_examples=10000, total_duration=516.435807, train/accuracy=0.032676, train/loss=6.112421, validation/accuracy=0.032600, validation/loss=6.134461, validation/num_examples=50000
I0329 19:51:08.695455 140501021517632 checkpoints.py:356] Saving checkpoint at step: 1030
I0329 19:51:10.784782 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_1030
I0329 19:51:10.799235 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_1030.
I0329 19:51:10.805526 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 1030: RAM USED (GB) 94.116749312
I0329 19:51:38.696606 140263548770048 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7264295220375061, loss=6.500598907470703
I0329 19:52:17.986639 140325095958272 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8322719931602478, loss=6.476532459259033
I0329 19:52:57.189463 140263548770048 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0873850584030151, loss=6.451879501342773
I0329 19:53:36.836965 140325095958272 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8491494655609131, loss=6.465823650360107
I0329 19:54:16.747485 140263548770048 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7650423645973206, loss=6.431346893310547
I0329 19:54:56.405279 140325095958272 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7370645999908447, loss=6.544428825378418
I0329 19:55:36.400977 140263548770048 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8616958856582642, loss=6.411882400512695
I0329 19:56:16.415956 140325095958272 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8021266460418701, loss=6.698910236358643
I0329 19:56:56.235244 140263548770048 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.696970522403717, loss=6.351317882537842
I0329 19:57:36.026983 140325095958272 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6710090041160583, loss=6.742898941040039
I0329 19:58:11.107002 140501021517632 submission_runner.py:371] Before eval at step 2089: RAM USED (GB) 86.294114304
I0329 19:58:11.107358 140501021517632 spec.py:298] Evaluating on the training split.
I0329 19:58:22.288267 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 19:58:29.076697 140501021517632 spec.py:326] Evaluating on the test split.
I0329 19:58:30.785498 140501021517632 submission_runner.py:380] Time since start: 959.06s, 	Step: 2089, 	{'train/accuracy': 0.0622265599668026, 'train/loss': 5.656254768371582, 'validation/accuracy': 0.05861999839544296, 'validation/loss': 5.687917232513428, 'validation/num_examples': 50000, 'test/accuracy': 0.047200001776218414, 'test/loss': 5.853387355804443, 'test/num_examples': 10000}
I0329 19:58:30.786139 140501021517632 submission_runner.py:390] After eval at step 2089: RAM USED (GB) 88.114515968
I0329 19:58:30.799319 140263548770048 logging_writer.py:48] [2089] global_step=2089, preemption_count=0, score=872.896991, test/accuracy=0.047200, test/loss=5.853387, test/num_examples=10000, total_duration=959.062541, train/accuracy=0.062227, train/loss=5.656255, validation/accuracy=0.058620, validation/loss=5.687917, validation/num_examples=50000
I0329 19:58:31.034608 140501021517632 checkpoints.py:356] Saving checkpoint at step: 2089
I0329 19:58:34.231878 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_2089
I0329 19:58:34.242718 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_2089.
I0329 19:58:34.245065 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 2089: RAM USED (GB) 100.965359616
I0329 19:58:39.044533 140325095958272 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8294970989227295, loss=6.267498970031738
I0329 19:59:18.273666 140325062387456 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8050050735473633, loss=6.6395792961120605
I0329 19:59:57.421115 140325095958272 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7090469598770142, loss=6.322195053100586
I0329 20:00:36.685447 140325062387456 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8392804861068726, loss=6.226825714111328
I0329 20:01:16.682836 140325095958272 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6297666430473328, loss=6.3816657066345215
I0329 20:01:56.597206 140325062387456 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6906221508979797, loss=6.217487812042236
I0329 20:02:36.763365 140325095958272 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7697783708572388, loss=6.195016860961914
I0329 20:03:16.692815 140325062387456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6333986520767212, loss=6.212149620056152
I0329 20:03:56.580171 140325095958272 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7907912731170654, loss=6.205356597900391
I0329 20:04:36.423208 140325062387456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6308155059814453, loss=6.350812911987305
I0329 20:05:16.467744 140325095958272 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6053466200828552, loss=6.367565631866455
I0329 20:05:34.463504 140501021517632 submission_runner.py:371] Before eval at step 3147: RAM USED (GB) 92.576903168
I0329 20:05:34.463790 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:05:45.757530 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:05:53.203986 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:05:54.865681 140501021517632 submission_runner.py:380] Time since start: 1402.42s, 	Step: 3147, 	{'train/accuracy': 0.0908203125, 'train/loss': 5.2635908126831055, 'validation/accuracy': 0.08574000000953674, 'validation/loss': 5.315304279327393, 'validation/num_examples': 50000, 'test/accuracy': 0.065700002014637, 'test/loss': 5.5355095863342285, 'test/num_examples': 10000}
I0329 20:05:54.866139 140501021517632 submission_runner.py:390] After eval at step 3147: RAM USED (GB) 92.923129856
I0329 20:05:54.879126 140325062387456 logging_writer.py:48] [3147] global_step=3147, preemption_count=0, score=1286.197410, test/accuracy=0.065700, test/loss=5.535510, test/num_examples=10000, total_duration=1402.421193, train/accuracy=0.090820, train/loss=5.263591, validation/accuracy=0.085740, validation/loss=5.315304, validation/num_examples=50000
I0329 20:05:55.855823 140501021517632 checkpoints.py:356] Saving checkpoint at step: 3147
I0329 20:05:57.669590 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_3147
I0329 20:05:57.680073 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_3147.
I0329 20:05:57.682466 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 3147: RAM USED (GB) 103.508553728
I0329 20:06:19.072931 140325095958272 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7242613434791565, loss=6.14231014251709
I0329 20:06:58.292314 140324844324608 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6630712151527405, loss=6.12389612197876
I0329 20:07:37.609683 140325095958272 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.632049024105072, loss=6.108438491821289
I0329 20:08:17.287390 140324844324608 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7208468914031982, loss=6.076663970947266
I0329 20:08:57.268600 140325095958272 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7239275574684143, loss=6.294550895690918
I0329 20:09:37.041398 140324844324608 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7503416538238525, loss=6.126514434814453
I0329 20:10:16.805594 140325095958272 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.72137051820755, loss=6.065453052520752
I0329 20:10:56.709220 140324844324608 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.634975790977478, loss=6.085461616516113
I0329 20:11:36.719993 140325095958272 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6891127824783325, loss=6.036446571350098
I0329 20:12:16.854863 140324844324608 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6130200028419495, loss=6.015451908111572
I0329 20:12:56.906541 140325095958272 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7440482974052429, loss=5.958791255950928
I0329 20:12:57.815435 140501021517632 submission_runner.py:371] Before eval at step 4204: RAM USED (GB) 98.8399616
I0329 20:12:57.815711 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:13:10.344126 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:13:18.850891 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:13:20.514046 140501021517632 submission_runner.py:380] Time since start: 1845.77s, 	Step: 4204, 	{'train/accuracy': 0.1139843687415123, 'train/loss': 5.018521785736084, 'validation/accuracy': 0.10049999505281448, 'validation/loss': 5.094003677368164, 'validation/num_examples': 50000, 'test/accuracy': 0.07860000431537628, 'test/loss': 5.3598432540893555, 'test/num_examples': 10000}
I0329 20:13:20.514574 140501021517632 submission_runner.py:390] After eval at step 4204: RAM USED (GB) 102.490771456
I0329 20:13:20.526238 140324844324608 logging_writer.py:48] [4204] global_step=4204, preemption_count=0, score=1699.393714, test/accuracy=0.078600, test/loss=5.359843, test/num_examples=10000, total_duration=1845.772055, train/accuracy=0.113984, train/loss=5.018522, validation/accuracy=0.100500, validation/loss=5.094004, validation/num_examples=50000
I0329 20:13:20.624550 140501021517632 checkpoints.py:356] Saving checkpoint at step: 4204
I0329 20:13:22.064039 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_4204
I0329 20:13:22.074514 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_4204.
I0329 20:13:22.076970 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 4204: RAM USED (GB) 108.567707648
I0329 20:14:00.255623 140325095958272 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5996375679969788, loss=6.152223587036133
I0329 20:14:39.487947 140324835931904 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.4363587498664856, loss=6.201643943786621
I0329 20:15:19.580242 140325095958272 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6073512434959412, loss=6.005617141723633
I0329 20:15:59.541751 140324835931904 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6561938524246216, loss=5.897768020629883
I0329 20:16:39.648393 140325095958272 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5196068286895752, loss=6.673351764678955
I0329 20:17:19.718640 140324835931904 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5767892003059387, loss=5.988418102264404
I0329 20:17:59.578020 140325095958272 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6694583892822266, loss=5.898972034454346
I0329 20:18:39.946856 140324835931904 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5936124920845032, loss=5.875712871551514
I0329 20:19:20.740731 140325095958272 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5717930793762207, loss=5.870741844177246
I0329 20:20:00.785280 140324835931904 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5225027799606323, loss=6.478182792663574
I0329 20:20:22.376734 140501021517632 submission_runner.py:371] Before eval at step 5255: RAM USED (GB) 106.726129664
I0329 20:20:22.377041 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:20:35.116368 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:20:43.770520 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:20:45.459863 140501021517632 submission_runner.py:380] Time since start: 2290.33s, 	Step: 5255, 	{'train/accuracy': 0.14158202707767487, 'train/loss': 4.828235149383545, 'validation/accuracy': 0.12731999158859253, 'validation/loss': 4.90830659866333, 'validation/num_examples': 50000, 'test/accuracy': 0.09910000115633011, 'test/loss': 5.191289901733398, 'test/num_examples': 10000}
I0329 20:20:45.460599 140501021517632 submission_runner.py:390] After eval at step 5255: RAM USED (GB) 108.433195008
I0329 20:20:45.475457 140325095958272 logging_writer.py:48] [5255] global_step=5255, preemption_count=0, score=2112.252730, test/accuracy=0.099100, test/loss=5.191290, test/num_examples=10000, total_duration=2290.332294, train/accuracy=0.141582, train/loss=4.828235, validation/accuracy=0.127320, validation/loss=4.908307, validation/num_examples=50000
I0329 20:20:45.577734 140501021517632 checkpoints.py:356] Saving checkpoint at step: 5255
I0329 20:20:47.097210 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_5255
I0329 20:20:47.107616 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_5255.
I0329 20:20:47.109618 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 5255: RAM USED (GB) 114.37963264
I0329 20:21:05.253877 140324835931904 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.619958221912384, loss=5.869189739227295
I0329 20:21:44.640897 140324827539200 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6396810412406921, loss=5.767550468444824
I0329 20:22:24.968561 140324835931904 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.48912250995635986, loss=6.454308986663818
I0329 20:23:05.129590 140324827539200 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5960426330566406, loss=5.862044811248779
I0329 20:23:45.307597 140324835931904 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.603102445602417, loss=5.757386684417725
I0329 20:24:25.662763 140324827539200 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6741989254951477, loss=5.71698522567749
I0329 20:25:05.815607 140324835931904 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4755752384662628, loss=6.142590522766113
I0329 20:25:46.033183 140324827539200 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4613111913204193, loss=6.022979736328125
I0329 20:26:26.302238 140324835931904 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5624313354492188, loss=5.714756011962891
I0329 20:27:06.227458 140324827539200 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5789785981178284, loss=5.7357306480407715
I0329 20:27:46.739765 140324835931904 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4182736873626709, loss=6.426239013671875
I0329 20:27:47.290179 140501021517632 submission_runner.py:371] Before eval at step 6303: RAM USED (GB) 112.234938368
I0329 20:27:47.290451 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:28:00.229463 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:28:09.016618 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:28:10.688634 140501021517632 submission_runner.py:380] Time since start: 2735.25s, 	Step: 6303, 	{'train/accuracy': 0.16765624284744263, 'train/loss': 4.5861735343933105, 'validation/accuracy': 0.1551000028848648, 'validation/loss': 4.659244060516357, 'validation/num_examples': 50000, 'test/accuracy': 0.11940000206232071, 'test/loss': 4.984973907470703, 'test/num_examples': 10000}
I0329 20:28:10.689175 140501021517632 submission_runner.py:390] After eval at step 6303: RAM USED (GB) 114.277904384
I0329 20:28:10.704211 140324827539200 logging_writer.py:48] [6303] global_step=6303, preemption_count=0, score=2524.108112, test/accuracy=0.119400, test/loss=4.984974, test/num_examples=10000, total_duration=2735.246203, train/accuracy=0.167656, train/loss=4.586174, validation/accuracy=0.155100, validation/loss=4.659244, validation/num_examples=50000
I0329 20:28:10.802923 140501021517632 checkpoints.py:356] Saving checkpoint at step: 6303
I0329 20:28:12.565008 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_6303
I0329 20:28:12.576562 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_6303.
I0329 20:28:12.579076 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 6303: RAM USED (GB) 120.942882816
I0329 20:28:51.100323 140324835931904 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.600584864616394, loss=5.689529895782471
I0329 20:29:30.840962 140324819146496 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5281829833984375, loss=6.093733787536621
I0329 20:30:11.094593 140324835931904 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6918331980705261, loss=5.625021934509277
I0329 20:30:51.099384 140324819146496 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5559477210044861, loss=5.751918315887451
I0329 20:31:31.264936 140324835931904 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5994521975517273, loss=5.742771625518799
I0329 20:32:11.563436 140324819146496 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6063824892044067, loss=5.64670467376709
I0329 20:32:51.817867 140324835931904 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5804494023323059, loss=5.695699214935303
I0329 20:33:32.262711 140324819146496 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5654818415641785, loss=5.595736026763916
I0329 20:34:12.428510 140324835931904 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6908396482467651, loss=5.555701732635498
I0329 20:34:52.606870 140324819146496 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4531523287296295, loss=6.55042839050293
I0329 20:35:12.821266 140501021517632 submission_runner.py:371] Before eval at step 7352: RAM USED (GB) 118.388846592
I0329 20:35:12.821605 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:35:25.829093 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:35:34.726124 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:35:36.390866 140501021517632 submission_runner.py:380] Time since start: 3180.78s, 	Step: 7352, 	{'train/accuracy': 0.2135351449251175, 'train/loss': 4.2203521728515625, 'validation/accuracy': 0.19606000185012817, 'validation/loss': 4.327794075012207, 'validation/num_examples': 50000, 'test/accuracy': 0.1469999998807907, 'test/loss': 4.699497699737549, 'test/num_examples': 10000}
I0329 20:35:36.391510 140501021517632 submission_runner.py:390] After eval at step 7352: RAM USED (GB) 120.547713024
I0329 20:35:36.406689 140324835931904 logging_writer.py:48] [7352] global_step=7352, preemption_count=0, score=2935.280995, test/accuracy=0.147000, test/loss=4.699498, test/num_examples=10000, total_duration=3180.776555, train/accuracy=0.213535, train/loss=4.220352, validation/accuracy=0.196060, validation/loss=4.327794, validation/num_examples=50000
I0329 20:35:36.508716 140501021517632 checkpoints.py:356] Saving checkpoint at step: 7352
I0329 20:35:38.218683 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_7352
I0329 20:35:38.231086 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_7352.
I0329 20:35:38.233536 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 7352: RAM USED (GB) 127.437152256
I0329 20:35:57.633625 140324819146496 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4037332236766815, loss=6.475461006164551
I0329 20:36:37.123855 140324810753792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6530001759529114, loss=5.59950065612793
I0329 20:37:17.330444 140324819146496 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6003304719924927, loss=5.748181343078613
I0329 20:37:57.197516 140324810753792 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5373574495315552, loss=5.598854064941406
I0329 20:38:37.422811 140324819146496 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.44888433814048767, loss=6.431922435760498
I0329 20:39:17.639908 140324810753792 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6435336470603943, loss=5.4502058029174805
I0329 20:39:57.708485 140324819146496 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5591728091239929, loss=5.5590620040893555
I0329 20:40:37.900216 140324810753792 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6056803464889526, loss=5.5442633628845215
I0329 20:41:18.111424 140324819146496 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.59078049659729, loss=5.450112819671631
I0329 20:41:58.390384 140324810753792 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5490902662277222, loss=5.678771018981934
I0329 20:42:38.358865 140501021517632 submission_runner.py:371] Before eval at step 8400: RAM USED (GB) 124.301635584
I0329 20:42:38.359282 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:42:51.756157 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:43:01.278765 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:43:02.931218 140501021517632 submission_runner.py:380] Time since start: 3626.31s, 	Step: 8400, 	{'train/accuracy': 0.23818358778953552, 'train/loss': 4.084075927734375, 'validation/accuracy': 0.21691998839378357, 'validation/loss': 4.193808555603027, 'validation/num_examples': 50000, 'test/accuracy': 0.16110001504421234, 'test/loss': 4.5953145027160645, 'test/num_examples': 10000}
I0329 20:43:02.931740 140501021517632 submission_runner.py:390] After eval at step 8400: RAM USED (GB) 126.796345344
I0329 20:43:02.945091 140324819146496 logging_writer.py:48] [8400] global_step=8400, preemption_count=0, score=3345.732813, test/accuracy=0.161100, test/loss=4.595315, test/num_examples=10000, total_duration=3626.314316, train/accuracy=0.238184, train/loss=4.084076, validation/accuracy=0.216920, validation/loss=4.193809, validation/num_examples=50000
I0329 20:43:03.034260 140501021517632 checkpoints.py:356] Saving checkpoint at step: 8400
I0329 20:43:04.347576 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_8400
I0329 20:43:04.359410 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_8400.
I0329 20:43:04.361793 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 8400: RAM USED (GB) 132.294950912
I0329 20:43:04.803329 140324810753792 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.573506236076355, loss=5.466104507446289
I0329 20:43:44.257064 140324802361088 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5770856142044067, loss=5.414557933807373
I0329 20:44:24.514079 140324810753792 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5514802932739258, loss=5.399170875549316
I0329 20:45:04.780029 140324802361088 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5614897608757019, loss=5.361079692840576
I0329 20:45:45.044140 140324810753792 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4877052307128906, loss=5.60933256149292
I0329 20:46:25.164609 140324802361088 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5604149103164673, loss=6.510425090789795
I0329 20:47:05.217776 140324810753792 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4889628291130066, loss=5.654782772064209
I0329 20:47:45.800471 140324802361088 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.36581429839134216, loss=6.500238418579102
I0329 20:48:25.904679 140324810753792 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6782453060150146, loss=5.310326099395752
I0329 20:49:05.940593 140324802361088 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5584262609481812, loss=5.2077765464782715
I0329 20:49:46.499298 140324810753792 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5728164315223694, loss=5.287325859069824
I0329 20:50:04.629453 140501021517632 submission_runner.py:371] Before eval at step 9447: RAM USED (GB) 130.7635712
I0329 20:50:04.629768 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:50:18.082665 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:50:28.192892 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:50:29.845610 140501021517632 submission_runner.py:380] Time since start: 4072.59s, 	Step: 9447, 	{'train/accuracy': 0.27292966842651367, 'train/loss': 3.7260539531707764, 'validation/accuracy': 0.247639998793602, 'validation/loss': 3.8667690753936768, 'validation/num_examples': 50000, 'test/accuracy': 0.18700000643730164, 'test/loss': 4.3147664070129395, 'test/num_examples': 10000}
I0329 20:50:29.846126 140501021517632 submission_runner.py:390] After eval at step 9447: RAM USED (GB) 135.758544896
I0329 20:50:29.858836 140324802361088 logging_writer.py:48] [9447] global_step=9447, preemption_count=0, score=3755.912010, test/accuracy=0.187000, test/loss=4.314766, test/num_examples=10000, total_duration=4072.585445, train/accuracy=0.272930, train/loss=3.726054, validation/accuracy=0.247640, validation/loss=3.866769, validation/num_examples=50000
I0329 20:50:29.958058 140501021517632 checkpoints.py:356] Saving checkpoint at step: 9447
I0329 20:50:30.872825 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_9447
I0329 20:50:30.886175 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_9447.
I0329 20:50:30.888734 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 9447: RAM USED (GB) 140.059291648
I0329 20:50:52.249726 140324810753792 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5281183123588562, loss=5.665150165557861
I0329 20:51:31.817730 140324793968384 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5712228417396545, loss=5.294913291931152
I0329 20:52:12.113180 140324810753792 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.49348968267440796, loss=5.676478862762451
I0329 20:52:52.335462 140324793968384 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5703743696212769, loss=5.903794288635254
I0329 20:53:32.647890 140324810753792 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6977812051773071, loss=5.224466323852539
I0329 20:54:13.000138 140324793968384 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5244150161743164, loss=5.26105260848999
I0329 20:54:53.023093 140324810753792 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5412949323654175, loss=5.294235706329346
I0329 20:55:33.489990 140324793968384 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.45354872941970825, loss=6.112308025360107
I0329 20:56:14.037347 140324810753792 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.4950808882713318, loss=5.461817264556885
I0329 20:56:54.383260 140324793968384 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6078245639801025, loss=5.158541202545166
I0329 20:57:31.204083 140501021517632 submission_runner.py:371] Before eval at step 10493: RAM USED (GB) 136.6070272
I0329 20:57:31.204569 140501021517632 spec.py:298] Evaluating on the training split.
I0329 20:57:44.558339 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 20:57:56.245153 140501021517632 spec.py:326] Evaluating on the test split.
I0329 20:57:57.917769 140501021517632 submission_runner.py:380] Time since start: 4519.16s, 	Step: 10493, 	{'train/accuracy': 0.28865233063697815, 'train/loss': 3.7527413368225098, 'validation/accuracy': 0.26589998602867126, 'validation/loss': 3.872021198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.20030000805854797, 'test/loss': 4.2981109619140625, 'test/num_examples': 10000}
I0329 20:57:57.918314 140501021517632 submission_runner.py:390] After eval at step 10493: RAM USED (GB) 145.172758528
I0329 20:57:57.933495 140324810753792 logging_writer.py:48] [10493] global_step=10493, preemption_count=0, score=4165.987896, test/accuracy=0.200300, test/loss=4.298111, test/num_examples=10000, total_duration=4519.157401, train/accuracy=0.288652, train/loss=3.752741, validation/accuracy=0.265900, validation/loss=3.872021, validation/num_examples=50000
I0329 20:57:58.023098 140501021517632 checkpoints.py:356] Saving checkpoint at step: 10493
I0329 20:57:58.863359 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_10493
I0329 20:57:58.874819 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_10493.
I0329 20:57:58.877401 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 10493: RAM USED (GB) 149.133500416
I0329 20:58:02.076438 140324793968384 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6310692429542542, loss=5.31181526184082
I0329 20:58:41.335412 140324785575680 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6117416620254517, loss=5.045553684234619
I0329 20:59:21.063183 140324793968384 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.459922194480896, loss=5.610238075256348
I0329 21:00:01.456856 140324785575680 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6794434785842896, loss=5.155585289001465
I0329 21:00:41.808228 140324793968384 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5173812508583069, loss=5.632212162017822
I0329 21:01:22.124365 140324785575680 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5447825193405151, loss=5.172605514526367
I0329 21:02:02.762953 140324793968384 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6202691197395325, loss=5.172395706176758
I0329 21:02:43.264712 140324785575680 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5248382687568665, loss=6.345643997192383
I0329 21:03:24.068431 140324793968384 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5454062223434448, loss=5.198301315307617
I0329 21:04:04.813968 140324785575680 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.43022802472114563, loss=6.327846050262451
I0329 21:04:45.464449 140324793968384 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5521936416625977, loss=5.101479530334473
I0329 21:04:59.262068 140501021517632 submission_runner.py:371] Before eval at step 11536: RAM USED (GB) 142.560018432
I0329 21:04:59.262328 140501021517632 spec.py:298] Evaluating on the training split.
I0329 21:05:13.377248 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 21:05:23.904464 140501021517632 spec.py:326] Evaluating on the test split.
I0329 21:05:25.567804 140501021517632 submission_runner.py:380] Time since start: 4967.22s, 	Step: 11536, 	{'train/accuracy': 0.3230077922344208, 'train/loss': 3.426758050918579, 'validation/accuracy': 0.29666000604629517, 'validation/loss': 3.5732076168060303, 'validation/num_examples': 50000, 'test/accuracy': 0.22340001165866852, 'test/loss': 4.076852321624756, 'test/num_examples': 10000}
I0329 21:05:25.568309 140501021517632 submission_runner.py:390] After eval at step 11536: RAM USED (GB) 147.955904512
I0329 21:05:25.579384 140324785575680 logging_writer.py:48] [11536] global_step=11536, preemption_count=0, score=4575.501129, test/accuracy=0.223400, test/loss=4.076852, test/num_examples=10000, total_duration=4967.218280, train/accuracy=0.323008, train/loss=3.426758, validation/accuracy=0.296660, validation/loss=3.573208, validation/num_examples=50000
I0329 21:05:25.681649 140501021517632 checkpoints.py:356] Saving checkpoint at step: 11536
I0329 21:05:26.504493 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_11536
I0329 21:05:26.518480 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_11536.
I0329 21:05:26.520588 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 11536: RAM USED (GB) 151.906951168
I0329 21:05:52.082389 140324793968384 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.47622549533843994, loss=6.186945915222168
I0329 21:06:31.984049 140324710106880 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5697125196456909, loss=5.073531150817871
I0329 21:07:12.207331 140324793968384 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.624401867389679, loss=5.069467544555664
I0329 21:07:52.437988 140324710106880 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5689747333526611, loss=5.072312831878662
I0329 21:08:32.573201 140324793968384 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6015577912330627, loss=5.0991034507751465
I0329 21:09:12.837883 140324710106880 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.4467105567455292, loss=6.375384330749512
I0329 21:09:52.990343 140324793968384 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5071821808815002, loss=5.140505790710449
I0329 21:10:33.286131 140324710106880 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5577488541603088, loss=6.261004447937012
I0329 21:11:13.511898 140324793968384 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5743157267570496, loss=5.116605758666992
I0329 21:11:54.277347 140324710106880 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4659975469112396, loss=5.534486770629883
I0329 21:12:26.952881 140501021517632 submission_runner.py:371] Before eval at step 12581: RAM USED (GB) 148.580503552
I0329 21:12:26.953178 140501021517632 spec.py:298] Evaluating on the training split.
I0329 21:12:40.762837 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 21:12:51.804418 140501021517632 spec.py:326] Evaluating on the test split.
I0329 21:12:53.478030 140501021517632 submission_runner.py:380] Time since start: 5414.91s, 	Step: 12581, 	{'train/accuracy': 0.34935545921325684, 'train/loss': 3.33976411819458, 'validation/accuracy': 0.31873998045921326, 'validation/loss': 3.494323253631592, 'validation/num_examples': 50000, 'test/accuracy': 0.24860000610351562, 'test/loss': 3.9840705394744873, 'test/num_examples': 10000}
I0329 21:12:53.478732 140501021517632 submission_runner.py:390] After eval at step 12581: RAM USED (GB) 152.159576064
I0329 21:12:53.507273 140324793968384 logging_writer.py:48] [12581] global_step=12581, preemption_count=0, score=4985.368884, test/accuracy=0.248600, test/loss=3.984071, test/num_examples=10000, total_duration=5414.908472, train/accuracy=0.349355, train/loss=3.339764, validation/accuracy=0.318740, validation/loss=3.494323, validation/num_examples=50000
I0329 21:12:53.632662 140501021517632 checkpoints.py:356] Saving checkpoint at step: 12581
I0329 21:12:54.513618 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_12581
I0329 21:12:54.527071 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_12581.
I0329 21:12:54.530958 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 12581: RAM USED (GB) 156.31706112
I0329 21:13:02.447846 140324710106880 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5360976457595825, loss=5.029294013977051
I0329 21:13:42.657182 140324701714176 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5976974368095398, loss=4.928974151611328
I0329 21:14:23.784459 140324710106880 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5778493285179138, loss=5.200943946838379
I0329 21:15:04.877908 140324701714176 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5834403038024902, loss=4.902785778045654
I0329 21:15:45.911767 140324710106880 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5747352838516235, loss=5.084508419036865
I0329 21:16:26.946629 140324701714176 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6075096130371094, loss=4.987444877624512
I0329 21:17:08.149358 140324710106880 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6113224625587463, loss=5.011878490447998
I0329 21:17:49.147527 140324701714176 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5773245096206665, loss=4.966945171356201
I0329 21:18:29.913957 140324710106880 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6164730191230774, loss=4.951632499694824
I0329 21:19:10.854701 140324701714176 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.461668998003006, loss=6.089966297149658
I0329 21:19:51.848553 140324710106880 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6533629298210144, loss=4.977270603179932
I0329 21:19:54.847703 140501021517632 submission_runner.py:371] Before eval at step 13609: RAM USED (GB) 154.236850176
I0329 21:19:54.847988 140501021517632 spec.py:298] Evaluating on the training split.
I0329 21:20:09.475051 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 21:20:20.148295 140501021517632 spec.py:326] Evaluating on the test split.
I0329 21:20:21.799882 140501021517632 submission_runner.py:380] Time since start: 5862.80s, 	Step: 13609, 	{'train/accuracy': 0.38359373807907104, 'train/loss': 3.0740554332733154, 'validation/accuracy': 0.34665998816490173, 'validation/loss': 3.251418352127075, 'validation/num_examples': 50000, 'test/accuracy': 0.2702000141143799, 'test/loss': 3.787921190261841, 'test/num_examples': 10000}
I0329 21:20:21.800504 140501021517632 submission_runner.py:390] After eval at step 13609: RAM USED (GB) 158.218870784
I0329 21:20:21.811878 140324701714176 logging_writer.py:48] [13609] global_step=13609, preemption_count=0, score=5390.752581, test/accuracy=0.270200, test/loss=3.787921, test/num_examples=10000, total_duration=5862.803661, train/accuracy=0.383594, train/loss=3.074055, validation/accuracy=0.346660, validation/loss=3.251418, validation/num_examples=50000
I0329 21:20:21.959749 140501021517632 checkpoints.py:356] Saving checkpoint at step: 13609
I0329 21:20:22.951792 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_13609
I0329 21:20:22.963600 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_13609.
I0329 21:20:22.973926 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 13609: RAM USED (GB) 162.928275456
I0329 21:20:59.757082 140324710106880 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5071220397949219, loss=5.184330940246582
I0329 21:21:41.169817 140324693321472 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6372438073158264, loss=4.863800048828125
I0329 21:22:22.569020 140324710106880 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.48995089530944824, loss=5.482293128967285
I0329 21:23:03.102975 140501021517632 submission_runner.py:371] Before eval at step 14000: RAM USED (GB) 159.796629504
I0329 21:23:03.103406 140501021517632 spec.py:298] Evaluating on the training split.
I0329 21:23:17.149048 140501021517632 spec.py:310] Evaluating on the validation split.
I0329 21:23:28.356289 140501021517632 spec.py:326] Evaluating on the test split.
I0329 21:23:30.018187 140501021517632 submission_runner.py:380] Time since start: 6051.06s, 	Step: 14000, 	{'train/accuracy': 0.3759765625, 'train/loss': 3.1078338623046875, 'validation/accuracy': 0.34981998801231384, 'validation/loss': 3.2564265727996826, 'validation/num_examples': 50000, 'test/accuracy': 0.2718999981880188, 'test/loss': 3.7835817337036133, 'test/num_examples': 10000}
I0329 21:23:30.018737 140501021517632 submission_runner.py:390] After eval at step 14000: RAM USED (GB) 164.318334976
I0329 21:23:30.036263 140324693321472 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5544.283802, test/accuracy=0.271900, test/loss=3.783582, test/num_examples=10000, total_duration=6051.059829, train/accuracy=0.375977, train/loss=3.107834, validation/accuracy=0.349820, validation/loss=3.256427, validation/num_examples=50000
I0329 21:23:30.175728 140501021517632 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 21:23:31.025180 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_14000
I0329 21:23:31.037523 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_14000.
I0329 21:23:31.046170 140501021517632 submission_runner.py:409] After logging and checkpointing eval at step 14000: RAM USED (GB) 168.542019584
I0329 21:23:31.056525 140324710106880 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5544.283802
I0329 21:23:31.218636 140501021517632 checkpoints.py:356] Saving checkpoint at step: 14000
I0329 21:23:32.442578 140501021517632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_14000
I0329 21:23:32.455863 140501021517632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/imagenet_vit_jax/trial_1/checkpoint_14000.
I0329 21:23:33.351647 140501021517632 submission_runner.py:543] Tuning trial 1/1
I0329 21:23:33.352574 140501021517632 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0329 21:23:33.354993 140501021517632 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 46.557777881622314, 'total_duration': 46.641414165496826, 'global_step': 1, 'preemption_count': 0}), (1030, {'train/accuracy': 0.03267578035593033, 'train/loss': 6.112421035766602, 'validation/accuracy': 0.032600000500679016, 'validation/loss': 6.134460926055908, 'validation/num_examples': 50000, 'test/accuracy': 0.027400001883506775, 'test/loss': 6.2289137840271, 'test/num_examples': 10000, 'score': 459.852942943573, 'total_duration': 516.4358065128326, 'global_step': 1030, 'preemption_count': 0}), (2089, {'train/accuracy': 0.0622265599668026, 'train/loss': 5.656254768371582, 'validation/accuracy': 0.05861999839544296, 'validation/loss': 5.687917232513428, 'validation/num_examples': 50000, 'test/accuracy': 0.047200001776218414, 'test/loss': 5.853387355804443, 'test/num_examples': 10000, 'score': 872.8969912528992, 'total_duration': 959.0625412464142, 'global_step': 2089, 'preemption_count': 0}), (3147, {'train/accuracy': 0.0908203125, 'train/loss': 5.2635908126831055, 'validation/accuracy': 0.08574000000953674, 'validation/loss': 5.315304279327393, 'validation/num_examples': 50000, 'test/accuracy': 0.065700002014637, 'test/loss': 5.5355095863342285, 'test/num_examples': 10000, 'score': 1286.197410106659, 'total_duration': 1402.4211926460266, 'global_step': 3147, 'preemption_count': 0}), (4204, {'train/accuracy': 0.1139843687415123, 'train/loss': 5.018521785736084, 'validation/accuracy': 0.10049999505281448, 'validation/loss': 5.094003677368164, 'validation/num_examples': 50000, 'test/accuracy': 0.07860000431537628, 'test/loss': 5.3598432540893555, 'test/num_examples': 10000, 'score': 1699.3937141895294, 'total_duration': 1845.7720546722412, 'global_step': 4204, 'preemption_count': 0}), (5255, {'train/accuracy': 0.14158202707767487, 'train/loss': 4.828235149383545, 'validation/accuracy': 0.12731999158859253, 'validation/loss': 4.90830659866333, 'validation/num_examples': 50000, 'test/accuracy': 0.09910000115633011, 'test/loss': 5.191289901733398, 'test/num_examples': 10000, 'score': 2112.252730369568, 'total_duration': 2290.3322937488556, 'global_step': 5255, 'preemption_count': 0}), (6303, {'train/accuracy': 0.16765624284744263, 'train/loss': 4.5861735343933105, 'validation/accuracy': 0.1551000028848648, 'validation/loss': 4.659244060516357, 'validation/num_examples': 50000, 'test/accuracy': 0.11940000206232071, 'test/loss': 4.984973907470703, 'test/num_examples': 10000, 'score': 2524.1081116199493, 'total_duration': 2735.2462029457092, 'global_step': 6303, 'preemption_count': 0}), (7352, {'train/accuracy': 0.2135351449251175, 'train/loss': 4.2203521728515625, 'validation/accuracy': 0.19606000185012817, 'validation/loss': 4.327794075012207, 'validation/num_examples': 50000, 'test/accuracy': 0.1469999998807907, 'test/loss': 4.699497699737549, 'test/num_examples': 10000, 'score': 2935.2809948921204, 'total_duration': 3180.776554584503, 'global_step': 7352, 'preemption_count': 0}), (8400, {'train/accuracy': 0.23818358778953552, 'train/loss': 4.084075927734375, 'validation/accuracy': 0.21691998839378357, 'validation/loss': 4.193808555603027, 'validation/num_examples': 50000, 'test/accuracy': 0.16110001504421234, 'test/loss': 4.5953145027160645, 'test/num_examples': 10000, 'score': 3345.732813358307, 'total_duration': 3626.3143162727356, 'global_step': 8400, 'preemption_count': 0}), (9447, {'train/accuracy': 0.27292966842651367, 'train/loss': 3.7260539531707764, 'validation/accuracy': 0.247639998793602, 'validation/loss': 3.8667690753936768, 'validation/num_examples': 50000, 'test/accuracy': 0.18700000643730164, 'test/loss': 4.3147664070129395, 'test/num_examples': 10000, 'score': 3755.912009716034, 'total_duration': 4072.5854454040527, 'global_step': 9447, 'preemption_count': 0}), (10493, {'train/accuracy': 0.28865233063697815, 'train/loss': 3.7527413368225098, 'validation/accuracy': 0.26589998602867126, 'validation/loss': 3.872021198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.20030000805854797, 'test/loss': 4.2981109619140625, 'test/num_examples': 10000, 'score': 4165.987896203995, 'total_duration': 4519.157400846481, 'global_step': 10493, 'preemption_count': 0}), (11536, {'train/accuracy': 0.3230077922344208, 'train/loss': 3.426758050918579, 'validation/accuracy': 0.29666000604629517, 'validation/loss': 3.5732076168060303, 'validation/num_examples': 50000, 'test/accuracy': 0.22340001165866852, 'test/loss': 4.076852321624756, 'test/num_examples': 10000, 'score': 4575.501128911972, 'total_duration': 4967.218279600143, 'global_step': 11536, 'preemption_count': 0}), (12581, {'train/accuracy': 0.34935545921325684, 'train/loss': 3.33976411819458, 'validation/accuracy': 0.31873998045921326, 'validation/loss': 3.494323253631592, 'validation/num_examples': 50000, 'test/accuracy': 0.24860000610351562, 'test/loss': 3.9840705394744873, 'test/num_examples': 10000, 'score': 4985.3688843250275, 'total_duration': 5414.90847158432, 'global_step': 12581, 'preemption_count': 0}), (13609, {'train/accuracy': 0.38359373807907104, 'train/loss': 3.0740554332733154, 'validation/accuracy': 0.34665998816490173, 'validation/loss': 3.251418352127075, 'validation/num_examples': 50000, 'test/accuracy': 0.2702000141143799, 'test/loss': 3.787921190261841, 'test/num_examples': 10000, 'score': 5390.752581119537, 'total_duration': 5862.803660869598, 'global_step': 13609, 'preemption_count': 0}), (14000, {'train/accuracy': 0.3759765625, 'train/loss': 3.1078338623046875, 'validation/accuracy': 0.34981998801231384, 'validation/loss': 3.2564265727996826, 'validation/num_examples': 50000, 'test/accuracy': 0.2718999981880188, 'test/loss': 3.7835817337036133, 'test/num_examples': 10000, 'score': 5544.2838015556335, 'total_duration': 6051.0598294734955, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0329 21:23:33.355106 140501021517632 submission_runner.py:546] Timing: 5544.2838015556335
I0329 21:23:33.355154 140501021517632 submission_runner.py:547] ====================
I0329 21:23:33.355260 140501021517632 submission_runner.py:606] Final imagenet_vit score: 5544.2838015556335
