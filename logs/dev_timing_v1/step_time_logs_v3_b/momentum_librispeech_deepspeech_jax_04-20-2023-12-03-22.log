I0420 12:03:44.478671 140028105549632 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax.
I0420 12:03:44.545719 140028105549632 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 12:03:45.366687 140028105549632 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0420 12:03:45.367300 140028105549632 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 12:03:45.371199 140028105549632 submission_runner.py:528] Using RNG seed 3022287582
I0420 12:03:48.004840 140028105549632 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 12:03:48.005019 140028105549632 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1.
I0420 12:03:48.005184 140028105549632 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/hparams.json.
I0420 12:03:48.126069 140028105549632 submission_runner.py:232] Initializing dataset.
I0420 12:03:48.126239 140028105549632 submission_runner.py:239] Initializing model.
I0420 12:04:04.635516 140028105549632 submission_runner.py:249] Initializing optimizer.
I0420 12:04:05.167832 140028105549632 submission_runner.py:256] Initializing metrics bundle.
I0420 12:04:05.168024 140028105549632 submission_runner.py:273] Initializing checkpoint and logger.
I0420 12:04:05.168941 140028105549632 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0420 12:04:05.169230 140028105549632 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 12:04:05.169317 140028105549632 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 12:04:05.954155 140028105549632 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0420 12:04:05.955058 140028105549632 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0420 12:04:05.962451 140028105549632 submission_runner.py:309] Starting training loop.
I0420 12:04:06.157410 140028105549632 input_pipeline.py:20] Loading split = train-clean-100
I0420 12:04:06.188540 140028105549632 input_pipeline.py:20] Loading split = train-clean-360
I0420 12:04:06.475667 140028105549632 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0420 12:05:01.354967 139851173177088 logging_writer.py:48] [0] global_step=0, grad_norm=21.32908058166504, loss=32.462890625
I0420 12:05:01.375880 140028105549632 spec.py:298] Evaluating on the training split.
I0420 12:05:01.503160 140028105549632 input_pipeline.py:20] Loading split = train-clean-100
I0420 12:05:01.531037 140028105549632 input_pipeline.py:20] Loading split = train-clean-360
I0420 12:05:01.793598 140028105549632 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0420 12:06:36.087889 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 12:06:36.177976 140028105549632 input_pipeline.py:20] Loading split = dev-clean
I0420 12:06:36.183274 140028105549632 input_pipeline.py:20] Loading split = dev-other
I0420 12:07:32.803856 140028105549632 spec.py:326] Evaluating on the test split.
I0420 12:07:32.895735 140028105549632 input_pipeline.py:20] Loading split = test-clean
I0420 12:08:08.892646 140028105549632 submission_runner.py:406] Time since start: 242.93s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.489872, dtype=float32), 'train/wer': 4.79716490015201, 'validation/ctc_loss': DeviceArray(31.266993, dtype=float32), 'validation/wer': 4.321894084844041, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.504843, dtype=float32), 'test/wer': 4.575812970974753, 'test/num_examples': 2472, 'score': 55.413275480270386, 'total_duration': 242.92882680892944, 'accumulated_submission_time': 55.413275480270386, 'accumulated_eval_time': 187.5154185295105, 'accumulated_logging_time': 0}
I0420 12:08:08.916287 139848262350592 logging_writer.py:48] [1] accumulated_eval_time=187.515419, accumulated_logging_time=0, accumulated_submission_time=55.413275, global_step=1, preemption_count=0, score=55.413275, test/ctc_loss=31.50484275817871, test/num_examples=2472, test/wer=4.575813, total_duration=242.928827, train/ctc_loss=32.489871978759766, train/wer=4.797165, validation/ctc_loss=31.266992568969727, validation/num_examples=5348, validation/wer=4.321894
I0420 12:08:09.002021 140028105549632 checkpoints.py:356] Saving checkpoint at step: 1
I0420 12:08:09.254381 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0420 12:08:09.255057 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0420 12:10:17.965058 139851902048000 logging_writer.py:48] [100] global_step=100, grad_norm=2.552095413208008, loss=6.017500400543213
I0420 12:12:14.554368 139851910440704 logging_writer.py:48] [200] global_step=200, grad_norm=3.8245413303375244, loss=5.9758172035217285
I0420 12:14:07.740490 139851902048000 logging_writer.py:48] [300] global_step=300, grad_norm=4.475863933563232, loss=5.983764171600342
I0420 12:16:00.607955 139851910440704 logging_writer.py:48] [400] global_step=400, grad_norm=9.753119468688965, loss=6.662752151489258
I0420 12:17:55.787641 139851902048000 logging_writer.py:48] [500] global_step=500, grad_norm=0.24998362362384796, loss=5.818477630615234
I0420 12:19:48.944299 139851910440704 logging_writer.py:48] [600] global_step=600, grad_norm=2.884941577911377, loss=5.877596855163574
I0420 12:21:43.121602 139851902048000 logging_writer.py:48] [700] global_step=700, grad_norm=2.0681800842285156, loss=5.627595901489258
I0420 12:23:37.552869 139851910440704 logging_writer.py:48] [800] global_step=800, grad_norm=0.8509437441825867, loss=5.341342449188232
I0420 12:25:33.538157 139851902048000 logging_writer.py:48] [900] global_step=900, grad_norm=0.7114786505699158, loss=4.992214202880859
I0420 12:27:28.068265 139851910440704 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1001695394515991, loss=4.639934062957764
I0420 12:29:23.398089 139854002411264 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5224689245224, loss=4.341251850128174
I0420 12:31:15.667072 139853994018560 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3589037656784058, loss=4.101889610290527
I0420 12:33:11.122694 139854002411264 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9910457730293274, loss=3.8425302505493164
I0420 12:35:09.598093 139853994018560 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.5796678066253662, loss=3.750189781188965
I0420 12:37:07.521937 139854002411264 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.096635103225708, loss=3.590186834335327
I0420 12:39:05.040249 139853994018560 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.058133840560913, loss=3.4739699363708496
I0420 12:41:02.383517 139854002411264 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9482352137565613, loss=3.354449510574341
I0420 12:42:55.897375 139853994018560 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6753535270690918, loss=3.3137242794036865
I0420 12:44:48.321961 139854002411264 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9732917547225952, loss=3.2197248935699463
I0420 12:46:43.912307 139853994018560 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8525702953338623, loss=3.135741949081421
I0420 12:48:09.716338 140028105549632 spec.py:298] Evaluating on the training split.
I0420 12:48:38.651731 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 12:49:12.822003 140028105549632 spec.py:326] Evaluating on the test split.
I0420 12:49:29.919914 140028105549632 submission_runner.py:406] Time since start: 2723.95s, 	Step: 2073, 	{'train/ctc_loss': DeviceArray(5.457917, dtype=float32), 'train/wer': 0.9297903235772693, 'validation/ctc_loss': DeviceArray(5.6516786, dtype=float32), 'validation/wer': 0.8880452295728855, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.4053006, dtype=float32), 'test/wer': 0.8889769057339589, 'test/num_examples': 2472, 'score': 2455.8511288166046, 'total_duration': 2723.9541280269623, 'accumulated_submission_time': 2455.8511288166046, 'accumulated_eval_time': 267.7158601284027, 'accumulated_logging_time': 0.3645315170288086}
I0420 12:49:29.939443 139850334332672 logging_writer.py:48] [2073] accumulated_eval_time=267.715860, accumulated_logging_time=0.364532, accumulated_submission_time=2455.851129, global_step=2073, preemption_count=0, score=2455.851129, test/ctc_loss=5.405300617218018, test/num_examples=2472, test/wer=0.888977, total_duration=2723.954128, train/ctc_loss=5.457917213439941, train/wer=0.929790, validation/ctc_loss=5.651678562164307, validation/num_examples=5348, validation/wer=0.888045
I0420 12:49:30.037730 140028105549632 checkpoints.py:356] Saving checkpoint at step: 2073
I0420 12:49:30.370790 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_2073
I0420 12:49:30.377578 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_2073.
I0420 12:50:01.744174 139850325939968 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8588170409202576, loss=3.0544707775115967
I0420 12:51:53.782996 139849117980416 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0184061527252197, loss=3.068532705307007
I0420 12:53:47.156529 139850325939968 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9775848984718323, loss=3.005021095275879
I0420 12:55:42.775370 139849117980416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8293781280517578, loss=2.9344215393066406
I0420 12:57:37.119194 139850325939968 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9474877119064331, loss=2.895565986633301
I0420 12:59:30.347367 139849117980416 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.031406044960022, loss=2.8935887813568115
I0420 13:01:22.979463 139850325939968 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9837144017219543, loss=2.850332260131836
I0420 13:03:19.981669 139849117980416 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0005552768707275, loss=2.7938268184661865
I0420 13:05:16.316951 139850325939968 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.876111626625061, loss=2.6980106830596924
I0420 13:07:09.870208 139849117980416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8988132476806641, loss=2.739713430404663
I0420 13:09:07.090654 139852691691264 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9654932022094727, loss=2.7142744064331055
I0420 13:10:59.101016 139852683298560 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8686347603797913, loss=2.607247829437256
I0420 13:12:53.829357 139852691691264 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8683674335479736, loss=2.601925849914551
I0420 13:14:49.603867 139852683298560 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8241312503814697, loss=2.5650887489318848
I0420 13:16:45.389010 139852691691264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9465014338493347, loss=2.5749826431274414
I0420 13:18:41.215265 139852683298560 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9813116788864136, loss=2.5511884689331055
I0420 13:20:35.038823 139852691691264 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.828018307685852, loss=2.600206136703491
I0420 13:22:32.101266 139852683298560 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7421690225601196, loss=2.4408528804779053
I0420 13:24:29.181063 139852691691264 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8424482345581055, loss=2.554288148880005
I0420 13:26:23.832735 139852683298560 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8070226907730103, loss=2.4952611923217773
I0420 13:28:20.922879 139852691691264 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8164774775505066, loss=2.4672133922576904
I0420 13:29:31.374271 140028105549632 spec.py:298] Evaluating on the training split.
I0420 13:30:08.973124 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 13:30:47.775533 140028105549632 spec.py:326] Evaluating on the test split.
I0420 13:31:06.693991 140028105549632 submission_runner.py:406] Time since start: 5220.73s, 	Step: 4160, 	{'train/ctc_loss': DeviceArray(1.1345952, dtype=float32), 'train/wer': 0.3561196494686093, 'validation/ctc_loss': DeviceArray(1.5530185, dtype=float32), 'validation/wer': 0.42054433713784023, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.1540608, dtype=float32), 'test/wer': 0.34897324964962523, 'test/num_examples': 2472, 'score': 4856.824326038361, 'total_duration': 5220.728524923325, 'accumulated_submission_time': 4856.824326038361, 'accumulated_eval_time': 363.0327754020691, 'accumulated_logging_time': 0.8243768215179443}
I0420 13:31:06.712924 139852763371264 logging_writer.py:48] [4160] accumulated_eval_time=363.032775, accumulated_logging_time=0.824377, accumulated_submission_time=4856.824326, global_step=4160, preemption_count=0, score=4856.824326, test/ctc_loss=1.1540608406066895, test/num_examples=2472, test/wer=0.348973, total_duration=5220.728525, train/ctc_loss=1.1345951557159424, train/wer=0.356120, validation/ctc_loss=1.5530184507369995, validation/num_examples=5348, validation/wer=0.420544
I0420 13:31:06.815733 140028105549632 checkpoints.py:356] Saving checkpoint at step: 4160
I0420 13:31:07.226885 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_4160
I0420 13:31:07.235741 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_4160.
I0420 13:31:52.973126 139852754978560 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9373840689659119, loss=2.346027374267578
I0420 13:33:44.749550 139852704622336 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7834105491638184, loss=2.503450632095337
I0420 13:35:40.916757 139852754978560 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6673538684844971, loss=2.441887617111206
I0420 13:37:35.549116 139852704622336 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6671079397201538, loss=2.348951816558838
I0420 13:39:27.381474 139852754978560 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7603210806846619, loss=2.3690497875213623
I0420 13:41:19.292368 139852704622336 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.698691725730896, loss=2.3971850872039795
I0420 13:43:13.676196 139852754978560 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.010181188583374, loss=2.3733627796173096
I0420 13:45:09.433401 139852704622336 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6781057119369507, loss=2.2939085960388184
I0420 13:47:01.379297 139852754978560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8970719575881958, loss=2.348783254623413
I0420 13:48:53.174149 139852704622336 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9464134573936462, loss=2.34598970413208
I0420 13:50:49.975601 139852108011264 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8674928545951843, loss=2.3396177291870117
I0420 13:52:46.724652 139852099618560 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0420 13:54:41.763050 139852108011264 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0420 13:56:35.521287 139852099618560 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0420 13:58:27.038616 139852108011264 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0420 14:00:18.402445 139852099618560 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0420 14:02:09.568742 139852108011264 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0420 14:04:00.837082 139852099618560 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0420 14:05:52.101695 139852108011264 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0420 14:07:43.857569 139852099618560 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0420 14:09:39.653512 139852763371264 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0420 14:11:07.355852 140028105549632 spec.py:298] Evaluating on the training split.
I0420 14:11:36.334432 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 14:12:10.046520 140028105549632 spec.py:326] Evaluating on the test split.
I0420 14:12:28.656005 140028105549632 submission_runner.py:406] Time since start: 7702.69s, 	Step: 6280, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7256.92062497139, 'total_duration': 7702.690320730209, 'accumulated_submission_time': 7256.92062497139, 'accumulated_eval_time': 444.32975125312805, 'accumulated_logging_time': 1.3683106899261475}
I0420 14:12:28.675551 139852763371264 logging_writer.py:48] [6280] accumulated_eval_time=444.329751, accumulated_logging_time=1.368311, accumulated_submission_time=7256.920625, global_step=6280, preemption_count=0, score=7256.920625, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7702.690321, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 14:12:28.775499 140028105549632 checkpoints.py:356] Saving checkpoint at step: 6280
I0420 14:12:29.132625 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_6280
I0420 14:12:29.141424 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_6280.
I0420 14:12:53.701333 139852754978560 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0420 14:14:50.756034 139851610789632 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0420 14:16:46.376476 139852754978560 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0420 14:18:37.657670 139851610789632 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0420 14:20:28.895830 139852754978560 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0420 14:22:20.304357 139851610789632 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0420 14:24:14.535156 139852754978560 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0420 14:26:05.872175 139851610789632 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0420 14:27:57.079988 139852754978560 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0420 14:29:49.701798 139851610789632 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0420 14:31:50.269255 139852108011264 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0420 14:33:47.515904 139852099618560 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0420 14:35:45.006882 139852108011264 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0420 14:37:41.276704 139852099618560 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0420 14:39:36.350426 139852108011264 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0420 14:41:31.335629 139852099618560 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0420 14:43:26.268461 139852108011264 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0420 14:45:21.582891 139852099618560 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0420 14:47:16.091198 139852108011264 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0420 14:49:09.889529 139852099618560 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0420 14:51:05.021896 139852108011264 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0420 14:52:29.450996 140028105549632 spec.py:298] Evaluating on the training split.
I0420 14:52:58.462345 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 14:53:34.192039 140028105549632 spec.py:326] Evaluating on the test split.
I0420 14:53:52.993966 140028105549632 submission_runner.py:406] Time since start: 10187.03s, 	Step: 8377, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9657.20673942566, 'total_duration': 10187.028648614883, 'accumulated_submission_time': 9657.20673942566, 'accumulated_eval_time': 527.8698945045471, 'accumulated_logging_time': 1.8560171127319336}
I0420 14:53:53.014146 139853572331264 logging_writer.py:48] [8377] accumulated_eval_time=527.869895, accumulated_logging_time=1.856017, accumulated_submission_time=9657.206739, global_step=8377, preemption_count=0, score=9657.206739, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=10187.028649, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 14:53:53.113785 140028105549632 checkpoints.py:356] Saving checkpoint at step: 8377
I0420 14:53:53.517285 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8377
I0420 14:53:53.526043 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_8377.
I0420 14:54:20.860437 139853563938560 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0420 14:56:17.166451 139853496796928 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0420 14:58:08.436151 139853563938560 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0420 14:59:59.679931 139853496796928 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0420 15:01:52.376349 139853563938560 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0420 15:03:43.598767 139853496796928 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0420 15:05:34.877432 139853563938560 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0420 15:07:26.158452 139853496796928 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0420 15:09:17.436775 139853563938560 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0420 15:11:11.839832 139853572331264 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0420 15:13:04.289734 139853563938560 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0420 15:14:57.100830 139853572331264 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0420 15:16:48.224134 139853563938560 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0420 15:18:39.422532 139853572331264 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0420 15:20:31.985195 139853563938560 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0420 15:22:27.784470 139853572331264 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0420 15:24:22.279461 139853563938560 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0420 15:26:13.471763 139853572331264 logging_writer.py:48] [10100] global_step=10100, grad_norm=nan, loss=nan
I0420 15:28:04.723041 139853563938560 logging_writer.py:48] [10200] global_step=10200, grad_norm=nan, loss=nan
I0420 15:29:59.009685 139853572331264 logging_writer.py:48] [10300] global_step=10300, grad_norm=nan, loss=nan
I0420 15:31:54.310921 139853563938560 logging_writer.py:48] [10400] global_step=10400, grad_norm=nan, loss=nan
I0420 15:33:51.102143 139853572331264 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0420 15:33:54.461045 140028105549632 spec.py:298] Evaluating on the training split.
I0420 15:34:24.146328 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 15:35:00.716100 140028105549632 spec.py:326] Evaluating on the test split.
I0420 15:35:19.575463 140028105549632 submission_runner.py:406] Time since start: 12673.61s, 	Step: 10504, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12058.117155075073, 'total_duration': 12673.609982967377, 'accumulated_submission_time': 12058.117155075073, 'accumulated_eval_time': 612.9813520908356, 'accumulated_logging_time': 2.3905129432678223}
I0420 15:35:19.596536 139852988651264 logging_writer.py:48] [10504] accumulated_eval_time=612.981352, accumulated_logging_time=2.390513, accumulated_submission_time=12058.117155, global_step=10504, preemption_count=0, score=12058.117155, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12673.609983, train/ctc_loss=nan, train/wer=0.941551, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 15:35:19.696972 140028105549632 checkpoints.py:356] Saving checkpoint at step: 10504
I0420 15:35:20.069061 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_10504
I0420 15:35:20.077870 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_10504.
I0420 15:37:07.916338 139852980258560 logging_writer.py:48] [10600] global_step=10600, grad_norm=nan, loss=nan
I0420 15:38:59.167814 139852904724224 logging_writer.py:48] [10700] global_step=10700, grad_norm=nan, loss=nan
I0420 15:40:52.889734 139852980258560 logging_writer.py:48] [10800] global_step=10800, grad_norm=nan, loss=nan
I0420 15:42:44.623210 139852904724224 logging_writer.py:48] [10900] global_step=10900, grad_norm=nan, loss=nan
I0420 15:44:35.844477 139852980258560 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0420 15:46:27.098122 139852904724224 logging_writer.py:48] [11100] global_step=11100, grad_norm=nan, loss=nan
I0420 15:48:18.386545 139852980258560 logging_writer.py:48] [11200] global_step=11200, grad_norm=nan, loss=nan
I0420 15:50:09.591803 139852904724224 logging_writer.py:48] [11300] global_step=11300, grad_norm=nan, loss=nan
I0420 15:52:05.405814 139851677931264 logging_writer.py:48] [11400] global_step=11400, grad_norm=nan, loss=nan
I0420 15:54:00.392408 139851669538560 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0420 15:55:55.440506 139851677931264 logging_writer.py:48] [11600] global_step=11600, grad_norm=nan, loss=nan
I0420 15:57:46.825300 139851669538560 logging_writer.py:48] [11700] global_step=11700, grad_norm=nan, loss=nan
I0420 15:59:39.500806 139851677931264 logging_writer.py:48] [11800] global_step=11800, grad_norm=nan, loss=nan
I0420 16:01:34.312890 139851669538560 logging_writer.py:48] [11900] global_step=11900, grad_norm=nan, loss=nan
I0420 16:03:25.525102 139851677931264 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0420 16:05:16.831224 139851669538560 logging_writer.py:48] [12100] global_step=12100, grad_norm=nan, loss=nan
I0420 16:07:10.239759 139851677931264 logging_writer.py:48] [12200] global_step=12200, grad_norm=nan, loss=nan
I0420 16:09:05.373849 139851669538560 logging_writer.py:48] [12300] global_step=12300, grad_norm=nan, loss=nan
I0420 16:11:00.230488 139851677931264 logging_writer.py:48] [12400] global_step=12400, grad_norm=nan, loss=nan
I0420 16:12:56.442714 139851669538560 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0420 16:14:52.224578 139851677931264 logging_writer.py:48] [12600] global_step=12600, grad_norm=nan, loss=nan
I0420 16:15:21.139631 140028105549632 spec.py:298] Evaluating on the training split.
I0420 16:15:50.692223 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 16:16:25.291783 140028105549632 spec.py:326] Evaluating on the test split.
I0420 16:16:43.148785 140028105549632 submission_runner.py:406] Time since start: 15157.18s, 	Step: 12627, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14459.153898954391, 'total_duration': 15157.183127880096, 'accumulated_submission_time': 14459.153898954391, 'accumulated_eval_time': 694.987377166748, 'accumulated_logging_time': 2.8951730728149414}
I0420 16:16:43.168513 139854002411264 logging_writer.py:48] [12627] accumulated_eval_time=694.987377, accumulated_logging_time=2.895173, accumulated_submission_time=14459.153899, global_step=12627, preemption_count=0, score=14459.153899, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=15157.183128, train/ctc_loss=nan, train/wer=0.942641, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 16:16:43.266780 140028105549632 checkpoints.py:356] Saving checkpoint at step: 12627
I0420 16:16:43.639276 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_12627
I0420 16:16:43.648011 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_12627.
I0420 16:18:06.132880 139853994018560 logging_writer.py:48] [12700] global_step=12700, grad_norm=nan, loss=nan
I0420 16:19:57.465950 139853910091520 logging_writer.py:48] [12800] global_step=12800, grad_norm=nan, loss=nan
I0420 16:21:48.641148 139853994018560 logging_writer.py:48] [12900] global_step=12900, grad_norm=nan, loss=nan
I0420 16:23:39.900563 139853910091520 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0420 16:25:32.205011 139853994018560 logging_writer.py:48] [13100] global_step=13100, grad_norm=nan, loss=nan
I0420 16:27:26.536678 139853910091520 logging_writer.py:48] [13200] global_step=13200, grad_norm=nan, loss=nan
I0420 16:29:23.473609 139853994018560 logging_writer.py:48] [13300] global_step=13300, grad_norm=nan, loss=nan
I0420 16:31:18.840518 139853674731264 logging_writer.py:48] [13400] global_step=13400, grad_norm=nan, loss=nan
I0420 16:33:10.238870 139853666338560 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0420 16:35:05.018301 139853674731264 logging_writer.py:48] [13600] global_step=13600, grad_norm=nan, loss=nan
I0420 16:36:58.911015 139853666338560 logging_writer.py:48] [13700] global_step=13700, grad_norm=nan, loss=nan
I0420 16:38:50.071868 139853674731264 logging_writer.py:48] [13800] global_step=13800, grad_norm=nan, loss=nan
I0420 16:40:41.273755 139853666338560 logging_writer.py:48] [13900] global_step=13900, grad_norm=nan, loss=nan
I0420 16:42:32.534075 139853674731264 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0420 16:44:24.400454 139853666338560 logging_writer.py:48] [14100] global_step=14100, grad_norm=nan, loss=nan
I0420 16:46:16.636456 139853674731264 logging_writer.py:48] [14200] global_step=14200, grad_norm=nan, loss=nan
I0420 16:48:12.288851 139853666338560 logging_writer.py:48] [14300] global_step=14300, grad_norm=nan, loss=nan
I0420 16:50:07.475125 139853674731264 logging_writer.py:48] [14400] global_step=14400, grad_norm=nan, loss=nan
I0420 16:52:05.698111 139854002411264 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0420 16:54:04.525068 139853994018560 logging_writer.py:48] [14600] global_step=14600, grad_norm=nan, loss=nan
I0420 16:56:03.130534 139854002411264 logging_writer.py:48] [14700] global_step=14700, grad_norm=nan, loss=nan
I0420 16:56:44.060195 140028105549632 spec.py:298] Evaluating on the training split.
I0420 16:57:13.203221 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 16:57:47.404261 140028105549632 spec.py:326] Evaluating on the test split.
I0420 16:58:04.902017 140028105549632 submission_runner.py:406] Time since start: 17638.94s, 	Step: 14738, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16859.541453123093, 'total_duration': 17638.936552286148, 'accumulated_submission_time': 16859.541453123093, 'accumulated_eval_time': 775.8262293338776, 'accumulated_logging_time': 3.3967018127441406}
I0420 16:58:04.921734 139853418731264 logging_writer.py:48] [14738] accumulated_eval_time=775.826229, accumulated_logging_time=3.396702, accumulated_submission_time=16859.541453, global_step=14738, preemption_count=0, score=16859.541453, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=17638.936552, train/ctc_loss=nan, train/wer=0.942824, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 16:58:05.026607 140028105549632 checkpoints.py:356] Saving checkpoint at step: 14738
I0420 16:58:05.370686 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_14738
I0420 16:58:05.377798 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_14738.
I0420 16:59:18.943614 139853410338560 logging_writer.py:48] [14800] global_step=14800, grad_norm=nan, loss=nan
I0420 17:01:15.173576 139853318018816 logging_writer.py:48] [14900] global_step=14900, grad_norm=nan, loss=nan
I0420 17:03:06.355988 139853410338560 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0420 17:04:57.679226 139853318018816 logging_writer.py:48] [15100] global_step=15100, grad_norm=nan, loss=nan
I0420 17:06:54.424561 139853410338560 logging_writer.py:48] [15200] global_step=15200, grad_norm=nan, loss=nan
I0420 17:08:51.668091 139853318018816 logging_writer.py:48] [15300] global_step=15300, grad_norm=nan, loss=nan
I0420 17:10:48.695595 139853410338560 logging_writer.py:48] [15400] global_step=15400, grad_norm=nan, loss=nan
I0420 17:12:48.069189 139853091051264 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0420 17:14:42.669961 139853082658560 logging_writer.py:48] [15600] global_step=15600, grad_norm=nan, loss=nan
I0420 17:16:39.440392 139853091051264 logging_writer.py:48] [15700] global_step=15700, grad_norm=nan, loss=nan
I0420 17:18:36.123609 139853082658560 logging_writer.py:48] [15800] global_step=15800, grad_norm=nan, loss=nan
I0420 17:20:32.962245 139853091051264 logging_writer.py:48] [15900] global_step=15900, grad_norm=nan, loss=nan
I0420 17:22:28.539162 140028105549632 spec.py:298] Evaluating on the training split.
I0420 17:22:57.829310 140028105549632 spec.py:310] Evaluating on the validation split.
I0420 17:23:34.329275 140028105549632 spec.py:326] Evaluating on the test split.
I0420 17:23:51.850358 140028105549632 submission_runner.py:406] Time since start: 19185.89s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18322.687009334564, 'total_duration': 19185.886197566986, 'accumulated_submission_time': 18322.687009334564, 'accumulated_eval_time': 859.1357712745667, 'accumulated_logging_time': 3.874772787094116}
I0420 17:23:51.870556 139853280491264 logging_writer.py:48] [16000] accumulated_eval_time=859.135771, accumulated_logging_time=3.874773, accumulated_submission_time=18322.687009, global_step=16000, preemption_count=0, score=18322.687009, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=19185.886198, train/ctc_loss=nan, train/wer=0.944086, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 17:23:51.971317 140028105549632 checkpoints.py:356] Saving checkpoint at step: 16000
I0420 17:23:52.312549 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0420 17:23:52.319315 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0420 17:23:52.330318 139853272098560 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18322.687009
I0420 17:23:52.392159 140028105549632 checkpoints.py:356] Saving checkpoint at step: 16000
I0420 17:23:52.815098 140028105549632 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0420 17:23:52.823699 140028105549632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0420 17:23:53.999893 140028105549632 submission_runner.py:567] Tuning trial 1/1
I0420 17:23:54.000140 140028105549632 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0420 17:23:54.005023 140028105549632 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.489872, dtype=float32), 'train/wer': 4.79716490015201, 'validation/ctc_loss': DeviceArray(31.266993, dtype=float32), 'validation/wer': 4.321894084844041, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.504843, dtype=float32), 'test/wer': 4.575812970974753, 'test/num_examples': 2472, 'score': 55.413275480270386, 'total_duration': 242.92882680892944, 'accumulated_submission_time': 55.413275480270386, 'accumulated_eval_time': 187.5154185295105, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2073, {'train/ctc_loss': DeviceArray(5.457917, dtype=float32), 'train/wer': 0.9297903235772693, 'validation/ctc_loss': DeviceArray(5.6516786, dtype=float32), 'validation/wer': 0.8880452295728855, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.4053006, dtype=float32), 'test/wer': 0.8889769057339589, 'test/num_examples': 2472, 'score': 2455.8511288166046, 'total_duration': 2723.9541280269623, 'accumulated_submission_time': 2455.8511288166046, 'accumulated_eval_time': 267.7158601284027, 'accumulated_logging_time': 0.3645315170288086, 'global_step': 2073, 'preemption_count': 0}), (4160, {'train/ctc_loss': DeviceArray(1.1345952, dtype=float32), 'train/wer': 0.3561196494686093, 'validation/ctc_loss': DeviceArray(1.5530185, dtype=float32), 'validation/wer': 0.42054433713784023, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.1540608, dtype=float32), 'test/wer': 0.34897324964962523, 'test/num_examples': 2472, 'score': 4856.824326038361, 'total_duration': 5220.728524923325, 'accumulated_submission_time': 4856.824326038361, 'accumulated_eval_time': 363.0327754020691, 'accumulated_logging_time': 0.8243768215179443, 'global_step': 4160, 'preemption_count': 0}), (6280, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7256.92062497139, 'total_duration': 7702.690320730209, 'accumulated_submission_time': 7256.92062497139, 'accumulated_eval_time': 444.32975125312805, 'accumulated_logging_time': 1.3683106899261475, 'global_step': 6280, 'preemption_count': 0}), (8377, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9657.20673942566, 'total_duration': 10187.028648614883, 'accumulated_submission_time': 9657.20673942566, 'accumulated_eval_time': 527.8698945045471, 'accumulated_logging_time': 1.8560171127319336, 'global_step': 8377, 'preemption_count': 0}), (10504, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12058.117155075073, 'total_duration': 12673.609982967377, 'accumulated_submission_time': 12058.117155075073, 'accumulated_eval_time': 612.9813520908356, 'accumulated_logging_time': 2.3905129432678223, 'global_step': 10504, 'preemption_count': 0}), (12627, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14459.153898954391, 'total_duration': 15157.183127880096, 'accumulated_submission_time': 14459.153898954391, 'accumulated_eval_time': 694.987377166748, 'accumulated_logging_time': 2.8951730728149414, 'global_step': 12627, 'preemption_count': 0}), (14738, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16859.541453123093, 'total_duration': 17638.936552286148, 'accumulated_submission_time': 16859.541453123093, 'accumulated_eval_time': 775.8262293338776, 'accumulated_logging_time': 3.3967018127441406, 'global_step': 14738, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18322.687009334564, 'total_duration': 19185.886197566986, 'accumulated_submission_time': 18322.687009334564, 'accumulated_eval_time': 859.1357712745667, 'accumulated_logging_time': 3.874772787094116, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0420 17:23:54.005187 140028105549632 submission_runner.py:570] Timing: 18322.687009334564
I0420 17:23:54.005235 140028105549632 submission_runner.py:571] ====================
I0420 17:23:54.006210 140028105549632 submission_runner.py:631] Final librispeech_deepspeech score: 18322.687009334564
