I0420 07:03:57.783166 139760323802944 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax.
I0420 07:03:57.847825 139760323802944 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 07:03:58.681446 139760323802944 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0420 07:03:58.682127 139760323802944 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 07:03:58.685836 139760323802944 submission_runner.py:528] Using RNG seed 2283818304
I0420 07:04:01.274621 139760323802944 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 07:04:01.274842 139760323802944 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1.
I0420 07:04:01.275076 139760323802944 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/hparams.json.
I0420 07:04:01.401641 139760323802944 submission_runner.py:232] Initializing dataset.
I0420 07:04:01.651924 139760323802944 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:04:01.656892 139760323802944 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:04:01.887485 139760323802944 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:04:01.938205 139760323802944 submission_runner.py:239] Initializing model.
I0420 07:04:08.805662 139760323802944 submission_runner.py:249] Initializing optimizer.
I0420 07:04:09.189196 139760323802944 submission_runner.py:256] Initializing metrics bundle.
I0420 07:04:09.189380 139760323802944 submission_runner.py:273] Initializing checkpoint and logger.
I0420 07:04:09.190274 139760323802944 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1 with prefix checkpoint_
I0420 07:04:09.190496 139760323802944 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 07:04:09.190556 139760323802944 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 07:04:09.918761 139760323802944 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/meta_data_0.json.
I0420 07:04:09.919762 139760323802944 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/flags_0.json.
I0420 07:04:09.925898 139760323802944 submission_runner.py:309] Starting training loop.
I0420 07:04:29.948119 139584054753024 logging_writer.py:48] [0] global_step=0, grad_norm=2.650205612182617, loss=0.7334140539169312
I0420 07:04:29.961405 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:04:29.970506 139760323802944 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:04:29.974667 139760323802944 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:04:30.033368 139760323802944 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0420 07:04:45.636721 139760323802944 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0420 07:05:59.525816 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:05:59.528583 139760323802944 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:05:59.532411 139760323802944 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:05:59.585478 139760323802944 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:07:01.170717 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:07:01.173234 139760323802944 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:07:01.176828 139760323802944 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:07:01.228363 139760323802944 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:08:03.211723 139760323802944 submission_runner.py:406] Time since start: 233.29s, 	Step: 1, 	{'train/accuracy': 0.5778578519821167, 'train/loss': 0.7335151433944702, 'train/mean_average_precision': 0.020356296140873784, 'validation/accuracy': 0.5620622038841248, 'validation/loss': 0.7427594065666199, 'validation/mean_average_precision': 0.02493806083026054, 'validation/num_examples': 43793, 'test/accuracy': 0.5578607320785522, 'test/loss': 0.7448195219039917, 'test/mean_average_precision': 0.026874383400702598, 'test/num_examples': 43793, 'score': 20.035348415374756, 'total_duration': 233.28576755523682, 'accumulated_submission_time': 20.035348415374756, 'accumulated_eval_time': 213.2502760887146, 'accumulated_logging_time': 0}
I0420 07:08:03.228879 139574357063424 logging_writer.py:48] [1] accumulated_eval_time=213.250276, accumulated_logging_time=0, accumulated_submission_time=20.035348, global_step=1, preemption_count=0, score=20.035348, test/accuracy=0.557861, test/loss=0.744820, test/mean_average_precision=0.026874, test/num_examples=43793, total_duration=233.285768, train/accuracy=0.577858, train/loss=0.733515, train/mean_average_precision=0.020356, validation/accuracy=0.562062, validation/loss=0.742759, validation/mean_average_precision=0.024938, validation/num_examples=43793
I0420 07:08:03.262087 139760323802944 checkpoints.py:356] Saving checkpoint at step: 1
I0420 07:08:03.358231 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_1
I0420 07:08:03.358725 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_1.
I0420 07:08:26.647248 139574365456128 logging_writer.py:48] [100] global_step=100, grad_norm=0.3978031277656555, loss=0.3382005989551544
I0420 07:08:49.803684 139575489980160 logging_writer.py:48] [200] global_step=200, grad_norm=0.2719411849975586, loss=0.19828176498413086
I0420 07:09:12.871563 139574365456128 logging_writer.py:48] [300] global_step=300, grad_norm=0.12542907893657684, loss=0.11399064213037491
I0420 07:09:35.926630 139575489980160 logging_writer.py:48] [400] global_step=400, grad_norm=0.06723132729530334, loss=0.08055740594863892
I0420 07:09:59.190305 139574365456128 logging_writer.py:48] [500] global_step=500, grad_norm=0.061250071972608566, loss=0.06582462787628174
I0420 07:10:22.159677 139575489980160 logging_writer.py:48] [600] global_step=600, grad_norm=0.04275289550423622, loss=0.05825665593147278
I0420 07:10:45.058344 139574365456128 logging_writer.py:48] [700] global_step=700, grad_norm=0.04487323388457298, loss=0.058046381920576096
I0420 07:11:07.894515 139575489980160 logging_writer.py:48] [800] global_step=800, grad_norm=0.01675110124051571, loss=0.05610586330294609
I0420 07:11:30.747079 139574365456128 logging_writer.py:48] [900] global_step=900, grad_norm=0.04979166015982628, loss=0.05415041744709015
I0420 07:11:53.746189 139575489980160 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.030221810564398766, loss=0.0532231330871582
I0420 07:12:03.563853 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:13:14.180856 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:13:16.704154 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:13:19.155214 139760323802944 submission_runner.py:406] Time since start: 549.23s, 	Step: 1044, 	{'train/accuracy': 0.9866205453872681, 'train/loss': 0.05707420036196709, 'train/mean_average_precision': 0.03820358242541502, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06731288135051727, 'validation/mean_average_precision': 0.03754715919322598, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0709485113620758, 'test/mean_average_precision': 0.03951491457475447, 'test/num_examples': 43793, 'score': 260.2317371368408, 'total_duration': 549.2292556762695, 'accumulated_submission_time': 260.2317371368408, 'accumulated_eval_time': 288.8415973186493, 'accumulated_logging_time': 0.14745306968688965}
I0420 07:13:19.162636 139574365456128 logging_writer.py:48] [1044] accumulated_eval_time=288.841597, accumulated_logging_time=0.147453, accumulated_submission_time=260.231737, global_step=1044, preemption_count=0, score=260.231737, test/accuracy=0.983142, test/loss=0.070949, test/mean_average_precision=0.039515, test/num_examples=43793, total_duration=549.229256, train/accuracy=0.986621, train/loss=0.057074, train/mean_average_precision=0.038204, validation/accuracy=0.984118, validation/loss=0.067313, validation/mean_average_precision=0.037547, validation/num_examples=43793
I0420 07:13:19.195488 139760323802944 checkpoints.py:356] Saving checkpoint at step: 1044
I0420 07:13:19.288687 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_1044
I0420 07:13:19.288910 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_1044.
I0420 07:13:32.408756 139575489980160 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.020454946905374527, loss=0.051613420248031616
I0420 07:13:55.257313 139575464802048 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.018717488273978233, loss=0.0490313284099102
I0420 07:14:17.968981 139575489980160 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.01814805343747139, loss=0.05202019587159157
I0420 07:14:41.010474 139575464802048 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.029502565041184425, loss=0.05171310529112816
I0420 07:15:04.232180 139575489980160 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.021252505481243134, loss=0.05055055394768715
I0420 07:15:27.099887 139575464802048 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.01299810130149126, loss=0.052607182413339615
I0420 07:15:49.689666 139575489980160 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.015385215170681477, loss=0.04616176337003708
I0420 07:16:12.643638 139575464802048 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.012948506511747837, loss=0.050393905490636826
I0420 07:16:35.650218 139575489980160 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.028743643313646317, loss=0.0505189374089241
I0420 07:16:58.487241 139575464802048 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.029906095936894417, loss=0.04193385690450668
I0420 07:17:19.408957 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:18:30.960874 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:18:33.504399 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:18:35.945719 139760323802944 submission_runner.py:406] Time since start: 866.02s, 	Step: 2093, 	{'train/accuracy': 0.9871287941932678, 'train/loss': 0.04772549867630005, 'train/mean_average_precision': 0.09793634593817449, 'validation/accuracy': 0.9845266938209534, 'validation/loss': 0.057399481534957886, 'validation/mean_average_precision': 0.09592870301279535, 'validation/num_examples': 43793, 'test/accuracy': 0.9834802746772766, 'test/loss': 0.060710810124874115, 'test/mean_average_precision': 0.09554441563105949, 'test/num_examples': 43793, 'score': 500.3433496952057, 'total_duration': 866.0197651386261, 'accumulated_submission_time': 500.3433496952057, 'accumulated_eval_time': 365.37832593917847, 'accumulated_logging_time': 0.28147125244140625}
I0420 07:18:35.952899 139575489980160 logging_writer.py:48] [2093] accumulated_eval_time=365.378326, accumulated_logging_time=0.281471, accumulated_submission_time=500.343350, global_step=2093, preemption_count=0, score=500.343350, test/accuracy=0.983480, test/loss=0.060711, test/mean_average_precision=0.095544, test/num_examples=43793, total_duration=866.019765, train/accuracy=0.987129, train/loss=0.047725, train/mean_average_precision=0.097936, validation/accuracy=0.984527, validation/loss=0.057399, validation/mean_average_precision=0.095929, validation/num_examples=43793
I0420 07:18:35.988547 139760323802944 checkpoints.py:356] Saving checkpoint at step: 2093
I0420 07:18:36.086927 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_2093
I0420 07:18:36.087146 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_2093.
I0420 07:18:37.976402 139575464802048 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.04000605270266533, loss=0.04731297865509987
I0420 07:19:00.792892 139575439623936 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.02997812069952488, loss=0.045512307435274124
I0420 07:19:23.701268 139575464802048 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.023856831714510918, loss=0.04733366146683693
I0420 07:19:46.544914 139575439623936 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.025262944400310516, loss=0.04406215623021126
I0420 07:20:09.662931 139575464802048 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.018221093341708183, loss=0.046765148639678955
I0420 07:20:32.939837 139575439623936 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.01988048292696476, loss=0.043826207518577576
I0420 07:20:55.941350 139575464802048 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.020358899608254433, loss=0.051289502531290054
I0420 07:21:18.958029 139575439623936 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.04192954674363136, loss=0.04593655839562416
I0420 07:21:41.872946 139575464802048 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0269454438239336, loss=0.047272760421037674
I0420 07:22:04.584107 139575439623936 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.028267480432987213, loss=0.04526408761739731
I0420 07:22:27.306406 139575464802048 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01643943227827549, loss=0.049524448812007904
I0420 07:22:36.159295 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:23:48.468960 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:23:50.979352 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:23:53.444883 139760323802944 submission_runner.py:406] Time since start: 1183.52s, 	Step: 3140, 	{'train/accuracy': 0.9878803491592407, 'train/loss': 0.043279703706502914, 'train/mean_average_precision': 0.14628434201328494, 'validation/accuracy': 0.9850414395332336, 'validation/loss': 0.052790023386478424, 'validation/mean_average_precision': 0.13490871777385782, 'validation/num_examples': 43793, 'test/accuracy': 0.9841499924659729, 'test/loss': 0.05573071911931038, 'test/mean_average_precision': 0.13903817717088632, 'test/num_examples': 43793, 'score': 740.4068112373352, 'total_duration': 1183.518930196762, 'accumulated_submission_time': 740.4068112373352, 'accumulated_eval_time': 442.6638789176941, 'accumulated_logging_time': 0.4231843948364258}
I0420 07:23:53.452991 139575439623936 logging_writer.py:48] [3140] accumulated_eval_time=442.663879, accumulated_logging_time=0.423184, accumulated_submission_time=740.406811, global_step=3140, preemption_count=0, score=740.406811, test/accuracy=0.984150, test/loss=0.055731, test/mean_average_precision=0.139038, test/num_examples=43793, total_duration=1183.518930, train/accuracy=0.987880, train/loss=0.043280, train/mean_average_precision=0.146284, validation/accuracy=0.985041, validation/loss=0.052790, validation/mean_average_precision=0.134909, validation/num_examples=43793
I0420 07:23:53.487445 139760323802944 checkpoints.py:356] Saving checkpoint at step: 3140
I0420 07:23:53.578689 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_3140
I0420 07:23:53.578927 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_3140.
I0420 07:24:07.581305 139575464802048 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.02032569982111454, loss=0.04921070113778114
I0420 07:24:30.526400 139575431231232 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02055141143500805, loss=0.04072696715593338
I0420 07:24:53.605804 139575464802048 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.012210226617753506, loss=0.04366257041692734
I0420 07:25:16.251858 139575431231232 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.022374024614691734, loss=0.0499204620718956
I0420 07:25:38.778383 139575464802048 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.015587028115987778, loss=0.04589914157986641
I0420 07:26:01.483521 139575431231232 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.013027390465140343, loss=0.048692986369132996
I0420 07:26:24.097719 139575464802048 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.012819178402423859, loss=0.040512219071388245
I0420 07:26:46.815726 139575431231232 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.01532009243965149, loss=0.03881622105836868
I0420 07:27:09.635236 139575464802048 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.012483120895922184, loss=0.04654335603117943
I0420 07:27:32.333621 139575431231232 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.016205085441470146, loss=0.04402470588684082
I0420 07:27:53.672489 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:29:06.671574 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:29:09.189572 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:29:11.638940 139760323802944 submission_runner.py:406] Time since start: 1501.71s, 	Step: 4195, 	{'train/accuracy': 0.9881506562232971, 'train/loss': 0.04167627915740013, 'train/mean_average_precision': 0.1888073007274624, 'validation/accuracy': 0.985254168510437, 'validation/loss': 0.05110965669155121, 'validation/mean_average_precision': 0.1603246334793367, 'validation/num_examples': 43793, 'test/accuracy': 0.9843306541442871, 'test/loss': 0.05379617214202881, 'test/mean_average_precision': 0.15931854936395778, 'test/num_examples': 43793, 'score': 980.4921507835388, 'total_duration': 1501.7129778862, 'accumulated_submission_time': 980.4921507835388, 'accumulated_eval_time': 520.630286693573, 'accumulated_logging_time': 0.5574007034301758}
I0420 07:29:11.646396 139575464802048 logging_writer.py:48] [4195] accumulated_eval_time=520.630287, accumulated_logging_time=0.557401, accumulated_submission_time=980.492151, global_step=4195, preemption_count=0, score=980.492151, test/accuracy=0.984331, test/loss=0.053796, test/mean_average_precision=0.159319, test/num_examples=43793, total_duration=1501.712978, train/accuracy=0.988151, train/loss=0.041676, train/mean_average_precision=0.188807, validation/accuracy=0.985254, validation/loss=0.051110, validation/mean_average_precision=0.160325, validation/num_examples=43793
I0420 07:29:11.680938 139760323802944 checkpoints.py:356] Saving checkpoint at step: 4195
I0420 07:29:11.771702 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_4195
I0420 07:29:11.772113 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_4195.
I0420 07:29:13.145926 139575431231232 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.012878517620265484, loss=0.0420466884970665
I0420 07:29:35.717209 139575422838528 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.013414776884019375, loss=0.0425427183508873
I0420 07:29:58.601259 139575431231232 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.018580757081508636, loss=0.044532179832458496
I0420 07:30:21.415607 139575422838528 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013693325221538544, loss=0.046088967472314835
I0420 07:30:44.186063 139575431231232 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.027520600706338882, loss=0.04652778059244156
I0420 07:31:07.032726 139575422838528 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.02588782086968422, loss=0.04577400162816048
I0420 07:31:29.852770 139575431231232 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.016265248879790306, loss=0.04181337356567383
I0420 07:31:52.787074 139575422838528 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.009750463999807835, loss=0.04182079806923866
I0420 07:32:15.664886 139575431231232 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.015516807325184345, loss=0.04616204649209976
I0420 07:32:38.725447 139575422838528 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.011714546009898186, loss=0.042314253747463226
I0420 07:33:01.720932 139575431231232 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.009962576441466808, loss=0.04305744171142578
I0420 07:33:11.793185 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:34:24.640350 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:34:27.140214 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:34:29.657726 139760323802944 submission_runner.py:406] Time since start: 1819.73s, 	Step: 5245, 	{'train/accuracy': 0.9884597063064575, 'train/loss': 0.03964655101299286, 'train/mean_average_precision': 0.2246681001674411, 'validation/accuracy': 0.985529363155365, 'validation/loss': 0.049825962632894516, 'validation/mean_average_precision': 0.18364790057967567, 'validation/num_examples': 43793, 'test/accuracy': 0.9846065640449524, 'test/loss': 0.052849091589450836, 'test/mean_average_precision': 0.1845455796827599, 'test/num_examples': 43793, 'score': 1220.5049600601196, 'total_duration': 1819.7317688465118, 'accumulated_submission_time': 1220.5049600601196, 'accumulated_eval_time': 598.494788646698, 'accumulated_logging_time': 0.69101881980896}
I0420 07:34:29.665583 139575422838528 logging_writer.py:48] [5245] accumulated_eval_time=598.494789, accumulated_logging_time=0.691019, accumulated_submission_time=1220.504960, global_step=5245, preemption_count=0, score=1220.504960, test/accuracy=0.984607, test/loss=0.052849, test/mean_average_precision=0.184546, test/num_examples=43793, total_duration=1819.731769, train/accuracy=0.988460, train/loss=0.039647, train/mean_average_precision=0.224668, validation/accuracy=0.985529, validation/loss=0.049826, validation/mean_average_precision=0.183648, validation/num_examples=43793
I0420 07:34:29.697696 139760323802944 checkpoints.py:356] Saving checkpoint at step: 5245
I0420 07:34:29.782011 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_5245
I0420 07:34:29.782380 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_5245.
I0420 07:34:42.625815 139575431231232 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.008762679062783718, loss=0.04070509970188141
I0420 07:35:05.756717 139575414445824 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.00967965554445982, loss=0.03992948308587074
I0420 07:35:28.925146 139575431231232 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.018726013600826263, loss=0.0420568510890007
I0420 07:35:51.881269 139575414445824 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.01459542103111744, loss=0.04003910720348358
I0420 07:36:14.852052 139575431231232 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.014862728305161, loss=0.043619103729724884
I0420 07:36:37.854936 139575414445824 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.010008455254137516, loss=0.040164172649383545
I0420 07:37:00.861314 139575431231232 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.009412501938641071, loss=0.038221947848796844
I0420 07:37:24.377096 139575414445824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.011074449867010117, loss=0.044914666563272476
I0420 07:37:47.257231 139575431231232 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.010615515522658825, loss=0.04148063063621521
I0420 07:38:10.381250 139575414445824 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.010029187425971031, loss=0.041534341871738434
I0420 07:38:29.867816 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:39:41.619439 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:39:44.117170 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:39:46.577790 139760323802944 submission_runner.py:406] Time since start: 2136.65s, 	Step: 6286, 	{'train/accuracy': 0.9886367321014404, 'train/loss': 0.03852221742272377, 'train/mean_average_precision': 0.25258361941607044, 'validation/accuracy': 0.9858590364456177, 'validation/loss': 0.04843391478061676, 'validation/mean_average_precision': 0.19577097594184756, 'validation/num_examples': 43793, 'test/accuracy': 0.9849308729171753, 'test/loss': 0.05121367424726486, 'test/mean_average_precision': 0.19298187868883007, 'test/num_examples': 43793, 'score': 1460.581959247589, 'total_duration': 2136.651834487915, 'accumulated_submission_time': 1460.581959247589, 'accumulated_eval_time': 675.2047262191772, 'accumulated_logging_time': 0.8160598278045654}
I0420 07:39:46.585754 139575431231232 logging_writer.py:48] [6286] accumulated_eval_time=675.204726, accumulated_logging_time=0.816060, accumulated_submission_time=1460.581959, global_step=6286, preemption_count=0, score=1460.581959, test/accuracy=0.984931, test/loss=0.051214, test/mean_average_precision=0.192982, test/num_examples=43793, total_duration=2136.651834, train/accuracy=0.988637, train/loss=0.038522, train/mean_average_precision=0.252584, validation/accuracy=0.985859, validation/loss=0.048434, validation/mean_average_precision=0.195771, validation/num_examples=43793
I0420 07:39:46.621150 139760323802944 checkpoints.py:356] Saving checkpoint at step: 6286
I0420 07:39:46.706181 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_6286
I0420 07:39:46.706572 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_6286.
I0420 07:39:50.170072 139575414445824 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.01429347787052393, loss=0.03978823497891426
I0420 07:40:12.721163 139575406053120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.010256820358335972, loss=0.039950817823410034
I0420 07:40:35.457060 139575414445824 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.008917827159166336, loss=0.036795541644096375
I0420 07:40:58.657745 139575406053120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.011222436092793941, loss=0.04251033812761307
I0420 07:41:21.691058 139575414445824 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.010508240200579166, loss=0.03784862905740738
I0420 07:41:44.783866 139575406053120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.009781114757061005, loss=0.04086729511618614
I0420 07:42:07.878474 139575414445824 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.012301349081099033, loss=0.04398355633020401
I0420 07:42:30.758319 139575406053120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.008630512282252312, loss=0.039107076823711395
I0420 07:42:53.287062 139575414445824 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.009954817593097687, loss=0.035568103194236755
I0420 07:43:16.028497 139575406053120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.009797687642276287, loss=0.039120450615882874
I0420 07:43:38.999129 139575414445824 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.012045501731336117, loss=0.04004007577896118
I0420 07:43:46.862859 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:44:58.358249 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:45:00.860966 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:45:03.336888 139760323802944 submission_runner.py:406] Time since start: 2453.41s, 	Step: 7335, 	{'train/accuracy': 0.9895780086517334, 'train/loss': 0.03556513041257858, 'train/mean_average_precision': 0.31001753019228945, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.04672541469335556, 'validation/mean_average_precision': 0.2120275757688204, 'validation/num_examples': 43793, 'test/accuracy': 0.9852160215377808, 'test/loss': 0.049437928944826126, 'test/mean_average_precision': 0.21066157579744885, 'test/num_examples': 43793, 'score': 1700.729764699936, 'total_duration': 2453.4109086990356, 'accumulated_submission_time': 1700.729764699936, 'accumulated_eval_time': 751.678774356842, 'accumulated_logging_time': 0.9452645778656006}
I0420 07:45:03.344521 139575406053120 logging_writer.py:48] [7335] accumulated_eval_time=751.678774, accumulated_logging_time=0.945265, accumulated_submission_time=1700.729765, global_step=7335, preemption_count=0, score=1700.729765, test/accuracy=0.985216, test/loss=0.049438, test/mean_average_precision=0.210662, test/num_examples=43793, total_duration=2453.410909, train/accuracy=0.989578, train/loss=0.035565, train/mean_average_precision=0.310018, validation/accuracy=0.986132, validation/loss=0.046725, validation/mean_average_precision=0.212028, validation/num_examples=43793
I0420 07:45:03.379693 139760323802944 checkpoints.py:356] Saving checkpoint at step: 7335
I0420 07:45:03.465513 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_7335
I0420 07:45:03.465914 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_7335.
I0420 07:45:18.182820 139575414445824 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.009071771055459976, loss=0.03606809675693512
I0420 07:45:40.562215 139575397660416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.009963304735720158, loss=0.04193824157118797
I0420 07:46:03.185765 139575414445824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01263249572366476, loss=0.037040825933218
I0420 07:46:25.657297 139575397660416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.012160424143075943, loss=0.04020623117685318
I0420 07:46:48.079870 139575414445824 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.01062466949224472, loss=0.042024414986371994
I0420 07:47:10.550625 139575397660416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.008286351338028908, loss=0.03643956407904625
I0420 07:47:33.281524 139575414445824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.009952831082046032, loss=0.03938942775130272
I0420 07:47:55.366378 139575397660416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.011578557081520557, loss=0.039871007204055786
I0420 07:48:18.393336 139575414445824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.009059068746864796, loss=0.04024708271026611
I0420 07:48:40.878124 139575397660416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.012491397559642792, loss=0.03661324083805084
I0420 07:49:03.445358 139575414445824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.008441023528575897, loss=0.03648974001407623
I0420 07:49:03.685870 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:50:17.847997 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:50:20.512037 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:50:23.183576 139760323802944 submission_runner.py:406] Time since start: 2773.26s, 	Step: 8402, 	{'train/accuracy': 0.9897110462188721, 'train/loss': 0.034641921520233154, 'train/mean_average_precision': 0.3182698847448101, 'validation/accuracy': 0.9863205552101135, 'validation/loss': 0.046030428260564804, 'validation/mean_average_precision': 0.22806243331232817, 'validation/num_examples': 43793, 'test/accuracy': 0.9853895902633667, 'test/loss': 0.048743635416030884, 'test/mean_average_precision': 0.22814002529091723, 'test/num_examples': 43793, 'score': 1940.9409573078156, 'total_duration': 2773.257586479187, 'accumulated_submission_time': 1940.9409573078156, 'accumulated_eval_time': 831.1764204502106, 'accumulated_logging_time': 1.074728012084961}
I0420 07:50:23.192456 139575397660416 logging_writer.py:48] [8402] accumulated_eval_time=831.176420, accumulated_logging_time=1.074728, accumulated_submission_time=1940.940957, global_step=8402, preemption_count=0, score=1940.940957, test/accuracy=0.985390, test/loss=0.048744, test/mean_average_precision=0.228140, test/num_examples=43793, total_duration=2773.257586, train/accuracy=0.989711, train/loss=0.034642, train/mean_average_precision=0.318270, validation/accuracy=0.986321, validation/loss=0.046030, validation/mean_average_precision=0.228062, validation/num_examples=43793
I0420 07:50:23.220915 139760323802944 checkpoints.py:356] Saving checkpoint at step: 8402
I0420 07:50:23.336777 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_8402
I0420 07:50:23.337059 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_8402.
I0420 07:50:45.653895 139575414445824 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.011940102092921734, loss=0.0395781509578228
I0420 07:51:08.553035 139575389267712 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.010652574710547924, loss=0.03746524453163147
I0420 07:51:31.870694 139575414445824 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.010183997452259064, loss=0.037814825773239136
I0420 07:51:54.875908 139575389267712 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01000580471009016, loss=0.03728282079100609
I0420 07:52:17.478215 139575414445824 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.010475869290530682, loss=0.0383308120071888
I0420 07:52:40.082981 139575389267712 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.009522978216409683, loss=0.03746899217367172
I0420 07:53:02.447743 139575414445824 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.011221571825444698, loss=0.041388608515262604
I0420 07:53:24.878761 139575389267712 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.010227869264781475, loss=0.03855789080262184
I0420 07:53:47.423723 139575414445824 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.008595689199864864, loss=0.03799310699105263
I0420 07:54:10.030067 139575389267712 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.01447256002575159, loss=0.04109877720475197
I0420 07:54:23.341868 139760323802944 spec.py:298] Evaluating on the training split.
I0420 07:55:33.776076 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 07:55:36.285499 139760323802944 spec.py:326] Evaluating on the test split.
I0420 07:55:38.751486 139760323802944 submission_runner.py:406] Time since start: 3088.83s, 	Step: 9460, 	{'train/accuracy': 0.9899895787239075, 'train/loss': 0.0336056612432003, 'train/mean_average_precision': 0.33899125470069913, 'validation/accuracy': 0.9864119291305542, 'validation/loss': 0.04601022228598595, 'validation/mean_average_precision': 0.23853721560585386, 'validation/num_examples': 43793, 'test/accuracy': 0.985514223575592, 'test/loss': 0.04892144352197647, 'test/mean_average_precision': 0.23302228368068428, 'test/num_examples': 43793, 'score': 2180.9366676807404, 'total_duration': 3088.82550573349, 'accumulated_submission_time': 2180.9366676807404, 'accumulated_eval_time': 906.5859742164612, 'accumulated_logging_time': 1.2285733222961426}
I0420 07:55:38.759332 139575414445824 logging_writer.py:48] [9460] accumulated_eval_time=906.585974, accumulated_logging_time=1.228573, accumulated_submission_time=2180.936668, global_step=9460, preemption_count=0, score=2180.936668, test/accuracy=0.985514, test/loss=0.048921, test/mean_average_precision=0.233022, test/num_examples=43793, total_duration=3088.825506, train/accuracy=0.989990, train/loss=0.033606, train/mean_average_precision=0.338991, validation/accuracy=0.986412, validation/loss=0.046010, validation/mean_average_precision=0.238537, validation/num_examples=43793
I0420 07:55:38.791025 139760323802944 checkpoints.py:356] Saving checkpoint at step: 9460
I0420 07:55:38.881678 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_9460
I0420 07:55:38.881892 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_9460.
I0420 07:55:48.062365 139575389267712 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.011708177626132965, loss=0.03836756944656372
I0420 07:56:10.367981 139575380875008 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.013311480171978474, loss=0.03972844034433365
I0420 07:56:32.804044 139575389267712 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.010143269784748554, loss=0.035919446498155594
I0420 07:56:54.797361 139575380875008 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.015185792930424213, loss=0.04183832183480263
I0420 07:57:17.228885 139575389267712 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.009185796603560448, loss=0.03862321004271507
I0420 07:57:39.458212 139575380875008 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.011495384387671947, loss=0.03233419358730316
I0420 07:58:01.763544 139575389267712 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.00851111114025116, loss=0.036855071783065796
I0420 07:58:24.125803 139575380875008 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.009552168659865856, loss=0.037687189877033234
I0420 07:58:46.413465 139575389267712 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.01112288050353527, loss=0.03479229658842087
I0420 07:59:08.848049 139575380875008 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.01281773392111063, loss=0.03691260516643524
I0420 07:59:31.268477 139575389267712 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.009628904052078724, loss=0.03460084646940231
I0420 07:59:38.903562 139760323802944 spec.py:298] Evaluating on the training split.
I0420 08:00:50.289703 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 08:00:52.822939 139760323802944 spec.py:326] Evaluating on the test split.
I0420 08:00:55.279046 139760323802944 submission_runner.py:406] Time since start: 3405.35s, 	Step: 10535, 	{'train/accuracy': 0.9905030727386475, 'train/loss': 0.031673625111579895, 'train/mean_average_precision': 0.38923823849114736, 'validation/accuracy': 0.9865162372589111, 'validation/loss': 0.045691825449466705, 'validation/mean_average_precision': 0.2433247904975927, 'validation/num_examples': 43793, 'test/accuracy': 0.9856077432632446, 'test/loss': 0.048573095351457596, 'test/mean_average_precision': 0.23513250284387469, 'test/num_examples': 43793, 'score': 2420.9499390125275, 'total_duration': 3405.3530921936035, 'accumulated_submission_time': 2420.9499390125275, 'accumulated_eval_time': 982.9614214897156, 'accumulated_logging_time': 1.359236478805542}
I0420 08:00:55.287126 139575380875008 logging_writer.py:48] [10535] accumulated_eval_time=982.961421, accumulated_logging_time=1.359236, accumulated_submission_time=2420.949939, global_step=10535, preemption_count=0, score=2420.949939, test/accuracy=0.985608, test/loss=0.048573, test/mean_average_precision=0.235133, test/num_examples=43793, total_duration=3405.353092, train/accuracy=0.990503, train/loss=0.031674, train/mean_average_precision=0.389238, validation/accuracy=0.986516, validation/loss=0.045692, validation/mean_average_precision=0.243325, validation/num_examples=43793
I0420 08:00:55.318722 139760323802944 checkpoints.py:356] Saving checkpoint at step: 10535
I0420 08:00:55.411828 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_10535
I0420 08:00:55.412235 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_10535.
I0420 08:01:10.147535 139575389267712 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.009152482263743877, loss=0.034909117966890335
I0420 08:01:32.533962 139575297046272 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.013603821396827698, loss=0.03722960874438286
I0420 08:01:55.056258 139575389267712 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.00992137473076582, loss=0.033204514533281326
I0420 08:02:17.343250 139575297046272 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.008290264755487442, loss=0.034571509808301926
I0420 08:02:39.444871 139575389267712 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.01062670350074768, loss=0.03452474996447563
I0420 08:03:01.766450 139575297046272 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.00999038852751255, loss=0.0321844108402729
I0420 08:03:24.367020 139575389267712 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.010727619752287865, loss=0.03597819060087204
I0420 08:03:47.049634 139575297046272 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.01077969092875719, loss=0.035644274204969406
I0420 08:04:09.224520 139575389267712 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.011438651010394096, loss=0.03458436578512192
I0420 08:04:31.357133 139575297046272 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.011901681311428547, loss=0.03614324703812599
I0420 08:04:53.387178 139575389267712 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.012520375661551952, loss=0.035244327038526535
I0420 08:04:55.622771 139760323802944 spec.py:298] Evaluating on the training split.
I0420 08:06:04.257975 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 08:06:06.766596 139760323802944 spec.py:326] Evaluating on the test split.
I0420 08:06:09.210794 139760323802944 submission_runner.py:406] Time since start: 3719.28s, 	Step: 11611, 	{'train/accuracy': 0.9905399680137634, 'train/loss': 0.0313858836889267, 'train/mean_average_precision': 0.3975584758349207, 'validation/accuracy': 0.9865503311157227, 'validation/loss': 0.04547663778066635, 'validation/mean_average_precision': 0.24729493261395127, 'validation/num_examples': 43793, 'test/accuracy': 0.9856292009353638, 'test/loss': 0.04834727942943573, 'test/mean_average_precision': 0.23727242329867002, 'test/num_examples': 43793, 'score': 2661.1519672870636, 'total_duration': 3719.2848029136658, 'accumulated_submission_time': 2661.1519672870636, 'accumulated_eval_time': 1056.5493795871735, 'accumulated_logging_time': 1.492788553237915}
I0420 08:06:09.218833 139575297046272 logging_writer.py:48] [11611] accumulated_eval_time=1056.549380, accumulated_logging_time=1.492789, accumulated_submission_time=2661.151967, global_step=11611, preemption_count=0, score=2661.151967, test/accuracy=0.985629, test/loss=0.048347, test/mean_average_precision=0.237272, test/num_examples=43793, total_duration=3719.284803, train/accuracy=0.990540, train/loss=0.031386, train/mean_average_precision=0.397558, validation/accuracy=0.986550, validation/loss=0.045477, validation/mean_average_precision=0.247295, validation/num_examples=43793
I0420 08:06:09.253639 139760323802944 checkpoints.py:356] Saving checkpoint at step: 11611
I0420 08:06:09.342069 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_11611
I0420 08:06:09.342451 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_11611.
I0420 08:06:29.518249 139575389267712 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.009666647762060165, loss=0.03363344818353653
I0420 08:06:52.242332 139575288653568 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.011547917500138283, loss=0.034475214779376984
I0420 08:07:14.993395 139575389267712 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.014718432910740376, loss=0.03266172111034393
I0420 08:07:37.540135 139760323802944 spec.py:298] Evaluating on the training split.
I0420 08:08:45.617873 139760323802944 spec.py:310] Evaluating on the validation split.
I0420 08:08:48.115779 139760323802944 spec.py:326] Evaluating on the test split.
I0420 08:08:50.557187 139760323802944 submission_runner.py:406] Time since start: 3880.63s, 	Step: 12000, 	{'train/accuracy': 0.9908250570297241, 'train/loss': 0.03028123453259468, 'train/mean_average_precision': 0.43101157715082966, 'validation/accuracy': 0.9865357279777527, 'validation/loss': 0.04525131732225418, 'validation/mean_average_precision': 0.24381830249888373, 'validation/num_examples': 43793, 'test/accuracy': 0.9856444001197815, 'test/loss': 0.04806329309940338, 'test/mean_average_precision': 0.23796365145257717, 'test/num_examples': 43793, 'score': 2749.346127986908, 'total_duration': 3880.6312170028687, 'accumulated_submission_time': 2749.346127986908, 'accumulated_eval_time': 1129.566383600235, 'accumulated_logging_time': 1.6248369216918945}
I0420 08:08:50.565240 139575288653568 logging_writer.py:48] [12000] accumulated_eval_time=1129.566384, accumulated_logging_time=1.624837, accumulated_submission_time=2749.346128, global_step=12000, preemption_count=0, score=2749.346128, test/accuracy=0.985644, test/loss=0.048063, test/mean_average_precision=0.237964, test/num_examples=43793, total_duration=3880.631217, train/accuracy=0.990825, train/loss=0.030281, train/mean_average_precision=0.431012, validation/accuracy=0.986536, validation/loss=0.045251, validation/mean_average_precision=0.243818, validation/num_examples=43793
I0420 08:08:50.598257 139760323802944 checkpoints.py:356] Saving checkpoint at step: 12000
I0420 08:08:50.683853 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_12000
I0420 08:08:50.684221 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_12000.
I0420 08:08:50.691461 139575389267712 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2749.346128
I0420 08:08:50.717346 139760323802944 checkpoints.py:356] Saving checkpoint at step: 12000
I0420 08:08:50.851991 139760323802944 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_12000
I0420 08:08:50.852397 139760323802944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/ogbg_jax/trial_1/checkpoint_12000.
I0420 08:08:50.980858 139760323802944 submission_runner.py:567] Tuning trial 1/1
I0420 08:08:50.981069 139760323802944 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 08:08:50.982102 139760323802944 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5778578519821167, 'train/loss': 0.7335151433944702, 'train/mean_average_precision': 0.020356296140873784, 'validation/accuracy': 0.5620622038841248, 'validation/loss': 0.7427594065666199, 'validation/mean_average_precision': 0.02493806083026054, 'validation/num_examples': 43793, 'test/accuracy': 0.5578607320785522, 'test/loss': 0.7448195219039917, 'test/mean_average_precision': 0.026874383400702598, 'test/num_examples': 43793, 'score': 20.035348415374756, 'total_duration': 233.28576755523682, 'accumulated_submission_time': 20.035348415374756, 'accumulated_eval_time': 213.2502760887146, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1044, {'train/accuracy': 0.9866205453872681, 'train/loss': 0.05707420036196709, 'train/mean_average_precision': 0.03820358242541502, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06731288135051727, 'validation/mean_average_precision': 0.03754715919322598, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0709485113620758, 'test/mean_average_precision': 0.03951491457475447, 'test/num_examples': 43793, 'score': 260.2317371368408, 'total_duration': 549.2292556762695, 'accumulated_submission_time': 260.2317371368408, 'accumulated_eval_time': 288.8415973186493, 'accumulated_logging_time': 0.14745306968688965, 'global_step': 1044, 'preemption_count': 0}), (2093, {'train/accuracy': 0.9871287941932678, 'train/loss': 0.04772549867630005, 'train/mean_average_precision': 0.09793634593817449, 'validation/accuracy': 0.9845266938209534, 'validation/loss': 0.057399481534957886, 'validation/mean_average_precision': 0.09592870301279535, 'validation/num_examples': 43793, 'test/accuracy': 0.9834802746772766, 'test/loss': 0.060710810124874115, 'test/mean_average_precision': 0.09554441563105949, 'test/num_examples': 43793, 'score': 500.3433496952057, 'total_duration': 866.0197651386261, 'accumulated_submission_time': 500.3433496952057, 'accumulated_eval_time': 365.37832593917847, 'accumulated_logging_time': 0.28147125244140625, 'global_step': 2093, 'preemption_count': 0}), (3140, {'train/accuracy': 0.9878803491592407, 'train/loss': 0.043279703706502914, 'train/mean_average_precision': 0.14628434201328494, 'validation/accuracy': 0.9850414395332336, 'validation/loss': 0.052790023386478424, 'validation/mean_average_precision': 0.13490871777385782, 'validation/num_examples': 43793, 'test/accuracy': 0.9841499924659729, 'test/loss': 0.05573071911931038, 'test/mean_average_precision': 0.13903817717088632, 'test/num_examples': 43793, 'score': 740.4068112373352, 'total_duration': 1183.518930196762, 'accumulated_submission_time': 740.4068112373352, 'accumulated_eval_time': 442.6638789176941, 'accumulated_logging_time': 0.4231843948364258, 'global_step': 3140, 'preemption_count': 0}), (4195, {'train/accuracy': 0.9881506562232971, 'train/loss': 0.04167627915740013, 'train/mean_average_precision': 0.1888073007274624, 'validation/accuracy': 0.985254168510437, 'validation/loss': 0.05110965669155121, 'validation/mean_average_precision': 0.1603246334793367, 'validation/num_examples': 43793, 'test/accuracy': 0.9843306541442871, 'test/loss': 0.05379617214202881, 'test/mean_average_precision': 0.15931854936395778, 'test/num_examples': 43793, 'score': 980.4921507835388, 'total_duration': 1501.7129778862, 'accumulated_submission_time': 980.4921507835388, 'accumulated_eval_time': 520.630286693573, 'accumulated_logging_time': 0.5574007034301758, 'global_step': 4195, 'preemption_count': 0}), (5245, {'train/accuracy': 0.9884597063064575, 'train/loss': 0.03964655101299286, 'train/mean_average_precision': 0.2246681001674411, 'validation/accuracy': 0.985529363155365, 'validation/loss': 0.049825962632894516, 'validation/mean_average_precision': 0.18364790057967567, 'validation/num_examples': 43793, 'test/accuracy': 0.9846065640449524, 'test/loss': 0.052849091589450836, 'test/mean_average_precision': 0.1845455796827599, 'test/num_examples': 43793, 'score': 1220.5049600601196, 'total_duration': 1819.7317688465118, 'accumulated_submission_time': 1220.5049600601196, 'accumulated_eval_time': 598.494788646698, 'accumulated_logging_time': 0.69101881980896, 'global_step': 5245, 'preemption_count': 0}), (6286, {'train/accuracy': 0.9886367321014404, 'train/loss': 0.03852221742272377, 'train/mean_average_precision': 0.25258361941607044, 'validation/accuracy': 0.9858590364456177, 'validation/loss': 0.04843391478061676, 'validation/mean_average_precision': 0.19577097594184756, 'validation/num_examples': 43793, 'test/accuracy': 0.9849308729171753, 'test/loss': 0.05121367424726486, 'test/mean_average_precision': 0.19298187868883007, 'test/num_examples': 43793, 'score': 1460.581959247589, 'total_duration': 2136.651834487915, 'accumulated_submission_time': 1460.581959247589, 'accumulated_eval_time': 675.2047262191772, 'accumulated_logging_time': 0.8160598278045654, 'global_step': 6286, 'preemption_count': 0}), (7335, {'train/accuracy': 0.9895780086517334, 'train/loss': 0.03556513041257858, 'train/mean_average_precision': 0.31001753019228945, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.04672541469335556, 'validation/mean_average_precision': 0.2120275757688204, 'validation/num_examples': 43793, 'test/accuracy': 0.9852160215377808, 'test/loss': 0.049437928944826126, 'test/mean_average_precision': 0.21066157579744885, 'test/num_examples': 43793, 'score': 1700.729764699936, 'total_duration': 2453.4109086990356, 'accumulated_submission_time': 1700.729764699936, 'accumulated_eval_time': 751.678774356842, 'accumulated_logging_time': 0.9452645778656006, 'global_step': 7335, 'preemption_count': 0}), (8402, {'train/accuracy': 0.9897110462188721, 'train/loss': 0.034641921520233154, 'train/mean_average_precision': 0.3182698847448101, 'validation/accuracy': 0.9863205552101135, 'validation/loss': 0.046030428260564804, 'validation/mean_average_precision': 0.22806243331232817, 'validation/num_examples': 43793, 'test/accuracy': 0.9853895902633667, 'test/loss': 0.048743635416030884, 'test/mean_average_precision': 0.22814002529091723, 'test/num_examples': 43793, 'score': 1940.9409573078156, 'total_duration': 2773.257586479187, 'accumulated_submission_time': 1940.9409573078156, 'accumulated_eval_time': 831.1764204502106, 'accumulated_logging_time': 1.074728012084961, 'global_step': 8402, 'preemption_count': 0}), (9460, {'train/accuracy': 0.9899895787239075, 'train/loss': 0.0336056612432003, 'train/mean_average_precision': 0.33899125470069913, 'validation/accuracy': 0.9864119291305542, 'validation/loss': 0.04601022228598595, 'validation/mean_average_precision': 0.23853721560585386, 'validation/num_examples': 43793, 'test/accuracy': 0.985514223575592, 'test/loss': 0.04892144352197647, 'test/mean_average_precision': 0.23302228368068428, 'test/num_examples': 43793, 'score': 2180.9366676807404, 'total_duration': 3088.82550573349, 'accumulated_submission_time': 2180.9366676807404, 'accumulated_eval_time': 906.5859742164612, 'accumulated_logging_time': 1.2285733222961426, 'global_step': 9460, 'preemption_count': 0}), (10535, {'train/accuracy': 0.9905030727386475, 'train/loss': 0.031673625111579895, 'train/mean_average_precision': 0.38923823849114736, 'validation/accuracy': 0.9865162372589111, 'validation/loss': 0.045691825449466705, 'validation/mean_average_precision': 0.2433247904975927, 'validation/num_examples': 43793, 'test/accuracy': 0.9856077432632446, 'test/loss': 0.048573095351457596, 'test/mean_average_precision': 0.23513250284387469, 'test/num_examples': 43793, 'score': 2420.9499390125275, 'total_duration': 3405.3530921936035, 'accumulated_submission_time': 2420.9499390125275, 'accumulated_eval_time': 982.9614214897156, 'accumulated_logging_time': 1.359236478805542, 'global_step': 10535, 'preemption_count': 0}), (11611, {'train/accuracy': 0.9905399680137634, 'train/loss': 0.0313858836889267, 'train/mean_average_precision': 0.3975584758349207, 'validation/accuracy': 0.9865503311157227, 'validation/loss': 0.04547663778066635, 'validation/mean_average_precision': 0.24729493261395127, 'validation/num_examples': 43793, 'test/accuracy': 0.9856292009353638, 'test/loss': 0.04834727942943573, 'test/mean_average_precision': 0.23727242329867002, 'test/num_examples': 43793, 'score': 2661.1519672870636, 'total_duration': 3719.2848029136658, 'accumulated_submission_time': 2661.1519672870636, 'accumulated_eval_time': 1056.5493795871735, 'accumulated_logging_time': 1.492788553237915, 'global_step': 11611, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9908250570297241, 'train/loss': 0.03028123453259468, 'train/mean_average_precision': 0.43101157715082966, 'validation/accuracy': 0.9865357279777527, 'validation/loss': 0.04525131732225418, 'validation/mean_average_precision': 0.24381830249888373, 'validation/num_examples': 43793, 'test/accuracy': 0.9856444001197815, 'test/loss': 0.04806329309940338, 'test/mean_average_precision': 0.23796365145257717, 'test/num_examples': 43793, 'score': 2749.346127986908, 'total_duration': 3880.6312170028687, 'accumulated_submission_time': 2749.346127986908, 'accumulated_eval_time': 1129.566383600235, 'accumulated_logging_time': 1.6248369216918945, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0420 08:08:50.982217 139760323802944 submission_runner.py:570] Timing: 2749.346127986908
I0420 08:08:50.982266 139760323802944 submission_runner.py:571] ====================
I0420 08:08:50.982381 139760323802944 submission_runner.py:631] Final ogbg score: 2749.346127986908
