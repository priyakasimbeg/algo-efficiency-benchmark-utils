python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_jax_05-02-2023-17-37-27.log
I0502 17:37:46.408401 140324676323136 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax.
I0502 17:37:46.554150 140324676323136 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 17:37:47.487497 140324676323136 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0502 17:37:47.488250 140324676323136 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 17:37:47.492341 140324676323136 submission_runner.py:538] Using RNG seed 694299437
I0502 17:37:50.090394 140324676323136 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 17:37:50.090588 140324676323136 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1.
I0502 17:37:50.090862 140324676323136 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1/hparams.json.
I0502 17:37:50.216597 140324676323136 submission_runner.py:241] Initializing dataset.
I0502 17:37:50.216784 140324676323136 submission_runner.py:248] Initializing model.
I0502 17:37:56.210048 140324676323136 submission_runner.py:258] Initializing optimizer.
I0502 17:37:59.091028 140324676323136 submission_runner.py:265] Initializing metrics bundle.
I0502 17:37:59.091232 140324676323136 submission_runner.py:282] Initializing checkpoint and logger.
I0502 17:37:59.095371 140324676323136 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1 with prefix checkpoint_
I0502 17:37:59.095642 140324676323136 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 17:37:59.095706 140324676323136 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 17:37:59.876900 140324676323136 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1/meta_data_0.json.
I0502 17:37:59.877911 140324676323136 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1/flags_0.json.
I0502 17:37:59.928467 140324676323136 submission_runner.py:318] Starting training loop.
I0502 17:38:25.823610 140148634281728 logging_writer.py:48] [0] global_step=0, grad_norm=11.121109962463379, loss=1.5278346538543701
I0502 17:38:25.831647 140324676323136 spec.py:298] Evaluating on the training split.
I0502 17:42:43.287293 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 17:47:00.169116 140324676323136 spec.py:326] Evaluating on the test split.
I0502 17:51:16.194969 140324676323136 submission_runner.py:415] Time since start: 796.27s, 	Step: 1, 	{'train/loss': 1.5302069680141648, 'validation/loss': 1.5123906516853933, 'validation/num_examples': 89000000, 'test/loss': 1.523783468310266, 'test/num_examples': 89274637, 'score': 25.902984857559204, 'total_duration': 796.2663962841034, 'accumulated_submission_time': 25.902984857559204, 'accumulated_eval_time': 770.3632137775421, 'accumulated_logging_time': 0}
I0502 17:51:16.212011 140135941084928 logging_writer.py:48] [1] accumulated_eval_time=770.363214, accumulated_logging_time=0, accumulated_submission_time=25.902985, global_step=1, preemption_count=0, score=25.902985, test/loss=1.523783, test/num_examples=89274637, total_duration=796.266396, train/loss=1.530207, validation/loss=1.512391, validation/num_examples=89000000
I0502 17:52:14.172438 140135857387264 logging_writer.py:48] [100] global_step=100, grad_norm=0.21071654558181763, loss=0.17095156013965607
I0502 17:53:16.701075 140324676323136 spec.py:298] Evaluating on the training split.
I0502 17:57:27.435133 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 18:01:38.273041 140324676323136 spec.py:326] Evaluating on the test split.
I0502 18:05:34.508409 140324676323136 submission_runner.py:415] Time since start: 1654.58s, 	Step: 182, 	{'train/loss': 0.14085781161972843, 'validation/loss': 0.13930211235955056, 'validation/num_examples': 89000000, 'test/loss': 0.14266886349815122, 'test/num_examples': 89274637, 'score': 146.38195204734802, 'total_duration': 1654.579845905304, 'accumulated_submission_time': 146.38195204734802, 'accumulated_eval_time': 1508.1704812049866, 'accumulated_logging_time': 0.024768352508544922}
I0502 18:05:34.516191 140135941084928 logging_writer.py:48] [182] accumulated_eval_time=1508.170481, accumulated_logging_time=0.024768, accumulated_submission_time=146.381952, global_step=182, preemption_count=0, score=146.381952, test/loss=0.142669, test/num_examples=89274637, total_duration=1654.579846, train/loss=0.140858, validation/loss=0.139302, validation/num_examples=89000000
I0502 18:05:37.052522 140135857387264 logging_writer.py:48] [200] global_step=200, grad_norm=0.024427128955721855, loss=0.1365031898021698
I0502 18:06:50.296318 140135941084928 logging_writer.py:48] [300] global_step=300, grad_norm=0.2959914207458496, loss=0.141459122300148
I0502 18:07:34.781621 140324676323136 spec.py:298] Evaluating on the training split.
I0502 18:11:52.930070 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 18:16:04.364955 140324676323136 spec.py:326] Evaluating on the test split.
I0502 18:20:14.021828 140324676323136 submission_runner.py:415] Time since start: 2534.09s, 	Step: 358, 	{'train/loss': 0.13642412849521707, 'validation/loss': 0.13625047191011236, 'validation/num_examples': 89000000, 'test/loss': 0.13986953539783084, 'test/num_examples': 89274637, 'score': 266.63829469680786, 'total_duration': 2534.0932693481445, 'accumulated_submission_time': 266.63829469680786, 'accumulated_eval_time': 2267.4106163978577, 'accumulated_logging_time': 0.0394594669342041}
I0502 18:20:14.029936 140135857387264 logging_writer.py:48] [358] accumulated_eval_time=2267.410616, accumulated_logging_time=0.039459, accumulated_submission_time=266.638295, global_step=358, preemption_count=0, score=266.638295, test/loss=0.139870, test/num_examples=89274637, total_duration=2534.093269, train/loss=0.136424, validation/loss=0.136250, validation/num_examples=89000000
I0502 18:20:28.939843 140135941084928 logging_writer.py:48] [400] global_step=400, grad_norm=0.4261934459209442, loss=0.12721586227416992
I0502 18:21:45.934429 140135857387264 logging_writer.py:48] [500] global_step=500, grad_norm=0.599270224571228, loss=0.13069207966327667
I0502 18:22:14.513108 140324676323136 spec.py:298] Evaluating on the training split.
I0502 18:26:25.435678 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 18:30:38.578764 140324676323136 spec.py:326] Evaluating on the test split.
I0502 18:34:39.296843 140324676323136 submission_runner.py:415] Time since start: 3399.37s, 	Step: 538, 	{'train/loss': 0.1330295138888889, 'validation/loss': 0.1338255393258427, 'validation/num_examples': 89000000, 'test/loss': 0.13672711993217065, 'test/num_examples': 89274637, 'score': 387.1118276119232, 'total_duration': 3399.368287086487, 'accumulated_submission_time': 387.1118276119232, 'accumulated_eval_time': 3012.194278240204, 'accumulated_logging_time': 0.05494379997253418}
I0502 18:34:39.304796 140135941084928 logging_writer.py:48] [538] accumulated_eval_time=3012.194278, accumulated_logging_time=0.054944, accumulated_submission_time=387.111828, global_step=538, preemption_count=0, score=387.111828, test/loss=0.136727, test/num_examples=89274637, total_duration=3399.368287, train/loss=0.133030, validation/loss=0.133826, validation/num_examples=89000000
I0502 18:35:09.762333 140135857387264 logging_writer.py:48] [600] global_step=600, grad_norm=0.46285280585289, loss=0.12900179624557495
I0502 18:36:28.291274 140135941084928 logging_writer.py:48] [700] global_step=700, grad_norm=0.5347928404808044, loss=0.1357404887676239
I0502 18:36:39.764442 140324676323136 spec.py:298] Evaluating on the training split.
I0502 18:40:52.371026 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 18:45:03.710562 140324676323136 spec.py:326] Evaluating on the test split.
I0502 18:49:09.350384 140324676323136 submission_runner.py:415] Time since start: 4269.42s, 	Step: 715, 	{'train/loss': 0.12948117736861317, 'validation/loss': 0.13012041573033709, 'validation/num_examples': 89000000, 'test/loss': 0.13322166742610222, 'test/num_examples': 89274637, 'score': 507.5625705718994, 'total_duration': 4269.42182970047, 'accumulated_submission_time': 507.5625705718994, 'accumulated_eval_time': 3761.780153989792, 'accumulated_logging_time': 0.0694575309753418}
I0502 18:49:09.358034 140135857387264 logging_writer.py:48] [715] accumulated_eval_time=3761.780154, accumulated_logging_time=0.069458, accumulated_submission_time=507.562571, global_step=715, preemption_count=0, score=507.562571, test/loss=0.133222, test/num_examples=89274637, total_duration=4269.421830, train/loss=0.129481, validation/loss=0.130120, validation/num_examples=89000000
I0502 18:49:58.969892 140135941084928 logging_writer.py:48] [800] global_step=800, grad_norm=0.04873111844062805, loss=0.12948909401893616
I0502 18:51:10.026311 140324676323136 spec.py:298] Evaluating on the training split.
I0502 18:55:15.290032 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 18:59:23.332167 140324676323136 spec.py:326] Evaluating on the test split.
I0502 19:03:27.781913 140324676323136 submission_runner.py:415] Time since start: 5127.85s, 	Step: 892, 	{'train/loss': 0.12796914252926095, 'validation/loss': 0.12965931460674157, 'validation/num_examples': 89000000, 'test/loss': 0.13242729847224133, 'test/num_examples': 89274637, 'score': 628.2222335338593, 'total_duration': 5127.853345155716, 'accumulated_submission_time': 628.2222335338593, 'accumulated_eval_time': 4499.535678863525, 'accumulated_logging_time': 0.08359408378601074}
I0502 19:03:27.789875 140135857387264 logging_writer.py:48] [892] accumulated_eval_time=4499.535679, accumulated_logging_time=0.083594, accumulated_submission_time=628.222234, global_step=892, preemption_count=0, score=628.222234, test/loss=0.132427, test/num_examples=89274637, total_duration=5127.853345, train/loss=0.127969, validation/loss=0.129659, validation/num_examples=89000000
I0502 19:03:29.006515 140135941084928 logging_writer.py:48] [900] global_step=900, grad_norm=0.2593381404876709, loss=0.1378108263015747
I0502 19:04:35.800236 140135857387264 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.13807430863380432, loss=0.12285000830888748
I0502 19:05:28.394575 140324676323136 spec.py:298] Evaluating on the training split.
I0502 19:09:38.495191 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 19:13:45.105329 140324676323136 spec.py:326] Evaluating on the test split.
I0502 19:17:50.680497 140324676323136 submission_runner.py:415] Time since start: 5990.75s, 	Step: 1069, 	{'train/loss': 0.12915867685482202, 'validation/loss': 0.12826698876404494, 'validation/num_examples': 89000000, 'test/loss': 0.1309331338978169, 'test/num_examples': 89274637, 'score': 748.8176002502441, 'total_duration': 5990.7519409656525, 'accumulated_submission_time': 748.8176002502441, 'accumulated_eval_time': 5241.821553230286, 'accumulated_logging_time': 0.0986943244934082}
I0502 19:17:50.688627 140135941084928 logging_writer.py:48] [1069] accumulated_eval_time=5241.821553, accumulated_logging_time=0.098694, accumulated_submission_time=748.817600, global_step=1069, preemption_count=0, score=748.817600, test/loss=0.130933, test/num_examples=89274637, total_duration=5990.751941, train/loss=0.129159, validation/loss=0.128267, validation/num_examples=89000000
I0502 19:17:56.859269 140135857387264 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6985639333724976, loss=0.12672457098960876
I0502 19:19:14.583682 140135941084928 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3142451047897339, loss=0.12490943819284439
I0502 19:19:51.032603 140324676323136 spec.py:298] Evaluating on the training split.
I0502 19:24:05.934477 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 19:28:14.057895 140324676323136 spec.py:326] Evaluating on the test split.
I0502 19:32:17.687381 140324676323136 submission_runner.py:415] Time since start: 6857.76s, 	Step: 1248, 	{'train/loss': 0.1281591531895293, 'validation/loss': 0.12873495505617977, 'validation/num_examples': 89000000, 'test/loss': 0.1314774206250763, 'test/num_examples': 89274637, 'score': 869.1520094871521, 'total_duration': 6857.758827924728, 'accumulated_submission_time': 869.1520094871521, 'accumulated_eval_time': 5988.476261138916, 'accumulated_logging_time': 0.11412382125854492}
I0502 19:32:17.695424 140135857387264 logging_writer.py:48] [1248] accumulated_eval_time=5988.476261, accumulated_logging_time=0.114124, accumulated_submission_time=869.152009, global_step=1248, preemption_count=0, score=869.152009, test/loss=0.131477, test/num_examples=89274637, total_duration=6857.758828, train/loss=0.128159, validation/loss=0.128735, validation/num_examples=89000000
I0502 19:32:40.453502 140135941084928 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.25783899426460266, loss=0.13109250366687775
I0502 19:34:01.238300 140135857387264 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.13665243983268738, loss=0.12054075300693512
I0502 19:34:18.312450 140324676323136 spec.py:298] Evaluating on the training split.
I0502 19:38:32.454995 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 19:42:38.439019 140324676323136 spec.py:326] Evaluating on the test split.
I0502 19:46:45.758246 140324676323136 submission_runner.py:415] Time since start: 7725.83s, 	Step: 1423, 	{'train/loss': 0.12996493375327678, 'validation/loss': 0.13165197752808988, 'validation/num_examples': 89000000, 'test/loss': 0.13434373303584532, 'test/num_examples': 89274637, 'score': 989.760294675827, 'total_duration': 7725.829684495926, 'accumulated_submission_time': 989.760294675827, 'accumulated_eval_time': 6735.921980857849, 'accumulated_logging_time': 0.12868094444274902}
I0502 19:46:45.766415 140135941084928 logging_writer.py:48] [1423] accumulated_eval_time=6735.921981, accumulated_logging_time=0.128681, accumulated_submission_time=989.760295, global_step=1423, preemption_count=0, score=989.760295, test/loss=0.134344, test/num_examples=89274637, total_duration=7725.829684, train/loss=0.129965, validation/loss=0.131652, validation/num_examples=89000000
I0502 19:47:28.130891 140135857387264 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3490343689918518, loss=0.1382090002298355
I0502 19:48:44.814908 140324676323136 spec.py:298] Evaluating on the training split.
I0502 19:52:56.189782 140324676323136 spec.py:310] Evaluating on the validation split.
I0502 19:57:02.122104 140324676323136 spec.py:326] Evaluating on the test split.
I0502 20:01:07.091262 140324676323136 submission_runner.py:415] Time since start: 8587.16s, 	Step: 1600, 	{'train/loss': 0.1268737321394695, 'validation/loss': 0.12746133707865168, 'validation/num_examples': 89000000, 'test/loss': 0.13016783255024605, 'test/num_examples': 89274637, 'score': 1108.800255060196, 'total_duration': 8587.162712335587, 'accumulated_submission_time': 1108.800255060196, 'accumulated_eval_time': 7478.198282241821, 'accumulated_logging_time': 0.14317727088928223}
I0502 20:01:07.099786 140135941084928 logging_writer.py:48] [1600] accumulated_eval_time=7478.198282, accumulated_logging_time=0.143177, accumulated_submission_time=1108.800255, global_step=1600, preemption_count=0, score=1108.800255, test/loss=0.130168, test/num_examples=89274637, total_duration=8587.162712, train/loss=0.126874, validation/loss=0.127461, validation/num_examples=89000000
I0502 20:01:07.112819 140135857387264 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1108.800255
I0502 20:01:10.307241 140324676323136 checkpoints.py:356] Saving checkpoint at step: 1600
I0502 20:01:40.658193 140324676323136 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1/checkpoint_1600
I0502 20:01:40.875206 140324676323136 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_lamb/criteo1tb_jax/trial_1/checkpoint_1600.
I0502 20:01:40.940879 140324676323136 submission_runner.py:578] Tuning trial 1/1
I0502 20:01:40.941091 140324676323136 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 20:01:40.942449 140324676323136 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/loss': 1.5302069680141648, 'validation/loss': 1.5123906516853933, 'validation/num_examples': 89000000, 'test/loss': 1.523783468310266, 'test/num_examples': 89274637, 'score': 25.902984857559204, 'total_duration': 796.2663962841034, 'accumulated_submission_time': 25.902984857559204, 'accumulated_eval_time': 770.3632137775421, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (182, {'train/loss': 0.14085781161972843, 'validation/loss': 0.13930211235955056, 'validation/num_examples': 89000000, 'test/loss': 0.14266886349815122, 'test/num_examples': 89274637, 'score': 146.38195204734802, 'total_duration': 1654.579845905304, 'accumulated_submission_time': 146.38195204734802, 'accumulated_eval_time': 1508.1704812049866, 'accumulated_logging_time': 0.024768352508544922, 'global_step': 182, 'preemption_count': 0}), (358, {'train/loss': 0.13642412849521707, 'validation/loss': 0.13625047191011236, 'validation/num_examples': 89000000, 'test/loss': 0.13986953539783084, 'test/num_examples': 89274637, 'score': 266.63829469680786, 'total_duration': 2534.0932693481445, 'accumulated_submission_time': 266.63829469680786, 'accumulated_eval_time': 2267.4106163978577, 'accumulated_logging_time': 0.0394594669342041, 'global_step': 358, 'preemption_count': 0}), (538, {'train/loss': 0.1330295138888889, 'validation/loss': 0.1338255393258427, 'validation/num_examples': 89000000, 'test/loss': 0.13672711993217065, 'test/num_examples': 89274637, 'score': 387.1118276119232, 'total_duration': 3399.368287086487, 'accumulated_submission_time': 387.1118276119232, 'accumulated_eval_time': 3012.194278240204, 'accumulated_logging_time': 0.05494379997253418, 'global_step': 538, 'preemption_count': 0}), (715, {'train/loss': 0.12948117736861317, 'validation/loss': 0.13012041573033709, 'validation/num_examples': 89000000, 'test/loss': 0.13322166742610222, 'test/num_examples': 89274637, 'score': 507.5625705718994, 'total_duration': 4269.42182970047, 'accumulated_submission_time': 507.5625705718994, 'accumulated_eval_time': 3761.780153989792, 'accumulated_logging_time': 0.0694575309753418, 'global_step': 715, 'preemption_count': 0}), (892, {'train/loss': 0.12796914252926095, 'validation/loss': 0.12965931460674157, 'validation/num_examples': 89000000, 'test/loss': 0.13242729847224133, 'test/num_examples': 89274637, 'score': 628.2222335338593, 'total_duration': 5127.853345155716, 'accumulated_submission_time': 628.2222335338593, 'accumulated_eval_time': 4499.535678863525, 'accumulated_logging_time': 0.08359408378601074, 'global_step': 892, 'preemption_count': 0}), (1069, {'train/loss': 0.12915867685482202, 'validation/loss': 0.12826698876404494, 'validation/num_examples': 89000000, 'test/loss': 0.1309331338978169, 'test/num_examples': 89274637, 'score': 748.8176002502441, 'total_duration': 5990.7519409656525, 'accumulated_submission_time': 748.8176002502441, 'accumulated_eval_time': 5241.821553230286, 'accumulated_logging_time': 0.0986943244934082, 'global_step': 1069, 'preemption_count': 0}), (1248, {'train/loss': 0.1281591531895293, 'validation/loss': 0.12873495505617977, 'validation/num_examples': 89000000, 'test/loss': 0.1314774206250763, 'test/num_examples': 89274637, 'score': 869.1520094871521, 'total_duration': 6857.758827924728, 'accumulated_submission_time': 869.1520094871521, 'accumulated_eval_time': 5988.476261138916, 'accumulated_logging_time': 0.11412382125854492, 'global_step': 1248, 'preemption_count': 0}), (1423, {'train/loss': 0.12996493375327678, 'validation/loss': 0.13165197752808988, 'validation/num_examples': 89000000, 'test/loss': 0.13434373303584532, 'test/num_examples': 89274637, 'score': 989.760294675827, 'total_duration': 7725.829684495926, 'accumulated_submission_time': 989.760294675827, 'accumulated_eval_time': 6735.921980857849, 'accumulated_logging_time': 0.12868094444274902, 'global_step': 1423, 'preemption_count': 0}), (1600, {'train/loss': 0.1268737321394695, 'validation/loss': 0.12746133707865168, 'validation/num_examples': 89000000, 'test/loss': 0.13016783255024605, 'test/num_examples': 89274637, 'score': 1108.800255060196, 'total_duration': 8587.162712335587, 'accumulated_submission_time': 1108.800255060196, 'accumulated_eval_time': 7478.198282241821, 'accumulated_logging_time': 0.14317727088928223, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0502 20:01:40.942601 140324676323136 submission_runner.py:581] Timing: 1108.800255060196
I0502 20:01:40.942662 140324676323136 submission_runner.py:582] ====================
I0502 20:01:40.942765 140324676323136 submission_runner.py:645] Final criteo1tb score: 1108.800255060196
