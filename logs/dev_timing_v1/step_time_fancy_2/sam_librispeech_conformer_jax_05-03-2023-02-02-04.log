python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_05-03-2023-02-02-04.log
I0503 02:02:25.061673 139761947293504 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax.
I0503 02:02:25.139362 139761947293504 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0503 02:02:25.982211 139761947293504 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0503 02:02:25.982906 139761947293504 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0503 02:02:25.987060 139761947293504 submission_runner.py:538] Using RNG seed 3092573012
I0503 02:02:28.570251 139761947293504 submission_runner.py:547] --- Tuning run 1/1 ---
I0503 02:02:28.570461 139761947293504 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1.
I0503 02:02:28.570650 139761947293504 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1/hparams.json.
I0503 02:02:28.694882 139761947293504 submission_runner.py:241] Initializing dataset.
I0503 02:02:28.695122 139761947293504 submission_runner.py:248] Initializing model.
I0503 02:02:34.609980 139761947293504 submission_runner.py:258] Initializing optimizer.
I0503 02:02:35.438692 139761947293504 submission_runner.py:265] Initializing metrics bundle.
I0503 02:02:35.438895 139761947293504 submission_runner.py:282] Initializing checkpoint and logger.
I0503 02:02:35.440022 139761947293504 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0503 02:02:35.440350 139761947293504 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0503 02:02:35.440416 139761947293504 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0503 02:02:36.191288 139761947293504 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0503 02:02:36.192207 139761947293504 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1/flags_0.json.
I0503 02:02:36.198481 139761947293504 submission_runner.py:318] Starting training loop.
I0503 02:02:36.396435 139761947293504 input_pipeline.py:20] Loading split = train-clean-100
I0503 02:02:36.428410 139761947293504 input_pipeline.py:20] Loading split = train-clean-360
I0503 02:02:36.752881 139761947293504 input_pipeline.py:20] Loading split = train-other-500
2023-05-03 02:03:50.663297: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-03 02:03:51.150946: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0503 02:03:53.520223 139586000910080 logging_writer.py:48] [0] global_step=0, grad_norm=38.85755157470703, loss=31.66949462890625
I0503 02:03:53.552180 139761947293504 spec.py:298] Evaluating on the training split.
I0503 02:03:53.655776 139761947293504 input_pipeline.py:20] Loading split = train-clean-100
I0503 02:03:53.684527 139761947293504 input_pipeline.py:20] Loading split = train-clean-360
I0503 02:03:53.971724 139761947293504 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0503 02:04:54.594416 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 02:04:54.658030 139761947293504 input_pipeline.py:20] Loading split = dev-clean
I0503 02:04:54.663416 139761947293504 input_pipeline.py:20] Loading split = dev-other
I0503 02:05:39.595117 139761947293504 spec.py:326] Evaluating on the test split.
I0503 02:05:39.661143 139761947293504 input_pipeline.py:20] Loading split = test-clean
I0503 02:06:10.187591 139761947293504 submission_runner.py:415] Time since start: 213.99s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.561775, dtype=float32), 'train/wer': 1.7173073319034629, 'validation/ctc_loss': DeviceArray(30.805853, dtype=float32), 'validation/wer': 1.4513309342106533, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.978437, dtype=float32), 'test/wer': 1.4997054820953426, 'test/num_examples': 2472, 'score': 77.35349702835083, 'total_duration': 213.98762679100037, 'accumulated_submission_time': 77.35349702835083, 'accumulated_eval_time': 136.63396954536438, 'accumulated_logging_time': 0}
I0503 02:06:10.211937 139581957592832 logging_writer.py:48] [1] accumulated_eval_time=136.633970, accumulated_logging_time=0, accumulated_submission_time=77.353497, global_step=1, preemption_count=0, score=77.353497, test/ctc_loss=30.978437423706055, test/num_examples=2472, test/wer=1.499705, total_duration=213.987627, train/ctc_loss=31.56177520751953, train/wer=1.717307, validation/ctc_loss=30.80585289001465, validation/num_examples=5348, validation/wer=1.451331
I0503 02:09:07.933509 139587417589504 logging_writer.py:48] [100] global_step=100, grad_norm=1.9415180683135986, loss=6.491322994232178
I0503 02:11:35.595659 139587425982208 logging_writer.py:48] [200] global_step=200, grad_norm=3.9597887992858887, loss=5.942531108856201
I0503 02:14:03.222415 139587417589504 logging_writer.py:48] [300] global_step=300, grad_norm=1.882790207862854, loss=5.849698066711426
I0503 02:16:30.766719 139587425982208 logging_writer.py:48] [400] global_step=400, grad_norm=2.4375269412994385, loss=5.827201843261719
I0503 02:18:58.597839 139587417589504 logging_writer.py:48] [500] global_step=500, grad_norm=0.34716832637786865, loss=5.806558132171631
I0503 02:21:26.487743 139587425982208 logging_writer.py:48] [600] global_step=600, grad_norm=2.9960711002349854, loss=5.809978008270264
I0503 02:23:54.503334 139587417589504 logging_writer.py:48] [700] global_step=700, grad_norm=1.858900547027588, loss=5.819252967834473
I0503 02:26:22.494336 139587425982208 logging_writer.py:48] [800] global_step=800, grad_norm=0.7246817946434021, loss=5.77669095993042
I0503 02:28:50.372235 139587417589504 logging_writer.py:48] [900] global_step=900, grad_norm=0.6567995548248291, loss=5.79641580581665
I0503 02:31:18.352903 139587425982208 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3516197204589844, loss=5.784731388092041
I0503 02:33:49.798741 139589593487104 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.361635684967041, loss=5.789875030517578
I0503 02:36:17.618714 139589585094400 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5577296018600464, loss=5.759479999542236
I0503 02:38:45.278782 139589593487104 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.1901004314422607, loss=5.743408679962158
I0503 02:41:12.918029 139589585094400 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.952730655670166, loss=5.621225833892822
I0503 02:43:40.543564 139589593487104 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8758795857429504, loss=5.511009216308594
I0503 02:46:08.222377 139589585094400 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.688895344734192, loss=5.437189102172852
I0503 02:46:10.872831 139761947293504 spec.py:298] Evaluating on the training split.
I0503 02:46:38.000565 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 02:47:14.076229 139761947293504 spec.py:326] Evaluating on the test split.
I0503 02:47:32.113660 139761947293504 submission_runner.py:415] Time since start: 2695.91s, 	Step: 1603, 	{'train/ctc_loss': DeviceArray(6.0718336, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.145872, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.136685, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2477.9813990592957, 'total_duration': 2695.912836790085, 'accumulated_submission_time': 2477.9813990592957, 'accumulated_eval_time': 217.87251996994019, 'accumulated_logging_time': 0.03630423545837402}
I0503 02:47:32.134540 139589593487104 logging_writer.py:48] [1603] accumulated_eval_time=217.872520, accumulated_logging_time=0.036304, accumulated_submission_time=2477.981399, global_step=1603, preemption_count=0, score=2477.981399, test/ctc_loss=6.136684894561768, test/num_examples=2472, test/wer=0.899580, total_duration=2695.912837, train/ctc_loss=6.071833610534668, train/wer=0.944636, validation/ctc_loss=6.145872116088867, validation/num_examples=5348, validation/wer=0.895995
I0503 02:49:56.585083 139589585094400 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.3847328424453735, loss=5.137097358703613
I0503 02:52:23.865460 139589593487104 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0141321420669556, loss=4.536625862121582
I0503 02:54:51.084070 139589585094400 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.5358225107192993, loss=4.098785400390625
I0503 02:57:18.276436 139589593487104 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.213698148727417, loss=3.833076238632202
I0503 02:59:48.912529 139589593487104 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2003614902496338, loss=3.5672667026519775
I0503 03:02:16.027789 139589585094400 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.5140624046325684, loss=3.5001978874206543
I0503 03:04:43.231850 139589593487104 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2927727699279785, loss=3.345276355743408
I0503 03:07:10.306736 139589585094400 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.972916841506958, loss=3.18548583984375
I0503 03:09:37.222583 139589593487104 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2608500719070435, loss=3.1285650730133057
I0503 03:12:04.230372 139589585094400 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8120951056480408, loss=3.041072368621826
I0503 03:14:31.036556 139589593487104 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8980743885040283, loss=2.9379000663757324
I0503 03:16:57.916693 139589585094400 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9974744319915771, loss=2.9073963165283203
I0503 03:19:24.692496 139589593487104 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0303224325180054, loss=2.820009231567383
I0503 03:21:51.496967 139589585094400 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1122087240219116, loss=2.809988498687744
I0503 03:24:21.581532 139589593487104 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.289777159690857, loss=2.8343470096588135
I0503 03:26:48.337671 139589585094400 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.504715919494629, loss=2.7699947357177734
I0503 03:27:33.511399 139761947293504 spec.py:298] Evaluating on the training split.
I0503 03:28:08.162247 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 03:28:45.635884 139761947293504 spec.py:326] Evaluating on the test split.
I0503 03:29:04.930485 139761947293504 submission_runner.py:415] Time since start: 5188.73s, 	Step: 3232, 	{'train/ctc_loss': DeviceArray(2.7790189, dtype=float32), 'train/wer': 0.6183043443518099, 'validation/ctc_loss': DeviceArray(3.1400936, dtype=float32), 'validation/wer': 0.6531177338903414, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.8370938, dtype=float32), 'test/wer': 0.6075396583592306, 'test/num_examples': 2472, 'score': 4879.319948911667, 'total_duration': 5188.729982614517, 'accumulated_submission_time': 4879.319948911667, 'accumulated_eval_time': 309.28965759277344, 'accumulated_logging_time': 0.07406139373779297}
I0503 03:29:04.953590 139589593487104 logging_writer.py:48] [3232] accumulated_eval_time=309.289658, accumulated_logging_time=0.074061, accumulated_submission_time=4879.319949, global_step=3232, preemption_count=0, score=4879.319949, test/ctc_loss=2.8370938301086426, test/num_examples=2472, test/wer=0.607540, total_duration=5188.729983, train/ctc_loss=2.7790188789367676, train/wer=0.618304, validation/ctc_loss=3.1400935649871826, validation/num_examples=5348, validation/wer=0.653118
I0503 03:30:46.119120 139589585094400 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9766034483909607, loss=2.6963212490081787
I0503 03:33:12.640935 139589593487104 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.711087942123413, loss=2.609229326248169
I0503 03:35:39.214617 139589585094400 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9146134853363037, loss=2.5909435749053955
I0503 03:38:05.672432 139589593487104 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.3012510538101196, loss=2.4398486614227295
I0503 03:40:32.256848 139589585094400 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.163975477218628, loss=2.4198787212371826
I0503 03:42:58.822051 139589593487104 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9719259738922119, loss=2.481778383255005
I0503 03:45:25.306430 139589585094400 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.065480351448059, loss=2.454155921936035
I0503 03:47:51.821362 139589593487104 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9925931692123413, loss=2.447627544403076
I0503 03:50:18.303868 139589585094400 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.008512258529663, loss=2.4386231899261475
I0503 03:52:48.474574 139589593487104 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6536416411399841, loss=2.284532308578491
I0503 03:55:14.956407 139589585094400 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.867884635925293, loss=2.25099778175354
I0503 03:57:41.497688 139589593487104 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9984959959983826, loss=2.1861846446990967
I0503 04:00:08.092162 139589585094400 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8250707983970642, loss=2.233042001724243
I0503 04:02:34.682538 139589593487104 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.0484726428985596, loss=2.18638277053833
I0503 04:05:01.305126 139589585094400 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7059672474861145, loss=2.142033338546753
I0503 04:07:27.892280 139589593487104 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6797366738319397, loss=2.0418412685394287
I0503 04:09:05.664199 139761947293504 spec.py:298] Evaluating on the training split.
I0503 04:09:43.716377 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 04:10:23.189352 139761947293504 spec.py:326] Evaluating on the test split.
I0503 04:10:43.135223 139761947293504 submission_runner.py:415] Time since start: 7686.93s, 	Step: 4868, 	{'train/ctc_loss': DeviceArray(0.86192334, dtype=float32), 'train/wer': 0.2865042090337408, 'validation/ctc_loss': DeviceArray(1.2114853, dtype=float32), 'validation/wer': 0.34904340611100926, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.9233871, dtype=float32), 'test/wer': 0.2889525318384011, 'test/num_examples': 2472, 'score': 7279.993797540665, 'total_duration': 7686.933353662491, 'accumulated_submission_time': 7279.993797540665, 'accumulated_eval_time': 406.7573606967926, 'accumulated_logging_time': 0.11205291748046875}
I0503 04:10:43.160345 139589301647104 logging_writer.py:48] [4868] accumulated_eval_time=406.757361, accumulated_logging_time=0.112053, accumulated_submission_time=7279.993798, global_step=4868, preemption_count=0, score=7279.993798, test/ctc_loss=0.9233871102333069, test/num_examples=2472, test/wer=0.288953, total_duration=7686.933354, train/ctc_loss=0.861923336982727, train/wer=0.286504, validation/ctc_loss=1.2114852666854858, validation/num_examples=5348, validation/wer=0.349043
I0503 04:11:31.516279 139589293254400 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6495308876037598, loss=2.133877754211426
I0503 04:13:57.991219 139589301647104 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7587230205535889, loss=2.021946668624878
I0503 04:16:24.530762 139589293254400 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7748287916183472, loss=2.065243721008301
I0503 04:18:54.288074 139588646287104 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7468847036361694, loss=2.0415313243865967
I0503 04:21:20.717772 139588637894400 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8832558393478394, loss=2.040286064147949
I0503 04:23:47.142819 139588646287104 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6121158003807068, loss=1.9832828044891357
I0503 04:26:13.620018 139588637894400 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8653395175933838, loss=1.972680926322937
I0503 04:28:40.039251 139588646287104 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.127942681312561, loss=1.9324841499328613
I0503 04:31:06.416039 139588637894400 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8300434350967407, loss=1.8980361223220825
I0503 04:33:32.894842 139588646287104 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7517978549003601, loss=1.9044976234436035
I0503 04:35:59.224945 139588637894400 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6833401918411255, loss=2.000403642654419
I0503 04:38:25.732339 139588646287104 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6772717833518982, loss=1.928260087966919
I0503 04:40:51.961811 139588637894400 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6743484735488892, loss=1.8175005912780762
I0503 04:43:21.513684 139589301647104 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7060883641242981, loss=1.9137018918991089
I0503 04:45:47.690831 139589293254400 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7959455251693726, loss=1.9054979085922241
I0503 04:48:13.739330 139589301647104 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7793225646018982, loss=1.8709701299667358
I0503 04:50:40.018695 139589293254400 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7849672436714172, loss=1.9147231578826904
I0503 04:50:44.099342 139761947293504 spec.py:298] Evaluating on the training split.
I0503 04:51:22.143617 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 04:52:01.773130 139761947293504 spec.py:326] Evaluating on the test split.
I0503 04:52:21.915695 139761947293504 submission_runner.py:415] Time since start: 10185.72s, 	Step: 6504, 	{'train/ctc_loss': DeviceArray(0.5831769, dtype=float32), 'train/wer': 0.19637864047167736, 'validation/ctc_loss': DeviceArray(0.9114908, dtype=float32), 'validation/wer': 0.26719022855985103, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6323804, dtype=float32), 'test/wer': 0.20512664269900271, 'test/num_examples': 2472, 'score': 9680.895449399948, 'total_duration': 10185.715163707733, 'accumulated_submission_time': 9680.895449399948, 'accumulated_eval_time': 504.5717406272888, 'accumulated_logging_time': 0.15175294876098633}
I0503 04:52:21.934961 139589301647104 logging_writer.py:48] [6504] accumulated_eval_time=504.571741, accumulated_logging_time=0.151753, accumulated_submission_time=9680.895449, global_step=6504, preemption_count=0, score=9680.895449, test/ctc_loss=0.6323804259300232, test/num_examples=2472, test/wer=0.205127, total_duration=10185.715164, train/ctc_loss=0.5831769108772278, train/wer=0.196379, validation/ctc_loss=0.911490797996521, validation/num_examples=5348, validation/wer=0.267190
I0503 04:54:43.725255 139589293254400 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7921009659767151, loss=1.88698148727417
I0503 04:57:10.071044 139589301647104 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8904036283493042, loss=1.9421237707138062
I0503 04:59:36.450022 139589293254400 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.576454222202301, loss=1.8425915241241455
I0503 05:02:02.810454 139589301647104 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.565414547920227, loss=1.870707631111145
I0503 05:04:29.047191 139589293254400 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5191607475280762, loss=1.8230561017990112
I0503 05:06:55.262385 139589301647104 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5624556541442871, loss=1.7974066734313965
I0503 05:09:21.336855 139589293254400 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5818612575531006, loss=1.8245140314102173
I0503 05:11:51.001116 139588646287104 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6527233719825745, loss=1.7604347467422485
I0503 05:14:17.189605 139588637894400 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6063081622123718, loss=1.7865042686462402
I0503 05:16:43.400334 139588646287104 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5411244034767151, loss=1.764913558959961
I0503 05:19:09.526409 139588637894400 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6099032759666443, loss=1.7588386535644531
I0503 05:21:35.638828 139588646287104 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5552061796188354, loss=1.7790532112121582
I0503 05:24:01.796357 139588637894400 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.830561101436615, loss=1.7320897579193115
I0503 05:26:27.953157 139588646287104 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7935649156570435, loss=1.754255771636963
I0503 05:28:54.171530 139588637894400 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5454481840133667, loss=1.6788305044174194
I0503 05:31:20.364249 139588646287104 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6071670055389404, loss=1.7171499729156494
I0503 05:32:22.937730 139761947293504 spec.py:298] Evaluating on the training split.
I0503 05:33:00.659306 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 05:33:40.377084 139761947293504 spec.py:326] Evaluating on the test split.
I0503 05:34:00.940118 139761947293504 submission_runner.py:415] Time since start: 12684.74s, 	Step: 8144, 	{'train/ctc_loss': DeviceArray(0.47626022, dtype=float32), 'train/wer': 0.16875231033508012, 'validation/ctc_loss': DeviceArray(0.8011825, dtype=float32), 'validation/wer': 0.23874808247064613, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5257697, dtype=float32), 'test/wer': 0.1749233237868909, 'test/num_examples': 2472, 'score': 12081.866146087646, 'total_duration': 12684.738296031952, 'accumulated_submission_time': 12081.866146087646, 'accumulated_eval_time': 602.570853471756, 'accumulated_logging_time': 0.18157005310058594}
I0503 05:34:00.964614 139588646287104 logging_writer.py:48] [8144] accumulated_eval_time=602.570853, accumulated_logging_time=0.181570, accumulated_submission_time=12081.866146, global_step=8144, preemption_count=0, score=12081.866146, test/ctc_loss=0.5257697105407715, test/num_examples=2472, test/wer=0.174923, total_duration=12684.738296, train/ctc_loss=0.4762602150440216, train/wer=0.168752, validation/ctc_loss=0.8011825084686279, validation/num_examples=5348, validation/wer=0.238748
I0503 05:35:24.248169 139588637894400 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5438683032989502, loss=1.7648022174835205
I0503 05:37:53.845140 139587699087104 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6976383924484253, loss=1.6821691989898682
I0503 05:40:19.846056 139587690694400 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8363927602767944, loss=1.6524839401245117
I0503 05:42:46.062263 139587699087104 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5343531966209412, loss=1.6798306703567505
I0503 05:45:12.144315 139587690694400 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5684890151023865, loss=1.7776010036468506
I0503 05:47:38.408528 139587699087104 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4922696650028229, loss=1.6557084321975708
I0503 05:50:04.491554 139587690694400 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7183584570884705, loss=1.610152006149292
I0503 05:52:30.578215 139587699087104 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.628286600112915, loss=1.6548891067504883
I0503 05:54:56.607317 139587690694400 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5100947022438049, loss=1.6974467039108276
I0503 05:57:22.784512 139587699087104 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6017425656318665, loss=1.6565285921096802
I0503 05:59:49.036977 139587690694400 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6535691618919373, loss=1.5792322158813477
I0503 06:02:18.851310 139588646287104 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6651636362075806, loss=1.6391812562942505
I0503 06:04:44.877946 139588637894400 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5107355117797852, loss=1.6282931566238403
I0503 06:07:10.778544 139588646287104 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5376155972480774, loss=1.6689939498901367
I0503 06:09:36.758913 139588637894400 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4799131751060486, loss=1.5745301246643066
I0503 06:12:02.820879 139588646287104 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.599056601524353, loss=1.5606008768081665
I0503 06:14:02.226871 139761947293504 spec.py:298] Evaluating on the training split.
I0503 06:14:40.577881 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 06:15:21.115128 139761947293504 spec.py:326] Evaluating on the test split.
I0503 06:15:41.727849 139761947293504 submission_runner.py:415] Time since start: 15185.53s, 	Step: 9783, 	{'train/ctc_loss': DeviceArray(0.4168689, dtype=float32), 'train/wer': 0.1464222650185915, 'validation/ctc_loss': DeviceArray(0.6911301, dtype=float32), 'validation/wer': 0.2061669673609972, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4467413, dtype=float32), 'test/wer': 0.14880263238072025, 'test/num_examples': 2472, 'score': 14483.090717554092, 'total_duration': 15185.527131080627, 'accumulated_submission_time': 14483.090717554092, 'accumulated_eval_time': 702.0696694850922, 'accumulated_logging_time': 0.22136592864990234}
I0503 06:15:41.752322 139589163407104 logging_writer.py:48] [9783] accumulated_eval_time=702.069669, accumulated_logging_time=0.221366, accumulated_submission_time=14483.090718, global_step=9783, preemption_count=0, score=14483.090718, test/ctc_loss=0.4467413127422333, test/num_examples=2472, test/wer=0.148803, total_duration=15185.527131, train/ctc_loss=0.4168688952922821, train/wer=0.146422, validation/ctc_loss=0.6911301016807556, validation/num_examples=5348, validation/wer=0.206167
I0503 06:16:08.167770 139589155014400 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5215914249420166, loss=1.617619276046753
I0503 06:18:34.188570 139589163407104 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.524554431438446, loss=1.5995447635650635
I0503 06:21:00.169504 139589155014400 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5178120136260986, loss=1.60980224609375
I0503 06:23:26.181949 139589163407104 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5190088748931885, loss=1.620320200920105
I0503 06:25:52.275740 139589155014400 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5057032704353333, loss=1.6294833421707153
I0503 06:28:22.035513 139589163407104 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5080437660217285, loss=1.5786329507827759
I0503 06:30:48.083604 139589155014400 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5230886936187744, loss=1.5854289531707764
I0503 06:33:14.108665 139589163407104 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6172722578048706, loss=1.4993810653686523
I0503 06:35:40.111922 139589155014400 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4938414990901947, loss=1.5686532258987427
I0503 06:38:05.988984 139589163407104 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5182436108589172, loss=1.5655245780944824
I0503 06:40:31.922686 139589155014400 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.556666374206543, loss=1.6115044355392456
I0503 06:42:57.799980 139589163407104 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7749085426330566, loss=1.5211153030395508
I0503 06:45:23.711941 139589155014400 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.47268757224082947, loss=1.539949893951416
I0503 06:47:49.604266 139589163407104 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.515339195728302, loss=1.5835087299346924
I0503 06:50:15.579795 139589155014400 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5501285195350647, loss=1.5009819269180298
I0503 06:52:41.403012 139589163407104 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.48647576570510864, loss=1.515745997428894
I0503 06:55:10.805772 139589163407104 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5682225227355957, loss=1.4698245525360107
I0503 06:55:42.572602 139761947293504 spec.py:298] Evaluating on the training split.
I0503 06:56:20.655786 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 06:57:00.269464 139761947293504 spec.py:326] Evaluating on the test split.
I0503 06:57:20.562834 139761947293504 submission_runner.py:415] Time since start: 17684.36s, 	Step: 11423, 	{'train/ctc_loss': DeviceArray(0.35733172, dtype=float32), 'train/wer': 0.12986301661242636, 'validation/ctc_loss': DeviceArray(0.645601, dtype=float32), 'validation/wer': 0.19677951548013006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.40855023, dtype=float32), 'test/wer': 0.1393374362724189, 'test/num_examples': 2472, 'score': 16883.873400211334, 'total_duration': 17684.360227823257, 'accumulated_submission_time': 16883.873400211334, 'accumulated_eval_time': 800.055844783783, 'accumulated_logging_time': 0.26218390464782715}
I0503 06:57:20.587389 139589378447104 logging_writer.py:48] [11423] accumulated_eval_time=800.055845, accumulated_logging_time=0.262184, accumulated_submission_time=16883.873400, global_step=11423, preemption_count=0, score=16883.873400, test/ctc_loss=0.4085502326488495, test/num_examples=2472, test/wer=0.139337, total_duration=17684.360228, train/ctc_loss=0.3573317229747772, train/wer=0.129863, validation/ctc_loss=0.6456009745597839, validation/num_examples=5348, validation/wer=0.196780
I0503 06:59:14.341966 139589370054400 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5113612413406372, loss=1.4809666872024536
I0503 07:01:40.276015 139589378447104 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4786372482776642, loss=1.5205284357070923
I0503 07:04:06.191518 139589370054400 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.4588790535926819, loss=1.5542162656784058
I0503 07:06:32.221817 139589378447104 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6441870331764221, loss=1.534778118133545
I0503 07:08:58.198873 139589370054400 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4920753538608551, loss=1.540977954864502
I0503 07:11:24.073561 139589378447104 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5820637941360474, loss=1.5114644765853882
I0503 07:13:50.093992 139589370054400 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3844320476055145, loss=1.4664984941482544
I0503 07:16:16.015839 139589378447104 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4661400318145752, loss=1.49244225025177
I0503 07:18:42.114994 139589370054400 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5422312021255493, loss=1.4998027086257935
I0503 07:21:11.754831 139589050767104 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.43936261534690857, loss=1.4124484062194824
I0503 07:23:37.523694 139589042374400 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.46630969643592834, loss=1.4729658365249634
I0503 07:26:03.310536 139589050767104 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5681095123291016, loss=1.5778725147247314
I0503 07:28:29.134390 139589042374400 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5027494430541992, loss=1.4823510646820068
I0503 07:30:55.058287 139589050767104 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.46768128871917725, loss=1.4774128198623657
I0503 07:33:20.967855 139589042374400 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6536588072776794, loss=1.4924852848052979
I0503 07:35:46.927049 139589050767104 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5323135256767273, loss=1.5045876502990723
I0503 07:37:21.409877 139761947293504 spec.py:298] Evaluating on the training split.
I0503 07:38:00.129966 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 07:38:40.216793 139761947293504 spec.py:326] Evaluating on the test split.
I0503 07:39:00.567380 139761947293504 submission_runner.py:415] Time since start: 20184.37s, 	Step: 13066, 	{'train/ctc_loss': DeviceArray(0.30299112, dtype=float32), 'train/wer': 0.11350269117761773, 'validation/ctc_loss': DeviceArray(0.6159735, dtype=float32), 'validation/wer': 0.1881060116354234, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38466606, dtype=float32), 'test/wer': 0.13210651392358783, 'test/num_examples': 2472, 'score': 19284.656959056854, 'total_duration': 20184.36531853676, 'accumulated_submission_time': 19284.656959056854, 'accumulated_eval_time': 899.2098596096039, 'accumulated_logging_time': 0.30417537689208984}
I0503 07:39:00.592028 139587924367104 logging_writer.py:48] [13066] accumulated_eval_time=899.209860, accumulated_logging_time=0.304175, accumulated_submission_time=19284.656959, global_step=13066, preemption_count=0, score=19284.656959, test/ctc_loss=0.3846660554409027, test/num_examples=2472, test/wer=0.132107, total_duration=20184.365319, train/ctc_loss=0.30299112200737, train/wer=0.113503, validation/ctc_loss=0.6159734725952148, validation/num_examples=5348, validation/wer=0.188106
I0503 07:39:51.656952 139587915974400 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5269668102264404, loss=1.4456573724746704
I0503 07:42:17.574078 139587924367104 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6004000902175903, loss=1.5038648843765259
I0503 07:44:43.425003 139587915974400 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4414729177951813, loss=1.4744936227798462
I0503 07:47:12.701313 139587924367104 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7614001631736755, loss=1.4313008785247803
I0503 07:49:38.555950 139587915974400 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5030019283294678, loss=1.4382880926132202
I0503 07:52:04.447482 139587924367104 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4738852083683014, loss=1.4674915075302124
I0503 07:54:30.358887 139587915974400 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5517207384109497, loss=1.4782544374465942
I0503 07:56:56.153040 139587924367104 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5349854826927185, loss=1.4930055141448975
I0503 07:59:22.024928 139587915974400 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.43442264199256897, loss=1.4668062925338745
I0503 08:01:47.891349 139587924367104 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.4857344329357147, loss=1.4755548238754272
I0503 08:04:13.942451 139587915974400 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.4602031409740448, loss=1.4154372215270996
I0503 08:06:40.021679 139587924367104 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5880826115608215, loss=1.4407927989959717
I0503 08:09:06.006697 139587915974400 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5293266177177429, loss=1.4192633628845215
I0503 08:11:31.893822 139587924367104 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.41621872782707214, loss=1.4813711643218994
I0503 08:14:01.541063 139587924367104 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4661366045475006, loss=1.421194314956665
I0503 08:16:27.345330 139587915974400 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.6001116633415222, loss=1.473654866218567
I0503 08:18:53.199709 139587924367104 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.541534960269928, loss=1.457349181175232
I0503 08:19:01.676151 139761947293504 spec.py:298] Evaluating on the training split.
I0503 08:19:40.224138 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 08:20:19.695223 139761947293504 spec.py:326] Evaluating on the test split.
I0503 08:20:40.104849 139761947293504 submission_runner.py:415] Time since start: 22683.90s, 	Step: 14707, 	{'train/ctc_loss': DeviceArray(0.27244896, dtype=float32), 'train/wer': 0.10261266485431109, 'validation/ctc_loss': DeviceArray(0.5938387, dtype=float32), 'validation/wer': 0.1805034298449575, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.36467674, dtype=float32), 'test/wer': 0.12436780208396807, 'test/num_examples': 2472, 'score': 21685.70412826538, 'total_duration': 22683.90416431427, 'accumulated_submission_time': 21685.70412826538, 'accumulated_eval_time': 997.6364254951477, 'accumulated_logging_time': 0.3437840938568115}
I0503 08:20:40.125890 139587924367104 logging_writer.py:48] [14707] accumulated_eval_time=997.636425, accumulated_logging_time=0.343784, accumulated_submission_time=21685.704128, global_step=14707, preemption_count=0, score=21685.704128, test/ctc_loss=0.36467674374580383, test/num_examples=2472, test/wer=0.124368, total_duration=22683.904164, train/ctc_loss=0.27244895696640015, train/wer=0.102613, validation/ctc_loss=0.5938386917114258, validation/num_examples=5348, validation/wer=0.180503
I0503 08:22:57.102685 139587915974400 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6344686150550842, loss=1.4646177291870117
I0503 08:25:22.912833 139587924367104 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.47227489948272705, loss=1.50543212890625
I0503 08:27:48.876276 139587915974400 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.48563265800476074, loss=1.4538390636444092
I0503 08:30:14.637765 139587924367104 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5329656004905701, loss=1.4360904693603516
I0503 08:32:40.422754 139587915974400 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6195895671844482, loss=1.4136487245559692
I0503 08:35:06.225754 139587924367104 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.583946704864502, loss=1.4686976671218872
I0503 08:37:32.032424 139587915974400 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.534587025642395, loss=1.4125502109527588
I0503 08:40:01.704339 139587924367104 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.47803205251693726, loss=1.459244966506958
I0503 08:42:27.723320 139587915974400 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5258146524429321, loss=1.4222990274429321
I0503 08:44:53.614390 139587924367104 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.49736011028289795, loss=1.4518746137619019
I0503 08:47:19.641966 139587915974400 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5290266275405884, loss=1.4178075790405273
I0503 08:49:45.537517 139587924367104 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5277444124221802, loss=1.4681779146194458
I0503 08:52:11.375787 139587915974400 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.4490078389644623, loss=1.4134870767593384
I0503 08:54:37.321817 139587924367104 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5523495674133301, loss=1.4504177570343018
I0503 08:57:03.141614 139587915974400 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.4157494008541107, loss=1.442954659461975
I0503 08:59:29.022384 139587924367104 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5180550217628479, loss=1.3981678485870361
I0503 09:00:40.148120 139761947293504 spec.py:298] Evaluating on the training split.
I0503 09:01:18.478199 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 09:01:57.975184 139761947293504 spec.py:326] Evaluating on the test split.
I0503 09:02:19.066182 139761947293504 submission_runner.py:415] Time since start: 25182.86s, 	Step: 16350, 	{'train/ctc_loss': DeviceArray(0.26693928, dtype=float32), 'train/wer': 0.10299945834919852, 'validation/ctc_loss': DeviceArray(0.56909376, dtype=float32), 'validation/wer': 0.1736823317156943, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.34746292, dtype=float32), 'test/wer': 0.11882274084455548, 'test/num_examples': 2472, 'score': 24085.693930864334, 'total_duration': 25182.86437892914, 'accumulated_submission_time': 24085.693930864334, 'accumulated_eval_time': 1096.5512337684631, 'accumulated_logging_time': 0.37592005729675293}
I0503 09:02:19.090158 139587924367104 logging_writer.py:48] [16350] accumulated_eval_time=1096.551234, accumulated_logging_time=0.375920, accumulated_submission_time=24085.693931, global_step=16350, preemption_count=0, score=24085.693931, test/ctc_loss=0.347462922334671, test/num_examples=2472, test/wer=0.118823, total_duration=25182.864379, train/ctc_loss=0.26693928241729736, train/wer=0.102999, validation/ctc_loss=0.5690937638282776, validation/num_examples=5348, validation/wer=0.173682
I0503 09:03:33.415503 139587915974400 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5126698613166809, loss=1.3948876857757568
I0503 09:06:02.734076 139587924367104 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3977251350879669, loss=1.3714947700500488
I0503 09:08:28.551468 139587915974400 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.4924427270889282, loss=1.3867145776748657
I0503 09:10:54.633211 139587924367104 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5052065253257751, loss=1.5181022882461548
I0503 09:13:20.549195 139587915974400 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.45806047320365906, loss=1.3949812650680542
I0503 09:15:46.436348 139587924367104 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.545440137386322, loss=1.4375706911087036
I0503 09:18:12.261457 139587915974400 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5364716649055481, loss=1.4531372785568237
I0503 09:20:38.030834 139587924367104 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.593839704990387, loss=1.4365636110305786
I0503 09:23:03.921053 139587915974400 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5032938718795776, loss=1.424879550933838
I0503 09:25:29.764875 139587924367104 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5887265205383301, loss=1.36357843875885
I0503 09:27:55.543148 139587915974400 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.42763853073120117, loss=1.3675622940063477
I0503 09:30:21.341734 139587924367104 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6279000639915466, loss=1.4623421430587769
I0503 09:32:50.735962 139589378447104 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.48798036575317383, loss=1.3659262657165527
I0503 09:35:16.669906 139589370054400 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.41512933373451233, loss=1.3728421926498413
I0503 09:37:42.687872 139589378447104 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.46155762672424316, loss=1.365742564201355
I0503 09:40:08.505563 139589370054400 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5216553211212158, loss=1.4026075601577759
I0503 09:42:19.350860 139761947293504 spec.py:298] Evaluating on the training split.
I0503 09:42:58.068895 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 09:43:38.078031 139761947293504 spec.py:326] Evaluating on the test split.
I0503 09:43:58.640151 139761947293504 submission_runner.py:415] Time since start: 27682.44s, 	Step: 17991, 	{'train/ctc_loss': DeviceArray(0.26419878, dtype=float32), 'train/wer': 0.09786615193421296, 'validation/ctc_loss': DeviceArray(0.5454914, dtype=float32), 'validation/wer': 0.1638703701917047, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.33130977, dtype=float32), 'test/wer': 0.11415107752929946, 'test/num_examples': 2472, 'score': 26485.916858434677, 'total_duration': 27682.439652442932, 'accumulated_submission_time': 26485.916858434677, 'accumulated_eval_time': 1195.8385837078094, 'accumulated_logging_time': 0.4164924621582031}
I0503 09:43:58.665404 139589163407104 logging_writer.py:48] [17991] accumulated_eval_time=1195.838584, accumulated_logging_time=0.416492, accumulated_submission_time=26485.916858, global_step=17991, preemption_count=0, score=26485.916858, test/ctc_loss=0.3313097655773163, test/num_examples=2472, test/wer=0.114151, total_duration=27682.439652, train/ctc_loss=0.26419878005981445, train/wer=0.097866, validation/ctc_loss=0.5454913973808289, validation/num_examples=5348, validation/wer=0.163870
I0503 09:44:13.319688 139589155014400 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.47824230790138245, loss=1.3506454229354858
I0503 09:46:39.210378 139589163407104 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5656523704528809, loss=1.442611813545227
I0503 09:49:05.273222 139589155014400 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.4104292392730713, loss=1.3787689208984375
I0503 09:51:31.199716 139589163407104 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4395917057991028, loss=1.3763846158981323
I0503 09:53:57.003791 139589155014400 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4391714632511139, loss=1.333624005317688
I0503 09:56:22.916812 139589163407104 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4780716300010681, loss=1.402369737625122
I0503 09:58:52.361411 139589163407104 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5325366258621216, loss=1.4203524589538574
I0503 10:01:18.167399 139589155014400 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.464591920375824, loss=1.4117482900619507
I0503 10:03:44.070468 139589163407104 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.4847963750362396, loss=1.3582695722579956
I0503 10:06:09.859941 139589155014400 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.478976309299469, loss=1.3926209211349487
I0503 10:08:35.620389 139589163407104 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.481646865606308, loss=1.4082300662994385
I0503 10:11:01.534728 139589155014400 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.46184512972831726, loss=1.3678234815597534
I0503 10:13:27.383116 139589163407104 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5179184079170227, loss=1.3570894002914429
I0503 10:15:53.234902 139589155014400 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.4460870325565338, loss=1.3828039169311523
I0503 10:18:19.273633 139589163407104 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4634411931037903, loss=1.327338695526123
I0503 10:20:45.197339 139589155014400 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.522759735584259, loss=1.3554296493530273
I0503 10:23:14.636554 139589163407104 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5261242389678955, loss=1.3065236806869507
I0503 10:23:59.528182 139761947293504 spec.py:298] Evaluating on the training split.
I0503 10:24:38.321607 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 10:25:17.984964 139761947293504 spec.py:326] Evaluating on the test split.
I0503 10:25:38.246477 139761947293504 submission_runner.py:415] Time since start: 30182.04s, 	Step: 19632, 	{'train/ctc_loss': DeviceArray(0.26177457, dtype=float32), 'train/wer': 0.09890521539557337, 'validation/ctc_loss': DeviceArray(0.5440555, dtype=float32), 'validation/wer': 0.16680334590782353, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32565856, dtype=float32), 'test/wer': 0.11321674486624825, 'test/num_examples': 2472, 'score': 28886.73997759819, 'total_duration': 30182.044240236282, 'accumulated_submission_time': 28886.73997759819, 'accumulated_eval_time': 1294.5531916618347, 'accumulated_logging_time': 0.45899200439453125}
I0503 10:25:38.271547 139588579727104 logging_writer.py:48] [19632] accumulated_eval_time=1294.553192, accumulated_logging_time=0.458992, accumulated_submission_time=28886.739978, global_step=19632, preemption_count=0, score=28886.739978, test/ctc_loss=0.32565855979919434, test/num_examples=2472, test/wer=0.113217, total_duration=30182.044240, train/ctc_loss=0.26177456974983215, train/wer=0.098905, validation/ctc_loss=0.5440555214881897, validation/num_examples=5348, validation/wer=0.166803
I0503 10:27:18.807642 139588571334400 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.5122995972633362, loss=1.3287187814712524
I0503 10:29:44.576689 139588579727104 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5225663781166077, loss=1.3631410598754883
I0503 10:32:10.472685 139588571334400 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.4930960237979889, loss=1.320461630821228
I0503 10:34:34.544781 139761947293504 spec.py:298] Evaluating on the training split.
I0503 10:35:13.706200 139761947293504 spec.py:310] Evaluating on the validation split.
I0503 10:35:54.462174 139761947293504 spec.py:326] Evaluating on the test split.
I0503 10:36:14.736665 139761947293504 submission_runner.py:415] Time since start: 30818.54s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.2561571, dtype=float32), 'train/wer': 0.09650169770552526, 'validation/ctc_loss': DeviceArray(0.5342639, dtype=float32), 'validation/wer': 0.16153556715453116, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32514325, dtype=float32), 'test/wer': 0.10984502264741129, 'test/num_examples': 2472, 'score': 29422.990351438522, 'total_duration': 30818.536103487015, 'accumulated_submission_time': 29422.990351438522, 'accumulated_eval_time': 1394.743069410324, 'accumulated_logging_time': 0.5015413761138916}
I0503 10:36:14.758913 139589593487104 logging_writer.py:48] [20000] accumulated_eval_time=1394.743069, accumulated_logging_time=0.501541, accumulated_submission_time=29422.990351, global_step=20000, preemption_count=0, score=29422.990351, test/ctc_loss=0.3251432478427887, test/num_examples=2472, test/wer=0.109845, total_duration=30818.536103, train/ctc_loss=0.2561571002006531, train/wer=0.096502, validation/ctc_loss=0.5342639088630676, validation/num_examples=5348, validation/wer=0.161536
I0503 10:36:14.785401 139589585094400 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=29422.990351
I0503 10:36:15.152841 139761947293504 checkpoints.py:356] Saving checkpoint at step: 20000
I0503 10:36:16.659574 139761947293504 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1/checkpoint_20000
I0503 10:36:16.692874 139761947293504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_sam/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0503 10:36:18.214646 139761947293504 submission_runner.py:578] Tuning trial 1/1
I0503 10:36:18.214986 139761947293504 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0503 10:36:18.225965 139761947293504 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.561775, dtype=float32), 'train/wer': 1.7173073319034629, 'validation/ctc_loss': DeviceArray(30.805853, dtype=float32), 'validation/wer': 1.4513309342106533, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.978437, dtype=float32), 'test/wer': 1.4997054820953426, 'test/num_examples': 2472, 'score': 77.35349702835083, 'total_duration': 213.98762679100037, 'accumulated_submission_time': 77.35349702835083, 'accumulated_eval_time': 136.63396954536438, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1603, {'train/ctc_loss': DeviceArray(6.0718336, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.145872, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.136685, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2477.9813990592957, 'total_duration': 2695.912836790085, 'accumulated_submission_time': 2477.9813990592957, 'accumulated_eval_time': 217.87251996994019, 'accumulated_logging_time': 0.03630423545837402, 'global_step': 1603, 'preemption_count': 0}), (3232, {'train/ctc_loss': DeviceArray(2.7790189, dtype=float32), 'train/wer': 0.6183043443518099, 'validation/ctc_loss': DeviceArray(3.1400936, dtype=float32), 'validation/wer': 0.6531177338903414, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.8370938, dtype=float32), 'test/wer': 0.6075396583592306, 'test/num_examples': 2472, 'score': 4879.319948911667, 'total_duration': 5188.729982614517, 'accumulated_submission_time': 4879.319948911667, 'accumulated_eval_time': 309.28965759277344, 'accumulated_logging_time': 0.07406139373779297, 'global_step': 3232, 'preemption_count': 0}), (4868, {'train/ctc_loss': DeviceArray(0.86192334, dtype=float32), 'train/wer': 0.2865042090337408, 'validation/ctc_loss': DeviceArray(1.2114853, dtype=float32), 'validation/wer': 0.34904340611100926, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.9233871, dtype=float32), 'test/wer': 0.2889525318384011, 'test/num_examples': 2472, 'score': 7279.993797540665, 'total_duration': 7686.933353662491, 'accumulated_submission_time': 7279.993797540665, 'accumulated_eval_time': 406.7573606967926, 'accumulated_logging_time': 0.11205291748046875, 'global_step': 4868, 'preemption_count': 0}), (6504, {'train/ctc_loss': DeviceArray(0.5831769, dtype=float32), 'train/wer': 0.19637864047167736, 'validation/ctc_loss': DeviceArray(0.9114908, dtype=float32), 'validation/wer': 0.26719022855985103, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6323804, dtype=float32), 'test/wer': 0.20512664269900271, 'test/num_examples': 2472, 'score': 9680.895449399948, 'total_duration': 10185.715163707733, 'accumulated_submission_time': 9680.895449399948, 'accumulated_eval_time': 504.5717406272888, 'accumulated_logging_time': 0.15175294876098633, 'global_step': 6504, 'preemption_count': 0}), (8144, {'train/ctc_loss': DeviceArray(0.47626022, dtype=float32), 'train/wer': 0.16875231033508012, 'validation/ctc_loss': DeviceArray(0.8011825, dtype=float32), 'validation/wer': 0.23874808247064613, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5257697, dtype=float32), 'test/wer': 0.1749233237868909, 'test/num_examples': 2472, 'score': 12081.866146087646, 'total_duration': 12684.738296031952, 'accumulated_submission_time': 12081.866146087646, 'accumulated_eval_time': 602.570853471756, 'accumulated_logging_time': 0.18157005310058594, 'global_step': 8144, 'preemption_count': 0}), (9783, {'train/ctc_loss': DeviceArray(0.4168689, dtype=float32), 'train/wer': 0.1464222650185915, 'validation/ctc_loss': DeviceArray(0.6911301, dtype=float32), 'validation/wer': 0.2061669673609972, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4467413, dtype=float32), 'test/wer': 0.14880263238072025, 'test/num_examples': 2472, 'score': 14483.090717554092, 'total_duration': 15185.527131080627, 'accumulated_submission_time': 14483.090717554092, 'accumulated_eval_time': 702.0696694850922, 'accumulated_logging_time': 0.22136592864990234, 'global_step': 9783, 'preemption_count': 0}), (11423, {'train/ctc_loss': DeviceArray(0.35733172, dtype=float32), 'train/wer': 0.12986301661242636, 'validation/ctc_loss': DeviceArray(0.645601, dtype=float32), 'validation/wer': 0.19677951548013006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.40855023, dtype=float32), 'test/wer': 0.1393374362724189, 'test/num_examples': 2472, 'score': 16883.873400211334, 'total_duration': 17684.360227823257, 'accumulated_submission_time': 16883.873400211334, 'accumulated_eval_time': 800.055844783783, 'accumulated_logging_time': 0.26218390464782715, 'global_step': 11423, 'preemption_count': 0}), (13066, {'train/ctc_loss': DeviceArray(0.30299112, dtype=float32), 'train/wer': 0.11350269117761773, 'validation/ctc_loss': DeviceArray(0.6159735, dtype=float32), 'validation/wer': 0.1881060116354234, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38466606, dtype=float32), 'test/wer': 0.13210651392358783, 'test/num_examples': 2472, 'score': 19284.656959056854, 'total_duration': 20184.36531853676, 'accumulated_submission_time': 19284.656959056854, 'accumulated_eval_time': 899.2098596096039, 'accumulated_logging_time': 0.30417537689208984, 'global_step': 13066, 'preemption_count': 0}), (14707, {'train/ctc_loss': DeviceArray(0.27244896, dtype=float32), 'train/wer': 0.10261266485431109, 'validation/ctc_loss': DeviceArray(0.5938387, dtype=float32), 'validation/wer': 0.1805034298449575, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.36467674, dtype=float32), 'test/wer': 0.12436780208396807, 'test/num_examples': 2472, 'score': 21685.70412826538, 'total_duration': 22683.90416431427, 'accumulated_submission_time': 21685.70412826538, 'accumulated_eval_time': 997.6364254951477, 'accumulated_logging_time': 0.3437840938568115, 'global_step': 14707, 'preemption_count': 0}), (16350, {'train/ctc_loss': DeviceArray(0.26693928, dtype=float32), 'train/wer': 0.10299945834919852, 'validation/ctc_loss': DeviceArray(0.56909376, dtype=float32), 'validation/wer': 0.1736823317156943, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.34746292, dtype=float32), 'test/wer': 0.11882274084455548, 'test/num_examples': 2472, 'score': 24085.693930864334, 'total_duration': 25182.86437892914, 'accumulated_submission_time': 24085.693930864334, 'accumulated_eval_time': 1096.5512337684631, 'accumulated_logging_time': 0.37592005729675293, 'global_step': 16350, 'preemption_count': 0}), (17991, {'train/ctc_loss': DeviceArray(0.26419878, dtype=float32), 'train/wer': 0.09786615193421296, 'validation/ctc_loss': DeviceArray(0.5454914, dtype=float32), 'validation/wer': 0.1638703701917047, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.33130977, dtype=float32), 'test/wer': 0.11415107752929946, 'test/num_examples': 2472, 'score': 26485.916858434677, 'total_duration': 27682.439652442932, 'accumulated_submission_time': 26485.916858434677, 'accumulated_eval_time': 1195.8385837078094, 'accumulated_logging_time': 0.4164924621582031, 'global_step': 17991, 'preemption_count': 0}), (19632, {'train/ctc_loss': DeviceArray(0.26177457, dtype=float32), 'train/wer': 0.09890521539557337, 'validation/ctc_loss': DeviceArray(0.5440555, dtype=float32), 'validation/wer': 0.16680334590782353, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32565856, dtype=float32), 'test/wer': 0.11321674486624825, 'test/num_examples': 2472, 'score': 28886.73997759819, 'total_duration': 30182.044240236282, 'accumulated_submission_time': 28886.73997759819, 'accumulated_eval_time': 1294.5531916618347, 'accumulated_logging_time': 0.45899200439453125, 'global_step': 19632, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.2561571, dtype=float32), 'train/wer': 0.09650169770552526, 'validation/ctc_loss': DeviceArray(0.5342639, dtype=float32), 'validation/wer': 0.16153556715453116, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.32514325, dtype=float32), 'test/wer': 0.10984502264741129, 'test/num_examples': 2472, 'score': 29422.990351438522, 'total_duration': 30818.536103487015, 'accumulated_submission_time': 29422.990351438522, 'accumulated_eval_time': 1394.743069410324, 'accumulated_logging_time': 0.5015413761138916, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0503 10:36:18.226175 139761947293504 submission_runner.py:581] Timing: 29422.990351438522
I0503 10:36:18.226245 139761947293504 submission_runner.py:582] ====================
I0503 10:36:18.227470 139761947293504 submission_runner.py:645] Final librispeech_conformer score: 29422.990351438522
