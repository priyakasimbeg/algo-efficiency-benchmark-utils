python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_jax_05-02-2023-19-21-11.log
I0502 19:21:30.829837 140257772291904 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax.
I0502 19:21:30.981313 140257772291904 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 19:21:31.852188 140257772291904 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0502 19:21:31.852823 140257772291904 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 19:21:31.857711 140257772291904 submission_runner.py:538] Using RNG seed 2364034914
I0502 19:21:34.538232 140257772291904 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 19:21:34.538447 140257772291904 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1.
I0502 19:21:34.538623 140257772291904 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1/hparams.json.
I0502 19:21:34.675157 140257772291904 submission_runner.py:241] Initializing dataset.
I0502 19:21:34.675396 140257772291904 submission_runner.py:248] Initializing model.
I0502 19:21:40.893226 140257772291904 submission_runner.py:258] Initializing optimizer.
I0502 19:21:42.387677 140257772291904 submission_runner.py:265] Initializing metrics bundle.
I0502 19:21:42.387879 140257772291904 submission_runner.py:282] Initializing checkpoint and logger.
I0502 19:21:42.391525 140257772291904 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1 with prefix checkpoint_
I0502 19:21:42.391795 140257772291904 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 19:21:42.391859 140257772291904 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 19:21:43.155689 140257772291904 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1/meta_data_0.json.
I0502 19:21:43.156584 140257772291904 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1/flags_0.json.
I0502 19:21:43.205791 140257772291904 submission_runner.py:318] Starting training loop.
I0502 19:22:12.182495 140081848362752 logging_writer.py:48] [0] global_step=0, grad_norm=18.081754684448242, loss=1.6790820360183716
I0502 19:22:12.192581 140257772291904 spec.py:298] Evaluating on the training split.
I0502 19:27:05.487643 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 19:31:53.920438 140257772291904 spec.py:326] Evaluating on the test split.
I0502 19:36:45.703113 140257772291904 submission_runner.py:415] Time since start: 902.50s, 	Step: 1, 	{'train/loss': 1.6795184519292679, 'validation/loss': 1.681331595505618, 'validation/num_examples': 89000000, 'test/loss': 1.6805418094279119, 'test/num_examples': 89274637, 'score': 28.986578941345215, 'total_duration': 902.4972152709961, 'accumulated_submission_time': 28.986578941345215, 'accumulated_eval_time': 873.5104098320007, 'accumulated_logging_time': 0}
I0502 19:36:45.723366 140068740097792 logging_writer.py:48] [1] accumulated_eval_time=873.510410, accumulated_logging_time=0, accumulated_submission_time=28.986579, global_step=1, preemption_count=0, score=28.986579, test/loss=1.680542, test/num_examples=89274637, total_duration=902.497215, train/loss=1.679518, validation/loss=1.681332, validation/num_examples=89000000
I0502 19:37:50.288687 140068731705088 logging_writer.py:48] [100] global_step=100, grad_norm=9.295381546020508, loss=0.29092541337013245
I0502 19:38:46.288132 140257772291904 spec.py:298] Evaluating on the training split.
I0502 19:43:35.563714 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 19:48:14.068083 140257772291904 spec.py:326] Evaluating on the test split.
I0502 19:53:04.126352 140257772291904 submission_runner.py:415] Time since start: 1880.92s, 	Step: 167, 	{'train/loss': 0.15355806441935133, 'validation/loss': 0.1534670224719101, 'validation/num_examples': 89000000, 'test/loss': 0.15716546682794127, 'test/num_examples': 89274637, 'score': 149.54194498062134, 'total_duration': 1880.9204688072205, 'accumulated_submission_time': 149.54194498062134, 'accumulated_eval_time': 1731.3485655784607, 'accumulated_logging_time': 0.027329683303833008}
I0502 19:53:04.135185 140068740097792 logging_writer.py:48] [167] accumulated_eval_time=1731.348566, accumulated_logging_time=0.027330, accumulated_submission_time=149.541945, global_step=167, preemption_count=0, score=149.541945, test/loss=0.157165, test/num_examples=89274637, total_duration=1880.920469, train/loss=0.153558, validation/loss=0.153467, validation/num_examples=89000000
I0502 19:53:12.719569 140068731705088 logging_writer.py:48] [200] global_step=200, grad_norm=0.12665948271751404, loss=0.14853166043758392
I0502 19:54:36.687015 140068740097792 logging_writer.py:48] [300] global_step=300, grad_norm=0.19741129875183105, loss=0.12889792025089264
I0502 19:55:04.447562 140257772291904 spec.py:298] Evaluating on the training split.
I0502 19:59:53.765994 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 20:04:34.712527 140257772291904 spec.py:326] Evaluating on the test split.
I0502 20:09:05.695233 140257772291904 submission_runner.py:415] Time since start: 2842.49s, 	Step: 334, 	{'train/loss': 0.12895920102959438, 'validation/loss': 0.13002358426966293, 'validation/num_examples': 89000000, 'test/loss': 0.13318787283335579, 'test/num_examples': 89274637, 'score': 269.8450040817261, 'total_duration': 2842.4893493652344, 'accumulated_submission_time': 269.8450040817261, 'accumulated_eval_time': 2572.5961565971375, 'accumulated_logging_time': 0.04318809509277344}
I0502 20:09:05.704042 140068731705088 logging_writer.py:48] [334] accumulated_eval_time=2572.596157, accumulated_logging_time=0.043188, accumulated_submission_time=269.845004, global_step=334, preemption_count=0, score=269.845004, test/loss=0.133188, test/num_examples=89274637, total_duration=2842.489349, train/loss=0.128959, validation/loss=0.130024, validation/num_examples=89000000
I0502 20:09:42.670701 140068740097792 logging_writer.py:48] [400] global_step=400, grad_norm=0.2687452435493469, loss=0.1292683333158493
I0502 20:11:05.758833 140257772291904 spec.py:298] Evaluating on the training split.
I0502 20:15:55.196056 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 20:20:34.496664 140257772291904 spec.py:326] Evaluating on the test split.
I0502 20:25:23.141583 140257772291904 submission_runner.py:415] Time since start: 3819.94s, 	Step: 500, 	{'train/loss': 0.12892452910860466, 'validation/loss': 0.12966331460674158, 'validation/num_examples': 89000000, 'test/loss': 0.13327652063149806, 'test/num_examples': 89274637, 'score': 389.8908112049103, 'total_duration': 3819.935711145401, 'accumulated_submission_time': 389.8908112049103, 'accumulated_eval_time': 3429.9788584709167, 'accumulated_logging_time': 0.058791399002075195}
I0502 20:25:23.150277 140068731705088 logging_writer.py:48] [500] accumulated_eval_time=3429.978858, accumulated_logging_time=0.058791, accumulated_submission_time=389.890811, global_step=500, preemption_count=0, score=389.890811, test/loss=0.133277, test/num_examples=89274637, total_duration=3819.935711, train/loss=0.128925, validation/loss=0.129663, validation/num_examples=89000000
I0502 20:25:23.285506 140068740097792 logging_writer.py:48] [500] global_step=500, grad_norm=0.22111815214157104, loss=0.12403523921966553
I0502 20:26:28.292513 140068731705088 logging_writer.py:48] [600] global_step=600, grad_norm=0.04528749734163284, loss=0.1318160742521286
I0502 20:27:23.261957 140257772291904 spec.py:298] Evaluating on the training split.
I0502 20:32:10.316154 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 20:36:49.015718 140257772291904 spec.py:326] Evaluating on the test split.
I0502 20:41:30.147219 140257772291904 submission_runner.py:415] Time since start: 4786.94s, 	Step: 666, 	{'train/loss': 0.12509806494751427, 'validation/loss': 0.12767765168539325, 'validation/num_examples': 89000000, 'test/loss': 0.13075597271820888, 'test/num_examples': 89274637, 'score': 509.9920377731323, 'total_duration': 4786.941342353821, 'accumulated_submission_time': 509.9920377731323, 'accumulated_eval_time': 4276.864053487778, 'accumulated_logging_time': 0.07575249671936035}
I0502 20:41:30.156320 140068740097792 logging_writer.py:48] [666] accumulated_eval_time=4276.864053, accumulated_logging_time=0.075752, accumulated_submission_time=509.992038, global_step=666, preemption_count=0, score=509.992038, test/loss=0.130756, test/num_examples=89274637, total_duration=4786.941342, train/loss=0.125098, validation/loss=0.127678, validation/num_examples=89000000
I0502 20:41:39.686076 140068731705088 logging_writer.py:48] [700] global_step=700, grad_norm=0.09684880077838898, loss=0.1335357427597046
I0502 20:43:05.690631 140068740097792 logging_writer.py:48] [800] global_step=800, grad_norm=0.8238475918769836, loss=0.13228341937065125
I0502 20:43:30.924038 140257772291904 spec.py:298] Evaluating on the training split.
I0502 20:48:18.897986 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 20:52:58.276349 140257772291904 spec.py:326] Evaluating on the test split.
I0502 20:57:44.468791 140257772291904 submission_runner.py:415] Time since start: 5761.26s, 	Step: 831, 	{'train/loss': 0.1313915126371068, 'validation/loss': 0.1308103370786517, 'validation/num_examples': 89000000, 'test/loss': 0.13387858412686685, 'test/num_examples': 89274637, 'score': 630.7505204677582, 'total_duration': 5761.262917518616, 'accumulated_submission_time': 630.7505204677582, 'accumulated_eval_time': 5130.408734560013, 'accumulated_logging_time': 0.09188485145568848}
I0502 20:57:44.482844 140068731705088 logging_writer.py:48] [831] accumulated_eval_time=5130.408735, accumulated_logging_time=0.091885, accumulated_submission_time=630.750520, global_step=831, preemption_count=0, score=630.750520, test/loss=0.133879, test/num_examples=89274637, total_duration=5761.262918, train/loss=0.131392, validation/loss=0.130810, validation/num_examples=89000000
I0502 20:58:23.665378 140068740097792 logging_writer.py:48] [900] global_step=900, grad_norm=0.06818489730358124, loss=0.12458829581737518
I0502 20:59:44.704660 140257772291904 spec.py:298] Evaluating on the training split.
I0502 21:04:35.578082 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 21:09:13.941385 140257772291904 spec.py:326] Evaluating on the test split.
I0502 21:13:47.111613 140257772291904 submission_runner.py:415] Time since start: 6723.91s, 	Step: 996, 	{'train/loss': 0.12634816285527042, 'validation/loss': 0.12751138202247192, 'validation/num_examples': 89000000, 'test/loss': 0.13018327926665219, 'test/num_examples': 89274637, 'score': 750.9625430107117, 'total_duration': 6723.905704975128, 'accumulated_submission_time': 750.9625430107117, 'accumulated_eval_time': 5972.8156042099, 'accumulated_logging_time': 0.1134178638458252}
I0502 21:13:47.120917 140068731705088 logging_writer.py:48] [996] accumulated_eval_time=5972.815604, accumulated_logging_time=0.113418, accumulated_submission_time=750.962543, global_step=996, preemption_count=0, score=750.962543, test/loss=0.130183, test/num_examples=89274637, total_duration=6723.905705, train/loss=0.126348, validation/loss=0.127511, validation/num_examples=89000000
I0502 21:13:47.716564 140068740097792 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.06651024520397186, loss=0.14143717288970947
I0502 21:14:55.907136 140068731705088 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.06500154733657837, loss=0.12887640297412872
I0502 21:15:47.129976 140257772291904 spec.py:298] Evaluating on the training split.
I0502 21:20:37.372706 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 21:25:15.952426 140257772291904 spec.py:326] Evaluating on the test split.
I0502 21:30:04.731182 140257772291904 submission_runner.py:415] Time since start: 7701.53s, 	Step: 1161, 	{'train/loss': 0.12608842660259728, 'validation/loss': 0.1272155617977528, 'validation/num_examples': 89000000, 'test/loss': 0.129798477926043, 'test/num_examples': 89274637, 'score': 870.9622111320496, 'total_duration': 7701.5252957344055, 'accumulated_submission_time': 870.9622111320496, 'accumulated_eval_time': 6830.416739225388, 'accumulated_logging_time': 0.12988662719726562}
I0502 21:30:04.743335 140068740097792 logging_writer.py:48] [1161] accumulated_eval_time=6830.416739, accumulated_logging_time=0.129887, accumulated_submission_time=870.962211, global_step=1161, preemption_count=0, score=870.962211, test/loss=0.129798, test/num_examples=89274637, total_duration=7701.525296, train/loss=0.126088, validation/loss=0.127216, validation/num_examples=89000000
I0502 21:30:18.608511 140068731705088 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.45712360739707947, loss=0.13286900520324707
I0502 21:31:44.939393 140068740097792 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.030884699895977974, loss=0.13670209050178528
I0502 21:32:04.834075 140257772291904 spec.py:298] Evaluating on the training split.
I0502 21:36:54.222604 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 21:41:31.451109 140257772291904 spec.py:326] Evaluating on the test split.
I0502 21:46:03.104262 140257772291904 submission_runner.py:415] Time since start: 8659.90s, 	Step: 1324, 	{'train/loss': 0.12684647066791413, 'validation/loss': 0.12717857303370786, 'validation/num_examples': 89000000, 'test/loss': 0.12976594909033345, 'test/num_examples': 89274637, 'score': 991.0431108474731, 'total_duration': 8659.898387432098, 'accumulated_submission_time': 991.0431108474731, 'accumulated_eval_time': 7668.68686914444, 'accumulated_logging_time': 0.14966964721679688}
I0502 21:46:03.114123 140068731705088 logging_writer.py:48] [1324] accumulated_eval_time=7668.686869, accumulated_logging_time=0.149670, accumulated_submission_time=991.043111, global_step=1324, preemption_count=0, score=991.043111, test/loss=0.129766, test/num_examples=89274637, total_duration=8659.898387, train/loss=0.126846, validation/loss=0.127179, validation/num_examples=89000000
I0502 21:46:48.066704 140068740097792 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.009279584512114525, loss=0.12214066088199615
I0502 21:48:03.798570 140257772291904 spec.py:298] Evaluating on the training split.
I0502 21:52:52.199350 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 21:57:32.222895 140257772291904 spec.py:326] Evaluating on the test split.
I0502 22:02:12.157105 140257772291904 submission_runner.py:415] Time since start: 9628.95s, 	Step: 1490, 	{'train/loss': 0.12746044885945548, 'validation/loss': 0.12687034831460675, 'validation/num_examples': 89000000, 'test/loss': 0.12944066073323826, 'test/num_examples': 89274637, 'score': 1111.717494726181, 'total_duration': 9628.951219797134, 'accumulated_submission_time': 1111.717494726181, 'accumulated_eval_time': 8517.045338630676, 'accumulated_logging_time': 0.1673600673675537}
I0502 22:02:12.166614 140068731705088 logging_writer.py:48] [1490] accumulated_eval_time=8517.045339, accumulated_logging_time=0.167360, accumulated_submission_time=1111.717495, global_step=1490, preemption_count=0, score=1111.717495, test/loss=0.129441, test/num_examples=89274637, total_duration=9628.951220, train/loss=0.127460, validation/loss=0.126870, validation/num_examples=89000000
I0502 22:02:13.447721 140068740097792 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.015220393426716328, loss=0.1305653303861618
I0502 22:03:24.780721 140257772291904 spec.py:298] Evaluating on the training split.
I0502 22:08:13.015699 140257772291904 spec.py:310] Evaluating on the validation split.
I0502 22:12:51.341157 140257772291904 spec.py:326] Evaluating on the test split.
I0502 22:17:31.566253 140257772291904 submission_runner.py:415] Time since start: 10548.36s, 	Step: 1600, 	{'train/loss': 0.12599890612783987, 'validation/loss': 0.12838424719101124, 'validation/num_examples': 89000000, 'test/loss': 0.13090520883327703, 'test/num_examples': 89274637, 'score': 1184.322539806366, 'total_duration': 10548.360373735428, 'accumulated_submission_time': 1184.322539806366, 'accumulated_eval_time': 9363.830827236176, 'accumulated_logging_time': 0.18427300453186035}
I0502 22:17:31.575488 140068731705088 logging_writer.py:48] [1600] accumulated_eval_time=9363.830827, accumulated_logging_time=0.184273, accumulated_submission_time=1184.322540, global_step=1600, preemption_count=0, score=1184.322540, test/loss=0.130905, test/num_examples=89274637, total_duration=10548.360374, train/loss=0.125999, validation/loss=0.128384, validation/num_examples=89000000
I0502 22:17:31.587943 140068740097792 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1184.322540
I0502 22:17:32.979269 140257772291904 checkpoints.py:356] Saving checkpoint at step: 1600
I0502 22:17:40.800940 140257772291904 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1/checkpoint_1600
I0502 22:17:40.807734 140257772291904 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_adafactor/criteo1tb_jax/trial_1/checkpoint_1600.
I0502 22:17:40.855278 140257772291904 submission_runner.py:578] Tuning trial 1/1
I0502 22:17:40.855458 140257772291904 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 22:17:40.857063 140257772291904 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/loss': 1.6795184519292679, 'validation/loss': 1.681331595505618, 'validation/num_examples': 89000000, 'test/loss': 1.6805418094279119, 'test/num_examples': 89274637, 'score': 28.986578941345215, 'total_duration': 902.4972152709961, 'accumulated_submission_time': 28.986578941345215, 'accumulated_eval_time': 873.5104098320007, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (167, {'train/loss': 0.15355806441935133, 'validation/loss': 0.1534670224719101, 'validation/num_examples': 89000000, 'test/loss': 0.15716546682794127, 'test/num_examples': 89274637, 'score': 149.54194498062134, 'total_duration': 1880.9204688072205, 'accumulated_submission_time': 149.54194498062134, 'accumulated_eval_time': 1731.3485655784607, 'accumulated_logging_time': 0.027329683303833008, 'global_step': 167, 'preemption_count': 0}), (334, {'train/loss': 0.12895920102959438, 'validation/loss': 0.13002358426966293, 'validation/num_examples': 89000000, 'test/loss': 0.13318787283335579, 'test/num_examples': 89274637, 'score': 269.8450040817261, 'total_duration': 2842.4893493652344, 'accumulated_submission_time': 269.8450040817261, 'accumulated_eval_time': 2572.5961565971375, 'accumulated_logging_time': 0.04318809509277344, 'global_step': 334, 'preemption_count': 0}), (500, {'train/loss': 0.12892452910860466, 'validation/loss': 0.12966331460674158, 'validation/num_examples': 89000000, 'test/loss': 0.13327652063149806, 'test/num_examples': 89274637, 'score': 389.8908112049103, 'total_duration': 3819.935711145401, 'accumulated_submission_time': 389.8908112049103, 'accumulated_eval_time': 3429.9788584709167, 'accumulated_logging_time': 0.058791399002075195, 'global_step': 500, 'preemption_count': 0}), (666, {'train/loss': 0.12509806494751427, 'validation/loss': 0.12767765168539325, 'validation/num_examples': 89000000, 'test/loss': 0.13075597271820888, 'test/num_examples': 89274637, 'score': 509.9920377731323, 'total_duration': 4786.941342353821, 'accumulated_submission_time': 509.9920377731323, 'accumulated_eval_time': 4276.864053487778, 'accumulated_logging_time': 0.07575249671936035, 'global_step': 666, 'preemption_count': 0}), (831, {'train/loss': 0.1313915126371068, 'validation/loss': 0.1308103370786517, 'validation/num_examples': 89000000, 'test/loss': 0.13387858412686685, 'test/num_examples': 89274637, 'score': 630.7505204677582, 'total_duration': 5761.262917518616, 'accumulated_submission_time': 630.7505204677582, 'accumulated_eval_time': 5130.408734560013, 'accumulated_logging_time': 0.09188485145568848, 'global_step': 831, 'preemption_count': 0}), (996, {'train/loss': 0.12634816285527042, 'validation/loss': 0.12751138202247192, 'validation/num_examples': 89000000, 'test/loss': 0.13018327926665219, 'test/num_examples': 89274637, 'score': 750.9625430107117, 'total_duration': 6723.905704975128, 'accumulated_submission_time': 750.9625430107117, 'accumulated_eval_time': 5972.8156042099, 'accumulated_logging_time': 0.1134178638458252, 'global_step': 996, 'preemption_count': 0}), (1161, {'train/loss': 0.12608842660259728, 'validation/loss': 0.1272155617977528, 'validation/num_examples': 89000000, 'test/loss': 0.129798477926043, 'test/num_examples': 89274637, 'score': 870.9622111320496, 'total_duration': 7701.5252957344055, 'accumulated_submission_time': 870.9622111320496, 'accumulated_eval_time': 6830.416739225388, 'accumulated_logging_time': 0.12988662719726562, 'global_step': 1161, 'preemption_count': 0}), (1324, {'train/loss': 0.12684647066791413, 'validation/loss': 0.12717857303370786, 'validation/num_examples': 89000000, 'test/loss': 0.12976594909033345, 'test/num_examples': 89274637, 'score': 991.0431108474731, 'total_duration': 8659.898387432098, 'accumulated_submission_time': 991.0431108474731, 'accumulated_eval_time': 7668.68686914444, 'accumulated_logging_time': 0.14966964721679688, 'global_step': 1324, 'preemption_count': 0}), (1490, {'train/loss': 0.12746044885945548, 'validation/loss': 0.12687034831460675, 'validation/num_examples': 89000000, 'test/loss': 0.12944066073323826, 'test/num_examples': 89274637, 'score': 1111.717494726181, 'total_duration': 9628.951219797134, 'accumulated_submission_time': 1111.717494726181, 'accumulated_eval_time': 8517.045338630676, 'accumulated_logging_time': 0.1673600673675537, 'global_step': 1490, 'preemption_count': 0}), (1600, {'train/loss': 0.12599890612783987, 'validation/loss': 0.12838424719101124, 'validation/num_examples': 89000000, 'test/loss': 0.13090520883327703, 'test/num_examples': 89274637, 'score': 1184.322539806366, 'total_duration': 10548.360373735428, 'accumulated_submission_time': 1184.322539806366, 'accumulated_eval_time': 9363.830827236176, 'accumulated_logging_time': 0.18427300453186035, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0502 22:17:40.857180 140257772291904 submission_runner.py:581] Timing: 1184.322539806366
I0502 22:17:40.857230 140257772291904 submission_runner.py:582] ====================
I0502 22:17:40.857337 140257772291904 submission_runner.py:645] Final criteo1tb score: 1184.322539806366
