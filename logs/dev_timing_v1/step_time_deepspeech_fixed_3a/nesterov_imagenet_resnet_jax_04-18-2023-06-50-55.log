I0418 06:51:16.676677 140667003242304 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax.
I0418 06:51:16.749267 140667003242304 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 06:51:17.549406 140667003242304 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0418 06:51:17.550257 140667003242304 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 06:51:17.554726 140667003242304 submission_runner.py:528] Using RNG seed 583080277
I0418 06:51:20.413144 140667003242304 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 06:51:20.413377 140667003242304 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1.
I0418 06:51:20.413591 140667003242304 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/hparams.json.
I0418 06:51:20.535687 140667003242304 submission_runner.py:232] Initializing dataset.
I0418 06:51:20.548676 140667003242304 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:20.556315 140667003242304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:51:20.556464 140667003242304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:51:20.820231 140667003242304 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:21.911872 140667003242304 submission_runner.py:239] Initializing model.
I0418 06:51:34.088685 140667003242304 submission_runner.py:249] Initializing optimizer.
I0418 06:51:35.077477 140667003242304 submission_runner.py:256] Initializing metrics bundle.
I0418 06:51:35.077700 140667003242304 submission_runner.py:273] Initializing checkpoint and logger.
I0418 06:51:35.078786 140667003242304 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0418 06:51:35.915794 140667003242304 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0418 06:51:35.916959 140667003242304 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/flags_0.json.
I0418 06:51:35.922941 140667003242304 submission_runner.py:309] Starting training loop.
I0418 06:52:21.832167 140490477377280 logging_writer.py:48] [0] global_step=0, grad_norm=0.5344582200050354, loss=6.924396514892578
I0418 06:52:21.845499 140667003242304 spec.py:298] Evaluating on the training split.
I0418 06:52:22.326140 140667003242304 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:22.332082 140667003242304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:52:22.332195 140667003242304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:52:22.391564 140667003242304 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:34.017788 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 06:52:34.687427 140667003242304 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:34.704511 140667003242304 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:52:34.704815 140667003242304 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:52:34.766669 140667003242304 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:51.888007 140667003242304 spec.py:326] Evaluating on the test split.
I0418 06:52:52.287128 140667003242304 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 06:52:52.292737 140667003242304 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0418 06:52:52.321460 140667003242304 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 06:53:01.372551 140667003242304 submission_runner.py:406] Time since start: 85.45s, 	Step: 1, 	{'train/accuracy': 0.0008171236841008067, 'train/loss': 6.9130072593688965, 'validation/accuracy': 0.0007800000021234155, 'validation/loss': 6.9127702713012695, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.912158012390137, 'test/num_examples': 10000, 'score': 45.922388792037964, 'total_duration': 85.44957613945007, 'accumulated_submission_time': 45.922388792037964, 'accumulated_eval_time': 39.527005672454834, 'accumulated_logging_time': 0}
I0418 06:53:01.389319 140461570242304 logging_writer.py:48] [1] accumulated_eval_time=39.527006, accumulated_logging_time=0, accumulated_submission_time=45.922389, global_step=1, preemption_count=0, score=45.922389, test/accuracy=0.001000, test/loss=6.912158, test/num_examples=10000, total_duration=85.449576, train/accuracy=0.000817, train/loss=6.913007, validation/accuracy=0.000780, validation/loss=6.912770, validation/num_examples=50000
I0418 06:53:01.491714 140667003242304 checkpoints.py:356] Saving checkpoint at step: 1
I0418 06:53:01.915105 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1
I0418 06:53:01.915896 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1.
I0418 06:53:35.718764 140461578635008 logging_writer.py:48] [100] global_step=100, grad_norm=0.5167592167854309, loss=6.883957386016846
I0418 06:54:09.616534 140461738030848 logging_writer.py:48] [200] global_step=200, grad_norm=0.5632033944129944, loss=6.797079563140869
I0418 06:54:43.557210 140461578635008 logging_writer.py:48] [300] global_step=300, grad_norm=0.586367130279541, loss=6.673442840576172
I0418 06:55:17.698851 140461738030848 logging_writer.py:48] [400] global_step=400, grad_norm=0.6144461631774902, loss=6.633859634399414
I0418 06:55:51.701946 140461578635008 logging_writer.py:48] [500] global_step=500, grad_norm=0.6641706228256226, loss=6.5259575843811035
I0418 06:56:25.728305 140461738030848 logging_writer.py:48] [600] global_step=600, grad_norm=0.6820127367973328, loss=6.454746723175049
I0418 06:56:59.864828 140461578635008 logging_writer.py:48] [700] global_step=700, grad_norm=0.865712583065033, loss=6.453579425811768
I0418 06:57:33.937336 140461738030848 logging_writer.py:48] [800] global_step=800, grad_norm=1.0006321668624878, loss=6.346712112426758
I0418 06:58:08.086371 140461578635008 logging_writer.py:48] [900] global_step=900, grad_norm=0.8900097012519836, loss=6.259836196899414
I0418 06:58:42.278821 140461738030848 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0312135219573975, loss=6.273059368133545
I0418 06:59:16.411866 140461578635008 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8107569813728333, loss=6.1476311683654785
I0418 06:59:50.593069 140461738030848 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0981955528259277, loss=6.191852569580078
I0418 07:00:24.788934 140461578635008 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.120953917503357, loss=6.122503757476807
I0418 07:00:59.096562 140461738030848 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0468838214874268, loss=6.051987171173096
I0418 07:01:31.959948 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:01:38.596574 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:01:46.366485 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:01:48.560130 140667003242304 submission_runner.py:406] Time since start: 612.64s, 	Step: 1498, 	{'train/accuracy': 0.07720822840929031, 'train/loss': 5.365084648132324, 'validation/accuracy': 0.06955999881029129, 'validation/loss': 5.445878505706787, 'validation/num_examples': 50000, 'test/accuracy': 0.049400001764297485, 'test/loss': 5.671300411224365, 'test/num_examples': 10000, 'score': 555.9447631835938, 'total_duration': 612.6371603012085, 'accumulated_submission_time': 555.9447631835938, 'accumulated_eval_time': 56.12717008590698, 'accumulated_logging_time': 0.5469112396240234}
I0418 07:01:48.568099 140461830285056 logging_writer.py:48] [1498] accumulated_eval_time=56.127170, accumulated_logging_time=0.546911, accumulated_submission_time=555.944763, global_step=1498, preemption_count=0, score=555.944763, test/accuracy=0.049400, test/loss=5.671300, test/num_examples=10000, total_duration=612.637160, train/accuracy=0.077208, train/loss=5.365085, validation/accuracy=0.069560, validation/loss=5.445879, validation/num_examples=50000
I0418 07:01:48.688313 140667003242304 checkpoints.py:356] Saving checkpoint at step: 1498
I0418 07:01:49.115323 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1498
I0418 07:01:49.116023 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_1498.
I0418 07:01:50.150720 140461838677760 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8606637716293335, loss=6.0485734939575195
I0418 07:02:24.183826 140491257505536 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9159696102142334, loss=5.919382572174072
I0418 07:02:58.149324 140461838677760 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9240443110466003, loss=5.874255180358887
I0418 07:03:32.176456 140491257505536 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1218031644821167, loss=5.855753421783447
I0418 07:04:06.185220 140461838677760 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9717350006103516, loss=5.749869346618652
I0418 07:04:39.977070 140491257505536 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0393853187561035, loss=5.6932783126831055
I0418 07:05:13.930817 140461838677760 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9072436094284058, loss=5.602264404296875
I0418 07:05:47.936333 140491257505536 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9734311699867249, loss=5.562370777130127
I0418 07:06:21.934057 140461838677760 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1524393558502197, loss=5.508042812347412
I0418 07:06:56.044221 140491257505536 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9184008836746216, loss=5.475046157836914
I0418 07:07:29.793474 140461838677760 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1468549966812134, loss=5.384261608123779
I0418 07:08:03.738192 140491257505536 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9781226515769958, loss=5.398617267608643
I0418 07:08:37.453644 140461838677760 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9054761528968811, loss=5.301366329193115
I0418 07:09:11.483156 140491257505536 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9547821283340454, loss=5.220325946807861
I0418 07:09:45.364231 140461838677760 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9606229662895203, loss=5.195617198944092
I0418 07:10:19.206203 140491257505536 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.106669306755066, loss=5.206217288970947
I0418 07:10:19.211865 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:10:25.931862 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:10:33.730873 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:10:35.864226 140667003242304 submission_runner.py:406] Time since start: 1139.94s, 	Step: 3001, 	{'train/accuracy': 0.20643335580825806, 'train/loss': 4.197149276733398, 'validation/accuracy': 0.19054000079631805, 'validation/loss': 4.312524795532227, 'validation/num_examples': 50000, 'test/accuracy': 0.13740000128746033, 'test/loss': 4.731720924377441, 'test/num_examples': 10000, 'score': 1066.0195486545563, 'total_duration': 1139.9412415027618, 'accumulated_submission_time': 1066.0195486545563, 'accumulated_eval_time': 72.77947306632996, 'accumulated_logging_time': 1.1061136722564697}
I0418 07:10:35.871222 140461838677760 logging_writer.py:48] [3001] accumulated_eval_time=72.779473, accumulated_logging_time=1.106114, accumulated_submission_time=1066.019549, global_step=3001, preemption_count=0, score=1066.019549, test/accuracy=0.137400, test/loss=4.731721, test/num_examples=10000, total_duration=1139.941242, train/accuracy=0.206433, train/loss=4.197149, validation/accuracy=0.190540, validation/loss=4.312525, validation/num_examples=50000
I0418 07:10:35.986857 140667003242304 checkpoints.py:356] Saving checkpoint at step: 3001
I0418 07:10:36.403919 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_3001
I0418 07:10:36.404779 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_3001.
I0418 07:11:10.506563 140491257505536 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9873649477958679, loss=5.1150126457214355
I0418 07:11:44.538230 140491223934720 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0035357475280762, loss=5.064157485961914
I0418 07:12:18.572170 140491257505536 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.908349871635437, loss=4.981502056121826
I0418 07:12:52.645567 140491223934720 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9009381532669067, loss=4.965975284576416
I0418 07:13:26.526192 140491257505536 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8940492272377014, loss=4.947842597961426
I0418 07:14:00.586747 140491223934720 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8725962042808533, loss=4.708115577697754
I0418 07:14:34.580904 140491257505536 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9902053475379944, loss=4.839038372039795
I0418 07:15:08.537374 140491223934720 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.900092601776123, loss=4.7679762840271
I0418 07:15:42.667989 140491257505536 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9164801239967346, loss=4.807107925415039
I0418 07:16:16.615116 140491223934720 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8817092776298523, loss=4.703343391418457
I0418 07:16:50.543814 140491257505536 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8388749361038208, loss=4.681042194366455
I0418 07:17:24.475580 140491223934720 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8185034394264221, loss=4.657382488250732
I0418 07:17:58.627889 140491257505536 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8987184166908264, loss=4.654426574707031
I0418 07:18:32.810308 140491223934720 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7728869318962097, loss=4.558056831359863
I0418 07:19:06.462039 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:19:13.027706 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:19:20.966394 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:19:23.020287 140667003242304 submission_runner.py:406] Time since start: 1667.10s, 	Step: 4500, 	{'train/accuracy': 0.3480149805545807, 'train/loss': 3.2009353637695312, 'validation/accuracy': 0.3256799876689911, 'validation/loss': 3.333479642868042, 'validation/num_examples': 50000, 'test/accuracy': 0.24400001764297485, 'test/loss': 3.904920816421509, 'test/num_examples': 10000, 'score': 1576.0553605556488, 'total_duration': 1667.097308397293, 'accumulated_submission_time': 1576.0553605556488, 'accumulated_eval_time': 89.33768820762634, 'accumulated_logging_time': 1.6503052711486816}
I0418 07:19:23.027853 140491257505536 logging_writer.py:48] [4500] accumulated_eval_time=89.337688, accumulated_logging_time=1.650305, accumulated_submission_time=1576.055361, global_step=4500, preemption_count=0, score=1576.055361, test/accuracy=0.244000, test/loss=3.904921, test/num_examples=10000, total_duration=1667.097308, train/accuracy=0.348015, train/loss=3.200935, validation/accuracy=0.325680, validation/loss=3.333480, validation/num_examples=50000
I0418 07:19:23.151215 140667003242304 checkpoints.py:356] Saving checkpoint at step: 4500
I0418 07:19:23.566528 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_4500
I0418 07:19:23.567363 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_4500.
I0418 07:19:23.921571 140491223934720 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8135327696800232, loss=4.496430397033691
I0418 07:19:57.664362 140488845801216 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8468062877655029, loss=4.4938764572143555
I0418 07:20:31.653372 140491223934720 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7769564986228943, loss=4.583587169647217
I0418 07:21:05.634526 140488845801216 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7658675909042358, loss=4.44700813293457
I0418 07:21:39.438157 140491223934720 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8110700249671936, loss=4.5137481689453125
I0418 07:22:13.379949 140488845801216 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7755061388015747, loss=4.458505153656006
I0418 07:22:47.293538 140491223934720 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7773399353027344, loss=4.472786903381348
I0418 07:23:20.916390 140488845801216 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7522395253181458, loss=4.417014122009277
I0418 07:23:54.767801 140491223934720 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7551364302635193, loss=4.353631973266602
I0418 07:24:28.744320 140488845801216 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7454014420509338, loss=4.394413948059082
I0418 07:25:02.461237 140491223934720 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7298183441162109, loss=4.2898173332214355
I0418 07:25:36.307866 140488845801216 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7239924669265747, loss=4.321506023406982
I0418 07:26:10.400460 140491223934720 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7823778390884399, loss=4.261075973510742
I0418 07:26:44.428466 140488845801216 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7683752775192261, loss=4.25689172744751
I0418 07:27:18.140348 140491223934720 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7136297225952148, loss=4.315739154815674
I0418 07:27:52.188337 140488845801216 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6923584938049316, loss=4.195736885070801
I0418 07:27:53.610761 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:28:00.268109 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:28:08.087472 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:28:10.236446 140667003242304 submission_runner.py:406] Time since start: 2194.31s, 	Step: 6006, 	{'train/accuracy': 0.43168047070503235, 'train/loss': 2.8336288928985596, 'validation/accuracy': 0.4035399854183197, 'validation/loss': 2.9681828022003174, 'validation/num_examples': 50000, 'test/accuracy': 0.30410000681877136, 'test/loss': 3.5501317977905273, 'test/num_examples': 10000, 'score': 2086.0764389038086, 'total_duration': 2194.3134660720825, 'accumulated_submission_time': 2086.0764389038086, 'accumulated_eval_time': 105.96333742141724, 'accumulated_logging_time': 2.201946973800659}
I0418 07:28:10.244158 140491223934720 logging_writer.py:48] [6006] accumulated_eval_time=105.963337, accumulated_logging_time=2.201947, accumulated_submission_time=2086.076439, global_step=6006, preemption_count=0, score=2086.076439, test/accuracy=0.304100, test/loss=3.550132, test/num_examples=10000, total_duration=2194.313466, train/accuracy=0.431680, train/loss=2.833629, validation/accuracy=0.403540, validation/loss=2.968183, validation/num_examples=50000
I0418 07:28:10.362766 140667003242304 checkpoints.py:356] Saving checkpoint at step: 6006
I0418 07:28:10.781875 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_6006
I0418 07:28:10.782667 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_6006.
I0418 07:28:43.065791 140488845801216 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6921621561050415, loss=4.201565265655518
I0418 07:29:17.032414 140488837408512 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6652206182479858, loss=4.233756065368652
I0418 07:29:51.169754 140488845801216 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7025887966156006, loss=4.147950172424316
I0418 07:30:25.130390 140488837408512 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6563929319381714, loss=4.1272687911987305
I0418 07:30:58.951663 140488845801216 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.686848521232605, loss=4.146140098571777
I0418 07:31:33.039633 140488837408512 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6600474119186401, loss=4.071080684661865
I0418 07:32:07.188190 140488845801216 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6714235544204712, loss=4.131551742553711
I0418 07:32:41.028028 140488837408512 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6379827260971069, loss=4.071631908416748
I0418 07:33:15.220947 140488845801216 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6491089463233948, loss=4.02203893661499
I0418 07:33:49.100265 140488837408512 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7039463520050049, loss=4.0980634689331055
I0418 07:34:23.033677 140488845801216 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6425044536590576, loss=4.059837818145752
I0418 07:34:56.914286 140488837408512 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6124131679534912, loss=4.07000207901001
I0418 07:35:30.782432 140488845801216 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6400256156921387, loss=4.060068607330322
I0418 07:36:04.881323 140488837408512 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6376558542251587, loss=4.045214653015137
I0418 07:36:39.156270 140488845801216 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6230922341346741, loss=3.951268196105957
I0418 07:36:40.951112 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:36:47.628871 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:36:55.407070 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:36:57.421029 140667003242304 submission_runner.py:406] Time since start: 2721.50s, 	Step: 7507, 	{'train/accuracy': 0.5023716688156128, 'train/loss': 2.514901638031006, 'validation/accuracy': 0.4688199758529663, 'validation/loss': 2.6698458194732666, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 3.2545018196105957, 'test/num_examples': 10000, 'score': 2596.2232024669647, 'total_duration': 2721.4980552196503, 'accumulated_submission_time': 2596.2232024669647, 'accumulated_eval_time': 122.43322825431824, 'accumulated_logging_time': 2.7523934841156006}
I0418 07:36:57.428519 140488837408512 logging_writer.py:48] [7507] accumulated_eval_time=122.433228, accumulated_logging_time=2.752393, accumulated_submission_time=2596.223202, global_step=7507, preemption_count=0, score=2596.223202, test/accuracy=0.367900, test/loss=3.254502, test/num_examples=10000, total_duration=2721.498055, train/accuracy=0.502372, train/loss=2.514902, validation/accuracy=0.468820, validation/loss=2.669846, validation/num_examples=50000
I0418 07:36:57.584200 140667003242304 checkpoints.py:356] Saving checkpoint at step: 7507
I0418 07:36:58.208533 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_7507
I0418 07:36:58.216876 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_7507.
I0418 07:37:30.056033 140488845801216 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6175235509872437, loss=3.9726014137268066
I0418 07:38:04.140192 140488829015808 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6212586760520935, loss=3.947368860244751
I0418 07:38:38.058133 140488845801216 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5985193252563477, loss=3.924790859222412
I0418 07:39:12.057435 140488829015808 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5964112877845764, loss=3.8967087268829346
I0418 07:39:46.139352 140488845801216 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5810618996620178, loss=3.872904062271118
I0418 07:40:20.052959 140488829015808 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5850277543067932, loss=3.933335065841675
I0418 07:40:54.161492 140488845801216 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6000820994377136, loss=3.885263442993164
I0418 07:41:28.269233 140488829015808 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5891068577766418, loss=3.9289207458496094
I0418 07:42:02.286301 140488845801216 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5983144640922546, loss=3.9828429222106934
I0418 07:42:36.227302 140488829015808 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5723458528518677, loss=3.885753631591797
I0418 07:43:10.078580 140488845801216 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5676937699317932, loss=3.8323309421539307
I0418 07:43:44.036969 140488829015808 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5437124967575073, loss=3.779750347137451
I0418 07:44:18.069821 140488845801216 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5689408183097839, loss=3.9381825923919678
I0418 07:44:51.965815 140488829015808 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5510286688804626, loss=3.759697914123535
I0418 07:45:25.930433 140488845801216 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5676498413085938, loss=3.84962797164917
I0418 07:45:28.415634 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:45:35.029724 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:45:42.998610 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:45:47.825532 140667003242304 submission_runner.py:406] Time since start: 3251.90s, 	Step: 9009, 	{'train/accuracy': 0.5494260191917419, 'train/loss': 2.130084753036499, 'validation/accuracy': 0.5134199857711792, 'validation/loss': 2.3026225566864014, 'validation/num_examples': 50000, 'test/accuracy': 0.39590001106262207, 'test/loss': 2.9769904613494873, 'test/num_examples': 10000, 'score': 3106.3970606327057, 'total_duration': 3251.902557373047, 'accumulated_submission_time': 3106.3970606327057, 'accumulated_eval_time': 141.84310150146484, 'accumulated_logging_time': 3.5556774139404297}
I0418 07:45:47.834731 140488829015808 logging_writer.py:48] [9009] accumulated_eval_time=141.843102, accumulated_logging_time=3.555677, accumulated_submission_time=3106.397061, global_step=9009, preemption_count=0, score=3106.397061, test/accuracy=0.395900, test/loss=2.976990, test/num_examples=10000, total_duration=3251.902557, train/accuracy=0.549426, train/loss=2.130085, validation/accuracy=0.513420, validation/loss=2.302623, validation/num_examples=50000
I0418 07:45:47.959364 140667003242304 checkpoints.py:356] Saving checkpoint at step: 9009
I0418 07:45:48.394996 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_9009
I0418 07:45:48.395991 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_9009.
I0418 07:46:19.827417 140488845801216 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5619856119155884, loss=3.892066240310669
I0418 07:46:53.841952 140490116687616 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5745622515678406, loss=3.831615924835205
I0418 07:47:27.870212 140488845801216 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5417689681053162, loss=3.7702322006225586
I0418 07:48:01.873270 140490116687616 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5462604761123657, loss=3.7119839191436768
I0418 07:48:36.004553 140488845801216 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5321264863014221, loss=3.7574784755706787
I0418 07:49:10.087435 140490116687616 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5494792461395264, loss=3.711592197418213
I0418 07:49:44.157340 140488845801216 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5585700273513794, loss=3.8409600257873535
I0418 07:50:18.261971 140490116687616 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5323765277862549, loss=3.7540364265441895
I0418 07:50:52.463248 140488845801216 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5148938298225403, loss=3.6073577404022217
I0418 07:51:26.536458 140490116687616 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5406501293182373, loss=3.734063148498535
I0418 07:52:00.733793 140488845801216 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5253178477287292, loss=3.720752000808716
I0418 07:52:34.917680 140490116687616 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5149482488632202, loss=3.671703338623047
I0418 07:53:09.016888 140488845801216 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5267918705940247, loss=3.7073974609375
I0418 07:53:43.391795 140490116687616 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5332453846931458, loss=3.7634191513061523
I0418 07:54:17.505161 140488845801216 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5046651363372803, loss=3.6698219776153564
I0418 07:54:18.629323 140667003242304 spec.py:298] Evaluating on the training split.
I0418 07:54:25.233810 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 07:54:33.524787 140667003242304 spec.py:326] Evaluating on the test split.
I0418 07:54:35.695999 140667003242304 submission_runner.py:406] Time since start: 3779.77s, 	Step: 10505, 	{'train/accuracy': 0.618183970451355, 'train/loss': 1.773877501487732, 'validation/accuracy': 0.5518999695777893, 'validation/loss': 2.072688102722168, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.7533833980560303, 'test/num_examples': 10000, 'score': 3616.6103620529175, 'total_duration': 3779.773030281067, 'accumulated_submission_time': 3616.6103620529175, 'accumulated_eval_time': 158.90975785255432, 'accumulated_logging_time': 4.12972617149353}
I0418 07:54:35.704066 140490116687616 logging_writer.py:48] [10505] accumulated_eval_time=158.909758, accumulated_logging_time=4.129726, accumulated_submission_time=3616.610362, global_step=10505, preemption_count=0, score=3616.610362, test/accuracy=0.425100, test/loss=2.753383, test/num_examples=10000, total_duration=3779.773030, train/accuracy=0.618184, train/loss=1.773878, validation/accuracy=0.551900, validation/loss=2.072688, validation/num_examples=50000
I0418 07:54:35.870748 140667003242304 checkpoints.py:356] Saving checkpoint at step: 10505
I0418 07:54:36.494402 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_10505
I0418 07:54:36.503345 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_10505.
I0418 07:55:09.077885 140488845801216 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5021916031837463, loss=3.6689720153808594
I0418 07:55:42.818794 140490099902208 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5166739225387573, loss=3.6897029876708984
I0418 07:56:16.730294 140488845801216 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5415303111076355, loss=3.6912479400634766
I0418 07:56:50.854655 140490099902208 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5181970000267029, loss=3.6311678886413574
I0418 07:57:24.732648 140488845801216 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5162931084632874, loss=3.6353352069854736
I0418 07:57:58.742309 140490099902208 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4907347857952118, loss=3.5901875495910645
I0418 07:58:32.606174 140488845801216 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5239419341087341, loss=3.6116068363189697
I0418 07:59:06.590357 140490099902208 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5201036334037781, loss=3.569289207458496
I0418 07:59:40.718385 140488845801216 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5263354182243347, loss=3.6137285232543945
I0418 08:00:14.672683 140490099902208 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5238621234893799, loss=3.649111747741699
I0418 08:00:48.566387 140488845801216 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5288360118865967, loss=3.649538993835449
I0418 08:01:22.383997 140490099902208 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.4996522068977356, loss=3.613649606704712
I0418 08:01:56.280572 140488845801216 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.505771279335022, loss=3.548921585083008
I0418 08:02:30.235881 140490099902208 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.529473602771759, loss=3.702240467071533
I0418 08:03:04.136566 140488845801216 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.49180957674980164, loss=3.572108268737793
I0418 08:03:06.556229 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:03:13.363209 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:03:23.169096 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:03:25.296155 140667003242304 submission_runner.py:406] Time since start: 4309.37s, 	Step: 12009, 	{'train/accuracy': 0.6291653513908386, 'train/loss': 1.7606170177459717, 'validation/accuracy': 0.5730599761009216, 'validation/loss': 2.01579213142395, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.6731443405151367, 'test/num_examples': 10000, 'score': 4126.64240193367, 'total_duration': 4309.372197151184, 'accumulated_submission_time': 4126.64240193367, 'accumulated_eval_time': 177.64868807792664, 'accumulated_logging_time': 4.94071626663208}
I0418 08:03:25.304707 140490099902208 logging_writer.py:48] [12009] accumulated_eval_time=177.648688, accumulated_logging_time=4.940716, accumulated_submission_time=4126.642402, global_step=12009, preemption_count=0, score=4126.642402, test/accuracy=0.454200, test/loss=2.673144, test/num_examples=10000, total_duration=4309.372197, train/accuracy=0.629165, train/loss=1.760617, validation/accuracy=0.573060, validation/loss=2.015792, validation/num_examples=50000
I0418 08:03:25.462338 140667003242304 checkpoints.py:356] Saving checkpoint at step: 12009
I0418 08:03:26.090999 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_12009
I0418 08:03:26.100563 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_12009.
I0418 08:03:57.227679 140488845801216 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.4842383861541748, loss=3.5192885398864746
I0418 08:04:31.018750 140490091509504 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5033344030380249, loss=3.6248722076416016
I0418 08:05:05.017490 140488845801216 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5076785683631897, loss=3.5255167484283447
I0418 08:05:38.821728 140490091509504 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4921782314777374, loss=3.5364253520965576
I0418 08:06:12.776766 140488845801216 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.519862949848175, loss=3.580747365951538
I0418 08:06:46.646007 140490091509504 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.49964913725852966, loss=3.4916329383850098
I0418 08:07:20.738032 140488845801216 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4932326674461365, loss=3.5296339988708496
I0418 08:07:54.793820 140490091509504 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5002270936965942, loss=3.4545366764068604
I0418 08:08:28.857124 140488845801216 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.4730190932750702, loss=3.4618079662323
I0418 08:09:02.775581 140490091509504 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5177150964736938, loss=3.6292405128479004
I0418 08:09:36.598940 140488845801216 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.49806496500968933, loss=3.5180675983428955
I0418 08:10:10.483585 140490091509504 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4904021918773651, loss=3.520956039428711
I0418 08:10:44.299107 140488845801216 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.49770307540893555, loss=3.538907766342163
I0418 08:11:18.173630 140490091509504 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.48524391651153564, loss=3.54555344581604
I0418 08:11:51.935527 140488845801216 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.4782092571258545, loss=3.4171180725097656
I0418 08:11:56.182296 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:12:03.677531 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:12:13.683445 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:12:15.577928 140667003242304 submission_runner.py:406] Time since start: 4839.65s, 	Step: 13514, 	{'train/accuracy': 0.6588408946990967, 'train/loss': 1.6286795139312744, 'validation/accuracy': 0.6006199717521667, 'validation/loss': 1.8863894939422607, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.5425360202789307, 'test/num_examples': 10000, 'score': 4636.703469753265, 'total_duration': 4839.653674840927, 'accumulated_submission_time': 4636.703469753265, 'accumulated_eval_time': 197.0430154800415, 'accumulated_logging_time': 5.7485620975494385}
I0418 08:12:15.592624 140490091509504 logging_writer.py:48] [13514] accumulated_eval_time=197.043015, accumulated_logging_time=5.748562, accumulated_submission_time=4636.703470, global_step=13514, preemption_count=0, score=4636.703470, test/accuracy=0.475900, test/loss=2.542536, test/num_examples=10000, total_duration=4839.653675, train/accuracy=0.658841, train/loss=1.628680, validation/accuracy=0.600620, validation/loss=1.886389, validation/num_examples=50000
I0418 08:12:15.782393 140667003242304 checkpoints.py:356] Saving checkpoint at step: 13514
I0418 08:12:16.608555 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_13514
I0418 08:12:16.620345 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_13514.
I0418 08:12:46.028298 140488845801216 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4851817786693573, loss=3.546708106994629
I0418 08:13:20.008422 140490083116800 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.48707833886146545, loss=3.418137788772583
I0418 08:13:53.760748 140488845801216 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4908851683139801, loss=3.4976110458374023
I0418 08:14:27.620052 140490083116800 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5061897039413452, loss=3.489994525909424
I0418 08:15:01.732328 140488845801216 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.4830179214477539, loss=3.4034676551818848
I0418 08:15:35.680253 140490083116800 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5062001943588257, loss=3.4466092586517334
I0418 08:16:09.534411 140488845801216 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.4973781108856201, loss=3.3998568058013916
I0418 08:16:43.453617 140490083116800 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.4748665392398834, loss=3.4070277214050293
I0418 08:17:17.303344 140488845801216 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.48422351479530334, loss=3.422001838684082
I0418 08:17:51.215688 140490083116800 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.47585564851760864, loss=3.414457321166992
I0418 08:18:24.976794 140488845801216 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.49390873312950134, loss=3.4955480098724365
I0418 08:18:58.955914 140490083116800 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4775635600090027, loss=3.405627727508545
I0418 08:19:32.804717 140488845801216 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.48883000016212463, loss=3.4777190685272217
I0418 08:20:06.713895 140490083116800 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.4871003329753876, loss=3.435176134109497
I0418 08:20:40.537924 140488845801216 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.47342100739479065, loss=3.3826262950897217
I0418 08:20:46.693370 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:20:53.625352 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:21:03.446115 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:21:05.424991 140667003242304 submission_runner.py:406] Time since start: 5369.50s, 	Step: 15020, 	{'train/accuracy': 0.6713966727256775, 'train/loss': 1.5677212476730347, 'validation/accuracy': 0.6105599999427795, 'validation/loss': 1.8372849225997925, 'validation/num_examples': 50000, 'test/accuracy': 0.4887000322341919, 'test/loss': 2.4889912605285645, 'test/num_examples': 10000, 'score': 5146.75480389595, 'total_duration': 5369.5009508132935, 'accumulated_submission_time': 5146.75480389595, 'accumulated_eval_time': 215.77353811264038, 'accumulated_logging_time': 6.796183109283447}
I0418 08:21:05.433671 140490083116800 logging_writer.py:48] [15020] accumulated_eval_time=215.773538, accumulated_logging_time=6.796183, accumulated_submission_time=5146.754804, global_step=15020, preemption_count=0, score=5146.754804, test/accuracy=0.488700, test/loss=2.488991, test/num_examples=10000, total_duration=5369.500951, train/accuracy=0.671397, train/loss=1.567721, validation/accuracy=0.610560, validation/loss=1.837285, validation/num_examples=50000
I0418 08:21:05.612194 140667003242304 checkpoints.py:356] Saving checkpoint at step: 15020
I0418 08:21:06.243515 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_15020
I0418 08:21:06.252621 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_15020.
I0418 08:21:33.795476 140488845801216 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.4924224019050598, loss=3.423203945159912
I0418 08:22:07.642829 140490074724096 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.47650888562202454, loss=3.383765459060669
I0418 08:22:41.552616 140488845801216 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.4892318844795227, loss=3.4442107677459717
I0418 08:23:15.515733 140490074724096 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.46867236495018005, loss=3.377178907394409
I0418 08:23:49.142149 140488845801216 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.4998369514942169, loss=3.480584144592285
I0418 08:24:23.065422 140490074724096 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4902068078517914, loss=3.4506564140319824
I0418 08:24:56.955840 140488845801216 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.46735724806785583, loss=3.3846182823181152
I0418 08:25:31.003669 140490074724096 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.47732317447662354, loss=3.3900253772735596
I0418 08:26:04.944667 140488845801216 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.45691508054733276, loss=3.322805404663086
I0418 08:26:38.860545 140490074724096 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.4746781289577484, loss=3.3797085285186768
I0418 08:27:12.812309 140488845801216 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.46028709411621094, loss=3.380950689315796
I0418 08:27:46.815670 140490074724096 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.46926453709602356, loss=3.325144052505493
I0418 08:28:20.820805 140488845801216 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.4752523899078369, loss=3.411670207977295
I0418 08:28:54.756016 140490074724096 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.4704357981681824, loss=3.3206448554992676
I0418 08:29:28.682735 140488845801216 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.4724143147468567, loss=3.404702663421631
I0418 08:29:36.497161 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:29:44.101405 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:29:53.993854 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:29:55.952155 140667003242304 submission_runner.py:406] Time since start: 5900.03s, 	Step: 16525, 	{'train/accuracy': 0.6822185516357422, 'train/loss': 1.5356703996658325, 'validation/accuracy': 0.6243799924850464, 'validation/loss': 1.7955424785614014, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.4629392623901367, 'test/num_examples': 10000, 'score': 5656.978587150574, 'total_duration': 5900.028401851654, 'accumulated_submission_time': 5656.978587150574, 'accumulated_eval_time': 235.2277238368988, 'accumulated_logging_time': 7.627901315689087}
I0418 08:29:55.965350 140490074724096 logging_writer.py:48] [16525] accumulated_eval_time=235.227724, accumulated_logging_time=7.627901, accumulated_submission_time=5656.978587, global_step=16525, preemption_count=0, score=5656.978587, test/accuracy=0.496800, test/loss=2.462939, test/num_examples=10000, total_duration=5900.028402, train/accuracy=0.682219, train/loss=1.535670, validation/accuracy=0.624380, validation/loss=1.795542, validation/num_examples=50000
I0418 08:29:56.114627 140667003242304 checkpoints.py:356] Saving checkpoint at step: 16525
I0418 08:29:56.754188 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_16525
I0418 08:29:56.764282 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_16525.
I0418 08:30:22.290189 140488845801216 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.47867751121520996, loss=3.3543694019317627
I0418 08:30:56.064409 140489789536000 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.46811196208000183, loss=3.3606045246124268
I0418 08:31:29.717954 140488845801216 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.46818846464157104, loss=3.2882699966430664
I0418 08:32:03.668316 140489789536000 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.47037336230278015, loss=3.316498041152954
I0418 08:32:37.324097 140488845801216 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.46816566586494446, loss=3.369713306427002
I0418 08:33:11.248272 140489789536000 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.4723644256591797, loss=3.333571434020996
I0418 08:33:45.023796 140488845801216 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.45664337277412415, loss=3.303900718688965
I0418 08:34:18.789222 140489789536000 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.46105435490608215, loss=3.242701530456543
I0418 08:34:52.751477 140488845801216 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.4796851575374603, loss=3.3450675010681152
I0418 08:35:26.598212 140489789536000 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.46512970328330994, loss=3.268432140350342
I0418 08:36:00.446371 140488845801216 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.47455883026123047, loss=3.3820602893829346
I0418 08:36:34.313232 140489789536000 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.47129446268081665, loss=3.2977895736694336
I0418 08:37:08.287552 140488845801216 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.46860426664352417, loss=3.3324015140533447
I0418 08:37:42.083552 140489789536000 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.48152467608451843, loss=3.4135818481445312
I0418 08:38:15.940115 140488845801216 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4598996341228485, loss=3.330890655517578
I0418 08:38:26.913271 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:38:34.541939 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:38:44.843781 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:38:46.727738 140667003242304 submission_runner.py:406] Time since start: 6430.80s, 	Step: 18034, 	{'train/accuracy': 0.7016502022743225, 'train/loss': 1.4310201406478882, 'validation/accuracy': 0.6400200128555298, 'validation/loss': 1.706071376800537, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.381673574447632, 'test/num_examples': 10000, 'score': 6167.102100849152, 'total_duration': 6430.803850889206, 'accumulated_submission_time': 6167.102100849152, 'accumulated_eval_time': 255.0412459373474, 'accumulated_logging_time': 8.44906997680664}
I0418 08:38:46.742584 140489789536000 logging_writer.py:48] [18034] accumulated_eval_time=255.041246, accumulated_logging_time=8.449070, accumulated_submission_time=6167.102101, global_step=18034, preemption_count=0, score=6167.102101, test/accuracy=0.510800, test/loss=2.381674, test/num_examples=10000, total_duration=6430.803851, train/accuracy=0.701650, train/loss=1.431020, validation/accuracy=0.640020, validation/loss=1.706071, validation/num_examples=50000
I0418 08:38:46.965411 140667003242304 checkpoints.py:356] Saving checkpoint at step: 18034
I0418 08:38:48.179400 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_18034
I0418 08:38:48.190409 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_18034.
I0418 08:39:10.933438 140488845801216 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4843471050262451, loss=3.314084053039551
I0418 08:39:44.895442 140488837408512 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.4956801235675812, loss=3.3842058181762695
I0418 08:40:18.836224 140488845801216 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4705734848976135, loss=3.308915138244629
I0418 08:40:52.736407 140488837408512 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.46340879797935486, loss=3.2933080196380615
I0418 08:41:26.585294 140488845801216 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4556238353252411, loss=3.284381151199341
I0418 08:42:00.542680 140488837408512 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.4665335714817047, loss=3.1761746406555176
I0418 08:42:34.684556 140488845801216 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.45958420634269714, loss=3.281851291656494
I0418 08:43:08.652989 140488837408512 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.47167035937309265, loss=3.268810749053955
I0418 08:43:42.594988 140488845801216 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.4683679938316345, loss=3.299771547317505
I0418 08:44:16.637920 140488837408512 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.48446395993232727, loss=3.2881271839141846
I0418 08:44:50.484161 140488845801216 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.46840032935142517, loss=3.259129285812378
I0418 08:45:24.445448 140488837408512 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.4741702377796173, loss=3.377297878265381
I0418 08:45:58.439762 140488845801216 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.45420822501182556, loss=3.2006094455718994
I0418 08:46:32.437371 140488837408512 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.46753716468811035, loss=3.2820842266082764
I0418 08:47:06.349098 140488845801216 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.448152631521225, loss=3.307738780975342
I0418 08:47:18.312702 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:47:25.553486 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:47:35.886830 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:47:37.772244 140667003242304 submission_runner.py:406] Time since start: 6961.85s, 	Step: 19537, 	{'train/accuracy': 0.7469507455825806, 'train/loss': 1.2282958030700684, 'validation/accuracy': 0.6518399715423584, 'validation/loss': 1.6428956985473633, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.3137733936309814, 'test/num_examples': 10000, 'score': 6677.196565628052, 'total_duration': 6961.848370552063, 'accumulated_submission_time': 6677.196565628052, 'accumulated_eval_time': 274.49986267089844, 'accumulated_logging_time': 9.923251628875732}
I0418 08:47:37.780929 140488837408512 logging_writer.py:48] [19537] accumulated_eval_time=274.499863, accumulated_logging_time=9.923252, accumulated_submission_time=6677.196566, global_step=19537, preemption_count=0, score=6677.196566, test/accuracy=0.520300, test/loss=2.313773, test/num_examples=10000, total_duration=6961.848371, train/accuracy=0.746951, train/loss=1.228296, validation/accuracy=0.651840, validation/loss=1.642896, validation/num_examples=50000
I0418 08:47:37.935034 140667003242304 checkpoints.py:356] Saving checkpoint at step: 19537
I0418 08:47:38.575604 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_19537
I0418 08:47:38.584699 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_19537.
I0418 08:48:00.220350 140488845801216 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.4781644344329834, loss=3.233128547668457
I0418 08:48:34.095776 140488829015808 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4612348973751068, loss=3.286860704421997
I0418 08:49:08.070190 140488845801216 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4615180790424347, loss=3.3171348571777344
I0418 08:49:42.059039 140488829015808 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.4493686258792877, loss=3.244335651397705
I0418 08:50:16.051941 140488845801216 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.47199201583862305, loss=3.3419904708862305
I0418 08:50:49.835483 140488829015808 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.464819997549057, loss=3.2791965007781982
I0418 08:51:23.801676 140488845801216 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.46986106038093567, loss=3.2809715270996094
I0418 08:51:57.604516 140488829015808 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4718093276023865, loss=3.2629470825195312
I0418 08:52:31.340166 140488845801216 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.45440858602523804, loss=3.2320022583007812
I0418 08:53:05.343546 140488829015808 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.4585610032081604, loss=3.1992011070251465
I0418 08:53:39.171654 140488845801216 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.4675467908382416, loss=3.2529919147491455
I0418 08:54:13.033368 140488829015808 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.46667495369911194, loss=3.2314651012420654
I0418 08:54:46.804653 140488845801216 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.46034345030784607, loss=3.3176510334014893
I0418 08:55:20.661083 140488829015808 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.45163673162460327, loss=3.2718896865844727
I0418 08:55:54.485713 140488845801216 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.464905321598053, loss=3.206056594848633
I0418 08:56:08.752812 140667003242304 spec.py:298] Evaluating on the training split.
I0418 08:56:16.584960 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 08:56:26.881055 140667003242304 spec.py:326] Evaluating on the test split.
I0418 08:56:28.993808 140667003242304 submission_runner.py:406] Time since start: 7493.07s, 	Step: 21044, 	{'train/accuracy': 0.7416493892669678, 'train/loss': 1.27472722530365, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.651536226272583, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.30598521232605, 'test/num_examples': 10000, 'score': 7187.344483137131, 'total_duration': 7493.069854736328, 'accumulated_submission_time': 7187.344483137131, 'accumulated_eval_time': 294.7398500442505, 'accumulated_logging_time': 10.739696264266968}
I0418 08:56:29.008250 140488829015808 logging_writer.py:48] [21044] accumulated_eval_time=294.739850, accumulated_logging_time=10.739696, accumulated_submission_time=7187.344483, global_step=21044, preemption_count=0, score=7187.344483, test/accuracy=0.531200, test/loss=2.305985, test/num_examples=10000, total_duration=7493.069855, train/accuracy=0.741649, train/loss=1.274727, validation/accuracy=0.657600, validation/loss=1.651536, validation/num_examples=50000
I0418 08:56:29.179821 140667003242304 checkpoints.py:356] Saving checkpoint at step: 21044
I0418 08:56:29.841306 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_21044
I0418 08:56:29.851517 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_21044.
I0418 08:56:49.173203 140488845801216 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4518103003501892, loss=3.1613128185272217
I0418 08:57:23.214556 140488820623104 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4623178243637085, loss=3.1981923580169678
I0418 08:57:57.115994 140488845801216 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.44747570157051086, loss=3.168609380722046
I0418 08:58:31.052149 140488820623104 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.4677141010761261, loss=3.302496910095215
I0418 08:59:05.164181 140488845801216 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.47647854685783386, loss=3.2944960594177246
I0418 08:59:39.088830 140488820623104 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.46357694268226624, loss=3.2238123416900635
I0418 09:00:13.068694 140488845801216 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.49182426929473877, loss=3.2574782371520996
I0418 09:00:47.173068 140488820623104 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.4726574122905731, loss=3.221480131149292
I0418 09:01:21.216902 140488845801216 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.4593014121055603, loss=3.212048053741455
I0418 09:01:55.228971 140488820623104 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4666353762149811, loss=3.212034225463867
I0418 09:02:29.051301 140488845801216 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.46424198150634766, loss=3.233307123184204
I0418 09:03:03.222570 140488820623104 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.4511355757713318, loss=3.1926589012145996
I0418 09:03:37.092889 140488845801216 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.46929067373275757, loss=3.1397247314453125
I0418 09:04:11.113779 140488820623104 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.47109779715538025, loss=3.2276833057403564
I0418 09:04:45.112832 140488845801216 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.4616938531398773, loss=3.16247820854187
I0418 09:04:59.899473 140667003242304 spec.py:298] Evaluating on the training split.
I0418 09:05:07.153861 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 09:05:17.314670 140667003242304 spec.py:326] Evaluating on the test split.
I0418 09:05:19.275667 140667003242304 submission_runner.py:406] Time since start: 8023.35s, 	Step: 22545, 	{'train/accuracy': 0.7410714030265808, 'train/loss': 1.2560571432113647, 'validation/accuracy': 0.6614199876785278, 'validation/loss': 1.6094505786895752, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.281141996383667, 'test/num_examples': 10000, 'score': 7697.365719795227, 'total_duration': 8023.35170173645, 'accumulated_submission_time': 7697.365719795227, 'accumulated_eval_time': 314.1150257587433, 'accumulated_logging_time': 11.608015298843384}
I0418 09:05:19.285949 140488820623104 logging_writer.py:48] [22545] accumulated_eval_time=314.115026, accumulated_logging_time=11.608015, accumulated_submission_time=7697.365720, global_step=22545, preemption_count=0, score=7697.365720, test/accuracy=0.530800, test/loss=2.281142, test/num_examples=10000, total_duration=8023.351702, train/accuracy=0.741071, train/loss=1.256057, validation/accuracy=0.661420, validation/loss=1.609451, validation/num_examples=50000
I0418 09:05:19.461850 140667003242304 checkpoints.py:356] Saving checkpoint at step: 22545
I0418 09:05:20.098783 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_22545
I0418 09:05:20.108146 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_22545.
I0418 09:05:39.126727 140488845801216 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.43207406997680664, loss=3.170121669769287
I0418 09:06:12.931112 140488812230400 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.4605320990085602, loss=3.2376041412353516
I0418 09:06:46.946688 140488845801216 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.4447535574436188, loss=3.2119598388671875
I0418 09:07:20.770056 140488812230400 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.4651537835597992, loss=3.188685894012451
I0418 09:07:54.670241 140488845801216 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4611961543560028, loss=3.194193124771118
I0418 09:08:28.459468 140488812230400 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4688257575035095, loss=3.190303325653076
I0418 09:09:02.397114 140488845801216 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.46230149269104004, loss=3.2178051471710205
I0418 09:09:36.289446 140488812230400 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.46110105514526367, loss=3.2575230598449707
I0418 09:10:10.154828 140488845801216 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4503401517868042, loss=3.1845531463623047
I0418 09:10:43.968219 140488812230400 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4520638585090637, loss=3.196658134460449
I0418 09:11:17.995605 140488845801216 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.4621492922306061, loss=3.1861941814422607
I0418 09:11:51.857257 140488812230400 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.44521984457969666, loss=3.219208002090454
I0418 09:12:25.726675 140488845801216 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.46045058965682983, loss=3.2043638229370117
I0418 09:12:59.772900 140488812230400 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.46011513471603394, loss=3.178593397140503
I0418 09:13:33.849586 140488845801216 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.46768707036972046, loss=3.192894220352173
I0418 09:13:50.187016 140667003242304 spec.py:298] Evaluating on the training split.
I0418 09:13:58.256402 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 09:14:08.682094 140667003242304 spec.py:326] Evaluating on the test split.
I0418 09:14:10.534682 140667003242304 submission_runner.py:406] Time since start: 8554.61s, 	Step: 24050, 	{'train/accuracy': 0.7525510191917419, 'train/loss': 1.2005138397216797, 'validation/accuracy': 0.6675800085067749, 'validation/loss': 1.575946569442749, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.236783981323242, 'test/num_examples': 10000, 'score': 8207.419023752213, 'total_duration': 8554.610857009888, 'accumulated_submission_time': 8207.419023752213, 'accumulated_eval_time': 334.46180987358093, 'accumulated_logging_time': 12.449401617050171}
I0418 09:14:10.549937 140488812230400 logging_writer.py:48] [24050] accumulated_eval_time=334.461810, accumulated_logging_time=12.449402, accumulated_submission_time=8207.419024, global_step=24050, preemption_count=0, score=8207.419024, test/accuracy=0.534500, test/loss=2.236784, test/num_examples=10000, total_duration=8554.610857, train/accuracy=0.752551, train/loss=1.200514, validation/accuracy=0.667580, validation/loss=1.575947, validation/num_examples=50000
I0418 09:14:10.716973 140667003242304 checkpoints.py:356] Saving checkpoint at step: 24050
I0418 09:14:11.375146 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_24050
I0418 09:14:11.386173 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_24050.
I0418 09:14:28.775692 140488845801216 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.4593037962913513, loss=3.170395851135254
I0418 09:15:02.533932 140488803837696 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.45504286885261536, loss=3.1986336708068848
I0418 09:15:36.475761 140488845801216 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.4558846354484558, loss=3.196807861328125
I0418 09:16:10.534793 140488803837696 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4612172544002533, loss=3.1981263160705566
I0418 09:16:44.270972 140488845801216 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4553089141845703, loss=3.1921236515045166
I0418 09:17:18.308943 140488803837696 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4525894820690155, loss=3.1529860496520996
I0418 09:17:52.157396 140488845801216 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.48214399814605713, loss=3.212944746017456
I0418 09:18:26.117310 140488803837696 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.4455966651439667, loss=3.1467597484588623
I0418 09:19:00.196721 140488845801216 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.4526754915714264, loss=3.1809167861938477
I0418 09:19:34.065358 140488803837696 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.44382742047309875, loss=3.1595237255096436
I0418 09:20:07.998718 140488845801216 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.452597975730896, loss=3.1680681705474854
I0418 09:20:42.082234 140488803837696 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.45899200439453125, loss=3.1926193237304688
I0418 09:21:16.092793 140488845801216 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.45585983991622925, loss=3.194119930267334
I0418 09:21:50.109056 140488803837696 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.455786794424057, loss=3.2114546298980713
I0418 09:22:24.040628 140488845801216 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4602970480918884, loss=3.173718214035034
I0418 09:22:41.408937 140667003242304 spec.py:298] Evaluating on the training split.
I0418 09:22:48.612308 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 09:22:59.281473 140667003242304 spec.py:326] Evaluating on the test split.
I0418 09:23:01.206169 140667003242304 submission_runner.py:406] Time since start: 9085.28s, 	Step: 25553, 	{'train/accuracy': 0.7464524507522583, 'train/loss': 1.238277792930603, 'validation/accuracy': 0.6677599549293518, 'validation/loss': 1.5943233966827393, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.2749319076538086, 'test/num_examples': 10000, 'score': 8717.419290542603, 'total_duration': 9085.282412052155, 'accumulated_submission_time': 8717.419290542603, 'accumulated_eval_time': 354.25822377204895, 'accumulated_logging_time': 13.307304620742798}
I0418 09:23:01.217697 140488803837696 logging_writer.py:48] [25553] accumulated_eval_time=354.258224, accumulated_logging_time=13.307305, accumulated_submission_time=8717.419291, global_step=25553, preemption_count=0, score=8717.419291, test/accuracy=0.533700, test/loss=2.274932, test/num_examples=10000, total_duration=9085.282412, train/accuracy=0.746452, train/loss=1.238278, validation/accuracy=0.667760, validation/loss=1.594323, validation/num_examples=50000
I0418 09:23:01.439180 140667003242304 checkpoints.py:356] Saving checkpoint at step: 25553
I0418 09:23:02.273320 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_25553
I0418 09:23:02.286952 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_25553.
I0418 09:23:18.600007 140488845801216 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4622501730918884, loss=3.174325704574585
I0418 09:23:52.331150 140488795444992 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.45845121145248413, loss=3.2330381870269775
I0418 09:24:26.335918 140488845801216 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.45849570631980896, loss=3.1612298488616943
I0418 09:25:00.330042 140488795444992 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.4395224153995514, loss=3.0952646732330322
I0418 09:25:34.153112 140488845801216 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4592784643173218, loss=3.1941847801208496
I0418 09:26:08.037293 140488795444992 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.44260045886039734, loss=3.1088497638702393
I0418 09:26:41.990501 140488845801216 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.44851335883140564, loss=3.1089437007904053
I0418 09:27:15.858768 140488795444992 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4349818229675293, loss=3.10148024559021
I0418 09:27:49.793477 140488845801216 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4569289982318878, loss=3.14167857170105
I0418 09:28:23.740787 140488795444992 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.45159244537353516, loss=3.1219711303710938
I0418 09:28:57.698524 140488845801216 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.46150219440460205, loss=3.121246337890625
I0418 09:29:31.595696 140488795444992 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.45814913511276245, loss=3.1757309436798096
I0418 09:30:05.232118 140488845801216 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4516204297542572, loss=3.1468939781188965
I0418 09:30:39.164764 140488795444992 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4569091498851776, loss=3.1349403858184814
I0418 09:31:13.062960 140488845801216 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4630560278892517, loss=3.182025909423828
I0418 09:31:32.448833 140667003242304 spec.py:298] Evaluating on the training split.
I0418 09:31:39.751494 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 09:31:50.254353 140667003242304 spec.py:326] Evaluating on the test split.
I0418 09:31:52.349726 140667003242304 submission_runner.py:406] Time since start: 9616.43s, 	Step: 27059, 	{'train/accuracy': 0.7604830861091614, 'train/loss': 1.1843198537826538, 'validation/accuracy': 0.674340009689331, 'validation/loss': 1.555627465248108, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.20552396774292, 'test/num_examples': 10000, 'score': 9227.551256418228, 'total_duration': 9616.42572927475, 'accumulated_submission_time': 9227.551256418228, 'accumulated_eval_time': 374.1580653190613, 'accumulated_logging_time': 14.401596784591675}
I0418 09:31:52.359683 140488795444992 logging_writer.py:48] [27059] accumulated_eval_time=374.158065, accumulated_logging_time=14.401597, accumulated_submission_time=9227.551256, global_step=27059, preemption_count=0, score=9227.551256, test/accuracy=0.553200, test/loss=2.205524, test/num_examples=10000, total_duration=9616.425729, train/accuracy=0.760483, train/loss=1.184320, validation/accuracy=0.674340, validation/loss=1.555627, validation/num_examples=50000
I0418 09:31:52.531121 140667003242304 checkpoints.py:356] Saving checkpoint at step: 27059
I0418 09:31:53.172752 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_27059
I0418 09:31:53.184131 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_27059.
I0418 09:32:07.471568 140488845801216 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4568283259868622, loss=3.1371681690216064
I0418 09:32:41.466443 140488178923264 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.4438830018043518, loss=3.131232261657715
I0418 09:33:15.456866 140488845801216 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.4665565490722656, loss=3.138760566711426
I0418 09:33:49.377946 140488178923264 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.453825443983078, loss=3.1391379833221436
I0418 09:34:23.245061 140488845801216 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.46832066774368286, loss=3.2196097373962402
I0418 09:34:57.077036 140488178923264 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.44792142510414124, loss=3.0685696601867676
I0418 09:35:31.057544 140488845801216 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.45684635639190674, loss=3.106443405151367
I0418 09:36:05.189683 140488178923264 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.44511324167251587, loss=3.1373348236083984
I0418 09:36:39.127683 140488845801216 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.46911919116973877, loss=3.1675853729248047
I0418 09:37:12.625859 140667003242304 spec.py:298] Evaluating on the training split.
I0418 09:37:20.402332 140667003242304 spec.py:310] Evaluating on the validation split.
I0418 09:37:31.064931 140667003242304 spec.py:326] Evaluating on the test split.
I0418 09:37:33.056558 140667003242304 submission_runner.py:406] Time since start: 9957.13s, 	Step: 28000, 	{'train/accuracy': 0.7608617544174194, 'train/loss': 1.1916030645370483, 'validation/accuracy': 0.6715999841690063, 'validation/loss': 1.583441972732544, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.2472946643829346, 'test/num_examples': 10000, 'score': 9546.973285913467, 'total_duration': 9957.13237118721, 'accumulated_submission_time': 9546.973285913467, 'accumulated_eval_time': 394.5875222682953, 'accumulated_logging_time': 15.2455153465271}
I0418 09:37:33.069362 140488178923264 logging_writer.py:48] [28000] accumulated_eval_time=394.587522, accumulated_logging_time=15.245515, accumulated_submission_time=9546.973286, global_step=28000, preemption_count=0, score=9546.973286, test/accuracy=0.541800, test/loss=2.247295, test/num_examples=10000, total_duration=9957.132371, train/accuracy=0.760862, train/loss=1.191603, validation/accuracy=0.671600, validation/loss=1.583442, validation/num_examples=50000
I0418 09:37:33.231805 140667003242304 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 09:37:34.066612 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_28000
I0418 09:37:34.081677 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0418 09:37:34.105752 140488845801216 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=9546.973286
I0418 09:37:34.267048 140667003242304 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 09:37:35.330528 140667003242304 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_28000
I0418 09:37:35.343100 140667003242304 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0418 09:37:35.746309 140667003242304 submission_runner.py:567] Tuning trial 1/1
I0418 09:37:35.747395 140667003242304 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0418 09:37:35.751177 140667003242304 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008171236841008067, 'train/loss': 6.9130072593688965, 'validation/accuracy': 0.0007800000021234155, 'validation/loss': 6.9127702713012695, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.912158012390137, 'test/num_examples': 10000, 'score': 45.922388792037964, 'total_duration': 85.44957613945007, 'accumulated_submission_time': 45.922388792037964, 'accumulated_eval_time': 39.527005672454834, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1498, {'train/accuracy': 0.07720822840929031, 'train/loss': 5.365084648132324, 'validation/accuracy': 0.06955999881029129, 'validation/loss': 5.445878505706787, 'validation/num_examples': 50000, 'test/accuracy': 0.049400001764297485, 'test/loss': 5.671300411224365, 'test/num_examples': 10000, 'score': 555.9447631835938, 'total_duration': 612.6371603012085, 'accumulated_submission_time': 555.9447631835938, 'accumulated_eval_time': 56.12717008590698, 'accumulated_logging_time': 0.5469112396240234, 'global_step': 1498, 'preemption_count': 0}), (3001, {'train/accuracy': 0.20643335580825806, 'train/loss': 4.197149276733398, 'validation/accuracy': 0.19054000079631805, 'validation/loss': 4.312524795532227, 'validation/num_examples': 50000, 'test/accuracy': 0.13740000128746033, 'test/loss': 4.731720924377441, 'test/num_examples': 10000, 'score': 1066.0195486545563, 'total_duration': 1139.9412415027618, 'accumulated_submission_time': 1066.0195486545563, 'accumulated_eval_time': 72.77947306632996, 'accumulated_logging_time': 1.1061136722564697, 'global_step': 3001, 'preemption_count': 0}), (4500, {'train/accuracy': 0.3480149805545807, 'train/loss': 3.2009353637695312, 'validation/accuracy': 0.3256799876689911, 'validation/loss': 3.333479642868042, 'validation/num_examples': 50000, 'test/accuracy': 0.24400001764297485, 'test/loss': 3.904920816421509, 'test/num_examples': 10000, 'score': 1576.0553605556488, 'total_duration': 1667.097308397293, 'accumulated_submission_time': 1576.0553605556488, 'accumulated_eval_time': 89.33768820762634, 'accumulated_logging_time': 1.6503052711486816, 'global_step': 4500, 'preemption_count': 0}), (6006, {'train/accuracy': 0.43168047070503235, 'train/loss': 2.8336288928985596, 'validation/accuracy': 0.4035399854183197, 'validation/loss': 2.9681828022003174, 'validation/num_examples': 50000, 'test/accuracy': 0.30410000681877136, 'test/loss': 3.5501317977905273, 'test/num_examples': 10000, 'score': 2086.0764389038086, 'total_duration': 2194.3134660720825, 'accumulated_submission_time': 2086.0764389038086, 'accumulated_eval_time': 105.96333742141724, 'accumulated_logging_time': 2.201946973800659, 'global_step': 6006, 'preemption_count': 0}), (7507, {'train/accuracy': 0.5023716688156128, 'train/loss': 2.514901638031006, 'validation/accuracy': 0.4688199758529663, 'validation/loss': 2.6698458194732666, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 3.2545018196105957, 'test/num_examples': 10000, 'score': 2596.2232024669647, 'total_duration': 2721.4980552196503, 'accumulated_submission_time': 2596.2232024669647, 'accumulated_eval_time': 122.43322825431824, 'accumulated_logging_time': 2.7523934841156006, 'global_step': 7507, 'preemption_count': 0}), (9009, {'train/accuracy': 0.5494260191917419, 'train/loss': 2.130084753036499, 'validation/accuracy': 0.5134199857711792, 'validation/loss': 2.3026225566864014, 'validation/num_examples': 50000, 'test/accuracy': 0.39590001106262207, 'test/loss': 2.9769904613494873, 'test/num_examples': 10000, 'score': 3106.3970606327057, 'total_duration': 3251.902557373047, 'accumulated_submission_time': 3106.3970606327057, 'accumulated_eval_time': 141.84310150146484, 'accumulated_logging_time': 3.5556774139404297, 'global_step': 9009, 'preemption_count': 0}), (10505, {'train/accuracy': 0.618183970451355, 'train/loss': 1.773877501487732, 'validation/accuracy': 0.5518999695777893, 'validation/loss': 2.072688102722168, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.7533833980560303, 'test/num_examples': 10000, 'score': 3616.6103620529175, 'total_duration': 3779.773030281067, 'accumulated_submission_time': 3616.6103620529175, 'accumulated_eval_time': 158.90975785255432, 'accumulated_logging_time': 4.12972617149353, 'global_step': 10505, 'preemption_count': 0}), (12009, {'train/accuracy': 0.6291653513908386, 'train/loss': 1.7606170177459717, 'validation/accuracy': 0.5730599761009216, 'validation/loss': 2.01579213142395, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.6731443405151367, 'test/num_examples': 10000, 'score': 4126.64240193367, 'total_duration': 4309.372197151184, 'accumulated_submission_time': 4126.64240193367, 'accumulated_eval_time': 177.64868807792664, 'accumulated_logging_time': 4.94071626663208, 'global_step': 12009, 'preemption_count': 0}), (13514, {'train/accuracy': 0.6588408946990967, 'train/loss': 1.6286795139312744, 'validation/accuracy': 0.6006199717521667, 'validation/loss': 1.8863894939422607, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.5425360202789307, 'test/num_examples': 10000, 'score': 4636.703469753265, 'total_duration': 4839.653674840927, 'accumulated_submission_time': 4636.703469753265, 'accumulated_eval_time': 197.0430154800415, 'accumulated_logging_time': 5.7485620975494385, 'global_step': 13514, 'preemption_count': 0}), (15020, {'train/accuracy': 0.6713966727256775, 'train/loss': 1.5677212476730347, 'validation/accuracy': 0.6105599999427795, 'validation/loss': 1.8372849225997925, 'validation/num_examples': 50000, 'test/accuracy': 0.4887000322341919, 'test/loss': 2.4889912605285645, 'test/num_examples': 10000, 'score': 5146.75480389595, 'total_duration': 5369.5009508132935, 'accumulated_submission_time': 5146.75480389595, 'accumulated_eval_time': 215.77353811264038, 'accumulated_logging_time': 6.796183109283447, 'global_step': 15020, 'preemption_count': 0}), (16525, {'train/accuracy': 0.6822185516357422, 'train/loss': 1.5356703996658325, 'validation/accuracy': 0.6243799924850464, 'validation/loss': 1.7955424785614014, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.4629392623901367, 'test/num_examples': 10000, 'score': 5656.978587150574, 'total_duration': 5900.028401851654, 'accumulated_submission_time': 5656.978587150574, 'accumulated_eval_time': 235.2277238368988, 'accumulated_logging_time': 7.627901315689087, 'global_step': 16525, 'preemption_count': 0}), (18034, {'train/accuracy': 0.7016502022743225, 'train/loss': 1.4310201406478882, 'validation/accuracy': 0.6400200128555298, 'validation/loss': 1.706071376800537, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.381673574447632, 'test/num_examples': 10000, 'score': 6167.102100849152, 'total_duration': 6430.803850889206, 'accumulated_submission_time': 6167.102100849152, 'accumulated_eval_time': 255.0412459373474, 'accumulated_logging_time': 8.44906997680664, 'global_step': 18034, 'preemption_count': 0}), (19537, {'train/accuracy': 0.7469507455825806, 'train/loss': 1.2282958030700684, 'validation/accuracy': 0.6518399715423584, 'validation/loss': 1.6428956985473633, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.3137733936309814, 'test/num_examples': 10000, 'score': 6677.196565628052, 'total_duration': 6961.848370552063, 'accumulated_submission_time': 6677.196565628052, 'accumulated_eval_time': 274.49986267089844, 'accumulated_logging_time': 9.923251628875732, 'global_step': 19537, 'preemption_count': 0}), (21044, {'train/accuracy': 0.7416493892669678, 'train/loss': 1.27472722530365, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.651536226272583, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.30598521232605, 'test/num_examples': 10000, 'score': 7187.344483137131, 'total_duration': 7493.069854736328, 'accumulated_submission_time': 7187.344483137131, 'accumulated_eval_time': 294.7398500442505, 'accumulated_logging_time': 10.739696264266968, 'global_step': 21044, 'preemption_count': 0}), (22545, {'train/accuracy': 0.7410714030265808, 'train/loss': 1.2560571432113647, 'validation/accuracy': 0.6614199876785278, 'validation/loss': 1.6094505786895752, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.281141996383667, 'test/num_examples': 10000, 'score': 7697.365719795227, 'total_duration': 8023.35170173645, 'accumulated_submission_time': 7697.365719795227, 'accumulated_eval_time': 314.1150257587433, 'accumulated_logging_time': 11.608015298843384, 'global_step': 22545, 'preemption_count': 0}), (24050, {'train/accuracy': 0.7525510191917419, 'train/loss': 1.2005138397216797, 'validation/accuracy': 0.6675800085067749, 'validation/loss': 1.575946569442749, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.236783981323242, 'test/num_examples': 10000, 'score': 8207.419023752213, 'total_duration': 8554.610857009888, 'accumulated_submission_time': 8207.419023752213, 'accumulated_eval_time': 334.46180987358093, 'accumulated_logging_time': 12.449401617050171, 'global_step': 24050, 'preemption_count': 0}), (25553, {'train/accuracy': 0.7464524507522583, 'train/loss': 1.238277792930603, 'validation/accuracy': 0.6677599549293518, 'validation/loss': 1.5943233966827393, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.2749319076538086, 'test/num_examples': 10000, 'score': 8717.419290542603, 'total_duration': 9085.282412052155, 'accumulated_submission_time': 8717.419290542603, 'accumulated_eval_time': 354.25822377204895, 'accumulated_logging_time': 13.307304620742798, 'global_step': 25553, 'preemption_count': 0}), (27059, {'train/accuracy': 0.7604830861091614, 'train/loss': 1.1843198537826538, 'validation/accuracy': 0.674340009689331, 'validation/loss': 1.555627465248108, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.20552396774292, 'test/num_examples': 10000, 'score': 9227.551256418228, 'total_duration': 9616.42572927475, 'accumulated_submission_time': 9227.551256418228, 'accumulated_eval_time': 374.1580653190613, 'accumulated_logging_time': 14.401596784591675, 'global_step': 27059, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7608617544174194, 'train/loss': 1.1916030645370483, 'validation/accuracy': 0.6715999841690063, 'validation/loss': 1.583441972732544, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.2472946643829346, 'test/num_examples': 10000, 'score': 9546.973285913467, 'total_duration': 9957.13237118721, 'accumulated_submission_time': 9546.973285913467, 'accumulated_eval_time': 394.5875222682953, 'accumulated_logging_time': 15.2455153465271, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0418 09:37:35.751310 140667003242304 submission_runner.py:570] Timing: 9546.973285913467
I0418 09:37:35.751356 140667003242304 submission_runner.py:571] ====================
I0418 09:37:35.751481 140667003242304 submission_runner.py:631] Final imagenet_resnet score: 9546.973285913467
