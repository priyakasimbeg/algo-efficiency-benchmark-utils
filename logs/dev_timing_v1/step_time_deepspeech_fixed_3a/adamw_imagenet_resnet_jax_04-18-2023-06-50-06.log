I0418 06:50:28.612346 140221955352384 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax.
I0418 06:50:28.690450 140221955352384 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 06:50:29.556533 140221955352384 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0418 06:50:29.557184 140221955352384 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 06:50:29.561327 140221955352384 submission_runner.py:528] Using RNG seed 425428324
I0418 06:50:32.199255 140221955352384 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 06:50:32.199438 140221955352384 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1.
I0418 06:50:32.200985 140221955352384 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/hparams.json.
I0418 06:50:32.323626 140221955352384 submission_runner.py:232] Initializing dataset.
I0418 06:50:32.335904 140221955352384 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:50:32.344290 140221955352384 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:50:32.344422 140221955352384 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:50:32.613571 140221955352384 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:50:33.695512 140221955352384 submission_runner.py:239] Initializing model.
I0418 06:50:44.886634 140221955352384 submission_runner.py:249] Initializing optimizer.
I0418 06:50:45.968498 140221955352384 submission_runner.py:256] Initializing metrics bundle.
I0418 06:50:45.968677 140221955352384 submission_runner.py:273] Initializing checkpoint and logger.
I0418 06:50:45.969799 140221955352384 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0418 06:50:46.806324 140221955352384 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0418 06:50:46.807279 140221955352384 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0418 06:50:46.812650 140221955352384 submission_runner.py:309] Starting training loop.
I0418 06:51:34.417874 140044379612928 logging_writer.py:48] [0] global_step=0, grad_norm=0.6015704870223999, loss=6.923205852508545
I0418 06:51:34.434172 140221955352384 spec.py:298] Evaluating on the training split.
I0418 06:51:34.905427 140221955352384 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:34.911675 140221955352384 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:51:34.911796 140221955352384 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:51:34.972256 140221955352384 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:46.465555 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 06:51:47.321499 140221955352384 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:51:47.335905 140221955352384 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 06:51:47.336217 140221955352384 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 06:51:47.397815 140221955352384 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 06:52:04.813837 140221955352384 spec.py:326] Evaluating on the test split.
I0418 06:52:05.234205 140221955352384 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 06:52:05.238950 140221955352384 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0418 06:52:05.268945 140221955352384 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 06:52:14.212990 140221955352384 submission_runner.py:406] Time since start: 87.40s, 	Step: 1, 	{'train/accuracy': 0.0008569834171794355, 'train/loss': 6.910048484802246, 'validation/accuracy': 0.0008199999574571848, 'validation/loss': 6.910060882568359, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.910253524780273, 'test/num_examples': 10000, 'score': 47.62135195732117, 'total_duration': 87.40028119087219, 'accumulated_submission_time': 47.62135195732117, 'accumulated_eval_time': 39.778767585754395, 'accumulated_logging_time': 0}
I0418 06:52:14.229095 140015766058752 logging_writer.py:48] [1] accumulated_eval_time=39.778768, accumulated_logging_time=0, accumulated_submission_time=47.621352, global_step=1, preemption_count=0, score=47.621352, test/accuracy=0.000800, test/loss=6.910254, test/num_examples=10000, total_duration=87.400281, train/accuracy=0.000857, train/loss=6.910048, validation/accuracy=0.000820, validation/loss=6.910061, validation/num_examples=50000
I0418 06:52:14.423827 140221955352384 checkpoints.py:356] Saving checkpoint at step: 1
I0418 06:52:15.097564 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1
I0418 06:52:15.098785 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1.
I0418 06:52:48.590855 140015774451456 logging_writer.py:48] [100] global_step=100, grad_norm=0.6050463318824768, loss=6.873969078063965
I0418 06:53:22.247438 140016000956160 logging_writer.py:48] [200] global_step=200, grad_norm=0.6423744559288025, loss=6.756139755249023
I0418 06:53:55.829689 140015774451456 logging_writer.py:48] [300] global_step=300, grad_norm=0.7302259206771851, loss=6.574745178222656
I0418 06:54:29.618269 140016000956160 logging_writer.py:48] [400] global_step=400, grad_norm=0.9399892687797546, loss=6.393800735473633
I0418 06:55:03.193477 140015774451456 logging_writer.py:48] [500] global_step=500, grad_norm=1.5884696245193481, loss=6.252903938293457
I0418 06:55:36.715846 140016000956160 logging_writer.py:48] [600] global_step=600, grad_norm=1.9196133613586426, loss=6.123753547668457
I0418 06:56:10.425191 140015774451456 logging_writer.py:48] [700] global_step=700, grad_norm=3.3459930419921875, loss=5.989806652069092
I0418 06:56:44.195654 140016000956160 logging_writer.py:48] [800] global_step=800, grad_norm=1.926939606666565, loss=5.8576507568359375
I0418 06:57:17.825595 140015774451456 logging_writer.py:48] [900] global_step=900, grad_norm=1.8537147045135498, loss=5.785772800445557
I0418 06:57:51.188158 140016000956160 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.3518762588500977, loss=5.626572608947754
I0418 06:58:24.837919 140015774451456 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.660583257675171, loss=5.614414215087891
I0418 06:58:58.487198 140016000956160 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.8950932025909424, loss=5.588691711425781
I0418 06:59:32.132132 140015774451456 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.6447231769561768, loss=5.425935745239258
I0418 07:00:05.912897 140016000956160 logging_writer.py:48] [1400] global_step=1400, grad_norm=5.545254230499268, loss=5.35573673248291
I0418 07:00:39.484288 140015774451456 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.5713768005371094, loss=5.24910831451416
I0418 07:00:45.291462 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:00:52.184680 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:00:59.910021 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:01:01.962789 140221955352384 submission_runner.py:406] Time since start: 615.15s, 	Step: 1519, 	{'train/accuracy': 0.11890146136283875, 'train/loss': 4.769593238830566, 'validation/accuracy': 0.10387999564409256, 'validation/loss': 4.878118515014648, 'validation/num_examples': 50000, 'test/accuracy': 0.0796000063419342, 'test/loss': 5.246491432189941, 'test/num_examples': 10000, 'score': 557.7915813922882, 'total_duration': 615.1500458717346, 'accumulated_submission_time': 557.7915813922882, 'accumulated_eval_time': 56.45005202293396, 'accumulated_logging_time': 0.8905539512634277}
I0418 07:01:01.973807 140016093210368 logging_writer.py:48] [1519] accumulated_eval_time=56.450052, accumulated_logging_time=0.890554, accumulated_submission_time=557.791581, global_step=1519, preemption_count=0, score=557.791581, test/accuracy=0.079600, test/loss=5.246491, test/num_examples=10000, total_duration=615.150046, train/accuracy=0.118901, train/loss=4.769593, validation/accuracy=0.103880, validation/loss=4.878119, validation/num_examples=50000
I0418 07:01:02.224784 140221955352384 checkpoints.py:356] Saving checkpoint at step: 1519
I0418 07:01:03.087273 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1519
I0418 07:01:03.088239 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1519.
I0418 07:01:30.711018 140016101603072 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.367120265960693, loss=5.319859504699707
I0418 07:02:04.324148 140041045124864 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.871533155441284, loss=5.137414455413818
I0418 07:02:37.842656 140016101603072 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.795614242553711, loss=5.073642730712891
I0418 07:03:11.520941 140041045124864 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.3915951251983643, loss=4.951091289520264
I0418 07:03:45.180170 140016101603072 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.999227285385132, loss=4.867768287658691
I0418 07:04:18.814031 140041045124864 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.7568047046661377, loss=4.822450637817383
I0418 07:04:52.412630 140016101603072 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.062061309814453, loss=4.738677024841309
I0418 07:05:26.030035 140041045124864 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.1459274291992188, loss=4.745699882507324
I0418 07:05:59.510928 140016101603072 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.675243377685547, loss=4.746926307678223
I0418 07:06:33.173401 140041045124864 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.307579517364502, loss=4.612886428833008
I0418 07:07:06.714820 140016101603072 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.0240108966827393, loss=4.599570274353027
I0418 07:07:40.368996 140041045124864 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.717510223388672, loss=4.567507266998291
I0418 07:08:13.827298 140016101603072 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.39018177986145, loss=4.412996768951416
I0418 07:08:47.371512 140041045124864 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.9170401096343994, loss=4.4597673416137695
I0418 07:09:21.000888 140016101603072 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.409463882446289, loss=4.322444438934326
I0418 07:09:33.146678 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:09:39.962762 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:09:47.764362 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:09:49.995226 140221955352384 submission_runner.py:406] Time since start: 1143.18s, 	Step: 3038, 	{'train/accuracy': 0.2700693607330322, 'train/loss': 3.573406934738159, 'validation/accuracy': 0.24521999061107635, 'validation/loss': 3.722559690475464, 'validation/num_examples': 50000, 'test/accuracy': 0.18450000882148743, 'test/loss': 4.257433891296387, 'test/num_examples': 10000, 'score': 1067.8279321193695, 'total_duration': 1143.182498216629, 'accumulated_submission_time': 1067.8279321193695, 'accumulated_eval_time': 73.2985589504242, 'accumulated_logging_time': 2.0203680992126465}
I0418 07:09:50.004285 140041045124864 logging_writer.py:48] [3038] accumulated_eval_time=73.298559, accumulated_logging_time=2.020368, accumulated_submission_time=1067.827932, global_step=3038, preemption_count=0, score=1067.827932, test/accuracy=0.184500, test/loss=4.257434, test/num_examples=10000, total_duration=1143.182498, train/accuracy=0.270069, train/loss=3.573407, validation/accuracy=0.245220, validation/loss=3.722560, validation/num_examples=50000
I0418 07:09:50.202065 140221955352384 checkpoints.py:356] Saving checkpoint at step: 3038
I0418 07:09:50.836648 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_3038
I0418 07:09:50.837611 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_3038.
I0418 07:10:12.107355 140016101603072 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.1724720001220703, loss=4.3410868644714355
I0418 07:10:45.740258 140040394987264 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.7739291191101074, loss=4.274447917938232
I0418 07:11:19.426844 140016101603072 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.000243663787842, loss=4.298954963684082
I0418 07:11:53.045525 140040394987264 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.066084384918213, loss=4.202104091644287
I0418 07:12:26.610033 140016101603072 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.30983304977417, loss=4.198084354400635
I0418 07:13:00.401081 140040394987264 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.4403045177459717, loss=4.104294300079346
I0418 07:13:34.091793 140016101603072 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.672739267349243, loss=4.037212371826172
I0418 07:14:07.754140 140040394987264 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.5249195098876953, loss=3.9943461418151855
I0418 07:14:41.275847 140016101603072 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.047865390777588, loss=4.1069440841674805
I0418 07:15:14.941823 140040394987264 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.5716047286987305, loss=3.9877896308898926
I0418 07:15:48.485472 140016101603072 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.1361653804779053, loss=3.8809590339660645
I0418 07:16:22.202521 140040394987264 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.095226764678955, loss=3.956450939178467
I0418 07:16:55.721012 140016101603072 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.37709641456604, loss=3.919647693634033
I0418 07:17:29.317820 140040394987264 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.6894407272338867, loss=3.838172197341919
I0418 07:18:02.868587 140016101603072 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.83758282661438, loss=3.779491424560547
I0418 07:18:21.045018 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:18:27.723863 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:18:35.604119 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:18:37.770386 140221955352384 submission_runner.py:406] Time since start: 1670.96s, 	Step: 4556, 	{'train/accuracy': 0.38932955265045166, 'train/loss': 2.850102663040161, 'validation/accuracy': 0.3567200005054474, 'validation/loss': 3.0206139087677, 'validation/num_examples': 50000, 'test/accuracy': 0.2766000032424927, 'test/loss': 3.6376664638519287, 'test/num_examples': 10000, 'score': 1578.0155239105225, 'total_duration': 1670.957671403885, 'accumulated_submission_time': 1578.0155239105225, 'accumulated_eval_time': 90.02390789985657, 'accumulated_logging_time': 2.8653059005737305}
I0418 07:18:37.778128 140040394987264 logging_writer.py:48] [4556] accumulated_eval_time=90.023908, accumulated_logging_time=2.865306, accumulated_submission_time=1578.015524, global_step=4556, preemption_count=0, score=1578.015524, test/accuracy=0.276600, test/loss=3.637666, test/num_examples=10000, total_duration=1670.957671, train/accuracy=0.389330, train/loss=2.850103, validation/accuracy=0.356720, validation/loss=3.020614, validation/num_examples=50000
I0418 07:18:38.033991 140221955352384 checkpoints.py:356] Saving checkpoint at step: 4556
I0418 07:18:38.902459 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_4556
I0418 07:18:38.903486 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_4556.
I0418 07:18:54.052521 140016101603072 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.246347188949585, loss=3.742882251739502
I0418 07:19:27.462092 140043934996224 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.9925408363342285, loss=3.8669631481170654
I0418 07:20:00.886072 140016101603072 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.252943515777588, loss=3.7619471549987793
I0418 07:20:34.448184 140043934996224 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.3428430557250977, loss=3.813168525695801
I0418 07:21:07.851613 140016101603072 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.75022029876709, loss=3.71488881111145
I0418 07:21:41.394327 140043934996224 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.263319969177246, loss=3.6442360877990723
I0418 07:22:14.821898 140016101603072 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.436711311340332, loss=3.605592727661133
I0418 07:22:48.256973 140043934996224 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6207926273345947, loss=3.5818300247192383
I0418 07:23:21.691829 140016101603072 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.449348211288452, loss=3.591369867324829
I0418 07:23:55.088393 140043934996224 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.4988716840744019, loss=3.5089149475097656
I0418 07:24:28.521684 140016101603072 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.551826000213623, loss=3.5245046615600586
I0418 07:25:01.907769 140043934996224 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.8728868961334229, loss=3.466759204864502
I0418 07:25:35.292478 140016101603072 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.07668137550354, loss=3.4710326194763184
I0418 07:26:08.784050 140043934996224 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.012359380722046, loss=3.447319507598877
I0418 07:26:42.116680 140016101603072 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.4505833387374878, loss=3.3924903869628906
I0418 07:27:08.966639 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:27:15.798362 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:27:23.688338 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:27:25.643766 140221955352384 submission_runner.py:406] Time since start: 2198.83s, 	Step: 6082, 	{'train/accuracy': 0.4662587642669678, 'train/loss': 2.4076318740844727, 'validation/accuracy': 0.43685999512672424, 'validation/loss': 2.563072919845581, 'validation/num_examples': 50000, 'test/accuracy': 0.3305000066757202, 'test/loss': 3.22933292388916, 'test/num_examples': 10000, 'score': 2088.059413909912, 'total_duration': 2198.831042289734, 'accumulated_submission_time': 2088.059413909912, 'accumulated_eval_time': 106.70098948478699, 'accumulated_logging_time': 4.000883340835571}
I0418 07:27:25.651468 140043934996224 logging_writer.py:48] [6082] accumulated_eval_time=106.700989, accumulated_logging_time=4.000883, accumulated_submission_time=2088.059414, global_step=6082, preemption_count=0, score=2088.059414, test/accuracy=0.330500, test/loss=3.229333, test/num_examples=10000, total_duration=2198.831042, train/accuracy=0.466259, train/loss=2.407632, validation/accuracy=0.436860, validation/loss=2.563073, validation/num_examples=50000
I0418 07:27:25.906970 140221955352384 checkpoints.py:356] Saving checkpoint at step: 6082
I0418 07:27:26.889682 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_6082
I0418 07:27:26.905077 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_6082.
I0418 07:27:33.312516 140016101603072 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.4861558675765991, loss=3.4480011463165283
I0418 07:28:06.858535 140043691738880 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.813039779663086, loss=3.469883918762207
I0418 07:28:40.340303 140016101603072 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.7010512351989746, loss=3.4249751567840576
I0418 07:29:13.773684 140043691738880 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.6574022769927979, loss=3.426332712173462
I0418 07:29:47.360011 140016101603072 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.2897852659225464, loss=3.3696529865264893
I0418 07:30:20.856616 140043691738880 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.2651485204696655, loss=3.337827682495117
I0418 07:30:54.325143 140016101603072 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.1562049388885498, loss=3.3656809329986572
I0418 07:31:27.668566 140043691738880 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.184408187866211, loss=3.4282565116882324
I0418 07:32:01.100434 140016101603072 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.149764060974121, loss=3.2602176666259766
I0418 07:32:34.458732 140043691738880 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.3549107313156128, loss=3.195371627807617
I0418 07:33:08.040185 140016101603072 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.3293778896331787, loss=3.302670478820801
I0418 07:33:41.731056 140043691738880 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.330981731414795, loss=3.2579362392425537
I0418 07:34:15.222398 140016101603072 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.325080394744873, loss=3.25579833984375
I0418 07:34:48.750727 140043691738880 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.2014142274856567, loss=3.2754602432250977
I0418 07:35:22.208594 140016101603072 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.3444453477859497, loss=3.27642822265625
I0418 07:35:55.469158 140043691738880 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.7283751964569092, loss=3.2732908725738525
I0418 07:35:57.225227 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:36:04.139325 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:36:12.001399 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:36:13.915514 140221955352384 submission_runner.py:406] Time since start: 2727.10s, 	Step: 7607, 	{'train/accuracy': 0.5345982313156128, 'train/loss': 2.101940870285034, 'validation/accuracy': 0.498119980096817, 'validation/loss': 2.2839956283569336, 'validation/num_examples': 50000, 'test/accuracy': 0.38100001215934753, 'test/loss': 2.9551000595092773, 'test/num_examples': 10000, 'score': 2598.356607198715, 'total_duration': 2727.1027987003326, 'accumulated_submission_time': 2598.356607198715, 'accumulated_eval_time': 123.39125752449036, 'accumulated_logging_time': 5.267918825149536}
I0418 07:36:13.923975 140016101603072 logging_writer.py:48] [7607] accumulated_eval_time=123.391258, accumulated_logging_time=5.267919, accumulated_submission_time=2598.356607, global_step=7607, preemption_count=0, score=2598.356607, test/accuracy=0.381000, test/loss=2.955100, test/num_examples=10000, total_duration=2727.102799, train/accuracy=0.534598, train/loss=2.101941, validation/accuracy=0.498120, validation/loss=2.283996, validation/num_examples=50000
I0418 07:36:14.186706 140221955352384 checkpoints.py:356] Saving checkpoint at step: 7607
I0418 07:36:15.168274 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_7607
I0418 07:36:15.182740 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_7607.
I0418 07:36:46.553267 140043691738880 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.3354922533035278, loss=3.2346816062927246
I0418 07:37:19.944462 140043683346176 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.1100245714187622, loss=3.1925106048583984
I0418 07:37:53.260118 140043691738880 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.104683756828308, loss=3.131463050842285
I0418 07:38:26.776105 140043683346176 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.3917791843414307, loss=3.057800054550171
I0418 07:39:00.204491 140043691738880 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.076064109802246, loss=3.1755521297454834
I0418 07:39:33.629567 140043683346176 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.6434959173202515, loss=3.1427078247070312
I0418 07:40:07.073600 140043691738880 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.080430269241333, loss=3.1603801250457764
I0418 07:40:40.465248 140043683346176 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.6296695470809937, loss=3.2540664672851562
I0418 07:41:13.922945 140043691738880 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9941387176513672, loss=3.21905255317688
I0418 07:41:47.467339 140043683346176 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8891457915306091, loss=3.1096832752227783
I0418 07:42:20.931299 140043691738880 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0974438190460205, loss=3.127964973449707
I0418 07:42:54.232218 140043683346176 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.199161171913147, loss=3.209975481033325
I0418 07:43:27.628660 140043691738880 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8870992660522461, loss=3.1024041175842285
I0418 07:44:01.002615 140043683346176 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.950275719165802, loss=3.1776223182678223
I0418 07:44:34.576715 140043691738880 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1016534566879272, loss=3.1086459159851074
I0418 07:44:45.355620 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:44:52.311758 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:45:00.903121 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:45:03.690490 140221955352384 submission_runner.py:406] Time since start: 3256.88s, 	Step: 9134, 	{'train/accuracy': 0.6017418503761292, 'train/loss': 1.7502098083496094, 'validation/accuracy': 0.5194999575614929, 'validation/loss': 2.1560373306274414, 'validation/num_examples': 50000, 'test/accuracy': 0.4009000062942505, 'test/loss': 2.8617897033691406, 'test/num_examples': 10000, 'score': 3108.5083680152893, 'total_duration': 3256.877753973007, 'accumulated_submission_time': 3108.5083680152893, 'accumulated_eval_time': 141.72609448432922, 'accumulated_logging_time': 6.539368629455566}
I0418 07:45:03.702409 140043683346176 logging_writer.py:48] [9134] accumulated_eval_time=141.726094, accumulated_logging_time=6.539369, accumulated_submission_time=3108.508368, global_step=9134, preemption_count=0, score=3108.508368, test/accuracy=0.400900, test/loss=2.861790, test/num_examples=10000, total_duration=3256.877754, train/accuracy=0.601742, train/loss=1.750210, validation/accuracy=0.519500, validation/loss=2.156037, validation/num_examples=50000
I0418 07:45:04.026592 140221955352384 checkpoints.py:356] Saving checkpoint at step: 9134
I0418 07:45:04.969142 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_9134
I0418 07:45:04.988891 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_9134.
I0418 07:45:27.263269 140043691738880 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0719575881958008, loss=3.0651934146881104
I0418 07:46:00.709020 140043666560768 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9727145433425903, loss=3.082963228225708
I0418 07:46:34.304304 140043691738880 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7576743364334106, loss=2.9883346557617188
I0418 07:47:07.652642 140043666560768 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9716888070106506, loss=3.0511441230773926
I0418 07:47:41.007344 140043691738880 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.0276345014572144, loss=3.058162212371826
I0418 07:48:14.394968 140043666560768 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8884464502334595, loss=3.0185046195983887
I0418 07:48:47.713918 140043691738880 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8271660804748535, loss=2.880427360534668
I0418 07:49:21.199511 140043666560768 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9805856943130493, loss=2.9666459560394287
I0418 07:49:54.569851 140043691738880 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.772965133190155, loss=2.9567770957946777
I0418 07:50:28.054357 140043666560768 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.790189802646637, loss=2.944563865661621
I0418 07:51:01.354350 140043691738880 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0520142316818237, loss=2.9743926525115967
I0418 07:51:34.935860 140043666560768 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7356851100921631, loss=2.960576057434082
I0418 07:52:08.401472 140043691738880 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6781666874885559, loss=2.980264186859131
I0418 07:52:41.808166 140043666560768 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8095195889472961, loss=2.9255764484405518
I0418 07:53:15.281923 140043691738880 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8333591222763062, loss=2.9009201526641846
I0418 07:53:35.116592 140221955352384 spec.py:298] Evaluating on the training split.
I0418 07:53:42.008407 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 07:53:51.338956 140221955352384 spec.py:326] Evaluating on the test split.
I0418 07:53:53.234101 140221955352384 submission_runner.py:406] Time since start: 3786.42s, 	Step: 10661, 	{'train/accuracy': 0.6206752061843872, 'train/loss': 1.6609508991241455, 'validation/accuracy': 0.5554999709129333, 'validation/loss': 1.9836561679840088, 'validation/num_examples': 50000, 'test/accuracy': 0.43570002913475037, 'test/loss': 2.661497116088867, 'test/num_examples': 10000, 'score': 3618.612957239151, 'total_duration': 3786.4213967323303, 'accumulated_submission_time': 3618.612957239151, 'accumulated_eval_time': 159.84357523918152, 'accumulated_logging_time': 7.843618869781494}
I0418 07:53:53.243459 140043666560768 logging_writer.py:48] [10661] accumulated_eval_time=159.843575, accumulated_logging_time=7.843619, accumulated_submission_time=3618.612957, global_step=10661, preemption_count=0, score=3618.612957, test/accuracy=0.435700, test/loss=2.661497, test/num_examples=10000, total_duration=3786.421397, train/accuracy=0.620675, train/loss=1.660951, validation/accuracy=0.555500, validation/loss=1.983656, validation/num_examples=50000
I0418 07:53:53.494621 140221955352384 checkpoints.py:356] Saving checkpoint at step: 10661
I0418 07:53:54.460484 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_10661
I0418 07:53:54.474570 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_10661.
I0418 07:54:07.892984 140043691738880 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.0647352933883667, loss=2.950350284576416
I0418 07:54:41.337772 140043381372672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7404942512512207, loss=2.912330150604248
I0418 07:55:14.860005 140043691738880 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8471639752388, loss=2.9109272956848145
I0418 07:55:48.245855 140043381372672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9137044548988342, loss=2.8473892211914062
I0418 07:56:21.668149 140043691738880 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6697763204574585, loss=2.959059953689575
I0418 07:56:55.011594 140043381372672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6476555466651917, loss=2.825139045715332
I0418 07:57:28.478519 140043691738880 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.8137674927711487, loss=2.969383716583252
I0418 07:58:01.946965 140043381372672 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.722806990146637, loss=2.902324676513672
I0418 07:58:35.344033 140043691738880 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6043461561203003, loss=2.835251569747925
I0418 07:59:08.623708 140043381372672 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6549221277236938, loss=2.8215973377227783
I0418 07:59:41.975959 140043691738880 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6850869059562683, loss=2.9348559379577637
I0418 08:00:15.639653 140043381372672 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6643719673156738, loss=2.979942560195923
I0418 08:00:49.002641 140043691738880 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7606699466705322, loss=2.8682172298431396
I0418 08:01:22.421829 140043381372672 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6003360152244568, loss=2.7658560276031494
I0418 08:01:56.072886 140043691738880 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7575698494911194, loss=2.766209363937378
I0418 08:02:24.782623 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:02:32.347856 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:02:42.275147 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:02:44.419243 140221955352384 submission_runner.py:406] Time since start: 4317.61s, 	Step: 12188, 	{'train/accuracy': 0.634207546710968, 'train/loss': 1.6309587955474854, 'validation/accuracy': 0.5759400129318237, 'validation/loss': 1.909151315689087, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.6158440113067627, 'test/num_examples': 10000, 'score': 4128.895737171173, 'total_duration': 4317.605779886246, 'accumulated_submission_time': 4128.895737171173, 'accumulated_eval_time': 179.47941374778748, 'accumulated_logging_time': 9.092377662658691}
I0418 08:02:44.434124 140043381372672 logging_writer.py:48] [12188] accumulated_eval_time=179.479414, accumulated_logging_time=9.092378, accumulated_submission_time=4128.895737, global_step=12188, preemption_count=0, score=4128.895737, test/accuracy=0.451300, test/loss=2.615844, test/num_examples=10000, total_duration=4317.605780, train/accuracy=0.634208, train/loss=1.630959, validation/accuracy=0.575940, validation/loss=1.909151, validation/num_examples=50000
I0418 08:02:44.687726 140221955352384 checkpoints.py:356] Saving checkpoint at step: 12188
I0418 08:02:45.680239 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_12188
I0418 08:02:45.696174 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_12188.
I0418 08:02:50.062158 140043691738880 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6191943287849426, loss=2.7882742881774902
I0418 08:03:23.414981 140043372979968 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7006546258926392, loss=2.8027892112731934
I0418 08:03:56.877106 140043691738880 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5871132612228394, loss=2.9045021533966064
I0418 08:04:30.306119 140043372979968 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5734662413597107, loss=2.802666664123535
I0418 08:05:03.563711 140043691738880 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5520715713500977, loss=2.7790679931640625
I0418 08:05:36.943986 140043372979968 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.593161940574646, loss=2.8357555866241455
I0418 08:06:10.466967 140043691738880 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.62187659740448, loss=2.8580594062805176
I0418 08:06:43.741245 140043372979968 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5674027800559998, loss=2.7704567909240723
I0418 08:07:17.203954 140043691738880 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.610333263874054, loss=2.8092775344848633
I0418 08:07:50.480075 140043372979968 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5735601782798767, loss=2.715712070465088
I0418 08:08:23.805367 140043691738880 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5219950675964355, loss=2.7701523303985596
I0418 08:08:57.227206 140043372979968 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5454031229019165, loss=2.8010947704315186
I0418 08:09:30.637494 140043691738880 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5696195363998413, loss=2.7001633644104004
I0418 08:10:04.096873 140043372979968 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5414102673530579, loss=2.801116943359375
I0418 08:10:37.474124 140043691738880 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5309481620788574, loss=2.7517433166503906
I0418 08:11:10.879181 140043372979968 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5792391300201416, loss=2.752681016921997
I0418 08:11:15.972447 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:11:22.916585 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:11:32.898772 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:11:34.828568 140221955352384 submission_runner.py:406] Time since start: 4848.01s, 	Step: 13717, 	{'train/accuracy': 0.6602957248687744, 'train/loss': 1.4725115299224854, 'validation/accuracy': 0.6019999980926514, 'validation/loss': 1.7582768201828003, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.465412139892578, 'test/num_examples': 10000, 'score': 4639.150049686432, 'total_duration': 4848.0149047374725, 'accumulated_submission_time': 4639.150049686432, 'accumulated_eval_time': 198.33454632759094, 'accumulated_logging_time': 10.374719142913818}
I0418 08:11:34.837649 140043691738880 logging_writer.py:48] [13717] accumulated_eval_time=198.334546, accumulated_logging_time=10.374719, accumulated_submission_time=4639.150050, global_step=13717, preemption_count=0, score=4639.150050, test/accuracy=0.475200, test/loss=2.465412, test/num_examples=10000, total_duration=4848.014905, train/accuracy=0.660296, train/loss=1.472512, validation/accuracy=0.602000, validation/loss=1.758277, validation/num_examples=50000
I0418 08:11:35.104765 140221955352384 checkpoints.py:356] Saving checkpoint at step: 13717
I0418 08:11:36.041777 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_13717
I0418 08:11:36.056681 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_13717.
I0418 08:12:04.012146 140043372979968 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4724709391593933, loss=2.594221353530884
I0418 08:12:37.402841 140043364587264 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6461971998214722, loss=2.8434507846832275
I0418 08:13:10.805017 140043372979968 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5552939176559448, loss=2.624061107635498
I0418 08:13:44.244387 140043364587264 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.556004524230957, loss=2.628451347351074
I0418 08:14:17.670490 140043372979968 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.4621642231941223, loss=2.706514596939087
I0418 08:14:51.126433 140043364587264 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6418308615684509, loss=2.7401039600372314
I0418 08:15:24.511910 140043372979968 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5390762090682983, loss=2.6653330326080322
I0418 08:15:57.817171 140043364587264 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5865655541419983, loss=2.699160575866699
I0418 08:16:31.297152 140043372979968 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5541179776191711, loss=2.5874342918395996
I0418 08:17:04.649784 140043364587264 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.587032675743103, loss=2.6661148071289062
I0418 08:17:38.204696 140043372979968 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6542837619781494, loss=2.657785654067993
I0418 08:18:11.616094 140043364587264 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.547279417514801, loss=2.6325225830078125
I0418 08:18:45.040070 140043372979968 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4771484434604645, loss=2.6306443214416504
I0418 08:19:18.531787 140043364587264 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5400798916816711, loss=2.6230108737945557
I0418 08:19:52.022210 140043372979968 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.4809350073337555, loss=2.590508222579956
I0418 08:20:06.090176 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:20:13.757911 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:20:23.665849 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:20:25.482007 140221955352384 submission_runner.py:406] Time since start: 5378.67s, 	Step: 15244, 	{'train/accuracy': 0.6758211255073547, 'train/loss': 1.4175982475280762, 'validation/accuracy': 0.6136999726295471, 'validation/loss': 1.7086646556854248, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.4089126586914062, 'test/num_examples': 10000, 'score': 5149.158976078033, 'total_duration': 5378.668406248093, 'accumulated_submission_time': 5149.158976078033, 'accumulated_eval_time': 217.72545409202576, 'accumulated_logging_time': 11.610625743865967}
I0418 08:20:25.495073 140043364587264 logging_writer.py:48] [15244] accumulated_eval_time=217.725454, accumulated_logging_time=11.610626, accumulated_submission_time=5149.158976, global_step=15244, preemption_count=0, score=5149.158976, test/accuracy=0.487800, test/loss=2.408913, test/num_examples=10000, total_duration=5378.668406, train/accuracy=0.675821, train/loss=1.417598, validation/accuracy=0.613700, validation/loss=1.708665, validation/num_examples=50000
I0418 08:20:25.783290 140221955352384 checkpoints.py:356] Saving checkpoint at step: 15244
I0418 08:20:26.804514 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_15244
I0418 08:20:26.820461 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_15244.
I0418 08:20:45.906263 140043372979968 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.49756520986557007, loss=2.690235137939453
I0418 08:21:19.374969 140043356194560 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5351589322090149, loss=2.581101417541504
I0418 08:21:52.628533 140043372979968 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.5573437809944153, loss=2.714655637741089
I0418 08:22:26.153426 140043356194560 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.502070426940918, loss=2.6026716232299805
I0418 08:22:59.518316 140043372979968 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5484136939048767, loss=2.6081438064575195
I0418 08:23:33.088628 140043356194560 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.45017072558403015, loss=2.654994010925293
I0418 08:24:06.383317 140043372979968 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5166832208633423, loss=2.602977752685547
I0418 08:24:39.891062 140043356194560 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.47949323058128357, loss=2.5189712047576904
I0418 08:25:13.344007 140043372979968 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.49770355224609375, loss=2.676173686981201
I0418 08:25:46.781865 140043356194560 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5731521844863892, loss=2.691066265106201
I0418 08:26:20.266086 140043372979968 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5460082292556763, loss=2.619673490524292
I0418 08:26:53.705779 140043356194560 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.4792625904083252, loss=2.6270358562469482
I0418 08:27:27.111815 140043372979968 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5845438241958618, loss=2.5500094890594482
I0418 08:28:00.563457 140043356194560 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.486654669046402, loss=2.6489052772521973
I0418 08:28:34.063041 140043372979968 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.49720752239227295, loss=2.6849334239959717
I0418 08:28:56.901375 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:29:04.259037 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:29:14.189467 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:29:16.163798 140221955352384 submission_runner.py:406] Time since start: 5909.35s, 	Step: 16770, 	{'train/accuracy': 0.6768375039100647, 'train/loss': 1.3877172470092773, 'validation/accuracy': 0.6132599711418152, 'validation/loss': 1.6937204599380493, 'validation/num_examples': 50000, 'test/accuracy': 0.4991000294685364, 'test/loss': 2.3574867248535156, 'test/num_examples': 10000, 'score': 5659.213871479034, 'total_duration': 5909.350280046463, 'accumulated_submission_time': 5659.213871479034, 'accumulated_eval_time': 236.9870364665985, 'accumulated_logging_time': 12.958433628082275}
I0418 08:29:16.179301 140043356194560 logging_writer.py:48] [16770] accumulated_eval_time=236.987036, accumulated_logging_time=12.958434, accumulated_submission_time=5659.213871, global_step=16770, preemption_count=0, score=5659.213871, test/accuracy=0.499100, test/loss=2.357487, test/num_examples=10000, total_duration=5909.350280, train/accuracy=0.676838, train/loss=1.387717, validation/accuracy=0.613260, validation/loss=1.693720, validation/num_examples=50000
I0418 08:29:16.468619 140221955352384 checkpoints.py:356] Saving checkpoint at step: 16770
I0418 08:29:17.503039 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_16770
I0418 08:29:17.519397 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_16770.
I0418 08:29:27.938123 140043372979968 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5067907571792603, loss=2.4532644748687744
I0418 08:30:01.431182 140043347801856 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7746667861938477, loss=2.590797185897827
I0418 08:30:35.006211 140043372979968 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5315719246864319, loss=2.5287773609161377
I0418 08:31:08.544807 140043347801856 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.4965928792953491, loss=2.5607352256774902
I0418 08:31:42.017273 140043372979968 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.552304744720459, loss=2.695157051086426
I0418 08:32:15.571905 140043347801856 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4615563750267029, loss=2.570634126663208
I0418 08:32:49.067620 140043372979968 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.47081130743026733, loss=2.52370023727417
I0418 08:33:22.636375 140043347801856 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5086361765861511, loss=2.6023311614990234
I0418 08:33:56.010483 140043372979968 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5329867601394653, loss=2.72902774810791
I0418 08:34:29.697432 140043347801856 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.5089185833930969, loss=2.6678860187530518
I0418 08:35:03.267860 140043372979968 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5288746356964111, loss=2.6281354427337646
I0418 08:35:36.825590 140043347801856 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.45090198516845703, loss=2.5131030082702637
I0418 08:36:10.233166 140043372979968 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.5348596572875977, loss=2.502528190612793
I0418 08:36:43.704795 140043347801856 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.49811410903930664, loss=2.526980400085449
I0418 08:37:17.051611 140043372979968 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.4459885060787201, loss=2.5559298992156982
I0418 08:37:47.715694 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:37:55.711385 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:38:05.748536 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:38:07.618500 140221955352384 submission_runner.py:406] Time since start: 6440.80s, 	Step: 18293, 	{'train/accuracy': 0.7380420565605164, 'train/loss': 1.1389410495758057, 'validation/accuracy': 0.6389600038528442, 'validation/loss': 1.5779491662979126, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.287818670272827, 'test/num_examples': 10000, 'score': 6169.378247976303, 'total_duration': 6440.804844856262, 'accumulated_submission_time': 6169.378247976303, 'accumulated_eval_time': 256.88886523246765, 'accumulated_logging_time': 14.329021453857422}
I0418 08:38:07.628809 140043347801856 logging_writer.py:48] [18293] accumulated_eval_time=256.888865, accumulated_logging_time=14.329021, accumulated_submission_time=6169.378248, global_step=18293, preemption_count=0, score=6169.378248, test/accuracy=0.509300, test/loss=2.287819, test/num_examples=10000, total_duration=6440.804845, train/accuracy=0.738042, train/loss=1.138941, validation/accuracy=0.638960, validation/loss=1.577949, validation/num_examples=50000
I0418 08:38:07.954202 140221955352384 checkpoints.py:356] Saving checkpoint at step: 18293
I0418 08:38:09.015889 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_18293
I0418 08:38:09.032056 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_18293.
I0418 08:38:11.756655 140043372979968 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.46375834941864014, loss=2.557543992996216
I0418 08:38:45.264778 140040126576384 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4472014605998993, loss=2.629106044769287
I0418 08:39:18.787171 140043372979968 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.45847809314727783, loss=2.5375614166259766
I0418 08:39:52.187168 140040126576384 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5374388694763184, loss=2.5173823833465576
I0418 08:40:25.600158 140043372979968 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.42369309067726135, loss=2.5882487297058105
I0418 08:40:59.149597 140040126576384 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.4552200138568878, loss=2.491384506225586
I0418 08:41:32.641695 140043372979968 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5232468247413635, loss=2.5609331130981445
I0418 08:42:06.116055 140040126576384 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4436720311641693, loss=2.4218380451202393
I0418 08:42:39.465585 140043372979968 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5055625438690186, loss=2.4952757358551025
I0418 08:43:13.099946 140040126576384 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.4503263533115387, loss=2.4221606254577637
I0418 08:43:46.511315 140043372979968 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3992469012737274, loss=2.505995273590088
I0418 08:44:19.893745 140040126576384 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4437577724456787, loss=2.523702621459961
I0418 08:44:53.515912 140043372979968 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4548119306564331, loss=2.496006488800049
I0418 08:45:26.850191 140040126576384 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5592246055603027, loss=2.6012930870056152
I0418 08:46:00.289417 140043372979968 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4815962612628937, loss=2.5394952297210693
I0418 08:46:33.779739 140040126576384 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4658072590827942, loss=2.520148754119873
I0418 08:46:39.221955 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:46:47.134746 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:46:57.262962 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:46:59.392072 140221955352384 submission_runner.py:406] Time since start: 6972.58s, 	Step: 19818, 	{'train/accuracy': 0.7272600531578064, 'train/loss': 1.1823911666870117, 'validation/accuracy': 0.6341999769210815, 'validation/loss': 1.600745439529419, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.296038866043091, 'test/num_examples': 10000, 'score': 6679.546818256378, 'total_duration': 6972.578386306763, 'accumulated_submission_time': 6679.546818256378, 'accumulated_eval_time': 277.0579707622528, 'accumulated_logging_time': 15.747066736221313}
I0418 08:46:59.406744 140043372979968 logging_writer.py:48] [19818] accumulated_eval_time=277.057971, accumulated_logging_time=15.747067, accumulated_submission_time=6679.546818, global_step=19818, preemption_count=0, score=6679.546818, test/accuracy=0.511400, test/loss=2.296039, test/num_examples=10000, total_duration=6972.578386, train/accuracy=0.727260, train/loss=1.182391, validation/accuracy=0.634200, validation/loss=1.600745, validation/num_examples=50000
I0418 08:46:59.686254 140221955352384 checkpoints.py:356] Saving checkpoint at step: 19818
I0418 08:47:00.695399 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_19818
I0418 08:47:00.713055 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_19818.
I0418 08:47:28.372628 140040126576384 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.43619397282600403, loss=2.4379525184631348
I0418 08:48:01.791284 140042064340736 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.48351550102233887, loss=2.537043571472168
I0418 08:48:35.125299 140040126576384 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.4614156484603882, loss=2.529956340789795
I0418 08:49:08.478203 140042064340736 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4879416525363922, loss=2.615868330001831
I0418 08:49:41.930488 140040126576384 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4585709869861603, loss=2.516981363296509
I0418 08:50:15.331021 140042064340736 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.46887606382369995, loss=2.456899642944336
I0418 08:50:48.993049 140040126576384 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.416681170463562, loss=2.5331451892852783
I0418 08:51:22.301839 140042064340736 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.46633124351501465, loss=2.528519868850708
I0418 08:51:55.774668 140040126576384 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5346887111663818, loss=2.483214855194092
I0418 08:52:29.067986 140042064340736 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5065653920173645, loss=2.5030832290649414
I0418 08:53:02.547701 140040126576384 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.44430169463157654, loss=2.5295515060424805
I0418 08:53:35.988577 140042064340736 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5093628764152527, loss=2.5414185523986816
I0418 08:54:09.337693 140040126576384 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.44476181268692017, loss=2.471113443374634
I0418 08:54:42.570659 140042064340736 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4477224051952362, loss=2.497732162475586
I0418 08:55:16.081041 140040126576384 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4266726076602936, loss=2.4733192920684814
I0418 08:55:30.821680 140221955352384 spec.py:298] Evaluating on the training split.
I0418 08:55:38.177072 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 08:55:48.207760 140221955352384 spec.py:326] Evaluating on the test split.
I0418 08:55:50.111234 140221955352384 submission_runner.py:406] Time since start: 7503.30s, 	Step: 21346, 	{'train/accuracy': 0.7373046875, 'train/loss': 1.148218035697937, 'validation/accuracy': 0.6543999910354614, 'validation/loss': 1.5366097688674927, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.2480223178863525, 'test/num_examples': 10000, 'score': 7189.633596658707, 'total_duration': 7503.297639369965, 'accumulated_submission_time': 7189.633596658707, 'accumulated_eval_time': 296.3466126918793, 'accumulated_logging_time': 17.073213815689087}
I0418 08:55:50.122885 140042064340736 logging_writer.py:48] [21346] accumulated_eval_time=296.346613, accumulated_logging_time=17.073214, accumulated_submission_time=7189.633597, global_step=21346, preemption_count=0, score=7189.633597, test/accuracy=0.525000, test/loss=2.248022, test/num_examples=10000, total_duration=7503.297639, train/accuracy=0.737305, train/loss=1.148218, validation/accuracy=0.654400, validation/loss=1.536610, validation/num_examples=50000
I0418 08:55:50.423230 140221955352384 checkpoints.py:356] Saving checkpoint at step: 21346
I0418 08:55:51.427835 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_21346
I0418 08:55:51.447769 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_21346.
I0418 08:56:09.882928 140040126576384 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.4593878984451294, loss=2.495210886001587
I0418 08:56:43.412348 140041963693824 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.46369919180870056, loss=2.563819169998169
I0418 08:57:16.912297 140040126576384 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5052084922790527, loss=2.467183828353882
I0418 08:57:50.347399 140041963693824 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.4322388172149658, loss=2.4078545570373535
I0418 08:58:23.908240 140040126576384 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.4646827280521393, loss=2.5108983516693115
I0418 08:58:57.420602 140041963693824 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.43908753991127014, loss=2.3623902797698975
I0418 08:59:30.758291 140040126576384 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.43614938855171204, loss=2.4545235633850098
I0418 09:00:04.128462 140041963693824 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.423042356967926, loss=2.5208258628845215
I0418 09:00:37.563311 140040126576384 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3668002486228943, loss=2.4299988746643066
I0418 09:01:10.907433 140041963693824 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.40042638778686523, loss=2.430269479751587
I0418 09:01:44.295645 140040126576384 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4341837167739868, loss=2.4899990558624268
I0418 09:02:17.739818 140041963693824 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.42157092690467834, loss=2.354435920715332
I0418 09:02:51.092700 140040126576384 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.4194360375404358, loss=2.4473257064819336
I0418 09:03:24.441771 140041963693824 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.42249077558517456, loss=2.492424488067627
I0418 09:03:57.691694 140040126576384 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.45211687684059143, loss=2.51523756980896
I0418 09:04:21.467544 140221955352384 spec.py:298] Evaluating on the training split.
I0418 09:04:29.644185 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 09:04:39.777813 140221955352384 spec.py:326] Evaluating on the test split.
I0418 09:04:41.908733 140221955352384 submission_runner.py:406] Time since start: 8035.10s, 	Step: 22873, 	{'train/accuracy': 0.7445192933082581, 'train/loss': 1.1153249740600586, 'validation/accuracy': 0.6567800045013428, 'validation/loss': 1.5141981840133667, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.230273485183716, 'test/num_examples': 10000, 'score': 7699.625548839569, 'total_duration': 8035.095364809036, 'accumulated_submission_time': 7699.625548839569, 'accumulated_eval_time': 316.78711104393005, 'accumulated_logging_time': 18.420674800872803}
I0418 09:04:41.923602 140041963693824 logging_writer.py:48] [22873] accumulated_eval_time=316.787111, accumulated_logging_time=18.420675, accumulated_submission_time=7699.625549, global_step=22873, preemption_count=0, score=7699.625549, test/accuracy=0.527300, test/loss=2.230273, test/num_examples=10000, total_duration=8035.095365, train/accuracy=0.744519, train/loss=1.115325, validation/accuracy=0.656780, validation/loss=1.514198, validation/num_examples=50000
I0418 09:04:42.199030 140221955352384 checkpoints.py:356] Saving checkpoint at step: 22873
I0418 09:04:43.202311 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_22873
I0418 09:04:43.221824 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_22873.
I0418 09:04:52.636179 140040126576384 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.4099947512149811, loss=2.402599334716797
I0418 09:05:26.168058 140041955301120 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4105292856693268, loss=2.4308385848999023
I0418 09:05:59.588532 140040126576384 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.43498754501342773, loss=2.396831512451172
I0418 09:06:32.908135 140041955301120 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.41611653566360474, loss=2.450939655303955
I0418 09:07:06.441431 140040126576384 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.4459553360939026, loss=2.4021496772766113
I0418 09:07:39.880005 140041955301120 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.36286184191703796, loss=2.315908908843994
I0418 09:08:13.307709 140040126576384 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.43434572219848633, loss=2.464876651763916
I0418 09:08:46.820275 140041955301120 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.423539400100708, loss=2.399454116821289
I0418 09:09:20.428139 140040126576384 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.4234004020690918, loss=2.3834524154663086
I0418 09:09:53.786720 140041955301120 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.39748215675354004, loss=2.4632949829101562
I0418 09:10:27.260479 140040126576384 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4502614438533783, loss=2.430311679840088
I0418 09:11:00.688438 140041955301120 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.43933793902397156, loss=2.479166269302368
I0418 09:11:34.165937 140040126576384 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.46466657519340515, loss=2.4747822284698486
I0418 09:12:07.631189 140041955301120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.41007837653160095, loss=2.4491775035858154
I0418 09:12:41.105219 140040126576384 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.41892513632774353, loss=2.4151439666748047
I0418 09:13:13.344117 140221955352384 spec.py:298] Evaluating on the training split.
I0418 09:13:20.763586 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 09:13:30.681469 140221955352384 spec.py:326] Evaluating on the test split.
I0418 09:13:32.800643 140221955352384 submission_runner.py:406] Time since start: 8565.99s, 	Step: 24398, 	{'train/accuracy': 0.7429846525192261, 'train/loss': 1.1136114597320557, 'validation/accuracy': 0.6581999659538269, 'validation/loss': 1.5025320053100586, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.2233598232269287, 'test/num_examples': 10000, 'score': 8209.725897550583, 'total_duration': 8565.98696756363, 'accumulated_submission_time': 8209.725897550583, 'accumulated_eval_time': 336.24263882637024, 'accumulated_logging_time': 19.739320755004883}
I0418 09:13:32.811124 140041955301120 logging_writer.py:48] [24398] accumulated_eval_time=336.242639, accumulated_logging_time=19.739321, accumulated_submission_time=8209.725898, global_step=24398, preemption_count=0, score=8209.725898, test/accuracy=0.526500, test/loss=2.223360, test/num_examples=10000, total_duration=8565.986968, train/accuracy=0.742985, train/loss=1.113611, validation/accuracy=0.658200, validation/loss=1.502532, validation/num_examples=50000
I0418 09:13:33.148298 140221955352384 checkpoints.py:356] Saving checkpoint at step: 24398
I0418 09:13:34.144688 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_24398
I0418 09:13:34.162753 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_24398.
I0418 09:13:35.199815 140040126576384 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.39594537019729614, loss=2.429737091064453
I0418 09:14:08.555613 140041946908416 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.39910247921943665, loss=2.4665071964263916
I0418 09:14:41.881225 140040126576384 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.40571829676628113, loss=2.385183811187744
I0418 09:15:15.215102 140041946908416 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.41112658381462097, loss=2.3999125957489014
I0418 09:15:48.496041 140040126576384 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.40604421496391296, loss=2.379645824432373
I0418 09:16:21.811079 140041946908416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.4129411280155182, loss=2.476074695587158
I0418 09:16:55.194186 140040126576384 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.40701526403427124, loss=2.412437677383423
I0418 09:17:28.579640 140041946908416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.4299491047859192, loss=2.4324281215667725
I0418 09:18:01.955276 140040126576384 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4206949472427368, loss=2.3489878177642822
I0418 09:18:35.487202 140041946908416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4081321060657501, loss=2.368147134780884
I0418 09:19:08.864073 140040126576384 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.39000949263572693, loss=2.4170000553131104
I0418 09:19:42.147122 140041946908416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4381367266178131, loss=2.393707275390625
I0418 09:20:15.534590 140040126576384 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.39864057302474976, loss=2.3074452877044678
I0418 09:20:48.989020 140041946908416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4311608374118805, loss=2.407057285308838
I0418 09:21:22.225078 140040126576384 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.5018014311790466, loss=2.403092861175537
I0418 09:21:55.542077 140041946908416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.4129475951194763, loss=2.396212577819824
I0418 09:22:04.379210 140221955352384 spec.py:298] Evaluating on the training split.
I0418 09:22:11.799346 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 09:22:21.933072 140221955352384 spec.py:326] Evaluating on the test split.
I0418 09:22:24.061630 140221955352384 submission_runner.py:406] Time since start: 9097.25s, 	Step: 25928, 	{'train/accuracy': 0.8056241869926453, 'train/loss': 0.8689529895782471, 'validation/accuracy': 0.6702399849891663, 'validation/loss': 1.4459069967269897, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.142677068710327, 'test/num_examples': 10000, 'score': 8719.915380716324, 'total_duration': 9097.24780511856, 'accumulated_submission_time': 8719.915380716324, 'accumulated_eval_time': 355.92391085624695, 'accumulated_logging_time': 21.11140775680542}
I0418 09:22:24.071758 140040126576384 logging_writer.py:48] [25928] accumulated_eval_time=355.923911, accumulated_logging_time=21.111408, accumulated_submission_time=8719.915381, global_step=25928, preemption_count=0, score=8719.915381, test/accuracy=0.543400, test/loss=2.142677, test/num_examples=10000, total_duration=9097.247805, train/accuracy=0.805624, train/loss=0.868953, validation/accuracy=0.670240, validation/loss=1.445907, validation/num_examples=50000
I0418 09:22:24.351575 140221955352384 checkpoints.py:356] Saving checkpoint at step: 25928
I0418 09:22:25.364462 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_25928
I0418 09:22:25.384586 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_25928.
I0418 09:22:49.888739 140041946908416 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.43665260076522827, loss=2.4394426345825195
I0418 09:23:23.258545 140041053513472 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3943258821964264, loss=2.3753607273101807
I0418 09:23:56.580587 140041946908416 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.43238502740859985, loss=2.375587224960327
I0418 09:24:30.020366 140041053513472 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.3888966143131256, loss=2.3556954860687256
I0418 09:25:03.354885 140041946908416 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4071106016635895, loss=2.380762815475464
I0418 09:25:36.739409 140041053513472 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.38587260246276855, loss=2.3754589557647705
I0418 09:26:10.041152 140041946908416 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.396968275308609, loss=2.329554557800293
I0418 09:26:43.431369 140041053513472 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.4273596704006195, loss=2.3676369190216064
I0418 09:27:16.707230 140041946908416 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4763796031475067, loss=2.4689435958862305
I0418 09:27:50.038502 140041053513472 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.40504729747772217, loss=2.3765206336975098
I0418 09:28:23.503288 140041946908416 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3799460530281067, loss=2.3204426765441895
I0418 09:28:56.829423 140041053513472 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4457724392414093, loss=2.3452696800231934
I0418 09:29:30.226743 140041946908416 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3752070963382721, loss=2.379899263381958
I0418 09:30:03.735063 140041053513472 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.511558473110199, loss=2.4356651306152344
I0418 09:30:37.005669 140041946908416 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.48723357915878296, loss=2.4189229011535645
I0418 09:30:55.500815 140221955352384 spec.py:298] Evaluating on the training split.
I0418 09:31:03.065134 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 09:31:13.262277 140221955352384 spec.py:326] Evaluating on the test split.
I0418 09:31:15.248821 140221955352384 submission_runner.py:406] Time since start: 9628.43s, 	Step: 27457, 	{'train/accuracy': 0.7792769074440002, 'train/loss': 0.9694718718528748, 'validation/accuracy': 0.669439971446991, 'validation/loss': 1.4692424535751343, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.204664707183838, 'test/num_examples': 10000, 'score': 9230.00445151329, 'total_duration': 9628.43493938446, 'accumulated_submission_time': 9230.00445151329, 'accumulated_eval_time': 375.67071533203125, 'accumulated_logging_time': 22.444871425628662}
I0418 09:31:15.259168 140041053513472 logging_writer.py:48] [27457] accumulated_eval_time=375.670715, accumulated_logging_time=22.444871, accumulated_submission_time=9230.004452, global_step=27457, preemption_count=0, score=9230.004452, test/accuracy=0.533700, test/loss=2.204665, test/num_examples=10000, total_duration=9628.434939, train/accuracy=0.779277, train/loss=0.969472, validation/accuracy=0.669440, validation/loss=1.469242, validation/num_examples=50000
I0418 09:31:15.538417 140221955352384 checkpoints.py:356] Saving checkpoint at step: 27457
I0418 09:31:16.569003 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_27457
I0418 09:31:16.591151 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_27457.
I0418 09:31:31.296659 140041946908416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.42684438824653625, loss=2.3445584774017334
I0418 09:32:04.754180 140040764126976 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.44382086396217346, loss=2.4231395721435547
I0418 09:32:38.377700 140041946908416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4067857563495636, loss=2.3798694610595703
I0418 09:33:11.722110 140040764126976 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4512551426887512, loss=2.3863158226013184
I0418 09:33:45.230692 140041946908416 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.4046246111392975, loss=2.382756233215332
I0418 09:34:17.891463 140221955352384 spec.py:298] Evaluating on the training split.
I0418 09:34:25.491214 140221955352384 spec.py:310] Evaluating on the validation split.
I0418 09:34:35.875218 140221955352384 spec.py:326] Evaluating on the test split.
I0418 09:34:37.831149 140221955352384 submission_runner.py:406] Time since start: 9831.02s, 	Step: 28000, 	{'train/accuracy': 0.7685546875, 'train/loss': 1.0088146924972534, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.4443968534469604, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.171128988265991, 'test/num_examples': 10000, 'score': 9411.293343305588, 'total_duration': 9831.017434835434, 'accumulated_submission_time': 9411.293343305588, 'accumulated_eval_time': 395.6093604564667, 'accumulated_logging_time': 23.79249620437622}
I0418 09:34:37.844207 140040764126976 logging_writer.py:48] [28000] accumulated_eval_time=395.609360, accumulated_logging_time=23.792496, accumulated_submission_time=9411.293343, global_step=28000, preemption_count=0, score=9411.293343, test/accuracy=0.534900, test/loss=2.171129, test/num_examples=10000, total_duration=9831.017435, train/accuracy=0.768555, train/loss=1.008815, validation/accuracy=0.669760, validation/loss=1.444397, validation/num_examples=50000
I0418 09:34:38.120060 140221955352384 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 09:34:39.485377 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0418 09:34:39.510757 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0418 09:34:39.532900 140041946908416 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=9411.293343
I0418 09:34:39.707793 140221955352384 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 09:34:41.047978 140221955352384 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0418 09:34:41.071198 140221955352384 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0418 09:34:41.471332 140221955352384 submission_runner.py:567] Tuning trial 1/1
I0418 09:34:41.472146 140221955352384 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 09:34:41.491021 140221955352384 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008569834171794355, 'train/loss': 6.910048484802246, 'validation/accuracy': 0.0008199999574571848, 'validation/loss': 6.910060882568359, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.910253524780273, 'test/num_examples': 10000, 'score': 47.62135195732117, 'total_duration': 87.40028119087219, 'accumulated_submission_time': 47.62135195732117, 'accumulated_eval_time': 39.778767585754395, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1519, {'train/accuracy': 0.11890146136283875, 'train/loss': 4.769593238830566, 'validation/accuracy': 0.10387999564409256, 'validation/loss': 4.878118515014648, 'validation/num_examples': 50000, 'test/accuracy': 0.0796000063419342, 'test/loss': 5.246491432189941, 'test/num_examples': 10000, 'score': 557.7915813922882, 'total_duration': 615.1500458717346, 'accumulated_submission_time': 557.7915813922882, 'accumulated_eval_time': 56.45005202293396, 'accumulated_logging_time': 0.8905539512634277, 'global_step': 1519, 'preemption_count': 0}), (3038, {'train/accuracy': 0.2700693607330322, 'train/loss': 3.573406934738159, 'validation/accuracy': 0.24521999061107635, 'validation/loss': 3.722559690475464, 'validation/num_examples': 50000, 'test/accuracy': 0.18450000882148743, 'test/loss': 4.257433891296387, 'test/num_examples': 10000, 'score': 1067.8279321193695, 'total_duration': 1143.182498216629, 'accumulated_submission_time': 1067.8279321193695, 'accumulated_eval_time': 73.2985589504242, 'accumulated_logging_time': 2.0203680992126465, 'global_step': 3038, 'preemption_count': 0}), (4556, {'train/accuracy': 0.38932955265045166, 'train/loss': 2.850102663040161, 'validation/accuracy': 0.3567200005054474, 'validation/loss': 3.0206139087677, 'validation/num_examples': 50000, 'test/accuracy': 0.2766000032424927, 'test/loss': 3.6376664638519287, 'test/num_examples': 10000, 'score': 1578.0155239105225, 'total_duration': 1670.957671403885, 'accumulated_submission_time': 1578.0155239105225, 'accumulated_eval_time': 90.02390789985657, 'accumulated_logging_time': 2.8653059005737305, 'global_step': 4556, 'preemption_count': 0}), (6082, {'train/accuracy': 0.4662587642669678, 'train/loss': 2.4076318740844727, 'validation/accuracy': 0.43685999512672424, 'validation/loss': 2.563072919845581, 'validation/num_examples': 50000, 'test/accuracy': 0.3305000066757202, 'test/loss': 3.22933292388916, 'test/num_examples': 10000, 'score': 2088.059413909912, 'total_duration': 2198.831042289734, 'accumulated_submission_time': 2088.059413909912, 'accumulated_eval_time': 106.70098948478699, 'accumulated_logging_time': 4.000883340835571, 'global_step': 6082, 'preemption_count': 0}), (7607, {'train/accuracy': 0.5345982313156128, 'train/loss': 2.101940870285034, 'validation/accuracy': 0.498119980096817, 'validation/loss': 2.2839956283569336, 'validation/num_examples': 50000, 'test/accuracy': 0.38100001215934753, 'test/loss': 2.9551000595092773, 'test/num_examples': 10000, 'score': 2598.356607198715, 'total_duration': 2727.1027987003326, 'accumulated_submission_time': 2598.356607198715, 'accumulated_eval_time': 123.39125752449036, 'accumulated_logging_time': 5.267918825149536, 'global_step': 7607, 'preemption_count': 0}), (9134, {'train/accuracy': 0.6017418503761292, 'train/loss': 1.7502098083496094, 'validation/accuracy': 0.5194999575614929, 'validation/loss': 2.1560373306274414, 'validation/num_examples': 50000, 'test/accuracy': 0.4009000062942505, 'test/loss': 2.8617897033691406, 'test/num_examples': 10000, 'score': 3108.5083680152893, 'total_duration': 3256.877753973007, 'accumulated_submission_time': 3108.5083680152893, 'accumulated_eval_time': 141.72609448432922, 'accumulated_logging_time': 6.539368629455566, 'global_step': 9134, 'preemption_count': 0}), (10661, {'train/accuracy': 0.6206752061843872, 'train/loss': 1.6609508991241455, 'validation/accuracy': 0.5554999709129333, 'validation/loss': 1.9836561679840088, 'validation/num_examples': 50000, 'test/accuracy': 0.43570002913475037, 'test/loss': 2.661497116088867, 'test/num_examples': 10000, 'score': 3618.612957239151, 'total_duration': 3786.4213967323303, 'accumulated_submission_time': 3618.612957239151, 'accumulated_eval_time': 159.84357523918152, 'accumulated_logging_time': 7.843618869781494, 'global_step': 10661, 'preemption_count': 0}), (12188, {'train/accuracy': 0.634207546710968, 'train/loss': 1.6309587955474854, 'validation/accuracy': 0.5759400129318237, 'validation/loss': 1.909151315689087, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.6158440113067627, 'test/num_examples': 10000, 'score': 4128.895737171173, 'total_duration': 4317.605779886246, 'accumulated_submission_time': 4128.895737171173, 'accumulated_eval_time': 179.47941374778748, 'accumulated_logging_time': 9.092377662658691, 'global_step': 12188, 'preemption_count': 0}), (13717, {'train/accuracy': 0.6602957248687744, 'train/loss': 1.4725115299224854, 'validation/accuracy': 0.6019999980926514, 'validation/loss': 1.7582768201828003, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.465412139892578, 'test/num_examples': 10000, 'score': 4639.150049686432, 'total_duration': 4848.0149047374725, 'accumulated_submission_time': 4639.150049686432, 'accumulated_eval_time': 198.33454632759094, 'accumulated_logging_time': 10.374719142913818, 'global_step': 13717, 'preemption_count': 0}), (15244, {'train/accuracy': 0.6758211255073547, 'train/loss': 1.4175982475280762, 'validation/accuracy': 0.6136999726295471, 'validation/loss': 1.7086646556854248, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.4089126586914062, 'test/num_examples': 10000, 'score': 5149.158976078033, 'total_duration': 5378.668406248093, 'accumulated_submission_time': 5149.158976078033, 'accumulated_eval_time': 217.72545409202576, 'accumulated_logging_time': 11.610625743865967, 'global_step': 15244, 'preemption_count': 0}), (16770, {'train/accuracy': 0.6768375039100647, 'train/loss': 1.3877172470092773, 'validation/accuracy': 0.6132599711418152, 'validation/loss': 1.6937204599380493, 'validation/num_examples': 50000, 'test/accuracy': 0.4991000294685364, 'test/loss': 2.3574867248535156, 'test/num_examples': 10000, 'score': 5659.213871479034, 'total_duration': 5909.350280046463, 'accumulated_submission_time': 5659.213871479034, 'accumulated_eval_time': 236.9870364665985, 'accumulated_logging_time': 12.958433628082275, 'global_step': 16770, 'preemption_count': 0}), (18293, {'train/accuracy': 0.7380420565605164, 'train/loss': 1.1389410495758057, 'validation/accuracy': 0.6389600038528442, 'validation/loss': 1.5779491662979126, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.287818670272827, 'test/num_examples': 10000, 'score': 6169.378247976303, 'total_duration': 6440.804844856262, 'accumulated_submission_time': 6169.378247976303, 'accumulated_eval_time': 256.88886523246765, 'accumulated_logging_time': 14.329021453857422, 'global_step': 18293, 'preemption_count': 0}), (19818, {'train/accuracy': 0.7272600531578064, 'train/loss': 1.1823911666870117, 'validation/accuracy': 0.6341999769210815, 'validation/loss': 1.600745439529419, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.296038866043091, 'test/num_examples': 10000, 'score': 6679.546818256378, 'total_duration': 6972.578386306763, 'accumulated_submission_time': 6679.546818256378, 'accumulated_eval_time': 277.0579707622528, 'accumulated_logging_time': 15.747066736221313, 'global_step': 19818, 'preemption_count': 0}), (21346, {'train/accuracy': 0.7373046875, 'train/loss': 1.148218035697937, 'validation/accuracy': 0.6543999910354614, 'validation/loss': 1.5366097688674927, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.2480223178863525, 'test/num_examples': 10000, 'score': 7189.633596658707, 'total_duration': 7503.297639369965, 'accumulated_submission_time': 7189.633596658707, 'accumulated_eval_time': 296.3466126918793, 'accumulated_logging_time': 17.073213815689087, 'global_step': 21346, 'preemption_count': 0}), (22873, {'train/accuracy': 0.7445192933082581, 'train/loss': 1.1153249740600586, 'validation/accuracy': 0.6567800045013428, 'validation/loss': 1.5141981840133667, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.230273485183716, 'test/num_examples': 10000, 'score': 7699.625548839569, 'total_duration': 8035.095364809036, 'accumulated_submission_time': 7699.625548839569, 'accumulated_eval_time': 316.78711104393005, 'accumulated_logging_time': 18.420674800872803, 'global_step': 22873, 'preemption_count': 0}), (24398, {'train/accuracy': 0.7429846525192261, 'train/loss': 1.1136114597320557, 'validation/accuracy': 0.6581999659538269, 'validation/loss': 1.5025320053100586, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.2233598232269287, 'test/num_examples': 10000, 'score': 8209.725897550583, 'total_duration': 8565.98696756363, 'accumulated_submission_time': 8209.725897550583, 'accumulated_eval_time': 336.24263882637024, 'accumulated_logging_time': 19.739320755004883, 'global_step': 24398, 'preemption_count': 0}), (25928, {'train/accuracy': 0.8056241869926453, 'train/loss': 0.8689529895782471, 'validation/accuracy': 0.6702399849891663, 'validation/loss': 1.4459069967269897, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.142677068710327, 'test/num_examples': 10000, 'score': 8719.915380716324, 'total_duration': 9097.24780511856, 'accumulated_submission_time': 8719.915380716324, 'accumulated_eval_time': 355.92391085624695, 'accumulated_logging_time': 21.11140775680542, 'global_step': 25928, 'preemption_count': 0}), (27457, {'train/accuracy': 0.7792769074440002, 'train/loss': 0.9694718718528748, 'validation/accuracy': 0.669439971446991, 'validation/loss': 1.4692424535751343, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.204664707183838, 'test/num_examples': 10000, 'score': 9230.00445151329, 'total_duration': 9628.43493938446, 'accumulated_submission_time': 9230.00445151329, 'accumulated_eval_time': 375.67071533203125, 'accumulated_logging_time': 22.444871425628662, 'global_step': 27457, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7685546875, 'train/loss': 1.0088146924972534, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.4443968534469604, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.171128988265991, 'test/num_examples': 10000, 'score': 9411.293343305588, 'total_duration': 9831.017434835434, 'accumulated_submission_time': 9411.293343305588, 'accumulated_eval_time': 395.6093604564667, 'accumulated_logging_time': 23.79249620437622, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0418 09:34:41.491146 140221955352384 submission_runner.py:570] Timing: 9411.293343305588
I0418 09:34:41.491195 140221955352384 submission_runner.py:571] ====================
I0418 09:34:41.491301 140221955352384 submission_runner.py:631] Final imagenet_resnet score: 9411.293343305588
