I0418 13:47:22.544644 139726215075648 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax.
I0418 13:47:22.606136 139726215075648 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 13:47:23.553032 139726215075648 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0418 13:47:23.553754 139726215075648 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 13:47:23.558031 139726215075648 submission_runner.py:528] Using RNG seed 608222297
I0418 13:47:26.092379 139726215075648 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 13:47:26.092562 139726215075648 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1.
I0418 13:47:26.092741 139726215075648 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/hparams.json.
I0418 13:47:26.214264 139726215075648 submission_runner.py:232] Initializing dataset.
I0418 13:47:26.447895 139726215075648 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:47:26.453421 139726215075648 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:47:26.684449 139726215075648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:47:26.743037 139726215075648 submission_runner.py:239] Initializing model.
I0418 13:47:34.688948 139726215075648 submission_runner.py:249] Initializing optimizer.
I0418 13:47:35.078043 139726215075648 submission_runner.py:256] Initializing metrics bundle.
I0418 13:47:35.078279 139726215075648 submission_runner.py:273] Initializing checkpoint and logger.
I0418 13:47:35.079297 139726215075648 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1 with prefix checkpoint_
I0418 13:47:35.079578 139726215075648 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 13:47:35.079659 139726215075648 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 13:47:35.879720 139726215075648 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/meta_data_0.json.
I0418 13:47:35.880809 139726215075648 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/flags_0.json.
I0418 13:47:35.886776 139726215075648 submission_runner.py:309] Starting training loop.
I0418 13:47:56.131529 139550030558976 logging_writer.py:48] [0] global_step=0, grad_norm=2.852597951889038, loss=0.7756453156471252
I0418 13:47:56.143987 139726215075648 spec.py:298] Evaluating on the training split.
I0418 13:47:56.151613 139726215075648 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:47:56.155165 139726215075648 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:47:56.211849 139726215075648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0418 13:48:12.018795 139726215075648 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0418 13:49:26.060370 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 13:49:26.063071 139726215075648 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:49:26.066984 139726215075648 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:49:26.118382 139726215075648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:50:28.127349 139726215075648 spec.py:326] Evaluating on the test split.
I0418 13:50:28.130052 139726215075648 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:50:28.133552 139726215075648 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:50:28.185297 139726215075648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:51:30.616916 139726215075648 submission_runner.py:406] Time since start: 234.73s, 	Step: 1, 	{'train/accuracy': 0.45287150144577026, 'train/loss': 0.775056004524231, 'train/mean_average_precision': 0.022378059176111787, 'validation/accuracy': 0.4569682478904724, 'validation/loss': 0.7732618451118469, 'validation/mean_average_precision': 0.02595133389640027, 'validation/num_examples': 43793, 'test/accuracy': 0.4588867425918579, 'test/loss': 0.7721359729766846, 'test/mean_average_precision': 0.02775132771831757, 'test/num_examples': 43793, 'score': 20.257025480270386, 'total_duration': 234.73008728027344, 'accumulated_submission_time': 20.257025480270386, 'accumulated_eval_time': 214.47287821769714, 'accumulated_logging_time': 0}
I0418 13:51:30.633419 139540416763648 logging_writer.py:48] [1] accumulated_eval_time=214.472878, accumulated_logging_time=0, accumulated_submission_time=20.257025, global_step=1, preemption_count=0, score=20.257025, test/accuracy=0.458887, test/loss=0.772136, test/mean_average_precision=0.027751, test/num_examples=43793, total_duration=234.730087, train/accuracy=0.452872, train/loss=0.775056, train/mean_average_precision=0.022378, validation/accuracy=0.456968, validation/loss=0.773262, validation/mean_average_precision=0.025951, validation/num_examples=43793
I0418 13:51:30.666657 139726215075648 checkpoints.py:356] Saving checkpoint at step: 1
I0418 13:51:30.759872 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_1
I0418 13:51:30.760229 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_1.
I0418 13:51:53.835897 139540425156352 logging_writer.py:48] [100] global_step=100, grad_norm=0.4234476685523987, loss=0.37628301978111267
I0418 13:52:16.881888 139541624887040 logging_writer.py:48] [200] global_step=200, grad_norm=0.23227562010288239, loss=0.20546536147594452
I0418 13:52:39.704491 139540425156352 logging_writer.py:48] [300] global_step=300, grad_norm=0.10167671740055084, loss=0.10794339329004288
I0418 13:53:02.531475 139541624887040 logging_writer.py:48] [400] global_step=400, grad_norm=0.04607755318284035, loss=0.07290653139352798
I0418 13:53:25.346974 139540425156352 logging_writer.py:48] [500] global_step=500, grad_norm=0.07081447541713715, loss=0.059027060866355896
I0418 13:53:48.762248 139541624887040 logging_writer.py:48] [600] global_step=600, grad_norm=0.041582874953746796, loss=0.05957331135869026
I0418 13:54:12.405855 139540425156352 logging_writer.py:48] [700] global_step=700, grad_norm=0.04804481938481331, loss=0.060273390263319016
I0418 13:54:35.455193 139541624887040 logging_writer.py:48] [800] global_step=800, grad_norm=0.02303333207964897, loss=0.05055917054414749
I0418 13:54:58.514487 139540425156352 logging_writer.py:48] [900] global_step=900, grad_norm=0.04153244197368622, loss=0.04809088259935379
I0418 13:55:21.625941 139541624887040 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.04908284917473793, loss=0.04905841499567032
I0418 13:55:30.885821 139726215075648 spec.py:298] Evaluating on the training split.
I0418 13:56:42.950978 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 13:56:45.461698 139726215075648 spec.py:326] Evaluating on the test split.
I0418 13:56:47.946427 139726215075648 submission_runner.py:406] Time since start: 552.06s, 	Step: 1041, 	{'train/accuracy': 0.9869406223297119, 'train/loss': 0.04963434115052223, 'train/mean_average_precision': 0.06954605093557754, 'validation/accuracy': 0.9843891263008118, 'validation/loss': 0.0589640736579895, 'validation/mean_average_precision': 0.06993176791245509, 'validation/num_examples': 43793, 'test/accuracy': 0.9833741188049316, 'test/loss': 0.06219257414340973, 'test/mean_average_precision': 0.06927957111791591, 'test/num_examples': 43793, 'score': 260.37417006492615, 'total_duration': 552.0596053600311, 'accumulated_submission_time': 260.37417006492615, 'accumulated_eval_time': 291.5334506034851, 'accumulated_logging_time': 0.14373445510864258}
I0418 13:56:47.953774 139540425156352 logging_writer.py:48] [1041] accumulated_eval_time=291.533451, accumulated_logging_time=0.143734, accumulated_submission_time=260.374170, global_step=1041, preemption_count=0, score=260.374170, test/accuracy=0.983374, test/loss=0.062193, test/mean_average_precision=0.069280, test/num_examples=43793, total_duration=552.059605, train/accuracy=0.986941, train/loss=0.049634, train/mean_average_precision=0.069546, validation/accuracy=0.984389, validation/loss=0.058964, validation/mean_average_precision=0.069932, validation/num_examples=43793
I0418 13:56:47.986283 139726215075648 checkpoints.py:356] Saving checkpoint at step: 1041
I0418 13:56:48.082593 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_1041
I0418 13:56:48.083069 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_1041.
I0418 13:57:01.987993 139541624887040 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.04911357909440994, loss=0.042474664747714996
I0418 13:57:25.380066 139541457393408 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.04114101082086563, loss=0.04391419515013695
I0418 13:57:48.435820 139541624887040 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.022322816774249077, loss=0.04725023731589317
I0418 13:58:12.584435 139541457393408 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.07207950204610825, loss=0.0415915846824646
I0418 13:58:35.886531 139541624887040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.059736866503953934, loss=0.050941839814186096
I0418 13:58:59.071112 139541457393408 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.07658154517412186, loss=0.04543611779808998
I0418 13:59:22.405231 139541624887040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.045154470950365067, loss=0.053643111139535904
I0418 13:59:45.673522 139541457393408 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.06364288181066513, loss=0.047906823456287384
I0418 14:00:08.770318 139541624887040 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.09254979342222214, loss=0.04796711727976799
I0418 14:00:31.813667 139541457393408 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.02319454960525036, loss=0.04762322083115578
I0418 14:00:48.219793 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:01:59.769749 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:02:02.328486 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:02:04.791095 139726215075648 submission_runner.py:406] Time since start: 868.90s, 	Step: 2072, 	{'train/accuracy': 0.9878095984458923, 'train/loss': 0.04332897067070007, 'train/mean_average_precision': 0.14847212878061883, 'validation/accuracy': 0.9850406646728516, 'validation/loss': 0.05267998203635216, 'validation/mean_average_precision': 0.14227792592708427, 'validation/num_examples': 43793, 'test/accuracy': 0.9840472340583801, 'test/loss': 0.05561130866408348, 'test/mean_average_precision': 0.14176738854946636, 'test/num_examples': 43793, 'score': 500.5006582736969, 'total_duration': 868.9042737483978, 'accumulated_submission_time': 500.5006582736969, 'accumulated_eval_time': 368.10473227500916, 'accumulated_logging_time': 0.2807760238647461}
I0418 14:02:04.799190 139541624887040 logging_writer.py:48] [2072] accumulated_eval_time=368.104732, accumulated_logging_time=0.280776, accumulated_submission_time=500.500658, global_step=2072, preemption_count=0, score=500.500658, test/accuracy=0.984047, test/loss=0.055611, test/mean_average_precision=0.141767, test/num_examples=43793, total_duration=868.904274, train/accuracy=0.987810, train/loss=0.043329, train/mean_average_precision=0.148472, validation/accuracy=0.985041, validation/loss=0.052680, validation/mean_average_precision=0.142278, validation/num_examples=43793
I0418 14:02:04.831462 139726215075648 checkpoints.py:356] Saving checkpoint at step: 2072
I0418 14:02:04.925482 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_2072
I0418 14:02:04.925695 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_2072.
I0418 14:02:11.464817 139541457393408 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.07853898406028748, loss=0.04245892912149429
I0418 14:02:33.875447 139541432215296 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.04104166105389595, loss=0.04507433995604515
I0418 14:02:56.312606 139541457393408 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.047144923359155655, loss=0.04681156575679779
I0418 14:03:19.069297 139541432215296 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.024664632976055145, loss=0.042619556188583374
I0418 14:03:41.789201 139541457393408 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05758408084511757, loss=0.04766683652997017
I0418 14:04:04.610366 139541432215296 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.028926735743880272, loss=0.043657418340444565
I0418 14:04:27.497180 139541457393408 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.023303376510739326, loss=0.039118386805057526
I0418 14:04:50.330434 139541432215296 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.04061497002840042, loss=0.051781635731458664
I0418 14:05:13.025811 139541457393408 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.03948496654629707, loss=0.042071182280778885
I0418 14:05:35.887810 139541432215296 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0482528917491436, loss=0.04698118567466736
I0418 14:05:58.846064 139541457393408 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.08614286035299301, loss=0.040641218423843384
I0418 14:06:05.128525 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:07:16.322832 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:07:18.890319 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:07:21.327474 139726215075648 submission_runner.py:406] Time since start: 1185.44s, 	Step: 3128, 	{'train/accuracy': 0.9883566498756409, 'train/loss': 0.04051351174712181, 'train/mean_average_precision': 0.18772714859422263, 'validation/accuracy': 0.9855082631111145, 'validation/loss': 0.049872659146785736, 'validation/mean_average_precision': 0.1669891690933493, 'validation/num_examples': 43793, 'test/accuracy': 0.984613299369812, 'test/loss': 0.052558328956365585, 'test/mean_average_precision': 0.168492747074537, 'test/num_examples': 43793, 'score': 740.6950356960297, 'total_duration': 1185.44065117836, 'accumulated_submission_time': 740.6950356960297, 'accumulated_eval_time': 444.3036472797394, 'accumulated_logging_time': 0.4156975746154785}
I0418 14:07:21.334722 139541432215296 logging_writer.py:48] [3128] accumulated_eval_time=444.303647, accumulated_logging_time=0.415698, accumulated_submission_time=740.695036, global_step=3128, preemption_count=0, score=740.695036, test/accuracy=0.984613, test/loss=0.052558, test/mean_average_precision=0.168493, test/num_examples=43793, total_duration=1185.440651, train/accuracy=0.988357, train/loss=0.040514, train/mean_average_precision=0.187727, validation/accuracy=0.985508, validation/loss=0.049873, validation/mean_average_precision=0.166989, validation/num_examples=43793
I0418 14:07:21.366593 139726215075648 checkpoints.py:356] Saving checkpoint at step: 3128
I0418 14:07:21.466232 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_3128
I0418 14:07:21.466755 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_3128.
I0418 14:07:38.351562 139541457393408 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.03609774634242058, loss=0.03893730789422989
I0418 14:08:01.489205 139541423822592 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.027567733079195023, loss=0.04305804893374443
I0418 14:08:24.906806 139541457393408 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.023276416584849358, loss=0.04022199660539627
I0418 14:08:49.619997 139541423822592 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05331134423613548, loss=0.03831374645233154
I0418 14:09:12.609179 139541457393408 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03383156284689903, loss=0.044104259461164474
I0418 14:09:35.472925 139541423822592 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.020076651126146317, loss=0.04148045554757118
I0418 14:09:58.384212 139541457393408 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.019507311284542084, loss=0.04397869482636452
I0418 14:10:21.333767 139541423822592 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.03685823827981949, loss=0.04436160624027252
I0418 14:10:44.541395 139541457393408 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01477018278092146, loss=0.038073353469371796
I0418 14:11:07.844030 139541423822592 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.019800642505288124, loss=0.039234958589076996
I0418 14:11:21.535636 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:12:32.813047 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:12:35.317646 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:12:37.766277 139726215075648 submission_runner.py:406] Time since start: 1501.88s, 	Step: 4161, 	{'train/accuracy': 0.9887091517448425, 'train/loss': 0.038798537105321884, 'train/mean_average_precision': 0.22767513262649647, 'validation/accuracy': 0.9856446981430054, 'validation/loss': 0.04899347573518753, 'validation/mean_average_precision': 0.18623011023377375, 'validation/num_examples': 43793, 'test/accuracy': 0.9847885370254517, 'test/loss': 0.05147777497768402, 'test/mean_average_precision': 0.19144040761263081, 'test/num_examples': 43793, 'score': 980.7557168006897, 'total_duration': 1501.8794567584991, 'accumulated_submission_time': 980.7557168006897, 'accumulated_eval_time': 520.5342631340027, 'accumulated_logging_time': 0.5553562641143799}
I0418 14:12:37.773876 139541457393408 logging_writer.py:48] [4161] accumulated_eval_time=520.534263, accumulated_logging_time=0.555356, accumulated_submission_time=980.755717, global_step=4161, preemption_count=0, score=980.755717, test/accuracy=0.984789, test/loss=0.051478, test/mean_average_precision=0.191440, test/num_examples=43793, total_duration=1501.879457, train/accuracy=0.988709, train/loss=0.038799, train/mean_average_precision=0.227675, validation/accuracy=0.985645, validation/loss=0.048993, validation/mean_average_precision=0.186230, validation/num_examples=43793
I0418 14:12:37.807395 139726215075648 checkpoints.py:356] Saving checkpoint at step: 4161
I0418 14:12:37.901171 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_4161
I0418 14:12:37.901369 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_4161.
I0418 14:12:47.009197 139541423822592 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.02192043699324131, loss=0.04607323557138443
I0418 14:13:09.865090 139541339961088 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0214572511613369, loss=0.039781827479600906
I0418 14:13:32.517068 139541423822592 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.019056379795074463, loss=0.03992714732885361
I0418 14:13:55.221912 139541339961088 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013667983934283257, loss=0.03940829634666443
I0418 14:14:18.482013 139541423822592 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.013930353336036205, loss=0.04080362617969513
I0418 14:14:41.564030 139541339961088 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.01606205850839615, loss=0.041909463703632355
I0418 14:15:04.727344 139541423822592 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01975969225168228, loss=0.04045885056257248
I0418 14:15:27.967615 139541339961088 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.021963756531476974, loss=0.041908372193574905
I0418 14:15:51.626475 139541423822592 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.023099252954125404, loss=0.045933328568935394
I0418 14:16:15.019076 139541339961088 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.013230611570179462, loss=0.03828117251396179
I0418 14:16:37.910001 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:17:49.594261 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:17:52.104622 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:17:54.560307 139726215075648 submission_runner.py:406] Time since start: 1818.67s, 	Step: 5199, 	{'train/accuracy': 0.988876461982727, 'train/loss': 0.037784282118082047, 'train/mean_average_precision': 0.2526714989221468, 'validation/accuracy': 0.9860400557518005, 'validation/loss': 0.04761360585689545, 'validation/mean_average_precision': 0.2131045247496872, 'validation/num_examples': 43793, 'test/accuracy': 0.9850951433181763, 'test/loss': 0.05040232837200165, 'test/mean_average_precision': 0.2157578062040597, 'test/num_examples': 43793, 'score': 1220.7551732063293, 'total_duration': 1818.6734614372253, 'accumulated_submission_time': 1220.7551732063293, 'accumulated_eval_time': 597.1845164299011, 'accumulated_logging_time': 0.6907846927642822}
I0418 14:17:54.568263 139541423822592 logging_writer.py:48] [5199] accumulated_eval_time=597.184516, accumulated_logging_time=0.690785, accumulated_submission_time=1220.755173, global_step=5199, preemption_count=0, score=1220.755173, test/accuracy=0.985095, test/loss=0.050402, test/mean_average_precision=0.215758, test/num_examples=43793, total_duration=1818.673461, train/accuracy=0.988876, train/loss=0.037784, train/mean_average_precision=0.252671, validation/accuracy=0.986040, validation/loss=0.047614, validation/mean_average_precision=0.213105, validation/num_examples=43793
I0418 14:17:54.602398 139726215075648 checkpoints.py:356] Saving checkpoint at step: 5199
I0418 14:17:54.691431 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_5199
I0418 14:17:54.691630 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_5199.
I0418 14:17:55.188440 139541339961088 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.01548020076006651, loss=0.04126924276351929
I0418 14:18:18.239280 139541331568384 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01835356280207634, loss=0.0394638366997242
I0418 14:18:40.937650 139541339961088 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.01545250415802002, loss=0.03979092836380005
I0418 14:19:03.639945 139541331568384 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014108587987720966, loss=0.04468246549367905
I0418 14:19:26.626966 139541339961088 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.021469129249453545, loss=0.03865617886185646
I0418 14:19:49.535360 139541331568384 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.014052926562726498, loss=0.04103721305727959
I0418 14:20:12.356638 139541339961088 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.015882765874266624, loss=0.03991786763072014
I0418 14:20:35.210983 139541331568384 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.01175320241600275, loss=0.039128247648477554
I0418 14:20:58.207854 139541339961088 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012306599877774715, loss=0.03918059542775154
I0418 14:21:21.351067 139541331568384 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01578347012400627, loss=0.038184311240911484
I0418 14:21:44.229893 139541339961088 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.015427898615598679, loss=0.04126249998807907
I0418 14:21:54.822728 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:23:07.357998 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:23:09.915569 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:23:12.376290 139726215075648 submission_runner.py:406] Time since start: 2136.49s, 	Step: 6247, 	{'train/accuracy': 0.9893081784248352, 'train/loss': 0.03622153028845787, 'train/mean_average_precision': 0.27382793817675777, 'validation/accuracy': 0.9862061142921448, 'validation/loss': 0.0464504174888134, 'validation/mean_average_precision': 0.2182739373091993, 'validation/num_examples': 43793, 'test/accuracy': 0.9853546023368835, 'test/loss': 0.049003567546606064, 'test/mean_average_precision': 0.23147248334731305, 'test/num_examples': 43793, 'score': 1460.8779428005219, 'total_duration': 2136.489456653595, 'accumulated_submission_time': 1460.8779428005219, 'accumulated_eval_time': 674.7380316257477, 'accumulated_logging_time': 0.8224136829376221}
I0418 14:23:12.384739 139541331568384 logging_writer.py:48] [6247] accumulated_eval_time=674.738032, accumulated_logging_time=0.822414, accumulated_submission_time=1460.877943, global_step=6247, preemption_count=0, score=1460.877943, test/accuracy=0.985355, test/loss=0.049004, test/mean_average_precision=0.231472, test/num_examples=43793, total_duration=2136.489457, train/accuracy=0.989308, train/loss=0.036222, train/mean_average_precision=0.273828, validation/accuracy=0.986206, validation/loss=0.046450, validation/mean_average_precision=0.218274, validation/num_examples=43793
I0418 14:23:12.416299 139726215075648 checkpoints.py:356] Saving checkpoint at step: 6247
I0418 14:23:12.500785 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_6247
I0418 14:23:12.501313 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_6247.
I0418 14:23:24.849035 139541339961088 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.016608713194727898, loss=0.042664535343647
I0418 14:23:47.765065 139541323175680 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.014462940394878387, loss=0.03919459506869316
I0418 14:24:10.356947 139541339961088 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.023057179525494576, loss=0.03882962837815285
I0418 14:24:33.111554 139541323175680 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.014799230732023716, loss=0.035856086760759354
I0418 14:24:55.906364 139541339961088 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01355733722448349, loss=0.04056199640035629
I0418 14:25:18.763014 139541323175680 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.012383798137307167, loss=0.0421675480902195
I0418 14:25:41.580761 139541339961088 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.019112499430775642, loss=0.04090755805373192
I0418 14:26:04.349254 139541323175680 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.013433584943413734, loss=0.03690585866570473
I0418 14:26:27.226302 139541339961088 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0118543216958642, loss=0.03883189707994461
I0418 14:26:49.752616 139541323175680 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.017625978216528893, loss=0.041857946664094925
I0418 14:27:12.381762 139541339961088 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.013130401261150837, loss=0.03913521766662598
I0418 14:27:12.621211 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:28:23.328603 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:28:25.816398 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:28:28.245669 139726215075648 submission_runner.py:406] Time since start: 2452.36s, 	Step: 7302, 	{'train/accuracy': 0.9896095991134644, 'train/loss': 0.03491789847612381, 'train/mean_average_precision': 0.320480503467046, 'validation/accuracy': 0.9863514304161072, 'validation/loss': 0.04598342254757881, 'validation/mean_average_precision': 0.23621090204519238, 'validation/num_examples': 43793, 'test/accuracy': 0.985480546951294, 'test/loss': 0.04873369261622429, 'test/mean_average_precision': 0.23459215802370503, 'test/num_examples': 43793, 'score': 1700.9895386695862, 'total_duration': 2452.3588359355927, 'accumulated_submission_time': 1700.9895386695862, 'accumulated_eval_time': 750.3624360561371, 'accumulated_logging_time': 0.947821855545044}
I0418 14:28:28.252986 139541323175680 logging_writer.py:48] [7302] accumulated_eval_time=750.362436, accumulated_logging_time=0.947822, accumulated_submission_time=1700.989539, global_step=7302, preemption_count=0, score=1700.989539, test/accuracy=0.985481, test/loss=0.048734, test/mean_average_precision=0.234592, test/num_examples=43793, total_duration=2452.358836, train/accuracy=0.989610, train/loss=0.034918, train/mean_average_precision=0.320481, validation/accuracy=0.986351, validation/loss=0.045983, validation/mean_average_precision=0.236211, validation/num_examples=43793
I0418 14:28:28.285841 139726215075648 checkpoints.py:356] Saving checkpoint at step: 7302
I0418 14:28:28.370924 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_7302
I0418 14:28:28.371448 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_7302.
I0418 14:28:50.770482 139541339961088 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.014242077246308327, loss=0.038980014622211456
I0418 14:29:13.521803 139541314782976 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.01124509796500206, loss=0.03740360587835312
I0418 14:29:36.033291 139541339961088 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.011581496335566044, loss=0.0381162129342556
I0418 14:29:58.436093 139541314782976 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.012116578407585621, loss=0.03849932923913002
I0418 14:30:21.014953 139541339961088 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.013544348068535328, loss=0.03548199683427811
I0418 14:30:43.687091 139541314782976 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.010820785537362099, loss=0.03643817827105522
I0418 14:31:06.527985 139541339961088 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.011915063485503197, loss=0.035496361553668976
I0418 14:31:29.202251 139541314782976 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.013038418255746365, loss=0.037348296493291855
I0418 14:31:51.935835 139541339961088 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.015204115770757198, loss=0.03682529553771019
I0418 14:32:14.887502 139541314782976 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.012456011958420277, loss=0.040984027087688446
I0418 14:32:28.430721 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:33:40.539973 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:33:43.098603 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:33:45.574960 139726215075648 submission_runner.py:406] Time since start: 2769.69s, 	Step: 8360, 	{'train/accuracy': 0.989924430847168, 'train/loss': 0.033730652183294296, 'train/mean_average_precision': 0.33401597766672714, 'validation/accuracy': 0.9864638447761536, 'validation/loss': 0.0457194484770298, 'validation/mean_average_precision': 0.23278031044521025, 'validation/num_examples': 43793, 'test/accuracy': 0.9856043457984924, 'test/loss': 0.04847075790166855, 'test/mean_average_precision': 0.23578345642332746, 'test/num_examples': 43793, 'score': 1941.0404896736145, 'total_duration': 2769.6881415843964, 'accumulated_submission_time': 1941.0404896736145, 'accumulated_eval_time': 827.5066411495209, 'accumulated_logging_time': 1.0740158557891846}
I0418 14:33:45.582870 139541339961088 logging_writer.py:48] [8360] accumulated_eval_time=827.506641, accumulated_logging_time=1.074016, accumulated_submission_time=1941.040490, global_step=8360, preemption_count=0, score=1941.040490, test/accuracy=0.985604, test/loss=0.048471, test/mean_average_precision=0.235783, test/num_examples=43793, total_duration=2769.688142, train/accuracy=0.989924, train/loss=0.033731, train/mean_average_precision=0.334016, validation/accuracy=0.986464, validation/loss=0.045719, validation/mean_average_precision=0.232780, validation/num_examples=43793
I0418 14:33:45.614927 139726215075648 checkpoints.py:356] Saving checkpoint at step: 8360
I0418 14:33:45.699869 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_8360
I0418 14:33:45.700206 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_8360.
I0418 14:33:55.129808 139541314782976 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.012828859500586987, loss=0.037406593561172485
I0418 14:34:18.232573 139541306390272 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.012198825366795063, loss=0.03809570148587227
I0418 14:34:41.751705 139541314782976 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.012132290750741959, loss=0.039746493101119995
I0418 14:35:04.693746 139541306390272 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0101621700450778, loss=0.03472023457288742
I0418 14:35:27.333321 139541314782976 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.011183622293174267, loss=0.036648862063884735
I0418 14:35:50.008709 139541306390272 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.011162812821567059, loss=0.038677893579006195
I0418 14:36:12.812911 139541314782976 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.008739407174289227, loss=0.034951917827129364
I0418 14:36:35.358433 139541306390272 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.011435983702540398, loss=0.03606899455189705
I0418 14:36:58.060021 139541314782976 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.010352328419685364, loss=0.03633637726306915
I0418 14:37:20.607701 139541306390272 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.010066943243145943, loss=0.03736204281449318
I0418 14:37:43.002940 139541314782976 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.01107827853411436, loss=0.03439491242170334
I0418 14:37:45.742134 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:38:56.550297 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:38:59.048845 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:39:01.472408 139726215075648 submission_runner.py:406] Time since start: 3085.59s, 	Step: 9413, 	{'train/accuracy': 0.9901948571205139, 'train/loss': 0.03297201544046402, 'train/mean_average_precision': 0.34387366857945734, 'validation/accuracy': 0.9865726828575134, 'validation/loss': 0.045427896082401276, 'validation/mean_average_precision': 0.24459801221863414, 'validation/num_examples': 43793, 'test/accuracy': 0.9856898784637451, 'test/loss': 0.04810192808508873, 'test/mean_average_precision': 0.24544142825888274, 'test/num_examples': 43793, 'score': 2181.0742032527924, 'total_duration': 3085.58557677269, 'accumulated_submission_time': 2181.0742032527924, 'accumulated_eval_time': 903.236873626709, 'accumulated_logging_time': 1.199629306793213}
I0418 14:39:01.480323 139541306390272 logging_writer.py:48] [9413] accumulated_eval_time=903.236874, accumulated_logging_time=1.199629, accumulated_submission_time=2181.074203, global_step=9413, preemption_count=0, score=2181.074203, test/accuracy=0.985690, test/loss=0.048102, test/mean_average_precision=0.245441, test/num_examples=43793, total_duration=3085.585577, train/accuracy=0.990195, train/loss=0.032972, train/mean_average_precision=0.343874, validation/accuracy=0.986573, validation/loss=0.045428, validation/mean_average_precision=0.244598, validation/num_examples=43793
I0418 14:39:01.513487 139726215075648 checkpoints.py:356] Saving checkpoint at step: 9413
I0418 14:39:01.595727 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_9413
I0418 14:39:01.596087 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_9413.
I0418 14:39:21.424267 139541314782976 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.011491338722407818, loss=0.036781225353479385
I0418 14:39:43.936085 139541297997568 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01275106891989708, loss=0.0395822636783123
I0418 14:40:06.999979 139541314782976 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.01181836985051632, loss=0.03930845111608505
I0418 14:40:30.234377 139541297997568 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.010732647962868214, loss=0.03341851383447647
I0418 14:40:53.293829 139541314782976 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.010458005592226982, loss=0.03674111142754555
I0418 14:41:16.368903 139541297997568 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.012768369168043137, loss=0.03667785972356796
I0418 14:41:39.803637 139541314782976 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.010803836397826672, loss=0.036083463579416275
I0418 14:42:03.460259 139541297997568 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.014344895258545876, loss=0.039549242705106735
I0418 14:42:26.837446 139541314782976 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.012485919520258904, loss=0.03484692797064781
I0418 14:42:49.781832 139541297997568 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.011497019790112972, loss=0.036661259829998016
I0418 14:43:01.766416 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:44:11.710005 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:44:14.431025 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:44:16.886759 139726215075648 submission_runner.py:406] Time since start: 3401.00s, 	Step: 10454, 	{'train/accuracy': 0.9908419847488403, 'train/loss': 0.030762428417801857, 'train/mean_average_precision': 0.4044914477898873, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.044601764529943466, 'validation/mean_average_precision': 0.24822118088761946, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.0471479631960392, 'test/mean_average_precision': 0.2540349606948588, 'test/num_examples': 43793, 'score': 2421.2353365421295, 'total_duration': 3400.999923467636, 'accumulated_submission_time': 2421.2353365421295, 'accumulated_eval_time': 978.3571648597717, 'accumulated_logging_time': 1.3236899375915527}
I0418 14:44:16.894495 139541314782976 logging_writer.py:48] [10454] accumulated_eval_time=978.357165, accumulated_logging_time=1.323690, accumulated_submission_time=2421.235337, global_step=10454, preemption_count=0, score=2421.235337, test/accuracy=0.985922, test/loss=0.047148, test/mean_average_precision=0.254035, test/num_examples=43793, total_duration=3400.999923, train/accuracy=0.990842, train/loss=0.030762, train/mean_average_precision=0.404491, validation/accuracy=0.986714, validation/loss=0.044602, validation/mean_average_precision=0.248221, validation/num_examples=43793
I0418 14:44:16.927651 139726215075648 checkpoints.py:356] Saving checkpoint at step: 10454
I0418 14:44:17.026835 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_10454
I0418 14:44:17.027041 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_10454.
I0418 14:44:27.748243 139541297997568 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.011358932591974735, loss=0.034967418760061264
I0418 14:44:50.449985 139541289604864 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.014350919984281063, loss=0.038335707038640976
I0418 14:45:13.327407 139541297997568 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.009873447008430958, loss=0.03362470492720604
I0418 14:45:35.945030 139541289604864 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0107278972864151, loss=0.03543375805020332
I0418 14:45:58.395752 139541297997568 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.01098972000181675, loss=0.032236047089099884
I0418 14:46:21.053209 139541289604864 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.011244750581681728, loss=0.03310614451766014
I0418 14:46:43.800275 139541297997568 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.011331038549542427, loss=0.035336513072252274
I0418 14:47:06.332329 139541289604864 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.011668836697936058, loss=0.03694375231862068
I0418 14:47:28.716926 139541297997568 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.012814070098102093, loss=0.03692870959639549
I0418 14:47:51.042603 139541289604864 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.012216751463711262, loss=0.035985708236694336
I0418 14:48:13.730520 139541297997568 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.010473176836967468, loss=0.03534819930791855
I0418 14:48:17.100467 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:49:26.715962 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:49:29.243197 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:49:31.658640 139726215075648 submission_runner.py:406] Time since start: 3715.77s, 	Step: 11516, 	{'train/accuracy': 0.9908264875411987, 'train/loss': 0.030532315373420715, 'train/mean_average_precision': 0.4059561968709382, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.04469451308250427, 'validation/mean_average_precision': 0.25491871245788117, 'validation/num_examples': 43793, 'test/accuracy': 0.9859472513198853, 'test/loss': 0.04723986238241196, 'test/mean_average_precision': 0.2549476054121646, 'test/num_examples': 43793, 'score': 2661.300474882126, 'total_duration': 3715.7718086242676, 'accumulated_submission_time': 2661.300474882126, 'accumulated_eval_time': 1052.9152913093567, 'accumulated_logging_time': 1.464315414428711}
I0418 14:49:31.666645 139541289604864 logging_writer.py:48] [11516] accumulated_eval_time=1052.915291, accumulated_logging_time=1.464315, accumulated_submission_time=2661.300475, global_step=11516, preemption_count=0, score=2661.300475, test/accuracy=0.985947, test/loss=0.047240, test/mean_average_precision=0.254948, test/num_examples=43793, total_duration=3715.771809, train/accuracy=0.990826, train/loss=0.030532, train/mean_average_precision=0.405956, validation/accuracy=0.986720, validation/loss=0.044695, validation/mean_average_precision=0.254919, validation/num_examples=43793
I0418 14:49:31.705353 139726215075648 checkpoints.py:356] Saving checkpoint at step: 11516
I0418 14:49:31.789358 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_11516
I0418 14:49:31.789726 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_11516.
I0418 14:49:50.959725 139541297997568 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.010176061652600765, loss=0.034548841416835785
I0418 14:50:13.881840 139541281212160 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.010536493733525276, loss=0.033601731061935425
I0418 14:50:36.751191 139541297997568 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.009976235218346119, loss=0.03175613284111023
I0418 14:50:59.560900 139541281212160 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.011910513043403625, loss=0.03503991663455963
I0418 14:51:22.116953 139726215075648 spec.py:298] Evaluating on the training split.
I0418 14:52:31.944198 139726215075648 spec.py:310] Evaluating on the validation split.
I0418 14:52:34.463381 139726215075648 spec.py:326] Evaluating on the test split.
I0418 14:52:36.924792 139726215075648 submission_runner.py:406] Time since start: 3901.04s, 	Step: 12000, 	{'train/accuracy': 0.9908080697059631, 'train/loss': 0.030198466032743454, 'train/mean_average_precision': 0.42101365919064165, 'validation/accuracy': 0.9866688847541809, 'validation/loss': 0.04500734061002731, 'validation/mean_average_precision': 0.25467206344181165, 'validation/num_examples': 43793, 'test/accuracy': 0.9858132600784302, 'test/loss': 0.04762967675924301, 'test/mean_average_precision': 0.2500787062679182, 'test/num_examples': 43793, 'score': 2771.6236226558685, 'total_duration': 3901.0379631519318, 'accumulated_submission_time': 2771.6236226558685, 'accumulated_eval_time': 1127.723085641861, 'accumulated_logging_time': 1.595766305923462}
I0418 14:52:36.932993 139541297997568 logging_writer.py:48] [12000] accumulated_eval_time=1127.723086, accumulated_logging_time=1.595766, accumulated_submission_time=2771.623623, global_step=12000, preemption_count=0, score=2771.623623, test/accuracy=0.985813, test/loss=0.047630, test/mean_average_precision=0.250079, test/num_examples=43793, total_duration=3901.037963, train/accuracy=0.990808, train/loss=0.030198, train/mean_average_precision=0.421014, validation/accuracy=0.986669, validation/loss=0.045007, validation/mean_average_precision=0.254672, validation/num_examples=43793
I0418 14:52:36.965850 139726215075648 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:52:37.063759 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000
I0418 14:52:37.064139 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:52:37.071553 139541281212160 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2771.623623
I0418 14:52:37.097481 139726215075648 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:52:37.251126 139726215075648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000
I0418 14:52:37.251487 139726215075648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:52:37.380489 139726215075648 submission_runner.py:567] Tuning trial 1/1
I0418 14:52:37.380725 139726215075648 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 14:52:37.381816 139726215075648 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.45287150144577026, 'train/loss': 0.775056004524231, 'train/mean_average_precision': 0.022378059176111787, 'validation/accuracy': 0.4569682478904724, 'validation/loss': 0.7732618451118469, 'validation/mean_average_precision': 0.02595133389640027, 'validation/num_examples': 43793, 'test/accuracy': 0.4588867425918579, 'test/loss': 0.7721359729766846, 'test/mean_average_precision': 0.02775132771831757, 'test/num_examples': 43793, 'score': 20.257025480270386, 'total_duration': 234.73008728027344, 'accumulated_submission_time': 20.257025480270386, 'accumulated_eval_time': 214.47287821769714, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1041, {'train/accuracy': 0.9869406223297119, 'train/loss': 0.04963434115052223, 'train/mean_average_precision': 0.06954605093557754, 'validation/accuracy': 0.9843891263008118, 'validation/loss': 0.0589640736579895, 'validation/mean_average_precision': 0.06993176791245509, 'validation/num_examples': 43793, 'test/accuracy': 0.9833741188049316, 'test/loss': 0.06219257414340973, 'test/mean_average_precision': 0.06927957111791591, 'test/num_examples': 43793, 'score': 260.37417006492615, 'total_duration': 552.0596053600311, 'accumulated_submission_time': 260.37417006492615, 'accumulated_eval_time': 291.5334506034851, 'accumulated_logging_time': 0.14373445510864258, 'global_step': 1041, 'preemption_count': 0}), (2072, {'train/accuracy': 0.9878095984458923, 'train/loss': 0.04332897067070007, 'train/mean_average_precision': 0.14847212878061883, 'validation/accuracy': 0.9850406646728516, 'validation/loss': 0.05267998203635216, 'validation/mean_average_precision': 0.14227792592708427, 'validation/num_examples': 43793, 'test/accuracy': 0.9840472340583801, 'test/loss': 0.05561130866408348, 'test/mean_average_precision': 0.14176738854946636, 'test/num_examples': 43793, 'score': 500.5006582736969, 'total_duration': 868.9042737483978, 'accumulated_submission_time': 500.5006582736969, 'accumulated_eval_time': 368.10473227500916, 'accumulated_logging_time': 0.2807760238647461, 'global_step': 2072, 'preemption_count': 0}), (3128, {'train/accuracy': 0.9883566498756409, 'train/loss': 0.04051351174712181, 'train/mean_average_precision': 0.18772714859422263, 'validation/accuracy': 0.9855082631111145, 'validation/loss': 0.049872659146785736, 'validation/mean_average_precision': 0.1669891690933493, 'validation/num_examples': 43793, 'test/accuracy': 0.984613299369812, 'test/loss': 0.052558328956365585, 'test/mean_average_precision': 0.168492747074537, 'test/num_examples': 43793, 'score': 740.6950356960297, 'total_duration': 1185.44065117836, 'accumulated_submission_time': 740.6950356960297, 'accumulated_eval_time': 444.3036472797394, 'accumulated_logging_time': 0.4156975746154785, 'global_step': 3128, 'preemption_count': 0}), (4161, {'train/accuracy': 0.9887091517448425, 'train/loss': 0.038798537105321884, 'train/mean_average_precision': 0.22767513262649647, 'validation/accuracy': 0.9856446981430054, 'validation/loss': 0.04899347573518753, 'validation/mean_average_precision': 0.18623011023377375, 'validation/num_examples': 43793, 'test/accuracy': 0.9847885370254517, 'test/loss': 0.05147777497768402, 'test/mean_average_precision': 0.19144040761263081, 'test/num_examples': 43793, 'score': 980.7557168006897, 'total_duration': 1501.8794567584991, 'accumulated_submission_time': 980.7557168006897, 'accumulated_eval_time': 520.5342631340027, 'accumulated_logging_time': 0.5553562641143799, 'global_step': 4161, 'preemption_count': 0}), (5199, {'train/accuracy': 0.988876461982727, 'train/loss': 0.037784282118082047, 'train/mean_average_precision': 0.2526714989221468, 'validation/accuracy': 0.9860400557518005, 'validation/loss': 0.04761360585689545, 'validation/mean_average_precision': 0.2131045247496872, 'validation/num_examples': 43793, 'test/accuracy': 0.9850951433181763, 'test/loss': 0.05040232837200165, 'test/mean_average_precision': 0.2157578062040597, 'test/num_examples': 43793, 'score': 1220.7551732063293, 'total_duration': 1818.6734614372253, 'accumulated_submission_time': 1220.7551732063293, 'accumulated_eval_time': 597.1845164299011, 'accumulated_logging_time': 0.6907846927642822, 'global_step': 5199, 'preemption_count': 0}), (6247, {'train/accuracy': 0.9893081784248352, 'train/loss': 0.03622153028845787, 'train/mean_average_precision': 0.27382793817675777, 'validation/accuracy': 0.9862061142921448, 'validation/loss': 0.0464504174888134, 'validation/mean_average_precision': 0.2182739373091993, 'validation/num_examples': 43793, 'test/accuracy': 0.9853546023368835, 'test/loss': 0.049003567546606064, 'test/mean_average_precision': 0.23147248334731305, 'test/num_examples': 43793, 'score': 1460.8779428005219, 'total_duration': 2136.489456653595, 'accumulated_submission_time': 1460.8779428005219, 'accumulated_eval_time': 674.7380316257477, 'accumulated_logging_time': 0.8224136829376221, 'global_step': 6247, 'preemption_count': 0}), (7302, {'train/accuracy': 0.9896095991134644, 'train/loss': 0.03491789847612381, 'train/mean_average_precision': 0.320480503467046, 'validation/accuracy': 0.9863514304161072, 'validation/loss': 0.04598342254757881, 'validation/mean_average_precision': 0.23621090204519238, 'validation/num_examples': 43793, 'test/accuracy': 0.985480546951294, 'test/loss': 0.04873369261622429, 'test/mean_average_precision': 0.23459215802370503, 'test/num_examples': 43793, 'score': 1700.9895386695862, 'total_duration': 2452.3588359355927, 'accumulated_submission_time': 1700.9895386695862, 'accumulated_eval_time': 750.3624360561371, 'accumulated_logging_time': 0.947821855545044, 'global_step': 7302, 'preemption_count': 0}), (8360, {'train/accuracy': 0.989924430847168, 'train/loss': 0.033730652183294296, 'train/mean_average_precision': 0.33401597766672714, 'validation/accuracy': 0.9864638447761536, 'validation/loss': 0.0457194484770298, 'validation/mean_average_precision': 0.23278031044521025, 'validation/num_examples': 43793, 'test/accuracy': 0.9856043457984924, 'test/loss': 0.04847075790166855, 'test/mean_average_precision': 0.23578345642332746, 'test/num_examples': 43793, 'score': 1941.0404896736145, 'total_duration': 2769.6881415843964, 'accumulated_submission_time': 1941.0404896736145, 'accumulated_eval_time': 827.5066411495209, 'accumulated_logging_time': 1.0740158557891846, 'global_step': 8360, 'preemption_count': 0}), (9413, {'train/accuracy': 0.9901948571205139, 'train/loss': 0.03297201544046402, 'train/mean_average_precision': 0.34387366857945734, 'validation/accuracy': 0.9865726828575134, 'validation/loss': 0.045427896082401276, 'validation/mean_average_precision': 0.24459801221863414, 'validation/num_examples': 43793, 'test/accuracy': 0.9856898784637451, 'test/loss': 0.04810192808508873, 'test/mean_average_precision': 0.24544142825888274, 'test/num_examples': 43793, 'score': 2181.0742032527924, 'total_duration': 3085.58557677269, 'accumulated_submission_time': 2181.0742032527924, 'accumulated_eval_time': 903.236873626709, 'accumulated_logging_time': 1.199629306793213, 'global_step': 9413, 'preemption_count': 0}), (10454, {'train/accuracy': 0.9908419847488403, 'train/loss': 0.030762428417801857, 'train/mean_average_precision': 0.4044914477898873, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.044601764529943466, 'validation/mean_average_precision': 0.24822118088761946, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.0471479631960392, 'test/mean_average_precision': 0.2540349606948588, 'test/num_examples': 43793, 'score': 2421.2353365421295, 'total_duration': 3400.999923467636, 'accumulated_submission_time': 2421.2353365421295, 'accumulated_eval_time': 978.3571648597717, 'accumulated_logging_time': 1.3236899375915527, 'global_step': 10454, 'preemption_count': 0}), (11516, {'train/accuracy': 0.9908264875411987, 'train/loss': 0.030532315373420715, 'train/mean_average_precision': 0.4059561968709382, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.04469451308250427, 'validation/mean_average_precision': 0.25491871245788117, 'validation/num_examples': 43793, 'test/accuracy': 0.9859472513198853, 'test/loss': 0.04723986238241196, 'test/mean_average_precision': 0.2549476054121646, 'test/num_examples': 43793, 'score': 2661.300474882126, 'total_duration': 3715.7718086242676, 'accumulated_submission_time': 2661.300474882126, 'accumulated_eval_time': 1052.9152913093567, 'accumulated_logging_time': 1.464315414428711, 'global_step': 11516, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9908080697059631, 'train/loss': 0.030198466032743454, 'train/mean_average_precision': 0.42101365919064165, 'validation/accuracy': 0.9866688847541809, 'validation/loss': 0.04500734061002731, 'validation/mean_average_precision': 0.25467206344181165, 'validation/num_examples': 43793, 'test/accuracy': 0.9858132600784302, 'test/loss': 0.04762967675924301, 'test/mean_average_precision': 0.2500787062679182, 'test/num_examples': 43793, 'score': 2771.6236226558685, 'total_duration': 3901.0379631519318, 'accumulated_submission_time': 2771.6236226558685, 'accumulated_eval_time': 1127.723085641861, 'accumulated_logging_time': 1.595766305923462, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0418 14:52:37.381937 139726215075648 submission_runner.py:570] Timing: 2771.6236226558685
I0418 14:52:37.382011 139726215075648 submission_runner.py:571] ====================
I0418 14:52:37.382153 139726215075648 submission_runner.py:631] Final ogbg score: 2771.6236226558685
