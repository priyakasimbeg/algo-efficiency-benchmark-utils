I0419 00:16:57.914228 140457377568576 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax.
I0419 00:16:57.978798 140457377568576 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0419 00:16:58.759969 140457377568576 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0419 00:16:58.760648 140457377568576 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0419 00:16:58.765036 140457377568576 submission_runner.py:528] Using RNG seed 2357120063
I0419 00:17:01.510874 140457377568576 submission_runner.py:537] --- Tuning run 1/1 ---
I0419 00:17:01.511097 140457377568576 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1.
I0419 00:17:01.511258 140457377568576 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/hparams.json.
I0419 00:17:01.633229 140457377568576 submission_runner.py:232] Initializing dataset.
I0419 00:17:01.633409 140457377568576 submission_runner.py:239] Initializing model.
I0419 00:17:07.162224 140457377568576 submission_runner.py:249] Initializing optimizer.
I0419 00:17:07.813956 140457377568576 submission_runner.py:256] Initializing metrics bundle.
I0419 00:17:07.814125 140457377568576 submission_runner.py:273] Initializing checkpoint and logger.
I0419 00:17:07.815124 140457377568576 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0419 00:17:07.815386 140457377568576 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0419 00:17:07.815454 140457377568576 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0419 00:17:08.446563 140457377568576 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0419 00:17:08.447445 140457377568576 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/flags_0.json.
I0419 00:17:08.454879 140457377568576 submission_runner.py:309] Starting training loop.
I0419 00:17:08.642425 140457377568576 input_pipeline.py:20] Loading split = train-clean-100
I0419 00:17:08.672328 140457377568576 input_pipeline.py:20] Loading split = train-clean-360
I0419 00:17:08.964670 140457377568576 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0419 00:18:10.581148 140282188265216 logging_writer.py:48] [0] global_step=0, grad_norm=58.667091369628906, loss=31.05599594116211
I0419 00:18:10.604669 140457377568576 spec.py:298] Evaluating on the training split.
I0419 00:18:10.701647 140457377568576 input_pipeline.py:20] Loading split = train-clean-100
I0419 00:18:10.727488 140457377568576 input_pipeline.py:20] Loading split = train-clean-360
I0419 00:18:10.994271 140457377568576 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0419 00:18:50.417956 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 00:18:50.475397 140457377568576 input_pipeline.py:20] Loading split = dev-clean
I0419 00:18:50.479767 140457377568576 input_pipeline.py:20] Loading split = dev-other
I0419 00:19:29.679468 140457377568576 spec.py:326] Evaluating on the test split.
I0419 00:19:29.737282 140457377568576 input_pipeline.py:20] Loading split = test-clean
I0419 00:19:57.784395 140457377568576 submission_runner.py:406] Time since start: 169.33s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.050974, dtype=float32), 'train/wer': 0.9467026216604544, 'validation/ctc_loss': DeviceArray(30.297382, dtype=float32), 'validation/wer': 0.915889202983145, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.346977, dtype=float32), 'test/wer': 0.9181646456644933, 'test/num_examples': 2472, 'score': 62.14959669113159, 'total_duration': 169.32798409461975, 'accumulated_submission_time': 62.14959669113159, 'accumulated_eval_time': 107.1782169342041, 'accumulated_logging_time': 0}
I0419 00:19:57.807588 140278480492288 logging_writer.py:48] [1] accumulated_eval_time=107.178217, accumulated_logging_time=0, accumulated_submission_time=62.149597, global_step=1, preemption_count=0, score=62.149597, test/ctc_loss=30.34697723388672, test/num_examples=2472, test/wer=0.918165, total_duration=169.327984, train/ctc_loss=31.050973892211914, train/wer=0.946703, validation/ctc_loss=30.297382354736328, validation/num_examples=5348, validation/wer=0.915889
I0419 00:19:57.978681 140457377568576 checkpoints.py:356] Saving checkpoint at step: 1
I0419 00:19:58.525376 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_1
I0419 00:19:58.526373 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_1.
I0419 00:21:27.580723 140284034889472 logging_writer.py:48] [100] global_step=100, grad_norm=20.61629295349121, loss=15.52087116241455
I0419 00:22:42.811869 140284043282176 logging_writer.py:48] [200] global_step=200, grad_norm=0.7576786875724792, loss=5.961950778961182
I0419 00:23:58.006791 140284034889472 logging_writer.py:48] [300] global_step=300, grad_norm=3.8830015659332275, loss=5.921442031860352
I0419 00:25:13.176771 140284043282176 logging_writer.py:48] [400] global_step=400, grad_norm=3.7155721187591553, loss=5.938098907470703
I0419 00:26:30.889288 140284034889472 logging_writer.py:48] [500] global_step=500, grad_norm=2.408226728439331, loss=5.8929123878479
I0419 00:27:48.325329 140284043282176 logging_writer.py:48] [600] global_step=600, grad_norm=2.061040163040161, loss=5.881865978240967
I0419 00:29:11.007322 140284034889472 logging_writer.py:48] [700] global_step=700, grad_norm=1.7575621604919434, loss=5.8501505851745605
I0419 00:30:29.861176 140284043282176 logging_writer.py:48] [800] global_step=800, grad_norm=0.9278382062911987, loss=5.852701663970947
I0419 00:31:52.140258 140284034889472 logging_writer.py:48] [900] global_step=900, grad_norm=1.1882833242416382, loss=5.8436760902404785
I0419 00:33:13.189339 140284043282176 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.619350790977478, loss=5.859323024749756
I0419 00:34:31.884600 140284841318144 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.11400662362575531, loss=5.792889595031738
I0419 00:35:47.099147 140284832925440 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9561169743537903, loss=5.819122791290283
I0419 00:37:02.389077 140284841318144 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.30891287326812744, loss=5.811628341674805
I0419 00:38:17.633651 140284832925440 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8052644729614258, loss=5.818704128265381
I0419 00:39:32.845396 140284841318144 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2259695678949356, loss=5.77464485168457
I0419 00:40:52.279159 140284832925440 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6856634020805359, loss=5.786570072174072
I0419 00:42:16.406188 140284841318144 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6400988698005676, loss=5.809326648712158
I0419 00:43:36.658922 140284832925440 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5525976419448853, loss=5.824202537536621
I0419 00:45:00.599954 140284841318144 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.594807505607605, loss=5.792414665222168
I0419 00:46:26.077652 140284832925440 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2613093852996826, loss=5.892008304595947
I0419 00:47:49.080324 140280925882112 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1800.0155029296875
I0419 00:49:02.172506 140280242108160 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1866.8287353515625
I0419 00:50:15.299211 140280925882112 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1823.998779296875
I0419 00:51:28.432310 140280242108160 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1788.8212890625
I0419 00:52:41.557668 140280925882112 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1886.084228515625
I0419 00:53:59.287731 140280242108160 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1815.3326416015625
I0419 00:55:21.990226 140280925882112 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1865.740234375
I0419 00:56:41.678783 140280242108160 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1879.437744140625
I0419 00:58:08.504939 140280925882112 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1879.9898681640625
I0419 00:59:32.177335 140280242108160 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1804.2020263671875
I0419 00:59:59.142228 140457377568576 spec.py:298] Evaluating on the training split.
I0419 01:00:25.573441 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 01:01:01.613892 140457377568576 spec.py:326] Evaluating on the test split.
I0419 01:01:18.587132 140457377568576 submission_runner.py:406] Time since start: 2650.13s, 	Step: 3037, 	{'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2462.728982448578, 'total_duration': 2650.130298137665, 'accumulated_submission_time': 2462.728982448578, 'accumulated_eval_time': 186.62124109268188, 'accumulated_logging_time': 0.7472124099731445}
I0419 01:01:18.607099 140284257638144 logging_writer.py:48] [3037] accumulated_eval_time=186.621241, accumulated_logging_time=0.747212, accumulated_submission_time=2462.728982, global_step=3037, preemption_count=0, score=2462.728982, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=2650.130298, train/ctc_loss=1767.6815185546875, train/wer=0.944636, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 01:01:18.785644 140457377568576 checkpoints.py:356] Saving checkpoint at step: 3037
I0419 01:01:19.695338 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_3037
I0419 01:01:19.714129 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_3037.
I0419 01:02:09.600702 140284257638144 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1838.6715087890625
I0419 01:03:22.992255 140284249245440 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1850.3665771484375
I0419 01:04:36.782978 140284257638144 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1793.7095947265625
I0419 01:05:50.511909 140284249245440 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1768.187744140625
I0419 01:07:04.107626 140284257638144 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1914.0054931640625
I0419 01:08:20.513756 140284249245440 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1809.1746826171875
I0419 01:09:39.292602 140284257638144 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1832.6165771484375
I0419 01:11:03.351258 140284249245440 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1782.2210693359375
I0419 01:12:28.267982 140284257638144 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1861.1285400390625
I0419 01:13:51.853108 140284249245440 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1796.73095703125
I0419 01:15:12.751044 140284257638144 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1850.9017333984375
I0419 01:16:31.197410 140284257638144 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1824.128662109375
I0419 01:17:44.505372 140284249245440 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1833.798095703125
I0419 01:18:57.721212 140284257638144 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1845.43115234375
I0419 01:20:10.829819 140284249245440 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1841.5814208984375
I0419 01:21:23.970462 140284257638144 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1833.798095703125
I0419 01:22:42.641289 140284249245440 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1783.3385009765625
I0419 01:24:03.079071 140284257638144 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1866.0123291015625
I0419 01:25:24.116833 140284249245440 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1848.6295166015625
I0419 01:26:50.074017 140284257638144 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1804.96533203125
I0419 01:28:12.261543 140284249245440 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1831.0435791015625
I0419 01:29:31.769070 140284257638144 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1888.5888671875
I0419 01:30:44.982945 140284249245440 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1832.7479248046875
I0419 01:31:58.112298 140284257638144 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1769.77685546875
I0419 01:33:11.538341 140284249245440 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1844.2347412109375
I0419 01:34:25.436997 140284257638144 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1813.789306640625
I0419 01:35:53.141561 140284249245440 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1866.556396484375
I0419 01:37:19.559905 140284257638144 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1886.3623046875
I0419 01:38:46.010164 140284249245440 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1846.2296142578125
I0419 01:40:07.304422 140284257638144 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1877.9212646484375
I0419 01:41:20.343654 140457377568576 spec.py:298] Evaluating on the training split.
I0419 01:41:46.684241 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 01:42:22.174957 140457377568576 spec.py:326] Evaluating on the test split.
I0419 01:42:39.969468 140457377568576 submission_runner.py:406] Time since start: 5131.51s, 	Step: 6087, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4863.317631483078, 'total_duration': 5131.5113253593445, 'accumulated_submission_time': 4863.317631483078, 'accumulated_eval_time': 266.24385237693787, 'accumulated_logging_time': 1.8821711540222168}
I0419 01:42:39.991406 140284257638144 logging_writer.py:48] [6087] accumulated_eval_time=266.243852, accumulated_logging_time=1.882171, accumulated_submission_time=4863.317631, global_step=6087, preemption_count=0, score=4863.317631, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5131.511325, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 01:42:40.205593 140457377568576 checkpoints.py:356] Saving checkpoint at step: 6087
I0419 01:42:41.112504 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_6087
I0419 01:42:41.130756 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_6087.
I0419 01:42:51.417163 140284249245440 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1798.7508544921875
I0419 01:44:07.678983 140284257638144 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1780.7333984375
I0419 01:45:20.918458 140284249245440 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1821.0145263671875
I0419 01:46:35.821246 140284257638144 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1827.6448974609375
I0419 01:47:49.214537 140284249245440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1856.9434814453125
I0419 01:49:07.443076 140284257638144 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1779.1243896484375
I0419 01:50:35.292386 140284249245440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1818.9443359375
I0419 01:52:01.287608 140284257638144 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1866.42041015625
I0419 01:53:26.219364 140284249245440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1811.7353515625
I0419 01:54:46.505211 140284257638144 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1897.97021484375
I0419 01:56:12.795044 140284249245440 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1794.84130859375
I0419 01:57:38.868052 140284257638144 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1778.3829345703125
I0419 01:58:56.345563 140283602278144 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1823.8687744140625
I0419 02:00:09.548766 140283593885440 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1788.196533203125
I0419 02:01:22.719940 140283602278144 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1854.5220947265625
I0419 02:02:39.783523 140283593885440 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1786.9482421875
I0419 02:04:04.540739 140283602278144 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1817.7818603515625
I0419 02:05:21.029490 140283593885440 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1836.1656494140625
I0419 02:06:43.632111 140283602278144 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1814.04638671875
I0419 02:08:06.273524 140283593885440 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1820.4964599609375
I0419 02:09:24.706001 140283602278144 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1765.6268310546875
I0419 02:10:43.888964 140283593885440 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1797.7403564453125
I0419 02:12:01.472199 140283602278144 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1831.4366455078125
I0419 02:13:15.225718 140283593885440 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1860.58740234375
I0419 02:14:28.363164 140283602278144 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1814.818115234375
I0419 02:15:41.535035 140283593885440 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1843.30517578125
I0419 02:16:57.738106 140283602278144 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1756.4200439453125
I0419 02:18:22.000940 140283593885440 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1805.8565673828125
I0419 02:19:45.832443 140283602278144 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1833.2728271484375
I0419 02:21:11.943952 140283593885440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1803.185302734375
I0419 02:22:34.347956 140283602278144 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1760.5269775390625
I0419 02:22:42.017237 140457377568576 spec.py:298] Evaluating on the training split.
I0419 02:23:08.827927 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 02:23:42.860450 140457377568576 spec.py:326] Evaluating on the test split.
I0419 02:24:02.207835 140457377568576 submission_runner.py:406] Time since start: 7613.75s, 	Step: 9110, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7264.160523414612, 'total_duration': 7613.749875068665, 'accumulated_submission_time': 7264.160523414612, 'accumulated_eval_time': 346.4314315319061, 'accumulated_logging_time': 3.0559661388397217}
I0419 02:24:02.227339 140283602278144 logging_writer.py:48] [9110] accumulated_eval_time=346.431432, accumulated_logging_time=3.055966, accumulated_submission_time=7264.160523, global_step=9110, preemption_count=0, score=7264.160523, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7613.749875, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 02:24:02.401313 140457377568576 checkpoints.py:356] Saving checkpoint at step: 9110
I0419 02:24:03.292356 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_9110
I0419 02:24:03.310752 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_9110.
I0419 02:25:09.910451 140283593885440 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1816.749755859375
I0419 02:26:26.442098 140283602278144 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1751.8526611328125
I0419 02:27:39.884430 140283593885440 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1796.1007080078125
I0419 02:28:53.315798 140283602278144 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1773.3321533203125
I0419 02:30:06.724139 140283593885440 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1858.156494140625
I0419 02:31:23.329186 140283602278144 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1795.974609375
I0419 02:32:44.533365 140283593885440 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1833.2728271484375
I0419 02:34:08.652524 140283602278144 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1777.271728515625
I0419 02:35:30.818176 140283593885440 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1837.0880126953125
I0419 02:36:55.602356 140283602278144 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1768.3099365234375
I0419 02:38:12.515528 140283593885440 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1845.9635009765625
I0419 02:39:37.733155 140284257638144 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1789.9468994140625
I0419 02:40:50.830833 140284249245440 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1774.3155517578125
I0419 02:42:04.143807 140284257638144 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1784.20849609375
I0419 02:43:17.375604 140284249245440 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1790.6982421875
I0419 02:44:31.041466 140284257638144 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1842.7745361328125
I0419 02:45:45.136424 140284249245440 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1771.7366943359375
I0419 02:47:10.009291 140284257638144 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1808.1524658203125
I0419 02:48:31.450290 140284249245440 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1851.3033447265625
I0419 02:49:55.089461 140284257638144 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1878.748046875
I0419 02:51:12.880538 140284249245440 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1806.23876953125
I0419 02:52:32.707159 140284257638144 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1906.8768310546875
I0419 02:53:51.825772 140284257638144 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1873.2490234375
I0419 02:55:04.983282 140284249245440 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1881.233154296875
I0419 02:56:18.255613 140284257638144 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1887.614013671875
I0419 02:57:31.714964 140284249245440 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1770.6336669921875
I0419 02:58:46.555389 140284257638144 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1918.164306640625
I0419 03:00:09.312439 140284249245440 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1893.197998046875
I0419 03:01:31.074695 140284257638144 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1817.394775390625
I0419 03:02:59.012296 140284249245440 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1834.32373046875
I0419 03:04:03.727329 140457377568576 spec.py:298] Evaluating on the training split.
I0419 03:04:30.463214 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 03:05:04.923747 140457377568576 spec.py:326] Evaluating on the test split.
I0419 03:05:22.531859 140457377568576 submission_runner.py:406] Time since start: 10094.07s, 	Step: 12179, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9664.539340019226, 'total_duration': 10094.073591470718, 'accumulated_submission_time': 9664.539340019226, 'accumulated_eval_time': 425.23264145851135, 'accumulated_logging_time': 4.165448904037476}
I0419 03:05:22.553748 140284298594048 logging_writer.py:48] [12179] accumulated_eval_time=425.232641, accumulated_logging_time=4.165449, accumulated_submission_time=9664.539340, global_step=12179, preemption_count=0, score=9664.539340, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10094.073591, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 03:05:22.732005 140457377568576 checkpoints.py:356] Saving checkpoint at step: 12179
I0419 03:05:23.654395 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_12179
I0419 03:05:23.672860 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_12179.
I0419 03:05:39.757843 140284290201344 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1906.4508056640625
I0419 03:06:52.855755 140283560314624 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0, loss=1829.86572265625
I0419 03:08:09.297096 140283643234048 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1791.8262939453125
I0419 03:09:22.530998 140283634841344 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1826.7320556640625
I0419 03:10:35.742165 140283643234048 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1767.08935546875
I0419 03:11:49.195784 140283634841344 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1847.8289794921875
I0419 03:13:09.257396 140283643234048 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1821.4031982421875
I0419 03:14:35.158377 140283634841344 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1883.8636474609375
I0419 03:15:59.995963 140283643234048 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1833.1414794921875
I0419 03:17:23.871844 140283634841344 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1777.271728515625
I0419 03:18:49.627604 140283643234048 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1848.6295166015625
I0419 03:20:14.956092 140283634841344 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1837.0880126953125
I0419 03:21:39.662406 140283643234048 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1850.7679443359375
I0419 03:22:52.770948 140283634841344 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1821.2735595703125
I0419 03:24:06.006811 140283643234048 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1875.8570556640625
I0419 03:25:19.197061 140283634841344 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1818.0400390625
I0419 03:26:34.007341 140283643234048 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1808.9190673828125
I0419 03:27:56.770865 140283634841344 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1769.409912109375
I0419 03:29:20.255419 140283643234048 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1827.6448974609375
I0419 03:30:42.509053 140283634841344 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1814.1749267578125
I0419 03:32:09.276223 140283643234048 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1872.2901611328125
I0419 03:33:36.157138 140283634841344 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1860.9932861328125
I0419 03:35:00.449883 140283643234048 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1813.275390625
I0419 03:36:18.952386 140284298594048 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1795.470703125
I0419 03:37:32.366987 140284290201344 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1803.3123779296875
I0419 03:38:45.476104 140284298594048 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1755.0955810546875
I0419 03:40:02.528192 140284290201344 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1769.0430908203125
I0419 03:41:25.468846 140284298594048 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1861.399169921875
I0419 03:42:49.002525 140284290201344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1806.111328125
I0419 03:44:09.478487 140284298594048 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1762.4661865234375
I0419 03:45:24.092972 140457377568576 spec.py:298] Evaluating on the training split.
I0419 03:45:50.973104 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 03:46:25.656731 140457377568576 spec.py:326] Evaluating on the test split.
I0419 03:46:44.383862 140457377568576 submission_runner.py:406] Time since start: 12575.93s, 	Step: 15189, 	{'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12064.92083787918, 'total_duration': 12575.925434827805, 'accumulated_submission_time': 12064.92083787918, 'accumulated_eval_time': 505.5200436115265, 'accumulated_logging_time': 5.313642263412476}
I0419 03:46:44.403845 140284334438144 logging_writer.py:48] [15189] accumulated_eval_time=505.520044, accumulated_logging_time=5.313642, accumulated_submission_time=12064.920838, global_step=15189, preemption_count=0, score=12064.920838, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=12575.925435, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 03:46:44.598273 140457377568576 checkpoints.py:356] Saving checkpoint at step: 15189
I0419 03:46:45.511806 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_15189
I0419 03:46:45.530100 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_15189.
I0419 03:46:54.351281 140284326045440 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1800.395263671875
I0419 03:48:07.564410 140283592877824 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1805.601806640625
I0419 03:49:20.782698 140284326045440 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1830.52001953125
I0419 03:50:37.420632 140283679078144 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1792.328125
I0419 03:51:50.553909 140283670685440 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1775.91552734375
I0419 03:53:03.683853 140283679078144 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1738.7640380859375
I0419 03:54:18.560100 140283670685440 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1859.7763671875
I0419 03:55:40.822659 140283679078144 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1815.847900390625
I0419 03:57:01.813568 140283670685440 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1771.3687744140625
I0419 03:58:19.277993 140283679078144 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.0, loss=1784.7060546875
I0419 03:59:41.312674 140283670685440 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0, loss=1837.8795166015625
I0419 04:01:05.018057 140283679078144 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.0, loss=1888.1708984375
I0419 04:02:33.150324 140283670685440 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0, loss=1787.072998046875
I0419 04:04:01.359530 140283679078144 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1761.8597412109375
I0419 04:05:14.373732 140283670685440 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.0, loss=1854.3875732421875
I0419 04:06:27.870227 140283679078144 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.0, loss=1831.69873046875
I0419 04:07:41.200173 140283670685440 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.0, loss=1895.8619384765625
I0419 04:09:03.239135 140283679078144 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0, loss=1896.985595703125
I0419 04:10:27.721473 140283670685440 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1862.482421875
I0419 04:11:57.224546 140283679078144 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.0, loss=1821.0145263671875
I0419 04:13:23.829560 140283670685440 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.0, loss=1818.29833984375
I0419 04:14:51.933229 140283679078144 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0, loss=1802.5504150390625
I0419 04:16:12.585706 140283670685440 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.0, loss=1813.1468505859375
I0419 04:17:39.029347 140283679078144 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1838.4073486328125
I0419 04:18:56.240305 140283679078144 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0, loss=1810.4541015625
I0419 04:20:09.435804 140283670685440 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0, loss=1796.73095703125
I0419 04:21:23.690841 140283679078144 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.0, loss=1796.3526611328125
I0419 04:22:39.741115 140283670685440 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.0, loss=1806.111328125
I0419 04:24:01.576477 140283679078144 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1831.0435791015625
I0419 04:25:23.420608 140283670685440 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.0, loss=1770.0216064453125
I0419 04:26:46.059471 140457377568576 spec.py:298] Evaluating on the training split.
I0419 04:27:13.102018 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 04:27:49.459433 140457377568576 spec.py:326] Evaluating on the test split.
I0419 04:28:08.017955 140457377568576 submission_runner.py:406] Time since start: 15059.56s, 	Step: 18200, 	{'train/ctc_loss': DeviceArray(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14465.40931725502, 'total_duration': 15059.560095787048, 'accumulated_submission_time': 14465.40931725502, 'accumulated_eval_time': 587.4756178855896, 'accumulated_logging_time': 6.469218492507935}
I0419 04:28:08.038476 140283679078144 logging_writer.py:48] [18200] accumulated_eval_time=587.475618, accumulated_logging_time=6.469218, accumulated_submission_time=14465.409317, global_step=18200, preemption_count=0, score=14465.409317, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=15059.560096, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 04:28:08.212311 140457377568576 checkpoints.py:356] Saving checkpoint at step: 18200
I0419 04:28:09.107973 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_18200
I0419 04:28:09.126319 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_18200.
I0419 04:28:09.939705 140283670685440 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.0, loss=1791.0740966796875
I0419 04:29:23.084339 140283586758400 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.0, loss=1834.9810791015625
I0419 04:30:36.713405 140283670685440 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0, loss=1842.1114501953125
I0419 04:31:54.689902 140283586758400 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1781.476806640625
I0419 04:33:14.071044 140283679078144 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.0, loss=1800.395263671875
I0419 04:34:28.291329 140283670685440 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0, loss=1771.0010986328125
I0419 04:35:41.458426 140283679078144 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0, loss=1911.861328125
I0419 04:36:54.690203 140283670685440 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.0, loss=1789.571533203125
I0419 04:38:09.248874 140283679078144 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1868.60009765625
I0419 04:39:35.585371 140283670685440 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.0, loss=1870.6484375
I0419 04:40:59.146202 140283679078144 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0, loss=1780.6094970703125
I0419 04:42:18.714595 140283670685440 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.0, loss=1840.389892578125
I0419 04:43:42.514288 140283679078144 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.0, loss=1864.6531982421875
I0419 04:45:04.340754 140283670685440 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1803.3123779296875
I0419 04:46:22.356973 140283679078144 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.0, loss=1785.701904296875
I0419 04:47:35.684566 140283670685440 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.0, loss=1808.2802734375
I0419 04:48:50.174103 140283679078144 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0, loss=1847.9622802734375
I0419 04:50:04.798798 140283670685440 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.0, loss=1797.2353515625
I0419 04:51:24.215003 140457377568576 spec.py:298] Evaluating on the training split.
I0419 04:51:51.251613 140457377568576 spec.py:310] Evaluating on the validation split.
I0419 04:52:26.067990 140457377568576 spec.py:326] Evaluating on the test split.
I0419 04:52:44.375909 140457377568576 submission_runner.py:406] Time since start: 16535.92s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15860.473261833191, 'total_duration': 16535.919248104095, 'accumulated_submission_time': 15860.473261833191, 'accumulated_eval_time': 667.6348195075989, 'accumulated_logging_time': 7.584079742431641}
I0419 04:52:44.397011 140283679078144 logging_writer.py:48] [20000] accumulated_eval_time=667.634820, accumulated_logging_time=7.584080, accumulated_submission_time=15860.473262, global_step=20000, preemption_count=0, score=15860.473262, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=16535.919248, train/ctc_loss=1746.1109619140625, train/wer=0.942824, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 04:52:44.570640 140457377568576 checkpoints.py:356] Saving checkpoint at step: 20000
I0419 04:52:45.467856 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000
I0419 04:52:45.486227 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0419 04:52:45.500962 140283670685440 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15860.473262
I0419 04:52:45.636728 140457377568576 checkpoints.py:356] Saving checkpoint at step: 20000
I0419 04:52:46.824576 140457377568576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000
I0419 04:52:46.842898 140457377568576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0419 04:52:47.934167 140457377568576 submission_runner.py:567] Tuning trial 1/1
I0419 04:52:47.934393 140457377568576 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0419 04:52:47.939228 140457377568576 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.050974, dtype=float32), 'train/wer': 0.9467026216604544, 'validation/ctc_loss': DeviceArray(30.297382, dtype=float32), 'validation/wer': 0.915889202983145, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.346977, dtype=float32), 'test/wer': 0.9181646456644933, 'test/num_examples': 2472, 'score': 62.14959669113159, 'total_duration': 169.32798409461975, 'accumulated_submission_time': 62.14959669113159, 'accumulated_eval_time': 107.1782169342041, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3037, {'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2462.728982448578, 'total_duration': 2650.130298137665, 'accumulated_submission_time': 2462.728982448578, 'accumulated_eval_time': 186.62124109268188, 'accumulated_logging_time': 0.7472124099731445, 'global_step': 3037, 'preemption_count': 0}), (6087, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4863.317631483078, 'total_duration': 5131.5113253593445, 'accumulated_submission_time': 4863.317631483078, 'accumulated_eval_time': 266.24385237693787, 'accumulated_logging_time': 1.8821711540222168, 'global_step': 6087, 'preemption_count': 0}), (9110, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7264.160523414612, 'total_duration': 7613.749875068665, 'accumulated_submission_time': 7264.160523414612, 'accumulated_eval_time': 346.4314315319061, 'accumulated_logging_time': 3.0559661388397217, 'global_step': 9110, 'preemption_count': 0}), (12179, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9664.539340019226, 'total_duration': 10094.073591470718, 'accumulated_submission_time': 9664.539340019226, 'accumulated_eval_time': 425.23264145851135, 'accumulated_logging_time': 4.165448904037476, 'global_step': 12179, 'preemption_count': 0}), (15189, {'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12064.92083787918, 'total_duration': 12575.925434827805, 'accumulated_submission_time': 12064.92083787918, 'accumulated_eval_time': 505.5200436115265, 'accumulated_logging_time': 5.313642263412476, 'global_step': 15189, 'preemption_count': 0}), (18200, {'train/ctc_loss': DeviceArray(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14465.40931725502, 'total_duration': 15059.560095787048, 'accumulated_submission_time': 14465.40931725502, 'accumulated_eval_time': 587.4756178855896, 'accumulated_logging_time': 6.469218492507935, 'global_step': 18200, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15860.473261833191, 'total_duration': 16535.919248104095, 'accumulated_submission_time': 15860.473261833191, 'accumulated_eval_time': 667.6348195075989, 'accumulated_logging_time': 7.584079742431641, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0419 04:52:47.939411 140457377568576 submission_runner.py:570] Timing: 15860.473261833191
I0419 04:52:47.939463 140457377568576 submission_runner.py:571] ====================
I0419 04:52:47.939922 140457377568576 submission_runner.py:631] Final librispeech_conformer score: 15860.473261833191
