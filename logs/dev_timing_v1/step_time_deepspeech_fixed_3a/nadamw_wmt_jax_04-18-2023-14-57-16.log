I0418 14:57:38.444644 139934763616064 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nadamw/wmt_jax.
I0418 14:57:38.508504 139934763616064 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 14:57:39.400236 139934763616064 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0418 14:57:39.401083 139934763616064 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 14:57:39.405838 139934763616064 submission_runner.py:528] Using RNG seed 58839901
I0418 14:57:42.058133 139934763616064 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 14:57:42.058314 139934763616064 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1.
I0418 14:57:42.058492 139934763616064 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/hparams.json.
I0418 14:57:42.178925 139934763616064 submission_runner.py:232] Initializing dataset.
I0418 14:57:42.187725 139934763616064 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:57:42.191084 139934763616064 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:57:42.191193 139934763616064 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:57:42.305082 139934763616064 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:57:44.187734 139934763616064 submission_runner.py:239] Initializing model.
I0418 14:57:55.888342 139934763616064 submission_runner.py:249] Initializing optimizer.
I0418 14:57:56.805973 139934763616064 submission_runner.py:256] Initializing metrics bundle.
I0418 14:57:56.806171 139934763616064 submission_runner.py:273] Initializing checkpoint and logger.
I0418 14:57:56.807195 139934763616064 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1 with prefix checkpoint_
I0418 14:57:56.807445 139934763616064 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 14:57:56.807508 139934763616064 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 14:57:57.535074 139934763616064 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/meta_data_0.json.
I0418 14:57:57.536201 139934763616064 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/flags_0.json.
I0418 14:57:57.540607 139934763616064 submission_runner.py:309] Starting training loop.
I0418 14:58:31.417286 139758588122880 logging_writer.py:48] [0] global_step=0, grad_norm=5.622434139251709, loss=11.020720481872559
I0418 14:58:31.431115 139934763616064 spec.py:298] Evaluating on the training split.
I0418 14:58:31.433481 139934763616064 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:58:31.435856 139934763616064 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:58:31.435959 139934763616064 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:58:31.465635 139934763616064 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:58:39.534215 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:03:46.556059 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 15:03:46.559768 139934763616064 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:03:46.563011 139934763616064 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 15:03:46.563118 139934763616064 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 15:03:46.592690 139934763616064 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:03:53.941778 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:08:53.987431 139934763616064 spec.py:326] Evaluating on the test split.
I0418 15:08:53.989627 139934763616064 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:08:53.992491 139934763616064 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 15:08:53.992628 139934763616064 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 15:08:54.020722 139934763616064 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 15:09:01.128721 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:13:55.412697 139934763616064 submission_runner.py:406] Time since start: 957.87s, 	Step: 1, 	{'train/accuracy': 0.0005860654637217522, 'train/loss': 11.000836372375488, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.004678726196289, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.006845474243164, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 33.890328884124756, 'total_duration': 957.8720049858093, 'accumulated_submission_time': 33.890328884124756, 'accumulated_eval_time': 923.9815039634705, 'accumulated_logging_time': 0}
I0418 15:13:55.430126 139747573368576 logging_writer.py:48] [1] accumulated_eval_time=923.981504, accumulated_logging_time=0, accumulated_submission_time=33.890329, global_step=1, preemption_count=0, score=33.890329, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.006845, test/num_examples=3003, total_duration=957.872005, train/accuracy=0.000586, train/bleu=0.000000, train/loss=11.000836, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.004679, validation/num_examples=3000
I0418 15:13:56.347931 139934763616064 checkpoints.py:356] Saving checkpoint at step: 1
I0418 15:13:59.732656 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_1
I0418 15:13:59.736035 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_1.
I0418 15:14:35.961561 139747581761280 logging_writer.py:48] [100] global_step=100, grad_norm=0.16785567998886108, loss=8.651009559631348
I0418 15:15:12.229537 139747682473728 logging_writer.py:48] [200] global_step=200, grad_norm=0.24456514418125153, loss=8.220372200012207
I0418 15:15:48.481156 139747581761280 logging_writer.py:48] [300] global_step=300, grad_norm=0.6356018781661987, loss=7.653083324432373
I0418 15:16:24.766501 139747682473728 logging_writer.py:48] [400] global_step=400, grad_norm=0.6526173949241638, loss=7.2692551612854
I0418 15:17:01.039538 139747581761280 logging_writer.py:48] [500] global_step=500, grad_norm=0.8710498213768005, loss=6.912927150726318
I0418 15:17:37.345006 139747682473728 logging_writer.py:48] [600] global_step=600, grad_norm=0.9106168150901794, loss=6.613150596618652
I0418 15:18:13.657021 139747581761280 logging_writer.py:48] [700] global_step=700, grad_norm=0.9924966096878052, loss=6.379815101623535
I0418 15:18:49.997344 139747682473728 logging_writer.py:48] [800] global_step=800, grad_norm=0.8052396774291992, loss=6.049002647399902
I0418 15:19:26.288999 139747581761280 logging_writer.py:48] [900] global_step=900, grad_norm=0.8938544988632202, loss=5.862600803375244
I0418 15:20:02.565820 139747682473728 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.212202548980713, loss=5.819650173187256
I0418 15:20:38.848527 139747581761280 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.685798168182373, loss=5.553787708282471
I0418 15:21:15.153929 139747682473728 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6965121030807495, loss=5.393570899963379
I0418 15:21:51.415933 139747581761280 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6919297575950623, loss=5.239452838897705
I0418 15:22:27.731039 139747682473728 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6818443536758423, loss=5.11814022064209
I0418 15:23:04.043966 139747581761280 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7121167778968811, loss=4.849023818969727
I0418 15:23:40.346010 139747682473728 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7260503172874451, loss=4.719362258911133
I0418 15:24:16.659354 139747581761280 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7592905759811401, loss=4.598883628845215
I0418 15:24:52.980483 139747682473728 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8557011485099792, loss=4.5658063888549805
I0418 15:25:29.268185 139747581761280 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6931552290916443, loss=4.42003059387207
I0418 15:26:05.550674 139747682473728 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7347927689552307, loss=4.303938388824463
I0418 15:26:41.849376 139747581761280 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5683885812759399, loss=4.288594722747803
I0418 15:27:18.152312 139747682473728 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7394537329673767, loss=4.284717082977295
I0418 15:27:54.472089 139747581761280 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7400001883506775, loss=4.185366630554199
I0418 15:27:59.994021 139934763616064 spec.py:298] Evaluating on the training split.
I0418 15:28:03.015435 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:30:57.809065 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 15:31:00.478310 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:33:41.851121 139934763616064 spec.py:326] Evaluating on the test split.
I0418 15:33:44.579367 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:36:15.917950 139934763616064 submission_runner.py:406] Time since start: 2298.38s, 	Step: 2317, 	{'train/accuracy': 0.49347472190856934, 'train/loss': 3.181772470474243, 'train/bleu': 20.87167532801479, 'validation/accuracy': 0.49447619915008545, 'validation/loss': 3.181593418121338, 'validation/bleu': 16.933483434755686, 'validation/num_examples': 3000, 'test/accuracy': 0.49062809348106384, 'test/loss': 3.2684431076049805, 'test/bleu': 15.6729969703032, 'test/num_examples': 3003, 'score': 874.119500875473, 'total_duration': 2298.3772542476654, 'accumulated_submission_time': 874.119500875473, 'accumulated_eval_time': 1419.9053964614868, 'accumulated_logging_time': 4.32679009437561}
I0418 15:36:15.926249 139747682473728 logging_writer.py:48] [2317] accumulated_eval_time=1419.905396, accumulated_logging_time=4.326790, accumulated_submission_time=874.119501, global_step=2317, preemption_count=0, score=874.119501, test/accuracy=0.490628, test/bleu=15.672997, test/loss=3.268443, test/num_examples=3003, total_duration=2298.377254, train/accuracy=0.493475, train/bleu=20.871675, train/loss=3.181772, validation/accuracy=0.494476, validation/bleu=16.933483, validation/loss=3.181593, validation/num_examples=3000
I0418 15:36:16.849208 139934763616064 checkpoints.py:356] Saving checkpoint at step: 2317
I0418 15:36:20.245260 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_2317
I0418 15:36:20.248685 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_2317.
I0418 15:36:50.697545 139747581761280 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5287567377090454, loss=4.072936058044434
I0418 15:37:26.994795 139747657295616 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5401608943939209, loss=4.086203575134277
I0418 15:38:03.303956 139747581761280 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6133612990379333, loss=4.002535820007324
I0418 15:38:39.559676 139747657295616 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5500708222389221, loss=4.0206522941589355
I0418 15:39:15.895957 139747581761280 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.48229458928108215, loss=3.9183928966522217
I0418 15:39:52.181260 139747657295616 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6223424673080444, loss=3.898050546646118
I0418 15:40:28.483930 139747581761280 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.4976290166378021, loss=3.8762013912200928
I0418 15:41:04.751849 139747657295616 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6325204372406006, loss=3.796473503112793
I0418 15:41:41.020465 139747581761280 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.43291324377059937, loss=3.827061653137207
I0418 15:42:17.303880 139747657295616 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.4422150254249573, loss=3.7833409309387207
I0418 15:42:53.547190 139747581761280 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.49115729331970215, loss=3.749863862991333
I0418 15:43:29.839982 139747657295616 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.46181774139404297, loss=3.67232084274292
I0418 15:44:06.076112 139747581761280 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.40348222851753235, loss=3.7371649742126465
I0418 15:44:42.347060 139747657295616 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5025027990341187, loss=3.734816312789917
I0418 15:45:18.623638 139747581761280 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.38842934370040894, loss=3.653122901916504
I0418 15:45:54.925396 139747657295616 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.3979634642601013, loss=3.6317832469940186
I0418 15:46:31.190881 139747581761280 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.34142008423805237, loss=3.6621487140655518
I0418 15:47:07.482503 139747657295616 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.377017080783844, loss=3.641477108001709
I0418 15:47:43.752243 139747581761280 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3491198420524597, loss=3.629580497741699
I0418 15:48:20.028552 139747657295616 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3907964527606964, loss=3.6358771324157715
I0418 15:48:56.328109 139747581761280 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3615550994873047, loss=3.490682601928711
I0418 15:49:32.584438 139747657295616 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3548903167247772, loss=3.5813398361206055
I0418 15:50:08.874301 139747581761280 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3443334698677063, loss=3.566260576248169
I0418 15:50:20.564627 139934763616064 spec.py:298] Evaluating on the training split.
I0418 15:50:23.578993 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:53:00.700004 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 15:53:03.385129 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:55:32.961318 139934763616064 spec.py:326] Evaluating on the test split.
I0418 15:55:35.677454 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 15:57:59.012091 139934763616064 submission_runner.py:406] Time since start: 3601.47s, 	Step: 4634, 	{'train/accuracy': 0.5655516982078552, 'train/loss': 2.490452766418457, 'train/bleu': 26.278699870195947, 'validation/accuracy': 0.5754051208496094, 'validation/loss': 2.4025630950927734, 'validation/bleu': 22.727010232745283, 'validation/num_examples': 3000, 'test/accuracy': 0.5761663913726807, 'test/loss': 2.397409200668335, 'test/bleu': 21.083049911573298, 'test/num_examples': 3003, 'score': 1714.4064447879791, 'total_duration': 3601.4713933467865, 'accumulated_submission_time': 1714.4064447879791, 'accumulated_eval_time': 1878.3527941703796, 'accumulated_logging_time': 8.660544633865356}
I0418 15:57:59.020023 139747657295616 logging_writer.py:48] [4634] accumulated_eval_time=1878.352794, accumulated_logging_time=8.660545, accumulated_submission_time=1714.406445, global_step=4634, preemption_count=0, score=1714.406445, test/accuracy=0.576166, test/bleu=21.083050, test/loss=2.397409, test/num_examples=3003, total_duration=3601.471393, train/accuracy=0.565552, train/bleu=26.278700, train/loss=2.490453, validation/accuracy=0.575405, validation/bleu=22.727010, validation/loss=2.402563, validation/num_examples=3000
I0418 15:57:59.938043 139934763616064 checkpoints.py:356] Saving checkpoint at step: 4634
I0418 15:58:03.230204 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_4634
I0418 15:58:03.233635 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_4634.
I0418 15:58:27.544963 139747581761280 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3790126442909241, loss=3.4418790340423584
I0418 15:59:03.842941 139747640510208 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.2952193319797516, loss=3.5134077072143555
I0418 15:59:40.115763 139747581761280 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.37733450531959534, loss=3.4704854488372803
I0418 16:00:16.406283 139747640510208 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.27640002965927124, loss=3.432213068008423
I0418 16:00:52.661678 139747581761280 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.34402742981910706, loss=3.500513792037964
I0418 16:01:28.931709 139747640510208 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.26026591658592224, loss=3.5048575401306152
I0418 16:02:05.220528 139747581761280 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.26129254698753357, loss=3.4695231914520264
I0418 16:02:41.495204 139747640510208 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.24557437002658844, loss=3.4588277339935303
I0418 16:03:17.771155 139747581761280 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.2423236072063446, loss=3.4016506671905518
I0418 16:03:54.070042 139747640510208 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.24353110790252686, loss=3.489314317703247
I0418 16:04:30.328178 139747581761280 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.25462806224823, loss=3.448448419570923
I0418 16:05:06.611871 139747640510208 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2816251814365387, loss=3.367173194885254
I0418 16:05:42.891604 139747581761280 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.2585027515888214, loss=3.3923492431640625
I0418 16:06:19.178159 139747640510208 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.2141473889350891, loss=3.354506254196167
I0418 16:06:55.428113 139747581761280 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.20281197130680084, loss=3.4041683673858643
I0418 16:07:31.707382 139747640510208 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.22078129649162292, loss=3.3794753551483154
I0418 16:08:08.006638 139747581761280 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.21247388422489166, loss=3.3824105262756348
I0418 16:08:44.283098 139747640510208 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.20435123145580292, loss=3.490821599960327
I0418 16:09:20.540311 139747581761280 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.23712322115898132, loss=3.3692500591278076
I0418 16:09:56.805777 139747640510208 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.22784028947353363, loss=3.3888678550720215
I0418 16:10:33.097797 139747581761280 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.20618154108524323, loss=3.415717363357544
I0418 16:11:09.353075 139747640510208 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.19663219153881073, loss=3.437159538269043
I0418 16:11:45.601011 139747581761280 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.20072032511234283, loss=3.350093364715576
I0418 16:12:03.457113 139934763616064 spec.py:298] Evaluating on the training split.
I0418 16:12:06.469609 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:14:49.798795 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 16:14:52.486522 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:17:21.944283 139934763616064 spec.py:326] Evaluating on the test split.
I0418 16:17:24.678534 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:19:53.718038 139934763616064 submission_runner.py:406] Time since start: 4916.18s, 	Step: 6951, 	{'train/accuracy': 0.5997586846351624, 'train/loss': 2.1748666763305664, 'train/bleu': 28.79832933340276, 'validation/accuracy': 0.6058945059776306, 'validation/loss': 2.132857322692871, 'validation/bleu': 24.839787666859866, 'validation/num_examples': 3000, 'test/accuracy': 0.6093312501907349, 'test/loss': 2.1117324829101562, 'test/bleu': 23.444056537491512, 'test/num_examples': 3003, 'score': 2554.599982023239, 'total_duration': 4916.177330493927, 'accumulated_submission_time': 2554.599982023239, 'accumulated_eval_time': 2348.6136474609375, 'accumulated_logging_time': 12.885159730911255}
I0418 16:19:53.727831 139747640510208 logging_writer.py:48] [6951] accumulated_eval_time=2348.613647, accumulated_logging_time=12.885160, accumulated_submission_time=2554.599982, global_step=6951, preemption_count=0, score=2554.599982, test/accuracy=0.609331, test/bleu=23.444057, test/loss=2.111732, test/num_examples=3003, total_duration=4916.177330, train/accuracy=0.599759, train/bleu=28.798329, train/loss=2.174867, validation/accuracy=0.605895, validation/bleu=24.839788, validation/loss=2.132857, validation/num_examples=3000
I0418 16:19:55.105589 139934763616064 checkpoints.py:356] Saving checkpoint at step: 6951
I0418 16:19:59.839008 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_6951
I0418 16:19:59.843796 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_6951.
I0418 16:20:17.937915 139747581761280 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.19254009425640106, loss=3.393143653869629
I0418 16:20:54.180542 139747632117504 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.2400001883506775, loss=3.3422915935516357
I0418 16:21:30.385701 139747581761280 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.19276587665081024, loss=3.2728114128112793
I0418 16:22:06.633410 139747632117504 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1959109604358673, loss=3.22314715385437
I0418 16:22:42.907282 139747581761280 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.18461336195468903, loss=3.1962366104125977
I0418 16:23:19.201497 139747632117504 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.17146146297454834, loss=3.2859458923339844
I0418 16:23:55.447989 139747581761280 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.17493093013763428, loss=3.3093252182006836
I0418 16:24:31.737166 139747632117504 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.18006360530853271, loss=3.304180860519409
I0418 16:25:07.990357 139747581761280 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1755436807870865, loss=3.1926865577697754
I0418 16:25:44.278493 139747632117504 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.17412932217121124, loss=3.3106284141540527
I0418 16:26:20.538682 139747581761280 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.16609100997447968, loss=3.315429210662842
I0418 16:26:56.824626 139747632117504 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.17412583529949188, loss=3.191458225250244
I0418 16:27:33.096534 139747581761280 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.16932456195354462, loss=3.2551472187042236
I0418 16:28:09.386194 139747632117504 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.18262872099876404, loss=3.207317352294922
I0418 16:28:45.583330 139747581761280 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.17435061931610107, loss=3.173758029937744
I0418 16:29:21.827477 139747632117504 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.16087976098060608, loss=3.3186612129211426
I0418 16:29:58.095868 139747581761280 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.15551349520683289, loss=3.17169189453125
I0418 16:30:34.352876 139747632117504 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.17820917069911957, loss=3.1569745540618896
I0418 16:31:10.627129 139747581761280 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.14686082303524017, loss=3.1861929893493652
I0418 16:31:46.910459 139747632117504 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.172328382730484, loss=3.234683036804199
I0418 16:32:23.157535 139747581761280 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.17361077666282654, loss=3.1795766353607178
I0418 16:32:59.400887 139747632117504 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.1592537760734558, loss=3.2724552154541016
I0418 16:33:35.649456 139747581761280 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.14586934447288513, loss=3.2571961879730225
I0418 16:34:00.017718 139934763616064 spec.py:298] Evaluating on the training split.
I0418 16:34:03.027846 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:36:59.000929 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 16:37:01.662831 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:39:31.443238 139934763616064 spec.py:326] Evaluating on the test split.
I0418 16:39:34.186670 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:41:53.021488 139934763616064 submission_runner.py:406] Time since start: 6235.48s, 	Step: 9269, 	{'train/accuracy': 0.6109622120857239, 'train/loss': 2.07981276512146, 'train/bleu': 29.182460390670823, 'validation/accuracy': 0.6238112449645996, 'validation/loss': 1.9694726467132568, 'validation/bleu': 25.883677883772638, 'validation/num_examples': 3000, 'test/accuracy': 0.6302016377449036, 'test/loss': 1.9331327676773071, 'test/bleu': 24.581422330536196, 'test/num_examples': 3003, 'score': 3394.746659755707, 'total_duration': 6235.480781316757, 'accumulated_submission_time': 3394.746659755707, 'accumulated_eval_time': 2821.6173419952393, 'accumulated_logging_time': 19.01468253135681}
I0418 16:41:53.030699 139747632117504 logging_writer.py:48] [9269] accumulated_eval_time=2821.617342, accumulated_logging_time=19.014683, accumulated_submission_time=3394.746660, global_step=9269, preemption_count=0, score=3394.746660, test/accuracy=0.630202, test/bleu=24.581422, test/loss=1.933133, test/num_examples=3003, total_duration=6235.480781, train/accuracy=0.610962, train/bleu=29.182460, train/loss=2.079813, validation/accuracy=0.623811, validation/bleu=25.883678, validation/loss=1.969473, validation/num_examples=3000
I0418 16:41:53.959931 139934763616064 checkpoints.py:356] Saving checkpoint at step: 9269
I0418 16:41:57.226245 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_9269
I0418 16:41:57.229679 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_9269.
I0418 16:42:08.852382 139747581761280 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.15494082868099213, loss=3.2575833797454834
I0418 16:42:45.136334 139747623724800 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.1886853128671646, loss=3.2138500213623047
I0418 16:43:21.370445 139747581761280 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.1849755346775055, loss=3.1610655784606934
I0418 16:43:57.661563 139747623724800 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.1331298053264618, loss=3.1131725311279297
I0418 16:44:33.907892 139747581761280 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1499425321817398, loss=3.1740734577178955
I0418 16:45:10.195459 139747623724800 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1645980030298233, loss=3.2577884197235107
I0418 16:45:46.449128 139747581761280 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.1753770112991333, loss=3.1577038764953613
I0418 16:46:22.700993 139747623724800 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.15509839355945587, loss=3.1551010608673096
I0418 16:46:58.987046 139747581761280 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.14345724880695343, loss=3.2237348556518555
I0418 16:47:35.194235 139747623724800 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.24803230166435242, loss=3.2222752571105957
I0418 16:48:11.452775 139747581761280 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.16251443326473236, loss=3.126171827316284
I0418 16:48:47.695861 139747623724800 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.15902796387672424, loss=3.120795249938965
I0418 16:49:23.948391 139747581761280 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.18173937499523163, loss=3.18530535697937
I0418 16:50:00.207068 139747623724800 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.15749529004096985, loss=3.0990185737609863
I0418 16:50:36.464487 139747581761280 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.13635991513729095, loss=3.1726019382476807
I0418 16:51:12.728274 139747623724800 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1307128369808197, loss=3.1099250316619873
I0418 16:51:49.013845 139747581761280 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17247611284255981, loss=3.1969714164733887
I0418 16:52:25.272008 139747623724800 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.15892568230628967, loss=3.1037893295288086
I0418 16:53:01.520878 139747581761280 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.13935774564743042, loss=3.0742204189300537
I0418 16:53:37.791614 139747623724800 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.14551590383052826, loss=3.131513833999634
I0418 16:54:14.057303 139747581761280 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.14199866354465485, loss=3.217170000076294
I0418 16:54:50.336437 139747623724800 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.14263947308063507, loss=3.131622076034546
I0418 16:55:26.627501 139747581761280 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1542735993862152, loss=3.1723499298095703
I0418 16:55:57.530022 139934763616064 spec.py:298] Evaluating on the training split.
I0418 16:56:00.539117 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 16:58:45.175048 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 16:58:47.846469 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:01:11.110479 139934763616064 spec.py:326] Evaluating on the test split.
I0418 17:01:13.829556 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:03:28.422639 139934763616064 submission_runner.py:406] Time since start: 7530.88s, 	Step: 11587, 	{'train/accuracy': 0.6180033683776855, 'train/loss': 2.0210702419281006, 'train/bleu': 30.06866163457872, 'validation/accuracy': 0.6361979246139526, 'validation/loss': 1.8801528215408325, 'validation/bleu': 26.549035899413262, 'validation/num_examples': 3000, 'test/accuracy': 0.6453779935836792, 'test/loss': 1.8333767652511597, 'test/bleu': 25.9668772174857, 'test/num_examples': 3003, 'score': 4235.01923251152, 'total_duration': 7530.881969451904, 'accumulated_submission_time': 4235.01923251152, 'accumulated_eval_time': 3272.509923219681, 'accumulated_logging_time': 23.225795030593872}
I0418 17:03:28.430717 139747623724800 logging_writer.py:48] [11587] accumulated_eval_time=3272.509923, accumulated_logging_time=23.225795, accumulated_submission_time=4235.019233, global_step=11587, preemption_count=0, score=4235.019233, test/accuracy=0.645378, test/bleu=25.966877, test/loss=1.833377, test/num_examples=3003, total_duration=7530.881969, train/accuracy=0.618003, train/bleu=30.068662, train/loss=2.021070, validation/accuracy=0.636198, validation/bleu=26.549036, validation/loss=1.880153, validation/num_examples=3000
I0418 17:03:29.349123 139934763616064 checkpoints.py:356] Saving checkpoint at step: 11587
I0418 17:03:32.649906 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_11587
I0418 17:03:32.653438 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_11587.
I0418 17:03:37.724672 139747581761280 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.16647383570671082, loss=3.1907267570495605
I0418 17:04:13.975982 139747615332096 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.14378266036510468, loss=3.105773448944092
I0418 17:04:50.196741 139747581761280 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.16054269671440125, loss=3.1393368244171143
I0418 17:05:26.469426 139747615332096 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.13875620067119598, loss=3.117025852203369
I0418 17:06:02.724307 139747581761280 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1565927416086197, loss=3.062605857849121
I0418 17:06:39.013787 139747615332096 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.17269620299339294, loss=3.1493499279022217
I0418 17:07:15.265079 139747581761280 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.18718409538269043, loss=3.03543758392334
I0418 17:07:51.517152 139747615332096 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.16437509655952454, loss=3.106950283050537
I0418 17:08:27.773324 139747581761280 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.14370118081569672, loss=3.0424327850341797
I0418 17:09:04.062672 139747615332096 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1427738517522812, loss=3.0959413051605225
I0418 17:09:40.337266 139747581761280 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.1564721316099167, loss=3.1133060455322266
I0418 17:10:16.600004 139747615332096 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1578502655029297, loss=3.0471975803375244
I0418 17:10:52.864085 139747581761280 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1534528136253357, loss=3.137305736541748
I0418 17:11:29.137449 139747615332096 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.16587845981121063, loss=3.083176374435425
I0418 17:12:05.433586 139747581761280 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.1963742971420288, loss=3.0932459831237793
I0418 17:12:41.701545 139747615332096 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.15662916004657745, loss=3.0850918292999268
I0418 17:13:17.998945 139747581761280 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.16113047301769257, loss=3.1139862537384033
I0418 17:13:54.277098 139747615332096 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.16692566871643066, loss=3.0629212856292725
I0418 17:14:30.568793 139747581761280 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1571280062198639, loss=3.0800957679748535
I0418 17:15:06.805712 139747615332096 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16433759033679962, loss=3.07981014251709
I0418 17:15:43.060000 139747581761280 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.15950025618076324, loss=3.041348695755005
I0418 17:16:19.363300 139747615332096 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.21437551081180573, loss=3.0859081745147705
I0418 17:16:55.611782 139747581761280 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.17765985429286957, loss=3.0724120140075684
I0418 17:17:31.885737 139747615332096 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.14691197872161865, loss=3.138791561126709
I0418 17:17:32.692988 139934763616064 spec.py:298] Evaluating on the training split.
I0418 17:17:35.701481 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:20:33.445538 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 17:20:36.117543 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:23:10.724529 139934763616064 spec.py:326] Evaluating on the test split.
I0418 17:23:13.431529 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:25:23.946866 139934763616064 submission_runner.py:406] Time since start: 8846.41s, 	Step: 13904, 	{'train/accuracy': 0.6368082165718079, 'train/loss': 1.8864449262619019, 'train/bleu': 30.950903276619442, 'validation/accuracy': 0.6472827196121216, 'validation/loss': 1.8050459623336792, 'validation/bleu': 27.34390434404175, 'validation/num_examples': 3000, 'test/accuracy': 0.6542559862136841, 'test/loss': 1.7537378072738647, 'test/bleu': 26.59012519016106, 'test/num_examples': 3003, 'score': 5075.03138589859, 'total_duration': 8846.406112670898, 'accumulated_submission_time': 5075.03138589859, 'accumulated_eval_time': 3743.7636756896973, 'accumulated_logging_time': 27.459548234939575}
I0418 17:25:23.955573 139747581761280 logging_writer.py:48] [13904] accumulated_eval_time=3743.763676, accumulated_logging_time=27.459548, accumulated_submission_time=5075.031386, global_step=13904, preemption_count=0, score=5075.031386, test/accuracy=0.654256, test/bleu=26.590125, test/loss=1.753738, test/num_examples=3003, total_duration=8846.406113, train/accuracy=0.636808, train/bleu=30.950903, train/loss=1.886445, validation/accuracy=0.647283, validation/bleu=27.343904, validation/loss=1.805046, validation/num_examples=3000
I0418 17:25:24.884717 139934763616064 checkpoints.py:356] Saving checkpoint at step: 13904
I0418 17:25:28.164835 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_13904
I0418 17:25:28.168295 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_13904.
I0418 17:26:03.346591 139747615332096 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.23049035668373108, loss=3.0744783878326416
I0418 17:26:39.625294 139747606939392 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.1565401405096054, loss=3.0483412742614746
I0418 17:27:15.881045 139747615332096 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.1462034285068512, loss=3.064476728439331
I0418 17:27:52.222850 139747606939392 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.15127475559711456, loss=3.065274477005005
I0418 17:28:28.454205 139747615332096 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.15461482107639313, loss=3.0543811321258545
I0418 17:29:04.734731 139747606939392 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1537030190229416, loss=3.09108304977417
I0418 17:29:40.984194 139747615332096 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.19760917127132416, loss=3.051600933074951
I0418 17:30:17.263387 139747606939392 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.20440702140331268, loss=3.1007213592529297
I0418 17:30:53.533756 139747615332096 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.14428618550300598, loss=3.009000778198242
I0418 17:31:29.800197 139747606939392 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.18435907363891602, loss=3.149451971054077
I0418 17:32:06.122531 139747615332096 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.23475417494773865, loss=3.075354814529419
I0418 17:32:42.373066 139747606939392 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1650867462158203, loss=3.0275955200195312
I0418 17:33:18.649867 139747615332096 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.18687458336353302, loss=3.0225601196289062
I0418 17:33:54.885283 139747606939392 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.16851377487182617, loss=3.0387723445892334
I0418 17:34:31.144364 139747615332096 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2058836966753006, loss=3.040642738342285
I0418 17:35:07.380203 139747606939392 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18324260413646698, loss=3.101491689682007
I0418 17:35:43.664396 139747615332096 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.16181229054927826, loss=3.0691685676574707
I0418 17:36:19.936811 139747606939392 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.16490107774734497, loss=3.018374443054199
I0418 17:36:56.266899 139747615332096 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3116335868835449, loss=2.943453311920166
I0418 17:37:32.546170 139747606939392 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1617632657289505, loss=2.962454080581665
I0418 17:38:08.825823 139747615332096 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.19926533102989197, loss=2.9825809001922607
I0418 17:38:45.114281 139747606939392 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.16416898369789124, loss=3.0855026245117188
I0418 17:39:21.388688 139747615332096 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.17790144681930542, loss=3.019954204559326
I0418 17:39:28.363224 139934763616064 spec.py:298] Evaluating on the training split.
I0418 17:39:31.395137 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:42:14.351378 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 17:42:17.037012 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:44:47.981152 139934763616064 spec.py:326] Evaluating on the test split.
I0418 17:44:50.708111 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 17:47:09.495772 139934763616064 submission_runner.py:406] Time since start: 10151.96s, 	Step: 16221, 	{'train/accuracy': 0.6355645060539246, 'train/loss': 1.8858647346496582, 'train/bleu': 31.327752904833012, 'validation/accuracy': 0.6536558866500854, 'validation/loss': 1.753091812133789, 'validation/bleu': 27.995044033511004, 'validation/num_examples': 3000, 'test/accuracy': 0.6634129285812378, 'test/loss': 1.6952828168869019, 'test/bleu': 27.376059183834972, 'test/num_examples': 3003, 'score': 5915.196911096573, 'total_duration': 10151.95507311821, 'accumulated_submission_time': 5915.196911096573, 'accumulated_eval_time': 4204.896178722382, 'accumulated_logging_time': 31.683973789215088}
I0418 17:47:09.505150 139747606939392 logging_writer.py:48] [16221] accumulated_eval_time=4204.896179, accumulated_logging_time=31.683974, accumulated_submission_time=5915.196911, global_step=16221, preemption_count=0, score=5915.196911, test/accuracy=0.663413, test/bleu=27.376059, test/loss=1.695283, test/num_examples=3003, total_duration=10151.955073, train/accuracy=0.635565, train/bleu=31.327753, train/loss=1.885865, validation/accuracy=0.653656, validation/bleu=27.995044, validation/loss=1.753092, validation/num_examples=3000
I0418 17:47:10.429041 139934763616064 checkpoints.py:356] Saving checkpoint at step: 16221
I0418 17:47:13.728682 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_16221
I0418 17:47:13.732085 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_16221.
I0418 17:47:42.760163 139747615332096 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1843464970588684, loss=2.970731735229492
I0418 17:48:19.024954 139747196925696 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.1691175103187561, loss=3.0843665599823
I0418 17:48:55.307756 139747615332096 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.17141908407211304, loss=2.978466749191284
I0418 17:49:31.565604 139747196925696 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2170087993144989, loss=3.004598617553711
I0418 17:50:07.829258 139747615332096 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.20534631609916687, loss=3.086923360824585
I0418 17:50:44.080646 139747196925696 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1777963489294052, loss=2.9744274616241455
I0418 17:51:20.319286 139747615332096 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15868240594863892, loss=3.0221478939056396
I0418 17:51:56.593952 139747196925696 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.24038144946098328, loss=3.0191805362701416
I0418 17:52:32.857401 139747615332096 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.22657309472560883, loss=3.0453808307647705
I0418 17:53:09.113561 139747196925696 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.1603160798549652, loss=2.981400966644287
I0418 17:53:45.374099 139747615332096 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.22867752611637115, loss=2.9352078437805176
I0418 17:54:21.654387 139747196925696 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.21816308796405792, loss=2.9684178829193115
I0418 17:54:57.893559 139747615332096 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.19354431331157684, loss=2.983322858810425
I0418 17:55:34.192831 139747196925696 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.16588060557842255, loss=2.956298828125
I0418 17:56:10.457429 139747615332096 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.17486344277858734, loss=2.948824882507324
I0418 17:56:46.779618 139747196925696 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.20315872132778168, loss=3.0283570289611816
I0418 17:57:23.020346 139747615332096 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.16991031169891357, loss=3.02168869972229
I0418 17:57:59.268630 139747196925696 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.21524573862552643, loss=2.933363914489746
I0418 17:58:35.551234 139747615332096 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.23992232978343964, loss=2.970332622528076
I0418 17:59:11.842384 139747196925696 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.17384198307991028, loss=2.948268413543701
I0418 17:59:48.091621 139747615332096 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1760895997285843, loss=3.004452705383301
I0418 18:00:24.406356 139747196925696 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.17926561832427979, loss=2.9968316555023193
I0418 18:01:00.673339 139747615332096 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.18223126232624054, loss=2.953037738800049
I0418 18:01:13.803821 139934763616064 spec.py:298] Evaluating on the training split.
I0418 18:01:16.809359 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 18:04:10.126142 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 18:04:12.802154 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 18:06:40.987146 139934763616064 spec.py:326] Evaluating on the test split.
I0418 18:06:43.709126 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 18:09:07.181216 139934763616064 submission_runner.py:406] Time since start: 11469.64s, 	Step: 18538, 	{'train/accuracy': 0.6408257484436035, 'train/loss': 1.8501912355422974, 'train/bleu': 31.424299957457862, 'validation/accuracy': 0.6572020053863525, 'validation/loss': 1.721042513847351, 'validation/bleu': 28.046222857113236, 'validation/num_examples': 3000, 'test/accuracy': 0.6679681539535522, 'test/loss': 1.660046100616455, 'test/bleu': 27.47594592557977, 'test/num_examples': 3003, 'score': 6755.240299940109, 'total_duration': 11469.640416145325, 'accumulated_submission_time': 6755.240299940109, 'accumulated_eval_time': 4678.273446798325, 'accumulated_logging_time': 35.92336368560791}
I0418 18:09:07.193369 139747196925696 logging_writer.py:48] [18538] accumulated_eval_time=4678.273447, accumulated_logging_time=35.923364, accumulated_submission_time=6755.240300, global_step=18538, preemption_count=0, score=6755.240300, test/accuracy=0.667968, test/bleu=27.475946, test/loss=1.660046, test/num_examples=3003, total_duration=11469.640416, train/accuracy=0.640826, train/bleu=31.424300, train/loss=1.850191, validation/accuracy=0.657202, validation/bleu=28.046223, validation/loss=1.721043, validation/num_examples=3000
I0418 18:09:08.578819 139934763616064 checkpoints.py:356] Saving checkpoint at step: 18538
I0418 18:09:13.420120 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_18538
I0418 18:09:13.424902 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_18538.
I0418 18:09:36.240139 139747615332096 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.158829927444458, loss=2.948986291885376
I0418 18:10:12.521908 139747188532992 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.24078309535980225, loss=2.8940834999084473
I0418 18:10:48.810917 139747615332096 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.1850011944770813, loss=3.0387160778045654
I0418 18:11:25.085335 139747188532992 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.19803960621356964, loss=2.975332021713257
I0418 18:12:01.360690 139747615332096 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.17361828684806824, loss=2.927992105484009
I0418 18:12:37.639831 139747188532992 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.17924952507019043, loss=2.927523374557495
I0418 18:13:13.926507 139747615332096 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.17180240154266357, loss=2.959613084793091
I0418 18:13:50.178980 139747188532992 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.1855550855398178, loss=2.925323247909546
I0418 18:14:26.431214 139747615332096 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.22494688630104065, loss=2.9508156776428223
I0418 18:15:02.716498 139747188532992 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.24163423478603363, loss=2.9602761268615723
I0418 18:15:39.001860 139747615332096 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1792975515127182, loss=2.989652633666992
I0418 18:16:15.255105 139747188532992 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2708720564842224, loss=2.9419689178466797
I0418 18:16:51.587670 139747615332096 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17733100056648254, loss=2.9711687564849854
I0418 18:17:27.841481 139747188532992 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.24728170037269592, loss=3.05047869682312
I0418 18:18:03.426277 139934763616064 spec.py:298] Evaluating on the training split.
I0418 18:18:06.423454 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 18:20:46.382878 139934763616064 spec.py:310] Evaluating on the validation split.
I0418 18:20:49.047657 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 18:23:07.763115 139934763616064 spec.py:326] Evaluating on the test split.
I0418 18:23:10.476751 139934763616064 workload.py:179] Translating evaluation dataset.
I0418 18:25:25.311223 139934763616064 submission_runner.py:406] Time since start: 12447.77s, 	Step: 20000, 	{'train/accuracy': 0.6493745446205139, 'train/loss': 1.770255446434021, 'train/bleu': 31.768069857387456, 'validation/accuracy': 0.6607853770256042, 'validation/loss': 1.6961477994918823, 'validation/bleu': 28.264702465107412, 'validation/num_examples': 3000, 'test/accuracy': 0.6702922582626343, 'test/loss': 1.637071132659912, 'test/bleu': 27.871506469668162, 'test/num_examples': 3003, 'score': 7285.220268726349, 'total_duration': 12447.770543336868, 'accumulated_submission_time': 7285.220268726349, 'accumulated_eval_time': 5120.15834903717, 'accumulated_logging_time': 42.171207904815674}
I0418 18:25:25.320529 139747615332096 logging_writer.py:48] [20000] accumulated_eval_time=5120.158349, accumulated_logging_time=42.171208, accumulated_submission_time=7285.220269, global_step=20000, preemption_count=0, score=7285.220269, test/accuracy=0.670292, test/bleu=27.871506, test/loss=1.637071, test/num_examples=3003, total_duration=12447.770543, train/accuracy=0.649375, train/bleu=31.768070, train/loss=1.770255, validation/accuracy=0.660785, validation/bleu=28.264702, validation/loss=1.696148, validation/num_examples=3000
I0418 18:25:26.259829 139934763616064 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:25:29.632190 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_20000
I0418 18:25:29.635749 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_20000.
I0418 18:25:29.646490 139747188532992 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7285.220269
I0418 18:25:30.185903 139934763616064 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:25:35.301783 139934763616064 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_20000
I0418 18:25:35.305254 139934763616064 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/wmt_jax/trial_1/checkpoint_20000.
I0418 18:25:35.343776 139934763616064 submission_runner.py:567] Tuning trial 1/1
I0418 18:25:35.343937 139934763616064 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 18:25:35.344865 139934763616064 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005860654637217522, 'train/loss': 11.000836372375488, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.004678726196289, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.006845474243164, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 33.890328884124756, 'total_duration': 957.8720049858093, 'accumulated_submission_time': 33.890328884124756, 'accumulated_eval_time': 923.9815039634705, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2317, {'train/accuracy': 0.49347472190856934, 'train/loss': 3.181772470474243, 'train/bleu': 20.87167532801479, 'validation/accuracy': 0.49447619915008545, 'validation/loss': 3.181593418121338, 'validation/bleu': 16.933483434755686, 'validation/num_examples': 3000, 'test/accuracy': 0.49062809348106384, 'test/loss': 3.2684431076049805, 'test/bleu': 15.6729969703032, 'test/num_examples': 3003, 'score': 874.119500875473, 'total_duration': 2298.3772542476654, 'accumulated_submission_time': 874.119500875473, 'accumulated_eval_time': 1419.9053964614868, 'accumulated_logging_time': 4.32679009437561, 'global_step': 2317, 'preemption_count': 0}), (4634, {'train/accuracy': 0.5655516982078552, 'train/loss': 2.490452766418457, 'train/bleu': 26.278699870195947, 'validation/accuracy': 0.5754051208496094, 'validation/loss': 2.4025630950927734, 'validation/bleu': 22.727010232745283, 'validation/num_examples': 3000, 'test/accuracy': 0.5761663913726807, 'test/loss': 2.397409200668335, 'test/bleu': 21.083049911573298, 'test/num_examples': 3003, 'score': 1714.4064447879791, 'total_duration': 3601.4713933467865, 'accumulated_submission_time': 1714.4064447879791, 'accumulated_eval_time': 1878.3527941703796, 'accumulated_logging_time': 8.660544633865356, 'global_step': 4634, 'preemption_count': 0}), (6951, {'train/accuracy': 0.5997586846351624, 'train/loss': 2.1748666763305664, 'train/bleu': 28.79832933340276, 'validation/accuracy': 0.6058945059776306, 'validation/loss': 2.132857322692871, 'validation/bleu': 24.839787666859866, 'validation/num_examples': 3000, 'test/accuracy': 0.6093312501907349, 'test/loss': 2.1117324829101562, 'test/bleu': 23.444056537491512, 'test/num_examples': 3003, 'score': 2554.599982023239, 'total_duration': 4916.177330493927, 'accumulated_submission_time': 2554.599982023239, 'accumulated_eval_time': 2348.6136474609375, 'accumulated_logging_time': 12.885159730911255, 'global_step': 6951, 'preemption_count': 0}), (9269, {'train/accuracy': 0.6109622120857239, 'train/loss': 2.07981276512146, 'train/bleu': 29.182460390670823, 'validation/accuracy': 0.6238112449645996, 'validation/loss': 1.9694726467132568, 'validation/bleu': 25.883677883772638, 'validation/num_examples': 3000, 'test/accuracy': 0.6302016377449036, 'test/loss': 1.9331327676773071, 'test/bleu': 24.581422330536196, 'test/num_examples': 3003, 'score': 3394.746659755707, 'total_duration': 6235.480781316757, 'accumulated_submission_time': 3394.746659755707, 'accumulated_eval_time': 2821.6173419952393, 'accumulated_logging_time': 19.01468253135681, 'global_step': 9269, 'preemption_count': 0}), (11587, {'train/accuracy': 0.6180033683776855, 'train/loss': 2.0210702419281006, 'train/bleu': 30.06866163457872, 'validation/accuracy': 0.6361979246139526, 'validation/loss': 1.8801528215408325, 'validation/bleu': 26.549035899413262, 'validation/num_examples': 3000, 'test/accuracy': 0.6453779935836792, 'test/loss': 1.8333767652511597, 'test/bleu': 25.9668772174857, 'test/num_examples': 3003, 'score': 4235.01923251152, 'total_duration': 7530.881969451904, 'accumulated_submission_time': 4235.01923251152, 'accumulated_eval_time': 3272.509923219681, 'accumulated_logging_time': 23.225795030593872, 'global_step': 11587, 'preemption_count': 0}), (13904, {'train/accuracy': 0.6368082165718079, 'train/loss': 1.8864449262619019, 'train/bleu': 30.950903276619442, 'validation/accuracy': 0.6472827196121216, 'validation/loss': 1.8050459623336792, 'validation/bleu': 27.34390434404175, 'validation/num_examples': 3000, 'test/accuracy': 0.6542559862136841, 'test/loss': 1.7537378072738647, 'test/bleu': 26.59012519016106, 'test/num_examples': 3003, 'score': 5075.03138589859, 'total_duration': 8846.406112670898, 'accumulated_submission_time': 5075.03138589859, 'accumulated_eval_time': 3743.7636756896973, 'accumulated_logging_time': 27.459548234939575, 'global_step': 13904, 'preemption_count': 0}), (16221, {'train/accuracy': 0.6355645060539246, 'train/loss': 1.8858647346496582, 'train/bleu': 31.327752904833012, 'validation/accuracy': 0.6536558866500854, 'validation/loss': 1.753091812133789, 'validation/bleu': 27.995044033511004, 'validation/num_examples': 3000, 'test/accuracy': 0.6634129285812378, 'test/loss': 1.6952828168869019, 'test/bleu': 27.376059183834972, 'test/num_examples': 3003, 'score': 5915.196911096573, 'total_duration': 10151.95507311821, 'accumulated_submission_time': 5915.196911096573, 'accumulated_eval_time': 4204.896178722382, 'accumulated_logging_time': 31.683973789215088, 'global_step': 16221, 'preemption_count': 0}), (18538, {'train/accuracy': 0.6408257484436035, 'train/loss': 1.8501912355422974, 'train/bleu': 31.424299957457862, 'validation/accuracy': 0.6572020053863525, 'validation/loss': 1.721042513847351, 'validation/bleu': 28.046222857113236, 'validation/num_examples': 3000, 'test/accuracy': 0.6679681539535522, 'test/loss': 1.660046100616455, 'test/bleu': 27.47594592557977, 'test/num_examples': 3003, 'score': 6755.240299940109, 'total_duration': 11469.640416145325, 'accumulated_submission_time': 6755.240299940109, 'accumulated_eval_time': 4678.273446798325, 'accumulated_logging_time': 35.92336368560791, 'global_step': 18538, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6493745446205139, 'train/loss': 1.770255446434021, 'train/bleu': 31.768069857387456, 'validation/accuracy': 0.6607853770256042, 'validation/loss': 1.6961477994918823, 'validation/bleu': 28.264702465107412, 'validation/num_examples': 3000, 'test/accuracy': 0.6702922582626343, 'test/loss': 1.637071132659912, 'test/bleu': 27.871506469668162, 'test/num_examples': 3003, 'score': 7285.220268726349, 'total_duration': 12447.770543336868, 'accumulated_submission_time': 7285.220268726349, 'accumulated_eval_time': 5120.15834903717, 'accumulated_logging_time': 42.171207904815674, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0418 18:25:35.344958 139934763616064 submission_runner.py:570] Timing: 7285.220268726349
I0418 18:25:35.344998 139934763616064 submission_runner.py:571] ====================
I0418 18:25:35.345094 139934763616064 submission_runner.py:631] Final wmt score: 7285.220268726349
