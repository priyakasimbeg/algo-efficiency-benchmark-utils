I0418 13:27:20.920060 139781129688896 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_momentum/ogbg_jax.
I0418 13:27:20.989092 139781129688896 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 13:27:21.823663 139781129688896 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0418 13:27:21.824300 139781129688896 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 13:27:21.828421 139781129688896 submission_runner.py:528] Using RNG seed 2265030649
I0418 13:27:24.489919 139781129688896 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 13:27:24.490134 139781129688896 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1.
I0418 13:27:24.490387 139781129688896 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/hparams.json.
I0418 13:27:24.613356 139781129688896 submission_runner.py:232] Initializing dataset.
I0418 13:27:24.852510 139781129688896 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:27:24.857471 139781129688896 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:27:25.080569 139781129688896 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:27:25.135067 139781129688896 submission_runner.py:239] Initializing model.
I0418 13:27:32.838220 139781129688896 submission_runner.py:249] Initializing optimizer.
I0418 13:27:33.179100 139781129688896 submission_runner.py:256] Initializing metrics bundle.
I0418 13:27:33.179326 139781129688896 submission_runner.py:273] Initializing checkpoint and logger.
I0418 13:27:33.180346 139781129688896 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1 with prefix checkpoint_
I0418 13:27:33.180611 139781129688896 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 13:27:33.180686 139781129688896 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 13:27:34.047244 139781129688896 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/meta_data_0.json.
I0418 13:27:34.048273 139781129688896 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/flags_0.json.
I0418 13:27:34.053822 139781129688896 submission_runner.py:309] Starting training loop.
I0418 13:27:53.724309 139605185652480 logging_writer.py:48] [0] global_step=0, grad_norm=2.5713484287261963, loss=0.7043629288673401
I0418 13:27:53.736273 139781129688896 spec.py:298] Evaluating on the training split.
I0418 13:27:53.744187 139781129688896 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:27:53.748610 139781129688896 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:27:53.806844 139781129688896 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0418 13:28:09.627856 139781129688896 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0418 13:29:24.971905 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 13:29:24.974908 139781129688896 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:29:24.978467 139781129688896 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:29:25.036249 139781129688896 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:30:28.300833 139781129688896 spec.py:326] Evaluating on the test split.
I0418 13:30:28.303507 139781129688896 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:30:28.307426 139781129688896 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0418 13:30:28.361477 139781129688896 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0418 13:31:32.418097 139781129688896 submission_runner.py:406] Time since start: 238.36s, 	Step: 1, 	{'train/accuracy': 0.5429479479789734, 'train/loss': 0.7052029967308044, 'train/mean_average_precision': 0.02293376236866475, 'validation/accuracy': 0.5445422530174255, 'validation/loss': 0.7040449976921082, 'validation/mean_average_precision': 0.026447045410745308, 'validation/num_examples': 43793, 'test/accuracy': 0.5437210202217102, 'test/loss': 0.704443633556366, 'test/mean_average_precision': 0.029862745806323487, 'test/num_examples': 43793, 'score': 19.682271480560303, 'total_duration': 238.3642337322235, 'accumulated_submission_time': 19.682271480560303, 'accumulated_eval_time': 218.6817877292633, 'accumulated_logging_time': 0}
I0418 13:31:32.438929 139595504748288 logging_writer.py:48] [1] accumulated_eval_time=218.681788, accumulated_logging_time=0, accumulated_submission_time=19.682271, global_step=1, preemption_count=0, score=19.682271, test/accuracy=0.543721, test/loss=0.704444, test/mean_average_precision=0.029863, test/num_examples=43793, total_duration=238.364234, train/accuracy=0.542948, train/loss=0.705203, train/mean_average_precision=0.022934, validation/accuracy=0.544542, validation/loss=0.704045, validation/mean_average_precision=0.026447, validation/num_examples=43793
I0418 13:31:32.465696 139781129688896 checkpoints.py:356] Saving checkpoint at step: 1
I0418 13:31:32.529623 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_1
I0418 13:31:32.529833 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_1.
I0418 13:31:56.580185 139595513140992 logging_writer.py:48] [100] global_step=100, grad_norm=0.06384989619255066, loss=0.0906374529004097
I0418 13:32:19.966112 139596637665024 logging_writer.py:48] [200] global_step=200, grad_norm=0.016484446823596954, loss=0.06001731753349304
I0418 13:32:43.333472 139595513140992 logging_writer.py:48] [300] global_step=300, grad_norm=0.009152206592261791, loss=0.055927738547325134
I0418 13:33:06.561727 139596637665024 logging_writer.py:48] [400] global_step=400, grad_norm=0.010548300109803677, loss=0.05349492281675339
I0418 13:33:29.960871 139595513140992 logging_writer.py:48] [500] global_step=500, grad_norm=0.01579524204134941, loss=0.06168464571237564
I0418 13:33:53.642075 139596637665024 logging_writer.py:48] [600] global_step=600, grad_norm=0.011382748372852802, loss=0.059669043868780136
I0418 13:34:17.377843 139595513140992 logging_writer.py:48] [700] global_step=700, grad_norm=0.013241026550531387, loss=0.05163770914077759
I0418 13:34:40.596329 139596637665024 logging_writer.py:48] [800] global_step=800, grad_norm=0.011301323771476746, loss=0.04781210795044899
I0418 13:35:04.062309 139595513140992 logging_writer.py:48] [900] global_step=900, grad_norm=0.035090986639261246, loss=0.05384253337979317
I0418 13:35:27.541017 139596637665024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0372791588306427, loss=0.05465918779373169
I0418 13:35:32.739957 139781129688896 spec.py:298] Evaluating on the training split.
I0418 13:36:47.427975 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 13:36:50.052199 139781129688896 spec.py:326] Evaluating on the test split.
I0418 13:36:52.613774 139781129688896 submission_runner.py:406] Time since start: 558.56s, 	Step: 1023, 	{'train/accuracy': 0.9866930246353149, 'train/loss': 0.054638881236314774, 'train/mean_average_precision': 0.03441651368058085, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06424511969089508, 'validation/mean_average_precision': 0.03772955234784554, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0674813985824585, 'test/mean_average_precision': 0.038605488087016665, 'test/num_examples': 43793, 'score': 259.88331842422485, 'total_duration': 558.5598776340485, 'accumulated_submission_time': 259.88331842422485, 'accumulated_eval_time': 298.55555272102356, 'accumulated_logging_time': 0.11185169219970703}
I0418 13:36:52.621558 139595513140992 logging_writer.py:48] [1023] accumulated_eval_time=298.555553, accumulated_logging_time=0.111852, accumulated_submission_time=259.883318, global_step=1023, preemption_count=0, score=259.883318, test/accuracy=0.983142, test/loss=0.067481, test/mean_average_precision=0.038605, test/num_examples=43793, total_duration=558.559878, train/accuracy=0.986693, train/loss=0.054639, train/mean_average_precision=0.034417, validation/accuracy=0.984118, validation/loss=0.064245, validation/mean_average_precision=0.037730, validation/num_examples=43793
I0418 13:36:52.646091 139781129688896 checkpoints.py:356] Saving checkpoint at step: 1023
I0418 13:36:52.715417 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_1023
I0418 13:36:52.715636 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_1023.
I0418 13:37:11.103690 139596637665024 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.037255577743053436, loss=0.04932955279946327
I0418 13:37:34.850933 139596612486912 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.026455378159880638, loss=0.052938032895326614
I0418 13:37:58.749008 139596637665024 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.07524612545967102, loss=0.048177532851696014
I0418 13:38:22.832176 139596612486912 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.042128175497055054, loss=0.052124131470918655
I0418 13:38:46.562137 139596637665024 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.024086330085992813, loss=0.051363565027713776
I0418 13:39:10.220383 139596612486912 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.030390912666916847, loss=0.05406709015369415
I0418 13:39:34.240103 139596637665024 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.019338982179760933, loss=0.05352860316634178
I0418 13:39:58.259053 139596612486912 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.05979588255286217, loss=0.05445953458547592
I0418 13:40:22.085963 139596637665024 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.223291277885437, loss=0.05446009710431099
I0418 13:40:45.929768 139596612486912 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.07148648053407669, loss=0.05013183131814003
I0418 13:40:52.917851 139781129688896 spec.py:298] Evaluating on the training split.
I0418 13:42:08.821068 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 13:42:11.420266 139781129688896 spec.py:326] Evaluating on the test split.
I0418 13:42:13.950797 139781129688896 submission_runner.py:406] Time since start: 879.90s, 	Step: 2030, 	{'train/accuracy': 0.9868617653846741, 'train/loss': 0.05151265859603882, 'train/mean_average_precision': 0.05409079046671736, 'validation/accuracy': 0.9841499924659729, 'validation/loss': 0.060921572148799896, 'validation/mean_average_precision': 0.05322389308875848, 'validation/num_examples': 43793, 'test/accuracy': 0.9831584692001343, 'test/loss': 0.06406452506780624, 'test/mean_average_precision': 0.054370858382504404, 'test/num_examples': 43793, 'score': 500.07626819610596, 'total_duration': 879.896918296814, 'accumulated_submission_time': 500.07626819610596, 'accumulated_eval_time': 379.588463306427, 'accumulated_logging_time': 0.21387648582458496}
I0418 13:42:13.958570 139596637665024 logging_writer.py:48] [2030] accumulated_eval_time=379.588463, accumulated_logging_time=0.213876, accumulated_submission_time=500.076268, global_step=2030, preemption_count=0, score=500.076268, test/accuracy=0.983158, test/loss=0.064065, test/mean_average_precision=0.054371, test/num_examples=43793, total_duration=879.896918, train/accuracy=0.986862, train/loss=0.051513, train/mean_average_precision=0.054091, validation/accuracy=0.984150, validation/loss=0.060922, validation/mean_average_precision=0.053224, validation/num_examples=43793
I0418 13:42:13.983632 139781129688896 checkpoints.py:356] Saving checkpoint at step: 2030
I0418 13:42:14.042786 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_2030
I0418 13:42:14.042979 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_2030.
I0418 13:42:30.894948 139596612486912 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.048540156334638596, loss=0.05255277454853058
I0418 13:42:54.529484 139596587308800 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0685732513666153, loss=0.0552978441119194
I0418 13:43:18.092416 139596612486912 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.03440546989440918, loss=0.053900137543678284
I0418 13:43:41.680682 139596587308800 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.037619780749082565, loss=0.050580911338329315
I0418 13:44:05.643218 139596612486912 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.04589904844760895, loss=0.05186620354652405
I0418 13:44:29.508804 139596587308800 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.05499916523694992, loss=0.054239433258771896
I0418 13:44:53.587062 139596612486912 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.04306812956929207, loss=0.05466189607977867
I0418 13:45:17.244055 139596587308800 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.07097727805376053, loss=0.04998650774359703
I0418 13:45:40.789696 139596612486912 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.07926090806722641, loss=0.05050402134656906
I0418 13:46:04.552927 139596587308800 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.040012966841459274, loss=0.05016939714550972
I0418 13:46:14.203790 139781129688896 spec.py:298] Evaluating on the training split.
I0418 13:47:28.542509 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 13:47:31.111070 139781129688896 spec.py:326] Evaluating on the test split.
I0418 13:47:33.628124 139781129688896 submission_runner.py:406] Time since start: 1199.57s, 	Step: 3042, 	{'train/accuracy': 0.987036406993866, 'train/loss': 0.04926370084285736, 'train/mean_average_precision': 0.08236493539280187, 'validation/accuracy': 0.9842790961265564, 'validation/loss': 0.05900658667087555, 'validation/mean_average_precision': 0.08129145142374103, 'validation/num_examples': 43793, 'test/accuracy': 0.9832751750946045, 'test/loss': 0.06226073205471039, 'test/mean_average_precision': 0.08162810923061385, 'test/num_examples': 43793, 'score': 740.2281839847565, 'total_duration': 1199.5742447376251, 'accumulated_submission_time': 740.2281839847565, 'accumulated_eval_time': 459.01275658607483, 'accumulated_logging_time': 0.30620479583740234}
I0418 13:47:33.635831 139596612486912 logging_writer.py:48] [3042] accumulated_eval_time=459.012757, accumulated_logging_time=0.306205, accumulated_submission_time=740.228184, global_step=3042, preemption_count=0, score=740.228184, test/accuracy=0.983275, test/loss=0.062261, test/mean_average_precision=0.081628, test/num_examples=43793, total_duration=1199.574245, train/accuracy=0.987036, train/loss=0.049264, train/mean_average_precision=0.082365, validation/accuracy=0.984279, validation/loss=0.059007, validation/mean_average_precision=0.081291, validation/num_examples=43793
I0418 13:47:33.661355 139781129688896 checkpoints.py:356] Saving checkpoint at step: 3042
I0418 13:47:33.721352 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_3042
I0418 13:47:33.721614 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_3042.
I0418 13:47:47.593209 139596587308800 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.1244950145483017, loss=0.048844821751117706
I0418 13:48:10.896717 139596578916096 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.030394112691283226, loss=0.045521702617406845
I0418 13:48:34.160558 139596587308800 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.030631300061941147, loss=0.04900047928094864
I0418 13:48:57.435998 139596578916096 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.039260704070329666, loss=0.05167312175035477
I0418 13:49:20.488369 139596587308800 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05038606375455856, loss=0.04975152760744095
I0418 13:49:43.501651 139596578916096 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.04005037620663643, loss=0.05247117206454277
I0418 13:50:06.650944 139596587308800 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.07015068829059601, loss=0.05332544445991516
I0418 13:50:29.587150 139596578916096 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.04724613204598427, loss=0.04316025227308273
I0418 13:50:52.530113 139596587308800 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.07306845486164093, loss=0.05059927701950073
I0418 13:51:15.463402 139596578916096 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.040146604180336, loss=0.04686688259243965
I0418 13:51:33.856731 139781129688896 spec.py:298] Evaluating on the training split.
I0418 13:52:45.892223 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 13:52:48.455180 139781129688896 spec.py:326] Evaluating on the test split.
I0418 13:52:50.977132 139781129688896 submission_runner.py:406] Time since start: 1516.92s, 	Step: 4080, 	{'train/accuracy': 0.9868795871734619, 'train/loss': 0.049820587038993835, 'train/mean_average_precision': 0.10489179664525483, 'validation/accuracy': 0.9842259287834167, 'validation/loss': 0.05969662964344025, 'validation/mean_average_precision': 0.10387286966390229, 'validation/num_examples': 43793, 'test/accuracy': 0.9832599759101868, 'test/loss': 0.06301921606063843, 'test/mean_average_precision': 0.1049174688726959, 'test/num_examples': 43793, 'score': 980.3539943695068, 'total_duration': 1516.923234462738, 'accumulated_submission_time': 980.3539943695068, 'accumulated_eval_time': 536.1331028938293, 'accumulated_logging_time': 0.3998680114746094}
I0418 13:52:50.985090 139596587308800 logging_writer.py:48] [4080] accumulated_eval_time=536.133103, accumulated_logging_time=0.399868, accumulated_submission_time=980.353994, global_step=4080, preemption_count=0, score=980.353994, test/accuracy=0.983260, test/loss=0.063019, test/mean_average_precision=0.104917, test/num_examples=43793, total_duration=1516.923234, train/accuracy=0.986880, train/loss=0.049821, train/mean_average_precision=0.104892, validation/accuracy=0.984226, validation/loss=0.059697, validation/mean_average_precision=0.103873, validation/num_examples=43793
I0418 13:52:51.010853 139781129688896 checkpoints.py:356] Saving checkpoint at step: 4080
I0418 13:52:51.066778 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_4080
I0418 13:52:51.066970 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_4080.
I0418 13:52:55.951450 139596578916096 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.04926044121384621, loss=0.05088034272193909
I0418 13:53:19.032119 139596570523392 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.029014434665441513, loss=0.045186661183834076
I0418 13:53:42.280413 139596578916096 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.07974007725715637, loss=0.041674915701150894
I0418 13:54:05.742733 139596570523392 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04858672618865967, loss=0.047992389649152756
I0418 13:54:28.752532 139596578916096 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.060060612857341766, loss=0.04861297458410263
I0418 13:54:51.804615 139596570523392 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0836249440908432, loss=0.04651842266321182
I0418 13:55:14.662238 139596578916096 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.04018717259168625, loss=0.047123122960329056
I0418 13:55:38.196247 139596570523392 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.03300228342413902, loss=0.04801749438047409
I0418 13:56:01.378877 139596578916096 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.06511977314949036, loss=0.04803505539894104
I0418 13:56:24.444680 139596570523392 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1414038985967636, loss=0.04625575244426727
I0418 13:56:47.442770 139596578916096 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.05498485267162323, loss=0.04527641087770462
I0418 13:56:51.124010 139781129688896 spec.py:298] Evaluating on the training split.
I0418 13:58:04.469752 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 13:58:07.078460 139781129688896 spec.py:326] Evaluating on the test split.
I0418 13:58:09.572539 139781129688896 submission_runner.py:406] Time since start: 1835.52s, 	Step: 5117, 	{'train/accuracy': 0.9871649742126465, 'train/loss': 0.04631287604570389, 'train/mean_average_precision': 0.12998175237861906, 'validation/accuracy': 0.9845786690711975, 'validation/loss': 0.05568525940179825, 'validation/mean_average_precision': 0.12287080933542872, 'validation/num_examples': 43793, 'test/accuracy': 0.983567476272583, 'test/loss': 0.058687541633844376, 'test/mean_average_precision': 0.12488761510871799, 'test/num_examples': 43793, 'score': 1220.4017510414124, 'total_duration': 1835.5186605453491, 'accumulated_submission_time': 1220.4017510414124, 'accumulated_eval_time': 614.5815954208374, 'accumulated_logging_time': 0.4898653030395508}
I0418 13:58:09.580597 139596570523392 logging_writer.py:48] [5117] accumulated_eval_time=614.581595, accumulated_logging_time=0.489865, accumulated_submission_time=1220.401751, global_step=5117, preemption_count=0, score=1220.401751, test/accuracy=0.983567, test/loss=0.058688, test/mean_average_precision=0.124888, test/num_examples=43793, total_duration=1835.518661, train/accuracy=0.987165, train/loss=0.046313, train/mean_average_precision=0.129982, validation/accuracy=0.984579, validation/loss=0.055685, validation/mean_average_precision=0.122871, validation/num_examples=43793
I0418 13:58:09.604725 139781129688896 checkpoints.py:356] Saving checkpoint at step: 5117
I0418 13:58:09.660771 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_5117
I0418 13:58:09.660964 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_5117.
I0418 13:58:28.925784 139596578916096 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.08269298821687698, loss=0.046516139060258865
I0418 13:58:51.630448 139596562130688 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.05568816140294075, loss=0.04783494397997856
I0418 13:59:14.478930 139596578916096 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05270608514547348, loss=0.045866210013628006
I0418 13:59:37.590394 139596562130688 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.10134806483983994, loss=0.047526873648166656
I0418 14:00:01.093736 139596578916096 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0946059450507164, loss=0.046886149793863297
I0418 14:00:24.126578 139596562130688 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.05195670574903488, loss=0.04590678587555885
I0418 14:00:47.139912 139596578916096 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.043718308210372925, loss=0.044766977429389954
I0418 14:01:10.388695 139596562130688 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0827106386423111, loss=0.04496573656797409
I0418 14:01:33.285264 139596578916096 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0534590445458889, loss=0.046097271144390106
I0418 14:01:56.158234 139596562130688 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.05666860193014145, loss=0.05052505061030388
I0418 14:02:09.758332 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:03:20.337025 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:03:22.890418 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:03:25.385500 139781129688896 submission_runner.py:406] Time since start: 2151.33s, 	Step: 6160, 	{'train/accuracy': 0.9876853227615356, 'train/loss': 0.04344348609447479, 'train/mean_average_precision': 0.1539017428606625, 'validation/accuracy': 0.985015869140625, 'validation/loss': 0.052276480942964554, 'validation/mean_average_precision': 0.1441541873087821, 'validation/num_examples': 43793, 'test/accuracy': 0.9839941263198853, 'test/loss': 0.05509023368358612, 'test/mean_average_precision': 0.141637436815036, 'test/num_examples': 43793, 'score': 1460.489696264267, 'total_duration': 2151.3316016197205, 'accumulated_submission_time': 1460.489696264267, 'accumulated_eval_time': 690.2087121009827, 'accumulated_logging_time': 0.5784273147583008}
I0418 14:03:25.393260 139596578916096 logging_writer.py:48] [6160] accumulated_eval_time=690.208712, accumulated_logging_time=0.578427, accumulated_submission_time=1460.489696, global_step=6160, preemption_count=0, score=1460.489696, test/accuracy=0.983994, test/loss=0.055090, test/mean_average_precision=0.141637, test/num_examples=43793, total_duration=2151.331602, train/accuracy=0.987685, train/loss=0.043443, train/mean_average_precision=0.153902, validation/accuracy=0.985016, validation/loss=0.052276, validation/mean_average_precision=0.144154, validation/num_examples=43793
I0418 14:03:25.418424 139781129688896 checkpoints.py:356] Saving checkpoint at step: 6160
I0418 14:03:25.476336 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_6160
I0418 14:03:25.476527 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_6160.
I0418 14:03:35.011355 139596562130688 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.04156627878546715, loss=0.046559009701013565
I0418 14:03:57.985430 139596553737984 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0505344420671463, loss=0.04515810310840607
I0418 14:04:21.110537 139596562130688 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.07935604453086853, loss=0.04700826108455658
I0418 14:04:44.023335 139596553737984 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.07812318950891495, loss=0.04842161014676094
I0418 14:05:07.272792 139596562130688 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03902459889650345, loss=0.04901343584060669
I0418 14:05:30.486942 139596553737984 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06456369161605835, loss=0.045082926750183105
I0418 14:05:53.821922 139596562130688 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.05423368141055107, loss=0.045707158744335175
I0418 14:06:17.159613 139596553737984 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.047719962894916534, loss=0.046183038502931595
I0418 14:06:40.572047 139596562130688 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.030148399993777275, loss=0.04683718830347061
I0418 14:07:04.020240 139596553737984 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.050414059311151505, loss=0.04507729411125183
I0418 14:07:25.497000 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:08:36.406399 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:08:39.063319 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:08:41.599722 139781129688896 submission_runner.py:406] Time since start: 2467.55s, 	Step: 7194, 	{'train/accuracy': 0.9878345131874084, 'train/loss': 0.04189693182706833, 'train/mean_average_precision': 0.1805107657654199, 'validation/accuracy': 0.9850792288780212, 'validation/loss': 0.05195567384362221, 'validation/mean_average_precision': 0.15986134781277692, 'validation/num_examples': 43793, 'test/accuracy': 0.9840627908706665, 'test/loss': 0.055033162236213684, 'test/mean_average_precision': 0.15360869780300943, 'test/num_examples': 43793, 'score': 1700.500934123993, 'total_duration': 2467.5457813739777, 'accumulated_submission_time': 1700.500934123993, 'accumulated_eval_time': 766.3113350868225, 'accumulated_logging_time': 0.6695899963378906}
I0418 14:08:41.607778 139596562130688 logging_writer.py:48] [7194] accumulated_eval_time=766.311335, accumulated_logging_time=0.669590, accumulated_submission_time=1700.500934, global_step=7194, preemption_count=0, score=1700.500934, test/accuracy=0.984063, test/loss=0.055033, test/mean_average_precision=0.153609, test/num_examples=43793, total_duration=2467.545781, train/accuracy=0.987835, train/loss=0.041897, train/mean_average_precision=0.180511, validation/accuracy=0.985079, validation/loss=0.051956, validation/mean_average_precision=0.159861, validation/num_examples=43793
I0418 14:08:41.631948 139781129688896 checkpoints.py:356] Saving checkpoint at step: 7194
I0418 14:08:41.705425 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_7194
I0418 14:08:41.705643 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_7194.
I0418 14:08:43.345722 139596553737984 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.03417972847819328, loss=0.04349329322576523
I0418 14:09:06.783759 139596545345280 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0542801097035408, loss=0.04388715326786041
I0418 14:09:30.036501 139596553737984 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.06629375368356705, loss=0.04413437098264694
I0418 14:09:53.147557 139596545345280 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0630965381860733, loss=0.04339654743671417
I0418 14:10:16.534406 139596553737984 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.04491671547293663, loss=0.041483324021101
I0418 14:10:39.768178 139596545345280 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0465492382645607, loss=0.04615884646773338
I0418 14:11:03.120695 139596553737984 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.07605545967817307, loss=0.04671446606516838
I0418 14:11:26.459451 139596545345280 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.040829043835401535, loss=0.04340916499495506
I0418 14:11:49.866264 139596553737984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0343342125415802, loss=0.04256461560726166
I0418 14:12:13.368379 139596545345280 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.04402969405055046, loss=0.04526906833052635
I0418 14:12:36.431083 139596553737984 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0691814050078392, loss=0.05072890222072601
I0418 14:12:41.809120 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:13:53.985792 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:13:56.510535 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:13:58.984038 139781129688896 submission_runner.py:406] Time since start: 2784.93s, 	Step: 8224, 	{'train/accuracy': 0.9879050850868225, 'train/loss': 0.04172813519835472, 'train/mean_average_precision': 0.19268153249144362, 'validation/accuracy': 0.9850134253501892, 'validation/loss': 0.05249375104904175, 'validation/mean_average_precision': 0.16435452907067313, 'validation/num_examples': 43793, 'test/accuracy': 0.9840350151062012, 'test/loss': 0.05536283180117607, 'test/mean_average_precision': 0.16440828060766188, 'test/num_examples': 43793, 'score': 1940.5952956676483, 'total_duration': 2784.930158853531, 'accumulated_submission_time': 1940.5952956676483, 'accumulated_eval_time': 843.4862151145935, 'accumulated_logging_time': 0.7757112979888916}
I0418 14:13:58.992202 139596545345280 logging_writer.py:48] [8224] accumulated_eval_time=843.486215, accumulated_logging_time=0.775711, accumulated_submission_time=1940.595296, global_step=8224, preemption_count=0, score=1940.595296, test/accuracy=0.984035, test/loss=0.055363, test/mean_average_precision=0.164408, test/num_examples=43793, total_duration=2784.930159, train/accuracy=0.987905, train/loss=0.041728, train/mean_average_precision=0.192682, validation/accuracy=0.985013, validation/loss=0.052494, validation/mean_average_precision=0.164355, validation/num_examples=43793
I0418 14:13:59.017512 139781129688896 checkpoints.py:356] Saving checkpoint at step: 8224
I0418 14:13:59.072024 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_8224
I0418 14:13:59.072217 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_8224.
I0418 14:14:17.449973 139596553737984 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.07096327096223831, loss=0.044228695333004
I0418 14:14:40.766101 139596536952576 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.03711063414812088, loss=0.04563962668180466
I0418 14:15:03.719040 139596553737984 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.07917892187833786, loss=0.05017605423927307
I0418 14:15:26.613386 139596536952576 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.09418082237243652, loss=0.04206821694970131
I0418 14:15:49.584951 139596553737984 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.07408695667982101, loss=0.04103582724928856
I0418 14:16:12.503994 139596536952576 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.052743736654520035, loss=0.04206278175115585
I0418 14:16:34.984034 139596553737984 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.05397490784525871, loss=0.046036675572395325
I0418 14:16:58.160359 139596536952576 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.035782814025878906, loss=0.04337886720895767
I0418 14:17:21.537712 139596553737984 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.042464543133974075, loss=0.03999483212828636
I0418 14:17:44.719128 139596536952576 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.044437117874622345, loss=0.043185796588659286
I0418 14:17:59.242926 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:19:13.072173 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:19:15.651254 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:19:18.129388 139781129688896 submission_runner.py:406] Time since start: 3104.08s, 	Step: 9263, 	{'train/accuracy': 0.988120436668396, 'train/loss': 0.04064522311091423, 'train/mean_average_precision': 0.20533786743130192, 'validation/accuracy': 0.985274076461792, 'validation/loss': 0.05050579085946083, 'validation/mean_average_precision': 0.18128174562249086, 'validation/num_examples': 43793, 'test/accuracy': 0.9843913316726685, 'test/loss': 0.053332891315221786, 'test/mean_average_precision': 0.17890813696071747, 'test/num_examples': 43793, 'score': 2180.7569694519043, 'total_duration': 3104.0755071640015, 'accumulated_submission_time': 2180.7569694519043, 'accumulated_eval_time': 922.3726365566254, 'accumulated_logging_time': 0.8640422821044922}
I0418 14:19:18.138065 139596553737984 logging_writer.py:48] [9263] accumulated_eval_time=922.372637, accumulated_logging_time=0.864042, accumulated_submission_time=2180.756969, global_step=9263, preemption_count=0, score=2180.756969, test/accuracy=0.984391, test/loss=0.053333, test/mean_average_precision=0.178908, test/num_examples=43793, total_duration=3104.075507, train/accuracy=0.988120, train/loss=0.040645, train/mean_average_precision=0.205338, validation/accuracy=0.985274, validation/loss=0.050506, validation/mean_average_precision=0.181282, validation/num_examples=43793
I0418 14:19:18.163124 139781129688896 checkpoints.py:356] Saving checkpoint at step: 9263
I0418 14:19:18.219336 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_9263
I0418 14:19:18.219532 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_9263.
I0418 14:19:27.045929 139596536952576 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0414060615003109, loss=0.04111393168568611
I0418 14:19:50.288915 139596528559872 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.05115951597690582, loss=0.0432131253182888
I0418 14:20:13.142337 139596536952576 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.08177375048398972, loss=0.05070796608924866
I0418 14:20:35.704773 139596528559872 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.04798911511898041, loss=0.04659401625394821
I0418 14:20:58.477656 139596536952576 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.056608300656080246, loss=0.04556809738278389
I0418 14:21:21.142772 139596528559872 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.038736578077077866, loss=0.045209914445877075
I0418 14:21:43.764850 139596536952576 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.04142870754003525, loss=0.040571026504039764
I0418 14:22:06.778811 139596528559872 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.06301242113113403, loss=0.046656619757413864
I0418 14:22:29.937107 139596536952576 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.051601484417915344, loss=0.04307815805077553
I0418 14:22:53.044596 139596528559872 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.06147431209683418, loss=0.042431339621543884
I0418 14:23:15.855775 139596536952576 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.09089858084917068, loss=0.03923405334353447
I0418 14:23:18.342387 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:24:29.931978 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:24:32.467173 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:24:34.970044 139781129688896 submission_runner.py:406] Time since start: 3420.92s, 	Step: 10312, 	{'train/accuracy': 0.9886313080787659, 'train/loss': 0.039485637098550797, 'train/mean_average_precision': 0.2288583739259451, 'validation/accuracy': 0.9856836199760437, 'validation/loss': 0.04979066550731659, 'validation/mean_average_precision': 0.18963492989043745, 'validation/num_examples': 43793, 'test/accuracy': 0.9846752285957336, 'test/loss': 0.05263019725680351, 'test/mean_average_precision': 0.1842432002058052, 'test/num_examples': 43793, 'score': 2420.8706505298615, 'total_duration': 3420.91614818573, 'accumulated_submission_time': 2420.8706505298615, 'accumulated_eval_time': 999.0002341270447, 'accumulated_logging_time': 0.9543282985687256}
I0418 14:24:34.978055 139596528559872 logging_writer.py:48] [10312] accumulated_eval_time=999.000234, accumulated_logging_time=0.954328, accumulated_submission_time=2420.870651, global_step=10312, preemption_count=0, score=2420.870651, test/accuracy=0.984675, test/loss=0.052630, test/mean_average_precision=0.184243, test/num_examples=43793, total_duration=3420.916148, train/accuracy=0.988631, train/loss=0.039486, train/mean_average_precision=0.228858, validation/accuracy=0.985684, validation/loss=0.049791, validation/mean_average_precision=0.189635, validation/num_examples=43793
I0418 14:24:35.002110 139781129688896 checkpoints.py:356] Saving checkpoint at step: 10312
I0418 14:24:35.057498 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_10312
I0418 14:24:35.057687 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_10312.
I0418 14:24:55.237193 139596536952576 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.06596751511096954, loss=0.043341971933841705
I0418 14:25:18.043432 139596520167168 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.03441968560218811, loss=0.04333226755261421
I0418 14:25:40.780144 139596536952576 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.055703043937683105, loss=0.04990732669830322
I0418 14:26:03.411261 139596520167168 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.036641668528318405, loss=0.041023992002010345
I0418 14:26:25.808274 139596536952576 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03799191489815712, loss=0.04060327634215355
I0418 14:26:48.706741 139596520167168 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.04395878314971924, loss=0.046086907386779785
I0418 14:27:11.349054 139596536952576 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.05113954469561577, loss=0.039459746330976486
I0418 14:27:33.650470 139596520167168 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.054815102368593216, loss=0.041449837386608124
I0418 14:27:56.132238 139596536952576 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.03710702806711197, loss=0.044332727789878845
I0418 14:28:18.665207 139596520167168 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.050228990614414215, loss=0.03957601264119148
I0418 14:28:35.123894 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:29:46.308212 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:29:48.839813 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:29:51.354637 139781129688896 submission_runner.py:406] Time since start: 3737.30s, 	Step: 11375, 	{'train/accuracy': 0.9886831045150757, 'train/loss': 0.03871716186404228, 'train/mean_average_precision': 0.24670198391595166, 'validation/accuracy': 0.9858212471008301, 'validation/loss': 0.04839019477367401, 'validation/mean_average_precision': 0.19438326396970398, 'validation/num_examples': 43793, 'test/accuracy': 0.984816312789917, 'test/loss': 0.050952132791280746, 'test/mean_average_precision': 0.18911796365170772, 'test/num_examples': 43793, 'score': 2660.927347421646, 'total_duration': 3737.3007578849792, 'accumulated_submission_time': 2660.927347421646, 'accumulated_eval_time': 1075.2309391498566, 'accumulated_logging_time': 1.0421357154846191}
I0418 14:29:51.363202 139596536952576 logging_writer.py:48] [11375] accumulated_eval_time=1075.230939, accumulated_logging_time=1.042136, accumulated_submission_time=2660.927347, global_step=11375, preemption_count=0, score=2660.927347, test/accuracy=0.984816, test/loss=0.050952, test/mean_average_precision=0.189118, test/num_examples=43793, total_duration=3737.300758, train/accuracy=0.988683, train/loss=0.038717, train/mean_average_precision=0.246702, validation/accuracy=0.985821, validation/loss=0.048390, validation/mean_average_precision=0.194383, validation/num_examples=43793
I0418 14:29:51.391475 139781129688896 checkpoints.py:356] Saving checkpoint at step: 11375
I0418 14:29:51.457368 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_11375
I0418 14:29:51.457578 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_11375.
I0418 14:29:57.253665 139596520167168 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.058036793023347855, loss=0.04380060359835625
I0418 14:30:19.412476 139596436338432 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.053290512412786484, loss=0.04077526926994324
I0418 14:30:41.577089 139596520167168 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.05517350882291794, loss=0.04201744496822357
I0418 14:31:04.072119 139596436338432 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.04072064906358719, loss=0.045169711112976074
I0418 14:31:26.524476 139596520167168 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.06059347838163376, loss=0.04190494865179062
I0418 14:31:49.169612 139596436338432 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04441552236676216, loss=0.04203154891729355
I0418 14:32:11.472577 139781129688896 spec.py:298] Evaluating on the training split.
I0418 14:33:21.773621 139781129688896 spec.py:310] Evaluating on the validation split.
I0418 14:33:24.377609 139781129688896 spec.py:326] Evaluating on the test split.
I0418 14:33:26.936085 139781129688896 submission_runner.py:406] Time since start: 3952.88s, 	Step: 12000, 	{'train/accuracy': 0.9890701174736023, 'train/loss': 0.03751709312200546, 'train/mean_average_precision': 0.24765517220597483, 'validation/accuracy': 0.9858996272087097, 'validation/loss': 0.04812009632587433, 'validation/mean_average_precision': 0.20808504750873733, 'validation/num_examples': 43793, 'test/accuracy': 0.9849826693534851, 'test/loss': 0.050752174109220505, 'test/mean_average_precision': 0.20403395279226924, 'test/num_examples': 43793, 'score': 2800.936535835266, 'total_duration': 3952.882199525833, 'accumulated_submission_time': 2800.936535835266, 'accumulated_eval_time': 1150.6944134235382, 'accumulated_logging_time': 1.1452651023864746}
I0418 14:33:26.945651 139596520167168 logging_writer.py:48] [12000] accumulated_eval_time=1150.694413, accumulated_logging_time=1.145265, accumulated_submission_time=2800.936536, global_step=12000, preemption_count=0, score=2800.936536, test/accuracy=0.984983, test/loss=0.050752, test/mean_average_precision=0.204034, test/num_examples=43793, total_duration=3952.882200, train/accuracy=0.989070, train/loss=0.037517, train/mean_average_precision=0.247655, validation/accuracy=0.985900, validation/loss=0.048120, validation/mean_average_precision=0.208085, validation/num_examples=43793
I0418 14:33:26.972595 139781129688896 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:33:27.050600 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_12000
I0418 14:33:27.050839 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:33:27.059540 139596436338432 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2800.936536
I0418 14:33:27.078310 139781129688896 checkpoints.py:356] Saving checkpoint at step: 12000
I0418 14:33:27.205365 139781129688896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_12000
I0418 14:33:27.205598 139781129688896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/ogbg_jax/trial_1/checkpoint_12000.
I0418 14:33:27.355959 139781129688896 submission_runner.py:567] Tuning trial 1/1
I0418 14:33:27.356189 139781129688896 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0418 14:33:27.357265 139781129688896 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5429479479789734, 'train/loss': 0.7052029967308044, 'train/mean_average_precision': 0.02293376236866475, 'validation/accuracy': 0.5445422530174255, 'validation/loss': 0.7040449976921082, 'validation/mean_average_precision': 0.026447045410745308, 'validation/num_examples': 43793, 'test/accuracy': 0.5437210202217102, 'test/loss': 0.704443633556366, 'test/mean_average_precision': 0.029862745806323487, 'test/num_examples': 43793, 'score': 19.682271480560303, 'total_duration': 238.3642337322235, 'accumulated_submission_time': 19.682271480560303, 'accumulated_eval_time': 218.6817877292633, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1023, {'train/accuracy': 0.9866930246353149, 'train/loss': 0.054638881236314774, 'train/mean_average_precision': 0.03441651368058085, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06424511969089508, 'validation/mean_average_precision': 0.03772955234784554, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0674813985824585, 'test/mean_average_precision': 0.038605488087016665, 'test/num_examples': 43793, 'score': 259.88331842422485, 'total_duration': 558.5598776340485, 'accumulated_submission_time': 259.88331842422485, 'accumulated_eval_time': 298.55555272102356, 'accumulated_logging_time': 0.11185169219970703, 'global_step': 1023, 'preemption_count': 0}), (2030, {'train/accuracy': 0.9868617653846741, 'train/loss': 0.05151265859603882, 'train/mean_average_precision': 0.05409079046671736, 'validation/accuracy': 0.9841499924659729, 'validation/loss': 0.060921572148799896, 'validation/mean_average_precision': 0.05322389308875848, 'validation/num_examples': 43793, 'test/accuracy': 0.9831584692001343, 'test/loss': 0.06406452506780624, 'test/mean_average_precision': 0.054370858382504404, 'test/num_examples': 43793, 'score': 500.07626819610596, 'total_duration': 879.896918296814, 'accumulated_submission_time': 500.07626819610596, 'accumulated_eval_time': 379.588463306427, 'accumulated_logging_time': 0.21387648582458496, 'global_step': 2030, 'preemption_count': 0}), (3042, {'train/accuracy': 0.987036406993866, 'train/loss': 0.04926370084285736, 'train/mean_average_precision': 0.08236493539280187, 'validation/accuracy': 0.9842790961265564, 'validation/loss': 0.05900658667087555, 'validation/mean_average_precision': 0.08129145142374103, 'validation/num_examples': 43793, 'test/accuracy': 0.9832751750946045, 'test/loss': 0.06226073205471039, 'test/mean_average_precision': 0.08162810923061385, 'test/num_examples': 43793, 'score': 740.2281839847565, 'total_duration': 1199.5742447376251, 'accumulated_submission_time': 740.2281839847565, 'accumulated_eval_time': 459.01275658607483, 'accumulated_logging_time': 0.30620479583740234, 'global_step': 3042, 'preemption_count': 0}), (4080, {'train/accuracy': 0.9868795871734619, 'train/loss': 0.049820587038993835, 'train/mean_average_precision': 0.10489179664525483, 'validation/accuracy': 0.9842259287834167, 'validation/loss': 0.05969662964344025, 'validation/mean_average_precision': 0.10387286966390229, 'validation/num_examples': 43793, 'test/accuracy': 0.9832599759101868, 'test/loss': 0.06301921606063843, 'test/mean_average_precision': 0.1049174688726959, 'test/num_examples': 43793, 'score': 980.3539943695068, 'total_duration': 1516.923234462738, 'accumulated_submission_time': 980.3539943695068, 'accumulated_eval_time': 536.1331028938293, 'accumulated_logging_time': 0.3998680114746094, 'global_step': 4080, 'preemption_count': 0}), (5117, {'train/accuracy': 0.9871649742126465, 'train/loss': 0.04631287604570389, 'train/mean_average_precision': 0.12998175237861906, 'validation/accuracy': 0.9845786690711975, 'validation/loss': 0.05568525940179825, 'validation/mean_average_precision': 0.12287080933542872, 'validation/num_examples': 43793, 'test/accuracy': 0.983567476272583, 'test/loss': 0.058687541633844376, 'test/mean_average_precision': 0.12488761510871799, 'test/num_examples': 43793, 'score': 1220.4017510414124, 'total_duration': 1835.5186605453491, 'accumulated_submission_time': 1220.4017510414124, 'accumulated_eval_time': 614.5815954208374, 'accumulated_logging_time': 0.4898653030395508, 'global_step': 5117, 'preemption_count': 0}), (6160, {'train/accuracy': 0.9876853227615356, 'train/loss': 0.04344348609447479, 'train/mean_average_precision': 0.1539017428606625, 'validation/accuracy': 0.985015869140625, 'validation/loss': 0.052276480942964554, 'validation/mean_average_precision': 0.1441541873087821, 'validation/num_examples': 43793, 'test/accuracy': 0.9839941263198853, 'test/loss': 0.05509023368358612, 'test/mean_average_precision': 0.141637436815036, 'test/num_examples': 43793, 'score': 1460.489696264267, 'total_duration': 2151.3316016197205, 'accumulated_submission_time': 1460.489696264267, 'accumulated_eval_time': 690.2087121009827, 'accumulated_logging_time': 0.5784273147583008, 'global_step': 6160, 'preemption_count': 0}), (7194, {'train/accuracy': 0.9878345131874084, 'train/loss': 0.04189693182706833, 'train/mean_average_precision': 0.1805107657654199, 'validation/accuracy': 0.9850792288780212, 'validation/loss': 0.05195567384362221, 'validation/mean_average_precision': 0.15986134781277692, 'validation/num_examples': 43793, 'test/accuracy': 0.9840627908706665, 'test/loss': 0.055033162236213684, 'test/mean_average_precision': 0.15360869780300943, 'test/num_examples': 43793, 'score': 1700.500934123993, 'total_duration': 2467.5457813739777, 'accumulated_submission_time': 1700.500934123993, 'accumulated_eval_time': 766.3113350868225, 'accumulated_logging_time': 0.6695899963378906, 'global_step': 7194, 'preemption_count': 0}), (8224, {'train/accuracy': 0.9879050850868225, 'train/loss': 0.04172813519835472, 'train/mean_average_precision': 0.19268153249144362, 'validation/accuracy': 0.9850134253501892, 'validation/loss': 0.05249375104904175, 'validation/mean_average_precision': 0.16435452907067313, 'validation/num_examples': 43793, 'test/accuracy': 0.9840350151062012, 'test/loss': 0.05536283180117607, 'test/mean_average_precision': 0.16440828060766188, 'test/num_examples': 43793, 'score': 1940.5952956676483, 'total_duration': 2784.930158853531, 'accumulated_submission_time': 1940.5952956676483, 'accumulated_eval_time': 843.4862151145935, 'accumulated_logging_time': 0.7757112979888916, 'global_step': 8224, 'preemption_count': 0}), (9263, {'train/accuracy': 0.988120436668396, 'train/loss': 0.04064522311091423, 'train/mean_average_precision': 0.20533786743130192, 'validation/accuracy': 0.985274076461792, 'validation/loss': 0.05050579085946083, 'validation/mean_average_precision': 0.18128174562249086, 'validation/num_examples': 43793, 'test/accuracy': 0.9843913316726685, 'test/loss': 0.053332891315221786, 'test/mean_average_precision': 0.17890813696071747, 'test/num_examples': 43793, 'score': 2180.7569694519043, 'total_duration': 3104.0755071640015, 'accumulated_submission_time': 2180.7569694519043, 'accumulated_eval_time': 922.3726365566254, 'accumulated_logging_time': 0.8640422821044922, 'global_step': 9263, 'preemption_count': 0}), (10312, {'train/accuracy': 0.9886313080787659, 'train/loss': 0.039485637098550797, 'train/mean_average_precision': 0.2288583739259451, 'validation/accuracy': 0.9856836199760437, 'validation/loss': 0.04979066550731659, 'validation/mean_average_precision': 0.18963492989043745, 'validation/num_examples': 43793, 'test/accuracy': 0.9846752285957336, 'test/loss': 0.05263019725680351, 'test/mean_average_precision': 0.1842432002058052, 'test/num_examples': 43793, 'score': 2420.8706505298615, 'total_duration': 3420.91614818573, 'accumulated_submission_time': 2420.8706505298615, 'accumulated_eval_time': 999.0002341270447, 'accumulated_logging_time': 0.9543282985687256, 'global_step': 10312, 'preemption_count': 0}), (11375, {'train/accuracy': 0.9886831045150757, 'train/loss': 0.03871716186404228, 'train/mean_average_precision': 0.24670198391595166, 'validation/accuracy': 0.9858212471008301, 'validation/loss': 0.04839019477367401, 'validation/mean_average_precision': 0.19438326396970398, 'validation/num_examples': 43793, 'test/accuracy': 0.984816312789917, 'test/loss': 0.050952132791280746, 'test/mean_average_precision': 0.18911796365170772, 'test/num_examples': 43793, 'score': 2660.927347421646, 'total_duration': 3737.3007578849792, 'accumulated_submission_time': 2660.927347421646, 'accumulated_eval_time': 1075.2309391498566, 'accumulated_logging_time': 1.0421357154846191, 'global_step': 11375, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9890701174736023, 'train/loss': 0.03751709312200546, 'train/mean_average_precision': 0.24765517220597483, 'validation/accuracy': 0.9858996272087097, 'validation/loss': 0.04812009632587433, 'validation/mean_average_precision': 0.20808504750873733, 'validation/num_examples': 43793, 'test/accuracy': 0.9849826693534851, 'test/loss': 0.050752174109220505, 'test/mean_average_precision': 0.20403395279226924, 'test/num_examples': 43793, 'score': 2800.936535835266, 'total_duration': 3952.882199525833, 'accumulated_submission_time': 2800.936535835266, 'accumulated_eval_time': 1150.6944134235382, 'accumulated_logging_time': 1.1452651023864746, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0418 14:33:27.357403 139781129688896 submission_runner.py:570] Timing: 2800.936535835266
I0418 14:33:27.357455 139781129688896 submission_runner.py:571] ====================
I0418 14:33:27.357579 139781129688896 submission_runner.py:631] Final ogbg score: 2800.936535835266
