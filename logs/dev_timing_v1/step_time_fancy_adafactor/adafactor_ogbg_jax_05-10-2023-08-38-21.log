python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_adafactor/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_05-10-2023-08-38-21.log
I0510 08:38:42.476126 140628902213440 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax.
I0510 08:38:42.551537 140628902213440 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0510 08:38:43.392627 140628902213440 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0510 08:38:43.393308 140628902213440 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0510 08:38:43.397932 140628902213440 submission_runner.py:544] Using RNG seed 2318711709
I0510 08:38:46.175818 140628902213440 submission_runner.py:553] --- Tuning run 1/1 ---
I0510 08:38:46.176053 140628902213440 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1.
I0510 08:38:46.176269 140628902213440 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1/hparams.json.
I0510 08:38:46.314787 140628902213440 submission_runner.py:241] Initializing dataset.
I0510 08:38:46.569418 140628902213440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:38:46.575162 140628902213440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0510 08:38:46.828344 140628902213440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:38:46.889989 140628902213440 submission_runner.py:248] Initializing model.
I0510 08:38:54.612992 140628902213440 submission_runner.py:258] Initializing optimizer.
I0510 08:38:55.838594 140628902213440 submission_runner.py:265] Initializing metrics bundle.
I0510 08:38:55.838782 140628902213440 submission_runner.py:283] Initializing checkpoint and logger.
I0510 08:38:55.839688 140628902213440 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1 with prefix checkpoint_
I0510 08:38:55.839915 140628902213440 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0510 08:38:55.839978 140628902213440 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0510 08:38:56.609620 140628902213440 submission_runner.py:304] Saving meta data to /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1/meta_data_0.json.
I0510 08:38:56.610517 140628902213440 submission_runner.py:307] Saving flags to /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1/flags_0.json.
I0510 08:38:56.615958 140628902213440 submission_runner.py:319] Starting training loop.
I0510 08:39:33.462337 140452745443072 logging_writer.py:48] [0] global_step=0, grad_norm=2.694753885269165, loss=0.7571121454238892
I0510 08:39:33.476703 140628902213440 spec.py:298] Evaluating on the training split.
I0510 08:39:33.485189 140628902213440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:39:33.489395 140628902213440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0510 08:39:33.556704 140628902213440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:41:10.027920 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 08:41:10.032115 140628902213440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:41:10.036403 140628902213440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0510 08:41:10.096249 140628902213440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:42:20.270661 140628902213440 spec.py:326] Evaluating on the test split.
I0510 08:42:20.274213 140628902213440 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:42:20.279050 140628902213440 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0510 08:42:20.340391 140628902213440 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0510 08:43:29.101130 140628902213440 submission_runner.py:421] Time since start: 272.49s, 	Step: 1, 	{'train/accuracy': 0.4624047875404358, 'train/loss': 0.7565916180610657, 'train/mean_average_precision': 0.02210039507599231, 'validation/accuracy': 0.45801880955696106, 'validation/loss': 0.7598409652709961, 'validation/mean_average_precision': 0.026780050627024137, 'validation/num_examples': 43793, 'test/accuracy': 0.45906469225883484, 'test/loss': 0.7593593597412109, 'test/mean_average_precision': 0.026907149556815963, 'test/num_examples': 43793, 'score': 36.86055588722229, 'total_duration': 272.48510241508484, 'accumulated_submission_time': 36.86055588722229, 'accumulated_eval_time': 235.62437057495117, 'accumulated_logging_time': 0}
I0510 08:43:29.117759 140443081307904 logging_writer.py:48] [1] accumulated_eval_time=235.624371, accumulated_logging_time=0, accumulated_submission_time=36.860556, global_step=1, preemption_count=0, score=36.860556, test/accuracy=0.459065, test/loss=0.759359, test/mean_average_precision=0.026907, test/num_examples=43793, total_duration=272.485102, train/accuracy=0.462405, train/loss=0.756592, train/mean_average_precision=0.022100, validation/accuracy=0.458019, validation/loss=0.759841, validation/mean_average_precision=0.026780, validation/num_examples=43793
I0510 08:43:56.449699 140443089700608 logging_writer.py:48] [100] global_step=100, grad_norm=0.4480295777320862, loss=0.3811202645301819
I0510 08:44:23.650794 140443081307904 logging_writer.py:48] [200] global_step=200, grad_norm=0.19216763973236084, loss=0.17555299401283264
I0510 08:44:51.081034 140443089700608 logging_writer.py:48] [300] global_step=300, grad_norm=0.10841624438762665, loss=0.08475399762392044
I0510 08:45:18.741883 140443081307904 logging_writer.py:48] [400] global_step=400, grad_norm=0.04742932692170143, loss=0.05947668105363846
I0510 08:45:46.234314 140443089700608 logging_writer.py:48] [500] global_step=500, grad_norm=0.036149658262729645, loss=0.06068999320268631
I0510 08:46:14.079204 140443081307904 logging_writer.py:48] [600] global_step=600, grad_norm=0.07193457335233688, loss=0.055734310299158096
I0510 08:46:41.494716 140443089700608 logging_writer.py:48] [700] global_step=700, grad_norm=0.04501961171627045, loss=0.05267723649740219
I0510 08:47:08.819443 140443081307904 logging_writer.py:48] [800] global_step=800, grad_norm=0.037893835455179214, loss=0.05283668637275696
I0510 08:47:29.320397 140628902213440 spec.py:298] Evaluating on the training split.
I0510 08:48:51.450410 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 08:48:54.116042 140628902213440 spec.py:326] Evaluating on the test split.
I0510 08:48:56.836563 140628902213440 submission_runner.py:421] Time since start: 600.22s, 	Step: 876, 	{'train/accuracy': 0.9868044853210449, 'train/loss': 0.052361130714416504, 'train/mean_average_precision': 0.056694921169614526, 'validation/accuracy': 0.9842385053634644, 'validation/loss': 0.06105823069810867, 'validation/mean_average_precision': 0.05436336375011584, 'validation/num_examples': 43793, 'test/accuracy': 0.9832393527030945, 'test/loss': 0.06419552862644196, 'test/mean_average_precision': 0.05522283395927061, 'test/num_examples': 43793, 'score': 277.04439973831177, 'total_duration': 600.2205333709717, 'accumulated_submission_time': 277.04439973831177, 'accumulated_eval_time': 323.1405141353607, 'accumulated_logging_time': 0.026659011840820312}
I0510 08:48:56.845430 140443089700608 logging_writer.py:48] [876] accumulated_eval_time=323.140514, accumulated_logging_time=0.026659, accumulated_submission_time=277.044400, global_step=876, preemption_count=0, score=277.044400, test/accuracy=0.983239, test/loss=0.064196, test/mean_average_precision=0.055223, test/num_examples=43793, total_duration=600.220533, train/accuracy=0.986804, train/loss=0.052361, train/mean_average_precision=0.056695, validation/accuracy=0.984239, validation/loss=0.061058, validation/mean_average_precision=0.054363, validation/num_examples=43793
I0510 08:49:03.811292 140443081307904 logging_writer.py:48] [900] global_step=900, grad_norm=0.04698827862739563, loss=0.05371853709220886
I0510 08:49:31.074718 140443089700608 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.02471303567290306, loss=0.05119968205690384
I0510 08:49:58.255546 140443081307904 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.022838164120912552, loss=0.0484577938914299
I0510 08:50:25.835633 140443089700608 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.06446079164743423, loss=0.05150586739182472
I0510 08:50:53.147363 140443081307904 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.026990674436092377, loss=0.04637036472558975
I0510 08:51:20.611650 140443089700608 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.05113580450415611, loss=0.04774176701903343
I0510 08:51:48.213163 140443081307904 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02607724256813526, loss=0.048806194216012955
I0510 08:52:15.700081 140443089700608 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.038569532334804535, loss=0.04802960902452469
I0510 08:52:43.161319 140443081307904 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.02403581701219082, loss=0.048596661537885666
I0510 08:52:56.935078 140628902213440 spec.py:298] Evaluating on the training split.
I0510 08:54:18.502479 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 08:54:21.175606 140628902213440 spec.py:326] Evaluating on the test split.
I0510 08:54:23.763350 140628902213440 submission_runner.py:421] Time since start: 927.15s, 	Step: 1751, 	{'train/accuracy': 0.9871984124183655, 'train/loss': 0.04791023209691048, 'train/mean_average_precision': 0.08976418396253805, 'validation/accuracy': 0.9845271110534668, 'validation/loss': 0.05736638233065605, 'validation/mean_average_precision': 0.08508438188552081, 'validation/num_examples': 43793, 'test/accuracy': 0.9835383892059326, 'test/loss': 0.06063702702522278, 'test/mean_average_precision': 0.0893671516612009, 'test/num_examples': 43793, 'score': 517.1165397167206, 'total_duration': 927.147323846817, 'accumulated_submission_time': 517.1165397167206, 'accumulated_eval_time': 409.9687490463257, 'accumulated_logging_time': 0.04513907432556152}
I0510 08:54:23.772160 140443089700608 logging_writer.py:48] [1751] accumulated_eval_time=409.968749, accumulated_logging_time=0.045139, accumulated_submission_time=517.116540, global_step=1751, preemption_count=0, score=517.116540, test/accuracy=0.983538, test/loss=0.060637, test/mean_average_precision=0.089367, test/num_examples=43793, total_duration=927.147324, train/accuracy=0.987198, train/loss=0.047910, train/mean_average_precision=0.089764, validation/accuracy=0.984527, validation/loss=0.057366, validation/mean_average_precision=0.085084, validation/num_examples=43793
I0510 08:54:37.302477 140443081307904 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03434557840228081, loss=0.05085499957203865
I0510 08:55:04.687943 140443089700608 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.05763282999396324, loss=0.05472759157419205
I0510 08:55:32.232181 140443081307904 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0196682158857584, loss=0.04404119402170181
I0510 08:56:00.522373 140443089700608 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.019054194912314415, loss=0.04451295733451843
I0510 08:56:28.524791 140443081307904 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.011759825982153416, loss=0.048422619700431824
I0510 08:56:56.256863 140443089700608 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04152759164571762, loss=0.05060052499175072
I0510 08:57:23.572613 140443081307904 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.015081364661455154, loss=0.04708036407828331
I0510 08:57:50.805700 140443089700608 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.010640702210366726, loss=0.047339294105768204
I0510 08:58:17.743345 140443081307904 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.025682225823402405, loss=0.04620036110281944
I0510 08:58:24.050400 140628902213440 spec.py:298] Evaluating on the training split.
I0510 08:59:46.270021 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 08:59:48.968874 140628902213440 spec.py:326] Evaluating on the test split.
I0510 08:59:51.553801 140628902213440 submission_runner.py:421] Time since start: 1254.94s, 	Step: 2624, 	{'train/accuracy': 0.987578809261322, 'train/loss': 0.04497658833861351, 'train/mean_average_precision': 0.143307008772338, 'validation/accuracy': 0.9847308993339539, 'validation/loss': 0.055124226957559586, 'validation/mean_average_precision': 0.12662962068982392, 'validation/num_examples': 43793, 'test/accuracy': 0.9837725758552551, 'test/loss': 0.058472197502851486, 'test/mean_average_precision': 0.12406104436367991, 'test/num_examples': 43793, 'score': 757.3764173984528, 'total_duration': 1254.937772512436, 'accumulated_submission_time': 757.3764173984528, 'accumulated_eval_time': 497.47211813926697, 'accumulated_logging_time': 0.06438279151916504}
I0510 08:59:51.562746 140443089700608 logging_writer.py:48] [2624] accumulated_eval_time=497.472118, accumulated_logging_time=0.064383, accumulated_submission_time=757.376417, global_step=2624, preemption_count=0, score=757.376417, test/accuracy=0.983773, test/loss=0.058472, test/mean_average_precision=0.124061, test/num_examples=43793, total_duration=1254.937773, train/accuracy=0.987579, train/loss=0.044977, train/mean_average_precision=0.143307, validation/accuracy=0.984731, validation/loss=0.055124, validation/mean_average_precision=0.126630, validation/num_examples=43793
I0510 09:00:12.359366 140443081307904 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.013151317834854126, loss=0.05217151716351509
I0510 09:00:39.493003 140443089700608 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.020051151514053345, loss=0.04917750507593155
I0510 09:01:07.015146 140443081307904 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.01454993151128292, loss=0.04516822472214699
I0510 09:01:34.815247 140443089700608 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.011604738421738148, loss=0.04369646683335304
I0510 09:02:02.445256 140443081307904 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.021224353462457657, loss=0.04119648039340973
I0510 09:02:30.230832 140443089700608 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.008187942206859589, loss=0.04435659945011139
I0510 09:02:57.621720 140443081307904 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.008334634825587273, loss=0.03911968693137169
I0510 09:03:24.943540 140443089700608 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.012144014239311218, loss=0.04598494619131088
I0510 09:03:51.667116 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:05:13.311143 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:05:15.972186 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:05:18.557582 140628902213440 submission_runner.py:421] Time since start: 1581.94s, 	Step: 3497, 	{'train/accuracy': 0.9880104064941406, 'train/loss': 0.042534440755844116, 'train/mean_average_precision': 0.16725186790804863, 'validation/accuracy': 0.9852338433265686, 'validation/loss': 0.05191824212670326, 'validation/mean_average_precision': 0.14900436616839402, 'validation/num_examples': 43793, 'test/accuracy': 0.9842333793640137, 'test/loss': 0.0549590140581131, 'test/mean_average_precision': 0.15162332572875165, 'test/num_examples': 43793, 'score': 997.4613633155823, 'total_duration': 1581.941551208496, 'accumulated_submission_time': 997.4613633155823, 'accumulated_eval_time': 584.3625438213348, 'accumulated_logging_time': 0.08494853973388672}
I0510 09:05:18.566691 140443081307904 logging_writer.py:48] [3497] accumulated_eval_time=584.362544, accumulated_logging_time=0.084949, accumulated_submission_time=997.461363, global_step=3497, preemption_count=0, score=997.461363, test/accuracy=0.984233, test/loss=0.054959, test/mean_average_precision=0.151623, test/num_examples=43793, total_duration=1581.941551, train/accuracy=0.988010, train/loss=0.042534, train/mean_average_precision=0.167252, validation/accuracy=0.985234, validation/loss=0.051918, validation/mean_average_precision=0.149004, validation/num_examples=43793
I0510 09:05:19.710635 140443089700608 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.010029369965195656, loss=0.04222424700856209
I0510 09:05:46.997205 140443081307904 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.010851451195776463, loss=0.04471289739012718
I0510 09:06:15.037050 140443089700608 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.014502333477139473, loss=0.04232316464185715
I0510 09:06:43.198849 140443081307904 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.009109246544539928, loss=0.04410931095480919
I0510 09:07:10.946651 140443089700608 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.024265306070446968, loss=0.043249599635601044
I0510 09:07:39.637086 140443081307904 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.010874011553823948, loss=0.04454471915960312
I0510 09:08:07.823017 140443089700608 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.011968833394348621, loss=0.041835520416498184
I0510 09:08:35.046109 140443081307904 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.011583931744098663, loss=0.04452820494771004
I0510 09:09:02.591572 140443089700608 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.01115639228373766, loss=0.04699702188372612
I0510 09:09:18.766926 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:10:41.112945 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:10:43.781027 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:10:46.388590 140628902213440 submission_runner.py:421] Time since start: 1909.77s, 	Step: 4360, 	{'train/accuracy': 0.9881784915924072, 'train/loss': 0.041370097547769547, 'train/mean_average_precision': 0.19997491956106173, 'validation/accuracy': 0.9852914810180664, 'validation/loss': 0.05136549845337868, 'validation/mean_average_precision': 0.1696711484323331, 'validation/num_examples': 43793, 'test/accuracy': 0.9843112826347351, 'test/loss': 0.054335374385118484, 'test/mean_average_precision': 0.16670155775581072, 'test/num_examples': 43793, 'score': 1237.6426224708557, 'total_duration': 1909.7725620269775, 'accumulated_submission_time': 1237.6426224708557, 'accumulated_eval_time': 671.9841847419739, 'accumulated_logging_time': 0.10462045669555664}
I0510 09:10:46.397730 140443081307904 logging_writer.py:48] [4360] accumulated_eval_time=671.984185, accumulated_logging_time=0.104620, accumulated_submission_time=1237.642622, global_step=4360, preemption_count=0, score=1237.642622, test/accuracy=0.984311, test/loss=0.054335, test/mean_average_precision=0.166702, test/num_examples=43793, total_duration=1909.772562, train/accuracy=0.988178, train/loss=0.041370, train/mean_average_precision=0.199975, validation/accuracy=0.985291, validation/loss=0.051365, validation/mean_average_precision=0.169671, validation/num_examples=43793
I0510 09:10:57.671706 140443089700608 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.008441559039056301, loss=0.04061829298734665
I0510 09:11:25.298766 140443081307904 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.006431311368942261, loss=0.043219488114118576
I0510 09:11:52.782286 140443089700608 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.011553620919585228, loss=0.04407300427556038
I0510 09:12:20.167142 140443081307904 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.011828885413706303, loss=0.04373873025178909
I0510 09:12:47.458793 140443089700608 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.007738688960671425, loss=0.041310474276542664
I0510 09:13:15.154648 140443081307904 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.009159502573311329, loss=0.043700892478227615
I0510 09:13:42.814726 140443089700608 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.01117615308612585, loss=0.042802199721336365
I0510 09:14:10.799754 140443081307904 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.007358547765761614, loss=0.044443901628255844
I0510 09:14:38.633953 140443089700608 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.007549199741333723, loss=0.038810111582279205
I0510 09:14:46.660168 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:16:07.858251 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:16:10.482808 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:16:13.080430 140628902213440 submission_runner.py:421] Time since start: 2236.46s, 	Step: 5230, 	{'train/accuracy': 0.9886438250541687, 'train/loss': 0.03896195441484451, 'train/mean_average_precision': 0.22945784289512783, 'validation/accuracy': 0.985725462436676, 'validation/loss': 0.04884536936879158, 'validation/mean_average_precision': 0.18586449116687756, 'validation/num_examples': 43793, 'test/accuracy': 0.9847813844680786, 'test/loss': 0.05163348466157913, 'test/mean_average_precision': 0.1903481542987342, 'test/num_examples': 43793, 'score': 1477.8862025737762, 'total_duration': 2236.464375734329, 'accumulated_submission_time': 1477.8862025737762, 'accumulated_eval_time': 758.4043798446655, 'accumulated_logging_time': 0.1245424747467041}
I0510 09:16:13.089827 140443081307904 logging_writer.py:48] [5230] accumulated_eval_time=758.404380, accumulated_logging_time=0.124542, accumulated_submission_time=1477.886203, global_step=5230, preemption_count=0, score=1477.886203, test/accuracy=0.984781, test/loss=0.051633, test/mean_average_precision=0.190348, test/num_examples=43793, total_duration=2236.464376, train/accuracy=0.988644, train/loss=0.038962, train/mean_average_precision=0.229458, validation/accuracy=0.985725, validation/loss=0.048845, validation/mean_average_precision=0.185864, validation/num_examples=43793
I0510 09:16:32.739463 140443089700608 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0073590404354035854, loss=0.04297148436307907
I0510 09:17:00.148290 140443081307904 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.010875067673623562, loss=0.04181630164384842
I0510 09:17:27.808842 140443089700608 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0066435676999390125, loss=0.041006676852703094
I0510 09:17:55.155519 140443081307904 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.007266914006322622, loss=0.03970523178577423
I0510 09:18:22.434835 140443089700608 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.008770117536187172, loss=0.04098016023635864
I0510 09:18:50.080813 140443081307904 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0053505427204072475, loss=0.042128436267375946
I0510 09:19:17.510656 140443089700608 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.010907511226832867, loss=0.042607154697179794
I0510 09:19:44.757935 140443081307904 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.005518714897334576, loss=0.03997258096933365
I0510 09:20:12.578172 140443089700608 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.005671391729265451, loss=0.03799647465348244
I0510 09:20:13.130061 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:21:33.420924 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:21:36.077604 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:21:38.670116 140628902213440 submission_runner.py:421] Time since start: 2562.05s, 	Step: 6103, 	{'train/accuracy': 0.988886296749115, 'train/loss': 0.03759628161787987, 'train/mean_average_precision': 0.2704537682026027, 'validation/accuracy': 0.9859393835067749, 'validation/loss': 0.04787285253405571, 'validation/mean_average_precision': 0.20734308447628733, 'validation/num_examples': 43793, 'test/accuracy': 0.9849953055381775, 'test/loss': 0.050583019852638245, 'test/mean_average_precision': 0.20675367968207412, 'test/num_examples': 43793, 'score': 1717.9084701538086, 'total_duration': 2562.0540647506714, 'accumulated_submission_time': 1717.9084701538086, 'accumulated_eval_time': 843.944406747818, 'accumulated_logging_time': 0.14387917518615723}
I0510 09:21:38.679269 140443081307904 logging_writer.py:48] [6103] accumulated_eval_time=843.944407, accumulated_logging_time=0.143879, accumulated_submission_time=1717.908470, global_step=6103, preemption_count=0, score=1717.908470, test/accuracy=0.984995, test/loss=0.050583, test/mean_average_precision=0.206754, test/num_examples=43793, total_duration=2562.054065, train/accuracy=0.988886, train/loss=0.037596, train/mean_average_precision=0.270454, validation/accuracy=0.985939, validation/loss=0.047873, validation/mean_average_precision=0.207343, validation/num_examples=43793
I0510 09:22:06.300415 140443089700608 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.006649546790868044, loss=0.04259553551673889
I0510 09:22:33.791052 140443081307904 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0060472916811704636, loss=0.036602336913347244
I0510 09:23:00.857623 140443089700608 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.007560465484857559, loss=0.03784694895148277
I0510 09:23:28.151469 140443081307904 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.006085946690291166, loss=0.04066578671336174
I0510 09:23:55.623592 140443089700608 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0106172114610672, loss=0.03995529189705849
I0510 09:24:23.024564 140443081307904 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.006778448820114136, loss=0.038416098803281784
I0510 09:24:50.359032 140443089700608 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.00742743257433176, loss=0.04035596176981926
I0510 09:25:17.718816 140443081307904 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.008294018916785717, loss=0.0377969965338707
I0510 09:25:38.700021 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:26:59.836628 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:27:02.450644 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:27:05.018952 140628902213440 submission_runner.py:421] Time since start: 2888.40s, 	Step: 6977, 	{'train/accuracy': 0.9893340468406677, 'train/loss': 0.036101773381233215, 'train/mean_average_precision': 0.29421981864969615, 'validation/accuracy': 0.9862162470817566, 'validation/loss': 0.04700120911002159, 'validation/mean_average_precision': 0.218502127613511, 'validation/num_examples': 43793, 'test/accuracy': 0.985248863697052, 'test/loss': 0.04994988068938255, 'test/mean_average_precision': 0.221621744715878, 'test/num_examples': 43793, 'score': 1957.9107959270477, 'total_duration': 2888.402923345566, 'accumulated_submission_time': 1957.9107959270477, 'accumulated_eval_time': 930.2633092403412, 'accumulated_logging_time': 0.16335701942443848}
I0510 09:27:05.028303 140443089700608 logging_writer.py:48] [6977] accumulated_eval_time=930.263309, accumulated_logging_time=0.163357, accumulated_submission_time=1957.910796, global_step=6977, preemption_count=0, score=1957.910796, test/accuracy=0.985249, test/loss=0.049950, test/mean_average_precision=0.221622, test/num_examples=43793, total_duration=2888.402923, train/accuracy=0.989334, train/loss=0.036102, train/mean_average_precision=0.294220, validation/accuracy=0.986216, validation/loss=0.047001, validation/mean_average_precision=0.218502, validation/num_examples=43793
I0510 09:27:11.640524 140443081307904 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.007141308393329382, loss=0.041593462228775024
I0510 09:27:39.154767 140443089700608 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.00900338776409626, loss=0.038106437772512436
I0510 09:28:06.701956 140443081307904 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.006365697830915451, loss=0.03735486790537834
I0510 09:28:33.753946 140443089700608 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0057042124681174755, loss=0.03682660683989525
I0510 09:29:01.091069 140443081307904 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.006320170126855373, loss=0.034203194081783295
I0510 09:29:28.653654 140443089700608 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0055394903756678104, loss=0.03898921236395836
I0510 09:29:55.761432 140443081307904 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.005381552968174219, loss=0.034900400787591934
I0510 09:30:22.407330 140443089700608 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.005809295456856489, loss=0.037670478224754333
I0510 09:30:49.565065 140443081307904 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0056881969794631, loss=0.03540049493312836
I0510 09:31:05.040038 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:32:23.893140 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:32:26.568047 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:32:29.167500 140628902213440 submission_runner.py:421] Time since start: 3212.55s, 	Step: 7857, 	{'train/accuracy': 0.989414632320404, 'train/loss': 0.03525463119149208, 'train/mean_average_precision': 0.32694846161414204, 'validation/accuracy': 0.9863465428352356, 'validation/loss': 0.04637893661856651, 'validation/mean_average_precision': 0.23109490875074898, 'validation/num_examples': 43793, 'test/accuracy': 0.9853659868240356, 'test/loss': 0.049292780458927155, 'test/mean_average_precision': 0.23185997893171936, 'test/num_examples': 43793, 'score': 2197.9051156044006, 'total_duration': 3212.55144405365, 'accumulated_submission_time': 2197.9051156044006, 'accumulated_eval_time': 1014.3907074928284, 'accumulated_logging_time': 0.18222641944885254}
I0510 09:32:29.177622 140443089700608 logging_writer.py:48] [7857] accumulated_eval_time=1014.390707, accumulated_logging_time=0.182226, accumulated_submission_time=2197.905116, global_step=7857, preemption_count=0, score=2197.905116, test/accuracy=0.985366, test/loss=0.049293, test/mean_average_precision=0.231860, test/num_examples=43793, total_duration=3212.551444, train/accuracy=0.989415, train/loss=0.035255, train/mean_average_precision=0.326948, validation/accuracy=0.986347, validation/loss=0.046379, validation/mean_average_precision=0.231095, validation/num_examples=43793
I0510 09:32:41.090323 140443081307904 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.005652511026710272, loss=0.03764519467949867
I0510 09:33:08.341947 140443089700608 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.007008361630141735, loss=0.03734501823782921
I0510 09:33:35.780423 140443081307904 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.005155778024345636, loss=0.038621097803115845
I0510 09:34:03.001205 140443089700608 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.014184191823005676, loss=0.03692599758505821
I0510 09:34:30.633664 140443081307904 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.005901351571083069, loss=0.03937520459294319
I0510 09:34:58.494377 140443089700608 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.010471564717590809, loss=0.03791923075914383
I0510 09:35:25.719701 140443081307904 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0075653973035514355, loss=0.038042839616537094
I0510 09:35:53.150801 140443089700608 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.009039768017828465, loss=0.037129778414964676
I0510 09:36:20.413609 140443081307904 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.006253795698285103, loss=0.03991558402776718
I0510 09:36:29.320321 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:37:50.569058 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:37:53.279463 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:37:56.158655 140628902213440 submission_runner.py:421] Time since start: 3539.54s, 	Step: 8734, 	{'train/accuracy': 0.9898630976676941, 'train/loss': 0.03396473824977875, 'train/mean_average_precision': 0.34083978738664505, 'validation/accuracy': 0.9864736199378967, 'validation/loss': 0.04558940976858139, 'validation/mean_average_precision': 0.23929977404477742, 'validation/num_examples': 43793, 'test/accuracy': 0.9854801297187805, 'test/loss': 0.048634909093379974, 'test/mean_average_precision': 0.23298583020949734, 'test/num_examples': 43793, 'score': 2438.0297372341156, 'total_duration': 3539.5426268577576, 'accumulated_submission_time': 2438.0297372341156, 'accumulated_eval_time': 1101.2290098667145, 'accumulated_logging_time': 0.2022240161895752}
I0510 09:37:56.168419 140443089700608 logging_writer.py:48] [8734] accumulated_eval_time=1101.229010, accumulated_logging_time=0.202224, accumulated_submission_time=2438.029737, global_step=8734, preemption_count=0, score=2438.029737, test/accuracy=0.985480, test/loss=0.048635, test/mean_average_precision=0.232986, test/num_examples=43793, total_duration=3539.542627, train/accuracy=0.989863, train/loss=0.033965, train/mean_average_precision=0.340840, validation/accuracy=0.986474, validation/loss=0.045589, validation/mean_average_precision=0.239300, validation/num_examples=43793
I0510 09:38:14.289446 140443081307904 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.006517106667160988, loss=0.03766287490725517
I0510 09:38:41.739770 140443089700608 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.007897678762674332, loss=0.0384371243417263
I0510 09:39:09.016022 140443081307904 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.006788535974919796, loss=0.038881830871105194
I0510 09:39:36.132791 140443089700608 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.007467024493962526, loss=0.038519810885190964
I0510 09:40:04.006758 140443081307904 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.005451546516269445, loss=0.03683915734291077
I0510 09:40:31.176549 140443089700608 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0058311899192631245, loss=0.038022976368665695
I0510 09:40:58.499064 140443081307904 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.005779420956969261, loss=0.03581733629107475
I0510 09:41:25.922754 140443089700608 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.00674218125641346, loss=0.03942127525806427
I0510 09:41:53.192277 140443081307904 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.00613943999633193, loss=0.03840344026684761
I0510 09:41:56.454499 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:43:14.172518 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:43:16.809717 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:43:19.396424 140628902213440 submission_runner.py:421] Time since start: 3862.78s, 	Step: 9613, 	{'train/accuracy': 0.9900215864181519, 'train/loss': 0.03319450095295906, 'train/mean_average_precision': 0.35784994929544667, 'validation/accuracy': 0.986478865146637, 'validation/loss': 0.04552479088306427, 'validation/mean_average_precision': 0.24447568119952576, 'validation/num_examples': 43793, 'test/accuracy': 0.9855917096138, 'test/loss': 0.04834149405360222, 'test/mean_average_precision': 0.23848020052834712, 'test/num_examples': 43793, 'score': 2678.29820394516, 'total_duration': 3862.7803728580475, 'accumulated_submission_time': 2678.29820394516, 'accumulated_eval_time': 1184.1708734035492, 'accumulated_logging_time': 0.2215726375579834}
I0510 09:43:19.406392 140443089700608 logging_writer.py:48] [9613] accumulated_eval_time=1184.170873, accumulated_logging_time=0.221573, accumulated_submission_time=2678.298204, global_step=9613, preemption_count=0, score=2678.298204, test/accuracy=0.985592, test/loss=0.048341, test/mean_average_precision=0.238480, test/num_examples=43793, total_duration=3862.780373, train/accuracy=0.990022, train/loss=0.033195, train/mean_average_precision=0.357850, validation/accuracy=0.986479, validation/loss=0.045525, validation/mean_average_precision=0.244476, validation/num_examples=43793
I0510 09:43:43.368859 140443081307904 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.008253291249275208, loss=0.03727143630385399
I0510 09:44:10.599483 140443089700608 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.008639452047646046, loss=0.0386819951236248
I0510 09:44:37.920886 140443081307904 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.010104788467288017, loss=0.03872624784708023
I0510 09:45:05.362894 140443089700608 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.006848706863820553, loss=0.0366947241127491
I0510 09:45:32.894088 140443081307904 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.006430116947740316, loss=0.03602279722690582
I0510 09:46:00.477719 140443089700608 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.006176897324621677, loss=0.0357486866414547
I0510 09:46:27.849792 140443081307904 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.007506303023546934, loss=0.035533055663108826
I0510 09:46:56.834335 140443089700608 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.006851375102996826, loss=0.036289606243371964
I0510 09:47:19.649722 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:48:41.181824 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:48:43.815913 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:48:46.386054 140628902213440 submission_runner.py:421] Time since start: 4189.77s, 	Step: 10480, 	{'train/accuracy': 0.9902150630950928, 'train/loss': 0.03237683326005936, 'train/mean_average_precision': 0.3617728452243728, 'validation/accuracy': 0.9866499900817871, 'validation/loss': 0.04525260627269745, 'validation/mean_average_precision': 0.25041496898059057, 'validation/num_examples': 43793, 'test/accuracy': 0.9857522249221802, 'test/loss': 0.048185210675001144, 'test/mean_average_precision': 0.24381919704493063, 'test/num_examples': 43793, 'score': 2918.5239613056183, 'total_duration': 4189.770018339157, 'accumulated_submission_time': 2918.5239613056183, 'accumulated_eval_time': 1270.9071674346924, 'accumulated_logging_time': 0.24116134643554688}
I0510 09:48:46.395842 140443081307904 logging_writer.py:48] [10480] accumulated_eval_time=1270.907167, accumulated_logging_time=0.241161, accumulated_submission_time=2918.523961, global_step=10480, preemption_count=0, score=2918.523961, test/accuracy=0.985752, test/loss=0.048185, test/mean_average_precision=0.243819, test/num_examples=43793, total_duration=4189.770018, train/accuracy=0.990215, train/loss=0.032377, train/mean_average_precision=0.361773, validation/accuracy=0.986650, validation/loss=0.045253, validation/mean_average_precision=0.250415, validation/num_examples=43793
I0510 09:48:52.589358 140443089700608 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.006661360617727041, loss=0.04143105819821358
I0510 09:49:20.614707 140443081307904 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.006930361967533827, loss=0.03299061208963394
I0510 09:49:48.018745 140443089700608 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.008495885878801346, loss=0.03722408786416054
I0510 09:50:15.266033 140443081307904 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.007221837528049946, loss=0.0354459173977375
I0510 09:50:42.285469 140443089700608 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0066830809228122234, loss=0.03481613099575043
I0510 09:51:09.399565 140443081307904 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.007630247622728348, loss=0.03368190675973892
I0510 09:51:37.144409 140443089700608 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0060088494792580605, loss=0.035156458616256714
I0510 09:52:05.045090 140443081307904 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.011536131612956524, loss=0.034735336899757385
I0510 09:52:32.674555 140443089700608 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.007823485881090164, loss=0.03738837316632271
I0510 09:52:46.563858 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:54:03.698555 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:54:06.398679 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:54:09.315806 140628902213440 submission_runner.py:421] Time since start: 4512.70s, 	Step: 11351, 	{'train/accuracy': 0.9906002283096313, 'train/loss': 0.031561873853206635, 'train/mean_average_precision': 0.40333428028259327, 'validation/accuracy': 0.9865239262580872, 'validation/loss': 0.04532726854085922, 'validation/mean_average_precision': 0.2522769098735859, 'validation/num_examples': 43793, 'test/accuracy': 0.9856271147727966, 'test/loss': 0.04815156012773514, 'test/mean_average_precision': 0.248453767780423, 'test/num_examples': 43793, 'score': 3158.673525094986, 'total_duration': 4512.6997611522675, 'accumulated_submission_time': 3158.673525094986, 'accumulated_eval_time': 1353.6590600013733, 'accumulated_logging_time': 0.261364221572876}
I0510 09:54:09.325519 140443081307904 logging_writer.py:48] [11351] accumulated_eval_time=1353.659060, accumulated_logging_time=0.261364, accumulated_submission_time=3158.673525, global_step=11351, preemption_count=0, score=3158.673525, test/accuracy=0.985627, test/loss=0.048152, test/mean_average_precision=0.248454, test/num_examples=43793, total_duration=4512.699761, train/accuracy=0.990600, train/loss=0.031562, train/mean_average_precision=0.403334, validation/accuracy=0.986524, validation/loss=0.045327, validation/mean_average_precision=0.252277, validation/num_examples=43793
I0510 09:54:23.081388 140443089700608 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.006371667142957449, loss=0.034418001770973206
I0510 09:54:50.867234 140443081307904 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.00668563786894083, loss=0.0336039662361145
I0510 09:55:18.519675 140443089700608 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.008196661248803139, loss=0.03350101411342621
I0510 09:55:45.916984 140443081307904 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.007084943819791079, loss=0.036913007497787476
I0510 09:56:13.560806 140443089700608 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.00686831446364522, loss=0.03831811994314194
I0510 09:56:40.910352 140443081307904 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.007845353335142136, loss=0.03474539518356323
I0510 09:57:08.098932 140628902213440 spec.py:298] Evaluating on the training split.
I0510 09:58:26.178833 140628902213440 spec.py:310] Evaluating on the validation split.
I0510 09:58:28.842918 140628902213440 spec.py:326] Evaluating on the test split.
I0510 09:58:31.445545 140628902213440 submission_runner.py:421] Time since start: 4774.83s, 	Step: 12000, 	{'train/accuracy': 0.9908316135406494, 'train/loss': 0.030297359451651573, 'train/mean_average_precision': 0.4162751289311041, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.045136310160160065, 'validation/mean_average_precision': 0.25587156347092527, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.04815448075532913, 'test/mean_average_precision': 0.2503194125314745, 'test/num_examples': 43793, 'score': 3337.4313197135925, 'total_duration': 4774.829484701157, 'accumulated_submission_time': 3337.4313197135925, 'accumulated_eval_time': 1437.0056021213531, 'accumulated_logging_time': 0.28065037727355957}
I0510 09:58:31.455903 140443089700608 logging_writer.py:48] [12000] accumulated_eval_time=1437.005602, accumulated_logging_time=0.280650, accumulated_submission_time=3337.431320, global_step=12000, preemption_count=0, score=3337.431320, test/accuracy=0.985748, test/loss=0.048154, test/mean_average_precision=0.250319, test/num_examples=43793, total_duration=4774.829485, train/accuracy=0.990832, train/loss=0.030297, train/mean_average_precision=0.416275, validation/accuracy=0.986672, validation/loss=0.045136, validation/mean_average_precision=0.255872, validation/num_examples=43793
I0510 09:58:31.472865 140443081307904 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3337.431320
I0510 09:58:31.503446 140628902213440 checkpoints.py:356] Saving checkpoint at step: 12000
I0510 09:58:31.555384 140628902213440 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1/checkpoint_12000
I0510 09:58:31.555586 140628902213440 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_adafactor/timing_adafactor/ogbg_jax/trial_1/checkpoint_12000.
I0510 09:58:31.733708 140628902213440 submission_runner.py:584] Tuning trial 1/1
I0510 09:58:31.733973 140628902213440 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0510 09:58:31.740409 140628902213440 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4624047875404358, 'train/loss': 0.7565916180610657, 'train/mean_average_precision': 0.02210039507599231, 'validation/accuracy': 0.45801880955696106, 'validation/loss': 0.7598409652709961, 'validation/mean_average_precision': 0.026780050627024137, 'validation/num_examples': 43793, 'test/accuracy': 0.45906469225883484, 'test/loss': 0.7593593597412109, 'test/mean_average_precision': 0.026907149556815963, 'test/num_examples': 43793, 'score': 36.86055588722229, 'total_duration': 272.48510241508484, 'accumulated_submission_time': 36.86055588722229, 'accumulated_eval_time': 235.62437057495117, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (876, {'train/accuracy': 0.9868044853210449, 'train/loss': 0.052361130714416504, 'train/mean_average_precision': 0.056694921169614526, 'validation/accuracy': 0.9842385053634644, 'validation/loss': 0.06105823069810867, 'validation/mean_average_precision': 0.05436336375011584, 'validation/num_examples': 43793, 'test/accuracy': 0.9832393527030945, 'test/loss': 0.06419552862644196, 'test/mean_average_precision': 0.05522283395927061, 'test/num_examples': 43793, 'score': 277.04439973831177, 'total_duration': 600.2205333709717, 'accumulated_submission_time': 277.04439973831177, 'accumulated_eval_time': 323.1405141353607, 'accumulated_logging_time': 0.026659011840820312, 'global_step': 876, 'preemption_count': 0}), (1751, {'train/accuracy': 0.9871984124183655, 'train/loss': 0.04791023209691048, 'train/mean_average_precision': 0.08976418396253805, 'validation/accuracy': 0.9845271110534668, 'validation/loss': 0.05736638233065605, 'validation/mean_average_precision': 0.08508438188552081, 'validation/num_examples': 43793, 'test/accuracy': 0.9835383892059326, 'test/loss': 0.06063702702522278, 'test/mean_average_precision': 0.0893671516612009, 'test/num_examples': 43793, 'score': 517.1165397167206, 'total_duration': 927.147323846817, 'accumulated_submission_time': 517.1165397167206, 'accumulated_eval_time': 409.9687490463257, 'accumulated_logging_time': 0.04513907432556152, 'global_step': 1751, 'preemption_count': 0}), (2624, {'train/accuracy': 0.987578809261322, 'train/loss': 0.04497658833861351, 'train/mean_average_precision': 0.143307008772338, 'validation/accuracy': 0.9847308993339539, 'validation/loss': 0.055124226957559586, 'validation/mean_average_precision': 0.12662962068982392, 'validation/num_examples': 43793, 'test/accuracy': 0.9837725758552551, 'test/loss': 0.058472197502851486, 'test/mean_average_precision': 0.12406104436367991, 'test/num_examples': 43793, 'score': 757.3764173984528, 'total_duration': 1254.937772512436, 'accumulated_submission_time': 757.3764173984528, 'accumulated_eval_time': 497.47211813926697, 'accumulated_logging_time': 0.06438279151916504, 'global_step': 2624, 'preemption_count': 0}), (3497, {'train/accuracy': 0.9880104064941406, 'train/loss': 0.042534440755844116, 'train/mean_average_precision': 0.16725186790804863, 'validation/accuracy': 0.9852338433265686, 'validation/loss': 0.05191824212670326, 'validation/mean_average_precision': 0.14900436616839402, 'validation/num_examples': 43793, 'test/accuracy': 0.9842333793640137, 'test/loss': 0.0549590140581131, 'test/mean_average_precision': 0.15162332572875165, 'test/num_examples': 43793, 'score': 997.4613633155823, 'total_duration': 1581.941551208496, 'accumulated_submission_time': 997.4613633155823, 'accumulated_eval_time': 584.3625438213348, 'accumulated_logging_time': 0.08494853973388672, 'global_step': 3497, 'preemption_count': 0}), (4360, {'train/accuracy': 0.9881784915924072, 'train/loss': 0.041370097547769547, 'train/mean_average_precision': 0.19997491956106173, 'validation/accuracy': 0.9852914810180664, 'validation/loss': 0.05136549845337868, 'validation/mean_average_precision': 0.1696711484323331, 'validation/num_examples': 43793, 'test/accuracy': 0.9843112826347351, 'test/loss': 0.054335374385118484, 'test/mean_average_precision': 0.16670155775581072, 'test/num_examples': 43793, 'score': 1237.6426224708557, 'total_duration': 1909.7725620269775, 'accumulated_submission_time': 1237.6426224708557, 'accumulated_eval_time': 671.9841847419739, 'accumulated_logging_time': 0.10462045669555664, 'global_step': 4360, 'preemption_count': 0}), (5230, {'train/accuracy': 0.9886438250541687, 'train/loss': 0.03896195441484451, 'train/mean_average_precision': 0.22945784289512783, 'validation/accuracy': 0.985725462436676, 'validation/loss': 0.04884536936879158, 'validation/mean_average_precision': 0.18586449116687756, 'validation/num_examples': 43793, 'test/accuracy': 0.9847813844680786, 'test/loss': 0.05163348466157913, 'test/mean_average_precision': 0.1903481542987342, 'test/num_examples': 43793, 'score': 1477.8862025737762, 'total_duration': 2236.464375734329, 'accumulated_submission_time': 1477.8862025737762, 'accumulated_eval_time': 758.4043798446655, 'accumulated_logging_time': 0.1245424747467041, 'global_step': 5230, 'preemption_count': 0}), (6103, {'train/accuracy': 0.988886296749115, 'train/loss': 0.03759628161787987, 'train/mean_average_precision': 0.2704537682026027, 'validation/accuracy': 0.9859393835067749, 'validation/loss': 0.04787285253405571, 'validation/mean_average_precision': 0.20734308447628733, 'validation/num_examples': 43793, 'test/accuracy': 0.9849953055381775, 'test/loss': 0.050583019852638245, 'test/mean_average_precision': 0.20675367968207412, 'test/num_examples': 43793, 'score': 1717.9084701538086, 'total_duration': 2562.0540647506714, 'accumulated_submission_time': 1717.9084701538086, 'accumulated_eval_time': 843.944406747818, 'accumulated_logging_time': 0.14387917518615723, 'global_step': 6103, 'preemption_count': 0}), (6977, {'train/accuracy': 0.9893340468406677, 'train/loss': 0.036101773381233215, 'train/mean_average_precision': 0.29421981864969615, 'validation/accuracy': 0.9862162470817566, 'validation/loss': 0.04700120911002159, 'validation/mean_average_precision': 0.218502127613511, 'validation/num_examples': 43793, 'test/accuracy': 0.985248863697052, 'test/loss': 0.04994988068938255, 'test/mean_average_precision': 0.221621744715878, 'test/num_examples': 43793, 'score': 1957.9107959270477, 'total_duration': 2888.402923345566, 'accumulated_submission_time': 1957.9107959270477, 'accumulated_eval_time': 930.2633092403412, 'accumulated_logging_time': 0.16335701942443848, 'global_step': 6977, 'preemption_count': 0}), (7857, {'train/accuracy': 0.989414632320404, 'train/loss': 0.03525463119149208, 'train/mean_average_precision': 0.32694846161414204, 'validation/accuracy': 0.9863465428352356, 'validation/loss': 0.04637893661856651, 'validation/mean_average_precision': 0.23109490875074898, 'validation/num_examples': 43793, 'test/accuracy': 0.9853659868240356, 'test/loss': 0.049292780458927155, 'test/mean_average_precision': 0.23185997893171936, 'test/num_examples': 43793, 'score': 2197.9051156044006, 'total_duration': 3212.55144405365, 'accumulated_submission_time': 2197.9051156044006, 'accumulated_eval_time': 1014.3907074928284, 'accumulated_logging_time': 0.18222641944885254, 'global_step': 7857, 'preemption_count': 0}), (8734, {'train/accuracy': 0.9898630976676941, 'train/loss': 0.03396473824977875, 'train/mean_average_precision': 0.34083978738664505, 'validation/accuracy': 0.9864736199378967, 'validation/loss': 0.04558940976858139, 'validation/mean_average_precision': 0.23929977404477742, 'validation/num_examples': 43793, 'test/accuracy': 0.9854801297187805, 'test/loss': 0.048634909093379974, 'test/mean_average_precision': 0.23298583020949734, 'test/num_examples': 43793, 'score': 2438.0297372341156, 'total_duration': 3539.5426268577576, 'accumulated_submission_time': 2438.0297372341156, 'accumulated_eval_time': 1101.2290098667145, 'accumulated_logging_time': 0.2022240161895752, 'global_step': 8734, 'preemption_count': 0}), (9613, {'train/accuracy': 0.9900215864181519, 'train/loss': 0.03319450095295906, 'train/mean_average_precision': 0.35784994929544667, 'validation/accuracy': 0.986478865146637, 'validation/loss': 0.04552479088306427, 'validation/mean_average_precision': 0.24447568119952576, 'validation/num_examples': 43793, 'test/accuracy': 0.9855917096138, 'test/loss': 0.04834149405360222, 'test/mean_average_precision': 0.23848020052834712, 'test/num_examples': 43793, 'score': 2678.29820394516, 'total_duration': 3862.7803728580475, 'accumulated_submission_time': 2678.29820394516, 'accumulated_eval_time': 1184.1708734035492, 'accumulated_logging_time': 0.2215726375579834, 'global_step': 9613, 'preemption_count': 0}), (10480, {'train/accuracy': 0.9902150630950928, 'train/loss': 0.03237683326005936, 'train/mean_average_precision': 0.3617728452243728, 'validation/accuracy': 0.9866499900817871, 'validation/loss': 0.04525260627269745, 'validation/mean_average_precision': 0.25041496898059057, 'validation/num_examples': 43793, 'test/accuracy': 0.9857522249221802, 'test/loss': 0.048185210675001144, 'test/mean_average_precision': 0.24381919704493063, 'test/num_examples': 43793, 'score': 2918.5239613056183, 'total_duration': 4189.770018339157, 'accumulated_submission_time': 2918.5239613056183, 'accumulated_eval_time': 1270.9071674346924, 'accumulated_logging_time': 0.24116134643554688, 'global_step': 10480, 'preemption_count': 0}), (11351, {'train/accuracy': 0.9906002283096313, 'train/loss': 0.031561873853206635, 'train/mean_average_precision': 0.40333428028259327, 'validation/accuracy': 0.9865239262580872, 'validation/loss': 0.04532726854085922, 'validation/mean_average_precision': 0.2522769098735859, 'validation/num_examples': 43793, 'test/accuracy': 0.9856271147727966, 'test/loss': 0.04815156012773514, 'test/mean_average_precision': 0.248453767780423, 'test/num_examples': 43793, 'score': 3158.673525094986, 'total_duration': 4512.6997611522675, 'accumulated_submission_time': 3158.673525094986, 'accumulated_eval_time': 1353.6590600013733, 'accumulated_logging_time': 0.261364221572876, 'global_step': 11351, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9908316135406494, 'train/loss': 0.030297359451651573, 'train/mean_average_precision': 0.4162751289311041, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.045136310160160065, 'validation/mean_average_precision': 0.25587156347092527, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.04815448075532913, 'test/mean_average_precision': 0.2503194125314745, 'test/num_examples': 43793, 'score': 3337.4313197135925, 'total_duration': 4774.829484701157, 'accumulated_submission_time': 3337.4313197135925, 'accumulated_eval_time': 1437.0056021213531, 'accumulated_logging_time': 0.28065037727355957, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0510 09:58:31.740562 140628902213440 submission_runner.py:587] Timing: 3337.4313197135925
I0510 09:58:31.740611 140628902213440 submission_runner.py:588] ====================
I0510 09:58:31.740758 140628902213440 submission_runner.py:651] Final ogbg score: 3337.4313197135925
