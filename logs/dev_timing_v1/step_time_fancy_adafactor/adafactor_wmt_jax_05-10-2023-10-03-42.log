python3 submission_runner.py --framework=jax --workload=wmt --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_adafactor/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_jax_05-10-2023-10-03-42.log
I0510 10:04:04.996329 140033517942592 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax.
I0510 10:04:05.070986 140033517942592 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0510 10:04:05.907450 140033517942592 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0510 10:04:05.908345 140033517942592 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0510 10:04:05.913483 140033517942592 submission_runner.py:544] Using RNG seed 311426531
I0510 10:04:08.746846 140033517942592 submission_runner.py:553] --- Tuning run 1/1 ---
I0510 10:04:08.747100 140033517942592 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1.
I0510 10:04:08.747310 140033517942592 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1/hparams.json.
I0510 10:04:08.882093 140033517942592 submission_runner.py:241] Initializing dataset.
I0510 10:04:08.892917 140033517942592 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0510 10:04:08.896142 140033517942592 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 10:04:08.896270 140033517942592 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 10:04:09.029248 140033517942592 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0510 10:04:11.479124 140033517942592 submission_runner.py:248] Initializing model.
I0510 10:04:24.812199 140033517942592 submission_runner.py:258] Initializing optimizer.
I0510 10:04:26.642199 140033517942592 submission_runner.py:265] Initializing metrics bundle.
I0510 10:04:26.642401 140033517942592 submission_runner.py:283] Initializing checkpoint and logger.
I0510 10:04:26.643373 140033517942592 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1 with prefix checkpoint_
I0510 10:04:26.643651 140033517942592 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0510 10:04:26.643715 140033517942592 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0510 10:04:27.398188 140033517942592 submission_runner.py:304] Saving meta data to /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1/meta_data_0.json.
I0510 10:04:27.399109 140033517942592 submission_runner.py:307] Saving flags to /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1/flags_0.json.
I0510 10:04:27.402890 140033517942592 submission_runner.py:319] Starting training loop.
I0510 10:06:05.666030 139857313654528 logging_writer.py:48] [0] global_step=0, grad_norm=5.604940414428711, loss=11.102252006530762
I0510 10:06:05.688213 140033517942592 spec.py:298] Evaluating on the training split.
I0510 10:06:05.691539 140033517942592 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0510 10:06:05.694647 140033517942592 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 10:06:05.694765 140033517942592 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 10:06:05.730871 140033517942592 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0510 10:06:14.822050 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 10:11:13.871784 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 10:11:13.875157 140033517942592 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0510 10:11:13.878258 140033517942592 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 10:11:13.878371 140033517942592 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 10:11:13.912967 140033517942592 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0510 10:11:22.046058 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 10:16:14.287652 140033517942592 spec.py:326] Evaluating on the test split.
I0510 10:16:14.290450 140033517942592 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0510 10:16:14.293544 140033517942592 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 10:16:14.293660 140033517942592 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 10:16:14.326945 140033517942592 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0510 10:16:22.036278 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 10:21:05.801740 140033517942592 submission_runner.py:421] Time since start: 998.40s, 	Step: 1, 	{'train/accuracy': 0.0006218975759111345, 'train/loss': 11.10664176940918, 'train/bleu': 4.0794304721412064e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.107779502868652, 'validation/bleu': 7.748653147275516e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.096091270446777, 'test/bleu': 1.892542666210706e-10, 'test/num_examples': 3003, 'score': 98.28512644767761, 'total_duration': 998.3987402915955, 'accumulated_submission_time': 98.28512644767761, 'accumulated_eval_time': 900.1134538650513, 'accumulated_logging_time': 0}
I0510 10:21:05.820192 139846180550400 logging_writer.py:48] [1] accumulated_eval_time=900.113454, accumulated_logging_time=0, accumulated_submission_time=98.285126, global_step=1, preemption_count=0, score=98.285126, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.096091, test/num_examples=3003, total_duration=998.398740, train/accuracy=0.000622, train/bleu=0.000000, train/loss=11.106642, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.107780, validation/num_examples=3000
I0510 10:21:45.169439 139846188943104 logging_writer.py:48] [100] global_step=100, grad_norm=0.18057198822498322, loss=8.64140796661377
I0510 10:22:24.465963 139846180550400 logging_writer.py:48] [200] global_step=200, grad_norm=0.43925970792770386, loss=8.076725959777832
I0510 10:23:04.302879 139846188943104 logging_writer.py:48] [300] global_step=300, grad_norm=0.5405095815658569, loss=7.495513916015625
I0510 10:23:43.829413 139846180550400 logging_writer.py:48] [400] global_step=400, grad_norm=0.518184244632721, loss=7.058549880981445
I0510 10:24:23.187452 139846188943104 logging_writer.py:48] [500] global_step=500, grad_norm=0.6473504900932312, loss=6.656162261962891
I0510 10:25:02.415057 139846180550400 logging_writer.py:48] [600] global_step=600, grad_norm=0.5952473282814026, loss=6.302546977996826
I0510 10:25:41.489112 139846188943104 logging_writer.py:48] [700] global_step=700, grad_norm=0.758441686630249, loss=5.9871015548706055
I0510 10:26:20.809529 139846180550400 logging_writer.py:48] [800] global_step=800, grad_norm=0.5368688106536865, loss=5.690999984741211
I0510 10:27:00.091330 139846188943104 logging_writer.py:48] [900] global_step=900, grad_norm=0.6278789043426514, loss=5.527344703674316
I0510 10:27:39.390617 139846180550400 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9960376620292664, loss=5.364825248718262
I0510 10:28:18.646810 139846188943104 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6554992198944092, loss=5.124906063079834
I0510 10:28:57.859796 139846180550400 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5528278946876526, loss=4.937838077545166
I0510 10:29:37.250893 139846188943104 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6017072796821594, loss=4.893363952636719
I0510 10:30:16.320024 139846180550400 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7559688687324524, loss=4.777077674865723
I0510 10:30:55.518064 139846188943104 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.621486246585846, loss=4.529558181762695
I0510 10:31:34.828924 139846180550400 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5724993944168091, loss=4.461112022399902
I0510 10:32:14.238910 139846188943104 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5788366794586182, loss=4.3594536781311035
I0510 10:32:53.521985 139846180550400 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7296624183654785, loss=4.27044153213501
I0510 10:33:32.494289 139846188943104 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.46031349897384644, loss=4.1871337890625
I0510 10:34:11.680381 139846180550400 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6589608788490295, loss=4.201395034790039
I0510 10:34:51.384793 139846188943104 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4732001721858978, loss=4.023815155029297
I0510 10:35:05.959455 140033517942592 spec.py:298] Evaluating on the training split.
I0510 10:35:08.801728 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 10:38:25.324928 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 10:38:28.020948 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 10:41:10.663531 140033517942592 spec.py:326] Evaluating on the test split.
I0510 10:41:13.428417 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 10:43:47.530003 140033517942592 submission_runner.py:421] Time since start: 2360.13s, 	Step: 2138, 	{'train/accuracy': 0.4949171543121338, 'train/loss': 3.1205086708068848, 'train/bleu': 20.71132525601454, 'validation/accuracy': 0.49427780508995056, 'validation/loss': 3.126310110092163, 'validation/bleu': 16.95735456551898, 'validation/num_examples': 3000, 'test/accuracy': 0.48854804039001465, 'test/loss': 3.2191989421844482, 'test/bleu': 15.111284543298662, 'test/num_examples': 3003, 'score': 938.3866350650787, 'total_duration': 2360.1270117759705, 'accumulated_submission_time': 938.3866350650787, 'accumulated_eval_time': 1421.6839497089386, 'accumulated_logging_time': 0.027788400650024414}
I0510 10:43:47.539573 139846180550400 logging_writer.py:48] [2138] accumulated_eval_time=1421.683950, accumulated_logging_time=0.027788, accumulated_submission_time=938.386635, global_step=2138, preemption_count=0, score=938.386635, test/accuracy=0.488548, test/bleu=15.111285, test/loss=3.219199, test/num_examples=3003, total_duration=2360.127012, train/accuracy=0.494917, train/bleu=20.711325, train/loss=3.120509, validation/accuracy=0.494278, validation/bleu=16.957355, validation/loss=3.126310, validation/num_examples=3000
I0510 10:44:12.277055 139846188943104 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5605374574661255, loss=4.089503288269043
I0510 10:44:51.347143 139846180550400 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5383681654930115, loss=4.059039115905762
I0510 10:45:30.489555 139846188943104 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4435489773750305, loss=3.9387693405151367
I0510 10:46:09.342530 139846180550400 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.43235430121421814, loss=3.9817140102386475
I0510 10:46:48.376512 139846188943104 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6400517821311951, loss=3.911350965499878
I0510 10:47:27.383525 139846180550400 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6358142495155334, loss=3.8865652084350586
I0510 10:48:06.320902 139846188943104 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5472682118415833, loss=3.8778076171875
I0510 10:48:45.467854 139846180550400 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.44902995228767395, loss=3.980518341064453
I0510 10:49:24.565535 139846188943104 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.41268399357795715, loss=3.7684528827667236
I0510 10:50:03.646412 139846180550400 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.37236297130584717, loss=3.782733917236328
I0510 10:50:42.699354 139846188943104 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3824896216392517, loss=3.683091640472412
I0510 10:51:21.584929 139846180550400 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3876763880252838, loss=3.674400568008423
I0510 10:52:00.423225 139846188943104 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.3700491487979889, loss=3.7058494091033936
I0510 10:52:39.381023 139846180550400 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.3090229332447052, loss=3.674159049987793
I0510 10:53:18.368164 139846188943104 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.3120691180229187, loss=3.6591997146606445
I0510 10:53:57.306890 139846180550400 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2844583988189697, loss=3.614553689956665
I0510 10:54:36.481614 139846188943104 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.2834021747112274, loss=3.6700599193573
I0510 10:55:15.896078 139846180550400 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.3092048466205597, loss=3.550319194793701
I0510 10:55:55.263502 139846188943104 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.28697410225868225, loss=3.622217893600464
I0510 10:56:33.987314 139846180550400 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.2567347288131714, loss=3.521479845046997
I0510 10:57:13.003648 139846188943104 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.36089229583740234, loss=3.548182725906372
I0510 10:57:47.635889 140033517942592 spec.py:298] Evaluating on the training split.
I0510 10:57:50.472511 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:00:30.465611 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 11:00:33.144585 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:03:04.308734 140033517942592 spec.py:326] Evaluating on the test split.
I0510 11:03:07.070828 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:05:33.048516 140033517942592 submission_runner.py:421] Time since start: 3665.65s, 	Step: 4290, 	{'train/accuracy': 0.5587763786315918, 'train/loss': 2.501476764678955, 'train/bleu': 26.17098898484752, 'validation/accuracy': 0.5708174705505371, 'validation/loss': 2.393796443939209, 'validation/bleu': 22.459583770356296, 'validation/num_examples': 3000, 'test/accuracy': 0.574981153011322, 'test/loss': 2.392484664916992, 'test/bleu': 21.272907217677858, 'test/num_examples': 3003, 'score': 1778.445957183838, 'total_duration': 3665.6455266475677, 'accumulated_submission_time': 1778.445957183838, 'accumulated_eval_time': 1887.0965161323547, 'accumulated_logging_time': 0.046364784240722656}
I0510 11:05:33.057820 139846180550400 logging_writer.py:48] [4290] accumulated_eval_time=1887.096516, accumulated_logging_time=0.046365, accumulated_submission_time=1778.445957, global_step=4290, preemption_count=0, score=1778.445957, test/accuracy=0.574981, test/bleu=21.272907, test/loss=2.392485, test/num_examples=3003, total_duration=3665.645527, train/accuracy=0.558776, train/bleu=26.170989, train/loss=2.501477, validation/accuracy=0.570817, validation/bleu=22.459584, validation/loss=2.393796, validation/num_examples=3000
I0510 11:05:37.334010 139846188943104 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.2770330011844635, loss=3.551602840423584
I0510 11:06:16.307446 139846180550400 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.2077476531267166, loss=3.541639804840088
I0510 11:06:55.025981 139846188943104 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.20191943645477295, loss=3.4454591274261475
I0510 11:07:33.978028 139846180550400 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.21872062981128693, loss=3.479898691177368
I0510 11:08:13.254312 139846188943104 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.23779861629009247, loss=3.4859678745269775
I0510 11:08:52.183965 139846180550400 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.22372829914093018, loss=3.5144824981689453
I0510 11:09:30.916324 139846188943104 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.2113874852657318, loss=3.422285556793213
I0510 11:10:10.090772 139846180550400 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.17857246100902557, loss=3.3907830715179443
I0510 11:10:48.982906 139846188943104 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.17261485755443573, loss=3.447768449783325
I0510 11:11:27.928577 139846180550400 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.20178857445716858, loss=3.46536922454834
I0510 11:12:06.922034 139846188943104 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.18451538681983948, loss=3.4103333950042725
I0510 11:12:45.579717 139846180550400 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.2252027839422226, loss=3.3663573265075684
I0510 11:13:24.725068 139846188943104 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.17423087358474731, loss=3.443913698196411
I0510 11:14:04.022091 139846180550400 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.17599697411060333, loss=3.3481616973876953
I0510 11:14:43.177479 139846188943104 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.17176757752895355, loss=3.360412359237671
I0510 11:15:22.110830 139846180550400 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.20668473839759827, loss=3.42387318611145
I0510 11:16:00.909044 139846188943104 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.1720767617225647, loss=3.2989389896392822
I0510 11:16:40.159337 139846180550400 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.15903039276599884, loss=3.322932243347168
I0510 11:17:19.290493 139846188943104 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.1949204057455063, loss=3.3130528926849365
I0510 11:17:58.214031 139846180550400 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.23244857788085938, loss=3.3800320625305176
I0510 11:18:36.977033 139846188943104 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.18856529891490936, loss=3.3748466968536377
I0510 11:19:16.160626 139846180550400 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.18297456204891205, loss=3.3902719020843506
I0510 11:19:33.184745 140033517942592 spec.py:298] Evaluating on the training split.
I0510 11:19:35.995806 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:22:24.546560 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 11:22:27.246860 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:24:52.592190 140033517942592 spec.py:326] Evaluating on the test split.
I0510 11:24:55.350466 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:27:10.212152 140033517942592 submission_runner.py:421] Time since start: 4962.81s, 	Step: 6445, 	{'train/accuracy': 0.6057388186454773, 'train/loss': 2.1112735271453857, 'train/bleu': 28.833344529957486, 'validation/accuracy': 0.6034147143363953, 'validation/loss': 2.138148546218872, 'validation/bleu': 24.854996303847244, 'validation/num_examples': 3000, 'test/accuracy': 0.6046017408370972, 'test/loss': 2.1150717735290527, 'test/bleu': 22.892786731679422, 'test/num_examples': 3003, 'score': 2618.5342071056366, 'total_duration': 4962.809171676636, 'accumulated_submission_time': 2618.5342071056366, 'accumulated_eval_time': 2344.1238672733307, 'accumulated_logging_time': 0.06595659255981445}
I0510 11:27:10.221865 139846188943104 logging_writer.py:48] [6445] accumulated_eval_time=2344.123867, accumulated_logging_time=0.065957, accumulated_submission_time=2618.534207, global_step=6445, preemption_count=0, score=2618.534207, test/accuracy=0.604602, test/bleu=22.892787, test/loss=2.115072, test/num_examples=3003, total_duration=4962.809172, train/accuracy=0.605739, train/bleu=28.833345, train/loss=2.111274, validation/accuracy=0.603415, validation/bleu=24.854996, validation/loss=2.138149, validation/num_examples=3000
I0510 11:27:32.314301 139846180550400 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.22061845660209656, loss=3.363389015197754
I0510 11:28:11.873544 139846188943104 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.17920298874378204, loss=3.3414955139160156
I0510 11:28:51.023998 139846180550400 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.17130599915981293, loss=3.322542428970337
I0510 11:29:30.242399 139846188943104 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1627165526151657, loss=3.333122730255127
I0510 11:30:09.344576 139846180550400 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.20070095360279083, loss=3.329385995864868
I0510 11:30:48.540654 139846188943104 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.27220794558525085, loss=3.345247268676758
I0510 11:31:27.944454 139846180550400 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.19051669538021088, loss=3.3047497272491455
I0510 11:32:07.115567 139846188943104 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.15921485424041748, loss=3.3311712741851807
I0510 11:32:46.361537 139846180550400 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.16721324622631073, loss=3.2062289714813232
I0510 11:33:25.346258 139846188943104 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.20037397742271423, loss=3.281250238418579
I0510 11:34:04.819148 139846180550400 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.1862446665763855, loss=3.277122974395752
I0510 11:34:44.136742 139846188943104 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.19421783089637756, loss=3.2473065853118896
I0510 11:35:23.000932 139846180550400 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.17638228833675385, loss=3.234694242477417
I0510 11:36:02.064130 139846188943104 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1468220055103302, loss=3.183654546737671
I0510 11:36:40.855576 139846180550400 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.18861708045005798, loss=3.2343742847442627
I0510 11:37:19.716938 139846188943104 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.171040877699852, loss=3.2052321434020996
I0510 11:37:58.832309 139846180550400 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.15880966186523438, loss=3.2120604515075684
I0510 11:38:38.189025 139846188943104 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.2126384973526001, loss=3.187039613723755
I0510 11:39:16.974927 139846180550400 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.18403294682502747, loss=3.1863553524017334
I0510 11:39:55.838275 139846188943104 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.15470869839191437, loss=3.1013829708099365
I0510 11:40:35.073281 139846180550400 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.21131150424480438, loss=3.1976616382598877
I0510 11:41:10.519273 140033517942592 spec.py:298] Evaluating on the training split.
I0510 11:41:13.325902 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:43:43.071652 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 11:43:45.760521 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:46:05.483200 140033517942592 spec.py:326] Evaluating on the test split.
I0510 11:46:08.238371 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 11:48:21.407711 140033517942592 submission_runner.py:421] Time since start: 6234.00s, 	Step: 8592, 	{'train/accuracy': 0.6130054593086243, 'train/loss': 2.0491323471069336, 'train/bleu': 28.940303905765937, 'validation/accuracy': 0.6251751184463501, 'validation/loss': 1.9527522325515747, 'validation/bleu': 25.982811415873588, 'validation/num_examples': 3000, 'test/accuracy': 0.6316542029380798, 'test/loss': 1.9123399257659912, 'test/bleu': 24.697483042317725, 'test/num_examples': 3003, 'score': 3458.7932896614075, 'total_duration': 6234.0047245025635, 'accumulated_submission_time': 3458.7932896614075, 'accumulated_eval_time': 2775.0122537612915, 'accumulated_logging_time': 0.08489060401916504}
I0510 11:48:21.418117 139846188943104 logging_writer.py:48] [8592] accumulated_eval_time=2775.012254, accumulated_logging_time=0.084891, accumulated_submission_time=3458.793290, global_step=8592, preemption_count=0, score=3458.793290, test/accuracy=0.631654, test/bleu=24.697483, test/loss=1.912340, test/num_examples=3003, total_duration=6234.004725, train/accuracy=0.613005, train/bleu=28.940304, train/loss=2.049132, validation/accuracy=0.625175, validation/bleu=25.982811, validation/loss=1.952752, validation/num_examples=3000
I0510 11:48:24.943892 139846180550400 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16652166843414307, loss=3.2108168601989746
I0510 11:49:03.774074 139846188943104 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.14316673576831818, loss=3.183962106704712
I0510 11:49:42.802001 139846180550400 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.15844283998012543, loss=3.217107057571411
I0510 11:50:21.890558 139846188943104 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3310689628124237, loss=3.185529947280884
I0510 11:51:00.859898 139846180550400 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15078596770763397, loss=3.215668201446533
I0510 11:51:39.796352 139846188943104 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.17078366875648499, loss=3.102062463760376
I0510 11:52:19.044297 139846180550400 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.2039867341518402, loss=3.1167500019073486
I0510 11:52:58.563775 139846188943104 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.19396273791790009, loss=3.1183807849884033
I0510 11:53:37.688426 139846180550400 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.2507217228412628, loss=3.1113979816436768
I0510 11:54:17.091783 139846188943104 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.17257989943027496, loss=3.1228199005126953
I0510 11:54:56.104014 139846180550400 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.16361269354820251, loss=3.1982460021972656
I0510 11:55:34.872208 139846188943104 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.170108824968338, loss=3.107982635498047
I0510 11:56:13.842551 139846180550400 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.15163293480873108, loss=3.1572625637054443
I0510 11:56:52.843138 139846188943104 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.23286741971969604, loss=3.0880682468414307
I0510 11:57:31.813426 139846180550400 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.2334650158882141, loss=3.1059131622314453
I0510 11:58:10.552207 139846188943104 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.21145060658454895, loss=3.143747091293335
I0510 11:58:49.503194 139846180550400 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.22262738645076752, loss=3.0828795433044434
I0510 11:59:28.416585 139846188943104 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.21625643968582153, loss=3.116643190383911
I0510 12:00:07.133125 139846180550400 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.19897811114788055, loss=3.1530280113220215
I0510 12:00:46.066035 139846188943104 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.15624244511127472, loss=3.133450508117676
I0510 12:01:24.979863 139846180550400 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.21957045793533325, loss=3.1152658462524414
I0510 12:02:04.423539 139846188943104 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.1958283632993698, loss=3.0810024738311768
I0510 12:02:21.613014 140033517942592 spec.py:298] Evaluating on the training split.
I0510 12:02:24.437640 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:04:48.732035 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 12:04:51.425219 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:07:07.584049 140033517942592 spec.py:326] Evaluating on the test split.
I0510 12:07:10.337361 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:09:20.187793 140033517942592 submission_runner.py:421] Time since start: 7492.78s, 	Step: 10745, 	{'train/accuracy': 0.6251749396324158, 'train/loss': 1.9724678993225098, 'train/bleu': 29.758918984350117, 'validation/accuracy': 0.6367310881614685, 'validation/loss': 1.8652809858322144, 'validation/bleu': 26.78234630657909, 'validation/num_examples': 3000, 'test/accuracy': 0.6461797952651978, 'test/loss': 1.8112452030181885, 'test/bleu': 25.999656267044557, 'test/num_examples': 3003, 'score': 4298.949460983276, 'total_duration': 7492.784815311432, 'accumulated_submission_time': 4298.949460983276, 'accumulated_eval_time': 3193.587000846863, 'accumulated_logging_time': 0.10575485229492188}
I0510 12:09:20.198457 139846180550400 logging_writer.py:48] [10745] accumulated_eval_time=3193.587001, accumulated_logging_time=0.105755, accumulated_submission_time=4298.949461, global_step=10745, preemption_count=0, score=4298.949461, test/accuracy=0.646180, test/bleu=25.999656, test/loss=1.811245, test/num_examples=3003, total_duration=7492.784815, train/accuracy=0.625175, train/bleu=29.758919, train/loss=1.972468, validation/accuracy=0.636731, validation/bleu=26.782346, validation/loss=1.865281, validation/num_examples=3000
I0510 12:09:42.153691 139846188943104 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2832651734352112, loss=3.1338672637939453
I0510 12:10:21.109900 139846180550400 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17377154529094696, loss=3.007981538772583
I0510 12:10:59.834099 139846188943104 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.14572308957576752, loss=3.1829590797424316
I0510 12:11:38.842960 139846180550400 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.20257163047790527, loss=3.10029935836792
I0510 12:12:18.191620 139846188943104 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.19631119072437286, loss=3.035313844680786
I0510 12:12:57.359233 139846180550400 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.17603757977485657, loss=3.0826070308685303
I0510 12:13:36.469353 139846188943104 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.20948658883571625, loss=3.0148355960845947
I0510 12:14:15.440277 139846180550400 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.18623772263526917, loss=3.092590093612671
I0510 12:14:54.384220 139846188943104 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.19714109599590302, loss=3.0436270236968994
I0510 12:15:33.152311 139846180550400 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.18521296977996826, loss=3.114222526550293
I0510 12:16:12.304676 139846188943104 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.19412429630756378, loss=3.0515854358673096
I0510 12:16:51.646170 139846180550400 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.23047581315040588, loss=3.1124796867370605
I0510 12:17:30.989880 139846188943104 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1674978882074356, loss=3.094625473022461
I0510 12:18:09.850230 139846180550400 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.1743418127298355, loss=3.041205883026123
I0510 12:18:49.203472 139846188943104 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.23273299634456635, loss=3.0436253547668457
I0510 12:19:28.557402 139846180550400 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.16516177356243134, loss=3.02694034576416
I0510 12:20:07.795259 139846188943104 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.16877932846546173, loss=3.0630993843078613
I0510 12:20:47.196503 139846180550400 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2874772846698761, loss=3.045278549194336
I0510 12:21:26.302039 139846188943104 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.38317281007766724, loss=3.0114758014678955
I0510 12:22:05.402159 139846180550400 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.17520581185817719, loss=3.0419061183929443
I0510 12:22:44.533868 139846188943104 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2529752552509308, loss=3.083548069000244
I0510 12:23:20.406441 140033517942592 spec.py:298] Evaluating on the training split.
I0510 12:23:23.208019 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:26:19.427474 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 12:26:22.121041 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:28:47.217168 140033517942592 spec.py:326] Evaluating on the test split.
I0510 12:28:49.964802 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:31:01.295011 140033517942592 submission_runner.py:421] Time since start: 8793.89s, 	Step: 12893, 	{'train/accuracy': 0.6465591788291931, 'train/loss': 1.8093706369400024, 'train/bleu': 31.63608692351997, 'validation/accuracy': 0.6492046117782593, 'validation/loss': 1.7860885858535767, 'validation/bleu': 27.615217720749044, 'validation/num_examples': 3000, 'test/accuracy': 0.657184362411499, 'test/loss': 1.7310973405838013, 'test/bleu': 26.660907824926678, 'test/num_examples': 3003, 'score': 5139.119298458099, 'total_duration': 8793.892016649246, 'accumulated_submission_time': 5139.119298458099, 'accumulated_eval_time': 3654.475510597229, 'accumulated_logging_time': 0.12565350532531738}
I0510 12:31:01.305362 139846180550400 logging_writer.py:48] [12893] accumulated_eval_time=3654.475511, accumulated_logging_time=0.125654, accumulated_submission_time=5139.119298, global_step=12893, preemption_count=0, score=5139.119298, test/accuracy=0.657184, test/bleu=26.660908, test/loss=1.731097, test/num_examples=3003, total_duration=8793.892017, train/accuracy=0.646559, train/bleu=31.636087, train/loss=1.809371, validation/accuracy=0.649205, validation/bleu=27.615218, validation/loss=1.786089, validation/num_examples=3000
I0510 12:31:04.413437 139846188943104 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.2068958580493927, loss=3.028165578842163
I0510 12:31:43.053715 139846180550400 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.1537652313709259, loss=3.056276559829712
I0510 12:32:22.008645 139846188943104 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.20661939680576324, loss=3.005845546722412
I0510 12:33:00.922548 139846180550400 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.1519864797592163, loss=3.0046279430389404
I0510 12:33:39.874276 139846188943104 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.19257283210754395, loss=3.0041794776916504
I0510 12:34:18.863651 139846180550400 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.19607678055763245, loss=3.047731399536133
I0510 12:34:57.629782 139846188943104 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.24387922883033752, loss=3.0646884441375732
I0510 12:35:36.719191 139846180550400 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.19513347744941711, loss=3.026277542114258
I0510 12:36:16.200942 139846188943104 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.168195441365242, loss=3.0178275108337402
I0510 12:36:55.058686 139846180550400 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.24879762530326843, loss=2.9379518032073975
I0510 12:37:34.415405 139846188943104 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.18535815179347992, loss=3.0052292346954346
I0510 12:38:13.932865 139846180550400 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.20157469809055328, loss=2.918973684310913
I0510 12:38:52.974828 139846188943104 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.16802597045898438, loss=2.99637508392334
I0510 12:39:31.807171 139846180550400 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.1530991941690445, loss=3.0200581550598145
I0510 12:40:10.697886 139846188943104 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2908555865287781, loss=2.968892812728882
I0510 12:40:49.883388 139846180550400 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.1887820065021515, loss=2.9951281547546387
I0510 12:41:28.950465 139846188943104 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.16304482519626617, loss=3.0036637783050537
I0510 12:42:07.820014 139846180550400 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.20450043678283691, loss=3.003688335418701
I0510 12:42:46.700608 139846188943104 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2854607105255127, loss=2.9942827224731445
I0510 12:43:25.608483 139846180550400 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.35838794708251953, loss=3.012894630432129
I0510 12:44:04.527107 139846188943104 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.20457814633846283, loss=2.9996542930603027
I0510 12:44:43.410488 139846180550400 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2008654773235321, loss=3.0248942375183105
I0510 12:45:01.624285 140033517942592 spec.py:298] Evaluating on the training split.
I0510 12:45:04.436423 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:48:48.346968 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 12:48:51.033333 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:51:32.170494 140033517942592 spec.py:326] Evaluating on the test split.
I0510 12:51:34.900921 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 12:53:54.893930 140033517942592 submission_runner.py:421] Time since start: 10167.49s, 	Step: 15048, 	{'train/accuracy': 0.6406798958778381, 'train/loss': 1.8438520431518555, 'train/bleu': 31.38206851348655, 'validation/accuracy': 0.6539038419723511, 'validation/loss': 1.7457467317581177, 'validation/bleu': 28.006477454938523, 'validation/num_examples': 3000, 'test/accuracy': 0.6631572842597961, 'test/loss': 1.6796035766601562, 'test/bleu': 27.1320811390829, 'test/num_examples': 3003, 'score': 5979.399863958359, 'total_duration': 10167.490891218185, 'accumulated_submission_time': 5979.399863958359, 'accumulated_eval_time': 4187.745041847229, 'accumulated_logging_time': 0.1455543041229248}
I0510 12:53:54.906604 139846188943104 logging_writer.py:48] [15048] accumulated_eval_time=4187.745042, accumulated_logging_time=0.145554, accumulated_submission_time=5979.399864, global_step=15048, preemption_count=0, score=5979.399864, test/accuracy=0.663157, test/bleu=27.132081, test/loss=1.679604, test/num_examples=3003, total_duration=10167.490891, train/accuracy=0.640680, train/bleu=31.382069, train/loss=1.843852, validation/accuracy=0.653904, validation/bleu=28.006477, validation/loss=1.745747, validation/num_examples=3000
I0510 12:54:15.994195 139846180550400 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.16681405901908875, loss=2.9762351512908936
I0510 12:54:54.918178 139846188943104 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.18338188529014587, loss=2.997802734375
I0510 12:55:33.888477 139846180550400 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.16191841661930084, loss=3.0369346141815186
I0510 12:56:12.824476 139846188943104 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1592462658882141, loss=2.9778640270233154
I0510 12:56:51.574983 139846180550400 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.2637656033039093, loss=3.006533622741699
I0510 12:57:30.746745 139846188943104 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.22050754725933075, loss=2.879683017730713
I0510 12:58:09.944145 139846180550400 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2724452316761017, loss=2.969297409057617
I0510 12:58:48.858382 139846188943104 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.23433801531791687, loss=2.979302167892456
I0510 12:59:27.608487 139846180550400 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1812530905008316, loss=3.0421032905578613
I0510 13:00:07.176733 139846188943104 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.18899616599082947, loss=2.997156858444214
I0510 13:00:46.543817 139846180550400 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.18732531368732452, loss=2.975133180618286
I0510 13:01:25.739795 139846188943104 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.34029462933540344, loss=2.994335174560547
I0510 13:02:05.193437 139846180550400 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.18056383728981018, loss=2.897972345352173
I0510 13:02:44.003437 139846188943104 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.22638706862926483, loss=2.998701333999634
I0510 13:03:23.289489 139846180550400 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.15852539241313934, loss=3.0041356086730957
I0510 13:04:02.243194 139846188943104 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.17131257057189941, loss=2.9567880630493164
I0510 13:04:41.400279 139846180550400 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.19629529118537903, loss=2.887453079223633
I0510 13:05:20.716310 139846188943104 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1746096909046173, loss=2.97580623626709
I0510 13:05:59.873388 139846180550400 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15074850618839264, loss=2.92222261428833
I0510 13:06:38.849125 139846188943104 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2973851263523102, loss=3.020761013031006
I0510 13:07:17.834293 139846180550400 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.32674548029899597, loss=3.099111795425415
I0510 13:07:55.188043 140033517942592 spec.py:298] Evaluating on the training split.
I0510 13:07:58.012491 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:10:33.317080 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 13:10:35.997923 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:12:55.430641 140033517942592 spec.py:326] Evaluating on the test split.
I0510 13:12:58.171288 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:15:07.030188 140033517942592 submission_runner.py:421] Time since start: 11439.63s, 	Step: 17197, 	{'train/accuracy': 0.6428076028823853, 'train/loss': 1.821254849433899, 'train/bleu': 31.20799557614278, 'validation/accuracy': 0.659061849117279, 'validation/loss': 1.7108757495880127, 'validation/bleu': 28.260321199283887, 'validation/num_examples': 3000, 'test/accuracy': 0.6690605282783508, 'test/loss': 1.6426277160644531, 'test/bleu': 27.651132696237433, 'test/num_examples': 3003, 'score': 6819.642137050629, 'total_duration': 11439.627197742462, 'accumulated_submission_time': 6819.642137050629, 'accumulated_eval_time': 4619.587123394012, 'accumulated_logging_time': 0.16791796684265137}
I0510 13:15:07.041296 139846188943104 logging_writer.py:48] [17197] accumulated_eval_time=4619.587123, accumulated_logging_time=0.167918, accumulated_submission_time=6819.642137, global_step=17197, preemption_count=0, score=6819.642137, test/accuracy=0.669061, test/bleu=27.651133, test/loss=1.642628, test/num_examples=3003, total_duration=11439.627198, train/accuracy=0.642808, train/bleu=31.207996, train/loss=1.821255, validation/accuracy=0.659062, validation/bleu=28.260321, validation/loss=1.710876, validation/num_examples=3000
I0510 13:15:08.593085 139846180550400 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3778974711894989, loss=2.9662389755249023
I0510 13:15:47.577065 139846188943104 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2020551562309265, loss=2.9319207668304443
I0510 13:16:26.726452 139846180550400 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.18419550359249115, loss=2.991929531097412
I0510 13:17:05.519323 139846188943104 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2415764182806015, loss=2.9417128562927246
I0510 13:17:44.561651 139846180550400 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.39492690563201904, loss=3.0551233291625977
I0510 13:18:23.668462 139846188943104 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2229805737733841, loss=2.9381866455078125
I0510 13:19:02.641359 139846180550400 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.19874325394630432, loss=2.9867424964904785
I0510 13:19:41.616214 139846188943104 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.21418112516403198, loss=2.9322314262390137
I0510 13:20:20.625858 139846180550400 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.18336202204227448, loss=2.9497668743133545
I0510 13:21:00.055295 139846188943104 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.24675068259239197, loss=2.9234771728515625
I0510 13:21:38.934835 139846180550400 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.18206815421581268, loss=2.9933485984802246
I0510 13:22:17.949557 139846188943104 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1560623049736023, loss=2.9703176021575928
I0510 13:22:57.502459 139846180550400 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2109643518924713, loss=2.9337270259857178
I0510 13:23:36.788524 139846188943104 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.21905958652496338, loss=2.9275853633880615
I0510 13:24:15.541122 139846180550400 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1860760599374771, loss=2.9604499340057373
I0510 13:24:54.508060 139846188943104 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.213266521692276, loss=2.945136547088623
I0510 13:25:33.466006 139846180550400 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.27251139283180237, loss=2.894580364227295
I0510 13:26:12.652396 139846188943104 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.2436351180076599, loss=2.8707916736602783
I0510 13:26:51.722142 139846180550400 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.16782334446907043, loss=2.873786449432373
I0510 13:27:30.446925 139846188943104 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.29015955328941345, loss=2.855424642562866
I0510 13:28:09.355304 139846180550400 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.31814655661582947, loss=2.895139217376709
I0510 13:28:48.460142 139846188943104 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.20596778392791748, loss=2.956298351287842
I0510 13:29:07.077755 140033517942592 spec.py:298] Evaluating on the training split.
I0510 13:29:09.903052 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:31:47.444126 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 13:31:50.135221 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:34:05.491650 140033517942592 spec.py:326] Evaluating on the test split.
I0510 13:34:08.236213 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:36:15.565345 140033517942592 submission_runner.py:421] Time since start: 12708.16s, 	Step: 19349, 	{'train/accuracy': 0.6570152044296265, 'train/loss': 1.7196663618087769, 'train/bleu': 32.61968947597813, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.6820471286773682, 'validation/bleu': 28.638508812785417, 'validation/num_examples': 3000, 'test/accuracy': 0.6708616614341736, 'test/loss': 1.6183403730392456, 'test/bleu': 27.67401174653705, 'test/num_examples': 3003, 'score': 7659.641139268875, 'total_duration': 12708.162348270416, 'accumulated_submission_time': 7659.641139268875, 'accumulated_eval_time': 5048.074641704559, 'accumulated_logging_time': 0.18776512145996094}
I0510 13:36:15.576829 139846180550400 logging_writer.py:48] [19349] accumulated_eval_time=5048.074642, accumulated_logging_time=0.187765, accumulated_submission_time=7659.641139, global_step=19349, preemption_count=0, score=7659.641139, test/accuracy=0.670862, test/bleu=27.674012, test/loss=1.618340, test/num_examples=3003, total_duration=12708.162348, train/accuracy=0.657015, train/bleu=32.619689, train/loss=1.719666, validation/accuracy=0.662062, validation/bleu=28.638509, validation/loss=1.682047, validation/num_examples=3000
I0510 13:36:35.755434 139846188943104 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.19648870825767517, loss=2.955573797225952
I0510 13:37:14.717484 139846180550400 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.22550320625305176, loss=2.8819661140441895
I0510 13:37:53.666459 139846188943104 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2010979950428009, loss=2.903621196746826
I0510 13:38:32.570392 139846180550400 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.16532857716083527, loss=2.9709208011627197
I0510 13:39:11.488095 139846188943104 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.20792558789253235, loss=2.9586241245269775
I0510 13:39:50.433234 139846180550400 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.2108003795146942, loss=2.8678781986236572
I0510 13:40:28.967961 140033517942592 spec.py:298] Evaluating on the training split.
I0510 13:40:31.772535 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:43:01.900602 140033517942592 spec.py:310] Evaluating on the validation split.
I0510 13:43:04.584723 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:45:23.675638 140033517942592 spec.py:326] Evaluating on the test split.
I0510 13:45:26.428116 140033517942592 workload.py:179] Translating evaluation dataset.
I0510 13:47:35.264788 140033517942592 submission_runner.py:421] Time since start: 13387.86s, 	Step: 20000, 	{'train/accuracy': 0.6529373526573181, 'train/loss': 1.7420909404754639, 'train/bleu': 31.874985357935557, 'validation/accuracy': 0.6630544066429138, 'validation/loss': 1.6798782348632812, 'validation/bleu': 28.489674989638065, 'validation/num_examples': 3000, 'test/accuracy': 0.6731276512145996, 'test/loss': 1.6144088506698608, 'test/bleu': 28.03205100921681, 'test/num_examples': 3003, 'score': 7913.014540910721, 'total_duration': 13387.861794233322, 'accumulated_submission_time': 7913.014540910721, 'accumulated_eval_time': 5474.371400117874, 'accumulated_logging_time': 0.20819854736328125}
I0510 13:47:35.276206 139846188943104 logging_writer.py:48] [20000] accumulated_eval_time=5474.371400, accumulated_logging_time=0.208199, accumulated_submission_time=7913.014541, global_step=20000, preemption_count=0, score=7913.014541, test/accuracy=0.673128, test/bleu=28.032051, test/loss=1.614409, test/num_examples=3003, total_duration=13387.861794, train/accuracy=0.652937, train/bleu=31.874985, train/loss=1.742091, validation/accuracy=0.663054, validation/bleu=28.489675, validation/loss=1.679878, validation/num_examples=3000
I0510 13:47:35.293436 139846180550400 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7913.014541
I0510 13:47:35.755119 140033517942592 checkpoints.py:356] Saving checkpoint at step: 20000
I0510 13:47:37.466400 140033517942592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1/checkpoint_20000
I0510 13:47:37.468610 140033517942592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_adafactor/timing_adafactor/wmt_jax/trial_1/checkpoint_20000.
I0510 13:47:37.529417 140033517942592 submission_runner.py:584] Tuning trial 1/1
I0510 13:47:37.529588 140033517942592 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0510 13:47:37.532333 140033517942592 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006218975759111345, 'train/loss': 11.10664176940918, 'train/bleu': 4.0794304721412064e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.107779502868652, 'validation/bleu': 7.748653147275516e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.096091270446777, 'test/bleu': 1.892542666210706e-10, 'test/num_examples': 3003, 'score': 98.28512644767761, 'total_duration': 998.3987402915955, 'accumulated_submission_time': 98.28512644767761, 'accumulated_eval_time': 900.1134538650513, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2138, {'train/accuracy': 0.4949171543121338, 'train/loss': 3.1205086708068848, 'train/bleu': 20.71132525601454, 'validation/accuracy': 0.49427780508995056, 'validation/loss': 3.126310110092163, 'validation/bleu': 16.95735456551898, 'validation/num_examples': 3000, 'test/accuracy': 0.48854804039001465, 'test/loss': 3.2191989421844482, 'test/bleu': 15.111284543298662, 'test/num_examples': 3003, 'score': 938.3866350650787, 'total_duration': 2360.1270117759705, 'accumulated_submission_time': 938.3866350650787, 'accumulated_eval_time': 1421.6839497089386, 'accumulated_logging_time': 0.027788400650024414, 'global_step': 2138, 'preemption_count': 0}), (4290, {'train/accuracy': 0.5587763786315918, 'train/loss': 2.501476764678955, 'train/bleu': 26.17098898484752, 'validation/accuracy': 0.5708174705505371, 'validation/loss': 2.393796443939209, 'validation/bleu': 22.459583770356296, 'validation/num_examples': 3000, 'test/accuracy': 0.574981153011322, 'test/loss': 2.392484664916992, 'test/bleu': 21.272907217677858, 'test/num_examples': 3003, 'score': 1778.445957183838, 'total_duration': 3665.6455266475677, 'accumulated_submission_time': 1778.445957183838, 'accumulated_eval_time': 1887.0965161323547, 'accumulated_logging_time': 0.046364784240722656, 'global_step': 4290, 'preemption_count': 0}), (6445, {'train/accuracy': 0.6057388186454773, 'train/loss': 2.1112735271453857, 'train/bleu': 28.833344529957486, 'validation/accuracy': 0.6034147143363953, 'validation/loss': 2.138148546218872, 'validation/bleu': 24.854996303847244, 'validation/num_examples': 3000, 'test/accuracy': 0.6046017408370972, 'test/loss': 2.1150717735290527, 'test/bleu': 22.892786731679422, 'test/num_examples': 3003, 'score': 2618.5342071056366, 'total_duration': 4962.809171676636, 'accumulated_submission_time': 2618.5342071056366, 'accumulated_eval_time': 2344.1238672733307, 'accumulated_logging_time': 0.06595659255981445, 'global_step': 6445, 'preemption_count': 0}), (8592, {'train/accuracy': 0.6130054593086243, 'train/loss': 2.0491323471069336, 'train/bleu': 28.940303905765937, 'validation/accuracy': 0.6251751184463501, 'validation/loss': 1.9527522325515747, 'validation/bleu': 25.982811415873588, 'validation/num_examples': 3000, 'test/accuracy': 0.6316542029380798, 'test/loss': 1.9123399257659912, 'test/bleu': 24.697483042317725, 'test/num_examples': 3003, 'score': 3458.7932896614075, 'total_duration': 6234.0047245025635, 'accumulated_submission_time': 3458.7932896614075, 'accumulated_eval_time': 2775.0122537612915, 'accumulated_logging_time': 0.08489060401916504, 'global_step': 8592, 'preemption_count': 0}), (10745, {'train/accuracy': 0.6251749396324158, 'train/loss': 1.9724678993225098, 'train/bleu': 29.758918984350117, 'validation/accuracy': 0.6367310881614685, 'validation/loss': 1.8652809858322144, 'validation/bleu': 26.78234630657909, 'validation/num_examples': 3000, 'test/accuracy': 0.6461797952651978, 'test/loss': 1.8112452030181885, 'test/bleu': 25.999656267044557, 'test/num_examples': 3003, 'score': 4298.949460983276, 'total_duration': 7492.784815311432, 'accumulated_submission_time': 4298.949460983276, 'accumulated_eval_time': 3193.587000846863, 'accumulated_logging_time': 0.10575485229492188, 'global_step': 10745, 'preemption_count': 0}), (12893, {'train/accuracy': 0.6465591788291931, 'train/loss': 1.8093706369400024, 'train/bleu': 31.63608692351997, 'validation/accuracy': 0.6492046117782593, 'validation/loss': 1.7860885858535767, 'validation/bleu': 27.615217720749044, 'validation/num_examples': 3000, 'test/accuracy': 0.657184362411499, 'test/loss': 1.7310973405838013, 'test/bleu': 26.660907824926678, 'test/num_examples': 3003, 'score': 5139.119298458099, 'total_duration': 8793.892016649246, 'accumulated_submission_time': 5139.119298458099, 'accumulated_eval_time': 3654.475510597229, 'accumulated_logging_time': 0.12565350532531738, 'global_step': 12893, 'preemption_count': 0}), (15048, {'train/accuracy': 0.6406798958778381, 'train/loss': 1.8438520431518555, 'train/bleu': 31.38206851348655, 'validation/accuracy': 0.6539038419723511, 'validation/loss': 1.7457467317581177, 'validation/bleu': 28.006477454938523, 'validation/num_examples': 3000, 'test/accuracy': 0.6631572842597961, 'test/loss': 1.6796035766601562, 'test/bleu': 27.1320811390829, 'test/num_examples': 3003, 'score': 5979.399863958359, 'total_duration': 10167.490891218185, 'accumulated_submission_time': 5979.399863958359, 'accumulated_eval_time': 4187.745041847229, 'accumulated_logging_time': 0.1455543041229248, 'global_step': 15048, 'preemption_count': 0}), (17197, {'train/accuracy': 0.6428076028823853, 'train/loss': 1.821254849433899, 'train/bleu': 31.20799557614278, 'validation/accuracy': 0.659061849117279, 'validation/loss': 1.7108757495880127, 'validation/bleu': 28.260321199283887, 'validation/num_examples': 3000, 'test/accuracy': 0.6690605282783508, 'test/loss': 1.6426277160644531, 'test/bleu': 27.651132696237433, 'test/num_examples': 3003, 'score': 6819.642137050629, 'total_duration': 11439.627197742462, 'accumulated_submission_time': 6819.642137050629, 'accumulated_eval_time': 4619.587123394012, 'accumulated_logging_time': 0.16791796684265137, 'global_step': 17197, 'preemption_count': 0}), (19349, {'train/accuracy': 0.6570152044296265, 'train/loss': 1.7196663618087769, 'train/bleu': 32.61968947597813, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.6820471286773682, 'validation/bleu': 28.638508812785417, 'validation/num_examples': 3000, 'test/accuracy': 0.6708616614341736, 'test/loss': 1.6183403730392456, 'test/bleu': 27.67401174653705, 'test/num_examples': 3003, 'score': 7659.641139268875, 'total_duration': 12708.162348270416, 'accumulated_submission_time': 7659.641139268875, 'accumulated_eval_time': 5048.074641704559, 'accumulated_logging_time': 0.18776512145996094, 'global_step': 19349, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6529373526573181, 'train/loss': 1.7420909404754639, 'train/bleu': 31.874985357935557, 'validation/accuracy': 0.6630544066429138, 'validation/loss': 1.6798782348632812, 'validation/bleu': 28.489674989638065, 'validation/num_examples': 3000, 'test/accuracy': 0.6731276512145996, 'test/loss': 1.6144088506698608, 'test/bleu': 28.03205100921681, 'test/num_examples': 3003, 'score': 7913.014540910721, 'total_duration': 13387.861794233322, 'accumulated_submission_time': 7913.014540910721, 'accumulated_eval_time': 5474.371400117874, 'accumulated_logging_time': 0.20819854736328125, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0510 13:47:37.532449 140033517942592 submission_runner.py:587] Timing: 7913.014540910721
I0510 13:47:37.532495 140033517942592 submission_runner.py:588] ====================
I0510 13:47:37.532615 140033517942592 submission_runner.py:651] Final wmt score: 7913.014540910721
