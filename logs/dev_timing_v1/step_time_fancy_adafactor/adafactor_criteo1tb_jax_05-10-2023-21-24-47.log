python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_adafactor/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_jax_05-10-2023-21-24-47.log
I0510 21:25:08.027534 139898121172800 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax.
I0510 21:25:08.173502 139898121172800 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0510 21:25:09.265913 139898121172800 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0510 21:25:09.266613 139898121172800 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0510 21:25:09.271732 139898121172800 submission_runner.py:544] Using RNG seed 3848934787
I0510 21:25:12.268498 139898121172800 submission_runner.py:553] --- Tuning run 1/1 ---
I0510 21:25:12.268784 139898121172800 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1.
I0510 21:25:12.269023 139898121172800 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1/hparams.json.
I0510 21:25:12.405537 139898121172800 submission_runner.py:241] Initializing dataset.
I0510 21:25:12.405763 139898121172800 submission_runner.py:248] Initializing model.
I0510 21:25:19.914823 139898121172800 submission_runner.py:258] Initializing optimizer.
I0510 21:25:21.448161 139898121172800 submission_runner.py:265] Initializing metrics bundle.
I0510 21:25:21.448352 139898121172800 submission_runner.py:283] Initializing checkpoint and logger.
I0510 21:25:21.452850 139898121172800 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1 with prefix checkpoint_
I0510 21:25:21.453130 139898121172800 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0510 21:25:21.453201 139898121172800 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0510 21:25:22.732216 139898121172800 submission_runner.py:304] Saving meta data to /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1/meta_data_0.json.
I0510 21:25:22.733357 139898121172800 submission_runner.py:307] Saving flags to /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1/flags_0.json.
I0510 21:25:22.777862 139898121172800 submission_runner.py:319] Starting training loop.
I0510 21:25:52.749519 139721896359680 logging_writer.py:48] [0] global_step=0, grad_norm=11.124606132507324, loss=1.0119742155075073
I0510 21:25:52.757999 139898121172800 spec.py:298] Evaluating on the training split.
I0510 21:30:43.820973 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 21:35:37.671771 139898121172800 spec.py:326] Evaluating on the test split.
I0510 21:40:28.808527 139898121172800 submission_runner.py:421] Time since start: 906.03s, 	Step: 1, 	{'train/loss': 1.0132387454585172, 'validation/loss': 1.0166629213483147, 'validation/num_examples': 89000000, 'test/loss': 1.0135875881522767, 'test/num_examples': 89274637, 'score': 29.97995400428772, 'total_duration': 906.0305557250977, 'accumulated_submission_time': 29.97995400428772, 'accumulated_eval_time': 876.0504288673401, 'accumulated_logging_time': 0}
I0510 21:40:28.825400 139708491224832 logging_writer.py:48] [1] accumulated_eval_time=876.050429, accumulated_logging_time=0, accumulated_submission_time=29.979954, global_step=1, preemption_count=0, score=29.979954, test/loss=1.013588, test/num_examples=89274637, total_duration=906.030556, train/loss=1.013239, validation/loss=1.016663, validation/num_examples=89000000
I0510 21:41:33.439687 139708482832128 logging_writer.py:48] [100] global_step=100, grad_norm=2.8767471313476562, loss=0.18878711760044098
I0510 21:42:28.886759 139898121172800 spec.py:298] Evaluating on the training split.
I0510 21:47:17.096418 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 21:52:01.304557 139898121172800 spec.py:326] Evaluating on the test split.
I0510 21:56:40.228686 139898121172800 submission_runner.py:421] Time since start: 1877.45s, 	Step: 167, 	{'train/loss': 0.14510677245015866, 'validation/loss': 0.14505714606741574, 'validation/num_examples': 89000000, 'test/loss': 0.14824788366263533, 'test/num_examples': 89274637, 'score': 150.03070855140686, 'total_duration': 1877.4507083892822, 'accumulated_submission_time': 150.03070855140686, 'accumulated_eval_time': 1727.3922884464264, 'accumulated_logging_time': 0.025051593780517578}
I0510 21:56:40.237771 139708491224832 logging_writer.py:48] [167] accumulated_eval_time=1727.392288, accumulated_logging_time=0.025052, accumulated_submission_time=150.030709, global_step=167, preemption_count=0, score=150.030709, test/loss=0.148248, test/num_examples=89274637, total_duration=1877.450708, train/loss=0.145107, validation/loss=0.145057, validation/num_examples=89000000
I0510 21:56:49.470987 139708482832128 logging_writer.py:48] [200] global_step=200, grad_norm=0.05841703340411186, loss=0.1302490532398224
I0510 21:58:20.886846 139708491224832 logging_writer.py:48] [300] global_step=300, grad_norm=0.32651832699775696, loss=0.1349189579486847
I0510 21:58:40.286981 139898121172800 spec.py:298] Evaluating on the training split.
I0510 22:03:26.095428 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 22:08:30.983404 139898121172800 spec.py:326] Evaluating on the test split.
I0510 22:13:10.870806 139898121172800 submission_runner.py:421] Time since start: 2868.09s, 	Step: 323, 	{'train/loss': 0.13012069915479557, 'validation/loss': 0.13058169662921348, 'validation/num_examples': 89000000, 'test/loss': 0.13307879370038772, 'test/num_examples': 89274637, 'score': 270.0705449581146, 'total_duration': 2868.0928406715393, 'accumulated_submission_time': 270.0705449581146, 'accumulated_eval_time': 2597.976038455963, 'accumulated_logging_time': 0.04132270812988281}
I0510 22:13:10.879566 139708482832128 logging_writer.py:48] [323] accumulated_eval_time=2597.976038, accumulated_logging_time=0.041323, accumulated_submission_time=270.070545, global_step=323, preemption_count=0, score=270.070545, test/loss=0.133079, test/num_examples=89274637, total_duration=2868.092841, train/loss=0.130121, validation/loss=0.130582, validation/num_examples=89000000
I0510 22:13:58.115530 139708491224832 logging_writer.py:48] [400] global_step=400, grad_norm=0.09651117026805878, loss=0.13322079181671143
I0510 22:15:10.972813 139898121172800 spec.py:298] Evaluating on the training split.
I0510 22:19:57.833646 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 22:25:01.813177 139898121172800 spec.py:326] Evaluating on the test split.
I0510 22:29:41.154520 139898121172800 submission_runner.py:421] Time since start: 3858.38s, 	Step: 486, 	{'train/loss': 0.1266243929719118, 'validation/loss': 0.12837011235955056, 'validation/num_examples': 89000000, 'test/loss': 0.13129757111193854, 'test/num_examples': 89274637, 'score': 390.154239654541, 'total_duration': 3858.3765511512756, 'accumulated_submission_time': 390.154239654541, 'accumulated_eval_time': 3468.157678604126, 'accumulated_logging_time': 0.05726170539855957}
I0510 22:29:41.164209 139708482832128 logging_writer.py:48] [486] accumulated_eval_time=3468.157679, accumulated_logging_time=0.057262, accumulated_submission_time=390.154240, global_step=486, preemption_count=0, score=390.154240, test/loss=0.131298, test/num_examples=89274637, total_duration=3858.376551, train/loss=0.126624, validation/loss=0.128370, validation/num_examples=89000000
I0510 22:29:42.926741 139708491224832 logging_writer.py:48] [500] global_step=500, grad_norm=0.06657543033361435, loss=0.11957137286663055
I0510 22:30:59.420916 139708482832128 logging_writer.py:48] [600] global_step=600, grad_norm=0.16167527437210083, loss=0.12154378741979599
I0510 22:31:41.598895 139898121172800 spec.py:298] Evaluating on the training split.
I0510 22:36:35.202848 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 22:41:23.235491 139898121172800 spec.py:326] Evaluating on the test split.
I0510 22:46:01.959404 139898121172800 submission_runner.py:421] Time since start: 4839.18s, 	Step: 651, 	{'train/loss': 0.12476258491925933, 'validation/loss': 0.1274617752808989, 'validation/num_examples': 89000000, 'test/loss': 0.13014156529138282, 'test/num_examples': 89274637, 'score': 510.57872700691223, 'total_duration': 4839.181427240372, 'accumulated_submission_time': 510.57872700691223, 'accumulated_eval_time': 4328.518111467361, 'accumulated_logging_time': 0.07478880882263184}
I0510 22:46:01.968195 139708491224832 logging_writer.py:48] [651] accumulated_eval_time=4328.518111, accumulated_logging_time=0.074789, accumulated_submission_time=510.578727, global_step=651, preemption_count=0, score=510.578727, test/loss=0.130142, test/num_examples=89274637, total_duration=4839.181427, train/loss=0.124763, validation/loss=0.127462, validation/num_examples=89000000
I0510 22:46:24.383512 139708482832128 logging_writer.py:48] [700] global_step=700, grad_norm=0.4102950096130371, loss=0.1284424513578415
I0510 22:47:52.372523 139708491224832 logging_writer.py:48] [800] global_step=800, grad_norm=0.08496236056089401, loss=0.12608015537261963
I0510 22:48:02.053350 139898121172800 spec.py:298] Evaluating on the training split.
I0510 22:52:52.839234 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 22:57:31.361192 139898121172800 spec.py:326] Evaluating on the test split.
I0510 23:02:13.361718 139898121172800 submission_runner.py:421] Time since start: 5810.58s, 	Step: 812, 	{'train/loss': 0.12662038459244504, 'validation/loss': 0.12711729213483147, 'validation/num_examples': 89000000, 'test/loss': 0.1298180019483025, 'test/num_examples': 89274637, 'score': 630.6544322967529, 'total_duration': 5810.583756446838, 'accumulated_submission_time': 630.6544322967529, 'accumulated_eval_time': 5179.8264083862305, 'accumulated_logging_time': 0.09060120582580566}
I0510 23:02:13.370454 139708482832128 logging_writer.py:48] [812] accumulated_eval_time=5179.826408, accumulated_logging_time=0.090601, accumulated_submission_time=630.654432, global_step=812, preemption_count=0, score=630.654432, test/loss=0.129818, test/num_examples=89274637, total_duration=5810.583756, train/loss=0.126620, validation/loss=0.127117, validation/num_examples=89000000
I0510 23:03:11.012941 139708491224832 logging_writer.py:48] [900] global_step=900, grad_norm=0.02591923251748085, loss=0.13040229678153992
I0510 23:04:13.933997 139898121172800 spec.py:298] Evaluating on the training split.
I0510 23:09:12.878873 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 23:13:51.839737 139898121172800 spec.py:326] Evaluating on the test split.
I0510 23:18:27.072176 139898121172800 submission_runner.py:421] Time since start: 6784.29s, 	Step: 974, 	{'train/loss': 0.1321894945336932, 'validation/loss': 0.13170085393258427, 'validation/num_examples': 89000000, 'test/loss': 0.13416693030070792, 'test/num_examples': 89274637, 'score': 751.2088348865509, 'total_duration': 6784.29415678978, 'accumulated_submission_time': 751.2088348865509, 'accumulated_eval_time': 6032.964469194412, 'accumulated_logging_time': 0.10616517066955566}
I0510 23:18:27.081839 139708482832128 logging_writer.py:48] [974] accumulated_eval_time=6032.964469, accumulated_logging_time=0.106165, accumulated_submission_time=751.208835, global_step=974, preemption_count=0, score=751.208835, test/loss=0.134167, test/num_examples=89274637, total_duration=6784.294157, train/loss=0.132189, validation/loss=0.131701, validation/num_examples=89000000
I0510 23:18:30.212853 139708491224832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.32423749566078186, loss=0.13320894539356232
I0510 23:20:01.245658 139708482832128 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.11086205393075943, loss=0.12505298852920532
I0510 23:20:28.288424 139898121172800 spec.py:298] Evaluating on the training split.
I0510 23:25:24.581007 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 23:30:06.741708 139898121172800 spec.py:326] Evaluating on the test split.
I0510 23:34:47.314049 139898121172800 submission_runner.py:421] Time since start: 7764.54s, 	Step: 1130, 	{'train/loss': 0.12677678998732408, 'validation/loss': 0.12663033707865168, 'validation/num_examples': 89000000, 'test/loss': 0.1290749353593003, 'test/num_examples': 89274637, 'score': 872.4059557914734, 'total_duration': 7764.536074399948, 'accumulated_submission_time': 872.4059557914734, 'accumulated_eval_time': 6891.990070819855, 'accumulated_logging_time': 0.12312531471252441}
I0510 23:34:47.323379 139708491224832 logging_writer.py:48] [1130] accumulated_eval_time=6891.990071, accumulated_logging_time=0.123125, accumulated_submission_time=872.405956, global_step=1130, preemption_count=0, score=872.405956, test/loss=0.129075, test/num_examples=89274637, total_duration=7764.536074, train/loss=0.126777, validation/loss=0.126630, validation/num_examples=89000000
I0510 23:35:30.435109 139708482832128 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.04041609913110733, loss=0.13125862181186676
I0510 23:36:47.629691 139898121172800 spec.py:298] Evaluating on the training split.
I0510 23:41:35.971773 139898121172800 spec.py:310] Evaluating on the validation split.
I0510 23:46:16.058484 139898121172800 spec.py:326] Evaluating on the test split.
I0510 23:50:54.344472 139898121172800 submission_runner.py:421] Time since start: 8731.57s, 	Step: 1286, 	{'train/loss': 0.12629157396868101, 'validation/loss': 0.12668542696629215, 'validation/num_examples': 89000000, 'test/loss': 0.1290829555543306, 'test/num_examples': 89274637, 'score': 992.7030181884766, 'total_duration': 8731.566506624222, 'accumulated_submission_time': 992.7030181884766, 'accumulated_eval_time': 7738.704791307449, 'accumulated_logging_time': 0.13941526412963867}
I0510 23:50:54.354634 139708491224832 logging_writer.py:48] [1286] accumulated_eval_time=7738.704791, accumulated_logging_time=0.139415, accumulated_submission_time=992.703018, global_step=1286, preemption_count=0, score=992.703018, test/loss=0.129083, test/num_examples=89274637, total_duration=8731.566507, train/loss=0.126292, validation/loss=0.126685, validation/num_examples=89000000
I0510 23:50:56.093755 139708482832128 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.2883268892765045, loss=0.14280201494693756
I0510 23:52:17.864556 139708491224832 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.03336428478360176, loss=0.11585460603237152
I0510 23:52:54.412126 139898121172800 spec.py:298] Evaluating on the training split.
I0510 23:57:54.660718 139898121172800 spec.py:310] Evaluating on the validation split.
I0511 00:02:34.910059 139898121172800 spec.py:326] Evaluating on the test split.
I0511 00:07:11.747367 139898121172800 submission_runner.py:421] Time since start: 9708.97s, 	Step: 1441, 	{'train/loss': 0.12336416574327976, 'validation/loss': 0.12638229213483146, 'validation/num_examples': 89000000, 'test/loss': 0.12889948799231746, 'test/num_examples': 89274637, 'score': 1112.7509336471558, 'total_duration': 9708.969395637512, 'accumulated_submission_time': 1112.7509336471558, 'accumulated_eval_time': 8596.039957523346, 'accumulated_logging_time': 0.1569962501525879}
I0511 00:07:11.757535 139708482832128 logging_writer.py:48] [1441] accumulated_eval_time=8596.039958, accumulated_logging_time=0.156996, accumulated_submission_time=1112.750934, global_step=1441, preemption_count=0, score=1112.750934, test/loss=0.128899, test/num_examples=89274637, total_duration=9708.969396, train/loss=0.123364, validation/loss=0.126382, validation/num_examples=89000000
I0511 00:07:44.640794 139708491224832 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.27777475118637085, loss=0.1413971185684204
I0511 00:09:12.045922 139898121172800 spec.py:298] Evaluating on the training split.
I0511 00:14:04.683025 139898121172800 spec.py:310] Evaluating on the validation split.
I0511 00:19:09.315786 139898121172800 spec.py:326] Evaluating on the test split.
I0511 00:23:43.975238 139898121172800 submission_runner.py:421] Time since start: 10701.20s, 	Step: 1597, 	{'train/loss': 0.12571309856670232, 'validation/loss': 0.12626947191011237, 'validation/num_examples': 89000000, 'test/loss': 0.12887902305332252, 'test/num_examples': 89274637, 'score': 1233.0298781394958, 'total_duration': 10701.19727563858, 'accumulated_submission_time': 1233.0298781394958, 'accumulated_eval_time': 9467.969217061996, 'accumulated_logging_time': 0.1744556427001953}
I0511 00:23:43.984166 139708482832128 logging_writer.py:48] [1597] accumulated_eval_time=9467.969217, accumulated_logging_time=0.174456, accumulated_submission_time=1233.029878, global_step=1597, preemption_count=0, score=1233.029878, test/loss=0.128879, test/num_examples=89274637, total_duration=10701.197276, train/loss=0.125713, validation/loss=0.126269, validation/num_examples=89000000
I0511 00:23:44.018102 139898121172800 spec.py:298] Evaluating on the training split.
I0511 00:28:38.323079 139898121172800 spec.py:310] Evaluating on the validation split.
I0511 00:33:44.327730 139898121172800 spec.py:326] Evaluating on the test split.
I0511 00:38:24.021896 139898121172800 submission_runner.py:421] Time since start: 11581.24s, 	Step: 1600, 	{'train/loss': 0.12322724084805003, 'validation/loss': 0.12638104494382021, 'validation/num_examples': 89000000, 'test/loss': 0.1289281299457986, 'test/num_examples': 89274637, 'score': 1233.056453704834, 'total_duration': 11581.243921279907, 'accumulated_submission_time': 1233.056453704834, 'accumulated_eval_time': 10347.972908496857, 'accumulated_logging_time': 0.1903829574584961}
I0511 00:38:24.031644 139708491224832 logging_writer.py:48] [1600] accumulated_eval_time=10347.972908, accumulated_logging_time=0.190383, accumulated_submission_time=1233.056454, global_step=1600, preemption_count=0, score=1233.056454, test/loss=0.128928, test/num_examples=89274637, total_duration=11581.243921, train/loss=0.123227, validation/loss=0.126381, validation/num_examples=89000000
I0511 00:38:24.044961 139708482832128 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1233.056454
I0511 00:38:26.883400 139898121172800 checkpoints.py:356] Saving checkpoint at step: 1600
I0511 00:38:42.295273 139898121172800 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1/checkpoint_1600
I0511 00:38:42.397042 139898121172800 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_adafactor/timing_adafactor/criteo1tb_jax/trial_1/checkpoint_1600.
I0511 00:38:42.637278 139898121172800 submission_runner.py:584] Tuning trial 1/1
I0511 00:38:42.637557 139898121172800 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0511 00:38:42.639461 139898121172800 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/loss': 1.0132387454585172, 'validation/loss': 1.0166629213483147, 'validation/num_examples': 89000000, 'test/loss': 1.0135875881522767, 'test/num_examples': 89274637, 'score': 29.97995400428772, 'total_duration': 906.0305557250977, 'accumulated_submission_time': 29.97995400428772, 'accumulated_eval_time': 876.0504288673401, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (167, {'train/loss': 0.14510677245015866, 'validation/loss': 0.14505714606741574, 'validation/num_examples': 89000000, 'test/loss': 0.14824788366263533, 'test/num_examples': 89274637, 'score': 150.03070855140686, 'total_duration': 1877.4507083892822, 'accumulated_submission_time': 150.03070855140686, 'accumulated_eval_time': 1727.3922884464264, 'accumulated_logging_time': 0.025051593780517578, 'global_step': 167, 'preemption_count': 0}), (323, {'train/loss': 0.13012069915479557, 'validation/loss': 0.13058169662921348, 'validation/num_examples': 89000000, 'test/loss': 0.13307879370038772, 'test/num_examples': 89274637, 'score': 270.0705449581146, 'total_duration': 2868.0928406715393, 'accumulated_submission_time': 270.0705449581146, 'accumulated_eval_time': 2597.976038455963, 'accumulated_logging_time': 0.04132270812988281, 'global_step': 323, 'preemption_count': 0}), (486, {'train/loss': 0.1266243929719118, 'validation/loss': 0.12837011235955056, 'validation/num_examples': 89000000, 'test/loss': 0.13129757111193854, 'test/num_examples': 89274637, 'score': 390.154239654541, 'total_duration': 3858.3765511512756, 'accumulated_submission_time': 390.154239654541, 'accumulated_eval_time': 3468.157678604126, 'accumulated_logging_time': 0.05726170539855957, 'global_step': 486, 'preemption_count': 0}), (651, {'train/loss': 0.12476258491925933, 'validation/loss': 0.1274617752808989, 'validation/num_examples': 89000000, 'test/loss': 0.13014156529138282, 'test/num_examples': 89274637, 'score': 510.57872700691223, 'total_duration': 4839.181427240372, 'accumulated_submission_time': 510.57872700691223, 'accumulated_eval_time': 4328.518111467361, 'accumulated_logging_time': 0.07478880882263184, 'global_step': 651, 'preemption_count': 0}), (812, {'train/loss': 0.12662038459244504, 'validation/loss': 0.12711729213483147, 'validation/num_examples': 89000000, 'test/loss': 0.1298180019483025, 'test/num_examples': 89274637, 'score': 630.6544322967529, 'total_duration': 5810.583756446838, 'accumulated_submission_time': 630.6544322967529, 'accumulated_eval_time': 5179.8264083862305, 'accumulated_logging_time': 0.09060120582580566, 'global_step': 812, 'preemption_count': 0}), (974, {'train/loss': 0.1321894945336932, 'validation/loss': 0.13170085393258427, 'validation/num_examples': 89000000, 'test/loss': 0.13416693030070792, 'test/num_examples': 89274637, 'score': 751.2088348865509, 'total_duration': 6784.29415678978, 'accumulated_submission_time': 751.2088348865509, 'accumulated_eval_time': 6032.964469194412, 'accumulated_logging_time': 0.10616517066955566, 'global_step': 974, 'preemption_count': 0}), (1130, {'train/loss': 0.12677678998732408, 'validation/loss': 0.12663033707865168, 'validation/num_examples': 89000000, 'test/loss': 0.1290749353593003, 'test/num_examples': 89274637, 'score': 872.4059557914734, 'total_duration': 7764.536074399948, 'accumulated_submission_time': 872.4059557914734, 'accumulated_eval_time': 6891.990070819855, 'accumulated_logging_time': 0.12312531471252441, 'global_step': 1130, 'preemption_count': 0}), (1286, {'train/loss': 0.12629157396868101, 'validation/loss': 0.12668542696629215, 'validation/num_examples': 89000000, 'test/loss': 0.1290829555543306, 'test/num_examples': 89274637, 'score': 992.7030181884766, 'total_duration': 8731.566506624222, 'accumulated_submission_time': 992.7030181884766, 'accumulated_eval_time': 7738.704791307449, 'accumulated_logging_time': 0.13941526412963867, 'global_step': 1286, 'preemption_count': 0}), (1441, {'train/loss': 0.12336416574327976, 'validation/loss': 0.12638229213483146, 'validation/num_examples': 89000000, 'test/loss': 0.12889948799231746, 'test/num_examples': 89274637, 'score': 1112.7509336471558, 'total_duration': 9708.969395637512, 'accumulated_submission_time': 1112.7509336471558, 'accumulated_eval_time': 8596.039957523346, 'accumulated_logging_time': 0.1569962501525879, 'global_step': 1441, 'preemption_count': 0}), (1597, {'train/loss': 0.12571309856670232, 'validation/loss': 0.12626947191011237, 'validation/num_examples': 89000000, 'test/loss': 0.12887902305332252, 'test/num_examples': 89274637, 'score': 1233.0298781394958, 'total_duration': 10701.19727563858, 'accumulated_submission_time': 1233.0298781394958, 'accumulated_eval_time': 9467.969217061996, 'accumulated_logging_time': 0.1744556427001953, 'global_step': 1597, 'preemption_count': 0}), (1600, {'train/loss': 0.12322724084805003, 'validation/loss': 0.12638104494382021, 'validation/num_examples': 89000000, 'test/loss': 0.1289281299457986, 'test/num_examples': 89274637, 'score': 1233.056453704834, 'total_duration': 11581.243921279907, 'accumulated_submission_time': 1233.056453704834, 'accumulated_eval_time': 10347.972908496857, 'accumulated_logging_time': 0.1903829574584961, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0511 00:38:42.639601 139898121172800 submission_runner.py:587] Timing: 1233.056453704834
I0511 00:38:42.639653 139898121172800 submission_runner.py:588] ====================
I0511 00:38:42.639781 139898121172800 submission_runner.py:651] Final criteo1tb score: 1233.056453704834
