python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_adafactor/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_05-11-2023-00-54-26.log
I0511 00:54:46.665037 140380603897664 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax.
I0511 00:54:46.738801 140380603897664 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0511 00:54:47.613149 140380603897664 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0511 00:54:47.614357 140380603897664 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0511 00:54:47.619155 140380603897664 submission_runner.py:544] Using RNG seed 1802648039
I0511 00:54:50.308858 140380603897664 submission_runner.py:553] --- Tuning run 1/1 ---
I0511 00:54:50.309064 140380603897664 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1.
I0511 00:54:50.309267 140380603897664 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1/hparams.json.
I0511 00:54:50.441959 140380603897664 submission_runner.py:241] Initializing dataset.
I0511 00:54:50.442170 140380603897664 submission_runner.py:248] Initializing model.
I0511 00:54:56.846107 140380603897664 submission_runner.py:258] Initializing optimizer.
I0511 00:54:59.211713 140380603897664 submission_runner.py:265] Initializing metrics bundle.
I0511 00:54:59.211911 140380603897664 submission_runner.py:283] Initializing checkpoint and logger.
I0511 00:54:59.212912 140380603897664 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0511 00:54:59.213193 140380603897664 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0511 00:54:59.213268 140380603897664 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0511 00:54:59.888115 140380603897664 submission_runner.py:304] Saving meta data to /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0511 00:54:59.889001 140380603897664 submission_runner.py:307] Saving flags to /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1/flags_0.json.
I0511 00:54:59.895340 140380603897664 submission_runner.py:319] Starting training loop.
I0511 00:55:00.104042 140380603897664 input_pipeline.py:20] Loading split = train-clean-100
I0511 00:55:00.140723 140380603897664 input_pipeline.py:20] Loading split = train-clean-360
I0511 00:55:00.481285 140380603897664 input_pipeline.py:20] Loading split = train-other-500
2023-05-11 00:56:49.185233: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-11 00:56:49.658144: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0511 00:56:51.438015 140205247952640 logging_writer.py:48] [0] global_step=0, grad_norm=54.79587173461914, loss=31.98455238342285
I0511 00:56:51.479545 140380603897664 spec.py:298] Evaluating on the training split.
I0511 00:56:51.596391 140380603897664 input_pipeline.py:20] Loading split = train-clean-100
I0511 00:56:51.631543 140380603897664 input_pipeline.py:20] Loading split = train-clean-360
I0511 00:56:51.745495 140380603897664 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0511 00:58:18.908291 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 00:58:18.981424 140380603897664 input_pipeline.py:20] Loading split = dev-clean
I0511 00:58:18.987112 140380603897664 input_pipeline.py:20] Loading split = dev-other
I0511 00:59:13.732946 140380603897664 spec.py:326] Evaluating on the test split.
I0511 00:59:13.804051 140380603897664 input_pipeline.py:20] Loading split = test-clean
I0511 00:59:50.527015 140380603897664 submission_runner.py:421] Time since start: 290.63s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.799274, dtype=float32), 'train/wer': 2.246881702517544, 'validation/ctc_loss': DeviceArray(30.60595, dtype=float32), 'validation/wer': 1.790774633619234, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.747873, dtype=float32), 'test/wer': 1.868218471350517, 'test/num_examples': 2472, 'score': 111.58399510383606, 'total_duration': 290.62999987602234, 'accumulated_submission_time': 111.58399510383606, 'accumulated_eval_time': 179.04584860801697, 'accumulated_logging_time': 0}
I0511 00:59:50.551979 140202924304128 logging_writer.py:48] [1] accumulated_eval_time=179.045849, accumulated_logging_time=0, accumulated_submission_time=111.583995, global_step=1, preemption_count=0, score=111.583995, test/ctc_loss=30.747873306274414, test/num_examples=2472, test/wer=1.868218, total_duration=290.630000, train/ctc_loss=31.799274444580078, train/wer=2.246882, validation/ctc_loss=30.60594940185547, validation/num_examples=5348, validation/wer=1.790775
I0511 01:02:25.604089 140207640717056 logging_writer.py:48] [100] global_step=100, grad_norm=12.153395652770996, loss=6.501028537750244
I0511 01:03:53.553942 140207649109760 logging_writer.py:48] [200] global_step=200, grad_norm=1.2775417566299438, loss=5.840574741363525
I0511 01:05:21.028897 140207640717056 logging_writer.py:48] [300] global_step=300, grad_norm=0.6601012945175171, loss=5.811051368713379
I0511 01:06:49.494475 140207649109760 logging_writer.py:48] [400] global_step=400, grad_norm=1.572566032409668, loss=5.812331676483154
I0511 01:08:18.230103 140207640717056 logging_writer.py:48] [500] global_step=500, grad_norm=0.537934422492981, loss=5.790019989013672
I0511 01:09:47.773197 140207649109760 logging_writer.py:48] [600] global_step=600, grad_norm=2.465925455093384, loss=5.792243957519531
I0511 01:11:17.119853 140207640717056 logging_writer.py:48] [700] global_step=700, grad_norm=0.3389011025428772, loss=5.745317459106445
I0511 01:12:46.539006 140207649109760 logging_writer.py:48] [800] global_step=800, grad_norm=3.65455961227417, loss=5.575228214263916
I0511 01:14:16.483760 140207640717056 logging_writer.py:48] [900] global_step=900, grad_norm=1.5696766376495361, loss=5.453121662139893
I0511 01:15:46.424825 140207649109760 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2356562614440918, loss=4.938783168792725
I0511 01:17:20.262231 140208066524928 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7679974436759949, loss=4.072923183441162
I0511 01:18:50.613729 140208058132224 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.020521640777588, loss=3.6443114280700684
I0511 01:20:20.526671 140208066524928 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9728679656982422, loss=3.325277805328369
I0511 01:21:51.124199 140208058132224 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9581441283226013, loss=3.169398307800293
I0511 01:23:21.661441 140208066524928 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.4062248468399048, loss=3.0261054039001465
I0511 01:24:51.898044 140208058132224 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0143356323242188, loss=2.9301795959472656
I0511 01:26:22.177151 140208066524928 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7158352732658386, loss=2.9066812992095947
I0511 01:27:53.064560 140208058132224 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9199087619781494, loss=2.752434253692627
I0511 01:29:23.743993 140208066524928 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1647957563400269, loss=2.7004384994506836
I0511 01:30:54.441098 140208058132224 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7788247466087341, loss=2.6327478885650635
I0511 01:32:28.608149 140208066524928 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8646866679191589, loss=2.4704084396362305
I0511 01:33:59.347515 140208058132224 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6639323234558105, loss=2.3933818340301514
I0511 01:35:29.588562 140208066524928 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7188479900360107, loss=2.431715965270996
I0511 01:37:00.603852 140208058132224 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9017158150672913, loss=2.2915642261505127
I0511 01:38:31.429151 140208066524928 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9095176458358765, loss=2.1995224952697754
I0511 01:39:50.723660 140380603897664 spec.py:298] Evaluating on the training split.
I0511 01:40:38.968363 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 01:41:24.029428 140380603897664 spec.py:326] Evaluating on the test split.
I0511 01:41:46.077354 140380603897664 submission_runner.py:421] Time since start: 2806.18s, 	Step: 2588, 	{'train/ctc_loss': DeviceArray(1.6087176, dtype=float32), 'train/wer': 0.42691705629132254, 'validation/ctc_loss': DeviceArray(2.0656269, dtype=float32), 'validation/wer': 0.4783837760132756, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.6876351, dtype=float32), 'test/wer': 0.4342615725224951, 'test/num_examples': 2472, 'score': 2511.711072921753, 'total_duration': 2806.178057909012, 'accumulated_submission_time': 2511.711072921753, 'accumulated_eval_time': 294.3956501483917, 'accumulated_logging_time': 0.03686094284057617}
I0511 01:41:46.099900 140208066524928 logging_writer.py:48] [2588] accumulated_eval_time=294.395650, accumulated_logging_time=0.036861, accumulated_submission_time=2511.711073, global_step=2588, preemption_count=0, score=2511.711073, test/ctc_loss=1.687635064125061, test/num_examples=2472, test/wer=0.434262, total_duration=2806.178058, train/ctc_loss=1.6087175607681274, train/wer=0.426917, validation/ctc_loss=2.065626859664917, validation/num_examples=5348, validation/wer=0.478384
I0511 01:41:58.116549 140208058132224 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7676904201507568, loss=2.245469570159912
I0511 01:43:28.898395 140208066524928 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.691999077796936, loss=2.177870750427246
I0511 01:45:00.602921 140208058132224 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2125329971313477, loss=2.2311363220214844
I0511 01:46:32.256396 140208066524928 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8063443303108215, loss=2.1513476371765137
I0511 01:48:03.904453 140208058132224 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9416443109512329, loss=2.039651393890381
I0511 01:49:38.673634 140208066524928 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6129473447799683, loss=2.006754159927368
I0511 01:51:09.841853 140208058132224 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6197471618652344, loss=2.0221214294433594
I0511 01:52:40.495335 140208066524928 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6135661005973816, loss=1.981708288192749
I0511 01:54:11.934681 140208058132224 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7687751054763794, loss=1.972074270248413
I0511 01:55:44.433424 140208066524928 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6726661324501038, loss=1.9987157583236694
I0511 01:57:15.900110 140208058132224 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8095675110816956, loss=1.999309778213501
I0511 01:58:46.711698 140208066524928 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.623321533203125, loss=1.8869398832321167
I0511 02:00:17.806625 140208058132224 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.734992504119873, loss=1.9741168022155762
I0511 02:01:49.883195 140208066524928 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0533825159072876, loss=1.940682053565979
I0511 02:03:21.241657 140208058132224 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5161102414131165, loss=1.902626872062683
I0511 02:04:52.260776 140208066524928 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7593917846679688, loss=1.8530524969100952
I0511 02:06:27.207565 140208066524928 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4879303574562073, loss=1.889836311340332
I0511 02:07:58.610090 140208058132224 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.572201669216156, loss=1.8691656589508057
I0511 02:09:29.870056 140208066524928 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5102208852767944, loss=1.8468964099884033
I0511 02:11:01.252571 140208058132224 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5753701329231262, loss=1.8094117641448975
I0511 02:12:32.525988 140208066524928 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5469774603843689, loss=1.8603332042694092
I0511 02:14:03.524965 140208058132224 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5853333473205566, loss=1.8394505977630615
I0511 02:15:34.885146 140208066524928 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5182262659072876, loss=1.7901124954223633
I0511 02:17:06.462123 140208058132224 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6270392537117004, loss=1.8473507165908813
I0511 02:18:37.863416 140208066524928 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.46417707204818726, loss=1.8197472095489502
I0511 02:20:09.142656 140208058132224 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.4598133862018585, loss=1.7991753816604614
I0511 02:21:44.252023 140208066524928 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6436729431152344, loss=1.782755970954895
I0511 02:21:47.020496 140380603897664 spec.py:298] Evaluating on the training split.
I0511 02:22:33.804904 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 02:23:19.108663 140380603897664 spec.py:326] Evaluating on the test split.
I0511 02:23:42.198718 140380603897664 submission_runner.py:421] Time since start: 5322.30s, 	Step: 5204, 	{'train/ctc_loss': DeviceArray(0.54742074, dtype=float32), 'train/wer': 0.19032043257065231, 'validation/ctc_loss': DeviceArray(0.87459457, dtype=float32), 'validation/wer': 0.2580439753398489, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5970747, dtype=float32), 'test/wer': 0.19647390977596327, 'test/num_examples': 2472, 'score': 4912.582359075546, 'total_duration': 5322.299490213394, 'accumulated_submission_time': 4912.582359075546, 'accumulated_eval_time': 409.57003593444824, 'accumulated_logging_time': 0.07523441314697266}
I0511 02:23:42.223244 140208066524928 logging_writer.py:48] [5204] accumulated_eval_time=409.570036, accumulated_logging_time=0.075234, accumulated_submission_time=4912.582359, global_step=5204, preemption_count=0, score=4912.582359, test/ctc_loss=0.5970746874809265, test/num_examples=2472, test/wer=0.196474, total_duration=5322.299490, train/ctc_loss=0.5474207401275635, train/wer=0.190320, validation/ctc_loss=0.8745945692062378, validation/num_examples=5348, validation/wer=0.258044
I0511 02:25:10.498271 140208058132224 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4845133423805237, loss=1.7460306882858276
I0511 02:26:41.362787 140208066524928 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4795275926589966, loss=1.7476210594177246
I0511 02:28:12.794428 140208058132224 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5585552453994751, loss=1.7938482761383057
I0511 02:29:44.498422 140208066524928 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9430081248283386, loss=1.7333799600601196
I0511 02:31:16.597918 140208058132224 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4764748215675354, loss=1.7536882162094116
I0511 02:32:48.351056 140208066524928 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.4768291711807251, loss=1.7434489727020264
I0511 02:34:20.459000 140208058132224 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.44904664158821106, loss=1.7161527872085571
I0511 02:35:52.185728 140208066524928 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6376172304153442, loss=1.7700436115264893
I0511 02:37:23.994180 140208058132224 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5081120133399963, loss=1.746151328086853
I0511 02:38:59.110275 140208066524928 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.447462797164917, loss=1.7009303569793701
I0511 02:40:30.702065 140208058132224 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4129970967769623, loss=1.6769448518753052
I0511 02:42:02.012953 140208066524928 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.44830259680747986, loss=1.6761573553085327
I0511 02:43:33.553820 140208058132224 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.48905742168426514, loss=1.687829852104187
I0511 02:45:04.751681 140208066524928 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4592552185058594, loss=1.6688677072525024
I0511 02:46:36.371693 140208058132224 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.0477389097213745, loss=1.7124431133270264
I0511 02:48:07.596166 140208066524928 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6200395226478577, loss=1.6824990510940552
I0511 02:49:39.539173 140208058132224 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.42736849188804626, loss=1.7164212465286255
I0511 02:51:11.184867 140208066524928 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4305906891822815, loss=1.6894917488098145
I0511 02:52:43.045169 140208058132224 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.37072741985321045, loss=1.6259058713912964
I0511 02:54:14.025733 140208066524928 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5473557114601135, loss=1.6919677257537842
I0511 02:55:49.930495 140208066524928 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.48667997121810913, loss=1.6978684663772583
I0511 02:57:21.083184 140208058132224 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4415788948535919, loss=1.6022497415542603
I0511 02:58:52.562155 140208066524928 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.43704313039779663, loss=1.6869508028030396
I0511 03:00:24.969256 140208058132224 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.35017913579940796, loss=1.593682050704956
I0511 03:01:56.149681 140208066524928 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.42305582761764526, loss=1.6154735088348389
I0511 03:03:27.868169 140208058132224 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.47814443707466125, loss=1.6230688095092773
I0511 03:03:42.581323 140380603897664 spec.py:298] Evaluating on the training split.
I0511 03:04:29.749089 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 03:05:15.454511 140380603897664 spec.py:326] Evaluating on the test split.
I0511 03:05:38.326403 140380603897664 submission_runner.py:421] Time since start: 7838.43s, 	Step: 7817, 	{'train/ctc_loss': DeviceArray(0.3863848, dtype=float32), 'train/wer': 0.14273289371728576, 'validation/ctc_loss': DeviceArray(0.72610027, dtype=float32), 'validation/wer': 0.21617188781367885, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46395552, dtype=float32), 'test/wer': 0.15641947474255072, 'test/num_examples': 2472, 'score': 7312.891610383987, 'total_duration': 7838.427540302277, 'accumulated_submission_time': 7312.891610383987, 'accumulated_eval_time': 525.3116726875305, 'accumulated_logging_time': 0.11542987823486328}
I0511 03:05:38.353064 140208066524928 logging_writer.py:48] [7817] accumulated_eval_time=525.311673, accumulated_logging_time=0.115430, accumulated_submission_time=7312.891610, global_step=7817, preemption_count=0, score=7312.891610, test/ctc_loss=0.46395552158355713, test/num_examples=2472, test/wer=0.156419, total_duration=7838.427540, train/ctc_loss=0.3863847851753235, train/wer=0.142733, validation/ctc_loss=0.7261002659797668, validation/num_examples=5348, validation/wer=0.216172
I0511 03:06:55.513069 140208058132224 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.48625507950782776, loss=1.5294785499572754
I0511 03:08:26.552600 140208066524928 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5397661328315735, loss=1.6210931539535522
I0511 03:09:58.560457 140208058132224 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.4580176770687103, loss=1.6051524877548218
I0511 03:11:30.661282 140208066524928 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4170091450214386, loss=1.5322455167770386
I0511 03:13:06.156818 140208066524928 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.44161269068717957, loss=1.593843936920166
I0511 03:14:37.397518 140208058132224 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4319184124469757, loss=1.5586577653884888
I0511 03:16:08.754624 140208066524928 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4620409905910492, loss=1.548906683921814
I0511 03:17:40.253842 140208058132224 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.37612172961235046, loss=1.574487328529358
I0511 03:19:12.392466 140208066524928 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4391493797302246, loss=1.5848076343536377
I0511 03:20:44.127493 140208058132224 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.35413050651550293, loss=1.5097522735595703
I0511 03:22:15.627181 140208066524928 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.4490097165107727, loss=1.5184708833694458
I0511 03:23:46.918627 140208058132224 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.39939773082733154, loss=1.4656744003295898
I0511 03:25:18.759044 140208066524928 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3773201107978821, loss=1.5279903411865234
I0511 03:26:50.720055 140208058132224 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4248836040496826, loss=1.5838276147842407
I0511 03:28:25.833514 140208066524928 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.30788087844848633, loss=1.5710155963897705
I0511 03:29:57.778684 140208058132224 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4392639100551605, loss=1.4909952878952026
I0511 03:31:28.963381 140208066524928 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3413023352622986, loss=1.4910471439361572
I0511 03:33:00.488749 140208058132224 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.41610491275787354, loss=1.5389533042907715
I0511 03:34:33.147881 140208066524928 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.32488781213760376, loss=1.4230352640151978
I0511 03:36:05.833265 140208058132224 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3310187757015228, loss=1.4508079290390015
I0511 03:37:36.924937 140208066524928 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.44348421692848206, loss=1.4542574882507324
I0511 03:39:08.735982 140208058132224 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3595553934574127, loss=1.4952619075775146
I0511 03:40:40.315564 140208066524928 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.41377830505371094, loss=1.5103720426559448
I0511 03:42:12.538469 140208058132224 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3851911723613739, loss=1.476278305053711
I0511 03:43:47.799731 140208066524928 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.47768867015838623, loss=1.4966871738433838
I0511 03:45:20.520002 140208058132224 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.34970876574516296, loss=1.473965048789978
I0511 03:45:38.856880 140380603897664 spec.py:298] Evaluating on the training split.
I0511 03:46:26.727009 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 03:47:12.524425 140380603897664 spec.py:326] Evaluating on the test split.
I0511 03:47:35.577897 140380603897664 submission_runner.py:421] Time since start: 10355.68s, 	Step: 10421, 	{'train/ctc_loss': DeviceArray(0.30250555, dtype=float32), 'train/wer': 0.11078239378357717, 'validation/ctc_loss': DeviceArray(0.60177046, dtype=float32), 'validation/wer': 0.17999208868392363, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.36971843, dtype=float32), 'test/wer': 0.1254443158044401, 'test/num_examples': 2472, 'score': 9713.344266176224, 'total_duration': 10355.67877316475, 'accumulated_submission_time': 9713.344266176224, 'accumulated_eval_time': 642.0289702415466, 'accumulated_logging_time': 0.1593930721282959}
I0511 03:47:35.604279 140208066524928 logging_writer.py:48] [10421] accumulated_eval_time=642.028970, accumulated_logging_time=0.159393, accumulated_submission_time=9713.344266, global_step=10421, preemption_count=0, score=9713.344266, test/ctc_loss=0.36971843242645264, test/num_examples=2472, test/wer=0.125444, total_duration=10355.678773, train/ctc_loss=0.3025055527687073, train/wer=0.110782, validation/ctc_loss=0.6017704606056213, validation/num_examples=5348, validation/wer=0.179992
I0511 03:48:48.955466 140208058132224 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3763171434402466, loss=1.4894832372665405
I0511 03:50:21.263908 140208066524928 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.33269479870796204, loss=1.4382380247116089
I0511 03:51:53.561328 140208058132224 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.434494286775589, loss=1.4514695405960083
I0511 03:53:25.213826 140208066524928 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3242954909801483, loss=1.432698130607605
I0511 03:54:56.492511 140208058132224 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4177402853965759, loss=1.47859525680542
I0511 03:56:28.139110 140208066524928 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5242136120796204, loss=1.4093130826950073
I0511 03:57:59.682309 140208058132224 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.559525728225708, loss=1.3743125200271606
I0511 03:59:31.549652 140208066524928 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4020203649997711, loss=1.4164530038833618
I0511 04:01:03.227188 140208058132224 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3590812087059021, loss=1.4102742671966553
I0511 04:02:39.545011 140208066524928 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.3487267792224884, loss=1.4227834939956665
I0511 04:04:11.271414 140208058132224 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3460165858268738, loss=1.438862919807434
I0511 04:05:43.778620 140208066524928 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3487481474876404, loss=1.4663844108581543
I0511 04:07:15.498507 140208058132224 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3780359923839569, loss=1.3851121664047241
I0511 04:08:48.058569 140208066524928 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4336157441139221, loss=1.4328150749206543
I0511 04:10:19.867947 140208058132224 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.33350157737731934, loss=1.4548429250717163
I0511 04:11:51.644353 140208066524928 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3993426561355591, loss=1.368097186088562
I0511 04:13:22.910665 140208058132224 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.37833356857299805, loss=1.3768199682235718
I0511 04:14:54.643247 140208066524928 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.29431939125061035, loss=1.4228674173355103
I0511 04:16:26.161141 140208058132224 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.44764581322669983, loss=1.464159607887268
I0511 04:18:02.005100 140208066524928 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.37557730078697205, loss=1.3713152408599854
I0511 04:19:33.811797 140208058132224 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3300171494483948, loss=1.401259422302246
I0511 04:21:05.686067 140208066524928 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3698841631412506, loss=1.3343318700790405
I0511 04:22:37.358918 140208058132224 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4161631464958191, loss=1.377050757408142
I0511 04:24:09.313871 140208066524928 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.39858824014663696, loss=1.41834557056427
I0511 04:25:40.535265 140208058132224 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.38911646604537964, loss=1.4065865278244019
I0511 04:27:11.965539 140208066524928 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3382934033870697, loss=1.3809372186660767
I0511 04:27:35.683333 140380603897664 spec.py:298] Evaluating on the training split.
I0511 04:28:22.570790 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 04:29:06.678465 140380603897664 spec.py:326] Evaluating on the test split.
I0511 04:29:30.048818 140380603897664 submission_runner.py:421] Time since start: 12870.15s, 	Step: 13027, 	{'train/ctc_loss': DeviceArray(0.25694755, dtype=float32), 'train/wer': 0.09711017852095066, 'validation/ctc_loss': DeviceArray(0.54597884, dtype=float32), 'validation/wer': 0.1658867909965364, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.330461, dtype=float32), 'test/wer': 0.11315581012735361, 'test/num_examples': 2472, 'score': 12113.373478412628, 'total_duration': 12870.149992227554, 'accumulated_submission_time': 12113.373478412628, 'accumulated_eval_time': 756.3910300731659, 'accumulated_logging_time': 0.20143985748291016}
I0511 04:29:30.075284 140208066524928 logging_writer.py:48] [13027] accumulated_eval_time=756.391030, accumulated_logging_time=0.201440, accumulated_submission_time=12113.373478, global_step=13027, preemption_count=0, score=12113.373478, test/ctc_loss=0.3304609954357147, test/num_examples=2472, test/wer=0.113156, total_duration=12870.149992, train/ctc_loss=0.2569475471973419, train/wer=0.097110, validation/ctc_loss=0.545978844165802, validation/num_examples=5348, validation/wer=0.165887
I0511 04:30:38.069727 140208058132224 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.442270427942276, loss=1.362359881401062
I0511 04:32:09.442577 140208066524928 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3327966034412384, loss=1.3543035984039307
I0511 04:33:41.387687 140208058132224 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3408334255218506, loss=1.4109060764312744
I0511 04:35:16.664196 140208066524928 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5051733255386353, loss=1.3805994987487793
I0511 04:36:48.415418 140208058132224 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.31158098578453064, loss=1.3112436532974243
I0511 04:38:20.284855 140208066524928 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.37414246797561646, loss=1.3852964639663696
I0511 04:39:51.666708 140208058132224 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.36072394251823425, loss=1.3175323009490967
I0511 04:41:23.616106 140208066524928 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.30662059783935547, loss=1.3446358442306519
I0511 04:42:54.958371 140208058132224 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3026832640171051, loss=1.3691447973251343
I0511 04:44:26.464121 140208066524928 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.49383851885795593, loss=1.3416517972946167
I0511 04:45:57.917689 140208058132224 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.32557904720306396, loss=1.3425570726394653
I0511 04:47:29.413093 140208066524928 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.33393141627311707, loss=1.3978345394134521
I0511 04:49:01.495615 140208058132224 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.4191322922706604, loss=1.345868706703186
I0511 04:50:32.823917 140208066524928 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3693995773792267, loss=1.3757394552230835
I0511 04:52:08.624012 140208066524928 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.28231218457221985, loss=1.322156310081482
I0511 04:53:40.626057 140208058132224 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.32749509811401367, loss=1.3631935119628906
I0511 04:55:12.347661 140208066524928 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4038040339946747, loss=1.3295800685882568
I0511 04:56:44.474880 140208058132224 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.34030744433403015, loss=1.319643497467041
I0511 04:58:17.182991 140208066524928 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.37792399525642395, loss=1.306194543838501
I0511 04:59:48.486563 140208058132224 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3425409495830536, loss=1.3756682872772217
I0511 05:01:20.958892 140208066524928 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.4310043454170227, loss=1.2733815908432007
I0511 05:02:52.582316 140208058132224 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.33069711923599243, loss=1.2828350067138672
I0511 05:04:24.336745 140208066524928 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3590574562549591, loss=1.3405756950378418
I0511 05:05:56.624929 140208058132224 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2802579998970032, loss=1.308938980102539
I0511 05:07:32.322252 140208066524928 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.338771790266037, loss=1.3795998096466064
I0511 05:09:04.253579 140208058132224 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3559257984161377, loss=1.2593073844909668
I0511 05:09:30.851459 140380603897664 spec.py:298] Evaluating on the training split.
I0511 05:10:18.399013 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 05:11:04.047224 140380603897664 spec.py:326] Evaluating on the test split.
I0511 05:11:27.245762 140380603897664 submission_runner.py:421] Time since start: 15387.35s, 	Step: 15630, 	{'train/ctc_loss': DeviceArray(0.2554871, dtype=float32), 'train/wer': 0.09322436263343487, 'validation/ctc_loss': DeviceArray(0.5145279, dtype=float32), 'validation/wer': 0.1534409400959006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3100391, dtype=float32), 'test/wer': 0.10551865618589158, 'test/num_examples': 2472, 'score': 14514.100474834442, 'total_duration': 15387.347114562988, 'accumulated_submission_time': 14514.100474834442, 'accumulated_eval_time': 872.7820889949799, 'accumulated_logging_time': 0.24349188804626465}
I0511 05:11:27.271167 140208066524928 logging_writer.py:48] [15630] accumulated_eval_time=872.782089, accumulated_logging_time=0.243492, accumulated_submission_time=14514.100475, global_step=15630, preemption_count=0, score=14514.100475, test/ctc_loss=0.31003910303115845, test/num_examples=2472, test/wer=0.105519, total_duration=15387.347115, train/ctc_loss=0.2554871141910553, train/wer=0.093224, validation/ctc_loss=0.5145279169082642, validation/num_examples=5348, validation/wer=0.153441
I0511 05:12:32.961836 140208058132224 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.37672051787376404, loss=1.3131954669952393
I0511 05:14:04.909047 140208066524928 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.40342068672180176, loss=1.3885496854782104
I0511 05:15:36.949377 140208058132224 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3087237775325775, loss=1.3513578176498413
I0511 05:17:08.878974 140208066524928 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3578283190727234, loss=1.3549047708511353
I0511 05:18:40.882707 140208058132224 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.30606240034103394, loss=1.3131647109985352
I0511 05:20:13.184939 140208066524928 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2935035228729248, loss=1.3540241718292236
I0511 05:21:44.728565 140208058132224 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3883070647716522, loss=1.2848639488220215
I0511 05:23:15.811566 140208066524928 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2770184874534607, loss=1.3432801961898804
I0511 05:24:51.089823 140208066524928 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3085707128047943, loss=1.3597685098648071
I0511 05:26:22.558799 140208058132224 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3833729326725006, loss=1.3070814609527588
I0511 05:27:54.433014 140208066524928 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3426802158355713, loss=1.2488281726837158
I0511 05:29:26.156386 140208058132224 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3059227168560028, loss=1.3088172674179077
I0511 05:30:57.912876 140208066524928 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3517359793186188, loss=1.3095444440841675
I0511 05:32:29.586330 140208058132224 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.32740020751953125, loss=1.3000514507293701
I0511 05:34:01.035914 140208066524928 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3721379339694977, loss=1.3183101415634155
I0511 05:35:32.033110 140208058132224 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.30008602142333984, loss=1.3414695262908936
I0511 05:37:04.347254 140208066524928 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.43874356150627136, loss=1.3197596073150635
I0511 05:38:35.175148 140208058132224 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3043382167816162, loss=1.3409619331359863
I0511 05:40:06.806727 140208066524928 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3403467833995819, loss=1.313841700553894
I0511 05:41:42.946507 140208066524928 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.337536096572876, loss=1.3033345937728882
I0511 05:43:15.297133 140208058132224 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3154890835285187, loss=1.3210369348526
I0511 05:44:47.594229 140208066524928 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3354952931404114, loss=1.219647765159607
I0511 05:46:20.171991 140208058132224 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3414658010005951, loss=1.2588261365890503
I0511 05:47:51.793639 140208066524928 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.29960787296295166, loss=1.2413488626480103
I0511 05:49:23.610342 140208058132224 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3375225067138672, loss=1.320760726928711
I0511 05:50:54.964484 140208066524928 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.2628367245197296, loss=1.212039589881897
I0511 05:51:28.193587 140380603897664 spec.py:298] Evaluating on the training split.
I0511 05:52:15.905117 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 05:53:01.029769 140380603897664 spec.py:326] Evaluating on the test split.
I0511 05:53:24.147013 140380603897664 submission_runner.py:421] Time since start: 17904.25s, 	Step: 18237, 	{'train/ctc_loss': DeviceArray(0.21298485, dtype=float32), 'train/wer': 0.08239742198966243, 'validation/ctc_loss': DeviceArray(0.5002963, dtype=float32), 'validation/wer': 0.14888710937876873, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.29544064, dtype=float32), 'test/wer': 0.09918144334084862, 'test/num_examples': 2472, 'score': 16914.973632097244, 'total_duration': 17904.248265981674, 'accumulated_submission_time': 16914.973632097244, 'accumulated_eval_time': 988.7321701049805, 'accumulated_logging_time': 0.28516364097595215}
I0511 05:53:24.173759 140208066524928 logging_writer.py:48] [18237] accumulated_eval_time=988.732170, accumulated_logging_time=0.285164, accumulated_submission_time=16914.973632, global_step=18237, preemption_count=0, score=16914.973632, test/ctc_loss=0.2954406440258026, test/num_examples=2472, test/wer=0.099181, total_duration=17904.248266, train/ctc_loss=0.2129848450422287, train/wer=0.082397, validation/ctc_loss=0.5002962946891785, validation/num_examples=5348, validation/wer=0.148887
I0511 05:54:22.644454 140208058132224 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4224061369895935, loss=1.2743237018585205
I0511 05:55:54.165328 140208066524928 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4095049202442169, loss=1.26554274559021
I0511 05:57:27.277029 140208058132224 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2908449172973633, loss=1.2570075988769531
I0511 05:59:03.449001 140208066524928 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.35411208868026733, loss=1.2427181005477905
I0511 06:00:35.154262 140208058132224 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.29144716262817383, loss=1.284010648727417
I0511 06:02:06.807116 140208066524928 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.28752484917640686, loss=1.2632629871368408
I0511 06:03:39.000194 140208058132224 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3620334267616272, loss=1.2995854616165161
I0511 06:05:10.646821 140208066524928 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2940656542778015, loss=1.2534703016281128
I0511 06:06:42.311340 140208058132224 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.29055365920066833, loss=1.237044334411621
I0511 06:08:14.487416 140208066524928 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.35045021772384644, loss=1.306221842765808
I0511 06:09:46.375080 140208058132224 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.29957470297813416, loss=1.321155071258545
I0511 06:11:18.479988 140208066524928 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.41482359170913696, loss=1.319014549255371
I0511 06:12:50.459888 140208058132224 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.39397746324539185, loss=1.3208415508270264
I0511 06:14:25.535133 140208066524928 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2874634563922882, loss=1.2321749925613403
I0511 06:15:57.005871 140208058132224 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.37449556589126587, loss=1.1983628273010254
I0511 06:17:28.700416 140208066524928 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3034558892250061, loss=1.2250710725784302
I0511 06:19:00.399383 140208058132224 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3448980152606964, loss=1.247146725654602
I0511 06:20:31.283528 140380603897664 spec.py:298] Evaluating on the training split.
I0511 06:21:19.243011 140380603897664 spec.py:310] Evaluating on the validation split.
I0511 06:22:05.207645 140380603897664 spec.py:326] Evaluating on the test split.
I0511 06:22:28.741668 140380603897664 submission_runner.py:421] Time since start: 19648.84s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.17929484, dtype=float32), 'train/wer': 0.07190514573961881, 'validation/ctc_loss': DeviceArray(0.4759323, dtype=float32), 'validation/wer': 0.1431851730359193, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.27985337, dtype=float32), 'test/wer': 0.09369731684033067, 'test/num_examples': 2472, 'score': 18542.043169260025, 'total_duration': 19648.84440588951, 'accumulated_submission_time': 18542.043169260025, 'accumulated_eval_time': 1106.188464641571, 'accumulated_logging_time': 0.3293585777282715}
I0511 06:22:28.763330 140208066524928 logging_writer.py:48] [20000] accumulated_eval_time=1106.188465, accumulated_logging_time=0.329359, accumulated_submission_time=18542.043169, global_step=20000, preemption_count=0, score=18542.043169, test/ctc_loss=0.27985337376594543, test/num_examples=2472, test/wer=0.093697, total_duration=19648.844406, train/ctc_loss=0.17929483950138092, train/wer=0.071905, validation/ctc_loss=0.4759323000907898, validation/num_examples=5348, validation/wer=0.143185
I0511 06:22:28.787822 140208058132224 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=18542.043169
I0511 06:22:29.268118 140380603897664 checkpoints.py:356] Saving checkpoint at step: 20000
I0511 06:22:29.922772 140380603897664 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1/checkpoint_20000
I0511 06:22:29.936879 140380603897664 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_adafactor/timing_adafactor/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0511 06:22:31.370386 140380603897664 submission_runner.py:584] Tuning trial 1/1
I0511 06:22:31.370677 140380603897664 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0511 06:22:31.383002 140380603897664 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.799274, dtype=float32), 'train/wer': 2.246881702517544, 'validation/ctc_loss': DeviceArray(30.60595, dtype=float32), 'validation/wer': 1.790774633619234, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.747873, dtype=float32), 'test/wer': 1.868218471350517, 'test/num_examples': 2472, 'score': 111.58399510383606, 'total_duration': 290.62999987602234, 'accumulated_submission_time': 111.58399510383606, 'accumulated_eval_time': 179.04584860801697, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2588, {'train/ctc_loss': DeviceArray(1.6087176, dtype=float32), 'train/wer': 0.42691705629132254, 'validation/ctc_loss': DeviceArray(2.0656269, dtype=float32), 'validation/wer': 0.4783837760132756, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.6876351, dtype=float32), 'test/wer': 0.4342615725224951, 'test/num_examples': 2472, 'score': 2511.711072921753, 'total_duration': 2806.178057909012, 'accumulated_submission_time': 2511.711072921753, 'accumulated_eval_time': 294.3956501483917, 'accumulated_logging_time': 0.03686094284057617, 'global_step': 2588, 'preemption_count': 0}), (5204, {'train/ctc_loss': DeviceArray(0.54742074, dtype=float32), 'train/wer': 0.19032043257065231, 'validation/ctc_loss': DeviceArray(0.87459457, dtype=float32), 'validation/wer': 0.2580439753398489, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5970747, dtype=float32), 'test/wer': 0.19647390977596327, 'test/num_examples': 2472, 'score': 4912.582359075546, 'total_duration': 5322.299490213394, 'accumulated_submission_time': 4912.582359075546, 'accumulated_eval_time': 409.57003593444824, 'accumulated_logging_time': 0.07523441314697266, 'global_step': 5204, 'preemption_count': 0}), (7817, {'train/ctc_loss': DeviceArray(0.3863848, dtype=float32), 'train/wer': 0.14273289371728576, 'validation/ctc_loss': DeviceArray(0.72610027, dtype=float32), 'validation/wer': 0.21617188781367885, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46395552, dtype=float32), 'test/wer': 0.15641947474255072, 'test/num_examples': 2472, 'score': 7312.891610383987, 'total_duration': 7838.427540302277, 'accumulated_submission_time': 7312.891610383987, 'accumulated_eval_time': 525.3116726875305, 'accumulated_logging_time': 0.11542987823486328, 'global_step': 7817, 'preemption_count': 0}), (10421, {'train/ctc_loss': DeviceArray(0.30250555, dtype=float32), 'train/wer': 0.11078239378357717, 'validation/ctc_loss': DeviceArray(0.60177046, dtype=float32), 'validation/wer': 0.17999208868392363, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.36971843, dtype=float32), 'test/wer': 0.1254443158044401, 'test/num_examples': 2472, 'score': 9713.344266176224, 'total_duration': 10355.67877316475, 'accumulated_submission_time': 9713.344266176224, 'accumulated_eval_time': 642.0289702415466, 'accumulated_logging_time': 0.1593930721282959, 'global_step': 10421, 'preemption_count': 0}), (13027, {'train/ctc_loss': DeviceArray(0.25694755, dtype=float32), 'train/wer': 0.09711017852095066, 'validation/ctc_loss': DeviceArray(0.54597884, dtype=float32), 'validation/wer': 0.1658867909965364, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.330461, dtype=float32), 'test/wer': 0.11315581012735361, 'test/num_examples': 2472, 'score': 12113.373478412628, 'total_duration': 12870.149992227554, 'accumulated_submission_time': 12113.373478412628, 'accumulated_eval_time': 756.3910300731659, 'accumulated_logging_time': 0.20143985748291016, 'global_step': 13027, 'preemption_count': 0}), (15630, {'train/ctc_loss': DeviceArray(0.2554871, dtype=float32), 'train/wer': 0.09322436263343487, 'validation/ctc_loss': DeviceArray(0.5145279, dtype=float32), 'validation/wer': 0.1534409400959006, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3100391, dtype=float32), 'test/wer': 0.10551865618589158, 'test/num_examples': 2472, 'score': 14514.100474834442, 'total_duration': 15387.347114562988, 'accumulated_submission_time': 14514.100474834442, 'accumulated_eval_time': 872.7820889949799, 'accumulated_logging_time': 0.24349188804626465, 'global_step': 15630, 'preemption_count': 0}), (18237, {'train/ctc_loss': DeviceArray(0.21298485, dtype=float32), 'train/wer': 0.08239742198966243, 'validation/ctc_loss': DeviceArray(0.5002963, dtype=float32), 'validation/wer': 0.14888710937876873, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.29544064, dtype=float32), 'test/wer': 0.09918144334084862, 'test/num_examples': 2472, 'score': 16914.973632097244, 'total_duration': 17904.248265981674, 'accumulated_submission_time': 16914.973632097244, 'accumulated_eval_time': 988.7321701049805, 'accumulated_logging_time': 0.28516364097595215, 'global_step': 18237, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.17929484, dtype=float32), 'train/wer': 0.07190514573961881, 'validation/ctc_loss': DeviceArray(0.4759323, dtype=float32), 'validation/wer': 0.1431851730359193, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.27985337, dtype=float32), 'test/wer': 0.09369731684033067, 'test/num_examples': 2472, 'score': 18542.043169260025, 'total_duration': 19648.84440588951, 'accumulated_submission_time': 18542.043169260025, 'accumulated_eval_time': 1106.188464641571, 'accumulated_logging_time': 0.3293585777282715, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0511 06:22:31.383222 140380603897664 submission_runner.py:587] Timing: 18542.043169260025
I0511 06:22:31.383311 140380603897664 submission_runner.py:588] ====================
I0511 06:22:31.384951 140380603897664 submission_runner.py:651] Final librispeech_conformer score: 18542.043169260025
