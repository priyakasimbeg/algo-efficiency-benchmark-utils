python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_jax_upgrade_a/sam --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-31-2023-00-40-54.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0531 00:41:16.904028 139825866020672 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax.
I0531 00:41:17.745555 139825866020672 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0531 00:41:17.746283 139825866020672 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 00:41:17.746399 139825866020672 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0531 00:41:17.752043 139825866020672 submission_runner.py:549] Using RNG seed 1199764323
I0531 00:41:23.017441 139825866020672 submission_runner.py:558] --- Tuning run 1/1 ---
I0531 00:41:23.017703 139825866020672 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1.
I0531 00:41:23.017931 139825866020672 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1/hparams.json.
I0531 00:41:23.211039 139825866020672 submission_runner.py:243] Initializing dataset.
I0531 00:41:23.211344 139825866020672 submission_runner.py:250] Initializing model.
I0531 00:41:25.757484 139825866020672 submission_runner.py:260] Initializing optimizer.
I0531 00:41:26.428193 139825866020672 submission_runner.py:267] Initializing metrics bundle.
I0531 00:41:26.428427 139825866020672 submission_runner.py:285] Initializing checkpoint and logger.
I0531 00:41:26.429529 139825866020672 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0531 00:41:26.429839 139825866020672 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0531 00:41:26.429925 139825866020672 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0531 00:41:27.221095 139825866020672 submission_runner.py:306] Saving meta data to /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0531 00:41:27.222201 139825866020672 submission_runner.py:309] Saving flags to /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0531 00:41:27.228908 139825866020672 submission_runner.py:321] Starting training loop.
I0531 00:41:27.519382 139825866020672 input_pipeline.py:20] Loading split = train-clean-100
I0531 00:41:27.555616 139825866020672 input_pipeline.py:20] Loading split = train-clean-360
I0531 00:41:27.971842 139825866020672 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0531 00:42:34.432572 139663327090432 logging_writer.py:48] [0] global_step=0, grad_norm=23.588773727416992, loss=32.61616897583008
I0531 00:42:34.458041 139825866020672 spec.py:298] Evaluating on the training split.
I0531 00:42:34.727159 139825866020672 input_pipeline.py:20] Loading split = train-clean-100
I0531 00:42:34.762818 139825866020672 input_pipeline.py:20] Loading split = train-clean-360
I0531 00:42:35.160375 139825866020672 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0531 00:44:35.864145 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 00:44:36.057248 139825866020672 input_pipeline.py:20] Loading split = dev-clean
I0531 00:44:36.062682 139825866020672 input_pipeline.py:20] Loading split = dev-other
I0531 00:45:36.074468 139825866020672 spec.py:326] Evaluating on the test split.
I0531 00:45:36.276430 139825866020672 input_pipeline.py:20] Loading split = test-clean
I0531 00:46:16.734248 139825866020672 submission_runner.py:426] Time since start: 289.50s, 	Step: 1, 	{'train/ctc_loss': Array(32.025578, dtype=float32), 'train/wer': 3.993404202153135, 'validation/ctc_loss': Array(30.998869, dtype=float32), 'validation/wer': 3.5688911615162713, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.147984, dtype=float32), 'test/wer': 3.91536164767534, 'test/num_examples': 2472, 'score': 67.22890901565552, 'total_duration': 289.5034034252167, 'accumulated_submission_time': 67.22890901565552, 'accumulated_data_selection_time': 4.911206007003784, 'accumulated_eval_time': 222.27430701255798, 'accumulated_logging_time': 0}
I0531 00:46:16.755448 139657538959104 logging_writer.py:48] [1] accumulated_data_selection_time=4.911206, accumulated_eval_time=222.274307, accumulated_logging_time=0, accumulated_submission_time=67.228909, global_step=1, preemption_count=0, score=67.228909, test/ctc_loss=31.14798355102539, test/num_examples=2472, test/wer=3.915362, total_duration=289.503403, train/ctc_loss=32.025577545166016, train/wer=3.993404, validation/ctc_loss=30.998868942260742, validation/num_examples=5348, validation/wer=3.568891
I0531 00:49:00.509746 139665954961152 logging_writer.py:48] [100] global_step=100, grad_norm=7.309545516967773, loss=8.717167854309082
I0531 00:51:29.351996 139665963353856 logging_writer.py:48] [200] global_step=200, grad_norm=0.9799528121948242, loss=6.163386821746826
I0531 00:53:57.507845 139665954961152 logging_writer.py:48] [300] global_step=300, grad_norm=0.9354215264320374, loss=5.902052879333496
I0531 00:56:27.587904 139665963353856 logging_writer.py:48] [400] global_step=400, grad_norm=0.8928511738777161, loss=5.83683443069458
I0531 00:58:55.759020 139665954961152 logging_writer.py:48] [500] global_step=500, grad_norm=1.385859489440918, loss=5.824683666229248
I0531 01:01:24.040073 139665963353856 logging_writer.py:48] [600] global_step=600, grad_norm=1.3220897912979126, loss=5.767920017242432
I0531 01:03:54.969366 139665954961152 logging_writer.py:48] [700] global_step=700, grad_norm=1.2005494832992554, loss=5.711665153503418
I0531 01:06:23.734236 139665963353856 logging_writer.py:48] [800] global_step=800, grad_norm=0.5425723195075989, loss=5.581241130828857
I0531 01:08:53.677406 139665954961152 logging_writer.py:48] [900] global_step=900, grad_norm=1.2289869785308838, loss=5.477236270904541
I0531 01:11:21.226792 139665963353856 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9168099164962769, loss=5.27424430847168
I0531 01:13:53.159651 139667988182784 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.843840479850769, loss=4.872317790985107
I0531 01:16:21.261109 139667979790080 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1091718673706055, loss=4.538227558135986
I0531 01:18:48.999176 139667988182784 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3401293754577637, loss=4.29128360748291
I0531 01:21:18.835752 139667979790080 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.4477986097335815, loss=4.03331995010376
I0531 01:23:45.758589 139667988182784 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.2575558423995972, loss=3.8523738384246826
I0531 01:26:15.035706 139667979790080 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.2732609510421753, loss=3.6980035305023193
I0531 01:26:17.855617 139825866020672 spec.py:298] Evaluating on the training split.
I0531 01:26:47.283696 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 01:27:23.020637 139825866020672 spec.py:326] Evaluating on the test split.
I0531 01:27:40.534551 139825866020672 submission_runner.py:426] Time since start: 2773.30s, 	Step: 1603, 	{'train/ctc_loss': Array(6.4825325, dtype=float32), 'train/wer': 0.9425140573829581, 'validation/ctc_loss': Array(6.3966494, dtype=float32), 'validation/wer': 0.8948470318092794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.2566867, dtype=float32), 'test/wer': 0.8980155586699977, 'test/num_examples': 2472, 'score': 2468.2977242469788, 'total_duration': 2773.303247451782, 'accumulated_submission_time': 2468.2977242469788, 'accumulated_data_selection_time': 170.02427697181702, 'accumulated_eval_time': 304.95090675354004, 'accumulated_logging_time': 0.03244924545288086}
I0531 01:27:40.554926 139667773142784 logging_writer.py:48] [1603] accumulated_data_selection_time=170.024277, accumulated_eval_time=304.950907, accumulated_logging_time=0.032449, accumulated_submission_time=2468.297724, global_step=1603, preemption_count=0, score=2468.297724, test/ctc_loss=6.256686687469482, test/num_examples=2472, test/wer=0.898016, total_duration=2773.303247, train/ctc_loss=6.482532501220703, train/wer=0.942514, validation/ctc_loss=6.396649360656738, validation/num_examples=5348, validation/wer=0.894847
I0531 01:30:04.578140 139667764750080 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.7030645608901978, loss=3.5507280826568604
I0531 01:32:31.056889 139667773142784 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.409635305404663, loss=3.442427635192871
I0531 01:34:58.189682 139667764750080 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.2819706201553345, loss=3.2990269660949707
I0531 01:37:24.884366 139667773142784 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.5712947845458984, loss=3.222993850708008
I0531 01:39:55.624125 139667773142784 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.863892912864685, loss=3.144242763519287
I0531 01:42:22.330945 139667764750080 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.8906209468841553, loss=3.0593910217285156
I0531 01:44:50.007965 139667773142784 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.7098792791366577, loss=3.0576882362365723
I0531 01:47:18.145609 139667764750080 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6764534711837769, loss=2.907566785812378
I0531 01:49:44.220326 139667773142784 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.8674226999282837, loss=2.835233688354492
I0531 01:52:11.485041 139667764750080 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.160639762878418, loss=2.8602852821350098
I0531 01:54:36.839750 139667773142784 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.6675604581832886, loss=2.7405807971954346
I0531 01:57:02.121236 139667764750080 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4784321784973145, loss=2.6945605278015137
I0531 01:59:28.039220 139667773142784 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.9220837354660034, loss=2.6570353507995605
I0531 02:01:54.161029 139667764750080 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5134774446487427, loss=2.645334482192993
I0531 02:04:23.570559 139667773142784 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.6352572441101074, loss=2.5868289470672607
I0531 02:06:49.802546 139667764750080 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.881405234336853, loss=2.5273258686065674
I0531 02:07:40.685050 139825866020672 spec.py:298] Evaluating on the training split.
I0531 02:08:20.029676 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 02:08:58.907650 139825866020672 spec.py:326] Evaluating on the test split.
I0531 02:09:18.789425 139825866020672 submission_runner.py:426] Time since start: 5271.56s, 	Step: 3236, 	{'train/ctc_loss': Array(3.9734957, dtype=float32), 'train/wer': 0.7613083664065206, 'validation/ctc_loss': Array(4.2293296, dtype=float32), 'validation/wer': 0.77187430655385, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.839546, dtype=float32), 'test/wer': 0.7345479657952999, 'test/num_examples': 2472, 'score': 4868.391775846481, 'total_duration': 5271.557901144028, 'accumulated_submission_time': 4868.391775846481, 'accumulated_data_selection_time': 360.7145667076111, 'accumulated_eval_time': 403.05272006988525, 'accumulated_logging_time': 0.06780481338500977}
I0531 02:09:18.809760 139667773142784 logging_writer.py:48] [3236] accumulated_data_selection_time=360.714567, accumulated_eval_time=403.052720, accumulated_logging_time=0.067805, accumulated_submission_time=4868.391776, global_step=3236, preemption_count=0, score=4868.391776, test/ctc_loss=3.839545965194702, test/num_examples=2472, test/wer=0.734548, total_duration=5271.557901, train/ctc_loss=3.9734957218170166, train/wer=0.761308, validation/ctc_loss=4.229329586029053, validation/num_examples=5348, validation/wer=0.771874
I0531 02:10:53.287358 139667764750080 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.8637415170669556, loss=2.5225937366485596
I0531 02:13:18.610907 139667773142784 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.6194589138031006, loss=2.425790309906006
I0531 02:15:45.932282 139667764750080 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.57681143283844, loss=2.3735294342041016
I0531 02:18:12.657480 139667773142784 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.6912771463394165, loss=2.353506326675415
I0531 02:20:41.698454 139667764750080 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.602234125137329, loss=2.363969087600708
I0531 02:23:10.681440 139667773142784 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.4845240116119385, loss=2.3490023612976074
I0531 02:25:36.766172 139667764750080 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.868401288986206, loss=2.286731719970703
I0531 02:28:02.196902 139667773142784 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.465201497077942, loss=2.2006545066833496
I0531 02:30:30.110119 139667764750080 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7383627891540527, loss=2.1825127601623535
I0531 02:33:00.984839 139667773142784 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.3217520713806152, loss=2.1079726219177246
I0531 02:35:26.688346 139667764750080 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.5281189680099487, loss=2.1508681774139404
I0531 02:37:52.744208 139667773142784 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.5669505596160889, loss=2.068554401397705
I0531 02:40:17.225444 139667764750080 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.727734923362732, loss=2.171978712081909
I0531 02:42:43.592365 139667773142784 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.4717360734939575, loss=2.1467080116271973
I0531 02:45:08.613028 139667764750080 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.4811296463012695, loss=2.0666685104370117
I0531 02:47:34.816277 139667773142784 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.7574291229248047, loss=2.0963335037231445
I0531 02:49:19.856797 139825866020672 spec.py:298] Evaluating on the training split.
I0531 02:50:06.977453 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 02:50:48.115929 139825866020672 spec.py:326] Evaluating on the test split.
I0531 02:51:08.799204 139825866020672 submission_runner.py:426] Time since start: 7781.57s, 	Step: 4872, 	{'train/ctc_loss': Array(0.7761537, dtype=float32), 'train/wer': 0.25030704598046505, 'validation/ctc_loss': Array(1.1797906, dtype=float32), 'validation/wer': 0.32200986020125616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.82466686, dtype=float32), 'test/wer': 0.25306197062945585, 'test/num_examples': 2472, 'score': 7269.399045705795, 'total_duration': 7781.566255331039, 'accumulated_submission_time': 7269.399045705795, 'accumulated_data_selection_time': 554.6540021896362, 'accumulated_eval_time': 511.99115014076233, 'accumulated_logging_time': 0.10689091682434082}
I0531 02:51:08.818569 139667988182784 logging_writer.py:48] [4872] accumulated_data_selection_time=554.654002, accumulated_eval_time=511.991150, accumulated_logging_time=0.106891, accumulated_submission_time=7269.399046, global_step=4872, preemption_count=0, score=7269.399046, test/ctc_loss=0.8246668577194214, test/num_examples=2472, test/wer=0.253062, total_duration=7781.566255, train/ctc_loss=0.7761536836624146, train/wer=0.250307, validation/ctc_loss=1.1797906160354614, validation/num_examples=5348, validation/wer=0.322010
I0531 02:51:50.722715 139667979790080 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.3240952491760254, loss=2.0391228199005127
I0531 02:54:15.326435 139667988182784 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.7823173999786377, loss=2.0853044986724854
I0531 02:56:40.632523 139667979790080 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.6914994716644287, loss=2.0771281719207764
I0531 02:59:10.587046 139666902742784 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.5002377033233643, loss=2.0807788372039795
I0531 03:01:39.304203 139666894350080 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.4550893306732178, loss=2.0019469261169434
I0531 03:04:05.539366 139666902742784 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.6161082983016968, loss=2.000013589859009
I0531 03:06:29.990561 139666894350080 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.2704085111618042, loss=1.9638607501983643
I0531 03:08:59.136305 139666902742784 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.627933144569397, loss=1.9638831615447998
I0531 03:11:26.419859 139666894350080 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9090272188186646, loss=1.9968818426132202
I0531 03:13:52.500474 139666902742784 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.502785325050354, loss=1.931788682937622
I0531 03:16:20.059445 139666894350080 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.6325515508651733, loss=1.938288927078247
I0531 03:18:45.123242 139666902742784 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.5359371900558472, loss=1.878817081451416
I0531 03:21:10.322715 139666894350080 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.6597511768341064, loss=1.9265886545181274
I0531 03:23:40.827425 139666902742784 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.8331797122955322, loss=1.9225655794143677
I0531 03:26:08.171375 139666894350080 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.4197523593902588, loss=1.901724934577942
I0531 03:28:32.832247 139666902742784 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.5794734954833984, loss=1.8305963277816772
I0531 03:30:57.430468 139666894350080 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.596124291419983, loss=1.9201244115829468
I0531 03:31:09.072487 139825866020672 spec.py:298] Evaluating on the training split.
I0531 03:31:55.773469 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 03:32:37.530893 139825866020672 spec.py:326] Evaluating on the test split.
I0531 03:32:58.936519 139825866020672 submission_runner.py:426] Time since start: 10291.70s, 	Step: 6509, 	{'train/ctc_loss': Array(0.6053485, dtype=float32), 'train/wer': 0.20314905381771722, 'validation/ctc_loss': Array(0.9637, dtype=float32), 'validation/wer': 0.27267026213470463, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64193285, dtype=float32), 'test/wer': 0.20784839436963012, 'test/num_examples': 2472, 'score': 9669.617339611053, 'total_duration': 10291.704429149628, 'accumulated_submission_time': 9669.617339611053, 'accumulated_data_selection_time': 756.8411738872528, 'accumulated_eval_time': 621.852071762085, 'accumulated_logging_time': 0.13996028900146484}
I0531 03:32:58.959161 139666974422784 logging_writer.py:48] [6509] accumulated_data_selection_time=756.841174, accumulated_eval_time=621.852072, accumulated_logging_time=0.139960, accumulated_submission_time=9669.617340, global_step=6509, preemption_count=0, score=9669.617340, test/ctc_loss=0.6419328451156616, test/num_examples=2472, test/wer=0.207848, total_duration=10291.704429, train/ctc_loss=0.605348527431488, train/wer=0.203149, validation/ctc_loss=0.963699996471405, validation/num_examples=5348, validation/wer=0.272670
I0531 03:35:15.228354 139666966030080 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.2297617197036743, loss=1.9283055067062378
I0531 03:37:39.667453 139666974422784 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.33876371383667, loss=1.8993358612060547
I0531 03:40:07.648357 139666966030080 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.7147488594055176, loss=1.8417034149169922
I0531 03:42:32.545061 139666974422784 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.5280028581619263, loss=1.8603112697601318
I0531 03:44:57.274311 139666966030080 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.3180168867111206, loss=1.7914928197860718
I0531 03:47:23.129637 139666974422784 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.3302420377731323, loss=1.8234589099884033
I0531 03:49:47.496201 139666966030080 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.2682355642318726, loss=1.8191686868667603
I0531 03:52:15.291024 139666974422784 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.2644894123077393, loss=1.7814160585403442
I0531 03:54:40.174522 139666966030080 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.1029387712478638, loss=1.784473180770874
I0531 03:57:07.444166 139666974422784 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.5222245454788208, loss=1.808115839958191
I0531 03:59:32.993047 139666966030080 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.3649024963378906, loss=1.8596044778823853
I0531 04:02:01.414484 139666974422784 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.8253495693206787, loss=1.745176911354065
I0531 04:04:25.779387 139666966030080 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.4092131853103638, loss=1.7553194761276245
I0531 04:06:53.870627 139666974422784 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.4142591953277588, loss=1.7633267641067505
I0531 04:09:22.247044 139666966030080 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.443177580833435, loss=1.732337236404419
I0531 04:11:46.650013 139666974422784 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.5394256114959717, loss=1.784669041633606
I0531 04:12:59.913512 139825866020672 spec.py:298] Evaluating on the training split.
I0531 04:13:46.907880 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 04:14:30.459337 139825866020672 spec.py:326] Evaluating on the test split.
I0531 04:14:51.104734 139825866020672 submission_runner.py:426] Time since start: 12803.87s, 	Step: 8152, 	{'train/ctc_loss': Array(0.486176, dtype=float32), 'train/wer': 0.16636043401683012, 'validation/ctc_loss': Array(0.8348518, dtype=float32), 'validation/wer': 0.2405715443467858, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52844304, dtype=float32), 'test/wer': 0.170515711006845, 'test/num_examples': 2472, 'score': 12070.536250829697, 'total_duration': 12803.871966362, 'accumulated_submission_time': 12070.536250829697, 'accumulated_data_selection_time': 958.084007024765, 'accumulated_eval_time': 733.0395314693451, 'accumulated_logging_time': 0.17579412460327148}
I0531 04:14:51.125784 139667988182784 logging_writer.py:48] [8152] accumulated_data_selection_time=958.084007, accumulated_eval_time=733.039531, accumulated_logging_time=0.175794, accumulated_submission_time=12070.536251, global_step=8152, preemption_count=0, score=12070.536251, test/ctc_loss=0.5284430384635925, test/num_examples=2472, test/wer=0.170516, total_duration=12803.871966, train/ctc_loss=0.4861760139465332, train/wer=0.166360, validation/ctc_loss=0.8348518013954163, validation/num_examples=5348, validation/wer=0.240572
I0531 04:16:01.934473 139667979790080 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0988045930862427, loss=1.6879485845565796
I0531 04:18:31.657188 139666790102784 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8448535203933716, loss=1.7358708381652832
I0531 04:20:56.098565 139666781710080 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.2057111263275146, loss=1.7272284030914307
I0531 04:23:22.298269 139666790102784 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.7774242162704468, loss=1.7575410604476929
I0531 04:25:48.126838 139666781710080 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.1323195695877075, loss=1.7089910507202148
I0531 04:28:12.620671 139666790102784 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.176227569580078, loss=1.7649365663528442
I0531 04:30:37.270566 139666781710080 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.356458306312561, loss=1.7067134380340576
I0531 04:33:03.977440 139666790102784 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.3789801597595215, loss=1.6926486492156982
I0531 04:35:30.467265 139666781710080 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.460018277168274, loss=1.6869690418243408
I0531 04:37:58.299339 139666790102784 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.2953870296478271, loss=1.6045490503311157
I0531 04:40:24.591569 139666781710080 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.7701396942138672, loss=1.731593370437622
I0531 04:42:54.355589 139666790102784 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.3817448616027832, loss=1.6603150367736816
I0531 04:45:20.415200 139666781710080 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.1172410249710083, loss=1.6376982927322388
I0531 04:47:46.521540 139666790102784 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3661965131759644, loss=1.621087908744812
I0531 04:50:11.005968 139666781710080 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.610466480255127, loss=1.6660548448562622
I0531 04:52:35.578444 139666790102784 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8051637411117554, loss=1.658166766166687
I0531 04:54:52.612681 139825866020672 spec.py:298] Evaluating on the training split.
I0531 04:55:40.273286 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 04:56:24.822314 139825866020672 spec.py:326] Evaluating on the test split.
I0531 04:56:46.269343 139825866020672 submission_runner.py:426] Time since start: 15319.04s, 	Step: 9795, 	{'train/ctc_loss': Array(0.45896414, dtype=float32), 'train/wer': 0.1558327242852799, 'validation/ctc_loss': Array(0.7900374, dtype=float32), 'validation/wer': 0.2261864562127951, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48336107, dtype=float32), 'test/wer': 0.15773972741860134, 'test/num_examples': 2472, 'score': 14471.98659491539, 'total_duration': 15319.037956476212, 'accumulated_submission_time': 14471.98659491539, 'accumulated_data_selection_time': 1165.526216506958, 'accumulated_eval_time': 846.6937882900238, 'accumulated_logging_time': 0.21076631546020508}
I0531 04:56:46.290583 139667332822784 logging_writer.py:48] [9795] accumulated_data_selection_time=1165.526217, accumulated_eval_time=846.693788, accumulated_logging_time=0.210766, accumulated_submission_time=14471.986595, global_step=9795, preemption_count=0, score=14471.986595, test/ctc_loss=0.48336106538772583, test/num_examples=2472, test/wer=0.157740, total_duration=15319.037956, train/ctc_loss=0.45896413922309875, train/wer=0.155833, validation/ctc_loss=0.7900373935699463, validation/num_examples=5348, validation/wer=0.226186
I0531 04:56:55.040303 139667324430080 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.733341097831726, loss=1.7194008827209473
I0531 04:59:19.214024 139667332822784 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.711280345916748, loss=1.7484979629516602
I0531 05:01:43.927458 139667324430080 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.783851385116577, loss=1.6726055145263672
I0531 05:04:08.159434 139667332822784 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.4188263416290283, loss=1.6553205251693726
I0531 05:06:37.386261 139667324430080 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0247154235839844, loss=1.6597014665603638
I0531 05:09:09.801944 139666575062784 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.4358878135681152, loss=1.6489357948303223
I0531 05:11:34.666801 139666566670080 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4373058080673218, loss=1.5942158699035645
I0531 05:14:00.863854 139666575062784 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4135172367095947, loss=1.6441575288772583
I0531 05:16:27.899066 139666566670080 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.5875117778778076, loss=1.699064016342163
I0531 05:18:53.152302 139666575062784 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.3763550519943237, loss=1.647566318511963
I0531 05:21:18.343601 139666566670080 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.4801653623580933, loss=1.6788358688354492
I0531 05:23:44.249278 139666575062784 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.4168469905853271, loss=1.674163818359375
I0531 05:26:08.635889 139666566670080 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.3368786573410034, loss=1.5991055965423584
I0531 05:28:33.779072 139666575062784 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.4013162851333618, loss=1.6137014627456665
I0531 05:31:01.777842 139666566670080 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.657543659210205, loss=1.6447434425354004
I0531 05:33:29.665093 139666575062784 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.3216307163238525, loss=1.677943468093872
I0531 05:35:57.971061 139667332822784 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.5167088508605957, loss=1.6343603134155273
I0531 05:36:47.742343 139825866020672 spec.py:298] Evaluating on the training split.
I0531 05:37:35.638945 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 05:38:19.776879 139825866020672 spec.py:326] Evaluating on the test split.
I0531 05:38:41.886376 139825866020672 submission_runner.py:426] Time since start: 17834.65s, 	Step: 11435, 	{'train/ctc_loss': Array(0.39143756, dtype=float32), 'train/wer': 0.1348988577627465, 'validation/ctc_loss': Array(0.71504027, dtype=float32), 'validation/wer': 0.20568457003926713, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43852636, dtype=float32), 'test/wer': 0.14394816484878029, 'test/num_examples': 2472, 'score': 16873.399893522263, 'total_duration': 17834.65354204178, 'accumulated_submission_time': 16873.399893522263, 'accumulated_data_selection_time': 1375.402270078659, 'accumulated_eval_time': 960.8339545726776, 'accumulated_logging_time': 0.2484734058380127}
I0531 05:38:41.915491 139667558102784 logging_writer.py:48] [11435] accumulated_data_selection_time=1375.402270, accumulated_eval_time=960.833955, accumulated_logging_time=0.248473, accumulated_submission_time=16873.399894, global_step=11435, preemption_count=0, score=16873.399894, test/ctc_loss=0.43852636218070984, test/num_examples=2472, test/wer=0.143948, total_duration=17834.653542, train/ctc_loss=0.3914375603199005, train/wer=0.134899, validation/ctc_loss=0.7150402665138245, validation/num_examples=5348, validation/wer=0.205685
I0531 05:40:17.586920 139667549710080 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.4477126598358154, loss=1.6015478372573853
I0531 05:42:42.353130 139667558102784 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.30728280544281, loss=1.6277430057525635
I0531 05:45:10.806697 139667549710080 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.5039103031158447, loss=1.5919760465621948
I0531 05:47:36.067496 139667558102784 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.6885607242584229, loss=1.6400458812713623
I0531 05:50:01.340209 139667549710080 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.5997953414916992, loss=1.6097989082336426
I0531 05:52:26.693228 139667558102784 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4918556213378906, loss=1.5814961194992065
I0531 05:54:53.506869 139667549710080 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1356254816055298, loss=1.576692819595337
I0531 05:57:21.048586 139667558102784 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.0596423149108887, loss=1.5900969505310059
I0531 05:59:46.451624 139667549710080 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.0627262592315674, loss=1.585974097251892
I0531 06:02:16.860413 139667558102784 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.4340637922286987, loss=1.5385568141937256
I0531 06:04:43.527897 139667549710080 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.156328797340393, loss=1.5778146982192993
I0531 06:07:08.152400 139667558102784 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.633610725402832, loss=1.5718417167663574
I0531 06:09:32.562893 139667549710080 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.3630346059799194, loss=1.6308594942092896
I0531 06:11:57.221782 139667558102784 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.4016519784927368, loss=1.563340425491333
I0531 06:14:21.953738 139667549710080 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.2521916627883911, loss=1.6593581438064575
I0531 06:16:46.809846 139667558102784 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2846556901931763, loss=1.5823287963867188
I0531 06:18:42.742437 139825866020672 spec.py:298] Evaluating on the training split.
I0531 06:19:30.964183 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 06:20:14.328674 139825866020672 spec.py:326] Evaluating on the test split.
I0531 06:20:36.577265 139825866020672 submission_runner.py:426] Time since start: 20349.34s, 	Step: 13081, 	{'train/ctc_loss': Array(0.33209395, dtype=float32), 'train/wer': 0.11858862684937206, 'validation/ctc_loss': Array(0.6892537, dtype=float32), 'validation/wer': 0.19801445262375902, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40222785, dtype=float32), 'test/wer': 0.13170028233095687, 'test/num_examples': 2472, 'score': 19274.188925027847, 'total_duration': 20349.344519853592, 'accumulated_submission_time': 19274.188925027847, 'accumulated_data_selection_time': 1583.1536037921906, 'accumulated_eval_time': 1074.6650240421295, 'accumulated_logging_time': 0.292935848236084}
I0531 06:20:36.601364 139667266262784 logging_writer.py:48] [13081] accumulated_data_selection_time=1583.153604, accumulated_eval_time=1074.665024, accumulated_logging_time=0.292936, accumulated_submission_time=19274.188925, global_step=13081, preemption_count=0, score=19274.188925, test/ctc_loss=0.40222784876823425, test/num_examples=2472, test/wer=0.131700, total_duration=20349.344520, train/ctc_loss=0.3320939540863037, train/wer=0.118589, validation/ctc_loss=0.6892536878585815, validation/num_examples=5348, validation/wer=0.198014
I0531 06:21:05.403692 139667257870080 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.1484369039535522, loss=1.5650746822357178
I0531 06:23:30.897043 139667266262784 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.5473078489303589, loss=1.505173683166504
I0531 06:25:55.395003 139667257870080 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4756808280944824, loss=1.6264594793319702
I0531 06:28:24.612449 139667266262784 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.8123146295547485, loss=1.6038551330566406
I0531 06:30:48.472604 139667257870080 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.6448280811309814, loss=1.5970321893692017
I0531 06:33:12.763332 139667266262784 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.4671704769134521, loss=1.6101044416427612
I0531 06:35:41.277784 139667257870080 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.3334825038909912, loss=1.5376570224761963
I0531 06:38:05.869813 139667266262784 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.8921904563903809, loss=1.6074483394622803
I0531 06:40:31.555803 139667257870080 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4080923795700073, loss=1.5360281467437744
I0531 06:42:58.121341 139667266262784 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.4022221565246582, loss=1.5864795446395874
I0531 06:45:22.182747 139667257870080 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.4156842231750488, loss=1.5172160863876343
I0531 06:47:46.523483 139667266262784 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.208832025527954, loss=1.5608116388320923
I0531 06:50:13.018684 139667257870080 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.3962749242782593, loss=1.5714622735977173
I0531 06:52:38.493937 139667266262784 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.247063159942627, loss=1.6297476291656494
I0531 06:55:05.478721 139667266262784 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4105461835861206, loss=1.5698609352111816
I0531 06:57:30.048807 139667257870080 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.620027780532837, loss=1.6095190048217773
I0531 06:59:54.358759 139667266262784 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.518375039100647, loss=1.610234022140503
I0531 07:00:37.393065 139825866020672 spec.py:298] Evaluating on the training split.
I0531 07:01:24.202353 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 07:02:08.578551 139825866020672 spec.py:326] Evaluating on the test split.
I0531 07:02:30.259361 139825866020672 submission_runner.py:426] Time since start: 22863.03s, 	Step: 14731, 	{'train/ctc_loss': Array(0.3137423, dtype=float32), 'train/wer': 0.10857109416978332, 'validation/ctc_loss': Array(0.6602473, dtype=float32), 'validation/wer': 0.19005489681521287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39142433, dtype=float32), 'test/wer': 0.1263786484674913, 'test/num_examples': 2472, 'score': 21674.94199204445, 'total_duration': 22863.02795982361, 'accumulated_submission_time': 21674.94199204445, 'accumulated_data_selection_time': 1797.2085614204407, 'accumulated_eval_time': 1187.5289161205292, 'accumulated_logging_time': 0.3330419063568115}
I0531 07:02:30.278324 139667266262784 logging_writer.py:48] [14731] accumulated_data_selection_time=1797.208561, accumulated_eval_time=1187.528916, accumulated_logging_time=0.333042, accumulated_submission_time=21674.941992, global_step=14731, preemption_count=0, score=21674.941992, test/ctc_loss=0.3914243280887604, test/num_examples=2472, test/wer=0.126379, total_duration=22863.027960, train/ctc_loss=0.3137423098087311, train/wer=0.108571, validation/ctc_loss=0.6602473258972168, validation/num_examples=5348, validation/wer=0.190055
I0531 07:04:12.722929 139667257870080 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.9670075178146362, loss=1.5521174669265747
I0531 07:06:38.044497 139667266262784 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.761467218399048, loss=1.468765139579773
I0531 07:09:02.522765 139667257870080 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.3600571155548096, loss=1.5861057043075562
I0531 07:11:26.746136 139667266262784 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.2540217638015747, loss=1.546281337738037
I0531 07:13:51.359412 139667257870080 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.3906910419464111, loss=1.5817188024520874
I0531 07:16:18.215467 139667266262784 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.7800637483596802, loss=1.5209580659866333
I0531 07:18:45.429301 139667257870080 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.055068016052246, loss=1.5095041990280151
I0531 07:21:16.003463 139667266262784 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.639251947402954, loss=1.5416823625564575
I0531 07:23:41.866916 139667257870080 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.1656136512756348, loss=1.5872118473052979
I0531 07:26:05.868865 139667266262784 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.3576635122299194, loss=1.5035425424575806
I0531 07:28:30.628424 139667257870080 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.3238638639450073, loss=1.5791962146759033
I0531 07:30:55.935128 139667266262784 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5926744937896729, loss=1.5435223579406738
I0531 07:33:20.280220 139825866020672 spec.py:298] Evaluating on the training split.
I0531 07:34:07.962032 139825866020672 spec.py:310] Evaluating on the validation split.
I0531 07:34:51.241230 139825866020672 spec.py:326] Evaluating on the test split.
I0531 07:35:13.652477 139825866020672 submission_runner.py:426] Time since start: 24826.42s, 	Step: 16000, 	{'train/ctc_loss': Array(0.3102782, dtype=float32), 'train/wer': 0.11030906272959827, 'validation/ctc_loss': Array(0.645182, dtype=float32), 'validation/wer': 0.18522127565147758, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37917826, dtype=float32), 'test/wer': 0.12446935998212581, 'test/num_examples': 2472, 'score': 23524.91489481926, 'total_duration': 24826.42056798935, 'accumulated_submission_time': 23524.91489481926, 'accumulated_data_selection_time': 1960.1905143260956, 'accumulated_eval_time': 1300.8982927799225, 'accumulated_logging_time': 0.3639504909515381}
I0531 07:35:13.678976 139667988182784 logging_writer.py:48] [16000] accumulated_data_selection_time=1960.190514, accumulated_eval_time=1300.898293, accumulated_logging_time=0.363950, accumulated_submission_time=23524.914895, global_step=16000, preemption_count=0, score=23524.914895, test/ctc_loss=0.3791782557964325, test/num_examples=2472, test/wer=0.124469, total_duration=24826.420568, train/ctc_loss=0.3102782070636749, train/wer=0.110309, validation/ctc_loss=0.6451820135116577, validation/num_examples=5348, validation/wer=0.185221
I0531 07:35:13.704281 139667979790080 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=23524.914895
I0531 07:35:13.905182 139825866020672 checkpoints.py:490] Saving checkpoint at step: 16000
I0531 07:35:15.000648 139825866020672 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0531 07:35:15.025773 139825866020672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_jax_upgrade_a/sam/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0531 07:35:16.285718 139825866020672 submission_runner.py:589] Tuning trial 1/1
I0531 07:35:16.286016 139825866020672 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0531 07:35:16.293980 139825866020672 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.025578, dtype=float32), 'train/wer': 3.993404202153135, 'validation/ctc_loss': Array(30.998869, dtype=float32), 'validation/wer': 3.5688911615162713, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.147984, dtype=float32), 'test/wer': 3.91536164767534, 'test/num_examples': 2472, 'score': 67.22890901565552, 'total_duration': 289.5034034252167, 'accumulated_submission_time': 67.22890901565552, 'accumulated_data_selection_time': 4.911206007003784, 'accumulated_eval_time': 222.27430701255798, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1603, {'train/ctc_loss': Array(6.4825325, dtype=float32), 'train/wer': 0.9425140573829581, 'validation/ctc_loss': Array(6.3966494, dtype=float32), 'validation/wer': 0.8948470318092794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.2566867, dtype=float32), 'test/wer': 0.8980155586699977, 'test/num_examples': 2472, 'score': 2468.2977242469788, 'total_duration': 2773.303247451782, 'accumulated_submission_time': 2468.2977242469788, 'accumulated_data_selection_time': 170.02427697181702, 'accumulated_eval_time': 304.95090675354004, 'accumulated_logging_time': 0.03244924545288086, 'global_step': 1603, 'preemption_count': 0}), (3236, {'train/ctc_loss': Array(3.9734957, dtype=float32), 'train/wer': 0.7613083664065206, 'validation/ctc_loss': Array(4.2293296, dtype=float32), 'validation/wer': 0.77187430655385, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.839546, dtype=float32), 'test/wer': 0.7345479657952999, 'test/num_examples': 2472, 'score': 4868.391775846481, 'total_duration': 5271.557901144028, 'accumulated_submission_time': 4868.391775846481, 'accumulated_data_selection_time': 360.7145667076111, 'accumulated_eval_time': 403.05272006988525, 'accumulated_logging_time': 0.06780481338500977, 'global_step': 3236, 'preemption_count': 0}), (4872, {'train/ctc_loss': Array(0.7761537, dtype=float32), 'train/wer': 0.25030704598046505, 'validation/ctc_loss': Array(1.1797906, dtype=float32), 'validation/wer': 0.32200986020125616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.82466686, dtype=float32), 'test/wer': 0.25306197062945585, 'test/num_examples': 2472, 'score': 7269.399045705795, 'total_duration': 7781.566255331039, 'accumulated_submission_time': 7269.399045705795, 'accumulated_data_selection_time': 554.6540021896362, 'accumulated_eval_time': 511.99115014076233, 'accumulated_logging_time': 0.10689091682434082, 'global_step': 4872, 'preemption_count': 0}), (6509, {'train/ctc_loss': Array(0.6053485, dtype=float32), 'train/wer': 0.20314905381771722, 'validation/ctc_loss': Array(0.9637, dtype=float32), 'validation/wer': 0.27267026213470463, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64193285, dtype=float32), 'test/wer': 0.20784839436963012, 'test/num_examples': 2472, 'score': 9669.617339611053, 'total_duration': 10291.704429149628, 'accumulated_submission_time': 9669.617339611053, 'accumulated_data_selection_time': 756.8411738872528, 'accumulated_eval_time': 621.852071762085, 'accumulated_logging_time': 0.13996028900146484, 'global_step': 6509, 'preemption_count': 0}), (8152, {'train/ctc_loss': Array(0.486176, dtype=float32), 'train/wer': 0.16636043401683012, 'validation/ctc_loss': Array(0.8348518, dtype=float32), 'validation/wer': 0.2405715443467858, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52844304, dtype=float32), 'test/wer': 0.170515711006845, 'test/num_examples': 2472, 'score': 12070.536250829697, 'total_duration': 12803.871966362, 'accumulated_submission_time': 12070.536250829697, 'accumulated_data_selection_time': 958.084007024765, 'accumulated_eval_time': 733.0395314693451, 'accumulated_logging_time': 0.17579412460327148, 'global_step': 8152, 'preemption_count': 0}), (9795, {'train/ctc_loss': Array(0.45896414, dtype=float32), 'train/wer': 0.1558327242852799, 'validation/ctc_loss': Array(0.7900374, dtype=float32), 'validation/wer': 0.2261864562127951, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48336107, dtype=float32), 'test/wer': 0.15773972741860134, 'test/num_examples': 2472, 'score': 14471.98659491539, 'total_duration': 15319.037956476212, 'accumulated_submission_time': 14471.98659491539, 'accumulated_data_selection_time': 1165.526216506958, 'accumulated_eval_time': 846.6937882900238, 'accumulated_logging_time': 0.21076631546020508, 'global_step': 9795, 'preemption_count': 0}), (11435, {'train/ctc_loss': Array(0.39143756, dtype=float32), 'train/wer': 0.1348988577627465, 'validation/ctc_loss': Array(0.71504027, dtype=float32), 'validation/wer': 0.20568457003926713, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43852636, dtype=float32), 'test/wer': 0.14394816484878029, 'test/num_examples': 2472, 'score': 16873.399893522263, 'total_duration': 17834.65354204178, 'accumulated_submission_time': 16873.399893522263, 'accumulated_data_selection_time': 1375.402270078659, 'accumulated_eval_time': 960.8339545726776, 'accumulated_logging_time': 0.2484734058380127, 'global_step': 11435, 'preemption_count': 0}), (13081, {'train/ctc_loss': Array(0.33209395, dtype=float32), 'train/wer': 0.11858862684937206, 'validation/ctc_loss': Array(0.6892537, dtype=float32), 'validation/wer': 0.19801445262375902, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40222785, dtype=float32), 'test/wer': 0.13170028233095687, 'test/num_examples': 2472, 'score': 19274.188925027847, 'total_duration': 20349.344519853592, 'accumulated_submission_time': 19274.188925027847, 'accumulated_data_selection_time': 1583.1536037921906, 'accumulated_eval_time': 1074.6650240421295, 'accumulated_logging_time': 0.292935848236084, 'global_step': 13081, 'preemption_count': 0}), (14731, {'train/ctc_loss': Array(0.3137423, dtype=float32), 'train/wer': 0.10857109416978332, 'validation/ctc_loss': Array(0.6602473, dtype=float32), 'validation/wer': 0.19005489681521287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39142433, dtype=float32), 'test/wer': 0.1263786484674913, 'test/num_examples': 2472, 'score': 21674.94199204445, 'total_duration': 22863.02795982361, 'accumulated_submission_time': 21674.94199204445, 'accumulated_data_selection_time': 1797.2085614204407, 'accumulated_eval_time': 1187.5289161205292, 'accumulated_logging_time': 0.3330419063568115, 'global_step': 14731, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.3102782, dtype=float32), 'train/wer': 0.11030906272959827, 'validation/ctc_loss': Array(0.645182, dtype=float32), 'validation/wer': 0.18522127565147758, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37917826, dtype=float32), 'test/wer': 0.12446935998212581, 'test/num_examples': 2472, 'score': 23524.91489481926, 'total_duration': 24826.42056798935, 'accumulated_submission_time': 23524.91489481926, 'accumulated_data_selection_time': 1960.1905143260956, 'accumulated_eval_time': 1300.8982927799225, 'accumulated_logging_time': 0.3639504909515381, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0531 07:35:16.294174 139825866020672 submission_runner.py:592] Timing: 23524.91489481926
I0531 07:35:16.294243 139825866020672 submission_runner.py:593] ====================
I0531 07:35:16.295940 139825866020672 submission_runner.py:661] Final librispeech_deepspeech score: 23524.91489481926
