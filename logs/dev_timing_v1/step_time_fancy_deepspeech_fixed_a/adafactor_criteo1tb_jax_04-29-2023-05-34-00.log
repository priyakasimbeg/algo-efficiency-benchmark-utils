python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_jax_04-29-2023-05-34-00.log
I0429 05:34:20.741986 139635594610496 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax.
I0429 05:34:20.913850 139635594610496 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 05:34:21.786538 139635594610496 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0429 05:34:21.787937 139635594610496 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 05:34:21.791945 139635594610496 submission_runner.py:538] Using RNG seed 259590735
I0429 05:34:24.554949 139635594610496 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 05:34:24.555152 139635594610496 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1.
I0429 05:34:24.557149 139635594610496 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1/hparams.json.
I0429 05:34:24.680598 139635594610496 submission_runner.py:241] Initializing dataset.
I0429 05:34:24.680826 139635594610496 submission_runner.py:248] Initializing model.
I0429 05:34:30.864224 139635594610496 submission_runner.py:258] Initializing optimizer.
I0429 05:34:32.252100 139635594610496 submission_runner.py:265] Initializing metrics bundle.
I0429 05:34:32.252304 139635594610496 submission_runner.py:282] Initializing checkpoint and logger.
I0429 05:34:32.256400 139635594610496 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1 with prefix checkpoint_
I0429 05:34:32.256650 139635594610496 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 05:34:32.256711 139635594610496 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 05:34:33.025583 139635594610496 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1/meta_data_0.json.
I0429 05:34:33.026678 139635594610496 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1/flags_0.json.
I0429 05:34:33.081116 139635594610496 submission_runner.py:318] Starting training loop.
I0429 05:35:01.236772 139459651622656 logging_writer.py:48] [0] global_step=0, grad_norm=2.053490400314331, loss=0.27177464962005615
I0429 05:35:01.245117 139635594610496 spec.py:298] Evaluating on the training split.
I0429 05:39:27.216892 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 05:43:44.462920 139635594610496 spec.py:326] Evaluating on the test split.
I0429 05:48:03.415938 139635594610496 submission_runner.py:415] Time since start: 810.33s, 	Step: 1, 	{'train/loss': 0.2752183432786056, 'validation/loss': 0.2760880449438202, 'validation/num_examples': 89000000, 'test/loss': 0.27746910917151085, 'test/num_examples': 89274637, 'score': 28.163795471191406, 'total_duration': 810.3347189426422, 'accumulated_submission_time': 28.163795471191406, 'accumulated_eval_time': 782.1707172393799, 'accumulated_logging_time': 0}
I0429 05:48:03.432260 139447932782336 logging_writer.py:48] [1] accumulated_eval_time=782.170717, accumulated_logging_time=0, accumulated_submission_time=28.163795, global_step=1, preemption_count=0, score=28.163795, test/loss=0.277469, test/num_examples=89274637, total_duration=810.334719, train/loss=0.275218, validation/loss=0.276088, validation/num_examples=89000000
I0429 05:49:01.491791 139447924389632 logging_writer.py:48] [100] global_step=100, grad_norm=0.3032451570034027, loss=0.1492154449224472
I0429 05:50:03.443396 139635594610496 spec.py:298] Evaluating on the training split.
I0429 05:54:11.665499 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 05:58:17.226472 139635594610496 spec.py:326] Evaluating on the test split.
I0429 06:02:09.111452 139635594610496 submission_runner.py:415] Time since start: 1656.03s, 	Step: 183, 	{'train/loss': 0.13786090236099613, 'validation/loss': 0.13881886516853933, 'validation/num_examples': 89000000, 'test/loss': 0.1419799780311624, 'test/num_examples': 89274637, 'score': 148.1647708415985, 'total_duration': 1656.0302503108978, 'accumulated_submission_time': 148.1647708415985, 'accumulated_eval_time': 1507.8387217521667, 'accumulated_logging_time': 0.023894786834716797}
I0429 06:02:09.119321 139447932782336 logging_writer.py:48] [183] accumulated_eval_time=1507.838722, accumulated_logging_time=0.023895, accumulated_submission_time=148.164771, global_step=183, preemption_count=0, score=148.164771, test/loss=0.141980, test/num_examples=89274637, total_duration=1656.030250, train/loss=0.137861, validation/loss=0.138819, validation/num_examples=89000000
I0429 06:02:11.212966 139447924389632 logging_writer.py:48] [200] global_step=200, grad_norm=0.0381452850997448, loss=0.14429177343845367
I0429 06:03:20.262016 139447932782336 logging_writer.py:48] [300] global_step=300, grad_norm=0.014227361418306828, loss=0.12531504034996033
I0429 06:04:09.538983 139635594610496 spec.py:298] Evaluating on the training split.
I0429 06:08:19.021366 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 06:12:28.846888 139635594610496 spec.py:326] Evaluating on the test split.
I0429 06:16:43.025008 139635594610496 submission_runner.py:415] Time since start: 2529.94s, 	Step: 366, 	{'train/loss': 0.12857966251106628, 'validation/loss': 0.1289160786516854, 'validation/num_examples': 89000000, 'test/loss': 0.13200676469846637, 'test/num_examples': 89274637, 'score': 268.5755832195282, 'total_duration': 2529.943785905838, 'accumulated_submission_time': 268.5755832195282, 'accumulated_eval_time': 2261.324662208557, 'accumulated_logging_time': 0.038201332092285156}
I0429 06:16:43.034889 139447924389632 logging_writer.py:48] [366] accumulated_eval_time=2261.324662, accumulated_logging_time=0.038201, accumulated_submission_time=268.575583, global_step=366, preemption_count=0, score=268.575583, test/loss=0.132007, test/num_examples=89274637, total_duration=2529.943786, train/loss=0.128580, validation/loss=0.128916, validation/num_examples=89000000
I0429 06:16:51.451777 139447932782336 logging_writer.py:48] [400] global_step=400, grad_norm=0.026418568566441536, loss=0.12662151455879211
I0429 06:18:07.204465 139447924389632 logging_writer.py:48] [500] global_step=500, grad_norm=0.10626351088285446, loss=0.1293918639421463
I0429 06:18:43.058275 139635594610496 spec.py:298] Evaluating on the training split.
I0429 06:23:02.140386 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 06:27:11.916512 139635594610496 spec.py:326] Evaluating on the test split.
I0429 06:31:10.798729 139635594610496 submission_runner.py:415] Time since start: 3397.72s, 	Step: 549, 	{'train/loss': 0.1279977738111663, 'validation/loss': 0.12765944943820226, 'validation/num_examples': 89000000, 'test/loss': 0.130395904046073, 'test/num_examples': 89274637, 'score': 388.5889585018158, 'total_duration': 3397.7175250053406, 'accumulated_submission_time': 388.5889585018158, 'accumulated_eval_time': 3009.065050125122, 'accumulated_logging_time': 0.05561661720275879}
I0429 06:31:10.807692 139447932782336 logging_writer.py:48] [549] accumulated_eval_time=3009.065050, accumulated_logging_time=0.055617, accumulated_submission_time=388.588959, global_step=549, preemption_count=0, score=388.588959, test/loss=0.130396, test/num_examples=89274637, total_duration=3397.717525, train/loss=0.127998, validation/loss=0.127659, validation/num_examples=89000000
I0429 06:31:32.345180 139447924389632 logging_writer.py:48] [600] global_step=600, grad_norm=0.07873190194368362, loss=0.12317796796560287
I0429 06:32:49.382619 139447932782336 logging_writer.py:48] [700] global_step=700, grad_norm=0.07301817089319229, loss=0.1240166425704956
I0429 06:33:11.303726 139635594610496 spec.py:298] Evaluating on the training split.
I0429 06:37:30.084331 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 06:41:39.965670 139635594610496 spec.py:326] Evaluating on the test split.
I0429 06:45:47.241132 139635594610496 submission_runner.py:415] Time since start: 4274.16s, 	Step: 730, 	{'train/loss': 0.12550320320045186, 'validation/loss': 0.12702685393258428, 'validation/num_examples': 89000000, 'test/loss': 0.12969735177976696, 'test/num_examples': 89274637, 'score': 509.0757827758789, 'total_duration': 4274.159910202026, 'accumulated_submission_time': 509.0757827758789, 'accumulated_eval_time': 3765.0023946762085, 'accumulated_logging_time': 0.07121515274047852}
I0429 06:45:47.250144 139447924389632 logging_writer.py:48] [730] accumulated_eval_time=3765.002395, accumulated_logging_time=0.071215, accumulated_submission_time=509.075783, global_step=730, preemption_count=0, score=509.075783, test/loss=0.129697, test/num_examples=89274637, total_duration=4274.159910, train/loss=0.125503, validation/loss=0.127027, validation/num_examples=89000000
I0429 06:46:22.805294 139447932782336 logging_writer.py:48] [800] global_step=800, grad_norm=0.03433120623230934, loss=0.12328037619590759
I0429 06:47:39.470593 139447924389632 logging_writer.py:48] [900] global_step=900, grad_norm=0.08728653192520142, loss=0.1280403584241867
I0429 06:47:47.748893 139635594610496 spec.py:298] Evaluating on the training split.
I0429 06:51:54.845351 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 06:56:05.572713 139635594610496 spec.py:326] Evaluating on the test split.
I0429 07:00:00.170432 139635594610496 submission_runner.py:415] Time since start: 5127.09s, 	Step: 912, 	{'train/loss': 0.12596436894229443, 'validation/loss': 0.12721805617977527, 'validation/num_examples': 89000000, 'test/loss': 0.13036026122402491, 'test/num_examples': 89274637, 'score': 629.5647361278534, 'total_duration': 5127.089214801788, 'accumulated_submission_time': 629.5647361278534, 'accumulated_eval_time': 4497.423867940903, 'accumulated_logging_time': 0.0872800350189209}
I0429 07:00:00.182063 139447932782336 logging_writer.py:48] [912] accumulated_eval_time=4497.423868, accumulated_logging_time=0.087280, accumulated_submission_time=629.564736, global_step=912, preemption_count=0, score=629.564736, test/loss=0.130360, test/num_examples=89274637, total_duration=5127.089215, train/loss=0.125964, validation/loss=0.127218, validation/num_examples=89000000
I0429 07:00:50.718869 139447924389632 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.05291389301419258, loss=0.13036225736141205
I0429 07:02:00.192565 139635594610496 spec.py:298] Evaluating on the training split.
I0429 07:06:20.112503 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 07:10:30.780451 139635594610496 spec.py:326] Evaluating on the test split.
I0429 07:14:36.979325 139635594610496 submission_runner.py:415] Time since start: 6003.90s, 	Step: 1087, 	{'train/loss': 0.12723692275742735, 'validation/loss': 0.12638069662921347, 'validation/num_examples': 89000000, 'test/loss': 0.12897533260202446, 'test/num_examples': 89274637, 'score': 749.565993309021, 'total_duration': 6003.898118495941, 'accumulated_submission_time': 749.565993309021, 'accumulated_eval_time': 5254.210586309433, 'accumulated_logging_time': 0.10582160949707031}
I0429 07:14:36.987234 139447932782336 logging_writer.py:48] [1087] accumulated_eval_time=5254.210586, accumulated_logging_time=0.105822, accumulated_submission_time=749.565993, global_step=1087, preemption_count=0, score=749.565993, test/loss=0.128975, test/num_examples=89274637, total_duration=6003.898118, train/loss=0.127237, validation/loss=0.126381, validation/num_examples=89000000
I0429 07:14:38.599236 139447924389632 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.011836295016109943, loss=0.12554752826690674
I0429 07:15:49.803912 139447932782336 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.09808213263750076, loss=0.12432962656021118
I0429 07:16:37.199523 139635594610496 spec.py:298] Evaluating on the training split.
I0429 07:21:01.572095 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 07:25:10.000221 139635594610496 spec.py:326] Evaluating on the test split.
I0429 07:29:06.194626 139635594610496 submission_runner.py:415] Time since start: 6873.11s, 	Step: 1264, 	{'train/loss': 0.12689668207680854, 'validation/loss': 0.1267282584269663, 'validation/num_examples': 89000000, 'test/loss': 0.12941067461299227, 'test/num_examples': 89274637, 'score': 869.7694563865662, 'total_duration': 6873.113407373428, 'accumulated_submission_time': 869.7694563865662, 'accumulated_eval_time': 6003.205612421036, 'accumulated_logging_time': 0.12025141716003418}
I0429 07:29:06.202872 139447924389632 logging_writer.py:48] [1264] accumulated_eval_time=6003.205612, accumulated_logging_time=0.120251, accumulated_submission_time=869.769456, global_step=1264, preemption_count=0, score=869.769456, test/loss=0.129411, test/num_examples=89274637, total_duration=6873.113407, train/loss=0.126897, validation/loss=0.126728, validation/num_examples=89000000
I0429 07:29:15.863949 139447932782336 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.017688386142253876, loss=0.12745064496994019
I0429 07:30:32.394323 139447924389632 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.04159151017665863, loss=0.12952834367752075
I0429 07:31:06.798844 139635594610496 spec.py:298] Evaluating on the training split.
I0429 07:35:29.074543 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 07:39:41.985595 139635594610496 spec.py:326] Evaluating on the test split.
I0429 07:43:37.943945 139635594610496 submission_runner.py:415] Time since start: 7744.86s, 	Step: 1446, 	{'train/loss': 0.12520757117983697, 'validation/loss': 0.12661103370786517, 'validation/num_examples': 89000000, 'test/loss': 0.12918522424235676, 'test/num_examples': 89274637, 'score': 990.3563940525055, 'total_duration': 7744.862736463547, 'accumulated_submission_time': 990.3563940525055, 'accumulated_eval_time': 6754.3506445884705, 'accumulated_logging_time': 0.13509559631347656}
I0429 07:43:37.951693 139447932782336 logging_writer.py:48] [1446] accumulated_eval_time=6754.350645, accumulated_logging_time=0.135096, accumulated_submission_time=990.356394, global_step=1446, preemption_count=0, score=990.356394, test/loss=0.129185, test/num_examples=89274637, total_duration=7744.862736, train/loss=0.125208, validation/loss=0.126611, validation/num_examples=89000000
I0429 07:44:01.435390 139447924389632 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.056930191814899445, loss=0.12421293556690216
I0429 07:45:16.333519 139635594610496 spec.py:298] Evaluating on the training split.
I0429 07:49:26.823316 139635594610496 spec.py:310] Evaluating on the validation split.
I0429 07:53:37.937375 139635594610496 spec.py:326] Evaluating on the test split.
I0429 07:57:33.269621 139635594610496 submission_runner.py:415] Time since start: 8580.19s, 	Step: 1600, 	{'train/loss': 0.12455977439178624, 'validation/loss': 0.12571683146067417, 'validation/num_examples': 89000000, 'test/loss': 0.1282824034333514, 'test/num_examples': 89274637, 'score': 1088.7295715808868, 'total_duration': 8580.188415288925, 'accumulated_submission_time': 1088.7295715808868, 'accumulated_eval_time': 7491.286687374115, 'accumulated_logging_time': 0.14941740036010742}
I0429 07:57:33.278395 139447932782336 logging_writer.py:48] [1600] accumulated_eval_time=7491.286687, accumulated_logging_time=0.149417, accumulated_submission_time=1088.729572, global_step=1600, preemption_count=0, score=1088.729572, test/loss=0.128282, test/num_examples=89274637, total_duration=8580.188415, train/loss=0.124560, validation/loss=0.125717, validation/num_examples=89000000
I0429 07:57:33.290487 139447924389632 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1088.729572
I0429 07:57:34.956260 139635594610496 checkpoints.py:356] Saving checkpoint at step: 1600
I0429 07:57:43.537180 139635594610496 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1/checkpoint_1600
I0429 07:57:43.555120 139635594610496 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_adafactor/criteo1tb_jax/trial_1/checkpoint_1600.
I0429 07:57:43.644152 139635594610496 submission_runner.py:578] Tuning trial 1/1
I0429 07:57:43.644386 139635594610496 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 07:57:43.646046 139635594610496 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/loss': 0.2752183432786056, 'validation/loss': 0.2760880449438202, 'validation/num_examples': 89000000, 'test/loss': 0.27746910917151085, 'test/num_examples': 89274637, 'score': 28.163795471191406, 'total_duration': 810.3347189426422, 'accumulated_submission_time': 28.163795471191406, 'accumulated_eval_time': 782.1707172393799, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (183, {'train/loss': 0.13786090236099613, 'validation/loss': 0.13881886516853933, 'validation/num_examples': 89000000, 'test/loss': 0.1419799780311624, 'test/num_examples': 89274637, 'score': 148.1647708415985, 'total_duration': 1656.0302503108978, 'accumulated_submission_time': 148.1647708415985, 'accumulated_eval_time': 1507.8387217521667, 'accumulated_logging_time': 0.023894786834716797, 'global_step': 183, 'preemption_count': 0}), (366, {'train/loss': 0.12857966251106628, 'validation/loss': 0.1289160786516854, 'validation/num_examples': 89000000, 'test/loss': 0.13200676469846637, 'test/num_examples': 89274637, 'score': 268.5755832195282, 'total_duration': 2529.943785905838, 'accumulated_submission_time': 268.5755832195282, 'accumulated_eval_time': 2261.324662208557, 'accumulated_logging_time': 0.038201332092285156, 'global_step': 366, 'preemption_count': 0}), (549, {'train/loss': 0.1279977738111663, 'validation/loss': 0.12765944943820226, 'validation/num_examples': 89000000, 'test/loss': 0.130395904046073, 'test/num_examples': 89274637, 'score': 388.5889585018158, 'total_duration': 3397.7175250053406, 'accumulated_submission_time': 388.5889585018158, 'accumulated_eval_time': 3009.065050125122, 'accumulated_logging_time': 0.05561661720275879, 'global_step': 549, 'preemption_count': 0}), (730, {'train/loss': 0.12550320320045186, 'validation/loss': 0.12702685393258428, 'validation/num_examples': 89000000, 'test/loss': 0.12969735177976696, 'test/num_examples': 89274637, 'score': 509.0757827758789, 'total_duration': 4274.159910202026, 'accumulated_submission_time': 509.0757827758789, 'accumulated_eval_time': 3765.0023946762085, 'accumulated_logging_time': 0.07121515274047852, 'global_step': 730, 'preemption_count': 0}), (912, {'train/loss': 0.12596436894229443, 'validation/loss': 0.12721805617977527, 'validation/num_examples': 89000000, 'test/loss': 0.13036026122402491, 'test/num_examples': 89274637, 'score': 629.5647361278534, 'total_duration': 5127.089214801788, 'accumulated_submission_time': 629.5647361278534, 'accumulated_eval_time': 4497.423867940903, 'accumulated_logging_time': 0.0872800350189209, 'global_step': 912, 'preemption_count': 0}), (1087, {'train/loss': 0.12723692275742735, 'validation/loss': 0.12638069662921347, 'validation/num_examples': 89000000, 'test/loss': 0.12897533260202446, 'test/num_examples': 89274637, 'score': 749.565993309021, 'total_duration': 6003.898118495941, 'accumulated_submission_time': 749.565993309021, 'accumulated_eval_time': 5254.210586309433, 'accumulated_logging_time': 0.10582160949707031, 'global_step': 1087, 'preemption_count': 0}), (1264, {'train/loss': 0.12689668207680854, 'validation/loss': 0.1267282584269663, 'validation/num_examples': 89000000, 'test/loss': 0.12941067461299227, 'test/num_examples': 89274637, 'score': 869.7694563865662, 'total_duration': 6873.113407373428, 'accumulated_submission_time': 869.7694563865662, 'accumulated_eval_time': 6003.205612421036, 'accumulated_logging_time': 0.12025141716003418, 'global_step': 1264, 'preemption_count': 0}), (1446, {'train/loss': 0.12520757117983697, 'validation/loss': 0.12661103370786517, 'validation/num_examples': 89000000, 'test/loss': 0.12918522424235676, 'test/num_examples': 89274637, 'score': 990.3563940525055, 'total_duration': 7744.862736463547, 'accumulated_submission_time': 990.3563940525055, 'accumulated_eval_time': 6754.3506445884705, 'accumulated_logging_time': 0.13509559631347656, 'global_step': 1446, 'preemption_count': 0}), (1600, {'train/loss': 0.12455977439178624, 'validation/loss': 0.12571683146067417, 'validation/num_examples': 89000000, 'test/loss': 0.1282824034333514, 'test/num_examples': 89274637, 'score': 1088.7295715808868, 'total_duration': 8580.188415288925, 'accumulated_submission_time': 1088.7295715808868, 'accumulated_eval_time': 7491.286687374115, 'accumulated_logging_time': 0.14941740036010742, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0429 07:57:43.646160 139635594610496 submission_runner.py:581] Timing: 1088.7295715808868
I0429 07:57:43.646205 139635594610496 submission_runner.py:582] ====================
I0429 07:57:43.646296 139635594610496 submission_runner.py:645] Final criteo1tb score: 1088.7295715808868
