python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_jax_upgrade_a/lamb --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-31-2023-00-41-50.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0531 00:42:12.707290 139653397636928 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax.
I0531 00:42:13.626268 139653397636928 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0531 00:42:13.627089 139653397636928 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 00:42:13.627692 139653397636928 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0531 00:42:13.633915 139653397636928 submission_runner.py:549] Using RNG seed 2210004625
I0531 00:42:19.137673 139653397636928 submission_runner.py:558] --- Tuning run 1/1 ---
I0531 00:42:19.137908 139653397636928 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1.
I0531 00:42:19.138120 139653397636928 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1/hparams.json.
I0531 00:42:19.317167 139653397636928 submission_runner.py:243] Initializing dataset.
I0531 00:42:19.317400 139653397636928 submission_runner.py:250] Initializing model.
I0531 00:42:21.827547 139653397636928 submission_runner.py:260] Initializing optimizer.
I0531 00:42:22.524074 139653397636928 submission_runner.py:267] Initializing metrics bundle.
I0531 00:42:22.524304 139653397636928 submission_runner.py:285] Initializing checkpoint and logger.
I0531 00:42:22.525686 139653397636928 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0531 00:42:22.526101 139653397636928 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0531 00:42:22.526226 139653397636928 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0531 00:42:23.319338 139653397636928 submission_runner.py:306] Saving meta data to /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0531 00:42:23.320635 139653397636928 submission_runner.py:309] Saving flags to /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0531 00:42:23.327916 139653397636928 submission_runner.py:321] Starting training loop.
I0531 00:42:23.625719 139653397636928 input_pipeline.py:20] Loading split = train-clean-100
I0531 00:42:23.662884 139653397636928 input_pipeline.py:20] Loading split = train-clean-360
I0531 00:42:24.066922 139653397636928 input_pipeline.py:20] Loading split = train-other-500
2023-05-31 00:43:21.509803: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-31 00:43:21.742101: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0531 00:43:26.538138 139493055133440 logging_writer.py:48] [0] global_step=0, grad_norm=29.90656089782715, loss=32.46563720703125
I0531 00:43:26.560475 139653397636928 spec.py:298] Evaluating on the training split.
I0531 00:43:26.818845 139653397636928 input_pipeline.py:20] Loading split = train-clean-100
I0531 00:43:26.853247 139653397636928 input_pipeline.py:20] Loading split = train-clean-360
I0531 00:43:27.156897 139653397636928 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0531 00:45:12.547172 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 00:45:12.740099 139653397636928 input_pipeline.py:20] Loading split = dev-clean
I0531 00:45:12.745602 139653397636928 input_pipeline.py:20] Loading split = dev-other
I0531 00:46:09.003063 139653397636928 spec.py:326] Evaluating on the test split.
I0531 00:46:09.203079 139653397636928 input_pipeline.py:20] Loading split = test-clean
I0531 00:46:46.478958 139653397636928 submission_runner.py:426] Time since start: 263.15s, 	Step: 1, 	{'train/ctc_loss': Array(30.852835, dtype=float32), 'train/wer': 3.2253919996668263, 'validation/ctc_loss': Array(29.565132, dtype=float32), 'validation/wer': 2.937780393443256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.723234, dtype=float32), 'test/wer': 3.0795401458371416, 'test/num_examples': 2472, 'score': 63.23236155509949, 'total_duration': 263.14890146255493, 'accumulated_submission_time': 63.23236155509949, 'accumulated_data_selection_time': 4.576021671295166, 'accumulated_eval_time': 199.91636323928833, 'accumulated_logging_time': 0}
I0531 00:46:46.504824 139488319715072 logging_writer.py:48] [1] accumulated_data_selection_time=4.576022, accumulated_eval_time=199.916363, accumulated_logging_time=0, accumulated_submission_time=63.232362, global_step=1, preemption_count=0, score=63.232362, test/ctc_loss=29.723234176635742, test/num_examples=2472, test/wer=3.079540, total_duration=263.148901, train/ctc_loss=30.852834701538086, train/wer=3.225392, validation/ctc_loss=29.56513214111328, validation/num_examples=5348, validation/wer=2.937780
I0531 00:48:14.161407 139495417689856 logging_writer.py:48] [100] global_step=100, grad_norm=49.11359405517578, loss=29.065195083618164
I0531 00:49:31.017889 139495426082560 logging_writer.py:48] [200] global_step=200, grad_norm=33.64455032348633, loss=21.74140739440918
I0531 00:50:48.468884 139495417689856 logging_writer.py:48] [300] global_step=300, grad_norm=16.351802825927734, loss=13.504097938537598
I0531 00:52:05.623173 139495426082560 logging_writer.py:48] [400] global_step=400, grad_norm=9.406346321105957, loss=9.05609130859375
I0531 00:53:22.429609 139495417689856 logging_writer.py:48] [500] global_step=500, grad_norm=2.1183769702911377, loss=6.859063625335693
I0531 00:54:38.455511 139495426082560 logging_writer.py:48] [600] global_step=600, grad_norm=0.9816411733627319, loss=6.1078782081604
I0531 00:55:55.559142 139495417689856 logging_writer.py:48] [700] global_step=700, grad_norm=1.3836618661880493, loss=5.891476631164551
I0531 00:57:12.355857 139495426082560 logging_writer.py:48] [800] global_step=800, grad_norm=0.7174733877182007, loss=5.844890594482422
I0531 00:58:32.035858 139495417689856 logging_writer.py:48] [900] global_step=900, grad_norm=0.5038042664527893, loss=5.795565605163574
I0531 00:59:52.139477 139495426082560 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2521075010299683, loss=5.786860942840576
I0531 01:01:13.389783 139492375652096 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.3972288370132446, loss=5.724974632263184
I0531 01:02:29.692827 139491176085248 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.166999340057373, loss=5.645699977874756
I0531 01:03:46.040744 139492375652096 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.1939953565597534, loss=5.5580267906188965
I0531 01:05:02.740864 139491176085248 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.3327716588974, loss=5.489163398742676
I0531 01:06:20.286353 139492375652096 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0967607498168945, loss=5.390746593475342
I0531 01:07:36.764514 139491176085248 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1999742984771729, loss=5.302678108215332
I0531 01:08:52.827335 139492375652096 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9562148451805115, loss=5.175806522369385
I0531 01:10:09.318042 139491176085248 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9383184909820557, loss=5.031547546386719
I0531 01:11:30.719277 139492375652096 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.6034772396087646, loss=4.855319023132324
I0531 01:12:52.839684 139491176085248 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3058165311813354, loss=4.651608943939209
I0531 01:14:14.895228 139495593936640 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2251863479614258, loss=4.519278049468994
I0531 01:15:31.433876 139495585543936 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.7314120531082153, loss=4.3866448402404785
I0531 01:16:48.364379 139495593936640 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2343899011611938, loss=4.2000837326049805
I0531 01:18:05.039845 139495585543936 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.910882592201233, loss=4.066370010375977
I0531 01:19:20.967449 139495593936640 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.5245330333709717, loss=4.016721725463867
I0531 01:20:37.212731 139495585543936 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.7570604085922241, loss=3.8886404037475586
I0531 01:22:01.681232 139495593936640 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.223752021789551, loss=3.7679765224456787
I0531 01:23:25.730260 139495585543936 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.9266252517700195, loss=3.667844772338867
I0531 01:24:53.087492 139495593936640 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.1619467735290527, loss=3.6531333923339844
I0531 01:26:17.349422 139495585543936 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.9158669710159302, loss=3.5360474586486816
I0531 01:26:46.565042 139653397636928 spec.py:298] Evaluating on the training split.
I0531 01:27:28.347529 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 01:28:08.541244 139653397636928 spec.py:326] Evaluating on the test split.
I0531 01:28:29.113116 139653397636928 submission_runner.py:426] Time since start: 2765.78s, 	Step: 3036, 	{'train/ctc_loss': Array(5.9139705, dtype=float32), 'train/wer': 0.8879425758480772, 'validation/ctc_loss': Array(5.8611503, dtype=float32), 'validation/wer': 0.8531968470511052, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7323685, dtype=float32), 'test/wer': 0.8537363150732232, 'test/num_examples': 2472, 'score': 2463.2427685260773, 'total_duration': 2765.780918598175, 'accumulated_submission_time': 2463.2427685260773, 'accumulated_data_selection_time': 455.2301621437073, 'accumulated_eval_time': 302.46022844314575, 'accumulated_logging_time': 0.03782081604003906}
I0531 01:28:29.132877 139495593936640 logging_writer.py:48] [3036] accumulated_data_selection_time=455.230162, accumulated_eval_time=302.460228, accumulated_logging_time=0.037821, accumulated_submission_time=2463.242769, global_step=3036, preemption_count=0, score=2463.242769, test/ctc_loss=5.732368469238281, test/num_examples=2472, test/wer=0.853736, total_duration=2765.780919, train/ctc_loss=5.913970470428467, train/wer=0.887943, validation/ctc_loss=5.86115026473999, validation/num_examples=5348, validation/wer=0.853197
I0531 01:29:22.107093 139495266256640 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.746596574783325, loss=3.4556469917297363
I0531 01:30:38.005773 139495257863936 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.7500070333480835, loss=3.387702226638794
I0531 01:31:54.054638 139495266256640 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.661792278289795, loss=3.3349664211273193
I0531 01:33:10.981963 139495257863936 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.0983123779296875, loss=3.2709624767303467
I0531 01:34:27.397778 139495266256640 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.2468793392181396, loss=3.259349822998047
I0531 01:35:51.118204 139495257863936 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.697927951812744, loss=3.163403034210205
I0531 01:37:16.673276 139495266256640 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.2706758975982666, loss=3.1000332832336426
I0531 01:38:38.056548 139495257863936 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.3569071292877197, loss=3.079216480255127
I0531 01:40:03.883384 139495266256640 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.2756855487823486, loss=3.050273895263672
I0531 01:41:24.885695 139495257863936 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.670780897140503, loss=2.9993245601654053
I0531 01:42:50.717194 139495266256640 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.271207094192505, loss=2.9287872314453125
I0531 01:44:11.217124 139495593936640 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.060727834701538, loss=2.91949725151062
I0531 01:45:27.123788 139495585543936 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.4972362518310547, loss=2.8183631896972656
I0531 01:46:43.309403 139495593936640 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.899130344390869, loss=2.8275208473205566
I0531 01:48:01.669200 139495585543936 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.3254170417785645, loss=2.760317802429199
I0531 01:49:22.770678 139495593936640 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.846195697784424, loss=2.754356622695923
I0531 01:50:47.875753 139495585543936 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.4168527126312256, loss=2.718607187271118
I0531 01:52:14.251185 139495593936640 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.068683624267578, loss=2.7296905517578125
I0531 01:53:37.576357 139495585543936 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.6459929943084717, loss=2.702528953552246
I0531 01:55:03.633546 139495593936640 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.014819383621216, loss=2.5779058933258057
I0531 01:56:28.568312 139495585543936 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.4512696266174316, loss=2.603360414505005
I0531 01:57:51.898564 139495593936640 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.7498667240142822, loss=2.567232608795166
I0531 01:59:07.789553 139495585543936 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.0625996589660645, loss=2.562899589538574
I0531 02:00:24.475438 139495593936640 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.144538164138794, loss=2.541940450668335
I0531 02:01:39.983889 139495585543936 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.694572687149048, loss=2.508094072341919
I0531 02:02:56.042914 139495593936640 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.9193344116210938, loss=2.4546875953674316
I0531 02:04:15.463599 139495585543936 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.261485576629639, loss=2.458319902420044
I0531 02:05:37.025012 139495593936640 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.2041079998016357, loss=2.4051175117492676
I0531 02:07:02.330229 139495585543936 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.620880603790283, loss=2.38925838470459
I0531 02:08:24.696720 139495593936640 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.7262678146362305, loss=2.383683443069458
I0531 02:08:29.801454 139653397636928 spec.py:298] Evaluating on the training split.
I0531 02:09:17.272529 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 02:10:00.685337 139653397636928 spec.py:326] Evaluating on the test split.
I0531 02:10:23.393707 139653397636928 submission_runner.py:426] Time since start: 5280.06s, 	Step: 6008, 	{'train/ctc_loss': Array(1.3196273, dtype=float32), 'train/wer': 0.37348107503396105, 'validation/ctc_loss': Array(1.7256984, dtype=float32), 'validation/wer': 0.4295458711613233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.315635, dtype=float32), 'test/wer': 0.3630085511750249, 'test/num_examples': 2472, 'score': 4863.857192277908, 'total_duration': 5280.0614449977875, 'accumulated_submission_time': 4863.857192277908, 'accumulated_data_selection_time': 1034.3520748615265, 'accumulated_eval_time': 416.0482099056244, 'accumulated_logging_time': 0.07343912124633789}
I0531 02:10:23.414949 139495593936640 logging_writer.py:48] [6008] accumulated_data_selection_time=1034.352075, accumulated_eval_time=416.048210, accumulated_logging_time=0.073439, accumulated_submission_time=4863.857192, global_step=6008, preemption_count=0, score=4863.857192, test/ctc_loss=1.3156349658966064, test/num_examples=2472, test/wer=0.363009, total_duration=5280.061445, train/ctc_loss=1.319627285003662, train/wer=0.373481, validation/ctc_loss=1.7256983518600464, validation/num_examples=5348, validation/wer=0.429546
I0531 02:11:33.084428 139495585543936 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.432248592376709, loss=2.363262176513672
I0531 02:12:51.693453 139495593936640 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.6075727939605713, loss=2.3291866779327393
I0531 02:14:07.313866 139495585543936 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.746868133544922, loss=2.326280355453491
I0531 02:15:23.137453 139495593936640 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.7475156784057617, loss=2.279247760772705
I0531 02:16:39.920873 139495585543936 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.6117639541625977, loss=2.290283203125
I0531 02:17:55.685690 139495593936640 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.773763418197632, loss=2.333591938018799
I0531 02:19:13.839558 139495585543936 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.6128926277160645, loss=2.2529184818267822
I0531 02:20:39.153665 139495593936640 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.9406094551086426, loss=2.3120522499084473
I0531 02:22:03.252568 139495585543936 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.2822458744049072, loss=2.2993013858795166
I0531 02:23:27.570349 139495593936640 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.555466890335083, loss=2.2345333099365234
I0531 02:24:53.142778 139495585543936 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.0910990238189697, loss=2.2192113399505615
I0531 02:26:17.011185 139495593936640 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.8798446655273438, loss=2.1857972145080566
I0531 02:27:35.968974 139495593936640 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.9701294898986816, loss=2.211104154586792
I0531 02:28:51.359171 139495585543936 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.4018306732177734, loss=2.189525842666626
I0531 02:30:06.714118 139495593936640 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.0469484329223633, loss=2.0885729789733887
I0531 02:31:22.408946 139495585543936 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.6864213943481445, loss=2.1060304641723633
I0531 02:32:39.933023 139495593936640 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.5449721813201904, loss=2.110755681991577
I0531 02:34:04.324237 139495585543936 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.6233389377593994, loss=2.1455421447753906
I0531 02:35:22.588945 139495593936640 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.544464349746704, loss=2.0767107009887695
I0531 02:36:42.328253 139495585543936 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.855501413345337, loss=2.066338300704956
I0531 02:38:07.643088 139495593936640 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.7812798023223877, loss=2.0285542011260986
I0531 02:39:27.932472 139495585543936 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.682366847991943, loss=2.1151137351989746
I0531 02:40:46.619841 139495266256640 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.866194248199463, loss=2.0269289016723633
I0531 02:42:02.228528 139495257863936 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.1530630588531494, loss=1.9647279977798462
I0531 02:43:18.424898 139495266256640 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.240816831588745, loss=2.045139789581299
I0531 02:44:35.130393 139495257863936 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.197474241256714, loss=2.037382125854492
I0531 02:45:50.825150 139495266256640 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.85364031791687, loss=1.9567921161651611
I0531 02:47:05.986732 139495257863936 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.1424736976623535, loss=1.9447563886642456
I0531 02:48:22.909922 139495266256640 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.0259714126586914, loss=2.0270509719848633
I0531 02:49:42.382812 139495257863936 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.736599922180176, loss=2.002803325653076
I0531 02:50:23.471547 139653397636928 spec.py:298] Evaluating on the training split.
I0531 02:51:13.203763 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 02:51:55.756744 139653397636928 spec.py:326] Evaluating on the test split.
I0531 02:52:17.026827 139653397636928 submission_runner.py:426] Time since start: 7793.69s, 	Step: 9052, 	{'train/ctc_loss': Array(0.6146764, dtype=float32), 'train/wer': 0.2055442225279504, 'validation/ctc_loss': Array(1.0068036, dtype=float32), 'validation/wer': 0.28274271821242847, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68008256, dtype=float32), 'test/wer': 0.21337314402941115, 'test/num_examples': 2472, 'score': 7263.859972953796, 'total_duration': 7793.694908618927, 'accumulated_submission_time': 7263.859972953796, 'accumulated_data_selection_time': 1563.9134638309479, 'accumulated_eval_time': 529.5995516777039, 'accumulated_logging_time': 0.11121010780334473}
I0531 02:52:17.047095 139494974416640 logging_writer.py:48] [9052] accumulated_data_selection_time=1563.913464, accumulated_eval_time=529.599552, accumulated_logging_time=0.111210, accumulated_submission_time=7263.859973, global_step=9052, preemption_count=0, score=7263.859973, test/ctc_loss=0.6800825595855713, test/num_examples=2472, test/wer=0.213373, total_duration=7793.694909, train/ctc_loss=0.6146764159202576, train/wer=0.205544, validation/ctc_loss=1.0068036317825317, validation/num_examples=5348, validation/wer=0.282743
I0531 02:52:53.867532 139494966023936 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.466426372528076, loss=2.0110816955566406
I0531 02:54:09.251085 139494974416640 logging_writer.py:48] [9200] global_step=9200, grad_norm=4.318873405456543, loss=2.032339334487915
I0531 02:55:28.753751 139495593936640 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.1569249629974365, loss=1.8663115501403809
I0531 02:56:44.255723 139495585543936 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.627162456512451, loss=1.890616774559021
I0531 02:57:59.207535 139495593936640 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.5755441188812256, loss=1.9724708795547485
I0531 02:59:14.442506 139495585543936 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.1411657333374023, loss=1.8968933820724487
I0531 03:00:33.417633 139495593936640 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.5942564010620117, loss=1.9204684495925903
I0531 03:01:57.454609 139495585543936 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.881330966949463, loss=1.9457138776779175
I0531 03:03:23.332266 139495593936640 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.57492733001709, loss=1.829408884048462
I0531 03:04:43.635099 139495585543936 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.5985331535339355, loss=1.8133492469787598
I0531 03:06:06.904389 139495593936640 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.337090969085693, loss=1.8276317119598389
I0531 03:07:33.068889 139495585543936 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.483483076095581, loss=1.9266653060913086
I0531 03:08:55.779631 139494974416640 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.1904168128967285, loss=1.8896907567977905
I0531 03:10:11.946492 139494966023936 logging_writer.py:48] [10400] global_step=10400, grad_norm=5.26108455657959, loss=1.861433982849121
I0531 03:11:27.385360 139494974416640 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.0567760467529297, loss=1.8739383220672607
I0531 03:12:42.543654 139494966023936 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.032465934753418, loss=1.8796130418777466
I0531 03:13:58.144701 139494974416640 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.7647757530212402, loss=1.8389114141464233
I0531 03:15:20.622632 139494966023936 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.9149951934814453, loss=1.8668497800827026
I0531 03:16:42.132977 139494974416640 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.85160231590271, loss=1.846814513206482
I0531 03:18:02.309215 139494966023936 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.7276577949523926, loss=1.797672986984253
I0531 03:19:25.391291 139494974416640 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.7568156719207764, loss=1.8304609060287476
I0531 03:20:51.041297 139494966023936 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.256099700927734, loss=1.7845743894577026
I0531 03:22:10.373260 139494974416640 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.9046401977539062, loss=1.7986221313476562
I0531 03:23:29.905621 139494974416640 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.364219665527344, loss=1.833621859550476
I0531 03:24:45.497690 139494966023936 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.4872491359710693, loss=1.7957664728164673
I0531 03:26:00.697524 139494974416640 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.156045436859131, loss=1.805010199546814
I0531 03:27:16.280004 139494966023936 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.865347862243652, loss=1.8374897241592407
I0531 03:28:33.076610 139494974416640 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.055013656616211, loss=1.8095155954360962
I0531 03:29:51.332931 139494966023936 logging_writer.py:48] [11900] global_step=11900, grad_norm=5.008918285369873, loss=1.794478178024292
I0531 03:31:14.939374 139494974416640 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.580874443054199, loss=1.7605692148208618
I0531 03:32:17.543429 139653397636928 spec.py:298] Evaluating on the training split.
I0531 03:33:06.255064 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 03:33:48.446193 139653397636928 spec.py:326] Evaluating on the test split.
I0531 03:34:10.138864 139653397636928 submission_runner.py:426] Time since start: 10306.81s, 	Step: 12075, 	{'train/ctc_loss': Array(0.49096847, dtype=float32), 'train/wer': 0.1639508215605986, 'validation/ctc_loss': Array(0.84410405, dtype=float32), 'validation/wer': 0.2413626759544231, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.53331095, dtype=float32), 'test/wer': 0.17289216582373612, 'test/num_examples': 2472, 'score': 9664.302209854126, 'total_duration': 10306.806659460068, 'accumulated_submission_time': 9664.302209854126, 'accumulated_data_selection_time': 2122.281770467758, 'accumulated_eval_time': 642.1907901763916, 'accumulated_logging_time': 0.14641928672790527}
I0531 03:34:10.164405 139495051216640 logging_writer.py:48] [12075] accumulated_data_selection_time=2122.281770, accumulated_eval_time=642.190790, accumulated_logging_time=0.146419, accumulated_submission_time=9664.302210, global_step=12075, preemption_count=0, score=9664.302210, test/ctc_loss=0.5333109498023987, test/num_examples=2472, test/wer=0.172892, total_duration=10306.806659, train/ctc_loss=0.4909684658050537, train/wer=0.163951, validation/ctc_loss=0.8441040515899658, validation/num_examples=5348, validation/wer=0.241363
I0531 03:34:30.138584 139495042823936 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.5873332023620605, loss=1.7549010515213013
I0531 03:35:45.922608 139495051216640 logging_writer.py:48] [12200] global_step=12200, grad_norm=3.3381364345550537, loss=1.763946294784546
I0531 03:37:01.926938 139495042823936 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.231253147125244, loss=1.7253559827804565
I0531 03:38:20.236236 139495593936640 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.714838981628418, loss=1.737338662147522
I0531 03:39:35.308403 139495585543936 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.004932165145874, loss=1.7584307193756104
I0531 03:40:52.475622 139495593936640 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.923306465148926, loss=1.7382404804229736
I0531 03:42:10.786896 139495585543936 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.2187986373901367, loss=1.6870696544647217
I0531 03:43:29.616798 139495593936640 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.709524393081665, loss=1.6790329217910767
I0531 03:44:49.653428 139495585543936 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.2151999473571777, loss=1.7058862447738647
I0531 03:46:10.248671 139495593936640 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.5661044120788574, loss=1.7336077690124512
I0531 03:47:37.708996 139495585543936 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.030311346054077, loss=1.7394154071807861
I0531 03:49:02.447474 139495593936640 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.39847469329834, loss=1.7499250173568726
I0531 03:50:27.326899 139495585543936 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.5211641788482666, loss=1.7313027381896973
I0531 03:51:53.969321 139495593936640 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.419789791107178, loss=1.7073965072631836
I0531 03:53:08.863422 139495585543936 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.4242732524871826, loss=1.6445518732070923
I0531 03:54:24.165828 139495593936640 logging_writer.py:48] [13600] global_step=13600, grad_norm=6.002748966217041, loss=1.7525893449783325
I0531 03:55:39.241312 139495585543936 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.329668045043945, loss=1.6174452304840088
I0531 03:56:55.999770 139495593936640 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.661545991897583, loss=1.656219482421875
I0531 03:58:17.147170 139495585543936 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.701900005340576, loss=1.7048072814941406
I0531 03:59:39.287348 139495593936640 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.9918718338012695, loss=1.7135188579559326
I0531 04:01:03.641060 139495585543936 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.249579429626465, loss=1.7210719585418701
I0531 04:02:24.355044 139495593936640 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.7687201499938965, loss=1.74971342086792
I0531 04:03:45.430756 139495585543936 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.759310007095337, loss=1.6744370460510254
I0531 04:05:09.737792 139495593936640 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.9510014057159424, loss=1.6562342643737793
I0531 04:06:30.671814 139495593936640 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.987555503845215, loss=1.7107806205749512
I0531 04:07:46.828407 139495585543936 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.077608585357666, loss=1.6726540327072144
I0531 04:09:03.011259 139495593936640 logging_writer.py:48] [14700] global_step=14700, grad_norm=6.841011047363281, loss=1.6343573331832886
I0531 04:10:20.125261 139495585543936 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.209988594055176, loss=1.6447746753692627
I0531 04:11:38.879360 139495593936640 logging_writer.py:48] [14900] global_step=14900, grad_norm=6.609997749328613, loss=1.7070952653884888
I0531 04:13:03.113611 139495585543936 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.146520614624023, loss=1.663557767868042
I0531 04:14:10.423616 139653397636928 spec.py:298] Evaluating on the training split.
I0531 04:14:57.526401 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 04:15:39.484547 139653397636928 spec.py:326] Evaluating on the test split.
I0531 04:16:02.027860 139653397636928 submission_runner.py:426] Time since start: 12818.70s, 	Step: 15081, 	{'train/ctc_loss': Array(0.41265416, dtype=float32), 'train/wer': 0.1424525429994129, 'validation/ctc_loss': Array(0.75794995, dtype=float32), 'validation/wer': 0.2190759196904939, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4576224, dtype=float32), 'test/wer': 0.14756362602319584, 'test/num_examples': 2472, 'score': 12064.50795340538, 'total_duration': 12818.695801973343, 'accumulated_submission_time': 12064.50795340538, 'accumulated_data_selection_time': 2691.5737109184265, 'accumulated_eval_time': 753.7909963130951, 'accumulated_logging_time': 0.18738937377929688}
I0531 04:16:02.052718 139495163856640 logging_writer.py:48] [15081] accumulated_data_selection_time=2691.573711, accumulated_eval_time=753.790996, accumulated_logging_time=0.187389, accumulated_submission_time=12064.507953, global_step=15081, preemption_count=0, score=12064.507953, test/ctc_loss=0.4576224088668823, test/num_examples=2472, test/wer=0.147564, total_duration=12818.695802, train/ctc_loss=0.41265416145324707, train/wer=0.142453, validation/ctc_loss=0.757949948310852, validation/num_examples=5348, validation/wer=0.219076
I0531 04:16:17.382734 139495155463936 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.7988057136535645, loss=1.6441736221313477
I0531 04:17:33.459044 139495163856640 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.8851139545440674, loss=1.6652824878692627
I0531 04:18:48.850165 139495155463936 logging_writer.py:48] [15300] global_step=15300, grad_norm=6.698634624481201, loss=1.6528682708740234
I0531 04:20:03.651202 139495163856640 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.155346393585205, loss=1.5736383199691772
I0531 04:21:25.211342 139495163856640 logging_writer.py:48] [15500] global_step=15500, grad_norm=6.344202518463135, loss=1.6273518800735474
I0531 04:22:41.015037 139495155463936 logging_writer.py:48] [15600] global_step=15600, grad_norm=8.62195110321045, loss=1.6670759916305542
I0531 04:23:56.388561 139495163856640 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.989497184753418, loss=1.648382544517517
I0531 04:25:14.162146 139495155463936 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.8147830963134766, loss=1.6026986837387085
I0531 04:26:35.234671 139495163856640 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.8968961238861084, loss=1.6553579568862915
I0531 04:27:54.466003 139653397636928 spec.py:298] Evaluating on the training split.
I0531 04:28:42.184043 139653397636928 spec.py:310] Evaluating on the validation split.
I0531 04:29:25.328915 139653397636928 spec.py:326] Evaluating on the test split.
I0531 04:29:47.604716 139653397636928 submission_runner.py:426] Time since start: 13644.27s, 	Step: 16000, 	{'train/ctc_loss': Array(0.4134784, dtype=float32), 'train/wer': 0.13984454604136548, 'validation/ctc_loss': Array(0.7356664, dtype=float32), 'validation/wer': 0.21256355584713793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44502452, dtype=float32), 'test/wer': 0.14413096906546422, 'test/num_examples': 2472, 'score': 12776.892210483551, 'total_duration': 13644.274249792099, 'accumulated_submission_time': 12776.892210483551, 'accumulated_data_selection_time': 2838.858940601349, 'accumulated_eval_time': 866.9272711277008, 'accumulated_logging_time': 0.2290058135986328}
I0531 04:29:47.623677 139495593936640 logging_writer.py:48] [16000] accumulated_data_selection_time=2838.858941, accumulated_eval_time=866.927271, accumulated_logging_time=0.229006, accumulated_submission_time=12776.892210, global_step=16000, preemption_count=0, score=12776.892210, test/ctc_loss=0.4450245201587677, test/num_examples=2472, test/wer=0.144131, total_duration=13644.274250, train/ctc_loss=0.41347840428352356, train/wer=0.139845, validation/ctc_loss=0.7356663942337036, validation/num_examples=5348, validation/wer=0.212564
I0531 04:29:47.644549 139495585543936 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12776.892210
I0531 04:29:47.794119 139653397636928 checkpoints.py:490] Saving checkpoint at step: 16000
I0531 04:29:48.612620 139653397636928 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0531 04:29:48.632627 139653397636928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_jax_upgrade_a/lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0531 04:29:49.775682 139653397636928 submission_runner.py:589] Tuning trial 1/1
I0531 04:29:49.775909 139653397636928 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0531 04:29:49.781894 139653397636928 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.852835, dtype=float32), 'train/wer': 3.2253919996668263, 'validation/ctc_loss': Array(29.565132, dtype=float32), 'validation/wer': 2.937780393443256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.723234, dtype=float32), 'test/wer': 3.0795401458371416, 'test/num_examples': 2472, 'score': 63.23236155509949, 'total_duration': 263.14890146255493, 'accumulated_submission_time': 63.23236155509949, 'accumulated_data_selection_time': 4.576021671295166, 'accumulated_eval_time': 199.91636323928833, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3036, {'train/ctc_loss': Array(5.9139705, dtype=float32), 'train/wer': 0.8879425758480772, 'validation/ctc_loss': Array(5.8611503, dtype=float32), 'validation/wer': 0.8531968470511052, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7323685, dtype=float32), 'test/wer': 0.8537363150732232, 'test/num_examples': 2472, 'score': 2463.2427685260773, 'total_duration': 2765.780918598175, 'accumulated_submission_time': 2463.2427685260773, 'accumulated_data_selection_time': 455.2301621437073, 'accumulated_eval_time': 302.46022844314575, 'accumulated_logging_time': 0.03782081604003906, 'global_step': 3036, 'preemption_count': 0}), (6008, {'train/ctc_loss': Array(1.3196273, dtype=float32), 'train/wer': 0.37348107503396105, 'validation/ctc_loss': Array(1.7256984, dtype=float32), 'validation/wer': 0.4295458711613233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.315635, dtype=float32), 'test/wer': 0.3630085511750249, 'test/num_examples': 2472, 'score': 4863.857192277908, 'total_duration': 5280.0614449977875, 'accumulated_submission_time': 4863.857192277908, 'accumulated_data_selection_time': 1034.3520748615265, 'accumulated_eval_time': 416.0482099056244, 'accumulated_logging_time': 0.07343912124633789, 'global_step': 6008, 'preemption_count': 0}), (9052, {'train/ctc_loss': Array(0.6146764, dtype=float32), 'train/wer': 0.2055442225279504, 'validation/ctc_loss': Array(1.0068036, dtype=float32), 'validation/wer': 0.28274271821242847, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68008256, dtype=float32), 'test/wer': 0.21337314402941115, 'test/num_examples': 2472, 'score': 7263.859972953796, 'total_duration': 7793.694908618927, 'accumulated_submission_time': 7263.859972953796, 'accumulated_data_selection_time': 1563.9134638309479, 'accumulated_eval_time': 529.5995516777039, 'accumulated_logging_time': 0.11121010780334473, 'global_step': 9052, 'preemption_count': 0}), (12075, {'train/ctc_loss': Array(0.49096847, dtype=float32), 'train/wer': 0.1639508215605986, 'validation/ctc_loss': Array(0.84410405, dtype=float32), 'validation/wer': 0.2413626759544231, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.53331095, dtype=float32), 'test/wer': 0.17289216582373612, 'test/num_examples': 2472, 'score': 9664.302209854126, 'total_duration': 10306.806659460068, 'accumulated_submission_time': 9664.302209854126, 'accumulated_data_selection_time': 2122.281770467758, 'accumulated_eval_time': 642.1907901763916, 'accumulated_logging_time': 0.14641928672790527, 'global_step': 12075, 'preemption_count': 0}), (15081, {'train/ctc_loss': Array(0.41265416, dtype=float32), 'train/wer': 0.1424525429994129, 'validation/ctc_loss': Array(0.75794995, dtype=float32), 'validation/wer': 0.2190759196904939, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4576224, dtype=float32), 'test/wer': 0.14756362602319584, 'test/num_examples': 2472, 'score': 12064.50795340538, 'total_duration': 12818.695801973343, 'accumulated_submission_time': 12064.50795340538, 'accumulated_data_selection_time': 2691.5737109184265, 'accumulated_eval_time': 753.7909963130951, 'accumulated_logging_time': 0.18738937377929688, 'global_step': 15081, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.4134784, dtype=float32), 'train/wer': 0.13984454604136548, 'validation/ctc_loss': Array(0.7356664, dtype=float32), 'validation/wer': 0.21256355584713793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44502452, dtype=float32), 'test/wer': 0.14413096906546422, 'test/num_examples': 2472, 'score': 12776.892210483551, 'total_duration': 13644.274249792099, 'accumulated_submission_time': 12776.892210483551, 'accumulated_data_selection_time': 2838.858940601349, 'accumulated_eval_time': 866.9272711277008, 'accumulated_logging_time': 0.2290058135986328, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0531 04:29:49.782071 139653397636928 submission_runner.py:592] Timing: 12776.892210483551
I0531 04:29:49.782142 139653397636928 submission_runner.py:593] ====================
I0531 04:29:49.783392 139653397636928 submission_runner.py:661] Final librispeech_deepspeech score: 12776.892210483551
