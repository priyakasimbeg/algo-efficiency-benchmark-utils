python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_shampoo --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_jax_05-02-2023-18-21-14.log
I0502 18:21:33.843493 140477875701568 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax.
I0502 18:21:34.041198 140477875701568 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 18:21:34.836759 140477875701568 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0502 18:21:34.838090 140477875701568 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 18:21:34.843711 140477875701568 submission_runner.py:538] Using RNG seed 1564679786
I0502 18:21:37.535897 140477875701568 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 18:21:37.536119 140477875701568 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1.
I0502 18:21:37.536288 140477875701568 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1/hparams.json.
I0502 18:21:37.671562 140477875701568 submission_runner.py:241] Initializing dataset.
I0502 18:21:37.671780 140477875701568 submission_runner.py:248] Initializing model.
I0502 18:21:43.985894 140477875701568 submission_runner.py:258] Initializing optimizer.
I0502 18:21:52.488685 140477875701568 submission_runner.py:265] Initializing metrics bundle.
I0502 18:21:52.488889 140477875701568 submission_runner.py:282] Initializing checkpoint and logger.
I0502 18:21:52.493668 140477875701568 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1 with prefix checkpoint_
I0502 18:21:52.493916 140477875701568 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 18:21:52.493981 140477875701568 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 18:21:53.240799 140477875701568 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1/meta_data_0.json.
I0502 18:21:53.242054 140477875701568 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1/flags_0.json.
I0502 18:21:53.303551 140477875701568 submission_runner.py:318] Starting training loop.
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:812: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:813: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:814: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
I0502 18:22:26.687883 140301291210496 logging_writer.py:48] [0] global_step=0, grad_norm=8.417272567749023, loss=1.4900671243667603
I0502 18:22:26.700530 140477875701568 spec.py:298] Evaluating on the training split.
I0502 18:27:17.860009 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 18:32:11.809191 140477875701568 spec.py:326] Evaluating on the test split.
I0502 18:37:06.121286 140477875701568 submission_runner.py:415] Time since start: 912.82s, 	Step: 1, 	{'train/loss': 1.48920131343704, 'validation/loss': 1.4903811235955056, 'validation/num_examples': 89000000, 'test/loss': 1.4900243615664324, 'test/num_examples': 89274637, 'score': 33.39673566818237, 'total_duration': 912.81764793396, 'accumulated_submission_time': 33.39673566818237, 'accumulated_eval_time': 879.4206545352936, 'accumulated_logging_time': 0}
I0502 18:37:06.137154 140288320104192 logging_writer.py:48] [1] accumulated_eval_time=879.420655, accumulated_logging_time=0, accumulated_submission_time=33.396736, global_step=1, preemption_count=0, score=33.396736, test/loss=1.490024, test/num_examples=89274637, total_duration=912.817648, train/loss=1.489201, validation/loss=1.490381, validation/num_examples=89000000
I0502 18:38:11.973607 140288311711488 logging_writer.py:48] [100] global_step=100, grad_norm=0.3105242848396301, loss=0.13523215055465698
I0502 18:39:07.051359 140477875701568 spec.py:298] Evaluating on the training split.
I0502 18:43:50.992287 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 18:48:37.862389 140477875701568 spec.py:326] Evaluating on the test split.
I0502 18:53:03.570315 140477875701568 submission_runner.py:415] Time since start: 1870.27s, 	Step: 165, 	{'train/loss': 0.14228361577475396, 'validation/loss': 0.14461401123595505, 'validation/num_examples': 89000000, 'test/loss': 0.15042327195348887, 'test/num_examples': 89274637, 'score': 154.301353931427, 'total_duration': 1870.2666800022125, 'accumulated_submission_time': 154.301353931427, 'accumulated_eval_time': 1715.9395360946655, 'accumulated_logging_time': 0.023140907287597656}
I0502 18:53:03.579975 140288320104192 logging_writer.py:48] [165] accumulated_eval_time=1715.939536, accumulated_logging_time=0.023141, accumulated_submission_time=154.301354, global_step=165, preemption_count=0, score=154.301354, test/loss=0.150423, test/num_examples=89274637, total_duration=1870.266680, train/loss=0.142284, validation/loss=0.144614, validation/num_examples=89000000
I0502 18:53:14.305953 140288311711488 logging_writer.py:48] [200] global_step=200, grad_norm=0.21483883261680603, loss=0.12963180243968964
I0502 18:54:39.896985 140288320104192 logging_writer.py:48] [300] global_step=300, grad_norm=1.2697919607162476, loss=0.1612263172864914
I0502 18:55:03.729589 140477875701568 spec.py:298] Evaluating on the training split.
I0502 18:59:52.932512 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 19:04:39.024942 140477875701568 spec.py:326] Evaluating on the test split.
I0502 19:09:28.211224 140477875701568 submission_runner.py:415] Time since start: 2854.91s, 	Step: 329, 	{'train/loss': 0.13580153846319673, 'validation/loss': 0.13328583146067416, 'validation/num_examples': 89000000, 'test/loss': 0.13653128603592082, 'test/num_examples': 89274637, 'score': 274.44173216819763, 'total_duration': 2854.907590866089, 'accumulated_submission_time': 274.44173216819763, 'accumulated_eval_time': 2580.421100616455, 'accumulated_logging_time': 0.0397031307220459}
I0502 19:09:28.221277 140288311711488 logging_writer.py:48] [329] accumulated_eval_time=2580.421101, accumulated_logging_time=0.039703, accumulated_submission_time=274.441732, global_step=329, preemption_count=0, score=274.441732, test/loss=0.136531, test/num_examples=89274637, total_duration=2854.907591, train/loss=0.135802, validation/loss=0.133286, validation/num_examples=89000000
I0502 19:10:09.573655 140288320104192 logging_writer.py:48] [400] global_step=400, grad_norm=1.6185505390167236, loss=0.1847476065158844
I0502 19:11:28.594819 140477875701568 spec.py:298] Evaluating on the training split.
I0502 19:16:13.123259 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 19:21:00.474385 140477875701568 spec.py:326] Evaluating on the test split.
I0502 19:25:26.503096 140477875701568 submission_runner.py:415] Time since start: 3813.20s, 	Step: 493, 	{'train/loss': 0.18359263843258486, 'validation/loss': 0.18569386516853933, 'validation/num_examples': 89000000, 'test/loss': 0.19054533932185017, 'test/num_examples': 89274637, 'score': 394.806040763855, 'total_duration': 3813.199454307556, 'accumulated_submission_time': 394.806040763855, 'accumulated_eval_time': 3418.329308986664, 'accumulated_logging_time': 0.05670881271362305}
I0502 19:25:26.511708 140288311711488 logging_writer.py:48] [493] accumulated_eval_time=3418.329309, accumulated_logging_time=0.056709, accumulated_submission_time=394.806041, global_step=493, preemption_count=0, score=394.806041, test/loss=0.190545, test/num_examples=89274637, total_duration=3813.199454, train/loss=0.183593, validation/loss=0.185694, validation/num_examples=89000000
I0502 19:25:27.843688 140288320104192 logging_writer.py:48] [500] global_step=500, grad_norm=0.5294861197471619, loss=0.1615445762872696
I0502 19:26:39.509427 140288311711488 logging_writer.py:48] [600] global_step=600, grad_norm=0.09114347398281097, loss=0.1335044801235199
I0502 19:27:26.587980 140477875701568 spec.py:298] Evaluating on the training split.
I0502 19:32:11.123890 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 19:36:56.123142 140477875701568 spec.py:326] Evaluating on the test split.
I0502 19:41:25.112525 140477875701568 submission_runner.py:415] Time since start: 4771.81s, 	Step: 656, 	{'train/loss': 0.1326060291596245, 'validation/loss': 0.13388393258426967, 'validation/num_examples': 89000000, 'test/loss': 0.13691156201508833, 'test/num_examples': 89274637, 'score': 514.8729293346405, 'total_duration': 4771.808862924576, 'accumulated_submission_time': 514.8729293346405, 'accumulated_eval_time': 4256.853763580322, 'accumulated_logging_time': 0.07243800163269043}
I0502 19:41:25.123830 140288320104192 logging_writer.py:48] [656] accumulated_eval_time=4256.853764, accumulated_logging_time=0.072438, accumulated_submission_time=514.872929, global_step=656, preemption_count=0, score=514.872929, test/loss=0.136912, test/num_examples=89274637, total_duration=4771.808863, train/loss=0.132606, validation/loss=0.133884, validation/num_examples=89000000
I0502 19:41:43.671523 140288311711488 logging_writer.py:48] [700] global_step=700, grad_norm=0.05324767529964447, loss=0.13172776997089386
I0502 19:43:09.911585 140288320104192 logging_writer.py:48] [800] global_step=800, grad_norm=0.23965363204479218, loss=0.1311807632446289
I0502 19:43:25.955236 140477875701568 spec.py:298] Evaluating on the training split.
I0502 19:48:11.026932 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 19:52:54.980340 140477875701568 spec.py:326] Evaluating on the test split.
I0502 19:57:48.740697 140477875701568 submission_runner.py:415] Time since start: 5755.44s, 	Step: 820, 	{'train/loss': 0.13185609167687062, 'validation/loss': 0.1323757415730337, 'validation/num_examples': 89000000, 'test/loss': 0.13498541584660825, 'test/num_examples': 89274637, 'score': 635.6938242912292, 'total_duration': 5755.437048196793, 'accumulated_submission_time': 635.6938242912292, 'accumulated_eval_time': 5119.639139175415, 'accumulated_logging_time': 0.09187602996826172}
I0502 19:57:48.750511 140288311711488 logging_writer.py:48] [820] accumulated_eval_time=5119.639139, accumulated_logging_time=0.091876, accumulated_submission_time=635.693824, global_step=820, preemption_count=0, score=635.693824, test/loss=0.134985, test/num_examples=89274637, total_duration=5755.437048, train/loss=0.131856, validation/loss=0.132376, validation/num_examples=89000000
I0502 19:58:39.081943 140288320104192 logging_writer.py:48] [900] global_step=900, grad_norm=0.596271812915802, loss=0.15627235174179077
I0502 19:59:49.510334 140477875701568 spec.py:298] Evaluating on the training split.
I0502 20:04:35.253607 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 20:09:20.248671 140477875701568 spec.py:326] Evaluating on the test split.
I0502 20:13:47.796649 140477875701568 submission_runner.py:415] Time since start: 6714.49s, 	Step: 983, 	{'train/loss': 0.13250111516036034, 'validation/loss': 0.13169340449438202, 'validation/num_examples': 89000000, 'test/loss': 0.1345446075574634, 'test/num_examples': 89274637, 'score': 756.4424571990967, 'total_duration': 6714.493009328842, 'accumulated_submission_time': 756.4424571990967, 'accumulated_eval_time': 5957.925377845764, 'accumulated_logging_time': 0.11056137084960938}
I0502 20:13:47.811334 140288311711488 logging_writer.py:48] [983] accumulated_eval_time=5957.925378, accumulated_logging_time=0.110561, accumulated_submission_time=756.442457, global_step=983, preemption_count=0, score=756.442457, test/loss=0.134545, test/num_examples=89274637, total_duration=6714.493009, train/loss=0.132501, validation/loss=0.131693, validation/num_examples=89000000
I0502 20:13:50.434751 140288320104192 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3154785633087158, loss=0.13211914896965027
I0502 20:15:09.906177 140288311711488 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.016930392012000084, loss=0.1279328465461731
I0502 20:15:48.394332 140477875701568 spec.py:298] Evaluating on the training split.
I0502 20:20:37.518005 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 20:25:22.320065 140477875701568 spec.py:326] Evaluating on the test split.
I0502 20:30:16.857044 140477875701568 submission_runner.py:415] Time since start: 7703.55s, 	Step: 1146, 	{'train/loss': 0.1284873911876552, 'validation/loss': 0.13168333707865168, 'validation/num_examples': 89000000, 'test/loss': 0.13448661796294956, 'test/num_examples': 89274637, 'score': 877.0161864757538, 'total_duration': 7703.553391933441, 'accumulated_submission_time': 877.0161864757538, 'accumulated_eval_time': 6826.388001203537, 'accumulated_logging_time': 0.1323227882385254}
I0502 20:30:16.865713 140288320104192 logging_writer.py:48] [1146] accumulated_eval_time=6826.388001, accumulated_logging_time=0.132323, accumulated_submission_time=877.016186, global_step=1146, preemption_count=0, score=877.016186, test/loss=0.134487, test/num_examples=89274637, total_duration=7703.553392, train/loss=0.128487, validation/loss=0.131683, validation/num_examples=89000000
I0502 20:30:43.923937 140288311711488 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.022210372611880302, loss=0.1254110038280487
I0502 20:32:10.491048 140288320104192 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.3467947840690613, loss=0.1473245471715927
I0502 20:32:16.992902 140477875701568 spec.py:298] Evaluating on the training split.
I0502 20:36:59.332874 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 20:41:44.684462 140477875701568 spec.py:326] Evaluating on the test split.
I0502 20:46:38.395863 140477875701568 submission_runner.py:415] Time since start: 8685.09s, 	Step: 1309, 	{'train/loss': 0.13185599062528744, 'validation/loss': 0.13148204494382024, 'validation/num_examples': 89000000, 'test/loss': 0.1343836995943204, 'test/num_examples': 89274637, 'score': 997.1328439712524, 'total_duration': 8685.092229366302, 'accumulated_submission_time': 997.1328439712524, 'accumulated_eval_time': 7687.790900230408, 'accumulated_logging_time': 0.1491706371307373}
I0502 20:46:38.404316 140288311711488 logging_writer.py:48] [1309] accumulated_eval_time=7687.790900, accumulated_logging_time=0.149171, accumulated_submission_time=997.132844, global_step=1309, preemption_count=0, score=997.132844, test/loss=0.134384, test/num_examples=89274637, total_duration=8685.092229, train/loss=0.131856, validation/loss=0.131482, validation/num_examples=89000000
I0502 20:47:38.113670 140288320104192 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.06403832882642746, loss=0.12696263194084167
I0502 20:48:38.509310 140477875701568 spec.py:298] Evaluating on the training split.
I0502 20:53:21.569263 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 20:58:07.914168 140477875701568 spec.py:326] Evaluating on the test split.
I0502 21:03:00.098101 140477875701568 submission_runner.py:415] Time since start: 9666.79s, 	Step: 1471, 	{'train/loss': 0.1288375910631956, 'validation/loss': 0.1315960224719101, 'validation/num_examples': 89000000, 'test/loss': 0.1343810448649598, 'test/num_examples': 89274637, 'score': 1117.2286396026611, 'total_duration': 9666.794464826584, 'accumulated_submission_time': 1117.2286396026611, 'accumulated_eval_time': 8549.379619598389, 'accumulated_logging_time': 0.16464710235595703}
I0502 21:03:00.109218 140288311711488 logging_writer.py:48] [1471] accumulated_eval_time=8549.379620, accumulated_logging_time=0.164647, accumulated_submission_time=1117.228640, global_step=1471, preemption_count=0, score=1117.228640, test/loss=0.134381, test/num_examples=89274637, total_duration=9666.794465, train/loss=0.128838, validation/loss=0.131596, validation/num_examples=89000000
I0502 21:03:05.688308 140288320104192 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.03479745239019394, loss=0.12715290486812592
I0502 21:04:31.332884 140477875701568 spec.py:298] Evaluating on the training split.
I0502 21:09:18.527527 140477875701568 spec.py:310] Evaluating on the validation split.
I0502 21:14:02.479479 140477875701568 spec.py:326] Evaluating on the test split.
I0502 21:18:30.137320 140477875701568 submission_runner.py:415] Time since start: 10596.83s, 	Step: 1600, 	{'train/loss': 0.12780320460170047, 'validation/loss': 0.12803405617977529, 'validation/num_examples': 89000000, 'test/loss': 0.13060123672079452, 'test/num_examples': 89274637, 'score': 1208.4426119327545, 'total_duration': 10596.833673238754, 'accumulated_submission_time': 1208.4426119327545, 'accumulated_eval_time': 9388.183984279633, 'accumulated_logging_time': 0.1835169792175293}
I0502 21:18:30.147428 140288311711488 logging_writer.py:48] [1600] accumulated_eval_time=9388.183984, accumulated_logging_time=0.183517, accumulated_submission_time=1208.442612, global_step=1600, preemption_count=0, score=1208.442612, test/loss=0.130601, test/num_examples=89274637, total_duration=10596.833673, train/loss=0.127803, validation/loss=0.128034, validation/num_examples=89000000
I0502 21:18:30.162192 140288320104192 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1208.442612
I0502 21:18:34.773343 140477875701568 checkpoints.py:356] Saving checkpoint at step: 1600
I0502 21:18:58.316133 140477875701568 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1/checkpoint_1600
I0502 21:18:58.488019 140477875701568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_shampoo/criteo1tb_jax/trial_1/checkpoint_1600.
I0502 21:18:58.717600 140477875701568 submission_runner.py:578] Tuning trial 1/1
I0502 21:18:58.717844 140477875701568 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0502 21:18:58.719849 140477875701568 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/loss': 1.48920131343704, 'validation/loss': 1.4903811235955056, 'validation/num_examples': 89000000, 'test/loss': 1.4900243615664324, 'test/num_examples': 89274637, 'score': 33.39673566818237, 'total_duration': 912.81764793396, 'accumulated_submission_time': 33.39673566818237, 'accumulated_eval_time': 879.4206545352936, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (165, {'train/loss': 0.14228361577475396, 'validation/loss': 0.14461401123595505, 'validation/num_examples': 89000000, 'test/loss': 0.15042327195348887, 'test/num_examples': 89274637, 'score': 154.301353931427, 'total_duration': 1870.2666800022125, 'accumulated_submission_time': 154.301353931427, 'accumulated_eval_time': 1715.9395360946655, 'accumulated_logging_time': 0.023140907287597656, 'global_step': 165, 'preemption_count': 0}), (329, {'train/loss': 0.13580153846319673, 'validation/loss': 0.13328583146067416, 'validation/num_examples': 89000000, 'test/loss': 0.13653128603592082, 'test/num_examples': 89274637, 'score': 274.44173216819763, 'total_duration': 2854.907590866089, 'accumulated_submission_time': 274.44173216819763, 'accumulated_eval_time': 2580.421100616455, 'accumulated_logging_time': 0.0397031307220459, 'global_step': 329, 'preemption_count': 0}), (493, {'train/loss': 0.18359263843258486, 'validation/loss': 0.18569386516853933, 'validation/num_examples': 89000000, 'test/loss': 0.19054533932185017, 'test/num_examples': 89274637, 'score': 394.806040763855, 'total_duration': 3813.199454307556, 'accumulated_submission_time': 394.806040763855, 'accumulated_eval_time': 3418.329308986664, 'accumulated_logging_time': 0.05670881271362305, 'global_step': 493, 'preemption_count': 0}), (656, {'train/loss': 0.1326060291596245, 'validation/loss': 0.13388393258426967, 'validation/num_examples': 89000000, 'test/loss': 0.13691156201508833, 'test/num_examples': 89274637, 'score': 514.8729293346405, 'total_duration': 4771.808862924576, 'accumulated_submission_time': 514.8729293346405, 'accumulated_eval_time': 4256.853763580322, 'accumulated_logging_time': 0.07243800163269043, 'global_step': 656, 'preemption_count': 0}), (820, {'train/loss': 0.13185609167687062, 'validation/loss': 0.1323757415730337, 'validation/num_examples': 89000000, 'test/loss': 0.13498541584660825, 'test/num_examples': 89274637, 'score': 635.6938242912292, 'total_duration': 5755.437048196793, 'accumulated_submission_time': 635.6938242912292, 'accumulated_eval_time': 5119.639139175415, 'accumulated_logging_time': 0.09187602996826172, 'global_step': 820, 'preemption_count': 0}), (983, {'train/loss': 0.13250111516036034, 'validation/loss': 0.13169340449438202, 'validation/num_examples': 89000000, 'test/loss': 0.1345446075574634, 'test/num_examples': 89274637, 'score': 756.4424571990967, 'total_duration': 6714.493009328842, 'accumulated_submission_time': 756.4424571990967, 'accumulated_eval_time': 5957.925377845764, 'accumulated_logging_time': 0.11056137084960938, 'global_step': 983, 'preemption_count': 0}), (1146, {'train/loss': 0.1284873911876552, 'validation/loss': 0.13168333707865168, 'validation/num_examples': 89000000, 'test/loss': 0.13448661796294956, 'test/num_examples': 89274637, 'score': 877.0161864757538, 'total_duration': 7703.553391933441, 'accumulated_submission_time': 877.0161864757538, 'accumulated_eval_time': 6826.388001203537, 'accumulated_logging_time': 0.1323227882385254, 'global_step': 1146, 'preemption_count': 0}), (1309, {'train/loss': 0.13185599062528744, 'validation/loss': 0.13148204494382024, 'validation/num_examples': 89000000, 'test/loss': 0.1343836995943204, 'test/num_examples': 89274637, 'score': 997.1328439712524, 'total_duration': 8685.092229366302, 'accumulated_submission_time': 997.1328439712524, 'accumulated_eval_time': 7687.790900230408, 'accumulated_logging_time': 0.1491706371307373, 'global_step': 1309, 'preemption_count': 0}), (1471, {'train/loss': 0.1288375910631956, 'validation/loss': 0.1315960224719101, 'validation/num_examples': 89000000, 'test/loss': 0.1343810448649598, 'test/num_examples': 89274637, 'score': 1117.2286396026611, 'total_duration': 9666.794464826584, 'accumulated_submission_time': 1117.2286396026611, 'accumulated_eval_time': 8549.379619598389, 'accumulated_logging_time': 0.16464710235595703, 'global_step': 1471, 'preemption_count': 0}), (1600, {'train/loss': 0.12780320460170047, 'validation/loss': 0.12803405617977529, 'validation/num_examples': 89000000, 'test/loss': 0.13060123672079452, 'test/num_examples': 89274637, 'score': 1208.4426119327545, 'total_duration': 10596.833673238754, 'accumulated_submission_time': 1208.4426119327545, 'accumulated_eval_time': 9388.183984279633, 'accumulated_logging_time': 0.1835169792175293, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0502 21:18:58.719980 140477875701568 submission_runner.py:581] Timing: 1208.4426119327545
I0502 21:18:58.720033 140477875701568 submission_runner.py:582] ====================
I0502 21:18:58.720152 140477875701568 submission_runner.py:645] Final criteo1tb score: 1208.4426119327545
