python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_shampoo --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_04-28-2023-06-05-58.log
I0428 06:06:21.011287 140341517031232 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax.
I0428 06:06:21.115618 140341517031232 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 06:06:22.207540 140341517031232 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0428 06:06:22.208304 140341517031232 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 06:06:22.213392 140341517031232 submission_runner.py:538] Using RNG seed 2748450639
I0428 06:06:25.279045 140341517031232 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 06:06:25.279247 140341517031232 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1.
I0428 06:06:25.279432 140341517031232 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1/hparams.json.
I0428 06:06:25.414196 140341517031232 submission_runner.py:241] Initializing dataset.
I0428 06:06:25.431272 140341517031232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 06:06:25.446923 140341517031232 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 06:06:25.447039 140341517031232 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 06:06:25.729703 140341517031232 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 06:06:27.059654 140341517031232 submission_runner.py:248] Initializing model.
I0428 06:06:39.979811 140341517031232 submission_runner.py:258] Initializing optimizer.
I0428 06:06:56.317409 140341517031232 submission_runner.py:265] Initializing metrics bundle.
I0428 06:06:56.317625 140341517031232 submission_runner.py:282] Initializing checkpoint and logger.
I0428 06:06:56.318620 140341517031232 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0428 06:06:57.378183 140341517031232 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0428 06:06:57.379097 140341517031232 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1/flags_0.json.
I0428 06:06:57.383995 140341517031232 submission_runner.py:318] Starting training loop.
2023-04-28 06:06:59.912714: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-28 06:07:00.047507: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-28 06:07:00.407809: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-28 06:07:00.546124: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-04-28 06:07:01.010219: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:812: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:813: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:814: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
I0428 06:10:09.597196 140164668040960 logging_writer.py:48] [0] global_step=0, grad_norm=0.6271162033081055, loss=6.9321722984313965
I0428 06:10:09.668111 140341517031232 spec.py:298] Evaluating on the training split.
I0428 06:10:10.204781 140341517031232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 06:10:10.213880 140341517031232 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 06:10:10.214038 140341517031232 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 06:10:10.291366 140341517031232 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 06:10:23.081903 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 06:10:24.049383 140341517031232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 06:10:24.062325 140341517031232 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 06:10:24.062707 140341517031232 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 06:10:24.135182 140341517031232 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 06:10:43.713508 140341517031232 spec.py:326] Evaluating on the test split.
I0428 06:10:44.218698 140341517031232 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 06:10:44.222726 140341517031232 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 06:10:44.256672 140341517031232 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 06:10:54.531174 140341517031232 submission_runner.py:415] Time since start: 237.15s, 	Step: 1, 	{'train/accuracy': 0.0006178252515383065, 'train/loss': 6.911553859710693, 'validation/accuracy': 0.0006599999614991248, 'validation/loss': 6.911994934082031, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911983966827393, 'test/num_examples': 10000, 'score': 192.28390622138977, 'total_duration': 237.14711570739746, 'accumulated_submission_time': 192.28390622138977, 'accumulated_eval_time': 44.86303496360779, 'accumulated_logging_time': 0}
I0428 06:10:54.547671 140136612337408 logging_writer.py:48] [1] accumulated_eval_time=44.863035, accumulated_logging_time=0, accumulated_submission_time=192.283906, global_step=1, preemption_count=0, score=192.283906, test/accuracy=0.001100, test/loss=6.911984, test/num_examples=10000, total_duration=237.147116, train/accuracy=0.000618, train/loss=6.911554, validation/accuracy=0.000660, validation/loss=6.911995, validation/num_examples=50000
I0428 06:12:07.285460 140136620730112 logging_writer.py:48] [100] global_step=100, grad_norm=0.6418828964233398, loss=6.730418682098389
I0428 06:13:20.737198 140136612337408 logging_writer.py:48] [200] global_step=200, grad_norm=0.7471106648445129, loss=6.531740188598633
I0428 06:14:34.577047 140136620730112 logging_writer.py:48] [300] global_step=300, grad_norm=0.7964208126068115, loss=6.406800270080566
I0428 06:16:00.738188 140136612337408 logging_writer.py:48] [400] global_step=400, grad_norm=0.9061806201934814, loss=6.161155700683594
I0428 06:17:12.755009 140136620730112 logging_writer.py:48] [500] global_step=500, grad_norm=1.196471095085144, loss=5.986813068389893
I0428 06:18:24.960908 140136612337408 logging_writer.py:48] [600] global_step=600, grad_norm=2.151811122894287, loss=5.856710433959961
I0428 06:19:24.981574 140341517031232 spec.py:298] Evaluating on the training split.
I0428 06:19:32.270177 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 06:19:40.440244 140341517031232 spec.py:326] Evaluating on the test split.
I0428 06:19:42.854217 140341517031232 submission_runner.py:415] Time since start: 765.47s, 	Step: 685, 	{'train/accuracy': 0.07647082209587097, 'train/loss': 5.362525463104248, 'validation/accuracy': 0.0709799975156784, 'validation/loss': 5.4121527671813965, 'validation/num_examples': 50000, 'test/accuracy': 0.05040000379085541, 'test/loss': 5.63770866394043, 'test/num_examples': 10000, 'score': 702.7012600898743, 'total_duration': 765.4701640605927, 'accumulated_submission_time': 702.7012600898743, 'accumulated_eval_time': 62.7356481552124, 'accumulated_logging_time': 0.02470993995666504}
I0428 06:19:42.862904 140136947881728 logging_writer.py:48] [685] accumulated_eval_time=62.735648, accumulated_logging_time=0.024710, accumulated_submission_time=702.701260, global_step=685, preemption_count=0, score=702.701260, test/accuracy=0.050400, test/loss=5.637709, test/num_examples=10000, total_duration=765.470164, train/accuracy=0.076471, train/loss=5.362525, validation/accuracy=0.070980, validation/loss=5.412153, validation/num_examples=50000
I0428 06:19:55.072342 140136956274432 logging_writer.py:48] [700] global_step=700, grad_norm=2.016845226287842, loss=5.787229061126709
I0428 06:21:07.368593 140136947881728 logging_writer.py:48] [800] global_step=800, grad_norm=1.6482045650482178, loss=5.630189418792725
I0428 06:22:19.838429 140136956274432 logging_writer.py:48] [900] global_step=900, grad_norm=4.990213394165039, loss=5.527374744415283
I0428 06:23:32.391160 140136947881728 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.8808395862579346, loss=5.467544078826904
I0428 06:24:44.811462 140136956274432 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.7514169216156006, loss=5.303886890411377
I0428 06:25:57.396303 140136947881728 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.869095802307129, loss=5.175042629241943
I0428 06:27:09.815470 140136956274432 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.419640302658081, loss=5.141860008239746
I0428 06:28:13.340005 140341517031232 spec.py:298] Evaluating on the training split.
I0428 06:28:20.289325 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 06:28:28.196281 140341517031232 spec.py:326] Evaluating on the test split.
I0428 06:28:30.182810 140341517031232 submission_runner.py:415] Time since start: 1292.80s, 	Step: 1391, 	{'train/accuracy': 0.18955276906490326, 'train/loss': 4.2153520584106445, 'validation/accuracy': 0.15534000098705292, 'validation/loss': 4.425232887268066, 'validation/num_examples': 50000, 'test/accuracy': 0.11100000888109207, 'test/loss': 4.853227138519287, 'test/num_examples': 10000, 'score': 1213.1606333255768, 'total_duration': 1292.7987296581268, 'accumulated_submission_time': 1213.1606333255768, 'accumulated_eval_time': 79.57839441299438, 'accumulated_logging_time': 0.04262542724609375}
I0428 06:28:30.192870 140136947881728 logging_writer.py:48] [1391] accumulated_eval_time=79.578394, accumulated_logging_time=0.042625, accumulated_submission_time=1213.160633, global_step=1391, preemption_count=0, score=1213.160633, test/accuracy=0.111000, test/loss=4.853227, test/num_examples=10000, total_duration=1292.798730, train/accuracy=0.189553, train/loss=4.215352, validation/accuracy=0.155340, validation/loss=4.425233, validation/num_examples=50000
I0428 06:28:39.281128 140136956274432 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.5704152584075928, loss=5.153499603271484
I0428 06:29:51.983767 140136947881728 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.6605072021484375, loss=4.993954181671143
I0428 06:31:04.535867 140136956274432 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.5157103538513184, loss=4.893569469451904
I0428 06:32:18.886219 140136947881728 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.2249093055725098, loss=4.751834392547607
I0428 06:33:31.843501 140136956274432 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.767122268676758, loss=4.819388389587402
I0428 06:34:44.482253 140136947881728 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.0532379150390625, loss=4.621708869934082
I0428 06:35:57.460223 140136956274432 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.2868998050689697, loss=4.553208351135254
I0428 06:37:00.398893 140341517031232 spec.py:298] Evaluating on the training split.
I0428 06:37:07.578466 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 06:37:15.503913 140341517031232 spec.py:326] Evaluating on the test split.
I0428 06:37:17.900719 140341517031232 submission_runner.py:415] Time since start: 1820.52s, 	Step: 2090, 	{'train/accuracy': 0.2664819657802582, 'train/loss': 3.6139535903930664, 'validation/accuracy': 0.2421799898147583, 'validation/loss': 3.7521095275878906, 'validation/num_examples': 50000, 'test/accuracy': 0.17470000684261322, 'test/loss': 4.296182632446289, 'test/num_examples': 10000, 'score': 1723.3490097522736, 'total_duration': 1820.516660451889, 'accumulated_submission_time': 1723.3490097522736, 'accumulated_eval_time': 97.0801854133606, 'accumulated_logging_time': 0.06169009208679199}
I0428 06:37:17.909298 140136947881728 logging_writer.py:48] [2090] accumulated_eval_time=97.080185, accumulated_logging_time=0.061690, accumulated_submission_time=1723.349010, global_step=2090, preemption_count=0, score=1723.349010, test/accuracy=0.174700, test/loss=4.296183, test/num_examples=10000, total_duration=1820.516660, train/accuracy=0.266482, train/loss=3.613954, validation/accuracy=0.242180, validation/loss=3.752110, validation/num_examples=50000
I0428 06:37:27.318181 140136956274432 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.9930596351623535, loss=4.4585723876953125
I0428 06:38:40.831171 140136947881728 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.887261390686035, loss=4.486663818359375
I0428 06:39:54.416225 140136956274432 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.438889980316162, loss=4.420156002044678
I0428 06:41:07.366943 140136947881728 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.080998182296753, loss=4.411001682281494
I0428 06:42:20.318904 140136956274432 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.3510100841522217, loss=4.299649715423584
I0428 06:43:33.402873 140136947881728 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.908750295639038, loss=4.340094566345215
I0428 06:44:46.185725 140136956274432 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.000031471252441, loss=4.225432395935059
I0428 06:45:48.316285 140341517031232 spec.py:298] Evaluating on the training split.
I0428 06:45:55.439018 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 06:46:03.476265 140341517031232 spec.py:326] Evaluating on the test split.
I0428 06:46:05.786363 140341517031232 submission_runner.py:415] Time since start: 2348.40s, 	Step: 2788, 	{'train/accuracy': 0.36680883169174194, 'train/loss': 2.9988722801208496, 'validation/accuracy': 0.3142800033092499, 'validation/loss': 3.3019864559173584, 'validation/num_examples': 50000, 'test/accuracy': 0.23070001602172852, 'test/loss': 3.926806926727295, 'test/num_examples': 10000, 'score': 2233.7391760349274, 'total_duration': 2348.4022886753082, 'accumulated_submission_time': 2233.7391760349274, 'accumulated_eval_time': 114.55021142959595, 'accumulated_logging_time': 0.07839298248291016}
I0428 06:46:05.796641 140136947881728 logging_writer.py:48] [2788] accumulated_eval_time=114.550211, accumulated_logging_time=0.078393, accumulated_submission_time=2233.739176, global_step=2788, preemption_count=0, score=2233.739176, test/accuracy=0.230700, test/loss=3.926807, test/num_examples=10000, total_duration=2348.402289, train/accuracy=0.366809, train/loss=2.998872, validation/accuracy=0.314280, validation/loss=3.301986, validation/num_examples=50000
I0428 06:46:16.344481 140136956274432 logging_writer.py:48] [2800] global_step=2800, grad_norm=6.570868015289307, loss=4.266335964202881
I0428 06:47:29.812265 140136947881728 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.67637038230896, loss=4.071163654327393
I0428 06:48:42.764062 140136956274432 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.997277021408081, loss=4.015023231506348
I0428 06:49:55.644101 140136947881728 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.213735580444336, loss=4.049720764160156
I0428 06:51:08.472946 140136956274432 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.453247308731079, loss=3.9829607009887695
I0428 06:52:21.387712 140136947881728 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.838996410369873, loss=3.856694459915161
I0428 06:53:34.514498 140136956274432 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.509195566177368, loss=3.961392402648926
I0428 06:54:36.314260 140341517031232 spec.py:298] Evaluating on the training split.
I0428 06:54:43.098551 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 06:54:51.249733 140341517031232 spec.py:326] Evaluating on the test split.
I0428 06:54:53.569066 140341517031232 submission_runner.py:415] Time since start: 2876.19s, 	Step: 3487, 	{'train/accuracy': 0.41097337007522583, 'train/loss': 2.727480173110962, 'validation/accuracy': 0.3790600001811981, 'validation/loss': 2.881439208984375, 'validation/num_examples': 50000, 'test/accuracy': 0.28760001063346863, 'test/loss': 3.528773307800293, 'test/num_examples': 10000, 'score': 2744.2396976947784, 'total_duration': 2876.1850051879883, 'accumulated_submission_time': 2744.2396976947784, 'accumulated_eval_time': 131.80496883392334, 'accumulated_logging_time': 0.09726190567016602}
I0428 06:54:53.578517 140136947881728 logging_writer.py:48] [3487] accumulated_eval_time=131.804969, accumulated_logging_time=0.097262, accumulated_submission_time=2744.239698, global_step=3487, preemption_count=0, score=2744.239698, test/accuracy=0.287600, test/loss=3.528773, test/num_examples=10000, total_duration=2876.185005, train/accuracy=0.410973, train/loss=2.727480, validation/accuracy=0.379060, validation/loss=2.881439, validation/num_examples=50000
I0428 06:55:04.594316 140136956274432 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.136806488037109, loss=3.807378053665161
I0428 06:56:17.765705 140136947881728 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.4086596965789795, loss=3.9106345176696777
I0428 06:57:31.399307 140136956274432 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.670642137527466, loss=3.701054096221924
I0428 06:58:44.794037 140136947881728 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.365492820739746, loss=3.759641170501709
I0428 06:59:58.609009 140136956274432 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.6033294200897217, loss=3.6089181900024414
I0428 07:01:12.106466 140136947881728 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.998703956604004, loss=3.650592803955078
I0428 07:02:25.552965 140136956274432 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.8687551021575928, loss=3.610973834991455
I0428 07:03:24.315060 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:03:31.201983 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:03:39.307155 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:03:41.621195 140341517031232 submission_runner.py:415] Time since start: 3404.24s, 	Step: 4181, 	{'train/accuracy': 0.49396124482154846, 'train/loss': 2.3393497467041016, 'validation/accuracy': 0.4331199824810028, 'validation/loss': 2.6353554725646973, 'validation/num_examples': 50000, 'test/accuracy': 0.32340002059936523, 'test/loss': 3.274982452392578, 'test/num_examples': 10000, 'score': 3254.9596230983734, 'total_duration': 3404.2371418476105, 'accumulated_submission_time': 3254.9596230983734, 'accumulated_eval_time': 149.11107110977173, 'accumulated_logging_time': 0.11504602432250977}
I0428 07:03:41.633315 140136947881728 logging_writer.py:48] [4181] accumulated_eval_time=149.111071, accumulated_logging_time=0.115046, accumulated_submission_time=3254.959623, global_step=4181, preemption_count=0, score=3254.959623, test/accuracy=0.323400, test/loss=3.274982, test/num_examples=10000, total_duration=3404.237142, train/accuracy=0.493961, train/loss=2.339350, validation/accuracy=0.433120, validation/loss=2.635355, validation/num_examples=50000
I0428 07:03:56.264508 140136956274432 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.9481968879699707, loss=3.6174821853637695
I0428 07:05:09.441965 140136947881728 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.2700300216674805, loss=3.583022356033325
I0428 07:06:23.164347 140136956274432 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.5755395889282227, loss=3.585707426071167
I0428 07:07:36.870397 140136947881728 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.362222671508789, loss=3.5461997985839844
I0428 07:08:49.949907 140136956274432 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.412374258041382, loss=3.5310585498809814
I0428 07:10:03.345597 140136947881728 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.1464312076568604, loss=3.514380931854248
I0428 07:11:16.384426 140136956274432 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.8373243808746338, loss=3.4233334064483643
I0428 07:12:14.851631 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:12:21.725799 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:12:30.067455 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:12:32.151216 140341517031232 submission_runner.py:415] Time since start: 3934.77s, 	Step: 4881, 	{'train/accuracy': 0.5005978941917419, 'train/loss': 2.2538259029388428, 'validation/accuracy': 0.46803998947143555, 'validation/loss': 2.4405150413513184, 'validation/num_examples': 50000, 'test/accuracy': 0.35610002279281616, 'test/loss': 3.1309781074523926, 'test/num_examples': 10000, 'score': 3768.1555137634277, 'total_duration': 3934.767132997513, 'accumulated_submission_time': 3768.1555137634277, 'accumulated_eval_time': 166.41059565544128, 'accumulated_logging_time': 0.13941383361816406}
I0428 07:12:32.161500 140136947881728 logging_writer.py:48] [4881] accumulated_eval_time=166.410596, accumulated_logging_time=0.139414, accumulated_submission_time=3768.155514, global_step=4881, preemption_count=0, score=3768.155514, test/accuracy=0.356100, test/loss=3.130978, test/num_examples=10000, total_duration=3934.767133, train/accuracy=0.500598, train/loss=2.253826, validation/accuracy=0.468040, validation/loss=2.440515, validation/num_examples=50000
I0428 07:12:47.161223 140136956274432 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.1060125827789307, loss=3.5625081062316895
I0428 07:14:00.419602 140136947881728 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.237820863723755, loss=3.303802013397217
I0428 07:15:13.484648 140136956274432 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.0525875091552734, loss=3.3929476737976074
I0428 07:16:26.875574 140136947881728 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.4579904079437256, loss=3.3669142723083496
I0428 07:17:40.295094 140136956274432 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6534619331359863, loss=3.278634548187256
I0428 07:18:53.390629 140136947881728 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.100553035736084, loss=3.359631061553955
I0428 07:20:06.602039 140136956274432 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.373093366622925, loss=3.2313411235809326
I0428 07:21:05.450970 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:21:12.293452 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:21:20.287187 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:21:22.605932 140341517031232 submission_runner.py:415] Time since start: 4465.22s, 	Step: 5581, 	{'train/accuracy': 0.5597097873687744, 'train/loss': 2.0040760040283203, 'validation/accuracy': 0.5027799606323242, 'validation/loss': 2.286299228668213, 'validation/num_examples': 50000, 'test/accuracy': 0.38420000672340393, 'test/loss': 2.9613351821899414, 'test/num_examples': 10000, 'score': 4281.427541732788, 'total_duration': 4465.221879482269, 'accumulated_submission_time': 4281.427541732788, 'accumulated_eval_time': 183.5655288696289, 'accumulated_logging_time': 0.15836215019226074}
I0428 07:21:22.615535 140136947881728 logging_writer.py:48] [5581] accumulated_eval_time=183.565529, accumulated_logging_time=0.158362, accumulated_submission_time=4281.427542, global_step=5581, preemption_count=0, score=4281.427542, test/accuracy=0.384200, test/loss=2.961335, test/num_examples=10000, total_duration=4465.221879, train/accuracy=0.559710, train/loss=2.004076, validation/accuracy=0.502780, validation/loss=2.286299, validation/num_examples=50000
I0428 07:21:37.305787 140136956274432 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.6183500289916992, loss=3.3193674087524414
I0428 07:22:51.053099 140136947881728 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.5183640718460083, loss=3.225999593734741
I0428 07:24:04.986322 140136956274432 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.05312442779541, loss=3.206247329711914
I0428 07:25:18.770453 140136947881728 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.322249174118042, loss=3.246983051300049
I0428 07:26:32.507208 140136956274432 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.3538079261779785, loss=3.267381191253662
I0428 07:27:45.802215 140136947881728 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.1453521251678467, loss=3.1079602241516113
I0428 07:28:59.278366 140136956274432 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.9059090614318848, loss=3.2027153968811035
I0428 07:29:52.778955 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:29:59.545171 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:30:08.856447 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:30:11.230218 140341517031232 submission_runner.py:415] Time since start: 4993.85s, 	Step: 6278, 	{'train/accuracy': 0.5601682066917419, 'train/loss': 1.9762367010116577, 'validation/accuracy': 0.5191999673843384, 'validation/loss': 2.1719493865966797, 'validation/num_examples': 50000, 'test/accuracy': 0.3946000039577484, 'test/loss': 2.8906848430633545, 'test/num_examples': 10000, 'score': 4791.572980880737, 'total_duration': 4993.846155643463, 'accumulated_submission_time': 4791.572980880737, 'accumulated_eval_time': 202.01675128936768, 'accumulated_logging_time': 0.17748260498046875}
I0428 07:30:11.239930 140136947881728 logging_writer.py:48] [6278] accumulated_eval_time=202.016751, accumulated_logging_time=0.177483, accumulated_submission_time=4791.572981, global_step=6278, preemption_count=0, score=4791.572981, test/accuracy=0.394600, test/loss=2.890685, test/num_examples=10000, total_duration=4993.846156, train/accuracy=0.560168, train/loss=1.976237, validation/accuracy=0.519200, validation/loss=2.171949, validation/num_examples=50000
I0428 07:30:30.929477 140136956274432 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.1767990589141846, loss=3.1960701942443848
I0428 07:31:44.481812 140136947881728 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.0033106803894043, loss=3.1488051414489746
I0428 07:32:57.884638 140136956274432 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.600627064704895, loss=3.2568724155426025
I0428 07:34:11.630928 140136947881728 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.6438432931900024, loss=3.0396485328674316
I0428 07:35:25.917447 140136956274432 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.5986608266830444, loss=3.1331987380981445
I0428 07:36:39.871037 140136947881728 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.4299627542495728, loss=3.1251161098480225
I0428 07:37:54.057063 140136956274432 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.8479042053222656, loss=3.0265703201293945
I0428 07:38:41.338572 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:38:48.325550 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:38:58.215515 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:39:00.567742 140341517031232 submission_runner.py:415] Time since start: 5523.18s, 	Step: 6966, 	{'train/accuracy': 0.6094746589660645, 'train/loss': 1.7303013801574707, 'validation/accuracy': 0.5480200052261353, 'validation/loss': 2.0321598052978516, 'validation/num_examples': 50000, 'test/accuracy': 0.4252000153064728, 'test/loss': 2.728846549987793, 'test/num_examples': 10000, 'score': 5301.655446767807, 'total_duration': 5523.183669567108, 'accumulated_submission_time': 5301.655446767807, 'accumulated_eval_time': 221.2458667755127, 'accumulated_logging_time': 0.1949753761291504}
I0428 07:39:00.579488 140136947881728 logging_writer.py:48] [6966] accumulated_eval_time=221.245867, accumulated_logging_time=0.194975, accumulated_submission_time=5301.655447, global_step=6966, preemption_count=0, score=5301.655447, test/accuracy=0.425200, test/loss=2.728847, test/num_examples=10000, total_duration=5523.183670, train/accuracy=0.609475, train/loss=1.730301, validation/accuracy=0.548020, validation/loss=2.032160, validation/num_examples=50000
I0428 07:39:27.126989 140136956274432 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0115407705307007, loss=3.0916943550109863
I0428 07:40:41.292010 140136947881728 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.8772233724594116, loss=3.093623638153076
I0428 07:41:55.049136 140136956274432 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.1385012865066528, loss=3.2285280227661133
I0428 07:43:08.712390 140136947881728 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.6893364191055298, loss=3.073212146759033
I0428 07:44:22.583580 140136956274432 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.5304242372512817, loss=3.0909295082092285
I0428 07:45:36.701545 140136947881728 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.0201236009597778, loss=3.021273136138916
I0428 07:46:51.075055 140136956274432 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.3008686304092407, loss=3.053635597229004
I0428 07:47:31.119716 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:47:38.156657 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:47:48.339571 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:47:50.413360 140341517031232 submission_runner.py:415] Time since start: 6053.03s, 	Step: 7659, 	{'train/accuracy': 0.6038145422935486, 'train/loss': 1.7866847515106201, 'validation/accuracy': 0.5588200092315674, 'validation/loss': 1.996261477470398, 'validation/num_examples': 50000, 'test/accuracy': 0.43460002541542053, 'test/loss': 2.6921334266662598, 'test/num_examples': 10000, 'score': 5812.178817987442, 'total_duration': 6053.028200387955, 'accumulated_submission_time': 5812.178817987442, 'accumulated_eval_time': 240.53838419914246, 'accumulated_logging_time': 0.21535325050354004}
I0428 07:47:50.423724 140136947881728 logging_writer.py:48] [7659] accumulated_eval_time=240.538384, accumulated_logging_time=0.215353, accumulated_submission_time=5812.178818, global_step=7659, preemption_count=0, score=5812.178818, test/accuracy=0.434600, test/loss=2.692133, test/num_examples=10000, total_duration=6053.028200, train/accuracy=0.603815, train/loss=1.786685, validation/accuracy=0.558820, validation/loss=1.996261, validation/num_examples=50000
I0428 07:48:24.356286 140136956274432 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.341528296470642, loss=2.985755205154419
I0428 07:49:38.247565 140136947881728 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.505622386932373, loss=3.0566391944885254
I0428 07:50:52.100644 140136956274432 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0802032947540283, loss=3.006619930267334
I0428 07:52:06.162465 140136947881728 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.4947805404663086, loss=3.003556966781616
I0428 07:53:19.718843 140136956274432 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.4712367057800293, loss=3.017448902130127
I0428 07:54:33.929829 140136947881728 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0472115278244019, loss=2.941372871398926
I0428 07:55:48.080194 140136956274432 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.4982757568359375, loss=3.1119508743286133
I0428 07:56:20.697952 140341517031232 spec.py:298] Evaluating on the training split.
I0428 07:56:27.808850 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 07:56:38.040108 140341517031232 spec.py:326] Evaluating on the test split.
I0428 07:56:40.213044 140341517031232 submission_runner.py:415] Time since start: 6582.83s, 	Step: 8346, 	{'train/accuracy': 0.6458665132522583, 'train/loss': 1.5729056596755981, 'validation/accuracy': 0.5776000022888184, 'validation/loss': 1.8913501501083374, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.5710623264312744, 'test/num_examples': 10000, 'score': 6322.436324119568, 'total_duration': 6582.828968763351, 'accumulated_submission_time': 6322.436324119568, 'accumulated_eval_time': 260.0534234046936, 'accumulated_logging_time': 0.23428964614868164}
I0428 07:56:40.226096 140136947881728 logging_writer.py:48] [8346] accumulated_eval_time=260.053423, accumulated_logging_time=0.234290, accumulated_submission_time=6322.436324, global_step=8346, preemption_count=0, score=6322.436324, test/accuracy=0.456600, test/loss=2.571062, test/num_examples=10000, total_duration=6582.828969, train/accuracy=0.645867, train/loss=1.572906, validation/accuracy=0.577600, validation/loss=1.891350, validation/num_examples=50000
I0428 07:57:22.181542 140136956274432 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.4400514364242554, loss=2.8656444549560547
I0428 07:58:36.509957 140136947881728 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.3895118236541748, loss=2.9689505100250244
I0428 07:59:50.870030 140136956274432 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.3842209577560425, loss=2.9428021907806396
I0428 08:01:05.293724 140136947881728 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.5981495380401611, loss=2.825603485107422
I0428 08:02:19.695082 140136956274432 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.1907910108566284, loss=2.9204599857330322
I0428 08:03:34.082536 140136947881728 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0739802122116089, loss=2.9001317024230957
I0428 08:04:49.035390 140136956274432 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9374434947967529, loss=2.8793883323669434
I0428 08:05:10.288766 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:05:17.394692 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:05:27.812173 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:05:30.069701 140341517031232 submission_runner.py:415] Time since start: 7112.69s, 	Step: 9032, 	{'train/accuracy': 0.6338887214660645, 'train/loss': 1.587334156036377, 'validation/accuracy': 0.5816400051116943, 'validation/loss': 1.8372917175292969, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.510772943496704, 'test/num_examples': 10000, 'score': 6832.482260227203, 'total_duration': 7112.685649394989, 'accumulated_submission_time': 6832.482260227203, 'accumulated_eval_time': 279.8343243598938, 'accumulated_logging_time': 0.25576233863830566}
I0428 08:05:30.079949 140136947881728 logging_writer.py:48] [9032] accumulated_eval_time=279.834324, accumulated_logging_time=0.255762, accumulated_submission_time=6832.482260, global_step=9032, preemption_count=0, score=6832.482260, test/accuracy=0.460700, test/loss=2.510773, test/num_examples=10000, total_duration=7112.685649, train/accuracy=0.633889, train/loss=1.587334, validation/accuracy=0.581640, validation/loss=1.837292, validation/num_examples=50000
I0428 08:06:23.698739 140136956274432 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.166160225868225, loss=2.9167020320892334
I0428 08:07:37.956825 140136947881728 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.3807450532913208, loss=2.895557403564453
I0428 08:08:52.515654 140136956274432 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.1726963520050049, loss=2.8984594345092773
I0428 08:10:07.043135 140136947881728 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0798112154006958, loss=2.8316006660461426
I0428 08:11:21.475529 140136956274432 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.052742600440979, loss=2.8552770614624023
I0428 08:12:36.241832 140136947881728 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.9651474952697754, loss=2.793433666229248
I0428 08:13:51.062183 140136956274432 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9978072643280029, loss=2.7706639766693115
I0428 08:14:00.238272 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:14:07.477931 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:14:17.982492 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:14:20.177269 140341517031232 submission_runner.py:415] Time since start: 7642.79s, 	Step: 9717, 	{'train/accuracy': 0.6717753410339355, 'train/loss': 1.412249207496643, 'validation/accuracy': 0.5955399870872498, 'validation/loss': 1.7602205276489258, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.4517154693603516, 'test/num_examples': 10000, 'score': 7342.62376999855, 'total_duration': 7642.793210268021, 'accumulated_submission_time': 7342.62376999855, 'accumulated_eval_time': 299.7732858657837, 'accumulated_logging_time': 0.27483034133911133}
I0428 08:14:20.187767 140136947881728 logging_writer.py:48] [9717] accumulated_eval_time=299.773286, accumulated_logging_time=0.274830, accumulated_submission_time=7342.623770, global_step=9717, preemption_count=0, score=7342.623770, test/accuracy=0.469000, test/loss=2.451715, test/num_examples=10000, total_duration=7642.793210, train/accuracy=0.671775, train/loss=1.412249, validation/accuracy=0.595540, validation/loss=1.760221, validation/num_examples=50000
I0428 08:15:26.021259 140136956274432 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9960078597068787, loss=2.939084053039551
I0428 08:16:41.274853 140136947881728 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.82878577709198, loss=2.8613686561584473
I0428 08:17:56.835839 140136956274432 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8741788268089294, loss=2.8790740966796875
I0428 08:19:11.909001 140136947881728 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7228750586509705, loss=2.7086710929870605
I0428 08:20:27.266646 140136956274432 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.851540207862854, loss=2.789640188217163
I0428 08:21:43.048144 140136947881728 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.835293710231781, loss=2.782294273376465
I0428 08:22:50.707742 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:22:58.052451 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:23:08.509591 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:23:10.908143 140341517031232 submission_runner.py:415] Time since start: 8173.52s, 	Step: 10394, 	{'train/accuracy': 0.6611128449440002, 'train/loss': 1.4844605922698975, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.7420099973678589, 'validation/num_examples': 50000, 'test/accuracy': 0.47530001401901245, 'test/loss': 2.445575714111328, 'test/num_examples': 10000, 'score': 7853.126092672348, 'total_duration': 8173.523186683655, 'accumulated_submission_time': 7853.126092672348, 'accumulated_eval_time': 319.9727535247803, 'accumulated_logging_time': 0.2949082851409912}
I0428 08:23:10.919061 140136956274432 logging_writer.py:48] [10394] accumulated_eval_time=319.972754, accumulated_logging_time=0.294908, accumulated_submission_time=7853.126093, global_step=10394, preemption_count=0, score=7853.126093, test/accuracy=0.475300, test/loss=2.445576, test/num_examples=10000, total_duration=8173.523187, train/accuracy=0.661113, train/loss=1.484461, validation/accuracy=0.607800, validation/loss=1.742010, validation/num_examples=50000
I0428 08:23:18.203763 140136947881728 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.0443251132965088, loss=2.7553727626800537
I0428 08:24:33.086385 140136956274432 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9336928129196167, loss=2.791482925415039
I0428 08:25:48.075319 140136947881728 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8428178429603577, loss=2.8251330852508545
I0428 08:27:02.866317 140136956274432 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8998638391494751, loss=2.796602725982666
I0428 08:28:18.060082 140136947881728 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8745722770690918, loss=2.722156047821045
I0428 08:29:33.271508 140136956274432 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9900843501091003, loss=2.7483458518981934
I0428 08:30:47.970940 140136947881728 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8754001259803772, loss=2.729560375213623
I0428 08:31:40.956021 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:31:48.302194 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:31:58.746818 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:32:00.997235 140341517031232 submission_runner.py:415] Time since start: 8703.61s, 	Step: 11074, 	{'train/accuracy': 0.7078284025192261, 'train/loss': 1.2878546714782715, 'validation/accuracy': 0.6266799569129944, 'validation/loss': 1.6700420379638672, 'validation/num_examples': 50000, 'test/accuracy': 0.4921000301837921, 'test/loss': 2.3638083934783936, 'test/num_examples': 10000, 'score': 8363.145352602005, 'total_duration': 8703.611953258514, 'accumulated_submission_time': 8363.145352602005, 'accumulated_eval_time': 340.0127046108246, 'accumulated_logging_time': 0.3154795169830322}
I0428 08:32:01.007508 140136956274432 logging_writer.py:48] [11074] accumulated_eval_time=340.012705, accumulated_logging_time=0.315480, accumulated_submission_time=8363.145353, global_step=11074, preemption_count=0, score=8363.145353, test/accuracy=0.492100, test/loss=2.363808, test/num_examples=10000, total_duration=8703.611953, train/accuracy=0.707828, train/loss=1.287855, validation/accuracy=0.626680, validation/loss=1.670042, validation/num_examples=50000
I0428 08:32:23.406982 140136947881728 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9111627340316772, loss=2.832951545715332
I0428 08:33:40.006889 140136956274432 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9117058515548706, loss=2.7219808101654053
I0428 08:34:55.569573 140136947881728 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.2305867671966553, loss=2.7516188621520996
I0428 08:36:11.393380 140136956274432 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.8365918397903442, loss=2.662123441696167
I0428 08:37:27.020670 140136947881728 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7616128325462341, loss=2.67344069480896
I0428 08:38:42.481436 140136956274432 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9947400093078613, loss=2.7009923458099365
I0428 08:39:58.810369 140136947881728 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0116108655929565, loss=2.773581027984619
I0428 08:40:31.465228 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:40:39.057215 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:40:49.486727 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:40:51.797485 140341517031232 submission_runner.py:415] Time since start: 9234.41s, 	Step: 11745, 	{'train/accuracy': 0.683992326259613, 'train/loss': 1.408329725265503, 'validation/accuracy': 0.6239199638366699, 'validation/loss': 1.6906616687774658, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.3727669715881348, 'test/num_examples': 10000, 'score': 8873.584961175919, 'total_duration': 9234.413438081741, 'accumulated_submission_time': 8873.584961175919, 'accumulated_eval_time': 360.344931602478, 'accumulated_logging_time': 0.33583927154541016}
I0428 08:40:51.808490 140136956274432 logging_writer.py:48] [11745] accumulated_eval_time=360.344932, accumulated_logging_time=0.335839, accumulated_submission_time=8873.584961, global_step=11745, preemption_count=0, score=8873.584961, test/accuracy=0.499000, test/loss=2.372767, test/num_examples=10000, total_duration=9234.413438, train/accuracy=0.683992, train/loss=1.408330, validation/accuracy=0.623920, validation/loss=1.690662, validation/num_examples=50000
I0428 08:41:35.272067 140136947881728 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7726288437843323, loss=2.71535325050354
I0428 08:42:50.255820 140136956274432 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.877702534198761, loss=2.710088014602661
I0428 08:44:05.793748 140136947881728 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9012357592582703, loss=2.6410164833068848
I0428 08:45:21.635348 140136956274432 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0342847108840942, loss=2.7199997901916504
I0428 08:46:36.890818 140136947881728 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6646059155464172, loss=2.6683971881866455
I0428 08:47:52.672264 140136956274432 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6739984750747681, loss=2.61647891998291
I0428 08:49:08.213563 140136947881728 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.783139169216156, loss=2.695674419403076
I0428 08:49:23.396786 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:49:31.258611 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:49:41.750552 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:49:43.899021 140341517031232 submission_runner.py:415] Time since start: 9766.51s, 	Step: 12421, 	{'train/accuracy': 0.7238520383834839, 'train/loss': 1.2480639219284058, 'validation/accuracy': 0.6360399723052979, 'validation/loss': 1.6349976062774658, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.3083598613739014, 'test/num_examples': 10000, 'score': 9385.156900167465, 'total_duration': 9766.507361412048, 'accumulated_submission_time': 9385.156900167465, 'accumulated_eval_time': 380.8395240306854, 'accumulated_logging_time': 0.35508227348327637}
I0428 08:49:43.909985 140136956274432 logging_writer.py:48] [12421] accumulated_eval_time=380.839524, accumulated_logging_time=0.355082, accumulated_submission_time=9385.156900, global_step=12421, preemption_count=0, score=9385.156900, test/accuracy=0.509200, test/loss=2.308360, test/num_examples=10000, total_duration=9766.507361, train/accuracy=0.723852, train/loss=1.248064, validation/accuracy=0.636040, validation/loss=1.634998, validation/num_examples=50000
I0428 08:50:44.528759 140136947881728 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7221922874450684, loss=2.6058309078216553
I0428 08:52:00.463381 140136956274432 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7156795859336853, loss=2.5982766151428223
I0428 08:53:17.462009 140136947881728 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6401402354240417, loss=2.650273323059082
I0428 08:54:33.244345 140136956274432 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7577972412109375, loss=2.572772979736328
I0428 08:55:49.075683 140136947881728 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7391487956047058, loss=2.674213409423828
I0428 08:57:04.534728 140136956274432 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7681766152381897, loss=2.5748469829559326
I0428 08:58:14.066902 140341517031232 spec.py:298] Evaluating on the training split.
I0428 08:58:22.077521 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 08:58:32.700316 140341517031232 spec.py:326] Evaluating on the test split.
I0428 08:58:35.078935 140341517031232 submission_runner.py:415] Time since start: 10297.69s, 	Step: 13096, 	{'train/accuracy': 0.7118940949440002, 'train/loss': 1.25304114818573, 'validation/accuracy': 0.6418600082397461, 'validation/loss': 1.5786457061767578, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.270508289337158, 'test/num_examples': 10000, 'score': 9895.296508789062, 'total_duration': 10297.693656921387, 'accumulated_submission_time': 9895.296508789062, 'accumulated_eval_time': 401.8503067493439, 'accumulated_logging_time': 0.37513017654418945}
I0428 08:58:35.090164 140136947881728 logging_writer.py:48] [13096] accumulated_eval_time=401.850307, accumulated_logging_time=0.375130, accumulated_submission_time=9895.296509, global_step=13096, preemption_count=0, score=9895.296509, test/accuracy=0.508100, test/loss=2.270508, test/num_examples=10000, total_duration=10297.693657, train/accuracy=0.711894, train/loss=1.253041, validation/accuracy=0.641860, validation/loss=1.578646, validation/num_examples=50000
I0428 08:58:41.260321 140136956274432 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0410122871398926, loss=2.6256608963012695
I0428 08:59:57.320960 140136947881728 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1110398769378662, loss=2.59442400932312
I0428 09:01:13.165410 140136956274432 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0436261892318726, loss=2.6669437885284424
I0428 09:02:29.591814 140136947881728 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7822439670562744, loss=2.639486074447632
I0428 09:03:45.464057 140136956274432 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.8882105946540833, loss=2.6253490447998047
I0428 09:05:01.328760 140136947881728 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7491616010665894, loss=2.60800838470459
I0428 09:06:17.641467 140136956274432 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6199188828468323, loss=2.5207886695861816
I0428 09:07:05.112347 140341517031232 spec.py:298] Evaluating on the training split.
I0428 09:07:13.093762 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 09:07:23.654682 140341517031232 spec.py:326] Evaluating on the test split.
I0428 09:07:25.757563 140341517031232 submission_runner.py:415] Time since start: 10828.37s, 	Step: 13764, 	{'train/accuracy': 0.7463129758834839, 'train/loss': 1.12124502658844, 'validation/accuracy': 0.6493200063705444, 'validation/loss': 1.5342923402786255, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.2243733406066895, 'test/num_examples': 10000, 'score': 10405.299838542938, 'total_duration': 10828.373514652252, 'accumulated_submission_time': 10405.299838542938, 'accumulated_eval_time': 422.49549293518066, 'accumulated_logging_time': 0.3973722457885742}
I0428 09:07:25.768687 140136947881728 logging_writer.py:48] [13764] accumulated_eval_time=422.495493, accumulated_logging_time=0.397372, accumulated_submission_time=10405.299839, global_step=13764, preemption_count=0, score=10405.299839, test/accuracy=0.523800, test/loss=2.224373, test/num_examples=10000, total_duration=10828.373515, train/accuracy=0.746313, train/loss=1.121245, validation/accuracy=0.649320, validation/loss=1.534292, validation/num_examples=50000
I0428 09:07:54.308571 140136956274432 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6299604177474976, loss=2.55108904838562
I0428 09:09:10.481910 140136947881728 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6618043780326843, loss=2.632873773574829
I0428 09:10:26.111437 140136956274432 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7756033539772034, loss=2.5652098655700684
I0428 09:11:42.088720 140136947881728 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6376147270202637, loss=2.4382898807525635
I0428 09:12:58.329301 140136956274432 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8046789765357971, loss=2.5299131870269775
I0428 09:14:14.244222 140136947881728 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6416411995887756, loss=2.533623456954956
I0428 09:15:30.435697 140136956274432 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6708766222000122, loss=2.6696271896362305
I0428 09:15:55.791330 140341517031232 spec.py:298] Evaluating on the training split.
I0428 09:16:03.655995 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 09:16:14.343911 140341517031232 spec.py:326] Evaluating on the test split.
I0428 09:16:16.473681 140341517031232 submission_runner.py:415] Time since start: 11359.09s, 	Step: 14438, 	{'train/accuracy': 0.7252072691917419, 'train/loss': 1.1978331804275513, 'validation/accuracy': 0.6535599827766418, 'validation/loss': 1.5284909009933472, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.22442364692688, 'test/num_examples': 10000, 'score': 10915.306142568588, 'total_duration': 11359.089628219604, 'accumulated_submission_time': 10915.306142568588, 'accumulated_eval_time': 443.17781114578247, 'accumulated_logging_time': 0.4168429374694824}
I0428 09:16:16.486537 140136947881728 logging_writer.py:48] [14438] accumulated_eval_time=443.177811, accumulated_logging_time=0.416843, accumulated_submission_time=10915.306143, global_step=14438, preemption_count=0, score=10915.306143, test/accuracy=0.525500, test/loss=2.224424, test/num_examples=10000, total_duration=11359.089628, train/accuracy=0.725207, train/loss=1.197833, validation/accuracy=0.653560, validation/loss=1.528491, validation/num_examples=50000
I0428 09:17:07.008201 140136956274432 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8214319944381714, loss=2.5572962760925293
I0428 09:18:23.115611 140136947881728 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.737714409828186, loss=2.559405565261841
I0428 09:19:38.822399 140136956274432 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7131877541542053, loss=2.555321216583252
I0428 09:20:55.401022 140136947881728 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8353280425071716, loss=2.5263051986694336
I0428 09:22:11.477102 140136956274432 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6724876165390015, loss=2.5271456241607666
I0428 09:23:27.421884 140136947881728 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7326021790504456, loss=2.5113959312438965
I0428 09:24:42.868280 140136956274432 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.604182243347168, loss=2.505315065383911
I0428 09:24:46.605875 140341517031232 spec.py:298] Evaluating on the training split.
I0428 09:24:54.610365 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 09:25:05.975259 140341517031232 spec.py:326] Evaluating on the test split.
I0428 09:25:08.303815 140341517031232 submission_runner.py:415] Time since start: 11890.92s, 	Step: 15107, 	{'train/accuracy': 0.7632732391357422, 'train/loss': 1.0305802822113037, 'validation/accuracy': 0.6623600125312805, 'validation/loss': 1.475264072418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1570370197296143, 'test/num_examples': 10000, 'score': 11425.407599687576, 'total_duration': 11890.91976237297, 'accumulated_submission_time': 11425.407599687576, 'accumulated_eval_time': 464.8757164478302, 'accumulated_logging_time': 0.4397313594818115}
I0428 09:25:08.315471 140136947881728 logging_writer.py:48] [15107] accumulated_eval_time=464.875716, accumulated_logging_time=0.439731, accumulated_submission_time=11425.407600, global_step=15107, preemption_count=0, score=11425.407600, test/accuracy=0.535300, test/loss=2.157037, test/num_examples=10000, total_duration=11890.919762, train/accuracy=0.763273, train/loss=1.030580, validation/accuracy=0.662360, validation/loss=1.475264, validation/num_examples=50000
I0428 09:26:19.866930 140136956274432 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5877710580825806, loss=2.46604323387146
I0428 09:27:35.971058 140136947881728 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7555302977561951, loss=2.5615222454071045
I0428 09:28:51.424166 140136956274432 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9579535126686096, loss=2.572176933288574
I0428 09:30:07.303237 140136947881728 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6251615881919861, loss=2.4703195095062256
I0428 09:31:23.182179 140136956274432 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.8720365762710571, loss=2.5736849308013916
I0428 09:32:38.932914 140136947881728 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9025536179542542, loss=2.5811877250671387
I0428 09:33:40.057738 140341517031232 spec.py:298] Evaluating on the training split.
I0428 09:33:48.132263 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 09:33:58.802622 140341517031232 spec.py:326] Evaluating on the test split.
I0428 09:34:01.107263 140341517031232 submission_runner.py:415] Time since start: 12423.72s, 	Step: 15781, 	{'train/accuracy': 0.7383809089660645, 'train/loss': 1.1557672023773193, 'validation/accuracy': 0.6578999757766724, 'validation/loss': 1.5034807920455933, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.1977415084838867, 'test/num_examples': 10000, 'score': 11937.133967638016, 'total_duration': 12423.72321009636, 'accumulated_submission_time': 11937.133967638016, 'accumulated_eval_time': 485.9252073764801, 'accumulated_logging_time': 0.45939016342163086}
I0428 09:34:01.119770 140136956274432 logging_writer.py:48] [15781] accumulated_eval_time=485.925207, accumulated_logging_time=0.459390, accumulated_submission_time=11937.133968, global_step=15781, preemption_count=0, score=11937.133968, test/accuracy=0.529100, test/loss=2.197742, test/num_examples=10000, total_duration=12423.723210, train/accuracy=0.738381, train/loss=1.155767, validation/accuracy=0.657900, validation/loss=1.503481, validation/num_examples=50000
I0428 09:34:16.351944 140136947881728 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6290752291679382, loss=2.4738893508911133
I0428 09:35:32.644855 140136956274432 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.700459897518158, loss=2.3983678817749023
I0428 09:36:48.708973 140136947881728 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7594963908195496, loss=2.6000208854675293
I0428 09:38:05.020761 140136956274432 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7545717358589172, loss=2.51011061668396
I0428 09:39:22.187821 140136947881728 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5990666747093201, loss=2.482830047607422
I0428 09:40:38.335117 140136956274432 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6546185612678528, loss=2.5256593227386475
I0428 09:41:54.807974 140136947881728 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6931011080741882, loss=2.496999502182007
I0428 09:42:31.333329 140341517031232 spec.py:298] Evaluating on the training split.
I0428 09:42:39.454817 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 09:42:50.167791 140341517031232 spec.py:326] Evaluating on the test split.
I0428 09:42:52.236785 140341517031232 submission_runner.py:415] Time since start: 12954.85s, 	Step: 16451, 	{'train/accuracy': 0.7737563848495483, 'train/loss': 1.0115196704864502, 'validation/accuracy': 0.6661399602890015, 'validation/loss': 1.4760863780975342, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.1511740684509277, 'test/num_examples': 10000, 'score': 12447.330748319626, 'total_duration': 12954.851610183716, 'accumulated_submission_time': 12447.330748319626, 'accumulated_eval_time': 506.8275065422058, 'accumulated_logging_time': 0.48078060150146484}
I0428 09:42:52.249559 140136956274432 logging_writer.py:48] [16451] accumulated_eval_time=506.827507, accumulated_logging_time=0.480781, accumulated_submission_time=12447.330748, global_step=16451, preemption_count=0, score=12447.330748, test/accuracy=0.539200, test/loss=2.151174, test/num_examples=10000, total_duration=12954.851610, train/accuracy=0.773756, train/loss=1.011520, validation/accuracy=0.666140, validation/loss=1.476086, validation/num_examples=50000
I0428 09:43:32.157308 140136947881728 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6834352612495422, loss=2.4441287517547607
I0428 09:44:48.026315 140136956274432 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5594708323478699, loss=2.513204574584961
I0428 09:46:04.739120 140136947881728 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4848962128162384, loss=2.523286819458008
I0428 09:47:21.687437 140136956274432 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6022806763648987, loss=2.4675121307373047
I0428 09:48:38.845614 140136947881728 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5530257821083069, loss=2.476684093475342
I0428 09:49:55.601149 140136956274432 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6006753444671631, loss=2.459153652191162
I0428 09:51:12.302876 140136947881728 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5433247089385986, loss=2.4814534187316895
I0428 09:51:22.603602 140341517031232 spec.py:298] Evaluating on the training split.
I0428 09:51:30.189540 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 09:51:40.500813 140341517031232 spec.py:326] Evaluating on the test split.
I0428 09:51:42.862620 140341517031232 submission_runner.py:415] Time since start: 13485.48s, 	Step: 17118, 	{'train/accuracy': 0.7515943646430969, 'train/loss': 1.0678850412368774, 'validation/accuracy': 0.6696599721908569, 'validation/loss': 1.4416676759719849, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.1143550872802734, 'test/num_examples': 10000, 'score': 12957.665975093842, 'total_duration': 13485.47724366188, 'accumulated_submission_time': 12957.665975093842, 'accumulated_eval_time': 527.0851635932922, 'accumulated_logging_time': 0.5042319297790527}
I0428 09:51:42.873775 140136956274432 logging_writer.py:48] [17118] accumulated_eval_time=527.085164, accumulated_logging_time=0.504232, accumulated_submission_time=12957.665975, global_step=17118, preemption_count=0, score=12957.665975, test/accuracy=0.545100, test/loss=2.114355, test/num_examples=10000, total_duration=13485.477244, train/accuracy=0.751594, train/loss=1.067885, validation/accuracy=0.669660, validation/loss=1.441668, validation/num_examples=50000
I0428 09:52:48.856343 140136947881728 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.553441047668457, loss=2.4711031913757324
I0428 09:54:05.266113 140136956274432 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.534940242767334, loss=2.4735195636749268
I0428 09:55:21.827488 140136947881728 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5437585115432739, loss=2.4853322505950928
I0428 09:56:38.211675 140136956274432 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5879102945327759, loss=2.4170515537261963
I0428 09:57:54.401880 140136947881728 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6606739163398743, loss=2.4468231201171875
I0428 09:59:10.891993 140136956274432 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7821643948554993, loss=2.3928141593933105
I0428 10:00:13.347281 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:00:21.075960 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:00:31.181869 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:00:33.514128 140341517031232 submission_runner.py:415] Time since start: 14016.13s, 	Step: 17783, 	{'train/accuracy': 0.777742326259613, 'train/loss': 0.9656094908714294, 'validation/accuracy': 0.6713399887084961, 'validation/loss': 1.4398607015609741, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.126520872116089, 'test/num_examples': 10000, 'score': 13468.122778177261, 'total_duration': 14016.12856221199, 'accumulated_submission_time': 13468.122778177261, 'accumulated_eval_time': 547.2504806518555, 'accumulated_logging_time': 0.5241694450378418}
I0428 10:00:33.527273 140136947881728 logging_writer.py:48] [17783] accumulated_eval_time=547.250481, accumulated_logging_time=0.524169, accumulated_submission_time=13468.122778, global_step=17783, preemption_count=0, score=13468.122778, test/accuracy=0.537100, test/loss=2.126521, test/num_examples=10000, total_duration=14016.128562, train/accuracy=0.777742, train/loss=0.965609, validation/accuracy=0.671340, validation/loss=1.439861, validation/num_examples=50000
I0428 10:00:47.585295 140136956274432 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5804508924484253, loss=2.4517064094543457
I0428 10:02:04.170655 140136947881728 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7065848112106323, loss=2.466808557510376
I0428 10:03:20.371624 140136956274432 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6187084317207336, loss=2.4218480587005615
I0428 10:04:36.890024 140136947881728 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6420907974243164, loss=2.457657814025879
I0428 10:05:53.958555 140136956274432 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6617462038993835, loss=2.4166107177734375
I0428 10:07:10.734741 140136947881728 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5845727920532227, loss=2.335374116897583
I0428 10:08:28.570922 140136956274432 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6317264437675476, loss=2.541726589202881
I0428 10:09:04.079736 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:09:11.818756 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:09:21.994200 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:09:24.120524 140341517031232 submission_runner.py:415] Time since start: 14546.74s, 	Step: 18449, 	{'train/accuracy': 0.7578125, 'train/loss': 1.0711852312088013, 'validation/accuracy': 0.6704199910163879, 'validation/loss': 1.455386996269226, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.141185760498047, 'test/num_examples': 10000, 'score': 13978.657371759415, 'total_duration': 14546.73531961441, 'accumulated_submission_time': 13978.657371759415, 'accumulated_eval_time': 567.290079832077, 'accumulated_logging_time': 0.5470216274261475}
I0428 10:09:24.132482 140136947881728 logging_writer.py:48] [18449] accumulated_eval_time=567.290080, accumulated_logging_time=0.547022, accumulated_submission_time=13978.657372, global_step=18449, preemption_count=0, score=13978.657372, test/accuracy=0.537700, test/loss=2.141186, test/num_examples=10000, total_duration=14546.735320, train/accuracy=0.757812, train/loss=1.071185, validation/accuracy=0.670420, validation/loss=1.455387, validation/num_examples=50000
I0428 10:10:05.000183 140136956274432 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.549927294254303, loss=2.409407138824463
I0428 10:11:21.913231 140136947881728 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6061874628067017, loss=2.435267925262451
I0428 10:12:38.920343 140136956274432 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7198370099067688, loss=2.4611587524414062
I0428 10:13:55.716969 140136947881728 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7167770266532898, loss=2.3650388717651367
I0428 10:15:12.933574 140136956274432 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7471452951431274, loss=2.4190783500671387
I0428 10:16:29.603876 140136947881728 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.565460741519928, loss=2.428732395172119
I0428 10:17:46.689385 140136956274432 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6636103391647339, loss=2.3809666633605957
I0428 10:17:54.679029 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:18:02.383596 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:18:12.562832 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:18:14.935155 140341517031232 submission_runner.py:415] Time since start: 15077.55s, 	Step: 19114, 	{'train/accuracy': 0.7921715378761292, 'train/loss': 0.9268390536308289, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.422592282295227, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.0956735610961914, 'test/num_examples': 10000, 'score': 14489.183903217316, 'total_duration': 15077.549934625626, 'accumulated_submission_time': 14489.183903217316, 'accumulated_eval_time': 587.545019865036, 'accumulated_logging_time': 0.570946216583252}
I0428 10:18:14.948221 140136947881728 logging_writer.py:48] [19114] accumulated_eval_time=587.545020, accumulated_logging_time=0.570946, accumulated_submission_time=14489.183903, global_step=19114, preemption_count=0, score=14489.183903, test/accuracy=0.544300, test/loss=2.095674, test/num_examples=10000, total_duration=15077.549935, train/accuracy=0.792172, train/loss=0.926839, validation/accuracy=0.675760, validation/loss=1.422592, validation/num_examples=50000
I0428 10:19:24.889159 140136956274432 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6376950144767761, loss=2.381330966949463
I0428 10:20:41.410364 140136947881728 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6383920907974243, loss=2.5156259536743164
I0428 10:21:58.344487 140136956274432 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5725957155227661, loss=2.3532462120056152
I0428 10:23:15.722530 140136947881728 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5167121887207031, loss=2.367543935775757
I0428 10:24:32.388384 140136956274432 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8307787179946899, loss=2.3960070610046387
I0428 10:25:49.337909 140136947881728 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7021224498748779, loss=2.4417145252227783
I0428 10:26:45.159254 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:26:52.751722 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:27:03.108400 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:27:05.429801 140341517031232 submission_runner.py:415] Time since start: 15608.04s, 	Step: 19777, 	{'train/accuracy': 0.7690330147743225, 'train/loss': 1.0022788047790527, 'validation/accuracy': 0.6773799657821655, 'validation/loss': 1.405854344367981, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.089869737625122, 'test/num_examples': 10000, 'score': 14999.377561569214, 'total_duration': 15608.044486284256, 'accumulated_submission_time': 14999.377561569214, 'accumulated_eval_time': 607.8142731189728, 'accumulated_logging_time': 0.5932877063751221}
I0428 10:27:05.441355 140136956274432 logging_writer.py:48] [19777] accumulated_eval_time=607.814273, accumulated_logging_time=0.593288, accumulated_submission_time=14999.377562, global_step=19777, preemption_count=0, score=14999.377562, test/accuracy=0.552300, test/loss=2.089870, test/num_examples=10000, total_duration=15608.044486, train/accuracy=0.769033, train/loss=1.002279, validation/accuracy=0.677380, validation/loss=1.405854, validation/num_examples=50000
I0428 10:27:26.418800 140136947881728 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6132872700691223, loss=2.3561861515045166
I0428 10:28:43.311738 140136956274432 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6170337796211243, loss=2.4207847118377686
I0428 10:30:00.722762 140136947881728 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5486981272697449, loss=2.4165382385253906
I0428 10:31:17.525022 140136956274432 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6110842227935791, loss=2.3839664459228516
I0428 10:32:34.980698 140136947881728 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6132990717887878, loss=2.3676862716674805
I0428 10:33:51.494237 140136956274432 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5826071500778198, loss=2.3746371269226074
I0428 10:35:08.172235 140136947881728 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.5499683618545532, loss=2.362097978591919
I0428 10:35:38.940528 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:35:46.502191 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:35:56.746253 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:35:58.841221 140341517031232 submission_runner.py:415] Time since start: 16141.46s, 	Step: 20441, 	{'train/accuracy': 0.8045679330825806, 'train/loss': 0.8797569870948792, 'validation/accuracy': 0.6815199851989746, 'validation/loss': 1.4011496305465698, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.0647764205932617, 'test/num_examples': 10000, 'score': 15512.85997390747, 'total_duration': 16141.45596909523, 'accumulated_submission_time': 15512.85997390747, 'accumulated_eval_time': 627.7137336730957, 'accumulated_logging_time': 0.6136393547058105}
I0428 10:35:58.853385 140136956274432 logging_writer.py:48] [20441] accumulated_eval_time=627.713734, accumulated_logging_time=0.613639, accumulated_submission_time=15512.859974, global_step=20441, preemption_count=0, score=15512.859974, test/accuracy=0.558900, test/loss=2.064776, test/num_examples=10000, total_duration=16141.455969, train/accuracy=0.804568, train/loss=0.879757, validation/accuracy=0.681520, validation/loss=1.401150, validation/num_examples=50000
I0428 10:36:45.023324 140136947881728 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6018081903457642, loss=2.417699098587036
I0428 10:38:01.819501 140136956274432 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7430274486541748, loss=2.4060025215148926
I0428 10:39:18.907925 140136947881728 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5534761548042297, loss=2.4329850673675537
I0428 10:40:36.387320 140136956274432 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6750781536102295, loss=2.2941527366638184
I0428 10:41:52.866429 140136947881728 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.5512562394142151, loss=2.3881773948669434
I0428 10:43:09.770039 140136956274432 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5939722657203674, loss=2.3322856426239014
I0428 10:44:26.484756 140136947881728 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6199485063552856, loss=2.4823639392852783
I0428 10:44:28.914773 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:44:36.376644 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:44:46.633917 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:44:48.980226 140341517031232 submission_runner.py:415] Time since start: 16671.59s, 	Step: 21105, 	{'train/accuracy': 0.7787587642669678, 'train/loss': 0.9671178460121155, 'validation/accuracy': 0.6822800040245056, 'validation/loss': 1.3983536958694458, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.0834474563598633, 'test/num_examples': 10000, 'score': 16022.903728485107, 'total_duration': 16671.594935655594, 'accumulated_submission_time': 16022.903728485107, 'accumulated_eval_time': 647.7779271602631, 'accumulated_logging_time': 0.6355164051055908}
I0428 10:44:48.993510 140136956274432 logging_writer.py:48] [21105] accumulated_eval_time=647.777927, accumulated_logging_time=0.635516, accumulated_submission_time=16022.903728, global_step=21105, preemption_count=0, score=16022.903728, test/accuracy=0.553500, test/loss=2.083447, test/num_examples=10000, total_duration=16671.594936, train/accuracy=0.778759, train/loss=0.967118, validation/accuracy=0.682280, validation/loss=1.398354, validation/num_examples=50000
I0428 10:46:04.575773 140136947881728 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.62923663854599, loss=2.448690414428711
I0428 10:47:21.634523 140136956274432 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6682003140449524, loss=2.4055962562561035
I0428 10:48:38.604656 140136947881728 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.5212904810905457, loss=2.4081971645355225
I0428 10:49:55.664927 140136956274432 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6151593923568726, loss=2.406393527984619
I0428 10:51:12.324012 140136947881728 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7391142845153809, loss=2.3975424766540527
I0428 10:52:29.876366 140136956274432 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5876777768135071, loss=2.3119335174560547
I0428 10:53:19.167694 140341517031232 spec.py:298] Evaluating on the training split.
I0428 10:53:26.569760 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 10:53:36.615624 140341517031232 spec.py:326] Evaluating on the test split.
I0428 10:53:38.713289 140341517031232 submission_runner.py:415] Time since start: 17201.33s, 	Step: 21766, 	{'train/accuracy': 0.8165258169174194, 'train/loss': 0.8262078762054443, 'validation/accuracy': 0.6835199594497681, 'validation/loss': 1.391174077987671, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.0642848014831543, 'test/num_examples': 10000, 'score': 16533.060859441757, 'total_duration': 17201.32803797722, 'accumulated_submission_time': 16533.060859441757, 'accumulated_eval_time': 667.3222894668579, 'accumulated_logging_time': 0.6578047275543213}
I0428 10:53:38.725498 140136947881728 logging_writer.py:48] [21766] accumulated_eval_time=667.322289, accumulated_logging_time=0.657805, accumulated_submission_time=16533.060859, global_step=21766, preemption_count=0, score=16533.060859, test/accuracy=0.555800, test/loss=2.064285, test/num_examples=10000, total_duration=17201.328038, train/accuracy=0.816526, train/loss=0.826208, validation/accuracy=0.683520, validation/loss=1.391174, validation/num_examples=50000
I0428 10:54:06.525796 140136956274432 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5669980049133301, loss=2.406015634536743
I0428 10:55:23.383637 140136947881728 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5821810364723206, loss=2.346364736557007
I0428 10:56:40.640981 140136956274432 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5888667106628418, loss=2.3669071197509766
I0428 10:57:58.722994 140136947881728 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.6161869168281555, loss=2.305196762084961
I0428 10:59:15.942115 140136956274432 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7218485474586487, loss=2.3206214904785156
I0428 11:00:33.013253 140136947881728 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6941449642181396, loss=2.3222296237945557
I0428 11:01:50.121159 140136956274432 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.603885293006897, loss=2.2919349670410156
I0428 11:02:09.191228 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:02:16.541718 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:02:26.730337 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:02:29.083686 140341517031232 submission_runner.py:415] Time since start: 17731.70s, 	Step: 22427, 	{'train/accuracy': 0.7811104655265808, 'train/loss': 0.9437940120697021, 'validation/accuracy': 0.6803999543190002, 'validation/loss': 1.3984954357147217, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 2.084567070007324, 'test/num_examples': 10000, 'score': 17043.509699583054, 'total_duration': 17731.698069810867, 'accumulated_submission_time': 17043.509699583054, 'accumulated_eval_time': 687.2131500244141, 'accumulated_logging_time': 0.6790440082550049}
I0428 11:02:29.095697 140136947881728 logging_writer.py:48] [22427] accumulated_eval_time=687.213150, accumulated_logging_time=0.679044, accumulated_submission_time=17043.509700, global_step=22427, preemption_count=0, score=17043.509700, test/accuracy=0.550900, test/loss=2.084567, test/num_examples=10000, total_duration=17731.698070, train/accuracy=0.781110, train/loss=0.943794, validation/accuracy=0.680400, validation/loss=1.398495, validation/num_examples=50000
I0428 11:03:26.720340 140136956274432 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5777481198310852, loss=2.3532402515411377
I0428 11:04:43.716782 140136947881728 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5752906203269958, loss=2.4121484756469727
I0428 11:06:01.553968 140136956274432 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5392186045646667, loss=2.316037654876709
I0428 11:07:18.996714 140136947881728 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5389279127120972, loss=2.301795482635498
I0428 11:08:36.513380 140136956274432 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.561410665512085, loss=2.316279411315918
I0428 11:09:53.774614 140136947881728 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6116002202033997, loss=2.3136515617370605
I0428 11:10:59.322321 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:11:06.684663 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:11:16.939215 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:11:19.053348 140341517031232 submission_runner.py:415] Time since start: 18261.67s, 	Step: 23087, 	{'train/accuracy': 0.8215680718421936, 'train/loss': 0.7927351593971252, 'validation/accuracy': 0.6871199607849121, 'validation/loss': 1.3672826290130615, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.0565474033355713, 'test/num_examples': 10000, 'score': 17553.719356298447, 'total_duration': 18261.66819667816, 'accumulated_submission_time': 17553.719356298447, 'accumulated_eval_time': 706.9430434703827, 'accumulated_logging_time': 0.7001516819000244}
I0428 11:11:19.065407 140136956274432 logging_writer.py:48] [23087] accumulated_eval_time=706.943043, accumulated_logging_time=0.700152, accumulated_submission_time=17553.719356, global_step=23087, preemption_count=0, score=17553.719356, test/accuracy=0.552900, test/loss=2.056547, test/num_examples=10000, total_duration=18261.668197, train/accuracy=0.821568, train/loss=0.792735, validation/accuracy=0.687120, validation/loss=1.367283, validation/num_examples=50000
I0428 11:11:30.664949 140136947881728 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7721502780914307, loss=2.2455222606658936
I0428 11:12:48.423434 140136956274432 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4805644452571869, loss=2.3377845287323
I0428 11:14:05.462048 140136947881728 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5737410187721252, loss=2.3653488159179688
I0428 11:15:22.450340 140136956274432 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.530092179775238, loss=2.3675696849823
I0428 11:16:39.601480 140136947881728 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.58997642993927, loss=2.3660123348236084
I0428 11:17:56.535346 140136956274432 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.491589218378067, loss=2.33968186378479
I0428 11:19:13.649350 140136947881728 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5962778329849243, loss=2.334319591522217
I0428 11:19:49.225953 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:19:56.605988 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:20:06.765155 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:20:08.878474 140341517031232 submission_runner.py:415] Time since start: 18791.49s, 	Step: 23749, 	{'train/accuracy': 0.7961375713348389, 'train/loss': 0.8955960869789124, 'validation/accuracy': 0.6913599967956543, 'validation/loss': 1.3609671592712402, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 2.0549275875091553, 'test/num_examples': 10000, 'score': 18063.863089323044, 'total_duration': 18791.493288993835, 'accumulated_submission_time': 18063.863089323044, 'accumulated_eval_time': 726.594398021698, 'accumulated_logging_time': 0.7212772369384766}
I0428 11:20:08.890547 140136956274432 logging_writer.py:48] [23749] accumulated_eval_time=726.594398, accumulated_logging_time=0.721277, accumulated_submission_time=18063.863089, global_step=23749, preemption_count=0, score=18063.863089, test/accuracy=0.562300, test/loss=2.054928, test/num_examples=10000, total_duration=18791.493289, train/accuracy=0.796138, train/loss=0.895596, validation/accuracy=0.691360, validation/loss=1.360967, validation/num_examples=50000
I0428 11:20:50.030060 140136947881728 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5476656556129456, loss=2.4403278827667236
I0428 11:22:07.187523 140136956274432 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6446365714073181, loss=2.2599172592163086
I0428 11:23:24.598562 140136947881728 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5401759743690491, loss=2.2736403942108154
I0428 11:24:41.714123 140136956274432 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5514281988143921, loss=2.299669027328491
I0428 11:25:59.274941 140136947881728 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6524777412414551, loss=2.373058319091797
I0428 11:27:17.061511 140136956274432 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5560020804405212, loss=2.297051191329956
I0428 11:28:34.420946 140136947881728 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4474485218524933, loss=2.247539520263672
I0428 11:28:38.899684 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:28:46.295897 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:28:56.718042 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:28:58.809196 140341517031232 submission_runner.py:415] Time since start: 19321.42s, 	Step: 24408, 	{'train/accuracy': 0.8318319320678711, 'train/loss': 0.7500761151313782, 'validation/accuracy': 0.6915599703788757, 'validation/loss': 1.3469876050949097, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.038921356201172, 'test/num_examples': 10000, 'score': 18573.855432271957, 'total_duration': 19321.42382979393, 'accumulated_submission_time': 18573.855432271957, 'accumulated_eval_time': 746.5025568008423, 'accumulated_logging_time': 0.7422075271606445}
I0428 11:28:58.822779 140136956274432 logging_writer.py:48] [24408] accumulated_eval_time=746.502557, accumulated_logging_time=0.742208, accumulated_submission_time=18573.855432, global_step=24408, preemption_count=0, score=18573.855432, test/accuracy=0.559800, test/loss=2.038921, test/num_examples=10000, total_duration=19321.423830, train/accuracy=0.831832, train/loss=0.750076, validation/accuracy=0.691560, validation/loss=1.346988, validation/num_examples=50000
I0428 11:30:11.666882 140136947881728 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6602615118026733, loss=2.4093077182769775
I0428 11:31:29.095556 140136956274432 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.53960782289505, loss=2.347672700881958
I0428 11:32:46.255914 140136947881728 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6854053735733032, loss=2.317560911178589
I0428 11:34:03.920897 140136956274432 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6580397486686707, loss=2.307743549346924
I0428 11:35:21.386945 140136947881728 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6199415922164917, loss=2.2665727138519287
I0428 11:36:38.644279 140136956274432 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5006087422370911, loss=2.2735233306884766
I0428 11:37:29.054612 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:37:36.514086 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:37:46.803226 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:37:49.062950 140341517031232 submission_runner.py:415] Time since start: 19851.68s, 	Step: 25067, 	{'train/accuracy': 0.8043686151504517, 'train/loss': 0.8656948208808899, 'validation/accuracy': 0.6936399936676025, 'validation/loss': 1.3493549823760986, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.042152166366577, 'test/num_examples': 10000, 'score': 19084.07062935829, 'total_duration': 19851.67785692215, 'accumulated_submission_time': 19084.07062935829, 'accumulated_eval_time': 766.5098187923431, 'accumulated_logging_time': 0.7645595073699951}
I0428 11:37:49.075111 140136947881728 logging_writer.py:48] [25067] accumulated_eval_time=766.509819, accumulated_logging_time=0.764560, accumulated_submission_time=19084.070629, global_step=25067, preemption_count=0, score=19084.070629, test/accuracy=0.564400, test/loss=2.042152, test/num_examples=10000, total_duration=19851.677857, train/accuracy=0.804369, train/loss=0.865695, validation/accuracy=0.693640, validation/loss=1.349355, validation/num_examples=50000
I0428 11:38:16.228870 140136956274432 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5473887920379639, loss=2.252403736114502
I0428 11:39:33.392942 140136947881728 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.514365553855896, loss=2.3003430366516113
I0428 11:40:51.111467 140136956274432 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5323925018310547, loss=2.1846513748168945
I0428 11:42:08.508478 140136947881728 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5575010776519775, loss=2.3049943447113037
I0428 11:43:25.987069 140136956274432 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5392801761627197, loss=2.3039450645446777
I0428 11:44:43.494963 140136947881728 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.5424742102622986, loss=2.3154187202453613
I0428 11:46:00.285765 140136956274432 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5089861154556274, loss=2.1731410026550293
I0428 11:46:19.583303 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:46:26.970330 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:46:37.256425 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:46:39.481967 140341517031232 submission_runner.py:415] Time since start: 20382.10s, 	Step: 25727, 	{'train/accuracy': 0.8465202450752258, 'train/loss': 0.6928266882896423, 'validation/accuracy': 0.69132000207901, 'validation/loss': 1.3404393196105957, 'validation/num_examples': 50000, 'test/accuracy': 0.5603000521659851, 'test/loss': 2.019710063934326, 'test/num_examples': 10000, 'score': 19594.561599493027, 'total_duration': 20382.09677553177, 'accumulated_submission_time': 19594.561599493027, 'accumulated_eval_time': 786.4073100090027, 'accumulated_logging_time': 0.7860202789306641}
I0428 11:46:39.494633 140136947881728 logging_writer.py:48] [25727] accumulated_eval_time=786.407310, accumulated_logging_time=0.786020, accumulated_submission_time=19594.561599, global_step=25727, preemption_count=0, score=19594.561599, test/accuracy=0.560300, test/loss=2.019710, test/num_examples=10000, total_duration=20382.096776, train/accuracy=0.846520, train/loss=0.692827, validation/accuracy=0.691320, validation/loss=1.340439, validation/num_examples=50000
I0428 11:47:37.464188 140136956274432 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.5284643769264221, loss=2.3073906898498535
I0428 11:48:54.556879 140136947881728 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6059593558311462, loss=2.2857439517974854
I0428 11:50:11.895800 140136956274432 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6207247972488403, loss=2.2907986640930176
I0428 11:51:28.815062 140136947881728 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.49046802520751953, loss=2.234421968460083
I0428 11:52:46.269349 140136956274432 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6432309150695801, loss=2.313667058944702
I0428 11:54:03.791699 140136947881728 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4631282389163971, loss=2.234987258911133
I0428 11:55:09.790792 140341517031232 spec.py:298] Evaluating on the training split.
I0428 11:55:17.237263 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 11:55:27.522625 140341517031232 spec.py:326] Evaluating on the test split.
I0428 11:55:29.707612 140341517031232 submission_runner.py:415] Time since start: 20912.32s, 	Step: 26388, 	{'train/accuracy': 0.8047472834587097, 'train/loss': 0.8466227650642395, 'validation/accuracy': 0.6929999589920044, 'validation/loss': 1.3519667387008667, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.0384764671325684, 'test/num_examples': 10000, 'score': 20104.841159820557, 'total_duration': 20912.322410583496, 'accumulated_submission_time': 20104.841159820557, 'accumulated_eval_time': 806.3229458332062, 'accumulated_logging_time': 0.8075153827667236}
I0428 11:55:29.720672 140136956274432 logging_writer.py:48] [26388] accumulated_eval_time=806.322946, accumulated_logging_time=0.807515, accumulated_submission_time=20104.841160, global_step=26388, preemption_count=0, score=20104.841160, test/accuracy=0.561800, test/loss=2.038476, test/num_examples=10000, total_duration=20912.322411, train/accuracy=0.804747, train/loss=0.846623, validation/accuracy=0.693000, validation/loss=1.351967, validation/num_examples=50000
I0428 11:55:40.976153 140136947881728 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.523813784122467, loss=2.2619481086730957
I0428 11:56:58.390962 140136956274432 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5989459156990051, loss=2.2879445552825928
I0428 11:58:15.746708 140136947881728 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5862312912940979, loss=2.283463716506958
I0428 11:59:33.592614 140136956274432 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6244021654129028, loss=2.3311429023742676
I0428 12:00:50.760124 140136947881728 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6016193628311157, loss=2.237240791320801
I0428 12:02:08.002774 140136956274432 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5649740099906921, loss=2.1787009239196777
I0428 12:03:25.456875 140136947881728 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.551662802696228, loss=2.3103444576263428
I0428 12:03:59.980322 140341517031232 spec.py:298] Evaluating on the training split.
I0428 12:04:07.407197 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 12:04:17.543588 140341517031232 spec.py:326] Evaluating on the test split.
I0428 12:04:19.886985 140341517031232 submission_runner.py:415] Time since start: 21442.50s, 	Step: 27047, 	{'train/accuracy': 0.8564453125, 'train/loss': 0.6700361371040344, 'validation/accuracy': 0.6983399987220764, 'validation/loss': 1.331665277481079, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.0138297080993652, 'test/num_examples': 10000, 'score': 20615.084086418152, 'total_duration': 21442.50190114975, 'accumulated_submission_time': 20615.084086418152, 'accumulated_eval_time': 826.2285442352295, 'accumulated_logging_time': 0.8294951915740967}
I0428 12:04:19.899616 140136956274432 logging_writer.py:48] [27047] accumulated_eval_time=826.228544, accumulated_logging_time=0.829495, accumulated_submission_time=20615.084086, global_step=27047, preemption_count=0, score=20615.084086, test/accuracy=0.569800, test/loss=2.013830, test/num_examples=10000, total_duration=21442.501901, train/accuracy=0.856445, train/loss=0.670036, validation/accuracy=0.698340, validation/loss=1.331665, validation/num_examples=50000
I0428 12:05:02.666290 140136947881728 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5038354396820068, loss=2.307276487350464
I0428 12:06:19.593122 140136956274432 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5229648351669312, loss=2.2956299781799316
I0428 12:07:36.978450 140136947881728 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5672835111618042, loss=2.296877145767212
I0428 12:08:54.479832 140136956274432 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.4794863164424896, loss=2.183713436126709
I0428 12:10:11.905810 140136947881728 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.4615536034107208, loss=2.2763993740081787
I0428 12:11:29.351948 140136956274432 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5288046598434448, loss=2.291654586791992
I0428 12:12:46.626044 140136947881728 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5394417643547058, loss=2.2994894981384277
I0428 12:12:50.260291 140341517031232 spec.py:298] Evaluating on the training split.
I0428 12:12:57.642441 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 12:13:08.063680 140341517031232 spec.py:326] Evaluating on the test split.
I0428 12:13:10.424714 140341517031232 submission_runner.py:415] Time since start: 21973.04s, 	Step: 27707, 	{'train/accuracy': 0.8135363459587097, 'train/loss': 0.8090198040008545, 'validation/accuracy': 0.6983999609947205, 'validation/loss': 1.3288151025772095, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 2.0029685497283936, 'test/num_examples': 10000, 'score': 21125.42817044258, 'total_duration': 21973.03961610794, 'accumulated_submission_time': 21125.42817044258, 'accumulated_eval_time': 846.3918883800507, 'accumulated_logging_time': 0.8509232997894287}
I0428 12:13:10.437459 140136956274432 logging_writer.py:48] [27707] accumulated_eval_time=846.391888, accumulated_logging_time=0.850923, accumulated_submission_time=21125.428170, global_step=27707, preemption_count=0, score=21125.428170, test/accuracy=0.566600, test/loss=2.002969, test/num_examples=10000, total_duration=21973.039616, train/accuracy=0.813536, train/loss=0.809020, validation/accuracy=0.698400, validation/loss=1.328815, validation/num_examples=50000
I0428 12:14:24.673036 140136947881728 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5123814940452576, loss=2.204348087310791
I0428 12:15:41.675110 140136956274432 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.563551127910614, loss=2.2749710083007812
I0428 12:16:55.744201 140341517031232 spec.py:298] Evaluating on the training split.
I0428 12:17:03.187032 140341517031232 spec.py:310] Evaluating on the validation split.
I0428 12:17:13.424027 140341517031232 spec.py:326] Evaluating on the test split.
I0428 12:17:15.472851 140341517031232 submission_runner.py:415] Time since start: 22218.09s, 	Step: 28000, 	{'train/accuracy': 0.8071189522743225, 'train/loss': 0.848994791507721, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.3459980487823486, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.0187625885009766, 'test/num_examples': 10000, 'score': 21350.722284793854, 'total_duration': 22218.0877430439, 'accumulated_submission_time': 21350.722284793854, 'accumulated_eval_time': 866.1194486618042, 'accumulated_logging_time': 0.8725626468658447}
I0428 12:17:15.486263 140136947881728 logging_writer.py:48] [28000] accumulated_eval_time=866.119449, accumulated_logging_time=0.872563, accumulated_submission_time=21350.722285, global_step=28000, preemption_count=0, score=21350.722285, test/accuracy=0.564400, test/loss=2.018763, test/num_examples=10000, total_duration=22218.087743, train/accuracy=0.807119, train/loss=0.848995, validation/accuracy=0.693400, validation/loss=1.345998, validation/num_examples=50000
I0428 12:17:15.504360 140136956274432 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=21350.722285
I0428 12:17:16.832854 140341517031232 checkpoints.py:356] Saving checkpoint at step: 28000
I0428 12:17:20.779301 140341517031232 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1/checkpoint_28000
I0428 12:17:20.857795 140341517031232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_shampoo/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0428 12:17:21.598297 140341517031232 submission_runner.py:578] Tuning trial 1/1
I0428 12:17:21.599475 140341517031232 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0428 12:17:21.616030 140341517031232 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006178252515383065, 'train/loss': 6.911553859710693, 'validation/accuracy': 0.0006599999614991248, 'validation/loss': 6.911994934082031, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911983966827393, 'test/num_examples': 10000, 'score': 192.28390622138977, 'total_duration': 237.14711570739746, 'accumulated_submission_time': 192.28390622138977, 'accumulated_eval_time': 44.86303496360779, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (685, {'train/accuracy': 0.07647082209587097, 'train/loss': 5.362525463104248, 'validation/accuracy': 0.0709799975156784, 'validation/loss': 5.4121527671813965, 'validation/num_examples': 50000, 'test/accuracy': 0.05040000379085541, 'test/loss': 5.63770866394043, 'test/num_examples': 10000, 'score': 702.7012600898743, 'total_duration': 765.4701640605927, 'accumulated_submission_time': 702.7012600898743, 'accumulated_eval_time': 62.7356481552124, 'accumulated_logging_time': 0.02470993995666504, 'global_step': 685, 'preemption_count': 0}), (1391, {'train/accuracy': 0.18955276906490326, 'train/loss': 4.2153520584106445, 'validation/accuracy': 0.15534000098705292, 'validation/loss': 4.425232887268066, 'validation/num_examples': 50000, 'test/accuracy': 0.11100000888109207, 'test/loss': 4.853227138519287, 'test/num_examples': 10000, 'score': 1213.1606333255768, 'total_duration': 1292.7987296581268, 'accumulated_submission_time': 1213.1606333255768, 'accumulated_eval_time': 79.57839441299438, 'accumulated_logging_time': 0.04262542724609375, 'global_step': 1391, 'preemption_count': 0}), (2090, {'train/accuracy': 0.2664819657802582, 'train/loss': 3.6139535903930664, 'validation/accuracy': 0.2421799898147583, 'validation/loss': 3.7521095275878906, 'validation/num_examples': 50000, 'test/accuracy': 0.17470000684261322, 'test/loss': 4.296182632446289, 'test/num_examples': 10000, 'score': 1723.3490097522736, 'total_duration': 1820.516660451889, 'accumulated_submission_time': 1723.3490097522736, 'accumulated_eval_time': 97.0801854133606, 'accumulated_logging_time': 0.06169009208679199, 'global_step': 2090, 'preemption_count': 0}), (2788, {'train/accuracy': 0.36680883169174194, 'train/loss': 2.9988722801208496, 'validation/accuracy': 0.3142800033092499, 'validation/loss': 3.3019864559173584, 'validation/num_examples': 50000, 'test/accuracy': 0.23070001602172852, 'test/loss': 3.926806926727295, 'test/num_examples': 10000, 'score': 2233.7391760349274, 'total_duration': 2348.4022886753082, 'accumulated_submission_time': 2233.7391760349274, 'accumulated_eval_time': 114.55021142959595, 'accumulated_logging_time': 0.07839298248291016, 'global_step': 2788, 'preemption_count': 0}), (3487, {'train/accuracy': 0.41097337007522583, 'train/loss': 2.727480173110962, 'validation/accuracy': 0.3790600001811981, 'validation/loss': 2.881439208984375, 'validation/num_examples': 50000, 'test/accuracy': 0.28760001063346863, 'test/loss': 3.528773307800293, 'test/num_examples': 10000, 'score': 2744.2396976947784, 'total_duration': 2876.1850051879883, 'accumulated_submission_time': 2744.2396976947784, 'accumulated_eval_time': 131.80496883392334, 'accumulated_logging_time': 0.09726190567016602, 'global_step': 3487, 'preemption_count': 0}), (4181, {'train/accuracy': 0.49396124482154846, 'train/loss': 2.3393497467041016, 'validation/accuracy': 0.4331199824810028, 'validation/loss': 2.6353554725646973, 'validation/num_examples': 50000, 'test/accuracy': 0.32340002059936523, 'test/loss': 3.274982452392578, 'test/num_examples': 10000, 'score': 3254.9596230983734, 'total_duration': 3404.2371418476105, 'accumulated_submission_time': 3254.9596230983734, 'accumulated_eval_time': 149.11107110977173, 'accumulated_logging_time': 0.11504602432250977, 'global_step': 4181, 'preemption_count': 0}), (4881, {'train/accuracy': 0.5005978941917419, 'train/loss': 2.2538259029388428, 'validation/accuracy': 0.46803998947143555, 'validation/loss': 2.4405150413513184, 'validation/num_examples': 50000, 'test/accuracy': 0.35610002279281616, 'test/loss': 3.1309781074523926, 'test/num_examples': 10000, 'score': 3768.1555137634277, 'total_duration': 3934.767132997513, 'accumulated_submission_time': 3768.1555137634277, 'accumulated_eval_time': 166.41059565544128, 'accumulated_logging_time': 0.13941383361816406, 'global_step': 4881, 'preemption_count': 0}), (5581, {'train/accuracy': 0.5597097873687744, 'train/loss': 2.0040760040283203, 'validation/accuracy': 0.5027799606323242, 'validation/loss': 2.286299228668213, 'validation/num_examples': 50000, 'test/accuracy': 0.38420000672340393, 'test/loss': 2.9613351821899414, 'test/num_examples': 10000, 'score': 4281.427541732788, 'total_duration': 4465.221879482269, 'accumulated_submission_time': 4281.427541732788, 'accumulated_eval_time': 183.5655288696289, 'accumulated_logging_time': 0.15836215019226074, 'global_step': 5581, 'preemption_count': 0}), (6278, {'train/accuracy': 0.5601682066917419, 'train/loss': 1.9762367010116577, 'validation/accuracy': 0.5191999673843384, 'validation/loss': 2.1719493865966797, 'validation/num_examples': 50000, 'test/accuracy': 0.3946000039577484, 'test/loss': 2.8906848430633545, 'test/num_examples': 10000, 'score': 4791.572980880737, 'total_duration': 4993.846155643463, 'accumulated_submission_time': 4791.572980880737, 'accumulated_eval_time': 202.01675128936768, 'accumulated_logging_time': 0.17748260498046875, 'global_step': 6278, 'preemption_count': 0}), (6966, {'train/accuracy': 0.6094746589660645, 'train/loss': 1.7303013801574707, 'validation/accuracy': 0.5480200052261353, 'validation/loss': 2.0321598052978516, 'validation/num_examples': 50000, 'test/accuracy': 0.4252000153064728, 'test/loss': 2.728846549987793, 'test/num_examples': 10000, 'score': 5301.655446767807, 'total_duration': 5523.183669567108, 'accumulated_submission_time': 5301.655446767807, 'accumulated_eval_time': 221.2458667755127, 'accumulated_logging_time': 0.1949753761291504, 'global_step': 6966, 'preemption_count': 0}), (7659, {'train/accuracy': 0.6038145422935486, 'train/loss': 1.7866847515106201, 'validation/accuracy': 0.5588200092315674, 'validation/loss': 1.996261477470398, 'validation/num_examples': 50000, 'test/accuracy': 0.43460002541542053, 'test/loss': 2.6921334266662598, 'test/num_examples': 10000, 'score': 5812.178817987442, 'total_duration': 6053.028200387955, 'accumulated_submission_time': 5812.178817987442, 'accumulated_eval_time': 240.53838419914246, 'accumulated_logging_time': 0.21535325050354004, 'global_step': 7659, 'preemption_count': 0}), (8346, {'train/accuracy': 0.6458665132522583, 'train/loss': 1.5729056596755981, 'validation/accuracy': 0.5776000022888184, 'validation/loss': 1.8913501501083374, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.5710623264312744, 'test/num_examples': 10000, 'score': 6322.436324119568, 'total_duration': 6582.828968763351, 'accumulated_submission_time': 6322.436324119568, 'accumulated_eval_time': 260.0534234046936, 'accumulated_logging_time': 0.23428964614868164, 'global_step': 8346, 'preemption_count': 0}), (9032, {'train/accuracy': 0.6338887214660645, 'train/loss': 1.587334156036377, 'validation/accuracy': 0.5816400051116943, 'validation/loss': 1.8372917175292969, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.510772943496704, 'test/num_examples': 10000, 'score': 6832.482260227203, 'total_duration': 7112.685649394989, 'accumulated_submission_time': 6832.482260227203, 'accumulated_eval_time': 279.8343243598938, 'accumulated_logging_time': 0.25576233863830566, 'global_step': 9032, 'preemption_count': 0}), (9717, {'train/accuracy': 0.6717753410339355, 'train/loss': 1.412249207496643, 'validation/accuracy': 0.5955399870872498, 'validation/loss': 1.7602205276489258, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.4517154693603516, 'test/num_examples': 10000, 'score': 7342.62376999855, 'total_duration': 7642.793210268021, 'accumulated_submission_time': 7342.62376999855, 'accumulated_eval_time': 299.7732858657837, 'accumulated_logging_time': 0.27483034133911133, 'global_step': 9717, 'preemption_count': 0}), (10394, {'train/accuracy': 0.6611128449440002, 'train/loss': 1.4844605922698975, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.7420099973678589, 'validation/num_examples': 50000, 'test/accuracy': 0.47530001401901245, 'test/loss': 2.445575714111328, 'test/num_examples': 10000, 'score': 7853.126092672348, 'total_duration': 8173.523186683655, 'accumulated_submission_time': 7853.126092672348, 'accumulated_eval_time': 319.9727535247803, 'accumulated_logging_time': 0.2949082851409912, 'global_step': 10394, 'preemption_count': 0}), (11074, {'train/accuracy': 0.7078284025192261, 'train/loss': 1.2878546714782715, 'validation/accuracy': 0.6266799569129944, 'validation/loss': 1.6700420379638672, 'validation/num_examples': 50000, 'test/accuracy': 0.4921000301837921, 'test/loss': 2.3638083934783936, 'test/num_examples': 10000, 'score': 8363.145352602005, 'total_duration': 8703.611953258514, 'accumulated_submission_time': 8363.145352602005, 'accumulated_eval_time': 340.0127046108246, 'accumulated_logging_time': 0.3154795169830322, 'global_step': 11074, 'preemption_count': 0}), (11745, {'train/accuracy': 0.683992326259613, 'train/loss': 1.408329725265503, 'validation/accuracy': 0.6239199638366699, 'validation/loss': 1.6906616687774658, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.3727669715881348, 'test/num_examples': 10000, 'score': 8873.584961175919, 'total_duration': 9234.413438081741, 'accumulated_submission_time': 8873.584961175919, 'accumulated_eval_time': 360.344931602478, 'accumulated_logging_time': 0.33583927154541016, 'global_step': 11745, 'preemption_count': 0}), (12421, {'train/accuracy': 0.7238520383834839, 'train/loss': 1.2480639219284058, 'validation/accuracy': 0.6360399723052979, 'validation/loss': 1.6349976062774658, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.3083598613739014, 'test/num_examples': 10000, 'score': 9385.156900167465, 'total_duration': 9766.507361412048, 'accumulated_submission_time': 9385.156900167465, 'accumulated_eval_time': 380.8395240306854, 'accumulated_logging_time': 0.35508227348327637, 'global_step': 12421, 'preemption_count': 0}), (13096, {'train/accuracy': 0.7118940949440002, 'train/loss': 1.25304114818573, 'validation/accuracy': 0.6418600082397461, 'validation/loss': 1.5786457061767578, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.270508289337158, 'test/num_examples': 10000, 'score': 9895.296508789062, 'total_duration': 10297.693656921387, 'accumulated_submission_time': 9895.296508789062, 'accumulated_eval_time': 401.8503067493439, 'accumulated_logging_time': 0.37513017654418945, 'global_step': 13096, 'preemption_count': 0}), (13764, {'train/accuracy': 0.7463129758834839, 'train/loss': 1.12124502658844, 'validation/accuracy': 0.6493200063705444, 'validation/loss': 1.5342923402786255, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.2243733406066895, 'test/num_examples': 10000, 'score': 10405.299838542938, 'total_duration': 10828.373514652252, 'accumulated_submission_time': 10405.299838542938, 'accumulated_eval_time': 422.49549293518066, 'accumulated_logging_time': 0.3973722457885742, 'global_step': 13764, 'preemption_count': 0}), (14438, {'train/accuracy': 0.7252072691917419, 'train/loss': 1.1978331804275513, 'validation/accuracy': 0.6535599827766418, 'validation/loss': 1.5284909009933472, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.22442364692688, 'test/num_examples': 10000, 'score': 10915.306142568588, 'total_duration': 11359.089628219604, 'accumulated_submission_time': 10915.306142568588, 'accumulated_eval_time': 443.17781114578247, 'accumulated_logging_time': 0.4168429374694824, 'global_step': 14438, 'preemption_count': 0}), (15107, {'train/accuracy': 0.7632732391357422, 'train/loss': 1.0305802822113037, 'validation/accuracy': 0.6623600125312805, 'validation/loss': 1.475264072418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1570370197296143, 'test/num_examples': 10000, 'score': 11425.407599687576, 'total_duration': 11890.91976237297, 'accumulated_submission_time': 11425.407599687576, 'accumulated_eval_time': 464.8757164478302, 'accumulated_logging_time': 0.4397313594818115, 'global_step': 15107, 'preemption_count': 0}), (15781, {'train/accuracy': 0.7383809089660645, 'train/loss': 1.1557672023773193, 'validation/accuracy': 0.6578999757766724, 'validation/loss': 1.5034807920455933, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.1977415084838867, 'test/num_examples': 10000, 'score': 11937.133967638016, 'total_duration': 12423.72321009636, 'accumulated_submission_time': 11937.133967638016, 'accumulated_eval_time': 485.9252073764801, 'accumulated_logging_time': 0.45939016342163086, 'global_step': 15781, 'preemption_count': 0}), (16451, {'train/accuracy': 0.7737563848495483, 'train/loss': 1.0115196704864502, 'validation/accuracy': 0.6661399602890015, 'validation/loss': 1.4760863780975342, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.1511740684509277, 'test/num_examples': 10000, 'score': 12447.330748319626, 'total_duration': 12954.851610183716, 'accumulated_submission_time': 12447.330748319626, 'accumulated_eval_time': 506.8275065422058, 'accumulated_logging_time': 0.48078060150146484, 'global_step': 16451, 'preemption_count': 0}), (17118, {'train/accuracy': 0.7515943646430969, 'train/loss': 1.0678850412368774, 'validation/accuracy': 0.6696599721908569, 'validation/loss': 1.4416676759719849, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.1143550872802734, 'test/num_examples': 10000, 'score': 12957.665975093842, 'total_duration': 13485.47724366188, 'accumulated_submission_time': 12957.665975093842, 'accumulated_eval_time': 527.0851635932922, 'accumulated_logging_time': 0.5042319297790527, 'global_step': 17118, 'preemption_count': 0}), (17783, {'train/accuracy': 0.777742326259613, 'train/loss': 0.9656094908714294, 'validation/accuracy': 0.6713399887084961, 'validation/loss': 1.4398607015609741, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.126520872116089, 'test/num_examples': 10000, 'score': 13468.122778177261, 'total_duration': 14016.12856221199, 'accumulated_submission_time': 13468.122778177261, 'accumulated_eval_time': 547.2504806518555, 'accumulated_logging_time': 0.5241694450378418, 'global_step': 17783, 'preemption_count': 0}), (18449, {'train/accuracy': 0.7578125, 'train/loss': 1.0711852312088013, 'validation/accuracy': 0.6704199910163879, 'validation/loss': 1.455386996269226, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.141185760498047, 'test/num_examples': 10000, 'score': 13978.657371759415, 'total_duration': 14546.73531961441, 'accumulated_submission_time': 13978.657371759415, 'accumulated_eval_time': 567.290079832077, 'accumulated_logging_time': 0.5470216274261475, 'global_step': 18449, 'preemption_count': 0}), (19114, {'train/accuracy': 0.7921715378761292, 'train/loss': 0.9268390536308289, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.422592282295227, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.0956735610961914, 'test/num_examples': 10000, 'score': 14489.183903217316, 'total_duration': 15077.549934625626, 'accumulated_submission_time': 14489.183903217316, 'accumulated_eval_time': 587.545019865036, 'accumulated_logging_time': 0.570946216583252, 'global_step': 19114, 'preemption_count': 0}), (19777, {'train/accuracy': 0.7690330147743225, 'train/loss': 1.0022788047790527, 'validation/accuracy': 0.6773799657821655, 'validation/loss': 1.405854344367981, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.089869737625122, 'test/num_examples': 10000, 'score': 14999.377561569214, 'total_duration': 15608.044486284256, 'accumulated_submission_time': 14999.377561569214, 'accumulated_eval_time': 607.8142731189728, 'accumulated_logging_time': 0.5932877063751221, 'global_step': 19777, 'preemption_count': 0}), (20441, {'train/accuracy': 0.8045679330825806, 'train/loss': 0.8797569870948792, 'validation/accuracy': 0.6815199851989746, 'validation/loss': 1.4011496305465698, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.0647764205932617, 'test/num_examples': 10000, 'score': 15512.85997390747, 'total_duration': 16141.45596909523, 'accumulated_submission_time': 15512.85997390747, 'accumulated_eval_time': 627.7137336730957, 'accumulated_logging_time': 0.6136393547058105, 'global_step': 20441, 'preemption_count': 0}), (21105, {'train/accuracy': 0.7787587642669678, 'train/loss': 0.9671178460121155, 'validation/accuracy': 0.6822800040245056, 'validation/loss': 1.3983536958694458, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.0834474563598633, 'test/num_examples': 10000, 'score': 16022.903728485107, 'total_duration': 16671.594935655594, 'accumulated_submission_time': 16022.903728485107, 'accumulated_eval_time': 647.7779271602631, 'accumulated_logging_time': 0.6355164051055908, 'global_step': 21105, 'preemption_count': 0}), (21766, {'train/accuracy': 0.8165258169174194, 'train/loss': 0.8262078762054443, 'validation/accuracy': 0.6835199594497681, 'validation/loss': 1.391174077987671, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.0642848014831543, 'test/num_examples': 10000, 'score': 16533.060859441757, 'total_duration': 17201.32803797722, 'accumulated_submission_time': 16533.060859441757, 'accumulated_eval_time': 667.3222894668579, 'accumulated_logging_time': 0.6578047275543213, 'global_step': 21766, 'preemption_count': 0}), (22427, {'train/accuracy': 0.7811104655265808, 'train/loss': 0.9437940120697021, 'validation/accuracy': 0.6803999543190002, 'validation/loss': 1.3984954357147217, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 2.084567070007324, 'test/num_examples': 10000, 'score': 17043.509699583054, 'total_duration': 17731.698069810867, 'accumulated_submission_time': 17043.509699583054, 'accumulated_eval_time': 687.2131500244141, 'accumulated_logging_time': 0.6790440082550049, 'global_step': 22427, 'preemption_count': 0}), (23087, {'train/accuracy': 0.8215680718421936, 'train/loss': 0.7927351593971252, 'validation/accuracy': 0.6871199607849121, 'validation/loss': 1.3672826290130615, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.0565474033355713, 'test/num_examples': 10000, 'score': 17553.719356298447, 'total_duration': 18261.66819667816, 'accumulated_submission_time': 17553.719356298447, 'accumulated_eval_time': 706.9430434703827, 'accumulated_logging_time': 0.7001516819000244, 'global_step': 23087, 'preemption_count': 0}), (23749, {'train/accuracy': 0.7961375713348389, 'train/loss': 0.8955960869789124, 'validation/accuracy': 0.6913599967956543, 'validation/loss': 1.3609671592712402, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 2.0549275875091553, 'test/num_examples': 10000, 'score': 18063.863089323044, 'total_duration': 18791.493288993835, 'accumulated_submission_time': 18063.863089323044, 'accumulated_eval_time': 726.594398021698, 'accumulated_logging_time': 0.7212772369384766, 'global_step': 23749, 'preemption_count': 0}), (24408, {'train/accuracy': 0.8318319320678711, 'train/loss': 0.7500761151313782, 'validation/accuracy': 0.6915599703788757, 'validation/loss': 1.3469876050949097, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.038921356201172, 'test/num_examples': 10000, 'score': 18573.855432271957, 'total_duration': 19321.42382979393, 'accumulated_submission_time': 18573.855432271957, 'accumulated_eval_time': 746.5025568008423, 'accumulated_logging_time': 0.7422075271606445, 'global_step': 24408, 'preemption_count': 0}), (25067, {'train/accuracy': 0.8043686151504517, 'train/loss': 0.8656948208808899, 'validation/accuracy': 0.6936399936676025, 'validation/loss': 1.3493549823760986, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.042152166366577, 'test/num_examples': 10000, 'score': 19084.07062935829, 'total_duration': 19851.67785692215, 'accumulated_submission_time': 19084.07062935829, 'accumulated_eval_time': 766.5098187923431, 'accumulated_logging_time': 0.7645595073699951, 'global_step': 25067, 'preemption_count': 0}), (25727, {'train/accuracy': 0.8465202450752258, 'train/loss': 0.6928266882896423, 'validation/accuracy': 0.69132000207901, 'validation/loss': 1.3404393196105957, 'validation/num_examples': 50000, 'test/accuracy': 0.5603000521659851, 'test/loss': 2.019710063934326, 'test/num_examples': 10000, 'score': 19594.561599493027, 'total_duration': 20382.09677553177, 'accumulated_submission_time': 19594.561599493027, 'accumulated_eval_time': 786.4073100090027, 'accumulated_logging_time': 0.7860202789306641, 'global_step': 25727, 'preemption_count': 0}), (26388, {'train/accuracy': 0.8047472834587097, 'train/loss': 0.8466227650642395, 'validation/accuracy': 0.6929999589920044, 'validation/loss': 1.3519667387008667, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.0384764671325684, 'test/num_examples': 10000, 'score': 20104.841159820557, 'total_duration': 20912.322410583496, 'accumulated_submission_time': 20104.841159820557, 'accumulated_eval_time': 806.3229458332062, 'accumulated_logging_time': 0.8075153827667236, 'global_step': 26388, 'preemption_count': 0}), (27047, {'train/accuracy': 0.8564453125, 'train/loss': 0.6700361371040344, 'validation/accuracy': 0.6983399987220764, 'validation/loss': 1.331665277481079, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.0138297080993652, 'test/num_examples': 10000, 'score': 20615.084086418152, 'total_duration': 21442.50190114975, 'accumulated_submission_time': 20615.084086418152, 'accumulated_eval_time': 826.2285442352295, 'accumulated_logging_time': 0.8294951915740967, 'global_step': 27047, 'preemption_count': 0}), (27707, {'train/accuracy': 0.8135363459587097, 'train/loss': 0.8090198040008545, 'validation/accuracy': 0.6983999609947205, 'validation/loss': 1.3288151025772095, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 2.0029685497283936, 'test/num_examples': 10000, 'score': 21125.42817044258, 'total_duration': 21973.03961610794, 'accumulated_submission_time': 21125.42817044258, 'accumulated_eval_time': 846.3918883800507, 'accumulated_logging_time': 0.8509232997894287, 'global_step': 27707, 'preemption_count': 0}), (28000, {'train/accuracy': 0.8071189522743225, 'train/loss': 0.848994791507721, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.3459980487823486, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.0187625885009766, 'test/num_examples': 10000, 'score': 21350.722284793854, 'total_duration': 22218.0877430439, 'accumulated_submission_time': 21350.722284793854, 'accumulated_eval_time': 866.1194486618042, 'accumulated_logging_time': 0.8725626468658447, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0428 12:17:21.616196 140341517031232 submission_runner.py:581] Timing: 21350.722284793854
I0428 12:17:21.616246 140341517031232 submission_runner.py:582] ====================
I0428 12:17:21.616417 140341517031232 submission_runner.py:645] Final imagenet_resnet score: 21350.722284793854
