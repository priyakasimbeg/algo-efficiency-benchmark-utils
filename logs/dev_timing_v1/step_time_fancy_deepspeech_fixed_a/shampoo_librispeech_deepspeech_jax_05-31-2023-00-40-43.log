python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_jax_upgrade_a/shampoo --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-31-2023-00-40-43.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0531 00:41:05.955530 140238962607936 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax.
I0531 00:41:06.978622 140238962607936 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0531 00:41:06.979675 140238962607936 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 00:41:06.979860 140238962607936 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0531 00:41:06.985866 140238962607936 submission_runner.py:549] Using RNG seed 3284579331
I0531 00:41:12.226551 140238962607936 submission_runner.py:558] --- Tuning run 1/1 ---
I0531 00:41:12.226827 140238962607936 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1.
I0531 00:41:12.227186 140238962607936 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1/hparams.json.
I0531 00:41:12.408653 140238962607936 submission_runner.py:243] Initializing dataset.
I0531 00:41:12.408896 140238962607936 submission_runner.py:250] Initializing model.
I0531 00:41:14.915377 140238962607936 submission_runner.py:260] Initializing optimizer.
I0531 00:41:19.340825 140238962607936 submission_runner.py:267] Initializing metrics bundle.
I0531 00:41:19.341054 140238962607936 submission_runner.py:285] Initializing checkpoint and logger.
I0531 00:41:19.342453 140238962607936 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0531 00:41:19.342714 140238962607936 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0531 00:41:19.342806 140238962607936 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0531 00:41:19.963310 140238962607936 submission_runner.py:306] Saving meta data to /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0531 00:41:19.964286 140238962607936 submission_runner.py:309] Saving flags to /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0531 00:41:19.970463 140238962607936 submission_runner.py:321] Starting training loop.
I0531 00:41:20.265782 140238962607936 input_pipeline.py:20] Loading split = train-clean-100
I0531 00:41:20.300084 140238962607936 input_pipeline.py:20] Loading split = train-clean-360
I0531 00:41:20.641880 140238962607936 input_pipeline.py:20] Loading split = train-other-500
2023-05-31 00:42:42.619055: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-31 00:42:42.655371: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/array_methods.py:795: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  return getattr(self.aval, name).fun(self, *args, **kwargs)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:593: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:594: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:477: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  power = jnp.eye(mat_m.shape[0], dtype=_MAT_INV_PTH_ROOT_DTYPE)
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0531 00:42:48.126522 140078177318656 logging_writer.py:48] [0] global_step=0, grad_norm=20.723182678222656, loss=32.53205108642578
I0531 00:42:48.157064 140238962607936 spec.py:298] Evaluating on the training split.
I0531 00:42:48.421128 140238962607936 input_pipeline.py:20] Loading split = train-clean-100
I0531 00:42:48.640793 140238962607936 input_pipeline.py:20] Loading split = train-clean-360
I0531 00:42:48.759097 140238962607936 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0531 00:44:16.982026 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 00:44:17.181702 140238962607936 input_pipeline.py:20] Loading split = dev-clean
I0531 00:44:17.187580 140238962607936 input_pipeline.py:20] Loading split = dev-other
I0531 00:45:08.588121 140238962607936 spec.py:326] Evaluating on the test split.
I0531 00:45:08.798591 140238962607936 input_pipeline.py:20] Loading split = test-clean
I0531 00:45:45.004845 140238962607936 submission_runner.py:426] Time since start: 265.03s, 	Step: 1, 	{'train/ctc_loss': Array(31.835112, dtype=float32), 'train/wer': 2.279563958936343, 'validation/ctc_loss': Array(30.910616, dtype=float32), 'validation/wer': 1.9400573088018216, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.001806, dtype=float32), 'test/wer': 2.1327158613125343, 'test/num_examples': 2472, 'score': 88.18640112876892, 'total_duration': 265.03253841400146, 'accumulated_submission_time': 88.18640112876892, 'accumulated_data_selection_time': 4.732028007507324, 'accumulated_eval_time': 176.84596276283264, 'accumulated_logging_time': 0}
I0531 00:45:45.025022 140072473065216 logging_writer.py:48] [1] accumulated_data_selection_time=4.732028, accumulated_eval_time=176.845963, accumulated_logging_time=0, accumulated_submission_time=88.186401, global_step=1, preemption_count=0, score=88.186401, test/ctc_loss=31.001806259155273, test/num_examples=2472, test/wer=2.132716, total_duration=265.032538, train/ctc_loss=31.835111618041992, train/wer=2.279564, validation/ctc_loss=30.910615921020508, validation/num_examples=5348, validation/wer=1.940057
I0531 00:47:51.815873 140080362669824 logging_writer.py:48] [100] global_step=100, grad_norm=1.0840084552764893, loss=5.877499580383301
I0531 00:49:26.089012 140080371062528 logging_writer.py:48] [200] global_step=200, grad_norm=1.6439303159713745, loss=5.643868923187256
I0531 00:50:59.948392 140080362669824 logging_writer.py:48] [300] global_step=300, grad_norm=1.433431625366211, loss=5.141574382781982
I0531 00:52:33.312486 140080371062528 logging_writer.py:48] [400] global_step=400, grad_norm=1.4321401119232178, loss=4.4849982261657715
I0531 00:54:07.048068 140080362669824 logging_writer.py:48] [500] global_step=500, grad_norm=1.9161814451217651, loss=4.108173370361328
I0531 00:55:40.882658 140080371062528 logging_writer.py:48] [600] global_step=600, grad_norm=4.305679798126221, loss=3.758784055709839
I0531 00:57:16.793059 140080362669824 logging_writer.py:48] [700] global_step=700, grad_norm=2.318537712097168, loss=3.4368135929107666
I0531 00:58:52.335286 140080371062528 logging_writer.py:48] [800] global_step=800, grad_norm=2.4229750633239746, loss=3.281543016433716
I0531 01:00:28.220340 140080362669824 logging_writer.py:48] [900] global_step=900, grad_norm=2.918747901916504, loss=3.1200761795043945
I0531 01:02:02.790579 140080371062528 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.8694703578948975, loss=2.962660789489746
I0531 01:03:41.210730 140080471774976 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.169095754623413, loss=2.8844401836395264
I0531 01:05:15.483052 140080463382272 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.392258405685425, loss=2.8328001499176025
I0531 01:06:49.936776 140080471774976 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.102017879486084, loss=2.760756254196167
I0531 01:08:24.881657 140080463382272 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.2087507247924805, loss=2.589538097381592
I0531 01:09:59.265639 140080471774976 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.1006829738616943, loss=2.531709671020508
I0531 01:11:33.477945 140080463382272 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.4355034828186035, loss=2.500948429107666
I0531 01:13:07.413714 140080471774976 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.680931568145752, loss=2.477259635925293
I0531 01:14:41.761837 140080463382272 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.140813112258911, loss=2.4135661125183105
I0531 01:16:16.718829 140080471774976 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.248507022857666, loss=2.3924338817596436
I0531 01:17:51.434147 140080463382272 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.662154197692871, loss=2.286754608154297
I0531 01:19:30.494603 140080471774976 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.4989945888519287, loss=2.31758975982666
I0531 01:21:05.952755 140080463382272 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.220367193222046, loss=2.250727653503418
I0531 01:22:40.402337 140080471774976 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.1850762367248535, loss=2.2355146408081055
I0531 01:24:15.091202 140080463382272 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.2948155403137207, loss=2.1372132301330566
I0531 01:25:45.175090 140238962607936 spec.py:298] Evaluating on the training split.
I0531 01:26:31.451075 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 01:27:14.658040 140238962607936 spec.py:326] Evaluating on the test split.
I0531 01:27:35.257926 140238962607936 submission_runner.py:426] Time since start: 2775.28s, 	Step: 2497, 	{'train/ctc_loss': Array(1.9162618, dtype=float32), 'train/wer': 0.4675907808284072, 'validation/ctc_loss': Array(2.3616385, dtype=float32), 'validation/wer': 0.5303765593493425, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.873541, dtype=float32), 'test/wer': 0.46196656713992645, 'test/num_examples': 2472, 'score': 2488.2972922325134, 'total_duration': 2775.2840163707733, 'accumulated_submission_time': 2488.2972922325134, 'accumulated_data_selection_time': 289.3287010192871, 'accumulated_eval_time': 286.92539978027344, 'accumulated_logging_time': 0.03166317939758301}
I0531 01:27:35.278679 140081127134976 logging_writer.py:48] [2497] accumulated_data_selection_time=289.328701, accumulated_eval_time=286.925400, accumulated_logging_time=0.031663, accumulated_submission_time=2488.297292, global_step=2497, preemption_count=0, score=2488.297292, test/ctc_loss=1.873540997505188, test/num_examples=2472, test/wer=0.461967, total_duration=2775.284016, train/ctc_loss=1.9162617921829224, train/wer=0.467591, validation/ctc_loss=2.3616385459899902, validation/num_examples=5348, validation/wer=0.530377
I0531 01:27:39.576572 140081118742272 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.4483871459960938, loss=2.1642208099365234
I0531 01:29:13.793391 140081127134976 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.9568266868591309, loss=2.126035213470459
I0531 01:30:47.838589 140081118742272 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.0956437587738037, loss=2.0333173274993896
I0531 01:32:22.498103 140081127134976 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.0853748321533203, loss=2.098278522491455
I0531 01:33:57.370031 140081118742272 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.7318456172943115, loss=2.0900917053222656
I0531 01:35:32.429110 140081127134976 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.2091710567474365, loss=2.0868091583251953
I0531 01:37:11.206747 140081127134976 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.981284499168396, loss=2.0546834468841553
I0531 01:38:46.988530 140081118742272 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.603853464126587, loss=2.068814516067505
I0531 01:40:21.173795 140081127134976 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.0142321586608887, loss=2.006324291229248
I0531 01:41:55.823161 140081118742272 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.522315263748169, loss=1.9570825099945068
I0531 01:43:30.503493 140081127134976 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.2046666145324707, loss=2.0153563022613525
I0531 01:45:05.513844 140081118742272 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.328878164291382, loss=1.9779608249664307
I0531 01:46:40.634496 140081127134976 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.182162284851074, loss=1.9592925310134888
I0531 01:48:14.913109 140081118742272 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.2205381393432617, loss=1.9712527990341187
I0531 01:49:51.848209 140081127134976 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.483360767364502, loss=1.9931023120880127
I0531 01:51:29.978919 140081118742272 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.906869888305664, loss=1.880204439163208
I0531 01:53:05.736627 140081127134976 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.550478935241699, loss=1.8793216943740845
I0531 01:54:44.941987 140081127134976 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.3662984371185303, loss=1.873021125793457
I0531 01:56:19.588964 140081118742272 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.9625589847564697, loss=1.8472365140914917
I0531 01:57:53.698309 140081127134976 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.0438590049743652, loss=1.8854684829711914
I0531 01:59:29.080728 140081118742272 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.019479513168335, loss=1.8365767002105713
I0531 02:01:03.181902 140081127134976 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.56095290184021, loss=1.814342737197876
I0531 02:02:38.862857 140081118742272 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.32584285736084, loss=1.8282279968261719
I0531 02:04:14.467027 140081127134976 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.091193199157715, loss=1.827959418296814
I0531 02:05:48.793884 140081118742272 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.7275233268737793, loss=1.866564154624939
I0531 02:07:24.795581 140081127134976 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.793976306915283, loss=1.9190425872802734
I0531 02:07:35.894259 140238962607936 spec.py:298] Evaluating on the training split.
I0531 02:08:21.875451 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 02:09:03.468694 140238962607936 spec.py:326] Evaluating on the test split.
I0531 02:09:25.757473 140238962607936 submission_runner.py:426] Time since start: 5285.78s, 	Step: 5013, 	{'train/ctc_loss': Array(0.6295858, dtype=float32), 'train/wer': 0.22274724982020616, 'validation/ctc_loss': Array(1.0331788, dtype=float32), 'validation/wer': 0.29620160348869745, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6761152, dtype=float32), 'test/wer': 0.22521479495460361, 'test/num_examples': 2472, 'score': 4888.8670110702515, 'total_duration': 5285.783096075058, 'accumulated_submission_time': 4888.8670110702515, 'accumulated_data_selection_time': 603.6657586097717, 'accumulated_eval_time': 396.78474855422974, 'accumulated_logging_time': 0.06890869140625}
I0531 02:09:25.777507 140080697054976 logging_writer.py:48] [5013] accumulated_data_selection_time=603.665759, accumulated_eval_time=396.784749, accumulated_logging_time=0.068909, accumulated_submission_time=4888.867011, global_step=5013, preemption_count=0, score=4888.867011, test/ctc_loss=0.6761152148246765, test/num_examples=2472, test/wer=0.225215, total_duration=5285.783096, train/ctc_loss=0.6295858025550842, train/wer=0.222747, validation/ctc_loss=1.0331788063049316, validation/num_examples=5348, validation/wer=0.296202
I0531 02:10:49.743183 140080688662272 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.1776602268218994, loss=1.8221354484558105
I0531 02:12:30.056919 140080697054976 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.044517993927002, loss=1.8090676069259644
I0531 02:14:04.974036 140080688662272 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.773503541946411, loss=1.7588673830032349
I0531 02:15:39.815104 140080697054976 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.4889323711395264, loss=1.765419363975525
I0531 02:17:15.394435 140080688662272 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.0559043884277344, loss=1.8017139434814453
I0531 02:18:50.493026 140080697054976 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.479961395263672, loss=1.8025130033493042
I0531 02:20:25.091478 140080688662272 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.140831232070923, loss=1.7723525762557983
I0531 02:22:00.023274 140080697054976 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.676301002502441, loss=1.7408134937286377
I0531 02:23:36.335129 140080688662272 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.634342670440674, loss=1.7431505918502808
I0531 02:25:11.306021 140080697054976 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.2112865447998047, loss=1.8232609033584595
I0531 02:26:46.718123 140080688662272 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.193805456161499, loss=1.784216046333313
I0531 02:28:24.667310 140080369374976 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.5192718505859375, loss=1.7828328609466553
I0531 02:29:58.982491 140080360982272 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.340660095214844, loss=1.8171207904815674
I0531 02:31:33.244096 140080369374976 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.8385488986968994, loss=1.8096482753753662
I0531 02:33:08.172465 140080360982272 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.8539578914642334, loss=1.7394126653671265
I0531 02:34:43.843636 140080369374976 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.8444676399230957, loss=1.7627094984054565
I0531 02:36:18.513852 140080360982272 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.7124621868133545, loss=1.7155026197433472
I0531 02:37:52.598295 140080369374976 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.6766879558563232, loss=1.754524827003479
I0531 02:39:26.910509 140080360982272 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.9392614364624023, loss=1.7142295837402344
I0531 02:41:01.393004 140080369374976 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.051881790161133, loss=1.7814934253692627
I0531 02:42:36.758990 140080360982272 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.178356170654297, loss=1.7162446975708008
I0531 02:44:11.134998 140080369374976 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.2065212726593018, loss=1.6804465055465698
I0531 02:45:49.735741 140080369374976 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.424603223800659, loss=1.7434641122817993
I0531 02:47:23.778294 140080360982272 logging_writer.py:48] [7400] global_step=7400, grad_norm=5.781767845153809, loss=1.7658729553222656
I0531 02:48:58.804224 140080369374976 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.395017385482788, loss=1.6734477281570435
I0531 02:49:26.133697 140238962607936 spec.py:298] Evaluating on the training split.
I0531 02:50:13.192495 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 02:50:56.082827 140238962607936 spec.py:326] Evaluating on the test split.
I0531 02:51:17.762768 140238962607936 submission_runner.py:426] Time since start: 7797.79s, 	Step: 7530, 	{'train/ctc_loss': Array(0.47335505, dtype=float32), 'train/wer': 0.16832445140239627, 'validation/ctc_loss': Array(0.9037752, dtype=float32), 'validation/wer': 0.25001688390626053, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5573295, dtype=float32), 'test/wer': 0.18056994292446124, 'test/num_examples': 2472, 'score': 7289.176803588867, 'total_duration': 7797.788488864899, 'accumulated_submission_time': 7289.176803588867, 'accumulated_data_selection_time': 927.8564352989197, 'accumulated_eval_time': 508.41004610061646, 'accumulated_logging_time': 0.10659909248352051}
I0531 02:51:17.784570 140080369374976 logging_writer.py:48] [7530] accumulated_data_selection_time=927.856435, accumulated_eval_time=508.410046, accumulated_logging_time=0.106599, accumulated_submission_time=7289.176804, global_step=7530, preemption_count=0, score=7289.176804, test/ctc_loss=0.5573294758796692, test/num_examples=2472, test/wer=0.180570, total_duration=7797.788489, train/ctc_loss=0.4733550548553467, train/wer=0.168324, validation/ctc_loss=0.9037752151489258, validation/num_examples=5348, validation/wer=0.250017
I0531 02:52:25.626419 140080360982272 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.2469398975372314, loss=1.7354050874710083
I0531 02:54:01.752466 140080369374976 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.714028835296631, loss=1.740953803062439
I0531 02:55:36.502830 140080360982272 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.0473837852478027, loss=1.706496000289917
I0531 02:57:11.346916 140080369374976 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.376231670379639, loss=1.72550630569458
I0531 02:58:46.038867 140080360982272 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.5070912837982178, loss=1.6988723278045654
I0531 03:00:21.228900 140080369374976 logging_writer.py:48] [8100] global_step=8100, grad_norm=5.0121750831604, loss=1.7048834562301636
I0531 03:01:56.178966 140080360982272 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.1530168056488037, loss=1.7540972232818604
I0531 03:03:35.976230 140080697054976 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.79833459854126, loss=1.7500935792922974
I0531 03:05:11.561029 140080688662272 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.6197187900543213, loss=1.7032849788665771
I0531 03:06:46.217143 140080697054976 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.935901641845703, loss=1.6753227710723877
I0531 03:08:21.585232 140080688662272 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.0232696533203125, loss=1.643738865852356
I0531 03:09:56.839258 140080697054976 logging_writer.py:48] [8700] global_step=8700, grad_norm=8.154077529907227, loss=1.711821436882019
I0531 03:11:32.536288 140080688662272 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.5065972805023193, loss=1.7126294374465942
I0531 03:13:08.148236 140080697054976 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.527852773666382, loss=1.663367748260498
I0531 03:14:43.370573 140080688662272 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.4640307426452637, loss=1.697843313217163
I0531 03:16:18.647889 140080697054976 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.414461374282837, loss=1.7265554666519165
I0531 03:17:55.742927 140080688662272 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.575709104537964, loss=1.6904584169387817
I0531 03:19:35.978551 140080369374976 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.721137285232544, loss=1.6464134454727173
I0531 03:21:11.912438 140080360982272 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.982557535171509, loss=1.646622657775879
I0531 03:22:46.692213 140080369374976 logging_writer.py:48] [9500] global_step=9500, grad_norm=6.324173450469971, loss=1.6422926187515259
I0531 03:24:22.575874 140080360982272 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.103878021240234, loss=1.7117372751235962
I0531 03:25:58.491903 140080369374976 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.6190638542175293, loss=1.7851800918579102
I0531 03:27:33.873763 140080360982272 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.2997331619262695, loss=1.6981085538864136
I0531 03:29:08.350451 140080369374976 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.3611741065979, loss=1.6747342348098755
I0531 03:30:44.347818 140080360982272 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.972278594970703, loss=1.683053970336914
I0531 03:31:18.552999 140238962607936 spec.py:298] Evaluating on the training split.
I0531 03:32:05.888882 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 03:32:47.577516 140238962607936 spec.py:326] Evaluating on the test split.
I0531 03:33:08.977324 140238962607936 submission_runner.py:426] Time since start: 10309.00s, 	Step: 10038, 	{'train/ctc_loss': Array(0.40004587, dtype=float32), 'train/wer': 0.13935112944946537, 'validation/ctc_loss': Array(0.75593793, dtype=float32), 'validation/wer': 0.2157280822776872, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46014366, dtype=float32), 'test/wer': 0.1479698576158268, 'test/num_examples': 2472, 'score': 9689.901355028152, 'total_duration': 10309.002899169922, 'accumulated_submission_time': 9689.901355028152, 'accumulated_data_selection_time': 1249.5761561393738, 'accumulated_eval_time': 618.8304512500763, 'accumulated_logging_time': 0.1433861255645752}
I0531 03:33:08.997994 140080369374976 logging_writer.py:48] [10038] accumulated_data_selection_time=1249.576156, accumulated_eval_time=618.830451, accumulated_logging_time=0.143386, accumulated_submission_time=9689.901355, global_step=10038, preemption_count=0, score=9689.901355, test/ctc_loss=0.46014365553855896, test/num_examples=2472, test/wer=0.147970, total_duration=10309.002899, train/ctc_loss=0.40004587173461914, train/wer=0.139351, validation/ctc_loss=0.755937933921814, validation/num_examples=5348, validation/wer=0.215728
I0531 03:34:09.394985 140080360982272 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.9801769256591797, loss=1.7257475852966309
I0531 03:35:44.947326 140080369374976 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.061948299407959, loss=1.6738026142120361
I0531 03:37:24.645764 140080697054976 logging_writer.py:48] [10300] global_step=10300, grad_norm=10.181458473205566, loss=1.6470738649368286
I0531 03:38:59.726683 140080688662272 logging_writer.py:48] [10400] global_step=10400, grad_norm=6.6560564041137695, loss=1.6558820009231567
I0531 03:40:34.364402 140080697054976 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.932046413421631, loss=1.6283615827560425
I0531 03:42:10.906150 140080688662272 logging_writer.py:48] [10600] global_step=10600, grad_norm=5.676036357879639, loss=1.6172863245010376
I0531 03:43:47.628736 140080697054976 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.640843152999878, loss=1.6716797351837158
I0531 03:45:23.228760 140080688662272 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.836995840072632, loss=1.670087456703186
I0531 03:46:58.860954 140080697054976 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.190371036529541, loss=1.6276153326034546
I0531 03:48:34.204498 140080688662272 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.781219005584717, loss=1.5442514419555664
I0531 03:50:09.654644 140080697054976 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.92380952835083, loss=1.6287671327590942
I0531 03:51:44.492125 140080688662272 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.713611602783203, loss=1.6185693740844727
I0531 03:53:19.828521 140080697054976 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.756304979324341, loss=1.621973991394043
I0531 03:54:59.032650 140080697054976 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.195833921432495, loss=1.6226505041122437
I0531 03:56:33.695021 140080688662272 logging_writer.py:48] [11500] global_step=11500, grad_norm=5.110889911651611, loss=1.6689541339874268
I0531 03:58:09.204051 140080697054976 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.9577105045318604, loss=1.5990080833435059
I0531 03:59:43.672182 140080688662272 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.3841471672058105, loss=1.6384296417236328
I0531 04:01:18.304001 140080697054976 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.197129726409912, loss=1.6423026323318481
I0531 04:02:52.951158 140080688662272 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.4687066078186035, loss=1.7075549364089966
I0531 04:04:28.528020 140080697054976 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.799664258956909, loss=1.6076050996780396
I0531 04:06:05.275540 140080688662272 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.2628848552703857, loss=1.5873525142669678
I0531 04:07:39.537761 140080697054976 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.5641889572143555, loss=1.648863673210144
I0531 04:09:14.057925 140080688662272 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.887904405593872, loss=1.6306761503219604
I0531 04:10:52.993268 140080369374976 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.105909585952759, loss=1.5110085010528564
I0531 04:12:28.778485 140080360982272 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.516468524932861, loss=1.682443618774414
I0531 04:13:09.750928 140238962607936 spec.py:298] Evaluating on the training split.
I0531 04:13:55.714157 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 04:14:39.431471 140238962607936 spec.py:326] Evaluating on the test split.
I0531 04:15:01.744200 140238962607936 submission_runner.py:426] Time since start: 12821.77s, 	Step: 12544, 	{'train/ctc_loss': Array(0.3643291, dtype=float32), 'train/wer': 0.12812302942007872, 'validation/ctc_loss': Array(0.7125251, dtype=float32), 'validation/wer': 0.20487414253876063, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41939658, dtype=float32), 'test/wer': 0.1374281477870534, 'test/num_examples': 2472, 'score': 12090.61033320427, 'total_duration': 12821.769832849503, 'accumulated_submission_time': 12090.61033320427, 'accumulated_data_selection_time': 1577.124088525772, 'accumulated_eval_time': 730.8198707103729, 'accumulated_logging_time': 0.17867493629455566}
I0531 04:15:01.770716 140080369374976 logging_writer.py:48] [12544] accumulated_data_selection_time=1577.124089, accumulated_eval_time=730.819871, accumulated_logging_time=0.178675, accumulated_submission_time=12090.610333, global_step=12544, preemption_count=0, score=12090.610333, test/ctc_loss=0.4193965792655945, test/num_examples=2472, test/wer=0.137428, total_duration=12821.769833, train/ctc_loss=0.36432909965515137, train/wer=0.128123, validation/ctc_loss=0.7125251293182373, validation/num_examples=5348, validation/wer=0.204874
I0531 04:15:55.839711 140080360982272 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.7143309116363525, loss=1.621737003326416
I0531 04:17:31.101337 140080369374976 logging_writer.py:48] [12700] global_step=12700, grad_norm=5.823477268218994, loss=1.6711695194244385
I0531 04:19:06.427186 140080360982272 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.1815054416656494, loss=1.561214804649353
I0531 04:20:42.378656 140080369374976 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.045275926589966, loss=1.6980504989624023
I0531 04:22:17.948424 140080360982272 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.201429605484009, loss=1.6295026540756226
I0531 04:23:55.308265 140080369374976 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.040712833404541, loss=1.595656156539917
I0531 04:25:30.894553 140080360982272 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.844642162322998, loss=1.5911211967468262
I0531 04:27:05.612520 140080369374976 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.504354000091553, loss=1.6170587539672852
I0531 04:28:44.049376 140080697054976 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.21401309967041, loss=1.5616531372070312
I0531 04:30:19.587360 140080688662272 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.8008317947387695, loss=1.6052460670471191
I0531 04:31:54.791511 140080697054976 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.199113368988037, loss=1.6352726221084595
I0531 04:33:30.540529 140080688662272 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.153834581375122, loss=1.5814322233200073
I0531 04:35:06.672714 140080697054976 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.887300491333008, loss=1.6303753852844238
I0531 04:36:41.384178 140080688662272 logging_writer.py:48] [13900] global_step=13900, grad_norm=5.6107306480407715, loss=1.5967925786972046
I0531 04:38:17.466508 140080697054976 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.384459972381592, loss=1.5289722681045532
I0531 04:39:52.075927 140080688662272 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.8152942657470703, loss=1.6302765607833862
I0531 04:41:27.057598 140080697054976 logging_writer.py:48] [14200] global_step=14200, grad_norm=5.638606071472168, loss=1.6322354078292847
I0531 04:43:03.188455 140080688662272 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.954696178436279, loss=1.59120512008667
I0531 04:44:38.584587 140080697054976 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.4220757484436035, loss=1.6297695636749268
I0531 04:46:18.661247 140080697054976 logging_writer.py:48] [14500] global_step=14500, grad_norm=6.119821071624756, loss=1.674055576324463
I0531 04:47:54.389216 140080688662272 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.297722816467285, loss=1.594609022140503
I0531 04:49:29.699041 140080697054976 logging_writer.py:48] [14700] global_step=14700, grad_norm=6.1066999435424805, loss=1.6066457033157349
I0531 04:51:05.088233 140080688662272 logging_writer.py:48] [14800] global_step=14800, grad_norm=5.604131698608398, loss=1.5965789556503296
I0531 04:52:40.645812 140080697054976 logging_writer.py:48] [14900] global_step=14900, grad_norm=4.277860641479492, loss=1.5835330486297607
I0531 04:54:15.461589 140080688662272 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.593100070953369, loss=1.6324162483215332
I0531 04:55:02.622118 140238962607936 spec.py:298] Evaluating on the training split.
I0531 04:55:49.516177 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 04:56:33.133871 140238962607936 spec.py:326] Evaluating on the test split.
I0531 04:56:54.258273 140238962607936 submission_runner.py:426] Time since start: 15334.28s, 	Step: 15051, 	{'train/ctc_loss': Array(0.36333278, dtype=float32), 'train/wer': 0.12310417122341305, 'validation/ctc_loss': Array(0.67571896, dtype=float32), 'validation/wer': 0.19142490520892627, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39203945, dtype=float32), 'test/wer': 0.12840980643064612, 'test/num_examples': 2472, 'score': 14491.414714813232, 'total_duration': 15334.283890724182, 'accumulated_submission_time': 14491.414714813232, 'accumulated_data_selection_time': 1901.0394034385681, 'accumulated_eval_time': 842.4521553516388, 'accumulated_logging_time': 0.22276926040649414}
I0531 04:56:54.280112 140081127134976 logging_writer.py:48] [15051] accumulated_data_selection_time=1901.039403, accumulated_eval_time=842.452155, accumulated_logging_time=0.222769, accumulated_submission_time=14491.414715, global_step=15051, preemption_count=0, score=14491.414715, test/ctc_loss=0.3920394480228424, test/num_examples=2472, test/wer=0.128410, total_duration=15334.283891, train/ctc_loss=0.3633327782154083, train/wer=0.123104, validation/ctc_loss=0.6757189631462097, validation/num_examples=5348, validation/wer=0.191425
I0531 04:57:42.050184 140081118742272 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.6204581260681152, loss=1.5844120979309082
I0531 04:59:16.888997 140081127134976 logging_writer.py:48] [15200] global_step=15200, grad_norm=5.449413299560547, loss=1.6204369068145752
I0531 05:00:52.027137 140081118742272 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.669585227966309, loss=1.6286625862121582
I0531 05:02:27.761181 140081127134976 logging_writer.py:48] [15400] global_step=15400, grad_norm=5.65891170501709, loss=1.6226781606674194
I0531 05:04:08.011013 140080471774976 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.021634578704834, loss=1.5827767848968506
I0531 05:05:43.403719 140080463382272 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.938232898712158, loss=1.5485273599624634
I0531 05:07:18.313363 140080471774976 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.4949700832366943, loss=1.549089789390564
I0531 05:08:54.965283 140080463382272 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.6314897537231445, loss=1.630459189414978
I0531 05:10:30.089948 140080471774976 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.0068435668945312, loss=1.5447814464569092
I0531 05:12:04.297402 140238962607936 spec.py:298] Evaluating on the training split.
I0531 05:12:50.333961 140238962607936 spec.py:310] Evaluating on the validation split.
I0531 05:13:33.536038 140238962607936 spec.py:326] Evaluating on the test split.
I0531 05:13:56.099270 140238962607936 submission_runner.py:426] Time since start: 16356.13s, 	Step: 16000, 	{'train/ctc_loss': Array(0.3426937, dtype=float32), 'train/wer': 0.11905749473549869, 'validation/ctc_loss': Array(0.6896118, dtype=float32), 'validation/wer': 0.19754170324846357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39417407, dtype=float32), 'test/wer': 0.12824731379359372, 'test/num_examples': 2472, 'score': 15401.404834270477, 'total_duration': 16356.126091003418, 'accumulated_submission_time': 15401.404834270477, 'accumulated_data_selection_time': 2024.575956106186, 'accumulated_eval_time': 954.2513537406921, 'accumulated_logging_time': 0.26049351692199707}
I0531 05:13:56.120334 140080328414976 logging_writer.py:48] [16000] accumulated_data_selection_time=2024.575956, accumulated_eval_time=954.251354, accumulated_logging_time=0.260494, accumulated_submission_time=15401.404834, global_step=16000, preemption_count=0, score=15401.404834, test/ctc_loss=0.39417406916618347, test/num_examples=2472, test/wer=0.128247, total_duration=16356.126091, train/ctc_loss=0.3426936864852905, train/wer=0.119057, validation/ctc_loss=0.6896117925643921, validation/num_examples=5348, validation/wer=0.197542
I0531 05:13:56.146962 140080320022272 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=15401.404834
I0531 05:13:56.696446 140238962607936 checkpoints.py:490] Saving checkpoint at step: 16000
I0531 05:13:58.106129 140238962607936 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0531 05:13:58.139317 140238962607936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_jax_upgrade_a/shampoo/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0531 05:13:59.369747 140238962607936 submission_runner.py:589] Tuning trial 1/1
I0531 05:13:59.369987 140238962607936 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0531 05:13:59.384760 140238962607936 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.835112, dtype=float32), 'train/wer': 2.279563958936343, 'validation/ctc_loss': Array(30.910616, dtype=float32), 'validation/wer': 1.9400573088018216, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.001806, dtype=float32), 'test/wer': 2.1327158613125343, 'test/num_examples': 2472, 'score': 88.18640112876892, 'total_duration': 265.03253841400146, 'accumulated_submission_time': 88.18640112876892, 'accumulated_data_selection_time': 4.732028007507324, 'accumulated_eval_time': 176.84596276283264, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2497, {'train/ctc_loss': Array(1.9162618, dtype=float32), 'train/wer': 0.4675907808284072, 'validation/ctc_loss': Array(2.3616385, dtype=float32), 'validation/wer': 0.5303765593493425, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.873541, dtype=float32), 'test/wer': 0.46196656713992645, 'test/num_examples': 2472, 'score': 2488.2972922325134, 'total_duration': 2775.2840163707733, 'accumulated_submission_time': 2488.2972922325134, 'accumulated_data_selection_time': 289.3287010192871, 'accumulated_eval_time': 286.92539978027344, 'accumulated_logging_time': 0.03166317939758301, 'global_step': 2497, 'preemption_count': 0}), (5013, {'train/ctc_loss': Array(0.6295858, dtype=float32), 'train/wer': 0.22274724982020616, 'validation/ctc_loss': Array(1.0331788, dtype=float32), 'validation/wer': 0.29620160348869745, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6761152, dtype=float32), 'test/wer': 0.22521479495460361, 'test/num_examples': 2472, 'score': 4888.8670110702515, 'total_duration': 5285.783096075058, 'accumulated_submission_time': 4888.8670110702515, 'accumulated_data_selection_time': 603.6657586097717, 'accumulated_eval_time': 396.78474855422974, 'accumulated_logging_time': 0.06890869140625, 'global_step': 5013, 'preemption_count': 0}), (7530, {'train/ctc_loss': Array(0.47335505, dtype=float32), 'train/wer': 0.16832445140239627, 'validation/ctc_loss': Array(0.9037752, dtype=float32), 'validation/wer': 0.25001688390626053, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5573295, dtype=float32), 'test/wer': 0.18056994292446124, 'test/num_examples': 2472, 'score': 7289.176803588867, 'total_duration': 7797.788488864899, 'accumulated_submission_time': 7289.176803588867, 'accumulated_data_selection_time': 927.8564352989197, 'accumulated_eval_time': 508.41004610061646, 'accumulated_logging_time': 0.10659909248352051, 'global_step': 7530, 'preemption_count': 0}), (10038, {'train/ctc_loss': Array(0.40004587, dtype=float32), 'train/wer': 0.13935112944946537, 'validation/ctc_loss': Array(0.75593793, dtype=float32), 'validation/wer': 0.2157280822776872, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46014366, dtype=float32), 'test/wer': 0.1479698576158268, 'test/num_examples': 2472, 'score': 9689.901355028152, 'total_duration': 10309.002899169922, 'accumulated_submission_time': 9689.901355028152, 'accumulated_data_selection_time': 1249.5761561393738, 'accumulated_eval_time': 618.8304512500763, 'accumulated_logging_time': 0.1433861255645752, 'global_step': 10038, 'preemption_count': 0}), (12544, {'train/ctc_loss': Array(0.3643291, dtype=float32), 'train/wer': 0.12812302942007872, 'validation/ctc_loss': Array(0.7125251, dtype=float32), 'validation/wer': 0.20487414253876063, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41939658, dtype=float32), 'test/wer': 0.1374281477870534, 'test/num_examples': 2472, 'score': 12090.61033320427, 'total_duration': 12821.769832849503, 'accumulated_submission_time': 12090.61033320427, 'accumulated_data_selection_time': 1577.124088525772, 'accumulated_eval_time': 730.8198707103729, 'accumulated_logging_time': 0.17867493629455566, 'global_step': 12544, 'preemption_count': 0}), (15051, {'train/ctc_loss': Array(0.36333278, dtype=float32), 'train/wer': 0.12310417122341305, 'validation/ctc_loss': Array(0.67571896, dtype=float32), 'validation/wer': 0.19142490520892627, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39203945, dtype=float32), 'test/wer': 0.12840980643064612, 'test/num_examples': 2472, 'score': 14491.414714813232, 'total_duration': 15334.283890724182, 'accumulated_submission_time': 14491.414714813232, 'accumulated_data_selection_time': 1901.0394034385681, 'accumulated_eval_time': 842.4521553516388, 'accumulated_logging_time': 0.22276926040649414, 'global_step': 15051, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.3426937, dtype=float32), 'train/wer': 0.11905749473549869, 'validation/ctc_loss': Array(0.6896118, dtype=float32), 'validation/wer': 0.19754170324846357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39417407, dtype=float32), 'test/wer': 0.12824731379359372, 'test/num_examples': 2472, 'score': 15401.404834270477, 'total_duration': 16356.126091003418, 'accumulated_submission_time': 15401.404834270477, 'accumulated_data_selection_time': 2024.575956106186, 'accumulated_eval_time': 954.2513537406921, 'accumulated_logging_time': 0.26049351692199707, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0531 05:13:59.384928 140238962607936 submission_runner.py:592] Timing: 15401.404834270477
I0531 05:13:59.384999 140238962607936 submission_runner.py:593] ====================
I0531 05:13:59.385897 140238962607936 submission_runner.py:661] Final librispeech_deepspeech score: 15401.404834270477
