python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_04-28-2023-08-36-05.log
I0428 08:36:27.465195 139774329104192 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax.
I0428 08:36:27.535888 139774329104192 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 08:36:28.375442 139774329104192 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0428 08:36:28.376127 139774329104192 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 08:36:28.379818 139774329104192 submission_runner.py:538] Using RNG seed 2831562027
I0428 08:36:31.239987 139774329104192 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 08:36:31.240198 139774329104192 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1.
I0428 08:36:31.242050 139774329104192 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1/hparams.json.
I0428 08:36:31.362155 139774329104192 submission_runner.py:241] Initializing dataset.
I0428 08:36:31.373662 139774329104192 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:36:31.380990 139774329104192 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 08:36:31.381106 139774329104192 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 08:36:31.642539 139774329104192 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:36:38.220060 139774329104192 submission_runner.py:248] Initializing model.
I0428 08:36:49.154443 139774329104192 submission_runner.py:258] Initializing optimizer.
I0428 08:36:51.040010 139774329104192 submission_runner.py:265] Initializing metrics bundle.
I0428 08:36:51.040219 139774329104192 submission_runner.py:282] Initializing checkpoint and logger.
I0428 08:36:51.041144 139774329104192 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0428 08:36:51.869289 139774329104192 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1/meta_data_0.json.
I0428 08:36:51.870525 139774329104192 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1/flags_0.json.
I0428 08:36:51.875389 139774329104192 submission_runner.py:318] Starting training loop.
I0428 08:38:38.877757 139595530368768 logging_writer.py:48] [0] global_step=0, grad_norm=0.31969061493873596, loss=6.907756805419922
I0428 08:38:38.901548 139774329104192 spec.py:298] Evaluating on the training split.
I0428 08:38:38.908632 139774329104192 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:38:38.914666 139774329104192 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 08:38:38.914785 139774329104192 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 08:38:38.975672 139774329104192 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:38:58.814495 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 08:38:58.821716 139774329104192 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:38:58.845688 139774329104192 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 08:38:58.845999 139774329104192 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 08:38:58.908908 139774329104192 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 08:39:18.737796 139774329104192 spec.py:326] Evaluating on the test split.
I0428 08:39:18.744260 139774329104192 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 08:39:18.749923 139774329104192 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 08:39:18.781415 139774329104192 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 08:39:29.870218 139774329104192 submission_runner.py:415] Time since start: 157.99s, 	Step: 1, 	{'train/accuracy': 0.0032617186661809683, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.002759999828413129, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0014000000664964318, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 107.02599263191223, 'total_duration': 157.99477076530457, 'accumulated_submission_time': 107.02599263191223, 'accumulated_eval_time': 50.96862816810608, 'accumulated_logging_time': 0}
I0428 08:39:29.885591 139545400030976 logging_writer.py:48] [1] accumulated_eval_time=50.968628, accumulated_logging_time=0, accumulated_submission_time=107.025993, global_step=1, preemption_count=0, score=107.025993, test/accuracy=0.001400, test/loss=6.907757, test/num_examples=10000, total_duration=157.994771, train/accuracy=0.003262, train/loss=6.907756, validation/accuracy=0.002760, validation/loss=6.907756, validation/num_examples=50000
I0428 08:41:29.891035 139592183072512 logging_writer.py:48] [100] global_step=100, grad_norm=0.4298735558986664, loss=6.897900581359863
I0428 08:42:16.765696 139592191465216 logging_writer.py:48] [200] global_step=200, grad_norm=0.6199318766593933, loss=6.861610412597656
I0428 08:43:03.328302 139592183072512 logging_writer.py:48] [300] global_step=300, grad_norm=0.7043009996414185, loss=6.782431602478027
I0428 08:43:50.048972 139592191465216 logging_writer.py:48] [400] global_step=400, grad_norm=0.6887302398681641, loss=6.72137451171875
I0428 08:44:36.405380 139592183072512 logging_writer.py:48] [500] global_step=500, grad_norm=1.0436347723007202, loss=6.617049217224121
I0428 08:45:23.201092 139592191465216 logging_writer.py:48] [600] global_step=600, grad_norm=1.3367364406585693, loss=6.590241432189941
I0428 08:46:09.654760 139592183072512 logging_writer.py:48] [700] global_step=700, grad_norm=0.8432297706604004, loss=6.756617546081543
I0428 08:46:30.051678 139774329104192 spec.py:298] Evaluating on the training split.
I0428 08:46:36.546473 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 08:46:43.293641 139774329104192 spec.py:326] Evaluating on the test split.
I0428 08:46:45.034014 139774329104192 submission_runner.py:415] Time since start: 593.16s, 	Step: 745, 	{'train/accuracy': 0.018808593973517418, 'train/loss': 6.263822078704834, 'validation/accuracy': 0.01735999993979931, 'validation/loss': 6.2798686027526855, 'validation/num_examples': 50000, 'test/accuracy': 0.013100001029670238, 'test/loss': 6.352320671081543, 'test/num_examples': 10000, 'score': 527.1725649833679, 'total_duration': 593.1585285663605, 'accumulated_submission_time': 527.1725649833679, 'accumulated_eval_time': 65.95091533660889, 'accumulated_logging_time': 0.02413630485534668}
I0428 08:46:45.044256 139545651681024 logging_writer.py:48] [745] accumulated_eval_time=65.950915, accumulated_logging_time=0.024136, accumulated_submission_time=527.172565, global_step=745, preemption_count=0, score=527.172565, test/accuracy=0.013100, test/loss=6.352321, test/num_examples=10000, total_duration=593.158529, train/accuracy=0.018809, train/loss=6.263822, validation/accuracy=0.017360, validation/loss=6.279869, validation/num_examples=50000
I0428 08:47:11.781439 139545660073728 logging_writer.py:48] [800] global_step=800, grad_norm=0.8822119235992432, loss=6.478127956390381
I0428 08:47:58.442413 139545651681024 logging_writer.py:48] [900] global_step=900, grad_norm=1.346081256866455, loss=6.63425350189209
I0428 08:48:45.285547 139545660073728 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9864327907562256, loss=6.36957311630249
I0428 08:49:31.799458 139545651681024 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.9665695428848267, loss=6.746964931488037
I0428 08:50:18.690175 139545660073728 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8437314033508301, loss=6.536721229553223
I0428 08:51:05.222766 139545651681024 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.089713215827942, loss=6.56235933303833
I0428 08:51:51.828173 139545660073728 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.137009620666504, loss=6.2170209884643555
I0428 08:52:38.368048 139545651681024 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.1052165031433105, loss=6.1650190353393555
I0428 08:53:25.355419 139545660073728 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.881103515625, loss=6.169025421142578
I0428 08:53:45.345826 139774329104192 spec.py:298] Evaluating on the training split.
I0428 08:53:51.845986 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 08:53:58.738632 139774329104192 spec.py:326] Evaluating on the test split.
I0428 08:54:00.428354 139774329104192 submission_runner.py:415] Time since start: 1028.55s, 	Step: 1644, 	{'train/accuracy': 0.05570312216877937, 'train/loss': 5.588287353515625, 'validation/accuracy': 0.05297999829053879, 'validation/loss': 5.628650665283203, 'validation/num_examples': 50000, 'test/accuracy': 0.042100001126527786, 'test/loss': 5.783792495727539, 'test/num_examples': 10000, 'score': 947.449645280838, 'total_duration': 1028.5527851581573, 'accumulated_submission_time': 947.449645280838, 'accumulated_eval_time': 81.03330445289612, 'accumulated_logging_time': 0.046282291412353516}
I0428 08:54:00.440964 139545651681024 logging_writer.py:48] [1644] accumulated_eval_time=81.033304, accumulated_logging_time=0.046282, accumulated_submission_time=947.449645, global_step=1644, preemption_count=0, score=947.449645, test/accuracy=0.042100, test/loss=5.783792, test/num_examples=10000, total_duration=1028.552785, train/accuracy=0.055703, train/loss=5.588287, validation/accuracy=0.052980, validation/loss=5.628651, validation/num_examples=50000
I0428 08:54:27.354336 139545660073728 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0096080303192139, loss=6.127830982208252
I0428 08:55:14.266621 139545651681024 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1644879579544067, loss=6.230992317199707
I0428 08:56:00.702182 139545660073728 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.164015531539917, loss=6.019145965576172
I0428 08:56:47.481579 139545651681024 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.755785346031189, loss=6.63401985168457
I0428 08:57:33.910493 139545660073728 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7343408465385437, loss=6.1562676429748535
I0428 08:58:20.712178 139545651681024 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.663129448890686, loss=5.937702655792236
I0428 08:59:07.293866 139545660073728 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9005829095840454, loss=5.9852190017700195
I0428 08:59:54.084022 139545651681024 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9036079049110413, loss=5.857490539550781
I0428 09:00:40.902076 139545660073728 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6126438975334167, loss=6.6687211990356445
I0428 09:01:00.695214 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:01:07.213229 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:01:14.154203 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:01:15.849837 139774329104192 submission_runner.py:415] Time since start: 1463.97s, 	Step: 2543, 	{'train/accuracy': 0.08722656220197678, 'train/loss': 5.146496295928955, 'validation/accuracy': 0.08201999962329865, 'validation/loss': 5.189975261688232, 'validation/num_examples': 50000, 'test/accuracy': 0.06050000339746475, 'test/loss': 5.418995380401611, 'test/num_examples': 10000, 'score': 1367.6769676208496, 'total_duration': 1463.9743514060974, 'accumulated_submission_time': 1367.6769676208496, 'accumulated_eval_time': 96.18785119056702, 'accumulated_logging_time': 0.07312345504760742}
I0428 09:01:15.860675 139545651681024 logging_writer.py:48] [2543] accumulated_eval_time=96.187851, accumulated_logging_time=0.073123, accumulated_submission_time=1367.676968, global_step=2543, preemption_count=0, score=1367.676968, test/accuracy=0.060500, test/loss=5.418995, test/num_examples=10000, total_duration=1463.974351, train/accuracy=0.087227, train/loss=5.146496, validation/accuracy=0.082020, validation/loss=5.189975, validation/num_examples=50000
I0428 09:01:43.329273 139545660073728 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7859575748443604, loss=5.891216278076172
I0428 09:02:29.892586 139545651681024 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8701198101043701, loss=6.337839126586914
I0428 09:03:16.929992 139545660073728 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6326217651367188, loss=6.476202011108398
I0428 09:04:03.664957 139545651681024 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7529398798942566, loss=5.964300632476807
I0428 09:04:50.567319 139545660073728 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9775087833404541, loss=5.708465576171875
I0428 09:05:37.191694 139545651681024 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8752625584602356, loss=6.08589506149292
I0428 09:06:24.180856 139545660073728 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.628344714641571, loss=6.612123966217041
I0428 09:07:10.943963 139545651681024 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.73850017786026, loss=5.659982204437256
I0428 09:07:57.862635 139545660073728 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7302057147026062, loss=5.542381763458252
I0428 09:08:16.016639 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:08:22.545869 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:08:29.687103 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:08:31.355190 139774329104192 submission_runner.py:415] Time since start: 1899.48s, 	Step: 3440, 	{'train/accuracy': 0.13539062440395355, 'train/loss': 4.67137336730957, 'validation/accuracy': 0.12751999497413635, 'validation/loss': 4.733416557312012, 'validation/num_examples': 50000, 'test/accuracy': 0.1007000058889389, 'test/loss': 5.028800010681152, 'test/num_examples': 10000, 'score': 1787.8109254837036, 'total_duration': 1899.4797015190125, 'accumulated_submission_time': 1787.8109254837036, 'accumulated_eval_time': 111.52636170387268, 'accumulated_logging_time': 0.09340214729309082}
I0428 09:08:31.366426 139545651681024 logging_writer.py:48] [3440] accumulated_eval_time=111.526362, accumulated_logging_time=0.093402, accumulated_submission_time=1787.810925, global_step=3440, preemption_count=0, score=1787.810925, test/accuracy=0.100700, test/loss=5.028800, test/num_examples=10000, total_duration=1899.479702, train/accuracy=0.135391, train/loss=4.671373, validation/accuracy=0.127520, validation/loss=4.733417, validation/num_examples=50000
I0428 09:09:00.477642 139545660073728 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6306257843971252, loss=6.584432125091553
I0428 09:09:48.225800 139545651681024 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7789303660392761, loss=5.575999736785889
I0428 09:10:35.222368 139545660073728 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7251016497612, loss=5.70680046081543
I0428 09:11:22.563286 139545651681024 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7674453258514404, loss=5.52828311920166
I0428 09:12:09.296164 139545660073728 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8819100856781006, loss=5.43826150894165
I0428 09:12:56.030481 139545651681024 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7756780982017517, loss=5.529737949371338
I0428 09:13:42.681665 139545660073728 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7055956125259399, loss=6.5198822021484375
I0428 09:14:29.870242 139545651681024 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7401942014694214, loss=5.704146385192871
I0428 09:15:16.739573 139545660073728 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6536166071891785, loss=6.290011882781982
I0428 09:15:31.564991 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:15:38.093148 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:15:45.359263 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:15:47.053938 139774329104192 submission_runner.py:415] Time since start: 2335.18s, 	Step: 4332, 	{'train/accuracy': 0.17460937798023224, 'train/loss': 4.370706081390381, 'validation/accuracy': 0.1621599942445755, 'validation/loss': 4.455780982971191, 'validation/num_examples': 50000, 'test/accuracy': 0.12759999930858612, 'test/loss': 4.804147243499756, 'test/num_examples': 10000, 'score': 2207.984035253525, 'total_duration': 2335.178426504135, 'accumulated_submission_time': 2207.984035253525, 'accumulated_eval_time': 127.01521921157837, 'accumulated_logging_time': 0.11716651916503906}
I0428 09:15:47.067987 139545651681024 logging_writer.py:48] [4332] accumulated_eval_time=127.015219, accumulated_logging_time=0.117167, accumulated_submission_time=2207.984035, global_step=4332, preemption_count=0, score=2207.984035, test/accuracy=0.127600, test/loss=4.804147, test/num_examples=10000, total_duration=2335.178427, train/accuracy=0.174609, train/loss=4.370706, validation/accuracy=0.162160, validation/loss=4.455781, validation/num_examples=50000
I0428 09:16:20.123872 139545660073728 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7785345315933228, loss=5.5293097496032715
I0428 09:17:07.835293 139545651681024 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.709378182888031, loss=6.255420684814453
I0428 09:17:54.828741 139545660073728 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6406322121620178, loss=5.3327226638793945
I0428 09:18:41.717118 139545651681024 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7869766354560852, loss=5.342894077301025
I0428 09:19:28.996058 139545660073728 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.821027398109436, loss=5.262985706329346
I0428 09:20:16.296917 139545651681024 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7132726311683655, loss=6.429335594177246
I0428 09:21:03.272599 139545660073728 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6415821313858032, loss=6.387104034423828
I0428 09:21:50.190885 139545651681024 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7463502883911133, loss=5.15138053894043
I0428 09:22:37.294421 139545660073728 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7251030206680298, loss=5.32982873916626
I0428 09:22:47.130464 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:22:53.650545 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:23:00.540852 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:23:02.222247 139774329104192 submission_runner.py:415] Time since start: 2770.35s, 	Step: 5222, 	{'train/accuracy': 0.21753905713558197, 'train/loss': 4.037476539611816, 'validation/accuracy': 0.20217999815940857, 'validation/loss': 4.138639450073242, 'validation/num_examples': 50000, 'test/accuracy': 0.15550000965595245, 'test/loss': 4.564212799072266, 'test/num_examples': 10000, 'score': 2628.025150537491, 'total_duration': 2770.346748113632, 'accumulated_submission_time': 2628.025150537491, 'accumulated_eval_time': 142.10692477226257, 'accumulated_logging_time': 0.1402263641357422}
I0428 09:23:02.236156 139545651681024 logging_writer.py:48] [5222] accumulated_eval_time=142.106925, accumulated_logging_time=0.140226, accumulated_submission_time=2628.025151, global_step=5222, preemption_count=0, score=2628.025151, test/accuracy=0.155500, test/loss=4.564213, test/num_examples=10000, total_duration=2770.346748, train/accuracy=0.217539, train/loss=4.037477, validation/accuracy=0.202180, validation/loss=4.138639, validation/num_examples=50000
I0428 09:23:40.139875 139545660073728 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7343572378158569, loss=5.07491397857666
I0428 09:24:27.616276 139545651681024 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7276262044906616, loss=5.086008071899414
I0428 09:25:15.386185 139545660073728 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8340768814086914, loss=5.194637775421143
I0428 09:26:02.329434 139545651681024 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6185508370399475, loss=6.234522342681885
I0428 09:26:49.734123 139545660073728 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7648813128471375, loss=4.994298934936523
I0428 09:27:37.085645 139545651681024 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7736879587173462, loss=5.0399041175842285
I0428 09:28:24.716520 139545660073728 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6421703696250916, loss=6.316027641296387
I0428 09:29:12.354751 139545651681024 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6931554079055786, loss=5.020472049713135
I0428 09:30:00.052348 139545660073728 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.664882481098175, loss=4.892186164855957
I0428 09:30:02.475903 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:30:08.986946 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:30:15.935035 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:30:17.611738 139774329104192 submission_runner.py:415] Time since start: 3205.74s, 	Step: 6106, 	{'train/accuracy': 0.2555468678474426, 'train/loss': 3.7493419647216797, 'validation/accuracy': 0.235959991812706, 'validation/loss': 3.865051746368408, 'validation/num_examples': 50000, 'test/accuracy': 0.17840000987052917, 'test/loss': 4.3103861808776855, 'test/num_examples': 10000, 'score': 3048.2380499839783, 'total_duration': 3205.7362217903137, 'accumulated_submission_time': 3048.2380499839783, 'accumulated_eval_time': 157.24267983436584, 'accumulated_logging_time': 0.1680469512939453}
I0428 09:30:17.625610 139545651681024 logging_writer.py:48] [6106] accumulated_eval_time=157.242680, accumulated_logging_time=0.168047, accumulated_submission_time=3048.238050, global_step=6106, preemption_count=0, score=3048.238050, test/accuracy=0.178400, test/loss=4.310386, test/num_examples=10000, total_duration=3205.736222, train/accuracy=0.255547, train/loss=3.749342, validation/accuracy=0.235960, validation/loss=3.865052, validation/num_examples=50000
I0428 09:31:03.318647 139545660073728 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7662985324859619, loss=4.9444379806518555
I0428 09:31:51.155039 139545651681024 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8007598519325256, loss=4.939090728759766
I0428 09:32:38.745758 139545660073728 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6715270280838013, loss=4.856347560882568
I0428 09:33:26.838339 139545651681024 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7075307369232178, loss=4.913796901702881
I0428 09:34:14.248495 139545660073728 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.868941605091095, loss=4.8246660232543945
I0428 09:35:02.251633 139545651681024 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6923072338104248, loss=5.420603275299072
I0428 09:35:49.870697 139545660073728 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6036425828933716, loss=4.834053039550781
I0428 09:36:37.820060 139545651681024 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6627448201179504, loss=4.801743030548096
I0428 09:37:17.656479 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:37:24.219620 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:37:31.344109 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:37:33.013550 139774329104192 submission_runner.py:415] Time since start: 3641.14s, 	Step: 6984, 	{'train/accuracy': 0.28974607586860657, 'train/loss': 3.5237984657287598, 'validation/accuracy': 0.2662400007247925, 'validation/loss': 3.6730382442474365, 'validation/num_examples': 50000, 'test/accuracy': 0.19680000841617584, 'test/loss': 4.183837413787842, 'test/num_examples': 10000, 'score': 3468.2391583919525, 'total_duration': 3641.138057947159, 'accumulated_submission_time': 3468.2391583919525, 'accumulated_eval_time': 172.59969520568848, 'accumulated_logging_time': 0.198502779006958}
I0428 09:37:33.027955 139545660073728 logging_writer.py:48] [6984] accumulated_eval_time=172.599695, accumulated_logging_time=0.198503, accumulated_submission_time=3468.239158, global_step=6984, preemption_count=0, score=3468.239158, test/accuracy=0.196800, test/loss=4.183837, test/num_examples=10000, total_duration=3641.138058, train/accuracy=0.289746, train/loss=3.523798, validation/accuracy=0.266240, validation/loss=3.673038, validation/num_examples=50000
I0428 09:37:41.580264 139545651681024 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5577393770217896, loss=5.49323034286499
I0428 09:38:29.855780 139545660073728 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7471514940261841, loss=5.267910957336426
I0428 09:39:17.560933 139545651681024 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7164630889892578, loss=4.8339104652404785
I0428 09:40:06.246594 139545660073728 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6443363428115845, loss=4.774868965148926
I0428 09:40:54.279961 139545651681024 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7199333310127258, loss=4.748055934906006
I0428 09:41:42.563416 139545660073728 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9264165759086609, loss=4.805821418762207
I0428 09:42:30.558190 139545651681024 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6596639156341553, loss=5.730541706085205
I0428 09:43:18.783337 139545660073728 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.687819242477417, loss=4.885945796966553
I0428 09:44:07.001528 139545651681024 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6851285696029663, loss=4.670767784118652
I0428 09:44:33.495579 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:44:40.081049 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:44:47.334546 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:44:49.010248 139774329104192 submission_runner.py:415] Time since start: 4077.13s, 	Step: 7856, 	{'train/accuracy': 0.3184570372104645, 'train/loss': 3.3432695865631104, 'validation/accuracy': 0.2991800010204315, 'validation/loss': 3.4610683917999268, 'validation/num_examples': 50000, 'test/accuracy': 0.22670000791549683, 'test/loss': 3.9797239303588867, 'test/num_examples': 10000, 'score': 3888.6776649951935, 'total_duration': 4077.13476228714, 'accumulated_submission_time': 3888.6776649951935, 'accumulated_eval_time': 188.1143114566803, 'accumulated_logging_time': 0.22864556312561035}
I0428 09:44:49.021830 139545660073728 logging_writer.py:48] [7856] accumulated_eval_time=188.114311, accumulated_logging_time=0.228646, accumulated_submission_time=3888.677665, global_step=7856, preemption_count=0, score=3888.677665, test/accuracy=0.226700, test/loss=3.979724, test/num_examples=10000, total_duration=4077.134762, train/accuracy=0.318457, train/loss=3.343270, validation/accuracy=0.299180, validation/loss=3.461068, validation/num_examples=50000
I0428 09:45:11.723738 139545651681024 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7786966562271118, loss=5.019004821777344
I0428 09:46:01.316944 139545660073728 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5099934935569763, loss=5.6001787185668945
I0428 09:46:51.232140 139545651681024 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.668846607208252, loss=5.369556903839111
I0428 09:47:40.592549 139545660073728 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6891031861305237, loss=5.107115268707275
I0428 09:48:30.055095 139545651681024 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6232143044471741, loss=4.608724594116211
I0428 09:49:19.140336 139545660073728 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6905511021614075, loss=5.857077598571777
I0428 09:50:08.265350 139545651681024 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8032171130180359, loss=4.636989593505859
I0428 09:50:57.084871 139545660073728 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6623007655143738, loss=5.480014801025391
I0428 09:51:46.270025 139545651681024 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7403556704521179, loss=4.529514312744141
I0428 09:51:49.199095 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:51:55.814539 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:52:03.290091 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:52:04.970411 139774329104192 submission_runner.py:415] Time since start: 4513.09s, 	Step: 8707, 	{'train/accuracy': 0.33406248688697815, 'train/loss': 3.229588270187378, 'validation/accuracy': 0.31129997968673706, 'validation/loss': 3.3596904277801514, 'validation/num_examples': 50000, 'test/accuracy': 0.23650000989437103, 'test/loss': 3.9032809734344482, 'test/num_examples': 10000, 'score': 4308.827301263809, 'total_duration': 4513.094922542572, 'accumulated_submission_time': 4308.827301263809, 'accumulated_eval_time': 203.88558101654053, 'accumulated_logging_time': 0.25287413597106934}
I0428 09:52:04.986032 139545660073728 logging_writer.py:48] [8707] accumulated_eval_time=203.885581, accumulated_logging_time=0.252874, accumulated_submission_time=4308.827301, global_step=8707, preemption_count=0, score=4308.827301, test/accuracy=0.236500, test/loss=3.903281, test/num_examples=10000, total_duration=4513.094923, train/accuracy=0.334062, train/loss=3.229588, validation/accuracy=0.311300, validation/loss=3.359690, validation/num_examples=50000
I0428 09:52:52.038609 139545651681024 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6637218594551086, loss=4.775925636291504
I0428 09:53:42.926662 139545660073728 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6436142921447754, loss=5.89301872253418
I0428 09:54:33.198028 139545651681024 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6374300122261047, loss=5.954828262329102
I0428 09:55:23.341148 139545660073728 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9148091673851013, loss=6.188659191131592
I0428 09:56:13.473507 139545651681024 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7221872806549072, loss=5.448686599731445
I0428 09:57:04.419820 139545660073728 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8070123791694641, loss=4.455700874328613
I0428 09:57:54.410985 139545651681024 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9159687757492065, loss=6.2473368644714355
I0428 09:58:44.601382 139545660073728 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7146733403205872, loss=4.503472328186035
I0428 09:59:05.365459 139774329104192 spec.py:298] Evaluating on the training split.
I0428 09:59:11.979059 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 09:59:19.787148 139774329104192 spec.py:326] Evaluating on the test split.
I0428 09:59:21.463871 139774329104192 submission_runner.py:415] Time since start: 4949.59s, 	Step: 9542, 	{'train/accuracy': 0.37013670802116394, 'train/loss': 3.058262825012207, 'validation/accuracy': 0.3323400020599365, 'validation/loss': 3.2533340454101562, 'validation/num_examples': 50000, 'test/accuracy': 0.25210002064704895, 'test/loss': 3.818544626235962, 'test/num_examples': 10000, 'score': 4729.180821657181, 'total_duration': 4949.588361263275, 'accumulated_submission_time': 4729.180821657181, 'accumulated_eval_time': 219.98390936851501, 'accumulated_logging_time': 0.278184175491333}
I0428 09:59:21.478331 139545651681024 logging_writer.py:48] [9542] accumulated_eval_time=219.983909, accumulated_logging_time=0.278184, accumulated_submission_time=4729.180822, global_step=9542, preemption_count=0, score=4729.180822, test/accuracy=0.252100, test/loss=3.818545, test/num_examples=10000, total_duration=4949.588361, train/accuracy=0.370137, train/loss=3.058263, validation/accuracy=0.332340, validation/loss=3.253334, validation/num_examples=50000
I0428 09:59:51.759960 139545660073728 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7948521375656128, loss=4.396683216094971
I0428 10:00:42.915363 139545651681024 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7592872977256775, loss=4.460939884185791
I0428 10:01:33.870687 139545660073728 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8242812156677246, loss=4.510566234588623
I0428 10:02:25.157194 139545651681024 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.0358057022094727, loss=4.533712863922119
I0428 10:03:15.652594 139545660073728 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8104230761528015, loss=6.117958068847656
I0428 10:04:06.895505 139545651681024 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7321863770484924, loss=4.370165824890137
I0428 10:04:57.835968 139545660073728 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8016687035560608, loss=6.037430286407471
I0428 10:05:49.107016 139545651681024 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8114667534828186, loss=4.452360153198242
I0428 10:06:21.903658 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:06:28.510731 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:06:36.171458 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:06:37.844182 139774329104192 submission_runner.py:415] Time since start: 5385.97s, 	Step: 10366, 	{'train/accuracy': 0.38859373331069946, 'train/loss': 2.9178740978240967, 'validation/accuracy': 0.3584199845790863, 'validation/loss': 3.0726478099823, 'validation/num_examples': 50000, 'test/accuracy': 0.2639999985694885, 'test/loss': 3.6615002155303955, 'test/num_examples': 10000, 'score': 5149.5784792900085, 'total_duration': 5385.968700170517, 'accumulated_submission_time': 5149.5784792900085, 'accumulated_eval_time': 235.92440557479858, 'accumulated_logging_time': 0.30434226989746094}
I0428 10:06:37.856793 139545660073728 logging_writer.py:48] [10366] accumulated_eval_time=235.924406, accumulated_logging_time=0.304342, accumulated_submission_time=5149.578479, global_step=10366, preemption_count=0, score=5149.578479, test/accuracy=0.264000, test/loss=3.661500, test/num_examples=10000, total_duration=5385.968700, train/accuracy=0.388594, train/loss=2.917874, validation/accuracy=0.358420, validation/loss=3.072648, validation/num_examples=50000
I0428 10:06:55.747773 139545651681024 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.0756571292877197, loss=5.784051895141602
I0428 10:07:47.220853 139545660073728 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.901369035243988, loss=5.812248706817627
I0428 10:08:38.127110 139545651681024 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.819410502910614, loss=4.458778381347656
I0428 10:09:29.370326 139545660073728 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8125066161155701, loss=4.252345085144043
I0428 10:10:20.662613 139545651681024 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7724035382270813, loss=4.301896572113037
I0428 10:11:12.186145 139545660073728 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8058900833129883, loss=5.810080528259277
I0428 10:12:03.081334 139545651681024 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.83707195520401, loss=4.324341773986816
I0428 10:12:54.150240 139545660073728 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8261346220970154, loss=4.204864501953125
I0428 10:13:38.076257 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:13:45.427757 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:13:52.702426 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:13:54.373563 139774329104192 submission_runner.py:415] Time since start: 5822.50s, 	Step: 11188, 	{'train/accuracy': 0.4095703065395355, 'train/loss': 2.813471555709839, 'validation/accuracy': 0.3700999915599823, 'validation/loss': 3.0176384449005127, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.5991904735565186, 'test/num_examples': 10000, 'score': 5569.771384716034, 'total_duration': 5822.498079299927, 'accumulated_submission_time': 5569.771384716034, 'accumulated_eval_time': 252.22167658805847, 'accumulated_logging_time': 0.3277134895324707}
I0428 10:13:54.387887 139545651681024 logging_writer.py:48] [11188] accumulated_eval_time=252.221677, accumulated_logging_time=0.327713, accumulated_submission_time=5569.771385, global_step=11188, preemption_count=0, score=5569.771385, test/accuracy=0.286500, test/loss=3.599190, test/num_examples=10000, total_duration=5822.498079, train/accuracy=0.409570, train/loss=2.813472, validation/accuracy=0.370100, validation/loss=3.017638, validation/num_examples=50000
I0428 10:14:01.151965 139545660073728 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.0107568502426147, loss=4.253924369812012
I0428 10:14:51.737217 139545651681024 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.8730103373527527, loss=4.225400447845459
I0428 10:15:42.202444 139545660073728 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0229836702346802, loss=5.364622592926025
I0428 10:16:33.646095 139545651681024 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8577694296836853, loss=4.215059280395508
I0428 10:17:24.645279 139545660073728 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.0445778369903564, loss=5.257848262786865
I0428 10:18:15.538473 139545651681024 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0006515979766846, loss=4.5223002433776855
I0428 10:19:06.412925 139545660073728 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9226489663124084, loss=5.406074523925781
I0428 10:19:57.286800 139545651681024 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9436337351799011, loss=6.0238566398620605
I0428 10:20:48.107528 139545660073728 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8459324240684509, loss=5.7813825607299805
I0428 10:20:54.717793 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:21:01.891346 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:21:09.327142 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:21:11.005776 139774329104192 submission_runner.py:415] Time since start: 6259.13s, 	Step: 12013, 	{'train/accuracy': 0.4491015672683716, 'train/loss': 2.6328914165496826, 'validation/accuracy': 0.39739999175071716, 'validation/loss': 2.88446044921875, 'validation/num_examples': 50000, 'test/accuracy': 0.3013000190258026, 'test/loss': 3.484825849533081, 'test/num_examples': 10000, 'score': 5990.068241119385, 'total_duration': 6259.130290269852, 'accumulated_submission_time': 5990.068241119385, 'accumulated_eval_time': 268.5096318721771, 'accumulated_logging_time': 0.35908985137939453}
I0428 10:21:11.018238 139545651681024 logging_writer.py:48] [12013] accumulated_eval_time=268.509632, accumulated_logging_time=0.359090, accumulated_submission_time=5990.068241, global_step=12013, preemption_count=0, score=5990.068241, test/accuracy=0.301300, test/loss=3.484826, test/num_examples=10000, total_duration=6259.130290, train/accuracy=0.449102, train/loss=2.632891, validation/accuracy=0.397400, validation/loss=2.884460, validation/num_examples=50000
I0428 10:21:56.022075 139545660073728 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9823400974273682, loss=5.899563789367676
I0428 10:22:47.197630 139545651681024 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0277013778686523, loss=4.185466766357422
I0428 10:23:38.569763 139545660073728 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.8749470114707947, loss=4.117618560791016
I0428 10:24:29.812075 139545651681024 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0452508926391602, loss=6.010695934295654
I0428 10:25:21.069876 139545660073728 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9085407257080078, loss=4.082562446594238
I0428 10:26:12.480627 139545651681024 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9962579607963562, loss=4.157835006713867
I0428 10:27:03.776218 139545660073728 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4142897129058838, loss=5.964237689971924
I0428 10:27:54.809674 139545651681024 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.2408864498138428, loss=4.026963233947754
I0428 10:28:11.218349 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:28:18.765044 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:28:26.314074 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:28:28.011076 139774329104192 submission_runner.py:415] Time since start: 6696.14s, 	Step: 12833, 	{'train/accuracy': 0.44431638717651367, 'train/loss': 2.6469717025756836, 'validation/accuracy': 0.41057997941970825, 'validation/loss': 2.82427716255188, 'validation/num_examples': 50000, 'test/accuracy': 0.31210002303123474, 'test/loss': 3.4135873317718506, 'test/num_examples': 10000, 'score': 6410.241943359375, 'total_duration': 6696.135591506958, 'accumulated_submission_time': 6410.241943359375, 'accumulated_eval_time': 285.3022961616516, 'accumulated_logging_time': 0.382326602935791}
I0428 10:28:28.025606 139545660073728 logging_writer.py:48] [12833] accumulated_eval_time=285.302296, accumulated_logging_time=0.382327, accumulated_submission_time=6410.241943, global_step=12833, preemption_count=0, score=6410.241943, test/accuracy=0.312100, test/loss=3.413587, test/num_examples=10000, total_duration=6696.135592, train/accuracy=0.444316, train/loss=2.646972, validation/accuracy=0.410580, validation/loss=2.824277, validation/num_examples=50000
I0428 10:29:02.606236 139545651681024 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.023269772529602, loss=3.9835047721862793
I0428 10:29:53.660929 139545660073728 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9991716146469116, loss=4.038387298583984
I0428 10:30:44.366400 139545651681024 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9113787412643433, loss=4.395451068878174
I0428 10:31:35.091980 139545660073728 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.2358819246292114, loss=4.132513999938965
I0428 10:32:25.968801 139545651681024 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.413390040397644, loss=5.620519161224365
I0428 10:33:17.178946 139545660073728 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0662801265716553, loss=4.614790916442871
I0428 10:34:07.924273 139545651681024 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0306885242462158, loss=4.228138446807861
I0428 10:34:59.195036 139545660073728 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0741195678710938, loss=3.9701478481292725
I0428 10:35:28.160459 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:35:36.138805 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:35:43.708584 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:35:45.397251 139774329104192 submission_runner.py:415] Time since start: 7133.52s, 	Step: 13658, 	{'train/accuracy': 0.4636327922344208, 'train/loss': 2.496321201324463, 'validation/accuracy': 0.4210599958896637, 'validation/loss': 2.7059812545776367, 'validation/num_examples': 50000, 'test/accuracy': 0.32430002093315125, 'test/loss': 3.3226606845855713, 'test/num_examples': 10000, 'score': 6830.3447144031525, 'total_duration': 7133.52175116539, 'accumulated_submission_time': 6830.3447144031525, 'accumulated_eval_time': 302.5390396118164, 'accumulated_logging_time': 0.41317319869995117}
I0428 10:35:45.411718 139545651681024 logging_writer.py:48] [13658] accumulated_eval_time=302.539040, accumulated_logging_time=0.413173, accumulated_submission_time=6830.344714, global_step=13658, preemption_count=0, score=6830.344714, test/accuracy=0.324300, test/loss=3.322661, test/num_examples=10000, total_duration=7133.521751, train/accuracy=0.463633, train/loss=2.496321, validation/accuracy=0.421060, validation/loss=2.705981, validation/num_examples=50000
I0428 10:36:07.453328 139545660073728 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.015493392944336, loss=4.091163158416748
I0428 10:36:58.771547 139545651681024 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1911633014678955, loss=4.014416217803955
I0428 10:37:49.624108 139545660073728 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3495601415634155, loss=4.020422458648682
I0428 10:38:40.977728 139545651681024 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.1895756721496582, loss=4.082462310791016
I0428 10:39:32.345076 139545660073728 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.990967869758606, loss=3.9436278343200684
I0428 10:40:23.835668 139545651681024 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9789788126945496, loss=4.337125301361084
I0428 10:41:15.147577 139545660073728 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.028656005859375, loss=4.595885276794434
I0428 10:42:06.483629 139545651681024 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.1470274925231934, loss=3.9348793029785156
I0428 10:42:45.739147 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:42:54.690146 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:43:02.626378 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:43:04.296149 139774329104192 submission_runner.py:415] Time since start: 7572.42s, 	Step: 14479, 	{'train/accuracy': 0.4676562249660492, 'train/loss': 2.449418067932129, 'validation/accuracy': 0.4341999888420105, 'validation/loss': 2.6226179599761963, 'validation/num_examples': 50000, 'test/accuracy': 0.3360000252723694, 'test/loss': 3.2415847778320312, 'test/num_examples': 10000, 'score': 7250.644949197769, 'total_duration': 7572.420679330826, 'accumulated_submission_time': 7250.644949197769, 'accumulated_eval_time': 321.096027135849, 'accumulated_logging_time': 0.4387240409851074}
I0428 10:43:04.313362 139545660073728 logging_writer.py:48] [14479] accumulated_eval_time=321.096027, accumulated_logging_time=0.438724, accumulated_submission_time=7250.644949, global_step=14479, preemption_count=0, score=7250.644949, test/accuracy=0.336000, test/loss=3.241585, test/num_examples=10000, total_duration=7572.420679, train/accuracy=0.467656, train/loss=2.449418, validation/accuracy=0.434200, validation/loss=2.622618, validation/num_examples=50000
I0428 10:43:15.685357 139545651681024 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.077960729598999, loss=3.9764697551727295
I0428 10:44:06.851364 139545660073728 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0946191549301147, loss=3.9269192218780518
I0428 10:44:57.665312 139545651681024 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9934716820716858, loss=4.7710957527160645
I0428 10:45:48.790431 139545660073728 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0650652647018433, loss=4.158969402313232
I0428 10:46:39.913502 139545651681024 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.232130765914917, loss=3.926128387451172
I0428 10:47:31.131949 139545660073728 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.0440396070480347, loss=4.437868118286133
I0428 10:48:21.645010 139545651681024 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0468109846115112, loss=4.39673376083374
I0428 10:49:12.680902 139545660073728 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9826563596725464, loss=4.314227104187012
I0428 10:50:03.590378 139545651681024 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.1272953748703003, loss=4.088248252868652
I0428 10:50:04.593538 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:50:12.913680 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:50:20.862356 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:50:22.535339 139774329104192 submission_runner.py:415] Time since start: 8010.66s, 	Step: 15303, 	{'train/accuracy': 0.48326170444488525, 'train/loss': 2.4012749195098877, 'validation/accuracy': 0.4479999840259552, 'validation/loss': 2.587911605834961, 'validation/num_examples': 50000, 'test/accuracy': 0.3424000144004822, 'test/loss': 3.195845365524292, 'test/num_examples': 10000, 'score': 7670.888507127762, 'total_duration': 8010.659863710403, 'accumulated_submission_time': 7670.888507127762, 'accumulated_eval_time': 339.03777527809143, 'accumulated_logging_time': 0.476729154586792}
I0428 10:50:22.550620 139545660073728 logging_writer.py:48] [15303] accumulated_eval_time=339.037775, accumulated_logging_time=0.476729, accumulated_submission_time=7670.888507, global_step=15303, preemption_count=0, score=7670.888507, test/accuracy=0.342400, test/loss=3.195845, test/num_examples=10000, total_duration=8010.659864, train/accuracy=0.483262, train/loss=2.401275, validation/accuracy=0.448000, validation/loss=2.587912, validation/num_examples=50000
I0428 10:51:13.040390 139545651681024 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.8516353368759155, loss=6.0114946365356445
I0428 10:52:04.333970 139545660073728 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.100818157196045, loss=4.051000118255615
I0428 10:52:55.789372 139545651681024 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.148125410079956, loss=4.902257442474365
I0428 10:53:46.719742 139545660073728 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.2226048707962036, loss=3.9124691486358643
I0428 10:54:37.781416 139545651681024 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.156944751739502, loss=5.4964094161987305
I0428 10:55:28.419513 139545660073728 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0499935150146484, loss=3.8072006702423096
I0428 10:56:19.705937 139545651681024 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.2285876274108887, loss=4.037299156188965
I0428 10:57:10.636060 139545660073728 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.3085105419158936, loss=5.339527130126953
I0428 10:57:22.739894 139774329104192 spec.py:298] Evaluating on the training split.
I0428 10:57:31.479317 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 10:57:40.165743 139774329104192 spec.py:326] Evaluating on the test split.
I0428 10:57:41.859055 139774329104192 submission_runner.py:415] Time since start: 8449.98s, 	Step: 16125, 	{'train/accuracy': 0.5134179592132568, 'train/loss': 2.260342597961426, 'validation/accuracy': 0.458979994058609, 'validation/loss': 2.5162487030029297, 'validation/num_examples': 50000, 'test/accuracy': 0.3547000288963318, 'test/loss': 3.153627395629883, 'test/num_examples': 10000, 'score': 8091.034668922424, 'total_duration': 8449.979655742645, 'accumulated_submission_time': 8091.034668922424, 'accumulated_eval_time': 358.15297269821167, 'accumulated_logging_time': 0.5192561149597168}
I0428 10:57:41.879850 139545651681024 logging_writer.py:48] [16125] accumulated_eval_time=358.152973, accumulated_logging_time=0.519256, accumulated_submission_time=8091.034669, global_step=16125, preemption_count=0, score=8091.034669, test/accuracy=0.354700, test/loss=3.153627, test/num_examples=10000, total_duration=8449.979656, train/accuracy=0.513418, train/loss=2.260343, validation/accuracy=0.458980, validation/loss=2.516249, validation/num_examples=50000
I0428 10:58:20.889686 139545660073728 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.385926604270935, loss=5.783195495605469
I0428 10:59:11.625402 139545651681024 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0427714586257935, loss=5.0874223709106445
I0428 11:00:02.742801 139545660073728 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.228112816810608, loss=5.497725486755371
I0428 11:00:53.485349 139545651681024 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.2031511068344116, loss=5.779523849487305
I0428 11:01:44.885720 139545660073728 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0989949703216553, loss=4.343420505523682
I0428 11:02:35.937112 139545651681024 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.1795189380645752, loss=3.6910109519958496
I0428 11:03:27.191970 139545660073728 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.2202304601669312, loss=3.9684031009674072
I0428 11:04:18.142590 139545651681024 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.1012368202209473, loss=3.8923938274383545
I0428 11:04:41.951425 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:04:49.851370 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:04:58.152637 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:04:59.829660 139774329104192 submission_runner.py:415] Time since start: 8887.95s, 	Step: 16947, 	{'train/accuracy': 0.5101367235183716, 'train/loss': 2.281947374343872, 'validation/accuracy': 0.4706399738788605, 'validation/loss': 2.471693754196167, 'validation/num_examples': 50000, 'test/accuracy': 0.3629000186920166, 'test/loss': 3.107588529586792, 'test/num_examples': 10000, 'score': 8511.057977437973, 'total_duration': 8887.954178333282, 'accumulated_submission_time': 8511.057977437973, 'accumulated_eval_time': 376.03114795684814, 'accumulated_logging_time': 0.5724232196807861}
I0428 11:04:59.842965 139545660073728 logging_writer.py:48] [16947] accumulated_eval_time=376.031148, accumulated_logging_time=0.572423, accumulated_submission_time=8511.057977, global_step=16947, preemption_count=0, score=8511.057977, test/accuracy=0.362900, test/loss=3.107589, test/num_examples=10000, total_duration=8887.954178, train/accuracy=0.510137, train/loss=2.281947, validation/accuracy=0.470640, validation/loss=2.471694, validation/num_examples=50000
I0428 11:05:27.772686 139545651681024 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1389347314834595, loss=3.9495394229888916
I0428 11:06:18.881252 139545660073728 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.1253700256347656, loss=3.9199681282043457
I0428 11:07:10.483496 139545651681024 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.086271047592163, loss=4.0241169929504395
I0428 11:08:01.129712 139545660073728 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.1380163431167603, loss=3.882244348526001
I0428 11:08:52.766079 139545651681024 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.3295005559921265, loss=3.8744964599609375
I0428 11:09:43.600950 139545660073728 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.054935336112976, loss=3.8845579624176025
I0428 11:10:34.543126 139545651681024 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.9841583371162415, loss=3.789735794067383
I0428 11:11:25.308703 139545660073728 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.064788818359375, loss=5.3474297523498535
I0428 11:11:59.936217 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:12:09.722666 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:12:19.189783 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:12:20.860476 139774329104192 submission_runner.py:415] Time since start: 9328.99s, 	Step: 17769, 	{'train/accuracy': 0.5201367139816284, 'train/loss': 2.285395383834839, 'validation/accuracy': 0.47849997878074646, 'validation/loss': 2.486689329147339, 'validation/num_examples': 50000, 'test/accuracy': 0.3694000244140625, 'test/loss': 3.098381280899048, 'test/num_examples': 10000, 'score': 8931.12339758873, 'total_duration': 9328.985008001328, 'accumulated_submission_time': 8931.12339758873, 'accumulated_eval_time': 396.95538997650146, 'accumulated_logging_time': 0.5976047515869141}
I0428 11:12:20.875279 139545651681024 logging_writer.py:48] [17769] accumulated_eval_time=396.955390, accumulated_logging_time=0.597605, accumulated_submission_time=8931.123398, global_step=17769, preemption_count=0, score=8931.123398, test/accuracy=0.369400, test/loss=3.098381, test/num_examples=10000, total_duration=9328.985008, train/accuracy=0.520137, train/loss=2.285395, validation/accuracy=0.478500, validation/loss=2.486689, validation/num_examples=50000
I0428 11:12:37.569835 139545660073728 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.2403866052627563, loss=3.904782772064209
I0428 11:13:28.416353 139545651681024 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.306505560874939, loss=4.076934814453125
I0428 11:14:19.531880 139545660073728 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.3293532133102417, loss=4.055851936340332
I0428 11:15:10.506998 139545651681024 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.1088494062423706, loss=4.015280723571777
I0428 11:16:01.555140 139545660073728 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.3244948387145996, loss=3.8472495079040527
I0428 11:16:52.228339 139545651681024 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.1151336431503296, loss=3.621248722076416
I0428 11:17:43.518467 139545660073728 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1836836338043213, loss=3.7973971366882324
I0428 11:18:34.100662 139545651681024 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0612019300460815, loss=3.772338628768921
I0428 11:19:21.281107 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:19:29.143851 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:19:37.804025 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:19:39.469241 139774329104192 submission_runner.py:415] Time since start: 9767.59s, 	Step: 18593, 	{'train/accuracy': 0.530468761920929, 'train/loss': 2.1735544204711914, 'validation/accuracy': 0.4897799789905548, 'validation/loss': 2.3675601482391357, 'validation/num_examples': 50000, 'test/accuracy': 0.379800021648407, 'test/loss': 3.0141055583953857, 'test/num_examples': 10000, 'score': 9351.499804496765, 'total_duration': 9767.593749523163, 'accumulated_submission_time': 9351.499804496765, 'accumulated_eval_time': 415.14348006248474, 'accumulated_logging_time': 0.6258835792541504}
I0428 11:19:39.490223 139545660073728 logging_writer.py:48] [18593] accumulated_eval_time=415.143480, accumulated_logging_time=0.625884, accumulated_submission_time=9351.499804, global_step=18593, preemption_count=0, score=9351.499804, test/accuracy=0.379800, test/loss=3.014106, test/num_examples=10000, total_duration=9767.593750, train/accuracy=0.530469, train/loss=2.173554, validation/accuracy=0.489780, validation/loss=2.367560, validation/num_examples=50000
I0428 11:19:43.577129 139545651681024 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.1115529537200928, loss=4.1895551681518555
I0428 11:20:34.889804 139545660073728 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.012176036834717, loss=5.677890777587891
I0428 11:21:26.242919 139545651681024 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.2319846153259277, loss=5.034992694854736
I0428 11:22:16.841564 139545660073728 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2710742950439453, loss=4.847657680511475
I0428 11:23:07.655679 139545651681024 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.1824707984924316, loss=3.718506097793579
I0428 11:23:58.290187 139545660073728 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.1684447526931763, loss=3.817969799041748
I0428 11:24:48.959856 139545651681024 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.2118767499923706, loss=3.6958465576171875
I0428 11:25:39.809066 139545660073728 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.2279117107391357, loss=4.120633602142334
I0428 11:26:30.742713 139545651681024 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.1990872621536255, loss=3.793560743331909
I0428 11:26:39.755361 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:26:50.274563 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:26:59.270180 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:27:00.956972 139774329104192 submission_runner.py:415] Time since start: 10209.08s, 	Step: 19419, 	{'train/accuracy': 0.5395898222923279, 'train/loss': 2.1370255947113037, 'validation/accuracy': 0.4953799843788147, 'validation/loss': 2.344198703765869, 'validation/num_examples': 50000, 'test/accuracy': 0.38700002431869507, 'test/loss': 2.976212739944458, 'test/num_examples': 10000, 'score': 9771.736384153366, 'total_duration': 10209.07812833786, 'accumulated_submission_time': 9771.736384153366, 'accumulated_eval_time': 436.341703414917, 'accumulated_logging_time': 0.6590654850006104}
I0428 11:27:00.973535 139545660073728 logging_writer.py:48] [19419] accumulated_eval_time=436.341703, accumulated_logging_time=0.659065, accumulated_submission_time=9771.736384, global_step=19419, preemption_count=0, score=9771.736384, test/accuracy=0.387000, test/loss=2.976213, test/num_examples=10000, total_duration=10209.078128, train/accuracy=0.539590, train/loss=2.137026, validation/accuracy=0.495380, validation/loss=2.344199, validation/num_examples=50000
I0428 11:27:42.705568 139545651681024 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.141883134841919, loss=3.634308099746704
I0428 11:28:33.884354 139545660073728 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.1195991039276123, loss=3.561424732208252
I0428 11:29:25.075231 139545651681024 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.2020883560180664, loss=4.072350978851318
I0428 11:30:15.942842 139545660073728 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1847453117370605, loss=3.7534406185150146
I0428 11:31:07.052749 139545651681024 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.2172479629516602, loss=3.7649612426757812
I0428 11:31:57.817301 139545660073728 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.5228761434555054, loss=4.730113506317139
I0428 11:32:49.195996 139545651681024 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.4212168455123901, loss=5.772177219390869
I0428 11:33:40.356014 139545660073728 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.4155018329620361, loss=4.624506950378418
I0428 11:34:01.394761 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:34:09.361854 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:34:19.147727 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:34:20.819663 139774329104192 submission_runner.py:415] Time since start: 10648.94s, 	Step: 20242, 	{'train/accuracy': 0.555957019329071, 'train/loss': 2.0108635425567627, 'validation/accuracy': 0.5078399777412415, 'validation/loss': 2.25386381149292, 'validation/num_examples': 50000, 'test/accuracy': 0.39170002937316895, 'test/loss': 2.9173407554626465, 'test/num_examples': 10000, 'score': 10192.12415933609, 'total_duration': 10648.942932844162, 'accumulated_submission_time': 10192.12415933609, 'accumulated_eval_time': 455.7653133869171, 'accumulated_logging_time': 0.6931803226470947}
I0428 11:34:20.837889 139545651681024 logging_writer.py:48] [20242] accumulated_eval_time=455.765313, accumulated_logging_time=0.693180, accumulated_submission_time=10192.124159, global_step=20242, preemption_count=0, score=10192.124159, test/accuracy=0.391700, test/loss=2.917341, test/num_examples=10000, total_duration=10648.942933, train/accuracy=0.555957, train/loss=2.010864, validation/accuracy=0.507840, validation/loss=2.253864, validation/num_examples=50000
I0428 11:34:51.269898 139545660073728 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1700128316879272, loss=3.5384955406188965
I0428 11:35:42.586126 139545651681024 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.2383712530136108, loss=3.8499765396118164
I0428 11:36:33.994018 139545660073728 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.243491768836975, loss=3.7916338443756104
I0428 11:37:24.853758 139545651681024 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.1565351486206055, loss=3.5697522163391113
I0428 11:38:16.075302 139545660073728 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.5033906698226929, loss=4.986987590789795
I0428 11:39:07.010107 139545651681024 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.2703531980514526, loss=3.557706117630005
I0428 11:39:57.808496 139545660073728 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.3189237117767334, loss=4.156451225280762
I0428 11:40:48.956780 139545651681024 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.4215009212493896, loss=3.612389087677002
I0428 11:41:20.937311 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:41:28.762440 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:41:38.522726 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:41:40.199087 139774329104192 submission_runner.py:415] Time since start: 11088.32s, 	Step: 21064, 	{'train/accuracy': 0.5453320145606995, 'train/loss': 2.0933563709259033, 'validation/accuracy': 0.4995400011539459, 'validation/loss': 2.299429178237915, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.9426355361938477, 'test/num_examples': 10000, 'score': 10612.193915843964, 'total_duration': 11088.322331666946, 'accumulated_submission_time': 10612.193915843964, 'accumulated_eval_time': 475.025776386261, 'accumulated_logging_time': 0.725426197052002}
I0428 11:41:40.217004 139545660073728 logging_writer.py:48] [21064] accumulated_eval_time=475.025776, accumulated_logging_time=0.725426, accumulated_submission_time=10612.193916, global_step=21064, preemption_count=0, score=10612.193916, test/accuracy=0.387400, test/loss=2.942636, test/num_examples=10000, total_duration=11088.322332, train/accuracy=0.545332, train/loss=2.093356, validation/accuracy=0.499540, validation/loss=2.299429, validation/num_examples=50000
I0428 11:41:59.496732 139545651681024 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.361101508140564, loss=4.394731521606445
I0428 11:42:50.652002 139545660073728 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.263263463973999, loss=5.704693794250488
I0428 11:43:41.723167 139545651681024 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.218949556350708, loss=4.025398254394531
I0428 11:44:32.637933 139545660073728 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.4047935009002686, loss=3.625044107437134
I0428 11:45:24.164329 139545651681024 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.2112475633621216, loss=3.8597676753997803
I0428 11:46:15.599062 139545660073728 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.1580711603164673, loss=3.5557429790496826
I0428 11:47:07.202966 139545651681024 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.3109757900238037, loss=3.5635058879852295
I0428 11:47:57.918509 139545660073728 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.635398507118225, loss=5.26647424697876
I0428 11:48:40.319162 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:48:48.485319 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:48:56.868850 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:48:58.539634 139774329104192 submission_runner.py:415] Time since start: 11526.66s, 	Step: 21884, 	{'train/accuracy': 0.5594531297683716, 'train/loss': 2.003148078918457, 'validation/accuracy': 0.5139200091362, 'validation/loss': 2.233449697494507, 'validation/num_examples': 50000, 'test/accuracy': 0.4043000340461731, 'test/loss': 2.8800857067108154, 'test/num_examples': 10000, 'score': 11032.267510652542, 'total_duration': 11526.662681102753, 'accumulated_submission_time': 11032.267510652542, 'accumulated_eval_time': 493.24475049972534, 'accumulated_logging_time': 0.7560896873474121}
I0428 11:48:58.557304 139545651681024 logging_writer.py:48] [21884] accumulated_eval_time=493.244750, accumulated_logging_time=0.756090, accumulated_submission_time=11032.267511, global_step=21884, preemption_count=0, score=11032.267511, test/accuracy=0.404300, test/loss=2.880086, test/num_examples=10000, total_duration=11526.662681, train/accuracy=0.559453, train/loss=2.003148, validation/accuracy=0.513920, validation/loss=2.233450, validation/num_examples=50000
I0428 11:49:07.419225 139545660073728 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.3403072357177734, loss=3.5856685638427734
I0428 11:49:58.251153 139545651681024 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.2379684448242188, loss=4.210004806518555
I0428 11:50:49.343366 139545660073728 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3617470264434814, loss=5.391181945800781
I0428 11:51:40.415897 139545651681024 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.4653877019882202, loss=5.553215026855469
I0428 11:52:31.606287 139545660073728 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3671696186065674, loss=3.878953695297241
I0428 11:53:22.505854 139545651681024 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.343604564666748, loss=4.456859111785889
I0428 11:54:13.954288 139545660073728 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.24848210811615, loss=3.5843663215637207
I0428 11:55:05.112769 139545651681024 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.2334529161453247, loss=3.5953757762908936
I0428 11:55:56.409375 139545660073728 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.411392331123352, loss=3.668992280960083
I0428 11:55:58.969303 139774329104192 spec.py:298] Evaluating on the training split.
I0428 11:56:07.096670 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 11:56:16.298919 139774329104192 spec.py:326] Evaluating on the test split.
I0428 11:56:17.975660 139774329104192 submission_runner.py:415] Time since start: 11966.10s, 	Step: 22706, 	{'train/accuracy': 0.5866601467132568, 'train/loss': 1.9081224203109741, 'validation/accuracy': 0.5212799906730652, 'validation/loss': 2.2187764644622803, 'validation/num_examples': 50000, 'test/accuracy': 0.4049000144004822, 'test/loss': 2.851747512817383, 'test/num_examples': 10000, 'score': 11452.649909973145, 'total_duration': 11966.098942518234, 'accumulated_submission_time': 11452.649909973145, 'accumulated_eval_time': 512.2498142719269, 'accumulated_logging_time': 0.7877943515777588}
I0428 11:56:17.987729 139545651681024 logging_writer.py:48] [22706] accumulated_eval_time=512.249814, accumulated_logging_time=0.787794, accumulated_submission_time=11452.649910, global_step=22706, preemption_count=0, score=11452.649910, test/accuracy=0.404900, test/loss=2.851748, test/num_examples=10000, total_duration=11966.098943, train/accuracy=0.586660, train/loss=1.908122, validation/accuracy=0.521280, validation/loss=2.218776, validation/num_examples=50000
I0428 11:57:06.715529 139545660073728 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.370059847831726, loss=3.4809889793395996
I0428 11:57:58.425768 139545651681024 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.2178616523742676, loss=4.826263904571533
I0428 11:58:49.401581 139545660073728 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.105982780456543, loss=3.7698638439178467
I0428 11:59:40.862200 139545651681024 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.3906337022781372, loss=3.6150593757629395
I0428 12:00:31.796354 139545660073728 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.3641014099121094, loss=3.738079071044922
I0428 12:01:23.062518 139545651681024 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5113043785095215, loss=3.5514585971832275
I0428 12:02:13.971261 139545660073728 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.211658000946045, loss=3.498312473297119
I0428 12:03:05.260527 139545651681024 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.1558022499084473, loss=3.4914703369140625
I0428 12:03:18.016492 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:03:25.652956 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:03:35.256247 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:03:36.938154 139774329104192 submission_runner.py:415] Time since start: 12405.06s, 	Step: 23526, 	{'train/accuracy': 0.5736913681030273, 'train/loss': 1.9353002309799194, 'validation/accuracy': 0.5300799608230591, 'validation/loss': 2.1478803157806396, 'validation/num_examples': 50000, 'test/accuracy': 0.40960001945495605, 'test/loss': 2.804401397705078, 'test/num_examples': 10000, 'score': 11872.646726846695, 'total_duration': 12405.061233520508, 'accumulated_submission_time': 11872.646726846695, 'accumulated_eval_time': 531.1700053215027, 'accumulated_logging_time': 0.8161451816558838}
I0428 12:03:36.956412 139545660073728 logging_writer.py:48] [23526] accumulated_eval_time=531.170005, accumulated_logging_time=0.816145, accumulated_submission_time=11872.646727, global_step=23526, preemption_count=0, score=11872.646727, test/accuracy=0.409600, test/loss=2.804401, test/num_examples=10000, total_duration=12405.061234, train/accuracy=0.573691, train/loss=1.935300, validation/accuracy=0.530080, validation/loss=2.147880, validation/num_examples=50000
I0428 12:04:15.653019 139545651681024 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3424586057662964, loss=3.536562204360962
I0428 12:05:07.245225 139545660073728 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.2266372442245483, loss=3.5704245567321777
I0428 12:05:58.264578 139545651681024 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.3444138765335083, loss=3.50689697265625
I0428 12:06:49.772483 139545660073728 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.7190673351287842, loss=5.617807388305664
I0428 12:07:41.050284 139545651681024 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2900633811950684, loss=3.586989641189575
I0428 12:08:32.462092 139545660073728 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.3213120698928833, loss=3.5520358085632324
I0428 12:09:23.415852 139545651681024 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.33805513381958, loss=3.381195306777954
I0428 12:10:14.575367 139545660073728 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.5431803464889526, loss=4.762673377990723
I0428 12:10:37.239020 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:10:45.061687 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:10:55.109645 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:10:56.774474 139774329104192 submission_runner.py:415] Time since start: 12844.90s, 	Step: 24346, 	{'train/accuracy': 0.5817577838897705, 'train/loss': 1.9220412969589233, 'validation/accuracy': 0.5311200022697449, 'validation/loss': 2.164311170578003, 'validation/num_examples': 50000, 'test/accuracy': 0.4117000102996826, 'test/loss': 2.8107802867889404, 'test/num_examples': 10000, 'score': 12292.899994373322, 'total_duration': 12844.897749900818, 'accumulated_submission_time': 12292.899994373322, 'accumulated_eval_time': 550.7041776180267, 'accumulated_logging_time': 0.8477580547332764}
I0428 12:10:56.792246 139545651681024 logging_writer.py:48] [24346] accumulated_eval_time=550.704178, accumulated_logging_time=0.847758, accumulated_submission_time=12292.899994, global_step=24346, preemption_count=0, score=12292.899994, test/accuracy=0.411700, test/loss=2.810780, test/num_examples=10000, total_duration=12844.897750, train/accuracy=0.581758, train/loss=1.922041, validation/accuracy=0.531120, validation/loss=2.164311, validation/num_examples=50000
I0428 12:11:24.932190 139545660073728 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.1425764560699463, loss=3.633732795715332
I0428 12:12:16.389578 139545651681024 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.4761438369750977, loss=5.075201034545898
I0428 12:13:07.259107 139545660073728 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.173535704612732, loss=4.0468058586120605
I0428 12:13:58.571089 139545651681024 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.518519401550293, loss=5.388510227203369
I0428 12:14:49.502996 139545660073728 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.3016952276229858, loss=3.9934699535369873
I0428 12:15:40.687824 139545651681024 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.3065567016601562, loss=3.4806644916534424
I0428 12:16:31.501528 139545660073728 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.2510977983474731, loss=3.7534167766571045
I0428 12:17:22.910814 139545651681024 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.2288954257965088, loss=3.9979665279388428
I0428 12:17:57.213336 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:18:05.128021 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:18:14.235568 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:18:15.899219 139774329104192 submission_runner.py:415] Time since start: 13284.02s, 	Step: 25168, 	{'train/accuracy': 0.57763671875, 'train/loss': 1.938732385635376, 'validation/accuracy': 0.5371400117874146, 'validation/loss': 2.1236259937286377, 'validation/num_examples': 50000, 'test/accuracy': 0.41750001907348633, 'test/loss': 2.7961947917938232, 'test/num_examples': 10000, 'score': 12713.29128408432, 'total_duration': 13284.022624015808, 'accumulated_submission_time': 12713.29128408432, 'accumulated_eval_time': 569.3889224529266, 'accumulated_logging_time': 0.8795332908630371}
I0428 12:18:15.918174 139545660073728 logging_writer.py:48] [25168] accumulated_eval_time=569.388922, accumulated_logging_time=0.879533, accumulated_submission_time=12713.291284, global_step=25168, preemption_count=0, score=12713.291284, test/accuracy=0.417500, test/loss=2.796195, test/num_examples=10000, total_duration=13284.022624, train/accuracy=0.577637, train/loss=1.938732, validation/accuracy=0.537140, validation/loss=2.123626, validation/num_examples=50000
I0428 12:18:33.005360 139545651681024 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2789382934570312, loss=3.5825083255767822
I0428 12:19:24.254580 139545660073728 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.261813998222351, loss=4.562079429626465
I0428 12:20:15.437119 139545651681024 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.4181030988693237, loss=3.497211217880249
I0428 12:21:06.829666 139545660073728 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.3859094381332397, loss=3.4301891326904297
I0428 12:21:57.596172 139545651681024 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.3338117599487305, loss=3.375257730484009
I0428 12:22:48.818855 139545660073728 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.3501074314117432, loss=3.4641947746276855
I0428 12:23:39.740755 139545651681024 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.4078724384307861, loss=3.5504837036132812
I0428 12:24:31.375117 139545660073728 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2068933248519897, loss=3.505453586578369
I0428 12:25:16.043361 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:25:24.081638 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:25:33.741842 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:25:35.405927 139774329104192 submission_runner.py:415] Time since start: 13723.53s, 	Step: 25989, 	{'train/accuracy': 0.5998437404632568, 'train/loss': 1.7852566242218018, 'validation/accuracy': 0.5520399808883667, 'validation/loss': 2.0235848426818848, 'validation/num_examples': 50000, 'test/accuracy': 0.4301000237464905, 'test/loss': 2.6935596466064453, 'test/num_examples': 10000, 'score': 13133.387439727783, 'total_duration': 13723.529441833496, 'accumulated_submission_time': 13133.387439727783, 'accumulated_eval_time': 588.7504587173462, 'accumulated_logging_time': 0.9117598533630371}
I0428 12:25:35.418494 139545651681024 logging_writer.py:48] [25989] accumulated_eval_time=588.750459, accumulated_logging_time=0.911760, accumulated_submission_time=13133.387440, global_step=25989, preemption_count=0, score=13133.387440, test/accuracy=0.430100, test/loss=2.693560, test/num_examples=10000, total_duration=13723.529442, train/accuracy=0.599844, train/loss=1.785257, validation/accuracy=0.552040, validation/loss=2.023585, validation/num_examples=50000
I0428 12:25:41.711529 139545660073728 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.3805991411209106, loss=3.3742501735687256
I0428 12:26:33.360718 139545651681024 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.401336431503296, loss=5.251582622528076
I0428 12:27:24.369419 139545660073728 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.4157501459121704, loss=4.275760173797607
I0428 12:28:15.887804 139545651681024 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.3412396907806396, loss=3.576305627822876
I0428 12:29:07.179172 139545660073728 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3601553440093994, loss=3.6164872646331787
I0428 12:29:58.490688 139545651681024 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.2806004285812378, loss=3.7256569862365723
I0428 12:30:49.947014 139545660073728 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.404886245727539, loss=3.5773961544036865
I0428 12:31:41.662724 139545651681024 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.6693766117095947, loss=5.080784320831299
I0428 12:32:33.082509 139545660073728 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.3141214847564697, loss=3.7701733112335205
I0428 12:32:35.589778 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:32:43.475720 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:32:52.673104 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:32:54.330433 139774329104192 submission_runner.py:415] Time since start: 14162.45s, 	Step: 26806, 	{'train/accuracy': 0.613964855670929, 'train/loss': 1.7197158336639404, 'validation/accuracy': 0.5508800148963928, 'validation/loss': 2.0125911235809326, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.679903745651245, 'test/num_examples': 10000, 'score': 13553.529470920563, 'total_duration': 14162.453840255737, 'accumulated_submission_time': 13553.529470920563, 'accumulated_eval_time': 607.4899682998657, 'accumulated_logging_time': 0.9383840560913086}
I0428 12:32:54.348887 139545651681024 logging_writer.py:48] [26806] accumulated_eval_time=607.489968, accumulated_logging_time=0.938384, accumulated_submission_time=13553.529471, global_step=26806, preemption_count=0, score=13553.529471, test/accuracy=0.433100, test/loss=2.679904, test/num_examples=10000, total_duration=14162.453840, train/accuracy=0.613965, train/loss=1.719716, validation/accuracy=0.550880, validation/loss=2.012591, validation/num_examples=50000
I0428 12:33:43.449349 139545660073728 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.34760320186615, loss=4.2117767333984375
I0428 12:34:34.428486 139545651681024 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.325273871421814, loss=3.7717394828796387
I0428 12:35:25.893080 139545660073728 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.3402531147003174, loss=3.3823933601379395
I0428 12:36:16.962988 139545651681024 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.2955491542816162, loss=3.4179861545562744
I0428 12:37:08.434000 139545660073728 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.2172452211380005, loss=3.512531042098999
I0428 12:37:59.605498 139545651681024 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.7293647527694702, loss=5.632262229919434
I0428 12:38:51.300846 139545660073728 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.279985785484314, loss=3.4031238555908203
I0428 12:39:42.309680 139545651681024 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.5218937397003174, loss=5.315555095672607
I0428 12:39:54.521043 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:40:02.322941 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:40:12.056558 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:40:13.735201 139774329104192 submission_runner.py:415] Time since start: 14601.86s, 	Step: 27625, 	{'train/accuracy': 0.6016796827316284, 'train/loss': 1.7657504081726074, 'validation/accuracy': 0.5518199801445007, 'validation/loss': 1.9917868375778198, 'validation/num_examples': 50000, 'test/accuracy': 0.43460002541542053, 'test/loss': 2.667407512664795, 'test/num_examples': 10000, 'score': 13973.672849178314, 'total_duration': 14601.858078718185, 'accumulated_submission_time': 13973.672849178314, 'accumulated_eval_time': 626.7024238109589, 'accumulated_logging_time': 0.9704618453979492}
I0428 12:40:13.747760 139545660073728 logging_writer.py:48] [27625] accumulated_eval_time=626.702424, accumulated_logging_time=0.970462, accumulated_submission_time=13973.672849, global_step=27625, preemption_count=0, score=13973.672849, test/accuracy=0.434600, test/loss=2.667408, test/num_examples=10000, total_duration=14601.858079, train/accuracy=0.601680, train/loss=1.765750, validation/accuracy=0.551820, validation/loss=1.991787, validation/num_examples=50000
I0428 12:40:52.947235 139545651681024 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.4115642309188843, loss=4.880832195281982
I0428 12:41:43.939875 139545660073728 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.4392589330673218, loss=4.600341320037842
I0428 12:42:35.293185 139545651681024 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4025468826293945, loss=3.3406643867492676
I0428 12:43:25.964933 139774329104192 spec.py:298] Evaluating on the training split.
I0428 12:43:33.813035 139774329104192 spec.py:310] Evaluating on the validation split.
I0428 12:43:42.809899 139774329104192 spec.py:326] Evaluating on the test split.
I0428 12:43:44.478960 139774329104192 submission_runner.py:415] Time since start: 14812.60s, 	Step: 28000, 	{'train/accuracy': 0.6074413657188416, 'train/loss': 1.8074811697006226, 'validation/accuracy': 0.5539399981498718, 'validation/loss': 2.0755293369293213, 'validation/num_examples': 50000, 'test/accuracy': 0.4349000155925751, 'test/loss': 2.7250735759735107, 'test/num_examples': 10000, 'score': 14165.870100736618, 'total_duration': 14812.60220694542, 'accumulated_submission_time': 14165.870100736618, 'accumulated_eval_time': 645.2151508331299, 'accumulated_logging_time': 0.9958095550537109}
I0428 12:43:44.498565 139545660073728 logging_writer.py:48] [28000] accumulated_eval_time=645.215151, accumulated_logging_time=0.995810, accumulated_submission_time=14165.870101, global_step=28000, preemption_count=0, score=14165.870101, test/accuracy=0.434900, test/loss=2.725074, test/num_examples=10000, total_duration=14812.602207, train/accuracy=0.607441, train/loss=1.807481, validation/accuracy=0.553940, validation/loss=2.075529, validation/num_examples=50000
I0428 12:43:44.529706 139545651681024 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=14165.870101
I0428 12:43:44.719638 139774329104192 checkpoints.py:356] Saving checkpoint at step: 28000
I0428 12:43:45.391491 139774329104192 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1/checkpoint_28000
I0428 12:43:45.403912 139774329104192 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_adafactor/imagenet_vit_jax/trial_1/checkpoint_28000.
I0428 12:43:46.381250 139774329104192 submission_runner.py:578] Tuning trial 1/1
I0428 12:43:46.381561 139774329104192 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0428 12:43:46.382878 139774329104192 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0032617186661809683, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.002759999828413129, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0014000000664964318, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 107.02599263191223, 'total_duration': 157.99477076530457, 'accumulated_submission_time': 107.02599263191223, 'accumulated_eval_time': 50.96862816810608, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (745, {'train/accuracy': 0.018808593973517418, 'train/loss': 6.263822078704834, 'validation/accuracy': 0.01735999993979931, 'validation/loss': 6.2798686027526855, 'validation/num_examples': 50000, 'test/accuracy': 0.013100001029670238, 'test/loss': 6.352320671081543, 'test/num_examples': 10000, 'score': 527.1725649833679, 'total_duration': 593.1585285663605, 'accumulated_submission_time': 527.1725649833679, 'accumulated_eval_time': 65.95091533660889, 'accumulated_logging_time': 0.02413630485534668, 'global_step': 745, 'preemption_count': 0}), (1644, {'train/accuracy': 0.05570312216877937, 'train/loss': 5.588287353515625, 'validation/accuracy': 0.05297999829053879, 'validation/loss': 5.628650665283203, 'validation/num_examples': 50000, 'test/accuracy': 0.042100001126527786, 'test/loss': 5.783792495727539, 'test/num_examples': 10000, 'score': 947.449645280838, 'total_duration': 1028.5527851581573, 'accumulated_submission_time': 947.449645280838, 'accumulated_eval_time': 81.03330445289612, 'accumulated_logging_time': 0.046282291412353516, 'global_step': 1644, 'preemption_count': 0}), (2543, {'train/accuracy': 0.08722656220197678, 'train/loss': 5.146496295928955, 'validation/accuracy': 0.08201999962329865, 'validation/loss': 5.189975261688232, 'validation/num_examples': 50000, 'test/accuracy': 0.06050000339746475, 'test/loss': 5.418995380401611, 'test/num_examples': 10000, 'score': 1367.6769676208496, 'total_duration': 1463.9743514060974, 'accumulated_submission_time': 1367.6769676208496, 'accumulated_eval_time': 96.18785119056702, 'accumulated_logging_time': 0.07312345504760742, 'global_step': 2543, 'preemption_count': 0}), (3440, {'train/accuracy': 0.13539062440395355, 'train/loss': 4.67137336730957, 'validation/accuracy': 0.12751999497413635, 'validation/loss': 4.733416557312012, 'validation/num_examples': 50000, 'test/accuracy': 0.1007000058889389, 'test/loss': 5.028800010681152, 'test/num_examples': 10000, 'score': 1787.8109254837036, 'total_duration': 1899.4797015190125, 'accumulated_submission_time': 1787.8109254837036, 'accumulated_eval_time': 111.52636170387268, 'accumulated_logging_time': 0.09340214729309082, 'global_step': 3440, 'preemption_count': 0}), (4332, {'train/accuracy': 0.17460937798023224, 'train/loss': 4.370706081390381, 'validation/accuracy': 0.1621599942445755, 'validation/loss': 4.455780982971191, 'validation/num_examples': 50000, 'test/accuracy': 0.12759999930858612, 'test/loss': 4.804147243499756, 'test/num_examples': 10000, 'score': 2207.984035253525, 'total_duration': 2335.178426504135, 'accumulated_submission_time': 2207.984035253525, 'accumulated_eval_time': 127.01521921157837, 'accumulated_logging_time': 0.11716651916503906, 'global_step': 4332, 'preemption_count': 0}), (5222, {'train/accuracy': 0.21753905713558197, 'train/loss': 4.037476539611816, 'validation/accuracy': 0.20217999815940857, 'validation/loss': 4.138639450073242, 'validation/num_examples': 50000, 'test/accuracy': 0.15550000965595245, 'test/loss': 4.564212799072266, 'test/num_examples': 10000, 'score': 2628.025150537491, 'total_duration': 2770.346748113632, 'accumulated_submission_time': 2628.025150537491, 'accumulated_eval_time': 142.10692477226257, 'accumulated_logging_time': 0.1402263641357422, 'global_step': 5222, 'preemption_count': 0}), (6106, {'train/accuracy': 0.2555468678474426, 'train/loss': 3.7493419647216797, 'validation/accuracy': 0.235959991812706, 'validation/loss': 3.865051746368408, 'validation/num_examples': 50000, 'test/accuracy': 0.17840000987052917, 'test/loss': 4.3103861808776855, 'test/num_examples': 10000, 'score': 3048.2380499839783, 'total_duration': 3205.7362217903137, 'accumulated_submission_time': 3048.2380499839783, 'accumulated_eval_time': 157.24267983436584, 'accumulated_logging_time': 0.1680469512939453, 'global_step': 6106, 'preemption_count': 0}), (6984, {'train/accuracy': 0.28974607586860657, 'train/loss': 3.5237984657287598, 'validation/accuracy': 0.2662400007247925, 'validation/loss': 3.6730382442474365, 'validation/num_examples': 50000, 'test/accuracy': 0.19680000841617584, 'test/loss': 4.183837413787842, 'test/num_examples': 10000, 'score': 3468.2391583919525, 'total_duration': 3641.138057947159, 'accumulated_submission_time': 3468.2391583919525, 'accumulated_eval_time': 172.59969520568848, 'accumulated_logging_time': 0.198502779006958, 'global_step': 6984, 'preemption_count': 0}), (7856, {'train/accuracy': 0.3184570372104645, 'train/loss': 3.3432695865631104, 'validation/accuracy': 0.2991800010204315, 'validation/loss': 3.4610683917999268, 'validation/num_examples': 50000, 'test/accuracy': 0.22670000791549683, 'test/loss': 3.9797239303588867, 'test/num_examples': 10000, 'score': 3888.6776649951935, 'total_duration': 4077.13476228714, 'accumulated_submission_time': 3888.6776649951935, 'accumulated_eval_time': 188.1143114566803, 'accumulated_logging_time': 0.22864556312561035, 'global_step': 7856, 'preemption_count': 0}), (8707, {'train/accuracy': 0.33406248688697815, 'train/loss': 3.229588270187378, 'validation/accuracy': 0.31129997968673706, 'validation/loss': 3.3596904277801514, 'validation/num_examples': 50000, 'test/accuracy': 0.23650000989437103, 'test/loss': 3.9032809734344482, 'test/num_examples': 10000, 'score': 4308.827301263809, 'total_duration': 4513.094922542572, 'accumulated_submission_time': 4308.827301263809, 'accumulated_eval_time': 203.88558101654053, 'accumulated_logging_time': 0.25287413597106934, 'global_step': 8707, 'preemption_count': 0}), (9542, {'train/accuracy': 0.37013670802116394, 'train/loss': 3.058262825012207, 'validation/accuracy': 0.3323400020599365, 'validation/loss': 3.2533340454101562, 'validation/num_examples': 50000, 'test/accuracy': 0.25210002064704895, 'test/loss': 3.818544626235962, 'test/num_examples': 10000, 'score': 4729.180821657181, 'total_duration': 4949.588361263275, 'accumulated_submission_time': 4729.180821657181, 'accumulated_eval_time': 219.98390936851501, 'accumulated_logging_time': 0.278184175491333, 'global_step': 9542, 'preemption_count': 0}), (10366, {'train/accuracy': 0.38859373331069946, 'train/loss': 2.9178740978240967, 'validation/accuracy': 0.3584199845790863, 'validation/loss': 3.0726478099823, 'validation/num_examples': 50000, 'test/accuracy': 0.2639999985694885, 'test/loss': 3.6615002155303955, 'test/num_examples': 10000, 'score': 5149.5784792900085, 'total_duration': 5385.968700170517, 'accumulated_submission_time': 5149.5784792900085, 'accumulated_eval_time': 235.92440557479858, 'accumulated_logging_time': 0.30434226989746094, 'global_step': 10366, 'preemption_count': 0}), (11188, {'train/accuracy': 0.4095703065395355, 'train/loss': 2.813471555709839, 'validation/accuracy': 0.3700999915599823, 'validation/loss': 3.0176384449005127, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.5991904735565186, 'test/num_examples': 10000, 'score': 5569.771384716034, 'total_duration': 5822.498079299927, 'accumulated_submission_time': 5569.771384716034, 'accumulated_eval_time': 252.22167658805847, 'accumulated_logging_time': 0.3277134895324707, 'global_step': 11188, 'preemption_count': 0}), (12013, {'train/accuracy': 0.4491015672683716, 'train/loss': 2.6328914165496826, 'validation/accuracy': 0.39739999175071716, 'validation/loss': 2.88446044921875, 'validation/num_examples': 50000, 'test/accuracy': 0.3013000190258026, 'test/loss': 3.484825849533081, 'test/num_examples': 10000, 'score': 5990.068241119385, 'total_duration': 6259.130290269852, 'accumulated_submission_time': 5990.068241119385, 'accumulated_eval_time': 268.5096318721771, 'accumulated_logging_time': 0.35908985137939453, 'global_step': 12013, 'preemption_count': 0}), (12833, {'train/accuracy': 0.44431638717651367, 'train/loss': 2.6469717025756836, 'validation/accuracy': 0.41057997941970825, 'validation/loss': 2.82427716255188, 'validation/num_examples': 50000, 'test/accuracy': 0.31210002303123474, 'test/loss': 3.4135873317718506, 'test/num_examples': 10000, 'score': 6410.241943359375, 'total_duration': 6696.135591506958, 'accumulated_submission_time': 6410.241943359375, 'accumulated_eval_time': 285.3022961616516, 'accumulated_logging_time': 0.382326602935791, 'global_step': 12833, 'preemption_count': 0}), (13658, {'train/accuracy': 0.4636327922344208, 'train/loss': 2.496321201324463, 'validation/accuracy': 0.4210599958896637, 'validation/loss': 2.7059812545776367, 'validation/num_examples': 50000, 'test/accuracy': 0.32430002093315125, 'test/loss': 3.3226606845855713, 'test/num_examples': 10000, 'score': 6830.3447144031525, 'total_duration': 7133.52175116539, 'accumulated_submission_time': 6830.3447144031525, 'accumulated_eval_time': 302.5390396118164, 'accumulated_logging_time': 0.41317319869995117, 'global_step': 13658, 'preemption_count': 0}), (14479, {'train/accuracy': 0.4676562249660492, 'train/loss': 2.449418067932129, 'validation/accuracy': 0.4341999888420105, 'validation/loss': 2.6226179599761963, 'validation/num_examples': 50000, 'test/accuracy': 0.3360000252723694, 'test/loss': 3.2415847778320312, 'test/num_examples': 10000, 'score': 7250.644949197769, 'total_duration': 7572.420679330826, 'accumulated_submission_time': 7250.644949197769, 'accumulated_eval_time': 321.096027135849, 'accumulated_logging_time': 0.4387240409851074, 'global_step': 14479, 'preemption_count': 0}), (15303, {'train/accuracy': 0.48326170444488525, 'train/loss': 2.4012749195098877, 'validation/accuracy': 0.4479999840259552, 'validation/loss': 2.587911605834961, 'validation/num_examples': 50000, 'test/accuracy': 0.3424000144004822, 'test/loss': 3.195845365524292, 'test/num_examples': 10000, 'score': 7670.888507127762, 'total_duration': 8010.659863710403, 'accumulated_submission_time': 7670.888507127762, 'accumulated_eval_time': 339.03777527809143, 'accumulated_logging_time': 0.476729154586792, 'global_step': 15303, 'preemption_count': 0}), (16125, {'train/accuracy': 0.5134179592132568, 'train/loss': 2.260342597961426, 'validation/accuracy': 0.458979994058609, 'validation/loss': 2.5162487030029297, 'validation/num_examples': 50000, 'test/accuracy': 0.3547000288963318, 'test/loss': 3.153627395629883, 'test/num_examples': 10000, 'score': 8091.034668922424, 'total_duration': 8449.979655742645, 'accumulated_submission_time': 8091.034668922424, 'accumulated_eval_time': 358.15297269821167, 'accumulated_logging_time': 0.5192561149597168, 'global_step': 16125, 'preemption_count': 0}), (16947, {'train/accuracy': 0.5101367235183716, 'train/loss': 2.281947374343872, 'validation/accuracy': 0.4706399738788605, 'validation/loss': 2.471693754196167, 'validation/num_examples': 50000, 'test/accuracy': 0.3629000186920166, 'test/loss': 3.107588529586792, 'test/num_examples': 10000, 'score': 8511.057977437973, 'total_duration': 8887.954178333282, 'accumulated_submission_time': 8511.057977437973, 'accumulated_eval_time': 376.03114795684814, 'accumulated_logging_time': 0.5724232196807861, 'global_step': 16947, 'preemption_count': 0}), (17769, {'train/accuracy': 0.5201367139816284, 'train/loss': 2.285395383834839, 'validation/accuracy': 0.47849997878074646, 'validation/loss': 2.486689329147339, 'validation/num_examples': 50000, 'test/accuracy': 0.3694000244140625, 'test/loss': 3.098381280899048, 'test/num_examples': 10000, 'score': 8931.12339758873, 'total_duration': 9328.985008001328, 'accumulated_submission_time': 8931.12339758873, 'accumulated_eval_time': 396.95538997650146, 'accumulated_logging_time': 0.5976047515869141, 'global_step': 17769, 'preemption_count': 0}), (18593, {'train/accuracy': 0.530468761920929, 'train/loss': 2.1735544204711914, 'validation/accuracy': 0.4897799789905548, 'validation/loss': 2.3675601482391357, 'validation/num_examples': 50000, 'test/accuracy': 0.379800021648407, 'test/loss': 3.0141055583953857, 'test/num_examples': 10000, 'score': 9351.499804496765, 'total_duration': 9767.593749523163, 'accumulated_submission_time': 9351.499804496765, 'accumulated_eval_time': 415.14348006248474, 'accumulated_logging_time': 0.6258835792541504, 'global_step': 18593, 'preemption_count': 0}), (19419, {'train/accuracy': 0.5395898222923279, 'train/loss': 2.1370255947113037, 'validation/accuracy': 0.4953799843788147, 'validation/loss': 2.344198703765869, 'validation/num_examples': 50000, 'test/accuracy': 0.38700002431869507, 'test/loss': 2.976212739944458, 'test/num_examples': 10000, 'score': 9771.736384153366, 'total_duration': 10209.07812833786, 'accumulated_submission_time': 9771.736384153366, 'accumulated_eval_time': 436.341703414917, 'accumulated_logging_time': 0.6590654850006104, 'global_step': 19419, 'preemption_count': 0}), (20242, {'train/accuracy': 0.555957019329071, 'train/loss': 2.0108635425567627, 'validation/accuracy': 0.5078399777412415, 'validation/loss': 2.25386381149292, 'validation/num_examples': 50000, 'test/accuracy': 0.39170002937316895, 'test/loss': 2.9173407554626465, 'test/num_examples': 10000, 'score': 10192.12415933609, 'total_duration': 10648.942932844162, 'accumulated_submission_time': 10192.12415933609, 'accumulated_eval_time': 455.7653133869171, 'accumulated_logging_time': 0.6931803226470947, 'global_step': 20242, 'preemption_count': 0}), (21064, {'train/accuracy': 0.5453320145606995, 'train/loss': 2.0933563709259033, 'validation/accuracy': 0.4995400011539459, 'validation/loss': 2.299429178237915, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.9426355361938477, 'test/num_examples': 10000, 'score': 10612.193915843964, 'total_duration': 11088.322331666946, 'accumulated_submission_time': 10612.193915843964, 'accumulated_eval_time': 475.025776386261, 'accumulated_logging_time': 0.725426197052002, 'global_step': 21064, 'preemption_count': 0}), (21884, {'train/accuracy': 0.5594531297683716, 'train/loss': 2.003148078918457, 'validation/accuracy': 0.5139200091362, 'validation/loss': 2.233449697494507, 'validation/num_examples': 50000, 'test/accuracy': 0.4043000340461731, 'test/loss': 2.8800857067108154, 'test/num_examples': 10000, 'score': 11032.267510652542, 'total_duration': 11526.662681102753, 'accumulated_submission_time': 11032.267510652542, 'accumulated_eval_time': 493.24475049972534, 'accumulated_logging_time': 0.7560896873474121, 'global_step': 21884, 'preemption_count': 0}), (22706, {'train/accuracy': 0.5866601467132568, 'train/loss': 1.9081224203109741, 'validation/accuracy': 0.5212799906730652, 'validation/loss': 2.2187764644622803, 'validation/num_examples': 50000, 'test/accuracy': 0.4049000144004822, 'test/loss': 2.851747512817383, 'test/num_examples': 10000, 'score': 11452.649909973145, 'total_duration': 11966.098942518234, 'accumulated_submission_time': 11452.649909973145, 'accumulated_eval_time': 512.2498142719269, 'accumulated_logging_time': 0.7877943515777588, 'global_step': 22706, 'preemption_count': 0}), (23526, {'train/accuracy': 0.5736913681030273, 'train/loss': 1.9353002309799194, 'validation/accuracy': 0.5300799608230591, 'validation/loss': 2.1478803157806396, 'validation/num_examples': 50000, 'test/accuracy': 0.40960001945495605, 'test/loss': 2.804401397705078, 'test/num_examples': 10000, 'score': 11872.646726846695, 'total_duration': 12405.061233520508, 'accumulated_submission_time': 11872.646726846695, 'accumulated_eval_time': 531.1700053215027, 'accumulated_logging_time': 0.8161451816558838, 'global_step': 23526, 'preemption_count': 0}), (24346, {'train/accuracy': 0.5817577838897705, 'train/loss': 1.9220412969589233, 'validation/accuracy': 0.5311200022697449, 'validation/loss': 2.164311170578003, 'validation/num_examples': 50000, 'test/accuracy': 0.4117000102996826, 'test/loss': 2.8107802867889404, 'test/num_examples': 10000, 'score': 12292.899994373322, 'total_duration': 12844.897749900818, 'accumulated_submission_time': 12292.899994373322, 'accumulated_eval_time': 550.7041776180267, 'accumulated_logging_time': 0.8477580547332764, 'global_step': 24346, 'preemption_count': 0}), (25168, {'train/accuracy': 0.57763671875, 'train/loss': 1.938732385635376, 'validation/accuracy': 0.5371400117874146, 'validation/loss': 2.1236259937286377, 'validation/num_examples': 50000, 'test/accuracy': 0.41750001907348633, 'test/loss': 2.7961947917938232, 'test/num_examples': 10000, 'score': 12713.29128408432, 'total_duration': 13284.022624015808, 'accumulated_submission_time': 12713.29128408432, 'accumulated_eval_time': 569.3889224529266, 'accumulated_logging_time': 0.8795332908630371, 'global_step': 25168, 'preemption_count': 0}), (25989, {'train/accuracy': 0.5998437404632568, 'train/loss': 1.7852566242218018, 'validation/accuracy': 0.5520399808883667, 'validation/loss': 2.0235848426818848, 'validation/num_examples': 50000, 'test/accuracy': 0.4301000237464905, 'test/loss': 2.6935596466064453, 'test/num_examples': 10000, 'score': 13133.387439727783, 'total_duration': 13723.529441833496, 'accumulated_submission_time': 13133.387439727783, 'accumulated_eval_time': 588.7504587173462, 'accumulated_logging_time': 0.9117598533630371, 'global_step': 25989, 'preemption_count': 0}), (26806, {'train/accuracy': 0.613964855670929, 'train/loss': 1.7197158336639404, 'validation/accuracy': 0.5508800148963928, 'validation/loss': 2.0125911235809326, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.679903745651245, 'test/num_examples': 10000, 'score': 13553.529470920563, 'total_duration': 14162.453840255737, 'accumulated_submission_time': 13553.529470920563, 'accumulated_eval_time': 607.4899682998657, 'accumulated_logging_time': 0.9383840560913086, 'global_step': 26806, 'preemption_count': 0}), (27625, {'train/accuracy': 0.6016796827316284, 'train/loss': 1.7657504081726074, 'validation/accuracy': 0.5518199801445007, 'validation/loss': 1.9917868375778198, 'validation/num_examples': 50000, 'test/accuracy': 0.43460002541542053, 'test/loss': 2.667407512664795, 'test/num_examples': 10000, 'score': 13973.672849178314, 'total_duration': 14601.858078718185, 'accumulated_submission_time': 13973.672849178314, 'accumulated_eval_time': 626.7024238109589, 'accumulated_logging_time': 0.9704618453979492, 'global_step': 27625, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6074413657188416, 'train/loss': 1.8074811697006226, 'validation/accuracy': 0.5539399981498718, 'validation/loss': 2.0755293369293213, 'validation/num_examples': 50000, 'test/accuracy': 0.4349000155925751, 'test/loss': 2.7250735759735107, 'test/num_examples': 10000, 'score': 14165.870100736618, 'total_duration': 14812.60220694542, 'accumulated_submission_time': 14165.870100736618, 'accumulated_eval_time': 645.2151508331299, 'accumulated_logging_time': 0.9958095550537109, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0428 12:43:46.383084 139774329104192 submission_runner.py:581] Timing: 14165.870100736618
I0428 12:43:46.383144 139774329104192 submission_runner.py:582] ====================
I0428 12:43:46.383329 139774329104192 submission_runner.py:645] Final imagenet_vit score: 14165.870100736618
