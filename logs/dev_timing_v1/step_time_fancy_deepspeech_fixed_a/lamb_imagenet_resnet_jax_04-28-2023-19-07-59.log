python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_04-28-2023-19-07-59.log
I0428 19:08:20.623888 140011389605696 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax.
I0428 19:08:20.691350 140011389605696 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 19:08:21.599041 140011389605696 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0428 19:08:21.599787 140011389605696 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 19:08:21.603873 140011389605696 submission_runner.py:538] Using RNG seed 1747903028
I0428 19:08:24.207948 140011389605696 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 19:08:24.208164 140011389605696 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1.
I0428 19:08:24.208355 140011389605696 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1/hparams.json.
I0428 19:08:24.333490 140011389605696 submission_runner.py:241] Initializing dataset.
I0428 19:08:24.346579 140011389605696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 19:08:24.353908 140011389605696 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:08:24.354027 140011389605696 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:08:24.617910 140011389605696 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 19:08:25.698610 140011389605696 submission_runner.py:248] Initializing model.
I0428 19:08:37.402026 140011389605696 submission_runner.py:258] Initializing optimizer.
I0428 19:08:38.502982 140011389605696 submission_runner.py:265] Initializing metrics bundle.
I0428 19:08:38.503185 140011389605696 submission_runner.py:282] Initializing checkpoint and logger.
I0428 19:08:38.504155 140011389605696 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0428 19:08:39.417591 140011389605696 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0428 19:08:39.418664 140011389605696 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1/flags_0.json.
I0428 19:08:39.423471 140011389605696 submission_runner.py:318] Starting training loop.
I0428 19:09:45.870655 139834022668032 logging_writer.py:48] [0] global_step=0, grad_norm=0.596986711025238, loss=6.921731948852539
I0428 19:09:45.887212 140011389605696 spec.py:298] Evaluating on the training split.
I0428 19:09:46.395184 140011389605696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 19:09:46.401533 140011389605696 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:09:46.401641 140011389605696 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:09:46.463511 140011389605696 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 19:09:58.537845 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 19:09:59.474189 140011389605696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 19:09:59.490221 140011389605696 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 19:09:59.490534 140011389605696 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 19:09:59.549621 140011389605696 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 19:10:17.446572 140011389605696 spec.py:326] Evaluating on the test split.
I0428 19:10:17.902651 140011389605696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 19:10:17.907697 140011389605696 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 19:10:17.938694 140011389605696 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 19:10:27.163326 140011389605696 submission_runner.py:415] Time since start: 107.74s, 	Step: 1, 	{'train/accuracy': 0.000996492337435484, 'train/loss': 6.911769390106201, 'validation/accuracy': 0.0012599999317899346, 'validation/loss': 6.911830425262451, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.911809921264648, 'test/num_examples': 10000, 'score': 66.46353602409363, 'total_duration': 107.73978853225708, 'accumulated_submission_time': 66.46353602409363, 'accumulated_eval_time': 41.27609443664551, 'accumulated_logging_time': 0}
I0428 19:10:27.181219 139805916641024 logging_writer.py:48] [1] accumulated_eval_time=41.276094, accumulated_logging_time=0, accumulated_submission_time=66.463536, global_step=1, preemption_count=0, score=66.463536, test/accuracy=0.001200, test/loss=6.911810, test/num_examples=10000, total_duration=107.739789, train/accuracy=0.000996, train/loss=6.911769, validation/accuracy=0.001260, validation/loss=6.911830, validation/num_examples=50000
I0428 19:11:03.425440 139805925033728 logging_writer.py:48] [100] global_step=100, grad_norm=0.6153069734573364, loss=6.923709869384766
I0428 19:11:39.688211 139805916641024 logging_writer.py:48] [200] global_step=200, grad_norm=0.5715172290802002, loss=6.9089837074279785
I0428 19:12:15.926925 139805925033728 logging_writer.py:48] [300] global_step=300, grad_norm=0.5789986252784729, loss=6.905241012573242
I0428 19:12:52.188436 139805916641024 logging_writer.py:48] [400] global_step=400, grad_norm=0.5788284540176392, loss=6.886523723602295
I0428 19:13:28.463985 139805925033728 logging_writer.py:48] [500] global_step=500, grad_norm=0.6072142720222473, loss=6.858767509460449
I0428 19:14:04.641769 139805916641024 logging_writer.py:48] [600] global_step=600, grad_norm=0.6418121457099915, loss=6.829119682312012
I0428 19:14:40.888713 139805925033728 logging_writer.py:48] [700] global_step=700, grad_norm=0.6431697607040405, loss=6.795829772949219
I0428 19:15:17.097071 139805916641024 logging_writer.py:48] [800] global_step=800, grad_norm=0.6979101300239563, loss=6.769594669342041
I0428 19:15:53.306608 139805925033728 logging_writer.py:48] [900] global_step=900, grad_norm=0.749401330947876, loss=6.7144389152526855
I0428 19:16:29.540446 139805916641024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7723476886749268, loss=6.670782089233398
I0428 19:17:05.794552 139805925033728 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.761008620262146, loss=6.674815654754639
I0428 19:17:42.048848 139805916641024 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8126175999641418, loss=6.601534366607666
I0428 19:18:18.253668 139805925033728 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8423177599906921, loss=6.581128120422363
I0428 19:18:54.473441 139805916641024 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8536999821662903, loss=6.556834697723389
I0428 19:18:57.199880 140011389605696 spec.py:298] Evaluating on the training split.
I0428 19:19:04.195552 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 19:19:12.036050 140011389605696 spec.py:326] Evaluating on the test split.
I0428 19:19:14.384098 140011389605696 submission_runner.py:415] Time since start: 634.96s, 	Step: 1409, 	{'train/accuracy': 0.02232142724096775, 'train/loss': 6.454865455627441, 'validation/accuracy': 0.019859999418258667, 'validation/loss': 6.4831061363220215, 'validation/num_examples': 50000, 'test/accuracy': 0.015500000678002834, 'test/loss': 6.5451340675354, 'test/num_examples': 10000, 'score': 576.4532551765442, 'total_duration': 634.9605531692505, 'accumulated_submission_time': 576.4532551765442, 'accumulated_eval_time': 58.46027708053589, 'accumulated_logging_time': 0.0257871150970459}
I0428 19:19:14.392387 139806210254592 logging_writer.py:48] [1409] accumulated_eval_time=58.460277, accumulated_logging_time=0.025787, accumulated_submission_time=576.453255, global_step=1409, preemption_count=0, score=576.453255, test/accuracy=0.015500, test/loss=6.545134, test/num_examples=10000, total_duration=634.960553, train/accuracy=0.022321, train/loss=6.454865, validation/accuracy=0.019860, validation/loss=6.483106, validation/num_examples=50000
I0428 19:19:47.736018 139806218647296 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9218060374259949, loss=6.5492963790893555
I0428 19:20:23.919122 139806210254592 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9060503840446472, loss=6.472909450531006
I0428 19:21:00.123904 139806218647296 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9193823933601379, loss=6.472439289093018
I0428 19:21:36.323695 139806210254592 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9894897937774658, loss=6.450321197509766
I0428 19:22:12.611980 139806218647296 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0257583856582642, loss=6.354813575744629
I0428 19:22:48.869738 139806210254592 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0080411434173584, loss=6.389575004577637
I0428 19:23:25.081683 139806218647296 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1066203117370605, loss=6.351836204528809
I0428 19:24:01.335498 139806210254592 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1840009689331055, loss=6.339900493621826
I0428 19:24:37.514261 139806218647296 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1599769592285156, loss=6.302626609802246
I0428 19:25:13.738240 139806210254592 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.187882423400879, loss=6.281702518463135
I0428 19:25:49.968987 139806218647296 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.3311163187026978, loss=6.229774475097656
I0428 19:26:26.209225 139806210254592 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.269234299659729, loss=6.255284786224365
I0428 19:27:02.441811 139806218647296 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.3919625282287598, loss=6.1482415199279785
I0428 19:27:38.700424 139806210254592 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.55827796459198, loss=6.134932041168213
I0428 19:27:44.708006 140011389605696 spec.py:298] Evaluating on the training split.
I0428 19:27:51.862144 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 19:27:59.701346 140011389605696 spec.py:326] Evaluating on the test split.
I0428 19:28:01.800599 140011389605696 submission_runner.py:415] Time since start: 1162.38s, 	Step: 2818, 	{'train/accuracy': 0.04878826439380646, 'train/loss': 5.87333345413208, 'validation/accuracy': 0.04559999704360962, 'validation/loss': 5.928921222686768, 'validation/num_examples': 50000, 'test/accuracy': 0.03420000150799751, 'test/loss': 6.0793681144714355, 'test/num_examples': 10000, 'score': 1086.7404198646545, 'total_duration': 1162.377066373825, 'accumulated_submission_time': 1086.7404198646545, 'accumulated_eval_time': 75.55284214019775, 'accumulated_logging_time': 0.04171419143676758}
I0428 19:28:01.808709 139806218647296 logging_writer.py:48] [2818] accumulated_eval_time=75.552842, accumulated_logging_time=0.041714, accumulated_submission_time=1086.740420, global_step=2818, preemption_count=0, score=1086.740420, test/accuracy=0.034200, test/loss=6.079368, test/num_examples=10000, total_duration=1162.377066, train/accuracy=0.048788, train/loss=5.873333, validation/accuracy=0.045600, validation/loss=5.928921, validation/num_examples=50000
I0428 19:28:31.877947 139806210254592 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.4965399503707886, loss=6.100127696990967
I0428 19:29:08.141036 139806218647296 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.2279486656188965, loss=6.0700364112854
I0428 19:29:44.377891 139806210254592 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.9022868871688843, loss=6.129347801208496
I0428 19:30:20.620269 139806218647296 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.162764072418213, loss=6.062413692474365
I0428 19:30:56.830832 139806210254592 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.7852731943130493, loss=6.023731708526611
I0428 19:31:33.051047 139806218647296 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.9335081577301025, loss=5.995664596557617
I0428 19:32:09.274371 139806210254592 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.077414035797119, loss=5.980466842651367
I0428 19:32:45.463248 139806218647296 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.7728607654571533, loss=5.996922492980957
I0428 19:33:21.745240 139806210254592 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.87070631980896, loss=5.891079902648926
I0428 19:33:58.036145 139806218647296 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.153609037399292, loss=5.850354194641113
I0428 19:34:34.257391 139806210254592 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.2929346561431885, loss=5.881620407104492
I0428 19:35:10.507774 139806218647296 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.4795610904693604, loss=5.8538126945495605
I0428 19:35:46.740552 139806210254592 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.256146192550659, loss=5.878129959106445
I0428 19:36:23.013421 139806218647296 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.3844990730285645, loss=5.832287311553955
I0428 19:36:31.929192 140011389605696 spec.py:298] Evaluating on the training split.
I0428 19:36:38.904384 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 19:36:46.923494 140011389605696 spec.py:326] Evaluating on the test split.
I0428 19:36:48.937662 140011389605696 submission_runner.py:415] Time since start: 1689.51s, 	Step: 4226, 	{'train/accuracy': 0.0786830335855484, 'train/loss': 5.425157070159912, 'validation/accuracy': 0.07158000022172928, 'validation/loss': 5.52325963973999, 'validation/num_examples': 50000, 'test/accuracy': 0.047200001776218414, 'test/loss': 5.765271186828613, 'test/num_examples': 10000, 'score': 1596.8324809074402, 'total_duration': 1689.5141172409058, 'accumulated_submission_time': 1596.8324809074402, 'accumulated_eval_time': 92.56127405166626, 'accumulated_logging_time': 0.057736873626708984}
I0428 19:36:48.945952 139806210254592 logging_writer.py:48] [4226] accumulated_eval_time=92.561274, accumulated_logging_time=0.057737, accumulated_submission_time=1596.832481, global_step=4226, preemption_count=0, score=1596.832481, test/accuracy=0.047200, test/loss=5.765271, test/num_examples=10000, total_duration=1689.514117, train/accuracy=0.078683, train/loss=5.425157, validation/accuracy=0.071580, validation/loss=5.523260, validation/num_examples=50000
I0428 19:37:16.141422 139806218647296 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.183281183242798, loss=5.819664001464844
I0428 19:37:52.337623 139806210254592 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.3763885498046875, loss=5.795559406280518
I0428 19:38:28.583921 139806218647296 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.351308822631836, loss=5.7548418045043945
I0428 19:39:04.817391 139806210254592 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.7701218128204346, loss=5.7929277420043945
I0428 19:39:41.089502 139806218647296 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.3423330783843994, loss=5.786154747009277
I0428 19:40:17.353058 139806210254592 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.858867645263672, loss=5.708104610443115
I0428 19:40:53.606148 139806218647296 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.038668394088745, loss=5.80159854888916
I0428 19:41:29.892271 139806210254592 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.445831298828125, loss=5.7329816818237305
I0428 19:42:06.136581 139806218647296 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.765810012817383, loss=5.803170204162598
I0428 19:42:42.392691 139806210254592 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.1155505180358887, loss=5.750458240509033
I0428 19:43:18.634511 139806218647296 logging_writer.py:48] [5300] global_step=5300, grad_norm=6.027279376983643, loss=5.764843463897705
I0428 19:43:54.879393 139806210254592 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.700753211975098, loss=5.722210884094238
I0428 19:44:31.286537 139806218647296 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.571042776107788, loss=5.751942157745361
I0428 19:45:07.530417 139806210254592 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.4991111755371094, loss=5.629844665527344
I0428 19:45:19.308166 140011389605696 spec.py:298] Evaluating on the training split.
I0428 19:45:26.170338 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 19:45:34.010546 140011389605696 spec.py:326] Evaluating on the test split.
I0428 19:45:36.061525 140011389605696 submission_runner.py:415] Time since start: 2216.64s, 	Step: 5634, 	{'train/accuracy': 0.10010761767625809, 'train/loss': 5.180556774139404, 'validation/accuracy': 0.0891599953174591, 'validation/loss': 5.299869060516357, 'validation/num_examples': 50000, 'test/accuracy': 0.06080000102519989, 'test/loss': 5.603649616241455, 'test/num_examples': 10000, 'score': 2107.165757894516, 'total_duration': 2216.6379899978638, 'accumulated_submission_time': 2107.165757894516, 'accumulated_eval_time': 109.31460428237915, 'accumulated_logging_time': 0.07408523559570312}
I0428 19:45:36.070442 139806218647296 logging_writer.py:48] [5634] accumulated_eval_time=109.314604, accumulated_logging_time=0.074085, accumulated_submission_time=2107.165758, global_step=5634, preemption_count=0, score=2107.165758, test/accuracy=0.060800, test/loss=5.603650, test/num_examples=10000, total_duration=2216.637990, train/accuracy=0.100108, train/loss=5.180557, validation/accuracy=0.089160, validation/loss=5.299869, validation/num_examples=50000
I0428 19:46:00.357599 139806210254592 logging_writer.py:48] [5700] global_step=5700, grad_norm=5.328835487365723, loss=5.676572322845459
I0428 19:46:36.564386 139806218647296 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.5843238830566406, loss=5.695662021636963
I0428 19:47:12.811928 139806210254592 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.6844892501831055, loss=5.587575912475586
I0428 19:47:49.057759 139806218647296 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.919736862182617, loss=5.729867458343506
I0428 19:48:25.285810 139806210254592 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.901579856872559, loss=5.598628520965576
I0428 19:49:01.508741 139806218647296 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.033512115478516, loss=5.60322380065918
I0428 19:49:37.746456 139806210254592 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.1687300205230713, loss=5.614513397216797
I0428 19:50:14.121468 139806218647296 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.4016189575195312, loss=5.7405104637146
I0428 19:50:50.361821 139806210254592 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.001516819000244, loss=5.626287937164307
I0428 19:51:26.596843 139806218647296 logging_writer.py:48] [6600] global_step=6600, grad_norm=5.6474223136901855, loss=5.616544723510742
I0428 19:52:02.837710 139806210254592 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.90666127204895, loss=5.597704887390137
I0428 19:52:39.061717 139806218647296 logging_writer.py:48] [6800] global_step=6800, grad_norm=7.176272392272949, loss=5.645834922790527
I0428 19:53:15.333736 139806210254592 logging_writer.py:48] [6900] global_step=6900, grad_norm=4.765647888183594, loss=5.571924209594727
I0428 19:53:51.592660 139806218647296 logging_writer.py:48] [7000] global_step=7000, grad_norm=6.792472839355469, loss=5.5481085777282715
I0428 19:54:06.307931 140011389605696 spec.py:298] Evaluating on the training split.
I0428 19:54:13.159761 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 19:54:21.040787 140011389605696 spec.py:326] Evaluating on the test split.
I0428 19:54:23.046335 140011389605696 submission_runner.py:415] Time since start: 2743.62s, 	Step: 7042, 	{'train/accuracy': 0.11069037020206451, 'train/loss': 5.036457061767578, 'validation/accuracy': 0.10087999701499939, 'validation/loss': 5.158253192901611, 'validation/num_examples': 50000, 'test/accuracy': 0.07129999995231628, 'test/loss': 5.493359088897705, 'test/num_examples': 10000, 'score': 2617.374045610428, 'total_duration': 2743.622799873352, 'accumulated_submission_time': 2617.374045610428, 'accumulated_eval_time': 126.05297827720642, 'accumulated_logging_time': 0.09147977828979492}
I0428 19:54:23.055155 139806210254592 logging_writer.py:48] [7042] accumulated_eval_time=126.052978, accumulated_logging_time=0.091480, accumulated_submission_time=2617.374046, global_step=7042, preemption_count=0, score=2617.374046, test/accuracy=0.071300, test/loss=5.493359, test/num_examples=10000, total_duration=2743.622800, train/accuracy=0.110690, train/loss=5.036457, validation/accuracy=0.100880, validation/loss=5.158253, validation/num_examples=50000
I0428 19:54:44.439465 139806218647296 logging_writer.py:48] [7100] global_step=7100, grad_norm=6.054407596588135, loss=5.577254772186279
I0428 19:55:20.701015 139806210254592 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.3848445415496826, loss=5.572618007659912
I0428 19:55:57.074141 139806218647296 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.650738000869751, loss=5.652381896972656
I0428 19:56:33.340373 139806210254592 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.031649112701416, loss=5.547123432159424
I0428 19:57:09.617511 139806218647296 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.03662109375, loss=5.513280868530273
I0428 19:57:45.844052 139806210254592 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.488919496536255, loss=5.582385063171387
I0428 19:58:22.078999 139806218647296 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.878183126449585, loss=5.615289211273193
I0428 19:58:58.317080 139806210254592 logging_writer.py:48] [7800] global_step=7800, grad_norm=4.61873722076416, loss=5.579107284545898
I0428 19:59:34.566128 139806218647296 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.198257923126221, loss=5.5558013916015625
I0428 20:00:10.839687 139806210254592 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.22608757019043, loss=5.476802349090576
I0428 20:00:47.090748 139806218647296 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.301649570465088, loss=5.541902542114258
I0428 20:01:23.331242 139806210254592 logging_writer.py:48] [8200] global_step=8200, grad_norm=7.254494667053223, loss=5.417018413543701
I0428 20:01:59.633864 139806218647296 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.7856266498565674, loss=5.482305526733398
I0428 20:02:35.844445 139806210254592 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.6241703033447266, loss=5.484978199005127
I0428 20:02:53.104250 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:02:59.837118 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:03:08.894702 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:03:11.239922 140011389605696 submission_runner.py:415] Time since start: 3271.82s, 	Step: 8449, 	{'train/accuracy': 0.12523914873600006, 'train/loss': 4.9058685302734375, 'validation/accuracy': 0.11379999667406082, 'validation/loss': 5.028566837310791, 'validation/num_examples': 50000, 'test/accuracy': 0.07380000501871109, 'test/loss': 5.430144309997559, 'test/num_examples': 10000, 'score': 3127.395161628723, 'total_duration': 3271.8163392543793, 'accumulated_submission_time': 3127.395161628723, 'accumulated_eval_time': 144.1885747909546, 'accumulated_logging_time': 0.10791921615600586}
I0428 20:03:11.257583 139806218647296 logging_writer.py:48] [8449] accumulated_eval_time=144.188575, accumulated_logging_time=0.107919, accumulated_submission_time=3127.395162, global_step=8449, preemption_count=0, score=3127.395162, test/accuracy=0.073800, test/loss=5.430144, test/num_examples=10000, total_duration=3271.816339, train/accuracy=0.125239, train/loss=4.905869, validation/accuracy=0.113800, validation/loss=5.028567, validation/num_examples=50000
I0428 20:03:30.145869 139806210254592 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.553633213043213, loss=5.545559406280518
I0428 20:04:06.413397 139806218647296 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.611812114715576, loss=5.525714874267578
I0428 20:04:42.693598 139806210254592 logging_writer.py:48] [8700] global_step=8700, grad_norm=5.057022571563721, loss=5.392330646514893
I0428 20:05:18.941546 139806218647296 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.472945213317871, loss=5.432615280151367
I0428 20:05:55.171654 139806210254592 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.169136047363281, loss=5.345541954040527
I0428 20:06:31.445621 139806218647296 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.687936544418335, loss=5.500483989715576
I0428 20:07:07.739460 139806210254592 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.8752756118774414, loss=5.496220588684082
I0428 20:07:44.044126 139806218647296 logging_writer.py:48] [9200] global_step=9200, grad_norm=4.328241348266602, loss=5.416236400604248
I0428 20:08:20.300411 139806210254592 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.2569756507873535, loss=5.459887504577637
I0428 20:08:56.497815 139806218647296 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.186211585998535, loss=5.429412364959717
I0428 20:09:32.759179 139806210254592 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.3451170921325684, loss=5.29658317565918
I0428 20:10:09.044867 139806218647296 logging_writer.py:48] [9600] global_step=9600, grad_norm=5.031766414642334, loss=5.351463794708252
I0428 20:10:45.319410 139806210254592 logging_writer.py:48] [9700] global_step=9700, grad_norm=4.9755539894104, loss=5.2495222091674805
I0428 20:11:21.606157 139806218647296 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.6285979747772217, loss=5.198113918304443
I0428 20:11:41.397214 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:11:48.254239 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:11:57.782633 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:11:59.787368 140011389605696 submission_runner.py:415] Time since start: 3800.36s, 	Step: 9856, 	{'train/accuracy': 0.15754543244838715, 'train/loss': 4.542609691619873, 'validation/accuracy': 0.1410999894142151, 'validation/loss': 4.677200794219971, 'validation/num_examples': 50000, 'test/accuracy': 0.10280000418424606, 'test/loss': 5.080394744873047, 'test/num_examples': 10000, 'score': 3637.5002348423004, 'total_duration': 3800.363835811615, 'accumulated_submission_time': 3637.5002348423004, 'accumulated_eval_time': 162.5787069797516, 'accumulated_logging_time': 0.13924479484558105}
I0428 20:11:59.796344 139806210254592 logging_writer.py:48] [9856] accumulated_eval_time=162.578707, accumulated_logging_time=0.139245, accumulated_submission_time=3637.500235, global_step=9856, preemption_count=0, score=3637.500235, test/accuracy=0.102800, test/loss=5.080395, test/num_examples=10000, total_duration=3800.363836, train/accuracy=0.157545, train/loss=4.542610, validation/accuracy=0.141100, validation/loss=4.677201, validation/num_examples=50000
I0428 20:12:16.107708 139806218647296 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.5385124683380127, loss=5.244831562042236
I0428 20:12:52.410945 139806210254592 logging_writer.py:48] [10000] global_step=10000, grad_norm=6.314006805419922, loss=5.200984954833984
I0428 20:13:28.653614 139806218647296 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.8786303997039795, loss=5.05629301071167
I0428 20:14:04.891185 139806210254592 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.561078071594238, loss=5.0723876953125
I0428 20:14:41.150837 139806218647296 logging_writer.py:48] [10300] global_step=10300, grad_norm=6.400343418121338, loss=5.053042888641357
I0428 20:15:17.385496 139806210254592 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.371927261352539, loss=4.981237411499023
I0428 20:15:53.658552 139806218647296 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.93052339553833, loss=4.984174728393555
I0428 20:16:29.928453 139806210254592 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.9076976776123047, loss=4.908457279205322
I0428 20:17:06.186170 139806218647296 logging_writer.py:48] [10700] global_step=10700, grad_norm=5.086413383483887, loss=4.845547199249268
I0428 20:17:42.448592 139806210254592 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.966514587402344, loss=4.804455757141113
I0428 20:18:18.690392 139806218647296 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.580699443817139, loss=4.704300880432129
I0428 20:18:54.987518 139806210254592 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.795553207397461, loss=4.755764961242676
I0428 20:19:31.220178 139806218647296 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.446381092071533, loss=4.679737091064453
I0428 20:20:07.514285 139806210254592 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.4174556732177734, loss=4.657564640045166
I0428 20:20:29.836986 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:20:36.787364 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:20:46.704257 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:20:48.597637 140011389605696 submission_runner.py:415] Time since start: 4329.17s, 	Step: 11263, 	{'train/accuracy': 0.2440808266401291, 'train/loss': 3.7768771648406982, 'validation/accuracy': 0.2254599928855896, 'validation/loss': 3.9149248600006104, 'validation/num_examples': 50000, 'test/accuracy': 0.15960000455379486, 'test/loss': 4.473261833190918, 'test/num_examples': 10000, 'score': 4147.5131413936615, 'total_duration': 4329.1730008125305, 'accumulated_submission_time': 4147.5131413936615, 'accumulated_eval_time': 181.33822894096375, 'accumulated_logging_time': 0.15566420555114746}
I0428 20:20:48.607004 139806218647296 logging_writer.py:48] [11263] accumulated_eval_time=181.338229, accumulated_logging_time=0.155664, accumulated_submission_time=4147.513141, global_step=11263, preemption_count=0, score=4147.513141, test/accuracy=0.159600, test/loss=4.473262, test/num_examples=10000, total_duration=4329.173001, train/accuracy=0.244081, train/loss=3.776877, validation/accuracy=0.225460, validation/loss=3.914925, validation/num_examples=50000
I0428 20:21:02.382365 139806210254592 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.313100814819336, loss=4.69823694229126
I0428 20:21:38.612396 139806218647296 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.9462993144989014, loss=4.561374664306641
I0428 20:22:14.847101 139806210254592 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.487991809844971, loss=4.484184265136719
I0428 20:22:51.095229 139806218647296 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.606613636016846, loss=4.439895153045654
I0428 20:23:27.306813 139806210254592 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.7116901874542236, loss=4.534140110015869
I0428 20:24:03.555160 139806218647296 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.152742385864258, loss=4.40347957611084
I0428 20:24:39.841280 139806210254592 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.3762335777282715, loss=4.520657539367676
I0428 20:25:16.079452 139806218647296 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.9400360584259033, loss=4.407055377960205
I0428 20:25:52.313938 139806210254592 logging_writer.py:48] [12100] global_step=12100, grad_norm=4.344630718231201, loss=4.425400733947754
I0428 20:26:28.546113 139806218647296 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.573277473449707, loss=4.301957130432129
I0428 20:27:04.759096 139806210254592 logging_writer.py:48] [12300] global_step=12300, grad_norm=5.054060459136963, loss=4.3150224685668945
I0428 20:27:41.011406 139806218647296 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.354833126068115, loss=4.223264694213867
I0428 20:28:17.245631 139806210254592 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.075647830963135, loss=4.2119927406311035
I0428 20:28:53.489507 139806218647296 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.204627513885498, loss=4.206751823425293
I0428 20:29:18.709912 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:29:26.199211 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:29:36.202018 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:29:38.442077 140011389605696 submission_runner.py:415] Time since start: 4859.02s, 	Step: 12671, 	{'train/accuracy': 0.33438295125961304, 'train/loss': 3.1869759559631348, 'validation/accuracy': 0.30399999022483826, 'validation/loss': 3.3563661575317383, 'validation/num_examples': 50000, 'test/accuracy': 0.2193000167608261, 'test/loss': 3.966228723526001, 'test/num_examples': 10000, 'score': 4657.587754011154, 'total_duration': 4859.017605066299, 'accumulated_submission_time': 4657.587754011154, 'accumulated_eval_time': 201.0694329738617, 'accumulated_logging_time': 0.17361235618591309}
I0428 20:29:38.456411 139806210254592 logging_writer.py:48] [12671] accumulated_eval_time=201.069433, accumulated_logging_time=0.173612, accumulated_submission_time=4657.587754, global_step=12671, preemption_count=0, score=4657.587754, test/accuracy=0.219300, test/loss=3.966229, test/num_examples=10000, total_duration=4859.017605, train/accuracy=0.334383, train/loss=3.186976, validation/accuracy=0.304000, validation/loss=3.356366, validation/num_examples=50000
I0428 20:29:49.338901 139806218647296 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.079648017883301, loss=4.17766809463501
I0428 20:30:25.715189 139806210254592 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.4636223316192627, loss=4.178501605987549
I0428 20:31:01.938466 139806218647296 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.632514476776123, loss=4.221088886260986
I0428 20:31:38.189693 139806210254592 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.546000957489014, loss=4.231032371520996
I0428 20:32:14.447588 139806218647296 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.7810471057891846, loss=4.118007183074951
I0428 20:32:50.703081 139806210254592 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.517382621765137, loss=4.071547508239746
I0428 20:33:26.986421 139806218647296 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.17667818069458, loss=4.027024269104004
I0428 20:34:03.208295 139806210254592 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.125487804412842, loss=4.024686813354492
I0428 20:34:39.456211 139806218647296 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.9777727127075195, loss=4.079071521759033
I0428 20:35:15.721006 139806210254592 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.823500156402588, loss=3.98157000541687
I0428 20:35:51.958865 139806218647296 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.7743399143218994, loss=4.1342644691467285
I0428 20:36:28.302209 139806210254592 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.424732208251953, loss=3.985661268234253
I0428 20:37:04.532752 139806218647296 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.7586681842803955, loss=3.8531365394592285
I0428 20:37:40.761015 139806210254592 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.280449867248535, loss=3.968086004257202
I0428 20:38:08.511521 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:38:15.482615 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:38:25.525492 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:38:27.530054 140011389605696 submission_runner.py:415] Time since start: 5388.11s, 	Step: 14078, 	{'train/accuracy': 0.3835698366165161, 'train/loss': 2.873321056365967, 'validation/accuracy': 0.35189998149871826, 'validation/loss': 3.0452945232391357, 'validation/num_examples': 50000, 'test/accuracy': 0.2629000246524811, 'test/loss': 3.681349039077759, 'test/num_examples': 10000, 'score': 5167.611377477646, 'total_duration': 5388.105275630951, 'accumulated_submission_time': 5167.611377477646, 'accumulated_eval_time': 220.08669471740723, 'accumulated_logging_time': 0.1994943618774414}
I0428 20:38:27.540021 139806218647296 logging_writer.py:48] [14078] accumulated_eval_time=220.086695, accumulated_logging_time=0.199494, accumulated_submission_time=5167.611377, global_step=14078, preemption_count=0, score=5167.611377, test/accuracy=0.262900, test/loss=3.681349, test/num_examples=10000, total_duration=5388.105276, train/accuracy=0.383570, train/loss=2.873321, validation/accuracy=0.351900, validation/loss=3.045295, validation/num_examples=50000
I0428 20:38:35.871655 139806210254592 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.272555351257324, loss=3.8021278381347656
I0428 20:39:12.078957 139806218647296 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.9985673427581787, loss=3.842672824859619
I0428 20:39:48.321144 139806210254592 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.350538730621338, loss=3.9176599979400635
I0428 20:40:24.520198 139806218647296 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.287166118621826, loss=3.913802146911621
I0428 20:41:00.753878 139806210254592 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.766160011291504, loss=3.823146104812622
I0428 20:41:36.937294 139806218647296 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.632053375244141, loss=3.755016803741455
I0428 20:42:13.266098 139806210254592 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.970714092254639, loss=3.836772918701172
I0428 20:42:49.478221 139806218647296 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.891404628753662, loss=3.8007569313049316
I0428 20:43:25.750350 139806210254592 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.51764178276062, loss=3.7667064666748047
I0428 20:44:01.977639 139806218647296 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.6977083683013916, loss=3.8648085594177246
I0428 20:44:38.206148 139806210254592 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.6020827293395996, loss=3.690812110900879
I0428 20:45:14.455069 139806218647296 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.1842193603515625, loss=3.779573440551758
I0428 20:45:50.714004 139806210254592 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.6658787727355957, loss=3.7086427211761475
I0428 20:46:26.956163 139806218647296 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.5587539672851562, loss=3.739687204360962
I0428 20:46:57.603399 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:47:05.378027 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:47:15.587417 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:47:17.578300 140011389605696 submission_runner.py:415] Time since start: 5918.15s, 	Step: 15486, 	{'train/accuracy': 0.43642377853393555, 'train/loss': 2.5825932025909424, 'validation/accuracy': 0.4041999876499176, 'validation/loss': 2.7469429969787598, 'validation/num_examples': 50000, 'test/accuracy': 0.30080002546310425, 'test/loss': 3.415835380554199, 'test/num_examples': 10000, 'score': 5677.645023345947, 'total_duration': 5918.152764081955, 'accumulated_submission_time': 5677.645023345947, 'accumulated_eval_time': 240.0595703125, 'accumulated_logging_time': 0.21902680397033691}
I0428 20:47:17.595318 139806210254592 logging_writer.py:48] [15486] accumulated_eval_time=240.059570, accumulated_logging_time=0.219027, accumulated_submission_time=5677.645023, global_step=15486, preemption_count=0, score=5677.645023, test/accuracy=0.300800, test/loss=3.415835, test/num_examples=10000, total_duration=5918.152764, train/accuracy=0.436424, train/loss=2.582593, validation/accuracy=0.404200, validation/loss=2.746943, validation/num_examples=50000
I0428 20:47:23.046513 139806218647296 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.3458452224731445, loss=3.689594268798828
I0428 20:47:59.360707 139806210254592 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.133745193481445, loss=3.686382532119751
I0428 20:48:35.566942 139806218647296 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.046008586883545, loss=3.7260618209838867
I0428 20:49:11.811441 139806210254592 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.1005353927612305, loss=3.652451753616333
I0428 20:49:48.037001 139806218647296 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.250211715698242, loss=3.566574811935425
I0428 20:50:24.222385 139806210254592 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.687324285507202, loss=3.666891098022461
I0428 20:51:00.466334 139806218647296 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.9168477058410645, loss=3.5396015644073486
I0428 20:51:36.720993 139806210254592 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.2412478923797607, loss=3.6532557010650635
I0428 20:52:12.971150 139806218647296 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.127021312713623, loss=3.7212538719177246
I0428 20:52:49.187108 139806210254592 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.070891857147217, loss=3.6077325344085693
I0428 20:53:25.413535 139806218647296 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.0646021366119385, loss=3.4934144020080566
I0428 20:54:01.745899 139806210254592 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.740705728530884, loss=3.537386894226074
I0428 20:54:37.956516 139806218647296 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.2739760875701904, loss=3.4890542030334473
I0428 20:55:14.196087 139806210254592 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.4751877784729004, loss=3.4306511878967285
I0428 20:55:47.760768 140011389605696 spec.py:298] Evaluating on the training split.
I0428 20:55:55.022113 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 20:56:05.258240 140011389605696 spec.py:326] Evaluating on the test split.
I0428 20:56:07.475024 140011389605696 submission_runner.py:415] Time since start: 6448.05s, 	Step: 16894, 	{'train/accuracy': 0.5151865482330322, 'train/loss': 2.1840884685516357, 'validation/accuracy': 0.45159998536109924, 'validation/loss': 2.499330759048462, 'validation/num_examples': 50000, 'test/accuracy': 0.34070003032684326, 'test/loss': 3.2064337730407715, 'test/num_examples': 10000, 'score': 6187.779090166092, 'total_duration': 6448.050338506699, 'accumulated_submission_time': 6187.779090166092, 'accumulated_eval_time': 259.7726502418518, 'accumulated_logging_time': 0.24741864204406738}
I0428 20:56:07.485483 139806218647296 logging_writer.py:48] [16894] accumulated_eval_time=259.772650, accumulated_logging_time=0.247419, accumulated_submission_time=6187.779090, global_step=16894, preemption_count=0, score=6187.779090, test/accuracy=0.340700, test/loss=3.206434, test/num_examples=10000, total_duration=6448.050339, train/accuracy=0.515187, train/loss=2.184088, validation/accuracy=0.451600, validation/loss=2.499331, validation/num_examples=50000
I0428 20:56:10.032655 139806210254592 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.7640328407287598, loss=3.4720849990844727
I0428 20:56:46.271438 139806218647296 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.035264015197754, loss=3.4402267932891846
I0428 20:57:22.490183 139806210254592 logging_writer.py:48] [17100] global_step=17100, grad_norm=4.0182390213012695, loss=3.4489636421203613
I0428 20:57:58.727558 139806218647296 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.4314024448394775, loss=3.520322322845459
I0428 20:58:34.963721 139806210254592 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.9741933345794678, loss=3.4542999267578125
I0428 20:59:11.219641 139806218647296 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.2259278297424316, loss=3.41235089302063
I0428 20:59:47.517080 139806210254592 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.9628167152404785, loss=3.3446097373962402
I0428 21:00:23.758576 139806218647296 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.1577816009521484, loss=3.4551358222961426
I0428 21:00:59.998233 139806210254592 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.3442256450653076, loss=3.413820743560791
I0428 21:01:36.237565 139806218647296 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.3111116886138916, loss=3.4653422832489014
I0428 21:02:12.446045 139806210254592 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.083695411682129, loss=3.3672311305999756
I0428 21:02:48.698503 139806218647296 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.173078775405884, loss=3.426723003387451
I0428 21:03:24.897588 139806210254592 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.699622869491577, loss=3.479898452758789
I0428 21:04:01.118554 139806218647296 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.0313949584960938, loss=3.4585165977478027
I0428 21:04:37.366479 139806210254592 logging_writer.py:48] [18300] global_step=18300, grad_norm=4.259737968444824, loss=3.357893943786621
I0428 21:04:37.578928 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:04:45.643658 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:04:55.803095 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:04:58.032290 140011389605696 submission_runner.py:415] Time since start: 6978.61s, 	Step: 18302, 	{'train/accuracy': 0.5287587642669678, 'train/loss': 2.101611614227295, 'validation/accuracy': 0.47193998098373413, 'validation/loss': 2.3951170444488525, 'validation/num_examples': 50000, 'test/accuracy': 0.36260002851486206, 'test/loss': 3.0879175662994385, 'test/num_examples': 10000, 'score': 6697.842190980911, 'total_duration': 6978.607776641846, 'accumulated_submission_time': 6697.842190980911, 'accumulated_eval_time': 280.22501277923584, 'accumulated_logging_time': 0.26804304122924805}
I0428 21:04:58.047133 139806218647296 logging_writer.py:48] [18302] accumulated_eval_time=280.225013, accumulated_logging_time=0.268043, accumulated_submission_time=6697.842191, global_step=18302, preemption_count=0, score=6697.842191, test/accuracy=0.362600, test/loss=3.087918, test/num_examples=10000, total_duration=6978.607777, train/accuracy=0.528759, train/loss=2.101612, validation/accuracy=0.471940, validation/loss=2.395117, validation/num_examples=50000
I0428 21:05:33.924306 139806210254592 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.2260570526123047, loss=3.38006591796875
I0428 21:06:10.220329 139806218647296 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.5361621379852295, loss=3.215665578842163
I0428 21:06:46.448817 139806210254592 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.299689531326294, loss=3.274136781692505
I0428 21:07:22.697133 139806218647296 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.530999183654785, loss=3.3380351066589355
I0428 21:07:58.928482 139806210254592 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.3467512130737305, loss=3.311694622039795
I0428 21:08:35.183325 139806218647296 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.240107297897339, loss=3.310049057006836
I0428 21:09:11.442349 139806210254592 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.498857259750366, loss=3.2694685459136963
I0428 21:09:47.655453 139806218647296 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.135098457336426, loss=3.188502073287964
I0428 21:10:23.883583 139806210254592 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.9424078464508057, loss=3.383394479751587
I0428 21:11:00.094216 139806218647296 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.0686213970184326, loss=3.213131904602051
I0428 21:11:36.323515 139806210254592 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.369558334350586, loss=3.315439462661743
I0428 21:12:12.579521 139806218647296 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.6620864868164062, loss=3.387667655944824
I0428 21:12:48.792264 139806210254592 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.149864673614502, loss=3.271646738052368
I0428 21:13:25.018412 139806218647296 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.2868709564208984, loss=3.1979904174804688
I0428 21:13:28.139907 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:13:35.614193 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:13:45.728260 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:13:47.875094 140011389605696 submission_runner.py:415] Time since start: 7508.45s, 	Step: 19710, 	{'train/accuracy': 0.5584542155265808, 'train/loss': 1.93849515914917, 'validation/accuracy': 0.5009799599647522, 'validation/loss': 2.235471725463867, 'validation/num_examples': 50000, 'test/accuracy': 0.37880000472068787, 'test/loss': 2.938297748565674, 'test/num_examples': 10000, 'score': 7207.903702259064, 'total_duration': 7508.4505405426025, 'accumulated_submission_time': 7207.903702259064, 'accumulated_eval_time': 299.9591600894928, 'accumulated_logging_time': 0.2939598560333252}
I0428 21:13:47.885948 139806210254592 logging_writer.py:48] [19710] accumulated_eval_time=299.959160, accumulated_logging_time=0.293960, accumulated_submission_time=7207.903702, global_step=19710, preemption_count=0, score=7207.903702, test/accuracy=0.378800, test/loss=2.938298, test/num_examples=10000, total_duration=7508.450541, train/accuracy=0.558454, train/loss=1.938495, validation/accuracy=0.500980, validation/loss=2.235472, validation/num_examples=50000
I0428 21:14:20.853933 139806218647296 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.8779966831207275, loss=3.195378303527832
I0428 21:14:57.071887 139806210254592 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.285954236984253, loss=3.3135881423950195
I0428 21:15:33.276534 139806218647296 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.0569376945495605, loss=3.2405340671539307
I0428 21:16:09.502360 139806210254592 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.3018710613250732, loss=3.2146382331848145
I0428 21:16:45.726718 139806218647296 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.0731000900268555, loss=3.23226261138916
I0428 21:17:21.975429 139806210254592 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.885395050048828, loss=3.1714253425598145
I0428 21:17:58.299565 139806218647296 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.0463948249816895, loss=3.2687735557556152
I0428 21:18:34.580741 139806210254592 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.4219233989715576, loss=3.2067461013793945
I0428 21:19:10.805603 139806218647296 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.1281721591949463, loss=3.1797780990600586
I0428 21:19:47.017340 139806210254592 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.2099838256835938, loss=3.1912648677825928
I0428 21:20:23.268967 139806218647296 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.124342918395996, loss=3.2124924659729004
I0428 21:20:59.492192 139806210254592 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.379641532897949, loss=3.07692813873291
I0428 21:21:35.737446 139806218647296 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.026803970336914, loss=3.114105701446533
I0428 21:22:11.909227 139806210254592 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.5398449897766113, loss=3.1225435733795166
I0428 21:22:17.935611 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:22:25.354640 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:22:35.866461 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:22:38.077811 140011389605696 submission_runner.py:415] Time since start: 8038.65s, 	Step: 21118, 	{'train/accuracy': 0.5651904940605164, 'train/loss': 1.9863258600234985, 'validation/accuracy': 0.5089200139045715, 'validation/loss': 2.243555784225464, 'validation/num_examples': 50000, 'test/accuracy': 0.39580002427101135, 'test/loss': 2.8911020755767822, 'test/num_examples': 10000, 'score': 7717.92222571373, 'total_duration': 8038.653203725815, 'accumulated_submission_time': 7717.92222571373, 'accumulated_eval_time': 320.1002571582794, 'accumulated_logging_time': 0.3158104419708252}
I0428 21:22:38.087636 139806218647296 logging_writer.py:48] [21118] accumulated_eval_time=320.100257, accumulated_logging_time=0.315810, accumulated_submission_time=7717.922226, global_step=21118, preemption_count=0, score=7717.922226, test/accuracy=0.395800, test/loss=2.891102, test/num_examples=10000, total_duration=8038.653204, train/accuracy=0.565190, train/loss=1.986326, validation/accuracy=0.508920, validation/loss=2.243556, validation/num_examples=50000
I0428 21:23:08.138817 139806210254592 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.0088424682617188, loss=3.0446479320526123
I0428 21:23:44.390182 139806218647296 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.370021104812622, loss=3.114543914794922
I0428 21:24:20.587510 139806210254592 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.5151896476745605, loss=3.1751065254211426
I0428 21:24:56.803379 139806218647296 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.7560763359069824, loss=3.1092591285705566
I0428 21:25:33.026605 139806210254592 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.9997758865356445, loss=3.225872755050659
I0428 21:26:09.254490 139806218647296 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.863574981689453, loss=3.0291662216186523
I0428 21:26:45.460905 139806210254592 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.957893133163452, loss=3.008535385131836
I0428 21:27:21.677702 139806218647296 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.492391586303711, loss=3.200343132019043
I0428 21:27:57.885206 139806210254592 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.8933682441711426, loss=3.0759339332580566
I0428 21:28:34.091816 139806218647296 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.9184887409210205, loss=3.133535385131836
I0428 21:29:10.291370 139806210254592 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.7560980319976807, loss=3.015160083770752
I0428 21:29:46.575055 139806218647296 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.844107151031494, loss=3.0374348163604736
I0428 21:30:22.766898 139806210254592 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.956825017929077, loss=3.129272937774658
I0428 21:30:58.972117 139806218647296 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.8819847106933594, loss=3.046746015548706
I0428 21:31:08.201881 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:31:15.637382 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:31:26.505264 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:31:28.460947 140011389605696 submission_runner.py:415] Time since start: 8569.04s, 	Step: 22527, 	{'train/accuracy': 0.5981544852256775, 'train/loss': 1.79996919631958, 'validation/accuracy': 0.5385400056838989, 'validation/loss': 2.077653169631958, 'validation/num_examples': 50000, 'test/accuracy': 0.41680002212524414, 'test/loss': 2.8010008335113525, 'test/num_examples': 10000, 'score': 8228.00821852684, 'total_duration': 8569.03599691391, 'accumulated_submission_time': 8228.00821852684, 'accumulated_eval_time': 340.3578794002533, 'accumulated_logging_time': 0.3339254856109619}
I0428 21:31:28.472012 139806210254592 logging_writer.py:48] [22527] accumulated_eval_time=340.357879, accumulated_logging_time=0.333925, accumulated_submission_time=8228.008219, global_step=22527, preemption_count=0, score=8228.008219, test/accuracy=0.416800, test/loss=2.801001, test/num_examples=10000, total_duration=8569.035997, train/accuracy=0.598154, train/loss=1.799969, validation/accuracy=0.538540, validation/loss=2.077653, validation/num_examples=50000
I0428 21:31:55.228421 139806218647296 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.9961354732513428, loss=3.0815048217773438
I0428 21:32:31.444900 139806210254592 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.66042160987854, loss=2.9568777084350586
I0428 21:33:07.678145 139806218647296 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.7532849311828613, loss=3.068655252456665
I0428 21:33:43.937542 139806210254592 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.974034547805786, loss=2.9798481464385986
I0428 21:34:20.134502 139806218647296 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.1843061447143555, loss=3.1394686698913574
I0428 21:34:56.308941 139806210254592 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.620150327682495, loss=2.9518868923187256
I0428 21:35:32.552893 139806218647296 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.6352596282958984, loss=3.072467088699341
I0428 21:36:08.876593 139806210254592 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.7997868061065674, loss=3.043165683746338
I0428 21:36:45.106053 139806218647296 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.693091869354248, loss=2.973170042037964
I0428 21:37:21.321534 139806210254592 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.83413028717041, loss=3.142423629760742
I0428 21:37:57.513774 139806218647296 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.0744411945343018, loss=3.1527259349823
I0428 21:38:33.702550 139806210254592 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.9888577461242676, loss=3.0668420791625977
I0428 21:39:09.874475 139806218647296 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.366891384124756, loss=3.0350565910339355
I0428 21:39:46.058120 139806210254592 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.1623241901397705, loss=3.0152618885040283
I0428 21:39:58.583774 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:40:06.204587 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:40:17.085835 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:40:19.195146 140011389605696 submission_runner.py:415] Time since start: 9099.77s, 	Step: 23936, 	{'train/accuracy': 0.5905413031578064, 'train/loss': 1.8641220331192017, 'validation/accuracy': 0.5371999740600586, 'validation/loss': 2.1278371810913086, 'validation/num_examples': 50000, 'test/accuracy': 0.41620001196861267, 'test/loss': 2.8204379081726074, 'test/num_examples': 10000, 'score': 8738.091413259506, 'total_duration': 9099.770626306534, 'accumulated_submission_time': 8738.091413259506, 'accumulated_eval_time': 360.9682402610779, 'accumulated_logging_time': 0.3536713123321533}
I0428 21:40:19.205473 139806218647296 logging_writer.py:48] [23936] accumulated_eval_time=360.968240, accumulated_logging_time=0.353671, accumulated_submission_time=8738.091413, global_step=23936, preemption_count=0, score=8738.091413, test/accuracy=0.416200, test/loss=2.820438, test/num_examples=10000, total_duration=9099.770626, train/accuracy=0.590541, train/loss=1.864122, validation/accuracy=0.537200, validation/loss=2.127837, validation/num_examples=50000
I0428 21:40:42.735832 139806210254592 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.4095776081085205, loss=3.038011074066162
I0428 21:41:19.011520 139806218647296 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.775465726852417, loss=3.059755325317383
I0428 21:41:55.331726 139806210254592 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.7610042095184326, loss=3.1127758026123047
I0428 21:42:31.578987 139806218647296 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.6902215480804443, loss=2.9873647689819336
I0428 21:43:07.763330 139806210254592 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.568830966949463, loss=2.9369423389434814
I0428 21:43:43.982697 139806218647296 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.8133957386016846, loss=3.127127170562744
I0428 21:44:20.173926 139806210254592 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.762526035308838, loss=2.999837875366211
I0428 21:44:56.382647 139806218647296 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.773968458175659, loss=2.9968154430389404
I0428 21:45:32.537708 139806210254592 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.718146800994873, loss=3.0239391326904297
I0428 21:46:08.753779 139806218647296 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.935657501220703, loss=3.0039572715759277
I0428 21:46:44.897572 139806210254592 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.0318448543548584, loss=2.923133373260498
I0428 21:47:21.116112 139806218647296 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.6230854988098145, loss=2.901597261428833
I0428 21:47:57.381310 139806210254592 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.2666828632354736, loss=2.8609495162963867
I0428 21:48:33.585550 139806218647296 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.6546671390533447, loss=3.010073184967041
I0428 21:48:49.356801 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:48:57.211527 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:49:07.784632 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:49:09.951321 140011389605696 submission_runner.py:415] Time since start: 9630.53s, 	Step: 25345, 	{'train/accuracy': 0.6044124364852905, 'train/loss': 1.7425518035888672, 'validation/accuracy': 0.5512799620628357, 'validation/loss': 2.0152411460876465, 'validation/num_examples': 50000, 'test/accuracy': 0.42810001969337463, 'test/loss': 2.717163324356079, 'test/num_examples': 10000, 'score': 9248.213064670563, 'total_duration': 9630.527769804, 'accumulated_submission_time': 9248.213064670563, 'accumulated_eval_time': 381.56271862983704, 'accumulated_logging_time': 0.37368130683898926}
I0428 21:49:09.961915 139806210254592 logging_writer.py:48] [25345] accumulated_eval_time=381.562719, accumulated_logging_time=0.373681, accumulated_submission_time=9248.213065, global_step=25345, preemption_count=0, score=9248.213065, test/accuracy=0.428100, test/loss=2.717163, test/num_examples=10000, total_duration=9630.527770, train/accuracy=0.604412, train/loss=1.742552, validation/accuracy=0.551280, validation/loss=2.015241, validation/num_examples=50000
I0428 21:49:30.219049 139806218647296 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.79990291595459, loss=2.9274613857269287
I0428 21:50:06.432473 139806210254592 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.4873294830322266, loss=2.954148054122925
I0428 21:50:42.651204 139806218647296 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.699678421020508, loss=2.953735828399658
I0428 21:51:18.875591 139806210254592 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.8584723472595215, loss=3.0504353046417236
I0428 21:51:55.091520 139806218647296 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.7541043758392334, loss=2.932961940765381
I0428 21:52:31.325514 139806210254592 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.0817580223083496, loss=2.9673092365264893
I0428 21:53:07.523370 139806218647296 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.462869882583618, loss=2.961881160736084
I0428 21:53:43.742039 139806210254592 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.610137462615967, loss=2.992142677307129
I0428 21:54:20.037688 139806218647296 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7804553508758545, loss=2.920708179473877
I0428 21:54:56.249223 139806210254592 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.6001596450805664, loss=3.0334036350250244
I0428 21:55:32.419356 139806218647296 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.4258387088775635, loss=2.954725742340088
I0428 21:56:08.593648 139806210254592 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.573127508163452, loss=2.919421911239624
I0428 21:56:44.799222 139806218647296 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.4569814205169678, loss=2.9961323738098145
I0428 21:57:21.027043 139806210254592 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.657231330871582, loss=2.9052138328552246
I0428 21:57:40.073642 140011389605696 spec.py:298] Evaluating on the training split.
I0428 21:57:47.902313 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 21:57:58.807617 140011389605696 spec.py:326] Evaluating on the test split.
I0428 21:58:01.032207 140011389605696 submission_runner.py:415] Time since start: 10161.61s, 	Step: 26754, 	{'train/accuracy': 0.6017617583274841, 'train/loss': 1.805702805519104, 'validation/accuracy': 0.5470600128173828, 'validation/loss': 2.0951192378997803, 'validation/num_examples': 50000, 'test/accuracy': 0.4245000183582306, 'test/loss': 2.8065717220306396, 'test/num_examples': 10000, 'score': 9758.297672510147, 'total_duration': 10161.607508182526, 'accumulated_submission_time': 9758.297672510147, 'accumulated_eval_time': 402.52009296417236, 'accumulated_logging_time': 0.39176034927368164}
I0428 21:58:01.043201 139806218647296 logging_writer.py:48] [26754] accumulated_eval_time=402.520093, accumulated_logging_time=0.391760, accumulated_submission_time=9758.297673, global_step=26754, preemption_count=0, score=9758.297673, test/accuracy=0.424500, test/loss=2.806572, test/num_examples=10000, total_duration=10161.607508, train/accuracy=0.601762, train/loss=1.805703, validation/accuracy=0.547060, validation/loss=2.095119, validation/num_examples=50000
I0428 21:58:18.064906 139806210254592 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.668407917022705, loss=3.0054373741149902
I0428 21:58:54.266250 139806218647296 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.5142669677734375, loss=2.889547824859619
I0428 21:59:30.449777 139806210254592 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.765132188796997, loss=2.948979139328003
I0428 22:00:06.737534 139806218647296 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.341517686843872, loss=2.8681259155273438
I0428 22:00:42.929020 139806210254592 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.667473554611206, loss=2.8301210403442383
I0428 22:01:19.136803 139806218647296 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.5176680088043213, loss=2.8325421810150146
I0428 22:01:55.316794 139806210254592 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.4978652000427246, loss=2.9431777000427246
I0428 22:02:31.495487 139806218647296 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.4906744956970215, loss=2.7938191890716553
I0428 22:03:07.717344 139806210254592 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.475283622741699, loss=2.8939948081970215
I0428 22:03:43.944449 139806218647296 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.4576144218444824, loss=2.8974645137786865
I0428 22:04:20.124967 139806210254592 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.7400028705596924, loss=2.9422640800476074
I0428 22:04:56.349305 139806218647296 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.490449905395508, loss=2.8687236309051514
I0428 22:05:32.033371 140011389605696 spec.py:298] Evaluating on the training split.
I0428 22:05:40.012073 140011389605696 spec.py:310] Evaluating on the validation split.
I0428 22:05:50.475634 140011389605696 spec.py:326] Evaluating on the test split.
I0428 22:05:53.045485 140011389605696 submission_runner.py:415] Time since start: 10633.62s, 	Step: 28000, 	{'train/accuracy': 0.6393893361091614, 'train/loss': 1.5555589199066162, 'validation/accuracy': 0.5769199728965759, 'validation/loss': 1.862410545349121, 'validation/num_examples': 50000, 'test/accuracy': 0.45500001311302185, 'test/loss': 2.552082061767578, 'test/num_examples': 10000, 'score': 10209.261482954025, 'total_duration': 10633.620640039444, 'accumulated_submission_time': 10209.261482954025, 'accumulated_eval_time': 423.5308735370636, 'accumulated_logging_time': 0.4117753505706787}
I0428 22:05:53.056715 139806210254592 logging_writer.py:48] [28000] accumulated_eval_time=423.530874, accumulated_logging_time=0.411775, accumulated_submission_time=10209.261483, global_step=28000, preemption_count=0, score=10209.261483, test/accuracy=0.455000, test/loss=2.552082, test/num_examples=10000, total_duration=10633.620640, train/accuracy=0.639389, train/loss=1.555559, validation/accuracy=0.576920, validation/loss=1.862411, validation/num_examples=50000
I0428 22:05:53.073987 139806218647296 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10209.261483
I0428 22:05:53.391797 140011389605696 checkpoints.py:356] Saving checkpoint at step: 28000
I0428 22:05:54.471141 140011389605696 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1/checkpoint_28000
I0428 22:05:54.494536 140011389605696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_lamb/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0428 22:05:54.945273 140011389605696 submission_runner.py:578] Tuning trial 1/1
I0428 22:05:54.945492 140011389605696 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0428 22:05:54.950095 140011389605696 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000996492337435484, 'train/loss': 6.911769390106201, 'validation/accuracy': 0.0012599999317899346, 'validation/loss': 6.911830425262451, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.911809921264648, 'test/num_examples': 10000, 'score': 66.46353602409363, 'total_duration': 107.73978853225708, 'accumulated_submission_time': 66.46353602409363, 'accumulated_eval_time': 41.27609443664551, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1409, {'train/accuracy': 0.02232142724096775, 'train/loss': 6.454865455627441, 'validation/accuracy': 0.019859999418258667, 'validation/loss': 6.4831061363220215, 'validation/num_examples': 50000, 'test/accuracy': 0.015500000678002834, 'test/loss': 6.5451340675354, 'test/num_examples': 10000, 'score': 576.4532551765442, 'total_duration': 634.9605531692505, 'accumulated_submission_time': 576.4532551765442, 'accumulated_eval_time': 58.46027708053589, 'accumulated_logging_time': 0.0257871150970459, 'global_step': 1409, 'preemption_count': 0}), (2818, {'train/accuracy': 0.04878826439380646, 'train/loss': 5.87333345413208, 'validation/accuracy': 0.04559999704360962, 'validation/loss': 5.928921222686768, 'validation/num_examples': 50000, 'test/accuracy': 0.03420000150799751, 'test/loss': 6.0793681144714355, 'test/num_examples': 10000, 'score': 1086.7404198646545, 'total_duration': 1162.377066373825, 'accumulated_submission_time': 1086.7404198646545, 'accumulated_eval_time': 75.55284214019775, 'accumulated_logging_time': 0.04171419143676758, 'global_step': 2818, 'preemption_count': 0}), (4226, {'train/accuracy': 0.0786830335855484, 'train/loss': 5.425157070159912, 'validation/accuracy': 0.07158000022172928, 'validation/loss': 5.52325963973999, 'validation/num_examples': 50000, 'test/accuracy': 0.047200001776218414, 'test/loss': 5.765271186828613, 'test/num_examples': 10000, 'score': 1596.8324809074402, 'total_duration': 1689.5141172409058, 'accumulated_submission_time': 1596.8324809074402, 'accumulated_eval_time': 92.56127405166626, 'accumulated_logging_time': 0.057736873626708984, 'global_step': 4226, 'preemption_count': 0}), (5634, {'train/accuracy': 0.10010761767625809, 'train/loss': 5.180556774139404, 'validation/accuracy': 0.0891599953174591, 'validation/loss': 5.299869060516357, 'validation/num_examples': 50000, 'test/accuracy': 0.06080000102519989, 'test/loss': 5.603649616241455, 'test/num_examples': 10000, 'score': 2107.165757894516, 'total_duration': 2216.6379899978638, 'accumulated_submission_time': 2107.165757894516, 'accumulated_eval_time': 109.31460428237915, 'accumulated_logging_time': 0.07408523559570312, 'global_step': 5634, 'preemption_count': 0}), (7042, {'train/accuracy': 0.11069037020206451, 'train/loss': 5.036457061767578, 'validation/accuracy': 0.10087999701499939, 'validation/loss': 5.158253192901611, 'validation/num_examples': 50000, 'test/accuracy': 0.07129999995231628, 'test/loss': 5.493359088897705, 'test/num_examples': 10000, 'score': 2617.374045610428, 'total_duration': 2743.622799873352, 'accumulated_submission_time': 2617.374045610428, 'accumulated_eval_time': 126.05297827720642, 'accumulated_logging_time': 0.09147977828979492, 'global_step': 7042, 'preemption_count': 0}), (8449, {'train/accuracy': 0.12523914873600006, 'train/loss': 4.9058685302734375, 'validation/accuracy': 0.11379999667406082, 'validation/loss': 5.028566837310791, 'validation/num_examples': 50000, 'test/accuracy': 0.07380000501871109, 'test/loss': 5.430144309997559, 'test/num_examples': 10000, 'score': 3127.395161628723, 'total_duration': 3271.8163392543793, 'accumulated_submission_time': 3127.395161628723, 'accumulated_eval_time': 144.1885747909546, 'accumulated_logging_time': 0.10791921615600586, 'global_step': 8449, 'preemption_count': 0}), (9856, {'train/accuracy': 0.15754543244838715, 'train/loss': 4.542609691619873, 'validation/accuracy': 0.1410999894142151, 'validation/loss': 4.677200794219971, 'validation/num_examples': 50000, 'test/accuracy': 0.10280000418424606, 'test/loss': 5.080394744873047, 'test/num_examples': 10000, 'score': 3637.5002348423004, 'total_duration': 3800.363835811615, 'accumulated_submission_time': 3637.5002348423004, 'accumulated_eval_time': 162.5787069797516, 'accumulated_logging_time': 0.13924479484558105, 'global_step': 9856, 'preemption_count': 0}), (11263, {'train/accuracy': 0.2440808266401291, 'train/loss': 3.7768771648406982, 'validation/accuracy': 0.2254599928855896, 'validation/loss': 3.9149248600006104, 'validation/num_examples': 50000, 'test/accuracy': 0.15960000455379486, 'test/loss': 4.473261833190918, 'test/num_examples': 10000, 'score': 4147.5131413936615, 'total_duration': 4329.1730008125305, 'accumulated_submission_time': 4147.5131413936615, 'accumulated_eval_time': 181.33822894096375, 'accumulated_logging_time': 0.15566420555114746, 'global_step': 11263, 'preemption_count': 0}), (12671, {'train/accuracy': 0.33438295125961304, 'train/loss': 3.1869759559631348, 'validation/accuracy': 0.30399999022483826, 'validation/loss': 3.3563661575317383, 'validation/num_examples': 50000, 'test/accuracy': 0.2193000167608261, 'test/loss': 3.966228723526001, 'test/num_examples': 10000, 'score': 4657.587754011154, 'total_duration': 4859.017605066299, 'accumulated_submission_time': 4657.587754011154, 'accumulated_eval_time': 201.0694329738617, 'accumulated_logging_time': 0.17361235618591309, 'global_step': 12671, 'preemption_count': 0}), (14078, {'train/accuracy': 0.3835698366165161, 'train/loss': 2.873321056365967, 'validation/accuracy': 0.35189998149871826, 'validation/loss': 3.0452945232391357, 'validation/num_examples': 50000, 'test/accuracy': 0.2629000246524811, 'test/loss': 3.681349039077759, 'test/num_examples': 10000, 'score': 5167.611377477646, 'total_duration': 5388.105275630951, 'accumulated_submission_time': 5167.611377477646, 'accumulated_eval_time': 220.08669471740723, 'accumulated_logging_time': 0.1994943618774414, 'global_step': 14078, 'preemption_count': 0}), (15486, {'train/accuracy': 0.43642377853393555, 'train/loss': 2.5825932025909424, 'validation/accuracy': 0.4041999876499176, 'validation/loss': 2.7469429969787598, 'validation/num_examples': 50000, 'test/accuracy': 0.30080002546310425, 'test/loss': 3.415835380554199, 'test/num_examples': 10000, 'score': 5677.645023345947, 'total_duration': 5918.152764081955, 'accumulated_submission_time': 5677.645023345947, 'accumulated_eval_time': 240.0595703125, 'accumulated_logging_time': 0.21902680397033691, 'global_step': 15486, 'preemption_count': 0}), (16894, {'train/accuracy': 0.5151865482330322, 'train/loss': 2.1840884685516357, 'validation/accuracy': 0.45159998536109924, 'validation/loss': 2.499330759048462, 'validation/num_examples': 50000, 'test/accuracy': 0.34070003032684326, 'test/loss': 3.2064337730407715, 'test/num_examples': 10000, 'score': 6187.779090166092, 'total_duration': 6448.050338506699, 'accumulated_submission_time': 6187.779090166092, 'accumulated_eval_time': 259.7726502418518, 'accumulated_logging_time': 0.24741864204406738, 'global_step': 16894, 'preemption_count': 0}), (18302, {'train/accuracy': 0.5287587642669678, 'train/loss': 2.101611614227295, 'validation/accuracy': 0.47193998098373413, 'validation/loss': 2.3951170444488525, 'validation/num_examples': 50000, 'test/accuracy': 0.36260002851486206, 'test/loss': 3.0879175662994385, 'test/num_examples': 10000, 'score': 6697.842190980911, 'total_duration': 6978.607776641846, 'accumulated_submission_time': 6697.842190980911, 'accumulated_eval_time': 280.22501277923584, 'accumulated_logging_time': 0.26804304122924805, 'global_step': 18302, 'preemption_count': 0}), (19710, {'train/accuracy': 0.5584542155265808, 'train/loss': 1.93849515914917, 'validation/accuracy': 0.5009799599647522, 'validation/loss': 2.235471725463867, 'validation/num_examples': 50000, 'test/accuracy': 0.37880000472068787, 'test/loss': 2.938297748565674, 'test/num_examples': 10000, 'score': 7207.903702259064, 'total_duration': 7508.4505405426025, 'accumulated_submission_time': 7207.903702259064, 'accumulated_eval_time': 299.9591600894928, 'accumulated_logging_time': 0.2939598560333252, 'global_step': 19710, 'preemption_count': 0}), (21118, {'train/accuracy': 0.5651904940605164, 'train/loss': 1.9863258600234985, 'validation/accuracy': 0.5089200139045715, 'validation/loss': 2.243555784225464, 'validation/num_examples': 50000, 'test/accuracy': 0.39580002427101135, 'test/loss': 2.8911020755767822, 'test/num_examples': 10000, 'score': 7717.92222571373, 'total_duration': 8038.653203725815, 'accumulated_submission_time': 7717.92222571373, 'accumulated_eval_time': 320.1002571582794, 'accumulated_logging_time': 0.3158104419708252, 'global_step': 21118, 'preemption_count': 0}), (22527, {'train/accuracy': 0.5981544852256775, 'train/loss': 1.79996919631958, 'validation/accuracy': 0.5385400056838989, 'validation/loss': 2.077653169631958, 'validation/num_examples': 50000, 'test/accuracy': 0.41680002212524414, 'test/loss': 2.8010008335113525, 'test/num_examples': 10000, 'score': 8228.00821852684, 'total_duration': 8569.03599691391, 'accumulated_submission_time': 8228.00821852684, 'accumulated_eval_time': 340.3578794002533, 'accumulated_logging_time': 0.3339254856109619, 'global_step': 22527, 'preemption_count': 0}), (23936, {'train/accuracy': 0.5905413031578064, 'train/loss': 1.8641220331192017, 'validation/accuracy': 0.5371999740600586, 'validation/loss': 2.1278371810913086, 'validation/num_examples': 50000, 'test/accuracy': 0.41620001196861267, 'test/loss': 2.8204379081726074, 'test/num_examples': 10000, 'score': 8738.091413259506, 'total_duration': 9099.770626306534, 'accumulated_submission_time': 8738.091413259506, 'accumulated_eval_time': 360.9682402610779, 'accumulated_logging_time': 0.3536713123321533, 'global_step': 23936, 'preemption_count': 0}), (25345, {'train/accuracy': 0.6044124364852905, 'train/loss': 1.7425518035888672, 'validation/accuracy': 0.5512799620628357, 'validation/loss': 2.0152411460876465, 'validation/num_examples': 50000, 'test/accuracy': 0.42810001969337463, 'test/loss': 2.717163324356079, 'test/num_examples': 10000, 'score': 9248.213064670563, 'total_duration': 9630.527769804, 'accumulated_submission_time': 9248.213064670563, 'accumulated_eval_time': 381.56271862983704, 'accumulated_logging_time': 0.37368130683898926, 'global_step': 25345, 'preemption_count': 0}), (26754, {'train/accuracy': 0.6017617583274841, 'train/loss': 1.805702805519104, 'validation/accuracy': 0.5470600128173828, 'validation/loss': 2.0951192378997803, 'validation/num_examples': 50000, 'test/accuracy': 0.4245000183582306, 'test/loss': 2.8065717220306396, 'test/num_examples': 10000, 'score': 9758.297672510147, 'total_duration': 10161.607508182526, 'accumulated_submission_time': 9758.297672510147, 'accumulated_eval_time': 402.52009296417236, 'accumulated_logging_time': 0.39176034927368164, 'global_step': 26754, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6393893361091614, 'train/loss': 1.5555589199066162, 'validation/accuracy': 0.5769199728965759, 'validation/loss': 1.862410545349121, 'validation/num_examples': 50000, 'test/accuracy': 0.45500001311302185, 'test/loss': 2.552082061767578, 'test/num_examples': 10000, 'score': 10209.261482954025, 'total_duration': 10633.620640039444, 'accumulated_submission_time': 10209.261482954025, 'accumulated_eval_time': 423.5308735370636, 'accumulated_logging_time': 0.4117753505706787, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0428 22:05:54.950237 140011389605696 submission_runner.py:581] Timing: 10209.261482954025
I0428 22:05:54.950279 140011389605696 submission_runner.py:582] ====================
I0428 22:05:54.950401 140011389605696 submission_runner.py:645] Final imagenet_resnet score: 10209.261482954025
