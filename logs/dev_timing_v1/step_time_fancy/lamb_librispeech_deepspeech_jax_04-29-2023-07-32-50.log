python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_04-29-2023-07-32-50.log
I0429 07:33:10.786244 140699730134848 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax.
I0429 07:33:10.859027 140699730134848 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0429 07:33:11.730228 140699730134848 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0429 07:33:11.730824 140699730134848 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0429 07:33:11.734725 140699730134848 submission_runner.py:538] Using RNG seed 3385550922
I0429 07:33:14.360355 140699730134848 submission_runner.py:547] --- Tuning run 1/1 ---
I0429 07:33:14.360550 140699730134848 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1.
I0429 07:33:14.360729 140699730134848 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1/hparams.json.
I0429 07:33:14.481986 140699730134848 submission_runner.py:241] Initializing dataset.
I0429 07:33:14.482226 140699730134848 submission_runner.py:248] Initializing model.
I0429 07:33:31.364335 140699730134848 submission_runner.py:258] Initializing optimizer.
I0429 07:33:32.024879 140699730134848 submission_runner.py:265] Initializing metrics bundle.
I0429 07:33:32.025088 140699730134848 submission_runner.py:282] Initializing checkpoint and logger.
I0429 07:33:32.026017 140699730134848 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0429 07:33:32.026277 140699730134848 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0429 07:33:32.026343 140699730134848 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0429 07:33:32.969707 140699730134848 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0429 07:33:32.970666 140699730134848 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0429 07:33:32.976857 140699730134848 submission_runner.py:318] Starting training loop.
I0429 07:33:33.175731 140699730134848 input_pipeline.py:20] Loading split = train-clean-100
I0429 07:33:33.207759 140699730134848 input_pipeline.py:20] Loading split = train-clean-360
I0429 07:33:33.518749 140699730134848 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0429 07:34:44.623316 140524031825664 logging_writer.py:48] [0] global_step=0, grad_norm=22.043359756469727, loss=32.65432357788086
I0429 07:34:44.650364 140699730134848 spec.py:298] Evaluating on the training split.
I0429 07:34:44.784223 140699730134848 input_pipeline.py:20] Loading split = train-clean-100
I0429 07:34:44.812966 140699730134848 input_pipeline.py:20] Loading split = train-clean-360
I0429 07:34:45.160948 140699730134848 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0429 07:36:20.723685 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 07:36:20.829095 140699730134848 input_pipeline.py:20] Loading split = dev-clean
I0429 07:36:20.834976 140699730134848 input_pipeline.py:20] Loading split = dev-other
I0429 07:37:15.648852 140699730134848 spec.py:326] Evaluating on the test split.
I0429 07:37:15.759310 140699730134848 input_pipeline.py:20] Loading split = test-clean
I0429 07:37:52.628473 140699730134848 submission_runner.py:415] Time since start: 259.65s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.168377, dtype=float32), 'train/wer': 4.792562938591924, 'validation/ctc_loss': DeviceArray(30.028465, dtype=float32), 'validation/wer': 4.182114636899536, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.980082, dtype=float32), 'test/wer': 4.616740803932322, 'test/num_examples': 2472, 'score': 71.67330384254456, 'total_duration': 259.65006971359253, 'accumulated_submission_time': 71.67330384254456, 'accumulated_eval_time': 187.9765989780426, 'accumulated_logging_time': 0}
I0429 07:37:52.648948 140521079035648 logging_writer.py:48] [1] accumulated_eval_time=187.976599, accumulated_logging_time=0, accumulated_submission_time=71.673304, global_step=1, preemption_count=0, score=71.673304, test/ctc_loss=29.98008155822754, test/num_examples=2472, test/wer=4.616741, total_duration=259.650070, train/ctc_loss=31.168376922607422, train/wer=4.792563, validation/ctc_loss=30.028465270996094, validation/num_examples=5348, validation/wer=4.182115
I0429 07:40:18.501062 140524343174912 logging_writer.py:48] [100] global_step=100, grad_norm=41.54750442504883, loss=31.095890045166016
I0429 07:42:13.847644 140524351567616 logging_writer.py:48] [200] global_step=200, grad_norm=38.15302658081055, loss=21.79654312133789
I0429 07:44:09.079227 140524343174912 logging_writer.py:48] [300] global_step=300, grad_norm=14.687695503234863, loss=12.948326110839844
I0429 07:46:04.398329 140524351567616 logging_writer.py:48] [400] global_step=400, grad_norm=5.737242221832275, loss=8.05495834350586
I0429 07:47:59.687317 140524343174912 logging_writer.py:48] [500] global_step=500, grad_norm=1.6229498386383057, loss=6.613007545471191
I0429 07:49:55.084781 140524351567616 logging_writer.py:48] [600] global_step=600, grad_norm=0.8429825901985168, loss=6.1019439697265625
I0429 07:51:50.652545 140524343174912 logging_writer.py:48] [700] global_step=700, grad_norm=0.4833213984966278, loss=5.884171009063721
I0429 07:53:46.019055 140524351567616 logging_writer.py:48] [800] global_step=800, grad_norm=0.5791674852371216, loss=5.848631381988525
I0429 07:55:41.261735 140524343174912 logging_writer.py:48] [900] global_step=900, grad_norm=0.7218378186225891, loss=5.824328899383545
I0429 07:57:36.752786 140524351567616 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5044988393783569, loss=5.7467265129089355
I0429 07:59:35.234337 140524838344448 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1539605855941772, loss=5.689899921417236
I0429 08:01:30.447840 140524829951744 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6273127794265747, loss=5.643749713897705
I0429 08:03:25.495938 140524838344448 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.759270966053009, loss=5.562417030334473
I0429 08:05:20.776637 140524829951744 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7446902394294739, loss=5.515404224395752
I0429 08:07:16.307549 140524838344448 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0444018840789795, loss=5.473937511444092
I0429 08:09:11.825718 140524829951744 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6884918212890625, loss=5.395867347717285
I0429 08:11:07.263347 140524838344448 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9066489934921265, loss=5.285048484802246
I0429 08:13:02.905889 140524829951744 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9156891703605652, loss=5.216916084289551
I0429 08:14:58.604939 140524838344448 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.5948386192321777, loss=5.061361312866211
I0429 08:16:53.969506 140524829951744 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0547362565994263, loss=4.906245231628418
I0429 08:17:53.432256 140699730134848 spec.py:298] Evaluating on the training split.
I0429 08:18:22.570358 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 08:18:57.465627 140699730134848 spec.py:326] Evaluating on the test split.
I0429 08:19:15.331985 140699730134848 submission_runner.py:415] Time since start: 2742.35s, 	Step: 2053, 	{'train/ctc_loss': DeviceArray(6.1825256, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.0798388, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.0263276, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2472.4168376922607, 'total_duration': 2742.3530128002167, 'accumulated_submission_time': 2472.4168376922607, 'accumulated_eval_time': 269.87428092956543, 'accumulated_logging_time': 0.03369498252868652}
I0429 08:19:15.354566 140524838344448 logging_writer.py:48] [2053] accumulated_eval_time=269.874281, accumulated_logging_time=0.033695, accumulated_submission_time=2472.416838, global_step=2053, preemption_count=0, score=2472.416838, test/ctc_loss=6.026327610015869, test/num_examples=2472, test/wer=0.899580, total_duration=2742.353013, train/ctc_loss=6.182525634765625, train/wer=0.944636, validation/ctc_loss=6.079838752746582, validation/num_examples=5348, validation/wer=0.895995
I0429 08:20:14.250064 140524838344448 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3285725116729736, loss=4.786754608154297
I0429 08:22:09.251817 140524829951744 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.6855417490005493, loss=4.553814888000488
I0429 08:24:04.048285 140524838344448 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.920580506324768, loss=4.4289069175720215
I0429 08:25:59.207626 140524829951744 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.6938889026641846, loss=4.271169662475586
I0429 08:27:54.360716 140524838344448 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.1124703884124756, loss=4.193411350250244
I0429 08:29:49.752129 140524829951744 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.0166516304016113, loss=4.0106964111328125
I0429 08:31:45.471102 140524838344448 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.3304355144500732, loss=3.9471724033355713
I0429 08:33:40.675173 140524829951744 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.779773235321045, loss=3.831845760345459
I0429 08:35:36.029979 140524838344448 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.6199769973754883, loss=3.769622802734375
I0429 08:37:31.369595 140524829951744 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.519923448562622, loss=3.6854145526885986
I0429 08:39:29.897607 140525493704448 logging_writer.py:48] [3100] global_step=3100, grad_norm=6.0451130867004395, loss=3.624237537384033
I0429 08:41:25.152165 140525485311744 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.5646326541900635, loss=3.5361149311065674
I0429 08:43:20.149873 140525493704448 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.816836357116699, loss=3.4291787147521973
I0429 08:45:15.427970 140525485311744 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.322486162185669, loss=3.366753101348877
I0429 08:47:10.676391 140525493704448 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.51983904838562, loss=3.3575525283813477
I0429 08:49:05.908407 140525485311744 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.69818377494812, loss=3.274285316467285
I0429 08:51:00.919729 140525493704448 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2755842208862305, loss=3.218425989151001
I0429 08:52:56.181905 140525485311744 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.6322786808013916, loss=3.206719398498535
I0429 08:54:51.417536 140525493704448 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.2325241565704346, loss=3.1658053398132324
I0429 08:56:46.744085 140525485311744 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.328031301498413, loss=3.0620956420898438
I0429 08:58:42.497271 140525493704448 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.6119000911712646, loss=3.077606678009033
I0429 08:59:15.779769 140699730134848 spec.py:298] Evaluating on the training split.
I0429 08:59:51.834946 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 09:00:28.967468 140699730134848 spec.py:326] Evaluating on the test split.
I0429 09:00:48.358764 140699730134848 submission_runner.py:415] Time since start: 5235.38s, 	Step: 4127, 	{'train/ctc_loss': DeviceArray(4.3428454, dtype=float32), 'train/wer': 0.8270250113203527, 'validation/ctc_loss': DeviceArray(4.515525, dtype=float32), 'validation/wer': 0.8093855222915802, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.235039, dtype=float32), 'test/wer': 0.7890236223671115, 'test/num_examples': 2472, 'score': 4872.797388553619, 'total_duration': 5235.377559185028, 'accumulated_submission_time': 4872.797388553619, 'accumulated_eval_time': 362.44918751716614, 'accumulated_logging_time': 0.07271170616149902}
I0429 09:00:48.380544 140524254664448 logging_writer.py:48] [4127] accumulated_eval_time=362.449188, accumulated_logging_time=0.072712, accumulated_submission_time=4872.797389, global_step=4127, preemption_count=0, score=4872.797389, test/ctc_loss=4.235039234161377, test/num_examples=2472, test/wer=0.789024, total_duration=5235.377559, train/ctc_loss=4.342845439910889, train/wer=0.827025, validation/ctc_loss=4.515524864196777, validation/num_examples=5348, validation/wer=0.809386
I0429 09:02:13.489148 140524246271744 logging_writer.py:48] [4200] global_step=4200, grad_norm=5.119697570800781, loss=3.0715315341949463
I0429 09:04:08.371004 140524254664448 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.605637550354004, loss=3.0087358951568604
I0429 09:06:02.792191 140524246271744 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.692444324493408, loss=2.9464752674102783
I0429 09:07:57.437293 140524254664448 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.7771353721618652, loss=2.9347376823425293
I0429 09:09:52.047217 140524246271744 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.2621917724609375, loss=2.912226915359497
I0429 09:11:47.322277 140524254664448 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.6165974140167236, loss=2.8353633880615234
I0429 09:13:42.579749 140524246271744 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.537473678588867, loss=2.8114049434661865
I0429 09:15:37.965100 140524254664448 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.191293239593506, loss=2.7916007041931152
I0429 09:17:33.268002 140524246271744 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.378075361251831, loss=2.79445743560791
I0429 09:19:28.632407 140524254664448 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.120875597000122, loss=2.7335989475250244
I0429 09:21:27.512779 140524254664448 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.263248920440674, loss=2.735910177230835
I0429 09:23:22.282381 140524246271744 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.357511043548584, loss=2.6764440536499023
I0429 09:25:17.250210 140524254664448 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.110650539398193, loss=2.6607754230499268
I0429 09:27:12.439368 140524246271744 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.760551452636719, loss=2.601431131362915
I0429 09:29:08.704295 140524254664448 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.770235061645508, loss=2.6665661334991455
I0429 09:31:03.953115 140524246271744 logging_writer.py:48] [5700] global_step=5700, grad_norm=5.238519191741943, loss=2.5287516117095947
I0429 09:32:58.838941 140524254664448 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.865419864654541, loss=2.57124662399292
I0429 09:34:53.633442 140524246271744 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.3393564224243164, loss=2.589026689529419
I0429 09:36:48.639124 140524254664448 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.6199705600738525, loss=2.5300838947296143
I0429 09:38:44.179549 140524246271744 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.75132417678833, loss=2.472013473510742
I0429 09:40:42.938757 140524254664448 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.2940797805786133, loss=2.4605624675750732
I0429 09:40:48.594451 140699730134848 spec.py:298] Evaluating on the training split.
I0429 09:41:29.067471 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 09:42:08.148166 140699730134848 spec.py:326] Evaluating on the test split.
I0429 09:42:28.222496 140699730134848 submission_runner.py:415] Time since start: 7735.24s, 	Step: 6206, 	{'train/ctc_loss': DeviceArray(1.2460097, dtype=float32), 'train/wer': 0.3605115148831644, 'validation/ctc_loss': DeviceArray(1.65578, dtype=float32), 'validation/wer': 0.4190392574940424, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.2854239, dtype=float32), 'test/wer': 0.35520890459651044, 'test/num_examples': 2472, 'score': 7272.96738576889, 'total_duration': 7735.241549253464, 'accumulated_submission_time': 7272.96738576889, 'accumulated_eval_time': 462.0732092857361, 'accumulated_logging_time': 0.10980629920959473}
I0429 09:42:28.244852 140524479944448 logging_writer.py:48] [6206] accumulated_eval_time=462.073209, accumulated_logging_time=0.109806, accumulated_submission_time=7272.967386, global_step=6206, preemption_count=0, score=7272.967386, test/ctc_loss=1.2854238748550415, test/num_examples=2472, test/wer=0.355209, total_duration=7735.241549, train/ctc_loss=1.2460097074508667, train/wer=0.360512, validation/ctc_loss=1.6557799577713013, validation/num_examples=5348, validation/wer=0.419039
I0429 09:44:17.088940 140524471551744 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.866083860397339, loss=2.428577423095703
I0429 09:46:11.954036 140524479944448 logging_writer.py:48] [6400] global_step=6400, grad_norm=4.648402214050293, loss=2.446828603744507
I0429 09:48:06.866452 140524471551744 logging_writer.py:48] [6500] global_step=6500, grad_norm=8.01821231842041, loss=2.369040012359619
I0429 09:50:01.965867 140524479944448 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.916731595993042, loss=2.3139779567718506
I0429 09:51:57.141153 140524471551744 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.050017833709717, loss=2.3536765575408936
I0429 09:53:52.158469 140524479944448 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.4981629848480225, loss=2.4023759365081787
I0429 09:55:47.340833 140524471551744 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.8917341232299805, loss=2.410835027694702
I0429 09:57:42.582561 140524479944448 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.232029676437378, loss=2.3791959285736084
I0429 09:59:37.558477 140524471551744 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.251593112945557, loss=2.303286552429199
I0429 10:01:32.223960 140524479944448 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.56843638420105, loss=2.2609975337982178
I0429 10:03:30.670794 140524479944448 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.7028937339782715, loss=2.2927656173706055
I0429 10:05:25.005166 140524471551744 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.56327748298645, loss=2.355924367904663
I0429 10:07:19.894657 140524479944448 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.001585483551025, loss=2.3241238594055176
I0429 10:09:14.897500 140524471551744 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.2299554347991943, loss=2.242353677749634
I0429 10:11:09.901681 140524479944448 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.664430618286133, loss=2.2731540203094482
I0429 10:13:04.881700 140524471551744 logging_writer.py:48] [7800] global_step=7800, grad_norm=4.71671199798584, loss=2.2122581005096436
I0429 10:15:00.162592 140524479944448 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.884251117706299, loss=2.2016451358795166
I0429 10:16:55.311412 140524471551744 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.9407451152801514, loss=2.1815927028656006
I0429 10:18:50.404368 140524479944448 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.3599860668182373, loss=2.189955949783325
I0429 10:20:45.448384 140524471551744 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.009216785430908, loss=2.213207483291626
I0429 10:22:28.553080 140699730134848 spec.py:298] Evaluating on the training split.
I0429 10:23:08.716379 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 10:23:48.067568 140699730134848 spec.py:326] Evaluating on the test split.
I0429 10:24:08.667069 140699730134848 submission_runner.py:415] Time since start: 10235.69s, 	Step: 8288, 	{'train/ctc_loss': DeviceArray(0.84740275, dtype=float32), 'train/wer': 0.2665385541790154, 'validation/ctc_loss': DeviceArray(1.2122554, dtype=float32), 'validation/wer': 0.3311175216355199, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8643842, dtype=float32), 'test/wer': 0.2641114699490179, 'test/num_examples': 2472, 'score': 9673.231229782104, 'total_duration': 10235.686563968658, 'accumulated_submission_time': 9673.231229782104, 'accumulated_eval_time': 562.1838192939758, 'accumulated_logging_time': 0.14803624153137207}
I0429 10:24:08.690890 140524910024448 logging_writer.py:48] [8288] accumulated_eval_time=562.183819, accumulated_logging_time=0.148036, accumulated_submission_time=9673.231230, global_step=8288, preemption_count=0, score=9673.231230, test/ctc_loss=0.8643841743469238, test/num_examples=2472, test/wer=0.264111, total_duration=10235.686564, train/ctc_loss=0.8474027514457703, train/wer=0.266539, validation/ctc_loss=1.2122553586959839, validation/num_examples=5348, validation/wer=0.331118
I0429 10:24:23.664425 140524901631744 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.6964783668518066, loss=2.157831907272339
I0429 10:26:18.082966 140524910024448 logging_writer.py:48] [8400] global_step=8400, grad_norm=6.9036102294921875, loss=2.1615986824035645
I0429 10:28:13.105273 140524901631744 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.1289727687835693, loss=2.186384677886963
I0429 10:30:07.463693 140524910024448 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.1035268306732178, loss=2.092129707336426
I0429 10:32:02.196018 140524901631744 logging_writer.py:48] [8700] global_step=8700, grad_norm=4.4009833335876465, loss=2.1972904205322266
I0429 10:33:57.113592 140524910024448 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.516613006591797, loss=2.1386709213256836
I0429 10:35:51.542320 140524901631744 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.5460855960845947, loss=2.1179921627044678
I0429 10:37:45.950221 140524910024448 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.6052510738372803, loss=2.078885316848755
I0429 10:39:40.373285 140524901631744 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.90229868888855, loss=2.055377244949341
I0429 10:41:35.045588 140524910024448 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.923126459121704, loss=2.0826075077056885
I0429 10:43:33.219896 140524910024448 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.2995083332061768, loss=2.0940072536468506
I0429 10:45:28.351300 140524901631744 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.9710283279418945, loss=2.1375088691711426
I0429 10:47:23.052474 140524910024448 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.63053560256958, loss=2.0118625164031982
I0429 10:49:17.772361 140524901631744 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.163735389709473, loss=2.0657520294189453
I0429 10:51:13.276393 140524910024448 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.444099187850952, loss=2.1027841567993164
I0429 10:53:07.600160 140524901631744 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.5847434997558594, loss=2.0406134128570557
I0429 10:55:03.329887 140524910024448 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.9142911434173584, loss=2.028526544570923
I0429 10:56:57.822620 140524901631744 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.121857166290283, loss=2.0326247215270996
I0429 10:58:52.140961 140524910024448 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.938190937042236, loss=2.04237699508667
I0429 11:00:46.702476 140524901631744 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.619593620300293, loss=2.0082621574401855
I0429 11:02:44.975893 140524910024448 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.2429964542388916, loss=2.0024631023406982
I0429 11:04:08.754473 140699730134848 spec.py:298] Evaluating on the training split.
I0429 11:04:48.477359 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 11:05:27.639135 140699730134848 spec.py:326] Evaluating on the test split.
I0429 11:05:48.254509 140699730134848 submission_runner.py:415] Time since start: 12735.27s, 	Step: 10374, 	{'train/ctc_loss': DeviceArray(0.6934049, dtype=float32), 'train/wer': 0.2239829089564896, 'validation/ctc_loss': DeviceArray(1.0498068, dtype=float32), 'validation/wer': 0.29353877027274744, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7217323, dtype=float32), 'test/wer': 0.22562102654723457, 'test/num_examples': 2472, 'score': 12073.251592874527, 'total_duration': 12735.274204015732, 'accumulated_submission_time': 12073.251592874527, 'accumulated_eval_time': 661.6804869174957, 'accumulated_logging_time': 0.1871929168701172}
I0429 11:05:48.277511 140524910024448 logging_writer.py:48] [10374] accumulated_eval_time=661.680487, accumulated_logging_time=0.187193, accumulated_submission_time=12073.251593, global_step=10374, preemption_count=0, score=12073.251593, test/ctc_loss=0.7217323184013367, test/num_examples=2472, test/wer=0.225621, total_duration=12735.274204, train/ctc_loss=0.6934049129486084, train/wer=0.223983, validation/ctc_loss=1.049806833267212, validation/num_examples=5348, validation/wer=0.293539
I0429 11:06:19.267132 140524901631744 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.639451742172241, loss=1.9713326692581177
I0429 11:08:13.871288 140524910024448 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.4005632400512695, loss=1.9462320804595947
I0429 11:10:08.974802 140524901631744 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.439159393310547, loss=2.013233184814453
I0429 11:12:04.074475 140524910024448 logging_writer.py:48] [10700] global_step=10700, grad_norm=5.246711254119873, loss=1.9971115589141846
I0429 11:13:58.973725 140524901631744 logging_writer.py:48] [10800] global_step=10800, grad_norm=4.768902778625488, loss=1.9799031019210815
I0429 11:15:54.822545 140524910024448 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.610676288604736, loss=1.9988787174224854
I0429 11:17:49.825691 140524901631744 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.1196818351745605, loss=1.9313318729400635
I0429 11:19:44.367086 140524910024448 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.916332244873047, loss=1.8960515260696411
I0429 11:21:38.904258 140524901631744 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.018187046051025, loss=1.9646674394607544
I0429 11:23:33.352829 140524910024448 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.516169548034668, loss=1.9076355695724487
I0429 11:25:31.097985 140524910024448 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.8259634971618652, loss=1.9392340183258057
I0429 11:27:24.926872 140524901631744 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.799945592880249, loss=1.9024690389633179
I0429 11:29:19.155291 140524910024448 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.702282905578613, loss=1.9461208581924438
I0429 11:31:14.383806 140524901631744 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.624395847320557, loss=1.933671236038208
I0429 11:33:09.487538 140524910024448 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.780358791351318, loss=1.9832426309585571
I0429 11:35:03.857472 140524901631744 logging_writer.py:48] [11900] global_step=11900, grad_norm=7.146746635437012, loss=1.8707085847854614
I0429 11:36:58.127386 140524910024448 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.9159562587738037, loss=1.9365793466567993
I0429 11:38:52.999961 140524901631744 logging_writer.py:48] [12100] global_step=12100, grad_norm=7.776583671569824, loss=1.9562721252441406
I0429 11:40:48.159823 140524910024448 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.523987293243408, loss=1.9771578311920166
I0429 11:42:42.819404 140524901631744 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.810792446136475, loss=1.9192370176315308
I0429 11:44:41.373604 140524254664448 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.6467714309692383, loss=1.8626915216445923
I0429 11:45:49.048166 140699730134848 spec.py:298] Evaluating on the training split.
I0429 11:46:29.454835 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 11:47:09.187195 140699730134848 spec.py:326] Evaluating on the test split.
I0429 11:47:29.538980 140699730134848 submission_runner.py:415] Time since start: 15236.56s, 	Step: 12460, 	{'train/ctc_loss': DeviceArray(0.6113836, dtype=float32), 'train/wer': 0.20262895369990025, 'validation/ctc_loss': DeviceArray(0.9481769, dtype=float32), 'validation/wer': 0.2689847465966869, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6243479, dtype=float32), 'test/wer': 0.19834257510206568, 'test/num_examples': 2472, 'score': 14473.977898836136, 'total_duration': 15236.558571338654, 'accumulated_submission_time': 14473.977898836136, 'accumulated_eval_time': 762.1678032875061, 'accumulated_logging_time': 0.2266387939453125}
I0429 11:47:29.562372 140524254664448 logging_writer.py:48] [12460] accumulated_eval_time=762.167803, accumulated_logging_time=0.226639, accumulated_submission_time=14473.977899, global_step=12460, preemption_count=0, score=14473.977899, test/ctc_loss=0.6243479251861572, test/num_examples=2472, test/wer=0.198343, total_duration=15236.558571, train/ctc_loss=0.6113836169242859, train/wer=0.202629, validation/ctc_loss=0.948176920413971, validation/num_examples=5348, validation/wer=0.268985
I0429 11:48:16.408540 140524246271744 logging_writer.py:48] [12500] global_step=12500, grad_norm=7.238746166229248, loss=1.8049238920211792
I0429 11:50:10.692167 140524254664448 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.657938480377197, loss=1.8670953512191772
I0429 11:52:05.047733 140524246271744 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.072482109069824, loss=1.8472704887390137
I0429 11:54:00.177648 140524254664448 logging_writer.py:48] [12800] global_step=12800, grad_norm=4.216814994812012, loss=1.8610501289367676
I0429 11:55:54.299719 140524246271744 logging_writer.py:48] [12900] global_step=12900, grad_norm=5.646132469177246, loss=1.8676522970199585
I0429 11:57:48.430883 140524254664448 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.237850666046143, loss=1.8169200420379639
I0429 11:59:42.657096 140524246271744 logging_writer.py:48] [13100] global_step=13100, grad_norm=6.653018474578857, loss=1.855444312095642
I0429 12:01:37.251161 140524254664448 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.5008528232574463, loss=1.8991237878799438
I0429 12:03:31.486705 140524246271744 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.222496271133423, loss=1.796034574508667
I0429 12:05:29.550133 140524254664448 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.5636260509490967, loss=1.8042916059494019
I0429 12:07:23.934544 140524246271744 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.6855034828186035, loss=1.8619160652160645
I0429 12:09:17.814057 140524254664448 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.711923122406006, loss=1.8069881200790405
I0429 12:11:12.035542 140524246271744 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.125317573547363, loss=1.8203203678131104
I0429 12:13:06.220693 140524254664448 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.781516075134277, loss=1.8232582807540894
I0429 12:15:00.606625 140524246271744 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.811190128326416, loss=1.8407460451126099
I0429 12:16:55.984189 140524254664448 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.9564576148986816, loss=1.8265352249145508
I0429 12:18:50.232063 140524246271744 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.670714855194092, loss=1.804374098777771
I0429 12:20:44.761414 140524254664448 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.581190824508667, loss=1.8076603412628174
I0429 12:22:39.209085 140524246271744 logging_writer.py:48] [14300] global_step=14300, grad_norm=8.243326187133789, loss=1.8473625183105469
I0429 12:24:33.009451 140524254664448 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.397707462310791, loss=1.7743258476257324
I0429 12:26:30.447363 140524254664448 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.431689262390137, loss=1.7611392736434937
I0429 12:27:30.366769 140699730134848 spec.py:298] Evaluating on the training split.
I0429 12:28:10.609132 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 12:28:49.818235 140699730134848 spec.py:326] Evaluating on the test split.
I0429 12:29:10.508459 140699730134848 submission_runner.py:415] Time since start: 17737.53s, 	Step: 14554, 	{'train/ctc_loss': DeviceArray(0.52294385, dtype=float32), 'train/wer': 0.17321272839427393, 'validation/ctc_loss': DeviceArray(0.8758386, dtype=float32), 'validation/wer': 0.24864687551254716, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5544774, dtype=float32), 'test/wer': 0.17679198911299332, 'test/num_examples': 2472, 'score': 16874.73937344551, 'total_duration': 17737.52787733078, 'accumulated_submission_time': 16874.73937344551, 'accumulated_eval_time': 862.3058698177338, 'accumulated_logging_time': 0.26541566848754883}
I0429 12:29:10.532710 140524910024448 logging_writer.py:48] [14554] accumulated_eval_time=862.305870, accumulated_logging_time=0.265416, accumulated_submission_time=16874.739373, global_step=14554, preemption_count=0, score=16874.739373, test/ctc_loss=0.5544773936271667, test/num_examples=2472, test/wer=0.176792, total_duration=17737.527877, train/ctc_loss=0.5229438543319702, train/wer=0.173213, validation/ctc_loss=0.875838577747345, validation/num_examples=5348, validation/wer=0.248647
I0429 12:30:04.153295 140524901631744 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.019989967346191, loss=1.7853789329528809
I0429 12:31:58.505294 140524910024448 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.64505672454834, loss=1.8394266366958618
I0429 12:33:53.423760 140524901631744 logging_writer.py:48] [14800] global_step=14800, grad_norm=6.068360328674316, loss=1.7800743579864502
I0429 12:35:47.902531 140524910024448 logging_writer.py:48] [14900] global_step=14900, grad_norm=8.998353004455566, loss=1.7656153440475464
I0429 12:37:41.928030 140524901631744 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.054462432861328, loss=1.764916181564331
I0429 12:39:35.898102 140524910024448 logging_writer.py:48] [15100] global_step=15100, grad_norm=5.374629974365234, loss=1.8040510416030884
I0429 12:41:30.184819 140524901631744 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.357633113861084, loss=1.835759162902832
I0429 12:43:24.684039 140524910024448 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.422682762145996, loss=1.758773684501648
I0429 12:45:19.291377 140524901631744 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.9222893714904785, loss=1.7835673093795776
I0429 12:47:16.799931 140524910024448 logging_writer.py:48] [15500] global_step=15500, grad_norm=8.616130828857422, loss=1.8049054145812988
I0429 12:49:10.477358 140524901631744 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.747269153594971, loss=1.7677950859069824
I0429 12:51:04.236566 140524910024448 logging_writer.py:48] [15700] global_step=15700, grad_norm=8.580778121948242, loss=1.7965810298919678
I0429 12:52:58.121709 140524901631744 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.842156410217285, loss=1.767101526260376
I0429 12:54:52.179733 140524910024448 logging_writer.py:48] [15900] global_step=15900, grad_norm=5.281014442443848, loss=1.765380859375
I0429 12:56:45.004620 140699730134848 spec.py:298] Evaluating on the training split.
I0429 12:57:25.953219 140699730134848 spec.py:310] Evaluating on the validation split.
I0429 12:58:05.498636 140699730134848 spec.py:326] Evaluating on the test split.
I0429 12:58:25.910322 140699730134848 submission_runner.py:415] Time since start: 19492.93s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.45904854, dtype=float32), 'train/wer': 0.15748199381159156, 'validation/ctc_loss': DeviceArray(0.83951527, dtype=float32), 'validation/wer': 0.23910505648872638, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.53509504, dtype=float32), 'test/wer': 0.17179534052363252, 'test/num_examples': 2472, 'score': 18529.17635011673, 'total_duration': 19492.931182146072, 'accumulated_submission_time': 18529.17635011673, 'accumulated_eval_time': 963.2093715667725, 'accumulated_logging_time': 0.30461883544921875}
I0429 12:58:25.934538 140524479944448 logging_writer.py:48] [16000] accumulated_eval_time=963.209372, accumulated_logging_time=0.304619, accumulated_submission_time=18529.176350, global_step=16000, preemption_count=0, score=18529.176350, test/ctc_loss=0.5350950360298157, test/num_examples=2472, test/wer=0.171795, total_duration=19492.931182, train/ctc_loss=0.4590485394001007, train/wer=0.157482, validation/ctc_loss=0.8395152688026428, validation/num_examples=5348, validation/wer=0.239105
I0429 12:58:25.959198 140524471551744 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18529.176350
I0429 12:58:26.108826 140699730134848 checkpoints.py:356] Saving checkpoint at step: 16000
I0429 12:58:26.797648 140699730134848 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0429 12:58:26.813370 140699730134848 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0429 12:58:28.146110 140699730134848 submission_runner.py:578] Tuning trial 1/1
I0429 12:58:28.146363 140699730134848 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 12:58:28.153387 140699730134848 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.168377, dtype=float32), 'train/wer': 4.792562938591924, 'validation/ctc_loss': DeviceArray(30.028465, dtype=float32), 'validation/wer': 4.182114636899536, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.980082, dtype=float32), 'test/wer': 4.616740803932322, 'test/num_examples': 2472, 'score': 71.67330384254456, 'total_duration': 259.65006971359253, 'accumulated_submission_time': 71.67330384254456, 'accumulated_eval_time': 187.9765989780426, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2053, {'train/ctc_loss': DeviceArray(6.1825256, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.0798388, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.0263276, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2472.4168376922607, 'total_duration': 2742.3530128002167, 'accumulated_submission_time': 2472.4168376922607, 'accumulated_eval_time': 269.87428092956543, 'accumulated_logging_time': 0.03369498252868652, 'global_step': 2053, 'preemption_count': 0}), (4127, {'train/ctc_loss': DeviceArray(4.3428454, dtype=float32), 'train/wer': 0.8270250113203527, 'validation/ctc_loss': DeviceArray(4.515525, dtype=float32), 'validation/wer': 0.8093855222915802, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(4.235039, dtype=float32), 'test/wer': 0.7890236223671115, 'test/num_examples': 2472, 'score': 4872.797388553619, 'total_duration': 5235.377559185028, 'accumulated_submission_time': 4872.797388553619, 'accumulated_eval_time': 362.44918751716614, 'accumulated_logging_time': 0.07271170616149902, 'global_step': 4127, 'preemption_count': 0}), (6206, {'train/ctc_loss': DeviceArray(1.2460097, dtype=float32), 'train/wer': 0.3605115148831644, 'validation/ctc_loss': DeviceArray(1.65578, dtype=float32), 'validation/wer': 0.4190392574940424, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.2854239, dtype=float32), 'test/wer': 0.35520890459651044, 'test/num_examples': 2472, 'score': 7272.96738576889, 'total_duration': 7735.241549253464, 'accumulated_submission_time': 7272.96738576889, 'accumulated_eval_time': 462.0732092857361, 'accumulated_logging_time': 0.10980629920959473, 'global_step': 6206, 'preemption_count': 0}), (8288, {'train/ctc_loss': DeviceArray(0.84740275, dtype=float32), 'train/wer': 0.2665385541790154, 'validation/ctc_loss': DeviceArray(1.2122554, dtype=float32), 'validation/wer': 0.3311175216355199, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.8643842, dtype=float32), 'test/wer': 0.2641114699490179, 'test/num_examples': 2472, 'score': 9673.231229782104, 'total_duration': 10235.686563968658, 'accumulated_submission_time': 9673.231229782104, 'accumulated_eval_time': 562.1838192939758, 'accumulated_logging_time': 0.14803624153137207, 'global_step': 8288, 'preemption_count': 0}), (10374, {'train/ctc_loss': DeviceArray(0.6934049, dtype=float32), 'train/wer': 0.2239829089564896, 'validation/ctc_loss': DeviceArray(1.0498068, dtype=float32), 'validation/wer': 0.29353877027274744, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7217323, dtype=float32), 'test/wer': 0.22562102654723457, 'test/num_examples': 2472, 'score': 12073.251592874527, 'total_duration': 12735.274204015732, 'accumulated_submission_time': 12073.251592874527, 'accumulated_eval_time': 661.6804869174957, 'accumulated_logging_time': 0.1871929168701172, 'global_step': 10374, 'preemption_count': 0}), (12460, {'train/ctc_loss': DeviceArray(0.6113836, dtype=float32), 'train/wer': 0.20262895369990025, 'validation/ctc_loss': DeviceArray(0.9481769, dtype=float32), 'validation/wer': 0.2689847465966869, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6243479, dtype=float32), 'test/wer': 0.19834257510206568, 'test/num_examples': 2472, 'score': 14473.977898836136, 'total_duration': 15236.558571338654, 'accumulated_submission_time': 14473.977898836136, 'accumulated_eval_time': 762.1678032875061, 'accumulated_logging_time': 0.2266387939453125, 'global_step': 12460, 'preemption_count': 0}), (14554, {'train/ctc_loss': DeviceArray(0.52294385, dtype=float32), 'train/wer': 0.17321272839427393, 'validation/ctc_loss': DeviceArray(0.8758386, dtype=float32), 'validation/wer': 0.24864687551254716, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5544774, dtype=float32), 'test/wer': 0.17679198911299332, 'test/num_examples': 2472, 'score': 16874.73937344551, 'total_duration': 17737.52787733078, 'accumulated_submission_time': 16874.73937344551, 'accumulated_eval_time': 862.3058698177338, 'accumulated_logging_time': 0.26541566848754883, 'global_step': 14554, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.45904854, dtype=float32), 'train/wer': 0.15748199381159156, 'validation/ctc_loss': DeviceArray(0.83951527, dtype=float32), 'validation/wer': 0.23910505648872638, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.53509504, dtype=float32), 'test/wer': 0.17179534052363252, 'test/num_examples': 2472, 'score': 18529.17635011673, 'total_duration': 19492.931182146072, 'accumulated_submission_time': 18529.17635011673, 'accumulated_eval_time': 963.2093715667725, 'accumulated_logging_time': 0.30461883544921875, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0429 12:58:28.153666 140699730134848 submission_runner.py:581] Timing: 18529.17635011673
I0429 12:58:28.153750 140699730134848 submission_runner.py:582] ====================
I0429 12:58:28.154497 140699730134848 submission_runner.py:645] Final librispeech_deepspeech score: 18529.17635011673
