python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_04-28-2023-13-16-54.log
I0428 13:17:15.654346 140637852153664 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax.
I0428 13:17:15.729226 140637852153664 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 13:17:16.607482 140637852153664 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0428 13:17:16.608290 140637852153664 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 13:17:16.611615 140637852153664 submission_runner.py:538] Using RNG seed 3749651152
I0428 13:17:19.322196 140637852153664 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 13:17:19.322413 140637852153664 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1.
I0428 13:17:19.322702 140637852153664 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1/hparams.json.
I0428 13:17:19.445211 140637852153664 submission_runner.py:241] Initializing dataset.
I0428 13:17:19.677695 140637852153664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:17:19.682967 140637852153664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 13:17:19.913015 140637852153664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:17:19.969210 140637852153664 submission_runner.py:248] Initializing model.
I0428 13:17:27.829136 140637852153664 submission_runner.py:258] Initializing optimizer.
I0428 13:17:28.928929 140637852153664 submission_runner.py:265] Initializing metrics bundle.
I0428 13:17:28.929119 140637852153664 submission_runner.py:282] Initializing checkpoint and logger.
I0428 13:17:28.930127 140637852153664 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1 with prefix checkpoint_
I0428 13:17:28.930375 140637852153664 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 13:17:28.930464 140637852153664 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 13:17:29.764343 140637852153664 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1/meta_data_0.json.
I0428 13:17:29.765341 140637852153664 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1/flags_0.json.
I0428 13:17:29.771167 140637852153664 submission_runner.py:318] Starting training loop.
I0428 13:18:04.620410 140461670921984 logging_writer.py:48] [0] global_step=0, grad_norm=2.895190954208374, loss=0.7277911305427551
I0428 13:18:04.633743 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:18:04.642173 140637852153664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:18:04.645751 140637852153664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 13:18:04.703504 140637852153664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:19:36.195655 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:19:36.198452 140637852153664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:19:36.202272 140637852153664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 13:19:36.256372 140637852153664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:20:39.546364 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:20:39.549040 140637852153664 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:20:39.552882 140637852153664 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 13:20:39.604997 140637852153664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 13:21:43.457744 140637852153664 submission_runner.py:415] Time since start: 253.69s, 	Step: 1, 	{'train/accuracy': 0.5128725171089172, 'train/loss': 0.7289538979530334, 'train/mean_average_precision': 0.021923046229307207, 'validation/accuracy': 0.5082302093505859, 'validation/loss': 0.7368706464767456, 'validation/mean_average_precision': 0.0251904383097207, 'validation/num_examples': 43793, 'test/accuracy': 0.5050364136695862, 'test/loss': 0.7402201294898987, 'test/mean_average_precision': 0.027849254263940037, 'test/num_examples': 43793, 'score': 34.862422943115234, 'total_duration': 253.68651223182678, 'accumulated_submission_time': 34.862422943115234, 'accumulated_eval_time': 218.8239462375641, 'accumulated_logging_time': 0}
I0428 13:21:43.474371 140451847341824 logging_writer.py:48] [1] accumulated_eval_time=218.823946, accumulated_logging_time=0, accumulated_submission_time=34.862423, global_step=1, preemption_count=0, score=34.862423, test/accuracy=0.505036, test/loss=0.740220, test/mean_average_precision=0.027849, test/num_examples=43793, total_duration=253.686512, train/accuracy=0.512873, train/loss=0.728954, train/mean_average_precision=0.021923, validation/accuracy=0.508230, validation/loss=0.736871, validation/mean_average_precision=0.025190, validation/num_examples=43793
I0428 13:22:07.475070 140451855734528 logging_writer.py:48] [100] global_step=100, grad_norm=0.4558408558368683, loss=0.36983710527420044
I0428 13:22:31.441031 140451847341824 logging_writer.py:48] [200] global_step=200, grad_norm=0.19394636154174805, loss=0.17211884260177612
I0428 13:22:55.533536 140451855734528 logging_writer.py:48] [300] global_step=300, grad_norm=0.07862970232963562, loss=0.08315955847501755
I0428 13:23:19.674325 140451847341824 logging_writer.py:48] [400] global_step=400, grad_norm=0.03157905489206314, loss=0.06474368274211884
I0428 13:23:43.718575 140451855734528 logging_writer.py:48] [500] global_step=500, grad_norm=0.30690017342567444, loss=0.05463457107543945
I0428 13:24:08.124980 140451847341824 logging_writer.py:48] [600] global_step=600, grad_norm=0.16202840209007263, loss=0.053304523229599
I0428 13:24:32.162174 140451855734528 logging_writer.py:48] [700] global_step=700, grad_norm=0.034335698932409286, loss=0.054143019020557404
I0428 13:24:55.928698 140451847341824 logging_writer.py:48] [800] global_step=800, grad_norm=0.09771405905485153, loss=0.05090964585542679
I0428 13:25:19.717848 140451855734528 logging_writer.py:48] [900] global_step=900, grad_norm=0.07041109353303909, loss=0.05240243300795555
I0428 13:25:43.525853 140451847341824 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.10040707886219025, loss=0.04852548614144325
I0428 13:25:43.530807 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:26:56.911732 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:26:59.564497 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:27:02.205492 140637852153664 submission_runner.py:415] Time since start: 572.43s, 	Step: 1001, 	{'train/accuracy': 0.9870203733444214, 'train/loss': 0.049658287316560745, 'train/mean_average_precision': 0.06690400140118424, 'validation/accuracy': 0.9843428134918213, 'validation/loss': 0.05868791043758392, 'validation/mean_average_precision': 0.07023620351634975, 'validation/num_examples': 43793, 'test/accuracy': 0.9833598136901855, 'test/loss': 0.0620211660861969, 'test/mean_average_precision': 0.06946202703760003, 'test/num_examples': 43793, 'score': 274.89994978904724, 'total_duration': 572.4342448711395, 'accumulated_submission_time': 274.89994978904724, 'accumulated_eval_time': 297.49855756759644, 'accumulated_logging_time': 0.026923418045043945}
I0428 13:27:02.213181 140451855734528 logging_writer.py:48] [1001] accumulated_eval_time=297.498558, accumulated_logging_time=0.026923, accumulated_submission_time=274.899950, global_step=1001, preemption_count=0, score=274.899950, test/accuracy=0.983360, test/loss=0.062021, test/mean_average_precision=0.069462, test/num_examples=43793, total_duration=572.434245, train/accuracy=0.987020, train/loss=0.049658, train/mean_average_precision=0.066904, validation/accuracy=0.984343, validation/loss=0.058688, validation/mean_average_precision=0.070236, validation/num_examples=43793
I0428 13:27:26.135287 140451847341824 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.08908843249082565, loss=0.051213428378105164
I0428 13:27:50.365140 140451855734528 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.019590985029935837, loss=0.048999253660440445
I0428 13:28:14.794494 140451847341824 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.01919672079384327, loss=0.04996415600180626
I0428 13:28:38.894633 140451855734528 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.085708849132061, loss=0.04909226670861244
I0428 13:29:02.965884 140451847341824 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.07409784942865372, loss=0.052811358124017715
I0428 13:29:26.986085 140451855734528 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02199312299489975, loss=0.049576446413993835
I0428 13:29:50.988715 140451847341824 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.05067334696650505, loss=0.053147342056035995
I0428 13:30:14.842241 140451855734528 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.04111196845769882, loss=0.04661531746387482
I0428 13:30:39.018886 140451847341824 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03870805352926254, loss=0.04712677001953125
I0428 13:31:02.410939 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:32:15.548644 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:32:18.358886 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:32:21.110211 140637852153664 submission_runner.py:415] Time since start: 891.34s, 	Step: 1998, 	{'train/accuracy': 0.987403929233551, 'train/loss': 0.04659853130578995, 'train/mean_average_precision': 0.11329854595336131, 'validation/accuracy': 0.9846355319023132, 'validation/loss': 0.0556311309337616, 'validation/mean_average_precision': 0.1045496447155659, 'validation/num_examples': 43793, 'test/accuracy': 0.9836816191673279, 'test/loss': 0.058565083891153336, 'test/mean_average_precision': 0.1055237954307041, 'test/num_examples': 43793, 'score': 515.0787501335144, 'total_duration': 891.3389601707458, 'accumulated_submission_time': 515.0787501335144, 'accumulated_eval_time': 376.19777250289917, 'accumulated_logging_time': 0.04479193687438965}
I0428 13:32:21.118827 140451855734528 logging_writer.py:48] [1998] accumulated_eval_time=376.197773, accumulated_logging_time=0.044792, accumulated_submission_time=515.078750, global_step=1998, preemption_count=0, score=515.078750, test/accuracy=0.983682, test/loss=0.058565, test/mean_average_precision=0.105524, test/num_examples=43793, total_duration=891.338960, train/accuracy=0.987404, train/loss=0.046599, train/mean_average_precision=0.113299, validation/accuracy=0.984636, validation/loss=0.055631, validation/mean_average_precision=0.104550, validation/num_examples=43793
I0428 13:32:21.900351 140451847341824 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.013752926141023636, loss=0.044186707586050034
I0428 13:32:47.167750 140451855734528 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.021372511982917786, loss=0.04705652594566345
I0428 13:33:11.916010 140451847341824 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.012691154144704342, loss=0.043211232870817184
I0428 13:33:36.062421 140451855734528 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04270247370004654, loss=0.04845350980758667
I0428 13:34:00.186472 140451847341824 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.017576731741428375, loss=0.04774933308362961
I0428 13:34:23.998020 140451855734528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0525747686624527, loss=0.04719123989343643
I0428 13:34:47.745239 140451847341824 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.016421090811491013, loss=0.04901788756251335
I0428 13:35:11.619105 140451855734528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.015535914339125156, loss=0.041717544198036194
I0428 13:35:35.567984 140451847341824 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03172587230801582, loss=0.04604944959282875
I0428 13:35:59.505447 140451855734528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.013109181076288223, loss=0.04470382630825043
I0428 13:36:21.248451 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:37:35.569918 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:37:38.196453 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:37:40.767970 140637852153664 submission_runner.py:415] Time since start: 1211.00s, 	Step: 2992, 	{'train/accuracy': 0.9879209995269775, 'train/loss': 0.04315934702754021, 'train/mean_average_precision': 0.15797097484792444, 'validation/accuracy': 0.985058069229126, 'validation/loss': 0.05260965973138809, 'validation/mean_average_precision': 0.1489676471958308, 'validation/num_examples': 43793, 'test/accuracy': 0.9840863943099976, 'test/loss': 0.0555478110909462, 'test/mean_average_precision': 0.14671757157608953, 'test/num_examples': 43793, 'score': 755.1878604888916, 'total_duration': 1210.9967346191406, 'accumulated_submission_time': 755.1878604888916, 'accumulated_eval_time': 455.7172667980194, 'accumulated_logging_time': 0.06450676918029785}
I0428 13:37:40.776655 140451847341824 logging_writer.py:48] [2992] accumulated_eval_time=455.717267, accumulated_logging_time=0.064507, accumulated_submission_time=755.187860, global_step=2992, preemption_count=0, score=755.187860, test/accuracy=0.984086, test/loss=0.055548, test/mean_average_precision=0.146718, test/num_examples=43793, total_duration=1210.996735, train/accuracy=0.987921, train/loss=0.043159, train/mean_average_precision=0.157971, validation/accuracy=0.985058, validation/loss=0.052610, validation/mean_average_precision=0.148968, validation/num_examples=43793
I0428 13:37:42.967089 140451855734528 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.021316636353731155, loss=0.04902312159538269
I0428 13:38:07.299706 140451847341824 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03550971299409866, loss=0.04158354923129082
I0428 13:38:31.524276 140451855734528 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.010876607149839401, loss=0.041546352207660675
I0428 13:38:55.499779 140451847341824 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.01883874461054802, loss=0.042988941073417664
I0428 13:39:19.615356 140451855734528 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.032128408551216125, loss=0.04576941579580307
I0428 13:39:43.774631 140451847341824 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.024749863892793655, loss=0.04556852951645851
I0428 13:40:07.594182 140451855734528 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.01140147726982832, loss=0.041343748569488525
I0428 13:40:31.611054 140451847341824 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.014922664500772953, loss=0.044351499527692795
I0428 13:40:55.547054 140451855734528 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.049031976610422134, loss=0.04443936422467232
I0428 13:41:19.809484 140451847341824 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.016932887956500053, loss=0.04085003212094307
I0428 13:41:40.912138 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:42:54.474550 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:42:57.283771 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:42:59.823693 140637852153664 submission_runner.py:415] Time since start: 1530.05s, 	Step: 3989, 	{'train/accuracy': 0.9881632328033447, 'train/loss': 0.04126148670911789, 'train/mean_average_precision': 0.19147178856246191, 'validation/accuracy': 0.9852647185325623, 'validation/loss': 0.050847526639699936, 'validation/mean_average_precision': 0.16216491786926704, 'validation/num_examples': 43793, 'test/accuracy': 0.9843732118606567, 'test/loss': 0.05357905477285385, 'test/mean_average_precision': 0.16155806440263829, 'test/num_examples': 43793, 'score': 995.3040821552277, 'total_duration': 1530.0524611473083, 'accumulated_submission_time': 995.3040821552277, 'accumulated_eval_time': 534.6287858486176, 'accumulated_logging_time': 0.08353900909423828}
I0428 13:42:59.831428 140451855734528 logging_writer.py:48] [3989] accumulated_eval_time=534.628786, accumulated_logging_time=0.083539, accumulated_submission_time=995.304082, global_step=3989, preemption_count=0, score=995.304082, test/accuracy=0.984373, test/loss=0.053579, test/mean_average_precision=0.161558, test/num_examples=43793, total_duration=1530.052461, train/accuracy=0.988163, train/loss=0.041261, train/mean_average_precision=0.191472, validation/accuracy=0.985265, validation/loss=0.050848, validation/mean_average_precision=0.162165, validation/num_examples=43793
I0428 13:43:02.833734 140451847341824 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.013592253439128399, loss=0.04191774129867554
I0428 13:43:27.516704 140451855734528 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0159272700548172, loss=0.03990193456411362
I0428 13:43:51.604167 140451847341824 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.014064584858715534, loss=0.045503608882427216
I0428 13:44:15.785091 140451855734528 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.019730526953935623, loss=0.04567045345902443
I0428 13:44:40.534289 140451847341824 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.007208055816590786, loss=0.04003304988145828
I0428 13:45:04.966074 140451855734528 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.007504219654947519, loss=0.04489501938223839
I0428 13:45:28.956172 140451847341824 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.008885135874152184, loss=0.04085702449083328
I0428 13:45:53.055202 140451855734528 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.007810681127011776, loss=0.0393674261868
I0428 13:46:17.341483 140451847341824 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.015045798383653164, loss=0.04293755441904068
I0428 13:46:41.512614 140451855734528 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.010312018916010857, loss=0.04181468114256859
I0428 13:46:59.908330 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:48:13.200511 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:48:15.752510 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:48:18.260124 140637852153664 submission_runner.py:415] Time since start: 1848.49s, 	Step: 4975, 	{'train/accuracy': 0.9886889457702637, 'train/loss': 0.038701288402080536, 'train/mean_average_precision': 0.25695395523305664, 'validation/accuracy': 0.9856759309768677, 'validation/loss': 0.04873199760913849, 'validation/mean_average_precision': 0.18970108448937542, 'validation/num_examples': 43793, 'test/accuracy': 0.9848319292068481, 'test/loss': 0.051362745463848114, 'test/mean_average_precision': 0.18662152083071867, 'test/num_examples': 43793, 'score': 1235.3632338047028, 'total_duration': 1848.4888925552368, 'accumulated_submission_time': 1235.3632338047028, 'accumulated_eval_time': 612.9805636405945, 'accumulated_logging_time': 0.10007452964782715}
I0428 13:48:18.268311 140451847341824 logging_writer.py:48] [4975] accumulated_eval_time=612.980564, accumulated_logging_time=0.100075, accumulated_submission_time=1235.363234, global_step=4975, preemption_count=0, score=1235.363234, test/accuracy=0.984832, test/loss=0.051363, test/mean_average_precision=0.186622, test/num_examples=43793, total_duration=1848.488893, train/accuracy=0.988689, train/loss=0.038701, train/mean_average_precision=0.256954, validation/accuracy=0.985676, validation/loss=0.048732, validation/mean_average_precision=0.189701, validation/num_examples=43793
I0428 13:48:24.542946 140451855734528 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.015057684853672981, loss=0.039306771010160446
I0428 13:48:48.382547 140451847341824 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.017923830077052116, loss=0.04123091325163841
I0428 13:49:12.150722 140451855734528 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.011409959755837917, loss=0.03744543343782425
I0428 13:49:35.964260 140451847341824 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.00750052509829402, loss=0.04062986001372337
I0428 13:49:59.923593 140451855734528 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.013819998130202293, loss=0.03923006355762482
I0428 13:50:23.858613 140451847341824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014413626864552498, loss=0.04229620471596718
I0428 13:50:47.574268 140451855734528 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.019931115210056305, loss=0.04268461465835571
I0428 13:51:11.273632 140451847341824 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.018077349290251732, loss=0.03752640634775162
I0428 13:51:35.052129 140451855734528 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01025049015879631, loss=0.03939667344093323
I0428 13:51:58.868808 140451847341824 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0067915949039161205, loss=0.03891278803348541
I0428 13:52:18.386359 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:53:31.526868 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:53:34.103859 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:53:36.657904 140637852153664 submission_runner.py:415] Time since start: 2166.89s, 	Step: 5982, 	{'train/accuracy': 0.9889551997184753, 'train/loss': 0.03711317852139473, 'train/mean_average_precision': 0.27546877668749326, 'validation/accuracy': 0.9860591292381287, 'validation/loss': 0.04730075225234032, 'validation/mean_average_precision': 0.2150307805962135, 'validation/num_examples': 43793, 'test/accuracy': 0.9850972294807434, 'test/loss': 0.05027942731976509, 'test/mean_average_precision': 0.21428511007142606, 'test/num_examples': 43793, 'score': 1475.4621942043304, 'total_duration': 2166.8866605758667, 'accumulated_submission_time': 1475.4621942043304, 'accumulated_eval_time': 691.2520570755005, 'accumulated_logging_time': 0.1185903549194336}
I0428 13:53:36.666416 140451855734528 logging_writer.py:48] [5982] accumulated_eval_time=691.252057, accumulated_logging_time=0.118590, accumulated_submission_time=1475.462194, global_step=5982, preemption_count=0, score=1475.462194, test/accuracy=0.985097, test/loss=0.050279, test/mean_average_precision=0.214285, test/num_examples=43793, total_duration=2166.886661, train/accuracy=0.988955, train/loss=0.037113, train/mean_average_precision=0.275469, validation/accuracy=0.986059, validation/loss=0.047301, validation/mean_average_precision=0.215031, validation/num_examples=43793
I0428 13:53:41.358045 140451847341824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.016366232186555862, loss=0.03804862126708031
I0428 13:54:05.965364 140451855734528 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.007928373292088509, loss=0.03839477151632309
I0428 13:54:30.225598 140451847341824 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.007663504220545292, loss=0.04193256050348282
I0428 13:54:54.284075 140451855734528 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.008988036774098873, loss=0.039092905819416046
I0428 13:55:18.362668 140451847341824 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.010997469536960125, loss=0.03886190056800842
I0428 13:55:42.308209 140451855734528 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.012792271561920643, loss=0.03793637454509735
I0428 13:56:06.541312 140451847341824 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.009071036241948605, loss=0.03835473582148552
I0428 13:56:30.396402 140451855734528 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.007941829040646553, loss=0.041586726903915405
I0428 13:56:54.434535 140451847341824 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.011693154461681843, loss=0.042453013360500336
I0428 13:57:18.601900 140451855734528 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.006053468212485313, loss=0.0367414727807045
I0428 13:57:36.713923 140637852153664 spec.py:298] Evaluating on the training split.
I0428 13:58:50.275470 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 13:58:53.137576 140637852153664 spec.py:326] Evaluating on the test split.
I0428 13:58:55.658261 140637852153664 submission_runner.py:415] Time since start: 2485.89s, 	Step: 6976, 	{'train/accuracy': 0.9895934462547302, 'train/loss': 0.03561761975288391, 'train/mean_average_precision': 0.3010449507579762, 'validation/accuracy': 0.9862710237503052, 'validation/loss': 0.046060603111982346, 'validation/mean_average_precision': 0.22534132984978453, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.048419851809740067, 'test/mean_average_precision': 0.23024986326554608, 'test/num_examples': 43793, 'score': 1715.491408586502, 'total_duration': 2485.887009859085, 'accumulated_submission_time': 1715.491408586502, 'accumulated_eval_time': 770.1963646411896, 'accumulated_logging_time': 0.13646435737609863}
I0428 13:58:55.666366 140451847341824 logging_writer.py:48] [6976] accumulated_eval_time=770.196365, accumulated_logging_time=0.136464, accumulated_submission_time=1715.491409, global_step=6976, preemption_count=0, score=1715.491409, test/accuracy=0.985449, test/loss=0.048420, test/mean_average_precision=0.230250, test/num_examples=43793, total_duration=2485.887010, train/accuracy=0.989593, train/loss=0.035618, train/mean_average_precision=0.301045, validation/accuracy=0.986271, validation/loss=0.046061, validation/mean_average_precision=0.225341, validation/num_examples=43793
I0428 13:59:01.779953 140451855734528 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.008522551506757736, loss=0.039810504764318466
I0428 13:59:25.588764 140451847341824 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.016661930829286575, loss=0.04375820606946945
I0428 13:59:49.530386 140451855734528 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.012984330765902996, loss=0.04011541232466698
I0428 14:00:13.577293 140451847341824 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.01071307621896267, loss=0.038466762751340866
I0428 14:00:37.313216 140451855734528 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.007505567744374275, loss=0.0380791500210762
I0428 14:01:01.127809 140451847341824 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.010012160986661911, loss=0.03949614614248276
I0428 14:01:24.980882 140451855734528 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.006886298768222332, loss=0.03652895614504814
I0428 14:01:48.963670 140451847341824 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.005461063701659441, loss=0.036534104496240616
I0428 14:02:13.166883 140451855734528 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.007022392936050892, loss=0.037329815328121185
I0428 14:02:37.143688 140451847341824 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.006313070189207792, loss=0.034101203083992004
I0428 14:02:55.756736 140637852153664 spec.py:298] Evaluating on the training split.
I0428 14:04:09.279439 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 14:04:11.851507 140637852153664 spec.py:326] Evaluating on the test split.
I0428 14:04:14.349529 140637852153664 submission_runner.py:415] Time since start: 2804.58s, 	Step: 7978, 	{'train/accuracy': 0.989698588848114, 'train/loss': 0.03438010811805725, 'train/mean_average_precision': 0.342283721803115, 'validation/accuracy': 0.9862840175628662, 'validation/loss': 0.04676683992147446, 'validation/mean_average_precision': 0.23892572024737857, 'validation/num_examples': 43793, 'test/accuracy': 0.9853529334068298, 'test/loss': 0.049651775509119034, 'test/mean_average_precision': 0.2386372214268253, 'test/num_examples': 43793, 'score': 1955.5628354549408, 'total_duration': 2804.5782976150513, 'accumulated_submission_time': 1955.5628354549408, 'accumulated_eval_time': 848.7891206741333, 'accumulated_logging_time': 0.15465521812438965}
I0428 14:04:14.359535 140451855734528 logging_writer.py:48] [7978] accumulated_eval_time=848.789121, accumulated_logging_time=0.154655, accumulated_submission_time=1955.562835, global_step=7978, preemption_count=0, score=1955.562835, test/accuracy=0.985353, test/loss=0.049652, test/mean_average_precision=0.238637, test/num_examples=43793, total_duration=2804.578298, train/accuracy=0.989699, train/loss=0.034380, train/mean_average_precision=0.342284, validation/accuracy=0.986284, validation/loss=0.046767, validation/mean_average_precision=0.238926, validation/num_examples=43793
I0428 14:04:19.847619 140451847341824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.007681410759687424, loss=0.038660719990730286
I0428 14:04:43.574476 140451855734528 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.011228770948946476, loss=0.03736944496631622
I0428 14:05:07.418401 140451847341824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.009358648210763931, loss=0.04088756814599037
I0428 14:05:31.650346 140451855734528 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.00876272190362215, loss=0.04079263284802437
I0428 14:05:55.640805 140451847341824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.006574421655386686, loss=0.03739757090806961
I0428 14:06:19.786872 140451855734528 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.012630653567612171, loss=0.03841311112046242
I0428 14:06:43.818325 140451847341824 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.00627078115940094, loss=0.03643398731946945
I0428 14:07:08.304795 140451855734528 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.006072986871004105, loss=0.038601115345954895
I0428 14:07:32.517871 140451847341824 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.00701836496591568, loss=0.03626320883631706
I0428 14:07:56.403613 140451855734528 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01883559674024582, loss=0.03922824189066887
I0428 14:08:14.508026 140637852153664 spec.py:298] Evaluating on the training split.
I0428 14:09:28.886286 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 14:09:31.456377 140637852153664 spec.py:326] Evaluating on the test split.
I0428 14:09:33.971664 140637852153664 submission_runner.py:415] Time since start: 3124.20s, 	Step: 8974, 	{'train/accuracy': 0.990311324596405, 'train/loss': 0.03222550451755524, 'train/mean_average_precision': 0.3907487971455917, 'validation/accuracy': 0.9865559935569763, 'validation/loss': 0.04532105103135109, 'validation/mean_average_precision': 0.2468578965817463, 'validation/num_examples': 43793, 'test/accuracy': 0.985596776008606, 'test/loss': 0.048191677778959274, 'test/mean_average_precision': 0.23907326454711134, 'test/num_examples': 43793, 'score': 2195.6922566890717, 'total_duration': 3124.2004160881042, 'accumulated_submission_time': 2195.6922566890717, 'accumulated_eval_time': 928.2527050971985, 'accumulated_logging_time': 0.17480015754699707}
I0428 14:09:33.981391 140451847341824 logging_writer.py:48] [8974] accumulated_eval_time=928.252705, accumulated_logging_time=0.174800, accumulated_submission_time=2195.692257, global_step=8974, preemption_count=0, score=2195.692257, test/accuracy=0.985597, test/loss=0.048192, test/mean_average_precision=0.239073, test/num_examples=43793, total_duration=3124.200416, train/accuracy=0.990311, train/loss=0.032226, train/mean_average_precision=0.390749, validation/accuracy=0.986556, validation/loss=0.045321, validation/mean_average_precision=0.246858, validation/num_examples=43793
I0428 14:09:40.749994 140451855734528 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.005902925506234169, loss=0.035051848739385605
I0428 14:10:05.470978 140451847341824 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.00649692676961422, loss=0.03803703188896179
I0428 14:10:29.698758 140451855734528 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.010492322035133839, loss=0.03374100476503372
I0428 14:10:54.010698 140451847341824 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0064999498426914215, loss=0.031944360584020615
I0428 14:11:18.302664 140451855734528 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.00527693796902895, loss=0.034849151968955994
I0428 14:11:42.370611 140451847341824 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.006279270630329847, loss=0.03698388487100601
I0428 14:12:06.735564 140451855734528 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.006374952849000692, loss=0.035078100860118866
I0428 14:12:30.943204 140451847341824 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.006949997507035732, loss=0.035531748086214066
I0428 14:12:55.218349 140451855734528 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.006545174866914749, loss=0.03736283630132675
I0428 14:13:19.648026 140451847341824 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.008195308037102222, loss=0.03746594488620758
I0428 14:13:34.068784 140637852153664 spec.py:298] Evaluating on the training split.
I0428 14:14:47.178671 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 14:14:49.797672 140637852153664 spec.py:326] Evaluating on the test split.
I0428 14:14:52.304268 140637852153664 submission_runner.py:415] Time since start: 3442.53s, 	Step: 9961, 	{'train/accuracy': 0.9904114007949829, 'train/loss': 0.0320143923163414, 'train/mean_average_precision': 0.40144502671564375, 'validation/accuracy': 0.9864062070846558, 'validation/loss': 0.04525984823703766, 'validation/mean_average_precision': 0.2510420509236008, 'validation/num_examples': 43793, 'test/accuracy': 0.9855470657348633, 'test/loss': 0.04784756153821945, 'test/mean_average_precision': 0.24847602429027763, 'test/num_examples': 43793, 'score': 2435.7603397369385, 'total_duration': 3442.533034324646, 'accumulated_submission_time': 2435.7603397369385, 'accumulated_eval_time': 1006.4881618022919, 'accumulated_logging_time': 0.19470000267028809}
I0428 14:14:52.313950 140451855734528 logging_writer.py:48] [9961] accumulated_eval_time=1006.488162, accumulated_logging_time=0.194700, accumulated_submission_time=2435.760340, global_step=9961, preemption_count=0, score=2435.760340, test/accuracy=0.985547, test/loss=0.047848, test/mean_average_precision=0.248476, test/num_examples=43793, total_duration=3442.533034, train/accuracy=0.990411, train/loss=0.032014, train/mean_average_precision=0.401445, validation/accuracy=0.986406, validation/loss=0.045260, validation/mean_average_precision=0.251042, validation/num_examples=43793
I0428 14:15:01.844839 140451847341824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.006033041048794985, loss=0.03494882583618164
I0428 14:15:25.518454 140451855734528 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0054497444070875645, loss=0.034835316240787506
I0428 14:15:49.196264 140451847341824 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0062268623150885105, loss=0.03626261651515961
I0428 14:16:12.841267 140451855734528 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.007944252341985703, loss=0.035124942660331726
I0428 14:16:36.469803 140451847341824 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.012609708122909069, loss=0.034158144146203995
I0428 14:17:00.427639 140451855734528 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.00636949623003602, loss=0.03511011600494385
I0428 14:17:24.499400 140451847341824 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.009118909947574139, loss=0.03642767667770386
I0428 14:17:48.298837 140451855734528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.00659222761169076, loss=0.03430085629224777
I0428 14:18:12.318493 140451847341824 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0068316152319312096, loss=0.03168952837586403
I0428 14:18:35.996636 140451855734528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.007095326203852892, loss=0.03673377260565758
I0428 14:18:52.543475 140637852153664 spec.py:298] Evaluating on the training split.
I0428 14:20:03.792900 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 14:20:06.377761 140637852153664 spec.py:326] Evaluating on the test split.
I0428 14:20:08.900454 140637852153664 submission_runner.py:415] Time since start: 3759.13s, 	Step: 10971, 	{'train/accuracy': 0.9907059073448181, 'train/loss': 0.030989263206720352, 'train/mean_average_precision': 0.4051596213318397, 'validation/accuracy': 0.9868101477622986, 'validation/loss': 0.04449894651770592, 'validation/mean_average_precision': 0.2581433995160812, 'validation/num_examples': 43793, 'test/accuracy': 0.985948920249939, 'test/loss': 0.04733545705676079, 'test/mean_average_precision': 0.2521985744702562, 'test/num_examples': 43793, 'score': 2675.9718136787415, 'total_duration': 3759.129203557968, 'accumulated_submission_time': 2675.9718136787415, 'accumulated_eval_time': 1082.845083475113, 'accumulated_logging_time': 0.21331262588500977}
I0428 14:20:08.909309 140451847341824 logging_writer.py:48] [10971] accumulated_eval_time=1082.845083, accumulated_logging_time=0.213313, accumulated_submission_time=2675.971814, global_step=10971, preemption_count=0, score=2675.971814, test/accuracy=0.985949, test/loss=0.047335, test/mean_average_precision=0.252199, test/num_examples=43793, total_duration=3759.129204, train/accuracy=0.990706, train/loss=0.030989, train/mean_average_precision=0.405160, validation/accuracy=0.986810, validation/loss=0.044499, validation/mean_average_precision=0.258143, validation/num_examples=43793
I0428 14:20:16.227833 140451855734528 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.006484327372163534, loss=0.0325622521340847
I0428 14:20:39.972079 140451847341824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.006841455120593309, loss=0.03341221809387207
I0428 14:21:04.158719 140451855734528 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.007645494770258665, loss=0.036575280129909515
I0428 14:21:27.733599 140451847341824 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.007685165386646986, loss=0.036369215697050095
I0428 14:21:51.583021 140451855734528 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.005659305490553379, loss=0.031325243413448334
I0428 14:22:15.612831 140451847341824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.008732814341783524, loss=0.036198489367961884
I0428 14:22:39.249184 140451855734528 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.007484236266463995, loss=0.03400378301739693
I0428 14:23:02.843247 140451847341824 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.009066562168300152, loss=0.031580690294504166
I0428 14:23:26.813552 140451855734528 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.006109451409429312, loss=0.03337788209319115
I0428 14:23:50.768179 140451847341824 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.007065545301884413, loss=0.036690603941679
I0428 14:24:09.052568 140637852153664 spec.py:298] Evaluating on the training split.
I0428 14:25:20.946062 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 14:25:23.542338 140637852153664 spec.py:326] Evaluating on the test split.
I0428 14:25:26.064999 140637852153664 submission_runner.py:415] Time since start: 4076.29s, 	Step: 11977, 	{'train/accuracy': 0.9910681843757629, 'train/loss': 0.029325047507882118, 'train/mean_average_precision': 0.4494240278518373, 'validation/accuracy': 0.9867959022521973, 'validation/loss': 0.044614579528570175, 'validation/mean_average_precision': 0.2648132078834961, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.047465045005083084, 'test/mean_average_precision': 0.25087148177990876, 'test/num_examples': 43793, 'score': 2916.0970923900604, 'total_duration': 4076.293705224991, 'accumulated_submission_time': 2916.0970923900604, 'accumulated_eval_time': 1159.8574211597443, 'accumulated_logging_time': 0.2310950756072998}
I0428 14:25:26.073778 140451855734528 logging_writer.py:48] [11977] accumulated_eval_time=1159.857421, accumulated_logging_time=0.231095, accumulated_submission_time=2916.097092, global_step=11977, preemption_count=0, score=2916.097092, test/accuracy=0.985973, test/loss=0.047465, test/mean_average_precision=0.250871, test/num_examples=43793, total_duration=4076.293705, train/accuracy=0.991068, train/loss=0.029325, train/mean_average_precision=0.449424, validation/accuracy=0.986796, validation/loss=0.044615, validation/mean_average_precision=0.264813, validation/num_examples=43793
I0428 14:25:31.496119 140637852153664 spec.py:298] Evaluating on the training split.
I0428 14:26:42.920067 140637852153664 spec.py:310] Evaluating on the validation split.
I0428 14:26:45.498051 140637852153664 spec.py:326] Evaluating on the test split.
I0428 14:26:48.018714 140637852153664 submission_runner.py:415] Time since start: 4158.25s, 	Step: 12000, 	{'train/accuracy': 0.9909038543701172, 'train/loss': 0.03015371970832348, 'train/mean_average_precision': 0.42570815118190025, 'validation/accuracy': 0.9867569804191589, 'validation/loss': 0.04485534504055977, 'validation/mean_average_precision': 0.25802231042023277, 'validation/num_examples': 43793, 'test/accuracy': 0.9858659505844116, 'test/loss': 0.04770790413022041, 'test/mean_average_precision': 0.24942094509498097, 'test/num_examples': 43793, 'score': 2921.5102438926697, 'total_duration': 4158.247465372086, 'accumulated_submission_time': 2921.5102438926697, 'accumulated_eval_time': 1236.3799622058868, 'accumulated_logging_time': 0.2485947608947754}
I0428 14:26:48.027230 140451847341824 logging_writer.py:48] [12000] accumulated_eval_time=1236.379962, accumulated_logging_time=0.248595, accumulated_submission_time=2921.510244, global_step=12000, preemption_count=0, score=2921.510244, test/accuracy=0.985866, test/loss=0.047708, test/mean_average_precision=0.249421, test/num_examples=43793, total_duration=4158.247465, train/accuracy=0.990904, train/loss=0.030154, train/mean_average_precision=0.425708, validation/accuracy=0.986757, validation/loss=0.044855, validation/mean_average_precision=0.258022, validation/num_examples=43793
I0428 14:26:48.043946 140451855734528 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2921.510244
I0428 14:26:48.069168 140637852153664 checkpoints.py:356] Saving checkpoint at step: 12000
I0428 14:26:48.112405 140637852153664 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1/checkpoint_12000
I0428 14:26:48.112576 140637852153664 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_adafactor/ogbg_jax/trial_1/checkpoint_12000.
I0428 14:26:48.267949 140637852153664 submission_runner.py:578] Tuning trial 1/1
I0428 14:26:48.268180 140637852153664 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0428 14:26:48.269921 140637852153664 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5128725171089172, 'train/loss': 0.7289538979530334, 'train/mean_average_precision': 0.021923046229307207, 'validation/accuracy': 0.5082302093505859, 'validation/loss': 0.7368706464767456, 'validation/mean_average_precision': 0.0251904383097207, 'validation/num_examples': 43793, 'test/accuracy': 0.5050364136695862, 'test/loss': 0.7402201294898987, 'test/mean_average_precision': 0.027849254263940037, 'test/num_examples': 43793, 'score': 34.862422943115234, 'total_duration': 253.68651223182678, 'accumulated_submission_time': 34.862422943115234, 'accumulated_eval_time': 218.8239462375641, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1001, {'train/accuracy': 0.9870203733444214, 'train/loss': 0.049658287316560745, 'train/mean_average_precision': 0.06690400140118424, 'validation/accuracy': 0.9843428134918213, 'validation/loss': 0.05868791043758392, 'validation/mean_average_precision': 0.07023620351634975, 'validation/num_examples': 43793, 'test/accuracy': 0.9833598136901855, 'test/loss': 0.0620211660861969, 'test/mean_average_precision': 0.06946202703760003, 'test/num_examples': 43793, 'score': 274.89994978904724, 'total_duration': 572.4342448711395, 'accumulated_submission_time': 274.89994978904724, 'accumulated_eval_time': 297.49855756759644, 'accumulated_logging_time': 0.026923418045043945, 'global_step': 1001, 'preemption_count': 0}), (1998, {'train/accuracy': 0.987403929233551, 'train/loss': 0.04659853130578995, 'train/mean_average_precision': 0.11329854595336131, 'validation/accuracy': 0.9846355319023132, 'validation/loss': 0.0556311309337616, 'validation/mean_average_precision': 0.1045496447155659, 'validation/num_examples': 43793, 'test/accuracy': 0.9836816191673279, 'test/loss': 0.058565083891153336, 'test/mean_average_precision': 0.1055237954307041, 'test/num_examples': 43793, 'score': 515.0787501335144, 'total_duration': 891.3389601707458, 'accumulated_submission_time': 515.0787501335144, 'accumulated_eval_time': 376.19777250289917, 'accumulated_logging_time': 0.04479193687438965, 'global_step': 1998, 'preemption_count': 0}), (2992, {'train/accuracy': 0.9879209995269775, 'train/loss': 0.04315934702754021, 'train/mean_average_precision': 0.15797097484792444, 'validation/accuracy': 0.985058069229126, 'validation/loss': 0.05260965973138809, 'validation/mean_average_precision': 0.1489676471958308, 'validation/num_examples': 43793, 'test/accuracy': 0.9840863943099976, 'test/loss': 0.0555478110909462, 'test/mean_average_precision': 0.14671757157608953, 'test/num_examples': 43793, 'score': 755.1878604888916, 'total_duration': 1210.9967346191406, 'accumulated_submission_time': 755.1878604888916, 'accumulated_eval_time': 455.7172667980194, 'accumulated_logging_time': 0.06450676918029785, 'global_step': 2992, 'preemption_count': 0}), (3989, {'train/accuracy': 0.9881632328033447, 'train/loss': 0.04126148670911789, 'train/mean_average_precision': 0.19147178856246191, 'validation/accuracy': 0.9852647185325623, 'validation/loss': 0.050847526639699936, 'validation/mean_average_precision': 0.16216491786926704, 'validation/num_examples': 43793, 'test/accuracy': 0.9843732118606567, 'test/loss': 0.05357905477285385, 'test/mean_average_precision': 0.16155806440263829, 'test/num_examples': 43793, 'score': 995.3040821552277, 'total_duration': 1530.0524611473083, 'accumulated_submission_time': 995.3040821552277, 'accumulated_eval_time': 534.6287858486176, 'accumulated_logging_time': 0.08353900909423828, 'global_step': 3989, 'preemption_count': 0}), (4975, {'train/accuracy': 0.9886889457702637, 'train/loss': 0.038701288402080536, 'train/mean_average_precision': 0.25695395523305664, 'validation/accuracy': 0.9856759309768677, 'validation/loss': 0.04873199760913849, 'validation/mean_average_precision': 0.18970108448937542, 'validation/num_examples': 43793, 'test/accuracy': 0.9848319292068481, 'test/loss': 0.051362745463848114, 'test/mean_average_precision': 0.18662152083071867, 'test/num_examples': 43793, 'score': 1235.3632338047028, 'total_duration': 1848.4888925552368, 'accumulated_submission_time': 1235.3632338047028, 'accumulated_eval_time': 612.9805636405945, 'accumulated_logging_time': 0.10007452964782715, 'global_step': 4975, 'preemption_count': 0}), (5982, {'train/accuracy': 0.9889551997184753, 'train/loss': 0.03711317852139473, 'train/mean_average_precision': 0.27546877668749326, 'validation/accuracy': 0.9860591292381287, 'validation/loss': 0.04730075225234032, 'validation/mean_average_precision': 0.2150307805962135, 'validation/num_examples': 43793, 'test/accuracy': 0.9850972294807434, 'test/loss': 0.05027942731976509, 'test/mean_average_precision': 0.21428511007142606, 'test/num_examples': 43793, 'score': 1475.4621942043304, 'total_duration': 2166.8866605758667, 'accumulated_submission_time': 1475.4621942043304, 'accumulated_eval_time': 691.2520570755005, 'accumulated_logging_time': 0.1185903549194336, 'global_step': 5982, 'preemption_count': 0}), (6976, {'train/accuracy': 0.9895934462547302, 'train/loss': 0.03561761975288391, 'train/mean_average_precision': 0.3010449507579762, 'validation/accuracy': 0.9862710237503052, 'validation/loss': 0.046060603111982346, 'validation/mean_average_precision': 0.22534132984978453, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.048419851809740067, 'test/mean_average_precision': 0.23024986326554608, 'test/num_examples': 43793, 'score': 1715.491408586502, 'total_duration': 2485.887009859085, 'accumulated_submission_time': 1715.491408586502, 'accumulated_eval_time': 770.1963646411896, 'accumulated_logging_time': 0.13646435737609863, 'global_step': 6976, 'preemption_count': 0}), (7978, {'train/accuracy': 0.989698588848114, 'train/loss': 0.03438010811805725, 'train/mean_average_precision': 0.342283721803115, 'validation/accuracy': 0.9862840175628662, 'validation/loss': 0.04676683992147446, 'validation/mean_average_precision': 0.23892572024737857, 'validation/num_examples': 43793, 'test/accuracy': 0.9853529334068298, 'test/loss': 0.049651775509119034, 'test/mean_average_precision': 0.2386372214268253, 'test/num_examples': 43793, 'score': 1955.5628354549408, 'total_duration': 2804.5782976150513, 'accumulated_submission_time': 1955.5628354549408, 'accumulated_eval_time': 848.7891206741333, 'accumulated_logging_time': 0.15465521812438965, 'global_step': 7978, 'preemption_count': 0}), (8974, {'train/accuracy': 0.990311324596405, 'train/loss': 0.03222550451755524, 'train/mean_average_precision': 0.3907487971455917, 'validation/accuracy': 0.9865559935569763, 'validation/loss': 0.04532105103135109, 'validation/mean_average_precision': 0.2468578965817463, 'validation/num_examples': 43793, 'test/accuracy': 0.985596776008606, 'test/loss': 0.048191677778959274, 'test/mean_average_precision': 0.23907326454711134, 'test/num_examples': 43793, 'score': 2195.6922566890717, 'total_duration': 3124.2004160881042, 'accumulated_submission_time': 2195.6922566890717, 'accumulated_eval_time': 928.2527050971985, 'accumulated_logging_time': 0.17480015754699707, 'global_step': 8974, 'preemption_count': 0}), (9961, {'train/accuracy': 0.9904114007949829, 'train/loss': 0.0320143923163414, 'train/mean_average_precision': 0.40144502671564375, 'validation/accuracy': 0.9864062070846558, 'validation/loss': 0.04525984823703766, 'validation/mean_average_precision': 0.2510420509236008, 'validation/num_examples': 43793, 'test/accuracy': 0.9855470657348633, 'test/loss': 0.04784756153821945, 'test/mean_average_precision': 0.24847602429027763, 'test/num_examples': 43793, 'score': 2435.7603397369385, 'total_duration': 3442.533034324646, 'accumulated_submission_time': 2435.7603397369385, 'accumulated_eval_time': 1006.4881618022919, 'accumulated_logging_time': 0.19470000267028809, 'global_step': 9961, 'preemption_count': 0}), (10971, {'train/accuracy': 0.9907059073448181, 'train/loss': 0.030989263206720352, 'train/mean_average_precision': 0.4051596213318397, 'validation/accuracy': 0.9868101477622986, 'validation/loss': 0.04449894651770592, 'validation/mean_average_precision': 0.2581433995160812, 'validation/num_examples': 43793, 'test/accuracy': 0.985948920249939, 'test/loss': 0.04733545705676079, 'test/mean_average_precision': 0.2521985744702562, 'test/num_examples': 43793, 'score': 2675.9718136787415, 'total_duration': 3759.129203557968, 'accumulated_submission_time': 2675.9718136787415, 'accumulated_eval_time': 1082.845083475113, 'accumulated_logging_time': 0.21331262588500977, 'global_step': 10971, 'preemption_count': 0}), (11977, {'train/accuracy': 0.9910681843757629, 'train/loss': 0.029325047507882118, 'train/mean_average_precision': 0.4494240278518373, 'validation/accuracy': 0.9867959022521973, 'validation/loss': 0.044614579528570175, 'validation/mean_average_precision': 0.2648132078834961, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.047465045005083084, 'test/mean_average_precision': 0.25087148177990876, 'test/num_examples': 43793, 'score': 2916.0970923900604, 'total_duration': 4076.293705224991, 'accumulated_submission_time': 2916.0970923900604, 'accumulated_eval_time': 1159.8574211597443, 'accumulated_logging_time': 0.2310950756072998, 'global_step': 11977, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9909038543701172, 'train/loss': 0.03015371970832348, 'train/mean_average_precision': 0.42570815118190025, 'validation/accuracy': 0.9867569804191589, 'validation/loss': 0.04485534504055977, 'validation/mean_average_precision': 0.25802231042023277, 'validation/num_examples': 43793, 'test/accuracy': 0.9858659505844116, 'test/loss': 0.04770790413022041, 'test/mean_average_precision': 0.24942094509498097, 'test/num_examples': 43793, 'score': 2921.5102438926697, 'total_duration': 4158.247465372086, 'accumulated_submission_time': 2921.5102438926697, 'accumulated_eval_time': 1236.3799622058868, 'accumulated_logging_time': 0.2485947608947754, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0428 14:26:48.270040 140637852153664 submission_runner.py:581] Timing: 2921.5102438926697
I0428 14:26:48.270082 140637852153664 submission_runner.py:582] ====================
I0428 14:26:48.270187 140637852153664 submission_runner.py:645] Final ogbg score: 2921.5102438926697
