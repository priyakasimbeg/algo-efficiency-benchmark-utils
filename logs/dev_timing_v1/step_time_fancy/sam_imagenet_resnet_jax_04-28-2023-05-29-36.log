python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_04-28-2023-05-29-36.log
I0428 05:29:59.718415 140533484173120 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax because --overwrite was set.
I0428 05:29:59.720592 140533484173120 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax.
I0428 05:29:59.788582 140533484173120 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 05:30:00.662147 140533484173120 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0428 05:30:00.662835 140533484173120 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 05:30:00.668785 140533484173120 submission_runner.py:538] Using RNG seed 4174303460
I0428 05:30:04.010276 140533484173120 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 05:30:04.010473 140533484173120 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1.
I0428 05:30:04.010648 140533484173120 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1/hparams.json.
I0428 05:30:04.145289 140533484173120 submission_runner.py:241] Initializing dataset.
I0428 05:30:04.161960 140533484173120 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:30:04.170982 140533484173120 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 05:30:04.171104 140533484173120 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 05:30:04.470446 140533484173120 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:30:05.802253 140533484173120 submission_runner.py:248] Initializing model.
I0428 05:30:18.120229 140533484173120 submission_runner.py:258] Initializing optimizer.
I0428 05:30:19.323975 140533484173120 submission_runner.py:265] Initializing metrics bundle.
I0428 05:30:19.324169 140533484173120 submission_runner.py:282] Initializing checkpoint and logger.
I0428 05:30:19.325193 140533484173120 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0428 05:30:20.165223 140533484173120 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0428 05:30:20.166176 140533484173120 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1/flags_0.json.
I0428 05:30:20.171235 140533484173120 submission_runner.py:318] Starting training loop.
I0428 05:31:22.155308 140357064972032 logging_writer.py:48] [0] global_step=0, grad_norm=0.6089588403701782, loss=6.931430816650391
I0428 05:31:22.173502 140533484173120 spec.py:298] Evaluating on the training split.
I0428 05:31:22.720279 140533484173120 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:31:22.728414 140533484173120 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 05:31:22.728541 140533484173120 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 05:31:22.803369 140533484173120 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:31:34.856627 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 05:31:35.752882 140533484173120 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:31:35.777927 140533484173120 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 05:31:35.778252 140533484173120 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 05:31:35.847577 140533484173120 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 05:31:53.562984 140533484173120 spec.py:326] Evaluating on the test split.
I0428 05:31:54.091879 140533484173120 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 05:31:54.098287 140533484173120 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 05:31:54.136107 140533484173120 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 05:32:03.728877 140533484173120 submission_runner.py:415] Time since start: 103.56s, 	Step: 1, 	{'train/accuracy': 0.0011160713620483875, 'train/loss': 6.911971569061279, 'validation/accuracy': 0.0011999999405816197, 'validation/loss': 6.912228107452393, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.912980079650879, 'test/num_examples': 10000, 'score': 62.00210475921631, 'total_duration': 103.55756402015686, 'accumulated_submission_time': 62.00210475921631, 'accumulated_eval_time': 41.5553138256073, 'accumulated_logging_time': 0}
I0428 05:32:03.745954 140325901297408 logging_writer.py:48] [1] accumulated_eval_time=41.555314, accumulated_logging_time=0, accumulated_submission_time=62.002105, global_step=1, preemption_count=0, score=62.002105, test/accuracy=0.001300, test/loss=6.912980, test/num_examples=10000, total_duration=103.557564, train/accuracy=0.001116, train/loss=6.911972, validation/accuracy=0.001200, validation/loss=6.912228, validation/num_examples=50000
I0428 05:33:07.580856 140325909690112 logging_writer.py:48] [100] global_step=100, grad_norm=0.5823975801467896, loss=6.899519443511963
I0428 05:34:11.500410 140325901297408 logging_writer.py:48] [200] global_step=200, grad_norm=0.6105383634567261, loss=6.830194473266602
I0428 05:35:15.511337 140325909690112 logging_writer.py:48] [300] global_step=300, grad_norm=0.687414824962616, loss=6.734091758728027
I0428 05:36:19.276487 140325901297408 logging_writer.py:48] [400] global_step=400, grad_norm=0.7192417979240417, loss=6.676957130432129
I0428 05:37:22.881285 140325909690112 logging_writer.py:48] [500] global_step=500, grad_norm=0.7120789885520935, loss=6.667953014373779
I0428 05:38:26.443026 140325901297408 logging_writer.py:48] [600] global_step=600, grad_norm=0.7284327745437622, loss=6.621849060058594
I0428 05:39:29.951613 140325909690112 logging_writer.py:48] [700] global_step=700, grad_norm=0.6749939322471619, loss=6.601197719573975
I0428 05:40:33.329357 140325901297408 logging_writer.py:48] [800] global_step=800, grad_norm=0.6589576601982117, loss=6.604542255401611
I0428 05:40:34.247813 140533484173120 spec.py:298] Evaluating on the training split.
I0428 05:40:41.283610 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 05:40:48.923081 140533484173120 spec.py:326] Evaluating on the test split.
I0428 05:40:51.312862 140533484173120 submission_runner.py:415] Time since start: 631.14s, 	Step: 803, 	{'train/accuracy': 0.016262754797935486, 'train/loss': 6.4442949295043945, 'validation/accuracy': 0.016699999570846558, 'validation/loss': 6.461048126220703, 'validation/num_examples': 50000, 'test/accuracy': 0.012800000607967377, 'test/loss': 6.544966697692871, 'test/num_examples': 10000, 'score': 572.4845790863037, 'total_duration': 631.1415548324585, 'accumulated_submission_time': 572.4845790863037, 'accumulated_eval_time': 58.62032628059387, 'accumulated_logging_time': 0.025949716567993164}
I0428 05:40:51.322262 140326865966848 logging_writer.py:48] [803] accumulated_eval_time=58.620326, accumulated_logging_time=0.025950, accumulated_submission_time=572.484579, global_step=803, preemption_count=0, score=572.484579, test/accuracy=0.012800, test/loss=6.544967, test/num_examples=10000, total_duration=631.141555, train/accuracy=0.016263, train/loss=6.444295, validation/accuracy=0.016700, validation/loss=6.461048, validation/num_examples=50000
I0428 05:41:53.526418 140326874359552 logging_writer.py:48] [900] global_step=900, grad_norm=0.6280804872512817, loss=6.550332069396973
I0428 05:42:56.793932 140326865966848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6056885123252869, loss=6.567952632904053
I0428 05:44:00.279514 140326874359552 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6410452127456665, loss=6.574193000793457
I0428 05:45:03.665156 140326865966848 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6180894374847412, loss=6.578607559204102
I0428 05:46:06.905448 140326874359552 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.601895809173584, loss=6.546932697296143
I0428 05:47:10.473291 140326865966848 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5608670115470886, loss=6.513659954071045
I0428 05:48:13.788240 140326874359552 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5675927996635437, loss=6.5248847007751465
I0428 05:49:17.190419 140326865966848 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5417034029960632, loss=6.535373687744141
I0428 05:49:21.980030 140533484173120 spec.py:298] Evaluating on the training split.
I0428 05:49:28.882500 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 05:49:36.796176 140533484173120 spec.py:326] Evaluating on the test split.
I0428 05:49:39.112904 140533484173120 submission_runner.py:415] Time since start: 1158.94s, 	Step: 1609, 	{'train/accuracy': 0.019909916445612907, 'train/loss': 6.368528842926025, 'validation/accuracy': 0.01735999993979931, 'validation/loss': 6.412729740142822, 'validation/num_examples': 50000, 'test/accuracy': 0.013300000689923763, 'test/loss': 6.5057196617126465, 'test/num_examples': 10000, 'score': 1083.124656677246, 'total_duration': 1158.9416053295135, 'accumulated_submission_time': 1083.124656677246, 'accumulated_eval_time': 75.75317525863647, 'accumulated_logging_time': 0.04309892654418945}
I0428 05:49:39.121653 140326874359552 logging_writer.py:48] [1609] accumulated_eval_time=75.753175, accumulated_logging_time=0.043099, accumulated_submission_time=1083.124657, global_step=1609, preemption_count=0, score=1083.124657, test/accuracy=0.013300, test/loss=6.505720, test/num_examples=10000, total_duration=1158.941605, train/accuracy=0.019910, train/loss=6.368529, validation/accuracy=0.017360, validation/loss=6.412730, validation/num_examples=50000
I0428 05:50:37.500152 140326865966848 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5309500098228455, loss=6.524666786193848
I0428 05:51:40.952916 140326874359552 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5400723218917847, loss=6.497326850891113
I0428 05:52:44.348285 140326865966848 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5427568554878235, loss=6.533539772033691
I0428 05:53:47.734331 140326874359552 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5205472111701965, loss=6.508993148803711
I0428 05:54:51.215028 140326865966848 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4853948950767517, loss=6.489411354064941
I0428 05:55:54.614912 140326874359552 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4908236265182495, loss=6.497428894042969
I0428 05:56:57.877306 140326865966848 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.46807730197906494, loss=6.463403701782227
I0428 05:58:01.373982 140326874359552 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4610005021095276, loss=6.466819763183594
I0428 05:58:09.286260 140533484173120 spec.py:298] Evaluating on the training split.
I0428 05:58:16.162541 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 05:58:24.025022 140533484173120 spec.py:326] Evaluating on the test split.
I0428 05:58:26.348334 140533484173120 submission_runner.py:415] Time since start: 1686.18s, 	Step: 2414, 	{'train/accuracy': 0.0207868292927742, 'train/loss': 6.289496421813965, 'validation/accuracy': 0.020719999447464943, 'validation/loss': 6.320240497589111, 'validation/num_examples': 50000, 'test/accuracy': 0.016200000420212746, 'test/loss': 6.419797420501709, 'test/num_examples': 10000, 'score': 1593.2700130939484, 'total_duration': 1686.177038192749, 'accumulated_submission_time': 1593.2700130939484, 'accumulated_eval_time': 92.81522536277771, 'accumulated_logging_time': 0.06109261512756348}
I0428 05:58:26.357319 140326865966848 logging_writer.py:48] [2414] accumulated_eval_time=92.815225, accumulated_logging_time=0.061093, accumulated_submission_time=1593.270013, global_step=2414, preemption_count=0, score=1593.270013, test/accuracy=0.016200, test/loss=6.419797, test/num_examples=10000, total_duration=1686.177038, train/accuracy=0.020787, train/loss=6.289496, validation/accuracy=0.020720, validation/loss=6.320240, validation/num_examples=50000
I0428 05:59:21.501121 140326874359552 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4483679234981537, loss=6.461891174316406
I0428 06:00:24.841035 140326865966848 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.43474242091178894, loss=6.480166435241699
I0428 06:01:28.156718 140326874359552 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4147080183029175, loss=6.469020366668701
I0428 06:02:31.441258 140326865966848 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.39797964692115784, loss=6.462884426116943
I0428 06:03:34.736451 140326874359552 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.39031046628952026, loss=6.468839645385742
I0428 06:04:38.121789 140326865966848 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.38291579484939575, loss=6.486867904663086
I0428 06:05:41.510453 140326874359552 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.3825055956840515, loss=6.435817718505859
I0428 06:06:44.754193 140326865966848 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.36315006017684937, loss=6.462358474731445
I0428 06:06:56.422892 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:07:03.332834 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:07:11.057529 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:07:13.384639 140533484173120 submission_runner.py:415] Time since start: 2213.21s, 	Step: 3220, 	{'train/accuracy': 0.021843111142516136, 'train/loss': 6.263242244720459, 'validation/accuracy': 0.021699998527765274, 'validation/loss': 6.3031487464904785, 'validation/num_examples': 50000, 'test/accuracy': 0.017900001257658005, 'test/loss': 6.402946949005127, 'test/num_examples': 10000, 'score': 2103.3170070648193, 'total_duration': 2213.2133100032806, 'accumulated_submission_time': 2103.3170070648193, 'accumulated_eval_time': 109.77695035934448, 'accumulated_logging_time': 0.07847166061401367}
I0428 06:07:13.395585 140326874359552 logging_writer.py:48] [3220] accumulated_eval_time=109.776950, accumulated_logging_time=0.078472, accumulated_submission_time=2103.317007, global_step=3220, preemption_count=0, score=2103.317007, test/accuracy=0.017900, test/loss=6.402947, test/num_examples=10000, total_duration=2213.213310, train/accuracy=0.021843, train/loss=6.263242, validation/accuracy=0.021700, validation/loss=6.303149, validation/num_examples=50000
I0428 06:08:04.685150 140326865966848 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.37489309906959534, loss=6.4159159660339355
I0428 06:09:08.252917 140326874359552 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.39449936151504517, loss=6.408857822418213
I0428 06:10:11.654025 140326865966848 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.3526456952095032, loss=6.4594340324401855
I0428 06:11:15.165001 140326874359552 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.36080074310302734, loss=6.3918681144714355
I0428 06:12:18.235433 140326865966848 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.3651506006717682, loss=6.4225640296936035
I0428 06:13:21.484299 140326874359552 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.3397602140903473, loss=6.4094038009643555
I0428 06:14:24.975716 140326865966848 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.335737943649292, loss=6.429945945739746
I0428 06:15:28.412590 140326874359552 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.33275362849235535, loss=6.402155876159668
I0428 06:15:43.953627 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:15:50.885449 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:15:58.616300 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:16:00.669192 140533484173120 submission_runner.py:415] Time since start: 2740.50s, 	Step: 4026, 	{'train/accuracy': 0.027722416445612907, 'train/loss': 6.159175395965576, 'validation/accuracy': 0.02582000009715557, 'validation/loss': 6.22192907333374, 'validation/num_examples': 50000, 'test/accuracy': 0.020100001245737076, 'test/loss': 6.3230695724487305, 'test/num_examples': 10000, 'score': 2613.856209039688, 'total_duration': 2740.4978930950165, 'accumulated_submission_time': 2613.856209039688, 'accumulated_eval_time': 126.49249243736267, 'accumulated_logging_time': 0.09806632995605469}
I0428 06:16:00.678820 140326865966848 logging_writer.py:48] [4026] accumulated_eval_time=126.492492, accumulated_logging_time=0.098066, accumulated_submission_time=2613.856209, global_step=4026, preemption_count=0, score=2613.856209, test/accuracy=0.020100, test/loss=6.323070, test/num_examples=10000, total_duration=2740.497893, train/accuracy=0.027722, train/loss=6.159175, validation/accuracy=0.025820, validation/loss=6.221929, validation/num_examples=50000
I0428 06:16:48.238778 140326874359552 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.33613166213035583, loss=6.406896114349365
I0428 06:17:51.727441 140326865966848 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.31865963339805603, loss=6.363489151000977
I0428 06:18:55.064390 140326874359552 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3308064639568329, loss=6.4152631759643555
I0428 06:19:58.458812 140326865966848 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.33435410261154175, loss=6.411685466766357
I0428 06:21:01.916231 140326874359552 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.33274045586586, loss=6.389503002166748
I0428 06:22:05.229336 140326865966848 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3385360538959503, loss=6.385173797607422
I0428 06:23:08.607719 140326874359552 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3560901880264282, loss=6.393022537231445
I0428 06:24:12.001215 140326865966848 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.3212607800960541, loss=6.453205108642578
I0428 06:24:30.750903 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:24:37.737828 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:24:45.758820 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:24:49.217809 140533484173120 submission_runner.py:415] Time since start: 3269.05s, 	Step: 4831, 	{'train/accuracy': 0.030731823295354843, 'train/loss': 6.113183975219727, 'validation/accuracy': 0.02850000001490116, 'validation/loss': 6.154013156890869, 'validation/num_examples': 50000, 'test/accuracy': 0.023500001057982445, 'test/loss': 6.274304389953613, 'test/num_examples': 10000, 'score': 3123.910295009613, 'total_duration': 3269.046510219574, 'accumulated_submission_time': 3123.910295009613, 'accumulated_eval_time': 144.95940136909485, 'accumulated_logging_time': 0.11565542221069336}
I0428 06:24:49.227007 140326874359552 logging_writer.py:48] [4831] accumulated_eval_time=144.959401, accumulated_logging_time=0.115655, accumulated_submission_time=3123.910295, global_step=4831, preemption_count=0, score=3123.910295, test/accuracy=0.023500, test/loss=6.274304, test/num_examples=10000, total_duration=3269.046510, train/accuracy=0.030732, train/loss=6.113184, validation/accuracy=0.028500, validation/loss=6.154013, validation/num_examples=50000
I0428 06:25:33.612466 140326865966848 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3171115219593048, loss=6.365159034729004
I0428 06:26:37.014150 140326874359552 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.37899354100227356, loss=6.329342842102051
I0428 06:27:40.506670 140326865966848 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.30171000957489014, loss=6.345401287078857
I0428 06:28:43.970643 140326874359552 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.304584801197052, loss=6.309657096862793
I0428 06:29:47.434884 140326865966848 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.2824849784374237, loss=6.372909069061279
I0428 06:30:50.756423 140326874359552 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.29638466238975525, loss=6.349370002746582
I0428 06:31:54.702263 140326865966848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.3641757369041443, loss=6.345426559448242
I0428 06:32:58.130053 140326874359552 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.4751203954219818, loss=6.373570919036865
I0428 06:33:19.401691 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:33:26.272728 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:33:34.062999 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:33:36.130791 140533484173120 submission_runner.py:415] Time since start: 3795.96s, 	Step: 5635, 	{'train/accuracy': 0.035893652588129044, 'train/loss': 6.062813758850098, 'validation/accuracy': 0.03313999995589256, 'validation/loss': 6.1202073097229, 'validation/num_examples': 50000, 'test/accuracy': 0.024000000208616257, 'test/loss': 6.2388458251953125, 'test/num_examples': 10000, 'score': 3634.0667254924774, 'total_duration': 3795.9594719409943, 'accumulated_submission_time': 3634.0667254924774, 'accumulated_eval_time': 161.68845772743225, 'accumulated_logging_time': 0.13326549530029297}
I0428 06:33:36.140338 140326865966848 logging_writer.py:48] [5635] accumulated_eval_time=161.688458, accumulated_logging_time=0.133265, accumulated_submission_time=3634.066725, global_step=5635, preemption_count=0, score=3634.066725, test/accuracy=0.024000, test/loss=6.238846, test/num_examples=10000, total_duration=3795.959472, train/accuracy=0.035894, train/loss=6.062814, validation/accuracy=0.033140, validation/loss=6.120207, validation/num_examples=50000
I0428 06:34:17.858537 140326874359552 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.2799963653087616, loss=6.342682838439941
I0428 06:35:21.224024 140326865966848 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.3752332031726837, loss=6.30097770690918
I0428 06:36:24.539834 140326874359552 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.31123292446136475, loss=6.246767044067383
I0428 06:37:27.981641 140326865966848 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4669242203235626, loss=6.314297199249268
I0428 06:38:33.617456 140326874359552 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.25058791041374207, loss=6.261979103088379
I0428 06:39:36.865610 140326865966848 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4065730571746826, loss=6.272221088409424
I0428 06:40:40.425601 140326874359552 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.37818825244903564, loss=6.288027763366699
I0428 06:41:43.786149 140326865966848 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4311162829399109, loss=6.295199871063232
I0428 06:42:06.184269 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:42:13.190973 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:42:21.164397 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:42:23.481656 140533484173120 submission_runner.py:415] Time since start: 4323.31s, 	Step: 6437, 	{'train/accuracy': 0.03252550959587097, 'train/loss': 6.047701835632324, 'validation/accuracy': 0.031019998714327812, 'validation/loss': 6.085089683532715, 'validation/num_examples': 50000, 'test/accuracy': 0.021900001913309097, 'test/loss': 6.220163822174072, 'test/num_examples': 10000, 'score': 4144.09125995636, 'total_duration': 4323.310355424881, 'accumulated_submission_time': 4144.09125995636, 'accumulated_eval_time': 178.9858214855194, 'accumulated_logging_time': 0.15191984176635742}
I0428 06:42:23.491528 140326874359552 logging_writer.py:48] [6437] accumulated_eval_time=178.985821, accumulated_logging_time=0.151920, accumulated_submission_time=4144.091260, global_step=6437, preemption_count=0, score=4144.091260, test/accuracy=0.021900, test/loss=6.220164, test/num_examples=10000, total_duration=4323.310355, train/accuracy=0.032526, train/loss=6.047702, validation/accuracy=0.031020, validation/loss=6.085090, validation/num_examples=50000
I0428 06:43:03.914245 140326865966848 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.40172067284584045, loss=6.335906982421875
I0428 06:44:07.706380 140326874359552 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.3644910156726837, loss=6.258433818817139
I0428 06:45:11.254819 140326865966848 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4662298560142517, loss=6.243228435516357
I0428 06:46:14.730340 140326874359552 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.2730136811733246, loss=6.221584320068359
I0428 06:47:20.606166 140326865966848 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4128616154193878, loss=6.299121856689453
I0428 06:48:24.083692 140326874359552 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7831210494041443, loss=6.222481727600098
I0428 06:49:27.644898 140326865966848 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.32095783948898315, loss=6.274054050445557
I0428 06:50:31.209391 140326874359552 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.448851615190506, loss=6.2246904373168945
I0428 06:50:53.841660 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:51:00.744295 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:51:08.607684 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:51:10.888112 140533484173120 submission_runner.py:415] Time since start: 4850.72s, 	Step: 7237, 	{'train/accuracy': 0.034259404987096786, 'train/loss': 6.004663944244385, 'validation/accuracy': 0.032999999821186066, 'validation/loss': 6.06771183013916, 'validation/num_examples': 50000, 'test/accuracy': 0.023600000888109207, 'test/loss': 6.210371017456055, 'test/num_examples': 10000, 'score': 4654.4230444431305, 'total_duration': 4850.716812372208, 'accumulated_submission_time': 4654.4230444431305, 'accumulated_eval_time': 196.03224968910217, 'accumulated_logging_time': 0.16998982429504395}
I0428 06:51:10.897602 140326865966848 logging_writer.py:48] [7237] accumulated_eval_time=196.032250, accumulated_logging_time=0.169990, accumulated_submission_time=4654.423044, global_step=7237, preemption_count=0, score=4654.423044, test/accuracy=0.023600, test/loss=6.210371, test/num_examples=10000, total_duration=4850.716812, train/accuracy=0.034259, train/loss=6.004664, validation/accuracy=0.033000, validation/loss=6.067712, validation/num_examples=50000
I0428 06:51:52.625194 140326874359552 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5116451382637024, loss=6.26893949508667
I0428 06:52:56.076930 140326865966848 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6021659970283508, loss=6.262695789337158
I0428 06:54:00.170030 140326874359552 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.34100452065467834, loss=6.3176350593566895
I0428 06:55:03.796577 140326865966848 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.3206063210964203, loss=6.271048545837402
I0428 06:56:07.701269 140326874359552 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.35511863231658936, loss=6.315544605255127
I0428 06:57:11.318157 140326865966848 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.40705835819244385, loss=6.328187465667725
I0428 06:58:16.626689 140326874359552 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.41901031136512756, loss=6.349130630493164
I0428 06:59:20.248703 140326865966848 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6337375044822693, loss=6.379274845123291
I0428 06:59:41.001163 140533484173120 spec.py:298] Evaluating on the training split.
I0428 06:59:47.795830 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 06:59:55.753692 140533484173120 spec.py:326] Evaluating on the test split.
I0428 06:59:57.931182 140533484173120 submission_runner.py:415] Time since start: 5377.76s, 	Step: 8034, 	{'train/accuracy': 0.03698979690670967, 'train/loss': 5.984800338745117, 'validation/accuracy': 0.03271999955177307, 'validation/loss': 6.074314117431641, 'validation/num_examples': 50000, 'test/accuracy': 0.025800000876188278, 'test/loss': 6.2109055519104, 'test/num_examples': 10000, 'score': 5164.507261514664, 'total_duration': 5377.759849786758, 'accumulated_submission_time': 5164.507261514664, 'accumulated_eval_time': 212.96221208572388, 'accumulated_logging_time': 0.18876123428344727}
I0428 06:59:57.940920 140326874359552 logging_writer.py:48] [8034] accumulated_eval_time=212.962212, accumulated_logging_time=0.188761, accumulated_submission_time=5164.507262, global_step=8034, preemption_count=0, score=5164.507262, test/accuracy=0.025800, test/loss=6.210906, test/num_examples=10000, total_duration=5377.759850, train/accuracy=0.036990, train/loss=5.984800, validation/accuracy=0.032720, validation/loss=6.074314, validation/num_examples=50000
I0428 07:00:41.864788 140326865966848 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.781343400478363, loss=6.4595232009887695
I0428 07:01:45.513194 140326874359552 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5788796544075012, loss=6.408560752868652
I0428 07:02:49.135233 140326865966848 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7053802609443665, loss=6.495057582855225
I0428 07:03:53.112328 140326874359552 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6333604454994202, loss=6.483600616455078
I0428 07:04:56.917430 140326865966848 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8356581330299377, loss=6.577042102813721
I0428 07:06:00.452163 140326874359552 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8185864686965942, loss=6.633786678314209
I0428 07:07:04.188265 140326865966848 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8131508827209473, loss=6.64145565032959
I0428 07:08:07.958703 140326874359552 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7819951176643372, loss=6.555156230926514
I0428 07:08:28.039248 140533484173120 spec.py:298] Evaluating on the training split.
I0428 07:08:34.862390 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 07:08:42.715225 140533484173120 spec.py:326] Evaluating on the test split.
I0428 07:08:46.436959 140533484173120 submission_runner.py:415] Time since start: 5906.27s, 	Step: 8833, 	{'train/accuracy': 0.0315290167927742, 'train/loss': 6.086880207061768, 'validation/accuracy': 0.031019998714327812, 'validation/loss': 6.123008728027344, 'validation/num_examples': 50000, 'test/accuracy': 0.02330000139772892, 'test/loss': 6.239058971405029, 'test/num_examples': 10000, 'score': 5674.5861604213715, 'total_duration': 5906.265658378601, 'accumulated_submission_time': 5674.5861604213715, 'accumulated_eval_time': 231.35989785194397, 'accumulated_logging_time': 0.20791983604431152}
I0428 07:08:46.446694 140326865966848 logging_writer.py:48] [8833] accumulated_eval_time=231.359898, accumulated_logging_time=0.207920, accumulated_submission_time=5674.586160, global_step=8833, preemption_count=0, score=5674.586160, test/accuracy=0.023300, test/loss=6.239059, test/num_examples=10000, total_duration=5906.265658, train/accuracy=0.031529, train/loss=6.086880, validation/accuracy=0.031020, validation/loss=6.123009, validation/num_examples=50000
I0428 07:09:29.866919 140326874359552 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.668224036693573, loss=6.528572082519531
I0428 07:10:36.396398 140326865966848 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8482394814491272, loss=6.675620079040527
I0428 07:11:40.054648 140326874359552 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7124632000923157, loss=6.575727939605713
I0428 07:12:43.876301 140326865966848 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6855790615081787, loss=6.512691497802734
I0428 07:13:47.558907 140326874359552 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8018827438354492, loss=6.593791484832764
I0428 07:14:51.251996 140326865966848 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5782852172851562, loss=6.500508785247803
I0428 07:15:54.951703 140326874359552 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5926510691642761, loss=6.515801906585693
I0428 07:16:58.738849 140326865966848 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6834937334060669, loss=6.543980121612549
I0428 07:17:16.926054 140533484173120 spec.py:298] Evaluating on the training split.
I0428 07:17:23.798926 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 07:17:31.920175 140533484173120 spec.py:326] Evaluating on the test split.
I0428 07:17:34.132991 140533484173120 submission_runner.py:415] Time since start: 6433.96s, 	Step: 9630, 	{'train/accuracy': 0.02666613459587097, 'train/loss': 6.182265758514404, 'validation/accuracy': 0.024779999628663063, 'validation/loss': 6.245525360107422, 'validation/num_examples': 50000, 'test/accuracy': 0.018400000408291817, 'test/loss': 6.35684871673584, 'test/num_examples': 10000, 'score': 6185.047253847122, 'total_duration': 6433.961688280106, 'accumulated_submission_time': 6185.047253847122, 'accumulated_eval_time': 248.56680941581726, 'accumulated_logging_time': 0.2258617877960205}
I0428 07:17:34.143004 140326874359552 logging_writer.py:48] [9630] accumulated_eval_time=248.566809, accumulated_logging_time=0.225862, accumulated_submission_time=6185.047254, global_step=9630, preemption_count=0, score=6185.047254, test/accuracy=0.018400, test/loss=6.356849, test/num_examples=10000, total_duration=6433.961688, train/accuracy=0.026666, train/loss=6.182266, validation/accuracy=0.024780, validation/loss=6.245525, validation/num_examples=50000
I0428 07:18:19.378353 140326865966848 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5458544492721558, loss=6.404914855957031
I0428 07:19:23.487286 140326874359552 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6337109804153442, loss=6.545706748962402
I0428 07:20:29.790214 140326865966848 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5427156686782837, loss=6.475069046020508
I0428 07:21:33.482450 140326874359552 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6648441553115845, loss=6.581589698791504
I0428 07:22:37.036950 140326865966848 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5852461457252502, loss=6.524888038635254
I0428 07:23:40.717314 140326874359552 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.47229695320129395, loss=6.382546424865723
I0428 07:24:44.328933 140326865966848 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5088145732879639, loss=6.417459964752197
I0428 07:25:48.060472 140326874359552 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5557666420936584, loss=6.548393249511719
I0428 07:26:04.286915 140533484173120 spec.py:298] Evaluating on the training split.
I0428 07:26:11.122676 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 07:26:19.148030 140533484173120 spec.py:326] Evaluating on the test split.
I0428 07:26:21.453806 140533484173120 submission_runner.py:415] Time since start: 6961.28s, 	Step: 10427, 	{'train/accuracy': 0.04043765738606453, 'train/loss': 5.97617769241333, 'validation/accuracy': 0.03709999844431877, 'validation/loss': 6.033982753753662, 'validation/num_examples': 50000, 'test/accuracy': 0.026100000366568565, 'test/loss': 6.1760783195495605, 'test/num_examples': 10000, 'score': 6695.172476053238, 'total_duration': 6961.282447814941, 'accumulated_submission_time': 6695.172476053238, 'accumulated_eval_time': 265.7336368560791, 'accumulated_logging_time': 0.2440931797027588}
I0428 07:26:21.472200 140326865966848 logging_writer.py:48] [10427] accumulated_eval_time=265.733637, accumulated_logging_time=0.244093, accumulated_submission_time=6695.172476, global_step=10427, preemption_count=0, score=6695.172476, test/accuracy=0.026100, test/loss=6.176078, test/num_examples=10000, total_duration=6961.282448, train/accuracy=0.040438, train/loss=5.976178, validation/accuracy=0.037100, validation/loss=6.033983, validation/num_examples=50000
I0428 07:27:08.584237 140326874359552 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5253779292106628, loss=6.449246406555176
I0428 07:28:12.271589 140326865966848 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5256938934326172, loss=6.485475540161133
I0428 07:29:15.858465 140326874359552 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.44453367590904236, loss=6.42608642578125
I0428 07:30:19.681865 140326865966848 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.47133105993270874, loss=6.425505638122559
I0428 07:31:24.553277 140326874359552 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5548313856124878, loss=6.400452136993408
I0428 07:32:28.251356 140326865966848 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.524040162563324, loss=6.440255165100098
I0428 07:33:32.038014 140326874359552 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6001601815223694, loss=6.487824440002441
I0428 07:34:35.781864 140326865966848 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5205764770507812, loss=6.486421585083008
I0428 07:34:52.026783 140533484173120 spec.py:298] Evaluating on the training split.
I0428 07:34:59.049267 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 07:35:06.990513 140533484173120 spec.py:326] Evaluating on the test split.
I0428 07:35:09.313511 140533484173120 submission_runner.py:415] Time since start: 7489.14s, 	Step: 11227, 	{'train/accuracy': 0.046097736805677414, 'train/loss': 5.911163330078125, 'validation/accuracy': 0.043880000710487366, 'validation/loss': 5.948585033416748, 'validation/num_examples': 50000, 'test/accuracy': 0.0340999998152256, 'test/loss': 6.103226661682129, 'test/num_examples': 10000, 'score': 7205.703100681305, 'total_duration': 7489.142204999924, 'accumulated_submission_time': 7205.703100681305, 'accumulated_eval_time': 283.02033495903015, 'accumulated_logging_time': 0.2762458324432373}
I0428 07:35:09.323553 140326874359552 logging_writer.py:48] [11227] accumulated_eval_time=283.020335, accumulated_logging_time=0.276246, accumulated_submission_time=7205.703101, global_step=11227, preemption_count=0, score=7205.703101, test/accuracy=0.034100, test/loss=6.103227, test/num_examples=10000, total_duration=7489.142205, train/accuracy=0.046098, train/loss=5.911163, validation/accuracy=0.043880, validation/loss=5.948585, validation/num_examples=50000
I0428 07:35:56.340951 140326865966848 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4009193480014801, loss=6.370965003967285
I0428 07:37:00.031157 140326874359552 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.34737128019332886, loss=6.374611854553223
I0428 07:38:03.761129 140326865966848 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.4474256634712219, loss=6.403623104095459
I0428 07:39:07.429981 140326874359552 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.41699036955833435, loss=6.404693126678467
I0428 07:40:11.095019 140326865966848 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.4087528586387634, loss=6.300727367401123
I0428 07:41:14.783115 140326874359552 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4024231731891632, loss=6.347680568695068
I0428 07:42:18.552584 140326865966848 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.423601359128952, loss=6.3461713790893555
I0428 07:43:22.148583 140326874359552 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4700584411621094, loss=6.371094703674316
I0428 07:43:39.657296 140533484173120 spec.py:298] Evaluating on the training split.
I0428 07:43:46.665394 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 07:43:54.549147 140533484173120 spec.py:326] Evaluating on the test split.
I0428 07:43:56.849096 140533484173120 submission_runner.py:415] Time since start: 8016.68s, 	Step: 12029, 	{'train/accuracy': 0.048728473484516144, 'train/loss': 5.868307590484619, 'validation/accuracy': 0.04332000017166138, 'validation/loss': 5.952307224273682, 'validation/num_examples': 50000, 'test/accuracy': 0.03070000186562538, 'test/loss': 6.1137824058532715, 'test/num_examples': 10000, 'score': 7716.018489599228, 'total_duration': 8016.677796125412, 'accumulated_submission_time': 7716.018489599228, 'accumulated_eval_time': 300.2121286392212, 'accumulated_logging_time': 0.29454588890075684}
I0428 07:43:56.860241 140326865966848 logging_writer.py:48] [12029] accumulated_eval_time=300.212129, accumulated_logging_time=0.294546, accumulated_submission_time=7716.018490, global_step=12029, preemption_count=0, score=7716.018490, test/accuracy=0.030700, test/loss=6.113782, test/num_examples=10000, total_duration=8016.677796, train/accuracy=0.048728, train/loss=5.868308, validation/accuracy=0.043320, validation/loss=5.952307, validation/num_examples=50000
I0428 07:44:42.692838 140326874359552 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.4654492735862732, loss=6.358358383178711
I0428 07:45:46.317590 140326865966848 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4525899887084961, loss=6.405077934265137
I0428 07:46:50.088482 140326874359552 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.39709073305130005, loss=6.301474571228027
I0428 07:47:53.870738 140326865966848 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.38234829902648926, loss=6.284762382507324
I0428 07:48:57.459581 140326874359552 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.36416706442832947, loss=6.311418533325195
I0428 07:50:01.077383 140326865966848 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.35335052013397217, loss=6.269993305206299
I0428 07:51:04.719950 140326874359552 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.30647599697113037, loss=6.273704528808594
I0428 07:52:08.418345 140326865966848 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3302118480205536, loss=6.195750713348389
I0428 07:52:27.182885 140533484173120 spec.py:298] Evaluating on the training split.
I0428 07:52:34.344923 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 07:52:44.157519 140533484173120 spec.py:326] Evaluating on the test split.
I0428 07:52:46.224275 140533484173120 submission_runner.py:415] Time since start: 8546.05s, 	Step: 12831, 	{'train/accuracy': 0.04209183529019356, 'train/loss': 5.92786979675293, 'validation/accuracy': 0.04031999781727791, 'validation/loss': 5.987668037414551, 'validation/num_examples': 50000, 'test/accuracy': 0.029000001028180122, 'test/loss': 6.1429619789123535, 'test/num_examples': 10000, 'score': 8226.322344779968, 'total_duration': 8546.05297613144, 'accumulated_submission_time': 8226.322344779968, 'accumulated_eval_time': 319.2535312175751, 'accumulated_logging_time': 0.3139650821685791}
I0428 07:52:46.235055 140326874359552 logging_writer.py:48] [12831] accumulated_eval_time=319.253531, accumulated_logging_time=0.313965, accumulated_submission_time=8226.322345, global_step=12831, preemption_count=0, score=8226.322345, test/accuracy=0.029000, test/loss=6.142962, test/num_examples=10000, total_duration=8546.052976, train/accuracy=0.042092, train/loss=5.927870, validation/accuracy=0.040320, validation/loss=5.987668, validation/num_examples=50000
I0428 07:53:30.839627 140326865966848 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3879454731941223, loss=6.211208343505859
I0428 07:54:34.609266 140326874359552 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3687117099761963, loss=6.269587516784668
I0428 07:55:38.297917 140326865966848 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.35515323281288147, loss=6.245187759399414
I0428 07:56:41.971037 140326874359552 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.39408838748931885, loss=6.241774082183838
I0428 07:57:45.670302 140326865966848 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4198746383190155, loss=6.227344512939453
I0428 07:58:49.354341 140326874359552 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3568514883518219, loss=6.275184631347656
I0428 07:59:52.915712 140326865966848 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2972252666950226, loss=6.200766563415527
I0428 08:00:56.480179 140326874359552 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3278597891330719, loss=6.258904933929443
I0428 08:01:16.521281 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:01:23.722162 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:01:33.644143 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:01:35.663520 140533484173120 submission_runner.py:415] Time since start: 9075.49s, 	Step: 13633, 	{'train/accuracy': 0.053073182702064514, 'train/loss': 5.790137767791748, 'validation/accuracy': 0.050939999520778656, 'validation/loss': 5.8576178550720215, 'validation/num_examples': 50000, 'test/accuracy': 0.03490000218153, 'test/loss': 6.027804374694824, 'test/num_examples': 10000, 'score': 8736.588426828384, 'total_duration': 9075.492210388184, 'accumulated_submission_time': 8736.588426828384, 'accumulated_eval_time': 338.3957381248474, 'accumulated_logging_time': 0.33441686630249023}
I0428 08:01:35.674996 140326865966848 logging_writer.py:48] [13633] accumulated_eval_time=338.395738, accumulated_logging_time=0.334417, accumulated_submission_time=8736.588427, global_step=13633, preemption_count=0, score=8736.588427, test/accuracy=0.034900, test/loss=6.027804, test/num_examples=10000, total_duration=9075.492210, train/accuracy=0.053073, train/loss=5.790138, validation/accuracy=0.050940, validation/loss=5.857618, validation/num_examples=50000
I0428 08:02:19.263612 140326874359552 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.35276105999946594, loss=6.283942222595215
I0428 08:03:23.099754 140326865966848 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2692900598049164, loss=6.185638427734375
I0428 08:04:26.907173 140326874359552 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2853884994983673, loss=6.153918266296387
I0428 08:05:30.603779 140326865966848 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3158586025238037, loss=6.211016654968262
I0428 08:06:34.375500 140326874359552 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.25556308031082153, loss=6.165555477142334
I0428 08:07:37.959522 140326865966848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2281528115272522, loss=6.172919273376465
I0428 08:08:41.648415 140326874359552 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.27707570791244507, loss=6.151712894439697
I0428 08:09:45.407217 140326865966848 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.25246334075927734, loss=6.1741623878479
I0428 08:10:06.111752 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:10:13.646900 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:10:23.774821 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:10:25.989426 140533484173120 submission_runner.py:415] Time since start: 9605.82s, 	Step: 14434, 	{'train/accuracy': 0.04404496029019356, 'train/loss': 5.880850315093994, 'validation/accuracy': 0.04335999861359596, 'validation/loss': 5.93757963180542, 'validation/num_examples': 50000, 'test/accuracy': 0.029100000858306885, 'test/loss': 6.105411052703857, 'test/num_examples': 10000, 'score': 9247.00608420372, 'total_duration': 9605.818126678467, 'accumulated_submission_time': 9247.00608420372, 'accumulated_eval_time': 358.2733910083771, 'accumulated_logging_time': 0.3546912670135498}
I0428 08:10:26.000388 140326874359552 logging_writer.py:48] [14434] accumulated_eval_time=358.273391, accumulated_logging_time=0.354691, accumulated_submission_time=9247.006084, global_step=14434, preemption_count=0, score=9247.006084, test/accuracy=0.029100, test/loss=6.105411, test/num_examples=10000, total_duration=9605.818127, train/accuracy=0.044045, train/loss=5.880850, validation/accuracy=0.043360, validation/loss=5.937580, validation/num_examples=50000
I0428 08:11:08.547140 140326865966848 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2613050937652588, loss=6.207202434539795
I0428 08:12:12.215960 140326874359552 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3118788003921509, loss=6.169397830963135
I0428 08:13:16.066980 140326865966848 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.24845454096794128, loss=6.156623840332031
I0428 08:14:19.714335 140326874359552 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.29668259620666504, loss=6.152279376983643
I0428 08:15:23.461656 140326865966848 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.30201008915901184, loss=6.158148288726807
I0428 08:16:27.185024 140326874359552 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3334203362464905, loss=6.209730625152588
I0428 08:17:30.931640 140326865966848 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2947213649749756, loss=6.166521072387695
I0428 08:18:34.852141 140326874359552 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.31781378388404846, loss=6.13202428817749
I0428 08:18:56.179194 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:19:03.924600 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:19:14.712897 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:19:16.970043 140533484173120 submission_runner.py:415] Time since start: 10136.80s, 	Step: 15235, 	{'train/accuracy': 0.058254942297935486, 'train/loss': 5.768155574798584, 'validation/accuracy': 0.05377999693155289, 'validation/loss': 5.826873779296875, 'validation/num_examples': 50000, 'test/accuracy': 0.03880000114440918, 'test/loss': 6.007757186889648, 'test/num_examples': 10000, 'score': 9757.166797161102, 'total_duration': 10136.798737049103, 'accumulated_submission_time': 9757.166797161102, 'accumulated_eval_time': 379.06422686576843, 'accumulated_logging_time': 0.37354445457458496}
I0428 08:19:16.981018 140326865966848 logging_writer.py:48] [15235] accumulated_eval_time=379.064227, accumulated_logging_time=0.373544, accumulated_submission_time=9757.166797, global_step=15235, preemption_count=0, score=9757.166797, test/accuracy=0.038800, test/loss=6.007757, test/num_examples=10000, total_duration=10136.798737, train/accuracy=0.058255, train/loss=5.768156, validation/accuracy=0.053780, validation/loss=5.826874, validation/num_examples=50000
I0428 08:19:59.074412 140326874359552 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.28043273091316223, loss=6.123380661010742
I0428 08:21:02.802406 140326865966848 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.22384172677993774, loss=6.099633693695068
I0428 08:22:06.473869 140326874359552 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.26769959926605225, loss=6.163930892944336
I0428 08:23:10.259438 140326865966848 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2599886655807495, loss=6.139491558074951
I0428 08:24:14.045203 140326874359552 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.25909024477005005, loss=6.156756401062012
I0428 08:25:17.720747 140326865966848 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.35677915811538696, loss=6.159944534301758
I0428 08:26:21.438899 140326874359552 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.25523000955581665, loss=6.201311111450195
I0428 08:27:25.292925 140326865966848 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.32595136761665344, loss=6.1827826499938965
I0428 08:27:47.318679 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:27:55.106899 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:28:05.560428 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:28:07.859236 140533484173120 submission_runner.py:415] Time since start: 10667.69s, 	Step: 16036, 	{'train/accuracy': 0.054587848484516144, 'train/loss': 5.759245872497559, 'validation/accuracy': 0.05031999945640564, 'validation/loss': 5.85275936126709, 'validation/num_examples': 50000, 'test/accuracy': 0.03360000252723694, 'test/loss': 6.036982536315918, 'test/num_examples': 10000, 'score': 10267.48531126976, 'total_duration': 10667.687936067581, 'accumulated_submission_time': 10267.48531126976, 'accumulated_eval_time': 399.6047580242157, 'accumulated_logging_time': 0.393521785736084}
I0428 08:28:07.869852 140326874359552 logging_writer.py:48] [16036] accumulated_eval_time=399.604758, accumulated_logging_time=0.393522, accumulated_submission_time=10267.485311, global_step=16036, preemption_count=0, score=10267.485311, test/accuracy=0.033600, test/loss=6.036983, test/num_examples=10000, total_duration=10667.687936, train/accuracy=0.054588, train/loss=5.759246, validation/accuracy=0.050320, validation/loss=5.852759, validation/num_examples=50000
I0428 08:28:49.254792 140326865966848 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.24948284029960632, loss=6.168872833251953
I0428 08:29:53.057812 140326874359552 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2862386703491211, loss=6.167418479919434
I0428 08:30:56.845602 140326865966848 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2584747076034546, loss=6.200238227844238
I0428 08:32:00.592900 140326874359552 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2449605017900467, loss=6.113697528839111
I0428 08:33:04.379880 140326865966848 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.2993082106113434, loss=6.164641380310059
I0428 08:34:08.005879 140326874359552 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2896912395954132, loss=6.158522129058838
I0428 08:35:11.741535 140326865966848 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2783740758895874, loss=6.2064104080200195
I0428 08:36:15.472302 140326874359552 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.2556227743625641, loss=6.044388294219971
I0428 08:36:38.083698 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:36:45.705988 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:36:56.492485 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:36:58.742804 140533484173120 submission_runner.py:415] Time since start: 11198.57s, 	Step: 16837, 	{'train/accuracy': 0.06499122828245163, 'train/loss': 5.6734843254089355, 'validation/accuracy': 0.061159998178482056, 'validation/loss': 5.727813720703125, 'validation/num_examples': 50000, 'test/accuracy': 0.04320000112056732, 'test/loss': 5.928177833557129, 'test/num_examples': 10000, 'score': 10777.680795907974, 'total_duration': 11198.571491479874, 'accumulated_submission_time': 10777.680795907974, 'accumulated_eval_time': 420.26382660865784, 'accumulated_logging_time': 0.4122769832611084}
I0428 08:36:58.753865 140326865966848 logging_writer.py:48] [16837] accumulated_eval_time=420.263827, accumulated_logging_time=0.412277, accumulated_submission_time=10777.680796, global_step=16837, preemption_count=0, score=10777.680796, test/accuracy=0.043200, test/loss=5.928178, test/num_examples=10000, total_duration=11198.571491, train/accuracy=0.064991, train/loss=5.673484, validation/accuracy=0.061160, validation/loss=5.727814, validation/num_examples=50000
I0428 08:37:39.572613 140326874359552 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2583198547363281, loss=6.117082595825195
I0428 08:38:43.419243 140326865966848 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.26066088676452637, loss=6.110404014587402
I0428 08:39:47.252115 140326874359552 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.28366589546203613, loss=6.131605625152588
I0428 08:40:50.942005 140326865966848 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2459612786769867, loss=6.113319396972656
I0428 08:41:54.712907 140326874359552 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.23851054906845093, loss=6.135250091552734
I0428 08:42:58.288260 140326865966848 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2751765251159668, loss=6.115421772003174
I0428 08:44:01.967013 140326874359552 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2504844069480896, loss=6.1050543785095215
I0428 08:45:05.645241 140326865966848 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.24325807392597198, loss=6.1368489265441895
I0428 08:45:28.808465 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:45:36.470688 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:45:47.638775 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:45:49.826133 140533484173120 submission_runner.py:415] Time since start: 11729.65s, 	Step: 17638, 	{'train/accuracy': 0.05255500599741936, 'train/loss': 5.7794599533081055, 'validation/accuracy': 0.04907999932765961, 'validation/loss': 5.836338043212891, 'validation/num_examples': 50000, 'test/accuracy': 0.036500003188848495, 'test/loss': 6.022609710693359, 'test/num_examples': 10000, 'score': 11287.716339588165, 'total_duration': 11729.654824733734, 'accumulated_submission_time': 11287.716339588165, 'accumulated_eval_time': 441.28146386146545, 'accumulated_logging_time': 0.43208956718444824}
I0428 08:45:49.838128 140326874359552 logging_writer.py:48] [17638] accumulated_eval_time=441.281464, accumulated_logging_time=0.432090, accumulated_submission_time=11287.716340, global_step=17638, preemption_count=0, score=11287.716340, test/accuracy=0.036500, test/loss=6.022610, test/num_examples=10000, total_duration=11729.654825, train/accuracy=0.052555, train/loss=5.779460, validation/accuracy=0.049080, validation/loss=5.836338, validation/num_examples=50000
I0428 08:46:29.885030 140326865966848 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.25711318850517273, loss=6.1633453369140625
I0428 08:47:33.742031 140326874359552 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2636425495147705, loss=6.153064250946045
I0428 08:48:37.425820 140326865966848 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.21896490454673767, loss=6.0572075843811035
I0428 08:49:41.252214 140326874359552 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.2696753740310669, loss=6.1249260902404785
I0428 08:50:45.063555 140326865966848 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.2824857234954834, loss=6.108231544494629
I0428 08:51:48.870165 140326874359552 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3360518217086792, loss=6.148598670959473
I0428 08:52:52.567746 140326865966848 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3676585853099823, loss=6.1433916091918945
I0428 08:53:56.227441 140326874359552 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.31404879689216614, loss=6.168095588684082
I0428 08:54:20.102223 140533484173120 spec.py:298] Evaluating on the training split.
I0428 08:54:27.969842 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 08:54:38.801773 140533484173120 spec.py:326] Evaluating on the test split.
I0428 08:54:40.881311 140533484173120 submission_runner.py:415] Time since start: 12260.71s, 	Step: 18439, 	{'train/accuracy': 0.060885682702064514, 'train/loss': 5.695693016052246, 'validation/accuracy': 0.057819999754428864, 'validation/loss': 5.7586798667907715, 'validation/num_examples': 50000, 'test/accuracy': 0.042900003492832184, 'test/loss': 5.953071594238281, 'test/num_examples': 10000, 'score': 11797.962356805801, 'total_duration': 12260.710004806519, 'accumulated_submission_time': 11797.962356805801, 'accumulated_eval_time': 462.06052017211914, 'accumulated_logging_time': 0.4522097110748291}
I0428 08:54:40.894410 140326865966848 logging_writer.py:48] [18439] accumulated_eval_time=462.060520, accumulated_logging_time=0.452210, accumulated_submission_time=11797.962357, global_step=18439, preemption_count=0, score=11797.962357, test/accuracy=0.042900, test/loss=5.953072, test/num_examples=10000, total_duration=12260.710005, train/accuracy=0.060886, train/loss=5.695693, validation/accuracy=0.057820, validation/loss=5.758680, validation/num_examples=50000
I0428 08:55:20.385748 140326874359552 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2494642585515976, loss=6.074779510498047
I0428 08:56:23.990879 140326865966848 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.2951730191707611, loss=6.186252593994141
I0428 08:57:27.767176 140326874359552 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.29319316148757935, loss=6.165501594543457
I0428 08:58:31.513258 140326865966848 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.2647847831249237, loss=6.16226053237915
I0428 08:59:35.329946 140326874359552 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.24731245636940002, loss=6.174409866333008
I0428 09:00:39.101256 140326865966848 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3302862346172333, loss=6.194439888000488
I0428 09:01:42.943738 140326874359552 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2925507128238678, loss=6.1444621086120605
I0428 09:02:46.498214 140326865966848 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.32042667269706726, loss=6.159886360168457
I0428 09:03:11.012547 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:03:18.969267 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:03:29.280873 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:03:31.349456 140533484173120 submission_runner.py:415] Time since start: 12791.18s, 	Step: 19240, 	{'train/accuracy': 0.0595703125, 'train/loss': 5.711216449737549, 'validation/accuracy': 0.05559999868273735, 'validation/loss': 5.760584831237793, 'validation/num_examples': 50000, 'test/accuracy': 0.03720000013709068, 'test/loss': 5.9653849601745605, 'test/num_examples': 10000, 'score': 12308.060494184494, 'total_duration': 12791.178140163422, 'accumulated_submission_time': 12308.060494184494, 'accumulated_eval_time': 482.3973891735077, 'accumulated_logging_time': 0.4748835563659668}
I0428 09:03:31.361876 140326874359552 logging_writer.py:48] [19240] accumulated_eval_time=482.397389, accumulated_logging_time=0.474884, accumulated_submission_time=12308.060494, global_step=19240, preemption_count=0, score=12308.060494, test/accuracy=0.037200, test/loss=5.965385, test/num_examples=10000, total_duration=12791.178140, train/accuracy=0.059570, train/loss=5.711216, validation/accuracy=0.055600, validation/loss=5.760585, validation/num_examples=50000
I0428 09:04:10.171339 140326865966848 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.28565025329589844, loss=6.177649021148682
I0428 09:05:14.013240 140326874359552 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.29187655448913574, loss=6.11114501953125
I0428 09:06:17.767264 140326865966848 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.29194992780685425, loss=6.140183925628662
I0428 09:07:21.523734 140326874359552 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3065405786037445, loss=6.057365894317627
I0428 09:08:25.181242 140326865966848 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.26825621724128723, loss=6.105828762054443
I0428 09:09:28.872817 140326874359552 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.25451192259788513, loss=6.125763893127441
I0428 09:10:32.510555 140326865966848 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.2680971026420593, loss=6.1954474449157715
I0428 09:11:36.269595 140326874359552 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.23207128047943115, loss=6.051149845123291
I0428 09:12:01.445565 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:12:09.004554 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:12:19.009290 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:12:21.070754 140533484173120 submission_runner.py:415] Time since start: 13320.90s, 	Step: 20041, 	{'train/accuracy': 0.07188695669174194, 'train/loss': 5.578878402709961, 'validation/accuracy': 0.06486000120639801, 'validation/loss': 5.691783428192139, 'validation/num_examples': 50000, 'test/accuracy': 0.04740000143647194, 'test/loss': 5.89654016494751, 'test/num_examples': 10000, 'score': 12818.125413179398, 'total_duration': 13320.899451732635, 'accumulated_submission_time': 12818.125413179398, 'accumulated_eval_time': 502.0225615501404, 'accumulated_logging_time': 0.4961819648742676}
I0428 09:12:21.082084 140326865966848 logging_writer.py:48] [20041] accumulated_eval_time=502.022562, accumulated_logging_time=0.496182, accumulated_submission_time=12818.125413, global_step=20041, preemption_count=0, score=12818.125413, test/accuracy=0.047400, test/loss=5.896540, test/num_examples=10000, total_duration=13320.899452, train/accuracy=0.071887, train/loss=5.578878, validation/accuracy=0.064860, validation/loss=5.691783, validation/num_examples=50000
I0428 09:12:59.386520 140326874359552 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.25478994846343994, loss=6.099740982055664
I0428 09:14:03.388096 140326865966848 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.2770552933216095, loss=6.082715034484863
I0428 09:15:07.164132 140326874359552 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.2616432309150696, loss=6.128625392913818
I0428 09:16:10.816226 140326865966848 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.2432294487953186, loss=6.062763690948486
I0428 09:17:14.565038 140326874359552 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.21477854251861572, loss=6.057884693145752
I0428 09:18:18.461547 140326865966848 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.29162874817848206, loss=6.132572174072266
I0428 09:19:24.706645 140326874359552 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.2778478264808655, loss=6.076486587524414
I0428 09:20:28.621969 140326865966848 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.25189992785453796, loss=6.076836109161377
I0428 09:20:51.312388 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:20:58.879984 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:21:08.745307 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:21:10.848679 140533484173120 submission_runner.py:415] Time since start: 13850.68s, 	Step: 20837, 	{'train/accuracy': 0.06544961780309677, 'train/loss': 5.610964775085449, 'validation/accuracy': 0.06377999484539032, 'validation/loss': 5.6721415519714355, 'validation/num_examples': 50000, 'test/accuracy': 0.04570000246167183, 'test/loss': 5.878154754638672, 'test/num_examples': 10000, 'score': 13328.337257146835, 'total_duration': 13850.677378177643, 'accumulated_submission_time': 13328.337257146835, 'accumulated_eval_time': 521.5588266849518, 'accumulated_logging_time': 0.5159170627593994}
I0428 09:21:10.860868 140326874359552 logging_writer.py:48] [20837] accumulated_eval_time=521.558827, accumulated_logging_time=0.515917, accumulated_submission_time=13328.337257, global_step=20837, preemption_count=0, score=13328.337257, test/accuracy=0.045700, test/loss=5.878155, test/num_examples=10000, total_duration=13850.677378, train/accuracy=0.065450, train/loss=5.610965, validation/accuracy=0.063780, validation/loss=5.672142, validation/num_examples=50000
I0428 09:21:51.645656 140326865966848 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.23533707857131958, loss=6.014866828918457
I0428 09:22:55.438670 140326874359552 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.25982239842414856, loss=6.10051965713501
I0428 09:23:59.267697 140326865966848 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.21743722259998322, loss=6.050692558288574
I0428 09:25:02.955997 140326874359552 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.24615474045276642, loss=6.0828657150268555
I0428 09:26:06.680330 140326865966848 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2470247447490692, loss=6.0892333984375
I0428 09:27:10.406694 140326874359552 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.22224414348602295, loss=6.086549282073975
I0428 09:28:14.170524 140326865966848 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.22568382322788239, loss=6.044912338256836
I0428 09:29:17.941132 140326874359552 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.21280677616596222, loss=6.0357537269592285
I0428 09:29:41.153679 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:29:48.772241 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:29:58.447190 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:30:00.658455 140533484173120 submission_runner.py:415] Time since start: 14380.49s, 	Step: 21638, 	{'train/accuracy': 0.06204161047935486, 'train/loss': 5.676631927490234, 'validation/accuracy': 0.05771999806165695, 'validation/loss': 5.756164073944092, 'validation/num_examples': 50000, 'test/accuracy': 0.042900003492832184, 'test/loss': 5.952383995056152, 'test/num_examples': 10000, 'score': 13838.611527442932, 'total_duration': 14380.48714518547, 'accumulated_submission_time': 13838.611527442932, 'accumulated_eval_time': 541.0635888576508, 'accumulated_logging_time': 0.536177396774292}
I0428 09:30:00.675400 140326865966848 logging_writer.py:48] [21638] accumulated_eval_time=541.063589, accumulated_logging_time=0.536177, accumulated_submission_time=13838.611527, global_step=21638, preemption_count=0, score=13838.611527, test/accuracy=0.042900, test/loss=5.952384, test/num_examples=10000, total_duration=14380.487145, train/accuracy=0.062042, train/loss=5.676632, validation/accuracy=0.057720, validation/loss=5.756164, validation/num_examples=50000
I0428 09:30:40.965070 140326874359552 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.21052983403205872, loss=6.043299674987793
I0428 09:31:44.663734 140326865966848 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.20185798406600952, loss=5.982566833496094
I0428 09:32:48.388098 140326874359552 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.21198409795761108, loss=6.1121602058410645
I0428 09:33:52.117504 140326865966848 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2478274405002594, loss=6.100790500640869
I0428 09:34:55.733277 140326874359552 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2262147217988968, loss=6.000482082366943
I0428 09:35:59.434306 140326865966848 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.23656323552131653, loss=5.965030670166016
I0428 09:37:03.197451 140326874359552 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.2455102801322937, loss=6.052700996398926
I0428 09:38:06.923332 140326865966848 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.21679559350013733, loss=6.073816776275635
I0428 09:38:30.714612 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:38:38.490641 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:38:48.243509 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:38:50.543780 140533484173120 submission_runner.py:415] Time since start: 14910.37s, 	Step: 22439, 	{'train/accuracy': 0.0686383917927742, 'train/loss': 5.606105327606201, 'validation/accuracy': 0.06374000012874603, 'validation/loss': 5.677907943725586, 'validation/num_examples': 50000, 'test/accuracy': 0.04820000380277634, 'test/loss': 5.890989780426025, 'test/num_examples': 10000, 'score': 14348.631863117218, 'total_duration': 14910.372482538223, 'accumulated_submission_time': 14348.631863117218, 'accumulated_eval_time': 560.892737865448, 'accumulated_logging_time': 0.5611984729766846}
I0428 09:38:50.557932 140326874359552 logging_writer.py:48] [22439] accumulated_eval_time=560.892738, accumulated_logging_time=0.561198, accumulated_submission_time=14348.631863, global_step=22439, preemption_count=0, score=14348.631863, test/accuracy=0.048200, test/loss=5.890990, test/num_examples=10000, total_duration=14910.372483, train/accuracy=0.068638, train/loss=5.606105, validation/accuracy=0.063740, validation/loss=5.677908, validation/num_examples=50000
I0428 09:39:30.056880 140326865966848 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.2615411877632141, loss=6.074440956115723
I0428 09:40:33.857391 140326874359552 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.24713653326034546, loss=6.044747352600098
I0428 09:41:37.671514 140326865966848 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.2537946105003357, loss=6.039366245269775
I0428 09:42:41.432191 140326874359552 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.22416740655899048, loss=6.064416885375977
I0428 09:43:45.133243 140326865966848 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.23395666480064392, loss=6.036949634552002
I0428 09:44:48.884189 140326874359552 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.25645631551742554, loss=6.059925556182861
I0428 09:45:52.620904 140326865966848 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.24056784808635712, loss=6.058851718902588
I0428 09:46:56.544330 140326874359552 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2705688774585724, loss=6.077517509460449
I0428 09:47:21.102403 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:47:28.567346 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:47:38.235143 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:47:40.313593 140533484173120 submission_runner.py:415] Time since start: 15440.14s, 	Step: 23240, 	{'train/accuracy': 0.07467713207006454, 'train/loss': 5.53300666809082, 'validation/accuracy': 0.07005999982357025, 'validation/loss': 5.60281229019165, 'validation/num_examples': 50000, 'test/accuracy': 0.048900000751018524, 'test/loss': 5.825078964233398, 'test/num_examples': 10000, 'score': 14859.15718126297, 'total_duration': 15440.142293691635, 'accumulated_submission_time': 14859.15718126297, 'accumulated_eval_time': 580.1039063930511, 'accumulated_logging_time': 0.5845417976379395}
I0428 09:47:40.325060 140326865966848 logging_writer.py:48] [23240] accumulated_eval_time=580.103906, accumulated_logging_time=0.584542, accumulated_submission_time=14859.157181, global_step=23240, preemption_count=0, score=14859.157181, test/accuracy=0.048900, test/loss=5.825079, test/num_examples=10000, total_duration=15440.142294, train/accuracy=0.074677, train/loss=5.533007, validation/accuracy=0.070060, validation/loss=5.602812, validation/num_examples=50000
I0428 09:48:19.120799 140326874359552 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.24166905879974365, loss=6.021313667297363
I0428 09:49:22.743236 140326865966848 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.29703623056411743, loss=6.096744537353516
I0428 09:50:26.521385 140326874359552 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.24399299919605255, loss=6.022186756134033
I0428 09:51:30.392040 140326865966848 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3115938901901245, loss=6.053053855895996
I0428 09:52:34.153632 140326874359552 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.26508888602256775, loss=6.055579662322998
I0428 09:53:37.944666 140326865966848 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.2788693904876709, loss=6.065641403198242
I0428 09:54:41.697927 140326874359552 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.22569455206394196, loss=6.009504318237305
I0428 09:55:45.498018 140326865966848 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2613402307033539, loss=6.077757835388184
I0428 09:56:10.631247 140533484173120 spec.py:298] Evaluating on the training split.
I0428 09:56:18.153762 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 09:56:27.919824 140533484173120 spec.py:326] Evaluating on the test split.
I0428 09:56:29.989998 140533484173120 submission_runner.py:415] Time since start: 15969.82s, 	Step: 24041, 	{'train/accuracy': 0.07308274507522583, 'train/loss': 5.564728736877441, 'validation/accuracy': 0.06545999646186829, 'validation/loss': 5.665772438049316, 'validation/num_examples': 50000, 'test/accuracy': 0.046800002455711365, 'test/loss': 5.874829292297363, 'test/num_examples': 10000, 'score': 15369.445124864578, 'total_duration': 15969.81868672371, 'accumulated_submission_time': 15369.445124864578, 'accumulated_eval_time': 599.4626421928406, 'accumulated_logging_time': 0.6042580604553223}
I0428 09:56:30.001718 140326874359552 logging_writer.py:48] [24041] accumulated_eval_time=599.462642, accumulated_logging_time=0.604258, accumulated_submission_time=15369.445125, global_step=24041, preemption_count=0, score=15369.445125, test/accuracy=0.046800, test/loss=5.874829, test/num_examples=10000, total_duration=15969.818687, train/accuracy=0.073083, train/loss=5.564729, validation/accuracy=0.065460, validation/loss=5.665772, validation/num_examples=50000
I0428 09:57:08.308493 140326865966848 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.2119186520576477, loss=6.027318477630615
I0428 09:58:12.068373 140326874359552 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.23818127810955048, loss=6.0820417404174805
I0428 09:59:15.717409 140326865966848 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.24000805616378784, loss=6.053437232971191
I0428 10:00:19.598209 140326874359552 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.24568569660186768, loss=6.106083393096924
I0428 10:01:23.293398 140326865966848 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.24366456270217896, loss=6.035116672515869
I0428 10:02:26.982084 140326874359552 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.22719424962997437, loss=5.981836318969727
I0428 10:03:30.800700 140326865966848 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.23440273106098175, loss=6.0497941970825195
I0428 10:04:34.611115 140326874359552 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.23413094878196716, loss=6.067279815673828
I0428 10:05:00.510366 140533484173120 spec.py:298] Evaluating on the training split.
I0428 10:05:08.003323 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 10:05:17.825592 140533484173120 spec.py:326] Evaluating on the test split.
I0428 10:05:19.879338 140533484173120 submission_runner.py:415] Time since start: 16499.71s, 	Step: 24842, 	{'train/accuracy': 0.07144849747419357, 'train/loss': 5.594442367553711, 'validation/accuracy': 0.06843999773263931, 'validation/loss': 5.664196968078613, 'validation/num_examples': 50000, 'test/accuracy': 0.047200001776218414, 'test/loss': 5.874866008758545, 'test/num_examples': 10000, 'score': 15879.935659646988, 'total_duration': 16499.708040237427, 'accumulated_submission_time': 15879.935659646988, 'accumulated_eval_time': 618.831595659256, 'accumulated_logging_time': 0.6240572929382324}
I0428 10:05:19.891505 140326865966848 logging_writer.py:48] [24842] accumulated_eval_time=618.831596, accumulated_logging_time=0.624057, accumulated_submission_time=15879.935660, global_step=24842, preemption_count=0, score=15879.935660, test/accuracy=0.047200, test/loss=5.874866, test/num_examples=10000, total_duration=16499.708040, train/accuracy=0.071448, train/loss=5.594442, validation/accuracy=0.068440, validation/loss=5.664197, validation/num_examples=50000
I0428 10:05:57.383180 140326874359552 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.2423357367515564, loss=6.030552864074707
I0428 10:07:01.015947 140326865966848 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2352493554353714, loss=6.088374614715576
I0428 10:08:04.731556 140326874359552 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2730361223220825, loss=6.038523197174072
I0428 10:09:08.519418 140326865966848 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.25861090421676636, loss=6.046441078186035
I0428 10:10:12.103262 140326874359552 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.2576172649860382, loss=5.9962077140808105
I0428 10:11:15.841486 140326865966848 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2813798487186432, loss=5.989302158355713
I0428 10:12:19.635088 140326874359552 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2655157446861267, loss=6.027596950531006
I0428 10:13:23.221097 140326865966848 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2301245629787445, loss=6.020071029663086
I0428 10:13:50.287151 140533484173120 spec.py:298] Evaluating on the training split.
I0428 10:13:57.768077 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 10:14:07.540930 140533484173120 spec.py:326] Evaluating on the test split.
I0428 10:14:09.838251 140533484173120 submission_runner.py:415] Time since start: 17029.67s, 	Step: 25644, 	{'train/accuracy': 0.0694754421710968, 'train/loss': 5.571657657623291, 'validation/accuracy': 0.06604000180959702, 'validation/loss': 5.6602373123168945, 'validation/num_examples': 50000, 'test/accuracy': 0.04670000076293945, 'test/loss': 5.8731842041015625, 'test/num_examples': 10000, 'score': 16390.312838315964, 'total_duration': 17029.666952848434, 'accumulated_submission_time': 16390.312838315964, 'accumulated_eval_time': 638.3826727867126, 'accumulated_logging_time': 0.6442582607269287}
I0428 10:14:09.850312 140326874359552 logging_writer.py:48] [25644] accumulated_eval_time=638.382673, accumulated_logging_time=0.644258, accumulated_submission_time=16390.312838, global_step=25644, preemption_count=0, score=16390.312838, test/accuracy=0.046700, test/loss=5.873184, test/num_examples=10000, total_duration=17029.666953, train/accuracy=0.069475, train/loss=5.571658, validation/accuracy=0.066040, validation/loss=5.660237, validation/num_examples=50000
I0428 10:14:46.174936 140326865966848 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.2539413273334503, loss=6.105247497558594
I0428 10:15:49.815864 140326874359552 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.24884586036205292, loss=6.058938026428223
I0428 10:16:53.512879 140326865966848 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.2700961232185364, loss=6.085720539093018
I0428 10:17:57.374516 140326874359552 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2502635419368744, loss=6.140378475189209
I0428 10:19:01.112929 140326865966848 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.25172802805900574, loss=6.00183629989624
I0428 10:20:04.809165 140326874359552 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.26215049624443054, loss=6.04793643951416
I0428 10:21:08.629524 140326865966848 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.24985240399837494, loss=6.017453193664551
I0428 10:22:12.365408 140326874359552 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.285207599401474, loss=6.095919609069824
I0428 10:22:40.017443 140533484173120 spec.py:298] Evaluating on the training split.
I0428 10:22:47.500206 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 10:22:57.159904 140533484173120 spec.py:326] Evaluating on the test split.
I0428 10:22:59.310260 140533484173120 submission_runner.py:415] Time since start: 17559.14s, 	Step: 26445, 	{'train/accuracy': 0.06740274280309677, 'train/loss': 5.612514972686768, 'validation/accuracy': 0.06350000202655792, 'validation/loss': 5.683044910430908, 'validation/num_examples': 50000, 'test/accuracy': 0.04340000078082085, 'test/loss': 5.896450519561768, 'test/num_examples': 10000, 'score': 16900.460692882538, 'total_duration': 17559.138955116272, 'accumulated_submission_time': 16900.460692882538, 'accumulated_eval_time': 657.6754631996155, 'accumulated_logging_time': 0.6654970645904541}
I0428 10:22:59.323004 140326865966848 logging_writer.py:48] [26445] accumulated_eval_time=657.675463, accumulated_logging_time=0.665497, accumulated_submission_time=16900.460693, global_step=26445, preemption_count=0, score=16900.460693, test/accuracy=0.043400, test/loss=5.896451, test/num_examples=10000, total_duration=17559.138955, train/accuracy=0.067403, train/loss=5.612515, validation/accuracy=0.063500, validation/loss=5.683045, validation/num_examples=50000
I0428 10:23:35.103573 140326874359552 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.25881749391555786, loss=6.076430320739746
I0428 10:24:38.822273 140326865966848 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2240913212299347, loss=6.054381370544434
I0428 10:25:42.531350 140326874359552 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2775329649448395, loss=6.065997123718262
I0428 10:26:46.337670 140326865966848 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.2712976634502411, loss=6.000754356384277
I0428 10:27:50.090790 140326874359552 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2867264151573181, loss=5.992870807647705
I0428 10:28:53.712037 140326865966848 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.28125056624412537, loss=5.993396759033203
I0428 10:29:57.419498 140326874359552 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2833353281021118, loss=6.0603766441345215
I0428 10:31:01.250607 140326865966848 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2886625826358795, loss=5.991701602935791
I0428 10:31:29.594249 140533484173120 spec.py:298] Evaluating on the training split.
I0428 10:31:36.998685 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 10:31:46.929286 140533484173120 spec.py:326] Evaluating on the test split.
I0428 10:31:49.231551 140533484173120 submission_runner.py:415] Time since start: 18089.06s, 	Step: 27246, 	{'train/accuracy': 0.06544961780309677, 'train/loss': 5.626082420349121, 'validation/accuracy': 0.062119998037815094, 'validation/loss': 5.680044174194336, 'validation/num_examples': 50000, 'test/accuracy': 0.04780000075697899, 'test/loss': 5.893544673919678, 'test/num_examples': 10000, 'score': 17410.71038031578, 'total_duration': 18089.06025505066, 'accumulated_submission_time': 17410.71038031578, 'accumulated_eval_time': 677.3127498626709, 'accumulated_logging_time': 0.6892518997192383}
I0428 10:31:49.244617 140326874359552 logging_writer.py:48] [27246] accumulated_eval_time=677.312750, accumulated_logging_time=0.689252, accumulated_submission_time=17410.710380, global_step=27246, preemption_count=0, score=17410.710380, test/accuracy=0.047800, test/loss=5.893545, test/num_examples=10000, total_duration=18089.060255, train/accuracy=0.065450, train/loss=5.626082, validation/accuracy=0.062120, validation/loss=5.680044, validation/num_examples=50000
I0428 10:32:24.259614 140326865966848 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.261279433965683, loss=6.020969390869141
I0428 10:33:28.007864 140326874359552 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.25484779477119446, loss=6.009437561035156
I0428 10:34:31.779745 140326865966848 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2548811733722687, loss=6.040032863616943
I0428 10:35:35.714286 140326874359552 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.293952077627182, loss=6.020541191101074
I0428 10:36:39.445860 140326865966848 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.27577444911003113, loss=6.017921447753906
I0428 10:37:43.187747 140326874359552 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.28932851552963257, loss=6.101550579071045
I0428 10:38:46.987264 140326865966848 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.28857243061065674, loss=5.982513427734375
I0428 10:39:49.681141 140533484173120 spec.py:298] Evaluating on the training split.
I0428 10:39:57.170342 140533484173120 spec.py:310] Evaluating on the validation split.
I0428 10:40:06.876484 140533484173120 spec.py:326] Evaluating on the test split.
I0428 10:40:08.918291 140533484173120 submission_runner.py:415] Time since start: 18588.75s, 	Step: 28000, 	{'train/accuracy': 0.06953523308038712, 'train/loss': 5.565247058868408, 'validation/accuracy': 0.06371999531984329, 'validation/loss': 5.6665754318237305, 'validation/num_examples': 50000, 'test/accuracy': 0.045100003480911255, 'test/loss': 5.875119209289551, 'test/num_examples': 10000, 'score': 17891.129177093506, 'total_duration': 18588.746992588043, 'accumulated_submission_time': 17891.129177093506, 'accumulated_eval_time': 696.549875497818, 'accumulated_logging_time': 0.7108449935913086}
I0428 10:40:08.931568 140326874359552 logging_writer.py:48] [28000] accumulated_eval_time=696.549875, accumulated_logging_time=0.710845, accumulated_submission_time=17891.129177, global_step=28000, preemption_count=0, score=17891.129177, test/accuracy=0.045100, test/loss=5.875119, test/num_examples=10000, total_duration=18588.746993, train/accuracy=0.069535, train/loss=5.565247, validation/accuracy=0.063720, validation/loss=5.666575, validation/num_examples=50000
I0428 10:40:08.951409 140326865966848 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=17891.129177
I0428 10:40:09.235008 140533484173120 checkpoints.py:356] Saving checkpoint at step: 28000
I0428 10:40:10.273246 140533484173120 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1/checkpoint_28000
I0428 10:40:10.292514 140533484173120 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_sam/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0428 10:40:11.031413 140533484173120 submission_runner.py:578] Tuning trial 1/1
I0428 10:40:11.032550 140533484173120 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0428 10:40:11.036631 140533484173120 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011160713620483875, 'train/loss': 6.911971569061279, 'validation/accuracy': 0.0011999999405816197, 'validation/loss': 6.912228107452393, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.912980079650879, 'test/num_examples': 10000, 'score': 62.00210475921631, 'total_duration': 103.55756402015686, 'accumulated_submission_time': 62.00210475921631, 'accumulated_eval_time': 41.5553138256073, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (803, {'train/accuracy': 0.016262754797935486, 'train/loss': 6.4442949295043945, 'validation/accuracy': 0.016699999570846558, 'validation/loss': 6.461048126220703, 'validation/num_examples': 50000, 'test/accuracy': 0.012800000607967377, 'test/loss': 6.544966697692871, 'test/num_examples': 10000, 'score': 572.4845790863037, 'total_duration': 631.1415548324585, 'accumulated_submission_time': 572.4845790863037, 'accumulated_eval_time': 58.62032628059387, 'accumulated_logging_time': 0.025949716567993164, 'global_step': 803, 'preemption_count': 0}), (1609, {'train/accuracy': 0.019909916445612907, 'train/loss': 6.368528842926025, 'validation/accuracy': 0.01735999993979931, 'validation/loss': 6.412729740142822, 'validation/num_examples': 50000, 'test/accuracy': 0.013300000689923763, 'test/loss': 6.5057196617126465, 'test/num_examples': 10000, 'score': 1083.124656677246, 'total_duration': 1158.9416053295135, 'accumulated_submission_time': 1083.124656677246, 'accumulated_eval_time': 75.75317525863647, 'accumulated_logging_time': 0.04309892654418945, 'global_step': 1609, 'preemption_count': 0}), (2414, {'train/accuracy': 0.0207868292927742, 'train/loss': 6.289496421813965, 'validation/accuracy': 0.020719999447464943, 'validation/loss': 6.320240497589111, 'validation/num_examples': 50000, 'test/accuracy': 0.016200000420212746, 'test/loss': 6.419797420501709, 'test/num_examples': 10000, 'score': 1593.2700130939484, 'total_duration': 1686.177038192749, 'accumulated_submission_time': 1593.2700130939484, 'accumulated_eval_time': 92.81522536277771, 'accumulated_logging_time': 0.06109261512756348, 'global_step': 2414, 'preemption_count': 0}), (3220, {'train/accuracy': 0.021843111142516136, 'train/loss': 6.263242244720459, 'validation/accuracy': 0.021699998527765274, 'validation/loss': 6.3031487464904785, 'validation/num_examples': 50000, 'test/accuracy': 0.017900001257658005, 'test/loss': 6.402946949005127, 'test/num_examples': 10000, 'score': 2103.3170070648193, 'total_duration': 2213.2133100032806, 'accumulated_submission_time': 2103.3170070648193, 'accumulated_eval_time': 109.77695035934448, 'accumulated_logging_time': 0.07847166061401367, 'global_step': 3220, 'preemption_count': 0}), (4026, {'train/accuracy': 0.027722416445612907, 'train/loss': 6.159175395965576, 'validation/accuracy': 0.02582000009715557, 'validation/loss': 6.22192907333374, 'validation/num_examples': 50000, 'test/accuracy': 0.020100001245737076, 'test/loss': 6.3230695724487305, 'test/num_examples': 10000, 'score': 2613.856209039688, 'total_duration': 2740.4978930950165, 'accumulated_submission_time': 2613.856209039688, 'accumulated_eval_time': 126.49249243736267, 'accumulated_logging_time': 0.09806632995605469, 'global_step': 4026, 'preemption_count': 0}), (4831, {'train/accuracy': 0.030731823295354843, 'train/loss': 6.113183975219727, 'validation/accuracy': 0.02850000001490116, 'validation/loss': 6.154013156890869, 'validation/num_examples': 50000, 'test/accuracy': 0.023500001057982445, 'test/loss': 6.274304389953613, 'test/num_examples': 10000, 'score': 3123.910295009613, 'total_duration': 3269.046510219574, 'accumulated_submission_time': 3123.910295009613, 'accumulated_eval_time': 144.95940136909485, 'accumulated_logging_time': 0.11565542221069336, 'global_step': 4831, 'preemption_count': 0}), (5635, {'train/accuracy': 0.035893652588129044, 'train/loss': 6.062813758850098, 'validation/accuracy': 0.03313999995589256, 'validation/loss': 6.1202073097229, 'validation/num_examples': 50000, 'test/accuracy': 0.024000000208616257, 'test/loss': 6.2388458251953125, 'test/num_examples': 10000, 'score': 3634.0667254924774, 'total_duration': 3795.9594719409943, 'accumulated_submission_time': 3634.0667254924774, 'accumulated_eval_time': 161.68845772743225, 'accumulated_logging_time': 0.13326549530029297, 'global_step': 5635, 'preemption_count': 0}), (6437, {'train/accuracy': 0.03252550959587097, 'train/loss': 6.047701835632324, 'validation/accuracy': 0.031019998714327812, 'validation/loss': 6.085089683532715, 'validation/num_examples': 50000, 'test/accuracy': 0.021900001913309097, 'test/loss': 6.220163822174072, 'test/num_examples': 10000, 'score': 4144.09125995636, 'total_duration': 4323.310355424881, 'accumulated_submission_time': 4144.09125995636, 'accumulated_eval_time': 178.9858214855194, 'accumulated_logging_time': 0.15191984176635742, 'global_step': 6437, 'preemption_count': 0}), (7237, {'train/accuracy': 0.034259404987096786, 'train/loss': 6.004663944244385, 'validation/accuracy': 0.032999999821186066, 'validation/loss': 6.06771183013916, 'validation/num_examples': 50000, 'test/accuracy': 0.023600000888109207, 'test/loss': 6.210371017456055, 'test/num_examples': 10000, 'score': 4654.4230444431305, 'total_duration': 4850.716812372208, 'accumulated_submission_time': 4654.4230444431305, 'accumulated_eval_time': 196.03224968910217, 'accumulated_logging_time': 0.16998982429504395, 'global_step': 7237, 'preemption_count': 0}), (8034, {'train/accuracy': 0.03698979690670967, 'train/loss': 5.984800338745117, 'validation/accuracy': 0.03271999955177307, 'validation/loss': 6.074314117431641, 'validation/num_examples': 50000, 'test/accuracy': 0.025800000876188278, 'test/loss': 6.2109055519104, 'test/num_examples': 10000, 'score': 5164.507261514664, 'total_duration': 5377.759849786758, 'accumulated_submission_time': 5164.507261514664, 'accumulated_eval_time': 212.96221208572388, 'accumulated_logging_time': 0.18876123428344727, 'global_step': 8034, 'preemption_count': 0}), (8833, {'train/accuracy': 0.0315290167927742, 'train/loss': 6.086880207061768, 'validation/accuracy': 0.031019998714327812, 'validation/loss': 6.123008728027344, 'validation/num_examples': 50000, 'test/accuracy': 0.02330000139772892, 'test/loss': 6.239058971405029, 'test/num_examples': 10000, 'score': 5674.5861604213715, 'total_duration': 5906.265658378601, 'accumulated_submission_time': 5674.5861604213715, 'accumulated_eval_time': 231.35989785194397, 'accumulated_logging_time': 0.20791983604431152, 'global_step': 8833, 'preemption_count': 0}), (9630, {'train/accuracy': 0.02666613459587097, 'train/loss': 6.182265758514404, 'validation/accuracy': 0.024779999628663063, 'validation/loss': 6.245525360107422, 'validation/num_examples': 50000, 'test/accuracy': 0.018400000408291817, 'test/loss': 6.35684871673584, 'test/num_examples': 10000, 'score': 6185.047253847122, 'total_duration': 6433.961688280106, 'accumulated_submission_time': 6185.047253847122, 'accumulated_eval_time': 248.56680941581726, 'accumulated_logging_time': 0.2258617877960205, 'global_step': 9630, 'preemption_count': 0}), (10427, {'train/accuracy': 0.04043765738606453, 'train/loss': 5.97617769241333, 'validation/accuracy': 0.03709999844431877, 'validation/loss': 6.033982753753662, 'validation/num_examples': 50000, 'test/accuracy': 0.026100000366568565, 'test/loss': 6.1760783195495605, 'test/num_examples': 10000, 'score': 6695.172476053238, 'total_duration': 6961.282447814941, 'accumulated_submission_time': 6695.172476053238, 'accumulated_eval_time': 265.7336368560791, 'accumulated_logging_time': 0.2440931797027588, 'global_step': 10427, 'preemption_count': 0}), (11227, {'train/accuracy': 0.046097736805677414, 'train/loss': 5.911163330078125, 'validation/accuracy': 0.043880000710487366, 'validation/loss': 5.948585033416748, 'validation/num_examples': 50000, 'test/accuracy': 0.0340999998152256, 'test/loss': 6.103226661682129, 'test/num_examples': 10000, 'score': 7205.703100681305, 'total_duration': 7489.142204999924, 'accumulated_submission_time': 7205.703100681305, 'accumulated_eval_time': 283.02033495903015, 'accumulated_logging_time': 0.2762458324432373, 'global_step': 11227, 'preemption_count': 0}), (12029, {'train/accuracy': 0.048728473484516144, 'train/loss': 5.868307590484619, 'validation/accuracy': 0.04332000017166138, 'validation/loss': 5.952307224273682, 'validation/num_examples': 50000, 'test/accuracy': 0.03070000186562538, 'test/loss': 6.1137824058532715, 'test/num_examples': 10000, 'score': 7716.018489599228, 'total_duration': 8016.677796125412, 'accumulated_submission_time': 7716.018489599228, 'accumulated_eval_time': 300.2121286392212, 'accumulated_logging_time': 0.29454588890075684, 'global_step': 12029, 'preemption_count': 0}), (12831, {'train/accuracy': 0.04209183529019356, 'train/loss': 5.92786979675293, 'validation/accuracy': 0.04031999781727791, 'validation/loss': 5.987668037414551, 'validation/num_examples': 50000, 'test/accuracy': 0.029000001028180122, 'test/loss': 6.1429619789123535, 'test/num_examples': 10000, 'score': 8226.322344779968, 'total_duration': 8546.05297613144, 'accumulated_submission_time': 8226.322344779968, 'accumulated_eval_time': 319.2535312175751, 'accumulated_logging_time': 0.3139650821685791, 'global_step': 12831, 'preemption_count': 0}), (13633, {'train/accuracy': 0.053073182702064514, 'train/loss': 5.790137767791748, 'validation/accuracy': 0.050939999520778656, 'validation/loss': 5.8576178550720215, 'validation/num_examples': 50000, 'test/accuracy': 0.03490000218153, 'test/loss': 6.027804374694824, 'test/num_examples': 10000, 'score': 8736.588426828384, 'total_duration': 9075.492210388184, 'accumulated_submission_time': 8736.588426828384, 'accumulated_eval_time': 338.3957381248474, 'accumulated_logging_time': 0.33441686630249023, 'global_step': 13633, 'preemption_count': 0}), (14434, {'train/accuracy': 0.04404496029019356, 'train/loss': 5.880850315093994, 'validation/accuracy': 0.04335999861359596, 'validation/loss': 5.93757963180542, 'validation/num_examples': 50000, 'test/accuracy': 0.029100000858306885, 'test/loss': 6.105411052703857, 'test/num_examples': 10000, 'score': 9247.00608420372, 'total_duration': 9605.818126678467, 'accumulated_submission_time': 9247.00608420372, 'accumulated_eval_time': 358.2733910083771, 'accumulated_logging_time': 0.3546912670135498, 'global_step': 14434, 'preemption_count': 0}), (15235, {'train/accuracy': 0.058254942297935486, 'train/loss': 5.768155574798584, 'validation/accuracy': 0.05377999693155289, 'validation/loss': 5.826873779296875, 'validation/num_examples': 50000, 'test/accuracy': 0.03880000114440918, 'test/loss': 6.007757186889648, 'test/num_examples': 10000, 'score': 9757.166797161102, 'total_duration': 10136.798737049103, 'accumulated_submission_time': 9757.166797161102, 'accumulated_eval_time': 379.06422686576843, 'accumulated_logging_time': 0.37354445457458496, 'global_step': 15235, 'preemption_count': 0}), (16036, {'train/accuracy': 0.054587848484516144, 'train/loss': 5.759245872497559, 'validation/accuracy': 0.05031999945640564, 'validation/loss': 5.85275936126709, 'validation/num_examples': 50000, 'test/accuracy': 0.03360000252723694, 'test/loss': 6.036982536315918, 'test/num_examples': 10000, 'score': 10267.48531126976, 'total_duration': 10667.687936067581, 'accumulated_submission_time': 10267.48531126976, 'accumulated_eval_time': 399.6047580242157, 'accumulated_logging_time': 0.393521785736084, 'global_step': 16036, 'preemption_count': 0}), (16837, {'train/accuracy': 0.06499122828245163, 'train/loss': 5.6734843254089355, 'validation/accuracy': 0.061159998178482056, 'validation/loss': 5.727813720703125, 'validation/num_examples': 50000, 'test/accuracy': 0.04320000112056732, 'test/loss': 5.928177833557129, 'test/num_examples': 10000, 'score': 10777.680795907974, 'total_duration': 11198.571491479874, 'accumulated_submission_time': 10777.680795907974, 'accumulated_eval_time': 420.26382660865784, 'accumulated_logging_time': 0.4122769832611084, 'global_step': 16837, 'preemption_count': 0}), (17638, {'train/accuracy': 0.05255500599741936, 'train/loss': 5.7794599533081055, 'validation/accuracy': 0.04907999932765961, 'validation/loss': 5.836338043212891, 'validation/num_examples': 50000, 'test/accuracy': 0.036500003188848495, 'test/loss': 6.022609710693359, 'test/num_examples': 10000, 'score': 11287.716339588165, 'total_duration': 11729.654824733734, 'accumulated_submission_time': 11287.716339588165, 'accumulated_eval_time': 441.28146386146545, 'accumulated_logging_time': 0.43208956718444824, 'global_step': 17638, 'preemption_count': 0}), (18439, {'train/accuracy': 0.060885682702064514, 'train/loss': 5.695693016052246, 'validation/accuracy': 0.057819999754428864, 'validation/loss': 5.7586798667907715, 'validation/num_examples': 50000, 'test/accuracy': 0.042900003492832184, 'test/loss': 5.953071594238281, 'test/num_examples': 10000, 'score': 11797.962356805801, 'total_duration': 12260.710004806519, 'accumulated_submission_time': 11797.962356805801, 'accumulated_eval_time': 462.06052017211914, 'accumulated_logging_time': 0.4522097110748291, 'global_step': 18439, 'preemption_count': 0}), (19240, {'train/accuracy': 0.0595703125, 'train/loss': 5.711216449737549, 'validation/accuracy': 0.05559999868273735, 'validation/loss': 5.760584831237793, 'validation/num_examples': 50000, 'test/accuracy': 0.03720000013709068, 'test/loss': 5.9653849601745605, 'test/num_examples': 10000, 'score': 12308.060494184494, 'total_duration': 12791.178140163422, 'accumulated_submission_time': 12308.060494184494, 'accumulated_eval_time': 482.3973891735077, 'accumulated_logging_time': 0.4748835563659668, 'global_step': 19240, 'preemption_count': 0}), (20041, {'train/accuracy': 0.07188695669174194, 'train/loss': 5.578878402709961, 'validation/accuracy': 0.06486000120639801, 'validation/loss': 5.691783428192139, 'validation/num_examples': 50000, 'test/accuracy': 0.04740000143647194, 'test/loss': 5.89654016494751, 'test/num_examples': 10000, 'score': 12818.125413179398, 'total_duration': 13320.899451732635, 'accumulated_submission_time': 12818.125413179398, 'accumulated_eval_time': 502.0225615501404, 'accumulated_logging_time': 0.4961819648742676, 'global_step': 20041, 'preemption_count': 0}), (20837, {'train/accuracy': 0.06544961780309677, 'train/loss': 5.610964775085449, 'validation/accuracy': 0.06377999484539032, 'validation/loss': 5.6721415519714355, 'validation/num_examples': 50000, 'test/accuracy': 0.04570000246167183, 'test/loss': 5.878154754638672, 'test/num_examples': 10000, 'score': 13328.337257146835, 'total_duration': 13850.677378177643, 'accumulated_submission_time': 13328.337257146835, 'accumulated_eval_time': 521.5588266849518, 'accumulated_logging_time': 0.5159170627593994, 'global_step': 20837, 'preemption_count': 0}), (21638, {'train/accuracy': 0.06204161047935486, 'train/loss': 5.676631927490234, 'validation/accuracy': 0.05771999806165695, 'validation/loss': 5.756164073944092, 'validation/num_examples': 50000, 'test/accuracy': 0.042900003492832184, 'test/loss': 5.952383995056152, 'test/num_examples': 10000, 'score': 13838.611527442932, 'total_duration': 14380.48714518547, 'accumulated_submission_time': 13838.611527442932, 'accumulated_eval_time': 541.0635888576508, 'accumulated_logging_time': 0.536177396774292, 'global_step': 21638, 'preemption_count': 0}), (22439, {'train/accuracy': 0.0686383917927742, 'train/loss': 5.606105327606201, 'validation/accuracy': 0.06374000012874603, 'validation/loss': 5.677907943725586, 'validation/num_examples': 50000, 'test/accuracy': 0.04820000380277634, 'test/loss': 5.890989780426025, 'test/num_examples': 10000, 'score': 14348.631863117218, 'total_duration': 14910.372482538223, 'accumulated_submission_time': 14348.631863117218, 'accumulated_eval_time': 560.892737865448, 'accumulated_logging_time': 0.5611984729766846, 'global_step': 22439, 'preemption_count': 0}), (23240, {'train/accuracy': 0.07467713207006454, 'train/loss': 5.53300666809082, 'validation/accuracy': 0.07005999982357025, 'validation/loss': 5.60281229019165, 'validation/num_examples': 50000, 'test/accuracy': 0.048900000751018524, 'test/loss': 5.825078964233398, 'test/num_examples': 10000, 'score': 14859.15718126297, 'total_duration': 15440.142293691635, 'accumulated_submission_time': 14859.15718126297, 'accumulated_eval_time': 580.1039063930511, 'accumulated_logging_time': 0.5845417976379395, 'global_step': 23240, 'preemption_count': 0}), (24041, {'train/accuracy': 0.07308274507522583, 'train/loss': 5.564728736877441, 'validation/accuracy': 0.06545999646186829, 'validation/loss': 5.665772438049316, 'validation/num_examples': 50000, 'test/accuracy': 0.046800002455711365, 'test/loss': 5.874829292297363, 'test/num_examples': 10000, 'score': 15369.445124864578, 'total_duration': 15969.81868672371, 'accumulated_submission_time': 15369.445124864578, 'accumulated_eval_time': 599.4626421928406, 'accumulated_logging_time': 0.6042580604553223, 'global_step': 24041, 'preemption_count': 0}), (24842, {'train/accuracy': 0.07144849747419357, 'train/loss': 5.594442367553711, 'validation/accuracy': 0.06843999773263931, 'validation/loss': 5.664196968078613, 'validation/num_examples': 50000, 'test/accuracy': 0.047200001776218414, 'test/loss': 5.874866008758545, 'test/num_examples': 10000, 'score': 15879.935659646988, 'total_duration': 16499.708040237427, 'accumulated_submission_time': 15879.935659646988, 'accumulated_eval_time': 618.831595659256, 'accumulated_logging_time': 0.6240572929382324, 'global_step': 24842, 'preemption_count': 0}), (25644, {'train/accuracy': 0.0694754421710968, 'train/loss': 5.571657657623291, 'validation/accuracy': 0.06604000180959702, 'validation/loss': 5.6602373123168945, 'validation/num_examples': 50000, 'test/accuracy': 0.04670000076293945, 'test/loss': 5.8731842041015625, 'test/num_examples': 10000, 'score': 16390.312838315964, 'total_duration': 17029.666952848434, 'accumulated_submission_time': 16390.312838315964, 'accumulated_eval_time': 638.3826727867126, 'accumulated_logging_time': 0.6442582607269287, 'global_step': 25644, 'preemption_count': 0}), (26445, {'train/accuracy': 0.06740274280309677, 'train/loss': 5.612514972686768, 'validation/accuracy': 0.06350000202655792, 'validation/loss': 5.683044910430908, 'validation/num_examples': 50000, 'test/accuracy': 0.04340000078082085, 'test/loss': 5.896450519561768, 'test/num_examples': 10000, 'score': 16900.460692882538, 'total_duration': 17559.138955116272, 'accumulated_submission_time': 16900.460692882538, 'accumulated_eval_time': 657.6754631996155, 'accumulated_logging_time': 0.6654970645904541, 'global_step': 26445, 'preemption_count': 0}), (27246, {'train/accuracy': 0.06544961780309677, 'train/loss': 5.626082420349121, 'validation/accuracy': 0.062119998037815094, 'validation/loss': 5.680044174194336, 'validation/num_examples': 50000, 'test/accuracy': 0.04780000075697899, 'test/loss': 5.893544673919678, 'test/num_examples': 10000, 'score': 17410.71038031578, 'total_duration': 18089.06025505066, 'accumulated_submission_time': 17410.71038031578, 'accumulated_eval_time': 677.3127498626709, 'accumulated_logging_time': 0.6892518997192383, 'global_step': 27246, 'preemption_count': 0}), (28000, {'train/accuracy': 0.06953523308038712, 'train/loss': 5.565247058868408, 'validation/accuracy': 0.06371999531984329, 'validation/loss': 5.6665754318237305, 'validation/num_examples': 50000, 'test/accuracy': 0.045100003480911255, 'test/loss': 5.875119209289551, 'test/num_examples': 10000, 'score': 17891.129177093506, 'total_duration': 18588.746992588043, 'accumulated_submission_time': 17891.129177093506, 'accumulated_eval_time': 696.549875497818, 'accumulated_logging_time': 0.7108449935913086, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0428 10:40:11.036770 140533484173120 submission_runner.py:581] Timing: 17891.129177093506
I0428 10:40:11.036820 140533484173120 submission_runner.py:582] ====================
I0428 10:40:11.036971 140533484173120 submission_runner.py:645] Final imagenet_resnet score: 17891.129177093506
