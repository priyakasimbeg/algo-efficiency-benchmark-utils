python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_04-28-2023-18-20-31.log
I0428 18:20:51.848360 140061639792448 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax.
I0428 18:20:51.929979 140061639792448 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 18:20:52.746213 140061639792448 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0428 18:20:52.746846 140061639792448 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 18:20:52.750415 140061639792448 submission_runner.py:538] Using RNG seed 2900718351
I0428 18:20:55.358781 140061639792448 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 18:20:55.358985 140061639792448 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1.
I0428 18:20:55.360633 140061639792448 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1/hparams.json.
I0428 18:20:55.482602 140061639792448 submission_runner.py:241] Initializing dataset.
I0428 18:20:55.482808 140061639792448 submission_runner.py:248] Initializing model.
I0428 18:21:12.314204 140061639792448 submission_runner.py:258] Initializing optimizer.
I0428 18:21:13.928598 140061639792448 submission_runner.py:265] Initializing metrics bundle.
I0428 18:21:13.928782 140061639792448 submission_runner.py:282] Initializing checkpoint and logger.
I0428 18:21:13.929708 140061639792448 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0428 18:21:13.929965 140061639792448 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 18:21:13.930027 140061639792448 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 18:21:14.847168 140061639792448 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0428 18:21:14.848100 140061639792448 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0428 18:21:14.854861 140061639792448 submission_runner.py:318] Starting training loop.
I0428 18:21:15.056694 140061639792448 input_pipeline.py:20] Loading split = train-clean-100
I0428 18:21:15.090229 140061639792448 input_pipeline.py:20] Loading split = train-clean-360
I0428 18:21:15.411497 140061639792448 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0428 18:22:47.899109 139885625202432 logging_writer.py:48] [0] global_step=0, grad_norm=20.639698028564453, loss=33.87603759765625
I0428 18:22:47.927749 140061639792448 spec.py:298] Evaluating on the training split.
I0428 18:22:48.060068 140061639792448 input_pipeline.py:20] Loading split = train-clean-100
I0428 18:22:48.087935 140061639792448 input_pipeline.py:20] Loading split = train-clean-360
I0428 18:22:48.378952 140061639792448 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0428 18:24:16.643319 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 18:24:16.742422 140061639792448 input_pipeline.py:20] Loading split = dev-clean
I0428 18:24:16.747724 140061639792448 input_pipeline.py:20] Loading split = dev-other
I0428 18:25:09.775370 140061639792448 spec.py:326] Evaluating on the test split.
I0428 18:25:09.881677 140061639792448 input_pipeline.py:20] Loading split = test-clean
I0428 18:25:45.659136 140061639792448 submission_runner.py:415] Time since start: 270.80s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.85629, dtype=float32), 'train/wer': 4.029194344376653, 'validation/ctc_loss': DeviceArray(30.648535, dtype=float32), 'validation/wer': 3.6268270798560525, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.78677, dtype=float32), 'test/wer': 3.9157678792679707, 'test/num_examples': 2472, 'score': 93.07267475128174, 'total_duration': 270.80288434028625, 'accumulated_submission_time': 93.07267475128174, 'accumulated_eval_time': 177.73003268241882, 'accumulated_logging_time': 0}
I0428 18:25:45.678740 139882890524416 logging_writer.py:48] [1] accumulated_eval_time=177.730033, accumulated_logging_time=0, accumulated_submission_time=93.072675, global_step=1, preemption_count=0, score=93.072675, test/ctc_loss=30.78676986694336, test/num_examples=2472, test/wer=3.915768, total_duration=270.802884, train/ctc_loss=31.856290817260742, train/wer=4.029194, validation/ctc_loss=30.648534774780273, validation/num_examples=5348, validation/wer=3.626827
I0428 18:28:44.011984 139886137878272 logging_writer.py:48] [100] global_step=100, grad_norm=6.64358377456665, loss=8.861847877502441
I0428 18:30:43.594979 139886146270976 logging_writer.py:48] [200] global_step=200, grad_norm=2.9711856842041016, loss=6.259636878967285
I0428 18:32:40.054567 139886137878272 logging_writer.py:48] [300] global_step=300, grad_norm=1.646239161491394, loss=5.846845626831055
I0428 18:34:37.641116 139886146270976 logging_writer.py:48] [400] global_step=400, grad_norm=0.5000394582748413, loss=5.809309005737305
I0428 18:36:36.634262 139886137878272 logging_writer.py:48] [500] global_step=500, grad_norm=1.52940833568573, loss=5.680545330047607
I0428 18:38:34.770868 139886146270976 logging_writer.py:48] [600] global_step=600, grad_norm=0.8297433257102966, loss=5.517910957336426
I0428 18:40:32.527248 139886137878272 logging_writer.py:48] [700] global_step=700, grad_norm=0.6884230971336365, loss=5.284542560577393
I0428 18:42:30.916256 139886146270976 logging_writer.py:48] [800] global_step=800, grad_norm=1.5628612041473389, loss=4.71435022354126
I0428 18:44:30.113949 139886137878272 logging_writer.py:48] [900] global_step=900, grad_norm=2.0796356201171875, loss=4.113208770751953
I0428 18:46:29.155311 139886146270976 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.655905842781067, loss=3.7746851444244385
I0428 18:48:30.654149 139884736018176 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4790096282958984, loss=3.511906147003174
I0428 18:50:30.194885 139884727625472 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.0711710453033447, loss=3.3268823623657227
I0428 18:52:26.732340 139884736018176 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.1377406120300293, loss=3.2011609077453613
I0428 18:54:22.364463 139884727625472 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.4070115089416504, loss=3.1117730140686035
I0428 18:56:18.730500 139884736018176 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4774346351623535, loss=2.9766061305999756
I0428 18:58:14.932692 139884727625472 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.6590161323547363, loss=2.9402048587799072
I0428 19:00:10.123839 139884736018176 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.392381429672241, loss=2.856606960296631
I0428 19:02:07.670584 139884727625472 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.200172185897827, loss=2.781355619430542
I0428 19:04:02.735270 139884736018176 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.4916677474975586, loss=2.7336575984954834
I0428 19:05:45.825042 140061639792448 spec.py:298] Evaluating on the training split.
I0428 19:06:15.790402 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 19:06:51.620526 140061639792448 spec.py:326] Evaluating on the test split.
I0428 19:07:10.026827 140061639792448 submission_runner.py:415] Time since start: 2755.17s, 	Step: 1991, 	{'train/ctc_loss': DeviceArray(6.004011, dtype=float32), 'train/wer': 0.9407221272476365, 'validation/ctc_loss': DeviceArray(5.91407, dtype=float32), 'validation/wer': 0.8941909714517265, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.8331447, dtype=float32), 'test/wer': 0.8968781102106311, 'test/num_examples': 2472, 'score': 2493.182278394699, 'total_duration': 2755.168666601181, 'accumulated_submission_time': 2493.182278394699, 'accumulated_eval_time': 261.92860889434814, 'accumulated_logging_time': 0.03133702278137207}
I0428 19:07:10.047168 139887288407808 logging_writer.py:48] [1991] accumulated_eval_time=261.928609, accumulated_logging_time=0.031337, accumulated_submission_time=2493.182278, global_step=1991, preemption_count=0, score=2493.182278, test/ctc_loss=5.833144664764404, test/num_examples=2472, test/wer=0.896878, total_duration=2755.168667, train/ctc_loss=6.004011154174805, train/wer=0.940722, validation/ctc_loss=5.914070129394531, validation/num_examples=5348, validation/wer=0.894191
I0428 19:07:21.965147 139887280015104 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.9845378398895264, loss=2.669299840927124
I0428 19:09:23.703181 139886633047808 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.220120906829834, loss=2.5700066089630127
I0428 19:11:21.009635 139886624655104 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.180972099304199, loss=2.502791166305542
I0428 19:13:17.964392 139886633047808 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.369748115539551, loss=2.442213773727417
I0428 19:15:17.674817 139886624655104 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.12577223777771, loss=2.3982651233673096
I0428 19:17:17.331655 139886633047808 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.2706899642944336, loss=2.514644145965576
I0428 19:19:14.447771 139886624655104 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.3993589878082275, loss=2.317242383956909
I0428 19:21:10.010536 139886633047808 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.6658804416656494, loss=2.3682758808135986
I0428 19:23:04.745417 139886624655104 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.7815961837768555, loss=2.276684522628784
I0428 19:24:59.418510 139886633047808 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.210700511932373, loss=2.2933497428894043
I0428 19:26:57.839250 139886624655104 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.8518667221069336, loss=2.3141894340515137
I0428 19:28:56.361084 139884736018176 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.360896348953247, loss=2.2342536449432373
I0428 19:30:52.922685 139884727625472 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.4719138145446777, loss=2.265077590942383
I0428 19:32:50.532738 139884736018176 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.4465038776397705, loss=2.189239263534546
I0428 19:34:48.506525 139884727625472 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.5383594036102295, loss=2.2002077102661133
I0428 19:36:44.162042 139884736018176 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.147674560546875, loss=2.143551826477051
I0428 19:38:42.714629 139884727625472 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.3896780014038086, loss=2.072756052017212
I0428 19:40:40.157719 139884736018176 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.810416340827942, loss=2.1603379249572754
I0428 19:42:35.686511 139884727625472 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.3312673568725586, loss=2.2138214111328125
I0428 19:44:32.021809 139884736018176 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.802041530609131, loss=2.1274266242980957
I0428 19:46:27.288516 139884727625472 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.043107748031616, loss=2.0797791481018066
I0428 19:47:11.065134 140061639792448 spec.py:298] Evaluating on the training split.
I0428 19:47:49.423364 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 19:48:27.834117 140061639792448 spec.py:326] Evaluating on the test split.
I0428 19:48:47.463129 140061639792448 submission_runner.py:415] Time since start: 5252.60s, 	Step: 4039, 	{'train/ctc_loss': DeviceArray(0.93676394, dtype=float32), 'train/wer': 0.2997789201715366, 'validation/ctc_loss': DeviceArray(1.3764915, dtype=float32), 'validation/wer': 0.3651168848710552, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.9658739, dtype=float32), 'test/wer': 0.295472548900128, 'test/num_examples': 2472, 'score': 4894.158715009689, 'total_duration': 5252.604964017868, 'accumulated_submission_time': 4894.158715009689, 'accumulated_eval_time': 358.3233675956726, 'accumulated_logging_time': 0.06658935546875}
I0428 19:48:47.484767 139884736018176 logging_writer.py:48] [4039] accumulated_eval_time=358.323368, accumulated_logging_time=0.066589, accumulated_submission_time=4894.158715, global_step=4039, preemption_count=0, score=4894.158715, test/ctc_loss=0.9658738970756531, test/num_examples=2472, test/wer=0.295473, total_duration=5252.604964, train/ctc_loss=0.9367639422416687, train/wer=0.299779, validation/ctc_loss=1.3764915466308594, validation/num_examples=5348, validation/wer=0.365117
I0428 19:50:00.726336 139884727625472 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.496567726135254, loss=2.060124635696411
I0428 19:52:04.186345 139887288407808 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.090094804763794, loss=2.064365863800049
I0428 19:54:02.748184 139887280015104 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.2462871074676514, loss=1.9877654314041138
I0428 19:55:58.842525 139887288407808 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.2699034214019775, loss=2.1116840839385986
I0428 19:57:54.784444 139887280015104 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.9926891326904297, loss=2.0829222202301025
I0428 19:59:50.126069 139887288407808 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.3892972469329834, loss=2.0760838985443115
I0428 20:01:47.332022 139887280015104 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.4100801944732666, loss=2.0211071968078613
I0428 20:03:42.586894 139887288407808 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.7624127864837646, loss=1.988599419593811
I0428 20:05:37.487600 139887280015104 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.139803171157837, loss=2.043341875076294
I0428 20:07:34.190170 139887288407808 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.545896291732788, loss=1.9762694835662842
I0428 20:09:33.131318 139887280015104 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.3221893310546875, loss=1.9917616844177246
I0428 20:11:35.093920 139887288407808 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.0277435779571533, loss=1.9826295375823975
I0428 20:13:32.887158 139887280015104 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.497403860092163, loss=2.005627393722534
I0428 20:15:28.300682 139887288407808 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.231761455535889, loss=1.9344072341918945
I0428 20:17:24.682872 139887280015104 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.027592658996582, loss=1.9502484798431396
I0428 20:19:22.175699 139887288407808 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.2412142753601074, loss=1.9349969625473022
I0428 20:21:17.347028 139887280015104 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.8345108032226562, loss=2.027390956878662
I0428 20:23:12.123834 139887288407808 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.680264472961426, loss=2.031850576400757
I0428 20:25:08.506407 139887280015104 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.791290760040283, loss=1.9288735389709473
I0428 20:27:03.684391 139887288407808 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.343779563903809, loss=1.9510380029678345
I0428 20:28:48.071500 140061639792448 spec.py:298] Evaluating on the training split.
I0428 20:29:27.833937 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 20:30:07.064150 140061639792448 spec.py:326] Evaluating on the test split.
I0428 20:30:27.157726 140061639792448 submission_runner.py:415] Time since start: 7752.30s, 	Step: 6090, 	{'train/ctc_loss': DeviceArray(0.66968435, dtype=float32), 'train/wer': 0.2260174686760388, 'validation/ctc_loss': DeviceArray(1.1332943, dtype=float32), 'validation/wer': 0.3084544954606412, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7472522, dtype=float32), 'test/wer': 0.23591899742042938, 'test/num_examples': 2472, 'score': 7294.7044150829315, 'total_duration': 7752.299315452576, 'accumulated_submission_time': 7294.7044150829315, 'accumulated_eval_time': 457.4061357975006, 'accumulated_logging_time': 0.10357666015625}
I0428 20:30:27.178315 139887288407808 logging_writer.py:48] [6090] accumulated_eval_time=457.406136, accumulated_logging_time=0.103577, accumulated_submission_time=7294.704415, global_step=6090, preemption_count=0, score=7294.704415, test/ctc_loss=0.7472522258758545, test/num_examples=2472, test/wer=0.235919, total_duration=7752.299315, train/ctc_loss=0.6696843504905701, train/wer=0.226017, validation/ctc_loss=1.1332943439483643, validation/num_examples=5348, validation/wer=0.308454
I0428 20:30:40.148410 139887280015104 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.2019457817077637, loss=1.994349479675293
I0428 20:32:42.319335 139887288407808 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.7932767868041992, loss=1.9456443786621094
I0428 20:34:37.647335 139887280015104 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.6487693786621094, loss=1.921458125114441
I0428 20:36:32.986698 139887288407808 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.950864553451538, loss=1.8519508838653564
I0428 20:38:28.399024 139887280015104 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.5710597038269043, loss=1.9323582649230957
I0428 20:40:24.324745 139887288407808 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.1527163982391357, loss=1.9638807773590088
I0428 20:42:23.360529 139887280015104 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.358766555786133, loss=1.948747992515564
I0428 20:44:23.435410 139887288407808 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.9355578422546387, loss=1.868525505065918
I0428 20:46:21.485519 139887280015104 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.1509194374084473, loss=1.9411081075668335
I0428 20:48:17.313714 139887288407808 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.969454288482666, loss=1.9059807062149048
I0428 20:50:15.299478 139887280015104 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.3247523307800293, loss=1.9153307676315308
I0428 20:52:11.609062 139887288407808 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.430525779724121, loss=1.9286693334579468
I0428 20:54:09.697313 139887288407808 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.452115297317505, loss=1.8577502965927124
I0428 20:56:04.057888 139887280015104 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.2973968982696533, loss=1.813211441040039
I0428 20:58:02.109862 139887288407808 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.2112202644348145, loss=1.8638519048690796
I0428 20:59:57.198241 139887280015104 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.9580917358398438, loss=1.910402774810791
I0428 21:01:55.897178 139887288407808 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.8002381324768066, loss=1.8818191289901733
I0428 21:03:53.449671 139887280015104 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.0083467960357666, loss=1.791696310043335
I0428 21:05:49.610064 139887288407808 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.3184924125671387, loss=1.8815025091171265
I0428 21:07:46.731410 139887280015104 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.074559450149536, loss=1.8299459218978882
I0428 21:09:42.494200 139887288407808 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.744149923324585, loss=1.8940322399139404
I0428 21:10:28.225647 140061639792448 spec.py:298] Evaluating on the training split.
I0428 21:11:08.277959 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 21:11:47.267983 140061639792448 spec.py:326] Evaluating on the test split.
I0428 21:12:07.446158 140061639792448 submission_runner.py:415] Time since start: 10252.59s, 	Step: 8141, 	{'train/ctc_loss': DeviceArray(0.5481206, dtype=float32), 'train/wer': 0.18412592026306696, 'validation/ctc_loss': DeviceArray(0.943212, dtype=float32), 'validation/wer': 0.26281970882497663, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.58737403, dtype=float32), 'test/wer': 0.18711027156581966, 'test/num_examples': 2472, 'score': 9695.712436199188, 'total_duration': 10252.588168859482, 'accumulated_submission_time': 9695.712436199188, 'accumulated_eval_time': 556.6235840320587, 'accumulated_logging_time': 0.1378180980682373}
I0428 21:12:07.468195 139886566487808 logging_writer.py:48] [8141] accumulated_eval_time=556.623584, accumulated_logging_time=0.137818, accumulated_submission_time=9695.712436, global_step=8141, preemption_count=0, score=9695.712436, test/ctc_loss=0.5873740315437317, test/num_examples=2472, test/wer=0.187110, total_duration=10252.588169, train/ctc_loss=0.5481206178665161, train/wer=0.184126, validation/ctc_loss=0.9432119727134705, validation/num_examples=5348, validation/wer=0.262820
I0428 21:13:16.181119 139886558095104 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.6401476860046387, loss=1.902504563331604
I0428 21:15:14.472173 139886238807808 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.1656494140625, loss=1.8008677959442139
I0428 21:17:10.249594 139886230415104 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.060532331466675, loss=1.8821440935134888
I0428 21:19:08.505460 139886238807808 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.669493198394775, loss=1.8606135845184326
I0428 21:21:06.069132 139886230415104 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.539388418197632, loss=1.8030803203582764
I0428 21:23:03.696807 139886238807808 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.563477039337158, loss=1.8296892642974854
I0428 21:25:02.427055 139886230415104 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.557732343673706, loss=1.8398411273956299
I0428 21:27:01.564762 139886238807808 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.8068315982818604, loss=1.8017914295196533
I0428 21:28:58.762281 139886230415104 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.8696129322052, loss=1.799598217010498
I0428 21:30:57.951727 139886238807808 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.260207176208496, loss=1.8321361541748047
I0428 21:32:55.869387 139886230415104 logging_writer.py:48] [9200] global_step=9200, grad_norm=4.8309431076049805, loss=1.8045614957809448
I0428 21:34:54.990616 139886238807808 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.9555444717407227, loss=1.7380788326263428
I0428 21:36:53.630704 139886230415104 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.7678375244140625, loss=1.85185706615448
I0428 21:38:48.672900 139886238807808 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.914663076400757, loss=1.780612826347351
I0428 21:40:43.341107 139886230415104 logging_writer.py:48] [9600] global_step=9600, grad_norm=5.67963981628418, loss=1.824735403060913
I0428 21:42:41.413022 139886238807808 logging_writer.py:48] [9700] global_step=9700, grad_norm=4.064334869384766, loss=1.772839903831482
I0428 21:44:40.855732 139886230415104 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.9489083290100098, loss=1.8078062534332275
I0428 21:46:40.443473 139886238807808 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.594882965087891, loss=1.7383124828338623
I0428 21:48:40.428750 139886230415104 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.469034194946289, loss=1.7972102165222168
I0428 21:50:39.232590 139886238807808 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.214512348175049, loss=1.7487176656723022
I0428 21:52:07.879452 140061639792448 spec.py:298] Evaluating on the training split.
I0428 21:52:47.130613 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 21:53:26.729989 140061639792448 spec.py:326] Evaluating on the test split.
I0428 21:53:46.753691 140061639792448 submission_runner.py:415] Time since start: 12751.90s, 	Step: 10177, 	{'train/ctc_loss': DeviceArray(0.4652067, dtype=float32), 'train/wer': 0.15882602361434256, 'validation/ctc_loss': DeviceArray(0.8326948, dtype=float32), 'validation/wer': 0.23388551746760702, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.50558466, dtype=float32), 'test/wer': 0.16295980338390917, 'test/num_examples': 2472, 'score': 12096.083129882812, 'total_duration': 12751.89566707611, 'accumulated_submission_time': 12096.083129882812, 'accumulated_eval_time': 655.4947185516357, 'accumulated_logging_time': 0.17429399490356445}
I0428 21:53:46.775630 139886996567808 logging_writer.py:48] [10177] accumulated_eval_time=655.494719, accumulated_logging_time=0.174294, accumulated_submission_time=12096.083130, global_step=10177, preemption_count=0, score=12096.083130, test/ctc_loss=0.5055846571922302, test/num_examples=2472, test/wer=0.162960, total_duration=12751.895667, train/ctc_loss=0.46520671248435974, train/wer=0.158826, validation/ctc_loss=0.8326948285102844, validation/num_examples=5348, validation/wer=0.233886
I0428 21:54:15.307132 139886988175104 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.6859941482543945, loss=1.789745569229126
I0428 21:56:14.373920 139886996567808 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.313962459564209, loss=1.769547700881958
I0428 21:58:12.702396 139886988175104 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.056845664978027, loss=1.7570134401321411
I0428 22:00:11.183575 139886996567808 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.751053333282471, loss=1.757076621055603
I0428 22:02:09.114566 139886988175104 logging_writer.py:48] [10600] global_step=10600, grad_norm=5.118637561798096, loss=1.7138984203338623
I0428 22:04:06.184215 139886996567808 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.491889476776123, loss=1.7825706005096436
I0428 22:06:01.395631 139886988175104 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.0338728427886963, loss=1.8015962839126587
I0428 22:07:56.956072 139886996567808 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.853890419006348, loss=1.8697357177734375
I0428 22:09:55.266566 139886988175104 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.9324898719787598, loss=1.8033689260482788
I0428 22:11:52.924180 139886996567808 logging_writer.py:48] [11100] global_step=11100, grad_norm=5.463080406188965, loss=1.7791844606399536
I0428 22:13:49.253175 139886988175104 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.226902961730957, loss=1.688389539718628
I0428 22:15:44.329030 139886996567808 logging_writer.py:48] [11300] global_step=11300, grad_norm=6.7337236404418945, loss=1.733597755432129
I0428 22:17:45.182340 139886341207808 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.038479328155518, loss=1.677552342414856
I0428 22:19:43.146510 139886332815104 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.040010929107666, loss=1.7777667045593262
I0428 22:21:41.431252 139886341207808 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.288266658782959, loss=1.8420912027359009
I0428 22:23:42.060317 139886332815104 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.9956462383270264, loss=1.714653491973877
I0428 22:25:42.986778 139886341207808 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.6477155685424805, loss=1.7401164770126343
I0428 22:27:37.842382 139886332815104 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.9471123218536377, loss=1.7160394191741943
I0428 22:29:32.271646 139886341207808 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.958242416381836, loss=1.7746814489364624
I0428 22:31:27.470971 139886332815104 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.8943493366241455, loss=1.7218718528747559
I0428 22:33:22.817986 139886341207808 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.480234622955322, loss=1.694050908088684
I0428 22:33:47.329505 140061639792448 spec.py:298] Evaluating on the training split.
I0428 22:34:27.985151 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 22:35:07.278969 140061639792448 spec.py:326] Evaluating on the test split.
I0428 22:35:27.588004 140061639792448 submission_runner.py:415] Time since start: 15252.73s, 	Step: 12222, 	{'train/ctc_loss': DeviceArray(0.4684616, dtype=float32), 'train/wer': 0.15828136419651, 'validation/ctc_loss': DeviceArray(0.81943816, dtype=float32), 'validation/wer': 0.23257339675250124, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.49178073, dtype=float32), 'test/wer': 0.16031929803180794, 'test/num_examples': 2472, 'score': 14496.595435142517, 'total_duration': 15252.729882240295, 'accumulated_submission_time': 14496.595435142517, 'accumulated_eval_time': 755.7500383853912, 'accumulated_logging_time': 0.21164608001708984}
I0428 22:35:27.609817 139886126167808 logging_writer.py:48] [12222] accumulated_eval_time=755.750038, accumulated_logging_time=0.211646, accumulated_submission_time=14496.595435, global_step=12222, preemption_count=0, score=14496.595435, test/ctc_loss=0.49178072810173035, test/num_examples=2472, test/wer=0.160319, total_duration=15252.729882, train/ctc_loss=0.4684616029262543, train/wer=0.158281, validation/ctc_loss=0.8194381594657898, validation/num_examples=5348, validation/wer=0.232573
I0428 22:37:01.557286 139886117775104 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.4999213218688965, loss=1.7879730463027954
I0428 22:39:02.222625 139886126167808 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.5829334259033203, loss=1.70875084400177
I0428 22:41:00.000335 139886117775104 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.698155879974365, loss=1.7424938678741455
I0428 22:42:59.635930 139886126167808 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.478973865509033, loss=1.7921719551086426
I0428 22:44:56.843304 139886117775104 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.435588836669922, loss=1.7380623817443848
I0428 22:46:53.560547 139886126167808 logging_writer.py:48] [12800] global_step=12800, grad_norm=7.694789886474609, loss=1.764905571937561
I0428 22:48:50.638946 139886117775104 logging_writer.py:48] [12900] global_step=12900, grad_norm=4.31956148147583, loss=1.6947542428970337
I0428 22:50:48.031519 139886126167808 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.118621349334717, loss=1.6602041721343994
I0428 22:52:44.309313 139886117775104 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.273738861083984, loss=1.6473314762115479
I0428 22:54:42.429082 139886126167808 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.111040115356445, loss=1.711139440536499
I0428 22:56:41.728364 139886117775104 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.847175121307373, loss=1.7478389739990234
I0428 22:58:43.973809 139886126167808 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.0264482498168945, loss=1.685077428817749
I0428 23:00:40.116954 139886117775104 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.338089942932129, loss=1.6700021028518677
I0428 23:02:37.862281 139886126167808 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.439406394958496, loss=1.691151738166809
I0428 23:04:32.483927 139886117775104 logging_writer.py:48] [13700] global_step=13700, grad_norm=6.60475492477417, loss=1.7457773685455322
I0428 23:06:28.722143 139886126167808 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.6285996437072754, loss=1.67524254322052
I0428 23:08:26.093087 139886117775104 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.8359498977661133, loss=1.63828444480896
I0428 23:10:26.339833 139886126167808 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.365982532501221, loss=1.6685606241226196
I0428 23:12:25.356092 139886117775104 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.963109493255615, loss=1.674777865409851
I0428 23:14:24.292711 139886126167808 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.304401159286499, loss=1.736795425415039
I0428 23:15:28.371566 140061639792448 spec.py:298] Evaluating on the training split.
I0428 23:16:08.367467 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 23:16:47.576594 140061639792448 spec.py:326] Evaluating on the test split.
I0428 23:17:08.665632 140061639792448 submission_runner.py:415] Time since start: 17753.81s, 	Step: 14255, 	{'train/ctc_loss': DeviceArray(0.42258728, dtype=float32), 'train/wer': 0.14134920128475104, 'validation/ctc_loss': DeviceArray(0.77475804, dtype=float32), 'validation/wer': 0.21971268415517756, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46195853, dtype=float32), 'test/wer': 0.1483151544695631, 'test/num_examples': 2472, 'score': 16897.314243793488, 'total_duration': 17753.80739212036, 'accumulated_submission_time': 16897.314243793488, 'accumulated_eval_time': 856.0407919883728, 'accumulated_logging_time': 0.2496347427368164}
I0428 23:17:08.688173 139886126167808 logging_writer.py:48] [14255] accumulated_eval_time=856.040792, accumulated_logging_time=0.249635, accumulated_submission_time=16897.314244, global_step=14255, preemption_count=0, score=16897.314244, test/ctc_loss=0.46195852756500244, test/num_examples=2472, test/wer=0.148315, total_duration=17753.807392, train/ctc_loss=0.4225872755050659, train/wer=0.141349, validation/ctc_loss=0.7747580409049988, validation/num_examples=5348, validation/wer=0.219713
I0428 23:18:01.727447 139886117775104 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.595426321029663, loss=1.712544322013855
I0428 23:19:58.068338 139886126167808 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.6061246395111084, loss=1.658564805984497
I0428 23:21:59.130764 139886126167808 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.253326416015625, loss=1.7045137882232666
I0428 23:23:54.615230 139886117775104 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.570734977722168, loss=1.7329615354537964
I0428 23:25:54.110891 139886126167808 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.347949743270874, loss=1.7009049654006958
I0428 23:27:54.194997 139886117775104 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.223576545715332, loss=1.6959929466247559
I0428 23:29:54.285017 139886126167808 logging_writer.py:48] [14900] global_step=14900, grad_norm=7.3156633377075195, loss=1.7144582271575928
I0428 23:31:50.927675 139886117775104 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.901000499725342, loss=1.6848788261413574
I0428 23:33:45.978935 139886126167808 logging_writer.py:48] [15100] global_step=15100, grad_norm=5.479201316833496, loss=1.678951621055603
I0428 23:35:41.004979 139886117775104 logging_writer.py:48] [15200] global_step=15200, grad_norm=7.4302825927734375, loss=1.7128612995147705
I0428 23:37:37.529982 139886126167808 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.7166829109191895, loss=1.7622132301330566
I0428 23:39:37.200239 139886117775104 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.7374796867370605, loss=1.707414984703064
I0428 23:41:35.334255 139886126167808 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.982060194015503, loss=1.6408915519714355
I0428 23:43:32.633780 139886117775104 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.3735809326171875, loss=1.6702853441238403
I0428 23:45:31.866578 139886126167808 logging_writer.py:48] [15700] global_step=15700, grad_norm=6.8598551750183105, loss=1.669485092163086
I0428 23:47:29.084846 139886117775104 logging_writer.py:48] [15800] global_step=15800, grad_norm=7.011843204498291, loss=1.7125647068023682
I0428 23:49:27.443506 139886126167808 logging_writer.py:48] [15900] global_step=15900, grad_norm=5.104798316955566, loss=1.7304280996322632
I0428 23:51:22.616253 140061639792448 spec.py:298] Evaluating on the training split.
I0428 23:52:03.087351 140061639792448 spec.py:310] Evaluating on the validation split.
I0428 23:52:42.692694 140061639792448 spec.py:326] Evaluating on the test split.
I0428 23:53:03.041507 140061639792448 submission_runner.py:415] Time since start: 19908.18s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.37088537, dtype=float32), 'train/wer': 0.13010218674432514, 'validation/ctc_loss': DeviceArray(0.75285023, dtype=float32), 'validation/wer': 0.2143098341518008, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44666612, dtype=float32), 'test/wer': 0.14472000487477912, 'test/num_examples': 2472, 'score': 18951.20355820656, 'total_duration': 19908.184634685516, 'accumulated_submission_time': 18951.20355820656, 'accumulated_eval_time': 956.464102268219, 'accumulated_logging_time': 0.2884972095489502}
I0428 23:53:03.063934 139886341207808 logging_writer.py:48] [16000] accumulated_eval_time=956.464102, accumulated_logging_time=0.288497, accumulated_submission_time=18951.203558, global_step=16000, preemption_count=0, score=18951.203558, test/ctc_loss=0.4466661214828491, test/num_examples=2472, test/wer=0.144720, total_duration=19908.184635, train/ctc_loss=0.37088537216186523, train/wer=0.130102, validation/ctc_loss=0.7528502345085144, validation/num_examples=5348, validation/wer=0.214310
I0428 23:53:03.087875 139886332815104 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18951.203558
I0428 23:53:03.210886 140061639792448 checkpoints.py:356] Saving checkpoint at step: 16000
I0428 23:53:03.543911 140061639792448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0428 23:53:03.550950 140061639792448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_adafactor/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0428 23:53:04.870431 140061639792448 submission_runner.py:578] Tuning trial 1/1
I0428 23:53:04.870670 140061639792448 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0428 23:53:04.880410 140061639792448 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.85629, dtype=float32), 'train/wer': 4.029194344376653, 'validation/ctc_loss': DeviceArray(30.648535, dtype=float32), 'validation/wer': 3.6268270798560525, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.78677, dtype=float32), 'test/wer': 3.9157678792679707, 'test/num_examples': 2472, 'score': 93.07267475128174, 'total_duration': 270.80288434028625, 'accumulated_submission_time': 93.07267475128174, 'accumulated_eval_time': 177.73003268241882, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1991, {'train/ctc_loss': DeviceArray(6.004011, dtype=float32), 'train/wer': 0.9407221272476365, 'validation/ctc_loss': DeviceArray(5.91407, dtype=float32), 'validation/wer': 0.8941909714517265, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.8331447, dtype=float32), 'test/wer': 0.8968781102106311, 'test/num_examples': 2472, 'score': 2493.182278394699, 'total_duration': 2755.168666601181, 'accumulated_submission_time': 2493.182278394699, 'accumulated_eval_time': 261.92860889434814, 'accumulated_logging_time': 0.03133702278137207, 'global_step': 1991, 'preemption_count': 0}), (4039, {'train/ctc_loss': DeviceArray(0.93676394, dtype=float32), 'train/wer': 0.2997789201715366, 'validation/ctc_loss': DeviceArray(1.3764915, dtype=float32), 'validation/wer': 0.3651168848710552, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.9658739, dtype=float32), 'test/wer': 0.295472548900128, 'test/num_examples': 2472, 'score': 4894.158715009689, 'total_duration': 5252.604964017868, 'accumulated_submission_time': 4894.158715009689, 'accumulated_eval_time': 358.3233675956726, 'accumulated_logging_time': 0.06658935546875, 'global_step': 4039, 'preemption_count': 0}), (6090, {'train/ctc_loss': DeviceArray(0.66968435, dtype=float32), 'train/wer': 0.2260174686760388, 'validation/ctc_loss': DeviceArray(1.1332943, dtype=float32), 'validation/wer': 0.3084544954606412, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7472522, dtype=float32), 'test/wer': 0.23591899742042938, 'test/num_examples': 2472, 'score': 7294.7044150829315, 'total_duration': 7752.299315452576, 'accumulated_submission_time': 7294.7044150829315, 'accumulated_eval_time': 457.4061357975006, 'accumulated_logging_time': 0.10357666015625, 'global_step': 6090, 'preemption_count': 0}), (8141, {'train/ctc_loss': DeviceArray(0.5481206, dtype=float32), 'train/wer': 0.18412592026306696, 'validation/ctc_loss': DeviceArray(0.943212, dtype=float32), 'validation/wer': 0.26281970882497663, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.58737403, dtype=float32), 'test/wer': 0.18711027156581966, 'test/num_examples': 2472, 'score': 9695.712436199188, 'total_duration': 10252.588168859482, 'accumulated_submission_time': 9695.712436199188, 'accumulated_eval_time': 556.6235840320587, 'accumulated_logging_time': 0.1378180980682373, 'global_step': 8141, 'preemption_count': 0}), (10177, {'train/ctc_loss': DeviceArray(0.4652067, dtype=float32), 'train/wer': 0.15882602361434256, 'validation/ctc_loss': DeviceArray(0.8326948, dtype=float32), 'validation/wer': 0.23388551746760702, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.50558466, dtype=float32), 'test/wer': 0.16295980338390917, 'test/num_examples': 2472, 'score': 12096.083129882812, 'total_duration': 12751.89566707611, 'accumulated_submission_time': 12096.083129882812, 'accumulated_eval_time': 655.4947185516357, 'accumulated_logging_time': 0.17429399490356445, 'global_step': 10177, 'preemption_count': 0}), (12222, {'train/ctc_loss': DeviceArray(0.4684616, dtype=float32), 'train/wer': 0.15828136419651, 'validation/ctc_loss': DeviceArray(0.81943816, dtype=float32), 'validation/wer': 0.23257339675250124, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.49178073, dtype=float32), 'test/wer': 0.16031929803180794, 'test/num_examples': 2472, 'score': 14496.595435142517, 'total_duration': 15252.729882240295, 'accumulated_submission_time': 14496.595435142517, 'accumulated_eval_time': 755.7500383853912, 'accumulated_logging_time': 0.21164608001708984, 'global_step': 12222, 'preemption_count': 0}), (14255, {'train/ctc_loss': DeviceArray(0.42258728, dtype=float32), 'train/wer': 0.14134920128475104, 'validation/ctc_loss': DeviceArray(0.77475804, dtype=float32), 'validation/wer': 0.21971268415517756, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46195853, dtype=float32), 'test/wer': 0.1483151544695631, 'test/num_examples': 2472, 'score': 16897.314243793488, 'total_duration': 17753.80739212036, 'accumulated_submission_time': 16897.314243793488, 'accumulated_eval_time': 856.0407919883728, 'accumulated_logging_time': 0.2496347427368164, 'global_step': 14255, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.37088537, dtype=float32), 'train/wer': 0.13010218674432514, 'validation/ctc_loss': DeviceArray(0.75285023, dtype=float32), 'validation/wer': 0.2143098341518008, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44666612, dtype=float32), 'test/wer': 0.14472000487477912, 'test/num_examples': 2472, 'score': 18951.20355820656, 'total_duration': 19908.184634685516, 'accumulated_submission_time': 18951.20355820656, 'accumulated_eval_time': 956.464102268219, 'accumulated_logging_time': 0.2884972095489502, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0428 23:53:04.880624 140061639792448 submission_runner.py:581] Timing: 18951.20355820656
I0428 23:53:04.880678 140061639792448 submission_runner.py:582] ====================
I0428 23:53:04.881479 140061639792448 submission_runner.py:645] Final librispeech_deepspeech score: 18951.20355820656
