python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_04-28-2023-22-08-26.log
I0428 22:08:48.554431 140516526462784 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax.
I0428 22:08:48.627716 140516526462784 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 22:08:49.532173 140516526462784 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0428 22:08:49.532889 140516526462784 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 22:08:49.536949 140516526462784 submission_runner.py:538] Using RNG seed 2681195052
I0428 22:08:52.238584 140516526462784 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 22:08:52.238802 140516526462784 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1.
I0428 22:08:52.239035 140516526462784 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1/hparams.json.
I0428 22:08:52.362546 140516526462784 submission_runner.py:241] Initializing dataset.
I0428 22:08:52.375456 140516526462784 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 22:08:52.382853 140516526462784 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 22:08:52.383005 140516526462784 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 22:08:52.642035 140516526462784 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 22:08:59.288545 140516526462784 submission_runner.py:248] Initializing model.
I0428 22:09:10.496611 140516526462784 submission_runner.py:258] Initializing optimizer.
I0428 22:09:11.148588 140516526462784 submission_runner.py:265] Initializing metrics bundle.
I0428 22:09:11.148794 140516526462784 submission_runner.py:282] Initializing checkpoint and logger.
I0428 22:09:11.149675 140516526462784 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0428 22:09:12.044810 140516526462784 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1/meta_data_0.json.
I0428 22:09:12.045882 140516526462784 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1/flags_0.json.
I0428 22:09:12.050792 140516526462784 submission_runner.py:318] Starting training loop.
I0428 22:10:23.857501 140335623689984 logging_writer.py:48] [0] global_step=0, grad_norm=0.33199143409729004, loss=6.907756805419922
I0428 22:10:23.873554 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:10:23.880072 140516526462784 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 22:10:23.886473 140516526462784 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 22:10:23.886584 140516526462784 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 22:10:23.948217 140516526462784 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 22:10:43.643098 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:10:43.652400 140516526462784 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0428 22:10:43.680154 140516526462784 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0428 22:10:43.680501 140516526462784 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0428 22:10:43.736221 140516526462784 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0428 22:11:04.569967 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:11:04.576938 140516526462784 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 22:11:04.581804 140516526462784 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0428 22:11:04.614765 140516526462784 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0428 22:11:15.575561 140516526462784 submission_runner.py:415] Time since start: 123.52s, 	Step: 1, 	{'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 71.82260036468506, 'total_duration': 123.52470660209656, 'accumulated_submission_time': 71.82260036468506, 'accumulated_eval_time': 51.70195937156677, 'accumulated_logging_time': 0}
I0428 22:11:15.591480 140285745018624 logging_writer.py:48] [1] accumulated_eval_time=51.701959, accumulated_logging_time=0, accumulated_submission_time=71.822600, global_step=1, preemption_count=0, score=71.822600, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=123.524707, train/accuracy=0.000859, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0428 22:12:36.293212 140335103342336 logging_writer.py:48] [100] global_step=100, grad_norm=0.3248506188392639, loss=6.907756328582764
I0428 22:13:19.986580 140335111735040 logging_writer.py:48] [200] global_step=200, grad_norm=0.3322920501232147, loss=6.907756328582764
I0428 22:14:03.880155 140335103342336 logging_writer.py:48] [300] global_step=300, grad_norm=0.36156168580055237, loss=6.907756328582764
I0428 22:14:47.630446 140335111735040 logging_writer.py:48] [400] global_step=400, grad_norm=0.3543384373188019, loss=6.907756805419922
I0428 22:15:31.358464 140335103342336 logging_writer.py:48] [500] global_step=500, grad_norm=0.3994106352329254, loss=6.9077558517456055
I0428 22:16:15.073314 140335111735040 logging_writer.py:48] [600] global_step=600, grad_norm=0.4136463701725006, loss=6.9077558517456055
I0428 22:16:58.780807 140335103342336 logging_writer.py:48] [700] global_step=700, grad_norm=0.37779107689857483, loss=6.9077558517456055
I0428 22:17:42.650429 140335111735040 logging_writer.py:48] [800] global_step=800, grad_norm=0.4533711373806, loss=6.907755374908447
I0428 22:18:15.913100 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:18:22.400234 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:18:29.361626 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:18:31.198260 140516526462784 submission_runner.py:415] Time since start: 559.15s, 	Step: 877, 	{'train/accuracy': 0.0016992187593132257, 'train/loss': 6.90775203704834, 'validation/accuracy': 0.0018599999602884054, 'validation/loss': 6.907751560211182, 'validation/num_examples': 50000, 'test/accuracy': 0.0020000000949949026, 'test/loss': 6.9077534675598145, 'test/num_examples': 10000, 'score': 492.1216480731964, 'total_duration': 559.1473796367645, 'accumulated_submission_time': 492.1216480731964, 'accumulated_eval_time': 66.98708009719849, 'accumulated_logging_time': 0.02552485466003418}
I0428 22:18:31.208872 140286072178432 logging_writer.py:48] [877] accumulated_eval_time=66.987080, accumulated_logging_time=0.025525, accumulated_submission_time=492.121648, global_step=877, preemption_count=0, score=492.121648, test/accuracy=0.002000, test/loss=6.907753, test/num_examples=10000, total_duration=559.147380, train/accuracy=0.001699, train/loss=6.907752, validation/accuracy=0.001860, validation/loss=6.907752, validation/num_examples=50000
I0428 22:18:41.954609 140286080571136 logging_writer.py:48] [900] global_step=900, grad_norm=0.3942895531654358, loss=6.907752990722656
I0428 22:19:25.866319 140286072178432 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.47792062163352966, loss=6.907754898071289
I0428 22:20:09.533739 140286080571136 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4489038586616516, loss=6.907755374908447
I0428 22:20:53.590347 140286072178432 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3989839255809784, loss=6.907754421234131
I0428 22:21:37.330004 140286080571136 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5004926919937134, loss=6.9077534675598145
I0428 22:22:20.967818 140286072178432 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.50972580909729, loss=6.907752990722656
I0428 22:23:04.645659 140286080571136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5330466628074646, loss=6.907752513885498
I0428 22:23:48.397919 140286072178432 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4363563656806946, loss=6.90775203704834
I0428 22:24:32.214932 140286080571136 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.43098899722099304, loss=6.907751083374023
I0428 22:25:15.886758 140286072178432 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.4521835446357727, loss=6.907752990722656
I0428 22:25:31.590927 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:25:38.102454 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:25:44.912445 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:25:46.641906 140516526462784 submission_runner.py:415] Time since start: 994.59s, 	Step: 1837, 	{'train/accuracy': 0.0027734374161809683, 'train/loss': 6.907746315002441, 'validation/accuracy': 0.002899999963119626, 'validation/loss': 6.9077467918396, 'validation/num_examples': 50000, 'test/accuracy': 0.003100000089034438, 'test/loss': 6.907748699188232, 'test/num_examples': 10000, 'score': 912.474556684494, 'total_duration': 994.5910148620605, 'accumulated_submission_time': 912.474556684494, 'accumulated_eval_time': 82.03802394866943, 'accumulated_logging_time': 0.051262617111206055}
I0428 22:25:46.653786 140286080571136 logging_writer.py:48] [1837] accumulated_eval_time=82.038024, accumulated_logging_time=0.051263, accumulated_submission_time=912.474557, global_step=1837, preemption_count=0, score=912.474557, test/accuracy=0.003100, test/loss=6.907749, test/num_examples=10000, total_duration=994.591015, train/accuracy=0.002773, train/loss=6.907746, validation/accuracy=0.002900, validation/loss=6.907747, validation/num_examples=50000
I0428 22:26:15.171870 140286072178432 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.46836549043655396, loss=6.90775203704834
I0428 22:26:58.818880 140286080571136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.521866500377655, loss=6.907751083374023
I0428 22:27:42.525719 140286072178432 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.543141782283783, loss=6.907749176025391
I0428 22:28:26.481638 140286080571136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5492912530899048, loss=6.907748222351074
I0428 22:29:10.285577 140286072178432 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5531402826309204, loss=6.907747745513916
I0428 22:29:54.093260 140286080571136 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4709666669368744, loss=6.907749176025391
I0428 22:30:38.175622 140286072178432 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5126109719276428, loss=6.907747745513916
I0428 22:31:22.177222 140286080571136 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5154576897621155, loss=6.907747745513916
I0428 22:32:05.925218 140286072178432 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.49240049719810486, loss=6.907745361328125
I0428 22:32:46.944797 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:32:53.438855 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:33:00.411518 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:33:02.143978 140516526462784 submission_runner.py:415] Time since start: 1430.09s, 	Step: 2795, 	{'train/accuracy': 0.004765625111758709, 'train/loss': 6.907737731933594, 'validation/accuracy': 0.004780000075697899, 'validation/loss': 6.907736778259277, 'validation/num_examples': 50000, 'test/accuracy': 0.004600000102072954, 'test/loss': 6.90773868560791, 'test/num_examples': 10000, 'score': 1332.738971233368, 'total_duration': 1430.093086719513, 'accumulated_submission_time': 1332.738971233368, 'accumulated_eval_time': 97.23717474937439, 'accumulated_logging_time': 0.07564115524291992}
I0428 22:33:02.154563 140286080571136 logging_writer.py:48] [2795] accumulated_eval_time=97.237175, accumulated_logging_time=0.075641, accumulated_submission_time=1332.738971, global_step=2795, preemption_count=0, score=1332.738971, test/accuracy=0.004600, test/loss=6.907739, test/num_examples=10000, total_duration=1430.093087, train/accuracy=0.004766, train/loss=6.907738, validation/accuracy=0.004780, validation/loss=6.907737, validation/num_examples=50000
I0428 22:33:05.263980 140286072178432 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.4913524091243744, loss=6.907745361328125
I0428 22:33:49.138741 140286080571136 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5712956190109253, loss=6.907742023468018
I0428 22:34:33.208335 140286072178432 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5738900303840637, loss=6.907739639282227
I0428 22:35:17.204033 140286080571136 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5726080536842346, loss=6.90773868560791
I0428 22:36:01.404694 140286072178432 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5701147317886353, loss=6.907738208770752
I0428 22:36:45.299007 140286080571136 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.47965434193611145, loss=6.907742023468018
I0428 22:37:29.213427 140286072178432 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5788125395774841, loss=6.907735347747803
I0428 22:38:13.194058 140286080571136 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5742786526679993, loss=6.907732009887695
I0428 22:38:57.221532 140286072178432 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.46013379096984863, loss=6.907737731933594
I0428 22:39:41.233280 140286080571136 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.49507448077201843, loss=6.9077348709106445
I0428 22:40:02.453036 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:40:08.972848 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:40:15.752232 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:40:17.489144 140516526462784 submission_runner.py:415] Time since start: 1865.44s, 	Step: 3749, 	{'train/accuracy': 0.005253906361758709, 'train/loss': 6.90771484375, 'validation/accuracy': 0.004720000084489584, 'validation/loss': 6.907716274261475, 'validation/num_examples': 50000, 'test/accuracy': 0.004100000020116568, 'test/loss': 6.90772008895874, 'test/num_examples': 10000, 'score': 1753.0104467868805, 'total_duration': 1865.4382455348969, 'accumulated_submission_time': 1753.0104467868805, 'accumulated_eval_time': 112.27321934700012, 'accumulated_logging_time': 0.09949588775634766}
I0428 22:40:17.504028 140286072178432 logging_writer.py:48] [3749] accumulated_eval_time=112.273219, accumulated_logging_time=0.099496, accumulated_submission_time=1753.010447, global_step=3749, preemption_count=0, score=1753.010447, test/accuracy=0.004100, test/loss=6.907720, test/num_examples=10000, total_duration=1865.438246, train/accuracy=0.005254, train/loss=6.907715, validation/accuracy=0.004720, validation/loss=6.907716, validation/num_examples=50000
I0428 22:40:40.812636 140286080571136 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6005271673202515, loss=6.9077229499816895
I0428 22:41:24.636058 140286072178432 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5891204476356506, loss=6.907722473144531
I0428 22:42:08.329863 140286080571136 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5953564643859863, loss=6.90772008895874
I0428 22:42:51.975059 140286072178432 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5260975360870361, loss=6.907721042633057
I0428 22:43:36.014623 140286080571136 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5070305466651917, loss=6.907722473144531
I0428 22:44:19.934248 140286072178432 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5971159338951111, loss=6.907711029052734
I0428 22:45:03.662563 140286080571136 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5667809247970581, loss=6.907711982727051
I0428 22:45:47.433019 140286072178432 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.4637627601623535, loss=6.907723426818848
I0428 22:46:31.278468 140286080571136 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.49221861362457275, loss=6.907710075378418
I0428 22:47:15.127748 140286072178432 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5229209661483765, loss=6.907700538635254
I0428 22:47:17.741869 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:47:24.235809 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:47:30.970997 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:47:32.706280 140516526462784 submission_runner.py:415] Time since start: 2300.66s, 	Step: 4707, 	{'train/accuracy': 0.0044531249441206455, 'train/loss': 6.907670021057129, 'validation/accuracy': 0.004359999671578407, 'validation/loss': 6.907674789428711, 'validation/num_examples': 50000, 'test/accuracy': 0.0037000002339482307, 'test/loss': 6.907679080963135, 'test/num_examples': 10000, 'score': 2173.221355199814, 'total_duration': 2300.6553785800934, 'accumulated_submission_time': 2173.221355199814, 'accumulated_eval_time': 127.23758316040039, 'accumulated_logging_time': 0.12731075286865234}
I0428 22:47:32.722285 140286080571136 logging_writer.py:48] [4707] accumulated_eval_time=127.237583, accumulated_logging_time=0.127311, accumulated_submission_time=2173.221355, global_step=4707, preemption_count=0, score=2173.221355, test/accuracy=0.003700, test/loss=6.907679, test/num_examples=10000, total_duration=2300.655379, train/accuracy=0.004453, train/loss=6.907670, validation/accuracy=0.004360, validation/loss=6.907675, validation/num_examples=50000
I0428 22:48:15.470940 140286072178432 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5896227955818176, loss=6.907684803009033
I0428 22:49:00.946647 140286080571136 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5289910435676575, loss=6.907687664031982
I0428 22:49:46.034524 140286072178432 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.4602701961994171, loss=6.9077043533325195
I0428 22:50:31.082960 140286080571136 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.611750602722168, loss=6.907662391662598
I0428 22:51:16.140146 140286072178432 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5520694851875305, loss=6.907656192779541
I0428 22:52:01.355577 140286080571136 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5762761831283569, loss=6.907648086547852
I0428 22:52:46.466207 140286072178432 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6020698547363281, loss=6.907627582550049
I0428 22:53:31.476970 140286080571136 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6091282963752747, loss=6.90761661529541
I0428 22:54:16.458805 140286072178432 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6097344756126404, loss=6.907596111297607
I0428 22:54:33.057158 140516526462784 spec.py:298] Evaluating on the training split.
I0428 22:54:39.580331 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 22:54:46.581095 140516526462784 spec.py:326] Evaluating on the test split.
I0428 22:54:48.304752 140516526462784 submission_runner.py:415] Time since start: 2736.25s, 	Step: 5638, 	{'train/accuracy': 0.005624999757856131, 'train/loss': 6.907540321350098, 'validation/accuracy': 0.005200000014156103, 'validation/loss': 6.907548427581787, 'validation/num_examples': 50000, 'test/accuracy': 0.004900000058114529, 'test/loss': 6.907555103302002, 'test/num_examples': 10000, 'score': 2593.5265860557556, 'total_duration': 2736.2538776397705, 'accumulated_submission_time': 2593.5265860557556, 'accumulated_eval_time': 142.48514866828918, 'accumulated_logging_time': 0.15622663497924805}
I0428 22:54:48.316702 140286080571136 logging_writer.py:48] [5638] accumulated_eval_time=142.485149, accumulated_logging_time=0.156227, accumulated_submission_time=2593.526586, global_step=5638, preemption_count=0, score=2593.526586, test/accuracy=0.004900, test/loss=6.907555, test/num_examples=10000, total_duration=2736.253878, train/accuracy=0.005625, train/loss=6.907540, validation/accuracy=0.005200, validation/loss=6.907548, validation/num_examples=50000
I0428 22:55:17.025426 140286072178432 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.47482630610466003, loss=6.907650470733643
I0428 22:56:02.506299 140286080571136 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5891216993331909, loss=6.9075727462768555
I0428 22:56:48.217169 140286072178432 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5314910411834717, loss=6.907580375671387
I0428 22:57:33.858787 140286080571136 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6153148412704468, loss=6.907516002655029
I0428 22:58:19.582366 140286072178432 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6104395389556885, loss=6.9075164794921875
I0428 22:59:05.293762 140286080571136 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4816460609436035, loss=6.907580852508545
I0428 22:59:51.275268 140286072178432 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6038867235183716, loss=6.907443523406982
I0428 23:00:36.907335 140286080571136 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6084349155426025, loss=6.907409191131592
I0428 23:01:22.501084 140286072178432 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6234157681465149, loss=6.907334327697754
I0428 23:01:48.556031 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:01:55.096982 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:02:01.855003 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:02:03.594814 140516526462784 submission_runner.py:415] Time since start: 3171.54s, 	Step: 6558, 	{'train/accuracy': 0.0037890623789280653, 'train/loss': 6.90718412399292, 'validation/accuracy': 0.003819999983534217, 'validation/loss': 6.907197952270508, 'validation/num_examples': 50000, 'test/accuracy': 0.003100000089034438, 'test/loss': 6.9072160720825195, 'test/num_examples': 10000, 'score': 3013.7352664470673, 'total_duration': 3171.5439031124115, 'accumulated_submission_time': 3013.7352664470673, 'accumulated_eval_time': 157.5238742828369, 'accumulated_logging_time': 0.18133234977722168}
I0428 23:02:03.609895 140286080571136 logging_writer.py:48] [6558] accumulated_eval_time=157.523874, accumulated_logging_time=0.181332, accumulated_submission_time=3013.735266, global_step=6558, preemption_count=0, score=3013.735266, test/accuracy=0.003100, test/loss=6.907216, test/num_examples=10000, total_duration=3171.543903, train/accuracy=0.003789, train/loss=6.907184, validation/accuracy=0.003820, validation/loss=6.907198, validation/num_examples=50000
I0428 23:02:23.625456 140286072178432 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.566740095615387, loss=6.907318592071533
I0428 23:03:09.625541 140286080571136 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6055518388748169, loss=6.907207489013672
I0428 23:03:56.309887 140286072178432 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.618828535079956, loss=6.907173156738281
I0428 23:04:42.874690 140286080571136 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.487059623003006, loss=6.907348155975342
I0428 23:05:29.414170 140286072178432 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6254519820213318, loss=6.907029628753662
I0428 23:06:16.191453 140286080571136 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5663596987724304, loss=6.906980514526367
I0428 23:07:03.041738 140286072178432 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6214152574539185, loss=6.906825542449951
I0428 23:07:50.116308 140286080571136 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6214467287063599, loss=6.906663417816162
I0428 23:08:36.858431 140286072178432 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4740670621395111, loss=6.906978607177734
I0428 23:09:04.024744 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:09:10.670533 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:09:17.636565 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:09:19.363862 140516526462784 submission_runner.py:415] Time since start: 3607.31s, 	Step: 7459, 	{'train/accuracy': 0.002832031110301614, 'train/loss': 6.906032085418701, 'validation/accuracy': 0.002940000034868717, 'validation/loss': 6.906071186065674, 'validation/num_examples': 50000, 'test/accuracy': 0.002300000051036477, 'test/loss': 6.9061408042907715, 'test/num_examples': 10000, 'score': 3434.1195731163025, 'total_duration': 3607.3129789829254, 'accumulated_submission_time': 3434.1195731163025, 'accumulated_eval_time': 172.8629755973816, 'accumulated_logging_time': 0.20996737480163574}
I0428 23:09:19.375411 140286080571136 logging_writer.py:48] [7459] accumulated_eval_time=172.862976, accumulated_logging_time=0.209967, accumulated_submission_time=3434.119573, global_step=7459, preemption_count=0, score=3434.119573, test/accuracy=0.002300, test/loss=6.906141, test/num_examples=10000, total_duration=3607.312979, train/accuracy=0.002832, train/loss=6.906032, validation/accuracy=0.002940, validation/loss=6.906071, validation/num_examples=50000
I0428 23:09:39.317287 140286072178432 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4901750683784485, loss=6.906999111175537
I0428 23:10:25.725970 140286080571136 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6279383301734924, loss=6.906225204467773
I0428 23:11:12.420119 140286072178432 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6097562909126282, loss=6.905920505523682
I0428 23:11:59.324381 140286080571136 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.48212119936943054, loss=6.906887054443359
I0428 23:12:45.918437 140286072178432 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6111758351325989, loss=6.905335426330566
I0428 23:13:32.635250 140286080571136 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6016192436218262, loss=6.905019760131836
I0428 23:14:19.217027 140286072178432 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5977320671081543, loss=6.904854774475098
I0428 23:15:05.348345 140286080571136 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4722398519515991, loss=6.905322551727295
I0428 23:15:51.910881 140286072178432 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5545179843902588, loss=6.904459476470947
I0428 23:16:19.388445 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:16:26.089685 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:16:33.224455 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:16:34.983183 140516526462784 submission_runner.py:415] Time since start: 4042.93s, 	Step: 8360, 	{'train/accuracy': 0.0018164062639698386, 'train/loss': 6.902470111846924, 'validation/accuracy': 0.0020200000144541264, 'validation/loss': 6.902583122253418, 'validation/num_examples': 50000, 'test/accuracy': 0.0021000001579523087, 'test/loss': 6.902647018432617, 'test/num_examples': 10000, 'score': 3854.103394985199, 'total_duration': 4042.932283639908, 'accumulated_submission_time': 3854.103394985199, 'accumulated_eval_time': 188.45768356323242, 'accumulated_logging_time': 0.23368120193481445}
I0428 23:16:35.000215 140286080571136 logging_writer.py:48] [8360] accumulated_eval_time=188.457684, accumulated_logging_time=0.233681, accumulated_submission_time=3854.103395, global_step=8360, preemption_count=0, score=3854.103395, test/accuracy=0.002100, test/loss=6.902647, test/num_examples=10000, total_duration=4042.932284, train/accuracy=0.001816, train/loss=6.902470, validation/accuracy=0.002020, validation/loss=6.902583, validation/num_examples=50000
I0428 23:16:54.348077 140286072178432 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.46859896183013916, loss=6.904676914215088
I0428 23:17:40.328597 140286080571136 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5955854058265686, loss=6.902817726135254
I0428 23:18:26.135678 140286072178432 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5776371359825134, loss=6.901813507080078
I0428 23:19:12.017529 140286080571136 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.599175214767456, loss=6.900817394256592
I0428 23:19:57.653481 140286072178432 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5472559928894043, loss=6.900887966156006
I0428 23:20:43.537934 140286080571136 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6009122729301453, loss=6.89769172668457
I0428 23:21:28.992706 140286072178432 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4909937083721161, loss=6.899684429168701
I0428 23:22:14.798983 140286080571136 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.47039416432380676, loss=6.900481224060059
I0428 23:23:00.638821 140286072178432 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.47541311383247375, loss=6.899018287658691
I0428 23:23:35.283689 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:23:42.182709 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:23:49.327041 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:23:51.075729 140516526462784 submission_runner.py:415] Time since start: 4479.02s, 	Step: 9277, 	{'train/accuracy': 0.0022656249348074198, 'train/loss': 6.884683132171631, 'validation/accuracy': 0.002099999925121665, 'validation/loss': 6.884961128234863, 'validation/num_examples': 50000, 'test/accuracy': 0.0020000000949949026, 'test/loss': 6.8856987953186035, 'test/num_examples': 10000, 'score': 4274.35679936409, 'total_duration': 4479.0248012542725, 'accumulated_submission_time': 4274.35679936409, 'accumulated_eval_time': 204.2496588230133, 'accumulated_logging_time': 0.2633669376373291}
I0428 23:23:51.093275 140286080571136 logging_writer.py:48] [9277] accumulated_eval_time=204.249659, accumulated_logging_time=0.263367, accumulated_submission_time=4274.356799, global_step=9277, preemption_count=0, score=4274.356799, test/accuracy=0.002000, test/loss=6.885699, test/num_examples=10000, total_duration=4479.024801, train/accuracy=0.002266, train/loss=6.884683, validation/accuracy=0.002100, validation/loss=6.884961, validation/num_examples=50000
I0428 23:24:03.105080 140286072178432 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6207351684570312, loss=6.889369964599609
I0428 23:24:48.987131 140286080571136 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6102226972579956, loss=6.887141704559326
I0428 23:25:34.946295 140286072178432 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5632892847061157, loss=6.887459754943848
I0428 23:26:20.498917 140286080571136 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5780912041664124, loss=6.878628730773926
I0428 23:27:06.556718 140286072178432 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5974676012992859, loss=6.876861572265625
I0428 23:27:52.358178 140286080571136 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5943053364753723, loss=6.872104644775391
I0428 23:28:38.088010 140286072178432 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6048632860183716, loss=6.8643269538879395
I0428 23:29:23.762578 140286080571136 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.49105674028396606, loss=6.872727394104004
I0428 23:30:09.614672 140286072178432 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5965297818183899, loss=6.853428363800049
I0428 23:30:51.212793 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:30:58.052657 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:31:05.171164 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:31:06.920336 140516526462784 submission_runner.py:415] Time since start: 4914.87s, 	Step: 10192, 	{'train/accuracy': 0.002070312388241291, 'train/loss': 6.823131084442139, 'validation/accuracy': 0.002059999853372574, 'validation/loss': 6.824239730834961, 'validation/num_examples': 50000, 'test/accuracy': 0.0019000000320374966, 'test/loss': 6.827970504760742, 'test/num_examples': 10000, 'score': 4694.413540363312, 'total_duration': 4914.869418382645, 'accumulated_submission_time': 4694.413540363312, 'accumulated_eval_time': 219.9571614265442, 'accumulated_logging_time': 0.32652974128723145}
I0428 23:31:06.937448 140286080571136 logging_writer.py:48] [10192] accumulated_eval_time=219.957161, accumulated_logging_time=0.326530, accumulated_submission_time=4694.413540, global_step=10192, preemption_count=0, score=4694.413540, test/accuracy=0.001900, test/loss=6.827971, test/num_examples=10000, total_duration=4914.869418, train/accuracy=0.002070, train/loss=6.823131, validation/accuracy=0.002060, validation/loss=6.824240, validation/num_examples=50000
I0428 23:31:11.568181 140286072178432 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5618589520454407, loss=6.851685047149658
I0428 23:31:57.756391 140286080571136 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.49048149585723877, loss=6.866298198699951
I0428 23:32:43.717738 140286072178432 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5382884740829468, loss=6.835885047912598
I0428 23:33:29.409949 140286080571136 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5891914963722229, loss=6.815601825714111
I0428 23:34:15.151574 140286072178432 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.46621420979499817, loss=6.840609550476074
I0428 23:35:00.902180 140286080571136 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5651010870933533, loss=6.797451019287109
I0428 23:35:46.570773 140286072178432 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5229890942573547, loss=6.7963762283325195
I0428 23:36:32.124747 140286080571136 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4911406934261322, loss=6.827502250671387
I0428 23:37:17.785210 140286072178432 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5272846221923828, loss=6.786258220672607
I0428 23:38:03.565030 140286080571136 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6562456488609314, loss=6.7414045333862305
I0428 23:38:07.202083 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:38:14.134254 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:38:21.247329 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:38:22.990916 140516526462784 submission_runner.py:415] Time since start: 5350.94s, 	Step: 11109, 	{'train/accuracy': 0.0033398435916751623, 'train/loss': 6.683769226074219, 'validation/accuracy': 0.003339999821037054, 'validation/loss': 6.688389778137207, 'validation/num_examples': 50000, 'test/accuracy': 0.002300000051036477, 'test/loss': 6.700666904449463, 'test/num_examples': 10000, 'score': 5114.647413015366, 'total_duration': 5350.9400045871735, 'accumulated_submission_time': 5114.647413015366, 'accumulated_eval_time': 235.74602389335632, 'accumulated_logging_time': 0.3572261333465576}
I0428 23:38:23.008216 140286072178432 logging_writer.py:48] [11109] accumulated_eval_time=235.746024, accumulated_logging_time=0.357226, accumulated_submission_time=5114.647413, global_step=11109, preemption_count=0, score=5114.647413, test/accuracy=0.002300, test/loss=6.700667, test/num_examples=10000, total_duration=5350.940005, train/accuracy=0.003340, train/loss=6.683769, validation/accuracy=0.003340, validation/loss=6.688390, validation/num_examples=50000
I0428 23:39:05.510485 140286080571136 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5063846111297607, loss=6.762654781341553
I0428 23:39:51.045690 140286072178432 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5367118716239929, loss=6.733826637268066
I0428 23:40:37.217590 140286080571136 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5727450847625732, loss=6.692278861999512
I0428 23:41:23.078644 140286072178432 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5812103748321533, loss=6.681920051574707
I0428 23:42:08.786373 140286080571136 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5896437168121338, loss=6.6677703857421875
I0428 23:42:54.633836 140286072178432 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5346818566322327, loss=6.7330193519592285
I0428 23:43:40.307329 140286080571136 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.576839804649353, loss=6.648096084594727
I0428 23:44:26.214025 140286072178432 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5282712578773499, loss=6.772714614868164
I0428 23:45:12.054233 140286080571136 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5848454236984253, loss=6.658438205718994
I0428 23:45:23.032816 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:45:29.951441 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:45:36.985069 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:45:38.737270 140516526462784 submission_runner.py:415] Time since start: 5786.69s, 	Step: 12025, 	{'train/accuracy': 0.0063281250186264515, 'train/loss': 6.51516056060791, 'validation/accuracy': 0.005219999700784683, 'validation/loss': 6.526479244232178, 'validation/num_examples': 50000, 'test/accuracy': 0.004399999976158142, 'test/loss': 6.549435138702393, 'test/num_examples': 10000, 'score': 5534.636485338211, 'total_duration': 5786.686399459839, 'accumulated_submission_time': 5534.636485338211, 'accumulated_eval_time': 251.4504623413086, 'accumulated_logging_time': 0.39244508743286133}
I0428 23:45:38.752089 140286072178432 logging_writer.py:48] [12025] accumulated_eval_time=251.450462, accumulated_logging_time=0.392445, accumulated_submission_time=5534.636485, global_step=12025, preemption_count=0, score=5534.636485, test/accuracy=0.004400, test/loss=6.549435, test/num_examples=10000, total_duration=5786.686399, train/accuracy=0.006328, train/loss=6.515161, validation/accuracy=0.005220, validation/loss=6.526479, validation/num_examples=50000
I0428 23:46:14.292823 140286080571136 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.585443377494812, loss=6.610323905944824
I0428 23:47:00.087491 140286072178432 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5288219451904297, loss=6.660471439361572
I0428 23:47:45.969289 140286080571136 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5373935699462891, loss=6.607890605926514
I0428 23:48:32.414518 140286072178432 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.49696776270866394, loss=6.654244899749756
I0428 23:49:18.088382 140286080571136 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.48415377736091614, loss=6.668845176696777
I0428 23:50:03.983124 140286072178432 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5809615254402161, loss=6.5267486572265625
I0428 23:50:49.786725 140286080571136 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.46651795506477356, loss=6.742865562438965
I0428 23:51:35.713361 140286072178432 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.461885929107666, loss=6.776974201202393
I0428 23:52:21.827388 140286080571136 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5742061138153076, loss=6.484681129455566
I0428 23:52:39.090775 140516526462784 spec.py:298] Evaluating on the training split.
I0428 23:52:46.649996 140516526462784 spec.py:310] Evaluating on the validation split.
I0428 23:52:53.962312 140516526462784 spec.py:326] Evaluating on the test split.
I0428 23:52:55.691229 140516526462784 submission_runner.py:415] Time since start: 6223.64s, 	Step: 12939, 	{'train/accuracy': 0.013710937462747097, 'train/loss': 6.29306697845459, 'validation/accuracy': 0.013039999641478062, 'validation/loss': 6.310734272003174, 'validation/num_examples': 50000, 'test/accuracy': 0.010400000959634781, 'test/loss': 6.3621087074279785, 'test/num_examples': 10000, 'score': 5954.945756673813, 'total_duration': 6223.640339136124, 'accumulated_submission_time': 5954.945756673813, 'accumulated_eval_time': 268.0508875846863, 'accumulated_logging_time': 0.4192776679992676}
I0428 23:52:55.707218 140286072178432 logging_writer.py:48] [12939] accumulated_eval_time=268.050888, accumulated_logging_time=0.419278, accumulated_submission_time=5954.945757, global_step=12939, preemption_count=0, score=5954.945757, test/accuracy=0.010400, test/loss=6.362109, test/num_examples=10000, total_duration=6223.640339, train/accuracy=0.013711, train/loss=6.293067, validation/accuracy=0.013040, validation/loss=6.310734, validation/num_examples=50000
I0428 23:53:24.863578 140286080571136 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.560947835445404, loss=6.500327110290527
I0428 23:54:11.040621 140286072178432 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.630706787109375, loss=6.500740051269531
I0428 23:54:56.991189 140286080571136 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5836036801338196, loss=6.43939208984375
I0428 23:55:42.717314 140286072178432 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5474185943603516, loss=6.4773945808410645
I0428 23:56:28.729266 140286080571136 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5299544334411621, loss=6.530137062072754
I0428 23:57:14.479924 140286072178432 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6025035381317139, loss=6.397468090057373
I0428 23:58:00.340579 140286080571136 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.609447181224823, loss=6.344544887542725
I0428 23:58:46.372761 140286072178432 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6039071083068848, loss=6.330615997314453
I0428 23:59:32.073674 140286080571136 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5061336755752563, loss=6.623495101928711
I0428 23:59:55.933933 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:00:03.930357 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:00:11.393422 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:00:13.113267 140516526462784 submission_runner.py:415] Time since start: 6661.06s, 	Step: 13853, 	{'train/accuracy': 0.02363281138241291, 'train/loss': 6.047488212585449, 'validation/accuracy': 0.023919999599456787, 'validation/loss': 6.066572189331055, 'validation/num_examples': 50000, 'test/accuracy': 0.020000001415610313, 'test/loss': 6.14048957824707, 'test/num_examples': 10000, 'score': 6375.14003610611, 'total_duration': 6661.062366962433, 'accumulated_submission_time': 6375.14003610611, 'accumulated_eval_time': 285.2301926612854, 'accumulated_logging_time': 0.450606107711792}
I0429 00:00:13.127075 140286072178432 logging_writer.py:48] [13853] accumulated_eval_time=285.230193, accumulated_logging_time=0.450606, accumulated_submission_time=6375.140036, global_step=13853, preemption_count=0, score=6375.140036, test/accuracy=0.020000, test/loss=6.140490, test/num_examples=10000, total_duration=6661.062367, train/accuracy=0.023633, train/loss=6.047488, validation/accuracy=0.023920, validation/loss=6.066572, validation/num_examples=50000
I0429 00:00:35.774406 140286080571136 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6060101389884949, loss=6.314395427703857
I0429 00:01:21.725261 140286072178432 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6676002740859985, loss=6.342838287353516
I0429 00:02:07.665050 140286080571136 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5883944034576416, loss=6.311308860778809
I0429 00:02:53.617624 140286072178432 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6595050096511841, loss=6.252175331115723
I0429 00:03:39.618455 140286080571136 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6660304069519043, loss=6.239405155181885
I0429 00:04:25.795746 140286072178432 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5331748127937317, loss=6.661930084228516
I0429 00:05:11.599961 140286080571136 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6590043306350708, loss=6.199891090393066
I0429 00:05:57.315270 140286072178432 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5628634095191956, loss=6.6630120277404785
I0429 00:06:42.947699 140286080571136 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6428267359733582, loss=6.208992958068848
I0429 00:07:13.581359 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:07:21.821325 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:07:29.602823 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:07:31.315698 140516526462784 submission_runner.py:415] Time since start: 7099.26s, 	Step: 14768, 	{'train/accuracy': 0.03996093571186066, 'train/loss': 5.802436351776123, 'validation/accuracy': 0.034539997577667236, 'validation/loss': 5.829770565032959, 'validation/num_examples': 50000, 'test/accuracy': 0.026600001379847527, 'test/loss': 5.931950569152832, 'test/num_examples': 10000, 'score': 6795.54832983017, 'total_duration': 7099.264804124832, 'accumulated_submission_time': 6795.54832983017, 'accumulated_eval_time': 302.9644935131073, 'accumulated_logging_time': 0.4932529926300049}
I0429 00:07:31.335422 140286072178432 logging_writer.py:48] [14768] accumulated_eval_time=302.964494, accumulated_logging_time=0.493253, accumulated_submission_time=6795.548330, global_step=14768, preemption_count=0, score=6795.548330, test/accuracy=0.026600, test/loss=5.931951, test/num_examples=10000, total_duration=7099.264804, train/accuracy=0.039961, train/loss=5.802436, validation/accuracy=0.034540, validation/loss=5.829771, validation/num_examples=50000
I0429 00:07:47.120337 140286080571136 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5945839881896973, loss=6.611934661865234
I0429 00:08:33.163403 140286072178432 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6039164066314697, loss=6.213247776031494
I0429 00:09:19.239188 140286080571136 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6950600147247314, loss=6.628252029418945
I0429 00:10:04.773340 140286072178432 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6710264682769775, loss=6.067138195037842
I0429 00:10:50.469527 140286080571136 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7217856645584106, loss=6.087040901184082
I0429 00:11:36.245222 140286072178432 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6848297119140625, loss=6.080979347229004
I0429 00:12:22.372318 140286080571136 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6579445600509644, loss=6.2505364418029785
I0429 00:13:08.337693 140286072178432 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7576792240142822, loss=6.011002540588379
I0429 00:13:54.175128 140286080571136 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7281913161277771, loss=6.07729959487915
I0429 00:14:31.696363 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:14:40.054037 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:14:47.538384 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:14:49.265437 140516526462784 submission_runner.py:415] Time since start: 7537.21s, 	Step: 15683, 	{'train/accuracy': 0.05269531160593033, 'train/loss': 5.556304931640625, 'validation/accuracy': 0.04910000041127205, 'validation/loss': 5.585467338562012, 'validation/num_examples': 50000, 'test/accuracy': 0.039000000804662704, 'test/loss': 5.737371921539307, 'test/num_examples': 10000, 'score': 7215.868463277817, 'total_duration': 7537.214565992355, 'accumulated_submission_time': 7215.868463277817, 'accumulated_eval_time': 320.5335536003113, 'accumulated_logging_time': 0.5366556644439697}
I0429 00:14:49.283387 140286072178432 logging_writer.py:48] [15683] accumulated_eval_time=320.533554, accumulated_logging_time=0.536656, accumulated_submission_time=7215.868463, global_step=15683, preemption_count=0, score=7215.868463, test/accuracy=0.039000, test/loss=5.737372, test/num_examples=10000, total_duration=7537.214566, train/accuracy=0.052695, train/loss=5.556305, validation/accuracy=0.049100, validation/loss=5.585467, validation/num_examples=50000
I0429 00:14:58.073911 140286080571136 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.7218425869941711, loss=6.010103225708008
I0429 00:15:44.274776 140286072178432 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7042394280433655, loss=6.047236919403076
I0429 00:16:30.166028 140286080571136 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6928797960281372, loss=6.433346748352051
I0429 00:17:15.977729 140286072178432 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7254155874252319, loss=5.947346210479736
I0429 00:18:02.208441 140286080571136 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6202065944671631, loss=6.677552700042725
I0429 00:18:47.953175 140286072178432 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7891466021537781, loss=5.96036958694458
I0429 00:19:33.700867 140286080571136 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6636092066764832, loss=6.0763840675354
I0429 00:20:19.476720 140286072178432 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7163915634155273, loss=6.280258655548096
I0429 00:21:05.770156 140286080571136 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.8348033428192139, loss=6.092080116271973
I0429 00:21:49.570074 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:21:58.698161 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:22:06.293376 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:22:08.017339 140516526462784 submission_runner.py:415] Time since start: 7975.97s, 	Step: 16597, 	{'train/accuracy': 0.06566406041383743, 'train/loss': 5.346097469329834, 'validation/accuracy': 0.06191999837756157, 'validation/loss': 5.388829708099365, 'validation/num_examples': 50000, 'test/accuracy': 0.048700001090765, 'test/loss': 5.561663627624512, 'test/num_examples': 10000, 'score': 7636.107887268066, 'total_duration': 7975.966424465179, 'accumulated_submission_time': 7636.107887268066, 'accumulated_eval_time': 338.98077416419983, 'accumulated_logging_time': 0.5847828388214111}
I0429 00:22:08.032865 140286072178432 logging_writer.py:48] [16597] accumulated_eval_time=338.980774, accumulated_logging_time=0.584783, accumulated_submission_time=7636.107887, global_step=16597, preemption_count=0, score=7636.107887, test/accuracy=0.048700, test/loss=5.561664, test/num_examples=10000, total_duration=7975.966424, train/accuracy=0.065664, train/loss=5.346097, validation/accuracy=0.061920, validation/loss=5.388830, validation/num_examples=50000
I0429 00:22:10.001592 140286080571136 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7760680913925171, loss=5.9400954246521
I0429 00:22:56.344927 140286072178432 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7669517993927002, loss=5.928152561187744
I0429 00:23:42.244314 140286080571136 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6350089907646179, loss=6.585574150085449
I0429 00:24:28.238187 140286072178432 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6670570373535156, loss=6.218225955963135
I0429 00:25:13.865746 140286080571136 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7614721059799194, loss=5.877434730529785
I0429 00:25:59.585854 140286072178432 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8662689924240112, loss=5.895686149597168
I0429 00:26:45.218738 140286080571136 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.7252382040023804, loss=6.149875164031982
I0429 00:27:30.889739 140286072178432 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6634880900382996, loss=6.20681619644165
I0429 00:28:16.609711 140286080571136 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7901443243026733, loss=6.441853046417236
I0429 00:29:02.923739 140286072178432 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7600526809692383, loss=5.9694623947143555
I0429 00:29:08.428747 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:29:17.303494 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:29:25.503217 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:29:27.207881 140516526462784 submission_runner.py:415] Time since start: 8415.16s, 	Step: 17513, 	{'train/accuracy': 0.08863280713558197, 'train/loss': 5.139340877532959, 'validation/accuracy': 0.08171999454498291, 'validation/loss': 5.184188365936279, 'validation/num_examples': 50000, 'test/accuracy': 0.0609000027179718, 'test/loss': 5.37208366394043, 'test/num_examples': 10000, 'score': 8056.475097417831, 'total_duration': 8415.157010793686, 'accumulated_submission_time': 8056.475097417831, 'accumulated_eval_time': 357.7599320411682, 'accumulated_logging_time': 0.612074613571167}
I0429 00:29:27.223923 140286080571136 logging_writer.py:48] [17513] accumulated_eval_time=357.759932, accumulated_logging_time=0.612075, accumulated_submission_time=8056.475097, global_step=17513, preemption_count=0, score=8056.475097, test/accuracy=0.060900, test/loss=5.372084, test/num_examples=10000, total_duration=8415.157011, train/accuracy=0.088633, train/loss=5.139341, validation/accuracy=0.081720, validation/loss=5.184188, validation/num_examples=50000
I0429 00:30:08.738758 140286072178432 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8714883327484131, loss=5.750442028045654
I0429 00:30:54.673330 140286080571136 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7405381202697754, loss=5.905895233154297
I0429 00:31:40.673760 140286072178432 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.8309433460235596, loss=5.808119297027588
I0429 00:32:26.543713 140286080571136 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.673762857913971, loss=6.106029033660889
I0429 00:33:12.406793 140286072178432 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7798823118209839, loss=5.843632698059082
I0429 00:33:58.303146 140286080571136 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.906191349029541, loss=5.71378755569458
I0429 00:34:44.168799 140286072178432 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8664403557777405, loss=5.630744457244873
I0429 00:35:29.988176 140286080571136 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8619154095649719, loss=5.72600793838501
I0429 00:36:15.756420 140286072178432 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8392269611358643, loss=5.689573287963867
I0429 00:36:27.640568 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:36:36.187874 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:36:44.471399 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:36:46.196992 140516526462784 submission_runner.py:415] Time since start: 8854.14s, 	Step: 18427, 	{'train/accuracy': 0.11124999821186066, 'train/loss': 4.975393772125244, 'validation/accuracy': 0.10185999423265457, 'validation/loss': 5.019129276275635, 'validation/num_examples': 50000, 'test/accuracy': 0.08020000159740448, 'test/loss': 5.244736194610596, 'test/num_examples': 10000, 'score': 8476.856865644455, 'total_duration': 8854.143136501312, 'accumulated_submission_time': 8476.856865644455, 'accumulated_eval_time': 376.3133590221405, 'accumulated_logging_time': 0.6458587646484375}
I0429 00:36:46.215051 140286080571136 logging_writer.py:48] [18427] accumulated_eval_time=376.313359, accumulated_logging_time=0.645859, accumulated_submission_time=8476.856866, global_step=18427, preemption_count=0, score=8476.856866, test/accuracy=0.080200, test/loss=5.244736, test/num_examples=10000, total_duration=8854.143137, train/accuracy=0.111250, train/loss=4.975394, validation/accuracy=0.101860, validation/loss=5.019129, validation/num_examples=50000
I0429 00:37:21.441789 140286072178432 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8561593294143677, loss=5.681357383728027
I0429 00:38:07.698225 140286080571136 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.98521888256073, loss=5.612703323364258
I0429 00:38:53.531278 140286072178432 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7583118677139282, loss=6.40805721282959
I0429 00:39:39.182690 140286080571136 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.8638972640037537, loss=5.538486480712891
I0429 00:40:25.001654 140286072178432 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8877188563346863, loss=5.538444519042969
I0429 00:41:10.812740 140286080571136 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8237290978431702, loss=6.547924995422363
I0429 00:41:56.687757 140286072178432 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.730771005153656, loss=6.061978816986084
I0429 00:42:42.610766 140286080571136 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9650275111198425, loss=5.536779880523682
I0429 00:43:28.553359 140286072178432 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7735391855239868, loss=5.813900470733643
I0429 00:43:46.546353 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:43:55.197168 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:44:03.573100 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:44:05.311641 140516526462784 submission_runner.py:415] Time since start: 9293.26s, 	Step: 19340, 	{'train/accuracy': 0.1360742151737213, 'train/loss': 4.734663963317871, 'validation/accuracy': 0.12573999166488647, 'validation/loss': 4.779326915740967, 'validation/num_examples': 50000, 'test/accuracy': 0.09930000454187393, 'test/loss': 5.024278163909912, 'test/num_examples': 10000, 'score': 8897.150470256805, 'total_duration': 9293.258126020432, 'accumulated_submission_time': 8897.150470256805, 'accumulated_eval_time': 395.0759916305542, 'accumulated_logging_time': 0.6843357086181641}
I0429 00:44:05.328976 140286080571136 logging_writer.py:48] [19340] accumulated_eval_time=395.075992, accumulated_logging_time=0.684336, accumulated_submission_time=8897.150470, global_step=19340, preemption_count=0, score=8897.150470, test/accuracy=0.099300, test/loss=5.024278, test/num_examples=10000, total_duration=9293.258126, train/accuracy=0.136074, train/loss=4.734664, validation/accuracy=0.125740, validation/loss=4.779327, validation/num_examples=50000
I0429 00:44:34.422959 140286072178432 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9059253334999084, loss=5.470966815948486
I0429 00:45:20.707306 140286080571136 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0434279441833496, loss=5.433241844177246
I0429 00:46:06.635305 140286072178432 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9445118308067322, loss=5.431007385253906
I0429 00:46:52.492496 140286080571136 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8995426297187805, loss=5.446395397186279
I0429 00:47:38.598181 140286072178432 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1575415134429932, loss=5.354787349700928
I0429 00:48:24.542733 140286080571136 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8254582285881042, loss=5.9764628410339355
I0429 00:49:10.600574 140286072178432 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.018322229385376, loss=5.35850191116333
I0429 00:49:56.380949 140286080571136 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0281739234924316, loss=5.3611040115356445
I0429 00:50:42.310155 140286072178432 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8548448085784912, loss=6.37786865234375
I0429 00:51:05.639390 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:51:14.997218 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:51:23.671364 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:51:25.408530 140516526462784 submission_runner.py:415] Time since start: 9733.36s, 	Step: 20252, 	{'train/accuracy': 0.1693750023841858, 'train/loss': 4.46301794052124, 'validation/accuracy': 0.15501999855041504, 'validation/loss': 4.524064540863037, 'validation/num_examples': 50000, 'test/accuracy': 0.1201000064611435, 'test/loss': 4.82213830947876, 'test/num_examples': 10000, 'score': 9317.424408435822, 'total_duration': 9733.355473518372, 'accumulated_submission_time': 9317.424408435822, 'accumulated_eval_time': 414.8429260253906, 'accumulated_logging_time': 0.7208709716796875}
I0429 00:51:25.431117 140286080571136 logging_writer.py:48] [20252] accumulated_eval_time=414.842926, accumulated_logging_time=0.720871, accumulated_submission_time=9317.424408, global_step=20252, preemption_count=0, score=9317.424408, test/accuracy=0.120100, test/loss=4.822138, test/num_examples=10000, total_duration=9733.355474, train/accuracy=0.169375, train/loss=4.463018, validation/accuracy=0.155020, validation/loss=4.524065, validation/num_examples=50000
I0429 00:51:48.547324 140286072178432 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1029125452041626, loss=5.319323539733887
I0429 00:52:34.612575 140286080571136 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.9314478039741516, loss=5.457608699798584
I0429 00:53:20.840218 140286072178432 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9752219319343567, loss=5.570258617401123
I0429 00:54:06.610857 140286080571136 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.9113798141479492, loss=6.43325138092041
I0429 00:54:52.237267 140286072178432 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9506128430366516, loss=5.533790588378906
I0429 00:55:37.793011 140286080571136 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0720818042755127, loss=5.36383056640625
I0429 00:56:23.522640 140286072178432 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1578463315963745, loss=5.211569786071777
I0429 00:57:09.368944 140286080571136 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9912669062614441, loss=5.522845268249512
I0429 00:57:55.421270 140286072178432 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0337791442871094, loss=5.292859077453613
I0429 00:58:25.880065 140516526462784 spec.py:298] Evaluating on the training split.
I0429 00:58:34.907023 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 00:58:43.117447 140516526462784 spec.py:326] Evaluating on the test split.
I0429 00:58:44.871871 140516526462784 submission_runner.py:415] Time since start: 10172.82s, 	Step: 21167, 	{'train/accuracy': 0.1922656148672104, 'train/loss': 4.295621395111084, 'validation/accuracy': 0.17509999871253967, 'validation/loss': 4.376280307769775, 'validation/num_examples': 50000, 'test/accuracy': 0.1355000138282776, 'test/loss': 4.690027236938477, 'test/num_examples': 10000, 'score': 9737.838124752045, 'total_duration': 10172.815248250961, 'accumulated_submission_time': 9737.838124752045, 'accumulated_eval_time': 433.82896542549133, 'accumulated_logging_time': 0.7613263130187988}
I0429 00:58:44.892475 140286080571136 logging_writer.py:48] [21167] accumulated_eval_time=433.828965, accumulated_logging_time=0.761326, accumulated_submission_time=9737.838125, global_step=21167, preemption_count=0, score=9737.838125, test/accuracy=0.135500, test/loss=4.690027, test/num_examples=10000, total_duration=10172.815248, train/accuracy=0.192266, train/loss=4.295621, validation/accuracy=0.175100, validation/loss=4.376280, validation/num_examples=50000
I0429 00:59:01.233794 140286072178432 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.2308696508407593, loss=5.310685634613037
I0429 00:59:47.401757 140286080571136 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.00906240940094, loss=5.190954208374023
I0429 01:00:33.239300 140286072178432 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9881972074508667, loss=5.185574054718018
I0429 01:01:19.313703 140286080571136 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8169374465942383, loss=6.32530403137207
I0429 01:02:05.607230 140286072178432 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.9516420960426331, loss=5.724424839019775
I0429 01:02:51.382714 140286080571136 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0478192567825317, loss=5.116296291351318
I0429 01:03:37.095321 140286072178432 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.0952529907226562, loss=5.349621772766113
I0429 01:04:22.726192 140286080571136 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.289129614830017, loss=5.124576568603516
I0429 01:05:08.558627 140286072178432 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0701117515563965, loss=5.329949378967285
I0429 01:05:45.121646 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:05:53.476477 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:06:02.576058 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:06:04.305510 140516526462784 submission_runner.py:415] Time since start: 10612.25s, 	Step: 22081, 	{'train/accuracy': 0.2122460901737213, 'train/loss': 4.083638668060303, 'validation/accuracy': 0.19971999526023865, 'validation/loss': 4.168559551239014, 'validation/num_examples': 50000, 'test/accuracy': 0.1519000083208084, 'test/loss': 4.520297050476074, 'test/num_examples': 10000, 'score': 10158.02670264244, 'total_duration': 10612.253339529037, 'accumulated_submission_time': 10158.02670264244, 'accumulated_eval_time': 453.01153206825256, 'accumulated_logging_time': 0.805086612701416}
I0429 01:06:04.318182 140286080571136 logging_writer.py:48] [22081] accumulated_eval_time=453.011532, accumulated_logging_time=0.805087, accumulated_submission_time=10158.026703, global_step=22081, preemption_count=0, score=10158.026703, test/accuracy=0.151900, test/loss=4.520297, test/num_examples=10000, total_duration=10612.253340, train/accuracy=0.212246, train/loss=4.083639, validation/accuracy=0.199720, validation/loss=4.168560, validation/num_examples=50000
I0429 01:06:13.891629 140286072178432 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.2812573909759521, loss=5.022899627685547
I0429 01:06:59.937124 140286080571136 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.125718593597412, loss=5.103667736053467
I0429 01:07:45.882816 140286072178432 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.1179044246673584, loss=5.064889907836914
I0429 01:08:31.534550 140286080571136 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.101966381072998, loss=5.005028247833252
I0429 01:09:17.087953 140286072178432 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.120961308479309, loss=5.018225193023682
I0429 01:10:03.351070 140286080571136 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0050909519195557, loss=5.919492721557617
I0429 01:10:49.001354 140286072178432 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.8612373471260071, loss=6.361283779144287
I0429 01:11:34.774631 140286080571136 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1231300830841064, loss=4.945040702819824
I0429 01:12:20.589420 140286072178432 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.3240034580230713, loss=4.983570098876953
I0429 01:13:04.430815 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:13:12.229082 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:13:20.804416 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:13:22.527427 140516526462784 submission_runner.py:415] Time since start: 11050.48s, 	Step: 22997, 	{'train/accuracy': 0.24406249821186066, 'train/loss': 3.900546073913574, 'validation/accuracy': 0.22741998732089996, 'validation/loss': 3.989886999130249, 'validation/num_examples': 50000, 'test/accuracy': 0.17720000445842743, 'test/loss': 4.387495040893555, 'test/num_examples': 10000, 'score': 10578.105655193329, 'total_duration': 11050.475373029709, 'accumulated_submission_time': 10578.105655193329, 'accumulated_eval_time': 471.1069643497467, 'accumulated_logging_time': 0.8341519832611084}
I0429 01:13:22.546525 140286080571136 logging_writer.py:48] [22997] accumulated_eval_time=471.106964, accumulated_logging_time=0.834152, accumulated_submission_time=10578.105655, global_step=22997, preemption_count=0, score=10578.105655, test/accuracy=0.177200, test/loss=4.387495, test/num_examples=10000, total_duration=11050.475373, train/accuracy=0.244062, train/loss=3.900546, validation/accuracy=0.227420, validation/loss=3.989887, validation/num_examples=50000
I0429 01:13:24.595552 140286072178432 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.1260987520217896, loss=4.961267471313477
I0429 01:14:11.124248 140286080571136 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0465909242630005, loss=5.029777526855469
I0429 01:14:56.976537 140286072178432 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.1196016073226929, loss=5.062366008758545
I0429 01:15:42.901153 140286080571136 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9355371594429016, loss=5.913036823272705
I0429 01:16:28.799113 140286072178432 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.199480414390564, loss=5.551642417907715
I0429 01:17:14.756129 140286080571136 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8302573561668396, loss=6.321888446807861
I0429 01:18:00.876759 140286072178432 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.12261962890625, loss=4.9395294189453125
I0429 01:18:46.628865 140286080571136 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.1613919734954834, loss=4.919195175170898
I0429 01:19:32.413553 140286072178432 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.9864027500152588, loss=5.521239280700684
I0429 01:20:18.383715 140286080571136 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.071179747581482, loss=6.33989143371582
I0429 01:20:23.022329 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:20:31.179275 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:20:39.691207 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:20:41.414229 140516526462784 submission_runner.py:415] Time since start: 11489.36s, 	Step: 23911, 	{'train/accuracy': 0.27519530057907104, 'train/loss': 3.694805145263672, 'validation/accuracy': 0.2483999878168106, 'validation/loss': 3.8292949199676514, 'validation/num_examples': 50000, 'test/accuracy': 0.18230000138282776, 'test/loss': 4.254889011383057, 'test/num_examples': 10000, 'score': 10998.548694849014, 'total_duration': 11489.362217664719, 'accumulated_submission_time': 10998.548694849014, 'accumulated_eval_time': 489.4976906776428, 'accumulated_logging_time': 0.8689968585968018}
I0429 01:20:41.428046 140286072178432 logging_writer.py:48] [23911] accumulated_eval_time=489.497691, accumulated_logging_time=0.868997, accumulated_submission_time=10998.548695, global_step=23911, preemption_count=0, score=10998.548695, test/accuracy=0.182300, test/loss=4.254889, test/num_examples=10000, total_duration=11489.362218, train/accuracy=0.275195, train/loss=3.694805, validation/accuracy=0.248400, validation/loss=3.829295, validation/num_examples=50000
I0429 01:21:22.994781 140286080571136 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1119203567504883, loss=5.172371864318848
I0429 01:22:08.864122 140286072178432 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.1378989219665527, loss=4.860522270202637
I0429 01:22:54.467454 140286080571136 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0406098365783691, loss=5.113767623901367
I0429 01:23:40.416695 140286072178432 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0465928316116333, loss=5.345556735992432
I0429 01:24:26.293956 140286080571136 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8640850782394409, loss=6.0855817794799805
I0429 01:25:11.928431 140286072178432 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3194806575775146, loss=4.760981559753418
I0429 01:25:57.990144 140286080571136 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.121533751487732, loss=4.777379989624023
I0429 01:26:43.961846 140286072178432 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0148663520812988, loss=6.191078186035156
I0429 01:27:29.558252 140286080571136 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.2767900228500366, loss=4.874382972717285
I0429 01:27:41.892893 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:27:49.938631 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:27:58.508467 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:28:00.239489 140516526462784 submission_runner.py:415] Time since start: 11928.19s, 	Step: 24828, 	{'train/accuracy': 0.287109375, 'train/loss': 3.5795443058013916, 'validation/accuracy': 0.2659600079059601, 'validation/loss': 3.6835296154022217, 'validation/num_examples': 50000, 'test/accuracy': 0.20360000431537628, 'test/loss': 4.1310930252075195, 'test/num_examples': 10000, 'score': 11418.982477426529, 'total_duration': 11928.187395334244, 'accumulated_submission_time': 11418.982477426529, 'accumulated_eval_time': 507.8430564403534, 'accumulated_logging_time': 0.8965466022491455}
I0429 01:28:00.259331 140286072178432 logging_writer.py:48] [24828] accumulated_eval_time=507.843056, accumulated_logging_time=0.896547, accumulated_submission_time=11418.982477, global_step=24828, preemption_count=0, score=11418.982477, test/accuracy=0.203600, test/loss=4.131093, test/num_examples=10000, total_duration=11928.187395, train/accuracy=0.287109, train/loss=3.579544, validation/accuracy=0.265960, validation/loss=3.683530, validation/num_examples=50000
I0429 01:28:34.006926 140286080571136 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.058933138847351, loss=5.820633888244629
I0429 01:29:19.789157 140286072178432 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8785797357559204, loss=6.160294055938721
I0429 01:30:05.520606 140286080571136 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0439889430999756, loss=5.321447849273682
I0429 01:30:51.485824 140286072178432 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.8804762363433838, loss=6.277309417724609
I0429 01:31:37.144062 140286080571136 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.2711013555526733, loss=4.84375524520874
I0429 01:32:23.011566 140286072178432 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.3726787567138672, loss=4.628904342651367
I0429 01:33:08.926341 140286080571136 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.134326457977295, loss=4.675338268280029
I0429 01:33:55.361614 140286072178432 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.3631354570388794, loss=4.771852493286133
I0429 01:34:41.575694 140286080571136 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.0184252262115479, loss=5.762782096862793
I0429 01:35:00.396208 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:35:08.658843 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:35:17.195214 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:35:18.934288 140516526462784 submission_runner.py:415] Time since start: 12366.88s, 	Step: 25742, 	{'train/accuracy': 0.30812498927116394, 'train/loss': 3.442582368850708, 'validation/accuracy': 0.2874999940395355, 'validation/loss': 3.5533387660980225, 'validation/num_examples': 50000, 'test/accuracy': 0.22130000591278076, 'test/loss': 3.995917320251465, 'test/num_examples': 10000, 'score': 11839.088171720505, 'total_duration': 12366.882110357285, 'accumulated_submission_time': 11839.088171720505, 'accumulated_eval_time': 526.3798241615295, 'accumulated_logging_time': 0.9304492473602295}
I0429 01:35:18.948180 140286072178432 logging_writer.py:48] [25742] accumulated_eval_time=526.379824, accumulated_logging_time=0.930449, accumulated_submission_time=11839.088172, global_step=25742, preemption_count=0, score=11839.088172, test/accuracy=0.221300, test/loss=3.995917, test/num_examples=10000, total_duration=12366.882110, train/accuracy=0.308125, train/loss=3.442582, validation/accuracy=0.287500, validation/loss=3.553339, validation/num_examples=50000
I0429 01:35:46.324337 140286080571136 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.3332844972610474, loss=4.559722423553467
I0429 01:36:32.393235 140286072178432 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2240203619003296, loss=5.038800239562988
I0429 01:37:18.227777 140286080571136 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9363806247711182, loss=6.181163787841797
I0429 01:38:03.976625 140286072178432 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.276179552078247, loss=4.64783239364624
I0429 01:38:49.701616 140286080571136 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.3536056280136108, loss=4.717697620391846
I0429 01:39:35.607682 140286072178432 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.2258440256118774, loss=4.575499057769775
I0429 01:40:21.296538 140286080571136 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.2757067680358887, loss=4.588984966278076
I0429 01:41:06.909229 140286072178432 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1151494979858398, loss=5.2252702713012695
I0429 01:41:52.487626 140286080571136 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.389414668083191, loss=4.626426696777344
I0429 01:42:19.213468 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:42:27.443063 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:42:35.888719 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:42:37.608220 140516526462784 submission_runner.py:415] Time since start: 12805.56s, 	Step: 26658, 	{'train/accuracy': 0.3365820348262787, 'train/loss': 3.2522027492523193, 'validation/accuracy': 0.3110799789428711, 'validation/loss': 3.391826868057251, 'validation/num_examples': 50000, 'test/accuracy': 0.23980000615119934, 'test/loss': 3.86156964302063, 'test/num_examples': 10000, 'score': 12259.323062896729, 'total_duration': 12805.556326150894, 'accumulated_submission_time': 12259.323062896729, 'accumulated_eval_time': 544.7735278606415, 'accumulated_logging_time': 0.9577479362487793}
I0429 01:42:37.622263 140286072178432 logging_writer.py:48] [26658] accumulated_eval_time=544.773528, accumulated_logging_time=0.957748, accumulated_submission_time=12259.323063, global_step=26658, preemption_count=0, score=12259.323063, test/accuracy=0.239800, test/loss=3.861570, test/num_examples=10000, total_duration=12805.556326, train/accuracy=0.336582, train/loss=3.252203, validation/accuracy=0.311080, validation/loss=3.391827, validation/num_examples=50000
I0429 01:42:57.556052 140286080571136 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0215415954589844, loss=5.958421230316162
I0429 01:43:43.611243 140286072178432 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.306855320930481, loss=4.525687217712402
I0429 01:44:29.448790 140286080571136 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2161561250686646, loss=4.988414287567139
I0429 01:45:15.247232 140286072178432 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.012221097946167, loss=5.52990198135376
I0429 01:46:01.105798 140286080571136 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.5028308629989624, loss=4.520120620727539
I0429 01:46:46.866991 140286072178432 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.6149283647537231, loss=4.700734615325928
I0429 01:47:32.674737 140286080571136 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0008797645568848, loss=6.1310811042785645
I0429 01:48:18.587281 140286072178432 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.2089107036590576, loss=4.735633373260498
I0429 01:49:04.519740 140286080571136 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.3144798278808594, loss=4.452054977416992
I0429 01:49:37.661875 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:49:45.907182 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:49:54.292948 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:49:56.024131 140516526462784 submission_runner.py:415] Time since start: 13243.97s, 	Step: 27573, 	{'train/accuracy': 0.3451171815395355, 'train/loss': 3.2231249809265137, 'validation/accuracy': 0.3191799819469452, 'validation/loss': 3.352444887161255, 'validation/num_examples': 50000, 'test/accuracy': 0.24320000410079956, 'test/loss': 3.8348381519317627, 'test/num_examples': 10000, 'score': 12679.33121085167, 'total_duration': 13243.972121953964, 'accumulated_submission_time': 12679.33121085167, 'accumulated_eval_time': 563.1346528530121, 'accumulated_logging_time': 0.985959529876709}
I0429 01:49:56.042848 140286072178432 logging_writer.py:48] [27573] accumulated_eval_time=563.134653, accumulated_logging_time=0.985960, accumulated_submission_time=12679.331211, global_step=27573, preemption_count=0, score=12679.331211, test/accuracy=0.243200, test/loss=3.834838, test/num_examples=10000, total_duration=13243.972122, train/accuracy=0.345117, train/loss=3.223125, validation/accuracy=0.319180, validation/loss=3.352445, validation/num_examples=50000
I0429 01:50:09.075565 140286080571136 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.2842458486557007, loss=4.704355239868164
I0429 01:50:55.623282 140286072178432 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8424278497695923, loss=6.009014129638672
I0429 01:51:41.593446 140286080571136 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.2186874151229858, loss=4.598942279815674
I0429 01:52:27.451800 140286072178432 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.3504031896591187, loss=4.446483135223389
I0429 01:53:12.788279 140516526462784 spec.py:298] Evaluating on the training split.
I0429 01:53:20.986557 140516526462784 spec.py:310] Evaluating on the validation split.
I0429 01:53:29.381327 140516526462784 spec.py:326] Evaluating on the test split.
I0429 01:53:31.109161 140516526462784 submission_runner.py:415] Time since start: 13459.06s, 	Step: 28000, 	{'train/accuracy': 0.36976560950279236, 'train/loss': 3.063415765762329, 'validation/accuracy': 0.3363399803638458, 'validation/loss': 3.2335710525512695, 'validation/num_examples': 50000, 'test/accuracy': 0.2578999996185303, 'test/loss': 3.7437798976898193, 'test/num_examples': 10000, 'score': 12876.054052114487, 'total_duration': 13459.057135105133, 'accumulated_submission_time': 12876.054052114487, 'accumulated_eval_time': 581.4544103145599, 'accumulated_logging_time': 1.0188660621643066}
I0429 01:53:31.124167 140286080571136 logging_writer.py:48] [28000] accumulated_eval_time=581.454410, accumulated_logging_time=1.018866, accumulated_submission_time=12876.054052, global_step=28000, preemption_count=0, score=12876.054052, test/accuracy=0.257900, test/loss=3.743780, test/num_examples=10000, total_duration=13459.057135, train/accuracy=0.369766, train/loss=3.063416, validation/accuracy=0.336340, validation/loss=3.233571, validation/num_examples=50000
I0429 01:53:31.157557 140286072178432 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=12876.054052
I0429 01:53:31.540767 140516526462784 checkpoints.py:356] Saving checkpoint at step: 28000
I0429 01:53:32.641303 140516526462784 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1/checkpoint_28000
I0429 01:53:32.663379 140516526462784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_lamb/imagenet_vit_jax/trial_1/checkpoint_28000.
I0429 01:53:33.181627 140516526462784 submission_runner.py:578] Tuning trial 1/1
I0429 01:53:33.181909 140516526462784 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0429 01:53:33.196337 140516526462784 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 71.82260036468506, 'total_duration': 123.52470660209656, 'accumulated_submission_time': 71.82260036468506, 'accumulated_eval_time': 51.70195937156677, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (877, {'train/accuracy': 0.0016992187593132257, 'train/loss': 6.90775203704834, 'validation/accuracy': 0.0018599999602884054, 'validation/loss': 6.907751560211182, 'validation/num_examples': 50000, 'test/accuracy': 0.0020000000949949026, 'test/loss': 6.9077534675598145, 'test/num_examples': 10000, 'score': 492.1216480731964, 'total_duration': 559.1473796367645, 'accumulated_submission_time': 492.1216480731964, 'accumulated_eval_time': 66.98708009719849, 'accumulated_logging_time': 0.02552485466003418, 'global_step': 877, 'preemption_count': 0}), (1837, {'train/accuracy': 0.0027734374161809683, 'train/loss': 6.907746315002441, 'validation/accuracy': 0.002899999963119626, 'validation/loss': 6.9077467918396, 'validation/num_examples': 50000, 'test/accuracy': 0.003100000089034438, 'test/loss': 6.907748699188232, 'test/num_examples': 10000, 'score': 912.474556684494, 'total_duration': 994.5910148620605, 'accumulated_submission_time': 912.474556684494, 'accumulated_eval_time': 82.03802394866943, 'accumulated_logging_time': 0.051262617111206055, 'global_step': 1837, 'preemption_count': 0}), (2795, {'train/accuracy': 0.004765625111758709, 'train/loss': 6.907737731933594, 'validation/accuracy': 0.004780000075697899, 'validation/loss': 6.907736778259277, 'validation/num_examples': 50000, 'test/accuracy': 0.004600000102072954, 'test/loss': 6.90773868560791, 'test/num_examples': 10000, 'score': 1332.738971233368, 'total_duration': 1430.093086719513, 'accumulated_submission_time': 1332.738971233368, 'accumulated_eval_time': 97.23717474937439, 'accumulated_logging_time': 0.07564115524291992, 'global_step': 2795, 'preemption_count': 0}), (3749, {'train/accuracy': 0.005253906361758709, 'train/loss': 6.90771484375, 'validation/accuracy': 0.004720000084489584, 'validation/loss': 6.907716274261475, 'validation/num_examples': 50000, 'test/accuracy': 0.004100000020116568, 'test/loss': 6.90772008895874, 'test/num_examples': 10000, 'score': 1753.0104467868805, 'total_duration': 1865.4382455348969, 'accumulated_submission_time': 1753.0104467868805, 'accumulated_eval_time': 112.27321934700012, 'accumulated_logging_time': 0.09949588775634766, 'global_step': 3749, 'preemption_count': 0}), (4707, {'train/accuracy': 0.0044531249441206455, 'train/loss': 6.907670021057129, 'validation/accuracy': 0.004359999671578407, 'validation/loss': 6.907674789428711, 'validation/num_examples': 50000, 'test/accuracy': 0.0037000002339482307, 'test/loss': 6.907679080963135, 'test/num_examples': 10000, 'score': 2173.221355199814, 'total_duration': 2300.6553785800934, 'accumulated_submission_time': 2173.221355199814, 'accumulated_eval_time': 127.23758316040039, 'accumulated_logging_time': 0.12731075286865234, 'global_step': 4707, 'preemption_count': 0}), (5638, {'train/accuracy': 0.005624999757856131, 'train/loss': 6.907540321350098, 'validation/accuracy': 0.005200000014156103, 'validation/loss': 6.907548427581787, 'validation/num_examples': 50000, 'test/accuracy': 0.004900000058114529, 'test/loss': 6.907555103302002, 'test/num_examples': 10000, 'score': 2593.5265860557556, 'total_duration': 2736.2538776397705, 'accumulated_submission_time': 2593.5265860557556, 'accumulated_eval_time': 142.48514866828918, 'accumulated_logging_time': 0.15622663497924805, 'global_step': 5638, 'preemption_count': 0}), (6558, {'train/accuracy': 0.0037890623789280653, 'train/loss': 6.90718412399292, 'validation/accuracy': 0.003819999983534217, 'validation/loss': 6.907197952270508, 'validation/num_examples': 50000, 'test/accuracy': 0.003100000089034438, 'test/loss': 6.9072160720825195, 'test/num_examples': 10000, 'score': 3013.7352664470673, 'total_duration': 3171.5439031124115, 'accumulated_submission_time': 3013.7352664470673, 'accumulated_eval_time': 157.5238742828369, 'accumulated_logging_time': 0.18133234977722168, 'global_step': 6558, 'preemption_count': 0}), (7459, {'train/accuracy': 0.002832031110301614, 'train/loss': 6.906032085418701, 'validation/accuracy': 0.002940000034868717, 'validation/loss': 6.906071186065674, 'validation/num_examples': 50000, 'test/accuracy': 0.002300000051036477, 'test/loss': 6.9061408042907715, 'test/num_examples': 10000, 'score': 3434.1195731163025, 'total_duration': 3607.3129789829254, 'accumulated_submission_time': 3434.1195731163025, 'accumulated_eval_time': 172.8629755973816, 'accumulated_logging_time': 0.20996737480163574, 'global_step': 7459, 'preemption_count': 0}), (8360, {'train/accuracy': 0.0018164062639698386, 'train/loss': 6.902470111846924, 'validation/accuracy': 0.0020200000144541264, 'validation/loss': 6.902583122253418, 'validation/num_examples': 50000, 'test/accuracy': 0.0021000001579523087, 'test/loss': 6.902647018432617, 'test/num_examples': 10000, 'score': 3854.103394985199, 'total_duration': 4042.932283639908, 'accumulated_submission_time': 3854.103394985199, 'accumulated_eval_time': 188.45768356323242, 'accumulated_logging_time': 0.23368120193481445, 'global_step': 8360, 'preemption_count': 0}), (9277, {'train/accuracy': 0.0022656249348074198, 'train/loss': 6.884683132171631, 'validation/accuracy': 0.002099999925121665, 'validation/loss': 6.884961128234863, 'validation/num_examples': 50000, 'test/accuracy': 0.0020000000949949026, 'test/loss': 6.8856987953186035, 'test/num_examples': 10000, 'score': 4274.35679936409, 'total_duration': 4479.0248012542725, 'accumulated_submission_time': 4274.35679936409, 'accumulated_eval_time': 204.2496588230133, 'accumulated_logging_time': 0.2633669376373291, 'global_step': 9277, 'preemption_count': 0}), (10192, {'train/accuracy': 0.002070312388241291, 'train/loss': 6.823131084442139, 'validation/accuracy': 0.002059999853372574, 'validation/loss': 6.824239730834961, 'validation/num_examples': 50000, 'test/accuracy': 0.0019000000320374966, 'test/loss': 6.827970504760742, 'test/num_examples': 10000, 'score': 4694.413540363312, 'total_duration': 4914.869418382645, 'accumulated_submission_time': 4694.413540363312, 'accumulated_eval_time': 219.9571614265442, 'accumulated_logging_time': 0.32652974128723145, 'global_step': 10192, 'preemption_count': 0}), (11109, {'train/accuracy': 0.0033398435916751623, 'train/loss': 6.683769226074219, 'validation/accuracy': 0.003339999821037054, 'validation/loss': 6.688389778137207, 'validation/num_examples': 50000, 'test/accuracy': 0.002300000051036477, 'test/loss': 6.700666904449463, 'test/num_examples': 10000, 'score': 5114.647413015366, 'total_duration': 5350.9400045871735, 'accumulated_submission_time': 5114.647413015366, 'accumulated_eval_time': 235.74602389335632, 'accumulated_logging_time': 0.3572261333465576, 'global_step': 11109, 'preemption_count': 0}), (12025, {'train/accuracy': 0.0063281250186264515, 'train/loss': 6.51516056060791, 'validation/accuracy': 0.005219999700784683, 'validation/loss': 6.526479244232178, 'validation/num_examples': 50000, 'test/accuracy': 0.004399999976158142, 'test/loss': 6.549435138702393, 'test/num_examples': 10000, 'score': 5534.636485338211, 'total_duration': 5786.686399459839, 'accumulated_submission_time': 5534.636485338211, 'accumulated_eval_time': 251.4504623413086, 'accumulated_logging_time': 0.39244508743286133, 'global_step': 12025, 'preemption_count': 0}), (12939, {'train/accuracy': 0.013710937462747097, 'train/loss': 6.29306697845459, 'validation/accuracy': 0.013039999641478062, 'validation/loss': 6.310734272003174, 'validation/num_examples': 50000, 'test/accuracy': 0.010400000959634781, 'test/loss': 6.3621087074279785, 'test/num_examples': 10000, 'score': 5954.945756673813, 'total_duration': 6223.640339136124, 'accumulated_submission_time': 5954.945756673813, 'accumulated_eval_time': 268.0508875846863, 'accumulated_logging_time': 0.4192776679992676, 'global_step': 12939, 'preemption_count': 0}), (13853, {'train/accuracy': 0.02363281138241291, 'train/loss': 6.047488212585449, 'validation/accuracy': 0.023919999599456787, 'validation/loss': 6.066572189331055, 'validation/num_examples': 50000, 'test/accuracy': 0.020000001415610313, 'test/loss': 6.14048957824707, 'test/num_examples': 10000, 'score': 6375.14003610611, 'total_duration': 6661.062366962433, 'accumulated_submission_time': 6375.14003610611, 'accumulated_eval_time': 285.2301926612854, 'accumulated_logging_time': 0.450606107711792, 'global_step': 13853, 'preemption_count': 0}), (14768, {'train/accuracy': 0.03996093571186066, 'train/loss': 5.802436351776123, 'validation/accuracy': 0.034539997577667236, 'validation/loss': 5.829770565032959, 'validation/num_examples': 50000, 'test/accuracy': 0.026600001379847527, 'test/loss': 5.931950569152832, 'test/num_examples': 10000, 'score': 6795.54832983017, 'total_duration': 7099.264804124832, 'accumulated_submission_time': 6795.54832983017, 'accumulated_eval_time': 302.9644935131073, 'accumulated_logging_time': 0.4932529926300049, 'global_step': 14768, 'preemption_count': 0}), (15683, {'train/accuracy': 0.05269531160593033, 'train/loss': 5.556304931640625, 'validation/accuracy': 0.04910000041127205, 'validation/loss': 5.585467338562012, 'validation/num_examples': 50000, 'test/accuracy': 0.039000000804662704, 'test/loss': 5.737371921539307, 'test/num_examples': 10000, 'score': 7215.868463277817, 'total_duration': 7537.214565992355, 'accumulated_submission_time': 7215.868463277817, 'accumulated_eval_time': 320.5335536003113, 'accumulated_logging_time': 0.5366556644439697, 'global_step': 15683, 'preemption_count': 0}), (16597, {'train/accuracy': 0.06566406041383743, 'train/loss': 5.346097469329834, 'validation/accuracy': 0.06191999837756157, 'validation/loss': 5.388829708099365, 'validation/num_examples': 50000, 'test/accuracy': 0.048700001090765, 'test/loss': 5.561663627624512, 'test/num_examples': 10000, 'score': 7636.107887268066, 'total_duration': 7975.966424465179, 'accumulated_submission_time': 7636.107887268066, 'accumulated_eval_time': 338.98077416419983, 'accumulated_logging_time': 0.5847828388214111, 'global_step': 16597, 'preemption_count': 0}), (17513, {'train/accuracy': 0.08863280713558197, 'train/loss': 5.139340877532959, 'validation/accuracy': 0.08171999454498291, 'validation/loss': 5.184188365936279, 'validation/num_examples': 50000, 'test/accuracy': 0.0609000027179718, 'test/loss': 5.37208366394043, 'test/num_examples': 10000, 'score': 8056.475097417831, 'total_duration': 8415.157010793686, 'accumulated_submission_time': 8056.475097417831, 'accumulated_eval_time': 357.7599320411682, 'accumulated_logging_time': 0.612074613571167, 'global_step': 17513, 'preemption_count': 0}), (18427, {'train/accuracy': 0.11124999821186066, 'train/loss': 4.975393772125244, 'validation/accuracy': 0.10185999423265457, 'validation/loss': 5.019129276275635, 'validation/num_examples': 50000, 'test/accuracy': 0.08020000159740448, 'test/loss': 5.244736194610596, 'test/num_examples': 10000, 'score': 8476.856865644455, 'total_duration': 8854.143136501312, 'accumulated_submission_time': 8476.856865644455, 'accumulated_eval_time': 376.3133590221405, 'accumulated_logging_time': 0.6458587646484375, 'global_step': 18427, 'preemption_count': 0}), (19340, {'train/accuracy': 0.1360742151737213, 'train/loss': 4.734663963317871, 'validation/accuracy': 0.12573999166488647, 'validation/loss': 4.779326915740967, 'validation/num_examples': 50000, 'test/accuracy': 0.09930000454187393, 'test/loss': 5.024278163909912, 'test/num_examples': 10000, 'score': 8897.150470256805, 'total_duration': 9293.258126020432, 'accumulated_submission_time': 8897.150470256805, 'accumulated_eval_time': 395.0759916305542, 'accumulated_logging_time': 0.6843357086181641, 'global_step': 19340, 'preemption_count': 0}), (20252, {'train/accuracy': 0.1693750023841858, 'train/loss': 4.46301794052124, 'validation/accuracy': 0.15501999855041504, 'validation/loss': 4.524064540863037, 'validation/num_examples': 50000, 'test/accuracy': 0.1201000064611435, 'test/loss': 4.82213830947876, 'test/num_examples': 10000, 'score': 9317.424408435822, 'total_duration': 9733.355473518372, 'accumulated_submission_time': 9317.424408435822, 'accumulated_eval_time': 414.8429260253906, 'accumulated_logging_time': 0.7208709716796875, 'global_step': 20252, 'preemption_count': 0}), (21167, {'train/accuracy': 0.1922656148672104, 'train/loss': 4.295621395111084, 'validation/accuracy': 0.17509999871253967, 'validation/loss': 4.376280307769775, 'validation/num_examples': 50000, 'test/accuracy': 0.1355000138282776, 'test/loss': 4.690027236938477, 'test/num_examples': 10000, 'score': 9737.838124752045, 'total_duration': 10172.815248250961, 'accumulated_submission_time': 9737.838124752045, 'accumulated_eval_time': 433.82896542549133, 'accumulated_logging_time': 0.7613263130187988, 'global_step': 21167, 'preemption_count': 0}), (22081, {'train/accuracy': 0.2122460901737213, 'train/loss': 4.083638668060303, 'validation/accuracy': 0.19971999526023865, 'validation/loss': 4.168559551239014, 'validation/num_examples': 50000, 'test/accuracy': 0.1519000083208084, 'test/loss': 4.520297050476074, 'test/num_examples': 10000, 'score': 10158.02670264244, 'total_duration': 10612.253339529037, 'accumulated_submission_time': 10158.02670264244, 'accumulated_eval_time': 453.01153206825256, 'accumulated_logging_time': 0.805086612701416, 'global_step': 22081, 'preemption_count': 0}), (22997, {'train/accuracy': 0.24406249821186066, 'train/loss': 3.900546073913574, 'validation/accuracy': 0.22741998732089996, 'validation/loss': 3.989886999130249, 'validation/num_examples': 50000, 'test/accuracy': 0.17720000445842743, 'test/loss': 4.387495040893555, 'test/num_examples': 10000, 'score': 10578.105655193329, 'total_duration': 11050.475373029709, 'accumulated_submission_time': 10578.105655193329, 'accumulated_eval_time': 471.1069643497467, 'accumulated_logging_time': 0.8341519832611084, 'global_step': 22997, 'preemption_count': 0}), (23911, {'train/accuracy': 0.27519530057907104, 'train/loss': 3.694805145263672, 'validation/accuracy': 0.2483999878168106, 'validation/loss': 3.8292949199676514, 'validation/num_examples': 50000, 'test/accuracy': 0.18230000138282776, 'test/loss': 4.254889011383057, 'test/num_examples': 10000, 'score': 10998.548694849014, 'total_duration': 11489.362217664719, 'accumulated_submission_time': 10998.548694849014, 'accumulated_eval_time': 489.4976906776428, 'accumulated_logging_time': 0.8689968585968018, 'global_step': 23911, 'preemption_count': 0}), (24828, {'train/accuracy': 0.287109375, 'train/loss': 3.5795443058013916, 'validation/accuracy': 0.2659600079059601, 'validation/loss': 3.6835296154022217, 'validation/num_examples': 50000, 'test/accuracy': 0.20360000431537628, 'test/loss': 4.1310930252075195, 'test/num_examples': 10000, 'score': 11418.982477426529, 'total_duration': 11928.187395334244, 'accumulated_submission_time': 11418.982477426529, 'accumulated_eval_time': 507.8430564403534, 'accumulated_logging_time': 0.8965466022491455, 'global_step': 24828, 'preemption_count': 0}), (25742, {'train/accuracy': 0.30812498927116394, 'train/loss': 3.442582368850708, 'validation/accuracy': 0.2874999940395355, 'validation/loss': 3.5533387660980225, 'validation/num_examples': 50000, 'test/accuracy': 0.22130000591278076, 'test/loss': 3.995917320251465, 'test/num_examples': 10000, 'score': 11839.088171720505, 'total_duration': 12366.882110357285, 'accumulated_submission_time': 11839.088171720505, 'accumulated_eval_time': 526.3798241615295, 'accumulated_logging_time': 0.9304492473602295, 'global_step': 25742, 'preemption_count': 0}), (26658, {'train/accuracy': 0.3365820348262787, 'train/loss': 3.2522027492523193, 'validation/accuracy': 0.3110799789428711, 'validation/loss': 3.391826868057251, 'validation/num_examples': 50000, 'test/accuracy': 0.23980000615119934, 'test/loss': 3.86156964302063, 'test/num_examples': 10000, 'score': 12259.323062896729, 'total_duration': 12805.556326150894, 'accumulated_submission_time': 12259.323062896729, 'accumulated_eval_time': 544.7735278606415, 'accumulated_logging_time': 0.9577479362487793, 'global_step': 26658, 'preemption_count': 0}), (27573, {'train/accuracy': 0.3451171815395355, 'train/loss': 3.2231249809265137, 'validation/accuracy': 0.3191799819469452, 'validation/loss': 3.352444887161255, 'validation/num_examples': 50000, 'test/accuracy': 0.24320000410079956, 'test/loss': 3.8348381519317627, 'test/num_examples': 10000, 'score': 12679.33121085167, 'total_duration': 13243.972121953964, 'accumulated_submission_time': 12679.33121085167, 'accumulated_eval_time': 563.1346528530121, 'accumulated_logging_time': 0.985959529876709, 'global_step': 27573, 'preemption_count': 0}), (28000, {'train/accuracy': 0.36976560950279236, 'train/loss': 3.063415765762329, 'validation/accuracy': 0.3363399803638458, 'validation/loss': 3.2335710525512695, 'validation/num_examples': 50000, 'test/accuracy': 0.2578999996185303, 'test/loss': 3.7437798976898193, 'test/num_examples': 10000, 'score': 12876.054052114487, 'total_duration': 13459.057135105133, 'accumulated_submission_time': 12876.054052114487, 'accumulated_eval_time': 581.4544103145599, 'accumulated_logging_time': 1.0188660621643066, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0429 01:53:33.196542 140516526462784 submission_runner.py:581] Timing: 12876.054052114487
I0429 01:53:33.196592 140516526462784 submission_runner.py:582] ====================
I0429 01:53:33.196757 140516526462784 submission_runner.py:645] Final imagenet_vit score: 12876.054052114487
