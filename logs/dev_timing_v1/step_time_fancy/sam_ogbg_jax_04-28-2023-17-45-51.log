python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/sam/jax/submission.py --tuning_search_space=baselines/sam/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy/timing_sam --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_jax_04-28-2023-17-45-51.log
I0428 17:46:13.026174 139630582925120 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_fancy/timing_sam/ogbg_jax because --overwrite was set.
I0428 17:46:13.027634 139630582925120 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy/timing_sam/ogbg_jax.
I0428 17:46:13.089926 139630582925120 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 17:46:13.949114 139630582925120 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0428 17:46:13.950002 139630582925120 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 17:46:13.955765 139630582925120 submission_runner.py:538] Using RNG seed 792007727
I0428 17:46:16.602785 139630582925120 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 17:46:16.603039 139630582925120 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1.
I0428 17:46:16.603208 139630582925120 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1/hparams.json.
I0428 17:46:16.745142 139630582925120 submission_runner.py:241] Initializing dataset.
I0428 17:46:17.005864 139630582925120 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:46:17.012538 139630582925120 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 17:46:17.260448 139630582925120 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:46:17.320878 139630582925120 submission_runner.py:248] Initializing model.
I0428 17:46:25.042319 139630582925120 submission_runner.py:258] Initializing optimizer.
I0428 17:46:25.496563 139630582925120 submission_runner.py:265] Initializing metrics bundle.
I0428 17:46:25.496767 139630582925120 submission_runner.py:282] Initializing checkpoint and logger.
I0428 17:46:25.498023 139630582925120 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1 with prefix checkpoint_
I0428 17:46:25.498306 139630582925120 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 17:46:25.498414 139630582925120 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 17:46:26.454164 139630582925120 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1/meta_data_0.json.
I0428 17:46:26.455165 139630582925120 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1/flags_0.json.
I0428 17:46:26.460701 139630582925120 submission_runner.py:318] Starting training loop.
I0428 17:46:54.292144 139454064883456 logging_writer.py:48] [0] global_step=0, grad_norm=2.806926965713501, loss=0.8041685819625854
I0428 17:46:54.305776 139630582925120 spec.py:298] Evaluating on the training split.
I0428 17:46:54.314552 139630582925120 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:46:54.318651 139630582925120 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 17:46:54.379693 139630582925120 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:48:34.565767 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 17:48:34.569074 139630582925120 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:48:34.573412 139630582925120 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 17:48:34.628309 139630582925120 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:49:43.656776 139630582925120 spec.py:326] Evaluating on the test split.
I0428 17:49:43.660092 139630582925120 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:49:43.664462 139630582925120 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0428 17:49:43.719698 139630582925120 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0428 17:50:54.596866 139630582925120 submission_runner.py:415] Time since start: 268.14s, 	Step: 1, 	{'train/accuracy': 0.4474038779735565, 'train/loss': 0.8037292957305908, 'train/mean_average_precision': 0.02198940286324916, 'validation/accuracy': 0.452945351600647, 'validation/loss': 0.8001374006271362, 'validation/mean_average_precision': 0.026602841651966674, 'validation/num_examples': 43793, 'test/accuracy': 0.45501387119293213, 'test/loss': 0.7988821864128113, 'test/mean_average_precision': 0.02830481325165795, 'test/num_examples': 43793, 'score': 27.84489369392395, 'total_duration': 268.1360971927643, 'accumulated_submission_time': 27.84489369392395, 'accumulated_eval_time': 240.29102301597595, 'accumulated_logging_time': 0}
I0428 17:50:54.614207 139444618860288 logging_writer.py:48] [1] accumulated_eval_time=240.291023, accumulated_logging_time=0, accumulated_submission_time=27.844894, global_step=1, preemption_count=0, score=27.844894, test/accuracy=0.455014, test/loss=0.798882, test/mean_average_precision=0.028305, test/num_examples=43793, total_duration=268.136097, train/accuracy=0.447404, train/loss=0.803729, train/mean_average_precision=0.021989, validation/accuracy=0.452945, validation/loss=0.800137, validation/mean_average_precision=0.026603, validation/num_examples=43793
I0428 17:51:22.296922 139444627252992 logging_writer.py:48] [100] global_step=100, grad_norm=0.4330655038356781, loss=0.3916240632534027
I0428 17:51:50.365200 139444618860288 logging_writer.py:48] [200] global_step=200, grad_norm=0.2972545027732849, loss=0.2600528597831726
I0428 17:52:17.179718 139444627252992 logging_writer.py:48] [300] global_step=300, grad_norm=0.17300398647785187, loss=0.15642058849334717
I0428 17:52:44.207243 139444618860288 logging_writer.py:48] [400] global_step=400, grad_norm=0.09656818956136703, loss=0.10693912208080292
I0428 17:53:11.216186 139444627252992 logging_writer.py:48] [500] global_step=500, grad_norm=0.06131273880600929, loss=0.07880516350269318
I0428 17:53:38.365002 139444618860288 logging_writer.py:48] [600] global_step=600, grad_norm=0.036049190908670425, loss=0.06986498832702637
I0428 17:54:05.562386 139444627252992 logging_writer.py:48] [700] global_step=700, grad_norm=0.028103670105338097, loss=0.06517186015844345
I0428 17:54:32.949278 139444618860288 logging_writer.py:48] [800] global_step=800, grad_norm=0.019790614023804665, loss=0.054593440145254135
I0428 17:54:54.648492 139630582925120 spec.py:298] Evaluating on the training split.
I0428 17:56:17.132271 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 17:56:19.800775 139630582925120 spec.py:326] Evaluating on the test split.
I0428 17:56:22.363225 139630582925120 submission_runner.py:415] Time since start: 595.90s, 	Step: 879, 	{'train/accuracy': 0.9867270588874817, 'train/loss': 0.05623584985733032, 'train/mean_average_precision': 0.043487523548067, 'validation/accuracy': 0.984115481376648, 'validation/loss': 0.06535669416189194, 'validation/mean_average_precision': 0.04344390175778309, 'validation/num_examples': 43793, 'test/accuracy': 0.9831374287605286, 'test/loss': 0.06847526133060455, 'test/mean_average_precision': 0.04440379058799089, 'test/num_examples': 43793, 'score': 267.8613471984863, 'total_duration': 595.9024682044983, 'accumulated_submission_time': 267.8613471984863, 'accumulated_eval_time': 328.00572085380554, 'accumulated_logging_time': 0.027959108352661133}
I0428 17:56:22.371903 139444627252992 logging_writer.py:48] [879] accumulated_eval_time=328.005721, accumulated_logging_time=0.027959, accumulated_submission_time=267.861347, global_step=879, preemption_count=0, score=267.861347, test/accuracy=0.983137, test/loss=0.068475, test/mean_average_precision=0.044404, test/num_examples=43793, total_duration=595.902468, train/accuracy=0.986727, train/loss=0.056236, train/mean_average_precision=0.043488, validation/accuracy=0.984115, validation/loss=0.065357, validation/mean_average_precision=0.043444, validation/num_examples=43793
I0428 17:56:28.480626 139444618860288 logging_writer.py:48] [900] global_step=900, grad_norm=0.018228678032755852, loss=0.05280158668756485
I0428 17:56:56.038559 139444627252992 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.02099747583270073, loss=0.053861770778894424
I0428 17:57:23.743974 139444618860288 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.018467847257852554, loss=0.050820376724004745
I0428 17:57:51.472496 139444627252992 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.019218383356928825, loss=0.04909781366586685
I0428 17:58:19.066077 139444618860288 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.023906899616122246, loss=0.052452459931373596
I0428 17:58:46.592385 139444627252992 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.019997820258140564, loss=0.04933615401387215
I0428 17:59:13.946199 139444618860288 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.023895656690001488, loss=0.0465145967900753
I0428 17:59:41.152539 139444627252992 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0294774379581213, loss=0.04958948865532875
I0428 18:00:08.535082 139444618860288 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.029611961916089058, loss=0.05090382695198059
I0428 18:00:22.412247 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:01:44.575742 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:01:47.199107 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:01:49.797806 139630582925120 submission_runner.py:415] Time since start: 923.34s, 	Step: 1752, 	{'train/accuracy': 0.9871780276298523, 'train/loss': 0.047059305012226105, 'train/mean_average_precision': 0.10052400760132467, 'validation/accuracy': 0.984489381313324, 'validation/loss': 0.05655616149306297, 'validation/mean_average_precision': 0.10096203050020441, 'validation/num_examples': 43793, 'test/accuracy': 0.9834887385368347, 'test/loss': 0.06012658029794693, 'test/mean_average_precision': 0.09731338357910031, 'test/num_examples': 43793, 'score': 507.8837742805481, 'total_duration': 923.3370501995087, 'accumulated_submission_time': 507.8837742805481, 'accumulated_eval_time': 415.39124369621277, 'accumulated_logging_time': 0.04739069938659668}
I0428 18:01:49.806475 139444627252992 logging_writer.py:48] [1752] accumulated_eval_time=415.391244, accumulated_logging_time=0.047391, accumulated_submission_time=507.883774, global_step=1752, preemption_count=0, score=507.883774, test/accuracy=0.983489, test/loss=0.060127, test/mean_average_precision=0.097313, test/num_examples=43793, total_duration=923.337050, train/accuracy=0.987178, train/loss=0.047059, train/mean_average_precision=0.100524, validation/accuracy=0.984489, validation/loss=0.056556, validation/mean_average_precision=0.100962, validation/num_examples=43793
I0428 18:02:03.169114 139444618860288 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.01789436861872673, loss=0.044268544763326645
I0428 18:02:30.568560 139444627252992 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03843191638588905, loss=0.05205247178673744
I0428 18:02:58.046395 139444618860288 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.023811589926481247, loss=0.042629506438970566
I0428 18:03:25.750035 139444627252992 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.028126655146479607, loss=0.0486033670604229
I0428 18:03:53.337241 139444618860288 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.023027531802654266, loss=0.048081472516059875
I0428 18:04:20.682742 139444627252992 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0251101516187191, loss=0.042266495525836945
I0428 18:04:48.150393 139444618860288 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.032627005130052567, loss=0.05280355364084244
I0428 18:05:15.761689 139444627252992 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0330791175365448, loss=0.04662336781620979
I0428 18:05:43.115292 139444618860288 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.02417459338903427, loss=0.0442950502038002
I0428 18:05:49.859407 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:07:11.187973 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:07:13.839183 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:07:16.413907 139630582925120 submission_runner.py:415] Time since start: 1249.95s, 	Step: 2626, 	{'train/accuracy': 0.9876143932342529, 'train/loss': 0.04406202211976051, 'train/mean_average_precision': 0.14648878010098978, 'validation/accuracy': 0.9849151968955994, 'validation/loss': 0.05368294194340706, 'validation/mean_average_precision': 0.1382055181913488, 'validation/num_examples': 43793, 'test/accuracy': 0.9839082360267639, 'test/loss': 0.05688140541315079, 'test/mean_average_precision': 0.13109430951307707, 'test/num_examples': 43793, 'score': 747.9164221286774, 'total_duration': 1249.9531540870667, 'accumulated_submission_time': 747.9164221286774, 'accumulated_eval_time': 501.9457104206085, 'accumulated_logging_time': 0.06868147850036621}
I0428 18:07:16.422658 139444627252992 logging_writer.py:48] [2626] accumulated_eval_time=501.945710, accumulated_logging_time=0.068681, accumulated_submission_time=747.916422, global_step=2626, preemption_count=0, score=747.916422, test/accuracy=0.983908, test/loss=0.056881, test/mean_average_precision=0.131094, test/num_examples=43793, total_duration=1249.953154, train/accuracy=0.987614, train/loss=0.044062, train/mean_average_precision=0.146489, validation/accuracy=0.984915, validation/loss=0.053683, validation/mean_average_precision=0.138206, validation/num_examples=43793
I0428 18:07:37.034042 139444618860288 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.03105289116501808, loss=0.04691791534423828
I0428 18:08:04.467134 139444627252992 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.016712354496121407, loss=0.04465416818857193
I0428 18:08:31.985802 139444618860288 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.022616587579250336, loss=0.04536699503660202
I0428 18:08:59.703171 139444627252992 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.015592951327562332, loss=0.046424441039562225
I0428 18:09:27.512646 139444618860288 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01830604486167431, loss=0.04502421244978905
I0428 18:09:54.985991 139444627252992 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.016969747841358185, loss=0.049605436623096466
I0428 18:10:22.422055 139444618860288 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02508680894970894, loss=0.04523707926273346
I0428 18:10:49.975417 139444627252992 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.029521262273192406, loss=0.04997197166085243
I0428 18:11:16.634758 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:12:37.669853 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:12:40.320413 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:12:42.912989 139630582925120 submission_runner.py:415] Time since start: 1576.45s, 	Step: 3498, 	{'train/accuracy': 0.9876424074172974, 'train/loss': 0.043965838849544525, 'train/mean_average_precision': 0.1646973315657152, 'validation/accuracy': 0.9847983121871948, 'validation/loss': 0.0540594644844532, 'validation/mean_average_precision': 0.1501046815276347, 'validation/num_examples': 43793, 'test/accuracy': 0.9838513731956482, 'test/loss': 0.05705874785780907, 'test/mean_average_precision': 0.15104521779783212, 'test/num_examples': 43793, 'score': 988.1093020439148, 'total_duration': 1576.452208995819, 'accumulated_submission_time': 988.1093020439148, 'accumulated_eval_time': 588.2238848209381, 'accumulated_logging_time': 0.08892083168029785}
I0428 18:12:42.922085 139444618860288 logging_writer.py:48] [3498] accumulated_eval_time=588.223885, accumulated_logging_time=0.088921, accumulated_submission_time=988.109302, global_step=3498, preemption_count=0, score=988.109302, test/accuracy=0.983851, test/loss=0.057059, test/mean_average_precision=0.151045, test/num_examples=43793, total_duration=1576.452209, train/accuracy=0.987642, train/loss=0.043966, train/mean_average_precision=0.164697, validation/accuracy=0.984798, validation/loss=0.054059, validation/mean_average_precision=0.150105, validation/num_examples=43793
I0428 18:12:43.791295 139444627252992 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.019916223362088203, loss=0.046699341386556625
I0428 18:13:11.290243 139444618860288 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.020400339737534523, loss=0.04395310580730438
I0428 18:13:39.787230 139444627252992 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.017770491540431976, loss=0.04029671102762222
I0428 18:14:07.651816 139444618860288 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.01719839498400688, loss=0.04440734535455704
I0428 18:14:35.681262 139444627252992 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.026869554072618484, loss=0.04176889359951019
I0428 18:15:03.279408 139444618860288 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.022739147767424583, loss=0.04441217705607414
I0428 18:15:30.671566 139444627252992 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.013827977702021599, loss=0.041580479592084885
I0428 18:15:58.348345 139444618860288 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.01500089280307293, loss=0.04270287975668907
I0428 18:16:25.939764 139444627252992 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.018236856907606125, loss=0.043891772627830505
I0428 18:16:43.113300 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:18:05.955421 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:18:08.593270 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:18:11.164104 139630582925120 submission_runner.py:415] Time since start: 1904.70s, 	Step: 4363, 	{'train/accuracy': 0.988119900226593, 'train/loss': 0.04061496630311012, 'train/mean_average_precision': 0.20308144544828297, 'validation/accuracy': 0.9853211641311646, 'validation/loss': 0.050038646906614304, 'validation/mean_average_precision': 0.16596586544292788, 'validation/num_examples': 43793, 'test/accuracy': 0.9844410419464111, 'test/loss': 0.05278030410408974, 'test/mean_average_precision': 0.16638581916281578, 'test/num_examples': 43793, 'score': 1228.2813785076141, 'total_duration': 1904.703331232071, 'accumulated_submission_time': 1228.2813785076141, 'accumulated_eval_time': 676.2746350765228, 'accumulated_logging_time': 0.1090700626373291}
I0428 18:18:11.173290 139444618860288 logging_writer.py:48] [4363] accumulated_eval_time=676.274635, accumulated_logging_time=0.109070, accumulated_submission_time=1228.281379, global_step=4363, preemption_count=0, score=1228.281379, test/accuracy=0.984441, test/loss=0.052780, test/mean_average_precision=0.166386, test/num_examples=43793, total_duration=1904.703331, train/accuracy=0.988120, train/loss=0.040615, train/mean_average_precision=0.203081, validation/accuracy=0.985321, validation/loss=0.050039, validation/mean_average_precision=0.165966, validation/num_examples=43793
I0428 18:18:21.578545 139444627252992 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.011762470006942749, loss=0.042448271065950394
I0428 18:18:49.101935 139444618860288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.01957869902253151, loss=0.040874455124139786
I0428 18:19:16.339203 139444627252992 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01158823724836111, loss=0.03983133286237717
I0428 18:19:43.731894 139444618860288 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.013716455549001694, loss=0.03890461102128029
I0428 18:20:11.290527 139444627252992 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.015971357002854347, loss=0.04169094190001488
I0428 18:20:38.526717 139444618860288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.010529926978051662, loss=0.03729898855090141
I0428 18:21:05.950843 139444627252992 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.023048926144838333, loss=0.04341435059905052
I0428 18:21:33.400524 139444618860288 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.01545199379324913, loss=0.04056302830576897
I0428 18:22:00.735286 139444627252992 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.02248191274702549, loss=0.042163699865341187
I0428 18:22:11.404888 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:23:34.058609 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:23:36.960081 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:23:39.567390 139630582925120 submission_runner.py:415] Time since start: 2233.11s, 	Step: 5240, 	{'train/accuracy': 0.988368809223175, 'train/loss': 0.03984415531158447, 'train/mean_average_precision': 0.20972131454011603, 'validation/accuracy': 0.9855983853340149, 'validation/loss': 0.04954816401004791, 'validation/mean_average_precision': 0.18420487243507075, 'validation/num_examples': 43793, 'test/accuracy': 0.9846532940864563, 'test/loss': 0.05229584500193596, 'test/mean_average_precision': 0.18897304117122113, 'test/num_examples': 43793, 'score': 1468.4950156211853, 'total_duration': 2233.1066329479218, 'accumulated_submission_time': 1468.4950156211853, 'accumulated_eval_time': 764.437097787857, 'accumulated_logging_time': 0.1285855770111084}
I0428 18:23:39.576605 139444618860288 logging_writer.py:48] [5240] accumulated_eval_time=764.437098, accumulated_logging_time=0.128586, accumulated_submission_time=1468.495016, global_step=5240, preemption_count=0, score=1468.495016, test/accuracy=0.984653, test/loss=0.052296, test/mean_average_precision=0.188973, test/num_examples=43793, total_duration=2233.106633, train/accuracy=0.988369, train/loss=0.039844, train/mean_average_precision=0.209721, validation/accuracy=0.985598, validation/loss=0.049548, validation/mean_average_precision=0.184205, validation/num_examples=43793
I0428 18:23:56.426425 139444627252992 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01275178324431181, loss=0.042789243161678314
I0428 18:24:24.223554 139444618860288 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.019340671598911285, loss=0.03964172676205635
I0428 18:24:51.936624 139444627252992 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013617114163935184, loss=0.042172472923994064
I0428 18:25:19.246647 139444618860288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.02297838218510151, loss=0.0406656339764595
I0428 18:25:46.789577 139444627252992 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.013848871923983097, loss=0.041128404438495636
I0428 18:26:14.326373 139444618860288 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.015514016151428223, loss=0.042853545397520065
I0428 18:26:41.724591 139444627252992 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.010742428712546825, loss=0.03826561197638512
I0428 18:27:09.063103 139444618860288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012209325097501278, loss=0.03752780333161354
I0428 18:27:36.387873 139444627252992 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01310967467725277, loss=0.04220262169837952
I0428 18:27:39.685696 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:29:03.361998 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:29:06.021311 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:29:08.608509 139630582925120 submission_runner.py:415] Time since start: 2562.15s, 	Step: 6113, 	{'train/accuracy': 0.9888556599617004, 'train/loss': 0.03834446147084236, 'train/mean_average_precision': 0.2390944515342653, 'validation/accuracy': 0.9859142303466797, 'validation/loss': 0.04787725582718849, 'validation/mean_average_precision': 0.19990934552465386, 'validation/num_examples': 43793, 'test/accuracy': 0.9850024580955505, 'test/loss': 0.05042821168899536, 'test/mean_average_precision': 0.2069670324166879, 'test/num_examples': 43793, 'score': 1708.586285352707, 'total_duration': 2562.147730588913, 'accumulated_submission_time': 1708.586285352707, 'accumulated_eval_time': 853.3598494529724, 'accumulated_logging_time': 0.14761829376220703}
I0428 18:29:08.617857 139444618860288 logging_writer.py:48] [6113] accumulated_eval_time=853.359849, accumulated_logging_time=0.147618, accumulated_submission_time=1708.586285, global_step=6113, preemption_count=0, score=1708.586285, test/accuracy=0.985002, test/loss=0.050428, test/mean_average_precision=0.206967, test/num_examples=43793, total_duration=2562.147731, train/accuracy=0.988856, train/loss=0.038344, train/mean_average_precision=0.239094, validation/accuracy=0.985914, validation/loss=0.047877, validation/mean_average_precision=0.199909, validation/num_examples=43793
I0428 18:29:33.020872 139444627252992 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01004223246127367, loss=0.03876129910349846
I0428 18:30:00.475574 139444618860288 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.010859075002372265, loss=0.04001212120056152
I0428 18:30:28.233223 139444627252992 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.010875296778976917, loss=0.03941677138209343
I0428 18:30:55.924143 139444618860288 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.013965051621198654, loss=0.03974252566695213
I0428 18:31:23.933300 139444627252992 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.012319878675043583, loss=0.04056449979543686
I0428 18:31:51.762916 139444618860288 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.011885841377079487, loss=0.03982379287481308
I0428 18:32:19.421351 139444627252992 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.012179262936115265, loss=0.04004356265068054
I0428 18:32:46.968401 139444618860288 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.01079506054520607, loss=0.043093595653772354
I0428 18:33:08.773291 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:34:31.865525 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:34:34.518204 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:34:37.109202 139630582925120 submission_runner.py:415] Time since start: 2890.65s, 	Step: 6980, 	{'train/accuracy': 0.9890006184577942, 'train/loss': 0.03759165108203888, 'train/mean_average_precision': 0.25417132140309673, 'validation/accuracy': 0.9858785271644592, 'validation/loss': 0.04785815253853798, 'validation/mean_average_precision': 0.20871214345267797, 'validation/num_examples': 43793, 'test/accuracy': 0.985045850276947, 'test/loss': 0.050616100430488586, 'test/mean_average_precision': 0.21582822814491018, 'test/num_examples': 43793, 'score': 1948.7240340709686, 'total_duration': 2890.6484472751617, 'accumulated_submission_time': 1948.7240340709686, 'accumulated_eval_time': 941.6957247257233, 'accumulated_logging_time': 0.16651058197021484}
I0428 18:34:37.118810 139444627252992 logging_writer.py:48] [6980] accumulated_eval_time=941.695725, accumulated_logging_time=0.166511, accumulated_submission_time=1948.724034, global_step=6980, preemption_count=0, score=1948.724034, test/accuracy=0.985046, test/loss=0.050616, test/mean_average_precision=0.215828, test/num_examples=43793, total_duration=2890.648447, train/accuracy=0.989001, train/loss=0.037592, train/mean_average_precision=0.254171, validation/accuracy=0.985879, validation/loss=0.047858, validation/mean_average_precision=0.208712, validation/num_examples=43793
I0428 18:34:43.119022 139444618860288 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.011902435682713985, loss=0.04067819565534592
I0428 18:35:10.697091 139444627252992 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.028977518901228905, loss=0.04157060384750366
I0428 18:35:38.390328 139444618860288 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.017662744969129562, loss=0.03934619575738907
I0428 18:36:06.120426 139444627252992 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.015815548598766327, loss=0.041235968470573425
I0428 18:36:34.111352 139444618860288 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.018082795664668083, loss=0.039192892611026764
I0428 18:37:01.715287 139444627252992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.01671733893454075, loss=0.04120877757668495
I0428 18:37:29.412269 139444618860288 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.011317265219986439, loss=0.03936421126127243
I0428 18:37:57.050741 139444627252992 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.013812469318509102, loss=0.0427292138338089
I0428 18:38:25.058682 139444618860288 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.013673784211277962, loss=0.03801480680704117
I0428 18:38:37.262660 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:39:59.791708 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:40:02.424896 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:40:05.051932 139630582925120 submission_runner.py:415] Time since start: 3218.59s, 	Step: 7845, 	{'train/accuracy': 0.9893801212310791, 'train/loss': 0.036087408661842346, 'train/mean_average_precision': 0.2894521611024182, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.046346280723810196, 'validation/mean_average_precision': 0.22130309387281447, 'validation/num_examples': 43793, 'test/accuracy': 0.9853512644767761, 'test/loss': 0.04901207610964775, 'test/mean_average_precision': 0.224792602401784, 'test/num_examples': 43793, 'score': 2188.84933924675, 'total_duration': 3218.5911762714386, 'accumulated_submission_time': 2188.84933924675, 'accumulated_eval_time': 1029.4849562644958, 'accumulated_logging_time': 0.1864626407623291}
I0428 18:40:05.061528 139444627252992 logging_writer.py:48] [7845] accumulated_eval_time=1029.484956, accumulated_logging_time=0.186463, accumulated_submission_time=2188.849339, global_step=7845, preemption_count=0, score=2188.849339, test/accuracy=0.985351, test/loss=0.049012, test/mean_average_precision=0.224793, test/num_examples=43793, total_duration=3218.591176, train/accuracy=0.989380, train/loss=0.036087, train/mean_average_precision=0.289452, validation/accuracy=0.986249, validation/loss=0.046346, validation/mean_average_precision=0.221303, validation/num_examples=43793
I0428 18:40:20.384234 139444618860288 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.014581537805497646, loss=0.040371742099523544
I0428 18:40:47.778330 139444627252992 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.010660271160304546, loss=0.03838397562503815
I0428 18:41:15.183147 139444618860288 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.011006314307451248, loss=0.03301767632365227
I0428 18:41:42.809441 139444627252992 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.014084998518228531, loss=0.038126684725284576
I0428 18:42:10.174120 139444618860288 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.012599487788975239, loss=0.04001195356249809
I0428 18:42:37.639030 139444627252992 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.012976767495274544, loss=0.04288439080119133
I0428 18:43:05.296043 139444618860288 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.011881092563271523, loss=0.04311408847570419
I0428 18:43:33.188598 139444627252992 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.010297365486621857, loss=0.03860672563314438
I0428 18:44:01.045963 139444618860288 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.013209779746830463, loss=0.035806965082883835
I0428 18:44:05.174991 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:45:28.729258 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:45:31.404484 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:45:33.994159 139630582925120 submission_runner.py:415] Time since start: 3547.53s, 	Step: 8716, 	{'train/accuracy': 0.9893041253089905, 'train/loss': 0.03564042970538139, 'train/mean_average_precision': 0.3081977554966004, 'validation/accuracy': 0.9862324595451355, 'validation/loss': 0.04633616656064987, 'validation/mean_average_precision': 0.23446866794792035, 'validation/num_examples': 43793, 'test/accuracy': 0.9853432178497314, 'test/loss': 0.04894130676984787, 'test/mean_average_precision': 0.24066154456529143, 'test/num_examples': 43793, 'score': 2428.9449002742767, 'total_duration': 3547.5333948135376, 'accumulated_submission_time': 2428.9449002742767, 'accumulated_eval_time': 1118.3040800094604, 'accumulated_logging_time': 0.2057783603668213}
I0428 18:45:34.003911 139444627252992 logging_writer.py:48] [8716] accumulated_eval_time=1118.304080, accumulated_logging_time=0.205778, accumulated_submission_time=2428.944900, global_step=8716, preemption_count=0, score=2428.944900, test/accuracy=0.985343, test/loss=0.048941, test/mean_average_precision=0.240662, test/num_examples=43793, total_duration=3547.533395, train/accuracy=0.989304, train/loss=0.035640, train/mean_average_precision=0.308198, validation/accuracy=0.986232, validation/loss=0.046336, validation/mean_average_precision=0.234469, validation/num_examples=43793
I0428 18:45:57.646273 139444618860288 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.015513379126787186, loss=0.039358045905828476
I0428 18:46:25.296386 139444627252992 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.017232472077012062, loss=0.03790857642889023
I0428 18:46:53.472351 139444618860288 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.013810915872454643, loss=0.040122970938682556
I0428 18:47:21.406976 139444627252992 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.012267256155610085, loss=0.03944418951869011
I0428 18:47:49.084993 139444618860288 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.024267539381980896, loss=0.041960541158914566
I0428 18:48:16.460788 139444627252992 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.012497222051024437, loss=0.03907770663499832
I0428 18:48:43.910552 139444618860288 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.018520301207900047, loss=0.04032238945364952
I0428 18:49:11.260277 139444627252992 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.012610090896487236, loss=0.0409746915102005
I0428 18:49:34.225246 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:50:56.792650 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:50:59.418240 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:51:02.013213 139630582925120 submission_runner.py:415] Time since start: 3875.55s, 	Step: 9585, 	{'train/accuracy': 0.9898356795310974, 'train/loss': 0.034361958503723145, 'train/mean_average_precision': 0.3233588994494616, 'validation/accuracy': 0.9864256978034973, 'validation/loss': 0.04543488100171089, 'validation/mean_average_precision': 0.24207247253032907, 'validation/num_examples': 43793, 'test/accuracy': 0.9855399131774902, 'test/loss': 0.047858309000730515, 'test/mean_average_precision': 0.24307308557163065, 'test/num_examples': 43793, 'score': 2669.1469678878784, 'total_duration': 3875.5524554252625, 'accumulated_submission_time': 2669.1469678878784, 'accumulated_eval_time': 1206.0920071601868, 'accumulated_logging_time': 0.22669434547424316}
I0428 18:51:02.022990 139444618860288 logging_writer.py:48] [9585] accumulated_eval_time=1206.092007, accumulated_logging_time=0.226694, accumulated_submission_time=2669.146968, global_step=9585, preemption_count=0, score=2669.146968, test/accuracy=0.985540, test/loss=0.047858, test/mean_average_precision=0.243073, test/num_examples=43793, total_duration=3875.552455, train/accuracy=0.989836, train/loss=0.034362, train/mean_average_precision=0.323359, validation/accuracy=0.986426, validation/loss=0.045435, validation/mean_average_precision=0.242072, validation/num_examples=43793
I0428 18:51:06.418109 139444627252992 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.013710222207009792, loss=0.037735212594270706
I0428 18:51:34.036021 139444618860288 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.011824244633316994, loss=0.038333989679813385
I0428 18:52:01.926013 139444627252992 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.014188882894814014, loss=0.040055323392152786
I0428 18:52:29.716362 139444618860288 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.01092037558555603, loss=0.035453591495752335
I0428 18:52:57.278514 139444627252992 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.012979420833289623, loss=0.03736579045653343
I0428 18:53:24.690800 139444618860288 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.017201215028762817, loss=0.03931843861937523
I0428 18:53:52.268220 139444627252992 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.017657790333032608, loss=0.03874015808105469
I0428 18:54:19.916768 139444618860288 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.012175961397588253, loss=0.035027600824832916
I0428 18:54:47.828073 139444627252992 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.013456718064844608, loss=0.037912994623184204
I0428 18:55:02.268868 139630582925120 spec.py:298] Evaluating on the training split.
I0428 18:56:22.666726 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 18:56:25.430367 139630582925120 spec.py:326] Evaluating on the test split.
I0428 18:56:28.100280 139630582925120 submission_runner.py:415] Time since start: 4201.64s, 	Step: 10453, 	{'train/accuracy': 0.9899397492408752, 'train/loss': 0.03381359204649925, 'train/mean_average_precision': 0.34048468442611146, 'validation/accuracy': 0.9864460229873657, 'validation/loss': 0.045375511050224304, 'validation/mean_average_precision': 0.24419030430626668, 'validation/num_examples': 43793, 'test/accuracy': 0.985552966594696, 'test/loss': 0.048138052225112915, 'test/mean_average_precision': 0.2370559155878407, 'test/num_examples': 43793, 'score': 2909.3750491142273, 'total_duration': 4201.6394572258, 'accumulated_submission_time': 2909.3750491142273, 'accumulated_eval_time': 1291.9233150482178, 'accumulated_logging_time': 0.24628591537475586}
I0428 18:56:28.110785 139444618860288 logging_writer.py:48] [10453] accumulated_eval_time=1291.923315, accumulated_logging_time=0.246286, accumulated_submission_time=2909.375049, global_step=10453, preemption_count=0, score=2909.375049, test/accuracy=0.985553, test/loss=0.048138, test/mean_average_precision=0.237056, test/num_examples=43793, total_duration=4201.639457, train/accuracy=0.989940, train/loss=0.033814, train/mean_average_precision=0.340485, validation/accuracy=0.986446, validation/loss=0.045376, validation/mean_average_precision=0.244190, validation/num_examples=43793
I0428 18:56:41.377906 139444627252992 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.015297029167413712, loss=0.03707894682884216
I0428 18:57:09.008532 139444618860288 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.012560127303004265, loss=0.03459591791033745
I0428 18:57:36.463789 139444627252992 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.015100434422492981, loss=0.03658337891101837
I0428 18:58:04.134938 139444618860288 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.015612789429724216, loss=0.03854777291417122
I0428 18:58:31.717522 139444627252992 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.015129112638533115, loss=0.033118247985839844
I0428 18:58:59.369783 139444618860288 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0147824352607131, loss=0.034305084496736526
I0428 18:59:27.247521 139444627252992 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.019251124933362007, loss=0.03702041134238243
I0428 18:59:55.199377 139444618860288 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.015530256554484367, loss=0.03421846032142639
I0428 19:00:23.105123 139444627252992 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.01902836561203003, loss=0.03724181279540062
I0428 19:00:28.303802 139630582925120 spec.py:298] Evaluating on the training split.
I0428 19:01:50.021365 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 19:01:52.704030 139630582925120 spec.py:326] Evaluating on the test split.
I0428 19:01:55.306960 139630582925120 submission_runner.py:415] Time since start: 4528.85s, 	Step: 11320, 	{'train/accuracy': 0.9903427958488464, 'train/loss': 0.032068971544504166, 'train/mean_average_precision': 0.3592347394406694, 'validation/accuracy': 0.9866489768028259, 'validation/loss': 0.045104626566171646, 'validation/mean_average_precision': 0.2554715866951401, 'validation/num_examples': 43793, 'test/accuracy': 0.9857433438301086, 'test/loss': 0.04783923551440239, 'test/mean_average_precision': 0.25291021339728825, 'test/num_examples': 43793, 'score': 3149.549549102783, 'total_duration': 4528.846181154251, 'accumulated_submission_time': 3149.549549102783, 'accumulated_eval_time': 1378.9264090061188, 'accumulated_logging_time': 0.2669408321380615}
I0428 19:01:55.317188 139444618860288 logging_writer.py:48] [11320] accumulated_eval_time=1378.926409, accumulated_logging_time=0.266941, accumulated_submission_time=3149.549549, global_step=11320, preemption_count=0, score=3149.549549, test/accuracy=0.985743, test/loss=0.047839, test/mean_average_precision=0.252910, test/num_examples=43793, total_duration=4528.846181, train/accuracy=0.990343, train/loss=0.032069, train/mean_average_precision=0.359235, validation/accuracy=0.986649, validation/loss=0.045105, validation/mean_average_precision=0.255472, validation/num_examples=43793
I0428 19:02:18.142902 139444627252992 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.01383263524621725, loss=0.03269565850496292
I0428 19:02:45.877243 139444618860288 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0229227002710104, loss=0.03381778299808502
I0428 19:03:13.821110 139444627252992 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.01539888046681881, loss=0.038156528025865555
I0428 19:03:41.678588 139444618860288 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.012775737792253494, loss=0.03581514209508896
I0428 19:04:09.436038 139444627252992 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.01650380901992321, loss=0.03568222001194954
I0428 19:04:37.486345 139444618860288 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.018367106094956398, loss=0.03309717774391174
I0428 19:05:05.135872 139630582925120 spec.py:298] Evaluating on the training split.
I0428 19:06:25.784991 139630582925120 spec.py:310] Evaluating on the validation split.
I0428 19:06:28.455275 139630582925120 spec.py:326] Evaluating on the test split.
I0428 19:06:31.040533 139630582925120 submission_runner.py:415] Time since start: 4804.58s, 	Step: 12000, 	{'train/accuracy': 0.9905117154121399, 'train/loss': 0.03195801377296448, 'train/mean_average_precision': 0.37188249851736827, 'validation/accuracy': 0.9866238236427307, 'validation/loss': 0.045053575187921524, 'validation/mean_average_precision': 0.24714099305307535, 'validation/num_examples': 43793, 'test/accuracy': 0.9857214689254761, 'test/loss': 0.04779404029250145, 'test/mean_average_precision': 0.2467841112431817, 'test/num_examples': 43793, 'score': 3339.3511765003204, 'total_duration': 4804.5797600746155, 'accumulated_submission_time': 3339.3511765003204, 'accumulated_eval_time': 1464.8310160636902, 'accumulated_logging_time': 0.2878873348236084}
I0428 19:06:31.050848 139444627252992 logging_writer.py:48] [12000] accumulated_eval_time=1464.831016, accumulated_logging_time=0.287887, accumulated_submission_time=3339.351177, global_step=12000, preemption_count=0, score=3339.351177, test/accuracy=0.985721, test/loss=0.047794, test/mean_average_precision=0.246784, test/num_examples=43793, total_duration=4804.579760, train/accuracy=0.990512, train/loss=0.031958, train/mean_average_precision=0.371882, validation/accuracy=0.986624, validation/loss=0.045054, validation/mean_average_precision=0.247141, validation/num_examples=43793
I0428 19:06:31.069247 139444618860288 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3339.351177
I0428 19:06:31.099342 139630582925120 checkpoints.py:356] Saving checkpoint at step: 12000
I0428 19:06:31.198291 139630582925120 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1/checkpoint_12000
I0428 19:06:31.198528 139630582925120 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy/timing_sam/ogbg_jax/trial_1/checkpoint_12000.
I0428 19:06:31.375125 139630582925120 submission_runner.py:578] Tuning trial 1/1
I0428 19:06:31.375389 139630582925120 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0013159053452895648, one_minus_beta1=0.2018302260773442, beta2=0.999, warmup_factor=0.05, weight_decay=0.07935861128365443, label_smoothing=0.1, dropout_rate=0.0, rho=0.01)
I0428 19:06:31.382038 139630582925120 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4474038779735565, 'train/loss': 0.8037292957305908, 'train/mean_average_precision': 0.02198940286324916, 'validation/accuracy': 0.452945351600647, 'validation/loss': 0.8001374006271362, 'validation/mean_average_precision': 0.026602841651966674, 'validation/num_examples': 43793, 'test/accuracy': 0.45501387119293213, 'test/loss': 0.7988821864128113, 'test/mean_average_precision': 0.02830481325165795, 'test/num_examples': 43793, 'score': 27.84489369392395, 'total_duration': 268.1360971927643, 'accumulated_submission_time': 27.84489369392395, 'accumulated_eval_time': 240.29102301597595, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (879, {'train/accuracy': 0.9867270588874817, 'train/loss': 0.05623584985733032, 'train/mean_average_precision': 0.043487523548067, 'validation/accuracy': 0.984115481376648, 'validation/loss': 0.06535669416189194, 'validation/mean_average_precision': 0.04344390175778309, 'validation/num_examples': 43793, 'test/accuracy': 0.9831374287605286, 'test/loss': 0.06847526133060455, 'test/mean_average_precision': 0.04440379058799089, 'test/num_examples': 43793, 'score': 267.8613471984863, 'total_duration': 595.9024682044983, 'accumulated_submission_time': 267.8613471984863, 'accumulated_eval_time': 328.00572085380554, 'accumulated_logging_time': 0.027959108352661133, 'global_step': 879, 'preemption_count': 0}), (1752, {'train/accuracy': 0.9871780276298523, 'train/loss': 0.047059305012226105, 'train/mean_average_precision': 0.10052400760132467, 'validation/accuracy': 0.984489381313324, 'validation/loss': 0.05655616149306297, 'validation/mean_average_precision': 0.10096203050020441, 'validation/num_examples': 43793, 'test/accuracy': 0.9834887385368347, 'test/loss': 0.06012658029794693, 'test/mean_average_precision': 0.09731338357910031, 'test/num_examples': 43793, 'score': 507.8837742805481, 'total_duration': 923.3370501995087, 'accumulated_submission_time': 507.8837742805481, 'accumulated_eval_time': 415.39124369621277, 'accumulated_logging_time': 0.04739069938659668, 'global_step': 1752, 'preemption_count': 0}), (2626, {'train/accuracy': 0.9876143932342529, 'train/loss': 0.04406202211976051, 'train/mean_average_precision': 0.14648878010098978, 'validation/accuracy': 0.9849151968955994, 'validation/loss': 0.05368294194340706, 'validation/mean_average_precision': 0.1382055181913488, 'validation/num_examples': 43793, 'test/accuracy': 0.9839082360267639, 'test/loss': 0.05688140541315079, 'test/mean_average_precision': 0.13109430951307707, 'test/num_examples': 43793, 'score': 747.9164221286774, 'total_duration': 1249.9531540870667, 'accumulated_submission_time': 747.9164221286774, 'accumulated_eval_time': 501.9457104206085, 'accumulated_logging_time': 0.06868147850036621, 'global_step': 2626, 'preemption_count': 0}), (3498, {'train/accuracy': 0.9876424074172974, 'train/loss': 0.043965838849544525, 'train/mean_average_precision': 0.1646973315657152, 'validation/accuracy': 0.9847983121871948, 'validation/loss': 0.0540594644844532, 'validation/mean_average_precision': 0.1501046815276347, 'validation/num_examples': 43793, 'test/accuracy': 0.9838513731956482, 'test/loss': 0.05705874785780907, 'test/mean_average_precision': 0.15104521779783212, 'test/num_examples': 43793, 'score': 988.1093020439148, 'total_duration': 1576.452208995819, 'accumulated_submission_time': 988.1093020439148, 'accumulated_eval_time': 588.2238848209381, 'accumulated_logging_time': 0.08892083168029785, 'global_step': 3498, 'preemption_count': 0}), (4363, {'train/accuracy': 0.988119900226593, 'train/loss': 0.04061496630311012, 'train/mean_average_precision': 0.20308144544828297, 'validation/accuracy': 0.9853211641311646, 'validation/loss': 0.050038646906614304, 'validation/mean_average_precision': 0.16596586544292788, 'validation/num_examples': 43793, 'test/accuracy': 0.9844410419464111, 'test/loss': 0.05278030410408974, 'test/mean_average_precision': 0.16638581916281578, 'test/num_examples': 43793, 'score': 1228.2813785076141, 'total_duration': 1904.703331232071, 'accumulated_submission_time': 1228.2813785076141, 'accumulated_eval_time': 676.2746350765228, 'accumulated_logging_time': 0.1090700626373291, 'global_step': 4363, 'preemption_count': 0}), (5240, {'train/accuracy': 0.988368809223175, 'train/loss': 0.03984415531158447, 'train/mean_average_precision': 0.20972131454011603, 'validation/accuracy': 0.9855983853340149, 'validation/loss': 0.04954816401004791, 'validation/mean_average_precision': 0.18420487243507075, 'validation/num_examples': 43793, 'test/accuracy': 0.9846532940864563, 'test/loss': 0.05229584500193596, 'test/mean_average_precision': 0.18897304117122113, 'test/num_examples': 43793, 'score': 1468.4950156211853, 'total_duration': 2233.1066329479218, 'accumulated_submission_time': 1468.4950156211853, 'accumulated_eval_time': 764.437097787857, 'accumulated_logging_time': 0.1285855770111084, 'global_step': 5240, 'preemption_count': 0}), (6113, {'train/accuracy': 0.9888556599617004, 'train/loss': 0.03834446147084236, 'train/mean_average_precision': 0.2390944515342653, 'validation/accuracy': 0.9859142303466797, 'validation/loss': 0.04787725582718849, 'validation/mean_average_precision': 0.19990934552465386, 'validation/num_examples': 43793, 'test/accuracy': 0.9850024580955505, 'test/loss': 0.05042821168899536, 'test/mean_average_precision': 0.2069670324166879, 'test/num_examples': 43793, 'score': 1708.586285352707, 'total_duration': 2562.147730588913, 'accumulated_submission_time': 1708.586285352707, 'accumulated_eval_time': 853.3598494529724, 'accumulated_logging_time': 0.14761829376220703, 'global_step': 6113, 'preemption_count': 0}), (6980, {'train/accuracy': 0.9890006184577942, 'train/loss': 0.03759165108203888, 'train/mean_average_precision': 0.25417132140309673, 'validation/accuracy': 0.9858785271644592, 'validation/loss': 0.04785815253853798, 'validation/mean_average_precision': 0.20871214345267797, 'validation/num_examples': 43793, 'test/accuracy': 0.985045850276947, 'test/loss': 0.050616100430488586, 'test/mean_average_precision': 0.21582822814491018, 'test/num_examples': 43793, 'score': 1948.7240340709686, 'total_duration': 2890.6484472751617, 'accumulated_submission_time': 1948.7240340709686, 'accumulated_eval_time': 941.6957247257233, 'accumulated_logging_time': 0.16651058197021484, 'global_step': 6980, 'preemption_count': 0}), (7845, {'train/accuracy': 0.9893801212310791, 'train/loss': 0.036087408661842346, 'train/mean_average_precision': 0.2894521611024182, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.046346280723810196, 'validation/mean_average_precision': 0.22130309387281447, 'validation/num_examples': 43793, 'test/accuracy': 0.9853512644767761, 'test/loss': 0.04901207610964775, 'test/mean_average_precision': 0.224792602401784, 'test/num_examples': 43793, 'score': 2188.84933924675, 'total_duration': 3218.5911762714386, 'accumulated_submission_time': 2188.84933924675, 'accumulated_eval_time': 1029.4849562644958, 'accumulated_logging_time': 0.1864626407623291, 'global_step': 7845, 'preemption_count': 0}), (8716, {'train/accuracy': 0.9893041253089905, 'train/loss': 0.03564042970538139, 'train/mean_average_precision': 0.3081977554966004, 'validation/accuracy': 0.9862324595451355, 'validation/loss': 0.04633616656064987, 'validation/mean_average_precision': 0.23446866794792035, 'validation/num_examples': 43793, 'test/accuracy': 0.9853432178497314, 'test/loss': 0.04894130676984787, 'test/mean_average_precision': 0.24066154456529143, 'test/num_examples': 43793, 'score': 2428.9449002742767, 'total_duration': 3547.5333948135376, 'accumulated_submission_time': 2428.9449002742767, 'accumulated_eval_time': 1118.3040800094604, 'accumulated_logging_time': 0.2057783603668213, 'global_step': 8716, 'preemption_count': 0}), (9585, {'train/accuracy': 0.9898356795310974, 'train/loss': 0.034361958503723145, 'train/mean_average_precision': 0.3233588994494616, 'validation/accuracy': 0.9864256978034973, 'validation/loss': 0.04543488100171089, 'validation/mean_average_precision': 0.24207247253032907, 'validation/num_examples': 43793, 'test/accuracy': 0.9855399131774902, 'test/loss': 0.047858309000730515, 'test/mean_average_precision': 0.24307308557163065, 'test/num_examples': 43793, 'score': 2669.1469678878784, 'total_duration': 3875.5524554252625, 'accumulated_submission_time': 2669.1469678878784, 'accumulated_eval_time': 1206.0920071601868, 'accumulated_logging_time': 0.22669434547424316, 'global_step': 9585, 'preemption_count': 0}), (10453, {'train/accuracy': 0.9899397492408752, 'train/loss': 0.03381359204649925, 'train/mean_average_precision': 0.34048468442611146, 'validation/accuracy': 0.9864460229873657, 'validation/loss': 0.045375511050224304, 'validation/mean_average_precision': 0.24419030430626668, 'validation/num_examples': 43793, 'test/accuracy': 0.985552966594696, 'test/loss': 0.048138052225112915, 'test/mean_average_precision': 0.2370559155878407, 'test/num_examples': 43793, 'score': 2909.3750491142273, 'total_duration': 4201.6394572258, 'accumulated_submission_time': 2909.3750491142273, 'accumulated_eval_time': 1291.9233150482178, 'accumulated_logging_time': 0.24628591537475586, 'global_step': 10453, 'preemption_count': 0}), (11320, {'train/accuracy': 0.9903427958488464, 'train/loss': 0.032068971544504166, 'train/mean_average_precision': 0.3592347394406694, 'validation/accuracy': 0.9866489768028259, 'validation/loss': 0.045104626566171646, 'validation/mean_average_precision': 0.2554715866951401, 'validation/num_examples': 43793, 'test/accuracy': 0.9857433438301086, 'test/loss': 0.04783923551440239, 'test/mean_average_precision': 0.25291021339728825, 'test/num_examples': 43793, 'score': 3149.549549102783, 'total_duration': 4528.846181154251, 'accumulated_submission_time': 3149.549549102783, 'accumulated_eval_time': 1378.9264090061188, 'accumulated_logging_time': 0.2669408321380615, 'global_step': 11320, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9905117154121399, 'train/loss': 0.03195801377296448, 'train/mean_average_precision': 0.37188249851736827, 'validation/accuracy': 0.9866238236427307, 'validation/loss': 0.045053575187921524, 'validation/mean_average_precision': 0.24714099305307535, 'validation/num_examples': 43793, 'test/accuracy': 0.9857214689254761, 'test/loss': 0.04779404029250145, 'test/mean_average_precision': 0.2467841112431817, 'test/num_examples': 43793, 'score': 3339.3511765003204, 'total_duration': 4804.5797600746155, 'accumulated_submission_time': 3339.3511765003204, 'accumulated_eval_time': 1464.8310160636902, 'accumulated_logging_time': 0.2878873348236084, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0428 19:06:31.382201 139630582925120 submission_runner.py:581] Timing: 3339.3511765003204
I0428 19:06:31.382252 139630582925120 submission_runner.py:582] ====================
I0428 19:06:31.382377 139630582925120 submission_runner.py:645] Final ogbg score: 3339.3511765003204
