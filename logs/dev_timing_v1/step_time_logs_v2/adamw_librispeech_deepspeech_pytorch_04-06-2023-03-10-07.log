WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 03:10:23.208113 140099702298432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 03:10:23.208158 140524506367808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 03:10:23.208184 140624007616320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 03:10:23.209043 140230278436672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 03:10:23.209058 140086918149952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 03:10:23.209204 140644816250688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 03:10:23.209239 139773909927744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 03:10:23.219292 139630859757376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 03:10:23.219563 140230278436672 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.219628 139630859757376 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.219687 140086918149952 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.219733 140644816250688 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.219796 139773909927744 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.228878 140099702298432 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.229135 140524506367808 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.229146 140624007616320 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 03:10:23.606266 139630859757376 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch.
W0406 03:10:23.615541 140230278436672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.616826 140644816250688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.616964 139773909927744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.617661 140086918149952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.619274 140099702298432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.628726 140524506367808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.630606 140624007616320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 03:10:23.638604 139630859757376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 03:10:23.641981 139630859757376 submission_runner.py:511] Using RNG seed 1396442687
I0406 03:10:23.642992 139630859757376 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 03:10:23.643098 139630859757376 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1.
I0406 03:10:23.643320 139630859757376 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0406 03:10:23.644292 139630859757376 submission_runner.py:230] Starting train once: RAM USED (GB) 5.658800128
I0406 03:10:23.644401 139630859757376 submission_runner.py:231] Initializing dataset.
I0406 03:10:23.644494 139630859757376 input_pipeline.py:20] Loading split = train-clean-100
I0406 03:10:23.673630 139630859757376 input_pipeline.py:20] Loading split = train-clean-360
I0406 03:10:23.988815 139630859757376 input_pipeline.py:20] Loading split = train-other-500
I0406 03:10:24.400251 139630859757376 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.891403776
I0406 03:10:24.400428 139630859757376 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0406 03:10:31.686893 139630859757376 submission_runner.py:251] After Initializing model: RAM USED (GB) 21.861691392
I0406 03:10:31.687089 139630859757376 submission_runner.py:252] Initializing optimizer.
I0406 03:10:31.687812 139630859757376 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 21.695275008
I0406 03:10:31.687963 139630859757376 submission_runner.py:261] Initializing metrics bundle.
I0406 03:10:31.688018 139630859757376 submission_runner.py:276] Initializing checkpoint and logger.
I0406 03:10:31.689825 139630859757376 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 03:10:31.689935 139630859757376 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 03:10:32.591785 139630859757376 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0406 03:10:32.592611 139630859757376 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0406 03:10:32.595686 139630859757376 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 21.267759104
I0406 03:10:32.596695 139630859757376 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 21.267759104
I0406 03:10:32.596785 139630859757376 submission_runner.py:313] Starting training loop.
I0406 03:10:35.023308 139630859757376 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 25.836412928
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0406 03:10:41.536945 139604673943296 logging_writer.py:48] [0] global_step=0, grad_norm=26.491243, loss=33.479939
I0406 03:10:41.546522 139630859757376 submission.py:119] 0) loss = 33.480, grad_norm = 26.491
I0406 03:10:41.547271 139630859757376 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.571193856
I0406 03:10:41.547870 139630859757376 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.571193856
I0406 03:10:41.548009 139630859757376 spec.py:298] Evaluating on the training split.
I0406 03:10:41.548733 139630859757376 input_pipeline.py:20] Loading split = train-clean-100
I0406 03:10:41.578456 139630859757376 input_pipeline.py:20] Loading split = train-clean-360
I0406 03:10:41.994196 139630859757376 input_pipeline.py:20] Loading split = train-other-500
I0406 03:10:55.881917 139630859757376 spec.py:310] Evaluating on the validation split.
I0406 03:10:55.883337 139630859757376 input_pipeline.py:20] Loading split = dev-clean
I0406 03:10:55.887010 139630859757376 input_pipeline.py:20] Loading split = dev-other
I0406 03:11:06.493526 139630859757376 spec.py:326] Evaluating on the test split.
I0406 03:11:06.494837 139630859757376 input_pipeline.py:20] Loading split = test-clean
I0406 03:11:12.930157 139630859757376 submission_runner.py:382] Time since start: 8.95s, 	Step: 1, 	{'train/ctc_loss': 31.98672551291664, 'train/wer': 2.1153516786211033, 'validation/ctc_loss': 30.79861738999397, 'validation/wer': 1.9513059431275044, 'validation/num_examples': 5348, 'test/ctc_loss': 31.004855151945286, 'test/wer': 1.9405886295777222, 'test/num_examples': 2472}
I0406 03:11:12.930996 139630859757376 submission_runner.py:396] After eval at step 1: RAM USED (GB) 44.98720768
I0406 03:11:12.944283 139601402394368 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=8.949720, test/ctc_loss=31.004855, test/num_examples=2472, test/wer=1.940589, total_duration=8.951600, train/ctc_loss=31.986726, train/wer=2.115352, validation/ctc_loss=30.798617, validation/num_examples=5348, validation/wer=1.951306
I0406 03:11:13.225771 139630859757376 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_1.
I0406 03:11:13.226274 139630859757376 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.017145344
I0406 03:11:13.228981 139630859757376 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.021048832
I0406 03:11:13.268765 139630859757376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.270145 140230278436672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.270165 140086918149952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.270208 140099702298432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.270292 139773909927744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.271145 140624007616320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.271263 140644816250688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:13.271288 140524506367808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 03:11:14.361417 139601394001664 logging_writer.py:48] [1] global_step=1, grad_norm=28.500135, loss=32.857430
I0406 03:11:14.364464 139630859757376 submission.py:119] 1) loss = 32.857, grad_norm = 28.500
I0406 03:11:14.365222 139630859757376 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 45.258952704
I0406 03:11:15.342381 139601402394368 logging_writer.py:48] [2] global_step=2, grad_norm=27.423990, loss=33.410706
I0406 03:11:15.345602 139630859757376 submission.py:119] 2) loss = 33.411, grad_norm = 27.424
I0406 03:11:16.162846 139601394001664 logging_writer.py:48] [3] global_step=3, grad_norm=29.799788, loss=33.452362
I0406 03:11:16.166144 139630859757376 submission.py:119] 3) loss = 33.452, grad_norm = 29.800
I0406 03:11:16.985625 139601402394368 logging_writer.py:48] [4] global_step=4, grad_norm=33.067093, loss=32.984909
I0406 03:11:16.988649 139630859757376 submission.py:119] 4) loss = 32.985, grad_norm = 33.067
I0406 03:11:17.797524 139601394001664 logging_writer.py:48] [5] global_step=5, grad_norm=39.494122, loss=33.183407
I0406 03:11:17.800612 139630859757376 submission.py:119] 5) loss = 33.183, grad_norm = 39.494
I0406 03:11:18.624235 139601402394368 logging_writer.py:48] [6] global_step=6, grad_norm=43.486935, loss=33.259422
I0406 03:11:18.627329 139630859757376 submission.py:119] 6) loss = 33.259, grad_norm = 43.487
I0406 03:11:19.445824 139601394001664 logging_writer.py:48] [7] global_step=7, grad_norm=50.027287, loss=32.179432
I0406 03:11:19.448958 139630859757376 submission.py:119] 7) loss = 32.179, grad_norm = 50.027
I0406 03:11:20.262022 139601402394368 logging_writer.py:48] [8] global_step=8, grad_norm=61.677147, loss=32.513683
I0406 03:11:20.265162 139630859757376 submission.py:119] 8) loss = 32.514, grad_norm = 61.677
I0406 03:11:21.073078 139601394001664 logging_writer.py:48] [9] global_step=9, grad_norm=68.056206, loss=31.869993
I0406 03:11:21.076282 139630859757376 submission.py:119] 9) loss = 31.870, grad_norm = 68.056
I0406 03:11:21.883229 139601402394368 logging_writer.py:48] [10] global_step=10, grad_norm=69.244682, loss=31.641798
I0406 03:11:21.886485 139630859757376 submission.py:119] 10) loss = 31.642, grad_norm = 69.245
I0406 03:11:22.694737 139601394001664 logging_writer.py:48] [11] global_step=11, grad_norm=67.566490, loss=31.625513
I0406 03:11:22.697851 139630859757376 submission.py:119] 11) loss = 31.626, grad_norm = 67.566
I0406 03:11:23.508345 139601402394368 logging_writer.py:48] [12] global_step=12, grad_norm=70.527290, loss=31.423418
I0406 03:11:23.511570 139630859757376 submission.py:119] 12) loss = 31.423, grad_norm = 70.527
I0406 03:11:24.325585 139601394001664 logging_writer.py:48] [13] global_step=13, grad_norm=65.016426, loss=30.616459
I0406 03:11:24.328598 139630859757376 submission.py:119] 13) loss = 30.616, grad_norm = 65.016
I0406 03:11:25.166030 139601402394368 logging_writer.py:48] [14] global_step=14, grad_norm=57.538792, loss=30.272245
I0406 03:11:25.169029 139630859757376 submission.py:119] 14) loss = 30.272, grad_norm = 57.539
I0406 03:11:25.979959 139601394001664 logging_writer.py:48] [15] global_step=15, grad_norm=50.195805, loss=29.220350
I0406 03:11:25.982905 139630859757376 submission.py:119] 15) loss = 29.220, grad_norm = 50.196
I0406 03:11:26.799383 139601402394368 logging_writer.py:48] [16] global_step=16, grad_norm=45.036041, loss=29.487801
I0406 03:11:26.802504 139630859757376 submission.py:119] 16) loss = 29.488, grad_norm = 45.036
I0406 03:11:27.623921 139601394001664 logging_writer.py:48] [17] global_step=17, grad_norm=38.544731, loss=29.050312
I0406 03:11:27.627180 139630859757376 submission.py:119] 17) loss = 29.050, grad_norm = 38.545
I0406 03:11:28.449822 139601402394368 logging_writer.py:48] [18] global_step=18, grad_norm=35.832104, loss=28.667067
I0406 03:11:28.452888 139630859757376 submission.py:119] 18) loss = 28.667, grad_norm = 35.832
I0406 03:11:29.259374 139601394001664 logging_writer.py:48] [19] global_step=19, grad_norm=31.522974, loss=28.212553
I0406 03:11:29.262427 139630859757376 submission.py:119] 19) loss = 28.213, grad_norm = 31.523
I0406 03:11:30.074331 139601402394368 logging_writer.py:48] [20] global_step=20, grad_norm=28.699436, loss=27.545401
I0406 03:11:30.077839 139630859757376 submission.py:119] 20) loss = 27.545, grad_norm = 28.699
I0406 03:11:30.903356 139601394001664 logging_writer.py:48] [21] global_step=21, grad_norm=28.296278, loss=27.762497
I0406 03:11:30.906430 139630859757376 submission.py:119] 21) loss = 27.762, grad_norm = 28.296
I0406 03:11:31.714288 139601402394368 logging_writer.py:48] [22] global_step=22, grad_norm=28.150623, loss=28.066006
I0406 03:11:31.717531 139630859757376 submission.py:119] 22) loss = 28.066, grad_norm = 28.151
I0406 03:11:32.539377 139601394001664 logging_writer.py:48] [23] global_step=23, grad_norm=25.904270, loss=27.140583
I0406 03:11:32.542808 139630859757376 submission.py:119] 23) loss = 27.141, grad_norm = 25.904
I0406 03:11:33.366131 139601402394368 logging_writer.py:48] [24] global_step=24, grad_norm=27.019400, loss=27.311962
I0406 03:11:33.369137 139630859757376 submission.py:119] 24) loss = 27.312, grad_norm = 27.019
I0406 03:11:34.192224 139601394001664 logging_writer.py:48] [25] global_step=25, grad_norm=27.081631, loss=27.129414
I0406 03:11:34.195191 139630859757376 submission.py:119] 25) loss = 27.129, grad_norm = 27.082
I0406 03:11:35.002109 139601402394368 logging_writer.py:48] [26] global_step=26, grad_norm=26.493847, loss=26.828497
I0406 03:11:35.005070 139630859757376 submission.py:119] 26) loss = 26.828, grad_norm = 26.494
I0406 03:11:35.829906 139601394001664 logging_writer.py:48] [27] global_step=27, grad_norm=28.577883, loss=26.757261
I0406 03:11:35.832863 139630859757376 submission.py:119] 27) loss = 26.757, grad_norm = 28.578
I0406 03:11:36.658283 139601402394368 logging_writer.py:48] [28] global_step=28, grad_norm=28.027859, loss=26.194584
I0406 03:11:36.661352 139630859757376 submission.py:119] 28) loss = 26.195, grad_norm = 28.028
I0406 03:11:37.479431 139601394001664 logging_writer.py:48] [29] global_step=29, grad_norm=27.423380, loss=25.800280
I0406 03:11:37.482426 139630859757376 submission.py:119] 29) loss = 25.800, grad_norm = 27.423
I0406 03:11:38.294248 139601402394368 logging_writer.py:48] [30] global_step=30, grad_norm=29.173372, loss=25.380356
I0406 03:11:38.297585 139630859757376 submission.py:119] 30) loss = 25.380, grad_norm = 29.173
I0406 03:11:39.107871 139601394001664 logging_writer.py:48] [31] global_step=31, grad_norm=29.555624, loss=25.299286
I0406 03:11:39.110956 139630859757376 submission.py:119] 31) loss = 25.299, grad_norm = 29.556
I0406 03:11:39.925948 139601402394368 logging_writer.py:48] [32] global_step=32, grad_norm=29.457644, loss=24.716454
I0406 03:11:39.929136 139630859757376 submission.py:119] 32) loss = 24.716, grad_norm = 29.458
I0406 03:11:40.756052 139601394001664 logging_writer.py:48] [33] global_step=33, grad_norm=29.935686, loss=24.786541
I0406 03:11:40.759146 139630859757376 submission.py:119] 33) loss = 24.787, grad_norm = 29.936
I0406 03:11:41.569517 139601402394368 logging_writer.py:48] [34] global_step=34, grad_norm=29.537828, loss=24.336096
I0406 03:11:41.572538 139630859757376 submission.py:119] 34) loss = 24.336, grad_norm = 29.538
I0406 03:11:42.399653 139601394001664 logging_writer.py:48] [35] global_step=35, grad_norm=30.311150, loss=23.916328
I0406 03:11:42.402666 139630859757376 submission.py:119] 35) loss = 23.916, grad_norm = 30.311
I0406 03:11:43.235108 139601402394368 logging_writer.py:48] [36] global_step=36, grad_norm=30.649359, loss=23.330275
I0406 03:11:43.238313 139630859757376 submission.py:119] 36) loss = 23.330, grad_norm = 30.649
I0406 03:11:44.049065 139601394001664 logging_writer.py:48] [37] global_step=37, grad_norm=29.750803, loss=22.392559
I0406 03:11:44.052484 139630859757376 submission.py:119] 37) loss = 22.393, grad_norm = 29.751
I0406 03:11:44.893140 139601402394368 logging_writer.py:48] [38] global_step=38, grad_norm=30.682076, loss=22.330040
I0406 03:11:44.896152 139630859757376 submission.py:119] 38) loss = 22.330, grad_norm = 30.682
I0406 03:11:45.722998 139601394001664 logging_writer.py:48] [39] global_step=39, grad_norm=30.059732, loss=21.952122
I0406 03:11:45.726295 139630859757376 submission.py:119] 39) loss = 21.952, grad_norm = 30.060
I0406 03:11:46.537277 139601402394368 logging_writer.py:48] [40] global_step=40, grad_norm=28.807602, loss=21.281900
I0406 03:11:46.540296 139630859757376 submission.py:119] 40) loss = 21.282, grad_norm = 28.808
I0406 03:11:47.363092 139601394001664 logging_writer.py:48] [41] global_step=41, grad_norm=27.986910, loss=20.729187
I0406 03:11:47.366236 139630859757376 submission.py:119] 41) loss = 20.729, grad_norm = 27.987
I0406 03:11:48.198120 139601402394368 logging_writer.py:48] [42] global_step=42, grad_norm=27.295488, loss=20.265106
I0406 03:11:48.201246 139630859757376 submission.py:119] 42) loss = 20.265, grad_norm = 27.295
I0406 03:11:49.034434 139601394001664 logging_writer.py:48] [43] global_step=43, grad_norm=26.319134, loss=19.562395
I0406 03:11:49.037718 139630859757376 submission.py:119] 43) loss = 19.562, grad_norm = 26.319
I0406 03:11:49.859592 139601402394368 logging_writer.py:48] [44] global_step=44, grad_norm=25.346003, loss=18.966925
I0406 03:11:49.862758 139630859757376 submission.py:119] 44) loss = 18.967, grad_norm = 25.346
I0406 03:11:50.668482 139601394001664 logging_writer.py:48] [45] global_step=45, grad_norm=23.929790, loss=18.476217
I0406 03:11:50.671629 139630859757376 submission.py:119] 45) loss = 18.476, grad_norm = 23.930
I0406 03:11:51.483210 139601402394368 logging_writer.py:48] [46] global_step=46, grad_norm=22.901501, loss=17.937265
I0406 03:11:51.486409 139630859757376 submission.py:119] 46) loss = 17.937, grad_norm = 22.902
I0406 03:11:52.295717 139601394001664 logging_writer.py:48] [47] global_step=47, grad_norm=22.163805, loss=17.795765
I0406 03:11:52.299448 139630859757376 submission.py:119] 47) loss = 17.796, grad_norm = 22.164
I0406 03:11:53.108264 139601402394368 logging_writer.py:48] [48] global_step=48, grad_norm=22.042990, loss=17.832716
I0406 03:11:53.112112 139630859757376 submission.py:119] 48) loss = 17.833, grad_norm = 22.043
I0406 03:11:53.919369 139601394001664 logging_writer.py:48] [49] global_step=49, grad_norm=20.890547, loss=17.309839
I0406 03:11:53.923127 139630859757376 submission.py:119] 49) loss = 17.310, grad_norm = 20.891
I0406 03:11:54.732592 139601402394368 logging_writer.py:48] [50] global_step=50, grad_norm=19.899446, loss=16.673304
I0406 03:11:54.736441 139630859757376 submission.py:119] 50) loss = 16.673, grad_norm = 19.899
I0406 03:11:55.548976 139601394001664 logging_writer.py:48] [51] global_step=51, grad_norm=18.983885, loss=16.144348
I0406 03:11:55.552849 139630859757376 submission.py:119] 51) loss = 16.144, grad_norm = 18.984
I0406 03:11:56.357794 139601402394368 logging_writer.py:48] [52] global_step=52, grad_norm=18.469917, loss=15.623436
I0406 03:11:56.361665 139630859757376 submission.py:119] 52) loss = 15.623, grad_norm = 18.470
I0406 03:11:57.165652 139601394001664 logging_writer.py:48] [53] global_step=53, grad_norm=18.309660, loss=15.323020
I0406 03:11:57.169763 139630859757376 submission.py:119] 53) loss = 15.323, grad_norm = 18.310
I0406 03:11:57.975660 139601402394368 logging_writer.py:48] [54] global_step=54, grad_norm=17.944429, loss=15.261114
I0406 03:11:57.979417 139630859757376 submission.py:119] 54) loss = 15.261, grad_norm = 17.944
I0406 03:11:58.795392 139601394001664 logging_writer.py:48] [55] global_step=55, grad_norm=17.713236, loss=15.060511
I0406 03:11:58.799106 139630859757376 submission.py:119] 55) loss = 15.061, grad_norm = 17.713
I0406 03:11:59.609034 139601402394368 logging_writer.py:48] [56] global_step=56, grad_norm=17.374809, loss=14.341224
I0406 03:11:59.612567 139630859757376 submission.py:119] 56) loss = 14.341, grad_norm = 17.375
I0406 03:12:00.417313 139601394001664 logging_writer.py:48] [57] global_step=57, grad_norm=16.614857, loss=14.419913
I0406 03:12:00.421062 139630859757376 submission.py:119] 57) loss = 14.420, grad_norm = 16.615
I0406 03:12:01.223951 139601402394368 logging_writer.py:48] [58] global_step=58, grad_norm=15.731566, loss=13.993344
I0406 03:12:01.227131 139630859757376 submission.py:119] 58) loss = 13.993, grad_norm = 15.732
I0406 03:12:02.035893 139601394001664 logging_writer.py:48] [59] global_step=59, grad_norm=15.354647, loss=13.796332
I0406 03:12:02.039738 139630859757376 submission.py:119] 59) loss = 13.796, grad_norm = 15.355
I0406 03:12:02.850090 139601402394368 logging_writer.py:48] [60] global_step=60, grad_norm=14.934673, loss=13.559220
I0406 03:12:02.854239 139630859757376 submission.py:119] 60) loss = 13.559, grad_norm = 14.935
I0406 03:12:03.667504 139601394001664 logging_writer.py:48] [61] global_step=61, grad_norm=14.321736, loss=13.317986
I0406 03:12:03.671300 139630859757376 submission.py:119] 61) loss = 13.318, grad_norm = 14.322
I0406 03:12:04.481148 139601402394368 logging_writer.py:48] [62] global_step=62, grad_norm=14.187994, loss=12.880531
I0406 03:12:04.484958 139630859757376 submission.py:119] 62) loss = 12.881, grad_norm = 14.188
I0406 03:12:05.292580 139601394001664 logging_writer.py:48] [63] global_step=63, grad_norm=13.598187, loss=12.581873
I0406 03:12:05.296306 139630859757376 submission.py:119] 63) loss = 12.582, grad_norm = 13.598
I0406 03:12:06.104430 139601402394368 logging_writer.py:48] [64] global_step=64, grad_norm=12.903435, loss=12.466808
I0406 03:12:06.108237 139630859757376 submission.py:119] 64) loss = 12.467, grad_norm = 12.903
I0406 03:12:06.913874 139601394001664 logging_writer.py:48] [65] global_step=65, grad_norm=12.444605, loss=12.283563
I0406 03:12:06.917662 139630859757376 submission.py:119] 65) loss = 12.284, grad_norm = 12.445
I0406 03:12:07.727959 139601402394368 logging_writer.py:48] [66] global_step=66, grad_norm=11.842362, loss=12.005562
I0406 03:12:07.731720 139630859757376 submission.py:119] 66) loss = 12.006, grad_norm = 11.842
I0406 03:12:08.543249 139601394001664 logging_writer.py:48] [67] global_step=67, grad_norm=11.518448, loss=11.861390
I0406 03:12:08.546986 139630859757376 submission.py:119] 67) loss = 11.861, grad_norm = 11.518
I0406 03:12:09.355738 139601402394368 logging_writer.py:48] [68] global_step=68, grad_norm=10.826859, loss=11.433331
I0406 03:12:09.359639 139630859757376 submission.py:119] 68) loss = 11.433, grad_norm = 10.827
I0406 03:12:10.173812 139601394001664 logging_writer.py:48] [69] global_step=69, grad_norm=10.797828, loss=11.684910
I0406 03:12:10.177545 139630859757376 submission.py:119] 69) loss = 11.685, grad_norm = 10.798
I0406 03:12:10.987886 139601402394368 logging_writer.py:48] [70] global_step=70, grad_norm=11.220315, loss=11.220852
I0406 03:12:10.991763 139630859757376 submission.py:119] 70) loss = 11.221, grad_norm = 11.220
I0406 03:12:11.789661 139601394001664 logging_writer.py:48] [71] global_step=71, grad_norm=10.447579, loss=11.149258
I0406 03:12:11.793367 139630859757376 submission.py:119] 71) loss = 11.149, grad_norm = 10.448
I0406 03:12:12.598127 139601402394368 logging_writer.py:48] [72] global_step=72, grad_norm=9.973821, loss=10.964297
I0406 03:12:12.601926 139630859757376 submission.py:119] 72) loss = 10.964, grad_norm = 9.974
I0406 03:12:13.404298 139601394001664 logging_writer.py:48] [73] global_step=73, grad_norm=9.676154, loss=10.845016
I0406 03:12:13.408040 139630859757376 submission.py:119] 73) loss = 10.845, grad_norm = 9.676
I0406 03:12:14.229257 139601402394368 logging_writer.py:48] [74] global_step=74, grad_norm=9.425987, loss=10.821613
I0406 03:12:14.233181 139630859757376 submission.py:119] 74) loss = 10.822, grad_norm = 9.426
I0406 03:12:15.045202 139601394001664 logging_writer.py:48] [75] global_step=75, grad_norm=9.313424, loss=10.540257
I0406 03:12:15.049083 139630859757376 submission.py:119] 75) loss = 10.540, grad_norm = 9.313
I0406 03:12:15.869167 139601402394368 logging_writer.py:48] [76] global_step=76, grad_norm=8.203850, loss=10.191069
I0406 03:12:15.873073 139630859757376 submission.py:119] 76) loss = 10.191, grad_norm = 8.204
I0406 03:12:16.682050 139601394001664 logging_writer.py:48] [77] global_step=77, grad_norm=8.442794, loss=10.215275
I0406 03:12:16.685812 139630859757376 submission.py:119] 77) loss = 10.215, grad_norm = 8.443
I0406 03:12:17.504519 139601402394368 logging_writer.py:48] [78] global_step=78, grad_norm=8.558303, loss=9.996070
I0406 03:12:17.507920 139630859757376 submission.py:119] 78) loss = 9.996, grad_norm = 8.558
I0406 03:12:18.320249 139601394001664 logging_writer.py:48] [79] global_step=79, grad_norm=7.897078, loss=9.959647
I0406 03:12:18.324031 139630859757376 submission.py:119] 79) loss = 9.960, grad_norm = 7.897
I0406 03:12:19.130044 139601402394368 logging_writer.py:48] [80] global_step=80, grad_norm=7.777668, loss=9.806087
I0406 03:12:19.133854 139630859757376 submission.py:119] 80) loss = 9.806, grad_norm = 7.778
I0406 03:12:19.944047 139601394001664 logging_writer.py:48] [81] global_step=81, grad_norm=7.409896, loss=9.797204
I0406 03:12:19.948225 139630859757376 submission.py:119] 81) loss = 9.797, grad_norm = 7.410
I0406 03:12:20.754350 139601402394368 logging_writer.py:48] [82] global_step=82, grad_norm=7.120939, loss=9.614103
I0406 03:12:20.757986 139630859757376 submission.py:119] 82) loss = 9.614, grad_norm = 7.121
I0406 03:12:21.564141 139601394001664 logging_writer.py:48] [83] global_step=83, grad_norm=7.405025, loss=9.755239
I0406 03:12:21.567744 139630859757376 submission.py:119] 83) loss = 9.755, grad_norm = 7.405
I0406 03:12:22.376809 139601402394368 logging_writer.py:48] [84] global_step=84, grad_norm=6.148169, loss=9.474792
I0406 03:12:22.380688 139630859757376 submission.py:119] 84) loss = 9.475, grad_norm = 6.148
I0406 03:12:23.186442 139601394001664 logging_writer.py:48] [85] global_step=85, grad_norm=6.937722, loss=9.361292
I0406 03:12:23.190213 139630859757376 submission.py:119] 85) loss = 9.361, grad_norm = 6.938
I0406 03:12:24.014484 139601402394368 logging_writer.py:48] [86] global_step=86, grad_norm=6.682048, loss=9.286553
I0406 03:12:24.018139 139630859757376 submission.py:119] 86) loss = 9.287, grad_norm = 6.682
I0406 03:12:24.826413 139601394001664 logging_writer.py:48] [87] global_step=87, grad_norm=6.432216, loss=9.042758
I0406 03:12:24.830226 139630859757376 submission.py:119] 87) loss = 9.043, grad_norm = 6.432
I0406 03:12:25.640601 139601402394368 logging_writer.py:48] [88] global_step=88, grad_norm=6.612082, loss=9.194408
I0406 03:12:25.644310 139630859757376 submission.py:119] 88) loss = 9.194, grad_norm = 6.612
I0406 03:12:26.452719 139601394001664 logging_writer.py:48] [89] global_step=89, grad_norm=6.587821, loss=9.220906
I0406 03:12:26.456363 139630859757376 submission.py:119] 89) loss = 9.221, grad_norm = 6.588
I0406 03:12:27.272678 139601402394368 logging_writer.py:48] [90] global_step=90, grad_norm=5.573787, loss=8.897713
I0406 03:12:27.276353 139630859757376 submission.py:119] 90) loss = 8.898, grad_norm = 5.574
I0406 03:12:28.081106 139601394001664 logging_writer.py:48] [91] global_step=91, grad_norm=5.913450, loss=8.861055
I0406 03:12:28.085120 139630859757376 submission.py:119] 91) loss = 8.861, grad_norm = 5.913
I0406 03:12:28.892967 139601402394368 logging_writer.py:48] [92] global_step=92, grad_norm=5.824971, loss=8.720951
I0406 03:12:28.896683 139630859757376 submission.py:119] 92) loss = 8.721, grad_norm = 5.825
I0406 03:12:29.707243 139601394001664 logging_writer.py:48] [93] global_step=93, grad_norm=5.488151, loss=8.667372
I0406 03:12:29.710510 139630859757376 submission.py:119] 93) loss = 8.667, grad_norm = 5.488
I0406 03:12:30.542547 139601402394368 logging_writer.py:48] [94] global_step=94, grad_norm=5.359912, loss=8.497049
I0406 03:12:30.546319 139630859757376 submission.py:119] 94) loss = 8.497, grad_norm = 5.360
I0406 03:12:31.370647 139601394001664 logging_writer.py:48] [95] global_step=95, grad_norm=4.973238, loss=8.527295
I0406 03:12:31.373999 139630859757376 submission.py:119] 95) loss = 8.527, grad_norm = 4.973
I0406 03:12:32.206153 139601402394368 logging_writer.py:48] [96] global_step=96, grad_norm=5.396545, loss=8.548620
I0406 03:12:32.209779 139630859757376 submission.py:119] 96) loss = 8.549, grad_norm = 5.397
I0406 03:12:33.022013 139601394001664 logging_writer.py:48] [97] global_step=97, grad_norm=4.821657, loss=8.481184
I0406 03:12:33.025270 139630859757376 submission.py:119] 97) loss = 8.481, grad_norm = 4.822
I0406 03:12:33.848374 139601402394368 logging_writer.py:48] [98] global_step=98, grad_norm=4.613131, loss=8.364967
I0406 03:12:33.851697 139630859757376 submission.py:119] 98) loss = 8.365, grad_norm = 4.613
I0406 03:12:34.667756 139601394001664 logging_writer.py:48] [99] global_step=99, grad_norm=4.680321, loss=8.285041
I0406 03:12:34.671127 139630859757376 submission.py:119] 99) loss = 8.285, grad_norm = 4.680
I0406 03:12:35.486847 139601402394368 logging_writer.py:48] [100] global_step=100, grad_norm=4.884502, loss=8.355021
I0406 03:12:35.490098 139630859757376 submission.py:119] 100) loss = 8.355, grad_norm = 4.885
I0406 03:17:57.297828 139601394001664 logging_writer.py:48] [500] global_step=500, grad_norm=0.556802, loss=5.738664
I0406 03:17:57.303208 139630859757376 submission.py:119] 500) loss = 5.739, grad_norm = 0.557
I0406 03:24:40.347752 139601402394368 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.553204, loss=4.346442
I0406 03:24:40.357931 139630859757376 submission.py:119] 1000) loss = 4.346, grad_norm = 1.553
I0406 03:31:24.731471 139601402394368 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.682563, loss=3.508572
I0406 03:31:24.739216 139630859757376 submission.py:119] 1500) loss = 3.509, grad_norm = 1.683
I0406 03:38:07.652099 139601394001664 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.207097, loss=3.036585
I0406 03:38:07.659228 139630859757376 submission.py:119] 2000) loss = 3.037, grad_norm = 2.207
I0406 03:44:51.131899 139601402394368 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.531161, loss=2.786481
I0406 03:44:51.139186 139630859757376 submission.py:119] 2500) loss = 2.786, grad_norm = 3.531
I0406 03:51:14.100693 139630859757376 submission_runner.py:373] Before eval at step 2978: RAM USED (GB) 43.006439424
I0406 03:51:14.100916 139630859757376 spec.py:298] Evaluating on the training split.
I0406 03:51:23.882009 139630859757376 spec.py:310] Evaluating on the validation split.
I0406 03:51:32.807052 139630859757376 spec.py:326] Evaluating on the test split.
I0406 03:51:37.836900 139630859757376 submission_runner.py:382] Time since start: 2441.19s, 	Step: 2978, 	{'train/ctc_loss': 5.087894911730625, 'train/wer': 0.8934409687184662, 'validation/ctc_loss': 4.9764604681535065, 'validation/wer': 0.8633322068266306, 'validation/num_examples': 5348, 'test/ctc_loss': 4.7748266017162395, 'test/wer': 0.8514004834155953, 'test/num_examples': 2472}
I0406 03:51:37.837667 139630859757376 submission_runner.py:396] After eval at step 2978: RAM USED (GB) 41.658830848
I0406 03:51:37.855736 139601402394368 logging_writer.py:48] [2978] global_step=2978, preemption_count=0, score=1493.860877, test/ctc_loss=4.774827, test/num_examples=2472, test/wer=0.851400, total_duration=2441.188344, train/ctc_loss=5.087895, train/wer=0.893441, validation/ctc_loss=4.976460, validation/num_examples=5348, validation/wer=0.863332
I0406 03:51:38.148743 139630859757376 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_2978.
I0406 03:51:38.149285 139630859757376 submission_runner.py:416] After logging and checkpointing eval at step 2978: RAM USED (GB) 41.670586368
I0406 03:51:56.572512 139601394001664 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.876711, loss=2.556872
I0406 03:51:56.575625 139630859757376 submission.py:119] 3000) loss = 2.557, grad_norm = 3.877
I0406 03:58:39.891905 139601402394368 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.283759, loss=2.396086
I0406 03:58:39.898783 139630859757376 submission.py:119] 3500) loss = 2.396, grad_norm = 3.284
I0406 04:05:21.131503 139601394001664 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.365530, loss=2.278083
I0406 04:05:21.143475 139630859757376 submission.py:119] 4000) loss = 2.278, grad_norm = 2.366
I0406 04:12:04.283492 139601402394368 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.326691, loss=2.230100
I0406 04:12:04.289924 139630859757376 submission.py:119] 4500) loss = 2.230, grad_norm = 4.327
I0406 04:18:44.934011 139601394001664 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.392605, loss=2.089806
I0406 04:18:44.938866 139630859757376 submission.py:119] 5000) loss = 2.090, grad_norm = 2.393
I0406 04:25:27.106263 139601402394368 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.242313, loss=2.152656
I0406 04:25:27.112980 139630859757376 submission.py:119] 5500) loss = 2.153, grad_norm = 2.242
I0406 04:31:38.520322 139630859757376 submission_runner.py:373] Before eval at step 5965: RAM USED (GB) 41.75880192
I0406 04:31:38.520534 139630859757376 spec.py:298] Evaluating on the training split.
I0406 04:31:49.441914 139630859757376 spec.py:310] Evaluating on the validation split.
I0406 04:31:58.951566 139630859757376 spec.py:326] Evaluating on the test split.
I0406 04:32:04.146948 139630859757376 submission_runner.py:382] Time since start: 4865.62s, 	Step: 5965, 	{'train/ctc_loss': 0.9287603946561244, 'train/wer': 0.2917828019745275, 'validation/ctc_loss': 1.1955587499372111, 'validation/wer': 0.32928112779413893, 'validation/num_examples': 5348, 'test/ctc_loss': 0.802346875201626, 'test/wer': 0.255438425446347, 'test/num_examples': 2472}
I0406 04:32:04.147751 139630859757376 submission_runner.py:396] After eval at step 5965: RAM USED (GB) 41.611747328
I0406 04:32:04.166510 139601402394368 logging_writer.py:48] [5965] global_step=5965, preemption_count=0, score=2947.481546, test/ctc_loss=0.802347, test/num_examples=2472, test/wer=0.255438, total_duration=4865.618895, train/ctc_loss=0.928760, train/wer=0.291783, validation/ctc_loss=1.195559, validation/num_examples=5348, validation/wer=0.329281
I0406 04:32:04.454866 139630859757376 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_5965.
I0406 04:32:04.455418 139630859757376 submission_runner.py:416] After logging and checkpointing eval at step 5965: RAM USED (GB) 41.619013632
I0406 04:32:33.180910 139601394001664 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.648602, loss=2.022524
I0406 04:32:33.184798 139630859757376 submission.py:119] 6000) loss = 2.023, grad_norm = 2.649
I0406 04:39:16.319729 139601402394368 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.966729, loss=1.941562
I0406 04:39:16.326608 139630859757376 submission.py:119] 6500) loss = 1.942, grad_norm = 1.967
I0406 04:45:58.672877 139601394001664 logging_writer.py:48] [7000] global_step=7000, grad_norm=6.659670, loss=2.094424
I0406 04:45:58.677628 139630859757376 submission.py:119] 7000) loss = 2.094, grad_norm = 6.660
I0406 04:52:41.177694 139601402394368 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.138771, loss=1.945747
I0406 04:52:41.184424 139630859757376 submission.py:119] 7500) loss = 1.946, grad_norm = 3.139
I0406 04:59:21.402217 139630859757376 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 41.717542912
I0406 04:59:21.402473 139630859757376 spec.py:298] Evaluating on the training split.
I0406 04:59:32.112497 139630859757376 spec.py:310] Evaluating on the validation split.
I0406 04:59:41.429559 139630859757376 spec.py:326] Evaluating on the test split.
I0406 04:59:46.701324 139630859757376 submission_runner.py:382] Time since start: 6528.49s, 	Step: 8000, 	{'train/ctc_loss': 0.7093991228443869, 'train/wer': 0.22962336706029945, 'validation/ctc_loss': 0.9602928353237392, 'validation/wer': 0.269676048858205, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6196902320310987, 'test/wer': 0.1993987772429062, 'test/num_examples': 2472}
I0406 04:59:46.702076 139630859757376 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 41.616130048
I0406 04:59:46.719976 139601402394368 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=3940.235180, test/ctc_loss=0.619690, test/num_examples=2472, test/wer=0.199399, total_duration=6528.489464, train/ctc_loss=0.709399, train/wer=0.229623, validation/ctc_loss=0.960293, validation/num_examples=5348, validation/wer=0.269676
I0406 04:59:47.010933 139630859757376 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_8000.
I0406 04:59:47.011448 139630859757376 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 41.621168128
I0406 04:59:47.020339 139601394001664 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=3940.235180
I0406 04:59:47.545505 139630859757376 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_8000.
I0406 04:59:47.714300 139630859757376 submission_runner.py:550] Tuning trial 1/1
I0406 04:59:47.714603 139630859757376 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 04:59:47.715107 139630859757376 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.98672551291664, 'train/wer': 2.1153516786211033, 'validation/ctc_loss': 30.79861738999397, 'validation/wer': 1.9513059431275044, 'validation/num_examples': 5348, 'test/ctc_loss': 31.004855151945286, 'test/wer': 1.9405886295777222, 'test/num_examples': 2472, 'score': 8.94972038269043, 'total_duration': 8.951599597930908, 'global_step': 1, 'preemption_count': 0}), (2978, {'train/ctc_loss': 5.087894911730625, 'train/wer': 0.8934409687184662, 'validation/ctc_loss': 4.9764604681535065, 'validation/wer': 0.8633322068266306, 'validation/num_examples': 5348, 'test/ctc_loss': 4.7748266017162395, 'test/wer': 0.8514004834155953, 'test/num_examples': 2472, 'score': 1493.8608770370483, 'total_duration': 2441.188343524933, 'global_step': 2978, 'preemption_count': 0}), (5965, {'train/ctc_loss': 0.9287603946561244, 'train/wer': 0.2917828019745275, 'validation/ctc_loss': 1.1955587499372111, 'validation/wer': 0.32928112779413893, 'validation/num_examples': 5348, 'test/ctc_loss': 0.802346875201626, 'test/wer': 0.255438425446347, 'test/num_examples': 2472, 'score': 2947.481546163559, 'total_duration': 4865.6188950538635, 'global_step': 5965, 'preemption_count': 0}), (8000, {'train/ctc_loss': 0.7093991228443869, 'train/wer': 0.22962336706029945, 'validation/ctc_loss': 0.9602928353237392, 'validation/wer': 0.269676048858205, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6196902320310987, 'test/wer': 0.1993987772429062, 'test/num_examples': 2472, 'score': 3940.235179901123, 'total_duration': 6528.489463806152, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0406 04:59:47.715297 139630859757376 submission_runner.py:553] Timing: 3940.235179901123
I0406 04:59:47.715387 139630859757376 submission_runner.py:554] ====================
I0406 04:59:47.715622 139630859757376 submission_runner.py:613] Final librispeech_deepspeech score: 3940.235179901123
