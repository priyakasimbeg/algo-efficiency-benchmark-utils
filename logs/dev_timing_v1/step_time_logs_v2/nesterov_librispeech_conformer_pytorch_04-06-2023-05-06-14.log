WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 05:06:36.728021 139954736666432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 05:06:36.728043 140546894645056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 05:06:36.728085 139857517819712 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 05:06:36.728810 140627012417344 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 05:06:36.729104 140361108789056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 05:06:36.729126 140191086606144 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 05:06:36.729165 140580454397760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 05:06:36.729503 140361108789056 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.729521 140191086606144 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.729561 140580454397760 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.729527 140573618386752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 05:06:36.729883 140573618386752 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.738596 139954736666432 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.738667 139857517819712 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.738701 140546894645056 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:36.739449 140627012417344 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:06:37.271420 140191086606144 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch.
W0406 05:06:37.374536 139954736666432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.375091 140546894645056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.375548 140627012417344 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.376360 139857517819712 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.376358 140573618386752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.377257 140580454397760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.377765 140361108789056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:06:37.379040 140191086606144 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 05:06:37.382533 140191086606144 submission_runner.py:511] Using RNG seed 1061263873
I0406 05:06:37.383607 140191086606144 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 05:06:37.383734 140191086606144 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1.
I0406 05:06:37.383947 140191086606144 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/hparams.json.
I0406 05:06:37.384888 140191086606144 submission_runner.py:230] Starting train once: RAM USED (GB) 5.747761152
I0406 05:06:37.385006 140191086606144 submission_runner.py:231] Initializing dataset.
I0406 05:06:37.385139 140191086606144 input_pipeline.py:20] Loading split = train-clean-100
I0406 05:06:37.413456 140191086606144 input_pipeline.py:20] Loading split = train-clean-360
I0406 05:06:37.769411 140191086606144 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0406 05:06:38.249676 140191086606144 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 6.126116864
I0406 05:06:38.249889 140191086606144 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0406 05:06:45.306334 140191086606144 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.220914176
I0406 05:06:45.306560 140191086606144 submission_runner.py:252] Initializing optimizer.
I0406 05:06:45.884268 140191086606144 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.21400832
I0406 05:06:45.884483 140191086606144 submission_runner.py:261] Initializing metrics bundle.
I0406 05:06:45.884535 140191086606144 submission_runner.py:276] Initializing checkpoint and logger.
I0406 05:06:45.886844 140191086606144 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 05:06:45.887032 140191086606144 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 05:06:46.675039 140191086606144 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0406 05:06:46.676160 140191086606144 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0406 05:06:46.680909 140191086606144 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.219963904
I0406 05:06:46.682158 140191086606144 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.219963904
I0406 05:06:46.682260 140191086606144 submission_runner.py:313] Starting training loop.
I0406 05:06:48.329277 140191086606144 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 24.762871808
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0406 05:06:53.282378 140164789683968 logging_writer.py:48] [0] global_step=0, grad_norm=65.104660, loss=32.242607
I0406 05:06:53.294920 140191086606144 submission.py:139] 0) loss = 32.243, grad_norm = 65.105
I0406 05:06:53.295615 140191086606144 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.73508864
I0406 05:06:53.296220 140191086606144 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.73508864
I0406 05:06:53.296349 140191086606144 spec.py:298] Evaluating on the training split.
I0406 05:06:53.297153 140191086606144 input_pipeline.py:20] Loading split = train-clean-100
I0406 05:06:53.323756 140191086606144 input_pipeline.py:20] Loading split = train-clean-360
I0406 05:06:53.748782 140191086606144 input_pipeline.py:20] Loading split = train-other-500
I0406 05:07:07.372786 140191086606144 spec.py:310] Evaluating on the validation split.
I0406 05:07:07.374022 140191086606144 input_pipeline.py:20] Loading split = dev-clean
I0406 05:07:07.377751 140191086606144 input_pipeline.py:20] Loading split = dev-other
I0406 05:07:17.614197 140191086606144 spec.py:326] Evaluating on the test split.
I0406 05:07:17.615406 140191086606144 input_pipeline.py:20] Loading split = test-clean
I0406 05:07:23.055693 140191086606144 submission_runner.py:382] Time since start: 6.61s, 	Step: 1, 	{'train/ctc_loss': 31.097259711201875, 'train/wer': 2.0820164434742, 'validation/ctc_loss': 30.193957831022704, 'validation/wer': 1.8697339834886304, 'validation/num_examples': 5348, 'test/ctc_loss': 30.261184592554358, 'test/wer': 1.9306765787175268, 'test/num_examples': 2472}
I0406 05:07:23.056509 140191086606144 submission_runner.py:396] After eval at step 1: RAM USED (GB) 45.357350912
I0406 05:07:23.071076 140162105329408 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.612455, test/ctc_loss=30.261185, test/num_examples=2472, test/wer=1.930677, total_duration=6.614719, train/ctc_loss=31.097260, train/wer=2.082016, validation/ctc_loss=30.193958, validation/num_examples=5348, validation/wer=1.869734
I0406 05:07:23.457733 140191086606144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0406 05:07:23.458331 140191086606144 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.384921088
I0406 05:07:23.461251 140191086606144 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.38853376
I0406 05:07:23.503616 140191086606144 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503686 140546894645056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503710 140361108789056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503724 139857517819712 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503708 140627012417344 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503721 139954736666432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503754 140573618386752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:23.503712 140580454397760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:07:24.585690 140162096936704 logging_writer.py:48] [1] global_step=1, grad_norm=65.169075, loss=31.580574
I0406 05:07:24.589143 140191086606144 submission.py:139] 1) loss = 31.581, grad_norm = 65.169
I0406 05:07:24.589851 140191086606144 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 45.563183104
I0406 05:07:25.472633 140162105329408 logging_writer.py:48] [2] global_step=2, grad_norm=89.214500, loss=31.089287
I0406 05:07:25.475693 140191086606144 submission.py:139] 2) loss = 31.089, grad_norm = 89.215
I0406 05:07:26.470369 140162096936704 logging_writer.py:48] [3] global_step=3, grad_norm=102.602219, loss=27.793930
I0406 05:07:26.473836 140191086606144 submission.py:139] 3) loss = 27.794, grad_norm = 102.602
I0406 05:07:27.274788 140162105329408 logging_writer.py:48] [4] global_step=4, grad_norm=88.158234, loss=20.065506
I0406 05:07:27.277904 140191086606144 submission.py:139] 4) loss = 20.066, grad_norm = 88.158
I0406 05:07:28.076924 140162096936704 logging_writer.py:48] [5] global_step=5, grad_norm=51.201153, loss=17.697174
I0406 05:07:28.079885 140191086606144 submission.py:139] 5) loss = 17.697, grad_norm = 51.201
I0406 05:07:28.881830 140162105329408 logging_writer.py:48] [6] global_step=6, grad_norm=89.504875, loss=19.883036
I0406 05:07:28.884808 140191086606144 submission.py:139] 6) loss = 19.883, grad_norm = 89.505
I0406 05:07:29.684147 140162096936704 logging_writer.py:48] [7] global_step=7, grad_norm=31.202253, loss=16.419565
I0406 05:07:29.687457 140191086606144 submission.py:139] 7) loss = 16.420, grad_norm = 31.202
I0406 05:07:30.486058 140162105329408 logging_writer.py:48] [8] global_step=8, grad_norm=35.577293, loss=16.908087
I0406 05:07:30.489676 140191086606144 submission.py:139] 8) loss = 16.908, grad_norm = 35.577
I0406 05:07:31.289331 140162096936704 logging_writer.py:48] [9] global_step=9, grad_norm=28.205460, loss=16.155203
I0406 05:07:31.292542 140191086606144 submission.py:139] 9) loss = 16.155, grad_norm = 28.205
I0406 05:07:32.092370 140162105329408 logging_writer.py:48] [10] global_step=10, grad_norm=27.821676, loss=16.199755
I0406 05:07:32.095443 140191086606144 submission.py:139] 10) loss = 16.200, grad_norm = 27.822
I0406 05:07:32.890759 140162096936704 logging_writer.py:48] [11] global_step=11, grad_norm=12.387616, loss=15.058078
I0406 05:07:32.894021 140191086606144 submission.py:139] 11) loss = 15.058, grad_norm = 12.388
I0406 05:07:33.693722 140162105329408 logging_writer.py:48] [12] global_step=12, grad_norm=15.861888, loss=15.941227
I0406 05:07:33.696905 140191086606144 submission.py:139] 12) loss = 15.941, grad_norm = 15.862
I0406 05:07:34.496434 140162096936704 logging_writer.py:48] [13] global_step=13, grad_norm=13.635286, loss=16.632120
I0406 05:07:34.499600 140191086606144 submission.py:139] 13) loss = 16.632, grad_norm = 13.635
I0406 05:07:35.298430 140162105329408 logging_writer.py:48] [14] global_step=14, grad_norm=15.569079, loss=15.780209
I0406 05:07:35.301445 140191086606144 submission.py:139] 14) loss = 15.780, grad_norm = 15.569
I0406 05:07:36.095836 140162096936704 logging_writer.py:48] [15] global_step=15, grad_norm=16.707842, loss=14.934932
I0406 05:07:36.098810 140191086606144 submission.py:139] 15) loss = 14.935, grad_norm = 16.708
I0406 05:07:36.898828 140162105329408 logging_writer.py:48] [16] global_step=16, grad_norm=11.898601, loss=16.066696
I0406 05:07:36.901946 140191086606144 submission.py:139] 16) loss = 16.067, grad_norm = 11.899
I0406 05:07:37.702590 140162096936704 logging_writer.py:48] [17] global_step=17, grad_norm=18.921007, loss=14.829457
I0406 05:07:37.705572 140191086606144 submission.py:139] 17) loss = 14.829, grad_norm = 18.921
I0406 05:07:38.504040 140162105329408 logging_writer.py:48] [18] global_step=18, grad_norm=27.844160, loss=14.089922
I0406 05:07:38.507529 140191086606144 submission.py:139] 18) loss = 14.090, grad_norm = 27.844
I0406 05:07:39.302480 140162096936704 logging_writer.py:48] [19] global_step=19, grad_norm=25.558895, loss=13.788408
I0406 05:07:39.305532 140191086606144 submission.py:139] 19) loss = 13.788, grad_norm = 25.559
I0406 05:07:40.105456 140162105329408 logging_writer.py:48] [20] global_step=20, grad_norm=26.546583, loss=13.675966
I0406 05:07:40.108545 140191086606144 submission.py:139] 20) loss = 13.676, grad_norm = 26.547
I0406 05:07:40.906061 140162096936704 logging_writer.py:48] [21] global_step=21, grad_norm=59.601707, loss=16.126682
I0406 05:07:40.909147 140191086606144 submission.py:139] 21) loss = 16.127, grad_norm = 59.602
I0406 05:07:41.709026 140162105329408 logging_writer.py:48] [22] global_step=22, grad_norm=32.119507, loss=14.350416
I0406 05:07:41.712159 140191086606144 submission.py:139] 22) loss = 14.350, grad_norm = 32.120
I0406 05:07:42.510251 140162096936704 logging_writer.py:48] [23] global_step=23, grad_norm=80.467766, loss=17.981447
I0406 05:07:42.513681 140191086606144 submission.py:139] 23) loss = 17.981, grad_norm = 80.468
I0406 05:07:43.310458 140162105329408 logging_writer.py:48] [24] global_step=24, grad_norm=43.482040, loss=14.561659
I0406 05:07:43.314106 140191086606144 submission.py:139] 24) loss = 14.562, grad_norm = 43.482
I0406 05:07:44.111450 140162096936704 logging_writer.py:48] [25] global_step=25, grad_norm=48.209698, loss=14.830312
I0406 05:07:44.114625 140191086606144 submission.py:139] 25) loss = 14.830, grad_norm = 48.210
I0406 05:07:44.917505 140162105329408 logging_writer.py:48] [26] global_step=26, grad_norm=59.266151, loss=13.411380
I0406 05:07:44.920518 140191086606144 submission.py:139] 26) loss = 13.411, grad_norm = 59.266
I0406 05:07:45.715767 140162096936704 logging_writer.py:48] [27] global_step=27, grad_norm=48.737614, loss=14.510018
I0406 05:07:45.718763 140191086606144 submission.py:139] 27) loss = 14.510, grad_norm = 48.738
I0406 05:07:46.511578 140162105329408 logging_writer.py:48] [28] global_step=28, grad_norm=39.013687, loss=13.381175
I0406 05:07:46.515133 140191086606144 submission.py:139] 28) loss = 13.381, grad_norm = 39.014
I0406 05:07:47.308706 140162096936704 logging_writer.py:48] [29] global_step=29, grad_norm=17.597000, loss=13.039693
I0406 05:07:47.311844 140191086606144 submission.py:139] 29) loss = 13.040, grad_norm = 17.597
I0406 05:07:48.110793 140162105329408 logging_writer.py:48] [30] global_step=30, grad_norm=68.285034, loss=12.597500
I0406 05:07:48.114153 140191086606144 submission.py:139] 30) loss = 12.597, grad_norm = 68.285
I0406 05:07:48.916192 140162096936704 logging_writer.py:48] [31] global_step=31, grad_norm=27.723656, loss=14.120981
I0406 05:07:48.919321 140191086606144 submission.py:139] 31) loss = 14.121, grad_norm = 27.724
I0406 05:07:49.720251 140162105329408 logging_writer.py:48] [32] global_step=32, grad_norm=29.293821, loss=12.839649
I0406 05:07:49.723565 140191086606144 submission.py:139] 32) loss = 12.840, grad_norm = 29.294
I0406 05:07:50.519643 140162096936704 logging_writer.py:48] [33] global_step=33, grad_norm=3.541907, loss=10.022690
I0406 05:07:50.523030 140191086606144 submission.py:139] 33) loss = 10.023, grad_norm = 3.542
I0406 05:07:51.321669 140162105329408 logging_writer.py:48] [34] global_step=34, grad_norm=103.350220, loss=12.965325
I0406 05:07:51.324842 140191086606144 submission.py:139] 34) loss = 12.965, grad_norm = 103.350
I0406 05:07:52.124759 140162096936704 logging_writer.py:48] [35] global_step=35, grad_norm=25.772110, loss=17.755688
I0406 05:07:52.127789 140191086606144 submission.py:139] 35) loss = 17.756, grad_norm = 25.772
I0406 05:07:52.927659 140162105329408 logging_writer.py:48] [36] global_step=36, grad_norm=24.878210, loss=17.911762
I0406 05:07:52.930809 140191086606144 submission.py:139] 36) loss = 17.912, grad_norm = 24.878
I0406 05:07:53.725539 140162096936704 logging_writer.py:48] [37] global_step=37, grad_norm=24.686972, loss=16.587593
I0406 05:07:53.728645 140191086606144 submission.py:139] 37) loss = 16.588, grad_norm = 24.687
I0406 05:07:54.524634 140162105329408 logging_writer.py:48] [38] global_step=38, grad_norm=24.139446, loss=13.724424
I0406 05:07:54.528066 140191086606144 submission.py:139] 38) loss = 13.724, grad_norm = 24.139
I0406 05:07:55.325348 140162096936704 logging_writer.py:48] [39] global_step=39, grad_norm=11.163245, loss=10.219393
I0406 05:07:55.328609 140191086606144 submission.py:139] 39) loss = 10.219, grad_norm = 11.163
I0406 05:07:56.128882 140162105329408 logging_writer.py:48] [40] global_step=40, grad_norm=56.163860, loss=15.865840
I0406 05:07:56.131943 140191086606144 submission.py:139] 40) loss = 15.866, grad_norm = 56.164
I0406 05:07:56.928918 140162096936704 logging_writer.py:48] [41] global_step=41, grad_norm=49.502041, loss=19.342667
I0406 05:07:56.931997 140191086606144 submission.py:139] 41) loss = 19.343, grad_norm = 49.502
I0406 05:07:57.726472 140162105329408 logging_writer.py:48] [42] global_step=42, grad_norm=37.125484, loss=13.647584
I0406 05:07:57.729478 140191086606144 submission.py:139] 42) loss = 13.648, grad_norm = 37.125
I0406 05:07:58.528009 140162096936704 logging_writer.py:48] [43] global_step=43, grad_norm=55.891232, loss=12.046511
I0406 05:07:58.531244 140191086606144 submission.py:139] 43) loss = 12.047, grad_norm = 55.891
I0406 05:07:59.334755 140162105329408 logging_writer.py:48] [44] global_step=44, grad_norm=23.083561, loss=14.376638
I0406 05:07:59.337901 140191086606144 submission.py:139] 44) loss = 14.377, grad_norm = 23.084
I0406 05:08:00.136637 140162096936704 logging_writer.py:48] [45] global_step=45, grad_norm=23.019966, loss=14.690150
I0406 05:08:00.139814 140191086606144 submission.py:139] 45) loss = 14.690, grad_norm = 23.020
I0406 05:08:00.937135 140162105329408 logging_writer.py:48] [46] global_step=46, grad_norm=22.693739, loss=13.550646
I0406 05:08:00.940192 140191086606144 submission.py:139] 46) loss = 13.551, grad_norm = 22.694
I0406 05:08:01.736881 140162096936704 logging_writer.py:48] [47] global_step=47, grad_norm=17.673840, loss=10.992040
I0406 05:08:01.739899 140191086606144 submission.py:139] 47) loss = 10.992, grad_norm = 17.674
I0406 05:08:02.539269 140162105329408 logging_writer.py:48] [48] global_step=48, grad_norm=57.564137, loss=12.695488
I0406 05:08:02.542795 140191086606144 submission.py:139] 48) loss = 12.695, grad_norm = 57.564
I0406 05:08:03.343997 140162096936704 logging_writer.py:48] [49] global_step=49, grad_norm=22.192904, loss=13.029415
I0406 05:08:03.347037 140191086606144 submission.py:139] 49) loss = 13.029, grad_norm = 22.193
I0406 05:08:04.142718 140162105329408 logging_writer.py:48] [50] global_step=50, grad_norm=19.890652, loss=11.568916
I0406 05:08:04.146347 140191086606144 submission.py:139] 50) loss = 11.569, grad_norm = 19.891
I0406 05:08:04.940649 140162096936704 logging_writer.py:48] [51] global_step=51, grad_norm=12.981270, loss=10.498466
I0406 05:08:04.943601 140191086606144 submission.py:139] 51) loss = 10.498, grad_norm = 12.981
I0406 05:08:05.743763 140162105329408 logging_writer.py:48] [52] global_step=52, grad_norm=1.819282, loss=10.306299
I0406 05:08:05.746884 140191086606144 submission.py:139] 52) loss = 10.306, grad_norm = 1.819
I0406 05:08:06.550191 140162096936704 logging_writer.py:48] [53] global_step=53, grad_norm=9.541521, loss=10.405287
I0406 05:08:06.553388 140191086606144 submission.py:139] 53) loss = 10.405, grad_norm = 9.542
I0406 05:08:07.349251 140162105329408 logging_writer.py:48] [54] global_step=54, grad_norm=8.758544, loss=10.407990
I0406 05:08:07.352530 140191086606144 submission.py:139] 54) loss = 10.408, grad_norm = 8.759
I0406 05:08:08.146487 140162096936704 logging_writer.py:48] [55] global_step=55, grad_norm=11.072630, loss=10.436545
I0406 05:08:08.149598 140191086606144 submission.py:139] 55) loss = 10.437, grad_norm = 11.073
I0406 05:08:08.946858 140162105329408 logging_writer.py:48] [56] global_step=56, grad_norm=12.179904, loss=10.525216
I0406 05:08:08.950044 140191086606144 submission.py:139] 56) loss = 10.525, grad_norm = 12.180
I0406 05:08:09.750351 140162096936704 logging_writer.py:48] [57] global_step=57, grad_norm=13.784224, loss=10.478906
I0406 05:08:09.753377 140191086606144 submission.py:139] 57) loss = 10.479, grad_norm = 13.784
I0406 05:08:10.553259 140162105329408 logging_writer.py:48] [58] global_step=58, grad_norm=14.476457, loss=10.692938
I0406 05:08:10.556367 140191086606144 submission.py:139] 58) loss = 10.693, grad_norm = 14.476
I0406 05:08:11.350409 140162096936704 logging_writer.py:48] [59] global_step=59, grad_norm=14.623138, loss=10.426025
I0406 05:08:11.353528 140191086606144 submission.py:139] 59) loss = 10.426, grad_norm = 14.623
I0406 05:08:12.147718 140162105329408 logging_writer.py:48] [60] global_step=60, grad_norm=14.961770, loss=10.603723
I0406 05:08:12.150771 140191086606144 submission.py:139] 60) loss = 10.604, grad_norm = 14.962
I0406 05:08:12.951623 140162096936704 logging_writer.py:48] [61] global_step=61, grad_norm=15.383391, loss=10.318524
I0406 05:08:12.954955 140191086606144 submission.py:139] 61) loss = 10.319, grad_norm = 15.383
I0406 05:08:13.760054 140162105329408 logging_writer.py:48] [62] global_step=62, grad_norm=15.591708, loss=10.634780
I0406 05:08:13.763443 140191086606144 submission.py:139] 62) loss = 10.635, grad_norm = 15.592
I0406 05:08:14.558699 140162096936704 logging_writer.py:48] [63] global_step=63, grad_norm=17.402100, loss=10.258567
I0406 05:08:14.561840 140191086606144 submission.py:139] 63) loss = 10.259, grad_norm = 17.402
I0406 05:08:15.358325 140162105329408 logging_writer.py:48] [64] global_step=64, grad_norm=17.383076, loss=10.747128
I0406 05:08:15.361390 140191086606144 submission.py:139] 64) loss = 10.747, grad_norm = 17.383
I0406 05:08:16.161776 140162096936704 logging_writer.py:48] [65] global_step=65, grad_norm=14.527952, loss=10.154382
I0406 05:08:16.165010 140191086606144 submission.py:139] 65) loss = 10.154, grad_norm = 14.528
I0406 05:08:16.966567 140162105329408 logging_writer.py:48] [66] global_step=66, grad_norm=15.305489, loss=10.371933
I0406 05:08:16.969593 140191086606144 submission.py:139] 66) loss = 10.372, grad_norm = 15.305
I0406 05:08:17.768962 140162096936704 logging_writer.py:48] [67] global_step=67, grad_norm=22.117987, loss=10.235836
I0406 05:08:17.772135 140191086606144 submission.py:139] 67) loss = 10.236, grad_norm = 22.118
I0406 05:08:18.569100 140162105329408 logging_writer.py:48] [68] global_step=68, grad_norm=19.761353, loss=11.105968
I0406 05:08:18.572499 140191086606144 submission.py:139] 68) loss = 11.106, grad_norm = 19.761
I0406 05:08:19.369259 140162096936704 logging_writer.py:48] [69] global_step=69, grad_norm=4.096100, loss=9.602109
I0406 05:08:19.372485 140191086606144 submission.py:139] 69) loss = 9.602, grad_norm = 4.096
I0406 05:08:20.174998 140162105329408 logging_writer.py:48] [70] global_step=70, grad_norm=1.730875, loss=9.562473
I0406 05:08:20.178342 140191086606144 submission.py:139] 70) loss = 9.562, grad_norm = 1.731
I0406 05:08:20.983361 140162096936704 logging_writer.py:48] [71] global_step=71, grad_norm=2.877612, loss=9.479819
I0406 05:08:20.986746 140191086606144 submission.py:139] 71) loss = 9.480, grad_norm = 2.878
I0406 05:08:21.784672 140162105329408 logging_writer.py:48] [72] global_step=72, grad_norm=4.035660, loss=9.352375
I0406 05:08:21.788783 140191086606144 submission.py:139] 72) loss = 9.352, grad_norm = 4.036
I0406 05:08:22.583671 140162096936704 logging_writer.py:48] [73] global_step=73, grad_norm=10.908173, loss=9.466912
I0406 05:08:22.587652 140191086606144 submission.py:139] 73) loss = 9.467, grad_norm = 10.908
I0406 05:08:23.386653 140162105329408 logging_writer.py:48] [74] global_step=74, grad_norm=15.398134, loss=9.837726
I0406 05:08:23.390394 140191086606144 submission.py:139] 74) loss = 9.838, grad_norm = 15.398
I0406 05:08:24.190162 140162096936704 logging_writer.py:48] [75] global_step=75, grad_norm=30.741344, loss=10.076655
I0406 05:08:24.193874 140191086606144 submission.py:139] 75) loss = 10.077, grad_norm = 30.741
I0406 05:08:24.993668 140162105329408 logging_writer.py:48] [76] global_step=76, grad_norm=21.424589, loss=12.442343
I0406 05:08:24.997680 140191086606144 submission.py:139] 76) loss = 12.442, grad_norm = 21.425
I0406 05:08:25.794162 140162096936704 logging_writer.py:48] [77] global_step=77, grad_norm=17.907974, loss=10.108255
I0406 05:08:25.798086 140191086606144 submission.py:139] 77) loss = 10.108, grad_norm = 17.908
I0406 05:08:26.592644 140162105329408 logging_writer.py:48] [78] global_step=78, grad_norm=54.200394, loss=11.534917
I0406 05:08:26.596378 140191086606144 submission.py:139] 78) loss = 11.535, grad_norm = 54.200
I0406 05:08:27.397778 140162096936704 logging_writer.py:48] [79] global_step=79, grad_norm=21.700371, loss=15.694029
I0406 05:08:27.401450 140191086606144 submission.py:139] 79) loss = 15.694, grad_norm = 21.700
I0406 05:08:28.205314 140162105329408 logging_writer.py:48] [80] global_step=80, grad_norm=21.517595, loss=14.476823
I0406 05:08:28.209086 140191086606144 submission.py:139] 80) loss = 14.477, grad_norm = 21.518
I0406 05:08:29.007541 140162096936704 logging_writer.py:48] [81] global_step=81, grad_norm=20.338026, loss=10.978702
I0406 05:08:29.011717 140191086606144 submission.py:139] 81) loss = 10.979, grad_norm = 20.338
I0406 05:08:29.807972 140162105329408 logging_writer.py:48] [82] global_step=82, grad_norm=64.368126, loss=12.477744
I0406 05:08:29.811708 140191086606144 submission.py:139] 82) loss = 12.478, grad_norm = 64.368
I0406 05:08:30.611182 140162096936704 logging_writer.py:48] [83] global_step=83, grad_norm=21.224413, loss=16.656248
I0406 05:08:30.614966 140191086606144 submission.py:139] 83) loss = 16.656, grad_norm = 21.224
I0406 05:08:31.415904 140162105329408 logging_writer.py:48] [84] global_step=84, grad_norm=21.297218, loss=15.475242
I0406 05:08:31.419664 140191086606144 submission.py:139] 84) loss = 15.475, grad_norm = 21.297
I0406 05:08:32.219774 140162096936704 logging_writer.py:48] [85] global_step=85, grad_norm=20.815809, loss=11.922629
I0406 05:08:32.223397 140191086606144 submission.py:139] 85) loss = 11.923, grad_norm = 20.816
I0406 05:08:33.020898 140162105329408 logging_writer.py:48] [86] global_step=86, grad_norm=36.713219, loss=9.902679
I0406 05:08:33.025224 140191086606144 submission.py:139] 86) loss = 9.903, grad_norm = 36.713
I0406 05:08:33.819258 140162096936704 logging_writer.py:48] [87] global_step=87, grad_norm=20.460325, loss=11.495549
I0406 05:08:33.823081 140191086606144 submission.py:139] 87) loss = 11.496, grad_norm = 20.460
I0406 05:08:34.623171 140162105329408 logging_writer.py:48] [88] global_step=88, grad_norm=1.715855, loss=8.647425
I0406 05:08:34.627090 140191086606144 submission.py:139] 88) loss = 8.647, grad_norm = 1.716
I0406 05:08:35.431280 140162096936704 logging_writer.py:48] [89] global_step=89, grad_norm=36.472672, loss=9.774776
I0406 05:08:35.435042 140191086606144 submission.py:139] 89) loss = 9.775, grad_norm = 36.473
I0406 05:08:36.232621 140162105329408 logging_writer.py:48] [90] global_step=90, grad_norm=20.560518, loss=13.198618
I0406 05:08:36.236448 140191086606144 submission.py:139] 90) loss = 13.199, grad_norm = 20.561
I0406 05:08:37.032591 140162096936704 logging_writer.py:48] [91] global_step=91, grad_norm=19.858656, loss=10.892621
I0406 05:08:37.036303 140191086606144 submission.py:139] 91) loss = 10.893, grad_norm = 19.859
I0406 05:08:37.838883 140162105329408 logging_writer.py:48] [92] global_step=92, grad_norm=21.686985, loss=8.900803
I0406 05:08:37.842750 140191086606144 submission.py:139] 92) loss = 8.901, grad_norm = 21.687
I0406 05:08:38.642606 140162096936704 logging_writer.py:48] [93] global_step=93, grad_norm=18.136614, loss=9.761521
I0406 05:08:38.646156 140191086606144 submission.py:139] 93) loss = 9.762, grad_norm = 18.137
I0406 05:08:39.445535 140162105329408 logging_writer.py:48] [94] global_step=94, grad_norm=38.165806, loss=9.768991
I0406 05:08:39.449198 140191086606144 submission.py:139] 94) loss = 9.769, grad_norm = 38.166
I0406 05:08:40.247193 140162096936704 logging_writer.py:48] [95] global_step=95, grad_norm=20.311884, loss=13.510381
I0406 05:08:40.250824 140191086606144 submission.py:139] 95) loss = 13.510, grad_norm = 20.312
I0406 05:08:41.048046 140162105329408 logging_writer.py:48] [96] global_step=96, grad_norm=19.733295, loss=11.322472
I0406 05:08:41.051801 140191086606144 submission.py:139] 96) loss = 11.322, grad_norm = 19.733
I0406 05:08:41.853638 140162096936704 logging_writer.py:48] [97] global_step=97, grad_norm=12.130954, loss=8.597754
I0406 05:08:41.857140 140191086606144 submission.py:139] 97) loss = 8.598, grad_norm = 12.131
I0406 05:08:42.661113 140162105329408 logging_writer.py:48] [98] global_step=98, grad_norm=6.844533, loss=8.512214
I0406 05:08:42.664745 140191086606144 submission.py:139] 98) loss = 8.512, grad_norm = 6.845
I0406 05:08:43.461512 140162096936704 logging_writer.py:48] [99] global_step=99, grad_norm=40.034637, loss=9.931684
I0406 05:08:43.465118 140191086606144 submission.py:139] 99) loss = 9.932, grad_norm = 40.035
I0406 05:08:44.261577 140162105329408 logging_writer.py:48] [100] global_step=100, grad_norm=19.842432, loss=14.541228
I0406 05:08:44.265356 140191086606144 submission.py:139] 100) loss = 14.541, grad_norm = 19.842
I0406 05:13:53.964886 140162096936704 logging_writer.py:48] [500] global_step=500, grad_norm=nan, loss=nan
I0406 05:13:53.969806 140191086606144 submission.py:139] 500) loss = nan, grad_norm = nan
I0406 05:20:12.162121 140162105329408 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0406 05:20:12.167461 140191086606144 submission.py:139] 1000) loss = nan, grad_norm = nan
I0406 05:26:32.276589 140162105329408 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0406 05:26:32.285763 140191086606144 submission.py:139] 1500) loss = nan, grad_norm = nan
I0406 05:32:50.459961 140162096936704 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0406 05:32:50.463780 140191086606144 submission.py:139] 2000) loss = nan, grad_norm = nan
I0406 05:39:10.324918 140162096936704 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0406 05:39:10.332026 140191086606144 submission.py:139] 2500) loss = nan, grad_norm = nan
I0406 05:45:28.554249 140162088544000 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0406 05:45:28.558674 140191086606144 submission.py:139] 3000) loss = nan, grad_norm = nan
I0406 05:47:24.427471 140191086606144 submission_runner.py:373] Before eval at step 3152: RAM USED (GB) 39.590760448
I0406 05:47:24.427885 140191086606144 spec.py:298] Evaluating on the training split.
I0406 05:47:34.293899 140191086606144 spec.py:310] Evaluating on the validation split.
I0406 05:47:43.610496 140191086606144 spec.py:326] Evaluating on the test split.
I0406 05:47:48.956716 140191086606144 submission_runner.py:382] Time since start: 2437.42s, 	Step: 3152, 	{'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 05:47:48.957653 140191086606144 submission_runner.py:396] After eval at step 3152: RAM USED (GB) 38.950424576
I0406 05:47:48.972191 140162088544000 logging_writer.py:48] [3152] global_step=3152, preemption_count=0, score=1411.472917, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2437.421297, train/ctc_loss=nan, train/wer=0.941612, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 05:47:49.394254 140191086606144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_3152.
I0406 05:47:49.395000 140191086606144 submission_runner.py:416] After logging and checkpointing eval at step 3152: RAM USED (GB) 38.965547008
I0406 05:52:13.204865 140162080151296 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0406 05:52:13.209279 140191086606144 submission.py:139] 3500) loss = nan, grad_norm = nan
I0406 05:58:31.176376 140162088544000 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0406 05:58:31.180598 140191086606144 submission.py:139] 4000) loss = nan, grad_norm = nan
I0406 06:04:50.914929 140162088544000 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0406 06:04:50.921496 140191086606144 submission.py:139] 4500) loss = nan, grad_norm = nan
I0406 06:11:09.045772 140162080151296 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0406 06:11:09.050401 140191086606144 submission.py:139] 5000) loss = nan, grad_norm = nan
I0406 06:17:28.795838 140162080151296 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0406 06:17:28.802563 140191086606144 submission.py:139] 5500) loss = nan, grad_norm = nan
I0406 06:23:46.703738 140162071758592 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0406 06:23:46.713479 140191086606144 submission.py:139] 6000) loss = nan, grad_norm = nan
I0406 06:27:50.383039 140191086606144 submission_runner.py:373] Before eval at step 6321: RAM USED (GB) 39.207927808
I0406 06:27:50.383487 140191086606144 spec.py:298] Evaluating on the training split.
I0406 06:28:00.082815 140191086606144 spec.py:310] Evaluating on the validation split.
I0406 06:28:09.428411 140191086606144 spec.py:326] Evaluating on the test split.
I0406 06:28:14.696080 140191086606144 submission_runner.py:382] Time since start: 4863.37s, 	Step: 6321, 	{'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 06:28:14.696863 140191086606144 submission_runner.py:396] After eval at step 6321: RAM USED (GB) 38.97694208
I0406 06:28:14.712674 140162080151296 logging_writer.py:48] [6321] global_step=6321, preemption_count=0, score=2782.731679, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4863.373869, train/ctc_loss=nan, train/wer=0.941612, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 06:28:15.088653 140191086606144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_6321.
I0406 06:28:15.089260 140191086606144 submission_runner.py:416] After logging and checkpointing eval at step 6321: RAM USED (GB) 38.981881856
I0406 06:30:31.119825 140162071758592 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0406 06:30:31.123246 140191086606144 submission.py:139] 6500) loss = nan, grad_norm = nan
I0406 06:36:49.106192 140162080151296 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0406 06:36:49.110482 140191086606144 submission.py:139] 7000) loss = nan, grad_norm = nan
I0406 06:43:08.776000 140162080151296 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0406 06:43:08.783221 140191086606144 submission.py:139] 7500) loss = nan, grad_norm = nan
I0406 06:49:26.677849 140162071758592 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0406 06:49:26.683218 140191086606144 submission.py:139] 8000) loss = nan, grad_norm = nan
I0406 06:55:46.316715 140162080151296 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0406 06:55:46.322552 140191086606144 submission.py:139] 8500) loss = nan, grad_norm = nan
I0406 07:02:04.193038 140162071758592 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0406 07:02:04.197968 140191086606144 submission.py:139] 9000) loss = nan, grad_norm = nan
I0406 07:08:15.549640 140191086606144 submission_runner.py:373] Before eval at step 9490: RAM USED (GB) 39.241138176
I0406 07:08:15.550115 140191086606144 spec.py:298] Evaluating on the training split.
I0406 07:08:25.448394 140191086606144 spec.py:310] Evaluating on the validation split.
I0406 07:08:35.198064 140191086606144 spec.py:326] Evaluating on the test split.
I0406 07:08:40.832226 140191086606144 submission_runner.py:382] Time since start: 7288.54s, 	Step: 9490, 	{'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 07:08:40.833002 140191086606144 submission_runner.py:396] After eval at step 9490: RAM USED (GB) 39.094009856
I0406 07:08:40.850677 140162080151296 logging_writer.py:48] [9490] global_step=9490, preemption_count=0, score=4153.301640, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7288.538531, train/ctc_loss=nan, train/wer=0.941612, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 07:08:41.229813 140191086606144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_9490.
I0406 07:08:41.230373 140191086606144 submission_runner.py:416] After logging and checkpointing eval at step 9490: RAM USED (GB) 39.09976064
I0406 07:08:49.552193 140162071758592 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0406 07:08:49.556106 140191086606144 submission.py:139] 9500) loss = nan, grad_norm = nan
I0406 07:15:06.842928 140191086606144 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 39.49924352
I0406 07:15:06.843144 140191086606144 spec.py:298] Evaluating on the training split.
I0406 07:15:16.179197 140191086606144 spec.py:310] Evaluating on the validation split.
I0406 07:15:25.435118 140191086606144 spec.py:326] Evaluating on the test split.
I0406 07:15:30.601815 140191086606144 submission_runner.py:382] Time since start: 7699.84s, 	Step: 10000, 	{'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0406 07:15:30.602640 140191086606144 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 39.448870912
I0406 07:15:30.618463 140162080151296 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4373.302424, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7699.835722, train/ctc_loss=nan, train/wer=0.941612, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0406 07:15:30.989523 140191086606144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0406 07:15:30.990133 140191086606144 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 39.4561536
I0406 07:15:30.998711 140162071758592 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4373.302424
I0406 07:15:31.689794 140191086606144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0406 07:15:31.837352 140191086606144 submission_runner.py:550] Tuning trial 1/1
I0406 07:15:31.837588 140191086606144 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 07:15:31.837924 140191086606144 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.097259711201875, 'train/wer': 2.0820164434742, 'validation/ctc_loss': 30.193957831022704, 'validation/wer': 1.8697339834886304, 'validation/num_examples': 5348, 'test/ctc_loss': 30.261184592554358, 'test/wer': 1.9306765787175268, 'test/num_examples': 2472, 'score': 6.612454652786255, 'total_duration': 6.614719390869141, 'global_step': 1, 'preemption_count': 0}), (3152, {'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1411.4729173183441, 'total_duration': 2437.4212968349457, 'global_step': 3152, 'preemption_count': 0}), (6321, {'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2782.731678724289, 'total_duration': 4863.373869419098, 'global_step': 6321, 'preemption_count': 0}), (9490, {'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4153.301639556885, 'total_duration': 7288.538530826569, 'global_step': 9490, 'preemption_count': 0}), (10000, {'train/ctc_loss': nan, 'train/wer': 0.9416115473591606, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4373.302423715591, 'total_duration': 7699.835722208023, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 07:15:31.838007 140191086606144 submission_runner.py:553] Timing: 4373.302423715591
I0406 07:15:31.838071 140191086606144 submission_runner.py:554] ====================
I0406 07:15:31.838219 140191086606144 submission_runner.py:613] Final librispeech_conformer score: 4373.302423715591
