WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 05:16:09.435041 139653446465344 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 05:16:09.435071 140549472933696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 05:16:09.435070 139765924620096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 05:16:09.435647 140097404544832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 05:16:09.435848 140413402781504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 05:16:09.435988 140073578403648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 05:16:09.436109 139910496442176 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 05:16:09.446393 140413402781504 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.446317 140218958939968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 05:16:09.446610 140218958939968 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.446583 140073578403648 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.446763 139910496442176 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.456044 139653446465344 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.456063 139765924620096 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.456094 140549472933696 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.456511 140097404544832 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 05:16:09.985215 140218958939968 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch.
W0406 05:16:10.001945 140413402781504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.002557 139765924620096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.003867 139910496442176 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.004040 140549472933696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.004769 139653446465344 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.005780 140097404544832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.005928 140073578403648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 05:16:10.018997 140218958939968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 05:16:10.022279 140218958939968 submission_runner.py:511] Using RNG seed 411261429
I0406 05:16:10.023434 140218958939968 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 05:16:10.023556 140218958939968 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1.
I0406 05:16:10.023739 140218958939968 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/hparams.json.
I0406 05:16:10.024645 140218958939968 submission_runner.py:230] Starting train once: RAM USED (GB) 5.75993856
I0406 05:16:10.024738 140218958939968 submission_runner.py:231] Initializing dataset.
I0406 05:16:10.024825 140218958939968 input_pipeline.py:20] Loading split = train-clean-100
I0406 05:16:10.053649 140218958939968 input_pipeline.py:20] Loading split = train-clean-360
I0406 05:16:10.366453 140218958939968 input_pipeline.py:20] Loading split = train-other-500
I0406 05:16:10.779178 140218958939968 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 6.060142592
I0406 05:16:10.779349 140218958939968 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0406 05:16:17.815658 140218958939968 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.545198592
I0406 05:16:17.815891 140218958939968 submission_runner.py:252] Initializing optimizer.
I0406 05:16:17.816978 140218958939968 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.19707136
I0406 05:16:17.817096 140218958939968 submission_runner.py:261] Initializing metrics bundle.
I0406 05:16:17.817145 140218958939968 submission_runner.py:276] Initializing checkpoint and logger.
I0406 05:16:17.818503 140218958939968 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 05:16:17.818666 140218958939968 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 05:16:18.599610 140218958939968 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0406 05:16:18.600625 140218958939968 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0406 05:16:18.605128 140218958939968 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.198365696
I0406 05:16:18.606297 140218958939968 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.198365696
I0406 05:16:18.606400 140218958939968 submission_runner.py:313] Starting training loop.
I0406 05:16:20.177550 140218958939968 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 25.086029824
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0406 05:16:25.268638 140192614700800 logging_writer.py:48] [0] global_step=0, grad_norm=42.924114, loss=32.967693
I0406 05:16:25.281644 140218958939968 submission.py:296] 0) loss = 32.968, grad_norm = 42.924
I0406 05:16:25.282453 140218958939968 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 33.048764416
I0406 05:16:25.284137 140218958939968 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 33.0508288
I0406 05:16:25.284294 140218958939968 spec.py:298] Evaluating on the training split.
I0406 05:16:25.285140 140218958939968 input_pipeline.py:20] Loading split = train-clean-100
I0406 05:16:25.314502 140218958939968 input_pipeline.py:20] Loading split = train-clean-360
I0406 05:16:25.759647 140218958939968 input_pipeline.py:20] Loading split = train-other-500
I0406 05:16:38.697857 140218958939968 spec.py:310] Evaluating on the validation split.
I0406 05:16:38.699408 140218958939968 input_pipeline.py:20] Loading split = dev-clean
I0406 05:16:38.703604 140218958939968 input_pipeline.py:20] Loading split = dev-other
I0406 05:16:48.756511 140218958939968 spec.py:326] Evaluating on the test split.
I0406 05:16:48.763479 140218958939968 input_pipeline.py:20] Loading split = test-clean
I0406 05:16:54.058313 140218958939968 submission_runner.py:382] Time since start: 6.68s, 	Step: 1, 	{'train/ctc_loss': 31.780920545127284, 'train/wer': 1.7285201340641314, 'validation/ctc_loss': 31.147968153506127, 'validation/wer': 1.430628107951528, 'validation/num_examples': 5348, 'test/ctc_loss': 31.23045357765017, 'test/wer': 1.4720817337964374, 'test/num_examples': 2472}
I0406 05:16:54.059064 140218958939968 submission_runner.py:396] After eval at step 1: RAM USED (GB) 45.742641152
I0406 05:16:54.073136 140178215749376 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.675213, test/ctc_loss=31.230454, test/num_examples=2472, test/wer=1.472082, total_duration=6.677338, train/ctc_loss=31.780921, train/wer=1.728520, validation/ctc_loss=31.147968, validation/num_examples=5348, validation/wer=1.430628
I0406 05:16:54.646015 140218958939968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0406 05:16:54.646638 140218958939968 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 45.76792576
I0406 05:16:54.649620 140218958939968 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 45.770764288
I0406 05:16:54.688640 140218958939968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.688731 140073578403648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.688716 139765924620096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.688710 139910496442176 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.688734 139653446465344 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.689000 140097404544832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.689806 140413402781504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:54.689899 140549472933696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 05:16:55.828491 140177867536128 logging_writer.py:48] [1] global_step=1, grad_norm=40.921158, loss=32.281586
I0406 05:16:55.831559 140218958939968 submission.py:296] 1) loss = 32.282, grad_norm = 40.921
I0406 05:16:55.832280 140218958939968 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 45.928165376
I0406 05:16:56.683858 140178215749376 logging_writer.py:48] [2] global_step=2, grad_norm=43.126381, loss=32.725964
I0406 05:16:56.687188 140218958939968 submission.py:296] 2) loss = 32.726, grad_norm = 43.126
I0406 05:16:57.690127 140177867536128 logging_writer.py:48] [3] global_step=3, grad_norm=47.007252, loss=32.825123
I0406 05:16:57.693305 140218958939968 submission.py:296] 3) loss = 32.825, grad_norm = 47.007
I0406 05:16:58.497225 140178215749376 logging_writer.py:48] [4] global_step=4, grad_norm=51.297729, loss=32.341511
I0406 05:16:58.500268 140218958939968 submission.py:296] 4) loss = 32.342, grad_norm = 51.298
I0406 05:16:59.302574 140177867536128 logging_writer.py:48] [5] global_step=5, grad_norm=58.832664, loss=32.385590
I0406 05:16:59.305603 140218958939968 submission.py:296] 5) loss = 32.386, grad_norm = 58.833
I0406 05:17:00.115155 140178215749376 logging_writer.py:48] [6] global_step=6, grad_norm=68.799301, loss=32.390793
I0406 05:17:00.118232 140218958939968 submission.py:296] 6) loss = 32.391, grad_norm = 68.799
I0406 05:17:00.921629 140177867536128 logging_writer.py:48] [7] global_step=7, grad_norm=82.798218, loss=31.001532
I0406 05:17:00.924774 140218958939968 submission.py:296] 7) loss = 31.002, grad_norm = 82.798
I0406 05:17:01.733044 140178215749376 logging_writer.py:48] [8] global_step=8, grad_norm=105.469284, loss=31.155073
I0406 05:17:01.736142 140218958939968 submission.py:296] 8) loss = 31.155, grad_norm = 105.469
I0406 05:17:02.541683 140177867536128 logging_writer.py:48] [9] global_step=9, grad_norm=121.675583, loss=30.242907
I0406 05:17:02.544852 140218958939968 submission.py:296] 9) loss = 30.243, grad_norm = 121.676
I0406 05:17:03.349810 140178215749376 logging_writer.py:48] [10] global_step=10, grad_norm=141.062286, loss=29.423773
I0406 05:17:03.352926 140218958939968 submission.py:296] 10) loss = 29.424, grad_norm = 141.062
I0406 05:17:04.158267 140177867536128 logging_writer.py:48] [11] global_step=11, grad_norm=159.003983, loss=28.721268
I0406 05:17:04.161274 140218958939968 submission.py:296] 11) loss = 28.721, grad_norm = 159.004
I0406 05:17:04.971089 140178215749376 logging_writer.py:48] [12] global_step=12, grad_norm=167.368927, loss=27.752258
I0406 05:17:04.974255 140218958939968 submission.py:296] 12) loss = 27.752, grad_norm = 167.369
I0406 05:17:05.780892 140177867536128 logging_writer.py:48] [13] global_step=13, grad_norm=172.237427, loss=26.107079
I0406 05:17:05.784135 140218958939968 submission.py:296] 13) loss = 26.107, grad_norm = 172.237
I0406 05:17:06.588794 140178215749376 logging_writer.py:48] [14] global_step=14, grad_norm=172.481262, loss=24.705755
I0406 05:17:06.592221 140218958939968 submission.py:296] 14) loss = 24.706, grad_norm = 172.481
I0406 05:17:07.391082 140177867536128 logging_writer.py:48] [15] global_step=15, grad_norm=168.272873, loss=22.677402
I0406 05:17:07.394408 140218958939968 submission.py:296] 15) loss = 22.677, grad_norm = 168.273
I0406 05:17:08.198878 140178215749376 logging_writer.py:48] [16] global_step=16, grad_norm=171.593597, loss=21.503843
I0406 05:17:08.201968 140218958939968 submission.py:296] 16) loss = 21.504, grad_norm = 171.594
I0406 05:17:09.007329 140177867536128 logging_writer.py:48] [17] global_step=17, grad_norm=167.189331, loss=19.731123
I0406 05:17:09.010696 140218958939968 submission.py:296] 17) loss = 19.731, grad_norm = 167.189
I0406 05:17:09.815462 140178215749376 logging_writer.py:48] [18] global_step=18, grad_norm=161.236893, loss=17.877163
I0406 05:17:09.818681 140218958939968 submission.py:296] 18) loss = 17.877, grad_norm = 161.237
I0406 05:17:10.620393 140177867536128 logging_writer.py:48] [19] global_step=19, grad_norm=150.677277, loss=15.969208
I0406 05:17:10.623755 140218958939968 submission.py:296] 19) loss = 15.969, grad_norm = 150.677
I0406 05:17:11.430321 140178215749376 logging_writer.py:48] [20] global_step=20, grad_norm=137.701920, loss=14.111098
I0406 05:17:11.433540 140218958939968 submission.py:296] 20) loss = 14.111, grad_norm = 137.702
I0406 05:17:12.243578 140177867536128 logging_writer.py:48] [21] global_step=21, grad_norm=124.074570, loss=12.676552
I0406 05:17:12.246698 140218958939968 submission.py:296] 21) loss = 12.677, grad_norm = 124.075
I0406 05:17:13.048628 140178215749376 logging_writer.py:48] [22] global_step=22, grad_norm=109.165878, loss=11.361421
I0406 05:17:13.051683 140218958939968 submission.py:296] 22) loss = 11.361, grad_norm = 109.166
I0406 05:17:13.853331 140177867536128 logging_writer.py:48] [23] global_step=23, grad_norm=86.128586, loss=9.994738
I0406 05:17:13.856589 140218958939968 submission.py:296] 23) loss = 9.995, grad_norm = 86.129
I0406 05:17:14.661122 140178215749376 logging_writer.py:48] [24] global_step=24, grad_norm=66.544395, loss=9.023310
I0406 05:17:14.664424 140218958939968 submission.py:296] 24) loss = 9.023, grad_norm = 66.544
I0406 05:17:15.468126 140177867536128 logging_writer.py:48] [25] global_step=25, grad_norm=47.102226, loss=8.388744
I0406 05:17:15.471152 140218958939968 submission.py:296] 25) loss = 8.389, grad_norm = 47.102
I0406 05:17:16.275127 140178215749376 logging_writer.py:48] [26] global_step=26, grad_norm=29.669142, loss=7.918408
I0406 05:17:16.278201 140218958939968 submission.py:296] 26) loss = 7.918, grad_norm = 29.669
I0406 05:17:17.080982 140177867536128 logging_writer.py:48] [27] global_step=27, grad_norm=15.097384, loss=7.582418
I0406 05:17:17.084233 140218958939968 submission.py:296] 27) loss = 7.582, grad_norm = 15.097
I0406 05:17:17.886700 140178215749376 logging_writer.py:48] [28] global_step=28, grad_norm=6.349471, loss=7.516738
I0406 05:17:17.890121 140218958939968 submission.py:296] 28) loss = 7.517, grad_norm = 6.349
I0406 05:17:18.699225 140177867536128 logging_writer.py:48] [29] global_step=29, grad_norm=7.102205, loss=7.502484
I0406 05:17:18.702283 140218958939968 submission.py:296] 29) loss = 7.502, grad_norm = 7.102
I0406 05:17:19.511039 140178215749376 logging_writer.py:48] [30] global_step=30, grad_norm=12.072894, loss=7.569115
I0406 05:17:19.514277 140218958939968 submission.py:296] 30) loss = 7.569, grad_norm = 12.073
I0406 05:17:20.319344 140177867536128 logging_writer.py:48] [31] global_step=31, grad_norm=16.092287, loss=7.675955
I0406 05:17:20.322418 140218958939968 submission.py:296] 31) loss = 7.676, grad_norm = 16.092
I0406 05:17:21.126855 140178215749376 logging_writer.py:48] [32] global_step=32, grad_norm=19.046207, loss=7.815289
I0406 05:17:21.130000 140218958939968 submission.py:296] 32) loss = 7.815, grad_norm = 19.046
I0406 05:17:21.935068 140177867536128 logging_writer.py:48] [33] global_step=33, grad_norm=20.869366, loss=7.953162
I0406 05:17:21.938025 140218958939968 submission.py:296] 33) loss = 7.953, grad_norm = 20.869
I0406 05:17:22.743415 140178215749376 logging_writer.py:48] [34] global_step=34, grad_norm=22.258955, loss=8.138629
I0406 05:17:22.746610 140218958939968 submission.py:296] 34) loss = 8.139, grad_norm = 22.259
I0406 05:17:23.550867 140177867536128 logging_writer.py:48] [35] global_step=35, grad_norm=23.266951, loss=8.267145
I0406 05:17:23.553939 140218958939968 submission.py:296] 35) loss = 8.267, grad_norm = 23.267
I0406 05:17:24.356808 140178215749376 logging_writer.py:48] [36] global_step=36, grad_norm=23.766130, loss=8.387822
I0406 05:17:24.360624 140218958939968 submission.py:296] 36) loss = 8.388, grad_norm = 23.766
I0406 05:17:25.163520 140177867536128 logging_writer.py:48] [37] global_step=37, grad_norm=24.384033, loss=8.523227
I0406 05:17:25.167384 140218958939968 submission.py:296] 37) loss = 8.523, grad_norm = 24.384
I0406 05:17:25.972741 140178215749376 logging_writer.py:48] [38] global_step=38, grad_norm=24.562544, loss=8.597202
I0406 05:17:25.976830 140218958939968 submission.py:296] 38) loss = 8.597, grad_norm = 24.563
I0406 05:17:26.788076 140177867536128 logging_writer.py:48] [39] global_step=39, grad_norm=25.084908, loss=8.686243
I0406 05:17:26.791738 140218958939968 submission.py:296] 39) loss = 8.686, grad_norm = 25.085
I0406 05:17:27.598048 140178215749376 logging_writer.py:48] [40] global_step=40, grad_norm=25.082365, loss=8.740097
I0406 05:17:27.601577 140218958939968 submission.py:296] 40) loss = 8.740, grad_norm = 25.082
I0406 05:17:28.408746 140177867536128 logging_writer.py:48] [41] global_step=41, grad_norm=25.375174, loss=8.803260
I0406 05:17:28.412225 140218958939968 submission.py:296] 41) loss = 8.803, grad_norm = 25.375
I0406 05:17:29.215030 140178215749376 logging_writer.py:48] [42] global_step=42, grad_norm=25.456102, loss=8.824760
I0406 05:17:29.218355 140218958939968 submission.py:296] 42) loss = 8.825, grad_norm = 25.456
I0406 05:17:30.022958 140177867536128 logging_writer.py:48] [43] global_step=43, grad_norm=25.568924, loss=8.845854
I0406 05:17:30.026645 140218958939968 submission.py:296] 43) loss = 8.846, grad_norm = 25.569
I0406 05:17:30.830955 140178215749376 logging_writer.py:48] [44] global_step=44, grad_norm=25.680977, loss=8.837415
I0406 05:17:30.834545 140218958939968 submission.py:296] 44) loss = 8.837, grad_norm = 25.681
I0406 05:17:31.640209 140177867536128 logging_writer.py:48] [45] global_step=45, grad_norm=25.680149, loss=8.824853
I0406 05:17:31.643911 140218958939968 submission.py:296] 45) loss = 8.825, grad_norm = 25.680
I0406 05:17:32.450355 140178215749376 logging_writer.py:48] [46] global_step=46, grad_norm=25.701345, loss=8.788821
I0406 05:17:32.453831 140218958939968 submission.py:296] 46) loss = 8.789, grad_norm = 25.701
I0406 05:17:33.262158 140177867536128 logging_writer.py:48] [47] global_step=47, grad_norm=25.522982, loss=8.712964
I0406 05:17:33.265843 140218958939968 submission.py:296] 47) loss = 8.713, grad_norm = 25.523
I0406 05:17:34.078613 140178215749376 logging_writer.py:48] [48] global_step=48, grad_norm=25.422819, loss=8.630604
I0406 05:17:34.081960 140218958939968 submission.py:296] 48) loss = 8.631, grad_norm = 25.423
I0406 05:17:34.886738 140177867536128 logging_writer.py:48] [49] global_step=49, grad_norm=25.183588, loss=8.549346
I0406 05:17:34.890171 140218958939968 submission.py:296] 49) loss = 8.549, grad_norm = 25.184
I0406 05:17:35.696799 140178215749376 logging_writer.py:48] [50] global_step=50, grad_norm=25.110691, loss=8.474366
I0406 05:17:35.700484 140218958939968 submission.py:296] 50) loss = 8.474, grad_norm = 25.111
I0406 05:17:36.507787 140177867536128 logging_writer.py:48] [51] global_step=51, grad_norm=24.762096, loss=8.368464
I0406 05:17:36.511309 140218958939968 submission.py:296] 51) loss = 8.368, grad_norm = 24.762
I0406 05:17:37.317287 140178215749376 logging_writer.py:48] [52] global_step=52, grad_norm=24.350554, loss=8.248657
I0406 05:17:37.320331 140218958939968 submission.py:296] 52) loss = 8.249, grad_norm = 24.351
I0406 05:17:38.124667 140177867536128 logging_writer.py:48] [53] global_step=53, grad_norm=23.648375, loss=8.100275
I0406 05:17:38.127823 140218958939968 submission.py:296] 53) loss = 8.100, grad_norm = 23.648
I0406 05:17:38.932628 140178215749376 logging_writer.py:48] [54] global_step=54, grad_norm=22.654305, loss=7.936394
I0406 05:17:38.935844 140218958939968 submission.py:296] 54) loss = 7.936, grad_norm = 22.654
I0406 05:17:39.741011 140177867536128 logging_writer.py:48] [55] global_step=55, grad_norm=21.434572, loss=7.789365
I0406 05:17:39.744256 140218958939968 submission.py:296] 55) loss = 7.789, grad_norm = 21.435
I0406 05:17:40.552175 140178215749376 logging_writer.py:48] [56] global_step=56, grad_norm=20.173798, loss=7.646173
I0406 05:17:40.555316 140218958939968 submission.py:296] 56) loss = 7.646, grad_norm = 20.174
I0406 05:17:41.369348 140177867536128 logging_writer.py:48] [57] global_step=57, grad_norm=18.065567, loss=7.499066
I0406 05:17:41.372331 140218958939968 submission.py:296] 57) loss = 7.499, grad_norm = 18.066
I0406 05:17:42.180660 140178215749376 logging_writer.py:48] [58] global_step=58, grad_norm=15.831388, loss=7.374574
I0406 05:17:42.184008 140218958939968 submission.py:296] 58) loss = 7.375, grad_norm = 15.831
I0406 05:17:42.987522 140177867536128 logging_writer.py:48] [59] global_step=59, grad_norm=12.582855, loss=7.259346
I0406 05:17:42.990639 140218958939968 submission.py:296] 59) loss = 7.259, grad_norm = 12.583
I0406 05:17:43.790884 140178215749376 logging_writer.py:48] [60] global_step=60, grad_norm=8.876947, loss=7.152863
I0406 05:17:43.794225 140218958939968 submission.py:296] 60) loss = 7.153, grad_norm = 8.877
I0406 05:17:44.599756 140177867536128 logging_writer.py:48] [61] global_step=61, grad_norm=4.900002, loss=7.106593
I0406 05:17:44.602817 140218958939968 submission.py:296] 61) loss = 7.107, grad_norm = 4.900
I0406 05:17:45.410377 140178215749376 logging_writer.py:48] [62] global_step=62, grad_norm=3.188051, loss=7.091724
I0406 05:17:45.413301 140218958939968 submission.py:296] 62) loss = 7.092, grad_norm = 3.188
I0406 05:17:46.216351 140177867536128 logging_writer.py:48] [63] global_step=63, grad_norm=8.120145, loss=7.104408
I0406 05:17:46.219420 140218958939968 submission.py:296] 63) loss = 7.104, grad_norm = 8.120
I0406 05:17:47.028187 140178215749376 logging_writer.py:48] [64] global_step=64, grad_norm=13.201927, loss=7.143087
I0406 05:17:47.031155 140218958939968 submission.py:296] 64) loss = 7.143, grad_norm = 13.202
I0406 05:17:47.838118 140177867536128 logging_writer.py:48] [65] global_step=65, grad_norm=18.528048, loss=7.232041
I0406 05:17:47.841346 140218958939968 submission.py:296] 65) loss = 7.232, grad_norm = 18.528
I0406 05:17:48.651525 140178215749376 logging_writer.py:48] [66] global_step=66, grad_norm=22.688856, loss=7.295519
I0406 05:17:48.655310 140218958939968 submission.py:296] 66) loss = 7.296, grad_norm = 22.689
I0406 05:17:49.461026 140177867536128 logging_writer.py:48] [67] global_step=67, grad_norm=25.383968, loss=7.348764
I0406 05:17:49.463995 140218958939968 submission.py:296] 67) loss = 7.349, grad_norm = 25.384
I0406 05:17:50.270190 140178215749376 logging_writer.py:48] [68] global_step=68, grad_norm=27.224142, loss=7.379580
I0406 05:17:50.273197 140218958939968 submission.py:296] 68) loss = 7.380, grad_norm = 27.224
I0406 05:17:51.080132 140177867536128 logging_writer.py:48] [69] global_step=69, grad_norm=29.937759, loss=7.428256
I0406 05:17:51.083132 140218958939968 submission.py:296] 69) loss = 7.428, grad_norm = 29.938
I0406 05:17:51.889684 140178215749376 logging_writer.py:48] [70] global_step=70, grad_norm=28.189390, loss=7.383595
I0406 05:17:51.892775 140218958939968 submission.py:296] 70) loss = 7.384, grad_norm = 28.189
I0406 05:17:52.697225 140177867536128 logging_writer.py:48] [71] global_step=71, grad_norm=27.111374, loss=7.352407
I0406 05:17:52.700287 140218958939968 submission.py:296] 71) loss = 7.352, grad_norm = 27.111
I0406 05:17:53.501241 140178215749376 logging_writer.py:48] [72] global_step=72, grad_norm=23.092939, loss=7.230816
I0406 05:17:53.505443 140218958939968 submission.py:296] 72) loss = 7.231, grad_norm = 23.093
I0406 05:17:54.308899 140177867536128 logging_writer.py:48] [73] global_step=73, grad_norm=20.450600, loss=7.161907
I0406 05:17:54.312166 140218958939968 submission.py:296] 73) loss = 7.162, grad_norm = 20.451
I0406 05:17:55.123080 140178215749376 logging_writer.py:48] [74] global_step=74, grad_norm=16.535982, loss=7.092986
I0406 05:17:55.126163 140218958939968 submission.py:296] 74) loss = 7.093, grad_norm = 16.536
I0406 05:17:55.938719 140177867536128 logging_writer.py:48] [75] global_step=75, grad_norm=13.898043, loss=7.032612
I0406 05:17:55.941796 140218958939968 submission.py:296] 75) loss = 7.033, grad_norm = 13.898
I0406 05:17:56.745586 140178215749376 logging_writer.py:48] [76] global_step=76, grad_norm=9.802056, loss=6.948216
I0406 05:17:56.748685 140218958939968 submission.py:296] 76) loss = 6.948, grad_norm = 9.802
I0406 05:17:57.557524 140177867536128 logging_writer.py:48] [77] global_step=77, grad_norm=6.503980, loss=6.918337
I0406 05:17:57.560724 140218958939968 submission.py:296] 77) loss = 6.918, grad_norm = 6.504
I0406 05:17:58.364894 140178215749376 logging_writer.py:48] [78] global_step=78, grad_norm=3.871738, loss=6.893815
I0406 05:17:58.368224 140218958939968 submission.py:296] 78) loss = 6.894, grad_norm = 3.872
I0406 05:17:59.172957 140177867536128 logging_writer.py:48] [79] global_step=79, grad_norm=2.413414, loss=6.862654
I0406 05:17:59.176255 140218958939968 submission.py:296] 79) loss = 6.863, grad_norm = 2.413
I0406 05:17:59.981225 140178215749376 logging_writer.py:48] [80] global_step=80, grad_norm=2.880194, loss=6.845131
I0406 05:17:59.984260 140218958939968 submission.py:296] 80) loss = 6.845, grad_norm = 2.880
I0406 05:18:00.786613 140177867536128 logging_writer.py:48] [81] global_step=81, grad_norm=4.406240, loss=6.841124
I0406 05:18:00.790018 140218958939968 submission.py:296] 81) loss = 6.841, grad_norm = 4.406
I0406 05:18:01.596362 140178215749376 logging_writer.py:48] [82] global_step=82, grad_norm=5.882571, loss=6.849271
I0406 05:18:01.599617 140218958939968 submission.py:296] 82) loss = 6.849, grad_norm = 5.883
I0406 05:18:02.409512 140177867536128 logging_writer.py:48] [83] global_step=83, grad_norm=6.784109, loss=6.838150
I0406 05:18:02.412676 140218958939968 submission.py:296] 83) loss = 6.838, grad_norm = 6.784
I0406 05:18:03.222794 140178215749376 logging_writer.py:48] [84] global_step=84, grad_norm=8.057883, loss=6.850688
I0406 05:18:03.225737 140218958939968 submission.py:296] 84) loss = 6.851, grad_norm = 8.058
I0406 05:18:04.034981 140177867536128 logging_writer.py:48] [85] global_step=85, grad_norm=8.478728, loss=6.839626
I0406 05:18:04.038110 140218958939968 submission.py:296] 85) loss = 6.840, grad_norm = 8.479
I0406 05:18:04.843413 140178215749376 logging_writer.py:48] [86] global_step=86, grad_norm=9.337236, loss=6.836117
I0406 05:18:04.846690 140218958939968 submission.py:296] 86) loss = 6.836, grad_norm = 9.337
I0406 05:18:05.649067 140177867536128 logging_writer.py:48] [87] global_step=87, grad_norm=9.834986, loss=6.835000
I0406 05:18:05.652235 140218958939968 submission.py:296] 87) loss = 6.835, grad_norm = 9.835
I0406 05:18:06.456046 140178215749376 logging_writer.py:48] [88] global_step=88, grad_norm=9.498096, loss=6.813743
I0406 05:18:06.459139 140218958939968 submission.py:296] 88) loss = 6.814, grad_norm = 9.498
I0406 05:18:07.266222 140177867536128 logging_writer.py:48] [89] global_step=89, grad_norm=9.298877, loss=6.790791
I0406 05:18:07.269345 140218958939968 submission.py:296] 89) loss = 6.791, grad_norm = 9.299
I0406 05:18:08.073799 140178215749376 logging_writer.py:48] [90] global_step=90, grad_norm=9.376335, loss=6.775534
I0406 05:18:08.076771 140218958939968 submission.py:296] 90) loss = 6.776, grad_norm = 9.376
I0406 05:18:08.883553 140177867536128 logging_writer.py:48] [91] global_step=91, grad_norm=8.477456, loss=6.731980
I0406 05:18:08.886617 140218958939968 submission.py:296] 91) loss = 6.732, grad_norm = 8.477
I0406 05:18:09.693528 140178215749376 logging_writer.py:48] [92] global_step=92, grad_norm=8.044546, loss=6.720439
I0406 05:18:09.697052 140218958939968 submission.py:296] 92) loss = 6.720, grad_norm = 8.045
I0406 05:18:10.507180 140177867536128 logging_writer.py:48] [93] global_step=93, grad_norm=7.261748, loss=6.695359
I0406 05:18:10.510268 140218958939968 submission.py:296] 93) loss = 6.695, grad_norm = 7.262
I0406 05:18:11.315755 140178215749376 logging_writer.py:48] [94] global_step=94, grad_norm=6.580830, loss=6.671923
I0406 05:18:11.318996 140218958939968 submission.py:296] 94) loss = 6.672, grad_norm = 6.581
I0406 05:18:12.125902 140177867536128 logging_writer.py:48] [95] global_step=95, grad_norm=5.548341, loss=6.642612
I0406 05:18:12.129083 140218958939968 submission.py:296] 95) loss = 6.643, grad_norm = 5.548
I0406 05:18:12.933506 140178215749376 logging_writer.py:48] [96] global_step=96, grad_norm=4.035851, loss=6.622002
I0406 05:18:12.936608 140218958939968 submission.py:296] 96) loss = 6.622, grad_norm = 4.036
I0406 05:18:13.739942 140177867536128 logging_writer.py:48] [97] global_step=97, grad_norm=3.200546, loss=6.595566
I0406 05:18:13.743268 140218958939968 submission.py:296] 97) loss = 6.596, grad_norm = 3.201
I0406 05:18:14.549667 140178215749376 logging_writer.py:48] [98] global_step=98, grad_norm=2.118162, loss=6.568822
I0406 05:18:14.552735 140218958939968 submission.py:296] 98) loss = 6.569, grad_norm = 2.118
I0406 05:18:15.358727 140177867536128 logging_writer.py:48] [99] global_step=99, grad_norm=2.049232, loss=6.558778
I0406 05:18:15.361899 140218958939968 submission.py:296] 99) loss = 6.559, grad_norm = 2.049
I0406 05:18:16.168719 140178215749376 logging_writer.py:48] [100] global_step=100, grad_norm=2.693194, loss=6.544314
I0406 05:18:16.171873 140218958939968 submission.py:296] 100) loss = 6.544, grad_norm = 2.693
I0406 05:23:35.325472 140177867536128 logging_writer.py:48] [500] global_step=500, grad_norm=0.615698, loss=5.796521
I0406 05:23:35.330654 140218958939968 submission.py:296] 500) loss = 5.797, grad_norm = 0.616
I0406 05:30:14.289039 140178215749376 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.918322, loss=5.582651
I0406 05:30:14.293873 140218958939968 submission.py:296] 1000) loss = 5.583, grad_norm = 3.918
I0406 05:36:55.361048 140184444204800 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.082413, loss=3.711167
I0406 05:36:55.372113 140218958939968 submission.py:296] 1500) loss = 3.711, grad_norm = 2.082
I0406 05:43:33.918185 140184435812096 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.067403, loss=2.967446
I0406 05:43:33.921948 140218958939968 submission.py:296] 2000) loss = 2.967, grad_norm = 1.067
I0406 05:50:13.710107 140184444204800 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.230007, loss=2.667536
I0406 05:50:13.717645 140218958939968 submission.py:296] 2500) loss = 2.668, grad_norm = 1.230
I0406 05:56:51.976113 140184435812096 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.993060, loss=2.520626
I0406 05:56:51.980702 140218958939968 submission.py:296] 3000) loss = 2.521, grad_norm = 0.993
I0406 05:56:55.163972 140218958939968 submission_runner.py:373] Before eval at step 3005: RAM USED (GB) 40.26933248
I0406 05:56:55.164217 140218958939968 spec.py:298] Evaluating on the training split.
I0406 05:57:05.543062 140218958939968 spec.py:310] Evaluating on the validation split.
I0406 05:57:15.075511 140218958939968 spec.py:326] Evaluating on the test split.
I0406 05:57:20.405672 140218958939968 submission_runner.py:382] Time since start: 2436.56s, 	Step: 3005, 	{'train/ctc_loss': 3.536553208442165, 'train/wer': 0.7479941985213835, 'validation/ctc_loss': 3.7333213249698614, 'validation/wer': 0.7560276155071695, 'validation/num_examples': 5348, 'test/ctc_loss': 3.4253044551261373, 'test/wer': 0.7013588446773505, 'test/num_examples': 2472}
I0406 05:57:20.406421 140218958939968 submission_runner.py:396] After eval at step 3005: RAM USED (GB) 39.433289728
I0406 05:57:20.421697 140184435812096 logging_writer.py:48] [3005] global_step=3005, preemption_count=0, score=2403.263419, test/ctc_loss=3.425304, test/num_examples=2472, test/wer=0.701359, total_duration=2436.556574, train/ctc_loss=3.536553, train/wer=0.747994, validation/ctc_loss=3.733321, validation/num_examples=5348, validation/wer=0.756028
I0406 05:57:21.004346 140218958939968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_3005.
I0406 05:57:21.004979 140218958939968 submission_runner.py:416] After logging and checkpointing eval at step 3005: RAM USED (GB) 39.439089664
I0406 06:03:57.698891 140184435812096 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.283506, loss=2.243212
I0406 06:03:57.705566 140218958939968 submission.py:296] 3500) loss = 2.243, grad_norm = 1.284
I0406 06:10:35.781303 140184427419392 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.014859, loss=2.130100
I0406 06:10:35.785875 140218958939968 submission.py:296] 4000) loss = 2.130, grad_norm = 1.015
I0406 06:17:15.317514 140184435812096 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.932124, loss=1.977271
I0406 06:17:15.324450 140218958939968 submission.py:296] 4500) loss = 1.977, grad_norm = 0.932
I0406 06:23:52.799812 140184427419392 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.686979, loss=1.926457
I0406 06:23:52.804262 140218958939968 submission.py:296] 5000) loss = 1.926, grad_norm = 0.687
I0406 06:30:32.366952 140184435812096 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.738331, loss=1.967044
I0406 06:30:32.378570 140218958939968 submission.py:296] 5500) loss = 1.967, grad_norm = 0.738
I0406 06:37:09.598677 140184427419392 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.619912, loss=1.837525
I0406 06:37:09.603743 140218958939968 submission.py:296] 6000) loss = 1.838, grad_norm = 0.620
I0406 06:37:21.526988 140218958939968 submission_runner.py:373] Before eval at step 6016: RAM USED (GB) 39.513796608
I0406 06:37:21.527195 140218958939968 spec.py:298] Evaluating on the training split.
I0406 06:37:32.596555 140218958939968 spec.py:310] Evaluating on the validation split.
I0406 06:37:42.385719 140218958939968 spec.py:326] Evaluating on the test split.
I0406 06:37:47.796618 140218958939968 submission_runner.py:382] Time since start: 4862.92s, 	Step: 6016, 	{'train/ctc_loss': 0.667957595478475, 'train/wer': 0.22731149268568263, 'validation/ctc_loss': 0.8879304762846595, 'validation/wer': 0.26683725196736346, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6058818814923543, 'test/wer': 0.2026283184043223, 'test/num_examples': 2472}
I0406 06:37:47.797325 140218958939968 submission_runner.py:396] After eval at step 6016: RAM USED (GB) 39.376396288
I0406 06:37:47.816687 140184427419392 logging_writer.py:48] [6016] global_step=6016, preemption_count=0, score=4799.785001, test/ctc_loss=0.605882, test/num_examples=2472, test/wer=0.202628, total_duration=4862.917472, train/ctc_loss=0.667958, train/wer=0.227311, validation/ctc_loss=0.887930, validation/num_examples=5348, validation/wer=0.266837
I0406 06:37:48.393818 140218958939968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_6016.
I0406 06:37:48.394464 140218958939968 submission_runner.py:416] After logging and checkpointing eval at step 6016: RAM USED (GB) 39.382687744
I0406 06:44:15.633043 140184427419392 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.569335, loss=1.744966
I0406 06:44:15.639384 140218958939968 submission.py:296] 6500) loss = 1.745, grad_norm = 0.569
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0406 06:50:53.250208 140184419026688 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.527497, loss=1.757311
I0406 06:50:53.254428 140218958939968 submission.py:296] 7000) loss = 1.757, grad_norm = 0.527
I0406 06:57:32.498076 140184427419392 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.622458, loss=1.755237
I0406 06:57:32.504380 140218958939968 submission.py:296] 7500) loss = 1.755, grad_norm = 0.622
I0406 07:04:09.495493 140184419026688 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.522200, loss=1.622679
I0406 07:04:09.501191 140218958939968 submission.py:296] 8000) loss = 1.623, grad_norm = 0.522
I0406 07:10:48.149183 140184427419392 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.960338, loss=1.735135
I0406 07:10:48.155786 140218958939968 submission.py:296] 8500) loss = 1.735, grad_norm = 0.960
I0406 07:17:25.500962 140184419026688 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.478395, loss=1.713065
I0406 07:17:25.506857 140218958939968 submission.py:296] 9000) loss = 1.713, grad_norm = 0.478
I0406 07:17:48.564570 140218958939968 submission_runner.py:373] Before eval at step 9030: RAM USED (GB) 39.479128064
I0406 07:17:48.564786 140218958939968 spec.py:298] Evaluating on the training split.
I0406 07:17:59.747760 140218958939968 spec.py:310] Evaluating on the validation split.
I0406 07:18:09.620562 140218958939968 spec.py:326] Evaluating on the test split.
I0406 07:18:15.039062 140218958939968 submission_runner.py:382] Time since start: 7289.95s, 	Step: 9030, 	{'train/ctc_loss': 0.5097408577330121, 'train/wer': 0.17924287444796325, 'validation/ctc_loss': 0.749168587612392, 'validation/wer': 0.22653406073480423, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4791035409542551, 'test/wer': 0.1636910202506449, 'test/num_examples': 2472}
I0406 07:18:15.039807 140218958939968 submission_runner.py:396] After eval at step 9030: RAM USED (GB) 39.254347776
I0406 07:18:15.056274 140184419026688 logging_writer.py:48] [9030] global_step=9030, preemption_count=0, score=7195.985963, test/ctc_loss=0.479104, test/num_examples=2472, test/wer=0.163691, total_duration=7289.954724, train/ctc_loss=0.509741, train/wer=0.179243, validation/ctc_loss=0.749169, validation/num_examples=5348, validation/wer=0.226534
I0406 07:18:15.630631 140218958939968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_9030.
I0406 07:18:15.631213 140218958939968 submission_runner.py:416] After logging and checkpointing eval at step 9030: RAM USED (GB) 39.260434432
I0406 07:24:31.502707 140184419026688 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.435497, loss=1.622329
I0406 07:24:31.508654 140218958939968 submission.py:296] 9500) loss = 1.622, grad_norm = 0.435
I0406 07:31:07.998805 140218958939968 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 39.415361536
I0406 07:31:07.999028 140218958939968 spec.py:298] Evaluating on the training split.
I0406 07:31:18.986840 140218958939968 spec.py:310] Evaluating on the validation split.
I0406 07:31:28.839056 140218958939968 spec.py:326] Evaluating on the test split.
I0406 07:31:34.207530 140218958939968 submission_runner.py:382] Time since start: 8089.39s, 	Step: 10000, 	{'train/ctc_loss': 0.4668591235739129, 'train/wer': 0.1665154733256922, 'validation/ctc_loss': 0.7070033384035312, 'validation/wer': 0.21407811519335682, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45016619015742954, 'test/wer': 0.15367741149229175, 'test/num_examples': 2472}
I0406 07:31:34.208255 140218958939968 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 39.286751232
I0406 07:31:34.222854 140184419026688 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7967.078743, test/ctc_loss=0.450166, test/num_examples=2472, test/wer=0.153677, total_duration=8089.391350, train/ctc_loss=0.466859, train/wer=0.166515, validation/ctc_loss=0.707003, validation/num_examples=5348, validation/wer=0.214078
I0406 07:31:34.786233 140218958939968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0406 07:31:34.786870 140218958939968 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 39.292678144
I0406 07:31:34.795858 140184410633984 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7967.078743
I0406 07:31:35.959138 140218958939968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0406 07:31:36.092395 140218958939968 submission_runner.py:550] Tuning trial 1/1
I0406 07:31:36.092653 140218958939968 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 07:31:36.093075 140218958939968 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.780920545127284, 'train/wer': 1.7285201340641314, 'validation/ctc_loss': 31.147968153506127, 'validation/wer': 1.430628107951528, 'validation/num_examples': 5348, 'test/ctc_loss': 31.23045357765017, 'test/wer': 1.4720817337964374, 'test/num_examples': 2472, 'score': 6.675212860107422, 'total_duration': 6.677338361740112, 'global_step': 1, 'preemption_count': 0}), (3005, {'train/ctc_loss': 3.536553208442165, 'train/wer': 0.7479941985213835, 'validation/ctc_loss': 3.7333213249698614, 'validation/wer': 0.7560276155071695, 'validation/num_examples': 5348, 'test/ctc_loss': 3.4253044551261373, 'test/wer': 0.7013588446773505, 'test/num_examples': 2472, 'score': 2403.2634189128876, 'total_duration': 2436.5565741062164, 'global_step': 3005, 'preemption_count': 0}), (6016, {'train/ctc_loss': 0.667957595478475, 'train/wer': 0.22731149268568263, 'validation/ctc_loss': 0.8879304762846595, 'validation/wer': 0.26683725196736346, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6058818814923543, 'test/wer': 0.2026283184043223, 'test/num_examples': 2472, 'score': 4799.785001039505, 'total_duration': 4862.917471647263, 'global_step': 6016, 'preemption_count': 0}), (9030, {'train/ctc_loss': 0.5097408577330121, 'train/wer': 0.17924287444796325, 'validation/ctc_loss': 0.749168587612392, 'validation/wer': 0.22653406073480423, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4791035409542551, 'test/wer': 0.1636910202506449, 'test/num_examples': 2472, 'score': 7195.985962867737, 'total_duration': 7289.954724311829, 'global_step': 9030, 'preemption_count': 0}), (10000, {'train/ctc_loss': 0.4668591235739129, 'train/wer': 0.1665154733256922, 'validation/ctc_loss': 0.7070033384035312, 'validation/wer': 0.21407811519335682, 'validation/num_examples': 5348, 'test/ctc_loss': 0.45016619015742954, 'test/wer': 0.15367741149229175, 'test/num_examples': 2472, 'score': 7967.07874250412, 'total_duration': 8089.391349554062, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 07:31:36.093159 140218958939968 submission_runner.py:553] Timing: 7967.07874250412
I0406 07:31:36.093213 140218958939968 submission_runner.py:554] ====================
I0406 07:31:36.093360 140218958939968 submission_runner.py:613] Final librispeech_conformer score: 7967.07874250412
