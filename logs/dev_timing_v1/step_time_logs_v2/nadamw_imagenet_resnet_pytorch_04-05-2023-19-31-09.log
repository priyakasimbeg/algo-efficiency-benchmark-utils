WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 19:31:31.306951 140438442006336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 19:31:31.306976 140196471293760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 19:31:31.307000 140614461241152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 19:31:31.307026 139818083182400 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 19:31:32.293312 140218720696128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 19:31:32.293383 140388172547904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 19:31:32.293461 139721688164160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 19:31:32.296969 140028517898048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 19:31:32.297414 140028517898048 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.303929 140218720696128 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.303967 140388172547904 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.304074 139721688164160 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.306058 140438442006336 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.306096 140196471293760 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.306110 139818083182400 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:32.306137 140614461241152 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:31:34.444391 140028517898048 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch.
W0405 19:31:34.464739 140388172547904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.466503 139721688164160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.467436 140218720696128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.468751 140196471293760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.469171 140438442006336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.469834 140614461241152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.469877 139818083182400 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:31:34.476004 140028517898048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 19:31:34.479431 140028517898048 submission_runner.py:511] Using RNG seed 2278887543
I0405 19:31:34.480491 140028517898048 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 19:31:34.480602 140028517898048 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1.
I0405 19:31:34.480786 140028517898048 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0405 19:31:34.481681 140028517898048 submission_runner.py:230] Starting train once: RAM USED (GB) 5.703897088
I0405 19:31:34.481771 140028517898048 submission_runner.py:231] Initializing dataset.
I0405 19:31:38.752941 140028517898048 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.704662016
I0405 19:31:38.753113 140028517898048 submission_runner.py:240] Initializing model.
I0405 19:31:43.348124 140028517898048 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.550757888
I0405 19:31:43.348301 140028517898048 submission_runner.py:252] Initializing optimizer.
I0405 19:31:43.349327 140028517898048 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.550757888
I0405 19:31:43.349426 140028517898048 submission_runner.py:261] Initializing metrics bundle.
I0405 19:31:43.349479 140028517898048 submission_runner.py:276] Initializing checkpoint and logger.
I0405 19:31:44.019417 140028517898048 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0405 19:31:44.020990 140028517898048 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0405 19:31:44.058381 140028517898048 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 17.600815104
I0405 19:31:44.059420 140028517898048 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 17.600811008
I0405 19:31:44.059537 140028517898048 submission_runner.py:313] Starting training loop.
I0405 19:31:46.516385 140028517898048 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 22.659878912
I0405 19:31:52.110228 139999584200448 logging_writer.py:48] [0] global_step=0, grad_norm=0.602840, loss=6.930937
I0405 19:31:52.119913 140028517898048 submission.py:296] 0) loss = 6.931, grad_norm = 0.603
I0405 19:31:52.120422 140028517898048 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.692066816
I0405 19:31:52.153446 140028517898048 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.694163968
I0405 19:31:52.153568 140028517898048 spec.py:298] Evaluating on the training split.
I0405 19:32:52.610530 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 19:33:49.501575 140028517898048 spec.py:326] Evaluating on the test split.
I0405 19:33:49.517831 140028517898048 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:33:49.524130 140028517898048 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 19:33:49.585551 140028517898048 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:02.052150 140028517898048 submission_runner.py:382] Time since start: 8.09s, 	Step: 1, 	{'train/accuracy': 0.0012555803571428572, 'train/loss': 6.9212440957828445, 'validation/accuracy': 0.001, 'validation/loss': 6.9218675, 'validation/num_examples': 50000, 'test/accuracy': 0.0009, 'test/loss': 6.9226734375, 'test/num_examples': 10000}
I0405 19:34:02.052530 140028517898048 submission_runner.py:396] After eval at step 1: RAM USED (GB) 91.578687488
I0405 19:34:02.061465 139976222152448 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=8.092612, test/accuracy=0.000900, test/loss=6.922673, test/num_examples=10000, total_duration=8.094494, train/accuracy=0.001256, train/loss=6.921244, validation/accuracy=0.001000, validation/loss=6.921868, validation/num_examples=50000
I0405 19:34:02.528667 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0405 19:34:02.529403 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 91.579916288
I0405 19:34:02.535052 140028517898048 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 91.579387904
I0405 19:34:02.543127 140028517898048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543029 140388172547904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543403 140196471293760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543418 140218720696128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543464 139721688164160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543455 139818083182400 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543444 140438442006336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.543476 140614461241152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:02.926864 139976138290944 logging_writer.py:48] [1] global_step=1, grad_norm=0.621817, loss=6.940171
I0405 19:34:02.930347 140028517898048 submission.py:296] 1) loss = 6.940, grad_norm = 0.622
I0405 19:34:02.930973 140028517898048 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 91.583401984
I0405 19:34:03.316089 139976222152448 logging_writer.py:48] [2] global_step=2, grad_norm=0.611987, loss=6.938193
I0405 19:34:03.319782 140028517898048 submission.py:296] 2) loss = 6.938, grad_norm = 0.612
I0405 19:34:03.707838 139976138290944 logging_writer.py:48] [3] global_step=3, grad_norm=0.610896, loss=6.930102
I0405 19:34:03.711587 140028517898048 submission.py:296] 3) loss = 6.930, grad_norm = 0.611
I0405 19:34:04.100329 139976222152448 logging_writer.py:48] [4] global_step=4, grad_norm=0.614141, loss=6.943873
I0405 19:34:04.104150 140028517898048 submission.py:296] 4) loss = 6.944, grad_norm = 0.614
I0405 19:34:04.492060 139976138290944 logging_writer.py:48] [5] global_step=5, grad_norm=0.598739, loss=6.925646
I0405 19:34:04.495722 140028517898048 submission.py:296] 5) loss = 6.926, grad_norm = 0.599
I0405 19:34:04.888711 139976222152448 logging_writer.py:48] [6] global_step=6, grad_norm=0.611410, loss=6.924208
I0405 19:34:04.892657 140028517898048 submission.py:296] 6) loss = 6.924, grad_norm = 0.611
I0405 19:34:05.281418 139976138290944 logging_writer.py:48] [7] global_step=7, grad_norm=0.625549, loss=6.921750
I0405 19:34:05.285283 140028517898048 submission.py:296] 7) loss = 6.922, grad_norm = 0.626
I0405 19:34:05.676869 139976222152448 logging_writer.py:48] [8] global_step=8, grad_norm=0.613021, loss=6.929191
I0405 19:34:05.686365 140028517898048 submission.py:296] 8) loss = 6.929, grad_norm = 0.613
I0405 19:34:06.071905 139976138290944 logging_writer.py:48] [9] global_step=9, grad_norm=0.631500, loss=6.939041
I0405 19:34:06.075428 140028517898048 submission.py:296] 9) loss = 6.939, grad_norm = 0.631
I0405 19:34:06.464110 139976222152448 logging_writer.py:48] [10] global_step=10, grad_norm=0.616772, loss=6.934858
I0405 19:34:06.467770 140028517898048 submission.py:296] 10) loss = 6.935, grad_norm = 0.617
I0405 19:34:06.855345 139976138290944 logging_writer.py:48] [11] global_step=11, grad_norm=0.611534, loss=6.934106
I0405 19:34:06.859056 140028517898048 submission.py:296] 11) loss = 6.934, grad_norm = 0.612
I0405 19:34:07.254725 139976222152448 logging_writer.py:48] [12] global_step=12, grad_norm=0.604856, loss=6.930789
I0405 19:34:07.259367 140028517898048 submission.py:296] 12) loss = 6.931, grad_norm = 0.605
I0405 19:34:07.651751 139976138290944 logging_writer.py:48] [13] global_step=13, grad_norm=0.611638, loss=6.937795
I0405 19:34:07.655861 140028517898048 submission.py:296] 13) loss = 6.938, grad_norm = 0.612
I0405 19:34:08.055328 139976222152448 logging_writer.py:48] [14] global_step=14, grad_norm=0.624334, loss=6.941979
I0405 19:34:08.059572 140028517898048 submission.py:296] 14) loss = 6.942, grad_norm = 0.624
I0405 19:34:08.447522 139976138290944 logging_writer.py:48] [15] global_step=15, grad_norm=0.605604, loss=6.926313
I0405 19:34:08.451787 140028517898048 submission.py:296] 15) loss = 6.926, grad_norm = 0.606
I0405 19:34:08.841645 139976222152448 logging_writer.py:48] [16] global_step=16, grad_norm=0.593819, loss=6.928136
I0405 19:34:08.845716 140028517898048 submission.py:296] 16) loss = 6.928, grad_norm = 0.594
I0405 19:34:09.236848 139976138290944 logging_writer.py:48] [17] global_step=17, grad_norm=0.612268, loss=6.930839
I0405 19:34:09.241994 140028517898048 submission.py:296] 17) loss = 6.931, grad_norm = 0.612
I0405 19:34:09.698862 139976222152448 logging_writer.py:48] [18] global_step=18, grad_norm=0.616388, loss=6.926567
I0405 19:34:09.704779 140028517898048 submission.py:296] 18) loss = 6.927, grad_norm = 0.616
I0405 19:34:10.096617 139976138290944 logging_writer.py:48] [19] global_step=19, grad_norm=0.607094, loss=6.925788
I0405 19:34:10.101577 140028517898048 submission.py:296] 19) loss = 6.926, grad_norm = 0.607
I0405 19:34:10.487639 139976222152448 logging_writer.py:48] [20] global_step=20, grad_norm=0.620431, loss=6.921453
I0405 19:34:10.492727 140028517898048 submission.py:296] 20) loss = 6.921, grad_norm = 0.620
I0405 19:34:10.880998 139976138290944 logging_writer.py:48] [21] global_step=21, grad_norm=0.606702, loss=6.927802
I0405 19:34:10.885092 140028517898048 submission.py:296] 21) loss = 6.928, grad_norm = 0.607
I0405 19:34:11.273391 139976222152448 logging_writer.py:48] [22] global_step=22, grad_norm=0.607867, loss=6.923876
I0405 19:34:11.276968 140028517898048 submission.py:296] 22) loss = 6.924, grad_norm = 0.608
I0405 19:34:11.664768 139976138290944 logging_writer.py:48] [23] global_step=23, grad_norm=0.603651, loss=6.931399
I0405 19:34:11.668781 140028517898048 submission.py:296] 23) loss = 6.931, grad_norm = 0.604
I0405 19:34:12.059119 139976222152448 logging_writer.py:48] [24] global_step=24, grad_norm=0.609057, loss=6.926562
I0405 19:34:12.063881 140028517898048 submission.py:296] 24) loss = 6.927, grad_norm = 0.609
I0405 19:34:12.457324 139976138290944 logging_writer.py:48] [25] global_step=25, grad_norm=0.610361, loss=6.924633
I0405 19:34:12.462126 140028517898048 submission.py:296] 25) loss = 6.925, grad_norm = 0.610
I0405 19:34:12.850625 139976222152448 logging_writer.py:48] [26] global_step=26, grad_norm=0.619302, loss=6.930843
I0405 19:34:12.854869 140028517898048 submission.py:296] 26) loss = 6.931, grad_norm = 0.619
I0405 19:34:13.249099 139976138290944 logging_writer.py:48] [27] global_step=27, grad_norm=0.592476, loss=6.913416
I0405 19:34:13.253517 140028517898048 submission.py:296] 27) loss = 6.913, grad_norm = 0.592
I0405 19:34:13.642128 139976222152448 logging_writer.py:48] [28] global_step=28, grad_norm=0.612346, loss=6.919287
I0405 19:34:13.646749 140028517898048 submission.py:296] 28) loss = 6.919, grad_norm = 0.612
I0405 19:34:14.036543 139976138290944 logging_writer.py:48] [29] global_step=29, grad_norm=0.613242, loss=6.925963
I0405 19:34:14.041443 140028517898048 submission.py:296] 29) loss = 6.926, grad_norm = 0.613
I0405 19:34:14.428521 139976222152448 logging_writer.py:48] [30] global_step=30, grad_norm=0.605972, loss=6.915164
I0405 19:34:14.432737 140028517898048 submission.py:296] 30) loss = 6.915, grad_norm = 0.606
I0405 19:34:14.821031 139976138290944 logging_writer.py:48] [31] global_step=31, grad_norm=0.599864, loss=6.932733
I0405 19:34:14.827648 140028517898048 submission.py:296] 31) loss = 6.933, grad_norm = 0.600
I0405 19:34:15.228219 139976222152448 logging_writer.py:48] [32] global_step=32, grad_norm=0.587279, loss=6.923684
I0405 19:34:15.231940 140028517898048 submission.py:296] 32) loss = 6.924, grad_norm = 0.587
I0405 19:34:15.628593 139976138290944 logging_writer.py:48] [33] global_step=33, grad_norm=0.627419, loss=6.915862
I0405 19:34:15.633559 140028517898048 submission.py:296] 33) loss = 6.916, grad_norm = 0.627
I0405 19:34:16.021923 139976222152448 logging_writer.py:48] [34] global_step=34, grad_norm=0.601102, loss=6.922505
I0405 19:34:16.025433 140028517898048 submission.py:296] 34) loss = 6.923, grad_norm = 0.601
I0405 19:34:16.414278 139976138290944 logging_writer.py:48] [35] global_step=35, grad_norm=0.601512, loss=6.929046
I0405 19:34:16.417931 140028517898048 submission.py:296] 35) loss = 6.929, grad_norm = 0.602
I0405 19:34:16.811860 139976222152448 logging_writer.py:48] [36] global_step=36, grad_norm=0.613465, loss=6.921359
I0405 19:34:16.816859 140028517898048 submission.py:296] 36) loss = 6.921, grad_norm = 0.613
I0405 19:34:17.212126 139976138290944 logging_writer.py:48] [37] global_step=37, grad_norm=0.612517, loss=6.922795
I0405 19:34:17.216423 140028517898048 submission.py:296] 37) loss = 6.923, grad_norm = 0.613
I0405 19:34:17.606406 139976222152448 logging_writer.py:48] [38] global_step=38, grad_norm=0.595279, loss=6.918971
I0405 19:34:17.610199 140028517898048 submission.py:296] 38) loss = 6.919, grad_norm = 0.595
I0405 19:34:18.000163 139976138290944 logging_writer.py:48] [39] global_step=39, grad_norm=0.605517, loss=6.926264
I0405 19:34:18.004393 140028517898048 submission.py:296] 39) loss = 6.926, grad_norm = 0.606
I0405 19:34:18.401200 139976222152448 logging_writer.py:48] [40] global_step=40, grad_norm=0.608594, loss=6.915188
I0405 19:34:18.404890 140028517898048 submission.py:296] 40) loss = 6.915, grad_norm = 0.609
I0405 19:34:18.816787 139976138290944 logging_writer.py:48] [41] global_step=41, grad_norm=0.589994, loss=6.917820
I0405 19:34:18.820926 140028517898048 submission.py:296] 41) loss = 6.918, grad_norm = 0.590
I0405 19:34:19.220002 139976222152448 logging_writer.py:48] [42] global_step=42, grad_norm=0.597640, loss=6.914850
I0405 19:34:19.224780 140028517898048 submission.py:296] 42) loss = 6.915, grad_norm = 0.598
I0405 19:34:19.627194 139976138290944 logging_writer.py:48] [43] global_step=43, grad_norm=0.612470, loss=6.919965
I0405 19:34:19.631302 140028517898048 submission.py:296] 43) loss = 6.920, grad_norm = 0.612
I0405 19:34:20.025551 139976222152448 logging_writer.py:48] [44] global_step=44, grad_norm=0.607592, loss=6.925570
I0405 19:34:20.031118 140028517898048 submission.py:296] 44) loss = 6.926, grad_norm = 0.608
I0405 19:34:20.420248 139976138290944 logging_writer.py:48] [45] global_step=45, grad_norm=0.588346, loss=6.923385
I0405 19:34:20.424989 140028517898048 submission.py:296] 45) loss = 6.923, grad_norm = 0.588
I0405 19:34:20.814484 139976222152448 logging_writer.py:48] [46] global_step=46, grad_norm=0.616757, loss=6.906949
I0405 19:34:20.817918 140028517898048 submission.py:296] 46) loss = 6.907, grad_norm = 0.617
I0405 19:34:21.206882 139976138290944 logging_writer.py:48] [47] global_step=47, grad_norm=0.603767, loss=6.915448
I0405 19:34:21.210687 140028517898048 submission.py:296] 47) loss = 6.915, grad_norm = 0.604
I0405 19:34:21.608527 139976222152448 logging_writer.py:48] [48] global_step=48, grad_norm=0.596042, loss=6.910659
I0405 19:34:21.614379 140028517898048 submission.py:296] 48) loss = 6.911, grad_norm = 0.596
I0405 19:34:22.001713 139976138290944 logging_writer.py:48] [49] global_step=49, grad_norm=0.607236, loss=6.906583
I0405 19:34:22.006048 140028517898048 submission.py:296] 49) loss = 6.907, grad_norm = 0.607
I0405 19:34:22.394949 139976222152448 logging_writer.py:48] [50] global_step=50, grad_norm=0.593233, loss=6.917890
I0405 19:34:22.398542 140028517898048 submission.py:296] 50) loss = 6.918, grad_norm = 0.593
I0405 19:34:22.785690 139976138290944 logging_writer.py:48] [51] global_step=51, grad_norm=0.601104, loss=6.918867
I0405 19:34:22.789447 140028517898048 submission.py:296] 51) loss = 6.919, grad_norm = 0.601
I0405 19:34:23.177858 139976222152448 logging_writer.py:48] [52] global_step=52, grad_norm=0.597649, loss=6.920657
I0405 19:34:23.182521 140028517898048 submission.py:296] 52) loss = 6.921, grad_norm = 0.598
I0405 19:34:23.573618 139976138290944 logging_writer.py:48] [53] global_step=53, grad_norm=0.590179, loss=6.909644
I0405 19:34:23.580945 140028517898048 submission.py:296] 53) loss = 6.910, grad_norm = 0.590
I0405 19:34:23.973114 139976222152448 logging_writer.py:48] [54] global_step=54, grad_norm=0.610009, loss=6.910434
I0405 19:34:23.976916 140028517898048 submission.py:296] 54) loss = 6.910, grad_norm = 0.610
I0405 19:34:24.368955 139976138290944 logging_writer.py:48] [55] global_step=55, grad_norm=0.588986, loss=6.910734
I0405 19:34:24.373803 140028517898048 submission.py:296] 55) loss = 6.911, grad_norm = 0.589
I0405 19:34:24.764994 139976222152448 logging_writer.py:48] [56] global_step=56, grad_norm=0.605296, loss=6.914141
I0405 19:34:24.768682 140028517898048 submission.py:296] 56) loss = 6.914, grad_norm = 0.605
I0405 19:34:25.157038 139976138290944 logging_writer.py:48] [57] global_step=57, grad_norm=0.601521, loss=6.913097
I0405 19:34:25.161091 140028517898048 submission.py:296] 57) loss = 6.913, grad_norm = 0.602
I0405 19:34:25.551909 139976222152448 logging_writer.py:48] [58] global_step=58, grad_norm=0.611916, loss=6.903273
I0405 19:34:25.556773 140028517898048 submission.py:296] 58) loss = 6.903, grad_norm = 0.612
I0405 19:34:25.948194 139976138290944 logging_writer.py:48] [59] global_step=59, grad_norm=0.614649, loss=6.908178
I0405 19:34:25.951883 140028517898048 submission.py:296] 59) loss = 6.908, grad_norm = 0.615
I0405 19:34:26.339504 139976222152448 logging_writer.py:48] [60] global_step=60, grad_norm=0.605161, loss=6.906780
I0405 19:34:26.344519 140028517898048 submission.py:296] 60) loss = 6.907, grad_norm = 0.605
I0405 19:34:26.738113 139976138290944 logging_writer.py:48] [61] global_step=61, grad_norm=0.579585, loss=6.909163
I0405 19:34:26.742082 140028517898048 submission.py:296] 61) loss = 6.909, grad_norm = 0.580
I0405 19:34:27.128651 139976222152448 logging_writer.py:48] [62] global_step=62, grad_norm=0.587040, loss=6.911190
I0405 19:34:27.132571 140028517898048 submission.py:296] 62) loss = 6.911, grad_norm = 0.587
I0405 19:34:27.521441 139976138290944 logging_writer.py:48] [63] global_step=63, grad_norm=0.610535, loss=6.904770
I0405 19:34:27.525519 140028517898048 submission.py:296] 63) loss = 6.905, grad_norm = 0.611
I0405 19:34:27.913702 139976222152448 logging_writer.py:48] [64] global_step=64, grad_norm=0.630241, loss=6.900492
I0405 19:34:27.917535 140028517898048 submission.py:296] 64) loss = 6.900, grad_norm = 0.630
I0405 19:34:28.305781 139976138290944 logging_writer.py:48] [65] global_step=65, grad_norm=0.598339, loss=6.908786
I0405 19:34:28.310243 140028517898048 submission.py:296] 65) loss = 6.909, grad_norm = 0.598
I0405 19:34:28.697789 139976222152448 logging_writer.py:48] [66] global_step=66, grad_norm=0.603797, loss=6.900151
I0405 19:34:28.702144 140028517898048 submission.py:296] 66) loss = 6.900, grad_norm = 0.604
I0405 19:34:29.089309 139976138290944 logging_writer.py:48] [67] global_step=67, grad_norm=0.604290, loss=6.907042
I0405 19:34:29.092957 140028517898048 submission.py:296] 67) loss = 6.907, grad_norm = 0.604
I0405 19:34:29.480879 139976222152448 logging_writer.py:48] [68] global_step=68, grad_norm=0.598515, loss=6.894503
I0405 19:34:29.485203 140028517898048 submission.py:296] 68) loss = 6.895, grad_norm = 0.599
I0405 19:34:29.940698 139976138290944 logging_writer.py:48] [69] global_step=69, grad_norm=0.574628, loss=6.896767
I0405 19:34:29.944942 140028517898048 submission.py:296] 69) loss = 6.897, grad_norm = 0.575
I0405 19:34:30.336271 139976222152448 logging_writer.py:48] [70] global_step=70, grad_norm=0.594526, loss=6.895888
I0405 19:34:30.341018 140028517898048 submission.py:296] 70) loss = 6.896, grad_norm = 0.595
I0405 19:34:30.731300 139976138290944 logging_writer.py:48] [71] global_step=71, grad_norm=0.600583, loss=6.902408
I0405 19:34:30.735352 140028517898048 submission.py:296] 71) loss = 6.902, grad_norm = 0.601
I0405 19:34:31.123918 139976222152448 logging_writer.py:48] [72] global_step=72, grad_norm=0.601328, loss=6.894137
I0405 19:34:31.127304 140028517898048 submission.py:296] 72) loss = 6.894, grad_norm = 0.601
I0405 19:34:31.514487 139976138290944 logging_writer.py:48] [73] global_step=73, grad_norm=0.614199, loss=6.898810
I0405 19:34:31.522572 140028517898048 submission.py:296] 73) loss = 6.899, grad_norm = 0.614
I0405 19:34:31.910454 139976222152448 logging_writer.py:48] [74] global_step=74, grad_norm=0.583458, loss=6.896991
I0405 19:34:31.914289 140028517898048 submission.py:296] 74) loss = 6.897, grad_norm = 0.583
I0405 19:34:32.316638 139976138290944 logging_writer.py:48] [75] global_step=75, grad_norm=0.590120, loss=6.896689
I0405 19:34:32.321612 140028517898048 submission.py:296] 75) loss = 6.897, grad_norm = 0.590
I0405 19:34:32.730643 139976222152448 logging_writer.py:48] [76] global_step=76, grad_norm=0.594098, loss=6.897827
I0405 19:34:32.735778 140028517898048 submission.py:296] 76) loss = 6.898, grad_norm = 0.594
I0405 19:34:33.129583 139976138290944 logging_writer.py:48] [77] global_step=77, grad_norm=0.603377, loss=6.899851
I0405 19:34:33.134138 140028517898048 submission.py:296] 77) loss = 6.900, grad_norm = 0.603
I0405 19:34:33.530028 139976222152448 logging_writer.py:48] [78] global_step=78, grad_norm=0.604452, loss=6.891995
I0405 19:34:33.537514 140028517898048 submission.py:296] 78) loss = 6.892, grad_norm = 0.604
I0405 19:34:33.937006 139976138290944 logging_writer.py:48] [79] global_step=79, grad_norm=0.621516, loss=6.908974
I0405 19:34:33.942153 140028517898048 submission.py:296] 79) loss = 6.909, grad_norm = 0.622
I0405 19:34:34.357048 139976222152448 logging_writer.py:48] [80] global_step=80, grad_norm=0.579513, loss=6.886972
I0405 19:34:34.362210 140028517898048 submission.py:296] 80) loss = 6.887, grad_norm = 0.580
I0405 19:34:34.778503 139976138290944 logging_writer.py:48] [81] global_step=81, grad_norm=0.585276, loss=6.895412
I0405 19:34:34.782152 140028517898048 submission.py:296] 81) loss = 6.895, grad_norm = 0.585
I0405 19:34:35.175516 139976222152448 logging_writer.py:48] [82] global_step=82, grad_norm=0.592725, loss=6.885519
I0405 19:34:35.179029 140028517898048 submission.py:296] 82) loss = 6.886, grad_norm = 0.593
I0405 19:34:35.567313 139976138290944 logging_writer.py:48] [83] global_step=83, grad_norm=0.604298, loss=6.896118
I0405 19:34:35.571187 140028517898048 submission.py:296] 83) loss = 6.896, grad_norm = 0.604
I0405 19:34:35.963952 139976222152448 logging_writer.py:48] [84] global_step=84, grad_norm=0.589120, loss=6.886373
I0405 19:34:35.967766 140028517898048 submission.py:296] 84) loss = 6.886, grad_norm = 0.589
I0405 19:34:36.367440 139976138290944 logging_writer.py:48] [85] global_step=85, grad_norm=0.586626, loss=6.886782
I0405 19:34:36.370950 140028517898048 submission.py:296] 85) loss = 6.887, grad_norm = 0.587
I0405 19:34:36.772082 139976222152448 logging_writer.py:48] [86] global_step=86, grad_norm=0.607512, loss=6.884523
I0405 19:34:36.775676 140028517898048 submission.py:296] 86) loss = 6.885, grad_norm = 0.608
I0405 19:34:37.163682 139976138290944 logging_writer.py:48] [87] global_step=87, grad_norm=0.595534, loss=6.881030
I0405 19:34:37.168395 140028517898048 submission.py:296] 87) loss = 6.881, grad_norm = 0.596
I0405 19:34:37.560044 139976222152448 logging_writer.py:48] [88] global_step=88, grad_norm=0.595185, loss=6.884911
I0405 19:34:37.563622 140028517898048 submission.py:296] 88) loss = 6.885, grad_norm = 0.595
I0405 19:34:37.951600 139976138290944 logging_writer.py:48] [89] global_step=89, grad_norm=0.595723, loss=6.882511
I0405 19:34:37.955231 140028517898048 submission.py:296] 89) loss = 6.883, grad_norm = 0.596
I0405 19:34:38.350922 139976222152448 logging_writer.py:48] [90] global_step=90, grad_norm=0.608153, loss=6.889109
I0405 19:34:38.355889 140028517898048 submission.py:296] 90) loss = 6.889, grad_norm = 0.608
I0405 19:34:38.746262 139976138290944 logging_writer.py:48] [91] global_step=91, grad_norm=0.584857, loss=6.881452
I0405 19:34:38.750406 140028517898048 submission.py:296] 91) loss = 6.881, grad_norm = 0.585
I0405 19:34:39.148255 139976222152448 logging_writer.py:48] [92] global_step=92, grad_norm=0.604770, loss=6.882441
I0405 19:34:39.152256 140028517898048 submission.py:296] 92) loss = 6.882, grad_norm = 0.605
I0405 19:34:39.541384 139976138290944 logging_writer.py:48] [93] global_step=93, grad_norm=0.601115, loss=6.876513
I0405 19:34:39.545098 140028517898048 submission.py:296] 93) loss = 6.877, grad_norm = 0.601
I0405 19:34:39.940489 139976222152448 logging_writer.py:48] [94] global_step=94, grad_norm=0.587291, loss=6.884255
I0405 19:34:39.944103 140028517898048 submission.py:296] 94) loss = 6.884, grad_norm = 0.587
I0405 19:34:40.337623 139976138290944 logging_writer.py:48] [95] global_step=95, grad_norm=0.588473, loss=6.881519
I0405 19:34:40.346261 140028517898048 submission.py:296] 95) loss = 6.882, grad_norm = 0.588
I0405 19:34:40.750398 139976222152448 logging_writer.py:48] [96] global_step=96, grad_norm=0.598987, loss=6.883309
I0405 19:34:40.755752 140028517898048 submission.py:296] 96) loss = 6.883, grad_norm = 0.599
I0405 19:34:41.148157 139976138290944 logging_writer.py:48] [97] global_step=97, grad_norm=0.608855, loss=6.873977
I0405 19:34:41.152632 140028517898048 submission.py:296] 97) loss = 6.874, grad_norm = 0.609
I0405 19:34:41.542348 139976222152448 logging_writer.py:48] [98] global_step=98, grad_norm=0.603310, loss=6.875649
I0405 19:34:41.546384 140028517898048 submission.py:296] 98) loss = 6.876, grad_norm = 0.603
I0405 19:34:41.946465 139976138290944 logging_writer.py:48] [99] global_step=99, grad_norm=0.604884, loss=6.878134
I0405 19:34:41.950674 140028517898048 submission.py:296] 99) loss = 6.878, grad_norm = 0.605
I0405 19:34:42.355165 139976222152448 logging_writer.py:48] [100] global_step=100, grad_norm=0.599932, loss=6.880044
I0405 19:34:42.358806 140028517898048 submission.py:296] 100) loss = 6.880, grad_norm = 0.600
I0405 19:37:16.767501 139976138290944 logging_writer.py:48] [500] global_step=500, grad_norm=1.220959, loss=6.340325
I0405 19:37:16.771676 140028517898048 submission.py:296] 500) loss = 6.340, grad_norm = 1.221
I0405 19:40:29.501707 139976222152448 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.727138, loss=5.666497
I0405 19:40:29.512006 140028517898048 submission.py:296] 1000) loss = 5.666, grad_norm = 2.727
I0405 19:42:32.903183 140028517898048 submission_runner.py:373] Before eval at step 1318: RAM USED (GB) 98.307715072
I0405 19:42:32.903402 140028517898048 spec.py:298] Evaluating on the training split.
I0405 19:43:19.041417 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 19:44:16.212686 140028517898048 spec.py:326] Evaluating on the test split.
I0405 19:44:17.586109 140028517898048 submission_runner.py:382] Time since start: 648.84s, 	Step: 1318, 	{'train/accuracy': 0.11676897321428571, 'train/loss': 4.802484006297831, 'validation/accuracy': 0.10698, 'validation/loss': 4.877955, 'validation/num_examples': 50000, 'test/accuracy': 0.0704, 'test/loss': 5.272696484375, 'test/num_examples': 10000}
I0405 19:44:17.586462 140028517898048 submission_runner.py:396] After eval at step 1318: RAM USED (GB) 98.320941056
I0405 19:44:17.594295 139976230545152 logging_writer.py:48] [1318] global_step=1318, preemption_count=0, score=516.048030, test/accuracy=0.070400, test/loss=5.272696, test/num_examples=10000, total_duration=648.843750, train/accuracy=0.116769, train/loss=4.802484, validation/accuracy=0.106980, validation/loss=4.877955, validation/num_examples=50000
I0405 19:44:18.052953 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_1318.
I0405 19:44:18.053770 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 1318: RAM USED (GB) 98.319609856
I0405 19:45:27.973257 139976238937856 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.792731, loss=5.196128
I0405 19:45:27.977501 140028517898048 submission.py:296] 1500) loss = 5.196, grad_norm = 4.793
I0405 19:48:38.834580 139976230545152 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.336823, loss=4.889551
I0405 19:48:38.839252 140028517898048 submission.py:296] 2000) loss = 4.890, grad_norm = 5.337
I0405 19:51:50.853303 139976238937856 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.235435, loss=4.578284
I0405 19:51:50.859223 140028517898048 submission.py:296] 2500) loss = 4.578, grad_norm = 5.235
I0405 19:52:48.259124 140028517898048 submission_runner.py:373] Before eval at step 2648: RAM USED (GB) 99.838644224
I0405 19:52:48.259360 140028517898048 spec.py:298] Evaluating on the training split.
I0405 19:53:31.055028 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 19:54:26.955533 140028517898048 spec.py:326] Evaluating on the test split.
I0405 19:54:28.318169 140028517898048 submission_runner.py:382] Time since start: 1264.20s, 	Step: 2648, 	{'train/accuracy': 0.26403061224489793, 'train/loss': 3.5774747887436225, 'validation/accuracy': 0.24542, 'validation/loss': 3.71512375, 'validation/num_examples': 50000, 'test/accuracy': 0.1816, 'test/loss': 4.279965625, 'test/num_examples': 10000}
I0405 19:54:28.318526 140028517898048 submission_runner.py:396] After eval at step 2648: RAM USED (GB) 99.902603264
I0405 19:54:28.327007 139976230545152 logging_writer.py:48] [2648] global_step=2648, preemption_count=0, score=1023.973747, test/accuracy=0.181600, test/loss=4.279966, test/num_examples=10000, total_duration=1264.199470, train/accuracy=0.264031, train/loss=3.577475, validation/accuracy=0.245420, validation/loss=3.715124, validation/num_examples=50000
I0405 19:54:28.806643 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_2648.
I0405 19:54:28.807491 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 2648: RAM USED (GB) 99.902242816
I0405 19:56:43.249161 139976238937856 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.733788, loss=4.312853
I0405 19:56:43.253421 140028517898048 submission.py:296] 3000) loss = 4.313, grad_norm = 4.734
I0405 19:59:54.009439 139976230545152 logging_writer.py:48] [3500] global_step=3500, grad_norm=6.055382, loss=3.999055
I0405 19:59:54.013488 140028517898048 submission.py:296] 3500) loss = 3.999, grad_norm = 6.055
I0405 20:02:58.979965 140028517898048 submission_runner.py:373] Before eval at step 3981: RAM USED (GB) 99.854036992
I0405 20:02:58.980189 140028517898048 spec.py:298] Evaluating on the training split.
I0405 20:03:42.399561 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 20:04:28.979490 140028517898048 spec.py:326] Evaluating on the test split.
I0405 20:04:30.357978 140028517898048 submission_runner.py:382] Time since start: 1874.92s, 	Step: 3981, 	{'train/accuracy': 0.38831313775510207, 'train/loss': 2.8517487195073343, 'validation/accuracy': 0.35606, 'validation/loss': 3.0319609375, 'validation/num_examples': 50000, 'test/accuracy': 0.2522, 'test/loss': 3.774562109375, 'test/num_examples': 10000}
I0405 20:04:30.358345 140028517898048 submission_runner.py:396] After eval at step 3981: RAM USED (GB) 99.900358656
I0405 20:04:30.366905 139976238937856 logging_writer.py:48] [3981] global_step=3981, preemption_count=0, score=1531.964161, test/accuracy=0.252200, test/loss=3.774562, test/num_examples=10000, total_duration=1874.920349, train/accuracy=0.388313, train/loss=2.851749, validation/accuracy=0.356060, validation/loss=3.031961, validation/num_examples=50000
I0405 20:04:30.823312 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_3981.
I0405 20:04:30.824145 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 3981: RAM USED (GB) 99.899666432
I0405 20:04:38.448755 139976230545152 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.976464, loss=3.930364
I0405 20:04:38.452540 140028517898048 submission.py:296] 4000) loss = 3.930, grad_norm = 3.976
I0405 20:07:48.862961 139976238937856 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.985130, loss=3.548832
I0405 20:07:48.867139 140028517898048 submission.py:296] 4500) loss = 3.549, grad_norm = 3.985
I0405 20:11:00.870616 139976230545152 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.968892, loss=3.625227
I0405 20:11:00.874834 140028517898048 submission.py:296] 5000) loss = 3.625, grad_norm = 2.969
I0405 20:13:01.171911 140028517898048 submission_runner.py:373] Before eval at step 5313: RAM USED (GB) 100.07232512
I0405 20:13:01.172122 140028517898048 spec.py:298] Evaluating on the training split.
I0405 20:13:49.759596 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 20:14:45.052226 140028517898048 spec.py:326] Evaluating on the test split.
I0405 20:14:46.408676 140028517898048 submission_runner.py:382] Time since start: 2477.11s, 	Step: 5313, 	{'train/accuracy': 0.47622369260204084, 'train/loss': 2.3489713084941006, 'validation/accuracy': 0.44098, 'validation/loss': 2.54427890625, 'validation/num_examples': 50000, 'test/accuracy': 0.3259, 'test/loss': 3.29502578125, 'test/num_examples': 10000}
I0405 20:14:46.409023 140028517898048 submission_runner.py:396] After eval at step 5313: RAM USED (GB) 100.051697664
I0405 20:14:46.417910 139976238937856 logging_writer.py:48] [5313] global_step=5313, preemption_count=0, score=2040.076675, test/accuracy=0.325900, test/loss=3.295026, test/num_examples=10000, total_duration=2477.112429, train/accuracy=0.476224, train/loss=2.348971, validation/accuracy=0.440980, validation/loss=2.544279, validation/num_examples=50000
I0405 20:14:46.881392 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_5313.
I0405 20:14:46.882172 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 5313: RAM USED (GB) 100.050993152
I0405 20:15:58.427972 139976230545152 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.292265, loss=3.548598
I0405 20:15:58.432191 140028517898048 submission.py:296] 5500) loss = 3.549, grad_norm = 5.292
I0405 20:19:09.110252 139976238937856 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.440737, loss=3.438434
I0405 20:19:09.115430 140028517898048 submission.py:296] 6000) loss = 3.438, grad_norm = 4.441
I0405 20:22:22.179349 139976230545152 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.204644, loss=3.332058
I0405 20:22:22.184097 140028517898048 submission.py:296] 6500) loss = 3.332, grad_norm = 3.205
I0405 20:23:16.883517 140028517898048 submission_runner.py:373] Before eval at step 6645: RAM USED (GB) 100.102848512
I0405 20:23:16.883735 140028517898048 spec.py:298] Evaluating on the training split.
I0405 20:24:00.141106 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 20:24:45.443411 140028517898048 spec.py:326] Evaluating on the test split.
I0405 20:24:46.815734 140028517898048 submission_runner.py:382] Time since start: 3092.82s, 	Step: 6645, 	{'train/accuracy': 0.5385443239795918, 'train/loss': 2.0992458109952965, 'validation/accuracy': 0.4906, 'validation/loss': 2.32219125, 'validation/num_examples': 50000, 'test/accuracy': 0.3611, 'test/loss': 3.080819921875, 'test/num_examples': 10000}
I0405 20:24:46.816055 140028517898048 submission_runner.py:396] After eval at step 6645: RAM USED (GB) 100.104085504
I0405 20:24:46.824171 139976238937856 logging_writer.py:48] [6645] global_step=6645, preemption_count=0, score=2547.876165, test/accuracy=0.361100, test/loss=3.080820, test/num_examples=10000, total_duration=3092.824027, train/accuracy=0.538544, train/loss=2.099246, validation/accuracy=0.490600, validation/loss=2.322191, validation/num_examples=50000
I0405 20:24:47.271577 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_6645.
I0405 20:24:47.272335 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 6645: RAM USED (GB) 100.10288128
I0405 20:27:02.721768 139976230545152 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.369554, loss=3.262585
I0405 20:27:02.725788 140028517898048 submission.py:296] 7000) loss = 3.263, grad_norm = 3.370
I0405 20:30:14.383393 139976238937856 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.396886, loss=3.187515
I0405 20:30:14.387494 140028517898048 submission.py:296] 7500) loss = 3.188, grad_norm = 2.397
I0405 20:33:17.330781 140028517898048 submission_runner.py:373] Before eval at step 7978: RAM USED (GB) 99.998642176
I0405 20:33:17.331009 140028517898048 spec.py:298] Evaluating on the training split.
I0405 20:34:03.003452 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 20:34:53.611984 140028517898048 spec.py:326] Evaluating on the test split.
I0405 20:34:54.964353 140028517898048 submission_runner.py:382] Time since start: 3693.27s, 	Step: 7978, 	{'train/accuracy': 0.5886280293367347, 'train/loss': 1.8077048476861448, 'validation/accuracy': 0.53796, 'validation/loss': 2.0557328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4, 'test/loss': 2.8543044921875, 'test/num_examples': 10000}
I0405 20:34:54.964726 140028517898048 submission_runner.py:396] After eval at step 7978: RAM USED (GB) 100.03965952
I0405 20:34:54.973105 139976230545152 logging_writer.py:48] [7978] global_step=7978, preemption_count=0, score=3055.751320, test/accuracy=0.400000, test/loss=2.854304, test/num_examples=10000, total_duration=3693.271396, train/accuracy=0.588628, train/loss=1.807705, validation/accuracy=0.537960, validation/loss=2.055733, validation/num_examples=50000
I0405 20:34:55.435420 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_7978.
I0405 20:34:55.436177 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 7978: RAM USED (GB) 100.038197248
I0405 20:35:04.203492 139976238937856 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.558503, loss=3.097697
I0405 20:35:04.206884 140028517898048 submission.py:296] 8000) loss = 3.098, grad_norm = 2.559
I0405 20:38:14.907227 139976230545152 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.649386, loss=3.081340
I0405 20:38:14.911132 140028517898048 submission.py:296] 8500) loss = 3.081, grad_norm = 2.649
I0405 20:41:27.791137 139976238937856 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.950133, loss=3.040678
I0405 20:41:27.795689 140028517898048 submission.py:296] 9000) loss = 3.041, grad_norm = 1.950
I0405 20:43:25.517983 140028517898048 submission_runner.py:373] Before eval at step 9311: RAM USED (GB) 100.107247616
I0405 20:43:25.518195 140028517898048 spec.py:298] Evaluating on the training split.
I0405 20:44:10.156019 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 20:44:56.161068 140028517898048 spec.py:326] Evaluating on the test split.
I0405 20:44:57.530694 140028517898048 submission_runner.py:382] Time since start: 4301.46s, 	Step: 9311, 	{'train/accuracy': 0.6066246811224489, 'train/loss': 1.7296615911989797, 'validation/accuracy': 0.55306, 'validation/loss': 1.99074703125, 'validation/num_examples': 50000, 'test/accuracy': 0.428, 'test/loss': 2.7393896484375, 'test/num_examples': 10000}
I0405 20:44:57.531056 140028517898048 submission_runner.py:396] After eval at step 9311: RAM USED (GB) 100.10812416
I0405 20:44:57.540014 139976230545152 logging_writer.py:48] [9311] global_step=9311, preemption_count=0, score=3563.652087, test/accuracy=0.428000, test/loss=2.739390, test/num_examples=10000, total_duration=4301.458498, train/accuracy=0.606625, train/loss=1.729662, validation/accuracy=0.553060, validation/loss=1.990747, validation/num_examples=50000
I0405 20:44:58.009193 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_9311.
I0405 20:44:58.009962 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 9311: RAM USED (GB) 100.107431936
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0405 20:46:10.218161 139976238937856 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.816620, loss=2.960323
I0405 20:46:10.221879 140028517898048 submission.py:296] 9500) loss = 2.960, grad_norm = 1.817
I0405 20:49:21.620828 139976230545152 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.350622, loss=2.925776
I0405 20:49:21.625710 140028517898048 submission.py:296] 10000) loss = 2.926, grad_norm = 2.351
I0405 20:52:33.259190 139976238937856 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.755346, loss=2.919754
I0405 20:52:33.264317 140028517898048 submission.py:296] 10500) loss = 2.920, grad_norm = 1.755
I0405 20:53:28.031120 140028517898048 submission_runner.py:373] Before eval at step 10645: RAM USED (GB) 100.30303232
I0405 20:53:28.031379 140028517898048 spec.py:298] Evaluating on the training split.
I0405 20:54:13.417350 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 20:55:04.980013 140028517898048 spec.py:326] Evaluating on the test split.
I0405 20:55:06.422303 140028517898048 submission_runner.py:382] Time since start: 4903.97s, 	Step: 10645, 	{'train/accuracy': 0.6437739158163265, 'train/loss': 1.5638893283143336, 'validation/accuracy': 0.57834, 'validation/loss': 1.856370625, 'validation/num_examples': 50000, 'test/accuracy': 0.4493, 'test/loss': 2.6019478515625, 'test/num_examples': 10000}
I0405 20:55:06.422801 140028517898048 submission_runner.py:396] After eval at step 10645: RAM USED (GB) 100.328370176
I0405 20:55:06.437431 139976230545152 logging_writer.py:48] [10645] global_step=10645, preemption_count=0, score=4071.471484, test/accuracy=0.449300, test/loss=2.601948, test/num_examples=10000, total_duration=4903.968510, train/accuracy=0.643774, train/loss=1.563889, validation/accuracy=0.578340, validation/loss=1.856371, validation/num_examples=50000
I0405 20:55:06.948797 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_10645.
I0405 20:55:06.949744 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 10645: RAM USED (GB) 100.326486016
I0405 20:57:22.469140 139976238937856 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.209947, loss=2.820127
I0405 20:57:22.477590 140028517898048 submission.py:296] 11000) loss = 2.820, grad_norm = 2.210
I0405 21:00:35.081565 139976230545152 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.808320, loss=2.748701
I0405 21:00:35.086556 140028517898048 submission.py:296] 11500) loss = 2.749, grad_norm = 1.808
I0405 21:03:37.199350 140028517898048 submission_runner.py:373] Before eval at step 11980: RAM USED (GB) 99.924242432
I0405 21:03:37.199623 140028517898048 spec.py:298] Evaluating on the training split.
I0405 21:04:21.517531 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 21:05:06.962393 140028517898048 spec.py:326] Evaluating on the test split.
I0405 21:05:08.323625 140028517898048 submission_runner.py:382] Time since start: 5513.14s, 	Step: 11980, 	{'train/accuracy': 0.6716557716836735, 'train/loss': 1.438639582419882, 'validation/accuracy': 0.6047, 'validation/loss': 1.745805, 'validation/num_examples': 50000, 'test/accuracy': 0.4658, 'test/loss': 2.4980029296875, 'test/num_examples': 10000}
I0405 21:05:08.323967 140028517898048 submission_runner.py:396] After eval at step 11980: RAM USED (GB) 99.906441216
I0405 21:05:08.331958 139976238937856 logging_writer.py:48] [11980] global_step=11980, preemption_count=0, score=4579.565976, test/accuracy=0.465800, test/loss=2.498003, test/num_examples=10000, total_duration=5513.138443, train/accuracy=0.671656, train/loss=1.438640, validation/accuracy=0.604700, validation/loss=1.745805, validation/num_examples=50000
I0405 21:05:08.796729 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_11980.
I0405 21:05:08.797495 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 11980: RAM USED (GB) 99.905744896
I0405 21:05:16.765361 139976230545152 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.365581, loss=2.749363
I0405 21:05:16.768821 140028517898048 submission.py:296] 12000) loss = 2.749, grad_norm = 1.366
I0405 21:08:28.169715 139976238937856 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.474903, loss=2.798518
I0405 21:08:28.175952 140028517898048 submission.py:296] 12500) loss = 2.799, grad_norm = 1.475
I0405 21:11:39.616602 139976230545152 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.428544, loss=2.725721
I0405 21:11:39.620392 140028517898048 submission.py:296] 13000) loss = 2.726, grad_norm = 1.429
I0405 21:13:39.095471 140028517898048 submission_runner.py:373] Before eval at step 13315: RAM USED (GB) 100.229988352
I0405 21:13:39.095688 140028517898048 spec.py:298] Evaluating on the training split.
I0405 21:14:23.226161 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 21:15:10.680861 140028517898048 spec.py:326] Evaluating on the test split.
I0405 21:15:12.029693 140028517898048 submission_runner.py:382] Time since start: 6115.04s, 	Step: 13315, 	{'train/accuracy': 0.6820790816326531, 'train/loss': 1.3804257451271524, 'validation/accuracy': 0.61408, 'validation/loss': 1.71340875, 'validation/num_examples': 50000, 'test/accuracy': 0.472, 'test/loss': 2.468335546875, 'test/num_examples': 10000}
I0405 21:15:12.030057 140028517898048 submission_runner.py:396] After eval at step 13315: RAM USED (GB) 100.321509376
I0405 21:15:12.038724 139976238937856 logging_writer.py:48] [13315] global_step=13315, preemption_count=0, score=5087.690595, test/accuracy=0.472000, test/loss=2.468336, test/num_examples=10000, total_duration=6115.035943, train/accuracy=0.682079, train/loss=1.380426, validation/accuracy=0.614080, validation/loss=1.713409, validation/num_examples=50000
I0405 21:15:12.513221 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_13315.
I0405 21:15:12.514042 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 13315: RAM USED (GB) 100.320821248
I0405 21:16:23.442568 139976230545152 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.590727, loss=2.696059
I0405 21:16:23.446512 140028517898048 submission.py:296] 13500) loss = 2.696, grad_norm = 1.591
I0405 21:19:35.783410 140028517898048 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.069474304
I0405 21:19:35.783630 140028517898048 spec.py:298] Evaluating on the training split.
I0405 21:20:20.740566 140028517898048 spec.py:310] Evaluating on the validation split.
I0405 21:21:07.496234 140028517898048 spec.py:326] Evaluating on the test split.
I0405 21:21:08.866886 140028517898048 submission_runner.py:382] Time since start: 6471.72s, 	Step: 14000, 	{'train/accuracy': 0.7023278061224489, 'train/loss': 1.291134581273916, 'validation/accuracy': 0.63036, 'validation/loss': 1.63353234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4842, 'test/loss': 2.3900658203125, 'test/num_examples': 10000}
I0405 21:21:08.867215 140028517898048 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.025008128
I0405 21:21:08.876209 139976238937856 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5349.811844, test/accuracy=0.484200, test/loss=2.390066, test/num_examples=10000, total_duration=6471.723908, train/accuracy=0.702328, train/loss=1.291135, validation/accuracy=0.630360, validation/loss=1.633532, validation/num_examples=50000
I0405 21:21:09.340458 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:21:09.341211 140028517898048 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.025475072
I0405 21:21:09.348827 139976230545152 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5349.811844
I0405 21:21:10.535615 140028517898048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:21:10.926496 140028517898048 submission_runner.py:550] Tuning trial 1/1
I0405 21:21:10.926739 140028517898048 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 21:21:10.927309 140028517898048 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012555803571428572, 'train/loss': 6.9212440957828445, 'validation/accuracy': 0.001, 'validation/loss': 6.9218675, 'validation/num_examples': 50000, 'test/accuracy': 0.0009, 'test/loss': 6.9226734375, 'test/num_examples': 10000, 'score': 8.092612028121948, 'total_duration': 8.094494342803955, 'global_step': 1, 'preemption_count': 0}), (1318, {'train/accuracy': 0.11676897321428571, 'train/loss': 4.802484006297831, 'validation/accuracy': 0.10698, 'validation/loss': 4.877955, 'validation/num_examples': 50000, 'test/accuracy': 0.0704, 'test/loss': 5.272696484375, 'test/num_examples': 10000, 'score': 516.0480298995972, 'total_duration': 648.8437502384186, 'global_step': 1318, 'preemption_count': 0}), (2648, {'train/accuracy': 0.26403061224489793, 'train/loss': 3.5774747887436225, 'validation/accuracy': 0.24542, 'validation/loss': 3.71512375, 'validation/num_examples': 50000, 'test/accuracy': 0.1816, 'test/loss': 4.279965625, 'test/num_examples': 10000, 'score': 1023.9737467765808, 'total_duration': 1264.199470281601, 'global_step': 2648, 'preemption_count': 0}), (3981, {'train/accuracy': 0.38831313775510207, 'train/loss': 2.8517487195073343, 'validation/accuracy': 0.35606, 'validation/loss': 3.0319609375, 'validation/num_examples': 50000, 'test/accuracy': 0.2522, 'test/loss': 3.774562109375, 'test/num_examples': 10000, 'score': 1531.9641613960266, 'total_duration': 1874.9203488826752, 'global_step': 3981, 'preemption_count': 0}), (5313, {'train/accuracy': 0.47622369260204084, 'train/loss': 2.3489713084941006, 'validation/accuracy': 0.44098, 'validation/loss': 2.54427890625, 'validation/num_examples': 50000, 'test/accuracy': 0.3259, 'test/loss': 3.29502578125, 'test/num_examples': 10000, 'score': 2040.076675415039, 'total_duration': 2477.1124289035797, 'global_step': 5313, 'preemption_count': 0}), (6645, {'train/accuracy': 0.5385443239795918, 'train/loss': 2.0992458109952965, 'validation/accuracy': 0.4906, 'validation/loss': 2.32219125, 'validation/num_examples': 50000, 'test/accuracy': 0.3611, 'test/loss': 3.080819921875, 'test/num_examples': 10000, 'score': 2547.8761649131775, 'total_duration': 3092.824026823044, 'global_step': 6645, 'preemption_count': 0}), (7978, {'train/accuracy': 0.5886280293367347, 'train/loss': 1.8077048476861448, 'validation/accuracy': 0.53796, 'validation/loss': 2.0557328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4, 'test/loss': 2.8543044921875, 'test/num_examples': 10000, 'score': 3055.751320362091, 'total_duration': 3693.271395921707, 'global_step': 7978, 'preemption_count': 0}), (9311, {'train/accuracy': 0.6066246811224489, 'train/loss': 1.7296615911989797, 'validation/accuracy': 0.55306, 'validation/loss': 1.99074703125, 'validation/num_examples': 50000, 'test/accuracy': 0.428, 'test/loss': 2.7393896484375, 'test/num_examples': 10000, 'score': 3563.6520869731903, 'total_duration': 4301.458498239517, 'global_step': 9311, 'preemption_count': 0}), (10645, {'train/accuracy': 0.6437739158163265, 'train/loss': 1.5638893283143336, 'validation/accuracy': 0.57834, 'validation/loss': 1.856370625, 'validation/num_examples': 50000, 'test/accuracy': 0.4493, 'test/loss': 2.6019478515625, 'test/num_examples': 10000, 'score': 4071.471484184265, 'total_duration': 4903.968509912491, 'global_step': 10645, 'preemption_count': 0}), (11980, {'train/accuracy': 0.6716557716836735, 'train/loss': 1.438639582419882, 'validation/accuracy': 0.6047, 'validation/loss': 1.745805, 'validation/num_examples': 50000, 'test/accuracy': 0.4658, 'test/loss': 2.4980029296875, 'test/num_examples': 10000, 'score': 4579.565976142883, 'total_duration': 5513.1384427547455, 'global_step': 11980, 'preemption_count': 0}), (13315, {'train/accuracy': 0.6820790816326531, 'train/loss': 1.3804257451271524, 'validation/accuracy': 0.61408, 'validation/loss': 1.71340875, 'validation/num_examples': 50000, 'test/accuracy': 0.472, 'test/loss': 2.468335546875, 'test/num_examples': 10000, 'score': 5087.690595149994, 'total_duration': 6115.035943031311, 'global_step': 13315, 'preemption_count': 0}), (14000, {'train/accuracy': 0.7023278061224489, 'train/loss': 1.291134581273916, 'validation/accuracy': 0.63036, 'validation/loss': 1.63353234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4842, 'test/loss': 2.3900658203125, 'test/num_examples': 10000, 'score': 5349.811844110489, 'total_duration': 6471.723908424377, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 21:21:10.927399 140028517898048 submission_runner.py:553] Timing: 5349.811844110489
I0405 21:21:10.927444 140028517898048 submission_runner.py:554] ====================
I0405 21:21:10.927539 140028517898048 submission_runner.py:613] Final imagenet_resnet score: 5349.811844110489
