WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 23:56:53.936459 139900306782016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 23:56:53.936496 139721949435712 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 23:56:53.937501 139712372315968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 23:56:53.937529 139861395498816 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 23:56:53.937572 140318548789056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 23:56:54.923409 139980197865280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 23:56:54.923433 140546115508032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 23:56:54.930469 140319791671104 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 23:56:54.930830 140319791671104 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.934126 139980197865280 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.934153 140546115508032 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.939857 139900306782016 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.939912 139721949435712 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.940007 139861395498816 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.939923 140318548789056 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:54.940034 139712372315968 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:56:56.061657 140319791671104 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch.
W0405 23:56:56.171402 139712372315968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.173082 139861395498816 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.173314 140318548789056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.173869 140319791671104 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.174201 139980197865280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.174379 139900306782016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.175025 140546115508032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:56:56.176764 139721949435712 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 23:56:56.177711 140319791671104 submission_runner.py:511] Using RNG seed 4264800723
I0405 23:56:56.178689 140319791671104 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 23:56:56.178800 140319791671104 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1.
I0405 23:56:56.178979 140319791671104 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/hparams.json.
I0405 23:56:56.179883 140319791671104 submission_runner.py:230] Starting train once: RAM USED (GB) 5.622636544
I0405 23:56:56.179974 140319791671104 submission_runner.py:231] Initializing dataset.
I0405 23:56:56.180156 140319791671104 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.622636544
I0405 23:56:56.180219 140319791671104 submission_runner.py:240] Initializing model.
I0405 23:57:00.053952 140319791671104 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.2942592
I0405 23:57:00.054128 140319791671104 submission_runner.py:252] Initializing optimizer.
I0405 23:57:00.174947 140319791671104 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.29829376
I0405 23:57:00.175128 140319791671104 submission_runner.py:261] Initializing metrics bundle.
I0405 23:57:00.175177 140319791671104 submission_runner.py:276] Initializing checkpoint and logger.
I0405 23:57:00.176129 140319791671104 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 23:57:00.176272 140319791671104 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 23:57:00.788734 140319791671104 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/meta_data_0.json.
I0405 23:57:00.789657 140319791671104 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/flags_0.json.
I0405 23:57:00.823184 140319791671104 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.353102336
I0405 23:57:00.824289 140319791671104 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.353102336
I0405 23:57:00.824418 140319791671104 submission_runner.py:313] Starting training loop.
I0405 23:57:01.053335 140319791671104 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:01.058755 140319791671104 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:57:01.156220 140319791671104 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:02.555892 140319791671104 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 15.590404096
I0405 23:57:06.267418 140281374549760 logging_writer.py:48] [0] global_step=0, grad_norm=2.520922, loss=0.717208
I0405 23:57:06.273250 140319791671104 submission.py:139] 0) loss = 0.717, grad_norm = 2.521
I0405 23:57:06.273727 140319791671104 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 21.937717248
I0405 23:57:06.274293 140319791671104 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 21.937717248
I0405 23:57:06.274403 140319791671104 spec.py:298] Evaluating on the training split.
I0405 23:57:06.278817 140319791671104 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:06.282793 140319791671104 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:57:06.333984 140319791671104 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0405 23:57:21.184519 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.415447 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.416084 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.419134 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.433651 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.433797 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.433999 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:21.436913 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.077616 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.244340 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.244910 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.250140 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.250182 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.250576 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.250748 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:35.251074 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0405 23:57:59.774314 140319791671104 spec.py:310] Evaluating on the validation split.
I0405 23:57:59.777137 140319791671104 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:59.781042 140319791671104 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:57:59.833045 140319791671104 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0405 23:58:12.914281 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.084981 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.086481 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.090278 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.091060 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.091917 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.091989 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:13.092482 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.382814 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.544264 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.546197 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.550418 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.550726 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.551427 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.551536 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:18.551543 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0405 23:58:42.126795 140319791671104 spec.py:326] Evaluating on the test split.
I0405 23:58:42.129505 140319791671104 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:58:42.133363 140319791671104 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:58:42.184635 140319791671104 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0405 23:58:55.226927 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.394787 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.395449 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.399930 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.400006 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.400505 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.400918 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:55.400936 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.704333 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.866850 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.867921 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.872174 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.872966 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.872994 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.873464 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:00.874121 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0405 23:59:25.625205 140319791671104 submission_runner.py:382] Time since start: 5.45s, 	Step: 1, 	{'train/accuracy': 0.559599264276412, 'train/loss': 0.7165730595588684, 'train/mean_average_precision': 0.02182539673766314, 'validation/accuracy': 0.5544530882750525, 'validation/loss': 0.7206974625587463, 'validation/mean_average_precision': 0.025711809814972657, 'validation/num_examples': 43793, 'test/accuracy': 0.5514554592938503, 'test/loss': 0.7224020957946777, 'test/mean_average_precision': 0.027600868549231536, 'test/num_examples': 43793}
I0405 23:59:25.625606 140319791671104 submission_runner.py:396] After eval at step 1: RAM USED (GB) 25.853177856
I0405 23:59:25.632625 140267851687680 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.448528, test/accuracy=0.551455, test/loss=0.722402, test/mean_average_precision=0.027601, test/num_examples=43793, total_duration=5.450520, train/accuracy=0.559599, train/loss=0.716573, train/mean_average_precision=0.021825, validation/accuracy=0.554453, validation/loss=0.720697, validation/mean_average_precision=0.025712, validation/num_examples=43793
I0405 23:59:25.695123 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_1.
I0405 23:59:25.695523 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 25.85217024
I0405 23:59:25.922891 140319791671104 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 25.90773248
I0405 23:59:25.925594 140319791671104 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.931966 140318548789056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.932690 140546115508032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.932817 139861395498816 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.933516 139980197865280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.933525 139721949435712 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.933536 139900306782016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.933526 139712372315968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:25.962194 140267860080384 logging_writer.py:48] [1] global_step=1, grad_norm=2.476032, loss=0.716507
I0405 23:59:25.966602 140319791671104 submission.py:139] 1) loss = 0.717, grad_norm = 2.476
I0405 23:59:25.967056 140319791671104 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 25.907757056
I0405 23:59:26.222717 140267851687680 logging_writer.py:48] [2] global_step=2, grad_norm=2.515038, loss=0.714841
I0405 23:59:26.226749 140319791671104 submission.py:139] 2) loss = 0.715, grad_norm = 2.515
I0405 23:59:26.488169 140267860080384 logging_writer.py:48] [3] global_step=3, grad_norm=2.471835, loss=0.711776
I0405 23:59:26.492059 140319791671104 submission.py:139] 3) loss = 0.712, grad_norm = 2.472
I0405 23:59:26.750977 140267851687680 logging_writer.py:48] [4] global_step=4, grad_norm=2.395099, loss=0.704988
I0405 23:59:26.754986 140319791671104 submission.py:139] 4) loss = 0.705, grad_norm = 2.395
I0405 23:59:27.008341 140267860080384 logging_writer.py:48] [5] global_step=5, grad_norm=2.247806, loss=0.694296
I0405 23:59:27.012197 140319791671104 submission.py:139] 5) loss = 0.694, grad_norm = 2.248
I0405 23:59:27.269293 140267851687680 logging_writer.py:48] [6] global_step=6, grad_norm=2.109397, loss=0.680175
I0405 23:59:27.273136 140319791671104 submission.py:139] 6) loss = 0.680, grad_norm = 2.109
I0405 23:59:27.538301 140267860080384 logging_writer.py:48] [7] global_step=7, grad_norm=2.027988, loss=0.664807
I0405 23:59:27.542268 140319791671104 submission.py:139] 7) loss = 0.665, grad_norm = 2.028
I0405 23:59:27.815589 140267851687680 logging_writer.py:48] [8] global_step=8, grad_norm=1.869921, loss=0.642564
I0405 23:59:27.819411 140319791671104 submission.py:139] 8) loss = 0.643, grad_norm = 1.870
I0405 23:59:28.100093 140267860080384 logging_writer.py:48] [9] global_step=9, grad_norm=1.717422, loss=0.624108
I0405 23:59:28.103962 140319791671104 submission.py:139] 9) loss = 0.624, grad_norm = 1.717
I0405 23:59:28.359649 140267851687680 logging_writer.py:48] [10] global_step=10, grad_norm=1.533696, loss=0.602923
I0405 23:59:28.363318 140319791671104 submission.py:139] 10) loss = 0.603, grad_norm = 1.534
I0405 23:59:28.622734 140267860080384 logging_writer.py:48] [11] global_step=11, grad_norm=1.440132, loss=0.583460
I0405 23:59:28.626473 140319791671104 submission.py:139] 11) loss = 0.583, grad_norm = 1.440
I0405 23:59:29.039618 140267851687680 logging_writer.py:48] [12] global_step=12, grad_norm=1.316915, loss=0.565902
I0405 23:59:29.043401 140319791671104 submission.py:139] 12) loss = 0.566, grad_norm = 1.317
I0405 23:59:29.298477 140267860080384 logging_writer.py:48] [13] global_step=13, grad_norm=1.195093, loss=0.550875
I0405 23:59:29.302393 140319791671104 submission.py:139] 13) loss = 0.551, grad_norm = 1.195
I0405 23:59:29.558808 140267851687680 logging_writer.py:48] [14] global_step=14, grad_norm=1.132033, loss=0.533907
I0405 23:59:29.562677 140319791671104 submission.py:139] 14) loss = 0.534, grad_norm = 1.132
I0405 23:59:29.821228 140267860080384 logging_writer.py:48] [15] global_step=15, grad_norm=1.045998, loss=0.521047
I0405 23:59:29.825494 140319791671104 submission.py:139] 15) loss = 0.521, grad_norm = 1.046
I0405 23:59:30.086088 140267851687680 logging_writer.py:48] [16] global_step=16, grad_norm=0.944744, loss=0.509384
I0405 23:59:30.089983 140319791671104 submission.py:139] 16) loss = 0.509, grad_norm = 0.945
I0405 23:59:30.349899 140267860080384 logging_writer.py:48] [17] global_step=17, grad_norm=0.871761, loss=0.496749
I0405 23:59:30.353684 140319791671104 submission.py:139] 17) loss = 0.497, grad_norm = 0.872
I0405 23:59:30.611444 140267851687680 logging_writer.py:48] [18] global_step=18, grad_norm=0.829311, loss=0.485667
I0405 23:59:30.615530 140319791671104 submission.py:139] 18) loss = 0.486, grad_norm = 0.829
I0405 23:59:30.870697 140267860080384 logging_writer.py:48] [19] global_step=19, grad_norm=0.783994, loss=0.475638
I0405 23:59:30.874646 140319791671104 submission.py:139] 19) loss = 0.476, grad_norm = 0.784
I0405 23:59:31.132323 140267851687680 logging_writer.py:48] [20] global_step=20, grad_norm=0.747904, loss=0.464152
I0405 23:59:31.136226 140319791671104 submission.py:139] 20) loss = 0.464, grad_norm = 0.748
I0405 23:59:31.394347 140267860080384 logging_writer.py:48] [21] global_step=21, grad_norm=0.710754, loss=0.454441
I0405 23:59:31.398249 140319791671104 submission.py:139] 21) loss = 0.454, grad_norm = 0.711
I0405 23:59:31.654023 140267851687680 logging_writer.py:48] [22] global_step=22, grad_norm=0.681054, loss=0.444578
I0405 23:59:31.658045 140319791671104 submission.py:139] 22) loss = 0.445, grad_norm = 0.681
I0405 23:59:31.914743 140267860080384 logging_writer.py:48] [23] global_step=23, grad_norm=0.638181, loss=0.436185
I0405 23:59:31.918518 140319791671104 submission.py:139] 23) loss = 0.436, grad_norm = 0.638
I0405 23:59:32.177864 140267851687680 logging_writer.py:48] [24] global_step=24, grad_norm=0.605704, loss=0.427225
I0405 23:59:32.181576 140319791671104 submission.py:139] 24) loss = 0.427, grad_norm = 0.606
I0405 23:59:32.435740 140267860080384 logging_writer.py:48] [25] global_step=25, grad_norm=0.575866, loss=0.418670
I0405 23:59:32.439465 140319791671104 submission.py:139] 25) loss = 0.419, grad_norm = 0.576
I0405 23:59:32.699365 140267851687680 logging_writer.py:48] [26] global_step=26, grad_norm=0.558402, loss=0.406171
I0405 23:59:32.703221 140319791671104 submission.py:139] 26) loss = 0.406, grad_norm = 0.558
I0405 23:59:32.958702 140267860080384 logging_writer.py:48] [27] global_step=27, grad_norm=0.527266, loss=0.399860
I0405 23:59:32.962494 140319791671104 submission.py:139] 27) loss = 0.400, grad_norm = 0.527
I0405 23:59:33.218606 140267851687680 logging_writer.py:48] [28] global_step=28, grad_norm=0.505783, loss=0.390950
I0405 23:59:33.222426 140319791671104 submission.py:139] 28) loss = 0.391, grad_norm = 0.506
I0405 23:59:33.476954 140267860080384 logging_writer.py:48] [29] global_step=29, grad_norm=0.489865, loss=0.384601
I0405 23:59:33.480679 140319791671104 submission.py:139] 29) loss = 0.385, grad_norm = 0.490
I0405 23:59:33.734486 140267851687680 logging_writer.py:48] [30] global_step=30, grad_norm=0.473202, loss=0.377953
I0405 23:59:33.738543 140319791671104 submission.py:139] 30) loss = 0.378, grad_norm = 0.473
I0405 23:59:33.992927 140267860080384 logging_writer.py:48] [31] global_step=31, grad_norm=0.454607, loss=0.369029
I0405 23:59:33.996888 140319791671104 submission.py:139] 31) loss = 0.369, grad_norm = 0.455
I0405 23:59:34.246824 140267851687680 logging_writer.py:48] [32] global_step=32, grad_norm=0.443900, loss=0.359072
I0405 23:59:34.250567 140319791671104 submission.py:139] 32) loss = 0.359, grad_norm = 0.444
I0405 23:59:34.506803 140267860080384 logging_writer.py:48] [33] global_step=33, grad_norm=0.432645, loss=0.351731
I0405 23:59:34.510663 140319791671104 submission.py:139] 33) loss = 0.352, grad_norm = 0.433
I0405 23:59:34.771773 140267851687680 logging_writer.py:48] [34] global_step=34, grad_norm=0.423363, loss=0.344076
I0405 23:59:34.775771 140319791671104 submission.py:139] 34) loss = 0.344, grad_norm = 0.423
I0405 23:59:35.033611 140267860080384 logging_writer.py:48] [35] global_step=35, grad_norm=0.412062, loss=0.339650
I0405 23:59:35.037296 140319791671104 submission.py:139] 35) loss = 0.340, grad_norm = 0.412
I0405 23:59:35.296358 140267851687680 logging_writer.py:48] [36] global_step=36, grad_norm=0.394857, loss=0.329650
I0405 23:59:35.300353 140319791671104 submission.py:139] 36) loss = 0.330, grad_norm = 0.395
I0405 23:59:35.554921 140267860080384 logging_writer.py:48] [37] global_step=37, grad_norm=0.381728, loss=0.321405
I0405 23:59:35.558793 140319791671104 submission.py:139] 37) loss = 0.321, grad_norm = 0.382
I0405 23:59:35.814715 140267851687680 logging_writer.py:48] [38] global_step=38, grad_norm=0.372217, loss=0.315685
I0405 23:59:35.818444 140319791671104 submission.py:139] 38) loss = 0.316, grad_norm = 0.372
I0405 23:59:36.074014 140267860080384 logging_writer.py:48] [39] global_step=39, grad_norm=0.364694, loss=0.305683
I0405 23:59:36.077910 140319791671104 submission.py:139] 39) loss = 0.306, grad_norm = 0.365
I0405 23:59:36.331658 140267851687680 logging_writer.py:48] [40] global_step=40, grad_norm=0.351290, loss=0.300945
I0405 23:59:36.335679 140319791671104 submission.py:139] 40) loss = 0.301, grad_norm = 0.351
I0405 23:59:36.588444 140267860080384 logging_writer.py:48] [41] global_step=41, grad_norm=0.346335, loss=0.290451
I0405 23:59:36.592294 140319791671104 submission.py:139] 41) loss = 0.290, grad_norm = 0.346
I0405 23:59:36.847398 140267851687680 logging_writer.py:48] [42] global_step=42, grad_norm=0.336567, loss=0.285872
I0405 23:59:36.851334 140319791671104 submission.py:139] 42) loss = 0.286, grad_norm = 0.337
I0405 23:59:37.104713 140267860080384 logging_writer.py:48] [43] global_step=43, grad_norm=0.326129, loss=0.277118
I0405 23:59:37.108558 140319791671104 submission.py:139] 43) loss = 0.277, grad_norm = 0.326
I0405 23:59:37.366484 140267851687680 logging_writer.py:48] [44] global_step=44, grad_norm=0.316894, loss=0.272050
I0405 23:59:37.370302 140319791671104 submission.py:139] 44) loss = 0.272, grad_norm = 0.317
I0405 23:59:37.629393 140267860080384 logging_writer.py:48] [45] global_step=45, grad_norm=0.312183, loss=0.265680
I0405 23:59:37.633466 140319791671104 submission.py:139] 45) loss = 0.266, grad_norm = 0.312
I0405 23:59:37.894998 140267851687680 logging_writer.py:48] [46] global_step=46, grad_norm=0.301695, loss=0.258267
I0405 23:59:37.898788 140319791671104 submission.py:139] 46) loss = 0.258, grad_norm = 0.302
I0405 23:59:38.149318 140267860080384 logging_writer.py:48] [47] global_step=47, grad_norm=0.295996, loss=0.250692
I0405 23:59:38.153131 140319791671104 submission.py:139] 47) loss = 0.251, grad_norm = 0.296
I0405 23:59:38.406989 140267851687680 logging_writer.py:48] [48] global_step=48, grad_norm=0.288261, loss=0.243980
I0405 23:59:38.410781 140319791671104 submission.py:139] 48) loss = 0.244, grad_norm = 0.288
I0405 23:59:38.666264 140267860080384 logging_writer.py:48] [49] global_step=49, grad_norm=0.278804, loss=0.240408
I0405 23:59:38.670120 140319791671104 submission.py:139] 49) loss = 0.240, grad_norm = 0.279
I0405 23:59:38.922062 140267851687680 logging_writer.py:48] [50] global_step=50, grad_norm=0.273470, loss=0.232936
I0405 23:59:38.926089 140319791671104 submission.py:139] 50) loss = 0.233, grad_norm = 0.273
I0405 23:59:39.181105 140267860080384 logging_writer.py:48] [51] global_step=51, grad_norm=0.262806, loss=0.226707
I0405 23:59:39.185437 140319791671104 submission.py:139] 51) loss = 0.227, grad_norm = 0.263
I0405 23:59:39.440261 140267851687680 logging_writer.py:48] [52] global_step=52, grad_norm=0.257823, loss=0.220784
I0405 23:59:39.443936 140319791671104 submission.py:139] 52) loss = 0.221, grad_norm = 0.258
I0405 23:59:39.698839 140267860080384 logging_writer.py:48] [53] global_step=53, grad_norm=0.249541, loss=0.213185
I0405 23:59:39.702628 140319791671104 submission.py:139] 53) loss = 0.213, grad_norm = 0.250
I0405 23:59:39.959120 140267851687680 logging_writer.py:48] [54] global_step=54, grad_norm=0.243649, loss=0.210836
I0405 23:59:39.962852 140319791671104 submission.py:139] 54) loss = 0.211, grad_norm = 0.244
I0405 23:59:40.219417 140267860080384 logging_writer.py:48] [55] global_step=55, grad_norm=0.233767, loss=0.203170
I0405 23:59:40.223157 140319791671104 submission.py:139] 55) loss = 0.203, grad_norm = 0.234
I0405 23:59:40.480213 140267851687680 logging_writer.py:48] [56] global_step=56, grad_norm=0.226812, loss=0.196200
I0405 23:59:40.483922 140319791671104 submission.py:139] 56) loss = 0.196, grad_norm = 0.227
I0405 23:59:40.745717 140267860080384 logging_writer.py:48] [57] global_step=57, grad_norm=0.218341, loss=0.195089
I0405 23:59:40.749462 140319791671104 submission.py:139] 57) loss = 0.195, grad_norm = 0.218
I0405 23:59:41.006677 140267851687680 logging_writer.py:48] [58] global_step=58, grad_norm=0.211339, loss=0.190308
I0405 23:59:41.010515 140319791671104 submission.py:139] 58) loss = 0.190, grad_norm = 0.211
I0405 23:59:41.269605 140267860080384 logging_writer.py:48] [59] global_step=59, grad_norm=0.206275, loss=0.183100
I0405 23:59:41.273414 140319791671104 submission.py:139] 59) loss = 0.183, grad_norm = 0.206
I0405 23:59:41.528675 140267851687680 logging_writer.py:48] [60] global_step=60, grad_norm=0.197083, loss=0.180851
I0405 23:59:41.532492 140319791671104 submission.py:139] 60) loss = 0.181, grad_norm = 0.197
I0405 23:59:41.790262 140267860080384 logging_writer.py:48] [61] global_step=61, grad_norm=0.193575, loss=0.173094
I0405 23:59:41.794105 140319791671104 submission.py:139] 61) loss = 0.173, grad_norm = 0.194
I0405 23:59:42.048271 140267851687680 logging_writer.py:48] [62] global_step=62, grad_norm=0.184537, loss=0.170503
I0405 23:59:42.052031 140319791671104 submission.py:139] 62) loss = 0.171, grad_norm = 0.185
I0405 23:59:42.309895 140267860080384 logging_writer.py:48] [63] global_step=63, grad_norm=0.179880, loss=0.165918
I0405 23:59:42.313680 140319791671104 submission.py:139] 63) loss = 0.166, grad_norm = 0.180
I0405 23:59:42.573714 140267851687680 logging_writer.py:48] [64] global_step=64, grad_norm=0.173331, loss=0.164347
I0405 23:59:42.577524 140319791671104 submission.py:139] 64) loss = 0.164, grad_norm = 0.173
I0405 23:59:42.837087 140267860080384 logging_writer.py:48] [65] global_step=65, grad_norm=0.169402, loss=0.158839
I0405 23:59:42.840854 140319791671104 submission.py:139] 65) loss = 0.159, grad_norm = 0.169
I0405 23:59:43.100192 140267851687680 logging_writer.py:48] [66] global_step=66, grad_norm=0.164074, loss=0.149587
I0405 23:59:43.104216 140319791671104 submission.py:139] 66) loss = 0.150, grad_norm = 0.164
I0405 23:59:43.375344 140267860080384 logging_writer.py:48] [67] global_step=67, grad_norm=0.160780, loss=0.150264
I0405 23:59:43.379257 140319791671104 submission.py:139] 67) loss = 0.150, grad_norm = 0.161
I0405 23:59:43.634397 140267851687680 logging_writer.py:48] [68] global_step=68, grad_norm=0.153905, loss=0.146125
I0405 23:59:43.638608 140319791671104 submission.py:139] 68) loss = 0.146, grad_norm = 0.154
I0405 23:59:43.904062 140267860080384 logging_writer.py:48] [69] global_step=69, grad_norm=0.149995, loss=0.147590
I0405 23:59:43.907915 140319791671104 submission.py:139] 69) loss = 0.148, grad_norm = 0.150
I0405 23:59:44.164034 140267851687680 logging_writer.py:48] [70] global_step=70, grad_norm=0.145476, loss=0.141071
I0405 23:59:44.168061 140319791671104 submission.py:139] 70) loss = 0.141, grad_norm = 0.145
I0405 23:59:44.427850 140267860080384 logging_writer.py:48] [71] global_step=71, grad_norm=0.140392, loss=0.135512
I0405 23:59:44.431782 140319791671104 submission.py:139] 71) loss = 0.136, grad_norm = 0.140
I0405 23:59:44.699954 140267851687680 logging_writer.py:48] [72] global_step=72, grad_norm=0.136473, loss=0.134717
I0405 23:59:44.703767 140319791671104 submission.py:139] 72) loss = 0.135, grad_norm = 0.136
I0405 23:59:44.965309 140267860080384 logging_writer.py:48] [73] global_step=73, grad_norm=0.131797, loss=0.131850
I0405 23:59:44.969104 140319791671104 submission.py:139] 73) loss = 0.132, grad_norm = 0.132
I0405 23:59:45.229545 140267851687680 logging_writer.py:48] [74] global_step=74, grad_norm=0.129362, loss=0.128478
I0405 23:59:45.233677 140319791671104 submission.py:139] 74) loss = 0.128, grad_norm = 0.129
I0405 23:59:45.492240 140267860080384 logging_writer.py:48] [75] global_step=75, grad_norm=0.125146, loss=0.132039
I0405 23:59:45.496402 140319791671104 submission.py:139] 75) loss = 0.132, grad_norm = 0.125
I0405 23:59:45.755028 140267851687680 logging_writer.py:48] [76] global_step=76, grad_norm=0.123045, loss=0.123598
I0405 23:59:45.759248 140319791671104 submission.py:139] 76) loss = 0.124, grad_norm = 0.123
I0405 23:59:46.018908 140267860080384 logging_writer.py:48] [77] global_step=77, grad_norm=0.118104, loss=0.125146
I0405 23:59:46.022661 140319791671104 submission.py:139] 77) loss = 0.125, grad_norm = 0.118
I0405 23:59:46.290697 140267851687680 logging_writer.py:48] [78] global_step=78, grad_norm=0.111289, loss=0.122468
I0405 23:59:46.294413 140319791671104 submission.py:139] 78) loss = 0.122, grad_norm = 0.111
I0405 23:59:46.557372 140267860080384 logging_writer.py:48] [79] global_step=79, grad_norm=0.109789, loss=0.117536
I0405 23:59:46.561119 140319791671104 submission.py:139] 79) loss = 0.118, grad_norm = 0.110
I0405 23:59:46.820552 140267851687680 logging_writer.py:48] [80] global_step=80, grad_norm=0.106879, loss=0.115555
I0405 23:59:46.824441 140319791671104 submission.py:139] 80) loss = 0.116, grad_norm = 0.107
I0405 23:59:47.079469 140267860080384 logging_writer.py:48] [81] global_step=81, grad_norm=0.102638, loss=0.111912
I0405 23:59:47.083369 140319791671104 submission.py:139] 81) loss = 0.112, grad_norm = 0.103
I0405 23:59:47.343032 140267851687680 logging_writer.py:48] [82] global_step=82, grad_norm=0.101503, loss=0.115279
I0405 23:59:47.347117 140319791671104 submission.py:139] 82) loss = 0.115, grad_norm = 0.102
I0405 23:59:47.605290 140267860080384 logging_writer.py:48] [83] global_step=83, grad_norm=0.099280, loss=0.110791
I0405 23:59:47.609002 140319791671104 submission.py:139] 83) loss = 0.111, grad_norm = 0.099
I0405 23:59:47.864760 140267851687680 logging_writer.py:48] [84] global_step=84, grad_norm=0.097295, loss=0.107740
I0405 23:59:47.868516 140319791671104 submission.py:139] 84) loss = 0.108, grad_norm = 0.097
I0405 23:59:48.124833 140267860080384 logging_writer.py:48] [85] global_step=85, grad_norm=0.094132, loss=0.104345
I0405 23:59:48.128652 140319791671104 submission.py:139] 85) loss = 0.104, grad_norm = 0.094
I0405 23:59:48.383112 140267851687680 logging_writer.py:48] [86] global_step=86, grad_norm=0.092767, loss=0.102180
I0405 23:59:48.387062 140319791671104 submission.py:139] 86) loss = 0.102, grad_norm = 0.093
I0405 23:59:48.649477 140267860080384 logging_writer.py:48] [87] global_step=87, grad_norm=0.089377, loss=0.105034
I0405 23:59:48.653259 140319791671104 submission.py:139] 87) loss = 0.105, grad_norm = 0.089
I0405 23:59:48.909483 140267851687680 logging_writer.py:48] [88] global_step=88, grad_norm=0.087741, loss=0.103764
I0405 23:59:48.913281 140319791671104 submission.py:139] 88) loss = 0.104, grad_norm = 0.088
I0405 23:59:49.173675 140267860080384 logging_writer.py:48] [89] global_step=89, grad_norm=0.088174, loss=0.102091
I0405 23:59:49.177648 140319791671104 submission.py:139] 89) loss = 0.102, grad_norm = 0.088
I0405 23:59:49.438212 140267851687680 logging_writer.py:48] [90] global_step=90, grad_norm=0.083878, loss=0.102516
I0405 23:59:49.442339 140319791671104 submission.py:139] 90) loss = 0.103, grad_norm = 0.084
I0405 23:59:49.702729 140267860080384 logging_writer.py:48] [91] global_step=91, grad_norm=0.080905, loss=0.101832
I0405 23:59:49.706641 140319791671104 submission.py:139] 91) loss = 0.102, grad_norm = 0.081
I0405 23:59:49.969974 140267851687680 logging_writer.py:48] [92] global_step=92, grad_norm=0.078849, loss=0.099674
I0405 23:59:49.973918 140319791671104 submission.py:139] 92) loss = 0.100, grad_norm = 0.079
I0405 23:59:50.235717 140267860080384 logging_writer.py:48] [93] global_step=93, grad_norm=0.077809, loss=0.094410
I0405 23:59:50.239564 140319791671104 submission.py:139] 93) loss = 0.094, grad_norm = 0.078
I0405 23:59:50.505384 140267851687680 logging_writer.py:48] [94] global_step=94, grad_norm=0.075226, loss=0.095085
I0405 23:59:50.509218 140319791671104 submission.py:139] 94) loss = 0.095, grad_norm = 0.075
I0405 23:59:50.768585 140267860080384 logging_writer.py:48] [95] global_step=95, grad_norm=0.074783, loss=0.091152
I0405 23:59:50.772359 140319791671104 submission.py:139] 95) loss = 0.091, grad_norm = 0.075
I0405 23:59:51.027452 140267851687680 logging_writer.py:48] [96] global_step=96, grad_norm=0.073802, loss=0.099119
I0405 23:59:51.031303 140319791671104 submission.py:139] 96) loss = 0.099, grad_norm = 0.074
I0405 23:59:51.289727 140267860080384 logging_writer.py:48] [97] global_step=97, grad_norm=0.072624, loss=0.090895
I0405 23:59:51.293478 140319791671104 submission.py:139] 97) loss = 0.091, grad_norm = 0.073
I0405 23:59:51.550136 140267851687680 logging_writer.py:48] [98] global_step=98, grad_norm=0.067600, loss=0.091948
I0405 23:59:51.554009 140319791671104 submission.py:139] 98) loss = 0.092, grad_norm = 0.068
I0405 23:59:51.815332 140267860080384 logging_writer.py:48] [99] global_step=99, grad_norm=0.067287, loss=0.091974
I0405 23:59:51.819125 140319791671104 submission.py:139] 99) loss = 0.092, grad_norm = 0.067
I0405 23:59:52.080047 140267851687680 logging_writer.py:48] [100] global_step=100, grad_norm=0.063532, loss=0.094509
I0405 23:59:52.083884 140319791671104 submission.py:139] 100) loss = 0.095, grad_norm = 0.064
I0406 00:01:33.735320 140267860080384 logging_writer.py:48] [500] global_step=500, grad_norm=0.016036, loss=0.058704
I0406 00:01:33.739923 140319791671104 submission.py:139] 500) loss = 0.059, grad_norm = 0.016
I0406 00:03:25.780880 140319791671104 submission_runner.py:373] Before eval at step 940: RAM USED (GB) 26.639110144
I0406 00:03:25.781091 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:03:40.122462 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.367188 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.368332 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.373043 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.373414 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.373520 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.373692 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:40.373892 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:53.922498 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.138515 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.141384 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.144757 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.144988 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.145863 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.150725 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:54.151074 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:04:18.985236 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:04:19.283565 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.551022 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.552034 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.556676 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.557588 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.557594 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.557620 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.557940 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.688481 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.927817 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.930516 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.931730 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.931963 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.937783 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.938237 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:19.939461 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:04:22.263432 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:04:22.563835 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.821834 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.823113 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.827834 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.828019 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.828047 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.828375 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.828475 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:22.960002 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.208733 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.212250 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.212299 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.212382 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.214032 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.215954 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:23.217876 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:04:25.509725 140319791671104 submission_runner.py:382] Time since start: 384.96s, 	Step: 940, 	{'train/accuracy': 0.9866184429947326, 'train/loss': 0.05509622395038605, 'train/mean_average_precision': 0.03388158444597807, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06435263156890869, 'validation/mean_average_precision': 0.03690384633927729, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06756363064050674, 'test/mean_average_precision': 0.03796400902033909, 'test/num_examples': 43793}
I0406 00:04:25.510190 140319791671104 submission_runner.py:396] After eval at step 940: RAM USED (GB) 27.554521088
I0406 00:04:25.517936 140267851687680 logging_writer.py:48] [940] global_step=940, preemption_count=0, score=244.609802, test/accuracy=0.983142, test/loss=0.067564, test/mean_average_precision=0.037964, test/num_examples=43793, total_duration=384.957063, train/accuracy=0.986618, train/loss=0.055096, train/mean_average_precision=0.033882, validation/accuracy=0.984118, validation/loss=0.064353, validation/mean_average_precision=0.036904, validation/num_examples=43793
I0406 00:04:25.579212 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_940.
I0406 00:04:25.579713 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 940: RAM USED (GB) 27.553447936
I0406 00:04:41.394224 140267860080384 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.073284, loss=0.053368
I0406 00:04:41.398348 140319791671104 submission.py:139] 1000) loss = 0.053, grad_norm = 0.073
I0406 00:06:48.538130 140267851687680 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.056617, loss=0.055054
I0406 00:06:48.542948 140319791671104 submission.py:139] 1500) loss = 0.055, grad_norm = 0.057
I0406 00:08:25.806609 140319791671104 submission_runner.py:373] Before eval at step 1881: RAM USED (GB) 27.907743744
I0406 00:08:25.806811 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:08:39.751367 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.043458 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.043860 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.049005 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.048872 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.049703 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.049767 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:40.049879 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.683486 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.963080 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.963229 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.963438 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.963806 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.970381 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.970548 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:53.970839 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:19.260121 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:09:19.566173 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.868920 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.870402 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.875309 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.875499 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.875663 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.875735 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:19.875842 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.006426 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.270051 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.276170 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.276251 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.276437 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.276595 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.283552 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:20.285499 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:22.612941 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:09:22.932835 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.235538 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.237055 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.241160 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.241295 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.242262 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.242550 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.242584 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.373538 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.634747 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.639971 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.640429 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.640870 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.640929 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.640952 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:23.644108 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:25.909099 140319791671104 submission_runner.py:382] Time since start: 684.98s, 	Step: 1881, 	{'train/accuracy': 0.9868329632357485, 'train/loss': 0.051937032490968704, 'train/mean_average_precision': 0.05138051089497944, 'validation/accuracy': 0.984128124627042, 'validation/loss': 0.061631906777620316, 'validation/mean_average_precision': 0.05056505745477827, 'validation/num_examples': 43793, 'test/accuracy': 0.9831547397669699, 'test/loss': 0.06489607691764832, 'test/mean_average_precision': 0.05223075359989259, 'test/num_examples': 43793}
I0406 00:09:25.909519 140319791671104 submission_runner.py:396] After eval at step 1881: RAM USED (GB) 28.7616
I0406 00:09:25.917746 140267860080384 logging_writer.py:48] [1881] global_step=1881, preemption_count=0, score=483.886936, test/accuracy=0.983155, test/loss=0.064896, test/mean_average_precision=0.052231, test/num_examples=43793, total_duration=684.982753, train/accuracy=0.986833, train/loss=0.051937, train/mean_average_precision=0.051381, validation/accuracy=0.984128, validation/loss=0.061632, validation/mean_average_precision=0.050565, validation/num_examples=43793
I0406 00:09:25.980789 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_1881.
I0406 00:09:25.981307 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 1881: RAM USED (GB) 28.73704448
I0406 00:09:56.658484 140267851687680 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.053974, loss=0.053839
I0406 00:09:56.662902 140319791671104 submission.py:139] 2000) loss = 0.054, grad_norm = 0.054
I0406 00:12:02.957917 140267860080384 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.043975, loss=0.053122
I0406 00:12:02.962553 140319791671104 submission.py:139] 2500) loss = 0.053, grad_norm = 0.044
I0406 00:13:26.322526 140319791671104 submission_runner.py:373] Before eval at step 2829: RAM USED (GB) 28.815458304
I0406 00:13:26.322751 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:13:40.211205 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.492664 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.494828 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.498608 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.498655 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.499113 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.499320 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:40.499666 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.445420 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.712035 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.717001 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.717275 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.717642 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.717785 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.718173 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:54.718196 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:21.050812 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:14:21.513293 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.798569 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.799787 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.804843 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.805047 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.805185 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.805231 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.806139 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:21.988650 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.255559 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.256676 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.259388 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.259849 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.260177 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.262739 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:22.266980 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:24.664003 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:14:25.097352 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.364457 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.365295 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.370385 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.370422 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.370385 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.370902 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.371113 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.555054 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.821079 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.822062 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.825745 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.826351 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.826543 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.827062 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:25.827606 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:28.215653 140319791671104 submission_runner.py:382] Time since start: 985.50s, 	Step: 2829, 	{'train/accuracy': 0.986964628501645, 'train/loss': 0.0493074432015419, 'train/mean_average_precision': 0.07647069842749069, 'validation/accuracy': 0.9842324513865717, 'validation/loss': 0.05906658619642258, 'validation/mean_average_precision': 0.07343331829508745, 'validation/num_examples': 43793, 'test/accuracy': 0.983261723216967, 'test/loss': 0.06220395117998123, 'test/mean_average_precision': 0.07355933635841087, 'test/num_examples': 43793}
I0406 00:14:28.216085 140319791671104 submission_runner.py:396] After eval at step 2829: RAM USED (GB) 29.427335168
I0406 00:14:28.224214 140267851687680 logging_writer.py:48] [2829] global_step=2829, preemption_count=0, score=723.186044, test/accuracy=0.983262, test/loss=0.062204, test/mean_average_precision=0.073559, test/num_examples=43793, total_duration=985.498704, train/accuracy=0.986965, train/loss=0.049307, train/mean_average_precision=0.076471, validation/accuracy=0.984232, validation/loss=0.059067, validation/mean_average_precision=0.073433, validation/num_examples=43793
I0406 00:14:28.285959 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_2829.
I0406 00:14:28.286456 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 2829: RAM USED (GB) 29.265235968
I0406 00:15:13.433495 140267860080384 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.052566, loss=0.054337
I0406 00:15:13.438897 140319791671104 submission.py:139] 3000) loss = 0.054, grad_norm = 0.053
I0406 00:17:22.929172 140267851687680 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.074461, loss=0.051047
I0406 00:17:22.934901 140319791671104 submission.py:139] 3500) loss = 0.051, grad_norm = 0.074
I0406 00:18:28.443861 140319791671104 submission_runner.py:373] Before eval at step 3753: RAM USED (GB) 29.546463232
I0406 00:18:28.444055 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:18:42.778165 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.049138 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.049332 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.054584 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.054984 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.055122 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.055138 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:43.055472 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.349104 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.599990 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.603955 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.604642 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.605597 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.606216 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.606248 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:18:57.606354 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:23.644443 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:19:24.084878 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.369681 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.369877 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.374010 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.374366 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.375121 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.375402 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.375624 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.576070 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.862400 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.866841 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.867316 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.867689 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.867949 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.868048 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:24.868596 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:27.401618 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:19:27.855963 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.122955 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.124239 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.128011 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.128455 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.128907 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.129706 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.130506 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.322643 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.585613 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.586387 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.591486 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.591518 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.591746 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.591841 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:28.592489 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:31.164643 140319791671104 submission_runner.py:382] Time since start: 1287.62s, 	Step: 3753, 	{'train/accuracy': 0.9872306792351656, 'train/loss': 0.04745419695973396, 'train/mean_average_precision': 0.0973552251893251, 'validation/accuracy': 0.9844614019405589, 'validation/loss': 0.05665910989046097, 'validation/mean_average_precision': 0.09899815965491963, 'validation/num_examples': 43793, 'test/accuracy': 0.9835018041874333, 'test/loss': 0.05970544368028641, 'test/mean_average_precision': 0.09857183551381282, 'test/num_examples': 43793}
I0406 00:19:31.165033 140319791671104 submission_runner.py:396] After eval at step 3753: RAM USED (GB) 30.095863808
I0406 00:19:31.173279 140267860080384 logging_writer.py:48] [3753] global_step=3753, preemption_count=0, score=962.432829, test/accuracy=0.983502, test/loss=0.059705, test/mean_average_precision=0.098572, test/num_examples=43793, total_duration=1287.620016, train/accuracy=0.987231, train/loss=0.047454, train/mean_average_precision=0.097355, validation/accuracy=0.984461, validation/loss=0.056659, validation/mean_average_precision=0.098998, validation/num_examples=43793
I0406 00:19:31.234762 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_3753.
I0406 00:19:31.235157 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 3753: RAM USED (GB) 30.094790656
I0406 00:20:35.214888 140267851687680 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.040642, loss=0.047267
I0406 00:20:35.220031 140319791671104 submission.py:139] 4000) loss = 0.047, grad_norm = 0.041
I0406 00:22:43.318322 140267860080384 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.049286, loss=0.045639
I0406 00:22:43.323673 140319791671104 submission.py:139] 4500) loss = 0.046, grad_norm = 0.049
I0406 00:23:31.313411 140319791671104 submission_runner.py:373] Before eval at step 4687: RAM USED (GB) 30.24068608
I0406 00:23:31.313621 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:23:45.686113 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.945530 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.945744 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.950361 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.951956 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.952442 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.952704 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:23:45.952728 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.428374 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.682992 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.683833 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.688296 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.688487 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.689174 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.689329 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:00.690675 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:29.026453 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:24:29.478534 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.753674 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.755022 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.760201 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.760266 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.760317 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.761217 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.762274 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:29.953876 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.227283 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.228007 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.232410 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.233056 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.233100 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.234536 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:30.235100 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:32.809916 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:24:33.278271 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.534989 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.535695 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.540673 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.541203 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.541238 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.542633 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.547789 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:33.726961 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.002631 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.003864 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.008003 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.009001 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.009030 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.009421 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:34.010247 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:36.585876 140319791671104 submission_runner.py:382] Time since start: 1590.49s, 	Step: 4687, 	{'train/accuracy': 0.9870325379814737, 'train/loss': 0.046341463923454285, 'train/mean_average_precision': 0.11926249266525668, 'validation/accuracy': 0.9845040257139076, 'validation/loss': 0.0549776665866375, 'validation/mean_average_precision': 0.1154109354346663, 'validation/num_examples': 43793, 'test/accuracy': 0.983511491665189, 'test/loss': 0.05795025825500488, 'test/mean_average_precision': 0.11916715338657256, 'test/num_examples': 43793}
I0406 00:24:36.586243 140319791671104 submission_runner.py:396] After eval at step 4687: RAM USED (GB) 30.709956608
I0406 00:24:36.594650 140267851687680 logging_writer.py:48] [4687] global_step=4687, preemption_count=0, score=1201.567326, test/accuracy=0.983511, test/loss=0.057950, test/mean_average_precision=0.119167, test/num_examples=43793, total_duration=1590.489537, train/accuracy=0.987033, train/loss=0.046341, train/mean_average_precision=0.119262, validation/accuracy=0.984504, validation/loss=0.054978, validation/mean_average_precision=0.115411, validation/num_examples=43793
I0406 00:24:36.658998 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_4687.
I0406 00:24:36.659405 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 4687: RAM USED (GB) 30.70887936
I0406 00:25:58.474705 140267860080384 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.098007, loss=0.044108
I0406 00:25:58.479968 140319791671104 submission.py:139] 5000) loss = 0.044, grad_norm = 0.098
I0406 00:28:09.890979 140267851687680 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.087880, loss=0.048459
I0406 00:28:09.896296 140319791671104 submission.py:139] 5500) loss = 0.048, grad_norm = 0.088
I0406 00:28:36.669313 140319791671104 submission_runner.py:373] Before eval at step 5604: RAM USED (GB) 30.898204672
I0406 00:28:36.669528 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:28:51.454330 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.710137 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.711472 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.715764 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.717051 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.717497 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.717555 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:28:51.718133 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.924303 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.181169 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.181673 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.186096 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.186843 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.187352 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.187627 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:06.187845 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:32.583982 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:29:33.037421 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.310940 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.316895 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.317211 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.317351 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.318115 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.318392 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.318912 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.488025 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.808681 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.809858 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.814230 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.814320 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.814746 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.814955 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:33.815366 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:36.388811 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:29:36.821356 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.089915 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.090876 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.096069 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.096694 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.097037 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.097179 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.097284 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.295153 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.561285 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.561609 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.566612 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.566678 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.566987 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.567020 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:37.568157 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:40.094173 140319791671104 submission_runner.py:382] Time since start: 1895.85s, 	Step: 5604, 	{'train/accuracy': 0.9874682880966389, 'train/loss': 0.04485354945063591, 'train/mean_average_precision': 0.13205002965831458, 'validation/accuracy': 0.9847934614319802, 'validation/loss': 0.05421420931816101, 'validation/mean_average_precision': 0.12681516004433893, 'validation/num_examples': 43793, 'test/accuracy': 0.9838299148470705, 'test/loss': 0.057487666606903076, 'test/mean_average_precision': 0.13025342224378808, 'test/num_examples': 43793}
I0406 00:29:40.094622 140319791671104 submission_runner.py:396] After eval at step 5604: RAM USED (GB) 31.249350656
I0406 00:29:40.102835 140267860080384 logging_writer.py:48] [5604] global_step=5604, preemption_count=0, score=1440.665998, test/accuracy=0.983830, test/loss=0.057488, test/mean_average_precision=0.130253, test/num_examples=43793, total_duration=1895.845412, train/accuracy=0.987468, train/loss=0.044854, train/mean_average_precision=0.132050, validation/accuracy=0.984793, validation/loss=0.054214, validation/mean_average_precision=0.126815, validation/num_examples=43793
I0406 00:29:40.165673 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_5604.
I0406 00:29:40.166105 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 5604: RAM USED (GB) 31.248048128
I0406 00:31:23.278405 140319791671104 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 31.313956864
I0406 00:31:23.278735 140319791671104 spec.py:298] Evaluating on the training split.
W0406 00:31:38.358959 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.618015 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.618472 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.623830 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.623988 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.624075 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.624397 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:38.624564 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.015659 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.271962 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.273107 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.277344 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.278625 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.278861 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.279105 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:53.279721 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:32:22.139024 140319791671104 spec.py:310] Evaluating on the validation split.
W0406 00:32:22.582273 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.876829 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.877690 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.882255 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.882872 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.883036 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.883334 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:22.883456 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.056765 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.320230 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.321570 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.325812 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.326474 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.326833 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.327434 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:23.327483 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:32:25.882461 140319791671104 spec.py:326] Evaluating on the test split.
W0406 00:32:26.333254 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.595189 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.596233 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.601067 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.601348 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.601904 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.602408 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.602448 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:26.795877 140319791671104 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.058628 140318548789056 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.060208 139712372315968 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.064453 140546115508032 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.064745 139861395498816 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.065255 139980197865280 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.065256 139721949435712 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:27.065698 139900306782016 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:32:29.580803 140319791671104 submission_runner.py:382] Time since start: 2062.45s, 	Step: 6000, 	{'train/accuracy': 0.9878890780223312, 'train/loss': 0.0432952344417572, 'train/mean_average_precision': 0.1484137053855274, 'validation/accuracy': 0.9850865506163398, 'validation/loss': 0.052696410566568375, 'validation/mean_average_precision': 0.14014480296955487, 'validation/num_examples': 43793, 'test/accuracy': 0.9840666262600566, 'test/loss': 0.05554327368736267, 'test/mean_average_precision': 0.13714080253779812, 'test/num_examples': 43793}
I0406 00:32:29.581207 140319791671104 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 31.622668288
I0406 00:32:29.590551 140267851687680 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1543.381794, test/accuracy=0.984067, test/loss=0.055543, test/mean_average_precision=0.137141, test/num_examples=43793, total_duration=2062.454515, train/accuracy=0.987889, train/loss=0.043295, train/mean_average_precision=0.148414, validation/accuracy=0.985087, validation/loss=0.052696, validation/mean_average_precision=0.140145, validation/num_examples=43793
I0406 00:32:29.653316 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:32:29.653743 140319791671104 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.621660672
I0406 00:32:29.661331 140267860080384 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1543.381794
I0406 00:32:29.763649 140319791671104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:32:29.930311 140319791671104 submission_runner.py:550] Tuning trial 1/1
I0406 00:32:29.930555 140319791671104 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 00:32:29.931629 140319791671104 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.559599264276412, 'train/loss': 0.7165730595588684, 'train/mean_average_precision': 0.02182539673766314, 'validation/accuracy': 0.5544530882750525, 'validation/loss': 0.7206974625587463, 'validation/mean_average_precision': 0.025711809814972657, 'validation/num_examples': 43793, 'test/accuracy': 0.5514554592938503, 'test/loss': 0.7224020957946777, 'test/mean_average_precision': 0.027600868549231536, 'test/num_examples': 43793, 'score': 5.448527812957764, 'total_duration': 5.450519800186157, 'global_step': 1, 'preemption_count': 0}), (940, {'train/accuracy': 0.9866184429947326, 'train/loss': 0.05509622395038605, 'train/mean_average_precision': 0.03388158444597807, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06435263156890869, 'validation/mean_average_precision': 0.03690384633927729, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06756363064050674, 'test/mean_average_precision': 0.03796400902033909, 'test/num_examples': 43793, 'score': 244.609801530838, 'total_duration': 384.957062959671, 'global_step': 940, 'preemption_count': 0}), (1881, {'train/accuracy': 0.9868329632357485, 'train/loss': 0.051937032490968704, 'train/mean_average_precision': 0.05138051089497944, 'validation/accuracy': 0.984128124627042, 'validation/loss': 0.061631906777620316, 'validation/mean_average_precision': 0.05056505745477827, 'validation/num_examples': 43793, 'test/accuracy': 0.9831547397669699, 'test/loss': 0.06489607691764832, 'test/mean_average_precision': 0.05223075359989259, 'test/num_examples': 43793, 'score': 483.886935710907, 'total_duration': 684.9827527999878, 'global_step': 1881, 'preemption_count': 0}), (2829, {'train/accuracy': 0.986964628501645, 'train/loss': 0.0493074432015419, 'train/mean_average_precision': 0.07647069842749069, 'validation/accuracy': 0.9842324513865717, 'validation/loss': 0.05906658619642258, 'validation/mean_average_precision': 0.07343331829508745, 'validation/num_examples': 43793, 'test/accuracy': 0.983261723216967, 'test/loss': 0.06220395117998123, 'test/mean_average_precision': 0.07355933635841087, 'test/num_examples': 43793, 'score': 723.1860435009003, 'total_duration': 985.4987044334412, 'global_step': 2829, 'preemption_count': 0}), (3753, {'train/accuracy': 0.9872306792351656, 'train/loss': 0.04745419695973396, 'train/mean_average_precision': 0.0973552251893251, 'validation/accuracy': 0.9844614019405589, 'validation/loss': 0.05665910989046097, 'validation/mean_average_precision': 0.09899815965491963, 'validation/num_examples': 43793, 'test/accuracy': 0.9835018041874333, 'test/loss': 0.05970544368028641, 'test/mean_average_precision': 0.09857183551381282, 'test/num_examples': 43793, 'score': 962.4328293800354, 'total_duration': 1287.6200156211853, 'global_step': 3753, 'preemption_count': 0}), (4687, {'train/accuracy': 0.9870325379814737, 'train/loss': 0.046341463923454285, 'train/mean_average_precision': 0.11926249266525668, 'validation/accuracy': 0.9845040257139076, 'validation/loss': 0.0549776665866375, 'validation/mean_average_precision': 0.1154109354346663, 'validation/num_examples': 43793, 'test/accuracy': 0.983511491665189, 'test/loss': 0.05795025825500488, 'test/mean_average_precision': 0.11916715338657256, 'test/num_examples': 43793, 'score': 1201.5673263072968, 'total_duration': 1590.4895374774933, 'global_step': 4687, 'preemption_count': 0}), (5604, {'train/accuracy': 0.9874682880966389, 'train/loss': 0.04485354945063591, 'train/mean_average_precision': 0.13205002965831458, 'validation/accuracy': 0.9847934614319802, 'validation/loss': 0.05421420931816101, 'validation/mean_average_precision': 0.12681516004433893, 'validation/num_examples': 43793, 'test/accuracy': 0.9838299148470705, 'test/loss': 0.057487666606903076, 'test/mean_average_precision': 0.13025342224378808, 'test/num_examples': 43793, 'score': 1440.6659977436066, 'total_duration': 1895.8454117774963, 'global_step': 5604, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9878890780223312, 'train/loss': 0.0432952344417572, 'train/mean_average_precision': 0.1484137053855274, 'validation/accuracy': 0.9850865506163398, 'validation/loss': 0.052696410566568375, 'validation/mean_average_precision': 0.14014480296955487, 'validation/num_examples': 43793, 'test/accuracy': 0.9840666262600566, 'test/loss': 0.05554327368736267, 'test/mean_average_precision': 0.13714080253779812, 'test/num_examples': 43793, 'score': 1543.3817937374115, 'total_duration': 2062.454514503479, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0406 00:32:29.931740 140319791671104 submission_runner.py:553] Timing: 1543.3817937374115
I0406 00:32:29.931789 140319791671104 submission_runner.py:554] ====================
I0406 00:32:29.931884 140319791671104 submission_runner.py:613] Final ogbg score: 1543.3817937374115
