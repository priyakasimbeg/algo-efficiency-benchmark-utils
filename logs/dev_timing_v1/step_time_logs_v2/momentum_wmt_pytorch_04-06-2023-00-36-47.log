WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 00:37:03.724588 139944276223808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 00:37:03.724634 140615425713984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 00:37:03.724563 140163090675520 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 00:37:03.725568 140309037188928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 00:37:03.725641 140622786467648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 00:37:03.725649 139969406109504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 00:37:03.725671 140038993049408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 00:37:03.736124 140309037188928 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.736001 139857252349760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 00:37:03.736237 140622786467648 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.736274 140038993049408 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.736434 139857252349760 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.736438 139969406109504 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.745510 140615425713984 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.745579 139944276223808 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:03.745566 140163090675520 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:07.666866 139857252349760 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/wmt_pytorch.
W0406 00:37:07.731415 139944276223808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.732888 140309037188928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.733946 140622786467648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.735934 140615425713984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.736307 139969406109504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.737283 140038993049408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.738431 139857252349760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:07.739164 140163090675520 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:37:07.742325 139857252349760 submission_runner.py:511] Using RNG seed 3672249416
I0406 00:37:07.743371 139857252349760 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 00:37:07.743492 139857252349760 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1.
I0406 00:37:07.743702 139857252349760 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/hparams.json.
I0406 00:37:07.744932 139857252349760 submission_runner.py:230] Starting train once: RAM USED (GB) 15.128252416
I0406 00:37:07.745055 139857252349760 submission_runner.py:231] Initializing dataset.
I0406 00:37:07.745236 139857252349760 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.1303168
I0406 00:37:07.745301 139857252349760 submission_runner.py:240] Initializing model.
I0406 00:37:11.220340 139857252349760 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.404066816
I0406 00:37:11.220528 139857252349760 submission_runner.py:252] Initializing optimizer.
I0406 00:37:11.335554 139857252349760 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.407736832
I0406 00:37:11.335747 139857252349760 submission_runner.py:261] Initializing metrics bundle.
I0406 00:37:11.335804 139857252349760 submission_runner.py:276] Initializing checkpoint and logger.
I0406 00:37:11.337575 139857252349760 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 00:37:11.337700 139857252349760 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 00:37:11.972350 139857252349760 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/meta_data_0.json.
I0406 00:37:11.973227 139857252349760 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/flags_0.json.
I0406 00:37:12.006661 139857252349760 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.460182016
I0406 00:37:12.007744 139857252349760 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.460182016
I0406 00:37:12.007863 139857252349760 submission_runner.py:313] Starting training loop.
I0406 00:37:12.018186 139857252349760 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:12.021576 139857252349760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:37:12.021687 139857252349760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:37:12.075914 139857252349760 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:14.232252 139857252349760 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 19.756146688
I0406 00:37:15.706728 139809616021248 logging_writer.py:48] [0] global_step=0, grad_norm=4.974663, loss=11.102160
I0406 00:37:15.711252 139857252349760 submission.py:139] 0) loss = 11.102, grad_norm = 4.975
I0406 00:37:15.712250 139857252349760 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 19.85026048
I0406 00:37:15.712779 139857252349760 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 19.85026048
I0406 00:37:15.712883 139857252349760 spec.py:298] Evaluating on the training split.
I0406 00:37:15.714811 139857252349760 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:15.717235 139857252349760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:37:15.717344 139857252349760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:37:15.744864 139857252349760 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:19.915776 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 00:41:53.916086 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 00:41:53.919620 139857252349760 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:41:53.923002 139857252349760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:41:53.923118 139857252349760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:41:53.952692 139857252349760 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:41:57.825194 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 00:46:26.382152 139857252349760 spec.py:326] Evaluating on the test split.
I0406 00:46:26.384880 139857252349760 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:46:26.387967 139857252349760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:46:26.388076 139857252349760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:46:26.415417 139857252349760 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:46:30.324775 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 00:51:04.289894 139857252349760 submission_runner.py:382] Time since start: 3.71s, 	Step: 1, 	{'train/accuracy': 0.0005381638308104518, 'train/loss': 11.100489213822796, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.11926928990341, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.107557666608564, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0406 00:51:04.290378 139857252349760 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.211757056
I0406 00:51:04.298504 139799540086528 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.703533, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.107558, test/num_examples=3003, total_duration=3.705551, train/accuracy=0.000538, train/bleu=0.000000, train/loss=11.100489, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.119269, validation/num_examples=3000
I0406 00:51:05.806220 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_1.
I0406 00:51:05.806838 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.210999296
I0406 00:51:05.811984 139857252349760 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.210982912
I0406 00:51:05.816745 139857252349760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.816768 140615425713984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.816799 140309037188928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.816787 139944276223808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.816788 140622786467648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.816850 139969406109504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.816886 140163090675520 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:05.817191 140038993049408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:06.241640 139799531693824 logging_writer.py:48] [1] global_step=1, grad_norm=4.976899, loss=11.108882
I0406 00:51:06.244732 139857252349760 submission.py:139] 1) loss = 11.109, grad_norm = 4.977
I0406 00:51:06.245505 139857252349760 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.210991104
I0406 00:51:06.678378 139799540086528 logging_writer.py:48] [2] global_step=2, grad_norm=4.992937, loss=11.094954
I0406 00:51:06.681417 139857252349760 submission.py:139] 2) loss = 11.095, grad_norm = 4.993
I0406 00:51:07.109848 139799531693824 logging_writer.py:48] [3] global_step=3, grad_norm=4.890993, loss=11.088386
I0406 00:51:07.112818 139857252349760 submission.py:139] 3) loss = 11.088, grad_norm = 4.891
I0406 00:51:07.546984 139799540086528 logging_writer.py:48] [4] global_step=4, grad_norm=4.874815, loss=11.078138
I0406 00:51:07.550162 139857252349760 submission.py:139] 4) loss = 11.078, grad_norm = 4.875
I0406 00:51:07.979753 139799531693824 logging_writer.py:48] [5] global_step=5, grad_norm=4.833074, loss=11.042589
I0406 00:51:07.982811 139857252349760 submission.py:139] 5) loss = 11.043, grad_norm = 4.833
I0406 00:51:08.413642 139799540086528 logging_writer.py:48] [6] global_step=6, grad_norm=4.698586, loss=11.003754
I0406 00:51:08.416723 139857252349760 submission.py:139] 6) loss = 11.004, grad_norm = 4.699
I0406 00:51:08.851315 139799531693824 logging_writer.py:48] [7] global_step=7, grad_norm=4.466519, loss=10.966349
I0406 00:51:08.854399 139857252349760 submission.py:139] 7) loss = 10.966, grad_norm = 4.467
I0406 00:51:09.282572 139799540086528 logging_writer.py:48] [8] global_step=8, grad_norm=4.298029, loss=10.903159
I0406 00:51:09.285805 139857252349760 submission.py:139] 8) loss = 10.903, grad_norm = 4.298
I0406 00:51:09.720771 139799531693824 logging_writer.py:48] [9] global_step=9, grad_norm=4.076530, loss=10.832693
I0406 00:51:09.724418 139857252349760 submission.py:139] 9) loss = 10.833, grad_norm = 4.077
I0406 00:51:10.153149 139799540086528 logging_writer.py:48] [10] global_step=10, grad_norm=3.789122, loss=10.762215
I0406 00:51:10.156727 139857252349760 submission.py:139] 10) loss = 10.762, grad_norm = 3.789
I0406 00:51:10.586585 139799531693824 logging_writer.py:48] [11] global_step=11, grad_norm=3.495692, loss=10.681311
I0406 00:51:10.589693 139857252349760 submission.py:139] 11) loss = 10.681, grad_norm = 3.496
I0406 00:51:11.024093 139799540086528 logging_writer.py:48] [12] global_step=12, grad_norm=3.200530, loss=10.596843
I0406 00:51:11.027123 139857252349760 submission.py:139] 12) loss = 10.597, grad_norm = 3.201
I0406 00:51:11.456621 139799531693824 logging_writer.py:48] [13] global_step=13, grad_norm=2.948413, loss=10.504065
I0406 00:51:11.459614 139857252349760 submission.py:139] 13) loss = 10.504, grad_norm = 2.948
I0406 00:51:11.890286 139799540086528 logging_writer.py:48] [14] global_step=14, grad_norm=2.704563, loss=10.424667
I0406 00:51:11.893631 139857252349760 submission.py:139] 14) loss = 10.425, grad_norm = 2.705
I0406 00:51:12.323832 139799531693824 logging_writer.py:48] [15] global_step=15, grad_norm=2.438854, loss=10.367270
I0406 00:51:12.326894 139857252349760 submission.py:139] 15) loss = 10.367, grad_norm = 2.439
I0406 00:51:12.756846 139799540086528 logging_writer.py:48] [16] global_step=16, grad_norm=2.277647, loss=10.301042
I0406 00:51:12.760027 139857252349760 submission.py:139] 16) loss = 10.301, grad_norm = 2.278
I0406 00:51:13.190423 139799531693824 logging_writer.py:48] [17] global_step=17, grad_norm=2.073421, loss=10.223165
I0406 00:51:13.193513 139857252349760 submission.py:139] 17) loss = 10.223, grad_norm = 2.073
I0406 00:51:13.623519 139799540086528 logging_writer.py:48] [18] global_step=18, grad_norm=1.939643, loss=10.167224
I0406 00:51:13.627255 139857252349760 submission.py:139] 18) loss = 10.167, grad_norm = 1.940
I0406 00:51:14.059932 139799531693824 logging_writer.py:48] [19] global_step=19, grad_norm=1.838415, loss=10.090718
I0406 00:51:14.063035 139857252349760 submission.py:139] 19) loss = 10.091, grad_norm = 1.838
I0406 00:51:14.490699 139799540086528 logging_writer.py:48] [20] global_step=20, grad_norm=1.735819, loss=10.059217
I0406 00:51:14.493751 139857252349760 submission.py:139] 20) loss = 10.059, grad_norm = 1.736
I0406 00:51:14.926696 139799531693824 logging_writer.py:48] [21] global_step=21, grad_norm=1.663398, loss=10.007665
I0406 00:51:14.930111 139857252349760 submission.py:139] 21) loss = 10.008, grad_norm = 1.663
I0406 00:51:15.357618 139799540086528 logging_writer.py:48] [22] global_step=22, grad_norm=1.599368, loss=9.942949
I0406 00:51:15.360702 139857252349760 submission.py:139] 22) loss = 9.943, grad_norm = 1.599
I0406 00:51:15.794100 139799531693824 logging_writer.py:48] [23] global_step=23, grad_norm=1.533849, loss=9.920366
I0406 00:51:15.797484 139857252349760 submission.py:139] 23) loss = 9.920, grad_norm = 1.534
I0406 00:51:16.232009 139799540086528 logging_writer.py:48] [24] global_step=24, grad_norm=1.455228, loss=9.867372
I0406 00:51:16.235213 139857252349760 submission.py:139] 24) loss = 9.867, grad_norm = 1.455
I0406 00:51:16.665277 139799531693824 logging_writer.py:48] [25] global_step=25, grad_norm=1.383719, loss=9.837055
I0406 00:51:16.668659 139857252349760 submission.py:139] 25) loss = 9.837, grad_norm = 1.384
I0406 00:51:17.100461 139799540086528 logging_writer.py:48] [26] global_step=26, grad_norm=1.304547, loss=9.784362
I0406 00:51:17.103703 139857252349760 submission.py:139] 26) loss = 9.784, grad_norm = 1.305
I0406 00:51:17.532887 139799531693824 logging_writer.py:48] [27] global_step=27, grad_norm=1.202570, loss=9.753154
I0406 00:51:17.535938 139857252349760 submission.py:139] 27) loss = 9.753, grad_norm = 1.203
I0406 00:51:17.969433 139799540086528 logging_writer.py:48] [28] global_step=28, grad_norm=1.130072, loss=9.731531
I0406 00:51:17.972677 139857252349760 submission.py:139] 28) loss = 9.732, grad_norm = 1.130
I0406 00:51:18.407012 139799531693824 logging_writer.py:48] [29] global_step=29, grad_norm=1.073337, loss=9.680797
I0406 00:51:18.410169 139857252349760 submission.py:139] 29) loss = 9.681, grad_norm = 1.073
I0406 00:51:18.839890 139799540086528 logging_writer.py:48] [30] global_step=30, grad_norm=0.985271, loss=9.665679
I0406 00:51:18.843063 139857252349760 submission.py:139] 30) loss = 9.666, grad_norm = 0.985
I0406 00:51:19.275062 139799531693824 logging_writer.py:48] [31] global_step=31, grad_norm=0.935565, loss=9.634299
I0406 00:51:19.278136 139857252349760 submission.py:139] 31) loss = 9.634, grad_norm = 0.936
I0406 00:51:19.706866 139799540086528 logging_writer.py:48] [32] global_step=32, grad_norm=0.945520, loss=9.597102
I0406 00:51:19.709905 139857252349760 submission.py:139] 32) loss = 9.597, grad_norm = 0.946
I0406 00:51:20.143893 139799531693824 logging_writer.py:48] [33] global_step=33, grad_norm=0.890905, loss=9.585831
I0406 00:51:20.146952 139857252349760 submission.py:139] 33) loss = 9.586, grad_norm = 0.891
I0406 00:51:20.575388 139799540086528 logging_writer.py:48] [34] global_step=34, grad_norm=0.882212, loss=9.565813
I0406 00:51:20.578498 139857252349760 submission.py:139] 34) loss = 9.566, grad_norm = 0.882
I0406 00:51:21.009964 139799531693824 logging_writer.py:48] [35] global_step=35, grad_norm=0.841782, loss=9.560440
I0406 00:51:21.013041 139857252349760 submission.py:139] 35) loss = 9.560, grad_norm = 0.842
I0406 00:51:21.446821 139799540086528 logging_writer.py:48] [36] global_step=36, grad_norm=0.801303, loss=9.532047
I0406 00:51:21.449797 139857252349760 submission.py:139] 36) loss = 9.532, grad_norm = 0.801
I0406 00:51:21.879028 139799531693824 logging_writer.py:48] [37] global_step=37, grad_norm=0.812480, loss=9.507313
I0406 00:51:21.882016 139857252349760 submission.py:139] 37) loss = 9.507, grad_norm = 0.812
I0406 00:51:22.311930 139799540086528 logging_writer.py:48] [38] global_step=38, grad_norm=0.774644, loss=9.492380
I0406 00:51:22.315052 139857252349760 submission.py:139] 38) loss = 9.492, grad_norm = 0.775
I0406 00:51:22.743855 139799531693824 logging_writer.py:48] [39] global_step=39, grad_norm=0.726315, loss=9.506533
I0406 00:51:22.746803 139857252349760 submission.py:139] 39) loss = 9.507, grad_norm = 0.726
I0406 00:51:23.177811 139799540086528 logging_writer.py:48] [40] global_step=40, grad_norm=0.716396, loss=9.468645
I0406 00:51:23.180888 139857252349760 submission.py:139] 40) loss = 9.469, grad_norm = 0.716
I0406 00:51:23.612772 139799531693824 logging_writer.py:48] [41] global_step=41, grad_norm=0.680055, loss=9.476347
I0406 00:51:23.615888 139857252349760 submission.py:139] 41) loss = 9.476, grad_norm = 0.680
I0406 00:51:24.047671 139799540086528 logging_writer.py:48] [42] global_step=42, grad_norm=0.649611, loss=9.427452
I0406 00:51:24.051080 139857252349760 submission.py:139] 42) loss = 9.427, grad_norm = 0.650
I0406 00:51:24.483394 139799531693824 logging_writer.py:48] [43] global_step=43, grad_norm=0.640302, loss=9.406552
I0406 00:51:24.486467 139857252349760 submission.py:139] 43) loss = 9.407, grad_norm = 0.640
I0406 00:51:24.917196 139799540086528 logging_writer.py:48] [44] global_step=44, grad_norm=0.601066, loss=9.399270
I0406 00:51:24.920376 139857252349760 submission.py:139] 44) loss = 9.399, grad_norm = 0.601
I0406 00:51:25.352712 139799531693824 logging_writer.py:48] [45] global_step=45, grad_norm=0.581978, loss=9.362425
I0406 00:51:25.355909 139857252349760 submission.py:139] 45) loss = 9.362, grad_norm = 0.582
I0406 00:51:25.792520 139799540086528 logging_writer.py:48] [46] global_step=46, grad_norm=0.550201, loss=9.359109
I0406 00:51:25.795628 139857252349760 submission.py:139] 46) loss = 9.359, grad_norm = 0.550
I0406 00:51:26.229301 139799531693824 logging_writer.py:48] [47] global_step=47, grad_norm=0.536256, loss=9.377808
I0406 00:51:26.232423 139857252349760 submission.py:139] 47) loss = 9.378, grad_norm = 0.536
I0406 00:51:26.662775 139799540086528 logging_writer.py:48] [48] global_step=48, grad_norm=0.519472, loss=9.334274
I0406 00:51:26.665939 139857252349760 submission.py:139] 48) loss = 9.334, grad_norm = 0.519
I0406 00:51:27.100242 139799531693824 logging_writer.py:48] [49] global_step=49, grad_norm=0.488909, loss=9.333942
I0406 00:51:27.103245 139857252349760 submission.py:139] 49) loss = 9.334, grad_norm = 0.489
I0406 00:51:27.533780 139799540086528 logging_writer.py:48] [50] global_step=50, grad_norm=0.499141, loss=9.311572
I0406 00:51:27.536906 139857252349760 submission.py:139] 50) loss = 9.312, grad_norm = 0.499
I0406 00:51:27.965577 139799531693824 logging_writer.py:48] [51] global_step=51, grad_norm=0.494238, loss=9.291500
I0406 00:51:27.968733 139857252349760 submission.py:139] 51) loss = 9.292, grad_norm = 0.494
I0406 00:51:28.401813 139799540086528 logging_writer.py:48] [52] global_step=52, grad_norm=0.490738, loss=9.300673
I0406 00:51:28.404940 139857252349760 submission.py:139] 52) loss = 9.301, grad_norm = 0.491
I0406 00:51:28.836573 139799531693824 logging_writer.py:48] [53] global_step=53, grad_norm=0.467006, loss=9.310657
I0406 00:51:28.839844 139857252349760 submission.py:139] 53) loss = 9.311, grad_norm = 0.467
I0406 00:51:29.274212 139799540086528 logging_writer.py:48] [54] global_step=54, grad_norm=0.466985, loss=9.289890
I0406 00:51:29.277697 139857252349760 submission.py:139] 54) loss = 9.290, grad_norm = 0.467
I0406 00:51:29.714491 139799531693824 logging_writer.py:48] [55] global_step=55, grad_norm=0.451547, loss=9.322900
I0406 00:51:29.717778 139857252349760 submission.py:139] 55) loss = 9.323, grad_norm = 0.452
I0406 00:51:30.146049 139799540086528 logging_writer.py:48] [56] global_step=56, grad_norm=0.444307, loss=9.270745
I0406 00:51:30.149380 139857252349760 submission.py:139] 56) loss = 9.271, grad_norm = 0.444
I0406 00:51:30.581379 139799531693824 logging_writer.py:48] [57] global_step=57, grad_norm=0.413399, loss=9.272604
I0406 00:51:30.584431 139857252349760 submission.py:139] 57) loss = 9.273, grad_norm = 0.413
I0406 00:51:31.013581 139799540086528 logging_writer.py:48] [58] global_step=58, grad_norm=0.388161, loss=9.272617
I0406 00:51:31.016838 139857252349760 submission.py:139] 58) loss = 9.273, grad_norm = 0.388
I0406 00:51:31.446768 139799531693824 logging_writer.py:48] [59] global_step=59, grad_norm=0.381410, loss=9.231000
I0406 00:51:31.449865 139857252349760 submission.py:139] 59) loss = 9.231, grad_norm = 0.381
I0406 00:51:31.886336 139799540086528 logging_writer.py:48] [60] global_step=60, grad_norm=0.360438, loss=9.257779
I0406 00:51:31.889748 139857252349760 submission.py:139] 60) loss = 9.258, grad_norm = 0.360
I0406 00:51:32.320700 139799531693824 logging_writer.py:48] [61] global_step=61, grad_norm=0.365389, loss=9.237416
I0406 00:51:32.323783 139857252349760 submission.py:139] 61) loss = 9.237, grad_norm = 0.365
I0406 00:51:32.756129 139799540086528 logging_writer.py:48] [62] global_step=62, grad_norm=0.358265, loss=9.233242
I0406 00:51:32.759424 139857252349760 submission.py:139] 62) loss = 9.233, grad_norm = 0.358
I0406 00:51:33.188519 139799531693824 logging_writer.py:48] [63] global_step=63, grad_norm=0.357913, loss=9.240974
I0406 00:51:33.191561 139857252349760 submission.py:139] 63) loss = 9.241, grad_norm = 0.358
I0406 00:51:33.626604 139799540086528 logging_writer.py:48] [64] global_step=64, grad_norm=0.353731, loss=9.225580
I0406 00:51:33.629700 139857252349760 submission.py:139] 64) loss = 9.226, grad_norm = 0.354
I0406 00:51:34.060972 139799531693824 logging_writer.py:48] [65] global_step=65, grad_norm=0.347297, loss=9.218733
I0406 00:51:34.064204 139857252349760 submission.py:139] 65) loss = 9.219, grad_norm = 0.347
I0406 00:51:34.498981 139799540086528 logging_writer.py:48] [66] global_step=66, grad_norm=0.355012, loss=9.184321
I0406 00:51:34.502094 139857252349760 submission.py:139] 66) loss = 9.184, grad_norm = 0.355
I0406 00:51:34.934795 139799531693824 logging_writer.py:48] [67] global_step=67, grad_norm=0.330665, loss=9.186727
I0406 00:51:34.937942 139857252349760 submission.py:139] 67) loss = 9.187, grad_norm = 0.331
I0406 00:51:35.367729 139799540086528 logging_writer.py:48] [68] global_step=68, grad_norm=0.316758, loss=9.175446
I0406 00:51:35.370975 139857252349760 submission.py:139] 68) loss = 9.175, grad_norm = 0.317
I0406 00:51:35.805830 139799531693824 logging_writer.py:48] [69] global_step=69, grad_norm=0.315251, loss=9.164798
I0406 00:51:35.809226 139857252349760 submission.py:139] 69) loss = 9.165, grad_norm = 0.315
I0406 00:51:36.240376 139799540086528 logging_writer.py:48] [70] global_step=70, grad_norm=0.293471, loss=9.169660
I0406 00:51:36.243551 139857252349760 submission.py:139] 70) loss = 9.170, grad_norm = 0.293
I0406 00:51:36.676964 139799531693824 logging_writer.py:48] [71] global_step=71, grad_norm=0.289446, loss=9.184008
I0406 00:51:36.680065 139857252349760 submission.py:139] 71) loss = 9.184, grad_norm = 0.289
I0406 00:51:37.110643 139799540086528 logging_writer.py:48] [72] global_step=72, grad_norm=0.278475, loss=9.171038
I0406 00:51:37.113684 139857252349760 submission.py:139] 72) loss = 9.171, grad_norm = 0.278
I0406 00:51:37.565987 139799531693824 logging_writer.py:48] [73] global_step=73, grad_norm=0.263590, loss=9.147591
I0406 00:51:37.569437 139857252349760 submission.py:139] 73) loss = 9.148, grad_norm = 0.264
I0406 00:51:38.008928 139799540086528 logging_writer.py:48] [74] global_step=74, grad_norm=0.256461, loss=9.157078
I0406 00:51:38.012161 139857252349760 submission.py:139] 74) loss = 9.157, grad_norm = 0.256
I0406 00:51:38.448797 139799531693824 logging_writer.py:48] [75] global_step=75, grad_norm=0.249908, loss=9.149281
I0406 00:51:38.451920 139857252349760 submission.py:139] 75) loss = 9.149, grad_norm = 0.250
I0406 00:51:38.881697 139799540086528 logging_writer.py:48] [76] global_step=76, grad_norm=0.250284, loss=9.148194
I0406 00:51:38.884648 139857252349760 submission.py:139] 76) loss = 9.148, grad_norm = 0.250
I0406 00:51:39.316544 139799531693824 logging_writer.py:48] [77] global_step=77, grad_norm=0.257033, loss=9.125308
I0406 00:51:39.319659 139857252349760 submission.py:139] 77) loss = 9.125, grad_norm = 0.257
I0406 00:51:39.748935 139799540086528 logging_writer.py:48] [78] global_step=78, grad_norm=0.250008, loss=9.126498
I0406 00:51:39.752160 139857252349760 submission.py:139] 78) loss = 9.126, grad_norm = 0.250
I0406 00:51:40.182001 139799531693824 logging_writer.py:48] [79] global_step=79, grad_norm=0.245924, loss=9.141315
I0406 00:51:40.185057 139857252349760 submission.py:139] 79) loss = 9.141, grad_norm = 0.246
I0406 00:51:40.618405 139799540086528 logging_writer.py:48] [80] global_step=80, grad_norm=0.254683, loss=9.127895
I0406 00:51:40.621652 139857252349760 submission.py:139] 80) loss = 9.128, grad_norm = 0.255
I0406 00:51:41.050154 139799531693824 logging_writer.py:48] [81] global_step=81, grad_norm=0.243250, loss=9.099157
I0406 00:51:41.053199 139857252349760 submission.py:139] 81) loss = 9.099, grad_norm = 0.243
I0406 00:51:41.483306 139799540086528 logging_writer.py:48] [82] global_step=82, grad_norm=0.245530, loss=9.099916
I0406 00:51:41.486421 139857252349760 submission.py:139] 82) loss = 9.100, grad_norm = 0.246
I0406 00:51:41.915107 139799531693824 logging_writer.py:48] [83] global_step=83, grad_norm=0.223887, loss=9.130582
I0406 00:51:41.918468 139857252349760 submission.py:139] 83) loss = 9.131, grad_norm = 0.224
I0406 00:51:42.348899 139799540086528 logging_writer.py:48] [84] global_step=84, grad_norm=0.235081, loss=9.117531
I0406 00:51:42.352053 139857252349760 submission.py:139] 84) loss = 9.118, grad_norm = 0.235
I0406 00:51:42.789451 139799531693824 logging_writer.py:48] [85] global_step=85, grad_norm=0.217097, loss=9.109890
I0406 00:51:42.792562 139857252349760 submission.py:139] 85) loss = 9.110, grad_norm = 0.217
I0406 00:51:43.224311 139799540086528 logging_writer.py:48] [86] global_step=86, grad_norm=0.218994, loss=9.128742
I0406 00:51:43.227355 139857252349760 submission.py:139] 86) loss = 9.129, grad_norm = 0.219
I0406 00:51:43.659055 139799531693824 logging_writer.py:48] [87] global_step=87, grad_norm=0.213871, loss=9.088903
I0406 00:51:43.662137 139857252349760 submission.py:139] 87) loss = 9.089, grad_norm = 0.214
I0406 00:51:44.090754 139799540086528 logging_writer.py:48] [88] global_step=88, grad_norm=0.211506, loss=9.072536
I0406 00:51:44.093771 139857252349760 submission.py:139] 88) loss = 9.073, grad_norm = 0.212
I0406 00:51:44.523972 139799531693824 logging_writer.py:48] [89] global_step=89, grad_norm=0.205709, loss=9.073189
I0406 00:51:44.527075 139857252349760 submission.py:139] 89) loss = 9.073, grad_norm = 0.206
I0406 00:51:44.955417 139799540086528 logging_writer.py:48] [90] global_step=90, grad_norm=0.202703, loss=9.105159
I0406 00:51:44.958384 139857252349760 submission.py:139] 90) loss = 9.105, grad_norm = 0.203
I0406 00:51:45.389831 139799531693824 logging_writer.py:48] [91] global_step=91, grad_norm=0.196893, loss=9.075087
I0406 00:51:45.392964 139857252349760 submission.py:139] 91) loss = 9.075, grad_norm = 0.197
I0406 00:51:45.825318 139799540086528 logging_writer.py:48] [92] global_step=92, grad_norm=0.201035, loss=9.116568
I0406 00:51:45.828225 139857252349760 submission.py:139] 92) loss = 9.117, grad_norm = 0.201
I0406 00:51:46.257861 139799531693824 logging_writer.py:48] [93] global_step=93, grad_norm=0.195976, loss=9.088365
I0406 00:51:46.261145 139857252349760 submission.py:139] 93) loss = 9.088, grad_norm = 0.196
I0406 00:51:46.691798 139799540086528 logging_writer.py:48] [94] global_step=94, grad_norm=0.191322, loss=9.090981
I0406 00:51:46.694970 139857252349760 submission.py:139] 94) loss = 9.091, grad_norm = 0.191
I0406 00:51:47.126373 139799531693824 logging_writer.py:48] [95] global_step=95, grad_norm=0.191809, loss=9.071750
I0406 00:51:47.129678 139857252349760 submission.py:139] 95) loss = 9.072, grad_norm = 0.192
I0406 00:51:47.559833 139799540086528 logging_writer.py:48] [96] global_step=96, grad_norm=0.195638, loss=9.042237
I0406 00:51:47.562890 139857252349760 submission.py:139] 96) loss = 9.042, grad_norm = 0.196
I0406 00:51:47.995620 139799531693824 logging_writer.py:48] [97] global_step=97, grad_norm=0.180002, loss=9.081255
I0406 00:51:47.998720 139857252349760 submission.py:139] 97) loss = 9.081, grad_norm = 0.180
I0406 00:51:48.427603 139799540086528 logging_writer.py:48] [98] global_step=98, grad_norm=0.181210, loss=9.083286
I0406 00:51:48.430971 139857252349760 submission.py:139] 98) loss = 9.083, grad_norm = 0.181
I0406 00:51:48.864955 139799531693824 logging_writer.py:48] [99] global_step=99, grad_norm=0.171883, loss=9.074517
I0406 00:51:48.868160 139857252349760 submission.py:139] 99) loss = 9.075, grad_norm = 0.172
I0406 00:51:49.297202 139799540086528 logging_writer.py:48] [100] global_step=100, grad_norm=0.177533, loss=9.069133
I0406 00:51:49.300350 139857252349760 submission.py:139] 100) loss = 9.069, grad_norm = 0.178
I0406 00:54:39.374847 139799531693824 logging_writer.py:48] [500] global_step=500, grad_norm=0.488338, loss=8.455377
I0406 00:54:39.378237 139857252349760 submission.py:139] 500) loss = 8.455, grad_norm = 0.488
I0406 00:58:12.544388 139799540086528 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.746942, loss=7.835585
I0406 00:58:12.547691 139857252349760 submission.py:139] 1000) loss = 7.836, grad_norm = 0.747
I0406 01:01:45.768333 139799531693824 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.696748, loss=7.404830
I0406 01:01:45.771797 139857252349760 submission.py:139] 1500) loss = 7.405, grad_norm = 0.697
I0406 01:05:05.811599 139857252349760 submission_runner.py:373] Before eval at step 1970: RAM USED (GB) 20.615725056
I0406 01:05:05.811890 139857252349760 spec.py:298] Evaluating on the training split.
I0406 01:05:09.714122 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:09:41.289843 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 01:09:45.030575 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:14:10.524370 139857252349760 spec.py:326] Evaluating on the test split.
I0406 01:14:14.346508 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:18:45.957674 139857252349760 submission_runner.py:382] Time since start: 1673.80s, 	Step: 1970, 	{'train/accuracy': 0.2836583849501485, 'train/loss': 5.899011173409473, 'train/bleu': 5.250842650827347, 'validation/accuracy': 0.2587072695936814, 'validation/loss': 6.184268096489814, 'validation/bleu': 2.6500455602103106, 'validation/num_examples': 3000, 'test/accuracy': 0.23899831503108476, 'test/loss': 6.469549270815176, 'test/bleu': 1.798571684994419, 'test/num_examples': 3003}
I0406 01:18:45.958187 139857252349760 submission_runner.py:396] After eval at step 1970: RAM USED (GB) 20.788772864
I0406 01:18:45.967584 139799540086528 logging_writer.py:48] [1970] global_step=1970, preemption_count=0, score=838.324036, test/accuracy=0.238998, test/bleu=1.798572, test/loss=6.469549, test/num_examples=3003, total_duration=1673.802799, train/accuracy=0.283658, train/bleu=5.250843, train/loss=5.899011, validation/accuracy=0.258707, validation/bleu=2.650046, validation/loss=6.184268, validation/num_examples=3000
I0406 01:18:47.546437 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_1970.
I0406 01:18:47.547187 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 1970: RAM USED (GB) 20.788948992
I0406 01:19:00.724956 139799531693824 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.639232, loss=7.027118
I0406 01:19:00.728936 139857252349760 submission.py:139] 2000) loss = 7.027, grad_norm = 0.639
I0406 01:22:33.508147 139799540086528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.635131, loss=6.737422
I0406 01:22:33.512194 139857252349760 submission.py:139] 2500) loss = 6.737, grad_norm = 0.635
I0406 01:26:06.924049 139799531693824 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.814068, loss=6.452132
I0406 01:26:06.927795 139857252349760 submission.py:139] 3000) loss = 6.452, grad_norm = 0.814
I0406 01:29:40.109424 139799540086528 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.602482, loss=6.019492
I0406 01:29:40.112754 139857252349760 submission.py:139] 3500) loss = 6.019, grad_norm = 0.602
I0406 01:32:47.621330 139857252349760 submission_runner.py:373] Before eval at step 3941: RAM USED (GB) 20.87911424
I0406 01:32:47.621552 139857252349760 spec.py:298] Evaluating on the training split.
I0406 01:32:51.514775 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:36:18.344377 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 01:36:22.098030 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:39:18.759366 139857252349760 spec.py:326] Evaluating on the test split.
I0406 01:39:22.581899 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:42:07.391295 139857252349760 submission_runner.py:382] Time since start: 3335.61s, 	Step: 3941, 	{'train/accuracy': 0.41132535890623395, 'train/loss': 4.365484281104257, 'train/bleu': 13.288880906145836, 'validation/accuracy': 0.39656048902059493, 'validation/loss': 4.490167899344088, 'validation/bleu': 9.12641697333711, 'validation/num_examples': 3000, 'test/accuracy': 0.38047760153390275, 'test/loss': 4.676311661146941, 'test/bleu': 7.708041940789616, 'test/num_examples': 3003}
I0406 01:42:07.391817 139857252349760 submission_runner.py:396] After eval at step 3941: RAM USED (GB) 21.049380864
I0406 01:42:07.401690 139799531693824 logging_writer.py:48] [3941] global_step=3941, preemption_count=0, score=1672.723812, test/accuracy=0.380478, test/bleu=7.708042, test/loss=4.676312, test/num_examples=3003, total_duration=3335.611250, train/accuracy=0.411325, train/bleu=13.288881, train/loss=4.365484, validation/accuracy=0.396560, validation/bleu=9.126417, validation/loss=4.490168, validation/num_examples=3000
I0406 01:42:09.059291 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_3941.
I0406 01:42:09.060015 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 3941: RAM USED (GB) 21.049356288
I0406 01:42:34.582199 139799540086528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.771157, loss=5.985714
I0406 01:42:34.586269 139857252349760 submission.py:139] 4000) loss = 5.986, grad_norm = 0.771
I0406 01:46:07.906064 139799531693824 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.648174, loss=5.670605
I0406 01:46:07.909265 139857252349760 submission.py:139] 4500) loss = 5.671, grad_norm = 0.648
I0406 01:49:40.974242 139799540086528 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.522023, loss=5.441041
I0406 01:49:40.977619 139857252349760 submission.py:139] 5000) loss = 5.441, grad_norm = 0.522
I0406 01:53:14.187122 139799531693824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.565078, loss=5.394195
I0406 01:53:14.190472 139857252349760 submission.py:139] 5500) loss = 5.394, grad_norm = 0.565
I0406 01:56:09.192075 139857252349760 submission_runner.py:373] Before eval at step 5911: RAM USED (GB) 21.109317632
I0406 01:56:09.192346 139857252349760 spec.py:298] Evaluating on the training split.
I0406 01:56:13.089899 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 01:58:55.409954 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 01:58:59.156616 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:01:21.159536 139857252349760 spec.py:326] Evaluating on the test split.
I0406 02:01:24.990695 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:03:40.576599 139857252349760 submission_runner.py:382] Time since start: 4737.18s, 	Step: 5911, 	{'train/accuracy': 0.506252372680525, 'train/loss': 3.458258078617691, 'train/bleu': 21.777561439213102, 'validation/accuracy': 0.50439548176712, 'validation/loss': 3.445240139613892, 'validation/bleu': 17.44384180804739, 'validation/num_examples': 3000, 'test/accuracy': 0.4998315031084771, 'test/loss': 3.532167291267213, 'test/bleu': 15.64117081901616, 'test/num_examples': 3003}
I0406 02:03:40.577126 139857252349760 submission_runner.py:396] After eval at step 5911: RAM USED (GB) 21.184667648
I0406 02:03:40.587056 139799540086528 logging_writer.py:48] [5911] global_step=5911, preemption_count=0, score=2507.062198, test/accuracy=0.499832, test/bleu=15.641171, test/loss=3.532167, test/num_examples=3003, total_duration=4737.181319, train/accuracy=0.506252, train/bleu=21.777561, train/loss=3.458258, validation/accuracy=0.504395, validation/bleu=17.443842, validation/loss=3.445240, validation/num_examples=3000
I0406 02:03:42.222214 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_5911.
I0406 02:03:42.222954 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 5911: RAM USED (GB) 21.183647744
I0406 02:04:20.487367 139799531693824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.527574, loss=5.188956
I0406 02:04:20.490766 139857252349760 submission.py:139] 6000) loss = 5.189, grad_norm = 0.528
I0406 02:07:53.507093 139799540086528 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.427291, loss=5.130105
I0406 02:07:53.511266 139857252349760 submission.py:139] 6500) loss = 5.130, grad_norm = 0.427
I0406 02:11:26.703940 139799531693824 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.441266, loss=5.103573
I0406 02:11:26.708026 139857252349760 submission.py:139] 7000) loss = 5.104, grad_norm = 0.441
I0406 02:14:59.618936 139799540086528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.443702, loss=4.965832
I0406 02:14:59.622165 139857252349760 submission.py:139] 7500) loss = 4.966, grad_norm = 0.444
I0406 02:17:42.266678 139857252349760 submission_runner.py:373] Before eval at step 7883: RAM USED (GB) 21.539938304
I0406 02:17:42.266952 139857252349760 spec.py:298] Evaluating on the training split.
I0406 02:17:46.178727 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:20:19.395817 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 02:20:23.151350 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:22:34.010514 139857252349760 spec.py:326] Evaluating on the test split.
I0406 02:22:37.840747 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:24:43.767832 139857252349760 submission_runner.py:382] Time since start: 6030.26s, 	Step: 7883, 	{'train/accuracy': 0.5515201165119242, 'train/loss': 3.0034525475605314, 'train/bleu': 24.971351784708776, 'validation/accuracy': 0.547792339834596, 'validation/loss': 3.005091575119961, 'validation/bleu': 20.350060710564833, 'validation/num_examples': 3000, 'test/accuracy': 0.5469990122596015, 'test/loss': 3.0476450380570563, 'test/bleu': 19.15392035372103, 'test/num_examples': 3003}
I0406 02:24:43.768390 139857252349760 submission_runner.py:396] After eval at step 7883: RAM USED (GB) 21.591240704
I0406 02:24:43.777782 139799531693824 logging_writer.py:48] [7883] global_step=7883, preemption_count=0, score=3341.396598, test/accuracy=0.546999, test/bleu=19.153920, test/loss=3.047645, test/num_examples=3003, total_duration=6030.255407, train/accuracy=0.551520, train/bleu=24.971352, train/loss=3.003453, validation/accuracy=0.547792, validation/bleu=20.350061, validation/loss=3.005092, validation/num_examples=3000
I0406 02:24:45.556505 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_7883.
I0406 02:24:45.557291 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 7883: RAM USED (GB) 21.590224896
I0406 02:25:35.710958 139799540086528 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.406049, loss=5.027913
I0406 02:25:35.714513 139857252349760 submission.py:139] 8000) loss = 5.028, grad_norm = 0.406
I0406 02:29:08.658127 139799531693824 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.414782, loss=4.855416
I0406 02:29:08.661763 139857252349760 submission.py:139] 8500) loss = 4.855, grad_norm = 0.415
I0406 02:32:41.625179 139799540086528 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.384106, loss=4.846613
I0406 02:32:41.628518 139857252349760 submission.py:139] 9000) loss = 4.847, grad_norm = 0.384
I0406 02:36:14.704094 139799531693824 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.367870, loss=4.800613
I0406 02:36:14.707576 139857252349760 submission.py:139] 9500) loss = 4.801, grad_norm = 0.368
I0406 02:38:45.563279 139857252349760 submission_runner.py:373] Before eval at step 9855: RAM USED (GB) 21.7293824
I0406 02:38:45.563510 139857252349760 spec.py:298] Evaluating on the training split.
I0406 02:38:49.470057 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:41:00.436996 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 02:41:04.191232 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:43:13.273008 139857252349760 spec.py:326] Evaluating on the test split.
I0406 02:43:17.086414 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:45:22.879066 139857252349760 submission_runner.py:382] Time since start: 7293.55s, 	Step: 9855, 	{'train/accuracy': 0.5683515680317947, 'train/loss': 2.8420997407551223, 'train/bleu': 26.67802876090026, 'validation/accuracy': 0.5743512169718913, 'validation/loss': 2.748776363591276, 'validation/bleu': 22.381926699250172, 'validation/num_examples': 3000, 'test/accuracy': 0.5729707745046773, 'test/loss': 2.775245845680088, 'test/bleu': 21.043354882029533, 'test/num_examples': 3003}
I0406 02:45:22.879445 139857252349760 submission_runner.py:396] After eval at step 9855: RAM USED (GB) 21.764399104
I0406 02:45:22.887341 139799540086528 logging_writer.py:48] [9855] global_step=9855, preemption_count=0, score=4175.539765, test/accuracy=0.572971, test/bleu=21.043355, test/loss=2.775246, test/num_examples=3003, total_duration=7293.552727, train/accuracy=0.568352, train/bleu=26.678029, train/loss=2.842100, validation/accuracy=0.574351, validation/bleu=22.381927, validation/loss=2.748776, validation/num_examples=3000
I0406 02:45:24.457021 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_9855.
I0406 02:45:24.457681 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 9855: RAM USED (GB) 21.76288768
I0406 02:46:26.102828 139857252349760 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 21.762359296
I0406 02:46:26.103029 139857252349760 spec.py:298] Evaluating on the training split.
I0406 02:46:30.015720 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:49:02.172214 139857252349760 spec.py:310] Evaluating on the validation split.
I0406 02:49:05.917869 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:51:21.991247 139857252349760 spec.py:326] Evaluating on the test split.
I0406 02:51:25.803362 139857252349760 workload.py:130] Translating evaluation dataset.
I0406 02:53:39.394836 139857252349760 submission_runner.py:382] Time since start: 7754.09s, 	Step: 10000, 	{'train/accuracy': 0.5699194476409666, 'train/loss': 2.8182909594361334, 'train/bleu': 26.467671319008783, 'validation/accuracy': 0.5718218001016726, 'validation/loss': 2.761737687691411, 'validation/bleu': 22.19669818558629, 'validation/num_examples': 3000, 'test/accuracy': 0.5735169368427169, 'test/loss': 2.775415976700947, 'test/bleu': 20.886209542828098, 'test/num_examples': 3003}
I0406 02:53:39.395231 139857252349760 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.816057856
I0406 02:53:39.403448 139799531693824 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4236.745967, test/accuracy=0.573517, test/bleu=20.886210, test/loss=2.775416, test/num_examples=3003, total_duration=7754.092994, train/accuracy=0.569919, train/bleu=26.467671, train/loss=2.818291, validation/accuracy=0.571822, validation/bleu=22.196698, validation/loss=2.761738, validation/num_examples=3000
I0406 02:53:40.995993 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_10000.
I0406 02:53:40.996625 139857252349760 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.815779328
I0406 02:53:41.003949 139799540086528 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4236.745967
I0406 02:53:43.759322 139857252349760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/wmt_pytorch/trial_1/checkpoint_10000.
I0406 02:53:43.782724 139857252349760 submission_runner.py:550] Tuning trial 1/1
I0406 02:53:43.782917 139857252349760 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 02:53:43.783760 139857252349760 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005381638308104518, 'train/loss': 11.100489213822796, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.11926928990341, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.107557666608564, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.7035329341888428, 'total_duration': 3.7055506706237793, 'global_step': 1, 'preemption_count': 0}), (1970, {'train/accuracy': 0.2836583849501485, 'train/loss': 5.899011173409473, 'train/bleu': 5.250842650827347, 'validation/accuracy': 0.2587072695936814, 'validation/loss': 6.184268096489814, 'validation/bleu': 2.6500455602103106, 'validation/num_examples': 3000, 'test/accuracy': 0.23899831503108476, 'test/loss': 6.469549270815176, 'test/bleu': 1.798571684994419, 'test/num_examples': 3003, 'score': 838.3240358829498, 'total_duration': 1673.802798986435, 'global_step': 1970, 'preemption_count': 0}), (3941, {'train/accuracy': 0.41132535890623395, 'train/loss': 4.365484281104257, 'train/bleu': 13.288880906145836, 'validation/accuracy': 0.39656048902059493, 'validation/loss': 4.490167899344088, 'validation/bleu': 9.12641697333711, 'validation/num_examples': 3000, 'test/accuracy': 0.38047760153390275, 'test/loss': 4.676311661146941, 'test/bleu': 7.708041940789616, 'test/num_examples': 3003, 'score': 1672.72381234169, 'total_duration': 3335.611249923706, 'global_step': 3941, 'preemption_count': 0}), (5911, {'train/accuracy': 0.506252372680525, 'train/loss': 3.458258078617691, 'train/bleu': 21.777561439213102, 'validation/accuracy': 0.50439548176712, 'validation/loss': 3.445240139613892, 'validation/bleu': 17.44384180804739, 'validation/num_examples': 3000, 'test/accuracy': 0.4998315031084771, 'test/loss': 3.532167291267213, 'test/bleu': 15.64117081901616, 'test/num_examples': 3003, 'score': 2507.0621984004974, 'total_duration': 4737.181318759918, 'global_step': 5911, 'preemption_count': 0}), (7883, {'train/accuracy': 0.5515201165119242, 'train/loss': 3.0034525475605314, 'train/bleu': 24.971351784708776, 'validation/accuracy': 0.547792339834596, 'validation/loss': 3.005091575119961, 'validation/bleu': 20.350060710564833, 'validation/num_examples': 3000, 'test/accuracy': 0.5469990122596015, 'test/loss': 3.0476450380570563, 'test/bleu': 19.15392035372103, 'test/num_examples': 3003, 'score': 3341.3965978622437, 'total_duration': 6030.255406856537, 'global_step': 7883, 'preemption_count': 0}), (9855, {'train/accuracy': 0.5683515680317947, 'train/loss': 2.8420997407551223, 'train/bleu': 26.67802876090026, 'validation/accuracy': 0.5743512169718913, 'validation/loss': 2.748776363591276, 'validation/bleu': 22.381926699250172, 'validation/num_examples': 3000, 'test/accuracy': 0.5729707745046773, 'test/loss': 2.775245845680088, 'test/bleu': 21.043354882029533, 'test/num_examples': 3003, 'score': 4175.539764881134, 'total_duration': 7293.552727460861, 'global_step': 9855, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5699194476409666, 'train/loss': 2.8182909594361334, 'train/bleu': 26.467671319008783, 'validation/accuracy': 0.5718218001016726, 'validation/loss': 2.761737687691411, 'validation/bleu': 22.19669818558629, 'validation/num_examples': 3000, 'test/accuracy': 0.5735169368427169, 'test/loss': 2.775415976700947, 'test/bleu': 20.886209542828098, 'test/num_examples': 3003, 'score': 4236.745967388153, 'total_duration': 7754.092993736267, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 02:53:43.783956 139857252349760 submission_runner.py:553] Timing: 4236.745967388153
I0406 02:53:43.784010 139857252349760 submission_runner.py:554] ====================
I0406 02:53:43.784091 139857252349760 submission_runner.py:613] Final wmt score: 4236.745967388153
