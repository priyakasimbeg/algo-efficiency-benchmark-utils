WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 19:32:06.684154 140048082859840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 19:32:06.684112 140354244765504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 19:32:06.684141 140585397643072 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 19:32:06.684809 140496282797888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 19:32:06.685065 139967055140672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 19:32:06.685434 139967055140672 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.685364 140474721830720 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 19:32:06.685434 140221037274944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 19:32:06.685467 139671174756160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 19:32:06.685748 140221037274944 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.685753 140474721830720 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.685910 139671174756160 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.694812 140585397643072 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.694862 140048082859840 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.694838 140354244765504 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:06.695554 140496282797888 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
W0405 19:32:08.837229 140221037274944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.837270 139967055140672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.837662 140496282797888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.838285 140048082859840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.839112 139671174756160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 19:32:08.850711 140474721830720 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch.
W0405 19:32:08.879943 140585397643072 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.882844 140474721830720 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.883401 140354244765504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 19:32:08.886401 140474721830720 submission_runner.py:511] Using RNG seed 2705454707
I0405 19:32:08.887434 140474721830720 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 19:32:08.887538 140474721830720 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1.
I0405 19:32:08.887735 140474721830720 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0405 19:32:08.888639 140474721830720 submission_runner.py:230] Starting train once: RAM USED (GB) 5.726859264
I0405 19:32:08.888725 140474721830720 submission_runner.py:231] Initializing dataset.
I0405 19:32:13.080688 140474721830720 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.708045312
I0405 19:32:13.080866 140474721830720 submission_runner.py:240] Initializing model.
I0405 19:32:17.622576 140474721830720 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.546661888
I0405 19:32:17.622748 140474721830720 submission_runner.py:252] Initializing optimizer.
I0405 19:32:17.623770 140474721830720 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.546661888
I0405 19:32:17.623873 140474721830720 submission_runner.py:261] Initializing metrics bundle.
I0405 19:32:17.623955 140474721830720 submission_runner.py:276] Initializing checkpoint and logger.
I0405 19:32:18.297535 140474721830720 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0405 19:32:18.299074 140474721830720 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0405 19:32:18.336584 140474721830720 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 17.595424768
I0405 19:32:18.337639 140474721830720 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 17.595424768
I0405 19:32:18.337756 140474721830720 submission_runner.py:313] Starting training loop.
I0405 19:32:20.803210 140474721830720 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 22.756528128
I0405 19:32:26.372505 140445791037184 logging_writer.py:48] [0] global_step=0, grad_norm=0.585103, loss=6.929079
I0405 19:32:26.382876 140474721830720 submission.py:119] 0) loss = 6.929, grad_norm = 0.585
I0405 19:32:26.383546 140474721830720 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.726927872
I0405 19:32:26.385251 140474721830720 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.731122176
I0405 19:32:26.385404 140474721830720 spec.py:298] Evaluating on the training split.
I0405 19:33:29.230397 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 19:34:26.201301 140474721830720 spec.py:326] Evaluating on the test split.
I0405 19:34:26.216655 140474721830720 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:26.222661 140474721830720 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 19:34:26.284527 140474721830720 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:38.504392 140474721830720 submission_runner.py:382] Time since start: 8.05s, 	Step: 1, 	{'train/accuracy': 0.0007374043367346938, 'train/loss': 6.9221197634327165, 'validation/accuracy': 0.00072, 'validation/loss': 6.923153125, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.92507109375, 'test/num_examples': 10000}
I0405 19:34:38.504818 140474721830720 submission_runner.py:396] After eval at step 1: RAM USED (GB) 91.584770048
I0405 19:34:38.513372 140422345127680 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=8.045021, test/accuracy=0.001100, test/loss=6.925071, test/num_examples=10000, total_duration=8.046991, train/accuracy=0.000737, train/loss=6.922120, validation/accuracy=0.000720, validation/loss=6.923153, validation/num_examples=50000
I0405 19:34:38.963782 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0405 19:34:38.964494 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 91.585486848
I0405 19:34:38.970952 140474721830720 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 91.585675264
I0405 19:34:38.978307 140474721830720 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.978399 140585397643072 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.978626 140354244765504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.979014 139671174756160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.979020 140048082859840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.979010 140496282797888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.979056 140221037274944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:38.979107 139967055140672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:39.357573 140422336734976 logging_writer.py:48] [1] global_step=1, grad_norm=0.611011, loss=6.931890
I0405 19:34:39.361021 140474721830720 submission.py:119] 1) loss = 6.932, grad_norm = 0.611
I0405 19:34:39.361653 140474721830720 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 91.59067648
I0405 19:34:39.742211 140422345127680 logging_writer.py:48] [2] global_step=2, grad_norm=0.606897, loss=6.927589
I0405 19:34:39.746014 140474721830720 submission.py:119] 2) loss = 6.928, grad_norm = 0.607
I0405 19:34:40.128718 140422336734976 logging_writer.py:48] [3] global_step=3, grad_norm=0.605417, loss=6.919124
I0405 19:34:40.132104 140474721830720 submission.py:119] 3) loss = 6.919, grad_norm = 0.605
I0405 19:34:40.515209 140422345127680 logging_writer.py:48] [4] global_step=4, grad_norm=0.594323, loss=6.931259
I0405 19:34:40.520574 140474721830720 submission.py:119] 4) loss = 6.931, grad_norm = 0.594
I0405 19:34:40.906435 140422336734976 logging_writer.py:48] [5] global_step=5, grad_norm=0.598524, loss=6.940412
I0405 19:34:40.910367 140474721830720 submission.py:119] 5) loss = 6.940, grad_norm = 0.599
I0405 19:34:41.293902 140422345127680 logging_writer.py:48] [6] global_step=6, grad_norm=0.610452, loss=6.932875
I0405 19:34:41.297518 140474721830720 submission.py:119] 6) loss = 6.933, grad_norm = 0.610
I0405 19:34:41.682827 140422336734976 logging_writer.py:48] [7] global_step=7, grad_norm=0.615154, loss=6.934780
I0405 19:34:41.686561 140474721830720 submission.py:119] 7) loss = 6.935, grad_norm = 0.615
I0405 19:34:42.080777 140422345127680 logging_writer.py:48] [8] global_step=8, grad_norm=0.615326, loss=6.932998
I0405 19:34:42.090243 140474721830720 submission.py:119] 8) loss = 6.933, grad_norm = 0.615
I0405 19:34:42.477485 140422336734976 logging_writer.py:48] [9] global_step=9, grad_norm=0.619538, loss=6.938479
I0405 19:34:42.484139 140474721830720 submission.py:119] 9) loss = 6.938, grad_norm = 0.620
I0405 19:34:42.869135 140422345127680 logging_writer.py:48] [10] global_step=10, grad_norm=0.602466, loss=6.920494
I0405 19:34:42.872667 140474721830720 submission.py:119] 10) loss = 6.920, grad_norm = 0.602
I0405 19:34:43.256920 140422336734976 logging_writer.py:48] [11] global_step=11, grad_norm=0.595450, loss=6.930360
I0405 19:34:43.260798 140474721830720 submission.py:119] 11) loss = 6.930, grad_norm = 0.595
I0405 19:34:43.647893 140422345127680 logging_writer.py:48] [12] global_step=12, grad_norm=0.599979, loss=6.929513
I0405 19:34:43.654885 140474721830720 submission.py:119] 12) loss = 6.930, grad_norm = 0.600
I0405 19:34:44.039881 140422336734976 logging_writer.py:48] [13] global_step=13, grad_norm=0.602806, loss=6.930211
I0405 19:34:44.043623 140474721830720 submission.py:119] 13) loss = 6.930, grad_norm = 0.603
I0405 19:34:44.430328 140422345127680 logging_writer.py:48] [14] global_step=14, grad_norm=0.615162, loss=6.929201
I0405 19:34:44.438937 140474721830720 submission.py:119] 14) loss = 6.929, grad_norm = 0.615
I0405 19:34:44.822033 140422336734976 logging_writer.py:48] [15] global_step=15, grad_norm=0.592398, loss=6.934490
I0405 19:34:44.825658 140474721830720 submission.py:119] 15) loss = 6.934, grad_norm = 0.592
I0405 19:34:45.209012 140422345127680 logging_writer.py:48] [16] global_step=16, grad_norm=0.590979, loss=6.932433
I0405 19:34:45.214053 140474721830720 submission.py:119] 16) loss = 6.932, grad_norm = 0.591
I0405 19:34:45.606993 140422336734976 logging_writer.py:48] [17] global_step=17, grad_norm=0.596870, loss=6.924633
I0405 19:34:45.613839 140474721830720 submission.py:119] 17) loss = 6.925, grad_norm = 0.597
I0405 19:34:45.998926 140422345127680 logging_writer.py:48] [18] global_step=18, grad_norm=0.611657, loss=6.939835
I0405 19:34:46.002671 140474721830720 submission.py:119] 18) loss = 6.940, grad_norm = 0.612
I0405 19:34:46.441998 140422336734976 logging_writer.py:48] [19] global_step=19, grad_norm=0.600785, loss=6.928188
I0405 19:34:46.445636 140474721830720 submission.py:119] 19) loss = 6.928, grad_norm = 0.601
I0405 19:34:46.829705 140422345127680 logging_writer.py:48] [20] global_step=20, grad_norm=0.606775, loss=6.931246
I0405 19:34:46.835069 140474721830720 submission.py:119] 20) loss = 6.931, grad_norm = 0.607
I0405 19:34:47.218161 140422336734976 logging_writer.py:48] [21] global_step=21, grad_norm=0.598728, loss=6.931381
I0405 19:34:47.222471 140474721830720 submission.py:119] 21) loss = 6.931, grad_norm = 0.599
I0405 19:34:47.606680 140422345127680 logging_writer.py:48] [22] global_step=22, grad_norm=0.600516, loss=6.927782
I0405 19:34:47.611896 140474721830720 submission.py:119] 22) loss = 6.928, grad_norm = 0.601
I0405 19:34:48.006730 140422336734976 logging_writer.py:48] [23] global_step=23, grad_norm=0.595251, loss=6.934035
I0405 19:34:48.010903 140474721830720 submission.py:119] 23) loss = 6.934, grad_norm = 0.595
I0405 19:34:48.394191 140422345127680 logging_writer.py:48] [24] global_step=24, grad_norm=0.606854, loss=6.925577
I0405 19:34:48.399485 140474721830720 submission.py:119] 24) loss = 6.926, grad_norm = 0.607
I0405 19:34:48.791935 140422336734976 logging_writer.py:48] [25] global_step=25, grad_norm=0.595746, loss=6.933243
I0405 19:34:48.796205 140474721830720 submission.py:119] 25) loss = 6.933, grad_norm = 0.596
I0405 19:34:49.181237 140422345127680 logging_writer.py:48] [26] global_step=26, grad_norm=0.610370, loss=6.922602
I0405 19:34:49.185281 140474721830720 submission.py:119] 26) loss = 6.923, grad_norm = 0.610
I0405 19:34:49.567131 140422336734976 logging_writer.py:48] [27] global_step=27, grad_norm=0.587918, loss=6.924972
I0405 19:34:49.571820 140474721830720 submission.py:119] 27) loss = 6.925, grad_norm = 0.588
I0405 19:34:49.958735 140422345127680 logging_writer.py:48] [28] global_step=28, grad_norm=0.602295, loss=6.912750
I0405 19:34:49.964638 140474721830720 submission.py:119] 28) loss = 6.913, grad_norm = 0.602
I0405 19:34:50.360344 140422336734976 logging_writer.py:48] [29] global_step=29, grad_norm=0.603639, loss=6.922826
I0405 19:34:50.364707 140474721830720 submission.py:119] 29) loss = 6.923, grad_norm = 0.604
I0405 19:34:50.755797 140422345127680 logging_writer.py:48] [30] global_step=30, grad_norm=0.603091, loss=6.922811
I0405 19:34:50.759358 140474721830720 submission.py:119] 30) loss = 6.923, grad_norm = 0.603
I0405 19:34:51.143903 140422336734976 logging_writer.py:48] [31] global_step=31, grad_norm=0.587040, loss=6.924071
I0405 19:34:51.154871 140474721830720 submission.py:119] 31) loss = 6.924, grad_norm = 0.587
I0405 19:34:51.538733 140422345127680 logging_writer.py:48] [32] global_step=32, grad_norm=0.579036, loss=6.912437
I0405 19:34:51.543869 140474721830720 submission.py:119] 32) loss = 6.912, grad_norm = 0.579
I0405 19:34:51.930003 140422336734976 logging_writer.py:48] [33] global_step=33, grad_norm=0.612639, loss=6.917607
I0405 19:34:51.934441 140474721830720 submission.py:119] 33) loss = 6.918, grad_norm = 0.613
I0405 19:34:52.317228 140422345127680 logging_writer.py:48] [34] global_step=34, grad_norm=0.595926, loss=6.926409
I0405 19:34:52.322355 140474721830720 submission.py:119] 34) loss = 6.926, grad_norm = 0.596
I0405 19:34:52.704073 140422336734976 logging_writer.py:48] [35] global_step=35, grad_norm=0.587071, loss=6.924394
I0405 19:34:52.708901 140474721830720 submission.py:119] 35) loss = 6.924, grad_norm = 0.587
I0405 19:34:53.153294 140422345127680 logging_writer.py:48] [36] global_step=36, grad_norm=0.606485, loss=6.914896
I0405 19:34:53.157343 140474721830720 submission.py:119] 36) loss = 6.915, grad_norm = 0.606
I0405 19:34:53.544636 140422336734976 logging_writer.py:48] [37] global_step=37, grad_norm=0.610655, loss=6.926846
I0405 19:34:53.548914 140474721830720 submission.py:119] 37) loss = 6.927, grad_norm = 0.611
I0405 19:34:53.944179 140422345127680 logging_writer.py:48] [38] global_step=38, grad_norm=0.588717, loss=6.919637
I0405 19:34:53.948521 140474721830720 submission.py:119] 38) loss = 6.920, grad_norm = 0.589
I0405 19:34:54.333293 140422336734976 logging_writer.py:48] [39] global_step=39, grad_norm=0.592311, loss=6.915369
I0405 19:34:54.337640 140474721830720 submission.py:119] 39) loss = 6.915, grad_norm = 0.592
I0405 19:34:54.721194 140422345127680 logging_writer.py:48] [40] global_step=40, grad_norm=0.604650, loss=6.918934
I0405 19:34:54.725110 140474721830720 submission.py:119] 40) loss = 6.919, grad_norm = 0.605
I0405 19:34:55.109654 140422336734976 logging_writer.py:48] [41] global_step=41, grad_norm=0.578168, loss=6.915599
I0405 19:34:55.113328 140474721830720 submission.py:119] 41) loss = 6.916, grad_norm = 0.578
I0405 19:34:55.509859 140422345127680 logging_writer.py:48] [42] global_step=42, grad_norm=0.592314, loss=6.917296
I0405 19:34:55.519362 140474721830720 submission.py:119] 42) loss = 6.917, grad_norm = 0.592
I0405 19:34:55.907215 140422336734976 logging_writer.py:48] [43] global_step=43, grad_norm=0.595657, loss=6.913831
I0405 19:34:55.910693 140474721830720 submission.py:119] 43) loss = 6.914, grad_norm = 0.596
I0405 19:34:56.303203 140422345127680 logging_writer.py:48] [44] global_step=44, grad_norm=0.594530, loss=6.916648
I0405 19:34:56.307092 140474721830720 submission.py:119] 44) loss = 6.917, grad_norm = 0.595
I0405 19:34:56.693781 140422336734976 logging_writer.py:48] [45] global_step=45, grad_norm=0.586285, loss=6.917949
I0405 19:34:56.698833 140474721830720 submission.py:119] 45) loss = 6.918, grad_norm = 0.586
I0405 19:34:57.095576 140422345127680 logging_writer.py:48] [46] global_step=46, grad_norm=0.610779, loss=6.915173
I0405 19:34:57.098956 140474721830720 submission.py:119] 46) loss = 6.915, grad_norm = 0.611
I0405 19:34:57.488486 140422336734976 logging_writer.py:48] [47] global_step=47, grad_norm=0.593043, loss=6.911060
I0405 19:34:57.493485 140474721830720 submission.py:119] 47) loss = 6.911, grad_norm = 0.593
I0405 19:34:57.877601 140422345127680 logging_writer.py:48] [48] global_step=48, grad_norm=0.588200, loss=6.919259
I0405 19:34:57.881200 140474721830720 submission.py:119] 48) loss = 6.919, grad_norm = 0.588
I0405 19:34:58.264810 140422336734976 logging_writer.py:48] [49] global_step=49, grad_norm=0.607190, loss=6.925471
I0405 19:34:58.269655 140474721830720 submission.py:119] 49) loss = 6.925, grad_norm = 0.607
I0405 19:34:58.654953 140422345127680 logging_writer.py:48] [50] global_step=50, grad_norm=0.588741, loss=6.914147
I0405 19:34:58.659476 140474721830720 submission.py:119] 50) loss = 6.914, grad_norm = 0.589
I0405 19:34:59.042938 140422336734976 logging_writer.py:48] [51] global_step=51, grad_norm=0.586485, loss=6.905636
I0405 19:34:59.047356 140474721830720 submission.py:119] 51) loss = 6.906, grad_norm = 0.586
I0405 19:34:59.440577 140422345127680 logging_writer.py:48] [52] global_step=52, grad_norm=0.582264, loss=6.907994
I0405 19:34:59.444147 140474721830720 submission.py:119] 52) loss = 6.908, grad_norm = 0.582
I0405 19:34:59.834386 140422336734976 logging_writer.py:48] [53] global_step=53, grad_norm=0.579829, loss=6.909064
I0405 19:34:59.838484 140474721830720 submission.py:119] 53) loss = 6.909, grad_norm = 0.580
I0405 19:35:00.225047 140422345127680 logging_writer.py:48] [54] global_step=54, grad_norm=0.598702, loss=6.913610
I0405 19:35:00.228704 140474721830720 submission.py:119] 54) loss = 6.914, grad_norm = 0.599
I0405 19:35:00.615709 140422336734976 logging_writer.py:48] [55] global_step=55, grad_norm=0.580950, loss=6.912729
I0405 19:35:00.622202 140474721830720 submission.py:119] 55) loss = 6.913, grad_norm = 0.581
I0405 19:35:01.007821 140422345127680 logging_writer.py:48] [56] global_step=56, grad_norm=0.598615, loss=6.913637
I0405 19:35:01.012369 140474721830720 submission.py:119] 56) loss = 6.914, grad_norm = 0.599
I0405 19:35:01.401860 140422336734976 logging_writer.py:48] [57] global_step=57, grad_norm=0.599297, loss=6.915818
I0405 19:35:01.406217 140474721830720 submission.py:119] 57) loss = 6.916, grad_norm = 0.599
I0405 19:35:01.795112 140422345127680 logging_writer.py:48] [58] global_step=58, grad_norm=0.605972, loss=6.902607
I0405 19:35:01.798944 140474721830720 submission.py:119] 58) loss = 6.903, grad_norm = 0.606
I0405 19:35:02.189285 140422336734976 logging_writer.py:48] [59] global_step=59, grad_norm=0.604532, loss=6.914335
I0405 19:35:02.192961 140474721830720 submission.py:119] 59) loss = 6.914, grad_norm = 0.605
I0405 19:35:02.584170 140422345127680 logging_writer.py:48] [60] global_step=60, grad_norm=0.591341, loss=6.903410
I0405 19:35:02.587603 140474721830720 submission.py:119] 60) loss = 6.903, grad_norm = 0.591
I0405 19:35:02.977607 140422336734976 logging_writer.py:48] [61] global_step=61, grad_norm=0.574175, loss=6.897240
I0405 19:35:02.982264 140474721830720 submission.py:119] 61) loss = 6.897, grad_norm = 0.574
I0405 19:35:03.372607 140422345127680 logging_writer.py:48] [62] global_step=62, grad_norm=0.577718, loss=6.902972
I0405 19:35:03.376915 140474721830720 submission.py:119] 62) loss = 6.903, grad_norm = 0.578
I0405 19:35:03.769016 140422336734976 logging_writer.py:48] [63] global_step=63, grad_norm=0.596678, loss=6.907063
I0405 19:35:03.773144 140474721830720 submission.py:119] 63) loss = 6.907, grad_norm = 0.597
I0405 19:35:04.155753 140422345127680 logging_writer.py:48] [64] global_step=64, grad_norm=0.614765, loss=6.904886
I0405 19:35:04.160792 140474721830720 submission.py:119] 64) loss = 6.905, grad_norm = 0.615
I0405 19:35:04.545585 140422336734976 logging_writer.py:48] [65] global_step=65, grad_norm=0.586894, loss=6.903077
I0405 19:35:04.549535 140474721830720 submission.py:119] 65) loss = 6.903, grad_norm = 0.587
I0405 19:35:04.936245 140422345127680 logging_writer.py:48] [66] global_step=66, grad_norm=0.587165, loss=6.907388
I0405 19:35:04.939688 140474721830720 submission.py:119] 66) loss = 6.907, grad_norm = 0.587
I0405 19:35:05.324206 140422336734976 logging_writer.py:48] [67] global_step=67, grad_norm=0.602334, loss=6.902635
I0405 19:35:05.328686 140474721830720 submission.py:119] 67) loss = 6.903, grad_norm = 0.602
I0405 19:35:05.723173 140422345127680 logging_writer.py:48] [68] global_step=68, grad_norm=0.596160, loss=6.903507
I0405 19:35:05.727490 140474721830720 submission.py:119] 68) loss = 6.904, grad_norm = 0.596
I0405 19:35:06.122566 140422336734976 logging_writer.py:48] [69] global_step=69, grad_norm=0.568862, loss=6.903457
I0405 19:35:06.128204 140474721830720 submission.py:119] 69) loss = 6.903, grad_norm = 0.569
I0405 19:35:06.530312 140422345127680 logging_writer.py:48] [70] global_step=70, grad_norm=0.590102, loss=6.897619
I0405 19:35:06.534608 140474721830720 submission.py:119] 70) loss = 6.898, grad_norm = 0.590
I0405 19:35:06.920682 140422336734976 logging_writer.py:48] [71] global_step=71, grad_norm=0.595475, loss=6.910451
I0405 19:35:06.924938 140474721830720 submission.py:119] 71) loss = 6.910, grad_norm = 0.595
I0405 19:35:07.309964 140422345127680 logging_writer.py:48] [72] global_step=72, grad_norm=0.597701, loss=6.900261
I0405 19:35:07.314142 140474721830720 submission.py:119] 72) loss = 6.900, grad_norm = 0.598
I0405 19:35:07.703248 140422336734976 logging_writer.py:48] [73] global_step=73, grad_norm=0.612194, loss=6.904925
I0405 19:35:07.707538 140474721830720 submission.py:119] 73) loss = 6.905, grad_norm = 0.612
I0405 19:35:08.092242 140422345127680 logging_writer.py:48] [74] global_step=74, grad_norm=0.576199, loss=6.897318
I0405 19:35:08.096076 140474721830720 submission.py:119] 74) loss = 6.897, grad_norm = 0.576
I0405 19:35:08.480516 140422336734976 logging_writer.py:48] [75] global_step=75, grad_norm=0.586593, loss=6.893113
I0405 19:35:08.483887 140474721830720 submission.py:119] 75) loss = 6.893, grad_norm = 0.587
I0405 19:35:08.880185 140422345127680 logging_writer.py:48] [76] global_step=76, grad_norm=0.581876, loss=6.899457
I0405 19:35:08.884314 140474721830720 submission.py:119] 76) loss = 6.899, grad_norm = 0.582
I0405 19:35:09.267916 140422336734976 logging_writer.py:48] [77] global_step=77, grad_norm=0.582053, loss=6.901761
I0405 19:35:09.272753 140474721830720 submission.py:119] 77) loss = 6.902, grad_norm = 0.582
I0405 19:35:09.659075 140422345127680 logging_writer.py:48] [78] global_step=78, grad_norm=0.598212, loss=6.897440
I0405 19:35:09.663616 140474721830720 submission.py:119] 78) loss = 6.897, grad_norm = 0.598
I0405 19:35:10.048885 140422336734976 logging_writer.py:48] [79] global_step=79, grad_norm=0.612558, loss=6.905030
I0405 19:35:10.052564 140474721830720 submission.py:119] 79) loss = 6.905, grad_norm = 0.613
I0405 19:35:10.437611 140422345127680 logging_writer.py:48] [80] global_step=80, grad_norm=0.581659, loss=6.889374
I0405 19:35:10.441441 140474721830720 submission.py:119] 80) loss = 6.889, grad_norm = 0.582
I0405 19:35:10.837704 140422336734976 logging_writer.py:48] [81] global_step=81, grad_norm=0.579068, loss=6.890454
I0405 19:35:10.841896 140474721830720 submission.py:119] 81) loss = 6.890, grad_norm = 0.579
I0405 19:35:11.232026 140422345127680 logging_writer.py:48] [82] global_step=82, grad_norm=0.583462, loss=6.887216
I0405 19:35:11.235722 140474721830720 submission.py:119] 82) loss = 6.887, grad_norm = 0.583
I0405 19:35:11.618922 140422336734976 logging_writer.py:48] [83] global_step=83, grad_norm=0.590702, loss=6.896249
I0405 19:35:11.622921 140474721830720 submission.py:119] 83) loss = 6.896, grad_norm = 0.591
I0405 19:35:12.008441 140422345127680 logging_writer.py:48] [84] global_step=84, grad_norm=0.580208, loss=6.893838
I0405 19:35:12.012697 140474721830720 submission.py:119] 84) loss = 6.894, grad_norm = 0.580
I0405 19:35:12.406274 140422336734976 logging_writer.py:48] [85] global_step=85, grad_norm=0.574363, loss=6.887579
I0405 19:35:12.409752 140474721830720 submission.py:119] 85) loss = 6.888, grad_norm = 0.574
I0405 19:35:12.793338 140422345127680 logging_writer.py:48] [86] global_step=86, grad_norm=0.601999, loss=6.891134
I0405 19:35:12.798248 140474721830720 submission.py:119] 86) loss = 6.891, grad_norm = 0.602
I0405 19:35:13.204447 140422336734976 logging_writer.py:48] [87] global_step=87, grad_norm=0.592581, loss=6.889863
I0405 19:35:13.208057 140474721830720 submission.py:119] 87) loss = 6.890, grad_norm = 0.593
I0405 19:35:13.591441 140422345127680 logging_writer.py:48] [88] global_step=88, grad_norm=0.596256, loss=6.890574
I0405 19:35:13.595247 140474721830720 submission.py:119] 88) loss = 6.891, grad_norm = 0.596
I0405 19:35:13.992342 140422336734976 logging_writer.py:48] [89] global_step=89, grad_norm=0.588460, loss=6.883830
I0405 19:35:13.996129 140474721830720 submission.py:119] 89) loss = 6.884, grad_norm = 0.588
I0405 19:35:14.381560 140422345127680 logging_writer.py:48] [90] global_step=90, grad_norm=0.605930, loss=6.890740
I0405 19:35:14.385186 140474721830720 submission.py:119] 90) loss = 6.891, grad_norm = 0.606
I0405 19:35:14.768468 140422336734976 logging_writer.py:48] [91] global_step=91, grad_norm=0.586936, loss=6.882461
I0405 19:35:14.773434 140474721830720 submission.py:119] 91) loss = 6.882, grad_norm = 0.587
I0405 19:35:15.160218 140422345127680 logging_writer.py:48] [92] global_step=92, grad_norm=0.607112, loss=6.890637
I0405 19:35:15.164413 140474721830720 submission.py:119] 92) loss = 6.891, grad_norm = 0.607
I0405 19:35:15.560651 140422336734976 logging_writer.py:48] [93] global_step=93, grad_norm=0.583593, loss=6.884423
I0405 19:35:15.564548 140474721830720 submission.py:119] 93) loss = 6.884, grad_norm = 0.584
I0405 19:35:15.948048 140422345127680 logging_writer.py:48] [94] global_step=94, grad_norm=0.578560, loss=6.881142
I0405 19:35:15.952235 140474721830720 submission.py:119] 94) loss = 6.881, grad_norm = 0.579
I0405 19:35:16.338627 140422336734976 logging_writer.py:48] [95] global_step=95, grad_norm=0.585648, loss=6.877529
I0405 19:35:16.342048 140474721830720 submission.py:119] 95) loss = 6.878, grad_norm = 0.586
I0405 19:35:16.726361 140422345127680 logging_writer.py:48] [96] global_step=96, grad_norm=0.595486, loss=6.884759
I0405 19:35:16.729771 140474721830720 submission.py:119] 96) loss = 6.885, grad_norm = 0.595
I0405 19:35:17.114837 140422336734976 logging_writer.py:48] [97] global_step=97, grad_norm=0.594579, loss=6.870335
I0405 19:35:17.119863 140474721830720 submission.py:119] 97) loss = 6.870, grad_norm = 0.595
I0405 19:35:17.508510 140422345127680 logging_writer.py:48] [98] global_step=98, grad_norm=0.596626, loss=6.872867
I0405 19:35:17.519565 140474721830720 submission.py:119] 98) loss = 6.873, grad_norm = 0.597
I0405 19:35:17.908043 140422336734976 logging_writer.py:48] [99] global_step=99, grad_norm=0.601705, loss=6.879082
I0405 19:35:17.913030 140474721830720 submission.py:119] 99) loss = 6.879, grad_norm = 0.602
I0405 19:35:18.298861 140422345127680 logging_writer.py:48] [100] global_step=100, grad_norm=0.593426, loss=6.888429
I0405 19:35:18.303393 140474721830720 submission.py:119] 100) loss = 6.888, grad_norm = 0.593
I0405 19:37:49.581943 140422336734976 logging_writer.py:48] [500] global_step=500, grad_norm=1.130989, loss=6.294961
I0405 19:37:49.587598 140474721830720 submission.py:119] 500) loss = 6.295, grad_norm = 1.131
I0405 19:40:58.718298 140422345127680 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.779743, loss=5.678771
I0405 19:40:58.723855 140474721830720 submission.py:119] 1000) loss = 5.679, grad_norm = 2.780
I0405 19:43:09.189202 140474721830720 submission_runner.py:373] Before eval at step 1343: RAM USED (GB) 98.231005184
I0405 19:43:09.189419 140474721830720 spec.py:298] Evaluating on the training split.
I0405 19:43:53.864372 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 19:44:50.133635 140474721830720 spec.py:326] Evaluating on the test split.
I0405 19:44:51.501112 140474721830720 submission_runner.py:382] Time since start: 650.85s, 	Step: 1343, 	{'train/accuracy': 0.1053890306122449, 'train/loss': 4.877003261021206, 'validation/accuracy': 0.09564, 'validation/loss': 4.962548125, 'validation/num_examples': 50000, 'test/accuracy': 0.0637, 'test/loss': 5.361549609375, 'test/num_examples': 10000}
I0405 19:44:51.501461 140474721830720 submission_runner.py:396] After eval at step 1343: RAM USED (GB) 98.231504896
I0405 19:44:51.509362 140422428989184 logging_writer.py:48] [1343] global_step=1343, preemption_count=0, score=515.782491, test/accuracy=0.063700, test/loss=5.361550, test/num_examples=10000, total_duration=650.851565, train/accuracy=0.105389, train/loss=4.877003, validation/accuracy=0.095640, validation/loss=4.962548, validation/num_examples=50000
I0405 19:44:51.980493 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_1343.
I0405 19:44:51.981270 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 1343: RAM USED (GB) 98.230276096
I0405 19:45:51.310946 140422437381888 logging_writer.py:48] [1500] global_step=1500, grad_norm=5.723402, loss=5.277635
I0405 19:45:51.314810 140474721830720 submission.py:119] 1500) loss = 5.278, grad_norm = 5.723
I0405 19:48:59.464214 140422428989184 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.178855, loss=4.898363
I0405 19:48:59.468538 140474721830720 submission.py:119] 2000) loss = 4.898, grad_norm = 3.179
I0405 19:52:08.266691 140422437381888 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.128840, loss=4.639368
I0405 19:52:08.273251 140474721830720 submission.py:119] 2500) loss = 4.639, grad_norm = 3.129
I0405 19:53:21.993632 140474721830720 submission_runner.py:373] Before eval at step 2693: RAM USED (GB) 99.699253248
I0405 19:53:21.993857 140474721830720 spec.py:298] Evaluating on the training split.
I0405 19:54:05.541284 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 19:55:03.804289 140474721830720 spec.py:326] Evaluating on the test split.
I0405 19:55:05.171536 140474721830720 submission_runner.py:382] Time since start: 1263.66s, 	Step: 2693, 	{'train/accuracy': 0.2517936862244898, 'train/loss': 3.6906324114118303, 'validation/accuracy': 0.22554, 'validation/loss': 3.833740625, 'validation/num_examples': 50000, 'test/accuracy': 0.1609, 'test/loss': 4.428838671875, 'test/num_examples': 10000}
I0405 19:55:05.171926 140474721830720 submission_runner.py:396] After eval at step 2693: RAM USED (GB) 99.748364288
I0405 19:55:05.179959 140422428989184 logging_writer.py:48] [2693] global_step=2693, preemption_count=0, score=1023.439783, test/accuracy=0.160900, test/loss=4.428839, test/num_examples=10000, total_duration=1263.655974, train/accuracy=0.251794, train/loss=3.690632, validation/accuracy=0.225540, validation/loss=3.833741, validation/num_examples=50000
I0405 19:55:05.658869 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_2693.
I0405 19:55:05.659716 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 2693: RAM USED (GB) 99.741155328
I0405 19:57:01.259219 140422437381888 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.820303, loss=4.370541
I0405 19:57:01.263014 140474721830720 submission.py:119] 3000) loss = 4.371, grad_norm = 3.820
I0405 20:00:09.177678 140422428989184 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.364346, loss=3.968287
I0405 20:00:09.181656 140474721830720 submission.py:119] 3500) loss = 3.968, grad_norm = 2.364
I0405 20:03:18.809188 140422437381888 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.552109, loss=4.006875
I0405 20:03:18.812951 140474721830720 submission.py:119] 4000) loss = 4.007, grad_norm = 2.552
I0405 20:03:35.768503 140474721830720 submission_runner.py:373] Before eval at step 4046: RAM USED (GB) 99.671310336
I0405 20:03:35.768718 140474721830720 spec.py:298] Evaluating on the training split.
I0405 20:04:19.334368 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 20:05:04.949817 140474721830720 spec.py:326] Evaluating on the test split.
I0405 20:05:06.316088 140474721830720 submission_runner.py:382] Time since start: 1877.43s, 	Step: 4046, 	{'train/accuracy': 0.33376514668367346, 'train/loss': 3.1891376728914222, 'validation/accuracy': 0.29876, 'validation/loss': 3.375463125, 'validation/num_examples': 50000, 'test/accuracy': 0.2205, 'test/loss': 4.0397578125, 'test/num_examples': 10000}
I0405 20:05:06.316433 140474721830720 submission_runner.py:396] After eval at step 4046: RAM USED (GB) 99.823583232
I0405 20:05:06.327377 140422428989184 logging_writer.py:48] [4046] global_step=4046, preemption_count=0, score=1531.305132, test/accuracy=0.220500, test/loss=4.039758, test/num_examples=10000, total_duration=1877.430918, train/accuracy=0.333765, train/loss=3.189138, validation/accuracy=0.298760, validation/loss=3.375463, validation/num_examples=50000
I0405 20:05:06.780986 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_4046.
I0405 20:05:06.781738 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 4046: RAM USED (GB) 99.886645248
I0405 20:07:57.677853 140422437381888 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.098625, loss=3.643772
I0405 20:07:57.681574 140474721830720 submission.py:119] 4500) loss = 3.644, grad_norm = 3.099
I0405 20:11:06.181456 140422428989184 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.411192, loss=3.667585
I0405 20:11:06.185825 140474721830720 submission.py:119] 5000) loss = 3.668, grad_norm = 2.411
I0405 20:13:37.002514 140474721830720 submission_runner.py:373] Before eval at step 5399: RAM USED (GB) 99.95075584
I0405 20:13:37.002763 140474721830720 spec.py:298] Evaluating on the training split.
I0405 20:14:26.523873 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 20:15:16.973470 140474721830720 spec.py:326] Evaluating on the test split.
I0405 20:15:18.321415 140474721830720 submission_runner.py:382] Time since start: 2478.66s, 	Step: 5399, 	{'train/accuracy': 0.44100765306122447, 'train/loss': 2.558208854830995, 'validation/accuracy': 0.4001, 'validation/loss': 2.78012875, 'validation/num_examples': 50000, 'test/accuracy': 0.302, 'test/loss': 3.455589453125, 'test/num_examples': 10000}
I0405 20:15:18.321882 140474721830720 submission_runner.py:396] After eval at step 5399: RAM USED (GB) 100.029022208
I0405 20:15:18.332138 140422437381888 logging_writer.py:48] [5399] global_step=5399, preemption_count=0, score=2039.270702, test/accuracy=0.302000, test/loss=3.455589, test/num_examples=10000, total_duration=2478.664844, train/accuracy=0.441008, train/loss=2.558209, validation/accuracy=0.400100, validation/loss=2.780129, validation/num_examples=50000
I0405 20:15:18.844824 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_5399.
I0405 20:15:18.845789 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 5399: RAM USED (GB) 100.028014592
I0405 20:15:57.154012 140422428989184 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.545878, loss=3.591136
I0405 20:15:57.157634 140474721830720 submission.py:119] 5500) loss = 3.591, grad_norm = 1.546
I0405 20:19:05.030297 140422437381888 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.882976, loss=3.455692
I0405 20:19:05.035415 140474721830720 submission.py:119] 6000) loss = 3.456, grad_norm = 1.883
I0405 20:22:14.878450 140422428989184 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.584028, loss=3.291129
I0405 20:22:14.882966 140474721830720 submission.py:119] 6500) loss = 3.291, grad_norm = 1.584
I0405 20:23:48.891445 140474721830720 submission_runner.py:373] Before eval at step 6752: RAM USED (GB) 99.996274688
I0405 20:23:48.891679 140474721830720 spec.py:298] Evaluating on the training split.
I0405 20:24:32.349607 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 20:25:23.814147 140474721830720 spec.py:326] Evaluating on the test split.
I0405 20:25:25.157436 140474721830720 submission_runner.py:382] Time since start: 3090.55s, 	Step: 6752, 	{'train/accuracy': 0.524055325255102, 'train/loss': 2.1674213020169004, 'validation/accuracy': 0.48284, 'validation/loss': 2.3822628125, 'validation/num_examples': 50000, 'test/accuracy': 0.3596, 'test/loss': 3.1165484375, 'test/num_examples': 10000}
I0405 20:25:25.157883 140474721830720 submission_runner.py:396] After eval at step 6752: RAM USED (GB) 99.96230656
I0405 20:25:25.166641 140422437381888 logging_writer.py:48] [6752] global_step=6752, preemption_count=0, score=2547.058900, test/accuracy=0.359600, test/loss=3.116548, test/num_examples=10000, total_duration=3090.553810, train/accuracy=0.524055, train/loss=2.167421, validation/accuracy=0.482840, validation/loss=2.382263, validation/num_examples=50000
I0405 20:25:25.656762 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_6752.
I0405 20:25:25.657756 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 6752: RAM USED (GB) 99.96222464
I0405 20:26:59.112902 140422428989184 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.527789, loss=3.238661
I0405 20:26:59.116535 140474721830720 submission.py:119] 7000) loss = 3.239, grad_norm = 1.528
I0405 20:30:07.696285 140422437381888 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.028284, loss=3.313520
I0405 20:30:07.700178 140474721830720 submission.py:119] 7500) loss = 3.314, grad_norm = 2.028
I0405 20:33:16.630261 140422428989184 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.130583, loss=3.133206
I0405 20:33:16.634045 140474721830720 submission.py:119] 8000) loss = 3.133, grad_norm = 1.131
I0405 20:33:56.001922 140474721830720 submission_runner.py:373] Before eval at step 8106: RAM USED (GB) 99.880558592
I0405 20:33:56.002126 140474721830720 spec.py:298] Evaluating on the training split.
I0405 20:34:40.595100 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 20:35:25.448245 140474721830720 spec.py:326] Evaluating on the test split.
I0405 20:35:26.800943 140474721830720 submission_runner.py:382] Time since start: 3697.66s, 	Step: 8106, 	{'train/accuracy': 0.5535315688775511, 'train/loss': 2.013788495744978, 'validation/accuracy': 0.50886, 'validation/loss': 2.24752328125, 'validation/num_examples': 50000, 'test/accuracy': 0.3819, 'test/loss': 2.9840154296875, 'test/num_examples': 10000}
I0405 20:35:26.801274 140474721830720 submission_runner.py:396] After eval at step 8106: RAM USED (GB) 99.979767808
I0405 20:35:26.808972 140422437381888 logging_writer.py:48] [8106] global_step=8106, preemption_count=0, score=3055.134122, test/accuracy=0.381900, test/loss=2.984015, test/num_examples=10000, total_duration=3697.664335, train/accuracy=0.553532, train/loss=2.013788, validation/accuracy=0.508860, validation/loss=2.247523, validation/num_examples=50000
I0405 20:35:27.264261 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_8106.
I0405 20:35:27.265058 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 8106: RAM USED (GB) 99.978563584
I0405 20:37:55.807083 140422428989184 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.106834, loss=3.158741
I0405 20:37:55.812201 140474721830720 submission.py:119] 8500) loss = 3.159, grad_norm = 1.107
I0405 20:41:05.371889 140422437381888 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.009512, loss=3.034908
I0405 20:41:05.376619 140474721830720 submission.py:119] 9000) loss = 3.035, grad_norm = 1.010
I0405 20:43:57.450264 140474721830720 submission_runner.py:373] Before eval at step 9460: RAM USED (GB) 100.068184064
I0405 20:43:57.450479 140474721830720 spec.py:298] Evaluating on the training split.
I0405 20:44:40.854264 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 20:45:34.052690 140474721830720 spec.py:326] Evaluating on the test split.
I0405 20:45:35.400124 140474721830720 submission_runner.py:382] Time since start: 4299.11s, 	Step: 9460, 	{'train/accuracy': 0.5860172193877551, 'train/loss': 1.8722417403240592, 'validation/accuracy': 0.53204, 'validation/loss': 2.13917515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4114, 'test/loss': 2.8341126953125, 'test/num_examples': 10000}
I0405 20:45:35.400455 140474721830720 submission_runner.py:396] After eval at step 9460: RAM USED (GB) 100.094574592
I0405 20:45:35.408528 140422428989184 logging_writer.py:48] [9460] global_step=9460, preemption_count=0, score=3563.060268, test/accuracy=0.411400, test/loss=2.834113, test/num_examples=10000, total_duration=4299.112677, train/accuracy=0.586017, train/loss=1.872242, validation/accuracy=0.532040, validation/loss=2.139175, validation/num_examples=50000
I0405 20:45:35.876297 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_9460.
I0405 20:45:35.879473 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 9460: RAM USED (GB) 100.092588032
I0405 20:45:51.226836 140422437381888 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.953126, loss=3.018885
I0405 20:45:51.230353 140474721830720 submission.py:119] 9500) loss = 3.019, grad_norm = 0.953
I0405 20:48:59.590574 140422428989184 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.984453, loss=2.920662
I0405 20:48:59.594283 140474721830720 submission.py:119] 10000) loss = 2.921, grad_norm = 0.984
I0405 20:52:08.501595 140422437381888 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.775477, loss=2.868320
I0405 20:52:08.508181 140474721830720 submission.py:119] 10500) loss = 2.868, grad_norm = 0.775
I0405 20:54:05.902292 140474721830720 submission_runner.py:373] Before eval at step 10814: RAM USED (GB) 99.906506752
I0405 20:54:05.905897 140474721830720 spec.py:298] Evaluating on the training split.
I0405 20:54:49.960072 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 20:55:39.967582 140474721830720 spec.py:326] Evaluating on the test split.
I0405 20:55:41.325241 140474721830720 submission_runner.py:382] Time since start: 4907.56s, 	Step: 10814, 	{'train/accuracy': 0.6214724170918368, 'train/loss': 1.6946090386838328, 'validation/accuracy': 0.5633, 'validation/loss': 1.97554015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4381, 'test/loss': 2.68604765625, 'test/num_examples': 10000}
I0405 20:55:41.325647 140474721830720 submission_runner.py:396] After eval at step 10814: RAM USED (GB) 99.862716416
I0405 20:55:41.340765 140422428989184 logging_writer.py:48] [10814] global_step=10814, preemption_count=0, score=4070.799388, test/accuracy=0.438100, test/loss=2.686048, test/num_examples=10000, total_duration=4907.564613, train/accuracy=0.621472, train/loss=1.694609, validation/accuracy=0.563300, validation/loss=1.975540, validation/num_examples=50000
I0405 20:55:41.858911 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_10814.
I0405 20:55:41.859889 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 10814: RAM USED (GB) 99.861782528
I0405 20:56:52.104937 140422437381888 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.777621, loss=2.791005
I0405 20:56:52.110128 140474721830720 submission.py:119] 11000) loss = 2.791, grad_norm = 0.778
I0405 21:00:01.448788 140422428989184 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.793436, loss=2.708171
I0405 21:00:01.453629 140474721830720 submission.py:119] 11500) loss = 2.708, grad_norm = 0.793
I0405 21:03:08.806794 140422437381888 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.710061, loss=2.779764
I0405 21:03:08.811749 140474721830720 submission.py:119] 12000) loss = 2.780, grad_norm = 0.710
I0405 21:04:12.164849 140474721830720 submission_runner.py:373] Before eval at step 12170: RAM USED (GB) 99.676319744
I0405 21:04:12.165069 140474721830720 spec.py:298] Evaluating on the training split.
I0405 21:04:56.201463 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 21:05:47.230759 140474721830720 spec.py:326] Evaluating on the test split.
I0405 21:05:48.594931 140474721830720 submission_runner.py:382] Time since start: 5513.83s, 	Step: 12170, 	{'train/accuracy': 0.6544363839285714, 'train/loss': 1.5370852412009726, 'validation/accuracy': 0.59036, 'validation/loss': 1.8404315625, 'validation/num_examples': 50000, 'test/accuracy': 0.4548, 'test/loss': 2.5992615234375, 'test/num_examples': 10000}
I0405 21:05:48.595257 140474721830720 submission_runner.py:396] After eval at step 12170: RAM USED (GB) 99.72824064
I0405 21:05:48.603350 140422428989184 logging_writer.py:48] [12170] global_step=12170, preemption_count=0, score=4578.884240, test/accuracy=0.454800, test/loss=2.599262, test/num_examples=10000, total_duration=5513.827210, train/accuracy=0.654436, train/loss=1.537085, validation/accuracy=0.590360, validation/loss=1.840432, validation/num_examples=50000
I0405 21:05:49.068874 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_12170.
I0405 21:05:49.069615 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 12170: RAM USED (GB) 99.727253504
I0405 21:07:53.663172 140422437381888 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.848654, loss=2.877982
I0405 21:07:53.667068 140474721830720 submission.py:119] 12500) loss = 2.878, grad_norm = 0.849
I0405 21:11:02.477986 140422428989184 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.862193, loss=2.666956
I0405 21:11:02.481779 140474721830720 submission.py:119] 13000) loss = 2.667, grad_norm = 0.862
I0405 21:14:10.221924 140422437381888 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.553179, loss=2.732164
I0405 21:14:10.226440 140474721830720 submission.py:119] 13500) loss = 2.732, grad_norm = 0.553
I0405 21:14:19.273620 140474721830720 submission_runner.py:373] Before eval at step 13525: RAM USED (GB) 99.98716928
I0405 21:14:19.273834 140474721830720 spec.py:298] Evaluating on the training split.
I0405 21:15:04.952109 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 21:15:57.122463 140474721830720 spec.py:326] Evaluating on the test split.
I0405 21:15:58.470824 140474721830720 submission_runner.py:382] Time since start: 6120.94s, 	Step: 13525, 	{'train/accuracy': 0.6742466517857143, 'train/loss': 1.4305556939572703, 'validation/accuracy': 0.60342, 'validation/loss': 1.7644440625, 'validation/num_examples': 50000, 'test/accuracy': 0.4641, 'test/loss': 2.514997265625, 'test/num_examples': 10000}
I0405 21:15:58.471275 140474721830720 submission_runner.py:396] After eval at step 13525: RAM USED (GB) 100.055580672
I0405 21:15:58.480957 140422428989184 logging_writer.py:48] [13525] global_step=13525, preemption_count=0, score=5086.830373, test/accuracy=0.464100, test/loss=2.514997, test/num_examples=10000, total_duration=6120.935886, train/accuracy=0.674247, train/loss=1.430556, validation/accuracy=0.603420, validation/loss=1.764444, validation/num_examples=50000
I0405 21:15:58.998692 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_13525.
I0405 21:15:58.999721 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 13525: RAM USED (GB) 100.054028288
I0405 21:18:59.216686 140474721830720 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.01670144
I0405 21:18:59.216913 140474721830720 spec.py:298] Evaluating on the training split.
I0405 21:19:42.507554 140474721830720 spec.py:310] Evaluating on the validation split.
I0405 21:20:27.128815 140474721830720 spec.py:326] Evaluating on the test split.
I0405 21:20:28.486355 140474721830720 submission_runner.py:382] Time since start: 6400.88s, 	Step: 14000, 	{'train/accuracy': 0.6582230548469388, 'train/loss': 1.5241585556341677, 'validation/accuracy': 0.58526, 'validation/loss': 1.8638371875, 'validation/num_examples': 50000, 'test/accuracy': 0.4558, 'test/loss': 2.61370625, 'test/num_examples': 10000}
I0405 21:20:28.486675 140474721830720 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 99.866062848
I0405 21:20:28.494814 140422437381888 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5266.228971, test/accuracy=0.455800, test/loss=2.613706, test/num_examples=10000, total_duration=6400.878924, train/accuracy=0.658223, train/loss=1.524159, validation/accuracy=0.585260, validation/loss=1.863837, validation/num_examples=50000
I0405 21:20:28.962626 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:20:28.963398 140474721830720 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 99.86514944
I0405 21:20:28.971234 140422428989184 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5266.228971
I0405 21:20:30.151581 140474721830720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:20:30.505165 140474721830720 submission_runner.py:550] Tuning trial 1/1
I0405 21:20:30.505362 140474721830720 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 21:20:30.506019 140474721830720 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007374043367346938, 'train/loss': 6.9221197634327165, 'validation/accuracy': 0.00072, 'validation/loss': 6.923153125, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.92507109375, 'test/num_examples': 10000, 'score': 8.045020818710327, 'total_duration': 8.046991348266602, 'global_step': 1, 'preemption_count': 0}), (1343, {'train/accuracy': 0.1053890306122449, 'train/loss': 4.877003261021206, 'validation/accuracy': 0.09564, 'validation/loss': 4.962548125, 'validation/num_examples': 50000, 'test/accuracy': 0.0637, 'test/loss': 5.361549609375, 'test/num_examples': 10000, 'score': 515.7824914455414, 'total_duration': 650.851565361023, 'global_step': 1343, 'preemption_count': 0}), (2693, {'train/accuracy': 0.2517936862244898, 'train/loss': 3.6906324114118303, 'validation/accuracy': 0.22554, 'validation/loss': 3.833740625, 'validation/num_examples': 50000, 'test/accuracy': 0.1609, 'test/loss': 4.428838671875, 'test/num_examples': 10000, 'score': 1023.4397826194763, 'total_duration': 1263.6559739112854, 'global_step': 2693, 'preemption_count': 0}), (4046, {'train/accuracy': 0.33376514668367346, 'train/loss': 3.1891376728914222, 'validation/accuracy': 0.29876, 'validation/loss': 3.375463125, 'validation/num_examples': 50000, 'test/accuracy': 0.2205, 'test/loss': 4.0397578125, 'test/num_examples': 10000, 'score': 1531.3051316738129, 'total_duration': 1877.4309182167053, 'global_step': 4046, 'preemption_count': 0}), (5399, {'train/accuracy': 0.44100765306122447, 'train/loss': 2.558208854830995, 'validation/accuracy': 0.4001, 'validation/loss': 2.78012875, 'validation/num_examples': 50000, 'test/accuracy': 0.302, 'test/loss': 3.455589453125, 'test/num_examples': 10000, 'score': 2039.2707023620605, 'total_duration': 2478.6648440361023, 'global_step': 5399, 'preemption_count': 0}), (6752, {'train/accuracy': 0.524055325255102, 'train/loss': 2.1674213020169004, 'validation/accuracy': 0.48284, 'validation/loss': 2.3822628125, 'validation/num_examples': 50000, 'test/accuracy': 0.3596, 'test/loss': 3.1165484375, 'test/num_examples': 10000, 'score': 2547.0589003562927, 'total_duration': 3090.5538098812103, 'global_step': 6752, 'preemption_count': 0}), (8106, {'train/accuracy': 0.5535315688775511, 'train/loss': 2.013788495744978, 'validation/accuracy': 0.50886, 'validation/loss': 2.24752328125, 'validation/num_examples': 50000, 'test/accuracy': 0.3819, 'test/loss': 2.9840154296875, 'test/num_examples': 10000, 'score': 3055.134121656418, 'total_duration': 3697.6643345355988, 'global_step': 8106, 'preemption_count': 0}), (9460, {'train/accuracy': 0.5860172193877551, 'train/loss': 1.8722417403240592, 'validation/accuracy': 0.53204, 'validation/loss': 2.13917515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4114, 'test/loss': 2.8341126953125, 'test/num_examples': 10000, 'score': 3563.060268163681, 'total_duration': 4299.112677335739, 'global_step': 9460, 'preemption_count': 0}), (10814, {'train/accuracy': 0.6214724170918368, 'train/loss': 1.6946090386838328, 'validation/accuracy': 0.5633, 'validation/loss': 1.97554015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4381, 'test/loss': 2.68604765625, 'test/num_examples': 10000, 'score': 4070.799387693405, 'total_duration': 4907.564613103867, 'global_step': 10814, 'preemption_count': 0}), (12170, {'train/accuracy': 0.6544363839285714, 'train/loss': 1.5370852412009726, 'validation/accuracy': 0.59036, 'validation/loss': 1.8404315625, 'validation/num_examples': 50000, 'test/accuracy': 0.4548, 'test/loss': 2.5992615234375, 'test/num_examples': 10000, 'score': 4578.88424038887, 'total_duration': 5513.827209711075, 'global_step': 12170, 'preemption_count': 0}), (13525, {'train/accuracy': 0.6742466517857143, 'train/loss': 1.4305556939572703, 'validation/accuracy': 0.60342, 'validation/loss': 1.7644440625, 'validation/num_examples': 50000, 'test/accuracy': 0.4641, 'test/loss': 2.514997265625, 'test/num_examples': 10000, 'score': 5086.830373287201, 'total_duration': 6120.935885667801, 'global_step': 13525, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6582230548469388, 'train/loss': 1.5241585556341677, 'validation/accuracy': 0.58526, 'validation/loss': 1.8638371875, 'validation/num_examples': 50000, 'test/accuracy': 0.4558, 'test/loss': 2.61370625, 'test/num_examples': 10000, 'score': 5266.228971242905, 'total_duration': 6400.878923892975, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 21:20:30.506121 140474721830720 submission_runner.py:553] Timing: 5266.228971242905
I0405 21:20:30.506162 140474721830720 submission_runner.py:554] ====================
I0405 21:20:30.506251 140474721830720 submission_runner.py:613] Final imagenet_resnet score: 5266.228971242905
