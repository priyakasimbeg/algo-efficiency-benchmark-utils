I0404 19:49:25.921873 140193239037760 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax.
I0404 19:49:25.975770 140193239037760 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 19:49:26.839307 140193239037760 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 19:49:26.840126 140193239037760 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 19:49:26.846566 140193239037760 submission_runner.py:511] Using RNG seed 1163227855
I0404 19:49:29.124135 140193239037760 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 19:49:29.124316 140193239037760 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1.
I0404 19:49:29.124488 140193239037760 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/hparams.json.
I0404 19:49:29.251748 140193239037760 submission_runner.py:230] Starting train once: RAM USED (GB) 4.233371648
I0404 19:49:29.251902 140193239037760 submission_runner.py:231] Initializing dataset.
I0404 19:49:29.263469 140193239037760 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:29.271644 140193239037760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:49:29.271763 140193239037760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:49:29.507717 140193239037760 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:49:36.172957 140193239037760 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.405047296
I0404 19:49:36.173175 140193239037760 submission_runner.py:240] Initializing model.
I0404 19:49:46.821325 140193239037760 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.418926592
I0404 19:49:46.821514 140193239037760 submission_runner.py:252] Initializing optimizer.
I0404 19:49:47.296280 140193239037760 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.418926592
I0404 19:49:47.296455 140193239037760 submission_runner.py:261] Initializing metrics bundle.
I0404 19:49:47.296503 140193239037760 submission_runner.py:276] Initializing checkpoint and logger.
I0404 19:49:47.297302 140193239037760 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0404 19:49:48.130045 140193239037760 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/meta_data_0.json.
I0404 19:49:48.131008 140193239037760 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/flags_0.json.
I0404 19:49:48.135430 140193239037760 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.416718848
I0404 19:49:48.135671 140193239037760 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.416718848
I0404 19:49:48.135731 140193239037760 submission_runner.py:313] Starting training loop.
I0404 19:49:51.248677 140193239037760 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.250622976
I0404 19:50:33.695565 140017494120192 logging_writer.py:48] [0] global_step=0, grad_norm=0.2750363051891327, loss=6.907756328582764
I0404 19:50:33.708342 140193239037760 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 46.541463552
I0404 19:50:33.708575 140193239037760 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 46.541463552
I0404 19:50:33.708652 140193239037760 spec.py:298] Evaluating on the training split.
I0404 19:50:33.714491 140193239037760 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:33.720780 140193239037760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:33.720924 140193239037760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:33.779831 140193239037760 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:53.377215 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 19:50:53.385306 140193239037760 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:50:53.398369 140193239037760 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 19:50:53.398651 140193239037760 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 19:50:53.462053 140193239037760 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 19:51:12.092043 140193239037760 spec.py:326] Evaluating on the test split.
I0404 19:51:12.098504 140193239037760 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:51:12.103115 140193239037760 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 19:51:12.134002 140193239037760 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 19:51:23.094351 140193239037760 submission_runner.py:382] Time since start: 45.57s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0404 19:51:23.094764 140193239037760 submission_runner.py:396] After eval at step 1: RAM USED (GB) 106.001473536
I0404 19:51:23.102487 139955871414016 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=45.492288, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=45.572917, train/accuracy=0.000977, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0404 19:51:23.209639 140193239037760 checkpoints.py:356] Saving checkpoint at step: 1
I0404 19:51:23.816048 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_1
I0404 19:51:23.817004 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_1.
I0404 19:51:23.818653 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 106.07202304
I0404 19:51:23.824390 140193239037760 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 106.07202304
I0404 19:51:37.182214 140193239037760 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.480425472
I0404 19:52:17.311228 140016856553216 logging_writer.py:48] [100] global_step=100, grad_norm=0.2740617096424103, loss=6.906895160675049
I0404 19:52:58.184368 140016864945920 logging_writer.py:48] [200] global_step=200, grad_norm=0.28995615243911743, loss=6.898902893066406
I0404 19:53:39.152873 140016856553216 logging_writer.py:48] [300] global_step=300, grad_norm=0.28210631012916565, loss=6.883166313171387
I0404 19:54:20.067471 140016864945920 logging_writer.py:48] [400] global_step=400, grad_norm=0.4367746412754059, loss=6.861698150634766
I0404 19:55:00.873005 140016856553216 logging_writer.py:48] [500] global_step=500, grad_norm=0.718643307685852, loss=6.743628978729248
I0404 19:55:41.857995 140016864945920 logging_writer.py:48] [600] global_step=600, grad_norm=0.9492285251617432, loss=6.6806840896606445
I0404 19:56:22.795479 140016856553216 logging_writer.py:48] [700] global_step=700, grad_norm=1.2310758829116821, loss=6.579444885253906
I0404 19:57:03.853504 140016864945920 logging_writer.py:48] [800] global_step=800, grad_norm=0.86921626329422, loss=6.539899826049805
I0404 19:57:44.854228 140016856553216 logging_writer.py:48] [900] global_step=900, grad_norm=0.6331881284713745, loss=6.685089111328125
I0404 19:58:23.922410 140193239037760 submission_runner.py:373] Before eval at step 997: RAM USED (GB) 79.790678016
I0404 19:58:23.922753 140193239037760 spec.py:298] Evaluating on the training split.
I0404 19:58:35.085970 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 19:58:41.597510 140193239037760 spec.py:326] Evaluating on the test split.
I0404 19:58:43.681662 140193239037760 submission_runner.py:382] Time since start: 515.78s, 	Step: 997, 	{'train/accuracy': 0.03501953184604645, 'train/loss': 6.060697555541992, 'validation/accuracy': 0.03328000009059906, 'validation/loss': 6.088078022003174, 'validation/num_examples': 50000, 'test/accuracy': 0.027800001204013824, 'test/loss': 6.1809983253479, 'test/num_examples': 10000}
I0404 19:58:43.682421 140193239037760 submission_runner.py:396] After eval at step 997: RAM USED (GB) 85.832241152
I0404 19:58:43.697225 139956248889088 logging_writer.py:48] [997] global_step=997, preemption_count=0, score=458.885335, test/accuracy=0.027800, test/loss=6.180998, test/num_examples=10000, total_duration=515.784222, train/accuracy=0.035020, train/loss=6.060698, validation/accuracy=0.033280, validation/loss=6.088078, validation/num_examples=50000
I0404 19:58:44.644676 140193239037760 checkpoints.py:356] Saving checkpoint at step: 997
I0404 19:58:45.373146 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_997
I0404 19:58:45.386626 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_997.
I0404 19:58:45.388757 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 997: RAM USED (GB) 93.311705088
I0404 19:58:47.047147 139956257281792 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1696406602859497, loss=6.476869583129883
I0404 19:59:28.026057 140017468942080 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0949246883392334, loss=6.48103141784668
I0404 20:00:08.844959 139956257281792 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3650389909744263, loss=6.471034049987793
I0404 20:00:49.833971 140017468942080 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8534146547317505, loss=6.419813632965088
I0404 20:01:30.758065 139956257281792 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2998145818710327, loss=6.368927478790283
I0404 20:02:11.619669 140017468942080 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.850757360458374, loss=6.359927654266357
I0404 20:02:52.824525 139956257281792 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9793968200683594, loss=6.36798620223999
I0404 20:03:34.128753 140017468942080 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8985837697982788, loss=6.304614543914795
I0404 20:04:15.356396 139956257281792 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8266335129737854, loss=6.288995265960693
I0404 20:04:56.447789 140017468942080 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8909059166908264, loss=6.25827169418335
I0404 20:05:37.591934 139956257281792 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7375829815864563, loss=6.174368858337402
I0404 20:05:45.674454 140193239037760 submission_runner.py:373] Before eval at step 2021: RAM USED (GB) 86.638272512
I0404 20:05:45.674753 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:05:56.821396 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:06:03.651127 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:06:05.351880 140193239037760 submission_runner.py:382] Time since start: 957.54s, 	Step: 2021, 	{'train/accuracy': 0.06671874970197678, 'train/loss': 5.562770366668701, 'validation/accuracy': 0.06379999965429306, 'validation/loss': 5.5974812507629395, 'validation/num_examples': 50000, 'test/accuracy': 0.048900000751018524, 'test/loss': 5.785606384277344, 'test/num_examples': 10000}
I0404 20:06:05.352607 140193239037760 submission_runner.py:396] After eval at step 2021: RAM USED (GB) 90.000252928
I0404 20:06:05.367986 140017468942080 logging_writer.py:48] [2021] global_step=2021, preemption_count=0, score=871.876195, test/accuracy=0.048900, test/loss=5.785606, test/num_examples=10000, total_duration=957.536308, train/accuracy=0.066719, train/loss=5.562770, validation/accuracy=0.063800, validation/loss=5.597481, validation/num_examples=50000
I0404 20:06:05.661153 140193239037760 checkpoints.py:356] Saving checkpoint at step: 2021
I0404 20:06:07.658403 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_2021
I0404 20:06:07.672984 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_2021.
I0404 20:06:07.675365 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 2021: RAM USED (GB) 99.165589504
I0404 20:06:40.256293 139956257281792 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8106164932250977, loss=6.241678237915039
I0404 20:07:21.161059 140017435371264 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7461918592453003, loss=6.181828498840332
I0404 20:08:02.086510 139956257281792 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8598742485046387, loss=6.462578773498535
I0404 20:08:43.179090 140017435371264 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9607756733894348, loss=6.216917037963867
I0404 20:09:24.293947 139956257281792 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1093305349349976, loss=6.138916969299316
I0404 20:10:05.450049 140017435371264 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7146400213241577, loss=6.173313140869141
I0404 20:10:46.641532 139956257281792 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8427895307540894, loss=6.186853408813477
I0404 20:11:27.785985 140017435371264 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.891017496585846, loss=6.116641998291016
I0404 20:12:08.899268 139956257281792 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7337058782577515, loss=6.556062698364258
I0404 20:12:50.109789 140017435371264 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7362590432167053, loss=6.186966896057129
I0404 20:13:07.888418 140193239037760 submission_runner.py:373] Before eval at step 3045: RAM USED (GB) 92.585189376
I0404 20:13:07.888689 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:13:19.255929 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:13:26.254785 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:13:27.929095 140193239037760 submission_runner.py:382] Time since start: 1399.75s, 	Step: 3045, 	{'train/accuracy': 0.1023242175579071, 'train/loss': 5.172773838043213, 'validation/accuracy': 0.09425999969244003, 'validation/loss': 5.228543281555176, 'validation/num_examples': 50000, 'test/accuracy': 0.07260000705718994, 'test/loss': 5.468011379241943, 'test/num_examples': 10000}
I0404 20:13:27.929616 140193239037760 submission_runner.py:396] After eval at step 3045: RAM USED (GB) 94.73570816
I0404 20:13:27.939668 139956257281792 logging_writer.py:48] [3045] global_step=3045, preemption_count=0, score=1284.960042, test/accuracy=0.072600, test/loss=5.468011, test/num_examples=10000, total_duration=1399.750044, train/accuracy=0.102324, train/loss=5.172774, validation/accuracy=0.094260, validation/loss=5.228543, validation/num_examples=50000
I0404 20:13:28.051259 140193239037760 checkpoints.py:356] Saving checkpoint at step: 3045
I0404 20:13:30.176205 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_3045
I0404 20:13:30.188875 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_3045.
I0404 20:13:30.190890 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 3045: RAM USED (GB) 103.450349568
I0404 20:13:54.630934 140017435371264 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7523486018180847, loss=6.076540470123291
I0404 20:14:35.699863 140017426978560 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7017187476158142, loss=6.141652584075928
I0404 20:15:16.528227 140017435371264 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8590670824050903, loss=6.035734176635742
I0404 20:15:57.485963 140017426978560 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.798103928565979, loss=6.052125930786133
I0404 20:16:38.672713 140017435371264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8455066084861755, loss=6.017461776733398
I0404 20:17:19.898209 140017426978560 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9143277406692505, loss=6.03217077255249
I0404 20:18:01.226791 140017435371264 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6015774011611938, loss=6.543484687805176
I0404 20:18:42.483325 140017426978560 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6140400171279907, loss=6.064409255981445
I0404 20:19:23.610075 140017435371264 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.804402232170105, loss=5.961496829986572
I0404 20:20:04.725340 140017426978560 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6033153533935547, loss=6.189223289489746
I0404 20:20:30.351318 140193239037760 submission_runner.py:373] Before eval at step 4064: RAM USED (GB) 98.527789056
I0404 20:20:30.351568 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:20:41.976135 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:20:49.944406 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:20:51.600731 140193239037760 submission_runner.py:382] Time since start: 1842.21s, 	Step: 4064, 	{'train/accuracy': 0.13499999046325684, 'train/loss': 4.857805252075195, 'validation/accuracy': 0.11738000065088272, 'validation/loss': 4.977529048919678, 'validation/num_examples': 50000, 'test/accuracy': 0.08590000122785568, 'test/loss': 5.272126197814941, 'test/num_examples': 10000}
I0404 20:20:51.601158 140193239037760 submission_runner.py:396] After eval at step 4064: RAM USED (GB) 101.195337728
I0404 20:20:51.608549 140017435371264 logging_writer.py:48] [4064] global_step=4064, preemption_count=0, score=1696.444488, test/accuracy=0.085900, test/loss=5.272126, test/num_examples=10000, total_duration=1842.213887, train/accuracy=0.135000, train/loss=4.857805, validation/accuracy=0.117380, validation/loss=4.977529, validation/num_examples=50000
I0404 20:20:52.969599 140193239037760 checkpoints.py:356] Saving checkpoint at step: 4064
I0404 20:20:53.871920 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_4064
I0404 20:20:53.873592 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_4064.
I0404 20:20:53.875837 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 4064: RAM USED (GB) 105.762922496
I0404 20:21:09.108021 140017426978560 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6514227986335754, loss=6.5326924324035645
I0404 20:21:50.102047 140016923694848 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.0973215103149414, loss=5.973196029663086
I0404 20:22:31.042688 140017426978560 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8123130202293396, loss=6.702755928039551
I0404 20:23:12.252351 140016923694848 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7036005258560181, loss=5.822843551635742
I0404 20:23:53.464178 140017426978560 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6928379535675049, loss=5.841866970062256
I0404 20:24:34.814532 140016923694848 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7962743043899536, loss=5.782904624938965
I0404 20:25:15.993842 140017426978560 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6740473508834839, loss=6.420100688934326
I0404 20:25:57.284270 140016923694848 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7099881768226624, loss=5.883852481842041
I0404 20:26:38.569486 140017426978560 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7874137759208679, loss=5.778536796569824
I0404 20:27:19.984135 140016923694848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6392097473144531, loss=6.605208873748779
I0404 20:27:53.917548 140193239037760 submission_runner.py:373] Before eval at step 5084: RAM USED (GB) 104.82311168
I0404 20:27:53.917841 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:28:06.083208 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:28:12.953773 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:28:14.628785 140193239037760 submission_runner.py:382] Time since start: 2285.78s, 	Step: 5084, 	{'train/accuracy': 0.1638476550579071, 'train/loss': 4.583833694458008, 'validation/accuracy': 0.15053999423980713, 'validation/loss': 4.666473865509033, 'validation/num_examples': 50000, 'test/accuracy': 0.11740000545978546, 'test/loss': 4.996120929718018, 'test/num_examples': 10000}
I0404 20:28:14.629239 140193239037760 submission_runner.py:396] After eval at step 5084: RAM USED (GB) 106.824704
I0404 20:28:14.640781 140017426978560 logging_writer.py:48] [5084] global_step=5084, preemption_count=0, score=2109.279014, test/accuracy=0.117400, test/loss=4.996121, test/num_examples=10000, total_duration=2285.779345, train/accuracy=0.163848, train/loss=4.583834, validation/accuracy=0.150540, validation/loss=4.666474, validation/num_examples=50000
I0404 20:28:14.741688 140193239037760 checkpoints.py:356] Saving checkpoint at step: 5084
I0404 20:28:16.716464 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_5084
I0404 20:28:16.728931 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_5084.
I0404 20:28:16.731393 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 5084: RAM USED (GB) 114.969485312
I0404 20:28:23.729113 140016923694848 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7437796592712402, loss=5.855051040649414
I0404 20:29:04.628518 140016915302144 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8379635810852051, loss=5.732094764709473
I0404 20:29:45.560046 140016923694848 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7286544442176819, loss=5.794866561889648
I0404 20:30:26.491858 140016915302144 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7530757188796997, loss=5.694351673126221
I0404 20:31:07.656704 140016923694848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.779629647731781, loss=5.7853593826293945
I0404 20:31:49.378836 140016915302144 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6693950891494751, loss=5.61462926864624
I0404 20:32:30.737097 140016923694848 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7592073678970337, loss=5.591200828552246
I0404 20:33:12.104598 140016915302144 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5839356184005737, loss=6.380727767944336
I0404 20:33:53.407939 140016923694848 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6924198865890503, loss=5.746972560882568
I0404 20:34:34.705999 140016915302144 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7360631823539734, loss=5.740347862243652
I0404 20:35:16.328985 140016923694848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6882638335227966, loss=5.495978832244873
I0404 20:35:16.915459 140193239037760 submission_runner.py:373] Before eval at step 6103: RAM USED (GB) 110.88470016
I0404 20:35:16.915865 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:35:29.225352 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:35:37.599800 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:35:39.274098 140193239037760 submission_runner.py:382] Time since start: 2728.78s, 	Step: 6103, 	{'train/accuracy': 0.20412108302116394, 'train/loss': 4.221634387969971, 'validation/accuracy': 0.1872600018978119, 'validation/loss': 4.314670085906982, 'validation/num_examples': 50000, 'test/accuracy': 0.14300000667572021, 'test/loss': 4.683260917663574, 'test/num_examples': 10000}
I0404 20:35:39.274732 140193239037760 submission_runner.py:396] After eval at step 6103: RAM USED (GB) 114.673577984
I0404 20:35:39.285318 140016915302144 logging_writer.py:48] [6103] global_step=6103, preemption_count=0, score=2520.829085, test/accuracy=0.143000, test/loss=4.683261, test/num_examples=10000, total_duration=2728.777278, train/accuracy=0.204121, train/loss=4.221634, validation/accuracy=0.187260, validation/loss=4.314670, validation/num_examples=50000
I0404 20:35:39.393041 140193239037760 checkpoints.py:356] Saving checkpoint at step: 6103
I0404 20:35:40.210078 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_6103
I0404 20:35:40.224080 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_6103.
I0404 20:35:40.226492 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 6103: RAM USED (GB) 118.580404224
I0404 20:36:20.062013 140016923694848 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8333680629730225, loss=5.632156848907471
I0404 20:37:01.136671 140016906909440 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7295384407043457, loss=5.898638725280762
I0404 20:37:42.547536 140016923694848 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7644698023796082, loss=5.500514984130859
I0404 20:38:23.661479 140016906909440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.466239869594574, loss=6.433979511260986
I0404 20:39:05.121353 140016923694848 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6272200345993042, loss=5.574296474456787
I0404 20:39:46.479403 140016906909440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7359260320663452, loss=5.4954376220703125
I0404 20:40:28.111997 140016923694848 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.630425751209259, loss=5.434663772583008
I0404 20:41:09.385562 140016906909440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5778266191482544, loss=6.376908779144287
I0404 20:41:50.789463 140016923694848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4861944913864136, loss=6.494210243225098
I0404 20:42:32.185549 140016906909440 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.675895631313324, loss=5.366556644439697
I0404 20:42:40.248413 140193239037760 submission_runner.py:373] Before eval at step 7121: RAM USED (GB) 116.747026432
I0404 20:42:40.248667 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:42:53.799582 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:43:02.453019 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:43:04.121215 140193239037760 submission_runner.py:382] Time since start: 3172.11s, 	Step: 7121, 	{'train/accuracy': 0.2347070276737213, 'train/loss': 4.046520709991455, 'validation/accuracy': 0.21383999288082123, 'validation/loss': 4.164851665496826, 'validation/num_examples': 50000, 'test/accuracy': 0.16030000150203705, 'test/loss': 4.580754280090332, 'test/num_examples': 10000}
I0404 20:43:04.121795 140193239037760 submission_runner.py:396] After eval at step 7121: RAM USED (GB) 121.249378304
I0404 20:43:04.134613 140016923694848 logging_writer.py:48] [7121] global_step=7121, preemption_count=0, score=2932.781233, test/accuracy=0.160300, test/loss=4.580754, test/num_examples=10000, total_duration=3172.110463, train/accuracy=0.234707, train/loss=4.046521, validation/accuracy=0.213840, validation/loss=4.164852, validation/num_examples=50000
I0404 20:43:04.234602 140193239037760 checkpoints.py:356] Saving checkpoint at step: 7121
I0404 20:43:04.927153 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_7121
I0404 20:43:04.942747 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_7121.
I0404 20:43:04.944607 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 7121: RAM USED (GB) 124.615450624
I0404 20:43:37.825148 140016906909440 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6253324747085571, loss=5.376889705657959
I0404 20:44:18.851901 140016898516736 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.47042229771614075, loss=6.228876113891602
I0404 20:45:00.099568 140016906909440 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7654192447662354, loss=5.221139907836914
I0404 20:45:41.664550 140016898516736 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.676533043384552, loss=5.375063896179199
I0404 20:46:23.177232 140016906909440 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6585089564323425, loss=6.095806121826172
I0404 20:47:04.527959 140016898516736 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6855995059013367, loss=5.355612277984619
I0404 20:47:45.953955 140016906909440 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6793375611305237, loss=5.235785007476807
I0404 20:48:27.488812 140016898516736 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7559618353843689, loss=5.42803430557251
I0404 20:49:09.456294 140016906909440 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7163311243057251, loss=5.194332122802734
I0404 20:49:51.288452 140016898516736 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7023189663887024, loss=5.273856163024902
I0404 20:50:05.341713 140193239037760 submission_runner.py:373] Before eval at step 8136: RAM USED (GB) 122.365775872
I0404 20:50:05.342067 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:50:19.420773 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:50:28.316582 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:50:29.973855 140193239037760 submission_runner.py:382] Time since start: 3617.20s, 	Step: 8136, 	{'train/accuracy': 0.28166013956069946, 'train/loss': 3.6851370334625244, 'validation/accuracy': 0.2540600001811981, 'validation/loss': 3.8377623558044434, 'validation/num_examples': 50000, 'test/accuracy': 0.19440001249313354, 'test/loss': 4.295205116271973, 'test/num_examples': 10000}
I0404 20:50:29.974418 140193239037760 submission_runner.py:396] After eval at step 8136: RAM USED (GB) 126.525198336
I0404 20:50:29.988789 140016906909440 logging_writer.py:48] [8136] global_step=8136, preemption_count=0, score=3343.656057, test/accuracy=0.194400, test/loss=4.295205, test/num_examples=10000, total_duration=3617.203716, train/accuracy=0.281660, train/loss=3.685137, validation/accuracy=0.254060, validation/loss=3.837762, validation/num_examples=50000
I0404 20:50:30.091814 140193239037760 checkpoints.py:356] Saving checkpoint at step: 8136
I0404 20:50:30.911566 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_8136
I0404 20:50:30.924718 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_8136.
I0404 20:50:30.927795 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 8136: RAM USED (GB) 130.27104768
I0404 20:50:57.594858 140016898516736 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6999711394309998, loss=5.195727825164795
I0404 20:51:38.674317 140016890124032 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6827259659767151, loss=5.230132102966309
I0404 20:52:20.046081 140016898516736 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4907187521457672, loss=6.395915985107422
I0404 20:53:01.359504 140016890124032 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.710418164730072, loss=5.2533440589904785
I0404 20:53:42.777566 140016898516736 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5358697772026062, loss=6.459851264953613
I0404 20:54:24.149469 140016890124032 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.643922746181488, loss=5.175726413726807
I0404 20:55:05.687888 140016898516736 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6536208987236023, loss=5.207906723022461
I0404 20:55:47.022786 140016890124032 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.49075186252593994, loss=5.736837387084961
I0404 20:56:28.655938 140016898516736 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6365783214569092, loss=5.15777587890625
I0404 20:57:09.974000 140016890124032 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5943183302879333, loss=5.084877967834473
I0404 20:57:31.309291 140193239037760 submission_runner.py:373] Before eval at step 9153: RAM USED (GB) 128.927305728
I0404 20:57:31.309580 140193239037760 spec.py:298] Evaluating on the training split.
I0404 20:57:45.384268 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 20:57:54.981790 140193239037760 spec.py:326] Evaluating on the test split.
I0404 20:57:56.662639 140193239037760 submission_runner.py:382] Time since start: 4063.17s, 	Step: 9153, 	{'train/accuracy': 0.3019140660762787, 'train/loss': 3.5538060665130615, 'validation/accuracy': 0.2780199944972992, 'validation/loss': 3.6878199577331543, 'validation/num_examples': 50000, 'test/accuracy': 0.21470001339912415, 'test/loss': 4.140396595001221, 'test/num_examples': 10000}
I0404 20:57:56.663364 140193239037760 submission_runner.py:396] After eval at step 9153: RAM USED (GB) 135.110049792
I0404 20:57:56.677243 140016898516736 logging_writer.py:48] [9153] global_step=9153, preemption_count=0, score=3754.471950, test/accuracy=0.214700, test/loss=4.140397, test/num_examples=10000, total_duration=4063.172050, train/accuracy=0.301914, train/loss=3.553806, validation/accuracy=0.278020, validation/loss=3.687820, validation/num_examples=50000
I0404 20:57:56.779876 140193239037760 checkpoints.py:356] Saving checkpoint at step: 9153
I0404 20:57:57.613680 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_9153
I0404 20:57:57.627503 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_9153.
I0404 20:57:57.629848 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 9153: RAM USED (GB) 139.181973504
I0404 20:58:17.431012 140016890124032 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6403325796127319, loss=5.257498264312744
I0404 20:58:58.686705 140015732520704 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6812246441841125, loss=5.124200820922852
I0404 20:59:40.004037 140016890124032 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7087261080741882, loss=5.16612434387207
I0404 21:00:21.745858 140015732520704 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.45878589153289795, loss=5.98687744140625
I0404 21:01:02.957618 140016890124032 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4329862892627716, loss=6.358774185180664
I0404 21:01:44.642199 140015732520704 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7042617201805115, loss=5.09113883972168
I0404 21:02:26.030229 140016890124032 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5982246994972229, loss=5.038260459899902
I0404 21:03:07.259663 140015732520704 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6317431926727295, loss=5.067420959472656
I0404 21:03:48.719923 140016890124032 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5749998688697815, loss=5.51298189163208
I0404 21:04:30.276599 140015732520704 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6375842690467834, loss=5.076624393463135
I0404 21:04:57.817303 140193239037760 submission_runner.py:373] Before eval at step 10168: RAM USED (GB) 134.596132864
I0404 21:04:57.817561 140193239037760 spec.py:298] Evaluating on the training split.
I0404 21:05:11.997319 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 21:05:22.008904 140193239037760 spec.py:326] Evaluating on the test split.
I0404 21:05:23.675198 140193239037760 submission_runner.py:382] Time since start: 4509.68s, 	Step: 10168, 	{'train/accuracy': 0.3335937559604645, 'train/loss': 3.4605894088745117, 'validation/accuracy': 0.3076399862766266, 'validation/loss': 3.5919923782348633, 'validation/num_examples': 50000, 'test/accuracy': 0.23250001668930054, 'test/loss': 4.089896202087402, 'test/num_examples': 10000}
I0404 21:05:23.675846 140193239037760 submission_runner.py:396] After eval at step 10168: RAM USED (GB) 141.56988416
I0404 21:05:23.689502 140016890124032 logging_writer.py:48] [10168] global_step=10168, preemption_count=0, score=4164.422290, test/accuracy=0.232500, test/loss=4.089896, test/num_examples=10000, total_duration=4509.679208, train/accuracy=0.333594, train/loss=3.460589, validation/accuracy=0.307640, validation/loss=3.591992, validation/num_examples=50000
I0404 21:05:23.841664 140193239037760 checkpoints.py:356] Saving checkpoint at step: 10168
I0404 21:05:24.711668 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_10168
I0404 21:05:24.726783 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_10168.
I0404 21:05:24.739962 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 10168: RAM USED (GB) 145.820278784
I0404 21:05:38.253450 140015732520704 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6845763921737671, loss=5.217441558837891
I0404 21:06:19.092086 140015724128000 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5574720501899719, loss=5.708276748657227
I0404 21:07:00.638308 140015732520704 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.48091086745262146, loss=6.15762186050415
I0404 21:07:41.981301 140015724128000 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.725837767124176, loss=5.306230545043945
I0404 21:08:23.481902 140015732520704 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5404791831970215, loss=6.3402886390686035
I0404 21:09:05.168531 140015724128000 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.504873514175415, loss=5.900214195251465
I0404 21:09:46.390389 140015732520704 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6137256622314453, loss=4.93818998336792
I0404 21:10:27.882258 140015724128000 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5793343186378479, loss=4.989505290985107
I0404 21:11:09.207401 140015732520704 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.49221670627593994, loss=5.766548156738281
I0404 21:11:50.688881 140015724128000 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5429803729057312, loss=6.256036758422852
I0404 21:12:25.085664 140193239037760 submission_runner.py:373] Before eval at step 11185: RAM USED (GB) 141.146968064
I0404 21:12:25.085958 140193239037760 spec.py:298] Evaluating on the training split.
I0404 21:12:39.558711 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 21:12:49.663819 140193239037760 spec.py:326] Evaluating on the test split.
I0404 21:12:51.337220 140193239037760 submission_runner.py:382] Time since start: 4956.95s, 	Step: 11185, 	{'train/accuracy': 0.36302733421325684, 'train/loss': 3.246645212173462, 'validation/accuracy': 0.3323200047016144, 'validation/loss': 3.392300844192505, 'validation/num_examples': 50000, 'test/accuracy': 0.2507000267505646, 'test/loss': 3.919424295425415, 'test/num_examples': 10000}
I0404 21:12:51.337805 140193239037760 submission_runner.py:396] After eval at step 11185: RAM USED (GB) 148.092637184
I0404 21:12:51.358509 140015732520704 logging_writer.py:48] [11185] global_step=11185, preemption_count=0, score=4574.928731, test/accuracy=0.250700, test/loss=3.919424, test/num_examples=10000, total_duration=4956.948428, train/accuracy=0.363027, train/loss=3.246645, validation/accuracy=0.332320, validation/loss=3.392301, validation/num_examples=50000
I0404 21:12:51.467063 140193239037760 checkpoints.py:356] Saving checkpoint at step: 11185
I0404 21:12:52.285594 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_11185
I0404 21:12:52.300489 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_11185.
I0404 21:12:52.302741 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 11185: RAM USED (GB) 152.06027264
I0404 21:12:58.886203 140015724128000 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6468291878700256, loss=4.845887184143066
I0404 21:13:39.845844 140015715735296 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7665386199951172, loss=4.936993598937988
I0404 21:14:21.072142 140015724128000 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5989106893539429, loss=5.016801357269287
I0404 21:15:02.608209 140015715735296 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5474249124526978, loss=5.631649494171143
I0404 21:15:44.218105 140015724128000 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6248313784599304, loss=4.9370317459106445
I0404 21:16:25.728316 140015715735296 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5490416884422302, loss=5.343197822570801
I0404 21:17:07.133798 140015724128000 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7075725793838501, loss=4.821041584014893
I0404 21:17:48.467045 140015715735296 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.43063169717788696, loss=6.322254657745361
I0404 21:18:29.852953 140015724128000 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7063092589378357, loss=4.766541481018066
I0404 21:19:11.302502 140015715735296 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5781342387199402, loss=4.808053016662598
I0404 21:19:52.471514 140015724128000 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.47395551204681396, loss=5.755375862121582
I0404 21:19:52.484742 140193239037760 submission_runner.py:373] Before eval at step 12201: RAM USED (GB) 146.237018112
I0404 21:19:52.485066 140193239037760 spec.py:298] Evaluating on the training split.
I0404 21:20:07.147200 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 21:20:17.369810 140193239037760 spec.py:326] Evaluating on the test split.
I0404 21:20:19.032945 140193239037760 submission_runner.py:382] Time since start: 5404.35s, 	Step: 12201, 	{'train/accuracy': 0.39054685831069946, 'train/loss': 3.096827268600464, 'validation/accuracy': 0.35659998655319214, 'validation/loss': 3.263266086578369, 'validation/num_examples': 50000, 'test/accuracy': 0.2661000192165375, 'test/loss': 3.810218095779419, 'test/num_examples': 10000}
I0404 21:20:19.033407 140193239037760 submission_runner.py:396] After eval at step 12201: RAM USED (GB) 153.422000128
I0404 21:20:19.043765 140015715735296 logging_writer.py:48] [12201] global_step=12201, preemption_count=0, score=4985.411438, test/accuracy=0.266100, test/loss=3.810218, test/num_examples=10000, total_duration=5404.345961, train/accuracy=0.390547, train/loss=3.096827, validation/accuracy=0.356600, validation/loss=3.263266, validation/num_examples=50000
I0404 21:20:19.148856 140193239037760 checkpoints.py:356] Saving checkpoint at step: 12201
I0404 21:20:20.004257 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_12201
I0404 21:20:20.018907 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_12201.
I0404 21:20:20.029703 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 12201: RAM USED (GB) 157.484388352
I0404 21:21:01.307199 140015724128000 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5983577370643616, loss=5.20242977142334
I0404 21:21:42.828580 140015707342592 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6151853799819946, loss=5.141203880310059
I0404 21:22:25.037579 140015724128000 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.581982433795929, loss=4.943594932556152
I0404 21:23:06.443092 140015707342592 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.608989417552948, loss=5.200692653656006
I0404 21:23:48.085386 140015724128000 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5952562689781189, loss=4.806356430053711
I0404 21:24:29.803855 140015707342592 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5107607841491699, loss=5.978885650634766
I0404 21:25:11.261819 140015724128000 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6634942889213562, loss=4.81365442276001
I0404 21:25:53.020487 140015707342592 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8692789673805237, loss=4.714473247528076
I0404 21:26:34.472040 140015724128000 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6916555166244507, loss=4.823477745056152
I0404 21:27:16.107354 140015707342592 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4631700813770294, loss=6.246391296386719
I0404 21:27:20.342417 140193239037760 submission_runner.py:373] Before eval at step 13212: RAM USED (GB) 154.085330944
I0404 21:27:20.342658 140193239037760 spec.py:298] Evaluating on the training split.
I0404 21:27:34.969447 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 21:27:45.211858 140193239037760 spec.py:326] Evaluating on the test split.
I0404 21:27:46.869342 140193239037760 submission_runner.py:382] Time since start: 5852.20s, 	Step: 13212, 	{'train/accuracy': 0.42970702052116394, 'train/loss': 2.7873871326446533, 'validation/accuracy': 0.37901997566223145, 'validation/loss': 3.0460221767425537, 'validation/num_examples': 50000, 'test/accuracy': 0.29100000858306885, 'test/loss': 3.6194326877593994, 'test/num_examples': 10000}
I0404 21:27:46.869914 140193239037760 submission_runner.py:396] After eval at step 13212: RAM USED (GB) 159.80630016
I0404 21:27:46.884415 140015724128000 logging_writer.py:48] [13212] global_step=13212, preemption_count=0, score=5394.509388, test/accuracy=0.291000, test/loss=3.619433, test/num_examples=10000, total_duration=5852.204654, train/accuracy=0.429707, train/loss=2.787387, validation/accuracy=0.379020, validation/loss=3.046022, validation/num_examples=50000
I0404 21:27:47.021112 140193239037760 checkpoints.py:356] Saving checkpoint at step: 13212
I0404 21:27:47.827231 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_13212
I0404 21:27:47.841888 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_13212.
I0404 21:27:47.843942 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 13212: RAM USED (GB) 163.90203392
I0404 21:28:24.583174 140015707342592 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.609483540058136, loss=5.460138320922852
I0404 21:29:06.841123 140015027877632 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6685853600502014, loss=4.74135160446167
I0404 21:29:48.824741 140015707342592 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.45083028078079224, loss=6.213932514190674
I0404 21:30:30.755433 140015027877632 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.591866135597229, loss=4.760071754455566
I0404 21:31:12.399763 140015707342592 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5292786359786987, loss=5.416950225830078
I0404 21:31:54.090952 140015027877632 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6718057990074158, loss=4.675126075744629
I0404 21:32:36.217640 140015707342592 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6526551246643066, loss=4.667415618896484
I0404 21:33:16.977792 140193239037760 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 160.351944704
I0404 21:33:16.978038 140193239037760 spec.py:298] Evaluating on the training split.
I0404 21:33:31.297638 140193239037760 spec.py:310] Evaluating on the validation split.
I0404 21:33:41.748662 140193239037760 spec.py:326] Evaluating on the test split.
I0404 21:33:43.414537 140193239037760 submission_runner.py:382] Time since start: 6208.84s, 	Step: 14000, 	{'train/accuracy': 0.4348241984844208, 'train/loss': 2.790219306945801, 'validation/accuracy': 0.3988399803638458, 'validation/loss': 2.9548580646514893, 'validation/num_examples': 50000, 'test/accuracy': 0.30330002307891846, 'test/loss': 3.5457165241241455, 'test/num_examples': 10000}
I0404 21:33:43.415086 140193239037760 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 164.128948224
I0404 21:33:43.429435 140015027877632 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5712.865486, test/accuracy=0.303300, test/loss=3.545717, test/num_examples=10000, total_duration=6208.836863, train/accuracy=0.434824, train/loss=2.790219, validation/accuracy=0.398840, validation/loss=2.954858, validation/num_examples=50000
I0404 21:33:43.523140 140193239037760 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:33:44.238934 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:33:44.252699 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:33:44.254528 140193239037760 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 167.560077312
I0404 21:33:44.262251 140015707342592 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5712.865486
I0404 21:33:44.339600 140193239037760 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 21:33:45.358040 140193239037760 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_14000
I0404 21:33:45.372035 140193239037760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/imagenet_vit_jax/trial_1/checkpoint_14000.
I0404 21:33:46.333199 140193239037760 submission_runner.py:550] Tuning trial 1/1
I0404 21:33:46.334439 140193239037760 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0404 21:33:46.348007 140193239037760 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 45.49228811264038, 'total_duration': 45.57291650772095, 'global_step': 1, 'preemption_count': 0}), (997, {'train/accuracy': 0.03501953184604645, 'train/loss': 6.060697555541992, 'validation/accuracy': 0.03328000009059906, 'validation/loss': 6.088078022003174, 'validation/num_examples': 50000, 'test/accuracy': 0.027800001204013824, 'test/loss': 6.1809983253479, 'test/num_examples': 10000, 'score': 458.8853347301483, 'total_duration': 515.7842218875885, 'global_step': 997, 'preemption_count': 0}), (2021, {'train/accuracy': 0.06671874970197678, 'train/loss': 5.562770366668701, 'validation/accuracy': 0.06379999965429306, 'validation/loss': 5.5974812507629395, 'validation/num_examples': 50000, 'test/accuracy': 0.048900000751018524, 'test/loss': 5.785606384277344, 'test/num_examples': 10000, 'score': 871.876195192337, 'total_duration': 957.5363082885742, 'global_step': 2021, 'preemption_count': 0}), (3045, {'train/accuracy': 0.1023242175579071, 'train/loss': 5.172773838043213, 'validation/accuracy': 0.09425999969244003, 'validation/loss': 5.228543281555176, 'validation/num_examples': 50000, 'test/accuracy': 0.07260000705718994, 'test/loss': 5.468011379241943, 'test/num_examples': 10000, 'score': 1284.960041999817, 'total_duration': 1399.7500438690186, 'global_step': 3045, 'preemption_count': 0}), (4064, {'train/accuracy': 0.13499999046325684, 'train/loss': 4.857805252075195, 'validation/accuracy': 0.11738000065088272, 'validation/loss': 4.977529048919678, 'validation/num_examples': 50000, 'test/accuracy': 0.08590000122785568, 'test/loss': 5.272126197814941, 'test/num_examples': 10000, 'score': 1696.444487810135, 'total_duration': 1842.2138867378235, 'global_step': 4064, 'preemption_count': 0}), (5084, {'train/accuracy': 0.1638476550579071, 'train/loss': 4.583833694458008, 'validation/accuracy': 0.15053999423980713, 'validation/loss': 4.666473865509033, 'validation/num_examples': 50000, 'test/accuracy': 0.11740000545978546, 'test/loss': 4.996120929718018, 'test/num_examples': 10000, 'score': 2109.2790143489838, 'total_duration': 2285.7793452739716, 'global_step': 5084, 'preemption_count': 0}), (6103, {'train/accuracy': 0.20412108302116394, 'train/loss': 4.221634387969971, 'validation/accuracy': 0.1872600018978119, 'validation/loss': 4.314670085906982, 'validation/num_examples': 50000, 'test/accuracy': 0.14300000667572021, 'test/loss': 4.683260917663574, 'test/num_examples': 10000, 'score': 2520.8290853500366, 'total_duration': 2728.7772777080536, 'global_step': 6103, 'preemption_count': 0}), (7121, {'train/accuracy': 0.2347070276737213, 'train/loss': 4.046520709991455, 'validation/accuracy': 0.21383999288082123, 'validation/loss': 4.164851665496826, 'validation/num_examples': 50000, 'test/accuracy': 0.16030000150203705, 'test/loss': 4.580754280090332, 'test/num_examples': 10000, 'score': 2932.7812325954437, 'total_duration': 3172.110463142395, 'global_step': 7121, 'preemption_count': 0}), (8136, {'train/accuracy': 0.28166013956069946, 'train/loss': 3.6851370334625244, 'validation/accuracy': 0.2540600001811981, 'validation/loss': 3.8377623558044434, 'validation/num_examples': 50000, 'test/accuracy': 0.19440001249313354, 'test/loss': 4.295205116271973, 'test/num_examples': 10000, 'score': 3343.6560566425323, 'total_duration': 3617.203715801239, 'global_step': 8136, 'preemption_count': 0}), (9153, {'train/accuracy': 0.3019140660762787, 'train/loss': 3.5538060665130615, 'validation/accuracy': 0.2780199944972992, 'validation/loss': 3.6878199577331543, 'validation/num_examples': 50000, 'test/accuracy': 0.21470001339912415, 'test/loss': 4.140396595001221, 'test/num_examples': 10000, 'score': 3754.4719495773315, 'total_duration': 4063.1720497608185, 'global_step': 9153, 'preemption_count': 0}), (10168, {'train/accuracy': 0.3335937559604645, 'train/loss': 3.4605894088745117, 'validation/accuracy': 0.3076399862766266, 'validation/loss': 3.5919923782348633, 'validation/num_examples': 50000, 'test/accuracy': 0.23250001668930054, 'test/loss': 4.089896202087402, 'test/num_examples': 10000, 'score': 4164.422289848328, 'total_duration': 4509.679207801819, 'global_step': 10168, 'preemption_count': 0}), (11185, {'train/accuracy': 0.36302733421325684, 'train/loss': 3.246645212173462, 'validation/accuracy': 0.3323200047016144, 'validation/loss': 3.392300844192505, 'validation/num_examples': 50000, 'test/accuracy': 0.2507000267505646, 'test/loss': 3.919424295425415, 'test/num_examples': 10000, 'score': 4574.928730726242, 'total_duration': 4956.9484276771545, 'global_step': 11185, 'preemption_count': 0}), (12201, {'train/accuracy': 0.39054685831069946, 'train/loss': 3.096827268600464, 'validation/accuracy': 0.35659998655319214, 'validation/loss': 3.263266086578369, 'validation/num_examples': 50000, 'test/accuracy': 0.2661000192165375, 'test/loss': 3.810218095779419, 'test/num_examples': 10000, 'score': 4985.411437749863, 'total_duration': 5404.345961093903, 'global_step': 12201, 'preemption_count': 0}), (13212, {'train/accuracy': 0.42970702052116394, 'train/loss': 2.7873871326446533, 'validation/accuracy': 0.37901997566223145, 'validation/loss': 3.0460221767425537, 'validation/num_examples': 50000, 'test/accuracy': 0.29100000858306885, 'test/loss': 3.6194326877593994, 'test/num_examples': 10000, 'score': 5394.509387731552, 'total_duration': 5852.204653978348, 'global_step': 13212, 'preemption_count': 0}), (14000, {'train/accuracy': 0.4348241984844208, 'train/loss': 2.790219306945801, 'validation/accuracy': 0.3988399803638458, 'validation/loss': 2.9548580646514893, 'validation/num_examples': 50000, 'test/accuracy': 0.30330002307891846, 'test/loss': 3.5457165241241455, 'test/num_examples': 10000, 'score': 5712.865485906601, 'total_duration': 6208.836863279343, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 21:33:46.348244 140193239037760 submission_runner.py:553] Timing: 5712.865485906601
I0404 21:33:46.348298 140193239037760 submission_runner.py:554] ====================
I0404 21:33:46.348443 140193239037760 submission_runner.py:613] Final imagenet_vit score: 5712.865485906601
