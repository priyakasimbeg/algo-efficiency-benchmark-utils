I0404 18:23:35.842221 140583447385920 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax.
I0404 18:23:35.891081 140583447385920 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 18:23:36.840174 140583447385920 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0404 18:23:36.841470 140583447385920 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 18:23:36.845042 140583447385920 submission_runner.py:511] Using RNG seed 1691254159
I0404 18:23:38.104778 140583447385920 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 18:23:38.104982 140583447385920 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1.
I0404 18:23:38.105184 140583447385920 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/hparams.json.
I0404 18:23:38.239920 140583447385920 submission_runner.py:230] Starting train once: RAM USED (GB) 4.149321728
I0404 18:23:38.240083 140583447385920 submission_runner.py:231] Initializing dataset.
I0404 18:23:38.251043 140583447385920 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:23:38.258448 140583447385920 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:23:38.258555 140583447385920 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:23:38.473208 140583447385920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:23:39.401833 140583447385920 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.222963712
I0404 18:23:39.402033 140583447385920 submission_runner.py:240] Initializing model.
I0404 18:23:50.519330 140583447385920 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.294895616
I0404 18:23:50.519512 140583447385920 submission_runner.py:252] Initializing optimizer.
I0404 18:23:51.448677 140583447385920 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.294866944
I0404 18:23:51.448845 140583447385920 submission_runner.py:261] Initializing metrics bundle.
I0404 18:23:51.448894 140583447385920 submission_runner.py:276] Initializing checkpoint and logger.
I0404 18:23:51.449580 140583447385920 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0404 18:23:52.199385 140583447385920 submission_runner.py:297] Saving meta data to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0404 18:23:52.200383 140583447385920 submission_runner.py:300] Saving flags to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/flags_0.json.
I0404 18:23:52.203085 140583447385920 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.294572032
I0404 18:23:52.203334 140583447385920 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.294572032
I0404 18:23:52.203428 140583447385920 submission_runner.py:313] Starting training loop.
I0404 18:23:55.998198 140583447385920 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 14.933860352
I0404 18:24:35.045537 140406532597504 logging_writer.py:48] [0] global_step=0, grad_norm=0.5472347736358643, loss=6.925157070159912
I0404 18:24:35.058542 140583447385920 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 24.978190336
I0404 18:24:35.058797 140583447385920 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 24.978190336
I0404 18:24:35.058892 140583447385920 spec.py:298] Evaluating on the training split.
I0404 18:24:35.539768 140583447385920 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:35.545725 140583447385920 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:24:35.545845 140583447385920 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:24:35.606743 140583447385920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:47.097419 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 18:24:47.803900 140583447385920 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:47.828917 140583447385920 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:24:47.829218 140583447385920 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:24:47.892800 140583447385920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:25:04.747766 140583447385920 spec.py:326] Evaluating on the test split.
I0404 18:25:05.154689 140583447385920 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 18:25:05.160003 140583447385920 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 18:25:05.188731 140583447385920 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 18:25:14.163371 140583447385920 submission_runner.py:382] Time since start: 42.86s, 	Step: 1, 	{'train/accuracy': 0.0007971938466653228, 'train/loss': 6.913003444671631, 'validation/accuracy': 0.0005200000014156103, 'validation/loss': 6.912536144256592, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.912220001220703, 'test/num_examples': 10000}
I0404 18:25:14.164055 140583447385920 submission_runner.py:396] After eval at step 1: RAM USED (GB) 65.773846528
I0404 18:25:14.171199 140375125632768 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=42.771701, test/accuracy=0.001000, test/loss=6.912220, test/num_examples=10000, total_duration=42.855497, train/accuracy=0.000797, train/loss=6.913003, validation/accuracy=0.000520, validation/loss=6.912536, validation/num_examples=50000
I0404 18:25:14.276408 140583447385920 checkpoints.py:356] Saving checkpoint at step: 1
I0404 18:25:14.717302 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_1
I0404 18:25:14.718274 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_1.
I0404 18:25:14.723691 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 65.735696384
I0404 18:25:14.729225 140583447385920 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 65.735671808
I0404 18:25:14.799479 140583447385920 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 66.051846144
I0404 18:25:48.432955 140375134025472 logging_writer.py:48] [100] global_step=100, grad_norm=0.5209843516349792, loss=6.887862205505371
I0404 18:26:22.180214 140375280834304 logging_writer.py:48] [200] global_step=200, grad_norm=0.5669398307800293, loss=6.8037109375
I0404 18:26:55.788732 140375134025472 logging_writer.py:48] [300] global_step=300, grad_norm=0.6244211792945862, loss=6.671831130981445
I0404 18:27:29.575351 140375280834304 logging_writer.py:48] [400] global_step=400, grad_norm=0.6439002752304077, loss=6.623546600341797
I0404 18:28:03.467760 140375134025472 logging_writer.py:48] [500] global_step=500, grad_norm=0.6650292873382568, loss=6.5653886795043945
I0404 18:28:37.180194 140375280834304 logging_writer.py:48] [600] global_step=600, grad_norm=0.6744101047515869, loss=6.442562580108643
I0404 18:29:10.993460 140375134025472 logging_writer.py:48] [700] global_step=700, grad_norm=0.7186532020568848, loss=6.425228595733643
I0404 18:29:44.690043 140375280834304 logging_writer.py:48] [800] global_step=800, grad_norm=0.663877546787262, loss=6.3810954093933105
I0404 18:30:18.450344 140375134025472 logging_writer.py:48] [900] global_step=900, grad_norm=1.1910037994384766, loss=6.316983699798584
I0404 18:30:52.139245 140375280834304 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.764166533946991, loss=6.257848739624023
I0404 18:31:26.000065 140375134025472 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7800522446632385, loss=6.200497150421143
I0404 18:31:59.882776 140375280834304 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7565798163414001, loss=6.161309242248535
I0404 18:32:33.778544 140375134025472 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.1050254106521606, loss=6.119913101196289
I0404 18:33:07.586272 140375280834304 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8859058022499084, loss=6.106356620788574
I0404 18:33:41.443386 140375134025472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9455509185791016, loss=6.032427787780762
I0404 18:33:44.886275 140583447385920 submission_runner.py:373] Before eval at step 1512: RAM USED (GB) 67.871617024
I0404 18:33:44.886465 140583447385920 spec.py:298] Evaluating on the training split.
I0404 18:33:51.784518 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 18:33:59.459954 140583447385920 spec.py:326] Evaluating on the test split.
I0404 18:34:01.696485 140583447385920 submission_runner.py:382] Time since start: 592.68s, 	Step: 1512, 	{'train/accuracy': 0.06816007196903229, 'train/loss': 5.464817523956299, 'validation/accuracy': 0.06295999884605408, 'validation/loss': 5.546486854553223, 'validation/num_examples': 50000, 'test/accuracy': 0.04740000143647194, 'test/loss': 5.776136875152588, 'test/num_examples': 10000}
I0404 18:34:01.697283 140583447385920 submission_runner.py:396] After eval at step 1512: RAM USED (GB) 74.180640768
I0404 18:34:01.704210 140375989655296 logging_writer.py:48] [1512] global_step=1512, preemption_count=0, score=547.732167, test/accuracy=0.047400, test/loss=5.776137, test/num_examples=10000, total_duration=592.681936, train/accuracy=0.068160, train/loss=5.464818, validation/accuracy=0.062960, validation/loss=5.546487, validation/num_examples=50000
I0404 18:34:01.842622 140583447385920 checkpoints.py:356] Saving checkpoint at step: 1512
I0404 18:34:02.414871 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_1512
I0404 18:34:02.415761 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_1512.
I0404 18:34:02.420636 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 1512: RAM USED (GB) 74.159529984
I0404 18:34:32.477997 140375998048000 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8199349045753479, loss=5.957609176635742
I0404 18:35:06.234923 140406633228032 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8195380568504333, loss=5.958230495452881
I0404 18:35:40.092492 140375998048000 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8454951643943787, loss=5.927490234375
I0404 18:36:14.066924 140406633228032 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8729009628295898, loss=5.838134288787842
I0404 18:36:47.817817 140375998048000 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8893590569496155, loss=5.741235733032227
I0404 18:37:21.676010 140406633228032 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1283049583435059, loss=5.6543145179748535
I0404 18:37:55.437292 140375998048000 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8152029514312744, loss=5.655744552612305
I0404 18:38:29.125757 140406633228032 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9230873584747314, loss=5.555013656616211
I0404 18:39:02.935217 140375998048000 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8220396637916565, loss=5.520098686218262
I0404 18:39:36.889720 140406633228032 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9687854051589966, loss=5.4880876541137695
I0404 18:40:10.708347 140375998048000 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9161356687545776, loss=5.394145488739014
I0404 18:40:44.529323 140406633228032 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9154207706451416, loss=5.410748481750488
I0404 18:41:18.487370 140375998048000 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9997169971466064, loss=5.351337432861328
I0404 18:41:52.374825 140406633228032 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8106765151023865, loss=5.2432684898376465
I0404 18:42:26.234837 140375998048000 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8318606615066528, loss=5.184989929199219
I0404 18:42:32.707446 140583447385920 submission_runner.py:373] Before eval at step 3021: RAM USED (GB) 75.308601344
I0404 18:42:32.707617 140583447385920 spec.py:298] Evaluating on the training split.
I0404 18:42:39.620268 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 18:42:47.397431 140583447385920 spec.py:326] Evaluating on the test split.
I0404 18:42:49.591960 140583447385920 submission_runner.py:382] Time since start: 1120.50s, 	Step: 3021, 	{'train/accuracy': 0.2066127210855484, 'train/loss': 4.170289993286133, 'validation/accuracy': 0.19035999476909637, 'validation/loss': 4.271543025970459, 'validation/num_examples': 50000, 'test/accuracy': 0.13870000839233398, 'test/loss': 4.690639972686768, 'test/num_examples': 10000}
I0404 18:42:49.592607 140583447385920 submission_runner.py:396] After eval at step 3021: RAM USED (GB) 80.520712192
I0404 18:42:49.599983 140406633228032 logging_writer.py:48] [3021] global_step=3021, preemption_count=0, score=1052.806318, test/accuracy=0.138700, test/loss=4.690640, test/num_examples=10000, total_duration=1120.503123, train/accuracy=0.206613, train/loss=4.170290, validation/accuracy=0.190360, validation/loss=4.271543, validation/num_examples=50000
I0404 18:42:49.717150 140583447385920 checkpoints.py:356] Saving checkpoint at step: 3021
I0404 18:42:50.142137 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_3021
I0404 18:42:50.142928 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_3021.
I0404 18:42:50.148360 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 3021: RAM USED (GB) 80.493735936
I0404 18:43:17.159890 140375998048000 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8964000940322876, loss=5.162160396575928
I0404 18:43:50.961574 140406599657216 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7786704897880554, loss=5.128684997558594
I0404 18:44:24.618470 140375998048000 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8350138068199158, loss=5.026295185089111
I0404 18:44:58.275227 140406599657216 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8404615521430969, loss=5.106891632080078
I0404 18:45:31.941370 140375998048000 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7833538055419922, loss=4.998541831970215
I0404 18:46:05.828762 140406599657216 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.78795325756073, loss=4.997344017028809
I0404 18:46:39.423484 140375998048000 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8647854328155518, loss=4.845337867736816
I0404 18:47:13.131573 140406599657216 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7892172932624817, loss=4.821511268615723
I0404 18:47:46.857094 140375998048000 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8034950494766235, loss=4.931722164154053
I0404 18:48:20.468558 140406599657216 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7119089961051941, loss=4.824904918670654
I0404 18:48:54.329010 140375998048000 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8410809636116028, loss=4.8671135902404785
I0404 18:49:27.867301 140406599657216 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7468351721763611, loss=4.787749767303467
I0404 18:50:01.610143 140375998048000 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7190234065055847, loss=4.692296504974365
I0404 18:50:35.097877 140406599657216 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7410141825675964, loss=4.590199947357178
I0404 18:51:08.745834 140375998048000 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7315877676010132, loss=4.644791126251221
I0404 18:51:20.370902 140583447385920 submission_runner.py:373] Before eval at step 4536: RAM USED (GB) 81.453289472
I0404 18:51:20.371130 140583447385920 spec.py:298] Evaluating on the training split.
I0404 18:51:26.942511 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 18:51:35.032734 140583447385920 spec.py:326] Evaluating on the test split.
I0404 18:51:37.087761 140583447385920 submission_runner.py:382] Time since start: 1648.17s, 	Step: 4536, 	{'train/accuracy': 0.3035714328289032, 'train/loss': 3.5385427474975586, 'validation/accuracy': 0.27932000160217285, 'validation/loss': 3.655106544494629, 'validation/num_examples': 50000, 'test/accuracy': 0.21050001680850983, 'test/loss': 4.137935161590576, 'test/num_examples': 10000}
I0404 18:51:37.088417 140583447385920 submission_runner.py:396] After eval at step 4536: RAM USED (GB) 86.672805888
I0404 18:51:37.095545 140406599657216 logging_writer.py:48] [4536] global_step=4536, preemption_count=0, score=1557.834292, test/accuracy=0.210500, test/loss=4.137935, test/num_examples=10000, total_duration=1648.166378, train/accuracy=0.303571, train/loss=3.538543, validation/accuracy=0.279320, validation/loss=3.655107, validation/num_examples=50000
I0404 18:51:37.236460 140583447385920 checkpoints.py:356] Saving checkpoint at step: 4536
I0404 18:51:37.923019 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_4536
I0404 18:51:37.931421 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_4536.
I0404 18:51:37.940517 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 4536: RAM USED (GB) 86.628675584
I0404 18:51:59.972788 140375998048000 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7280387878417969, loss=4.608778953552246
I0404 18:52:33.995600 140406591264512 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7719430327415466, loss=4.590054512023926
I0404 18:53:07.994413 140375998048000 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7257933020591736, loss=4.594663619995117
I0404 18:53:41.957948 140406591264512 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6891905665397644, loss=4.450901985168457
I0404 18:54:15.913570 140375998048000 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.738770067691803, loss=4.483020782470703
I0404 18:54:49.795274 140406591264512 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6681683659553528, loss=4.525454044342041
I0404 18:55:23.794326 140375998048000 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6733654737472534, loss=4.5201849937438965
I0404 18:55:57.632946 140406591264512 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7144352793693542, loss=4.456783294677734
I0404 18:56:31.690850 140375998048000 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6301017999649048, loss=4.385553359985352
I0404 18:57:05.658069 140406591264512 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6633502244949341, loss=4.4435715675354
I0404 18:57:39.591336 140375998048000 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6747613549232483, loss=4.368948936462402
I0404 18:58:13.520882 140406591264512 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7284737825393677, loss=4.431479454040527
I0404 18:58:47.357241 140375998048000 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6563326716423035, loss=4.27466344833374
I0404 18:59:21.204673 140406591264512 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6417858004570007, loss=4.315697193145752
I0404 18:59:55.120457 140375998048000 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6083911657333374, loss=4.297160625457764
I0404 19:00:08.170978 140583447385920 submission_runner.py:373] Before eval at step 6040: RAM USED (GB) 87.37789952
I0404 19:00:08.171223 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:00:15.099245 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:00:24.078333 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:00:26.203311 140583447385920 submission_runner.py:382] Time since start: 2175.97s, 	Step: 6040, 	{'train/accuracy': 0.4053531587123871, 'train/loss': 2.972665548324585, 'validation/accuracy': 0.37793999910354614, 'validation/loss': 3.097357749938965, 'validation/num_examples': 50000, 'test/accuracy': 0.287200003862381, 'test/loss': 3.639880657196045, 'test/num_examples': 10000}
I0404 19:00:26.203959 140583447385920 submission_runner.py:396] After eval at step 6040: RAM USED (GB) 92.717457408
I0404 19:00:26.211145 140406591264512 logging_writer.py:48] [6040] global_step=6040, preemption_count=0, score=2059.752881, test/accuracy=0.287200, test/loss=3.639881, test/num_examples=10000, total_duration=2175.966519, train/accuracy=0.405353, train/loss=2.972666, validation/accuracy=0.377940, validation/loss=3.097358, validation/num_examples=50000
I0404 19:00:26.544979 140583447385920 checkpoints.py:356] Saving checkpoint at step: 6040
I0404 19:00:27.080126 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_6040
I0404 19:00:27.089324 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_6040.
I0404 19:00:27.097338 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 6040: RAM USED (GB) 92.683157504
I0404 19:00:47.704471 140375998048000 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6553347706794739, loss=4.1516432762146
I0404 19:01:21.556614 140406096373504 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6091212034225464, loss=4.288362503051758
I0404 19:01:55.532247 140375998048000 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6221410632133484, loss=4.295836448669434
I0404 19:02:29.497905 140406096373504 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6042611598968506, loss=4.192232131958008
I0404 19:03:03.339325 140375998048000 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6074540615081787, loss=4.214517593383789
I0404 19:03:37.293021 140406096373504 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6129547357559204, loss=4.236537933349609
I0404 19:04:11.337653 140375998048000 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6279242038726807, loss=4.179388046264648
I0404 19:04:45.319622 140406096373504 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6116413474082947, loss=4.181527137756348
I0404 19:05:19.311145 140375998048000 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6015132069587708, loss=4.086304187774658
I0404 19:05:53.333961 140406096373504 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6195237040519714, loss=4.068422794342041
I0404 19:06:27.206319 140375998048000 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6015816926956177, loss=4.078609466552734
I0404 19:07:01.176695 140406096373504 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6029995679855347, loss=4.127849102020264
I0404 19:07:35.305238 140375998048000 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.583443820476532, loss=4.124838829040527
I0404 19:08:09.120428 140406096373504 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5822401642799377, loss=4.077702045440674
I0404 19:08:43.172300 140375998048000 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.569408655166626, loss=4.0098066329956055
I0404 19:08:57.209101 140583447385920 submission_runner.py:373] Before eval at step 7543: RAM USED (GB) 93.669175296
I0404 19:08:57.209273 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:09:04.133335 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:09:13.922728 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:09:15.883697 140583447385920 submission_runner.py:382] Time since start: 2705.00s, 	Step: 7543, 	{'train/accuracy': 0.4898158311843872, 'train/loss': 2.481121301651001, 'validation/accuracy': 0.45848000049591064, 'validation/loss': 2.6173229217529297, 'validation/num_examples': 50000, 'test/accuracy': 0.3493000268936157, 'test/loss': 3.256502151489258, 'test/num_examples': 10000}
I0404 19:09:15.884479 140583447385920 submission_runner.py:396] After eval at step 7543: RAM USED (GB) 98.868224
I0404 19:09:15.894034 140406096373504 logging_writer.py:48] [7543] global_step=7543, preemption_count=0, score=2558.689395, test/accuracy=0.349300, test/loss=3.256502, test/num_examples=10000, total_duration=2705.004788, train/accuracy=0.489816, train/loss=2.481121, validation/accuracy=0.458480, validation/loss=2.617323, validation/num_examples=50000
I0404 19:09:16.007646 140583447385920 checkpoints.py:356] Saving checkpoint at step: 7543
I0404 19:09:16.597617 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_7543
I0404 19:09:16.606981 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_7543.
I0404 19:09:16.614215 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 7543: RAM USED (GB) 98.827821056
I0404 19:09:36.209331 140375998048000 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6050540804862976, loss=4.045802593231201
I0404 19:10:09.917936 140406087980800 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5476874709129333, loss=3.98933482170105
I0404 19:10:43.742031 140375998048000 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5489793419837952, loss=4.068291664123535
I0404 19:11:17.383182 140406087980800 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5748392343521118, loss=4.020447254180908
I0404 19:11:51.158681 140375998048000 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5456753373146057, loss=3.9105663299560547
I0404 19:12:24.917177 140406087980800 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5638612508773804, loss=4.035955429077148
I0404 19:12:58.479791 140375998048000 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5418977737426758, loss=3.917323350906372
I0404 19:13:32.216355 140406087980800 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5623350739479065, loss=4.006069660186768
I0404 19:14:05.990415 140375998048000 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5502538084983826, loss=3.8930935859680176
I0404 19:14:39.713047 140406087980800 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5575398802757263, loss=3.850515127182007
I0404 19:15:13.469256 140375998048000 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.548203706741333, loss=3.876105785369873
I0404 19:15:47.175348 140406087980800 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5657179355621338, loss=3.971336841583252
I0404 19:16:20.975306 140375998048000 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5323201417922974, loss=3.8966329097747803
I0404 19:16:54.549284 140406087980800 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5296571850776672, loss=3.941237688064575
I0404 19:17:28.173708 140375998048000 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.530160665512085, loss=3.915961980819702
I0404 19:17:46.776963 140583447385920 submission_runner.py:373] Before eval at step 9057: RAM USED (GB) 99.474386944
I0404 19:17:46.777140 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:17:53.718270 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:18:03.097324 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:18:05.004678 140583447385920 submission_runner.py:382] Time since start: 3234.57s, 	Step: 9057, 	{'train/accuracy': 0.5224210619926453, 'train/loss': 2.2883875370025635, 'validation/accuracy': 0.4820599853992462, 'validation/loss': 2.49238657951355, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 3.1433842182159424, 'test/num_examples': 10000}
I0404 19:18:05.005336 140583447385920 submission_runner.py:396] After eval at step 9057: RAM USED (GB) 104.733605888
I0404 19:18:05.013792 140406087980800 logging_writer.py:48] [9057] global_step=9057, preemption_count=0, score=3054.437408, test/accuracy=0.367900, test/loss=3.143384, test/num_examples=10000, total_duration=3234.572634, train/accuracy=0.522421, train/loss=2.288388, validation/accuracy=0.482060, validation/loss=2.492387, validation/num_examples=50000
I0404 19:18:05.138286 140583447385920 checkpoints.py:356] Saving checkpoint at step: 9057
I0404 19:18:05.756255 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_9057
I0404 19:18:05.765574 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_9057.
I0404 19:18:05.771613 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 9057: RAM USED (GB) 104.749703168
I0404 19:18:20.568047 140375998048000 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5310085415840149, loss=3.857959270477295
I0404 19:18:54.357010 140406079588096 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5425001382827759, loss=3.9298465251922607
I0404 19:19:28.161504 140375998048000 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5277233123779297, loss=3.8002429008483887
I0404 19:20:01.968968 140406079588096 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5054891109466553, loss=3.8078222274780273
I0404 19:20:35.854407 140375998048000 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5021312832832336, loss=3.82511568069458
I0404 19:21:09.722030 140406079588096 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5099967122077942, loss=3.7595889568328857
I0404 19:21:43.546297 140375998048000 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5049919486045837, loss=3.6938788890838623
I0404 19:22:17.446868 140406079588096 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4830157160758972, loss=3.729137897491455
I0404 19:22:51.239105 140375998048000 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4905948340892792, loss=3.7756547927856445
I0404 19:23:25.075141 140406079588096 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.49486300349235535, loss=3.7389144897460938
I0404 19:23:58.894717 140375998048000 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5001285076141357, loss=3.7151124477386475
I0404 19:24:32.491168 140406079588096 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.47533246874809265, loss=3.636993408203125
I0404 19:25:06.338570 140375998048000 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.49687957763671875, loss=3.743633270263672
I0404 19:25:39.941063 140406079588096 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.507258951663971, loss=3.8111863136291504
I0404 19:26:13.699926 140375998048000 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5101277232170105, loss=3.7905075550079346
I0404 19:26:35.794046 140583447385920 submission_runner.py:373] Before eval at step 10567: RAM USED (GB) 105.33967872
I0404 19:26:35.794220 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:26:42.749556 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:26:52.301245 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:26:54.445875 140583447385920 submission_runner.py:382] Time since start: 3763.59s, 	Step: 10567, 	{'train/accuracy': 0.6032764315605164, 'train/loss': 1.9140851497650146, 'validation/accuracy': 0.5450800061225891, 'validation/loss': 2.1905577182769775, 'validation/num_examples': 50000, 'test/accuracy': 0.4141000211238861, 'test/loss': 2.8720176219940186, 'test/num_examples': 10000}
I0404 19:26:54.446520 140583447385920 submission_runner.py:396] After eval at step 10567: RAM USED (GB) 110.49658368
I0404 19:26:54.454582 140406079588096 logging_writer.py:48] [10567] global_step=10567, preemption_count=0, score=3552.697839, test/accuracy=0.414100, test/loss=2.872018, test/num_examples=10000, total_duration=3763.588611, train/accuracy=0.603276, train/loss=1.914085, validation/accuracy=0.545080, validation/loss=2.190558, validation/num_examples=50000
I0404 19:26:54.604972 140583447385920 checkpoints.py:356] Saving checkpoint at step: 10567
I0404 19:26:55.220391 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_10567
I0404 19:26:55.229183 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_10567.
I0404 19:26:55.234574 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 10567: RAM USED (GB) 110.608760832
I0404 19:27:06.753567 140375998048000 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4947151243686676, loss=3.70717453956604
I0404 19:27:40.540866 140406071195392 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4857338070869446, loss=3.7723453044891357
I0404 19:28:14.241642 140375998048000 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.48666051030158997, loss=3.646524667739868
I0404 19:28:48.092563 140406071195392 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4926810562610626, loss=3.7393317222595215
I0404 19:29:21.935347 140375998048000 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.48205798864364624, loss=3.6209287643432617
I0404 19:29:55.683153 140406071195392 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5000194907188416, loss=3.703817844390869
I0404 19:30:29.412972 140375998048000 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.47234630584716797, loss=3.5689806938171387
I0404 19:31:03.237541 140406071195392 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.48787596821784973, loss=3.626711368560791
I0404 19:31:36.926426 140375998048000 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.480116069316864, loss=3.6209797859191895
I0404 19:32:10.692868 140406071195392 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.48006802797317505, loss=3.640336513519287
I0404 19:32:44.637386 140375998048000 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4889073073863983, loss=3.75917649269104
I0404 19:33:18.440708 140406071195392 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.48565343022346497, loss=3.6586358547210693
I0404 19:33:52.167354 140375998048000 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4921887516975403, loss=3.6238298416137695
I0404 19:34:25.934786 140406071195392 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4741649925708771, loss=3.584064483642578
I0404 19:34:59.792673 140375998048000 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.47045499086380005, loss=3.6054744720458984
I0404 19:35:25.307349 140583447385920 submission_runner.py:373] Before eval at step 12077: RAM USED (GB) 111.233388544
I0404 19:35:25.307583 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:35:32.194882 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:35:41.831018 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:35:43.994763 140583447385920 submission_runner.py:382] Time since start: 4293.10s, 	Step: 12077, 	{'train/accuracy': 0.6156529188156128, 'train/loss': 1.823398470878601, 'validation/accuracy': 0.5655800104141235, 'validation/loss': 2.072016954421997, 'validation/num_examples': 50000, 'test/accuracy': 0.4433000087738037, 'test/loss': 2.7486162185668945, 'test/num_examples': 10000}
I0404 19:35:43.995408 140583447385920 submission_runner.py:396] After eval at step 12077: RAM USED (GB) 116.445343744
I0404 19:35:44.003135 140406071195392 logging_writer.py:48] [12077] global_step=12077, preemption_count=0, score=4057.606870, test/accuracy=0.443300, test/loss=2.748616, test/num_examples=10000, total_duration=4293.101534, train/accuracy=0.615653, train/loss=1.823398, validation/accuracy=0.565580, validation/loss=2.072017, validation/num_examples=50000
I0404 19:35:44.142786 140583447385920 checkpoints.py:356] Saving checkpoint at step: 12077
I0404 19:35:44.738242 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_12077
I0404 19:35:44.743216 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_12077.
I0404 19:35:44.747664 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 12077: RAM USED (GB) 116.528939008
I0404 19:35:52.857053 140375998048000 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.48434457182884216, loss=3.6230082511901855
I0404 19:36:26.562494 140406062802688 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4698076844215393, loss=3.545771598815918
I0404 19:37:00.158473 140375998048000 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.4666764438152313, loss=3.543301820755005
I0404 19:37:33.898098 140406062802688 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4822121560573578, loss=3.6388766765594482
I0404 19:38:07.575481 140375998048000 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.48884180188179016, loss=3.5864880084991455
I0404 19:38:41.362893 140406062802688 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4671902060508728, loss=3.6058342456817627
I0404 19:39:14.936647 140375998048000 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4692024886608124, loss=3.575867176055908
I0404 19:39:48.556918 140406062802688 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4755958616733551, loss=3.549668550491333
I0404 19:40:22.282928 140375998048000 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.4636731445789337, loss=3.5195770263671875
I0404 19:40:56.052402 140406062802688 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.46220868825912476, loss=3.4596920013427734
I0404 19:41:29.727208 140375998048000 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4708947241306305, loss=3.615985870361328
I0404 19:42:03.414863 140406062802688 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4669242203235626, loss=3.4783079624176025
I0404 19:42:37.246596 140375998048000 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4660204350948334, loss=3.4782896041870117
I0404 19:43:10.979865 140406062802688 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.4661458730697632, loss=3.5228664875030518
I0404 19:43:44.729002 140375998048000 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.45657533407211304, loss=3.5504744052886963
I0404 19:44:14.971388 140583447385920 submission_runner.py:373] Before eval at step 13592: RAM USED (GB) 117.15659776
I0404 19:44:14.971568 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:44:21.861911 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:44:31.884689 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:44:33.864351 140583447385920 submission_runner.py:382] Time since start: 4822.77s, 	Step: 13592, 	{'train/accuracy': 0.6439931392669678, 'train/loss': 1.684508204460144, 'validation/accuracy': 0.5845999717712402, 'validation/loss': 1.948517084121704, 'validation/num_examples': 50000, 'test/accuracy': 0.4562000334262848, 'test/loss': 2.6412148475646973, 'test/num_examples': 10000}
I0404 19:44:33.865008 140583447385920 submission_runner.py:396] After eval at step 13592: RAM USED (GB) 123.134660608
I0404 19:44:33.872974 140406062802688 logging_writer.py:48] [13592] global_step=13592, preemption_count=0, score=4562.660496, test/accuracy=0.456200, test/loss=2.641215, test/num_examples=10000, total_duration=4822.765832, train/accuracy=0.643993, train/loss=1.684508, validation/accuracy=0.584600, validation/loss=1.948517, validation/num_examples=50000
I0404 19:44:34.027726 140583447385920 checkpoints.py:356] Saving checkpoint at step: 13592
I0404 19:44:34.693512 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_13592
I0404 19:44:34.704621 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_13592.
I0404 19:44:34.709112 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 13592: RAM USED (GB) 123.246747648
I0404 19:44:37.738815 140375998048000 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4676855504512787, loss=3.461027145385742
I0404 19:45:11.414443 140406054409984 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.4627230763435364, loss=3.4929556846618652
I0404 19:45:44.968885 140375998048000 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4643917381763458, loss=3.456878662109375
I0404 19:46:18.799623 140406054409984 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.45986440777778625, loss=3.445690393447876
I0404 19:46:51.914274 140583447385920 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 123.688521728
I0404 19:46:51.914547 140583447385920 spec.py:298] Evaluating on the training split.
I0404 19:46:58.645199 140583447385920 spec.py:310] Evaluating on the validation split.
I0404 19:47:08.514104 140583447385920 spec.py:326] Evaluating on the test split.
I0404 19:47:10.644018 140583447385920 submission_runner.py:382] Time since start: 4979.71s, 	Step: 14000, 	{'train/accuracy': 0.6394889950752258, 'train/loss': 1.7431714534759521, 'validation/accuracy': 0.5880599617958069, 'validation/loss': 1.9770714044570923, 'validation/num_examples': 50000, 'test/accuracy': 0.4707000255584717, 'test/loss': 2.5970265865325928, 'test/num_examples': 10000}
I0404 19:47:10.644680 140583447385920 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 128.869900288
I0404 19:47:10.653544 140375998048000 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4698.394197, test/accuracy=0.470700, test/loss=2.597027, test/num_examples=10000, total_duration=4979.709591, train/accuracy=0.639489, train/loss=1.743171, validation/accuracy=0.588060, validation/loss=1.977071, validation/num_examples=50000
I0404 19:47:10.804863 140583447385920 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 19:47:11.468650 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_14000
I0404 19:47:11.478622 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0404 19:47:11.482927 140583447385920 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 129.02412288
I0404 19:47:11.492538 140406054409984 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4698.394197
I0404 19:47:11.611388 140583447385920 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 19:47:12.550595 140583447385920 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_14000
I0404 19:47:12.558964 140583447385920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0404 19:47:12.743081 140583447385920 submission_runner.py:550] Tuning trial 1/1
I0404 19:47:12.744906 140583447385920 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0404 19:47:12.747549 140583447385920 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007971938466653228, 'train/loss': 6.913003444671631, 'validation/accuracy': 0.0005200000014156103, 'validation/loss': 6.912536144256592, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.912220001220703, 'test/num_examples': 10000, 'score': 42.771700620651245, 'total_duration': 42.85549736022949, 'global_step': 1, 'preemption_count': 0}), (1512, {'train/accuracy': 0.06816007196903229, 'train/loss': 5.464817523956299, 'validation/accuracy': 0.06295999884605408, 'validation/loss': 5.546486854553223, 'validation/num_examples': 50000, 'test/accuracy': 0.04740000143647194, 'test/loss': 5.776136875152588, 'test/num_examples': 10000, 'score': 547.7321665287018, 'total_duration': 592.6819362640381, 'global_step': 1512, 'preemption_count': 0}), (3021, {'train/accuracy': 0.2066127210855484, 'train/loss': 4.170289993286133, 'validation/accuracy': 0.19035999476909637, 'validation/loss': 4.271543025970459, 'validation/num_examples': 50000, 'test/accuracy': 0.13870000839233398, 'test/loss': 4.690639972686768, 'test/num_examples': 10000, 'score': 1052.806317806244, 'total_duration': 1120.5031225681305, 'global_step': 3021, 'preemption_count': 0}), (4536, {'train/accuracy': 0.3035714328289032, 'train/loss': 3.5385427474975586, 'validation/accuracy': 0.27932000160217285, 'validation/loss': 3.655106544494629, 'validation/num_examples': 50000, 'test/accuracy': 0.21050001680850983, 'test/loss': 4.137935161590576, 'test/num_examples': 10000, 'score': 1557.8342924118042, 'total_duration': 1648.1663784980774, 'global_step': 4536, 'preemption_count': 0}), (6040, {'train/accuracy': 0.4053531587123871, 'train/loss': 2.972665548324585, 'validation/accuracy': 0.37793999910354614, 'validation/loss': 3.097357749938965, 'validation/num_examples': 50000, 'test/accuracy': 0.287200003862381, 'test/loss': 3.639880657196045, 'test/num_examples': 10000, 'score': 2059.7528812885284, 'total_duration': 2175.966519355774, 'global_step': 6040, 'preemption_count': 0}), (7543, {'train/accuracy': 0.4898158311843872, 'train/loss': 2.481121301651001, 'validation/accuracy': 0.45848000049591064, 'validation/loss': 2.6173229217529297, 'validation/num_examples': 50000, 'test/accuracy': 0.3493000268936157, 'test/loss': 3.256502151489258, 'test/num_examples': 10000, 'score': 2558.689394712448, 'total_duration': 2705.004788160324, 'global_step': 7543, 'preemption_count': 0}), (9057, {'train/accuracy': 0.5224210619926453, 'train/loss': 2.2883875370025635, 'validation/accuracy': 0.4820599853992462, 'validation/loss': 2.49238657951355, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 3.1433842182159424, 'test/num_examples': 10000, 'score': 3054.4374079704285, 'total_duration': 3234.5726342201233, 'global_step': 9057, 'preemption_count': 0}), (10567, {'train/accuracy': 0.6032764315605164, 'train/loss': 1.9140851497650146, 'validation/accuracy': 0.5450800061225891, 'validation/loss': 2.1905577182769775, 'validation/num_examples': 50000, 'test/accuracy': 0.4141000211238861, 'test/loss': 2.8720176219940186, 'test/num_examples': 10000, 'score': 3552.69783949852, 'total_duration': 3763.588611125946, 'global_step': 10567, 'preemption_count': 0}), (12077, {'train/accuracy': 0.6156529188156128, 'train/loss': 1.823398470878601, 'validation/accuracy': 0.5655800104141235, 'validation/loss': 2.072016954421997, 'validation/num_examples': 50000, 'test/accuracy': 0.4433000087738037, 'test/loss': 2.7486162185668945, 'test/num_examples': 10000, 'score': 4057.606869697571, 'total_duration': 4293.101534128189, 'global_step': 12077, 'preemption_count': 0}), (13592, {'train/accuracy': 0.6439931392669678, 'train/loss': 1.684508204460144, 'validation/accuracy': 0.5845999717712402, 'validation/loss': 1.948517084121704, 'validation/num_examples': 50000, 'test/accuracy': 0.4562000334262848, 'test/loss': 2.6412148475646973, 'test/num_examples': 10000, 'score': 4562.660496234894, 'total_duration': 4822.765831708908, 'global_step': 13592, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6394889950752258, 'train/loss': 1.7431714534759521, 'validation/accuracy': 0.5880599617958069, 'validation/loss': 1.9770714044570923, 'validation/num_examples': 50000, 'test/accuracy': 0.4707000255584717, 'test/loss': 2.5970265865325928, 'test/num_examples': 10000, 'score': 4698.3941967487335, 'total_duration': 4979.709591388702, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 19:47:12.747658 140583447385920 submission_runner.py:553] Timing: 4698.3941967487335
I0404 19:47:12.747708 140583447385920 submission_runner.py:554] ====================
I0404 19:47:12.747805 140583447385920 submission_runner.py:613] Final imagenet_resnet score: 4698.3941967487335
