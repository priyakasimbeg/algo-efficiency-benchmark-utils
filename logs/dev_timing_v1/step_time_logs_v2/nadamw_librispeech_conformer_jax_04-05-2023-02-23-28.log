I0405 02:23:48.332919 139666009474880 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax.
I0405 02:23:48.390370 139666009474880 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 02:23:49.269950 139666009474880 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0405 02:23:49.270571 139666009474880 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 02:23:49.275918 139666009474880 submission_runner.py:511] Using RNG seed 1563329684
I0405 02:23:51.759175 139666009474880 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 02:23:51.759362 139666009474880 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1.
I0405 02:23:51.759520 139666009474880 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/hparams.json.
I0405 02:23:51.887024 139666009474880 submission_runner.py:230] Starting train once: RAM USED (GB) 4.265951232
I0405 02:23:51.887189 139666009474880 submission_runner.py:231] Initializing dataset.
I0405 02:23:51.887347 139666009474880 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.265951232
I0405 02:23:51.887403 139666009474880 submission_runner.py:240] Initializing model.
I0405 02:23:57.579237 139666009474880 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.687544832
I0405 02:23:57.579440 139666009474880 submission_runner.py:252] Initializing optimizer.
I0405 02:23:58.408195 139666009474880 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.687462912
I0405 02:23:58.408360 139666009474880 submission_runner.py:261] Initializing metrics bundle.
I0405 02:23:58.408407 139666009474880 submission_runner.py:276] Initializing checkpoint and logger.
I0405 02:23:58.409242 139666009474880 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0405 02:23:58.409484 139666009474880 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 02:23:58.409540 139666009474880 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 02:23:59.206938 139666009474880 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0405 02:23:59.207763 139666009474880 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/flags_0.json.
I0405 02:23:59.213981 139666009474880 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.685394432
I0405 02:23:59.214189 139666009474880 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.685394432
I0405 02:23:59.214251 139666009474880 submission_runner.py:313] Starting training loop.
I0405 02:23:59.410072 139666009474880 input_pipeline.py:20] Loading split = train-clean-100
I0405 02:23:59.442112 139666009474880 input_pipeline.py:20] Loading split = train-clean-360
I0405 02:23:59.767263 139666009474880 input_pipeline.py:20] Loading split = train-other-500
I0405 02:24:03.187780 139666009474880 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.470148608
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 02:25:04.060430 139491561953024 logging_writer.py:48] [0] global_step=0, grad_norm=51.93593215942383, loss=31.410762786865234
I0405 02:25:04.083140 139666009474880 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.088903168
I0405 02:25:04.083601 139666009474880 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.088903168
I0405 02:25:04.083734 139666009474880 spec.py:298] Evaluating on the training split.
I0405 02:25:04.194033 139666009474880 input_pipeline.py:20] Loading split = train-clean-100
I0405 02:25:04.472103 139666009474880 input_pipeline.py:20] Loading split = train-clean-360
I0405 02:25:04.864687 139666009474880 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 02:26:00.516431 139666009474880 spec.py:310] Evaluating on the validation split.
I0405 02:26:00.578870 139666009474880 input_pipeline.py:20] Loading split = dev-clean
I0405 02:26:00.583970 139666009474880 input_pipeline.py:20] Loading split = dev-other
I0405 02:26:39.735355 139666009474880 spec.py:326] Evaluating on the test split.
I0405 02:26:39.802471 139666009474880 input_pipeline.py:20] Loading split = test-clean
I0405 02:27:07.799125 139666009474880 submission_runner.py:382] Time since start: 64.87s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.742676, dtype=float32), 'train/wer': 1.6245236657434978, 'validation/ctc_loss': DeviceArray(30.48048, dtype=float32), 'validation/wer': 1.4929811189688276, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.582283, dtype=float32), 'test/wer': 1.5334023927040805, 'test/num_examples': 2472}
I0405 02:27:07.800290 139666009474880 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.201631744
I0405 02:27:07.816200 139487275378432 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=64.673420, test/ctc_loss=30.58228302001953, test/num_examples=2472, test/wer=1.533402, total_duration=64.869292, train/ctc_loss=31.74267578125, train/wer=1.624524, validation/ctc_loss=30.480480194091797, validation/num_examples=5348, validation/wer=1.492981
I0405 02:27:08.181242 139666009474880 checkpoints.py:356] Saving checkpoint at step: 1
I0405 02:27:09.560419 139666009474880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_1
I0405 02:27:09.593956 139666009474880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_1.
I0405 02:27:09.601414 139666009474880 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.468355072
I0405 02:27:09.661650 139666009474880 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.467068928
I0405 02:27:27.812705 139666009474880 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.798083072
I0405 02:28:42.670725 139492745881344 logging_writer.py:48] [100] global_step=100, grad_norm=4.499141693115234, loss=6.897050380706787
I0405 02:29:58.124259 139492754274048 logging_writer.py:48] [200] global_step=200, grad_norm=1.309090495109558, loss=6.08486270904541
I0405 02:31:13.358255 139492745881344 logging_writer.py:48] [300] global_step=300, grad_norm=0.6837589144706726, loss=5.843977928161621
I0405 02:32:28.442739 139492754274048 logging_writer.py:48] [400] global_step=400, grad_norm=0.7254065871238708, loss=5.8361406326293945
I0405 02:33:43.770128 139492745881344 logging_writer.py:48] [500] global_step=500, grad_norm=0.359104722738266, loss=5.827665328979492
I0405 02:34:59.208811 139492754274048 logging_writer.py:48] [600] global_step=600, grad_norm=2.4500210285186768, loss=5.819396018981934
I0405 02:36:14.617815 139492745881344 logging_writer.py:48] [700] global_step=700, grad_norm=0.6550154685974121, loss=5.793980121612549
I0405 02:37:29.815234 139492754274048 logging_writer.py:48] [800] global_step=800, grad_norm=0.29790937900543213, loss=5.8073344230651855
I0405 02:38:50.342173 139492745881344 logging_writer.py:48] [900] global_step=900, grad_norm=0.3449768126010895, loss=5.791148662567139
I0405 02:40:11.574471 139492754274048 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4226573705673218, loss=5.776446342468262
I0405 02:41:31.042724 139493560702720 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.3380073308944702, loss=5.788718223571777
I0405 02:42:46.244702 139493552310016 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.565800130367279, loss=5.771524906158447
I0405 02:44:01.469644 139493560702720 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.606330156326294, loss=5.781491756439209
I0405 02:45:16.657634 139493552310016 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.421744465827942, loss=5.603816986083984
I0405 02:46:31.833511 139493560702720 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0922698974609375, loss=5.5688347816467285
I0405 02:47:46.685734 139493552310016 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.211230516433716, loss=5.536422252655029
I0405 02:49:01.362936 139493560702720 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0519181489944458, loss=5.5224175453186035
I0405 02:50:15.884146 139493552310016 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6577457189559937, loss=5.4963226318359375
I0405 02:51:30.433041 139493560702720 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8836836814880371, loss=5.509949684143066
I0405 02:52:44.835215 139493552310016 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.158351182937622, loss=5.495460033416748
I0405 02:54:04.092043 139493560702720 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.4008309841156006, loss=5.494202613830566
I0405 02:55:18.666564 139493552310016 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.9112493991851807, loss=5.4934515953063965
I0405 02:56:33.237225 139493560702720 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.164468765258789, loss=5.5905442237854
I0405 02:57:47.696151 139493552310016 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0790501832962036, loss=5.5479655265808105
I0405 02:59:02.544739 139493560702720 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.7579426765441895, loss=5.593014717102051
I0405 03:00:16.803575 139493552310016 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.871293544769287, loss=5.531132698059082
I0405 03:01:31.139900 139493560702720 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.6637688875198364, loss=5.555510520935059
I0405 03:02:45.670653 139493552310016 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6825891137123108, loss=5.52411413192749
I0405 03:04:00.765754 139493560702720 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9178818464279175, loss=5.531802177429199
I0405 03:05:23.312423 139493552310016 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9149246215820312, loss=5.555025577545166
I0405 03:06:49.471302 139492905342720 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9278103709220886, loss=5.531332015991211
I0405 03:07:09.943809 139666009474880 submission_runner.py:373] Before eval at step 3129: RAM USED (GB) 23.044251648
I0405 03:07:09.944053 139666009474880 spec.py:298] Evaluating on the training split.
I0405 03:07:36.882551 139666009474880 spec.py:310] Evaluating on the validation split.
I0405 03:08:13.008563 139666009474880 spec.py:326] Evaluating on the test split.
I0405 03:08:31.219718 139666009474880 submission_runner.py:382] Time since start: 2590.73s, 	Step: 3129, 	{'train/ctc_loss': DeviceArray(5.808312, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.775593, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6667857, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 03:08:31.221144 139666009474880 submission_runner.py:396] After eval at step 3129: RAM USED (GB) 22.398427136
I0405 03:08:31.240569 139492977022720 logging_writer.py:48] [3129] global_step=3129, preemption_count=0, score=2457.005229, test/ctc_loss=5.666785717010498, test/num_examples=2472, test/wer=0.899580, total_duration=2590.726774, train/ctc_loss=5.808311939239502, train/wer=0.944636, validation/ctc_loss=5.775592803955078, validation/num_examples=5348, validation/wer=0.895995
I0405 03:08:31.653505 139666009474880 checkpoints.py:356] Saving checkpoint at step: 3129
I0405 03:08:33.085862 139666009474880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_3129
I0405 03:08:33.117808 139666009474880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_3129.
I0405 03:08:33.125087 139666009474880 submission_runner.py:416] After logging and checkpointing eval at step 3129: RAM USED (GB) 22.536921088
I0405 03:09:26.470853 139492968630016 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6418119072914124, loss=5.501466751098633
I0405 03:10:40.650476 139492926666496 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1702847480773926, loss=5.54648494720459
I0405 03:11:54.918781 139492968630016 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8067225813865662, loss=5.5302934646606445
I0405 03:13:09.227192 139492926666496 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8780749440193176, loss=5.519724369049072
I0405 03:14:23.561109 139492968630016 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8893827199935913, loss=5.476624965667725
I0405 03:15:38.018274 139492926666496 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.0650616884231567, loss=5.488631248474121
I0405 03:16:54.124181 139492968630016 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.4942145347595215, loss=5.551304817199707
I0405 03:18:11.679841 139492926666496 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.4424234926700592, loss=5.559639930725098
I0405 03:19:30.377977 139492968630016 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.466221809387207, loss=5.548583984375
I0405 03:20:48.800753 139492926666496 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9623134136199951, loss=5.530843257904053
I0405 03:22:08.058439 139492977022720 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7283285856246948, loss=5.495153903961182
I0405 03:23:22.317776 139492968630016 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.2386224269866943, loss=5.510778903961182
I0405 03:24:36.500069 139492977022720 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.36061471700668335, loss=5.4951701164245605
I0405 03:25:50.776995 139492968630016 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6546353101730347, loss=5.500060558319092
I0405 03:27:05.147962 139492977022720 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5363101363182068, loss=5.504608631134033
I0405 03:28:19.512794 139492968630016 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6550873517990112, loss=5.494478225708008
I0405 03:29:33.821690 139492977022720 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6332686543464661, loss=5.481541633605957
I0405 03:30:54.086266 139492968630016 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.33630669116973877, loss=5.46745491027832
I0405 03:32:12.467133 139492977022720 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.46061035990715027, loss=5.457172870635986
I0405 03:33:27.775673 139492968630016 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3537256717681885, loss=5.493612766265869
I0405 03:34:46.200605 139492649342720 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.8167496919631958, loss=5.45478630065918
I0405 03:36:00.542223 139492640950016 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.30719587206840515, loss=5.457494735717773
I0405 03:37:15.082669 139492649342720 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.9852592349052429, loss=5.482872486114502
I0405 03:38:29.638622 139492640950016 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.1484419107437134, loss=5.451862335205078
I0405 03:39:43.918011 139492649342720 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5502746105194092, loss=5.390321254730225
I0405 03:40:58.231486 139492640950016 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.738572597503662, loss=5.157228946685791
I0405 03:42:12.638249 139492649342720 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.742451548576355, loss=4.388703346252441
I0405 03:43:26.930321 139492640950016 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.106105089187622, loss=3.848123073577881
I0405 03:44:41.314518 139492649342720 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.1440315246582031, loss=3.570042133331299
I0405 03:45:57.177328 139492640950016 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.189038634300232, loss=3.296389579772949
I0405 03:47:19.671907 139492977022720 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7404288053512573, loss=3.112293004989624
I0405 03:48:34.099364 139492968630016 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6296260952949524, loss=2.99426007270813
I0405 03:48:34.110021 139666009474880 submission_runner.py:373] Before eval at step 6301: RAM USED (GB) 23.311568896
I0405 03:48:34.110170 139666009474880 spec.py:298] Evaluating on the training split.
I0405 03:49:09.584309 139666009474880 spec.py:310] Evaluating on the validation split.
I0405 03:49:48.185548 139666009474880 spec.py:326] Evaluating on the test split.
I0405 03:50:08.051618 139666009474880 submission_runner.py:382] Time since start: 5074.89s, 	Step: 6301, 	{'train/ctc_loss': DeviceArray(2.8997095, dtype=float32), 'train/wer': 0.6955224675705192, 'validation/ctc_loss': DeviceArray(3.229979, dtype=float32), 'validation/wer': 0.7001997124911963, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.946555, dtype=float32), 'test/wer': 0.679280157617858, 'test/num_examples': 2472}
I0405 03:50:08.053056 139666009474880 submission_runner.py:396] After eval at step 6301: RAM USED (GB) 22.404866048
I0405 03:50:08.071695 139492761982720 logging_writer.py:48] [6301] global_step=6301, preemption_count=0, score=4850.080855, test/ctc_loss=2.9465548992156982, test/num_examples=2472, test/wer=0.679280, total_duration=5074.892856, train/ctc_loss=2.899709463119507, train/wer=0.695522, validation/ctc_loss=3.2299790382385254, validation/num_examples=5348, validation/wer=0.700200
I0405 03:50:08.412540 139666009474880 checkpoints.py:356] Saving checkpoint at step: 6301
I0405 03:50:09.829897 139666009474880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_6301
I0405 03:50:09.863116 139666009474880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_6301.
I0405 03:50:09.879572 139666009474880 submission_runner.py:416] After logging and checkpointing eval at step 6301: RAM USED (GB) 22.446252032
I0405 03:51:24.279125 139492753590016 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6487913131713867, loss=2.9143474102020264
I0405 03:52:38.918425 139492703233792 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6808525919914246, loss=2.821866989135742
I0405 03:53:53.580681 139492753590016 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5713756084442139, loss=2.745678663253784
I0405 03:55:08.166043 139492703233792 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7101871967315674, loss=2.6614644527435303
I0405 03:56:22.690041 139492753590016 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5790374875068665, loss=2.512463092803955
I0405 03:57:37.039391 139492703233792 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5885138511657715, loss=2.4537837505340576
I0405 03:58:51.594131 139492753590016 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5178605318069458, loss=2.451467275619507
I0405 04:00:09.379287 139492703233792 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.662590503692627, loss=2.4031660556793213
I0405 04:01:27.889063 139492753590016 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.59897780418396, loss=2.394315719604492
I0405 04:02:47.593085 139492761982720 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.555352509021759, loss=2.2985966205596924
I0405 04:04:02.056855 139492753590016 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5614108443260193, loss=2.1900343894958496
I0405 04:05:16.454083 139492761982720 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6020572781562805, loss=2.248239278793335
I0405 04:06:30.884108 139492753590016 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6033602952957153, loss=2.184311866760254
I0405 04:07:45.315136 139492761982720 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4788832366466522, loss=2.092085123062134
I0405 04:09:00.736944 139492753590016 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6821133494377136, loss=2.1509318351745605
I0405 04:10:22.432778 139492761982720 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6177619099617004, loss=2.132702350616455
I0405 04:11:40.861541 139492753590016 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.43969783186912537, loss=2.1175715923309326
I0405 04:12:59.722375 139492761982720 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5565899014472961, loss=2.028141736984253
I0405 04:14:20.648253 139492753590016 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6669525504112244, loss=1.9862322807312012
I0405 04:15:40.473002 139492761982720 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4707925319671631, loss=1.972255825996399
I0405 04:16:54.811098 139492753590016 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.44484907388687134, loss=1.9719833135604858
I0405 04:18:09.161722 139492761982720 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5028051733970642, loss=2.0079309940338135
I0405 04:19:23.519764 139492753590016 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.44467994570732117, loss=1.9422216415405273
I0405 04:20:38.036193 139492761982720 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5336232781410217, loss=1.9104734659194946
I0405 04:21:52.479818 139492753590016 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5132799744606018, loss=1.9102040529251099
I0405 04:23:06.932852 139492761982720 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5915961861610413, loss=1.9169225692749023
I0405 04:24:24.949456 139492753590016 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.593907356262207, loss=1.8740874528884888
I0405 04:25:45.360046 139492761982720 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5516601204872131, loss=1.935090184211731
I0405 04:27:04.926873 139492753590016 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5713812708854675, loss=1.8445967435836792
I0405 04:28:28.825742 139492434302720 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4558539390563965, loss=1.8571362495422363
I0405 04:29:43.111584 139492425910016 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5621971487998962, loss=1.7685916423797607
I0405 04:30:10.277474 139666009474880 submission_runner.py:373] Before eval at step 9438: RAM USED (GB) 22.51030528
I0405 04:30:10.277698 139666009474880 spec.py:298] Evaluating on the training split.
I0405 04:30:46.783987 139666009474880 spec.py:310] Evaluating on the validation split.
I0405 04:31:25.601307 139666009474880 spec.py:326] Evaluating on the test split.
I0405 04:31:46.168663 139666009474880 submission_runner.py:382] Time since start: 7571.06s, 	Step: 9438, 	{'train/ctc_loss': DeviceArray(0.58203846, dtype=float32), 'train/wer': 0.2126234113994444, 'validation/ctc_loss': DeviceArray(0.93448687, dtype=float32), 'validation/wer': 0.28336018678424296, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6527644, dtype=float32), 'test/wer': 0.21906038632624458, 'test/num_examples': 2472}
I0405 04:31:46.169950 139666009474880 submission_runner.py:396] After eval at step 9438: RAM USED (GB) 22.425378816
I0405 04:31:46.188503 139492434302720 logging_writer.py:48] [9438] global_step=9438, preemption_count=0, score=7242.731764, test/ctc_loss=0.6527643799781799, test/num_examples=2472, test/wer=0.219060, total_duration=7571.060153, train/ctc_loss=0.5820384621620178, train/wer=0.212623, validation/ctc_loss=0.9344868659973145, validation/num_examples=5348, validation/wer=0.283360
I0405 04:31:46.478664 139666009474880 checkpoints.py:356] Saving checkpoint at step: 9438
I0405 04:31:47.861856 139666009474880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_9438
I0405 04:31:47.894244 139666009474880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_9438.
I0405 04:31:47.904937 139666009474880 submission_runner.py:416] After logging and checkpointing eval at step 9438: RAM USED (GB) 22.453374976
I0405 04:32:34.587965 139492425910016 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.47455790638923645, loss=1.8249989748001099
I0405 04:33:48.815606 139492367161088 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5963678956031799, loss=1.852877140045166
I0405 04:35:03.580057 139492425910016 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4679194688796997, loss=1.8048542737960815
I0405 04:36:18.162431 139492367161088 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4705975651741028, loss=1.8171697854995728
I0405 04:37:32.664926 139492425910016 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.44482672214508057, loss=1.7674705982208252
I0405 04:38:45.898480 139666009474880 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 23.228469248
I0405 04:38:45.898718 139666009474880 spec.py:298] Evaluating on the training split.
I0405 04:39:22.618204 139666009474880 spec.py:310] Evaluating on the validation split.
I0405 04:40:01.925711 139666009474880 spec.py:326] Evaluating on the test split.
I0405 04:40:21.796709 139666009474880 submission_runner.py:382] Time since start: 8086.68s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(0.54358226, dtype=float32), 'train/wer': 0.19421085151169246, 'validation/ctc_loss': DeviceArray(0.8781449, dtype=float32), 'validation/wer': 0.26658240793447113, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5981344, dtype=float32), 'test/wer': 0.20327828895253183, 'test/num_examples': 2472}
I0405 04:40:21.798079 139666009474880 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 23.176052736
I0405 04:40:21.813470 139492434302720 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7659.337561, test/ctc_loss=0.5981343984603882, test/num_examples=2472, test/wer=0.203278, total_duration=8086.682561, train/ctc_loss=0.5435822606086731, train/wer=0.194211, validation/ctc_loss=0.8781449198722839, validation/num_examples=5348, validation/wer=0.266582
I0405 04:40:22.132543 139666009474880 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 04:40:23.511660 139666009474880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 04:40:23.543838 139666009474880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 04:40:23.558647 139666009474880 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 23.194107904
I0405 04:40:23.565693 139492425910016 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7659.337561
I0405 04:40:23.783041 139666009474880 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 04:40:25.634324 139666009474880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 04:40:25.666882 139666009474880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 04:40:27.223351 139666009474880 submission_runner.py:550] Tuning trial 1/1
I0405 04:40:27.223582 139666009474880 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 04:40:27.228835 139666009474880 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.742676, dtype=float32), 'train/wer': 1.6245236657434978, 'validation/ctc_loss': DeviceArray(30.48048, dtype=float32), 'validation/wer': 1.4929811189688276, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.582283, dtype=float32), 'test/wer': 1.5334023927040805, 'test/num_examples': 2472, 'score': 64.67342042922974, 'total_duration': 64.86929154396057, 'global_step': 1, 'preemption_count': 0}), (3129, {'train/ctc_loss': DeviceArray(5.808312, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.775593, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6667857, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2457.005228996277, 'total_duration': 2590.7267742156982, 'global_step': 3129, 'preemption_count': 0}), (6301, {'train/ctc_loss': DeviceArray(2.8997095, dtype=float32), 'train/wer': 0.6955224675705192, 'validation/ctc_loss': DeviceArray(3.229979, dtype=float32), 'validation/wer': 0.7001997124911963, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.946555, dtype=float32), 'test/wer': 0.679280157617858, 'test/num_examples': 2472, 'score': 4850.080855131149, 'total_duration': 5074.892856359482, 'global_step': 6301, 'preemption_count': 0}), (9438, {'train/ctc_loss': DeviceArray(0.58203846, dtype=float32), 'train/wer': 0.2126234113994444, 'validation/ctc_loss': DeviceArray(0.93448687, dtype=float32), 'validation/wer': 0.28336018678424296, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6527644, dtype=float32), 'test/wer': 0.21906038632624458, 'test/num_examples': 2472, 'score': 7242.731763601303, 'total_duration': 7571.06015253067, 'global_step': 9438, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(0.54358226, dtype=float32), 'train/wer': 0.19421085151169246, 'validation/ctc_loss': DeviceArray(0.8781449, dtype=float32), 'validation/wer': 0.26658240793447113, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5981344, dtype=float32), 'test/wer': 0.20327828895253183, 'test/num_examples': 2472, 'score': 7659.337561368942, 'total_duration': 8086.6825613975525, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 04:40:27.229004 139666009474880 submission_runner.py:553] Timing: 7659.337561368942
I0405 04:40:27.229074 139666009474880 submission_runner.py:554] ====================
I0405 04:40:27.229522 139666009474880 submission_runner.py:613] Final librispeech_conformer score: 7659.337561368942
