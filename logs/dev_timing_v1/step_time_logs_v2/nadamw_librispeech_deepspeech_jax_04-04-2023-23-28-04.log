I0404 23:28:17.370075 140344867505984 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax.
I0404 23:28:17.425571 140344867505984 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 23:28:18.250317 140344867505984 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 23:28:18.251110 140344867505984 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 23:28:18.254538 140344867505984 submission_runner.py:511] Using RNG seed 790964958
I0404 23:28:19.643820 140344867505984 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 23:28:19.644071 140344867505984 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1.
I0404 23:28:19.644289 140344867505984 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/hparams.json.
I0404 23:28:19.763901 140344867505984 submission_runner.py:230] Starting train once: RAM USED (GB) 4.339269632
I0404 23:28:19.764076 140344867505984 submission_runner.py:231] Initializing dataset.
I0404 23:28:19.764238 140344867505984 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.339269632
I0404 23:28:19.764299 140344867505984 submission_runner.py:240] Initializing model.
I0404 23:28:36.503552 140344867505984 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.932134912
I0404 23:28:36.503799 140344867505984 submission_runner.py:252] Initializing optimizer.
I0404 23:28:37.173346 140344867505984 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.93237248
I0404 23:28:37.173572 140344867505984 submission_runner.py:261] Initializing metrics bundle.
I0404 23:28:37.173659 140344867505984 submission_runner.py:276] Initializing checkpoint and logger.
I0404 23:28:37.174674 140344867505984 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0404 23:28:37.175005 140344867505984 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 23:28:37.175073 140344867505984 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 23:28:38.256885 140344867505984 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0404 23:28:38.257941 140344867505984 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0404 23:28:38.261636 140344867505984 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.930476032
I0404 23:28:38.261831 140344867505984 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.930476032
I0404 23:28:38.261887 140344867505984 submission_runner.py:313] Starting training loop.
I0404 23:28:38.462880 140344867505984 input_pipeline.py:20] Loading split = train-clean-100
I0404 23:28:38.497519 140344867505984 input_pipeline.py:20] Loading split = train-clean-360
I0404 23:28:38.955417 140344867505984 input_pipeline.py:20] Loading split = train-other-500
I0404 23:28:43.233740 140344867505984 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 10.437328896
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0404 23:29:36.708291 140168862471936 logging_writer.py:48] [0] global_step=0, grad_norm=21.80446434020996, loss=32.84259796142578
I0404 23:29:36.723773 140344867505984 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 14.307815424
I0404 23:29:36.724096 140344867505984 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 14.307815424
I0404 23:29:36.724177 140344867505984 spec.py:298] Evaluating on the training split.
I0404 23:29:36.852030 140344867505984 input_pipeline.py:20] Loading split = train-clean-100
I0404 23:29:37.104236 140344867505984 input_pipeline.py:20] Loading split = train-clean-360
I0404 23:29:37.203907 140344867505984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0404 23:31:07.320517 140344867505984 spec.py:310] Evaluating on the validation split.
I0404 23:31:07.407824 140344867505984 input_pipeline.py:20] Loading split = dev-clean
I0404 23:31:07.412618 140344867505984 input_pipeline.py:20] Loading split = dev-other
I0404 23:32:00.719503 140344867505984 spec.py:326] Evaluating on the test split.
I0404 23:32:00.806255 140344867505984 input_pipeline.py:20] Loading split = test-clean
I0404 23:32:35.220800 140344867505984 submission_runner.py:382] Time since start: 58.46s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.023235, dtype=float32), 'train/wer': 4.494065343689482, 'validation/ctc_loss': DeviceArray(30.905153, dtype=float32), 'validation/wer': 4.172485986357803, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.905594, dtype=float32), 'test/wer': 4.305709585034428, 'test/num_examples': 2472}
I0404 23:32:35.221911 140344867505984 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.817708544
I0404 23:32:35.233535 140167213995776 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=58.261152, test/ctc_loss=30.905593872070312, test/num_examples=2472, test/wer=4.305710, total_duration=58.462213, train/ctc_loss=32.02323532104492, train/wer=4.494065, validation/ctc_loss=30.905153274536133, validation/num_examples=5348, validation/wer=4.172486
I0404 23:32:35.372353 140344867505984 checkpoints.py:356] Saving checkpoint at step: 1
I0404 23:32:35.794421 140344867505984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0404 23:32:35.795311 140344867505984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0404 23:32:35.801489 140344867505984 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.77744896
I0404 23:32:35.848552 140344867505984 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.7761792
I0404 23:32:54.570944 140344867505984 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 22.193504256
I0404 23:34:49.066951 140170798114560 logging_writer.py:48] [100] global_step=100, grad_norm=17.456350326538086, loss=10.160140991210938
I0404 23:36:45.313874 140170806507264 logging_writer.py:48] [200] global_step=200, grad_norm=0.8535619974136353, loss=6.511085510253906
I0404 23:38:41.288402 140170798114560 logging_writer.py:48] [300] global_step=300, grad_norm=0.7739102840423584, loss=6.002212047576904
I0404 23:40:38.070043 140170806507264 logging_writer.py:48] [400] global_step=400, grad_norm=0.6366356015205383, loss=5.888834476470947
I0404 23:42:34.411396 140170798114560 logging_writer.py:48] [500] global_step=500, grad_norm=0.4530723989009857, loss=5.839189529418945
I0404 23:44:30.656574 140170806507264 logging_writer.py:48] [600] global_step=600, grad_norm=0.39256227016448975, loss=5.764220237731934
I0404 23:46:27.278611 140170798114560 logging_writer.py:48] [700] global_step=700, grad_norm=0.4325016140937805, loss=5.653463363647461
I0404 23:48:23.271147 140170806507264 logging_writer.py:48] [800] global_step=800, grad_norm=0.4420097768306732, loss=5.574184417724609
I0404 23:50:19.251834 140170798114560 logging_writer.py:48] [900] global_step=900, grad_norm=0.6704890131950378, loss=5.492681503295898
I0404 23:52:15.656398 140170806507264 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5975203514099121, loss=5.417904853820801
I0404 23:54:15.171303 140168086411008 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.797158420085907, loss=5.135496139526367
I0404 23:56:11.003188 140167968978688 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6979820728302002, loss=4.782712459564209
I0404 23:58:06.262373 140168086411008 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.927441418170929, loss=4.402504920959473
I0405 00:00:01.797988 140167968978688 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.126985788345337, loss=4.028271675109863
I0405 00:01:58.058433 140168086411008 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.5932118892669678, loss=3.8458986282348633
I0405 00:03:53.767389 140167968978688 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.4555906057357788, loss=3.647040605545044
I0405 00:05:49.573409 140168086411008 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.686297655105591, loss=3.48836350440979
I0405 00:07:45.350215 140167968978688 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.2829155921936035, loss=3.354445457458496
I0405 00:09:40.882276 140168086411008 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.126141309738159, loss=3.2648887634277344
I0405 00:11:36.263752 140167968978688 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.3772401809692383, loss=3.144819498062134
I0405 00:12:36.520784 140344867505984 submission_runner.py:373] Before eval at step 2053: RAM USED (GB) 22.088146944
I0405 00:12:36.520996 140344867505984 spec.py:298] Evaluating on the training split.
I0405 00:13:04.266990 140344867505984 spec.py:310] Evaluating on the validation split.
I0405 00:13:37.776423 140344867505984 spec.py:326] Evaluating on the test split.
I0405 00:13:54.981399 140344867505984 submission_runner.py:382] Time since start: 2638.26s, 	Step: 2053, 	{'train/ctc_loss': DeviceArray(5.963202, dtype=float32), 'train/wer': 0.9276585446231798, 'validation/ctc_loss': DeviceArray(5.9225454, dtype=float32), 'validation/wer': 0.8872058582330751, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.7715607, dtype=float32), 'test/wer': 0.8877378993764345, 'test/num_examples': 2472}
I0405 00:13:54.982815 140344867505984 submission_runner.py:396] After eval at step 2053: RAM USED (GB) 21.752987648
I0405 00:13:55.002193 140168086411008 logging_writer.py:48] [2053] global_step=2053, preemption_count=0, score=2454.629913, test/ctc_loss=5.7715606689453125, test/num_examples=2472, test/wer=0.887738, total_duration=2638.256358, train/ctc_loss=5.963201999664307, train/wer=0.927659, validation/ctc_loss=5.922545433044434, validation/num_examples=5348, validation/wer=0.887206
I0405 00:13:55.152475 140344867505984 checkpoints.py:356] Saving checkpoint at step: 2053
I0405 00:13:55.737515 140344867505984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2053
I0405 00:13:55.749304 140344867505984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2053.
I0405 00:13:55.755637 140344867505984 submission_runner.py:416] After logging and checkpointing eval at step 2053: RAM USED (GB) 21.726572544
I0405 00:14:54.396745 140168086411008 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.3905093669891357, loss=3.0684046745300293
I0405 00:16:49.622497 140167968978688 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.360732555389404, loss=3.0003015995025635
I0405 00:18:45.603751 140168086411008 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.3529341220855713, loss=2.910830497741699
I0405 00:20:41.043411 140167968978688 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.370534658432007, loss=2.8543992042541504
I0405 00:22:36.216128 140168086411008 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.8467249870300293, loss=2.874079465866089
I0405 00:24:31.536802 140167968978688 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.149409294128418, loss=2.770247220993042
I0405 00:26:26.343489 140168086411008 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.015885353088379, loss=2.668253183364868
I0405 00:28:21.238161 140167968978688 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.46616792678833, loss=2.634028434753418
I0405 00:30:16.260180 140168086411008 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.911168098449707, loss=2.5746538639068604
I0405 00:32:11.544587 140167968978688 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.2278852462768555, loss=2.5660009384155273
I0405 00:34:09.785802 140168086411008 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.9242732524871826, loss=2.5416581630706787
I0405 00:36:05.307965 140167968978688 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.045543670654297, loss=2.491621732711792
I0405 00:38:00.723089 140168086411008 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.8273074626922607, loss=2.3927032947540283
I0405 00:39:55.206906 140167968978688 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.462750196456909, loss=2.469494104385376
I0405 00:41:49.013851 140168086411008 logging_writer.py:48] [3500] global_step=3500, grad_norm=6.185305118560791, loss=2.4195220470428467
I0405 00:43:44.853784 140167968978688 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.451472282409668, loss=2.374282121658325
I0405 00:45:40.153064 140168086411008 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2344038486480713, loss=2.3237318992614746
I0405 00:47:35.626472 140167968978688 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.063699245452881, loss=2.3722734451293945
I0405 00:49:30.894712 140168086411008 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.1278724670410156, loss=2.3422176837921143
I0405 00:51:26.257701 140167968978688 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.104990005493164, loss=2.2685461044311523
I0405 00:53:21.354768 140168086411008 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.465312480926514, loss=2.2904891967773438
I0405 00:53:55.765550 140344867505984 submission_runner.py:373] Before eval at step 4128: RAM USED (GB) 22.558367744
I0405 00:53:55.766012 140344867505984 spec.py:298] Evaluating on the training split.
I0405 00:54:31.983790 140344867505984 spec.py:310] Evaluating on the validation split.
I0405 00:55:08.524520 140344867505984 spec.py:326] Evaluating on the test split.
I0405 00:55:27.607444 140344867505984 submission_runner.py:382] Time since start: 5117.50s, 	Step: 4128, 	{'train/ctc_loss': DeviceArray(1.7882931, dtype=float32), 'train/wer': 0.45507817702367953, 'validation/ctc_loss': DeviceArray(2.2406468, dtype=float32), 'validation/wer': 0.511630599426912, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.760041, dtype=float32), 'test/wer': 0.4409034590620112, 'test/num_examples': 2472}
I0405 00:55:27.608624 140344867505984 submission_runner.py:396] After eval at step 4128: RAM USED (GB) 21.509992448
I0405 00:55:27.628065 140168086411008 logging_writer.py:48] [4128] global_step=4128, preemption_count=0, score=4850.276249, test/ctc_loss=1.7600409984588623, test/num_examples=2472, test/wer=0.440903, total_duration=5117.500383, train/ctc_loss=1.7882931232452393, train/wer=0.455078, validation/ctc_loss=2.2406468391418457, validation/num_examples=5348, validation/wer=0.511631
I0405 00:55:27.778498 140344867505984 checkpoints.py:356] Saving checkpoint at step: 4128
I0405 00:55:28.427093 140344867505984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4128
I0405 00:55:28.442277 140344867505984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4128.
I0405 00:55:28.448854 140344867505984 submission_runner.py:416] After logging and checkpointing eval at step 4128: RAM USED (GB) 21.47858432
I0405 00:56:51.943262 140167968978688 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.475283622741699, loss=2.2276151180267334
I0405 00:58:46.720985 140167700543232 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.1355667114257812, loss=2.1822853088378906
I0405 01:00:41.782819 140167968978688 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.6819329261779785, loss=2.227464437484741
I0405 01:02:37.476281 140167700543232 logging_writer.py:48] [4500] global_step=4500, grad_norm=7.5871124267578125, loss=2.2086069583892822
I0405 01:04:32.775503 140167968978688 logging_writer.py:48] [4600] global_step=4600, grad_norm=6.442210674285889, loss=2.2046515941619873
I0405 01:06:28.305703 140167700543232 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.4567763805389404, loss=2.1580750942230225
I0405 01:08:23.792510 140167968978688 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.152251720428467, loss=2.125859260559082
I0405 01:10:19.117842 140167700543232 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.628321647644043, loss=2.0632693767547607
I0405 01:12:14.305909 140167968978688 logging_writer.py:48] [5000] global_step=5000, grad_norm=5.7889084815979, loss=2.050034999847412
I0405 01:14:09.609614 140167700543232 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.7157695293426514, loss=2.0721147060394287
I0405 01:16:08.816462 140168086411008 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.7118237018585205, loss=2.0591189861297607
I0405 01:18:03.729627 140167968978688 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.923670530319214, loss=2.0388436317443848
I0405 01:19:58.563230 140168086411008 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.270229816436768, loss=2.1182332038879395
I0405 01:21:53.299205 140167968978688 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.9006338119506836, loss=2.065216302871704
I0405 01:23:48.765282 140168086411008 logging_writer.py:48] [5600] global_step=5600, grad_norm=5.524981498718262, loss=2.0749924182891846
I0405 01:25:44.253761 140167968978688 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.120017051696777, loss=1.9984817504882812
I0405 01:27:39.794249 140168086411008 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.135730266571045, loss=1.9651015996932983
I0405 01:29:36.567557 140167968978688 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.333832263946533, loss=1.9867819547653198
I0405 01:31:31.790419 140168086411008 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.5400710105896, loss=2.0325775146484375
I0405 01:33:27.030414 140167968978688 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.083644390106201, loss=1.9733856916427612
I0405 01:35:26.018465 140168086411008 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.673410654067993, loss=1.9025262594223022
I0405 01:35:29.347821 140344867505984 submission_runner.py:373] Before eval at step 6204: RAM USED (GB) 23.354724352
I0405 01:35:29.348059 140344867505984 spec.py:298] Evaluating on the training split.
I0405 01:36:08.006975 140344867505984 spec.py:310] Evaluating on the validation split.
I0405 01:36:44.692515 140344867505984 spec.py:326] Evaluating on the test split.
I0405 01:37:03.687524 140344867505984 submission_runner.py:382] Time since start: 7611.08s, 	Step: 6204, 	{'train/ctc_loss': DeviceArray(0.68455976, dtype=float32), 'train/wer': 0.2303609173954067, 'validation/ctc_loss': DeviceArray(1.1118026, dtype=float32), 'validation/wer': 0.301585157599205, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7552925, dtype=float32), 'test/wer': 0.23535027319074606, 'test/num_examples': 2472}
I0405 01:37:03.688895 140344867505984 submission_runner.py:396] After eval at step 6204: RAM USED (GB) 21.445296128
I0405 01:37:03.708952 140168086411008 logging_writer.py:48] [6204] global_step=6204, preemption_count=0, score=7246.746023, test/ctc_loss=0.7552924752235413, test/num_examples=2472, test/wer=0.235350, total_duration=7611.083315, train/ctc_loss=0.6845597624778748, train/wer=0.230361, validation/ctc_loss=1.111802577972412, validation/num_examples=5348, validation/wer=0.301585
I0405 01:37:03.858542 140344867505984 checkpoints.py:356] Saving checkpoint at step: 6204
I0405 01:37:04.482911 140344867505984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6204
I0405 01:37:04.498314 140344867505984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6204.
I0405 01:37:04.504971 140344867505984 submission_runner.py:416] After logging and checkpointing eval at step 6204: RAM USED (GB) 21.416357888
I0405 01:38:55.426320 140167968978688 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.635128021240234, loss=1.951422929763794
I0405 01:40:50.047959 140167230781184 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.4340546131134033, loss=1.9312320947647095
I0405 01:42:45.920347 140167968978688 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.1317310333251953, loss=1.9934509992599487
I0405 01:44:40.300728 140167230781184 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.8795828819274902, loss=1.933712363243103
I0405 01:46:35.142837 140167968978688 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.315570831298828, loss=1.9014673233032227
I0405 01:48:30.024024 140167230781184 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.771669864654541, loss=1.9219321012496948
I0405 01:50:25.052968 140167968978688 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.5086350440979004, loss=1.8999210596084595
I0405 01:52:19.780574 140167230781184 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.6198720932006836, loss=1.919759750366211
I0405 01:54:14.794214 140167968978688 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.1535158157348633, loss=1.8739436864852905
I0405 01:56:08.905100 140167230781184 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.377143859863281, loss=1.8634008169174194
I0405 01:58:05.852380 140168086411008 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.152255058288574, loss=1.8745200634002686
I0405 01:59:59.605999 140167968978688 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.761942386627197, loss=1.8806824684143066
I0405 02:01:54.794363 140168086411008 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.100747108459473, loss=1.8430395126342773
I0405 02:03:50.320775 140167968978688 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.7831742763519287, loss=1.9203275442123413
I0405 02:05:45.730989 140168086411008 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.6334211826324463, loss=1.8197391033172607
I0405 02:07:40.893310 140167968978688 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.151207447052002, loss=1.8196890354156494
I0405 02:09:35.700473 140168086411008 logging_writer.py:48] [7900] global_step=7900, grad_norm=4.301738739013672, loss=1.8684604167938232
I0405 02:11:29.103050 140344867505984 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 22.83384832
I0405 02:11:29.103255 140344867505984 spec.py:298] Evaluating on the training split.
I0405 02:12:07.415495 140344867505984 spec.py:310] Evaluating on the validation split.
I0405 02:12:45.330363 140344867505984 spec.py:326] Evaluating on the test split.
I0405 02:13:05.300696 140344867505984 submission_runner.py:382] Time since start: 9770.84s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(0.54799527, dtype=float32), 'train/wer': 0.18403690477437193, 'validation/ctc_loss': DeviceArray(0.91029954, dtype=float32), 'validation/wer': 0.2573975629287306, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5892423, dtype=float32), 'test/wer': 0.18887737899376433, 'test/num_examples': 2472}
I0405 02:13:05.301975 140344867505984 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 22.073827328
I0405 02:13:05.325103 140170373895936 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9307.562197, test/ctc_loss=0.5892422795295715, test/num_examples=2472, test/wer=0.188877, total_duration=9770.836489, train/ctc_loss=0.5479952692985535, train/wer=0.184037, validation/ctc_loss=0.91029953956604, validation/num_examples=5348, validation/wer=0.257398
I0405 02:13:05.475262 140344867505984 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 02:13:06.141661 140344867505984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 02:13:06.156917 140344867505984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 02:13:06.164268 140344867505984 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 22.041198592
I0405 02:13:06.171295 140170365503232 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9307.562197
I0405 02:13:06.291932 140344867505984 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 02:13:07.229149 140344867505984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 02:13:07.244438 140344867505984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 02:13:08.727315 140344867505984 submission_runner.py:550] Tuning trial 1/1
I0405 02:13:08.727545 140344867505984 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 02:13:08.732180 140344867505984 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.023235, dtype=float32), 'train/wer': 4.494065343689482, 'validation/ctc_loss': DeviceArray(30.905153, dtype=float32), 'validation/wer': 4.172485986357803, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.905594, dtype=float32), 'test/wer': 4.305709585034428, 'test/num_examples': 2472, 'score': 58.261152267456055, 'total_duration': 58.46221303939819, 'global_step': 1, 'preemption_count': 0}), (2053, {'train/ctc_loss': DeviceArray(5.963202, dtype=float32), 'train/wer': 0.9276585446231798, 'validation/ctc_loss': DeviceArray(5.9225454, dtype=float32), 'validation/wer': 0.8872058582330751, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.7715607, dtype=float32), 'test/wer': 0.8877378993764345, 'test/num_examples': 2472, 'score': 2454.629913330078, 'total_duration': 2638.256358385086, 'global_step': 2053, 'preemption_count': 0}), (4128, {'train/ctc_loss': DeviceArray(1.7882931, dtype=float32), 'train/wer': 0.45507817702367953, 'validation/ctc_loss': DeviceArray(2.2406468, dtype=float32), 'validation/wer': 0.511630599426912, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.760041, dtype=float32), 'test/wer': 0.4409034590620112, 'test/num_examples': 2472, 'score': 4850.276249408722, 'total_duration': 5117.500383138657, 'global_step': 4128, 'preemption_count': 0}), (6204, {'train/ctc_loss': DeviceArray(0.68455976, dtype=float32), 'train/wer': 0.2303609173954067, 'validation/ctc_loss': DeviceArray(1.1118026, dtype=float32), 'validation/wer': 0.301585157599205, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7552925, dtype=float32), 'test/wer': 0.23535027319074606, 'test/num_examples': 2472, 'score': 7246.746022939682, 'total_duration': 7611.08331489563, 'global_step': 6204, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(0.54799527, dtype=float32), 'train/wer': 0.18403690477437193, 'validation/ctc_loss': DeviceArray(0.91029954, dtype=float32), 'validation/wer': 0.2573975629287306, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5892423, dtype=float32), 'test/wer': 0.18887737899376433, 'test/num_examples': 2472, 'score': 9307.562197446823, 'total_duration': 9770.836488723755, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0405 02:13:08.732325 140344867505984 submission_runner.py:553] Timing: 9307.562197446823
I0405 02:13:08.732374 140344867505984 submission_runner.py:554] ====================
I0405 02:13:08.733197 140344867505984 submission_runner.py:613] Final librispeech_deepspeech score: 9307.562197446823
