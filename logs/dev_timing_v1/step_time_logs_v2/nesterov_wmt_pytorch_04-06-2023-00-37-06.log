WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 00:37:21.245128 140492124292928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 00:37:21.279941 140167596291904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 00:37:21.280781 140383153772352 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 00:37:22.183149 139680516011840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 00:37:22.183392 140331440240448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 00:37:22.234059 139667572893504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 00:37:22.242589 140459865937728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 00:37:22.243575 139800543508288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 00:37:22.243918 139800543508288 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.244852 139667572893504 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.245331 139680516011840 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.245463 140331440240448 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.249906 140492124292928 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.249926 140167596291904 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.249956 140383153772352 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:22.253237 140459865937728 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:37:26.091876 139800543508288 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch.
W0406 00:37:26.158742 139667572893504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.159328 140383153772352 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.160676 140167596291904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.161749 140459865937728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.162776 140331440240448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.162973 139680516011840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.163483 139800543508288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:37:26.163625 140492124292928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:37:26.167176 139800543508288 submission_runner.py:511] Using RNG seed 1140138960
I0406 00:37:26.168111 139800543508288 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 00:37:26.168219 139800543508288 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1.
I0406 00:37:26.168394 139800543508288 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/hparams.json.
I0406 00:37:26.169408 139800543508288 submission_runner.py:230] Starting train once: RAM USED (GB) 15.141486592
I0406 00:37:26.169496 139800543508288 submission_runner.py:231] Initializing dataset.
I0406 00:37:26.169648 139800543508288 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.141486592
I0406 00:37:26.169702 139800543508288 submission_runner.py:240] Initializing model.
I0406 00:37:29.786681 139800543508288 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.449831424
I0406 00:37:29.786859 139800543508288 submission_runner.py:252] Initializing optimizer.
I0406 00:37:29.897773 139800543508288 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.452862464
I0406 00:37:29.897955 139800543508288 submission_runner.py:261] Initializing metrics bundle.
I0406 00:37:29.898012 139800543508288 submission_runner.py:276] Initializing checkpoint and logger.
I0406 00:37:29.899282 139800543508288 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 00:37:29.899398 139800543508288 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 00:37:30.597051 139800543508288 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/meta_data_0.json.
I0406 00:37:30.599683 139800543508288 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/flags_0.json.
I0406 00:37:30.634801 139800543508288 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.508912128
I0406 00:37:30.636147 139800543508288 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.508912128
I0406 00:37:30.636297 139800543508288 submission_runner.py:313] Starting training loop.
I0406 00:37:30.647201 139800543508288 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:30.651417 139800543508288 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:37:30.651539 139800543508288 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:37:30.703138 139800543508288 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:32.947940 139800543508288 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 19.803140096
I0406 00:37:34.442774 139760274237184 logging_writer.py:48] [0] global_step=0, grad_norm=4.874121, loss=11.130675
I0406 00:37:34.448064 139800543508288 submission.py:139] 0) loss = 11.131, grad_norm = 4.874
I0406 00:37:34.449170 139800543508288 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 19.900977152
I0406 00:37:34.449685 139800543508288 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 19.900977152
I0406 00:37:34.449780 139800543508288 spec.py:298] Evaluating on the training split.
I0406 00:37:34.451920 139800543508288 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:34.454699 139800543508288 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:37:34.454859 139800543508288 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:37:34.483880 139800543508288 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:37:38.573937 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 00:42:11.857620 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 00:42:11.861288 139800543508288 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:42:11.864712 139800543508288 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:42:11.864869 139800543508288 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:42:11.895018 139800543508288 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:42:15.695861 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 00:46:41.892593 139800543508288 spec.py:326] Evaluating on the test split.
I0406 00:46:41.895208 139800543508288 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:46:41.898065 139800543508288 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:46:41.898186 139800543508288 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:46:41.925928 139800543508288 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:46:45.784004 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 00:51:17.383807 139800543508288 submission_runner.py:382] Time since start: 3.81s, 	Step: 1, 	{'train/accuracy': 0.0006483019039603285, 'train/loss': 11.145271519073724, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.141378439201002, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.126150427052467, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0406 00:51:17.384270 139800543508288 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.292399104
I0406 00:51:17.392534 139742824670976 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.811969, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.126150, test/num_examples=3003, total_duration=3.814263, train/accuracy=0.000648, train/bleu=0.000000, train/loss=11.145272, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.141378, validation/num_examples=3000
I0406 00:51:18.925564 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_1.
I0406 00:51:18.926205 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.292182016
I0406 00:51:18.930389 139800543508288 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.29215744
I0406 00:51:18.934014 139800543508288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934044 139667572893504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934056 140492124292928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934064 140383153772352 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934073 140459865937728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934071 139680516011840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934084 140167596291904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:18.934083 140331440240448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 00:51:19.354508 139742816278272 logging_writer.py:48] [1] global_step=1, grad_norm=4.820903, loss=11.125793
I0406 00:51:19.357863 139800543508288 submission.py:139] 1) loss = 11.126, grad_norm = 4.821
I0406 00:51:19.358656 139800543508288 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.29205504
I0406 00:51:19.786126 139742824670976 logging_writer.py:48] [2] global_step=2, grad_norm=4.863239, loss=11.130170
I0406 00:51:19.789168 139800543508288 submission.py:139] 2) loss = 11.130, grad_norm = 4.863
I0406 00:51:20.218953 139742816278272 logging_writer.py:48] [3] global_step=3, grad_norm=4.807553, loss=11.114563
I0406 00:51:20.221958 139800543508288 submission.py:139] 3) loss = 11.115, grad_norm = 4.808
I0406 00:51:20.650234 139742824670976 logging_writer.py:48] [4] global_step=4, grad_norm=4.765692, loss=11.094654
I0406 00:51:20.653247 139800543508288 submission.py:139] 4) loss = 11.095, grad_norm = 4.766
I0406 00:51:21.085769 139742816278272 logging_writer.py:48] [5] global_step=5, grad_norm=4.688471, loss=11.069826
I0406 00:51:21.088748 139800543508288 submission.py:139] 5) loss = 11.070, grad_norm = 4.688
I0406 00:51:21.517681 139742824670976 logging_writer.py:48] [6] global_step=6, grad_norm=4.631832, loss=11.030447
I0406 00:51:21.520766 139800543508288 submission.py:139] 6) loss = 11.030, grad_norm = 4.632
I0406 00:51:21.952046 139742816278272 logging_writer.py:48] [7] global_step=7, grad_norm=4.414243, loss=10.974300
I0406 00:51:21.955087 139800543508288 submission.py:139] 7) loss = 10.974, grad_norm = 4.414
I0406 00:51:22.388322 139742824670976 logging_writer.py:48] [8] global_step=8, grad_norm=4.238073, loss=10.921610
I0406 00:51:22.391189 139800543508288 submission.py:139] 8) loss = 10.922, grad_norm = 4.238
I0406 00:51:22.817970 139742816278272 logging_writer.py:48] [9] global_step=9, grad_norm=3.994993, loss=10.841324
I0406 00:51:22.820926 139800543508288 submission.py:139] 9) loss = 10.841, grad_norm = 3.995
I0406 00:51:23.253320 139742824670976 logging_writer.py:48] [10] global_step=10, grad_norm=3.758150, loss=10.763252
I0406 00:51:23.256490 139800543508288 submission.py:139] 10) loss = 10.763, grad_norm = 3.758
I0406 00:51:23.685719 139742816278272 logging_writer.py:48] [11] global_step=11, grad_norm=3.524824, loss=10.681776
I0406 00:51:23.688780 139800543508288 submission.py:139] 11) loss = 10.682, grad_norm = 3.525
I0406 00:51:24.123562 139742824670976 logging_writer.py:48] [12] global_step=12, grad_norm=3.141325, loss=10.604011
I0406 00:51:24.126573 139800543508288 submission.py:139] 12) loss = 10.604, grad_norm = 3.141
I0406 00:51:24.560596 139742816278272 logging_writer.py:48] [13] global_step=13, grad_norm=2.886942, loss=10.506534
I0406 00:51:24.564080 139800543508288 submission.py:139] 13) loss = 10.507, grad_norm = 2.887
I0406 00:51:24.992077 139742824670976 logging_writer.py:48] [14] global_step=14, grad_norm=2.638746, loss=10.433598
I0406 00:51:24.995237 139800543508288 submission.py:139] 14) loss = 10.434, grad_norm = 2.639
I0406 00:51:25.427911 139742816278272 logging_writer.py:48] [15] global_step=15, grad_norm=2.431110, loss=10.360952
I0406 00:51:25.431030 139800543508288 submission.py:139] 15) loss = 10.361, grad_norm = 2.431
I0406 00:51:25.859879 139742824670976 logging_writer.py:48] [16] global_step=16, grad_norm=2.164739, loss=10.289012
I0406 00:51:25.862964 139800543508288 submission.py:139] 16) loss = 10.289, grad_norm = 2.165
I0406 00:51:26.297219 139742816278272 logging_writer.py:48] [17] global_step=17, grad_norm=2.022593, loss=10.219146
I0406 00:51:26.300259 139800543508288 submission.py:139] 17) loss = 10.219, grad_norm = 2.023
I0406 00:51:26.734075 139742824670976 logging_writer.py:48] [18] global_step=18, grad_norm=1.828652, loss=10.175341
I0406 00:51:26.737124 139800543508288 submission.py:139] 18) loss = 10.175, grad_norm = 1.829
I0406 00:51:27.165989 139742816278272 logging_writer.py:48] [19] global_step=19, grad_norm=1.747690, loss=10.108368
I0406 00:51:27.169298 139800543508288 submission.py:139] 19) loss = 10.108, grad_norm = 1.748
I0406 00:51:27.600477 139742824670976 logging_writer.py:48] [20] global_step=20, grad_norm=1.626438, loss=10.045277
I0406 00:51:27.603684 139800543508288 submission.py:139] 20) loss = 10.045, grad_norm = 1.626
I0406 00:51:28.030807 139742816278272 logging_writer.py:48] [21] global_step=21, grad_norm=1.541496, loss=10.005267
I0406 00:51:28.033794 139800543508288 submission.py:139] 21) loss = 10.005, grad_norm = 1.541
I0406 00:51:28.463864 139742824670976 logging_writer.py:48] [22] global_step=22, grad_norm=1.457925, loss=9.961932
I0406 00:51:28.467125 139800543508288 submission.py:139] 22) loss = 9.962, grad_norm = 1.458
I0406 00:51:28.894093 139742816278272 logging_writer.py:48] [23] global_step=23, grad_norm=1.387838, loss=9.916162
I0406 00:51:28.897353 139800543508288 submission.py:139] 23) loss = 9.916, grad_norm = 1.388
I0406 00:51:29.328546 139742824670976 logging_writer.py:48] [24] global_step=24, grad_norm=1.310459, loss=9.883201
I0406 00:51:29.331554 139800543508288 submission.py:139] 24) loss = 9.883, grad_norm = 1.310
I0406 00:51:29.760207 139742816278272 logging_writer.py:48] [25] global_step=25, grad_norm=1.258855, loss=9.838496
I0406 00:51:29.763221 139800543508288 submission.py:139] 25) loss = 9.838, grad_norm = 1.259
I0406 00:51:30.191082 139742824670976 logging_writer.py:48] [26] global_step=26, grad_norm=1.201798, loss=9.792854
I0406 00:51:30.194366 139800543508288 submission.py:139] 26) loss = 9.793, grad_norm = 1.202
I0406 00:51:30.623280 139742816278272 logging_writer.py:48] [27] global_step=27, grad_norm=1.120564, loss=9.773960
I0406 00:51:30.626587 139800543508288 submission.py:139] 27) loss = 9.774, grad_norm = 1.121
I0406 00:51:31.054278 139742824670976 logging_writer.py:48] [28] global_step=28, grad_norm=1.060654, loss=9.757911
I0406 00:51:31.057452 139800543508288 submission.py:139] 28) loss = 9.758, grad_norm = 1.061
I0406 00:51:31.490328 139742816278272 logging_writer.py:48] [29] global_step=29, grad_norm=1.005576, loss=9.708589
I0406 00:51:31.493556 139800543508288 submission.py:139] 29) loss = 9.709, grad_norm = 1.006
I0406 00:51:31.923998 139742824670976 logging_writer.py:48] [30] global_step=30, grad_norm=0.978312, loss=9.673034
I0406 00:51:31.927212 139800543508288 submission.py:139] 30) loss = 9.673, grad_norm = 0.978
I0406 00:51:32.357795 139742816278272 logging_writer.py:48] [31] global_step=31, grad_norm=0.905284, loss=9.634570
I0406 00:51:32.361053 139800543508288 submission.py:139] 31) loss = 9.635, grad_norm = 0.905
I0406 00:51:32.792277 139742824670976 logging_writer.py:48] [32] global_step=32, grad_norm=0.853692, loss=9.618662
I0406 00:51:32.795288 139800543508288 submission.py:139] 32) loss = 9.619, grad_norm = 0.854
I0406 00:51:33.228341 139742816278272 logging_writer.py:48] [33] global_step=33, grad_norm=0.812033, loss=9.609411
I0406 00:51:33.231415 139800543508288 submission.py:139] 33) loss = 9.609, grad_norm = 0.812
I0406 00:51:33.661869 139742824670976 logging_writer.py:48] [34] global_step=34, grad_norm=0.773179, loss=9.579027
I0406 00:51:33.664802 139800543508288 submission.py:139] 34) loss = 9.579, grad_norm = 0.773
I0406 00:51:34.095208 139742816278272 logging_writer.py:48] [35] global_step=35, grad_norm=0.732513, loss=9.577568
I0406 00:51:34.098468 139800543508288 submission.py:139] 35) loss = 9.578, grad_norm = 0.733
I0406 00:51:34.531381 139742824670976 logging_writer.py:48] [36] global_step=36, grad_norm=0.697640, loss=9.550523
I0406 00:51:34.534528 139800543508288 submission.py:139] 36) loss = 9.551, grad_norm = 0.698
I0406 00:51:34.964182 139742816278272 logging_writer.py:48] [37] global_step=37, grad_norm=0.680030, loss=9.527958
I0406 00:51:34.967322 139800543508288 submission.py:139] 37) loss = 9.528, grad_norm = 0.680
I0406 00:51:35.395265 139742824670976 logging_writer.py:48] [38] global_step=38, grad_norm=0.652321, loss=9.519711
I0406 00:51:35.399005 139800543508288 submission.py:139] 38) loss = 9.520, grad_norm = 0.652
I0406 00:51:35.828329 139742816278272 logging_writer.py:48] [39] global_step=39, grad_norm=0.617818, loss=9.518533
I0406 00:51:35.831392 139800543508288 submission.py:139] 39) loss = 9.519, grad_norm = 0.618
I0406 00:51:36.259546 139742824670976 logging_writer.py:48] [40] global_step=40, grad_norm=0.620691, loss=9.488850
I0406 00:51:36.262954 139800543508288 submission.py:139] 40) loss = 9.489, grad_norm = 0.621
I0406 00:51:36.698260 139742816278272 logging_writer.py:48] [41] global_step=41, grad_norm=0.596452, loss=9.466053
I0406 00:51:36.701687 139800543508288 submission.py:139] 41) loss = 9.466, grad_norm = 0.596
I0406 00:51:37.131266 139742824670976 logging_writer.py:48] [42] global_step=42, grad_norm=0.578962, loss=9.459721
I0406 00:51:37.134304 139800543508288 submission.py:139] 42) loss = 9.460, grad_norm = 0.579
I0406 00:51:37.561956 139742816278272 logging_writer.py:48] [43] global_step=43, grad_norm=0.581665, loss=9.415297
I0406 00:51:37.565016 139800543508288 submission.py:139] 43) loss = 9.415, grad_norm = 0.582
I0406 00:51:37.994664 139742824670976 logging_writer.py:48] [44] global_step=44, grad_norm=0.566829, loss=9.416343
I0406 00:51:37.997669 139800543508288 submission.py:139] 44) loss = 9.416, grad_norm = 0.567
I0406 00:51:38.425426 139742816278272 logging_writer.py:48] [45] global_step=45, grad_norm=0.536219, loss=9.392597
I0406 00:51:38.428553 139800543508288 submission.py:139] 45) loss = 9.393, grad_norm = 0.536
I0406 00:51:38.860461 139742824670976 logging_writer.py:48] [46] global_step=46, grad_norm=0.526550, loss=9.383540
I0406 00:51:38.863497 139800543508288 submission.py:139] 46) loss = 9.384, grad_norm = 0.527
I0406 00:51:39.292209 139742816278272 logging_writer.py:48] [47] global_step=47, grad_norm=0.498373, loss=9.401859
I0406 00:51:39.295251 139800543508288 submission.py:139] 47) loss = 9.402, grad_norm = 0.498
I0406 00:51:39.725659 139742824670976 logging_writer.py:48] [48] global_step=48, grad_norm=0.495543, loss=9.358614
I0406 00:51:39.728996 139800543508288 submission.py:139] 48) loss = 9.359, grad_norm = 0.496
I0406 00:51:40.164063 139742816278272 logging_writer.py:48] [49] global_step=49, grad_norm=0.472539, loss=9.338660
I0406 00:51:40.167134 139800543508288 submission.py:139] 49) loss = 9.339, grad_norm = 0.473
I0406 00:51:40.595232 139742824670976 logging_writer.py:48] [50] global_step=50, grad_norm=0.454011, loss=9.379440
I0406 00:51:40.598507 139800543508288 submission.py:139] 50) loss = 9.379, grad_norm = 0.454
I0406 00:51:41.029109 139742816278272 logging_writer.py:48] [51] global_step=51, grad_norm=0.445461, loss=9.322172
I0406 00:51:41.032077 139800543508288 submission.py:139] 51) loss = 9.322, grad_norm = 0.445
I0406 00:51:41.459848 139742824670976 logging_writer.py:48] [52] global_step=52, grad_norm=0.428478, loss=9.328497
I0406 00:51:41.462840 139800543508288 submission.py:139] 52) loss = 9.328, grad_norm = 0.428
I0406 00:51:41.891635 139742816278272 logging_writer.py:48] [53] global_step=53, grad_norm=0.415095, loss=9.324176
I0406 00:51:41.894746 139800543508288 submission.py:139] 53) loss = 9.324, grad_norm = 0.415
I0406 00:51:42.323241 139742824670976 logging_writer.py:48] [54] global_step=54, grad_norm=0.402603, loss=9.313179
I0406 00:51:42.326642 139800543508288 submission.py:139] 54) loss = 9.313, grad_norm = 0.403
I0406 00:51:42.756381 139742816278272 logging_writer.py:48] [55] global_step=55, grad_norm=0.391664, loss=9.334685
I0406 00:51:42.759632 139800543508288 submission.py:139] 55) loss = 9.335, grad_norm = 0.392
I0406 00:51:43.191310 139742824670976 logging_writer.py:48] [56] global_step=56, grad_norm=0.394155, loss=9.285385
I0406 00:51:43.194385 139800543508288 submission.py:139] 56) loss = 9.285, grad_norm = 0.394
I0406 00:51:43.625252 139742816278272 logging_writer.py:48] [57] global_step=57, grad_norm=0.369603, loss=9.277529
I0406 00:51:43.628477 139800543508288 submission.py:139] 57) loss = 9.278, grad_norm = 0.370
I0406 00:51:44.060682 139742824670976 logging_writer.py:48] [58] global_step=58, grad_norm=0.367427, loss=9.306976
I0406 00:51:44.063795 139800543508288 submission.py:139] 58) loss = 9.307, grad_norm = 0.367
I0406 00:51:44.497390 139742816278272 logging_writer.py:48] [59] global_step=59, grad_norm=0.354012, loss=9.265102
I0406 00:51:44.500441 139800543508288 submission.py:139] 59) loss = 9.265, grad_norm = 0.354
I0406 00:51:44.933218 139742824670976 logging_writer.py:48] [60] global_step=60, grad_norm=0.340982, loss=9.257589
I0406 00:51:44.936230 139800543508288 submission.py:139] 60) loss = 9.258, grad_norm = 0.341
I0406 00:51:45.363803 139742816278272 logging_writer.py:48] [61] global_step=61, grad_norm=0.335279, loss=9.264783
I0406 00:51:45.366976 139800543508288 submission.py:139] 61) loss = 9.265, grad_norm = 0.335
I0406 00:51:45.798296 139742824670976 logging_writer.py:48] [62] global_step=62, grad_norm=0.336207, loss=9.267229
I0406 00:51:45.801349 139800543508288 submission.py:139] 62) loss = 9.267, grad_norm = 0.336
I0406 00:51:46.233076 139742816278272 logging_writer.py:48] [63] global_step=63, grad_norm=0.323075, loss=9.255551
I0406 00:51:46.235919 139800543508288 submission.py:139] 63) loss = 9.256, grad_norm = 0.323
I0406 00:51:46.666020 139742824670976 logging_writer.py:48] [64] global_step=64, grad_norm=0.311530, loss=9.258324
I0406 00:51:46.669184 139800543508288 submission.py:139] 64) loss = 9.258, grad_norm = 0.312
I0406 00:51:47.098880 139742816278272 logging_writer.py:48] [65] global_step=65, grad_norm=0.310715, loss=9.240546
I0406 00:51:47.101948 139800543508288 submission.py:139] 65) loss = 9.241, grad_norm = 0.311
I0406 00:51:47.532051 139742824670976 logging_writer.py:48] [66] global_step=66, grad_norm=0.310082, loss=9.229804
I0406 00:51:47.535328 139800543508288 submission.py:139] 66) loss = 9.230, grad_norm = 0.310
I0406 00:51:47.972721 139742816278272 logging_writer.py:48] [67] global_step=67, grad_norm=0.297093, loss=9.201365
I0406 00:51:47.975767 139800543508288 submission.py:139] 67) loss = 9.201, grad_norm = 0.297
I0406 00:51:48.408991 139742824670976 logging_writer.py:48] [68] global_step=68, grad_norm=0.294449, loss=9.192369
I0406 00:51:48.412123 139800543508288 submission.py:139] 68) loss = 9.192, grad_norm = 0.294
I0406 00:51:48.841336 139742816278272 logging_writer.py:48] [69] global_step=69, grad_norm=0.289128, loss=9.180171
I0406 00:51:48.844331 139800543508288 submission.py:139] 69) loss = 9.180, grad_norm = 0.289
I0406 00:51:49.275138 139742824670976 logging_writer.py:48] [70] global_step=70, grad_norm=0.287927, loss=9.196289
I0406 00:51:49.278259 139800543508288 submission.py:139] 70) loss = 9.196, grad_norm = 0.288
I0406 00:51:49.708423 139742816278272 logging_writer.py:48] [71] global_step=71, grad_norm=0.270415, loss=9.205593
I0406 00:51:49.711323 139800543508288 submission.py:139] 71) loss = 9.206, grad_norm = 0.270
I0406 00:51:50.140279 139742824670976 logging_writer.py:48] [72] global_step=72, grad_norm=0.273765, loss=9.188175
I0406 00:51:50.143393 139800543508288 submission.py:139] 72) loss = 9.188, grad_norm = 0.274
I0406 00:51:50.574254 139742816278272 logging_writer.py:48] [73] global_step=73, grad_norm=0.255762, loss=9.208340
I0406 00:51:50.577564 139800543508288 submission.py:139] 73) loss = 9.208, grad_norm = 0.256
I0406 00:51:51.010910 139742824670976 logging_writer.py:48] [74] global_step=74, grad_norm=0.259745, loss=9.166218
I0406 00:51:51.013725 139800543508288 submission.py:139] 74) loss = 9.166, grad_norm = 0.260
I0406 00:51:51.446768 139742816278272 logging_writer.py:48] [75] global_step=75, grad_norm=0.257851, loss=9.180549
I0406 00:51:51.450008 139800543508288 submission.py:139] 75) loss = 9.181, grad_norm = 0.258
I0406 00:51:51.880767 139742824670976 logging_writer.py:48] [76] global_step=76, grad_norm=0.254596, loss=9.159022
I0406 00:51:51.883762 139800543508288 submission.py:139] 76) loss = 9.159, grad_norm = 0.255
I0406 00:51:52.315304 139742816278272 logging_writer.py:48] [77] global_step=77, grad_norm=0.243267, loss=9.170831
I0406 00:51:52.318353 139800543508288 submission.py:139] 77) loss = 9.171, grad_norm = 0.243
I0406 00:51:52.753963 139742824670976 logging_writer.py:48] [78] global_step=78, grad_norm=0.234344, loss=9.163180
I0406 00:51:52.757026 139800543508288 submission.py:139] 78) loss = 9.163, grad_norm = 0.234
I0406 00:51:53.192035 139742816278272 logging_writer.py:48] [79] global_step=79, grad_norm=0.239457, loss=9.137330
I0406 00:51:53.195040 139800543508288 submission.py:139] 79) loss = 9.137, grad_norm = 0.239
I0406 00:51:53.625358 139742824670976 logging_writer.py:48] [80] global_step=80, grad_norm=0.228076, loss=9.159657
I0406 00:51:53.628458 139800543508288 submission.py:139] 80) loss = 9.160, grad_norm = 0.228
I0406 00:51:54.056934 139742816278272 logging_writer.py:48] [81] global_step=81, grad_norm=0.227417, loss=9.147857
I0406 00:51:54.060158 139800543508288 submission.py:139] 81) loss = 9.148, grad_norm = 0.227
I0406 00:51:54.491322 139742824670976 logging_writer.py:48] [82] global_step=82, grad_norm=0.230350, loss=9.124610
I0406 00:51:54.494194 139800543508288 submission.py:139] 82) loss = 9.125, grad_norm = 0.230
I0406 00:51:54.922444 139742816278272 logging_writer.py:48] [83] global_step=83, grad_norm=0.221055, loss=9.168263
I0406 00:51:54.925647 139800543508288 submission.py:139] 83) loss = 9.168, grad_norm = 0.221
I0406 00:51:55.365019 139742824670976 logging_writer.py:48] [84] global_step=84, grad_norm=0.215370, loss=9.122994
I0406 00:51:55.367951 139800543508288 submission.py:139] 84) loss = 9.123, grad_norm = 0.215
I0406 00:51:55.801096 139742816278272 logging_writer.py:48] [85] global_step=85, grad_norm=0.210119, loss=9.156792
I0406 00:51:55.804089 139800543508288 submission.py:139] 85) loss = 9.157, grad_norm = 0.210
I0406 00:51:56.236551 139742824670976 logging_writer.py:48] [86] global_step=86, grad_norm=0.204641, loss=9.140094
I0406 00:51:56.239754 139800543508288 submission.py:139] 86) loss = 9.140, grad_norm = 0.205
I0406 00:51:56.671456 139742816278272 logging_writer.py:48] [87] global_step=87, grad_norm=0.207455, loss=9.113924
I0406 00:51:56.674353 139800543508288 submission.py:139] 87) loss = 9.114, grad_norm = 0.207
I0406 00:51:57.101287 139742824670976 logging_writer.py:48] [88] global_step=88, grad_norm=0.199055, loss=9.110663
I0406 00:51:57.104452 139800543508288 submission.py:139] 88) loss = 9.111, grad_norm = 0.199
I0406 00:51:57.534986 139742816278272 logging_writer.py:48] [89] global_step=89, grad_norm=0.198602, loss=9.110588
I0406 00:51:57.537917 139800543508288 submission.py:139] 89) loss = 9.111, grad_norm = 0.199
I0406 00:51:57.965453 139742824670976 logging_writer.py:48] [90] global_step=90, grad_norm=0.198247, loss=9.122202
I0406 00:51:57.968450 139800543508288 submission.py:139] 90) loss = 9.122, grad_norm = 0.198
I0406 00:51:58.398581 139742816278272 logging_writer.py:48] [91] global_step=91, grad_norm=0.200040, loss=9.108700
I0406 00:51:58.401750 139800543508288 submission.py:139] 91) loss = 9.109, grad_norm = 0.200
I0406 00:51:58.834244 139742824670976 logging_writer.py:48] [92] global_step=92, grad_norm=0.195343, loss=9.120035
I0406 00:51:58.837142 139800543508288 submission.py:139] 92) loss = 9.120, grad_norm = 0.195
I0406 00:51:59.265066 139742816278272 logging_writer.py:48] [93] global_step=93, grad_norm=0.189316, loss=9.095189
I0406 00:51:59.268344 139800543508288 submission.py:139] 93) loss = 9.095, grad_norm = 0.189
I0406 00:51:59.700905 139742824670976 logging_writer.py:48] [94] global_step=94, grad_norm=0.185124, loss=9.126161
I0406 00:51:59.703880 139800543508288 submission.py:139] 94) loss = 9.126, grad_norm = 0.185
I0406 00:52:00.132262 139742816278272 logging_writer.py:48] [95] global_step=95, grad_norm=0.183768, loss=9.097963
I0406 00:52:00.135178 139800543508288 submission.py:139] 95) loss = 9.098, grad_norm = 0.184
I0406 00:52:00.565138 139742824670976 logging_writer.py:48] [96] global_step=96, grad_norm=0.175354, loss=9.099264
I0406 00:52:00.568416 139800543508288 submission.py:139] 96) loss = 9.099, grad_norm = 0.175
I0406 00:52:01.007263 139742816278272 logging_writer.py:48] [97] global_step=97, grad_norm=0.178760, loss=9.093569
I0406 00:52:01.010809 139800543508288 submission.py:139] 97) loss = 9.094, grad_norm = 0.179
I0406 00:52:01.439153 139742824670976 logging_writer.py:48] [98] global_step=98, grad_norm=0.174429, loss=9.118134
I0406 00:52:01.442393 139800543508288 submission.py:139] 98) loss = 9.118, grad_norm = 0.174
I0406 00:52:01.873845 139742816278272 logging_writer.py:48] [99] global_step=99, grad_norm=0.166757, loss=9.099166
I0406 00:52:01.877013 139800543508288 submission.py:139] 99) loss = 9.099, grad_norm = 0.167
I0406 00:52:02.306573 139742824670976 logging_writer.py:48] [100] global_step=100, grad_norm=0.173338, loss=9.098027
I0406 00:52:02.309598 139800543508288 submission.py:139] 100) loss = 9.098, grad_norm = 0.173
I0406 00:54:51.800984 139742816278272 logging_writer.py:48] [500] global_step=500, grad_norm=0.747296, loss=8.428506
I0406 00:54:51.804587 139800543508288 submission.py:139] 500) loss = 8.429, grad_norm = 0.747
I0406 00:58:24.321502 139742824670976 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.833699, loss=7.805661
I0406 00:58:24.324815 139800543508288 submission.py:139] 1000) loss = 7.806, grad_norm = 0.834
I0406 01:01:56.777254 139742816278272 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.684885, loss=7.419346
I0406 01:01:56.780868 139800543508288 submission.py:139] 1500) loss = 7.419, grad_norm = 0.685
I0406 01:05:19.192943 139800543508288 submission_runner.py:373] Before eval at step 1977: RAM USED (GB) 20.6420992
I0406 01:05:19.193176 139800543508288 spec.py:298] Evaluating on the training split.
I0406 01:05:23.073265 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:09:45.234278 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 01:09:48.942552 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:14:12.573554 139800543508288 spec.py:326] Evaluating on the test split.
I0406 01:14:16.363011 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:18:44.943529 139800543508288 submission_runner.py:382] Time since start: 1668.56s, 	Step: 1977, 	{'train/accuracy': 0.28555041925234287, 'train/loss': 5.8568398066048015, 'train/bleu': 5.507969793164372, 'validation/accuracy': 0.26238980297826436, 'validation/loss': 6.127237263022157, 'validation/bleu': 2.756886544522115, 'validation/num_examples': 3000, 'test/accuracy': 0.24162454244378595, 'test/loss': 6.419359711812213, 'test/bleu': 1.973895131933799, 'test/num_examples': 3003}
I0406 01:18:44.943969 139800543508288 submission_runner.py:396] After eval at step 1977: RAM USED (GB) 20.85384192
I0406 01:18:44.952269 139742824670976 logging_writer.py:48] [1977] global_step=1977, preemption_count=0, score=838.747619, test/accuracy=0.241625, test/bleu=1.973895, test/loss=6.419360, test/num_examples=3003, total_duration=1668.556572, train/accuracy=0.285550, train/bleu=5.507970, train/loss=5.856840, validation/accuracy=0.262390, validation/bleu=2.756887, validation/loss=6.127237, validation/num_examples=3000
I0406 01:18:46.428215 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_1977.
I0406 01:18:46.428823 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 1977: RAM USED (GB) 20.853563392
I0406 01:18:56.596807 139742816278272 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.908782, loss=7.129645
I0406 01:18:56.600046 139800543508288 submission.py:139] 2000) loss = 7.130, grad_norm = 0.909
I0406 01:22:28.928870 139742824670976 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.732449, loss=6.701171
I0406 01:22:28.932469 139800543508288 submission.py:139] 2500) loss = 6.701, grad_norm = 0.732
I0406 01:26:01.335785 139742816278272 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.790894, loss=6.419791
I0406 01:26:01.339125 139800543508288 submission.py:139] 3000) loss = 6.420, grad_norm = 0.791
I0406 01:29:33.780780 139742824670976 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.722214, loss=6.146235
I0406 01:29:33.784082 139800543508288 submission.py:139] 3500) loss = 6.146, grad_norm = 0.722
I0406 01:32:46.666756 139800543508288 submission_runner.py:373] Before eval at step 3955: RAM USED (GB) 20.958846976
I0406 01:32:46.666994 139800543508288 spec.py:298] Evaluating on the training split.
I0406 01:32:50.527636 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:36:09.470840 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 01:36:13.183165 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:39:28.640661 139800543508288 spec.py:326] Evaluating on the test split.
I0406 01:39:32.432264 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:42:40.731544 139800543508288 submission_runner.py:382] Time since start: 3316.03s, 	Step: 3955, 	{'train/accuracy': 0.41813827166152895, 'train/loss': 4.310372093556261, 'train/bleu': 15.045892645722512, 'validation/accuracy': 0.4051282687133452, 'validation/loss': 4.411665695403653, 'validation/bleu': 10.44370688077085, 'validation/num_examples': 3000, 'test/accuracy': 0.3907501016791587, 'test/loss': 4.61432623031782, 'test/bleu': 8.399913139250941, 'test/num_examples': 3003}
I0406 01:42:40.731983 139800543508288 submission_runner.py:396] After eval at step 3955: RAM USED (GB) 21.028913152
I0406 01:42:40.740264 139742816278272 logging_writer.py:48] [3955] global_step=3955, preemption_count=0, score=1673.363508, test/accuracy=0.390750, test/bleu=8.399913, test/loss=4.614326, test/num_examples=3003, total_duration=3316.028211, train/accuracy=0.418138, train/bleu=15.045893, train/loss=4.310372, validation/accuracy=0.405128, validation/bleu=10.443707, validation/loss=4.411666, validation/num_examples=3000
I0406 01:42:42.179867 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_3955.
I0406 01:42:42.180480 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 3955: RAM USED (GB) 21.029511168
I0406 01:43:01.698060 139742824670976 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.606200, loss=5.957251
I0406 01:43:01.701735 139800543508288 submission.py:139] 4000) loss = 5.957, grad_norm = 0.606
I0406 01:46:33.837588 139742816278272 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.514794, loss=5.605182
I0406 01:46:33.840815 139800543508288 submission.py:139] 4500) loss = 5.605, grad_norm = 0.515
I0406 01:50:06.402872 139742824670976 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.522607, loss=5.349775
I0406 01:50:06.406566 139800543508288 submission.py:139] 5000) loss = 5.350, grad_norm = 0.523
I0406 01:53:38.825155 139742816278272 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.586129, loss=5.271820
I0406 01:53:38.828866 139800543508288 submission.py:139] 5500) loss = 5.272, grad_norm = 0.586
I0406 01:56:42.471431 139800543508288 submission_runner.py:373] Before eval at step 5933: RAM USED (GB) 21.069000704
I0406 01:56:42.471662 139800543508288 spec.py:298] Evaluating on the training split.
I0406 01:56:46.342717 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 01:59:13.010660 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 01:59:16.713012 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:01:35.628627 139800543508288 spec.py:326] Evaluating on the test split.
I0406 02:01:39.410045 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:03:51.225132 139800543508288 submission_runner.py:382] Time since start: 4751.84s, 	Step: 5933, 	{'train/accuracy': 0.5105299301819847, 'train/loss': 3.4439449324711, 'train/bleu': 21.51260693119096, 'validation/accuracy': 0.5096279029398272, 'validation/loss': 3.408562820051828, 'validation/bleu': 17.53505739625153, 'validation/num_examples': 3000, 'test/accuracy': 0.5052582650630411, 'test/loss': 3.5123580123177036, 'test/bleu': 16.29493185622555, 'test/num_examples': 3003}
I0406 02:03:51.225553 139800543508288 submission_runner.py:396] After eval at step 5933: RAM USED (GB) 21.116338176
I0406 02:03:51.233714 139742824670976 logging_writer.py:48] [5933] global_step=5933, preemption_count=0, score=2508.113515, test/accuracy=0.505258, test/bleu=16.294932, test/loss=3.512358, test/num_examples=3003, total_duration=4751.835127, train/accuracy=0.510530, train/bleu=21.512607, train/loss=3.443945, validation/accuracy=0.509628, validation/bleu=17.535057, validation/loss=3.408563, validation/num_examples=3000
I0406 02:03:52.679820 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_5933.
I0406 02:03:52.680399 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 5933: RAM USED (GB) 21.1158016
I0406 02:04:21.498926 139742816278272 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.522043, loss=5.214661
I0406 02:04:21.502329 139800543508288 submission.py:139] 6000) loss = 5.215, grad_norm = 0.522
I0406 02:07:53.789035 139742824670976 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.518481, loss=5.123787
I0406 02:07:53.792101 139800543508288 submission.py:139] 6500) loss = 5.124, grad_norm = 0.518
I0406 02:11:26.190505 139742816278272 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.410845, loss=5.017038
I0406 02:11:26.193645 139800543508288 submission.py:139] 7000) loss = 5.017, grad_norm = 0.411
I0406 02:14:58.553144 139742824670976 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.395065, loss=4.909485
I0406 02:14:58.556819 139800543508288 submission.py:139] 7500) loss = 4.909, grad_norm = 0.395
I0406 02:17:52.943645 139800543508288 submission_runner.py:373] Before eval at step 7912: RAM USED (GB) 21.471858688
I0406 02:17:52.943885 139800543508288 spec.py:298] Evaluating on the training split.
I0406 02:17:56.797538 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:20:12.491148 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 02:20:16.209659 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:22:22.476562 139800543508288 spec.py:326] Evaluating on the test split.
I0406 02:22:26.270689 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:24:29.777112 139800543508288 submission_runner.py:382] Time since start: 6022.31s, 	Step: 7912, 	{'train/accuracy': 0.553193939114816, 'train/loss': 2.990178155079906, 'train/bleu': 24.909672020240023, 'validation/accuracy': 0.5517600525721937, 'validation/loss': 2.960407341508475, 'validation/bleu': 20.69726697517848, 'validation/num_examples': 3000, 'test/accuracy': 0.5492069025623147, 'test/loss': 3.0143336746847944, 'test/bleu': 19.167001999270802, 'test/num_examples': 3003}
I0406 02:24:29.777563 139800543508288 submission_runner.py:396] After eval at step 7912: RAM USED (GB) 21.535379456
I0406 02:24:29.785888 139742816278272 logging_writer.py:48] [7912] global_step=7912, preemption_count=0, score=3342.689569, test/accuracy=0.549207, test/bleu=19.167002, test/loss=3.014334, test/num_examples=3003, total_duration=6022.306348, train/accuracy=0.553194, train/bleu=24.909672, train/loss=2.990178, validation/accuracy=0.551760, validation/bleu=20.697267, validation/loss=2.960407, validation/num_examples=3000
I0406 02:24:31.213127 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_7912.
I0406 02:24:31.213779 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 7912: RAM USED (GB) 21.535424512
I0406 02:25:08.849779 139742824670976 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.404197, loss=4.947523
I0406 02:25:08.852592 139800543508288 submission.py:139] 8000) loss = 4.948, grad_norm = 0.404
I0406 02:28:41.073600 139742816278272 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.389975, loss=4.905920
I0406 02:28:41.076665 139800543508288 submission.py:139] 8500) loss = 4.906, grad_norm = 0.390
I0406 02:32:13.334146 139742824670976 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.363492, loss=4.862808
I0406 02:32:13.337406 139800543508288 submission.py:139] 9000) loss = 4.863, grad_norm = 0.363
I0406 02:35:45.701623 139742816278272 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.375068, loss=4.716431
I0406 02:35:45.704991 139800543508288 submission.py:139] 9500) loss = 4.716, grad_norm = 0.375
I0406 02:38:31.618397 139800543508288 submission_runner.py:373] Before eval at step 9892: RAM USED (GB) 21.7163776
I0406 02:38:31.618608 139800543508288 spec.py:298] Evaluating on the training split.
I0406 02:38:35.492377 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:40:45.668189 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 02:40:49.389524 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:42:59.453593 139800543508288 spec.py:326] Evaluating on the test split.
I0406 02:43:03.244072 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:45:10.985517 139800543508288 submission_runner.py:382] Time since start: 7260.98s, 	Step: 9892, 	{'train/accuracy': 0.5738004832585433, 'train/loss': 2.7820992981244967, 'train/bleu': 27.154941019476222, 'validation/accuracy': 0.5762978760337751, 'validation/loss': 2.729567635553186, 'validation/bleu': 22.81581222987529, 'validation/num_examples': 3000, 'test/accuracy': 0.5762826099587474, 'test/loss': 2.750657101562954, 'test/bleu': 21.06336848079256, 'test/num_examples': 3003}
I0406 02:45:10.985978 139800543508288 submission_runner.py:396] After eval at step 9892: RAM USED (GB) 21.762473984
I0406 02:45:10.993885 139742824670976 logging_writer.py:48] [9892] global_step=9892, preemption_count=0, score=4177.410925, test/accuracy=0.576283, test/bleu=21.063368, test/loss=2.750657, test/num_examples=3003, total_duration=7260.981237, train/accuracy=0.573800, train/bleu=27.154941, train/loss=2.782099, validation/accuracy=0.576298, validation/bleu=22.815812, validation/loss=2.729568, validation/num_examples=3000
I0406 02:45:12.406404 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_9892.
I0406 02:45:12.407031 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 9892: RAM USED (GB) 21.761441792
I0406 02:45:58.151743 139800543508288 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 21.77212416
I0406 02:45:58.151933 139800543508288 spec.py:298] Evaluating on the training split.
I0406 02:46:02.001594 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:48:22.543651 139800543508288 spec.py:310] Evaluating on the validation split.
I0406 02:48:26.265841 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:50:33.782618 139800543508288 spec.py:326] Evaluating on the test split.
I0406 02:50:37.572976 139800543508288 workload.py:130] Translating evaluation dataset.
I0406 02:52:44.424972 139800543508288 submission_runner.py:382] Time since start: 7707.51s, 	Step: 10000, 	{'train/accuracy': 0.569660010545863, 'train/loss': 2.8165102220362686, 'train/bleu': 26.54089128720751, 'validation/accuracy': 0.5768310374328899, 'validation/loss': 2.708234786301472, 'validation/bleu': 22.751994190345716, 'validation/num_examples': 3000, 'test/accuracy': 0.5791993492533845, 'test/loss': 2.7260785253616873, 'test/bleu': 21.25032322655987, 'test/num_examples': 3003}
I0406 02:52:44.425396 139800543508288 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.857021952
I0406 02:52:44.433436 139742816278272 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4222.868975, test/accuracy=0.579199, test/bleu=21.250323, test/loss=2.726079, test/num_examples=3003, total_duration=7707.513539, train/accuracy=0.569660, train/bleu=26.540891, train/loss=2.816510, validation/accuracy=0.576831, validation/bleu=22.751994, validation/loss=2.708235, validation/num_examples=3000
I0406 02:52:45.894062 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_10000.
I0406 02:52:45.894779 139800543508288 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.855985664
I0406 02:52:45.902332 139742824670976 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4222.868975
I0406 02:52:48.719926 139800543508288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/wmt_pytorch/trial_1/checkpoint_10000.
I0406 02:52:48.741121 139800543508288 submission_runner.py:550] Tuning trial 1/1
I0406 02:52:48.741304 139800543508288 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 02:52:48.741967 139800543508288 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006483019039603285, 'train/loss': 11.145271519073724, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.141378439201002, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.126150427052467, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.81196928024292, 'total_duration': 3.814263343811035, 'global_step': 1, 'preemption_count': 0}), (1977, {'train/accuracy': 0.28555041925234287, 'train/loss': 5.8568398066048015, 'train/bleu': 5.507969793164372, 'validation/accuracy': 0.26238980297826436, 'validation/loss': 6.127237263022157, 'validation/bleu': 2.756886544522115, 'validation/num_examples': 3000, 'test/accuracy': 0.24162454244378595, 'test/loss': 6.419359711812213, 'test/bleu': 1.973895131933799, 'test/num_examples': 3003, 'score': 838.7476193904877, 'total_duration': 1668.5565717220306, 'global_step': 1977, 'preemption_count': 0}), (3955, {'train/accuracy': 0.41813827166152895, 'train/loss': 4.310372093556261, 'train/bleu': 15.045892645722512, 'validation/accuracy': 0.4051282687133452, 'validation/loss': 4.411665695403653, 'validation/bleu': 10.44370688077085, 'validation/num_examples': 3000, 'test/accuracy': 0.3907501016791587, 'test/loss': 4.61432623031782, 'test/bleu': 8.399913139250941, 'test/num_examples': 3003, 'score': 1673.3635084629059, 'total_duration': 3316.0282106399536, 'global_step': 3955, 'preemption_count': 0}), (5933, {'train/accuracy': 0.5105299301819847, 'train/loss': 3.4439449324711, 'train/bleu': 21.51260693119096, 'validation/accuracy': 0.5096279029398272, 'validation/loss': 3.408562820051828, 'validation/bleu': 17.53505739625153, 'validation/num_examples': 3000, 'test/accuracy': 0.5052582650630411, 'test/loss': 3.5123580123177036, 'test/bleu': 16.29493185622555, 'test/num_examples': 3003, 'score': 2508.1135149002075, 'total_duration': 4751.83512711525, 'global_step': 5933, 'preemption_count': 0}), (7912, {'train/accuracy': 0.553193939114816, 'train/loss': 2.990178155079906, 'train/bleu': 24.909672020240023, 'validation/accuracy': 0.5517600525721937, 'validation/loss': 2.960407341508475, 'validation/bleu': 20.69726697517848, 'validation/num_examples': 3000, 'test/accuracy': 0.5492069025623147, 'test/loss': 3.0143336746847944, 'test/bleu': 19.167001999270802, 'test/num_examples': 3003, 'score': 3342.6895685195923, 'total_duration': 6022.306347846985, 'global_step': 7912, 'preemption_count': 0}), (9892, {'train/accuracy': 0.5738004832585433, 'train/loss': 2.7820992981244967, 'train/bleu': 27.154941019476222, 'validation/accuracy': 0.5762978760337751, 'validation/loss': 2.729567635553186, 'validation/bleu': 22.81581222987529, 'validation/num_examples': 3000, 'test/accuracy': 0.5762826099587474, 'test/loss': 2.750657101562954, 'test/bleu': 21.06336848079256, 'test/num_examples': 3003, 'score': 4177.410924911499, 'total_duration': 7260.981236934662, 'global_step': 9892, 'preemption_count': 0}), (10000, {'train/accuracy': 0.569660010545863, 'train/loss': 2.8165102220362686, 'train/bleu': 26.54089128720751, 'validation/accuracy': 0.5768310374328899, 'validation/loss': 2.708234786301472, 'validation/bleu': 22.751994190345716, 'validation/num_examples': 3000, 'test/accuracy': 0.5791993492533845, 'test/loss': 2.7260785253616873, 'test/bleu': 21.25032322655987, 'test/num_examples': 3003, 'score': 4222.868975400925, 'total_duration': 7707.513539075851, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 02:52:48.742052 139800543508288 submission_runner.py:553] Timing: 4222.868975400925
I0406 02:52:48.742097 139800543508288 submission_runner.py:554] ====================
I0406 02:52:48.742169 139800543508288 submission_runner.py:613] Final wmt score: 4222.868975400925
