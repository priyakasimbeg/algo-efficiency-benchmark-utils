WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 07:33:05.734875 140360249509696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 07:33:05.734910 139715188377408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 07:33:05.734908 140170737874752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 07:33:05.735827 139754964412224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 07:33:05.736155 139754964412224 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.736001 140157487273792 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 07:33:05.736135 140227623827264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 07:33:05.736327 140157487273792 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.736203 140217094047552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 07:33:05.736197 140027256256320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 07:33:05.736437 140227623827264 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.736513 140217094047552 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.736560 140027256256320 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.745563 140360249509696 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.745587 140170737874752 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.745614 139715188377408 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:33:05.752301 140217094047552 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch.
W0406 07:33:05.790470 140217094047552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.790712 140170737874752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.791277 140227623827264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.791608 139754964412224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.791981 140157487273792 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.792574 140027256256320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.793173 140360249509696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:33:05.794650 139715188377408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 07:33:05.796636 140217094047552 submission_runner.py:511] Using RNG seed 271874490
I0406 07:33:05.797961 140217094047552 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 07:33:05.798087 140217094047552 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1.
I0406 07:33:05.798264 140217094047552 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/hparams.json.
I0406 07:33:05.799297 140217094047552 submission_runner.py:230] Starting train once: RAM USED (GB) 5.534224384
I0406 07:33:05.799397 140217094047552 submission_runner.py:231] Initializing dataset.
I0406 07:33:05.799585 140217094047552 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.534224384
I0406 07:33:05.799668 140217094047552 submission_runner.py:240] Initializing model.
I0406 07:33:20.910197 140217094047552 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.117631488
I0406 07:33:20.910416 140217094047552 submission_runner.py:252] Initializing optimizer.
I0406 07:33:20.911144 140217094047552 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.117631488
I0406 07:33:20.911270 140217094047552 submission_runner.py:261] Initializing metrics bundle.
I0406 07:33:20.911325 140217094047552 submission_runner.py:276] Initializing checkpoint and logger.
I0406 07:33:20.916922 140217094047552 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 07:33:20.917075 140217094047552 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 07:33:21.520366 140217094047552 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0406 07:33:21.521469 140217094047552 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/flags_0.json.
I0406 07:33:21.566775 140217094047552 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.168094208
I0406 07:33:21.567845 140217094047552 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.168094208
I0406 07:33:21.567996 140217094047552 submission_runner.py:313] Starting training loop.
I0406 07:35:54.341513 140217094047552 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 54.057684992
I0406 07:35:57.523161 140134481655552 logging_writer.py:48] [0] global_step=0, grad_norm=7.023392, loss=0.787559
I0406 07:35:57.533330 140217094047552 submission.py:296] 0) loss = 0.788, grad_norm = 7.023
I0406 07:35:57.533778 140217094047552 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 58.394853376
I0406 07:35:57.534376 140217094047552 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 58.394853376
I0406 07:35:57.534489 140217094047552 spec.py:298] Evaluating on the training split.
I0406 07:45:26.217071 140217094047552 spec.py:310] Evaluating on the validation split.
I0406 07:50:28.115826 140217094047552 spec.py:326] Evaluating on the test split.
I0406 07:54:18.115560 140217094047552 submission_runner.py:382] Time since start: 155.97s, 	Step: 1, 	{'train/loss': 0.7875576847348694, 'validation/loss': 0.7850843146067416, 'validation/num_examples': 89000000, 'test/loss': 0.7871806188357843, 'test/num_examples': 89274637}
I0406 07:54:18.116037 140217094047552 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.51054336
I0406 07:54:18.128489 140077346817792 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=155.965003, test/loss=0.787181, test/num_examples=89274637, total_duration=155.966891, train/loss=0.787558, validation/loss=0.785084, validation/num_examples=89000000
I0406 07:54:32.289443 140217094047552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_1.
I0406 07:54:32.289881 140217094047552 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.686417408
I0406 07:54:32.313730 140217094047552 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.645391872
I0406 07:54:32.316986 140217094047552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.317003 140227623827264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.316998 140157487273792 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.317000 140170737874752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.316999 140027256256320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.317003 139754964412224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.317009 140360249509696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:32.317052 139715188377408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:54:34.188960 140077338425088 logging_writer.py:48] [1] global_step=1, grad_norm=7.022037, loss=0.787596
I0406 07:54:34.201681 140217094047552 submission.py:296] 1) loss = 0.788, grad_norm = 7.022
I0406 07:54:34.202103 140217094047552 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.837740032
I0406 07:54:36.003118 140077346817792 logging_writer.py:48] [2] global_step=2, grad_norm=6.865115, loss=0.772364
I0406 07:54:36.006474 140217094047552 submission.py:296] 2) loss = 0.772, grad_norm = 6.865
I0406 07:54:37.789436 140077338425088 logging_writer.py:48] [3] global_step=3, grad_norm=6.600308, loss=0.746383
I0406 07:54:37.796679 140217094047552 submission.py:296] 3) loss = 0.746, grad_norm = 6.600
I0406 07:54:39.557675 140077346817792 logging_writer.py:48] [4] global_step=4, grad_norm=6.252455, loss=0.711849
I0406 07:54:39.569608 140217094047552 submission.py:296] 4) loss = 0.712, grad_norm = 6.252
I0406 07:54:41.348155 140077338425088 logging_writer.py:48] [5] global_step=5, grad_norm=5.837410, loss=0.670703
I0406 07:54:41.361574 140217094047552 submission.py:296] 5) loss = 0.671, grad_norm = 5.837
I0406 07:54:43.125660 140077346817792 logging_writer.py:48] [6] global_step=6, grad_norm=5.401959, loss=0.624871
I0406 07:54:43.132979 140217094047552 submission.py:296] 6) loss = 0.625, grad_norm = 5.402
I0406 07:54:44.901507 140077338425088 logging_writer.py:48] [7] global_step=7, grad_norm=4.996397, loss=0.575809
I0406 07:54:44.913793 140217094047552 submission.py:296] 7) loss = 0.576, grad_norm = 4.996
I0406 07:54:46.695813 140077346817792 logging_writer.py:48] [8] global_step=8, grad_norm=4.617854, loss=0.524753
I0406 07:54:46.709510 140217094047552 submission.py:296] 8) loss = 0.525, grad_norm = 4.618
I0406 07:54:48.549068 140077338425088 logging_writer.py:48] [9] global_step=9, grad_norm=4.200856, loss=0.472681
I0406 07:54:48.564478 140217094047552 submission.py:296] 9) loss = 0.473, grad_norm = 4.201
I0406 07:54:50.365937 140077346817792 logging_writer.py:48] [10] global_step=10, grad_norm=3.655890, loss=0.422492
I0406 07:54:50.377578 140217094047552 submission.py:296] 10) loss = 0.422, grad_norm = 3.656
I0406 07:54:52.216813 140077338425088 logging_writer.py:48] [11] global_step=11, grad_norm=3.212894, loss=0.376659
I0406 07:54:52.226857 140217094047552 submission.py:296] 11) loss = 0.377, grad_norm = 3.213
I0406 07:54:54.143754 140077346817792 logging_writer.py:48] [12] global_step=12, grad_norm=2.773913, loss=0.335332
I0406 07:54:54.150463 140217094047552 submission.py:296] 12) loss = 0.335, grad_norm = 2.774
I0406 07:54:56.023725 140077338425088 logging_writer.py:48] [13] global_step=13, grad_norm=2.396566, loss=0.296429
I0406 07:54:56.035943 140217094047552 submission.py:296] 13) loss = 0.296, grad_norm = 2.397
I0406 07:54:57.828572 140077346817792 logging_writer.py:48] [14] global_step=14, grad_norm=2.044106, loss=0.265336
I0406 07:54:57.842277 140217094047552 submission.py:296] 14) loss = 0.265, grad_norm = 2.044
I0406 07:54:59.616126 140077338425088 logging_writer.py:48] [15] global_step=15, grad_norm=1.697220, loss=0.236257
I0406 07:54:59.624755 140217094047552 submission.py:296] 15) loss = 0.236, grad_norm = 1.697
I0406 07:55:01.420470 140077346817792 logging_writer.py:48] [16] global_step=16, grad_norm=1.346695, loss=0.211583
I0406 07:55:01.433225 140217094047552 submission.py:296] 16) loss = 0.212, grad_norm = 1.347
I0406 07:55:03.218719 140077338425088 logging_writer.py:48] [17] global_step=17, grad_norm=1.017817, loss=0.193914
I0406 07:55:03.228306 140217094047552 submission.py:296] 17) loss = 0.194, grad_norm = 1.018
I0406 07:55:05.033524 140077346817792 logging_writer.py:48] [18] global_step=18, grad_norm=0.711632, loss=0.179242
I0406 07:55:05.040495 140217094047552 submission.py:296] 18) loss = 0.179, grad_norm = 0.712
I0406 07:55:06.793551 140077338425088 logging_writer.py:48] [19] global_step=19, grad_norm=0.421664, loss=0.169526
I0406 07:55:06.806536 140217094047552 submission.py:296] 19) loss = 0.170, grad_norm = 0.422
I0406 07:55:08.560670 140077346817792 logging_writer.py:48] [20] global_step=20, grad_norm=0.164380, loss=0.167157
I0406 07:55:08.574844 140217094047552 submission.py:296] 20) loss = 0.167, grad_norm = 0.164
I0406 07:55:10.384362 140077338425088 logging_writer.py:48] [21] global_step=21, grad_norm=0.188243, loss=0.163899
I0406 07:55:10.398366 140217094047552 submission.py:296] 21) loss = 0.164, grad_norm = 0.188
I0406 07:55:12.180688 140077346817792 logging_writer.py:48] [22] global_step=22, grad_norm=0.399233, loss=0.170140
I0406 07:55:12.197857 140217094047552 submission.py:296] 22) loss = 0.170, grad_norm = 0.399
I0406 07:55:13.954005 140077338425088 logging_writer.py:48] [23] global_step=23, grad_norm=0.572033, loss=0.175483
I0406 07:55:13.968146 140217094047552 submission.py:296] 23) loss = 0.175, grad_norm = 0.572
I0406 07:55:15.811588 140077346817792 logging_writer.py:48] [24] global_step=24, grad_norm=0.689845, loss=0.179105
I0406 07:55:15.820670 140217094047552 submission.py:296] 24) loss = 0.179, grad_norm = 0.690
I0406 07:55:17.670825 140077338425088 logging_writer.py:48] [25] global_step=25, grad_norm=0.801992, loss=0.185147
I0406 07:55:17.676254 140217094047552 submission.py:296] 25) loss = 0.185, grad_norm = 0.802
I0406 07:55:19.523212 140077346817792 logging_writer.py:48] [26] global_step=26, grad_norm=0.914283, loss=0.193938
I0406 07:55:19.529785 140217094047552 submission.py:296] 26) loss = 0.194, grad_norm = 0.914
I0406 07:55:21.313144 140077338425088 logging_writer.py:48] [27] global_step=27, grad_norm=0.990753, loss=0.200684
I0406 07:55:21.318462 140217094047552 submission.py:296] 27) loss = 0.201, grad_norm = 0.991
I0406 07:55:23.081268 140077346817792 logging_writer.py:48] [28] global_step=28, grad_norm=1.033879, loss=0.204795
I0406 07:55:23.084636 140217094047552 submission.py:296] 28) loss = 0.205, grad_norm = 1.034
I0406 07:55:24.937477 140077338425088 logging_writer.py:48] [29] global_step=29, grad_norm=1.058837, loss=0.206033
I0406 07:55:24.940520 140217094047552 submission.py:296] 29) loss = 0.206, grad_norm = 1.059
I0406 07:55:26.820887 140077346817792 logging_writer.py:48] [30] global_step=30, grad_norm=1.108187, loss=0.212365
I0406 07:55:26.826361 140217094047552 submission.py:296] 30) loss = 0.212, grad_norm = 1.108
I0406 07:55:28.674479 140077338425088 logging_writer.py:48] [31] global_step=31, grad_norm=1.096281, loss=0.209643
I0406 07:55:28.680549 140217094047552 submission.py:296] 31) loss = 0.210, grad_norm = 1.096
I0406 07:55:30.530117 140077346817792 logging_writer.py:48] [32] global_step=32, grad_norm=1.097958, loss=0.210060
I0406 07:55:30.533329 140217094047552 submission.py:296] 32) loss = 0.210, grad_norm = 1.098
I0406 07:55:32.382442 140077338425088 logging_writer.py:48] [33] global_step=33, grad_norm=1.085578, loss=0.208234
I0406 07:55:32.385406 140217094047552 submission.py:296] 33) loss = 0.208, grad_norm = 1.086
I0406 07:55:34.253497 140077346817792 logging_writer.py:48] [34] global_step=34, grad_norm=1.054793, loss=0.205311
I0406 07:55:34.256653 140217094047552 submission.py:296] 34) loss = 0.205, grad_norm = 1.055
I0406 07:55:36.109508 140077338425088 logging_writer.py:48] [35] global_step=35, grad_norm=1.016110, loss=0.201450
I0406 07:55:36.112850 140217094047552 submission.py:296] 35) loss = 0.201, grad_norm = 1.016
I0406 07:55:37.966354 140077346817792 logging_writer.py:48] [36] global_step=36, grad_norm=0.957198, loss=0.194721
I0406 07:55:37.969510 140217094047552 submission.py:296] 36) loss = 0.195, grad_norm = 0.957
I0406 07:55:39.805551 140077338425088 logging_writer.py:48] [37] global_step=37, grad_norm=0.894438, loss=0.188589
I0406 07:55:39.808712 140217094047552 submission.py:296] 37) loss = 0.189, grad_norm = 0.894
I0406 07:55:41.646979 140077346817792 logging_writer.py:48] [38] global_step=38, grad_norm=0.821339, loss=0.182266
I0406 07:55:41.652332 140217094047552 submission.py:296] 38) loss = 0.182, grad_norm = 0.821
I0406 07:55:43.517842 140077338425088 logging_writer.py:48] [39] global_step=39, grad_norm=0.737588, loss=0.176609
I0406 07:55:43.520939 140217094047552 submission.py:296] 39) loss = 0.177, grad_norm = 0.738
I0406 07:55:45.364913 140077346817792 logging_writer.py:48] [40] global_step=40, grad_norm=0.614473, loss=0.165390
I0406 07:55:45.369885 140217094047552 submission.py:296] 40) loss = 0.165, grad_norm = 0.614
I0406 07:55:47.249255 140077338425088 logging_writer.py:48] [41] global_step=41, grad_norm=0.524955, loss=0.161579
I0406 07:55:47.252318 140217094047552 submission.py:296] 41) loss = 0.162, grad_norm = 0.525
I0406 07:55:49.130552 140077346817792 logging_writer.py:48] [42] global_step=42, grad_norm=0.430542, loss=0.157930
I0406 07:55:49.133752 140217094047552 submission.py:296] 42) loss = 0.158, grad_norm = 0.431
I0406 07:55:51.024408 140077338425088 logging_writer.py:48] [43] global_step=43, grad_norm=0.332442, loss=0.156516
I0406 07:55:51.029541 140217094047552 submission.py:296] 43) loss = 0.157, grad_norm = 0.332
I0406 07:55:52.915463 140077346817792 logging_writer.py:48] [44] global_step=44, grad_norm=0.207234, loss=0.152216
I0406 07:55:52.918456 140217094047552 submission.py:296] 44) loss = 0.152, grad_norm = 0.207
I0406 07:55:54.787238 140077338425088 logging_writer.py:48] [45] global_step=45, grad_norm=0.087389, loss=0.147457
I0406 07:55:54.790648 140217094047552 submission.py:296] 45) loss = 0.147, grad_norm = 0.087
I0406 07:55:56.627744 140077346817792 logging_writer.py:48] [46] global_step=46, grad_norm=0.090254, loss=0.149225
I0406 07:55:56.632573 140217094047552 submission.py:296] 46) loss = 0.149, grad_norm = 0.090
I0406 07:55:58.498094 140077338425088 logging_writer.py:48] [47] global_step=47, grad_norm=0.165702, loss=0.146635
I0406 07:55:58.501455 140217094047552 submission.py:296] 47) loss = 0.147, grad_norm = 0.166
I0406 07:56:00.341380 140077346817792 logging_writer.py:48] [48] global_step=48, grad_norm=0.190887, loss=0.148395
I0406 07:56:00.344412 140217094047552 submission.py:296] 48) loss = 0.148, grad_norm = 0.191
I0406 07:56:02.185643 140077338425088 logging_writer.py:48] [49] global_step=49, grad_norm=0.216480, loss=0.145973
I0406 07:56:02.188983 140217094047552 submission.py:296] 49) loss = 0.146, grad_norm = 0.216
I0406 07:56:04.027642 140077346817792 logging_writer.py:48] [50] global_step=50, grad_norm=0.202664, loss=0.144008
I0406 07:56:04.030646 140217094047552 submission.py:296] 50) loss = 0.144, grad_norm = 0.203
I0406 07:56:05.885263 140077338425088 logging_writer.py:48] [51] global_step=51, grad_norm=0.152195, loss=0.144174
I0406 07:56:05.888337 140217094047552 submission.py:296] 51) loss = 0.144, grad_norm = 0.152
I0406 07:56:07.757044 140077346817792 logging_writer.py:48] [52] global_step=52, grad_norm=0.089643, loss=0.143962
I0406 07:56:07.760324 140217094047552 submission.py:296] 52) loss = 0.144, grad_norm = 0.090
I0406 07:56:09.622073 140077338425088 logging_writer.py:48] [53] global_step=53, grad_norm=0.039436, loss=0.142184
I0406 07:56:09.625719 140217094047552 submission.py:296] 53) loss = 0.142, grad_norm = 0.039
I0406 07:56:11.472337 140077346817792 logging_writer.py:48] [54] global_step=54, grad_norm=0.055460, loss=0.141326
I0406 07:56:11.475577 140217094047552 submission.py:296] 54) loss = 0.141, grad_norm = 0.055
I0406 07:56:13.350752 140077338425088 logging_writer.py:48] [55] global_step=55, grad_norm=0.103582, loss=0.142125
I0406 07:56:13.363346 140217094047552 submission.py:296] 55) loss = 0.142, grad_norm = 0.104
I0406 07:56:15.234312 140077346817792 logging_writer.py:48] [56] global_step=56, grad_norm=0.119536, loss=0.141387
I0406 07:56:15.286415 140217094047552 submission.py:296] 56) loss = 0.141, grad_norm = 0.120
I0406 07:56:17.148451 140077338425088 logging_writer.py:48] [57] global_step=57, grad_norm=0.126745, loss=0.142964
I0406 07:56:17.158800 140217094047552 submission.py:296] 57) loss = 0.143, grad_norm = 0.127
I0406 07:56:19.059685 140077346817792 logging_writer.py:48] [58] global_step=58, grad_norm=0.096066, loss=0.141103
I0406 07:56:19.084844 140217094047552 submission.py:296] 58) loss = 0.141, grad_norm = 0.096
I0406 07:56:20.949853 140077338425088 logging_writer.py:48] [59] global_step=59, grad_norm=0.057573, loss=0.139698
I0406 07:56:20.964644 140217094047552 submission.py:296] 59) loss = 0.140, grad_norm = 0.058
I0406 07:56:22.840671 140077346817792 logging_writer.py:48] [60] global_step=60, grad_norm=0.040964, loss=0.141083
I0406 07:56:22.852171 140217094047552 submission.py:296] 60) loss = 0.141, grad_norm = 0.041
I0406 07:56:24.756908 140077338425088 logging_writer.py:48] [61] global_step=61, grad_norm=0.033149, loss=0.139333
I0406 07:56:24.772459 140217094047552 submission.py:296] 61) loss = 0.139, grad_norm = 0.033
I0406 07:56:26.616547 140077346817792 logging_writer.py:48] [62] global_step=62, grad_norm=0.034560, loss=0.140522
I0406 07:56:26.633973 140217094047552 submission.py:296] 62) loss = 0.141, grad_norm = 0.035
I0406 07:56:28.473554 140077338425088 logging_writer.py:48] [63] global_step=63, grad_norm=0.037216, loss=0.141093
I0406 07:56:28.497609 140217094047552 submission.py:296] 63) loss = 0.141, grad_norm = 0.037
I0406 07:56:30.368282 140077346817792 logging_writer.py:48] [64] global_step=64, grad_norm=0.035761, loss=0.140571
I0406 07:56:30.390432 140217094047552 submission.py:296] 64) loss = 0.141, grad_norm = 0.036
I0406 07:56:32.188041 140077338425088 logging_writer.py:48] [65] global_step=65, grad_norm=0.039289, loss=0.137773
I0406 07:56:32.204190 140217094047552 submission.py:296] 65) loss = 0.138, grad_norm = 0.039
I0406 07:56:34.004319 140077346817792 logging_writer.py:48] [66] global_step=66, grad_norm=0.028860, loss=0.139959
I0406 07:56:34.017740 140217094047552 submission.py:296] 66) loss = 0.140, grad_norm = 0.029
I0406 07:56:35.807167 140077338425088 logging_writer.py:48] [67] global_step=67, grad_norm=0.032870, loss=0.140223
I0406 07:56:35.825432 140217094047552 submission.py:296] 67) loss = 0.140, grad_norm = 0.033
I0406 07:56:37.629341 140077346817792 logging_writer.py:48] [68] global_step=68, grad_norm=0.029412, loss=0.140425
I0406 07:56:37.644868 140217094047552 submission.py:296] 68) loss = 0.140, grad_norm = 0.029
I0406 07:56:39.462216 140077338425088 logging_writer.py:48] [69] global_step=69, grad_norm=0.024851, loss=0.138426
I0406 07:56:39.491777 140217094047552 submission.py:296] 69) loss = 0.138, grad_norm = 0.025
I0406 07:56:41.328850 140077346817792 logging_writer.py:48] [70] global_step=70, grad_norm=0.023294, loss=0.140739
I0406 07:56:41.348599 140217094047552 submission.py:296] 70) loss = 0.141, grad_norm = 0.023
I0406 07:56:43.178917 140077338425088 logging_writer.py:48] [71] global_step=71, grad_norm=0.031439, loss=0.136661
I0406 07:56:43.188826 140217094047552 submission.py:296] 71) loss = 0.137, grad_norm = 0.031
I0406 07:56:45.035298 140077346817792 logging_writer.py:48] [72] global_step=72, grad_norm=0.018632, loss=0.138179
I0406 07:56:45.053744 140217094047552 submission.py:296] 72) loss = 0.138, grad_norm = 0.019
I0406 07:56:46.924743 140077338425088 logging_writer.py:48] [73] global_step=73, grad_norm=0.017828, loss=0.136658
I0406 07:56:46.933789 140217094047552 submission.py:296] 73) loss = 0.137, grad_norm = 0.018
I0406 07:56:48.751561 140077346817792 logging_writer.py:48] [74] global_step=74, grad_norm=0.016546, loss=0.138201
I0406 07:56:48.761173 140217094047552 submission.py:296] 74) loss = 0.138, grad_norm = 0.017
I0406 07:56:50.590434 140077338425088 logging_writer.py:48] [75] global_step=75, grad_norm=0.016080, loss=0.139170
I0406 07:56:50.605993 140217094047552 submission.py:296] 75) loss = 0.139, grad_norm = 0.016
I0406 07:56:52.482780 140077346817792 logging_writer.py:48] [76] global_step=76, grad_norm=0.013653, loss=0.139858
I0406 07:56:52.492488 140217094047552 submission.py:296] 76) loss = 0.140, grad_norm = 0.014
I0406 07:56:54.305335 140077338425088 logging_writer.py:48] [77] global_step=77, grad_norm=0.029317, loss=0.135846
I0406 07:56:54.318354 140217094047552 submission.py:296] 77) loss = 0.136, grad_norm = 0.029
I0406 07:56:56.150722 140077346817792 logging_writer.py:48] [78] global_step=78, grad_norm=0.013822, loss=0.137434
I0406 07:56:56.170092 140217094047552 submission.py:296] 78) loss = 0.137, grad_norm = 0.014
I0406 07:56:58.005988 140077338425088 logging_writer.py:48] [79] global_step=79, grad_norm=0.011325, loss=0.137289
I0406 07:56:58.019302 140217094047552 submission.py:296] 79) loss = 0.137, grad_norm = 0.011
I0406 07:56:59.831802 140077346817792 logging_writer.py:48] [80] global_step=80, grad_norm=0.016577, loss=0.138971
I0406 07:56:59.848479 140217094047552 submission.py:296] 80) loss = 0.139, grad_norm = 0.017
I0406 07:57:01.753144 140077338425088 logging_writer.py:48] [81] global_step=81, grad_norm=0.027268, loss=0.135439
I0406 07:57:01.764826 140217094047552 submission.py:296] 81) loss = 0.135, grad_norm = 0.027
I0406 07:57:03.604765 140077346817792 logging_writer.py:48] [82] global_step=82, grad_norm=0.012189, loss=0.136301
I0406 07:57:03.623241 140217094047552 submission.py:296] 82) loss = 0.136, grad_norm = 0.012
I0406 07:57:05.434978 140077338425088 logging_writer.py:48] [83] global_step=83, grad_norm=0.009763, loss=0.135294
I0406 07:57:05.446340 140217094047552 submission.py:296] 83) loss = 0.135, grad_norm = 0.010
I0406 07:57:07.258729 140077346817792 logging_writer.py:48] [84] global_step=84, grad_norm=0.028354, loss=0.138446
I0406 07:57:07.266642 140217094047552 submission.py:296] 84) loss = 0.138, grad_norm = 0.028
I0406 07:57:09.098082 140077338425088 logging_writer.py:48] [85] global_step=85, grad_norm=0.015004, loss=0.137396
I0406 07:57:09.104908 140217094047552 submission.py:296] 85) loss = 0.137, grad_norm = 0.015
I0406 07:57:10.915721 140077346817792 logging_writer.py:48] [86] global_step=86, grad_norm=0.014108, loss=0.136577
I0406 07:57:10.931451 140217094047552 submission.py:296] 86) loss = 0.137, grad_norm = 0.014
I0406 07:57:12.749567 140077338425088 logging_writer.py:48] [87] global_step=87, grad_norm=0.015125, loss=0.137334
I0406 07:57:12.769592 140217094047552 submission.py:296] 87) loss = 0.137, grad_norm = 0.015
I0406 07:57:14.514004 140077346817792 logging_writer.py:48] [88] global_step=88, grad_norm=0.011158, loss=0.136429
I0406 07:57:14.541055 140217094047552 submission.py:296] 88) loss = 0.136, grad_norm = 0.011
I0406 07:57:16.326674 140077338425088 logging_writer.py:48] [89] global_step=89, grad_norm=0.012386, loss=0.136330
I0406 07:57:16.342129 140217094047552 submission.py:296] 89) loss = 0.136, grad_norm = 0.012
I0406 07:57:18.101717 140077346817792 logging_writer.py:48] [90] global_step=90, grad_norm=0.012881, loss=0.137579
I0406 07:57:18.109797 140217094047552 submission.py:296] 90) loss = 0.138, grad_norm = 0.013
I0406 07:57:19.913660 140077338425088 logging_writer.py:48] [91] global_step=91, grad_norm=0.036154, loss=0.133142
I0406 07:57:19.938710 140217094047552 submission.py:296] 91) loss = 0.133, grad_norm = 0.036
I0406 07:57:21.683851 140077346817792 logging_writer.py:48] [92] global_step=92, grad_norm=0.050520, loss=0.134761
I0406 07:57:21.692738 140217094047552 submission.py:296] 92) loss = 0.135, grad_norm = 0.051
I0406 07:57:23.446028 140077338425088 logging_writer.py:48] [93] global_step=93, grad_norm=0.026589, loss=0.133873
I0406 07:57:23.458722 140217094047552 submission.py:296] 93) loss = 0.134, grad_norm = 0.027
I0406 07:57:25.217207 140077346817792 logging_writer.py:48] [94] global_step=94, grad_norm=0.033844, loss=0.136461
I0406 07:57:25.224511 140217094047552 submission.py:296] 94) loss = 0.136, grad_norm = 0.034
I0406 07:57:26.992225 140077338425088 logging_writer.py:48] [95] global_step=95, grad_norm=0.036832, loss=0.135294
I0406 07:57:27.004674 140217094047552 submission.py:296] 95) loss = 0.135, grad_norm = 0.037
I0406 07:57:28.776993 140077346817792 logging_writer.py:48] [96] global_step=96, grad_norm=0.026109, loss=0.134819
I0406 07:57:28.788269 140217094047552 submission.py:296] 96) loss = 0.135, grad_norm = 0.026
I0406 07:57:30.537542 140077338425088 logging_writer.py:48] [97] global_step=97, grad_norm=0.035895, loss=0.131653
I0406 07:57:30.561367 140217094047552 submission.py:296] 97) loss = 0.132, grad_norm = 0.036
I0406 07:57:32.354075 140077346817792 logging_writer.py:48] [98] global_step=98, grad_norm=0.048420, loss=0.133237
I0406 07:57:32.367707 140217094047552 submission.py:296] 98) loss = 0.133, grad_norm = 0.048
I0406 07:57:34.140851 140077338425088 logging_writer.py:48] [99] global_step=99, grad_norm=0.036589, loss=0.132596
I0406 07:57:34.149464 140217094047552 submission.py:296] 99) loss = 0.133, grad_norm = 0.037
I0406 07:57:35.888997 140077346817792 logging_writer.py:48] [100] global_step=100, grad_norm=0.020982, loss=0.131739
I0406 07:57:35.893947 140217094047552 submission.py:296] 100) loss = 0.132, grad_norm = 0.021
I0406 08:03:32.881368 140217094047552 submission_runner.py:373] Before eval at step 301: RAM USED (GB) 112.866480128
I0406 08:03:32.881573 140217094047552 spec.py:298] Evaluating on the training split.
I0406 08:13:54.296567 140217094047552 spec.py:310] Evaluating on the validation split.
I0406 08:18:49.956424 140217094047552 spec.py:326] Evaluating on the test split.
I0406 08:23:41.412245 140217094047552 submission_runner.py:382] Time since start: 1811.18s, 	Step: 301, 	{'train/loss': 0.12703983848633532, 'validation/loss': 0.12755252808988765, 'validation/num_examples': 89000000, 'test/loss': 0.13008536791922212, 'test/num_examples': 89274637}
I0406 08:23:41.412673 140217094047552 submission_runner.py:396] After eval at step 301: RAM USED (GB) 115.806580736
I0406 08:23:41.421391 140082279335680 logging_writer.py:48] [301] global_step=301, preemption_count=0, score=669.617690, test/loss=0.130085, test/num_examples=89274637, total_duration=1811.180894, train/loss=0.127040, validation/loss=0.127553, validation/num_examples=89000000
I0406 08:23:55.714188 140217094047552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_301.
I0406 08:23:55.714716 140217094047552 submission_runner.py:416] After logging and checkpointing eval at step 301: RAM USED (GB) 115.853959168
I0406 08:29:46.779984 140082270942976 logging_writer.py:48] [500] global_step=500, grad_norm=0.023709, loss=0.124679
I0406 08:29:46.783716 140217094047552 submission.py:296] 500) loss = 0.125, grad_norm = 0.024
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0406 08:32:56.909470 140217094047552 submission_runner.py:373] Before eval at step 606: RAM USED (GB) 118.424907776
I0406 08:32:56.909719 140217094047552 spec.py:298] Evaluating on the training split.
I0406 08:43:37.918395 140217094047552 spec.py:310] Evaluating on the validation split.
I0406 08:48:24.778217 140217094047552 spec.py:326] Evaluating on the test split.
I0406 08:52:28.174220 140217094047552 submission_runner.py:382] Time since start: 3575.21s, 	Step: 606, 	{'train/loss': 0.12504920089306246, 'validation/loss': 0.1265411011235955, 'validation/num_examples': 89000000, 'test/loss': 0.12889943198536893, 'test/num_examples': 89274637}
I0406 08:52:28.174649 140217094047552 submission_runner.py:396] After eval at step 606: RAM USED (GB) 120.73240576
I0406 08:52:28.183126 140082279335680 logging_writer.py:48] [606] global_step=606, preemption_count=0, score=1169.980277, test/loss=0.128899, test/num_examples=89274637, total_duration=3575.209661, train/loss=0.125049, validation/loss=0.126541, validation/num_examples=89000000
I0406 08:52:41.806903 140217094047552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_606.
I0406 08:52:41.807322 140217094047552 submission_runner.py:416] After logging and checkpointing eval at step 606: RAM USED (GB) 120.794714112
I0406 08:58:23.484095 140217094047552 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 121.577631744
I0406 08:58:23.484283 140217094047552 spec.py:298] Evaluating on the training split.
I0406 09:08:53.127092 140217094047552 spec.py:310] Evaluating on the validation split.
I0406 09:13:58.817900 140217094047552 spec.py:326] Evaluating on the test split.
I0406 09:18:50.584827 140217094047552 submission_runner.py:382] Time since start: 5101.79s, 	Step: 800, 	{'train/loss': 0.1253609225713128, 'validation/loss': 0.1260771011235955, 'validation/num_examples': 89000000, 'test/loss': 0.12858768610842966, 'test/num_examples': 89274637}
I0406 09:18:50.585311 140217094047552 submission_runner.py:396] After eval at step 800: RAM USED (GB) 123.541671936
I0406 09:18:50.594122 140112036869888 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1485.638331, test/loss=0.128588, test/num_examples=89274637, total_duration=5101.787500, train/loss=0.125361, validation/loss=0.126077, validation/num_examples=89000000
I0406 09:19:05.942891 140217094047552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:19:05.943434 140217094047552 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 123.61992192
I0406 09:19:05.950942 140112028477184 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1485.638331
I0406 09:19:23.590626 140217094047552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:21:02.495990 140217094047552 submission_runner.py:550] Tuning trial 1/1
I0406 09:21:02.496218 140217094047552 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 09:21:02.498926 140217094047552 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 0.7875576847348694, 'validation/loss': 0.7850843146067416, 'validation/num_examples': 89000000, 'test/loss': 0.7871806188357843, 'test/num_examples': 89274637, 'score': 155.96500325202942, 'total_duration': 155.96689128875732, 'global_step': 1, 'preemption_count': 0}), (301, {'train/loss': 0.12703983848633532, 'validation/loss': 0.12755252808988765, 'validation/num_examples': 89000000, 'test/loss': 0.13008536791922212, 'test/num_examples': 89274637, 'score': 669.6176903247833, 'total_duration': 1811.1808941364288, 'global_step': 301, 'preemption_count': 0}), (606, {'train/loss': 0.12504920089306246, 'validation/loss': 0.1265411011235955, 'validation/num_examples': 89000000, 'test/loss': 0.12889943198536893, 'test/num_examples': 89274637, 'score': 1169.9802770614624, 'total_duration': 3575.2096614837646, 'global_step': 606, 'preemption_count': 0}), (800, {'train/loss': 0.1253609225713128, 'validation/loss': 0.1260771011235955, 'validation/num_examples': 89000000, 'test/loss': 0.12858768610842966, 'test/num_examples': 89274637, 'score': 1485.6383309364319, 'total_duration': 5101.787499904633, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0406 09:21:02.502235 140217094047552 submission_runner.py:553] Timing: 1485.6383309364319
I0406 09:21:02.502302 140217094047552 submission_runner.py:554] ====================
I0406 09:21:02.502389 140217094047552 submission_runner.py:613] Final criteo1tb score: 1485.6383309364319
