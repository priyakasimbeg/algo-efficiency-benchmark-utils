I0405 00:48:31.198062 140259904800576 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax.
I0405 00:48:31.244219 140259904800576 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 00:48:32.129902 140259904800576 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0405 00:48:32.130732 140259904800576 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 00:48:32.134312 140259904800576 submission_runner.py:511] Using RNG seed 2675954754
I0405 00:48:33.432203 140259904800576 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 00:48:33.432415 140259904800576 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1.
I0405 00:48:33.432595 140259904800576 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/hparams.json.
I0405 00:48:33.561819 140259904800576 submission_runner.py:230] Starting train once: RAM USED (GB) 4.323397632
I0405 00:48:33.561954 140259904800576 submission_runner.py:231] Initializing dataset.
I0405 00:48:33.562124 140259904800576 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.32338944
I0405 00:48:33.562195 140259904800576 submission_runner.py:240] Initializing model.
I0405 00:48:48.590212 140259904800576 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.918028288
I0405 00:48:48.590437 140259904800576 submission_runner.py:252] Initializing optimizer.
I0405 00:48:49.148874 140259904800576 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.918024192
I0405 00:48:49.149039 140259904800576 submission_runner.py:261] Initializing metrics bundle.
I0405 00:48:49.149087 140259904800576 submission_runner.py:276] Initializing checkpoint and logger.
I0405 00:48:49.150032 140259904800576 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0405 00:48:49.150293 140259904800576 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 00:48:49.150363 140259904800576 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 00:48:50.201320 140259904800576 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0405 00:48:50.202340 140259904800576 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0405 00:48:50.205593 140259904800576 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.9137152
I0405 00:48:50.205788 140259904800576 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.9137152
I0405 00:48:50.205851 140259904800576 submission_runner.py:313] Starting training loop.
I0405 00:48:50.406761 140259904800576 input_pipeline.py:20] Loading split = train-clean-100
I0405 00:48:50.444459 140259904800576 input_pipeline.py:20] Loading split = train-clean-360
I0405 00:48:50.827558 140259904800576 input_pipeline.py:20] Loading split = train-other-500
I0405 00:48:55.845625 140259904800576 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.844260864
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 00:49:44.605787 140082723936000 logging_writer.py:48] [0] global_step=0, grad_norm=20.346656799316406, loss=33.72841262817383
I0405 00:49:44.616758 140259904800576 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 14.17910272
I0405 00:49:44.617003 140259904800576 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 14.17910272
I0405 00:49:44.617087 140259904800576 spec.py:298] Evaluating on the training split.
I0405 00:49:44.736883 140259904800576 input_pipeline.py:20] Loading split = train-clean-100
I0405 00:49:44.762263 140259904800576 input_pipeline.py:20] Loading split = train-clean-360
I0405 00:49:45.039861 140259904800576 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 00:51:20.721131 140259904800576 spec.py:310] Evaluating on the validation split.
I0405 00:51:20.814097 140259904800576 input_pipeline.py:20] Loading split = dev-clean
I0405 00:51:20.819324 140259904800576 input_pipeline.py:20] Loading split = dev-other
I0405 00:52:15.990404 140259904800576 spec.py:326] Evaluating on the test split.
I0405 00:52:16.085121 140259904800576 input_pipeline.py:20] Loading split = test-clean
I0405 00:52:51.443232 140259904800576 submission_runner.py:382] Time since start: 54.41s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.26988, dtype=float32), 'train/wer': 4.878146929596235, 'validation/ctc_loss': DeviceArray(31.260836, dtype=float32), 'validation/wer': 4.395208829800577, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.169798, dtype=float32), 'test/wer': 4.656023398939736, 'test/num_examples': 2472}
I0405 00:52:51.444422 140259904800576 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.466927104
I0405 00:52:51.459021 140080106690304 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=54.210212, test/ctc_loss=31.169797897338867, test/num_examples=2472, test/wer=4.656023, total_duration=54.411178, train/ctc_loss=32.26987838745117, train/wer=4.878147, validation/ctc_loss=31.260835647583008, validation/num_examples=5348, validation/wer=4.395209
I0405 00:52:51.560515 140259904800576 checkpoints.py:356] Saving checkpoint at step: 1
I0405 00:52:51.852270 140259904800576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0405 00:52:51.853119 140259904800576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0405 00:52:51.858046 140259904800576 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.464363008
I0405 00:52:51.903683 140259904800576 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.462061056
I0405 00:53:08.177612 140259904800576 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.893197824
I0405 00:55:03.558947 140084384962304 logging_writer.py:48] [100] global_step=100, grad_norm=2.966041088104248, loss=6.285971164703369
I0405 00:57:00.153149 140084393355008 logging_writer.py:48] [200] global_step=200, grad_norm=0.7404653429985046, loss=6.470839023590088
I0405 00:58:56.593104 140084384962304 logging_writer.py:48] [300] global_step=300, grad_norm=15.712963104248047, loss=8.043789863586426
I0405 01:00:53.132516 140084393355008 logging_writer.py:48] [400] global_step=400, grad_norm=2.77262806892395, loss=8.666499137878418
I0405 01:02:49.785748 140084384962304 logging_writer.py:48] [500] global_step=500, grad_norm=1.3135026693344116, loss=8.413907051086426
I0405 01:04:46.252366 140084393355008 logging_writer.py:48] [600] global_step=600, grad_norm=2.23014497756958, loss=8.440773010253906
I0405 01:06:42.830430 140084384962304 logging_writer.py:48] [700] global_step=700, grad_norm=2.9881985187530518, loss=8.481890678405762
I0405 01:08:39.476019 140084393355008 logging_writer.py:48] [800] global_step=800, grad_norm=1.383957028388977, loss=8.400667190551758
I0405 01:10:36.190426 140084384962304 logging_writer.py:48] [900] global_step=900, grad_norm=0.7497391700744629, loss=8.311617851257324
I0405 01:12:32.906197 140084393355008 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.74587881565094, loss=6.721783638000488
I0405 01:14:32.533269 140081859913472 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1810.070068359375
I0405 01:16:28.364291 140081851520768 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1796.478759765625
I0405 01:18:24.084193 140081859913472 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1772.8408203125
I0405 01:20:19.647284 140081851520768 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1793.9608154296875
I0405 01:22:15.651148 140081859913472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1809.1746826171875
I0405 01:24:12.034361 140081851520768 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1836.033935546875
I0405 01:26:08.108243 140081859913472 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1800.1422119140625
I0405 01:28:04.580114 140081851520768 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1816.749755859375
I0405 01:30:01.438563 140081859913472 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1840.786865234375
I0405 01:31:57.384130 140081851520768 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1929.8773193359375
I0405 01:32:52.688516 140259904800576 submission_runner.py:373] Before eval at step 2049: RAM USED (GB) 21.578674176
I0405 01:32:52.688732 140259904800576 spec.py:298] Evaluating on the training split.
I0405 01:33:20.503839 140259904800576 spec.py:310] Evaluating on the validation split.
I0405 01:33:55.247672 140259904800576 spec.py:326] Evaluating on the test split.
I0405 01:34:13.260590 140259904800576 submission_runner.py:382] Time since start: 2642.48s, 	Step: 2049, 	{'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 01:34:13.261854 140259904800576 submission_runner.py:396] After eval at step 2049: RAM USED (GB) 21.276356608
I0405 01:34:13.284055 140085279856384 logging_writer.py:48] [2049] global_step=2049, preemption_count=0, score=2451.703553, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=2642.480490, train/ctc_loss=1767.6815185546875, train/wer=0.944636, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 01:34:13.385198 140259904800576 checkpoints.py:356] Saving checkpoint at step: 2049
I0405 01:34:13.792362 140259904800576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2049
I0405 01:34:13.802491 140259904800576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2049.
I0405 01:34:13.806015 140259904800576 submission_runner.py:416] After logging and checkpointing eval at step 2049: RAM USED (GB) 21.250809856
I0405 01:35:17.264594 140085279856384 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1880.26611328125
I0405 01:37:12.626019 140085271463680 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1842.7745361328125
I0405 01:39:07.842395 140085279856384 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1862.753662109375
I0405 01:41:03.063228 140085271463680 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1914.291748046875
I0405 01:42:58.591003 140085279856384 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1791.95166015625
I0405 01:44:55.301485 140085271463680 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1823.998779296875
I0405 01:46:50.496420 140085279856384 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1833.5355224609375
I0405 01:48:46.086525 140085271463680 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1870.51171875
I0405 01:50:42.107539 140085279856384 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1820.1082763671875
I0405 01:52:37.691258 140085271463680 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1780.1142578125
I0405 01:54:36.540900 140085279856384 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1842.3765869140625
I0405 01:56:31.680874 140085271463680 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1821.2735595703125
I0405 01:58:27.104916 140085279856384 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1815.9765625
I0405 02:00:22.423151 140085271463680 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1865.8760986328125
I0405 02:02:17.700194 140085279856384 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1889.2857666015625
I0405 02:04:13.525902 140085271463680 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1835.7706298828125
I0405 02:06:09.378623 140085279856384 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1924.075439453125
I0405 02:08:05.657595 140085271463680 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1816.87890625
I0405 02:10:01.397108 140085279856384 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1894.7393798828125
I0405 02:11:57.069967 140085271463680 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1854.9251708984375
I0405 02:13:52.425206 140085279856384 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1898.5330810546875
I0405 02:14:14.082237 140259904800576 submission_runner.py:373] Before eval at step 4120: RAM USED (GB) 21.150928896
I0405 02:14:14.082470 140259904800576 spec.py:298] Evaluating on the training split.
I0405 02:14:42.168532 140259904800576 spec.py:310] Evaluating on the validation split.
I0405 02:15:15.892435 140259904800576 spec.py:326] Evaluating on the test split.
I0405 02:15:33.865022 140259904800576 submission_runner.py:382] Time since start: 5123.87s, 	Step: 4120, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 02:15:33.866576 140259904800576 submission_runner.py:396] After eval at step 4120: RAM USED (GB) 21.084688384
I0405 02:15:33.889362 140085279856384 logging_writer.py:48] [4120] global_step=4120, preemption_count=0, score=4848.491288, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5123.873582, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 02:15:33.991081 140259904800576 checkpoints.py:356] Saving checkpoint at step: 4120
I0405 02:15:34.379677 140259904800576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4120
I0405 02:15:34.380547 140259904800576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4120.
I0405 02:15:34.384410 140259904800576 submission_runner.py:416] After logging and checkpointing eval at step 4120: RAM USED (GB) 21.058039808
I0405 02:17:11.278222 140084624496384 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1813.4039306640625
I0405 02:19:06.357552 140084616103680 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1795.8486328125
I0405 02:21:01.435281 140084624496384 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1811.7353515625
I0405 02:22:56.570235 140084616103680 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1747.7860107421875
I0405 02:24:52.577289 140084624496384 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1834.718017578125
I0405 02:26:48.711332 140084616103680 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1799.8890380859375
I0405 02:28:44.390653 140084624496384 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1802.5504150390625
I0405 02:30:40.091394 140084616103680 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1822.699951171875
I0405 02:32:35.996452 140084624496384 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1817.7818603515625
I0405 02:34:31.832157 140084616103680 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1855.3284912109375
I0405 02:36:31.383857 140085279856384 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1845.5643310546875
I0405 02:38:26.825440 140085271463680 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1866.6925048828125
I0405 02:40:22.459548 140085279856384 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1816.87890625
I0405 02:42:17.699370 140085271463680 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1891.6590576171875
I0405 02:44:13.175394 140085279856384 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1810.070068359375
I0405 02:46:08.951098 140085271463680 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1818.8150634765625
I0405 02:48:04.205007 140085279856384 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1808.2802734375
I0405 02:49:59.616480 140085271463680 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1810.4541015625
I0405 02:51:55.041829 140085279856384 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1855.462890625
I0405 02:53:50.927868 140085271463680 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1771.6138916015625
I0405 02:55:34.926295 140259904800576 submission_runner.py:373] Before eval at step 6188: RAM USED (GB) 22.764982272
I0405 02:55:34.926816 140259904800576 spec.py:298] Evaluating on the training split.
I0405 02:56:04.433642 140259904800576 spec.py:310] Evaluating on the validation split.
I0405 02:56:39.317353 140259904800576 spec.py:326] Evaluating on the test split.
I0405 02:56:57.974849 140259904800576 submission_runner.py:382] Time since start: 7604.72s, 	Step: 6188, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 02:56:57.976553 140259904800576 submission_runner.py:396] After eval at step 6188: RAM USED (GB) 20.843487232
I0405 02:56:58.000743 140084624496384 logging_writer.py:48] [6188] global_step=6188, preemption_count=0, score=7245.557680, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7604.717454, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 02:56:58.109949 140259904800576 checkpoints.py:356] Saving checkpoint at step: 6188
I0405 02:56:58.600656 140259904800576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6188
I0405 02:56:58.610635 140259904800576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6188.
I0405 02:56:58.614464 140259904800576 submission_runner.py:416] After logging and checkpointing eval at step 6188: RAM USED (GB) 20.817596416
I0405 02:57:13.795287 140084616103680 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1837.351806640625
I0405 02:59:09.408260 140084557354752 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1846.2296142578125
I0405 03:01:04.632832 140084616103680 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1778.01220703125
I0405 03:03:00.528504 140084557354752 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1834.718017578125
I0405 03:04:56.083798 140084616103680 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1835.2442626953125
I0405 03:06:52.044580 140084557354752 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1816.2342529296875
I0405 03:08:47.937489 140084616103680 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1828.297607421875
I0405 03:10:43.842883 140084557354752 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1857.0782470703125
I0405 03:12:39.555673 140084616103680 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1821.4031982421875
I0405 03:14:34.259179 140084557354752 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1760.6480712890625
I0405 03:16:29.586460 140084616103680 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1856.4049072265625
I0405 03:18:28.434980 140084624496384 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1844.766357421875
I0405 03:20:23.928063 140084616103680 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1792.7047119140625
I0405 03:22:19.572141 140084624496384 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1832.4854736328125
I0405 03:24:16.028518 140084616103680 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1846.36279296875
I0405 03:26:11.536582 140084624496384 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1837.483642578125
I0405 03:28:07.148747 140084616103680 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1759.3170166015625
I0405 03:30:02.730215 140084624496384 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1845.43115234375
I0405 03:31:56.800465 140259904800576 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 21.80159488
I0405 03:31:56.800698 140259904800576 spec.py:298] Evaluating on the training split.
I0405 03:32:25.974729 140259904800576 spec.py:310] Evaluating on the validation split.
I0405 03:33:01.618326 140259904800576 spec.py:326] Evaluating on the test split.
I0405 03:33:19.316273 140259904800576 submission_runner.py:382] Time since start: 9786.59s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 03:33:19.317636 140259904800576 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 20.914581504
I0405 03:33:19.336101 140084624496384 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9340.639073, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=9786.590336, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 03:33:19.438097 140259904800576 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 03:33:19.864789 140259904800576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 03:33:19.875116 140259904800576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 03:33:19.879240 140259904800576 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 20.888424448
I0405 03:33:19.885950 140084616103680 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9340.639073
I0405 03:33:19.956535 140259904800576 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 03:33:20.495405 140259904800576 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 03:33:20.505798 140259904800576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 03:33:21.819486 140259904800576 submission_runner.py:550] Tuning trial 1/1
I0405 03:33:21.819755 140259904800576 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 03:33:21.823460 140259904800576 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.26988, dtype=float32), 'train/wer': 4.878146929596235, 'validation/ctc_loss': DeviceArray(31.260836, dtype=float32), 'validation/wer': 4.395208829800577, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.169798, dtype=float32), 'test/wer': 4.656023398939736, 'test/num_examples': 2472, 'score': 54.21021223068237, 'total_duration': 54.41117835044861, 'global_step': 1, 'preemption_count': 0}), (2049, {'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2451.7035529613495, 'total_duration': 2642.4804899692535, 'global_step': 2049, 'preemption_count': 0}), (4120, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4848.4912877082825, 'total_duration': 5123.87358212471, 'global_step': 4120, 'preemption_count': 0}), (6188, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7245.557680130005, 'total_duration': 7604.717453956604, 'global_step': 6188, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9340.63907289505, 'total_duration': 9786.590335607529, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0405 03:33:21.823636 140259904800576 submission_runner.py:553] Timing: 9340.63907289505
I0405 03:33:21.823691 140259904800576 submission_runner.py:554] ====================
I0405 03:33:21.824055 140259904800576 submission_runner.py:613] Final librispeech_deepspeech score: 9340.63907289505
