I0404 22:34:58.022525 140423430260544 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov_v2/wmt_jax.
I0404 22:34:58.070555 140423430260544 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 22:34:58.957799 140423430260544 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0404 22:34:58.958665 140423430260544 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 22:34:58.962505 140423430260544 submission_runner.py:511] Using RNG seed 479788250
I0404 22:35:00.345041 140423430260544 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 22:35:00.345279 140423430260544 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1.
I0404 22:35:00.345500 140423430260544 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/hparams.json.
I0404 22:35:00.466089 140423430260544 submission_runner.py:230] Starting train once: RAM USED (GB) 4.21638144
I0404 22:35:00.466286 140423430260544 submission_runner.py:231] Initializing dataset.
I0404 22:35:00.474741 140423430260544 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:35:00.478935 140423430260544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:35:00.479062 140423430260544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:35:00.559240 140423430260544 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:35:02.437716 140423430260544 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.33022976
I0404 22:35:02.437907 140423430260544 submission_runner.py:240] Initializing model.
I0404 22:35:14.299257 140423430260544 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.402489344
I0404 22:35:14.299461 140423430260544 submission_runner.py:252] Initializing optimizer.
I0404 22:35:14.909455 140423430260544 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.402202624
I0404 22:35:14.909632 140423430260544 submission_runner.py:261] Initializing metrics bundle.
I0404 22:35:14.909681 140423430260544 submission_runner.py:276] Initializing checkpoint and logger.
I0404 22:35:14.910513 140423430260544 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1 with prefix checkpoint_
I0404 22:35:14.910774 140423430260544 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0404 22:35:14.910846 140423430260544 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0404 22:35:15.894779 140423430260544 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/meta_data_0.json.
I0404 22:35:15.895802 140423430260544 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/flags_0.json.
I0404 22:35:15.898738 140423430260544 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.397770752
I0404 22:35:15.898932 140423430260544 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.397770752
I0404 22:35:15.898993 140423430260544 submission_runner.py:313] Starting training loop.
I0404 22:35:16.642686 140423430260544 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.530608128
I0404 22:35:45.830517 140247149045504 logging_writer.py:48] [0] global_step=0, grad_norm=4.91054630279541, loss=11.020231246948242
I0404 22:35:45.840685 140423430260544 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 11.184488448
I0404 22:35:45.840898 140423430260544 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 11.184488448
I0404 22:35:45.840970 140423430260544 spec.py:298] Evaluating on the training split.
I0404 22:35:45.843352 140423430260544 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:35:45.845788 140423430260544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:35:45.845894 140423430260544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:35:45.875652 140423430260544 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0404 22:35:54.061899 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 22:40:57.792983 140423430260544 spec.py:310] Evaluating on the validation split.
I0404 22:40:57.796429 140423430260544 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:40:57.799779 140423430260544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:40:57.799892 140423430260544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:40:57.829328 140423430260544 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:41:05.392164 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 22:46:01.622028 140423430260544 spec.py:326] Evaluating on the test split.
I0404 22:46:01.624607 140423430260544 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:46:01.627321 140423430260544 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 22:46:01.627433 140423430260544 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 22:46:01.660626 140423430260544 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0404 22:46:08.357108 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 22:50:59.139842 140423430260544 submission_runner.py:382] Time since start: 29.94s, 	Step: 1, 	{'train/accuracy': 0.000747470126952976, 'train/loss': 10.976425170898438, 'train/bleu': 2.9145221699308466e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.985247611999512, 'validation/bleu': 3.0644377908982816e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.970314979553223, 'test/bleu': 1.2553809087045416e-10, 'test/num_examples': 3003}
I0404 22:50:59.140491 140423430260544 submission_runner.py:396] After eval at step 1: RAM USED (GB) 11.609174016
I0404 22:50:59.148137 140236050429696 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=29.736066, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.970315, test/num_examples=3003, total_duration=29.941957, train/accuracy=0.000747, train/bleu=0.000000, train/loss=10.976425, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.985248, validation/num_examples=3000
I0404 22:50:59.837277 140423430260544 checkpoints.py:356] Saving checkpoint at step: 1
I0404 22:51:02.390699 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_1
I0404 22:51:02.393948 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_1.
I0404 22:51:02.399031 140423430260544 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 12.308729856
I0404 22:51:02.401634 140423430260544 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 12.308729856
I0404 22:51:02.469724 140423430260544 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 12.30753792
I0404 22:51:38.090490 140236058822400 logging_writer.py:48] [100] global_step=100, grad_norm=0.18039128184318542, loss=9.071796417236328
I0404 22:52:13.776893 140236176320256 logging_writer.py:48] [200] global_step=200, grad_norm=0.12918242812156677, loss=8.891002655029297
I0404 22:52:49.556475 140236058822400 logging_writer.py:48] [300] global_step=300, grad_norm=0.14403392374515533, loss=8.79508113861084
I0404 22:53:25.347147 140236176320256 logging_writer.py:48] [400] global_step=400, grad_norm=0.23956455290317535, loss=8.663285255432129
I0404 22:54:01.175498 140236058822400 logging_writer.py:48] [500] global_step=500, grad_norm=0.638849139213562, loss=8.397997856140137
I0404 22:54:37.006730 140236176320256 logging_writer.py:48] [600] global_step=600, grad_norm=0.7672542333602905, loss=8.273069381713867
I0404 22:55:12.887115 140236058822400 logging_writer.py:48] [700] global_step=700, grad_norm=0.6912783980369568, loss=8.140554428100586
I0404 22:55:48.743633 140236176320256 logging_writer.py:48] [800] global_step=800, grad_norm=0.7423132658004761, loss=8.0123872756958
I0404 22:56:24.584668 140236058822400 logging_writer.py:48] [900] global_step=900, grad_norm=0.8042401075363159, loss=7.912578105926514
I0404 22:57:00.455958 140236176320256 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7893055081367493, loss=7.856963634490967
I0404 22:57:36.308953 140236058822400 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7640153169631958, loss=7.7761054039001465
I0404 22:58:12.175626 140236176320256 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7073231935501099, loss=7.656242847442627
I0404 22:58:48.058120 140236058822400 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8916559219360352, loss=7.645881175994873
I0404 22:59:23.940552 140236176320256 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6483673453330994, loss=7.509591102600098
I0404 22:59:59.819370 140236058822400 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8876636028289795, loss=7.431893348693848
I0404 23:00:35.669153 140236176320256 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6505979895591736, loss=7.328506946563721
I0404 23:01:11.548955 140236058822400 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7868503332138062, loss=7.250425815582275
I0404 23:01:47.431889 140236176320256 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.782494306564331, loss=7.216645240783691
I0404 23:02:23.346507 140236058822400 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5754919648170471, loss=7.18709135055542
I0404 23:02:59.252757 140236176320256 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6908643245697021, loss=7.1065239906311035
I0404 23:03:35.125205 140236058822400 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7078490257263184, loss=6.894881725311279
I0404 23:04:11.012746 140236176320256 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6908915638923645, loss=6.887093544006348
I0404 23:04:46.905197 140236058822400 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7373508214950562, loss=6.848630428314209
I0404 23:05:02.401289 140423430260544 submission_runner.py:373] Before eval at step 2345: RAM USED (GB) 12.002955264
I0404 23:05:02.401496 140423430260544 spec.py:298] Evaluating on the training split.
I0404 23:05:05.428992 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:09:55.551654 140423430260544 spec.py:310] Evaluating on the validation split.
I0404 23:09:58.252072 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:14:43.282989 140423430260544 spec.py:326] Evaluating on the test split.
I0404 23:14:46.049293 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:19:36.920640 140423430260544 submission_runner.py:382] Time since start: 1786.50s, 	Step: 2345, 	{'train/accuracy': 0.30683284997940063, 'train/loss': 5.52842378616333, 'train/bleu': 6.747917320100717, 'validation/accuracy': 0.2848817706108093, 'validation/loss': 5.808018207550049, 'validation/bleu': 3.5875445158267176, 'validation/num_examples': 3000, 'test/accuracy': 0.263924241065979, 'test/loss': 6.105105876922607, 'test/bleu': 2.552302556716208, 'test/num_examples': 3003}
I0404 23:19:36.921145 140423430260544 submission_runner.py:396] After eval at step 2345: RAM USED (GB) 12.203831296
I0404 23:19:36.928865 140236058822400 logging_writer.py:48] [2345] global_step=2345, preemption_count=0, score=866.372925, test/accuracy=0.263924, test/bleu=2.552303, test/loss=6.105106, test/num_examples=3003, total_duration=1786.501707, train/accuracy=0.306833, train/bleu=6.747917, train/loss=5.528424, validation/accuracy=0.284882, validation/bleu=3.587545, validation/loss=5.808018, validation/num_examples=3000
I0404 23:19:37.633940 140423430260544 checkpoints.py:356] Saving checkpoint at step: 2345
I0404 23:19:40.178914 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_2345
I0404 23:19:40.182056 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_2345.
I0404 23:19:40.186816 140423430260544 submission_runner.py:416] After logging and checkpointing eval at step 2345: RAM USED (GB) 12.92683264
I0404 23:20:00.241961 140236184712960 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6431416869163513, loss=6.738643646240234
I0404 23:20:36.029774 140236142749440 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7834625244140625, loss=6.707109451293945
I0404 23:21:11.831400 140236184712960 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6130198836326599, loss=6.613555908203125
I0404 23:21:47.697253 140236142749440 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8188820481300354, loss=6.675594806671143
I0404 23:22:23.548934 140236184712960 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7465382218360901, loss=6.538058757781982
I0404 23:22:59.403900 140236142749440 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8060814142227173, loss=6.4854254722595215
I0404 23:23:35.293232 140236184712960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6168349385261536, loss=6.449025630950928
I0404 23:24:11.191854 140236142749440 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8925309181213379, loss=6.367263317108154
I0404 23:24:47.074220 140236184712960 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.620380163192749, loss=6.274006366729736
I0404 23:25:22.961922 140236142749440 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.739467442035675, loss=6.286388397216797
I0404 23:25:58.848149 140236184712960 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6642290353775024, loss=6.14677619934082
I0404 23:26:34.724617 140236142749440 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9582977294921875, loss=6.146780490875244
I0404 23:27:10.628875 140236184712960 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5893344283103943, loss=6.073540687561035
I0404 23:27:46.470834 140236142749440 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5834425687789917, loss=6.004742622375488
I0404 23:28:22.343920 140236184712960 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0167261362075806, loss=5.990183353424072
I0404 23:28:58.216455 140236142749440 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7206035256385803, loss=5.901820182800293
I0404 23:29:34.140234 140236184712960 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6886096596717834, loss=5.833643436431885
I0404 23:30:10.049843 140236142749440 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.644949734210968, loss=5.745619297027588
I0404 23:30:45.963069 140236184712960 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6094748377799988, loss=5.7668023109436035
I0404 23:31:21.833042 140236142749440 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6047155261039734, loss=5.691917896270752
I0404 23:31:57.746148 140236184712960 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5581486225128174, loss=5.717555999755859
I0404 23:32:33.604047 140236142749440 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5316476225852966, loss=5.674999713897705
I0404 23:33:09.507133 140236184712960 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5477597713470459, loss=5.595892429351807
I0404 23:33:40.428483 140423430260544 submission_runner.py:373] Before eval at step 4688: RAM USED (GB) 12.32787456
I0404 23:33:40.428689 140423430260544 spec.py:298] Evaluating on the training split.
I0404 23:33:43.473873 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:36:33.971455 140423430260544 spec.py:310] Evaluating on the validation split.
I0404 23:36:36.653129 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:39:26.220664 140423430260544 spec.py:326] Evaluating on the test split.
I0404 23:39:28.953201 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:42:21.487630 140423430260544 submission_runner.py:382] Time since start: 3504.53s, 	Step: 4688, 	{'train/accuracy': 0.46537235379219055, 'train/loss': 3.8526670932769775, 'train/bleu': 17.00474173912425, 'validation/accuracy': 0.45587778091430664, 'validation/loss': 3.9329237937927246, 'validation/bleu': 13.530390517926056, 'validation/num_examples': 3000, 'test/accuracy': 0.4491313695907593, 'test/loss': 4.071622371673584, 'test/bleu': 11.911102323545911, 'test/num_examples': 3003}
I0404 23:42:21.488091 140423430260544 submission_runner.py:396] After eval at step 4688: RAM USED (GB) 12.487647232
I0404 23:42:21.495625 140236142749440 logging_writer.py:48] [4688] global_step=4688, preemption_count=0, score=1703.530411, test/accuracy=0.449131, test/bleu=11.911102, test/loss=4.071622, test/num_examples=3003, total_duration=3504.528960, train/accuracy=0.465372, train/bleu=17.004742, train/loss=3.852667, validation/accuracy=0.455878, validation/bleu=13.530391, validation/loss=3.932924, validation/num_examples=3000
I0404 23:42:22.185775 140423430260544 checkpoints.py:356] Saving checkpoint at step: 4688
I0404 23:42:24.746921 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_4688
I0404 23:42:24.750200 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_4688.
I0404 23:42:24.755050 140423430260544 submission_runner.py:416] After logging and checkpointing eval at step 4688: RAM USED (GB) 13.211164672
I0404 23:42:29.417632 140236184712960 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7188071012496948, loss=5.471683979034424
I0404 23:43:05.159996 140235855419136 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6030564904212952, loss=5.46967887878418
I0404 23:43:40.935972 140236184712960 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.686420202255249, loss=5.459183692932129
I0404 23:44:16.784635 140235855419136 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5676200985908508, loss=5.405356407165527
I0404 23:44:52.646271 140236184712960 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5095673203468323, loss=5.46403169631958
I0404 23:45:28.522930 140235855419136 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5751131772994995, loss=5.495640754699707
I0404 23:46:04.449881 140236184712960 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6006916165351868, loss=5.368955135345459
I0404 23:46:40.303020 140235855419136 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5663279891014099, loss=5.366832256317139
I0404 23:47:16.162842 140236184712960 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.47276681661605835, loss=5.317409515380859
I0404 23:47:52.033327 140235855419136 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5898358821868896, loss=5.208446502685547
I0404 23:48:27.934247 140236184712960 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4739588797092438, loss=5.199254512786865
I0404 23:49:03.820974 140235855419136 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.4715006947517395, loss=5.206710338592529
I0404 23:49:39.723721 140236184712960 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5230706930160522, loss=5.21681547164917
I0404 23:50:15.599315 140235855419136 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5164291262626648, loss=5.18110466003418
I0404 23:50:51.480083 140236184712960 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5049334764480591, loss=5.2746381759643555
I0404 23:51:27.357055 140235855419136 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5458425879478455, loss=5.227595329284668
I0404 23:52:03.243257 140236184712960 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5325300693511963, loss=5.0830230712890625
I0404 23:52:39.104063 140235855419136 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5742481350898743, loss=5.172611713409424
I0404 23:53:14.972911 140236184712960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5644887089729309, loss=5.128492832183838
I0404 23:53:50.866574 140235855419136 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.440764844417572, loss=5.05642032623291
I0404 23:54:26.762975 140236184712960 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.46136710047721863, loss=5.007957935333252
I0404 23:55:02.653102 140235855419136 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4675903916358948, loss=5.069756031036377
I0404 23:55:38.523778 140236184712960 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.44062376022338867, loss=5.01492166519165
I0404 23:56:14.399040 140235855419136 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.42852991819381714, loss=4.982737064361572
I0404 23:56:24.878686 140423430260544 submission_runner.py:373] Before eval at step 7031: RAM USED (GB) 12.785291264
I0404 23:56:24.878882 140423430260544 spec.py:298] Evaluating on the training split.
I0404 23:56:27.927392 140423430260544 workload.py:179] Translating evaluation dataset.
I0404 23:59:16.646460 140423430260544 spec.py:310] Evaluating on the validation split.
I0404 23:59:19.328778 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:01:52.846166 140423430260544 spec.py:326] Evaluating on the test split.
I0405 00:01:55.588711 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:04:20.686067 140423430260544 submission_runner.py:382] Time since start: 4868.98s, 	Step: 7031, 	{'train/accuracy': 0.5378724932670593, 'train/loss': 3.101649045944214, 'train/bleu': 24.00020441434387, 'validation/accuracy': 0.534537672996521, 'validation/loss': 3.111851692199707, 'validation/bleu': 19.692882327966732, 'validation/num_examples': 3000, 'test/accuracy': 0.5316019058227539, 'test/loss': 3.17801570892334, 'test/bleu': 18.049036774323298, 'test/num_examples': 3003}
I0405 00:04:20.686515 140423430260544 submission_runner.py:396] After eval at step 7031: RAM USED (GB) 12.932173824
I0405 00:04:20.693987 140236184712960 logging_writer.py:48] [7031] global_step=7031, preemption_count=0, score=2540.622995, test/accuracy=0.531602, test/bleu=18.049037, test/loss=3.178016, test/num_examples=3003, total_duration=4868.979195, train/accuracy=0.537872, train/bleu=24.000204, train/loss=3.101649, validation/accuracy=0.534538, validation/bleu=19.692882, validation/loss=3.111852, validation/num_examples=3000
I0405 00:04:21.386302 140423430260544 checkpoints.py:356] Saving checkpoint at step: 7031
I0405 00:04:23.958619 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_7031
I0405 00:04:23.961773 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_7031.
I0405 00:04:23.966605 140423430260544 submission_runner.py:416] After logging and checkpointing eval at step 7031: RAM USED (GB) 13.655707648
I0405 00:04:49.032918 140235855419136 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.47136884927749634, loss=5.036971569061279
I0405 00:05:24.847294 140235847026432 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4267841577529907, loss=4.9822306632995605
I0405 00:06:00.691052 140235855419136 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.427291601896286, loss=4.990542888641357
I0405 00:06:36.514410 140235847026432 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.3869231641292572, loss=4.919875144958496
I0405 00:07:12.384042 140235855419136 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4169410467147827, loss=4.867347717285156
I0405 00:07:48.239197 140235847026432 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4669646918773651, loss=4.991709232330322
I0405 00:08:24.118782 140235855419136 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.42645466327667236, loss=4.894383907318115
I0405 00:08:59.988382 140235847026432 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4008854329586029, loss=4.874613285064697
I0405 00:09:35.843048 140235855419136 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.420278936624527, loss=4.923913478851318
I0405 00:10:11.701895 140235847026432 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.40376198291778564, loss=4.931466579437256
I0405 00:10:47.587248 140235855419136 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.38526442646980286, loss=4.895544528961182
I0405 00:11:23.394539 140235847026432 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3982073962688446, loss=4.8523406982421875
I0405 00:11:59.285584 140235855419136 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.460361510515213, loss=4.882544994354248
I0405 00:12:35.180160 140235847026432 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.40647637844085693, loss=4.834494113922119
I0405 00:13:11.017066 140235855419136 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3968353569507599, loss=4.857726573944092
I0405 00:13:46.882703 140235847026432 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.39650213718414307, loss=4.873010635375977
I0405 00:14:22.722157 140235855419136 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4012286067008972, loss=4.778855800628662
I0405 00:14:58.578207 140235847026432 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.43041813373565674, loss=4.776639938354492
I0405 00:15:34.422335 140235855419136 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3862641155719757, loss=4.779454231262207
I0405 00:16:10.311816 140235847026432 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3958508372306824, loss=4.903080940246582
I0405 00:16:46.179713 140235855419136 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.41008949279785156, loss=4.815820693969727
I0405 00:17:22.057244 140235847026432 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3901183307170868, loss=4.852453231811523
I0405 00:17:57.925769 140235855419136 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.37581995129585266, loss=4.7886199951171875
I0405 00:18:24.198442 140423430260544 submission_runner.py:373] Before eval at step 9375: RAM USED (GB) 13.224112128
I0405 00:18:24.198634 140423430260544 spec.py:298] Evaluating on the training split.
I0405 00:18:27.232967 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:21:11.940892 140423430260544 spec.py:310] Evaluating on the validation split.
I0405 00:21:14.625560 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:23:38.274609 140423430260544 spec.py:326] Evaluating on the test split.
I0405 00:23:41.005974 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:25:59.404939 140423430260544 submission_runner.py:382] Time since start: 6188.30s, 	Step: 9375, 	{'train/accuracy': 0.5667976140975952, 'train/loss': 2.8519818782806396, 'train/bleu': 25.889684911538268, 'validation/accuracy': 0.5681888461112976, 'validation/loss': 2.7966043949127197, 'validation/bleu': 21.75657116973749, 'validation/num_examples': 3000, 'test/accuracy': 0.5683574676513672, 'test/loss': 2.824789047241211, 'test/bleu': 20.475314734068153, 'test/num_examples': 3003}
I0405 00:25:59.405398 140423430260544 submission_runner.py:396] After eval at step 9375: RAM USED (GB) 13.272068096
I0405 00:25:59.412680 140235847026432 logging_writer.py:48] [9375] global_step=9375, preemption_count=0, score=3377.586708, test/accuracy=0.568357, test/bleu=20.475315, test/loss=2.824789, test/num_examples=3003, total_duration=6188.298987, train/accuracy=0.566798, train/bleu=25.889685, train/loss=2.851982, validation/accuracy=0.568189, validation/bleu=21.756571, validation/loss=2.796604, validation/num_examples=3000
I0405 00:26:00.126770 140423430260544 checkpoints.py:356] Saving checkpoint at step: 9375
I0405 00:26:02.677928 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_9375
I0405 00:26:02.681060 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_9375.
I0405 00:26:02.686268 140423430260544 submission_runner.py:416] After logging and checkpointing eval at step 9375: RAM USED (GB) 13.996015616
I0405 00:26:12.007207 140235855419136 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4062400758266449, loss=4.819465160369873
I0405 00:26:47.769019 140235838633728 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.38245874643325806, loss=4.734139442443848
I0405 00:27:23.545285 140235855419136 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3724144697189331, loss=4.74151086807251
I0405 00:27:59.330622 140235838633728 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.38336610794067383, loss=4.717161178588867
I0405 00:28:35.191796 140235855419136 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3615562617778778, loss=4.750502586364746
I0405 00:29:11.011138 140235838633728 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3696081340312958, loss=4.744143962860107
I0405 00:29:46.210890 140423430260544 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 13.278396416
I0405 00:29:46.211077 140423430260544 spec.py:298] Evaluating on the training split.
I0405 00:29:49.242393 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:32:31.354532 140423430260544 spec.py:310] Evaluating on the validation split.
I0405 00:32:34.039055 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:35:05.582824 140423430260544 spec.py:326] Evaluating on the test split.
I0405 00:35:08.319398 140423430260544 workload.py:179] Translating evaluation dataset.
I0405 00:37:36.221501 140423430260544 submission_runner.py:382] Time since start: 6870.31s, 	Step: 10000, 	{'train/accuracy': 0.5725184679031372, 'train/loss': 2.793478012084961, 'train/bleu': 26.81671146498904, 'validation/accuracy': 0.5753927230834961, 'validation/loss': 2.7198538780212402, 'validation/bleu': 22.68557825547986, 'validation/num_examples': 3000, 'test/accuracy': 0.575631856918335, 'test/loss': 2.735879898071289, 'test/bleu': 21.34708048467996, 'test/num_examples': 3003}
I0405 00:37:36.222038 140423430260544 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 13.322461184
I0405 00:37:36.230152 140235855419136 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3600.278145, test/accuracy=0.575632, test/bleu=21.347080, test/loss=2.735880, test/num_examples=3003, total_duration=6870.311441, train/accuracy=0.572518, train/bleu=26.816711, train/loss=2.793478, validation/accuracy=0.575393, validation/bleu=22.685578, validation/loss=2.719854, validation/num_examples=3000
I0405 00:37:36.947850 140423430260544 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 00:37:39.441860 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_10000
I0405 00:37:39.445191 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_10000.
I0405 00:37:39.450083 140423430260544 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 14.045364224
I0405 00:37:39.456608 140235838633728 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=3600.278145
I0405 00:37:39.838419 140423430260544 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 00:37:43.591721 140423430260544 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_10000
I0405 00:37:43.594948 140423430260544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov_v2/wmt_jax/trial_1/checkpoint_10000.
I0405 00:37:43.664098 140423430260544 submission_runner.py:550] Tuning trial 1/1
I0405 00:37:43.664326 140423430260544 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 00:37:43.665800 140423430260544 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000747470126952976, 'train/loss': 10.976425170898438, 'train/bleu': 2.9145221699308466e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.985247611999512, 'validation/bleu': 3.0644377908982816e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.970314979553223, 'test/bleu': 1.2553809087045416e-10, 'test/num_examples': 3003, 'score': 29.73606562614441, 'total_duration': 29.941956520080566, 'global_step': 1, 'preemption_count': 0}), (2345, {'train/accuracy': 0.30683284997940063, 'train/loss': 5.52842378616333, 'train/bleu': 6.747917320100717, 'validation/accuracy': 0.2848817706108093, 'validation/loss': 5.808018207550049, 'validation/bleu': 3.5875445158267176, 'validation/num_examples': 3000, 'test/accuracy': 0.263924241065979, 'test/loss': 6.105105876922607, 'test/bleu': 2.552302556716208, 'test/num_examples': 3003, 'score': 866.3729248046875, 'total_duration': 1786.5017066001892, 'global_step': 2345, 'preemption_count': 0}), (4688, {'train/accuracy': 0.46537235379219055, 'train/loss': 3.8526670932769775, 'train/bleu': 17.00474173912425, 'validation/accuracy': 0.45587778091430664, 'validation/loss': 3.9329237937927246, 'validation/bleu': 13.530390517926056, 'validation/num_examples': 3000, 'test/accuracy': 0.4491313695907593, 'test/loss': 4.071622371673584, 'test/bleu': 11.911102323545911, 'test/num_examples': 3003, 'score': 1703.5304112434387, 'total_duration': 3504.528959751129, 'global_step': 4688, 'preemption_count': 0}), (7031, {'train/accuracy': 0.5378724932670593, 'train/loss': 3.101649045944214, 'train/bleu': 24.00020441434387, 'validation/accuracy': 0.534537672996521, 'validation/loss': 3.111851692199707, 'validation/bleu': 19.692882327966732, 'validation/num_examples': 3000, 'test/accuracy': 0.5316019058227539, 'test/loss': 3.17801570892334, 'test/bleu': 18.049036774323298, 'test/num_examples': 3003, 'score': 2540.6229951381683, 'total_duration': 4868.979194879532, 'global_step': 7031, 'preemption_count': 0}), (9375, {'train/accuracy': 0.5667976140975952, 'train/loss': 2.8519818782806396, 'train/bleu': 25.889684911538268, 'validation/accuracy': 0.5681888461112976, 'validation/loss': 2.7966043949127197, 'validation/bleu': 21.75657116973749, 'validation/num_examples': 3000, 'test/accuracy': 0.5683574676513672, 'test/loss': 2.824789047241211, 'test/bleu': 20.475314734068153, 'test/num_examples': 3003, 'score': 3377.5867080688477, 'total_duration': 6188.298986911774, 'global_step': 9375, 'preemption_count': 0}), (10000, {'train/accuracy': 0.5725184679031372, 'train/loss': 2.793478012084961, 'train/bleu': 26.81671146498904, 'validation/accuracy': 0.5753927230834961, 'validation/loss': 2.7198538780212402, 'validation/bleu': 22.68557825547986, 'validation/num_examples': 3000, 'test/accuracy': 0.575631856918335, 'test/loss': 2.735879898071289, 'test/bleu': 21.34708048467996, 'test/num_examples': 3003, 'score': 3600.278144598007, 'total_duration': 6870.311441421509, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 00:37:43.665916 140423430260544 submission_runner.py:553] Timing: 3600.278144598007
I0405 00:37:43.665962 140423430260544 submission_runner.py:554] ====================
I0405 00:37:43.666059 140423430260544 submission_runner.py:613] Final wmt score: 3600.278144598007
