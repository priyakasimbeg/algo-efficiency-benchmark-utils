WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 19:32:05.924043 140636523849536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 19:32:05.924076 140581496067904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 19:32:05.924093 140415653533504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 19:32:05.924108 140015236769600 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 19:32:05.925089 139643526821696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 19:32:05.925233 140464732006208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 19:32:05.925463 139643526821696 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.925469 140310243850048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 19:32:05.925504 139972874770240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 19:32:05.925677 140464732006208 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.925828 140310243850048 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.925875 139972874770240 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.934714 140636523849536 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.934745 140581496067904 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.934768 140415653533504 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:05.934791 140015236769600 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 19:32:08.052644 139643526821696 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch.
W0405 19:32:08.163220 139643526821696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.164055 140415653533504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.164379 140581496067904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.164480 140015236769600 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.164914 140310243850048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.165084 140636523849536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.166983 140464732006208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 19:32:08.167526 139972874770240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 19:32:08.169478 139643526821696 submission_runner.py:511] Using RNG seed 3033482548
I0405 19:32:08.170502 139643526821696 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 19:32:08.170652 139643526821696 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1.
I0405 19:32:08.170834 139643526821696 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/hparams.json.
I0405 19:32:08.171844 139643526821696 submission_runner.py:230] Starting train once: RAM USED (GB) 5.719805952
I0405 19:32:08.171936 139643526821696 submission_runner.py:231] Initializing dataset.
I0405 19:32:12.466253 139643526821696 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.75753728
I0405 19:32:12.466420 139643526821696 submission_runner.py:240] Initializing model.
I0405 19:32:17.111521 139643526821696 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.560006656
I0405 19:32:17.111717 139643526821696 submission_runner.py:252] Initializing optimizer.
I0405 19:32:17.361294 139643526821696 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.562877952
I0405 19:32:17.361499 139643526821696 submission_runner.py:261] Initializing metrics bundle.
I0405 19:32:17.361546 139643526821696 submission_runner.py:276] Initializing checkpoint and logger.
I0405 19:32:18.029458 139643526821696 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0405 19:32:18.031066 139643526821696 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0405 19:32:18.069892 139643526821696 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 17.611411456
I0405 19:32:18.071031 139643526821696 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 17.611411456
I0405 19:32:18.071174 139643526821696 submission_runner.py:313] Starting training loop.
I0405 19:32:20.408252 139643526821696 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 22.778257408
I0405 19:32:25.901196 139615176480512 logging_writer.py:48] [0] global_step=0, grad_norm=0.527851, loss=6.934239
I0405 19:32:25.910658 139643526821696 submission.py:139] 0) loss = 6.934, grad_norm = 0.528
I0405 19:32:25.911218 139643526821696 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.918125056
I0405 19:32:25.911859 139643526821696 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.918125056
I0405 19:32:25.912009 139643526821696 spec.py:298] Evaluating on the training split.
I0405 19:33:27.067877 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 19:34:24.718390 139643526821696 spec.py:326] Evaluating on the test split.
I0405 19:34:24.734217 139643526821696 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:24.740284 139643526821696 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 19:34:24.804944 139643526821696 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 19:34:37.238368 139643526821696 submission_runner.py:382] Time since start: 7.84s, 	Step: 1, 	{'train/accuracy': 0.0011160714285714285, 'train/loss': 6.921477025868941, 'validation/accuracy': 0.00096, 'validation/loss': 6.921369375, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.92442890625, 'test/num_examples': 10000}
I0405 19:34:37.238762 139643526821696 submission_runner.py:396] After eval at step 1: RAM USED (GB) 91.755159552
I0405 19:34:37.247322 139591050843904 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=7.839142, test/accuracy=0.001000, test/loss=6.924429, test/num_examples=10000, total_duration=7.841300, train/accuracy=0.001116, train/loss=6.921477, validation/accuracy=0.000960, validation/loss=6.921369, validation/num_examples=50000
I0405 19:34:37.549355 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_1.
I0405 19:34:37.549988 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 91.755343872
I0405 19:34:37.577158 139643526821696 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 91.756130304
I0405 19:34:37.585080 139643526821696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.584985 140310243850048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.584879 140415653533504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.585178 140636523849536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.585214 140464732006208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.585287 139972874770240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.585467 140581496067904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.585484 140015236769600 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 19:34:37.947359 139591042451200 logging_writer.py:48] [1] global_step=1, grad_norm=0.551881, loss=6.923967
I0405 19:34:37.951008 139643526821696 submission.py:139] 1) loss = 6.924, grad_norm = 0.552
I0405 19:34:37.951645 139643526821696 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 91.763675136
I0405 19:34:38.322664 139591050843904 logging_writer.py:48] [2] global_step=2, grad_norm=0.540506, loss=6.930362
I0405 19:34:38.326118 139643526821696 submission.py:139] 2) loss = 6.930, grad_norm = 0.541
I0405 19:34:38.699869 139591042451200 logging_writer.py:48] [3] global_step=3, grad_norm=0.538227, loss=6.926275
I0405 19:34:38.703309 139643526821696 submission.py:139] 3) loss = 6.926, grad_norm = 0.538
I0405 19:34:39.079279 139591050843904 logging_writer.py:48] [4] global_step=4, grad_norm=0.540790, loss=6.932524
I0405 19:34:39.083370 139643526821696 submission.py:139] 4) loss = 6.933, grad_norm = 0.541
I0405 19:34:39.462174 139591042451200 logging_writer.py:48] [5] global_step=5, grad_norm=0.533934, loss=6.926912
I0405 19:34:39.466305 139643526821696 submission.py:139] 5) loss = 6.927, grad_norm = 0.534
I0405 19:34:39.847205 139591050843904 logging_writer.py:48] [6] global_step=6, grad_norm=0.545851, loss=6.931866
I0405 19:34:39.851231 139643526821696 submission.py:139] 6) loss = 6.932, grad_norm = 0.546
I0405 19:34:40.229035 139591042451200 logging_writer.py:48] [7] global_step=7, grad_norm=0.547601, loss=6.928481
I0405 19:34:40.233339 139643526821696 submission.py:139] 7) loss = 6.928, grad_norm = 0.548
I0405 19:34:40.613812 139591050843904 logging_writer.py:48] [8] global_step=8, grad_norm=0.542463, loss=6.920926
I0405 19:34:40.617961 139643526821696 submission.py:139] 8) loss = 6.921, grad_norm = 0.542
I0405 19:34:40.995076 139591042451200 logging_writer.py:48] [9] global_step=9, grad_norm=0.552543, loss=6.925259
I0405 19:34:40.999254 139643526821696 submission.py:139] 9) loss = 6.925, grad_norm = 0.553
I0405 19:34:41.374380 139591050843904 logging_writer.py:48] [10] global_step=10, grad_norm=0.539676, loss=6.928270
I0405 19:34:41.379129 139643526821696 submission.py:139] 10) loss = 6.928, grad_norm = 0.540
I0405 19:34:41.759834 139591042451200 logging_writer.py:48] [11] global_step=11, grad_norm=0.537892, loss=6.923491
I0405 19:34:41.764666 139643526821696 submission.py:139] 11) loss = 6.923, grad_norm = 0.538
I0405 19:34:42.139691 139591050843904 logging_writer.py:48] [12] global_step=12, grad_norm=0.528538, loss=6.924313
I0405 19:34:42.144597 139643526821696 submission.py:139] 12) loss = 6.924, grad_norm = 0.529
I0405 19:34:42.523553 139591042451200 logging_writer.py:48] [13] global_step=13, grad_norm=0.542365, loss=6.933626
I0405 19:34:42.529092 139643526821696 submission.py:139] 13) loss = 6.934, grad_norm = 0.542
I0405 19:34:42.905500 139591050843904 logging_writer.py:48] [14] global_step=14, grad_norm=0.550214, loss=6.927396
I0405 19:34:42.909328 139643526821696 submission.py:139] 14) loss = 6.927, grad_norm = 0.550
I0405 19:34:43.286988 139591042451200 logging_writer.py:48] [15] global_step=15, grad_norm=0.527739, loss=6.924590
I0405 19:34:43.290853 139643526821696 submission.py:139] 15) loss = 6.925, grad_norm = 0.528
I0405 19:34:43.668309 139591050843904 logging_writer.py:48] [16] global_step=16, grad_norm=0.526284, loss=6.930561
I0405 19:34:43.673852 139643526821696 submission.py:139] 16) loss = 6.931, grad_norm = 0.526
I0405 19:34:44.049573 139591042451200 logging_writer.py:48] [17] global_step=17, grad_norm=0.537485, loss=6.926636
I0405 19:34:44.054945 139643526821696 submission.py:139] 17) loss = 6.927, grad_norm = 0.537
I0405 19:34:44.430905 139591050843904 logging_writer.py:48] [18] global_step=18, grad_norm=0.540190, loss=6.925969
I0405 19:34:44.434694 139643526821696 submission.py:139] 18) loss = 6.926, grad_norm = 0.540
I0405 19:34:44.826215 139591042451200 logging_writer.py:48] [19] global_step=19, grad_norm=0.540293, loss=6.921247
I0405 19:34:44.837374 139643526821696 submission.py:139] 19) loss = 6.921, grad_norm = 0.540
I0405 19:34:45.217408 139591050843904 logging_writer.py:48] [20] global_step=20, grad_norm=0.546032, loss=6.929439
I0405 19:34:45.221042 139643526821696 submission.py:139] 20) loss = 6.929, grad_norm = 0.546
I0405 19:34:45.597872 139591042451200 logging_writer.py:48] [21] global_step=21, grad_norm=0.535096, loss=6.929532
I0405 19:34:45.602549 139643526821696 submission.py:139] 21) loss = 6.930, grad_norm = 0.535
I0405 19:34:45.980031 139591050843904 logging_writer.py:48] [22] global_step=22, grad_norm=0.538604, loss=6.931605
I0405 19:34:45.983955 139643526821696 submission.py:139] 22) loss = 6.932, grad_norm = 0.539
I0405 19:34:46.359781 139591042451200 logging_writer.py:48] [23] global_step=23, grad_norm=0.536167, loss=6.930822
I0405 19:34:46.363473 139643526821696 submission.py:139] 23) loss = 6.931, grad_norm = 0.536
I0405 19:34:46.738983 139591050843904 logging_writer.py:48] [24] global_step=24, grad_norm=0.540852, loss=6.927270
I0405 19:34:46.743006 139643526821696 submission.py:139] 24) loss = 6.927, grad_norm = 0.541
I0405 19:34:47.119213 139591042451200 logging_writer.py:48] [25] global_step=25, grad_norm=0.527417, loss=6.917308
I0405 19:34:47.123623 139643526821696 submission.py:139] 25) loss = 6.917, grad_norm = 0.527
I0405 19:34:47.508045 139591050843904 logging_writer.py:48] [26] global_step=26, grad_norm=0.544051, loss=6.930285
I0405 19:34:47.511691 139643526821696 submission.py:139] 26) loss = 6.930, grad_norm = 0.544
I0405 19:34:47.887800 139591042451200 logging_writer.py:48] [27] global_step=27, grad_norm=0.529621, loss=6.924031
I0405 19:34:47.891905 139643526821696 submission.py:139] 27) loss = 6.924, grad_norm = 0.530
I0405 19:34:48.268933 139591050843904 logging_writer.py:48] [28] global_step=28, grad_norm=0.541898, loss=6.923251
I0405 19:34:48.273483 139643526821696 submission.py:139] 28) loss = 6.923, grad_norm = 0.542
I0405 19:34:48.648547 139591042451200 logging_writer.py:48] [29] global_step=29, grad_norm=0.548042, loss=6.924434
I0405 19:34:48.652609 139643526821696 submission.py:139] 29) loss = 6.924, grad_norm = 0.548
I0405 19:34:49.030802 139591050843904 logging_writer.py:48] [30] global_step=30, grad_norm=0.537339, loss=6.922249
I0405 19:34:49.034451 139643526821696 submission.py:139] 30) loss = 6.922, grad_norm = 0.537
I0405 19:34:49.409822 139591042451200 logging_writer.py:48] [31] global_step=31, grad_norm=0.528942, loss=6.919446
I0405 19:34:49.415291 139643526821696 submission.py:139] 31) loss = 6.919, grad_norm = 0.529
I0405 19:34:49.792511 139591050843904 logging_writer.py:48] [32] global_step=32, grad_norm=0.514274, loss=6.916986
I0405 19:34:49.797102 139643526821696 submission.py:139] 32) loss = 6.917, grad_norm = 0.514
I0405 19:34:50.175900 139591042451200 logging_writer.py:48] [33] global_step=33, grad_norm=0.546350, loss=6.913649
I0405 19:34:50.181125 139643526821696 submission.py:139] 33) loss = 6.914, grad_norm = 0.546
I0405 19:34:50.556318 139591050843904 logging_writer.py:48] [34] global_step=34, grad_norm=0.535204, loss=6.921518
I0405 19:34:50.560205 139643526821696 submission.py:139] 34) loss = 6.922, grad_norm = 0.535
I0405 19:34:50.935786 139591042451200 logging_writer.py:48] [35] global_step=35, grad_norm=0.527321, loss=6.920227
I0405 19:34:50.940292 139643526821696 submission.py:139] 35) loss = 6.920, grad_norm = 0.527
I0405 19:34:51.329317 139591050843904 logging_writer.py:48] [36] global_step=36, grad_norm=0.548327, loss=6.921711
I0405 19:34:51.333581 139643526821696 submission.py:139] 36) loss = 6.922, grad_norm = 0.548
I0405 19:34:51.709694 139591042451200 logging_writer.py:48] [37] global_step=37, grad_norm=0.539747, loss=6.923025
I0405 19:34:51.713301 139643526821696 submission.py:139] 37) loss = 6.923, grad_norm = 0.540
I0405 19:34:52.090283 139591050843904 logging_writer.py:48] [38] global_step=38, grad_norm=0.521370, loss=6.913909
I0405 19:34:52.095134 139643526821696 submission.py:139] 38) loss = 6.914, grad_norm = 0.521
I0405 19:34:52.473283 139591042451200 logging_writer.py:48] [39] global_step=39, grad_norm=0.533882, loss=6.919300
I0405 19:34:52.477143 139643526821696 submission.py:139] 39) loss = 6.919, grad_norm = 0.534
I0405 19:34:52.858924 139591050843904 logging_writer.py:48] [40] global_step=40, grad_norm=0.540041, loss=6.916086
I0405 19:34:52.863538 139643526821696 submission.py:139] 40) loss = 6.916, grad_norm = 0.540
I0405 19:34:53.241739 139591042451200 logging_writer.py:48] [41] global_step=41, grad_norm=0.521557, loss=6.918584
I0405 19:34:53.246868 139643526821696 submission.py:139] 41) loss = 6.919, grad_norm = 0.522
I0405 19:34:53.625764 139591050843904 logging_writer.py:48] [42] global_step=42, grad_norm=0.530781, loss=6.918818
I0405 19:34:53.629548 139643526821696 submission.py:139] 42) loss = 6.919, grad_norm = 0.531
I0405 19:34:54.010457 139591042451200 logging_writer.py:48] [43] global_step=43, grad_norm=0.540596, loss=6.921086
I0405 19:34:54.015195 139643526821696 submission.py:139] 43) loss = 6.921, grad_norm = 0.541
I0405 19:34:54.392426 139591050843904 logging_writer.py:48] [44] global_step=44, grad_norm=0.535499, loss=6.916992
I0405 19:34:54.396500 139643526821696 submission.py:139] 44) loss = 6.917, grad_norm = 0.535
I0405 19:34:54.770313 139591042451200 logging_writer.py:48] [45] global_step=45, grad_norm=0.517882, loss=6.917339
I0405 19:34:54.774025 139643526821696 submission.py:139] 45) loss = 6.917, grad_norm = 0.518
I0405 19:34:55.149887 139591050843904 logging_writer.py:48] [46] global_step=46, grad_norm=0.550340, loss=6.923826
I0405 19:34:55.154506 139643526821696 submission.py:139] 46) loss = 6.924, grad_norm = 0.550
I0405 19:34:55.539850 139591042451200 logging_writer.py:48] [47] global_step=47, grad_norm=0.535261, loss=6.917036
I0405 19:34:55.544519 139643526821696 submission.py:139] 47) loss = 6.917, grad_norm = 0.535
I0405 19:34:55.921398 139591050843904 logging_writer.py:48] [48] global_step=48, grad_norm=0.529550, loss=6.922101
I0405 19:34:55.925781 139643526821696 submission.py:139] 48) loss = 6.922, grad_norm = 0.530
I0405 19:34:56.304783 139591042451200 logging_writer.py:48] [49] global_step=49, grad_norm=0.541610, loss=6.918698
I0405 19:34:56.309032 139643526821696 submission.py:139] 49) loss = 6.919, grad_norm = 0.542
I0405 19:34:56.686042 139591050843904 logging_writer.py:48] [50] global_step=50, grad_norm=0.526662, loss=6.911427
I0405 19:34:56.690902 139643526821696 submission.py:139] 50) loss = 6.911, grad_norm = 0.527
I0405 19:34:57.068443 139591042451200 logging_writer.py:48] [51] global_step=51, grad_norm=0.527673, loss=6.911908
I0405 19:34:57.071990 139643526821696 submission.py:139] 51) loss = 6.912, grad_norm = 0.528
I0405 19:34:57.448784 139591050843904 logging_writer.py:48] [52] global_step=52, grad_norm=0.519689, loss=6.913144
I0405 19:34:57.452588 139643526821696 submission.py:139] 52) loss = 6.913, grad_norm = 0.520
I0405 19:34:57.849617 139591042451200 logging_writer.py:48] [53] global_step=53, grad_norm=0.523155, loss=6.915738
I0405 19:34:57.854096 139643526821696 submission.py:139] 53) loss = 6.916, grad_norm = 0.523
I0405 19:34:58.236138 139591050843904 logging_writer.py:48] [54] global_step=54, grad_norm=0.540819, loss=6.909938
I0405 19:34:58.240983 139643526821696 submission.py:139] 54) loss = 6.910, grad_norm = 0.541
I0405 19:34:58.617406 139591042451200 logging_writer.py:48] [55] global_step=55, grad_norm=0.516585, loss=6.912275
I0405 19:34:58.621115 139643526821696 submission.py:139] 55) loss = 6.912, grad_norm = 0.517
I0405 19:34:58.999954 139591050843904 logging_writer.py:48] [56] global_step=56, grad_norm=0.536036, loss=6.910132
I0405 19:34:59.006274 139643526821696 submission.py:139] 56) loss = 6.910, grad_norm = 0.536
I0405 19:34:59.449087 139591042451200 logging_writer.py:48] [57] global_step=57, grad_norm=0.534198, loss=6.919341
I0405 19:34:59.453701 139643526821696 submission.py:139] 57) loss = 6.919, grad_norm = 0.534
I0405 19:34:59.832973 139591050843904 logging_writer.py:48] [58] global_step=58, grad_norm=0.545251, loss=6.902051
I0405 19:34:59.837002 139643526821696 submission.py:139] 58) loss = 6.902, grad_norm = 0.545
I0405 19:35:00.214899 139591042451200 logging_writer.py:48] [59] global_step=59, grad_norm=0.545944, loss=6.915325
I0405 19:35:00.219786 139643526821696 submission.py:139] 59) loss = 6.915, grad_norm = 0.546
I0405 19:35:00.595207 139591050843904 logging_writer.py:48] [60] global_step=60, grad_norm=0.528167, loss=6.906518
I0405 19:35:00.601746 139643526821696 submission.py:139] 60) loss = 6.907, grad_norm = 0.528
I0405 19:35:00.977035 139591042451200 logging_writer.py:48] [61] global_step=61, grad_norm=0.512610, loss=6.907818
I0405 19:35:00.981168 139643526821696 submission.py:139] 61) loss = 6.908, grad_norm = 0.513
I0405 19:35:01.358254 139591050843904 logging_writer.py:48] [62] global_step=62, grad_norm=0.517545, loss=6.913787
I0405 19:35:01.362066 139643526821696 submission.py:139] 62) loss = 6.914, grad_norm = 0.518
I0405 19:35:01.741003 139591042451200 logging_writer.py:48] [63] global_step=63, grad_norm=0.537664, loss=6.901195
I0405 19:35:01.745379 139643526821696 submission.py:139] 63) loss = 6.901, grad_norm = 0.538
I0405 19:35:02.123845 139591050843904 logging_writer.py:48] [64] global_step=64, grad_norm=0.557693, loss=6.911244
I0405 19:35:02.127863 139643526821696 submission.py:139] 64) loss = 6.911, grad_norm = 0.558
I0405 19:35:02.504969 139591042451200 logging_writer.py:48] [65] global_step=65, grad_norm=0.524929, loss=6.905158
I0405 19:35:02.509198 139643526821696 submission.py:139] 65) loss = 6.905, grad_norm = 0.525
I0405 19:35:02.890288 139591050843904 logging_writer.py:48] [66] global_step=66, grad_norm=0.526473, loss=6.904819
I0405 19:35:02.894049 139643526821696 submission.py:139] 66) loss = 6.905, grad_norm = 0.526
I0405 19:35:03.269562 139591042451200 logging_writer.py:48] [67] global_step=67, grad_norm=0.534390, loss=6.914541
I0405 19:35:03.273597 139643526821696 submission.py:139] 67) loss = 6.915, grad_norm = 0.534
I0405 19:35:03.649350 139591050843904 logging_writer.py:48] [68] global_step=68, grad_norm=0.534305, loss=6.912697
I0405 19:35:03.653695 139643526821696 submission.py:139] 68) loss = 6.913, grad_norm = 0.534
I0405 19:35:04.034593 139591042451200 logging_writer.py:48] [69] global_step=69, grad_norm=0.513127, loss=6.910816
I0405 19:35:04.039008 139643526821696 submission.py:139] 69) loss = 6.911, grad_norm = 0.513
I0405 19:35:04.416763 139591050843904 logging_writer.py:48] [70] global_step=70, grad_norm=0.526821, loss=6.900581
I0405 19:35:04.420994 139643526821696 submission.py:139] 70) loss = 6.901, grad_norm = 0.527
I0405 19:35:04.796408 139591042451200 logging_writer.py:48] [71] global_step=71, grad_norm=0.531933, loss=6.908076
I0405 19:35:04.800974 139643526821696 submission.py:139] 71) loss = 6.908, grad_norm = 0.532
I0405 19:35:05.176045 139591050843904 logging_writer.py:48] [72] global_step=72, grad_norm=0.528880, loss=6.896239
I0405 19:35:05.179994 139643526821696 submission.py:139] 72) loss = 6.896, grad_norm = 0.529
I0405 19:35:05.556574 139591042451200 logging_writer.py:48] [73] global_step=73, grad_norm=0.542347, loss=6.908344
I0405 19:35:05.560557 139643526821696 submission.py:139] 73) loss = 6.908, grad_norm = 0.542
I0405 19:35:05.942302 139591050843904 logging_writer.py:48] [74] global_step=74, grad_norm=0.511875, loss=6.895479
I0405 19:35:05.945930 139643526821696 submission.py:139] 74) loss = 6.895, grad_norm = 0.512
I0405 19:35:06.323980 139591042451200 logging_writer.py:48] [75] global_step=75, grad_norm=0.518401, loss=6.901130
I0405 19:35:06.328600 139643526821696 submission.py:139] 75) loss = 6.901, grad_norm = 0.518
I0405 19:35:06.704636 139591050843904 logging_writer.py:48] [76] global_step=76, grad_norm=0.521557, loss=6.898211
I0405 19:35:06.708353 139643526821696 submission.py:139] 76) loss = 6.898, grad_norm = 0.522
I0405 19:35:07.084915 139591042451200 logging_writer.py:48] [77] global_step=77, grad_norm=0.528938, loss=6.896021
I0405 19:35:07.090281 139643526821696 submission.py:139] 77) loss = 6.896, grad_norm = 0.529
I0405 19:35:07.467330 139591050843904 logging_writer.py:48] [78] global_step=78, grad_norm=0.534255, loss=6.897762
I0405 19:35:07.472536 139643526821696 submission.py:139] 78) loss = 6.898, grad_norm = 0.534
I0405 19:35:07.850030 139591042451200 logging_writer.py:48] [79] global_step=79, grad_norm=0.546379, loss=6.906949
I0405 19:35:07.853621 139643526821696 submission.py:139] 79) loss = 6.907, grad_norm = 0.546
I0405 19:35:08.234937 139591050843904 logging_writer.py:48] [80] global_step=80, grad_norm=0.515259, loss=6.894453
I0405 19:35:08.238942 139643526821696 submission.py:139] 80) loss = 6.894, grad_norm = 0.515
I0405 19:35:08.617056 139591042451200 logging_writer.py:48] [81] global_step=81, grad_norm=0.514884, loss=6.900726
I0405 19:35:08.622101 139643526821696 submission.py:139] 81) loss = 6.901, grad_norm = 0.515
I0405 19:35:09.005612 139591050843904 logging_writer.py:48] [82] global_step=82, grad_norm=0.521860, loss=6.897160
I0405 19:35:09.009945 139643526821696 submission.py:139] 82) loss = 6.897, grad_norm = 0.522
I0405 19:35:09.388535 139591042451200 logging_writer.py:48] [83] global_step=83, grad_norm=0.535150, loss=6.903688
I0405 19:35:09.392966 139643526821696 submission.py:139] 83) loss = 6.904, grad_norm = 0.535
I0405 19:35:09.770795 139591050843904 logging_writer.py:48] [84] global_step=84, grad_norm=0.525790, loss=6.909557
I0405 19:35:09.774868 139643526821696 submission.py:139] 84) loss = 6.910, grad_norm = 0.526
I0405 19:35:10.159997 139591042451200 logging_writer.py:48] [85] global_step=85, grad_norm=0.517621, loss=6.896683
I0405 19:35:10.164471 139643526821696 submission.py:139] 85) loss = 6.897, grad_norm = 0.518
I0405 19:35:10.542304 139591050843904 logging_writer.py:48] [86] global_step=86, grad_norm=0.534412, loss=6.891167
I0405 19:35:10.546453 139643526821696 submission.py:139] 86) loss = 6.891, grad_norm = 0.534
I0405 19:35:10.929136 139591042451200 logging_writer.py:48] [87] global_step=87, grad_norm=0.530165, loss=6.893881
I0405 19:35:10.933118 139643526821696 submission.py:139] 87) loss = 6.894, grad_norm = 0.530
I0405 19:35:11.309068 139591050843904 logging_writer.py:48] [88] global_step=88, grad_norm=0.528706, loss=6.898524
I0405 19:35:11.313956 139643526821696 submission.py:139] 88) loss = 6.899, grad_norm = 0.529
I0405 19:35:11.701325 139591042451200 logging_writer.py:48] [89] global_step=89, grad_norm=0.523530, loss=6.889325
I0405 19:35:11.705759 139643526821696 submission.py:139] 89) loss = 6.889, grad_norm = 0.524
I0405 19:35:12.087228 139591050843904 logging_writer.py:48] [90] global_step=90, grad_norm=0.538458, loss=6.898218
I0405 19:35:12.090732 139643526821696 submission.py:139] 90) loss = 6.898, grad_norm = 0.538
I0405 19:35:12.466537 139591042451200 logging_writer.py:48] [91] global_step=91, grad_norm=0.524911, loss=6.895418
I0405 19:35:12.471931 139643526821696 submission.py:139] 91) loss = 6.895, grad_norm = 0.525
I0405 19:35:12.850363 139591050843904 logging_writer.py:48] [92] global_step=92, grad_norm=0.536721, loss=6.891386
I0405 19:35:12.855937 139643526821696 submission.py:139] 92) loss = 6.891, grad_norm = 0.537
I0405 19:35:13.232168 139591042451200 logging_writer.py:48] [93] global_step=93, grad_norm=0.527254, loss=6.888127
I0405 19:35:13.237437 139643526821696 submission.py:139] 93) loss = 6.888, grad_norm = 0.527
I0405 19:35:13.613637 139591050843904 logging_writer.py:48] [94] global_step=94, grad_norm=0.516651, loss=6.889664
I0405 19:35:13.618474 139643526821696 submission.py:139] 94) loss = 6.890, grad_norm = 0.517
I0405 19:35:13.992273 139591042451200 logging_writer.py:48] [95] global_step=95, grad_norm=0.526731, loss=6.892194
I0405 19:35:13.995870 139643526821696 submission.py:139] 95) loss = 6.892, grad_norm = 0.527
I0405 19:35:14.371215 139591050843904 logging_writer.py:48] [96] global_step=96, grad_norm=0.527592, loss=6.891553
I0405 19:35:14.374890 139643526821696 submission.py:139] 96) loss = 6.892, grad_norm = 0.528
I0405 19:35:14.751516 139591042451200 logging_writer.py:48] [97] global_step=97, grad_norm=0.534307, loss=6.883662
I0405 19:35:14.755041 139643526821696 submission.py:139] 97) loss = 6.884, grad_norm = 0.534
I0405 19:35:15.133136 139591050843904 logging_writer.py:48] [98] global_step=98, grad_norm=0.526395, loss=6.881728
I0405 19:35:15.137099 139643526821696 submission.py:139] 98) loss = 6.882, grad_norm = 0.526
I0405 19:35:15.514049 139591042451200 logging_writer.py:48] [99] global_step=99, grad_norm=0.537230, loss=6.891080
I0405 19:35:15.518532 139643526821696 submission.py:139] 99) loss = 6.891, grad_norm = 0.537
I0405 19:35:15.895176 139591050843904 logging_writer.py:48] [100] global_step=100, grad_norm=0.528943, loss=6.891603
I0405 19:35:15.899792 139643526821696 submission.py:139] 100) loss = 6.892, grad_norm = 0.529
I0405 19:37:44.571981 139591042451200 logging_writer.py:48] [500] global_step=500, grad_norm=0.626211, loss=6.565680
I0405 19:37:44.575885 139643526821696 submission.py:139] 500) loss = 6.566, grad_norm = 0.626
I0405 19:40:50.788583 139591050843904 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.811869, loss=6.254553
I0405 19:40:50.797122 139643526821696 submission.py:139] 1000) loss = 6.255, grad_norm = 0.812
I0405 19:43:07.738181 139643526821696 submission_runner.py:373] Before eval at step 1365: RAM USED (GB) 98.29414912
I0405 19:43:07.738397 139643526821696 spec.py:298] Evaluating on the training split.
I0405 19:43:54.334037 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 19:44:51.577533 139643526821696 spec.py:326] Evaluating on the test split.
I0405 19:44:52.951224 139643526821696 submission_runner.py:382] Time since start: 649.63s, 	Step: 1365, 	{'train/accuracy': 0.06823979591836735, 'train/loss': 5.520933812978316, 'validation/accuracy': 0.06338, 'validation/loss': 5.582079375, 'validation/num_examples': 50000, 'test/accuracy': 0.0415, 'test/loss': 5.824325, 'test/num_examples': 10000}
I0405 19:44:52.951556 139643526821696 submission_runner.py:396] After eval at step 1365: RAM USED (GB) 98.339098624
I0405 19:44:52.969963 139591059236608 logging_writer.py:48] [1365] global_step=1365, preemption_count=0, score=474.510319, test/accuracy=0.041500, test/loss=5.824325, test/num_examples=10000, total_duration=649.634698, train/accuracy=0.068240, train/loss=5.520934, validation/accuracy=0.063380, validation/loss=5.582079, validation/num_examples=50000
I0405 19:44:53.288839 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_1365.
I0405 19:44:53.289586 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 1365: RAM USED (GB) 98.334707712
I0405 19:45:43.637832 139591067629312 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.878317, loss=5.999528
I0405 19:45:43.641616 139643526821696 submission.py:139] 1500) loss = 6.000, grad_norm = 0.878
I0405 19:48:48.915863 139591059236608 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.820247, loss=5.747874
I0405 19:48:48.919624 139643526821696 submission.py:139] 2000) loss = 5.748, grad_norm = 0.820
I0405 19:51:54.154777 139591067629312 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.995745, loss=5.446559
I0405 19:51:54.158550 139643526821696 submission.py:139] 2500) loss = 5.447, grad_norm = 0.996
I0405 19:53:23.530104 139643526821696 submission_runner.py:373] Before eval at step 2739: RAM USED (GB) 99.753766912
I0405 19:53:23.530346 139643526821696 spec.py:298] Evaluating on the training split.
I0405 19:54:06.656709 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 19:55:03.223046 139643526821696 spec.py:326] Evaluating on the test split.
I0405 19:55:04.590571 139643526821696 submission_runner.py:382] Time since start: 1265.43s, 	Step: 2739, 	{'train/accuracy': 0.1703204719387755, 'train/loss': 4.397937696807238, 'validation/accuracy': 0.15308, 'validation/loss': 4.506046875, 'validation/num_examples': 50000, 'test/accuracy': 0.1089, 'test/loss': 4.9748921875, 'test/num_examples': 10000}
I0405 19:55:04.590991 139643526821696 submission_runner.py:396] After eval at step 2739: RAM USED (GB) 99.766607872
I0405 19:55:04.600324 139591059236608 logging_writer.py:48] [2739] global_step=2739, preemption_count=0, score=937.782114, test/accuracy=0.108900, test/loss=4.974892, test/num_examples=10000, total_duration=1265.426732, train/accuracy=0.170320, train/loss=4.397938, validation/accuracy=0.153080, validation/loss=4.506047, validation/num_examples=50000
I0405 19:55:04.932543 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_2739.
I0405 19:55:04.933348 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 2739: RAM USED (GB) 99.765633024
I0405 19:56:41.685378 139591067629312 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.995935, loss=5.178184
I0405 19:56:41.689431 139643526821696 submission.py:139] 3000) loss = 5.178, grad_norm = 0.996
I0405 19:59:46.772398 139591059236608 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.900884, loss=4.851006
I0405 19:59:46.776935 139643526821696 submission.py:139] 3500) loss = 4.851, grad_norm = 0.901
I0405 20:02:53.320930 139591067629312 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.890324, loss=4.760715
I0405 20:02:53.325314 139643526821696 submission.py:139] 4000) loss = 4.761, grad_norm = 0.890
I0405 20:03:35.103940 139643526821696 submission_runner.py:373] Before eval at step 4114: RAM USED (GB) 99.887788032
I0405 20:03:35.104150 139643526821696 spec.py:298] Evaluating on the training split.
I0405 20:04:19.013719 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 20:05:04.184900 139643526821696 spec.py:326] Evaluating on the test split.
I0405 20:05:05.552021 139643526821696 submission_runner.py:382] Time since start: 1877.00s, 	Step: 4114, 	{'train/accuracy': 0.30751753826530615, 'train/loss': 3.4386565539301657, 'validation/accuracy': 0.28324, 'validation/loss': 3.5701009375, 'validation/num_examples': 50000, 'test/accuracy': 0.2008, 'test/loss': 4.155472265625, 'test/num_examples': 10000}
I0405 20:05:05.552345 139643526821696 submission_runner.py:396] After eval at step 4114: RAM USED (GB) 99.823054848
I0405 20:05:05.560059 139591059236608 logging_writer.py:48] [4114] global_step=4114, preemption_count=0, score=1401.024216, test/accuracy=0.200800, test/loss=4.155472, test/num_examples=10000, total_duration=1877.001523, train/accuracy=0.307518, train/loss=3.438657, validation/accuracy=0.283240, validation/loss=3.570101, validation/num_examples=50000
I0405 20:05:05.874953 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_4114.
I0405 20:05:05.875686 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 4114: RAM USED (GB) 99.82257152
I0405 20:07:28.820771 139591067629312 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.874541, loss=4.454585
I0405 20:07:28.824886 139643526821696 submission.py:139] 4500) loss = 4.455, grad_norm = 0.875
I0405 20:10:33.815331 139591059236608 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.854024, loss=4.415182
I0405 20:10:33.820065 139643526821696 submission.py:139] 5000) loss = 4.415, grad_norm = 0.854
I0405 20:13:35.944763 139643526821696 submission_runner.py:373] Before eval at step 5490: RAM USED (GB) 100.205948928
I0405 20:13:35.944989 139643526821696 spec.py:298] Evaluating on the training split.
I0405 20:14:21.359588 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 20:15:18.106867 139643526821696 spec.py:326] Evaluating on the test split.
I0405 20:15:19.456701 139643526821696 submission_runner.py:382] Time since start: 2477.84s, 	Step: 5490, 	{'train/accuracy': 0.4182676977040816, 'train/loss': 2.8356759207589284, 'validation/accuracy': 0.38388, 'validation/loss': 2.99616625, 'validation/num_examples': 50000, 'test/accuracy': 0.2816, 'test/loss': 3.6439734375, 'test/num_examples': 10000}
I0405 20:15:19.457038 139643526821696 submission_runner.py:396] After eval at step 5490: RAM USED (GB) 100.211511296
I0405 20:15:19.466114 139591067629312 logging_writer.py:48] [5490] global_step=5490, preemption_count=0, score=1864.078113, test/accuracy=0.281600, test/loss=3.643973, test/num_examples=10000, total_duration=2477.840860, train/accuracy=0.418268, train/loss=2.835676, validation/accuracy=0.383880, validation/loss=2.996166, validation/num_examples=50000
I0405 20:15:19.781732 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_5490.
I0405 20:15:19.782447 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 5490: RAM USED (GB) 100.210528256
I0405 20:15:23.852962 139591059236608 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.737677, loss=4.388541
I0405 20:15:23.857402 139643526821696 submission.py:139] 5500) loss = 4.389, grad_norm = 0.738
I0405 20:18:28.685133 139591067629312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.733825, loss=4.182394
I0405 20:18:28.689543 139643526821696 submission.py:139] 6000) loss = 4.182, grad_norm = 0.734
I0405 20:21:34.909534 139591059236608 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.680413, loss=4.150032
I0405 20:21:34.914594 139643526821696 submission.py:139] 6500) loss = 4.150, grad_norm = 0.680
I0405 20:23:50.094479 139643526821696 submission_runner.py:373] Before eval at step 6867: RAM USED (GB) 100.06274048
I0405 20:23:50.094689 139643526821696 spec.py:298] Evaluating on the training split.
I0405 20:24:33.586254 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 20:25:19.000615 139643526821696 spec.py:326] Evaluating on the test split.
I0405 20:25:20.350053 139643526821696 submission_runner.py:382] Time since start: 3091.99s, 	Step: 6867, 	{'train/accuracy': 0.48375717474489793, 'train/loss': 2.421766164351483, 'validation/accuracy': 0.4462, 'validation/loss': 2.6234925, 'validation/num_examples': 50000, 'test/accuracy': 0.3257, 'test/loss': 3.342599609375, 'test/num_examples': 10000}
I0405 20:25:20.350380 139643526821696 submission_runner.py:396] After eval at step 6867: RAM USED (GB) 100.032237568
I0405 20:25:20.359154 139591067629312 logging_writer.py:48] [6867] global_step=6867, preemption_count=0, score=2327.388846, test/accuracy=0.325700, test/loss=3.342600, test/num_examples=10000, total_duration=3091.990970, train/accuracy=0.483757, train/loss=2.421766, validation/accuracy=0.446200, validation/loss=2.623492, validation/num_examples=50000
I0405 20:25:20.663845 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_6867.
I0405 20:25:20.664581 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 6867: RAM USED (GB) 100.031778816
I0405 20:26:10.093199 139591059236608 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.624223, loss=4.054696
I0405 20:26:10.096940 139643526821696 submission.py:139] 7000) loss = 4.055, grad_norm = 0.624
I0405 20:29:15.068148 139591067629312 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.621878, loss=3.990847
I0405 20:29:15.072086 139643526821696 submission.py:139] 7500) loss = 3.991, grad_norm = 0.622
I0405 20:32:21.115074 139591059236608 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.613618, loss=3.898658
I0405 20:32:21.119472 139643526821696 submission.py:139] 8000) loss = 3.899, grad_norm = 0.614
I0405 20:33:50.902487 139643526821696 submission_runner.py:373] Before eval at step 8244: RAM USED (GB) 100.057468928
I0405 20:33:50.902697 139643526821696 spec.py:298] Evaluating on the training split.
I0405 20:34:36.131835 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 20:35:30.021165 139643526821696 spec.py:326] Evaluating on the test split.
I0405 20:35:31.376515 139643526821696 submission_runner.py:382] Time since start: 3692.80s, 	Step: 8244, 	{'train/accuracy': 0.5337611607142857, 'train/loss': 2.2621487987284756, 'validation/accuracy': 0.49278, 'validation/loss': 2.4702965625, 'validation/num_examples': 50000, 'test/accuracy': 0.3539, 'test/loss': 3.2199623046875, 'test/num_examples': 10000}
I0405 20:35:31.376883 139643526821696 submission_runner.py:396] After eval at step 8244: RAM USED (GB) 100.037185536
I0405 20:35:31.384920 139591067629312 logging_writer.py:48] [8244] global_step=8244, preemption_count=0, score=2790.611395, test/accuracy=0.353900, test/loss=3.219962, test/num_examples=10000, total_duration=3692.799713, train/accuracy=0.533761, train/loss=2.262149, validation/accuracy=0.492780, validation/loss=2.470297, validation/num_examples=50000
I0405 20:35:31.693392 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_8244.
I0405 20:35:31.694142 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 8244: RAM USED (GB) 100.03666944
I0405 20:37:06.566167 139591059236608 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.592003, loss=3.850334
I0405 20:37:06.569800 139643526821696 submission.py:139] 8500) loss = 3.850, grad_norm = 0.592
I0405 20:40:13.075174 139591067629312 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.561147, loss=3.789496
I0405 20:40:13.080373 139643526821696 submission.py:139] 9000) loss = 3.789, grad_norm = 0.561
I0405 20:43:17.711506 139591059236608 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.522302, loss=3.740750
I0405 20:43:17.715507 139643526821696 submission.py:139] 9500) loss = 3.741, grad_norm = 0.522
I0405 20:44:02.048898 139643526821696 submission_runner.py:373] Before eval at step 9621: RAM USED (GB) 100.080234496
I0405 20:44:02.049185 139643526821696 spec.py:298] Evaluating on the training split.
I0405 20:44:47.362945 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 20:45:33.760760 139643526821696 spec.py:326] Evaluating on the test split.
I0405 20:45:35.120060 139643526821696 submission_runner.py:382] Time since start: 4303.95s, 	Step: 9621, 	{'train/accuracy': 0.5738400829081632, 'train/loss': 2.0828390316087373, 'validation/accuracy': 0.52558, 'validation/loss': 2.31306453125, 'validation/num_examples': 50000, 'test/accuracy': 0.399, 'test/loss': 2.965539453125, 'test/num_examples': 10000}
I0405 20:45:35.120411 139643526821696 submission_runner.py:396] After eval at step 9621: RAM USED (GB) 100.069576704
I0405 20:45:35.129297 139591067629312 logging_writer.py:48] [9621] global_step=9621, preemption_count=0, score=3253.955359, test/accuracy=0.399000, test/loss=2.965539, test/num_examples=10000, total_duration=4303.945737, train/accuracy=0.573840, train/loss=2.082839, validation/accuracy=0.525580, validation/loss=2.313065, validation/num_examples=50000
I0405 20:45:35.435396 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_9621.
I0405 20:45:35.436164 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 9621: RAM USED (GB) 100.068634624
I0405 20:47:55.714858 139591059236608 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.533797, loss=3.665941
I0405 20:47:55.719229 139643526821696 submission.py:139] 10000) loss = 3.666, grad_norm = 0.534
I0405 20:51:01.886077 139591067629312 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.514146, loss=3.671745
I0405 20:51:01.891269 139643526821696 submission.py:139] 10500) loss = 3.672, grad_norm = 0.514
I0405 20:54:05.599061 139643526821696 submission_runner.py:373] Before eval at step 10998: RAM USED (GB) 100.160180224
I0405 20:54:05.599277 139643526821696 spec.py:298] Evaluating on the training split.
I0405 20:54:49.359017 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 20:55:47.032058 139643526821696 spec.py:326] Evaluating on the test split.
I0405 20:55:48.385114 139643526821696 submission_runner.py:382] Time since start: 4907.50s, 	Step: 10998, 	{'train/accuracy': 0.6202168367346939, 'train/loss': 1.8683242797851562, 'validation/accuracy': 0.56684, 'validation/loss': 2.11090390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4333, 'test/loss': 2.80099296875, 'test/num_examples': 10000}
I0405 20:55:48.385482 139643526821696 submission_runner.py:396] After eval at step 10998: RAM USED (GB) 100.09573376
I0405 20:55:48.395899 139591059236608 logging_writer.py:48] [10998] global_step=10998, preemption_count=0, score=3717.046772, test/accuracy=0.433300, test/loss=2.800993, test/num_examples=10000, total_duration=4907.495512, train/accuracy=0.620217, train/loss=1.868324, validation/accuracy=0.566840, validation/loss=2.110904, validation/num_examples=50000
I0405 20:55:48.708543 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_10998.
I0405 20:55:48.709258 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 10998: RAM USED (GB) 100.095270912
I0405 20:55:49.817104 139591067629312 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.495684, loss=3.515730
I0405 20:55:49.820690 139643526821696 submission.py:139] 11000) loss = 3.516, grad_norm = 0.496
I0405 20:58:55.961872 139591059236608 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.522671, loss=3.543657
I0405 20:58:55.972130 139643526821696 submission.py:139] 11500) loss = 3.544, grad_norm = 0.523
I0405 21:02:00.598839 139591067629312 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.493926, loss=3.465680
I0405 21:02:00.602839 139643526821696 submission.py:139] 12000) loss = 3.466, grad_norm = 0.494
I0405 21:04:18.844511 139643526821696 submission_runner.py:373] Before eval at step 12375: RAM USED (GB) 100.222922752
I0405 21:04:18.844739 139643526821696 spec.py:298] Evaluating on the training split.
I0405 21:05:02.697138 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 21:05:48.852725 139643526821696 spec.py:326] Evaluating on the test split.
I0405 21:05:50.210980 139643526821696 submission_runner.py:382] Time since start: 5520.74s, 	Step: 12375, 	{'train/accuracy': 0.641960299744898, 'train/loss': 1.8321656207649075, 'validation/accuracy': 0.57974, 'validation/loss': 2.0968653125, 'validation/num_examples': 50000, 'test/accuracy': 0.4475, 'test/loss': 2.762286328125, 'test/num_examples': 10000}
I0405 21:05:50.211353 139643526821696 submission_runner.py:396] After eval at step 12375: RAM USED (GB) 100.3416576
I0405 21:05:50.222888 139591059236608 logging_writer.py:48] [12375] global_step=12375, preemption_count=0, score=4180.097910, test/accuracy=0.447500, test/loss=2.762286, test/num_examples=10000, total_duration=5520.737525, train/accuracy=0.641960, train/loss=1.832166, validation/accuracy=0.579740, validation/loss=2.096865, validation/num_examples=50000
I0405 21:05:50.552179 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_12375.
I0405 21:05:50.552909 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 12375: RAM USED (GB) 100.340690944
I0405 21:06:37.004268 139591067629312 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.503423, loss=3.590853
I0405 21:06:37.008006 139643526821696 submission.py:139] 12500) loss = 3.591, grad_norm = 0.503
I0405 21:09:43.309041 139591059236608 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.489486, loss=3.383487
I0405 21:09:43.318246 139643526821696 submission.py:139] 13000) loss = 3.383, grad_norm = 0.489
I0405 21:12:48.088136 139591067629312 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.484839, loss=3.413258
I0405 21:12:48.093107 139643526821696 submission.py:139] 13500) loss = 3.413, grad_norm = 0.485
I0405 21:14:20.865006 139643526821696 submission_runner.py:373] Before eval at step 13752: RAM USED (GB) 100.198797312
I0405 21:14:20.865220 139643526821696 spec.py:298] Evaluating on the training split.
I0405 21:15:04.377702 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 21:16:01.269525 139643526821696 spec.py:326] Evaluating on the test split.
I0405 21:16:02.631803 139643526821696 submission_runner.py:382] Time since start: 6122.76s, 	Step: 13752, 	{'train/accuracy': 0.6644013073979592, 'train/loss': 1.6712789730149873, 'validation/accuracy': 0.60406, 'validation/loss': 1.94928125, 'validation/num_examples': 50000, 'test/accuracy': 0.4629, 'test/loss': 2.645347265625, 'test/num_examples': 10000}
I0405 21:16:02.632125 139643526821696 submission_runner.py:396] After eval at step 13752: RAM USED (GB) 100.203835392
I0405 21:16:02.639796 139591059236608 logging_writer.py:48] [13752] global_step=13752, preemption_count=0, score=4643.365165, test/accuracy=0.462900, test/loss=2.645347, test/num_examples=10000, total_duration=6122.761356, train/accuracy=0.664401, train/loss=1.671279, validation/accuracy=0.604060, validation/loss=1.949281, validation/num_examples=50000
I0405 21:16:02.953549 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_13752.
I0405 21:16:02.954327 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 13752: RAM USED (GB) 100.203311104
I0405 21:17:35.819249 139643526821696 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.168925184
I0405 21:17:35.819480 139643526821696 spec.py:298] Evaluating on the training split.
I0405 21:18:19.337483 139643526821696 spec.py:310] Evaluating on the validation split.
I0405 21:19:04.660444 139643526821696 spec.py:326] Evaluating on the test split.
I0405 21:19:06.008397 139643526821696 submission_runner.py:382] Time since start: 6317.72s, 	Step: 14000, 	{'train/accuracy': 0.6817402742346939, 'train/loss': 1.5181009720782845, 'validation/accuracy': 0.6185, 'validation/loss': 1.811365, 'validation/num_examples': 50000, 'test/accuracy': 0.4866, 'test/loss': 2.484880078125, 'test/num_examples': 10000}
I0405 21:19:06.008722 139643526821696 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.049113088
I0405 21:19:06.016648 139591067629312 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4727.762219, test/accuracy=0.486600, test/loss=2.484880, test/num_examples=10000, total_duration=6317.716497, train/accuracy=0.681740, train/loss=1.518101, validation/accuracy=0.618500, validation/loss=1.811365, validation/num_examples=50000
I0405 21:19:06.329967 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:19:06.330690 139643526821696 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.04813824
I0405 21:19:06.338617 139591059236608 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4727.762219
I0405 21:19:07.094014 139643526821696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0405 21:19:07.388677 139643526821696 submission_runner.py:550] Tuning trial 1/1
I0405 21:19:07.388886 139643526821696 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 21:19:07.389510 139643526821696 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011160714285714285, 'train/loss': 6.921477025868941, 'validation/accuracy': 0.00096, 'validation/loss': 6.921369375, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.92442890625, 'test/num_examples': 10000, 'score': 7.839141607284546, 'total_duration': 7.841299772262573, 'global_step': 1, 'preemption_count': 0}), (1365, {'train/accuracy': 0.06823979591836735, 'train/loss': 5.520933812978316, 'validation/accuracy': 0.06338, 'validation/loss': 5.582079375, 'validation/num_examples': 50000, 'test/accuracy': 0.0415, 'test/loss': 5.824325, 'test/num_examples': 10000, 'score': 474.5103192329407, 'total_duration': 649.634697675705, 'global_step': 1365, 'preemption_count': 0}), (2739, {'train/accuracy': 0.1703204719387755, 'train/loss': 4.397937696807238, 'validation/accuracy': 0.15308, 'validation/loss': 4.506046875, 'validation/num_examples': 50000, 'test/accuracy': 0.1089, 'test/loss': 4.9748921875, 'test/num_examples': 10000, 'score': 937.7821135520935, 'total_duration': 1265.4267320632935, 'global_step': 2739, 'preemption_count': 0}), (4114, {'train/accuracy': 0.30751753826530615, 'train/loss': 3.4386565539301657, 'validation/accuracy': 0.28324, 'validation/loss': 3.5701009375, 'validation/num_examples': 50000, 'test/accuracy': 0.2008, 'test/loss': 4.155472265625, 'test/num_examples': 10000, 'score': 1401.0242156982422, 'total_duration': 1877.0015227794647, 'global_step': 4114, 'preemption_count': 0}), (5490, {'train/accuracy': 0.4182676977040816, 'train/loss': 2.8356759207589284, 'validation/accuracy': 0.38388, 'validation/loss': 2.99616625, 'validation/num_examples': 50000, 'test/accuracy': 0.2816, 'test/loss': 3.6439734375, 'test/num_examples': 10000, 'score': 1864.0781128406525, 'total_duration': 2477.840859889984, 'global_step': 5490, 'preemption_count': 0}), (6867, {'train/accuracy': 0.48375717474489793, 'train/loss': 2.421766164351483, 'validation/accuracy': 0.4462, 'validation/loss': 2.6234925, 'validation/num_examples': 50000, 'test/accuracy': 0.3257, 'test/loss': 3.342599609375, 'test/num_examples': 10000, 'score': 2327.3888463974, 'total_duration': 3091.9909703731537, 'global_step': 6867, 'preemption_count': 0}), (8244, {'train/accuracy': 0.5337611607142857, 'train/loss': 2.2621487987284756, 'validation/accuracy': 0.49278, 'validation/loss': 2.4702965625, 'validation/num_examples': 50000, 'test/accuracy': 0.3539, 'test/loss': 3.2199623046875, 'test/num_examples': 10000, 'score': 2790.6113953590393, 'total_duration': 3692.799713373184, 'global_step': 8244, 'preemption_count': 0}), (9621, {'train/accuracy': 0.5738400829081632, 'train/loss': 2.0828390316087373, 'validation/accuracy': 0.52558, 'validation/loss': 2.31306453125, 'validation/num_examples': 50000, 'test/accuracy': 0.399, 'test/loss': 2.965539453125, 'test/num_examples': 10000, 'score': 3253.9553587436676, 'total_duration': 4303.945737361908, 'global_step': 9621, 'preemption_count': 0}), (10998, {'train/accuracy': 0.6202168367346939, 'train/loss': 1.8683242797851562, 'validation/accuracy': 0.56684, 'validation/loss': 2.11090390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4333, 'test/loss': 2.80099296875, 'test/num_examples': 10000, 'score': 3717.046772003174, 'total_duration': 4907.49551153183, 'global_step': 10998, 'preemption_count': 0}), (12375, {'train/accuracy': 0.641960299744898, 'train/loss': 1.8321656207649075, 'validation/accuracy': 0.57974, 'validation/loss': 2.0968653125, 'validation/num_examples': 50000, 'test/accuracy': 0.4475, 'test/loss': 2.762286328125, 'test/num_examples': 10000, 'score': 4180.09790968895, 'total_duration': 5520.737525224686, 'global_step': 12375, 'preemption_count': 0}), (13752, {'train/accuracy': 0.6644013073979592, 'train/loss': 1.6712789730149873, 'validation/accuracy': 0.60406, 'validation/loss': 1.94928125, 'validation/num_examples': 50000, 'test/accuracy': 0.4629, 'test/loss': 2.645347265625, 'test/num_examples': 10000, 'score': 4643.365164756775, 'total_duration': 6122.761355876923, 'global_step': 13752, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6817402742346939, 'train/loss': 1.5181009720782845, 'validation/accuracy': 0.6185, 'validation/loss': 1.811365, 'validation/num_examples': 50000, 'test/accuracy': 0.4866, 'test/loss': 2.484880078125, 'test/num_examples': 10000, 'score': 4727.7622191905975, 'total_duration': 6317.716497421265, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 21:19:07.389597 139643526821696 submission_runner.py:553] Timing: 4727.7622191905975
I0405 21:19:07.389646 139643526821696 submission_runner.py:554] ====================
I0405 21:19:07.389735 139643526821696 submission_runner.py:613] Final imagenet_resnet score: 4727.7622191905975
