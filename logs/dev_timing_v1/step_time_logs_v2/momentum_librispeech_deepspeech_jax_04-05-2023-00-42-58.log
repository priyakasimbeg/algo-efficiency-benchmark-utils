I0405 00:43:12.122041 139641567868736 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax.
I0405 00:43:12.169891 139641567868736 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 00:43:12.988058 139641567868736 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0405 00:43:12.988659 139641567868736 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 00:43:12.992442 139641567868736 submission_runner.py:511] Using RNG seed 771939093
I0405 00:43:14.311138 139641567868736 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 00:43:14.311329 139641567868736 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1.
I0405 00:43:14.311508 139641567868736 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/hparams.json.
I0405 00:43:14.435317 139641567868736 submission_runner.py:230] Starting train once: RAM USED (GB) 4.372492288
I0405 00:43:14.435483 139641567868736 submission_runner.py:231] Initializing dataset.
I0405 00:43:14.435644 139641567868736 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.372492288
I0405 00:43:14.435702 139641567868736 submission_runner.py:240] Initializing model.
I0405 00:43:29.982082 139641567868736 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.973877248
I0405 00:43:29.982280 139641567868736 submission_runner.py:252] Initializing optimizer.
I0405 00:43:30.543489 139641567868736 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.973901824
I0405 00:43:30.543653 139641567868736 submission_runner.py:261] Initializing metrics bundle.
I0405 00:43:30.543702 139641567868736 submission_runner.py:276] Initializing checkpoint and logger.
I0405 00:43:30.544692 139641567868736 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0405 00:43:30.544977 139641567868736 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 00:43:30.545048 139641567868736 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 00:43:31.705124 139641567868736 submission_runner.py:297] Saving meta data to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0405 00:43:31.706108 139641567868736 submission_runner.py:300] Saving flags to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0405 00:43:31.709279 139641567868736 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.971882496
I0405 00:43:31.709492 139641567868736 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.971882496
I0405 00:43:31.709576 139641567868736 submission_runner.py:313] Starting training loop.
I0405 00:43:31.914155 139641567868736 input_pipeline.py:20] Loading split = train-clean-100
I0405 00:43:31.950481 139641567868736 input_pipeline.py:20] Loading split = train-clean-360
I0405 00:43:32.364525 139641567868736 input_pipeline.py:20] Loading split = train-other-500
I0405 00:43:37.079804 139641567868736 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.634050048
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 00:44:26.700907 139462654818048 logging_writer.py:48] [0] global_step=0, grad_norm=26.418174743652344, loss=32.545936584472656
I0405 00:44:26.714753 139641567868736 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 14.20058624
I0405 00:44:26.715067 139641567868736 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 14.20058624
I0405 00:44:26.715161 139641567868736 spec.py:298] Evaluating on the training split.
I0405 00:44:26.846022 139641567868736 input_pipeline.py:20] Loading split = train-clean-100
I0405 00:44:26.873444 139641567868736 input_pipeline.py:20] Loading split = train-clean-360
I0405 00:44:27.232026 139641567868736 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 00:45:52.308291 139641567868736 spec.py:310] Evaluating on the validation split.
I0405 00:45:52.401141 139641567868736 input_pipeline.py:20] Loading split = dev-clean
I0405 00:45:52.406131 139641567868736 input_pipeline.py:20] Loading split = dev-other
I0405 00:46:45.470413 139641567868736 spec.py:326] Evaluating on the test split.
I0405 00:46:45.567775 139641567868736 input_pipeline.py:20] Loading split = test-clean
I0405 00:47:19.383081 139641567868736 submission_runner.py:382] Time since start: 55.01s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.782244, dtype=float32), 'train/wer': 3.642733690106824, 'validation/ctc_loss': DeviceArray(30.717455, dtype=float32), 'validation/wer': 3.342038996999489, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.91936, dtype=float32), 'test/wer': 3.3901448215627727, 'test/num_examples': 2472}
I0405 00:47:19.384467 139641567868736 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.863477248
I0405 00:47:19.397183 139461178423040 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=54.800871, test/ctc_loss=30.91935920715332, test/num_examples=2472, test/wer=3.390145, total_duration=55.005540, train/ctc_loss=31.782243728637695, train/wer=3.642734, validation/ctc_loss=30.71745491027832, validation/num_examples=5348, validation/wer=3.342039
I0405 00:47:19.491861 139641567868736 checkpoints.py:356] Saving checkpoint at step: 1
I0405 00:47:19.771821 139641567868736 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0405 00:47:19.772604 139641567868736 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0405 00:47:19.776950 139641567868736 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.855670272
I0405 00:47:19.820184 139641567868736 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.854400512
I0405 00:47:35.814854 139641567868736 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 22.280720384
I0405 00:49:30.010491 139465615034112 logging_writer.py:48] [100] global_step=100, grad_norm=7.037108421325684, loss=6.061115741729736
I0405 00:51:24.894191 139465623426816 logging_writer.py:48] [200] global_step=200, grad_norm=6.931432247161865, loss=5.991642951965332
I0405 00:53:20.846875 139465615034112 logging_writer.py:48] [300] global_step=300, grad_norm=7.326358795166016, loss=6.135590553283691
I0405 00:55:16.973501 139465623426816 logging_writer.py:48] [400] global_step=400, grad_norm=3.2312119007110596, loss=5.952646255493164
I0405 00:57:12.330138 139465615034112 logging_writer.py:48] [500] global_step=500, grad_norm=3.356410264968872, loss=5.936681747436523
I0405 00:59:07.167136 139465623426816 logging_writer.py:48] [600] global_step=600, grad_norm=0.3017879128456116, loss=5.742880344390869
I0405 01:01:01.809640 139465615034112 logging_writer.py:48] [700] global_step=700, grad_norm=2.6044137477874756, loss=5.660740375518799
I0405 01:02:57.437948 139465623426816 logging_writer.py:48] [800] global_step=800, grad_norm=0.8755021691322327, loss=5.419949054718018
I0405 01:04:51.975320 139465615034112 logging_writer.py:48] [900] global_step=900, grad_norm=1.0917121171951294, loss=5.032166957855225
I0405 01:06:46.310736 139465623426816 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7993229627609253, loss=4.598657608032227
I0405 01:08:45.513503 139467723790080 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.7152681350708008, loss=4.307968616485596
I0405 01:10:39.759122 139467715397376 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9842578768730164, loss=4.047267436981201
I0405 01:12:33.700527 139467723790080 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0679750442504883, loss=3.8095943927764893
I0405 01:14:28.480855 139467715397376 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2844858169555664, loss=3.685880184173584
I0405 01:16:22.993025 139467723790080 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9574275016784668, loss=3.540184497833252
I0405 01:18:16.969073 139467715397376 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0689843893051147, loss=3.4292900562286377
I0405 01:20:10.917080 139467723790080 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.1497348546981812, loss=3.3877291679382324
I0405 01:22:04.986274 139467715397376 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0485869646072388, loss=3.225008010864258
I0405 01:23:59.240940 139467723790080 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8002769351005554, loss=3.164536952972412
I0405 01:25:53.493644 139467715397376 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2981454133987427, loss=3.138411283493042
I0405 01:27:20.459323 139641567868736 submission_runner.py:373] Before eval at step 2074: RAM USED (GB) 23.614418944
I0405 01:27:20.459848 139641567868736 spec.py:298] Evaluating on the training split.
I0405 01:27:48.951556 139641567868736 spec.py:310] Evaluating on the validation split.
I0405 01:28:25.871163 139641567868736 spec.py:326] Evaluating on the test split.
I0405 01:28:44.196132 139641567868736 submission_runner.py:382] Time since start: 2628.75s, 	Step: 2074, 	{'train/ctc_loss': DeviceArray(5.5533066, dtype=float32), 'train/wer': 0.9365203600337789, 'validation/ctc_loss': DeviceArray(5.8372793, dtype=float32), 'validation/wer': 0.8932454727011355, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.5533276, dtype=float32), 'test/wer': 0.8937095037881096, 'test/num_examples': 2472}
I0405 01:28:44.197910 139641567868736 submission_runner.py:396] After eval at step 2074: RAM USED (GB) 21.759008768
I0405 01:28:44.215331 139465174030080 logging_writer.py:48] [2074] global_step=2074, preemption_count=0, score=2451.955157, test/ctc_loss=5.553327560424805, test/num_examples=2472, test/wer=0.893710, total_duration=2628.747246, train/ctc_loss=5.553306579589844, train/wer=0.936520, validation/ctc_loss=5.837279319763184, validation/num_examples=5348, validation/wer=0.893245
I0405 01:28:44.325350 139641567868736 checkpoints.py:356] Saving checkpoint at step: 2074
I0405 01:28:44.750716 139641567868736 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2074
I0405 01:28:44.761200 139641567868736 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2074.
I0405 01:28:44.765208 139641567868736 submission_runner.py:416] After logging and checkpointing eval at step 2074: RAM USED (GB) 21.736701952
I0405 01:29:15.828634 139465165637376 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8853533267974854, loss=3.0654444694519043
I0405 01:31:11.108892 139465123673856 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9878693222999573, loss=2.976081132888794
I0405 01:33:06.407846 139465165637376 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.094761848449707, loss=2.9457335472106934
I0405 01:35:01.393123 139465123673856 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.990181565284729, loss=2.874066114425659
I0405 01:36:56.686405 139465165637376 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9422867894172668, loss=2.8582348823547363
I0405 01:38:51.812412 139465123673856 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8602226376533508, loss=2.761568546295166
I0405 01:40:47.014704 139465165637376 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9020987153053284, loss=2.813577175140381
I0405 01:42:42.405812 139465123673856 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.5545858144760132, loss=2.801612377166748
I0405 01:44:38.123455 139465165637376 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8694013953208923, loss=2.703951358795166
I0405 01:46:32.830725 139465123673856 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9680708646774292, loss=2.741018295288086
I0405 01:48:32.242894 139467068430080 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.2671160697937012, loss=2.722323179244995
I0405 01:50:28.060068 139467060037376 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8667526841163635, loss=2.676903247833252
I0405 01:52:23.568708 139467068430080 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8476876616477966, loss=2.618844985961914
I0405 01:54:17.963115 139467060037376 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1681770086288452, loss=2.6887123584747314
I0405 01:56:12.108396 139467068430080 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8388317227363586, loss=2.671891450881958
I0405 01:58:06.909433 139467060037376 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1998976469039917, loss=2.65282940864563
I0405 02:00:01.554510 139467068430080 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.922600507736206, loss=2.5976011753082275
I0405 02:01:56.611117 139467060037376 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.2732466459274292, loss=2.622964859008789
I0405 02:03:50.545729 139467068430080 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9864724278450012, loss=2.5903759002685547
I0405 02:05:44.794255 139467060037376 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.0549709796905518, loss=2.608994245529175
I0405 02:07:38.725713 139467068430080 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1848.362548828125
I0405 02:08:44.936222 139641567868736 submission_runner.py:373] Before eval at step 4156: RAM USED (GB) 22.511484928
I0405 02:08:44.936777 139641567868736 spec.py:298] Evaluating on the training split.
I0405 02:09:13.278176 139641567868736 spec.py:310] Evaluating on the validation split.
I0405 02:09:48.175434 139641567868736 spec.py:326] Evaluating on the test split.
I0405 02:10:06.101470 139641567868736 submission_runner.py:382] Time since start: 5113.22s, 	Step: 4156, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 02:10:06.103208 139641567868736 submission_runner.py:396] After eval at step 4156: RAM USED (GB) 21.182304256
I0405 02:10:06.123614 139465399310080 logging_writer.py:48] [4156] global_step=4156, preemption_count=0, score=4848.511174, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5113.224091, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 02:10:06.227863 139641567868736 checkpoints.py:356] Saving checkpoint at step: 4156
I0405 02:10:06.628413 139641567868736 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4156
I0405 02:10:06.639138 139641567868736 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4156.
I0405 02:10:06.642816 139641567868736 submission_runner.py:416] After logging and checkpointing eval at step 4156: RAM USED (GB) 21.156954112
I0405 02:10:57.784172 139465390917376 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1855.1939697265625
I0405 02:12:51.869677 139465340561152 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1806.6212158203125
I0405 02:14:46.236069 139465390917376 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1841.4488525390625
I0405 02:16:39.885588 139465340561152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1832.3543701171875
I0405 02:18:33.412735 139465390917376 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1838.0113525390625
I0405 02:20:28.738630 139465340561152 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1783.83544921875
I0405 02:22:23.268788 139465390917376 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1853.3135986328125
I0405 02:24:17.267783 139465340561152 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1800.648681640625
I0405 02:26:11.133898 139465390917376 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1807.13134765625
I0405 02:28:05.260832 139465340561152 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1801.916015625
I0405 02:30:03.709095 139465399310080 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1809.8143310546875
I0405 02:31:58.666723 139465390917376 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1803.058349609375
I0405 02:33:53.809869 139465399310080 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1793.8353271484375
I0405 02:35:47.352451 139465390917376 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1786.6988525390625
I0405 02:37:41.146096 139465399310080 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1795.0931396484375
I0405 02:39:34.747464 139465390917376 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1865.196533203125
I0405 02:41:28.363142 139465399310080 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1807.6417236328125
I0405 02:43:22.056760 139465390917376 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1836.033935546875
I0405 02:45:15.796325 139465399310080 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1787.3226318359375
I0405 02:47:10.018813 139465390917376 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1900.9298095703125
I0405 02:49:07.480529 139465399310080 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1894.7393798828125
I0405 02:50:07.666872 139641567868736 submission_runner.py:373] Before eval at step 6254: RAM USED (GB) 22.392889344
I0405 02:50:07.667117 139641567868736 spec.py:298] Evaluating on the training split.
I0405 02:50:36.324068 139641567868736 spec.py:310] Evaluating on the validation split.
I0405 02:51:11.241496 139641567868736 spec.py:326] Evaluating on the test split.
I0405 02:51:29.489579 139641567868736 submission_runner.py:382] Time since start: 7595.96s, 	Step: 6254, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 02:51:29.490853 139641567868736 submission_runner.py:396] After eval at step 6254: RAM USED (GB) 21.281898496
I0405 02:51:29.510634 139465399310080 logging_writer.py:48] [6254] global_step=6254, preemption_count=0, score=7245.898451, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7595.955518, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 02:51:29.622722 139641567868736 checkpoints.py:356] Saving checkpoint at step: 6254
I0405 02:51:30.066583 139641567868736 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6254
I0405 02:51:30.077151 139641567868736 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6254.
I0405 02:51:30.081029 139641567868736 submission_runner.py:416] After logging and checkpointing eval at step 6254: RAM USED (GB) 21.260259328
I0405 02:52:23.535195 139465390917376 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1830.52001953125
I0405 02:54:17.476440 139465332168448 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1829.212158203125
I0405 02:56:12.481466 139465390917376 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1780.9810791015625
I0405 02:58:07.278921 139465332168448 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1795.8486328125
I0405 03:00:01.510975 139465390917376 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1823.4791259765625
I0405 03:01:56.908583 139465332168448 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1815.2039794921875
I0405 03:03:51.202470 139465390917376 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1756.6611328125
I0405 03:05:45.239855 139465332168448 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1796.2265625
I0405 03:07:38.837389 139465390917376 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1833.5355224609375
I0405 03:09:32.373033 139465332168448 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1836.4290771484375
I0405 03:11:30.015617 139465399310080 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1783.3385009765625
I0405 03:13:23.911841 139465390917376 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1827.7755126953125
I0405 03:15:18.067747 139465399310080 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1777.3951416015625
I0405 03:17:12.220717 139465390917376 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1817.523681640625
I0405 03:19:06.441105 139465399310080 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1829.473388671875
I0405 03:21:00.225116 139465390917376 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1723.429443359375
I0405 03:22:54.353508 139465399310080 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1777.7655029296875
I0405 03:24:47.284915 139641567868736 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 22.20167168
I0405 03:24:47.285180 139641567868736 spec.py:298] Evaluating on the training split.
I0405 03:25:16.287874 139641567868736 spec.py:310] Evaluating on the validation split.
I0405 03:25:52.338770 139641567868736 spec.py:326] Evaluating on the test split.
I0405 03:26:10.719959 139641567868736 submission_runner.py:382] Time since start: 9675.57s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0405 03:26:10.721231 139641567868736 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 21.376237568
I0405 03:26:10.740317 139467140110080 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9240.096396, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=9675.573135, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0405 03:26:10.846316 139641567868736 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 03:26:11.290501 139641567868736 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 03:26:11.300766 139641567868736 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 03:26:11.304811 139641567868736 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 21.35259136
I0405 03:26:11.311730 139467131717376 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9240.096396
I0405 03:26:11.380616 139641567868736 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 03:26:11.960476 139641567868736 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 03:26:11.970697 139641567868736 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 03:26:13.260698 139641567868736 submission_runner.py:550] Tuning trial 1/1
I0405 03:26:13.260924 139641567868736 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 03:26:13.264666 139641567868736 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.782244, dtype=float32), 'train/wer': 3.642733690106824, 'validation/ctc_loss': DeviceArray(30.717455, dtype=float32), 'validation/wer': 3.342038996999489, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.91936, dtype=float32), 'test/wer': 3.3901448215627727, 'test/num_examples': 2472, 'score': 54.80087113380432, 'total_duration': 55.005539894104004, 'global_step': 1, 'preemption_count': 0}), (2074, {'train/ctc_loss': DeviceArray(5.5533066, dtype=float32), 'train/wer': 0.9365203600337789, 'validation/ctc_loss': DeviceArray(5.8372793, dtype=float32), 'validation/wer': 0.8932454727011355, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.5533276, dtype=float32), 'test/wer': 0.8937095037881096, 'test/num_examples': 2472, 'score': 2451.955156803131, 'total_duration': 2628.747246026993, 'global_step': 2074, 'preemption_count': 0}), (4156, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4848.511173725128, 'total_duration': 5113.224091291428, 'global_step': 4156, 'preemption_count': 0}), (6254, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7245.898451328278, 'total_duration': 7595.955518245697, 'global_step': 6254, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9240.096396446228, 'total_duration': 9675.573135137558, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0405 03:26:13.264817 139641567868736 submission_runner.py:553] Timing: 9240.096396446228
I0405 03:26:13.264872 139641567868736 submission_runner.py:554] ====================
I0405 03:26:13.265218 139641567868736 submission_runner.py:613] Final librispeech_deepspeech score: 9240.096396446228
