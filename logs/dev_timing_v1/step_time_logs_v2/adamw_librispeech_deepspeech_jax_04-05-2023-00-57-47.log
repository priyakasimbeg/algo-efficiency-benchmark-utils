I0405 00:58:01.589732 140700938106688 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax.
I0405 00:58:01.633999 140700938106688 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 00:58:02.547511 140700938106688 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0405 00:58:02.548161 140700938106688 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 00:58:02.551580 140700938106688 submission_runner.py:511] Using RNG seed 2102414192
I0405 00:58:03.865295 140700938106688 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 00:58:03.865477 140700938106688 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1.
I0405 00:58:03.865650 140700938106688 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/hparams.json.
I0405 00:58:03.992662 140700938106688 submission_runner.py:230] Starting train once: RAM USED (GB) 4.374810624
I0405 00:58:03.992827 140700938106688 submission_runner.py:231] Initializing dataset.
I0405 00:58:03.992979 140700938106688 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.374810624
I0405 00:58:03.993040 140700938106688 submission_runner.py:240] Initializing model.
I0405 00:58:19.238929 140700938106688 submission_runner.py:251] After Initializing model: RAM USED (GB) 9.000521728
I0405 00:58:19.239122 140700938106688 submission_runner.py:252] Initializing optimizer.
I0405 00:58:19.903336 140700938106688 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.999829504
I0405 00:58:19.903525 140700938106688 submission_runner.py:261] Initializing metrics bundle.
I0405 00:58:19.903574 140700938106688 submission_runner.py:276] Initializing checkpoint and logger.
I0405 00:58:19.904566 140700938106688 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0405 00:58:19.904823 140700938106688 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 00:58:19.904889 140700938106688 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 00:58:20.938513 140700938106688 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0405 00:58:20.939461 140700938106688 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0405 00:58:20.942821 140700938106688 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.999378944
I0405 00:58:20.943011 140700938106688 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.999378944
I0405 00:58:20.943077 140700938106688 submission_runner.py:313] Starting training loop.
I0405 00:58:21.148400 140700938106688 input_pipeline.py:20] Loading split = train-clean-100
I0405 00:58:21.180362 140700938106688 input_pipeline.py:20] Loading split = train-clean-360
I0405 00:58:21.577075 140700938106688 input_pipeline.py:20] Loading split = train-other-500
I0405 00:58:25.816706 140700938106688 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 10.526334976
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 00:59:17.590491 140525836584704 logging_writer.py:48] [0] global_step=0, grad_norm=22.95441436767578, loss=32.51388931274414
I0405 00:59:17.606701 140700938106688 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 14.3282176
I0405 00:59:17.607048 140700938106688 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 14.3282176
I0405 00:59:17.607186 140700938106688 spec.py:298] Evaluating on the training split.
I0405 00:59:17.737630 140700938106688 input_pipeline.py:20] Loading split = train-clean-100
I0405 00:59:17.766277 140700938106688 input_pipeline.py:20] Loading split = train-clean-360
I0405 00:59:18.105257 140700938106688 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 01:00:40.728925 140700938106688 spec.py:310] Evaluating on the validation split.
I0405 01:00:40.830369 140700938106688 input_pipeline.py:20] Loading split = dev-clean
I0405 01:00:40.835606 140700938106688 input_pipeline.py:20] Loading split = dev-other
I0405 01:01:32.798614 140700938106688 spec.py:326] Evaluating on the test split.
I0405 01:01:32.900913 140700938106688 input_pipeline.py:20] Loading split = test-clean
I0405 01:02:07.116333 140700938106688 submission_runner.py:382] Time since start: 56.66s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.82726, dtype=float32), 'train/wer': 3.826244716073548, 'validation/ctc_loss': DeviceArray(31.091131, dtype=float32), 'validation/wer': 3.489700817181063, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.140835, dtype=float32), 'test/wer': 3.7721446996932952, 'test/num_examples': 2472}
I0405 01:02:07.117306 140700938106688 submission_runner.py:396] After eval at step 1: RAM USED (GB) 22.009008128
I0405 01:02:07.126716 140522437998336 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=56.458560, test/ctc_loss=31.14083480834961, test/num_examples=2472, test/wer=3.772145, total_duration=56.663953, train/ctc_loss=31.827259063720703, train/wer=3.826245, validation/ctc_loss=31.09113121032715, validation/num_examples=5348, validation/wer=3.489701
I0405 01:02:07.274041 140700938106688 checkpoints.py:356] Saving checkpoint at step: 1
I0405 01:02:07.696042 140700938106688 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0405 01:02:07.696952 140700938106688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0405 01:02:07.703905 140700938106688 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.997187072
I0405 01:02:07.748137 140700938106688 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.9942912
I0405 01:02:25.980856 140700938106688 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 22.410170368
I0405 01:04:18.835968 140526332483328 logging_writer.py:48] [100] global_step=100, grad_norm=4.683817386627197, loss=7.82145881652832
I0405 01:06:13.281492 140526340876032 logging_writer.py:48] [200] global_step=200, grad_norm=1.5540045499801636, loss=6.279114246368408
I0405 01:08:08.751072 140526332483328 logging_writer.py:48] [300] global_step=300, grad_norm=0.9050363898277283, loss=5.893716812133789
I0405 01:10:05.478914 140526340876032 logging_writer.py:48] [400] global_step=400, grad_norm=0.4889046847820282, loss=5.842034816741943
I0405 01:12:02.399017 140526332483328 logging_writer.py:48] [500] global_step=500, grad_norm=1.1163588762283325, loss=5.806083679199219
I0405 01:13:57.824414 140526340876032 logging_writer.py:48] [600] global_step=600, grad_norm=0.49540573358535767, loss=5.715052127838135
I0405 01:15:52.746423 140526332483328 logging_writer.py:48] [700] global_step=700, grad_norm=0.7511297464370728, loss=5.603067874908447
I0405 01:17:47.532130 140526340876032 logging_writer.py:48] [800] global_step=800, grad_norm=1.6180533170700073, loss=5.506160259246826
I0405 01:19:41.607530 140526332483328 logging_writer.py:48] [900] global_step=900, grad_norm=0.6407737731933594, loss=5.391927242279053
I0405 01:21:35.563524 140526340876032 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3331514596939087, loss=5.135331630706787
I0405 01:23:32.703719 140526500337408 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2288293838500977, loss=4.69241189956665
I0405 01:25:26.794744 140526491944704 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.9816904067993164, loss=4.383416175842285
I0405 01:27:20.679625 140526500337408 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.9398915767669678, loss=4.112273693084717
I0405 01:29:14.355078 140526491944704 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.2698304653167725, loss=3.911353349685669
I0405 01:31:08.994663 140526500337408 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.2717766761779785, loss=3.6888139247894287
I0405 01:33:03.321266 140526491944704 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.5919036865234375, loss=3.5408761501312256
I0405 01:34:57.321259 140526500337408 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.248570203781128, loss=3.4972269535064697
I0405 01:36:51.245227 140526491944704 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.9084054231643677, loss=3.310938596725464
I0405 01:38:44.587680 140526500337408 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.955321192741394, loss=3.1851725578308105
I0405 01:40:37.977803 140526491944704 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.875258445739746, loss=3.133774757385254
I0405 01:42:08.289396 140700938106688 submission_runner.py:373] Before eval at step 2078: RAM USED (GB) 23.721209856
I0405 01:42:08.289924 140700938106688 spec.py:298] Evaluating on the training split.
I0405 01:42:37.604892 140700938106688 spec.py:310] Evaluating on the validation split.
I0405 01:43:11.807917 140700938106688 spec.py:326] Evaluating on the test split.
I0405 01:43:29.353446 140700938106688 submission_runner.py:382] Time since start: 2627.34s, 	Step: 2078, 	{'train/ctc_loss': DeviceArray(5.784349, dtype=float32), 'train/wer': 0.9171644250375893, 'validation/ctc_loss': DeviceArray(5.7499228, dtype=float32), 'validation/wer': 0.8776543912628197, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6213746, dtype=float32), 'test/wer': 0.877561797981029, 'test/num_examples': 2472}
I0405 01:43:29.354649 140700938106688 submission_runner.py:396] After eval at step 2078: RAM USED (GB) 21.7160704
I0405 01:43:29.371000 140525701154560 logging_writer.py:48] [2078] global_step=2078, preemption_count=0, score=2452.743361, test/ctc_loss=5.621374607086182, test/num_examples=2472, test/wer=0.877562, total_duration=2627.344127, train/ctc_loss=5.784348964691162, train/wer=0.917164, validation/ctc_loss=5.749922752380371, validation/num_examples=5348, validation/wer=0.877654
I0405 01:43:29.531608 140700938106688 checkpoints.py:356] Saving checkpoint at step: 2078
I0405 01:43:30.113647 140700938106688 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2078
I0405 01:43:30.128988 140700938106688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_2078.
I0405 01:43:30.135761 140700938106688 submission_runner.py:416] After logging and checkpointing eval at step 2078: RAM USED (GB) 21.694291968
I0405 01:43:56.670239 140525692761856 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.2686901092529297, loss=3.073878526687622
I0405 01:45:51.675498 140524115719936 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.157198905944824, loss=3.011279344558716
I0405 01:47:45.056424 140525692761856 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.391690254211426, loss=2.866856098175049
I0405 01:49:40.713790 140524115719936 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.2206637859344482, loss=2.84340238571167
I0405 01:51:34.756012 140525692761856 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.970759391784668, loss=2.786174774169922
I0405 01:53:28.458319 140524115719936 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.1051042079925537, loss=2.7341480255126953
I0405 01:55:22.390650 140525692761856 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.958343029022217, loss=2.665616273880005
I0405 01:57:15.637866 140524115719936 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.4384264945983887, loss=2.62204909324646
I0405 01:59:09.278119 140525692761856 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.6360421180725098, loss=2.6083664894104004
I0405 02:01:03.479954 140524115719936 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.595573902130127, loss=2.602335214614868
I0405 02:03:00.873413 140525701154560 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.537323474884033, loss=2.5243375301361084
I0405 02:04:55.773656 140525692761856 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.354194402694702, loss=2.5393989086151123
I0405 02:06:49.876947 140525701154560 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.478307008743286, loss=2.4834229946136475
I0405 02:08:43.782572 140525692761856 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.932093620300293, loss=2.4119741916656494
I0405 02:10:37.747054 140525701154560 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.88496732711792, loss=2.3917877674102783
I0405 02:12:32.356617 140525692761856 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.2990691661834717, loss=2.309451103210449
I0405 02:14:25.331819 140525701154560 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.887009620666504, loss=2.4012837409973145
I0405 02:16:18.296669 140525692761856 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.425793170928955, loss=2.3484480381011963
I0405 02:18:11.507381 140525701154560 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.3669748306274414, loss=2.3080012798309326
I0405 02:20:06.125870 140525692761856 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.1137797832489014, loss=2.3037326335906982
I0405 02:21:59.564882 140525701154560 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.6831440925598145, loss=2.2211039066314697
I0405 02:23:30.480044 140700938106688 submission_runner.py:373] Before eval at step 4178: RAM USED (GB) 22.677082112
I0405 02:23:30.480514 140700938106688 spec.py:298] Evaluating on the training split.
I0405 02:24:06.983485 140700938106688 spec.py:310] Evaluating on the validation split.
I0405 02:24:43.896529 140700938106688 spec.py:326] Evaluating on the test split.
I0405 02:25:03.090280 140700938106688 submission_runner.py:382] Time since start: 5109.54s, 	Step: 4178, 	{'train/ctc_loss': DeviceArray(1.5516946, dtype=float32), 'train/wer': 0.4137016221399462, 'validation/ctc_loss': DeviceArray(2.0053608, dtype=float32), 'validation/wer': 0.4713021833302781, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.5424823, dtype=float32), 'test/wer': 0.400970893506388, 'test/num_examples': 2472}
I0405 02:25:03.091815 140700938106688 submission_runner.py:396] After eval at step 4178: RAM USED (GB) 21.583310848
I0405 02:25:03.113348 140525701154560 logging_writer.py:48] [4178] global_step=4178, preemption_count=0, score=4848.560870, test/ctc_loss=1.5424822568893433, test/num_examples=2472, test/wer=0.400971, total_duration=5109.535431, train/ctc_loss=1.551694631576538, train/wer=0.413702, validation/ctc_loss=2.0053608417510986, validation/num_examples=5348, validation/wer=0.471302
I0405 02:25:03.271631 140700938106688 checkpoints.py:356] Saving checkpoint at step: 4178
I0405 02:25:03.952868 140700938106688 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4178
I0405 02:25:03.968470 140700938106688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_4178.
I0405 02:25:03.971782 140700938106688 submission_runner.py:416] After logging and checkpointing eval at step 4178: RAM USED (GB) 21.642461184
I0405 02:25:30.220782 140525692761856 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.2567179203033447, loss=2.2360782623291016
I0405 02:27:24.030076 140524107327232 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.8290624618530273, loss=2.2262959480285645
I0405 02:29:17.273501 140525692761856 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.3227925300598145, loss=2.1538469791412354
I0405 02:31:10.387211 140524107327232 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.560133934020996, loss=2.194460153579712
I0405 02:33:03.328192 140525692761856 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.677682399749756, loss=2.1969058513641357
I0405 02:34:57.181113 140524107327232 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.3953471183776855, loss=2.2028677463531494
I0405 02:36:51.336225 140525692761856 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.420628786087036, loss=2.0460314750671387
I0405 02:38:44.624426 140524107327232 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.2816977500915527, loss=2.152409553527832
I0405 02:40:37.908604 140525692761856 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.3659937381744385, loss=2.1556153297424316
I0405 02:42:32.568341 140524107327232 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.2182445526123047, loss=2.0547335147857666
I0405 02:44:31.947710 140525701154560 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.7806780338287354, loss=2.122007131576538
I0405 02:46:26.242478 140525692761856 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.4064407348632812, loss=2.06898832321167
I0405 02:48:19.373390 140525701154560 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.9139766693115234, loss=2.1151468753814697
I0405 02:50:12.679888 140525692761856 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.121811389923096, loss=2.062325954437256
I0405 02:52:05.814631 140525701154560 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.179692268371582, loss=2.05702805519104
I0405 02:53:59.048470 140525692761856 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.281740188598633, loss=2.06866192817688
I0405 02:55:52.309660 140525701154560 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.840324878692627, loss=2.005890130996704
I0405 02:57:46.017768 140525692761856 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.9727988243103027, loss=2.0071120262145996
I0405 02:59:39.111364 140525701154560 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.9996345043182373, loss=1.9656825065612793
I0405 03:01:32.202237 140525692761856 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.4687130451202393, loss=2.016932487487793
I0405 03:03:29.124733 140525701154560 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.366607904434204, loss=1.967555046081543
I0405 03:05:04.356033 140700938106688 submission_runner.py:373] Before eval at step 6285: RAM USED (GB) 22.899986432
I0405 03:05:04.356251 140700938106688 spec.py:298] Evaluating on the training split.
I0405 03:05:42.038946 140700938106688 spec.py:310] Evaluating on the validation split.
I0405 03:06:20.443998 140700938106688 spec.py:326] Evaluating on the test split.
I0405 03:06:40.545595 140700938106688 submission_runner.py:382] Time since start: 7603.41s, 	Step: 6285, 	{'train/ctc_loss': DeviceArray(0.7152029, dtype=float32), 'train/wer': 0.24517292091655202, 'validation/ctc_loss': DeviceArray(1.1778822, dtype=float32), 'validation/wer': 0.32511649895319783, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.80152214, dtype=float32), 'test/wer': 0.2536916295980338, 'test/num_examples': 2472}
I0405 03:06:40.546911 140700938106688 submission_runner.py:396] After eval at step 6285: RAM USED (GB) 22.558109696
I0405 03:06:40.566284 140525701154560 logging_writer.py:48] [6285] global_step=6285, preemption_count=0, score=7244.477826, test/ctc_loss=0.8015221357345581, test/num_examples=2472, test/wer=0.253692, total_duration=7603.411530, train/ctc_loss=0.7152029275894165, train/wer=0.245173, validation/ctc_loss=1.177882194519043, validation/num_examples=5348, validation/wer=0.325116
I0405 03:06:40.714708 140700938106688 checkpoints.py:356] Saving checkpoint at step: 6285
I0405 03:06:41.328978 140700938106688 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6285
I0405 03:06:41.344319 140700938106688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_6285.
I0405 03:06:41.347160 140700938106688 submission_runner.py:416] After logging and checkpointing eval at step 6285: RAM USED (GB) 22.580047872
I0405 03:06:59.855921 140525692761856 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.1939961910247803, loss=1.9905339479446411
I0405 03:08:55.656730 140524098934528 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.5982325077056885, loss=2.02193021774292
I0405 03:10:51.855198 140525692761856 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.3445427417755127, loss=2.015488624572754
I0405 03:12:47.378946 140524098934528 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.566840410232544, loss=1.894602656364441
I0405 03:14:42.777690 140525692761856 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.8664205074310303, loss=1.98361074924469
I0405 03:16:38.175089 140524098934528 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.6144661903381348, loss=1.954332709312439
I0405 03:18:32.278196 140525692761856 logging_writer.py:48] [6900] global_step=6900, grad_norm=7.7257280349731445, loss=1.9351271390914917
I0405 03:20:26.620836 140524098934528 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.4662020206451416, loss=1.9596948623657227
I0405 03:22:20.631798 140525692761856 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.2732791900634766, loss=1.9439278841018677
I0405 03:24:14.456389 140524098934528 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.271463394165039, loss=1.9963791370391846
I0405 03:26:12.767676 140525701154560 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.438871145248413, loss=1.9802976846694946
I0405 03:28:07.293997 140525692761856 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.4081029891967773, loss=1.9366801977157593
I0405 03:30:01.376856 140525701154560 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.4722023010253906, loss=1.8789124488830566
I0405 03:31:56.447557 140525692761856 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.4003593921661377, loss=1.893946886062622
I0405 03:33:51.693079 140525701154560 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.619173765182495, loss=1.8672921657562256
I0405 03:35:44.735678 140525692761856 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.051778793334961, loss=1.875005841255188
I0405 03:37:37.791503 140525701154560 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.6781530380249023, loss=1.9080201387405396
I0405 03:39:29.811470 140700938106688 submission_runner.py:373] Before eval at step 8000: RAM USED (GB) 23.437193216
I0405 03:39:29.811706 140700938106688 spec.py:298] Evaluating on the training split.
I0405 03:40:07.850671 140700938106688 spec.py:310] Evaluating on the validation split.
I0405 03:40:45.977722 140700938106688 spec.py:326] Evaluating on the test split.
I0405 03:41:05.043271 140700938106688 submission_runner.py:382] Time since start: 9668.87s, 	Step: 8000, 	{'train/ctc_loss': DeviceArray(0.5906045, dtype=float32), 'train/wer': 0.1973839918734095, 'validation/ctc_loss': DeviceArray(0.9744539, dtype=float32), 'validation/wer': 0.2718598346341981, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.63128304, dtype=float32), 'test/wer': 0.20333922369142649, 'test/num_examples': 2472}
I0405 03:41:05.044633 140700938106688 submission_runner.py:396] After eval at step 8000: RAM USED (GB) 22.203322368
I0405 03:41:05.064197 140525916657408 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9209.302333, test/ctc_loss=0.6312830448150635, test/num_examples=2472, test/wer=0.203339, total_duration=9668.866300, train/ctc_loss=0.5906044840812683, train/wer=0.197384, validation/ctc_loss=0.9744539260864258, validation/num_examples=5348, validation/wer=0.271860
I0405 03:41:05.218511 140700938106688 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 03:41:05.815535 140700938106688 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 03:41:05.831120 140700938106688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 03:41:05.834128 140700938106688 submission_runner.py:416] After logging and checkpointing eval at step 8000: RAM USED (GB) 22.233468928
I0405 03:41:05.841430 140525908264704 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=9209.302333
I0405 03:41:05.925235 140700938106688 checkpoints.py:356] Saving checkpoint at step: 8000
I0405 03:41:06.791273 140700938106688 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000
I0405 03:41:06.806989 140700938106688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_deepspeech_jax/trial_1/checkpoint_8000.
I0405 03:41:08.147359 140700938106688 submission_runner.py:550] Tuning trial 1/1
I0405 03:41:08.147571 140700938106688 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 03:41:08.152165 140700938106688 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.82726, dtype=float32), 'train/wer': 3.826244716073548, 'validation/ctc_loss': DeviceArray(31.091131, dtype=float32), 'validation/wer': 3.489700817181063, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.140835, dtype=float32), 'test/wer': 3.7721446996932952, 'test/num_examples': 2472, 'score': 56.45856046676636, 'total_duration': 56.663952589035034, 'global_step': 1, 'preemption_count': 0}), (2078, {'train/ctc_loss': DeviceArray(5.784349, dtype=float32), 'train/wer': 0.9171644250375893, 'validation/ctc_loss': DeviceArray(5.7499228, dtype=float32), 'validation/wer': 0.8776543912628197, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6213746, dtype=float32), 'test/wer': 0.877561797981029, 'test/num_examples': 2472, 'score': 2452.7433614730835, 'total_duration': 2627.3441269397736, 'global_step': 2078, 'preemption_count': 0}), (4178, {'train/ctc_loss': DeviceArray(1.5516946, dtype=float32), 'train/wer': 0.4137016221399462, 'validation/ctc_loss': DeviceArray(2.0053608, dtype=float32), 'validation/wer': 0.4713021833302781, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.5424823, dtype=float32), 'test/wer': 0.400970893506388, 'test/num_examples': 2472, 'score': 4848.560869693756, 'total_duration': 5109.535431146622, 'global_step': 4178, 'preemption_count': 0}), (6285, {'train/ctc_loss': DeviceArray(0.7152029, dtype=float32), 'train/wer': 0.24517292091655202, 'validation/ctc_loss': DeviceArray(1.1778822, dtype=float32), 'validation/wer': 0.32511649895319783, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.80152214, dtype=float32), 'test/wer': 0.2536916295980338, 'test/num_examples': 2472, 'score': 7244.477825641632, 'total_duration': 7603.411529541016, 'global_step': 6285, 'preemption_count': 0}), (8000, {'train/ctc_loss': DeviceArray(0.5906045, dtype=float32), 'train/wer': 0.1973839918734095, 'validation/ctc_loss': DeviceArray(0.9744539, dtype=float32), 'validation/wer': 0.2718598346341981, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.63128304, dtype=float32), 'test/wer': 0.20333922369142649, 'test/num_examples': 2472, 'score': 9209.302332878113, 'total_duration': 9668.866299629211, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0405 03:41:08.152296 140700938106688 submission_runner.py:553] Timing: 9209.302332878113
I0405 03:41:08.152350 140700938106688 submission_runner.py:554] ====================
I0405 03:41:08.152708 140700938106688 submission_runner.py:613] Final librispeech_deepspeech score: 9209.302332878113
