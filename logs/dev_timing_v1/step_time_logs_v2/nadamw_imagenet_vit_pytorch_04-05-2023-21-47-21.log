WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 21:47:43.486893 140456207234880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 21:47:43.486931 140414331557696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 21:47:43.486954 140499121784640 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 21:47:43.488035 140327832201024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 21:47:43.488635 139759451625280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 21:47:44.473413 140629920233280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 21:47:44.473430 140616655038272 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 21:47:44.481272 139816912303936 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 21:47:44.481575 139816912303936 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.484044 140629920233280 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.484077 140616655038272 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.490512 140456207234880 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.490542 140414331557696 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.490565 140499121784640 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.490688 140327832201024 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:44.490686 139759451625280 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:47:46.535862 139816912303936 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch.
W0405 21:47:46.571820 139816912303936 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:47:46.572584 140414331557696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:47:46.573081 140499121784640 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:47:46.573584 140456207234880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:47:46.574028 140327832201024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:47:46.574112 140629920233280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 21:47:46.577096 139816912303936 submission_runner.py:511] Using RNG seed 1448633115
I0405 21:47:46.578217 139816912303936 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 21:47:46.578355 139816912303936 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1.
I0405 21:47:46.578553 139816912303936 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0405 21:47:46.579970 139816912303936 submission_runner.py:230] Starting train once: RAM USED (GB) 5.639725056
I0405 21:47:46.580084 139816912303936 submission_runner.py:231] Initializing dataset.
W0405 21:47:46.599973 139759451625280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:47:46.603585 140616655038272 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 21:47:50.979879 139816912303936 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.954817024
I0405 21:47:50.980086 139816912303936 submission_runner.py:240] Initializing model.
I0405 21:47:55.264647 139816912303936 submission_runner.py:251] After Initializing model: RAM USED (GB) 17.978802176
I0405 21:47:55.264878 139816912303936 submission_runner.py:252] Initializing optimizer.
I0405 21:47:55.266200 139816912303936 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 17.978802176
I0405 21:47:55.266328 139816912303936 submission_runner.py:261] Initializing metrics bundle.
I0405 21:47:55.266373 139816912303936 submission_runner.py:276] Initializing checkpoint and logger.
I0405 21:47:55.988508 139816912303936 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0405 21:47:55.990310 139816912303936 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0405 21:47:56.034036 139816912303936 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.032697344
I0405 21:47:56.035318 139816912303936 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.032693248
I0405 21:47:56.035449 139816912303936 submission_runner.py:313] Starting training loop.
I0405 21:47:58.779601 139816912303936 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.4515456
I0405 21:48:02.606846 139788099237632 logging_writer.py:48] [0] global_step=0, grad_norm=0.339542, loss=6.907756
I0405 21:48:02.618212 139816912303936 submission.py:296] 0) loss = 6.908, grad_norm = 0.340
I0405 21:48:02.619038 139816912303936 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.749267456
I0405 21:48:02.619651 139816912303936 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.749259264
I0405 21:48:02.619786 139816912303936 spec.py:298] Evaluating on the training split.
I0405 21:48:51.463603 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 21:49:35.155439 139816912303936 spec.py:326] Evaluating on the test split.
I0405 21:49:35.170813 139816912303936 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:49:35.176874 139816912303936 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 21:49:35.257758 139816912303936 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:49:48.409621 139816912303936 submission_runner.py:382] Time since start: 6.58s, 	Step: 1, 	{'train/accuracy': 0.00203125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00278, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0022, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0405 21:49:48.410736 139816912303936 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.550025216
I0405 21:49:48.421721 139783066089216 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.582731, test/accuracy=0.002200, test/loss=6.907755, test/num_examples=10000, total_duration=6.584921, train/accuracy=0.002031, train/loss=6.907756, validation/accuracy=0.002780, validation/loss=6.907756, validation/num_examples=50000
I0405 21:49:48.844689 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0405 21:49:48.845372 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.551749632
I0405 21:49:48.860562 139816912303936 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.551467008
I0405 21:49:48.868164 139816912303936 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868059 140456207234880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868057 140327832201024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868059 140499121784640 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868054 140629920233280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868047 140414331557696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868098 140616655038272 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:48.868103 139759451625280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:49:49.434085 139783057696512 logging_writer.py:48] [1] global_step=1, grad_norm=0.347554, loss=6.907756
I0405 21:49:49.437836 139816912303936 submission.py:296] 1) loss = 6.908, grad_norm = 0.348
I0405 21:49:49.438795 139816912303936 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.586115072
I0405 21:49:49.870876 139783066089216 logging_writer.py:48] [2] global_step=2, grad_norm=0.357514, loss=6.907754
I0405 21:49:49.874585 139816912303936 submission.py:296] 2) loss = 6.908, grad_norm = 0.358
I0405 21:49:50.286152 139783057696512 logging_writer.py:48] [3] global_step=3, grad_norm=0.344365, loss=6.907751
I0405 21:49:50.289883 139816912303936 submission.py:296] 3) loss = 6.908, grad_norm = 0.344
I0405 21:49:50.694584 139783066089216 logging_writer.py:48] [4] global_step=4, grad_norm=0.338899, loss=6.907750
I0405 21:49:50.698233 139816912303936 submission.py:296] 4) loss = 6.908, grad_norm = 0.339
I0405 21:49:51.111777 139783057696512 logging_writer.py:48] [5] global_step=5, grad_norm=0.340172, loss=6.907751
I0405 21:49:51.115718 139816912303936 submission.py:296] 5) loss = 6.908, grad_norm = 0.340
I0405 21:49:51.523102 139783066089216 logging_writer.py:48] [6] global_step=6, grad_norm=0.349647, loss=6.907732
I0405 21:49:51.526820 139816912303936 submission.py:296] 6) loss = 6.908, grad_norm = 0.350
I0405 21:49:51.937483 139783057696512 logging_writer.py:48] [7] global_step=7, grad_norm=0.343157, loss=6.907739
I0405 21:49:51.941300 139816912303936 submission.py:296] 7) loss = 6.908, grad_norm = 0.343
I0405 21:49:52.369663 139783066089216 logging_writer.py:48] [8] global_step=8, grad_norm=0.348792, loss=6.907736
I0405 21:49:52.375569 139816912303936 submission.py:296] 8) loss = 6.908, grad_norm = 0.349
I0405 21:49:52.787375 139783057696512 logging_writer.py:48] [9] global_step=9, grad_norm=0.353475, loss=6.907728
I0405 21:49:52.790903 139816912303936 submission.py:296] 9) loss = 6.908, grad_norm = 0.353
I0405 21:49:53.205662 139783066089216 logging_writer.py:48] [10] global_step=10, grad_norm=0.347850, loss=6.907741
I0405 21:49:53.209696 139816912303936 submission.py:296] 10) loss = 6.908, grad_norm = 0.348
I0405 21:49:53.619090 139783057696512 logging_writer.py:48] [11] global_step=11, grad_norm=0.341172, loss=6.907709
I0405 21:49:53.622583 139816912303936 submission.py:296] 11) loss = 6.908, grad_norm = 0.341
I0405 21:49:54.039240 139783066089216 logging_writer.py:48] [12] global_step=12, grad_norm=0.341131, loss=6.907720
I0405 21:49:54.042882 139816912303936 submission.py:296] 12) loss = 6.908, grad_norm = 0.341
I0405 21:49:54.448421 139783057696512 logging_writer.py:48] [13] global_step=13, grad_norm=0.348001, loss=6.907673
I0405 21:49:54.452736 139816912303936 submission.py:296] 13) loss = 6.908, grad_norm = 0.348
I0405 21:49:54.861874 139783066089216 logging_writer.py:48] [14] global_step=14, grad_norm=0.350093, loss=6.907728
I0405 21:49:54.868698 139816912303936 submission.py:296] 14) loss = 6.908, grad_norm = 0.350
I0405 21:49:55.280225 139783057696512 logging_writer.py:48] [15] global_step=15, grad_norm=0.340327, loss=6.907671
I0405 21:49:55.284413 139816912303936 submission.py:296] 15) loss = 6.908, grad_norm = 0.340
I0405 21:49:55.712143 139783066089216 logging_writer.py:48] [16] global_step=16, grad_norm=0.331899, loss=6.907652
I0405 21:49:55.716571 139816912303936 submission.py:296] 16) loss = 6.908, grad_norm = 0.332
I0405 21:49:56.132765 139783057696512 logging_writer.py:48] [17] global_step=17, grad_norm=0.347519, loss=6.907650
I0405 21:49:56.136834 139816912303936 submission.py:296] 17) loss = 6.908, grad_norm = 0.348
I0405 21:49:56.560600 139783066089216 logging_writer.py:48] [18] global_step=18, grad_norm=0.344305, loss=6.907612
I0405 21:49:56.564162 139816912303936 submission.py:296] 18) loss = 6.908, grad_norm = 0.344
I0405 21:49:56.976419 139783057696512 logging_writer.py:48] [19] global_step=19, grad_norm=0.345968, loss=6.907630
I0405 21:49:56.981272 139816912303936 submission.py:296] 19) loss = 6.908, grad_norm = 0.346
I0405 21:49:57.391616 139783066089216 logging_writer.py:48] [20] global_step=20, grad_norm=0.346088, loss=6.907569
I0405 21:49:57.396477 139816912303936 submission.py:296] 20) loss = 6.908, grad_norm = 0.346
I0405 21:49:57.804400 139783057696512 logging_writer.py:48] [21] global_step=21, grad_norm=0.335656, loss=6.907499
I0405 21:49:57.808120 139816912303936 submission.py:296] 21) loss = 6.907, grad_norm = 0.336
I0405 21:49:58.221612 139783066089216 logging_writer.py:48] [22] global_step=22, grad_norm=0.352855, loss=6.907604
I0405 21:49:58.225676 139816912303936 submission.py:296] 22) loss = 6.908, grad_norm = 0.353
I0405 21:49:58.634247 139783057696512 logging_writer.py:48] [23] global_step=23, grad_norm=0.333621, loss=6.907571
I0405 21:49:58.637841 139816912303936 submission.py:296] 23) loss = 6.908, grad_norm = 0.334
I0405 21:49:59.054533 139783066089216 logging_writer.py:48] [24] global_step=24, grad_norm=0.352572, loss=6.907481
I0405 21:49:59.064793 139816912303936 submission.py:296] 24) loss = 6.907, grad_norm = 0.353
I0405 21:49:59.498476 139783057696512 logging_writer.py:48] [25] global_step=25, grad_norm=0.350709, loss=6.907368
I0405 21:49:59.502780 139816912303936 submission.py:296] 25) loss = 6.907, grad_norm = 0.351
I0405 21:49:59.911715 139783066089216 logging_writer.py:48] [26] global_step=26, grad_norm=0.355347, loss=6.907500
I0405 21:49:59.915570 139816912303936 submission.py:296] 26) loss = 6.908, grad_norm = 0.355
I0405 21:50:00.333844 139783057696512 logging_writer.py:48] [27] global_step=27, grad_norm=0.349774, loss=6.907428
I0405 21:50:00.337352 139816912303936 submission.py:296] 27) loss = 6.907, grad_norm = 0.350
I0405 21:50:00.767331 139783066089216 logging_writer.py:48] [28] global_step=28, grad_norm=0.364287, loss=6.907336
I0405 21:50:00.771641 139816912303936 submission.py:296] 28) loss = 6.907, grad_norm = 0.364
I0405 21:50:01.191102 139783057696512 logging_writer.py:48] [29] global_step=29, grad_norm=0.355986, loss=6.907308
I0405 21:50:01.194777 139816912303936 submission.py:296] 29) loss = 6.907, grad_norm = 0.356
I0405 21:50:01.615439 139783066089216 logging_writer.py:48] [30] global_step=30, grad_norm=0.354821, loss=6.907196
I0405 21:50:01.619042 139816912303936 submission.py:296] 30) loss = 6.907, grad_norm = 0.355
I0405 21:50:02.035135 139783057696512 logging_writer.py:48] [31] global_step=31, grad_norm=0.351599, loss=6.907382
I0405 21:50:02.038672 139816912303936 submission.py:296] 31) loss = 6.907, grad_norm = 0.352
I0405 21:50:02.480991 139783066089216 logging_writer.py:48] [32] global_step=32, grad_norm=0.346813, loss=6.907354
I0405 21:50:02.484616 139816912303936 submission.py:296] 32) loss = 6.907, grad_norm = 0.347
I0405 21:50:02.893962 139783057696512 logging_writer.py:48] [33] global_step=33, grad_norm=0.353229, loss=6.907171
I0405 21:50:02.900948 139816912303936 submission.py:296] 33) loss = 6.907, grad_norm = 0.353
I0405 21:50:03.309101 139783066089216 logging_writer.py:48] [34] global_step=34, grad_norm=0.334406, loss=6.907270
I0405 21:50:03.313371 139816912303936 submission.py:296] 34) loss = 6.907, grad_norm = 0.334
I0405 21:50:03.728736 139783057696512 logging_writer.py:48] [35] global_step=35, grad_norm=0.360029, loss=6.907026
I0405 21:50:03.734299 139816912303936 submission.py:296] 35) loss = 6.907, grad_norm = 0.360
I0405 21:50:04.156024 139783066089216 logging_writer.py:48] [36] global_step=36, grad_norm=0.360626, loss=6.907053
I0405 21:50:04.160383 139816912303936 submission.py:296] 36) loss = 6.907, grad_norm = 0.361
I0405 21:50:04.579524 139783057696512 logging_writer.py:48] [37] global_step=37, grad_norm=0.357196, loss=6.907233
I0405 21:50:04.584615 139816912303936 submission.py:296] 37) loss = 6.907, grad_norm = 0.357
I0405 21:50:05.011306 139783066089216 logging_writer.py:48] [38] global_step=38, grad_norm=0.350589, loss=6.906944
I0405 21:50:05.015460 139816912303936 submission.py:296] 38) loss = 6.907, grad_norm = 0.351
I0405 21:50:05.456232 139783057696512 logging_writer.py:48] [39] global_step=39, grad_norm=0.371542, loss=6.906693
I0405 21:50:05.460590 139816912303936 submission.py:296] 39) loss = 6.907, grad_norm = 0.372
I0405 21:50:05.883714 139783066089216 logging_writer.py:48] [40] global_step=40, grad_norm=0.358920, loss=6.906686
I0405 21:50:05.888134 139816912303936 submission.py:296] 40) loss = 6.907, grad_norm = 0.359
I0405 21:50:06.305942 139783057696512 logging_writer.py:48] [41] global_step=41, grad_norm=0.352851, loss=6.906735
I0405 21:50:06.310061 139816912303936 submission.py:296] 41) loss = 6.907, grad_norm = 0.353
I0405 21:50:06.728949 139783066089216 logging_writer.py:48] [42] global_step=42, grad_norm=0.363753, loss=6.906629
I0405 21:50:06.732567 139816912303936 submission.py:296] 42) loss = 6.907, grad_norm = 0.364
I0405 21:50:07.160670 139783057696512 logging_writer.py:48] [43] global_step=43, grad_norm=0.355482, loss=6.906634
I0405 21:50:07.165230 139816912303936 submission.py:296] 43) loss = 6.907, grad_norm = 0.355
I0405 21:50:07.588345 139783066089216 logging_writer.py:48] [44] global_step=44, grad_norm=0.363727, loss=6.906358
I0405 21:50:07.592148 139816912303936 submission.py:296] 44) loss = 6.906, grad_norm = 0.364
I0405 21:50:08.018265 139783057696512 logging_writer.py:48] [45] global_step=45, grad_norm=0.363094, loss=6.906536
I0405 21:50:08.023215 139816912303936 submission.py:296] 45) loss = 6.907, grad_norm = 0.363
I0405 21:50:08.433041 139783066089216 logging_writer.py:48] [46] global_step=46, grad_norm=0.369112, loss=6.906153
I0405 21:50:08.436682 139816912303936 submission.py:296] 46) loss = 6.906, grad_norm = 0.369
I0405 21:50:08.859657 139783057696512 logging_writer.py:48] [47] global_step=47, grad_norm=0.363028, loss=6.906039
I0405 21:50:08.863526 139816912303936 submission.py:296] 47) loss = 6.906, grad_norm = 0.363
I0405 21:50:09.288840 139783066089216 logging_writer.py:48] [48] global_step=48, grad_norm=0.375105, loss=6.905886
I0405 21:50:09.292657 139816912303936 submission.py:296] 48) loss = 6.906, grad_norm = 0.375
I0405 21:50:09.702296 139783057696512 logging_writer.py:48] [49] global_step=49, grad_norm=0.360214, loss=6.906363
I0405 21:50:09.705718 139816912303936 submission.py:296] 49) loss = 6.906, grad_norm = 0.360
I0405 21:50:10.123862 139783066089216 logging_writer.py:48] [50] global_step=50, grad_norm=0.392345, loss=6.905579
I0405 21:50:10.127577 139816912303936 submission.py:296] 50) loss = 6.906, grad_norm = 0.392
I0405 21:50:10.534103 139783057696512 logging_writer.py:48] [51] global_step=51, grad_norm=0.384515, loss=6.905601
I0405 21:50:10.538793 139816912303936 submission.py:296] 51) loss = 6.906, grad_norm = 0.385
I0405 21:50:10.947399 139783066089216 logging_writer.py:48] [52] global_step=52, grad_norm=0.382513, loss=6.905562
I0405 21:50:10.951129 139816912303936 submission.py:296] 52) loss = 6.906, grad_norm = 0.383
I0405 21:50:11.365218 139783057696512 logging_writer.py:48] [53] global_step=53, grad_norm=0.374200, loss=6.905292
I0405 21:50:11.369103 139816912303936 submission.py:296] 53) loss = 6.905, grad_norm = 0.374
I0405 21:50:11.793574 139783066089216 logging_writer.py:48] [54] global_step=54, grad_norm=0.390579, loss=6.904966
I0405 21:50:11.797187 139816912303936 submission.py:296] 54) loss = 6.905, grad_norm = 0.391
I0405 21:50:12.233266 139783057696512 logging_writer.py:48] [55] global_step=55, grad_norm=0.385166, loss=6.905013
I0405 21:50:12.237927 139816912303936 submission.py:296] 55) loss = 6.905, grad_norm = 0.385
I0405 21:50:12.642815 139783066089216 logging_writer.py:48] [56] global_step=56, grad_norm=0.379933, loss=6.905300
I0405 21:50:12.646877 139816912303936 submission.py:296] 56) loss = 6.905, grad_norm = 0.380
I0405 21:50:13.060919 139783057696512 logging_writer.py:48] [57] global_step=57, grad_norm=0.390785, loss=6.904690
I0405 21:50:13.065853 139816912303936 submission.py:296] 57) loss = 6.905, grad_norm = 0.391
I0405 21:50:13.474342 139783066089216 logging_writer.py:48] [58] global_step=58, grad_norm=0.389399, loss=6.905297
I0405 21:50:13.477897 139816912303936 submission.py:296] 58) loss = 6.905, grad_norm = 0.389
I0405 21:50:13.884747 139783057696512 logging_writer.py:48] [59] global_step=59, grad_norm=0.408448, loss=6.904003
I0405 21:50:13.889092 139816912303936 submission.py:296] 59) loss = 6.904, grad_norm = 0.408
I0405 21:50:14.301128 139783066089216 logging_writer.py:48] [60] global_step=60, grad_norm=0.387732, loss=6.904888
I0405 21:50:14.305429 139816912303936 submission.py:296] 60) loss = 6.905, grad_norm = 0.388
I0405 21:50:14.713888 139783057696512 logging_writer.py:48] [61] global_step=61, grad_norm=0.399863, loss=6.904237
I0405 21:50:14.720867 139816912303936 submission.py:296] 61) loss = 6.904, grad_norm = 0.400
I0405 21:50:15.141014 139783066089216 logging_writer.py:48] [62] global_step=62, grad_norm=0.394041, loss=6.904300
I0405 21:50:15.144763 139816912303936 submission.py:296] 62) loss = 6.904, grad_norm = 0.394
I0405 21:50:15.554007 139783057696512 logging_writer.py:48] [63] global_step=63, grad_norm=0.422818, loss=6.903863
I0405 21:50:15.557483 139816912303936 submission.py:296] 63) loss = 6.904, grad_norm = 0.423
I0405 21:50:15.964767 139783066089216 logging_writer.py:48] [64] global_step=64, grad_norm=0.410920, loss=6.903269
I0405 21:50:15.968401 139816912303936 submission.py:296] 64) loss = 6.903, grad_norm = 0.411
I0405 21:50:16.376214 139783057696512 logging_writer.py:48] [65] global_step=65, grad_norm=0.416425, loss=6.903333
I0405 21:50:16.380246 139816912303936 submission.py:296] 65) loss = 6.903, grad_norm = 0.416
I0405 21:50:16.792666 139783066089216 logging_writer.py:48] [66] global_step=66, grad_norm=0.401821, loss=6.903031
I0405 21:50:16.796435 139816912303936 submission.py:296] 66) loss = 6.903, grad_norm = 0.402
I0405 21:50:17.204599 139783057696512 logging_writer.py:48] [67] global_step=67, grad_norm=0.401384, loss=6.903690
I0405 21:50:17.208059 139816912303936 submission.py:296] 67) loss = 6.904, grad_norm = 0.401
I0405 21:50:17.616925 139783066089216 logging_writer.py:48] [68] global_step=68, grad_norm=0.407697, loss=6.902770
I0405 21:50:17.620577 139816912303936 submission.py:296] 68) loss = 6.903, grad_norm = 0.408
I0405 21:50:18.034976 139783057696512 logging_writer.py:48] [69] global_step=69, grad_norm=0.425791, loss=6.902111
I0405 21:50:18.038739 139816912303936 submission.py:296] 69) loss = 6.902, grad_norm = 0.426
I0405 21:50:18.469918 139783066089216 logging_writer.py:48] [70] global_step=70, grad_norm=0.420415, loss=6.902571
I0405 21:50:18.477983 139816912303936 submission.py:296] 70) loss = 6.903, grad_norm = 0.420
I0405 21:50:18.889632 139783057696512 logging_writer.py:48] [71] global_step=71, grad_norm=0.419016, loss=6.901782
I0405 21:50:18.893497 139816912303936 submission.py:296] 71) loss = 6.902, grad_norm = 0.419
I0405 21:50:19.300795 139783066089216 logging_writer.py:48] [72] global_step=72, grad_norm=0.410985, loss=6.901424
I0405 21:50:19.305343 139816912303936 submission.py:296] 72) loss = 6.901, grad_norm = 0.411
I0405 21:50:19.711758 139783057696512 logging_writer.py:48] [73] global_step=73, grad_norm=0.421536, loss=6.902492
I0405 21:50:19.715279 139816912303936 submission.py:296] 73) loss = 6.902, grad_norm = 0.422
I0405 21:50:20.130665 139783066089216 logging_writer.py:48] [74] global_step=74, grad_norm=0.441234, loss=6.900598
I0405 21:50:20.134109 139816912303936 submission.py:296] 74) loss = 6.901, grad_norm = 0.441
I0405 21:50:20.554118 139783057696512 logging_writer.py:48] [75] global_step=75, grad_norm=0.409086, loss=6.901761
I0405 21:50:20.557978 139816912303936 submission.py:296] 75) loss = 6.902, grad_norm = 0.409
I0405 21:50:20.966925 139783066089216 logging_writer.py:48] [76] global_step=76, grad_norm=0.444687, loss=6.900908
I0405 21:50:20.970955 139816912303936 submission.py:296] 76) loss = 6.901, grad_norm = 0.445
I0405 21:50:21.382800 139783057696512 logging_writer.py:48] [77] global_step=77, grad_norm=0.410523, loss=6.901269
I0405 21:50:21.386714 139816912303936 submission.py:296] 77) loss = 6.901, grad_norm = 0.411
I0405 21:50:21.808017 139783066089216 logging_writer.py:48] [78] global_step=78, grad_norm=0.439670, loss=6.900428
I0405 21:50:21.812544 139816912303936 submission.py:296] 78) loss = 6.900, grad_norm = 0.440
I0405 21:50:22.239030 139783057696512 logging_writer.py:48] [79] global_step=79, grad_norm=0.450347, loss=6.899150
I0405 21:50:22.243279 139816912303936 submission.py:296] 79) loss = 6.899, grad_norm = 0.450
I0405 21:50:22.675131 139783066089216 logging_writer.py:48] [80] global_step=80, grad_norm=0.449001, loss=6.898571
I0405 21:50:22.678793 139816912303936 submission.py:296] 80) loss = 6.899, grad_norm = 0.449
I0405 21:50:23.088387 139783057696512 logging_writer.py:48] [81] global_step=81, grad_norm=0.417353, loss=6.899307
I0405 21:50:23.094095 139816912303936 submission.py:296] 81) loss = 6.899, grad_norm = 0.417
I0405 21:50:23.501990 139783066089216 logging_writer.py:48] [82] global_step=82, grad_norm=0.445989, loss=6.899008
I0405 21:50:23.505893 139816912303936 submission.py:296] 82) loss = 6.899, grad_norm = 0.446
I0405 21:50:23.937287 139783057696512 logging_writer.py:48] [83] global_step=83, grad_norm=0.409239, loss=6.900479
I0405 21:50:23.941535 139816912303936 submission.py:296] 83) loss = 6.900, grad_norm = 0.409
I0405 21:50:24.376127 139783066089216 logging_writer.py:48] [84] global_step=84, grad_norm=0.444120, loss=6.898667
I0405 21:50:24.383771 139816912303936 submission.py:296] 84) loss = 6.899, grad_norm = 0.444
I0405 21:50:24.790432 139783057696512 logging_writer.py:48] [85] global_step=85, grad_norm=0.448912, loss=6.898469
I0405 21:50:24.795265 139816912303936 submission.py:296] 85) loss = 6.898, grad_norm = 0.449
I0405 21:50:25.208286 139783066089216 logging_writer.py:48] [86] global_step=86, grad_norm=0.447618, loss=6.898754
I0405 21:50:25.211731 139816912303936 submission.py:296] 86) loss = 6.899, grad_norm = 0.448
I0405 21:50:25.642014 139783057696512 logging_writer.py:48] [87] global_step=87, grad_norm=0.423697, loss=6.897439
I0405 21:50:25.646023 139816912303936 submission.py:296] 87) loss = 6.897, grad_norm = 0.424
I0405 21:50:26.069893 139783066089216 logging_writer.py:48] [88] global_step=88, grad_norm=0.436921, loss=6.897829
I0405 21:50:26.073447 139816912303936 submission.py:296] 88) loss = 6.898, grad_norm = 0.437
I0405 21:50:26.480277 139783057696512 logging_writer.py:48] [89] global_step=89, grad_norm=0.454313, loss=6.895427
I0405 21:50:26.483969 139816912303936 submission.py:296] 89) loss = 6.895, grad_norm = 0.454
I0405 21:50:26.904494 139783066089216 logging_writer.py:48] [90] global_step=90, grad_norm=0.455414, loss=6.896933
I0405 21:50:26.908373 139816912303936 submission.py:296] 90) loss = 6.897, grad_norm = 0.455
I0405 21:50:27.318632 139783057696512 logging_writer.py:48] [91] global_step=91, grad_norm=0.446761, loss=6.897943
I0405 21:50:27.322574 139816912303936 submission.py:296] 91) loss = 6.898, grad_norm = 0.447
I0405 21:50:27.730881 139783066089216 logging_writer.py:48] [92] global_step=92, grad_norm=0.449880, loss=6.896334
I0405 21:50:27.734555 139816912303936 submission.py:296] 92) loss = 6.896, grad_norm = 0.450
I0405 21:50:28.172088 139783057696512 logging_writer.py:48] [93] global_step=93, grad_norm=0.434709, loss=6.893708
I0405 21:50:28.175781 139816912303936 submission.py:296] 93) loss = 6.894, grad_norm = 0.435
I0405 21:50:28.582241 139783066089216 logging_writer.py:48] [94] global_step=94, grad_norm=0.453288, loss=6.897050
I0405 21:50:28.586517 139816912303936 submission.py:296] 94) loss = 6.897, grad_norm = 0.453
I0405 21:50:28.994452 139783057696512 logging_writer.py:48] [95] global_step=95, grad_norm=0.434949, loss=6.895235
I0405 21:50:28.998518 139816912303936 submission.py:296] 95) loss = 6.895, grad_norm = 0.435
I0405 21:50:29.414011 139783066089216 logging_writer.py:48] [96] global_step=96, grad_norm=0.439242, loss=6.894091
I0405 21:50:29.417753 139816912303936 submission.py:296] 96) loss = 6.894, grad_norm = 0.439
I0405 21:50:29.824339 139783057696512 logging_writer.py:48] [97] global_step=97, grad_norm=0.458258, loss=6.893235
I0405 21:50:29.828182 139816912303936 submission.py:296] 97) loss = 6.893, grad_norm = 0.458
I0405 21:50:30.248102 139783066089216 logging_writer.py:48] [98] global_step=98, grad_norm=0.452709, loss=6.892421
I0405 21:50:30.251919 139816912303936 submission.py:296] 98) loss = 6.892, grad_norm = 0.453
I0405 21:50:30.677151 139783057696512 logging_writer.py:48] [99] global_step=99, grad_norm=0.438470, loss=6.893421
I0405 21:50:30.681214 139816912303936 submission.py:296] 99) loss = 6.893, grad_norm = 0.438
I0405 21:50:31.095350 139783066089216 logging_writer.py:48] [100] global_step=100, grad_norm=0.447338, loss=6.894719
I0405 21:50:31.099077 139816912303936 submission.py:296] 100) loss = 6.895, grad_norm = 0.447
I0405 21:53:15.306543 139783057696512 logging_writer.py:48] [500] global_step=500, grad_norm=0.929369, loss=6.640832
I0405 21:53:15.310850 139816912303936 submission.py:296] 500) loss = 6.641, grad_norm = 0.929
I0405 21:56:40.427984 139783066089216 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.322563, loss=6.390858
I0405 21:56:40.433573 139816912303936 submission.py:296] 1000) loss = 6.391, grad_norm = 1.323
I0405 21:56:49.061537 139816912303936 submission_runner.py:373] Before eval at step 1022: RAM USED (GB) 98.590388224
I0405 21:56:49.061745 139816912303936 spec.py:298] Evaluating on the training split.
I0405 21:57:33.457509 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 21:58:17.807232 139816912303936 spec.py:326] Evaluating on the test split.
I0405 21:58:19.252182 139816912303936 submission_runner.py:382] Time since start: 533.03s, 	Step: 1022, 	{'train/accuracy': 0.040390625, 'train/loss': 5.835340576171875, 'validation/accuracy': 0.03842, 'validation/loss': 5.86711375, 'validation/num_examples': 50000, 'test/accuracy': 0.0282, 'test/loss': 5.984708203125, 'test/num_examples': 10000}
I0405 21:58:19.252588 139816912303936 submission_runner.py:396] After eval at step 1022: RAM USED (GB) 98.480001024
I0405 21:58:19.260417 139773687600896 logging_writer.py:48] [1022] global_step=1022, preemption_count=0, score=424.441564, test/accuracy=0.028200, test/loss=5.984708, test/num_examples=10000, total_duration=533.026105, train/accuracy=0.040391, train/loss=5.835341, validation/accuracy=0.038420, validation/loss=5.867114, validation/num_examples=50000
I0405 21:58:19.682278 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_1022.
I0405 21:58:19.683004 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 1022: RAM USED (GB) 98.478862336
I0405 22:01:37.297982 139773695993600 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.015850, loss=6.103043
I0405 22:01:37.305562 139816912303936 submission.py:296] 1500) loss = 6.103, grad_norm = 1.016
I0405 22:05:02.679792 139773687600896 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.084553, loss=5.844886
I0405 22:05:02.684370 139816912303936 submission.py:296] 2000) loss = 5.845, grad_norm = 1.085
I0405 22:05:19.892874 139816912303936 submission_runner.py:373] Before eval at step 2043: RAM USED (GB) 100.109377536
I0405 22:05:19.893099 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:06:03.154218 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:06:47.812564 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:06:49.221393 139816912303936 submission_runner.py:382] Time since start: 1043.86s, 	Step: 2043, 	{'train/accuracy': 0.093359375, 'train/loss': 5.129528198242188, 'validation/accuracy': 0.08426, 'validation/loss': 5.177661875, 'validation/num_examples': 50000, 'test/accuracy': 0.0647, 'test/loss': 5.392371875, 'test/num_examples': 10000}
I0405 22:06:49.221764 139816912303936 submission_runner.py:396] After eval at step 2043: RAM USED (GB) 100.161204224
I0405 22:06:49.229038 139773695993600 logging_writer.py:48] [2043] global_step=2043, preemption_count=0, score=842.193032, test/accuracy=0.064700, test/loss=5.392372, test/num_examples=10000, total_duration=1043.857364, train/accuracy=0.093359, train/loss=5.129528, validation/accuracy=0.084260, validation/loss=5.177662, validation/num_examples=50000
I0405 22:06:49.650001 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_2043.
I0405 22:06:49.650797 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 2043: RAM USED (GB) 100.160294912
I0405 22:09:56.422574 139773687600896 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.922497, loss=6.026160
I0405 22:09:56.427042 139816912303936 submission.py:296] 2500) loss = 6.026, grad_norm = 0.922
I0405 22:13:23.727648 139773695993600 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.357594, loss=5.808339
I0405 22:13:23.732211 139816912303936 submission.py:296] 3000) loss = 5.808, grad_norm = 1.358
I0405 22:13:50.056714 139816912303936 submission_runner.py:373] Before eval at step 3065: RAM USED (GB) 100.536737792
I0405 22:13:50.056934 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:14:33.450069 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:15:18.310866 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:15:19.718798 139816912303936 submission_runner.py:382] Time since start: 1554.02s, 	Step: 3065, 	{'train/accuracy': 0.16287109375, 'train/loss': 4.476615295410157, 'validation/accuracy': 0.15128, 'validation/loss': 4.5514915625, 'validation/num_examples': 50000, 'test/accuracy': 0.1117, 'test/loss': 4.88459453125, 'test/num_examples': 10000}
I0405 22:15:19.719184 139816912303936 submission_runner.py:396] After eval at step 3065: RAM USED (GB) 100.568080384
I0405 22:15:19.726814 139773687600896 logging_writer.py:48] [3065] global_step=3065, preemption_count=0, score=1260.151222, test/accuracy=0.111700, test/loss=4.884595, test/num_examples=10000, total_duration=1554.021226, train/accuracy=0.162871, train/loss=4.476615, validation/accuracy=0.151280, validation/loss=4.551492, validation/num_examples=50000
I0405 22:15:20.158516 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_3065.
I0405 22:15:20.159274 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 3065: RAM USED (GB) 100.5889536
I0405 22:18:18.752668 139773695993600 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.110999, loss=5.478312
I0405 22:18:18.757854 139816912303936 submission.py:296] 3500) loss = 5.478, grad_norm = 1.111
I0405 22:21:45.817448 139773687600896 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.179079, loss=5.322175
I0405 22:21:45.823602 139816912303936 submission.py:296] 4000) loss = 5.322, grad_norm = 1.179
I0405 22:22:20.431386 139816912303936 submission_runner.py:373] Before eval at step 4085: RAM USED (GB) 100.512718848
I0405 22:22:20.431701 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:23:04.017608 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:23:48.450711 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:23:49.861037 139816912303936 submission_runner.py:382] Time since start: 2064.40s, 	Step: 4085, 	{'train/accuracy': 0.23255859375, 'train/loss': 3.941384582519531, 'validation/accuracy': 0.21422, 'validation/loss': 4.039335625, 'validation/num_examples': 50000, 'test/accuracy': 0.1623, 'test/loss': 4.434147265625, 'test/num_examples': 10000}
I0405 22:23:49.861383 139816912303936 submission_runner.py:396] After eval at step 4085: RAM USED (GB) 100.55471104
I0405 22:23:49.869251 139773695993600 logging_writer.py:48] [4085] global_step=4085, preemption_count=0, score=1677.995377, test/accuracy=0.162300, test/loss=4.434147, test/num_examples=10000, total_duration=2064.395802, train/accuracy=0.232559, train/loss=3.941385, validation/accuracy=0.214220, validation/loss=4.039336, validation/num_examples=50000
I0405 22:23:50.300085 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_4085.
I0405 22:23:50.300788 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 4085: RAM USED (GB) 100.553801728
I0405 22:26:41.877211 139773687600896 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.185652, loss=4.892679
I0405 22:26:41.881563 139816912303936 submission.py:296] 4500) loss = 4.893, grad_norm = 1.186
I0405 22:30:06.343254 139773695993600 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.980089, loss=5.477638
I0405 22:30:06.347193 139816912303936 submission.py:296] 5000) loss = 5.478, grad_norm = 0.980
I0405 22:30:50.442335 139816912303936 submission_runner.py:373] Before eval at step 5103: RAM USED (GB) 100.667109376
I0405 22:30:50.442560 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:31:34.167319 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:32:19.611679 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:32:21.023616 139816912303936 submission_runner.py:382] Time since start: 2574.41s, 	Step: 5103, 	{'train/accuracy': 0.282265625, 'train/loss': 3.5796585083007812, 'validation/accuracy': 0.2569, 'validation/loss': 3.708304375, 'validation/num_examples': 50000, 'test/accuracy': 0.198, 'test/loss': 4.149203515625, 'test/num_examples': 10000}
I0405 22:32:21.023954 139816912303936 submission_runner.py:396] After eval at step 5103: RAM USED (GB) 100.644999168
I0405 22:32:21.032365 139773687600896 logging_writer.py:48] [5103] global_step=5103, preemption_count=0, score=2095.704417, test/accuracy=0.198000, test/loss=4.149204, test/num_examples=10000, total_duration=2574.406772, train/accuracy=0.282266, train/loss=3.579659, validation/accuracy=0.256900, validation/loss=3.708304, validation/num_examples=50000
I0405 22:32:21.462962 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_5103.
I0405 22:32:21.463656 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 5103: RAM USED (GB) 100.644089856
I0405 22:35:04.680499 139773695993600 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.954025, loss=5.087758
I0405 22:35:04.687935 139816912303936 submission.py:296] 5500) loss = 5.088, grad_norm = 0.954
I0405 22:38:29.826788 139773687600896 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.098486, loss=5.104378
I0405 22:38:29.830690 139816912303936 submission.py:296] 6000) loss = 5.104, grad_norm = 1.098
I0405 22:39:21.642090 139816912303936 submission_runner.py:373] Before eval at step 6128: RAM USED (GB) 100.816617472
I0405 22:39:21.642361 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:40:05.998610 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:40:50.357061 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:40:51.766783 139816912303936 submission_runner.py:382] Time since start: 3085.61s, 	Step: 6128, 	{'train/accuracy': 0.34013671875, 'train/loss': 3.2015447998046875, 'validation/accuracy': 0.31212, 'validation/loss': 3.35121875, 'validation/num_examples': 50000, 'test/accuracy': 0.2384, 'test/loss': 3.839837890625, 'test/num_examples': 10000}
I0405 22:40:51.767146 139816912303936 submission_runner.py:396] After eval at step 6128: RAM USED (GB) 100.791226368
I0405 22:40:51.775897 139773695993600 logging_writer.py:48] [6128] global_step=6128, preemption_count=0, score=2513.470667, test/accuracy=0.238400, test/loss=3.839838, test/num_examples=10000, total_duration=3085.606475, train/accuracy=0.340137, train/loss=3.201545, validation/accuracy=0.312120, validation/loss=3.351219, validation/num_examples=50000
I0405 22:40:52.196312 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_6128.
I0405 22:40:52.197098 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 6128: RAM USED (GB) 100.790550528
I0405 22:43:27.039210 139773687600896 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.891689, loss=4.872934
I0405 22:43:27.045131 139816912303936 submission.py:296] 6500) loss = 4.873, grad_norm = 0.892
I0405 22:46:52.664891 139773695993600 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.955642, loss=4.917870
I0405 22:46:52.670698 139816912303936 submission.py:296] 7000) loss = 4.918, grad_norm = 0.956
I0405 22:47:52.514248 139816912303936 submission_runner.py:373] Before eval at step 7147: RAM USED (GB) 100.64658432
I0405 22:47:52.514469 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:48:36.472666 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:49:22.037398 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:49:23.443731 139816912303936 submission_runner.py:382] Time since start: 3596.48s, 	Step: 7147, 	{'train/accuracy': 0.384296875, 'train/loss': 2.952892150878906, 'validation/accuracy': 0.34854, 'validation/loss': 3.1233821875, 'validation/num_examples': 50000, 'test/accuracy': 0.2713, 'test/loss': 3.6297921875, 'test/num_examples': 10000}
I0405 22:49:23.444076 139816912303936 submission_runner.py:396] After eval at step 7147: RAM USED (GB) 100.779597824
I0405 22:49:23.452400 139773687600896 logging_writer.py:48] [7147] global_step=7147, preemption_count=0, score=2931.358279, test/accuracy=0.271300, test/loss=3.629792, test/num_examples=10000, total_duration=3596.478690, train/accuracy=0.384297, train/loss=2.952892, validation/accuracy=0.348540, validation/loss=3.123382, validation/num_examples=50000
I0405 22:49:23.874017 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_7147.
I0405 22:49:23.874783 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 7147: RAM USED (GB) 100.778971136
I0405 22:51:48.471971 139773695993600 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.931752, loss=4.754759
I0405 22:51:48.475965 139816912303936 submission.py:296] 7500) loss = 4.755, grad_norm = 0.932
I0405 22:55:15.923349 139773687600896 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.894016, loss=4.490545
I0405 22:55:15.929791 139816912303936 submission.py:296] 8000) loss = 4.491, grad_norm = 0.894
I0405 22:56:24.175940 139816912303936 submission_runner.py:373] Before eval at step 8167: RAM USED (GB) 100.748214272
I0405 22:56:24.176140 139816912303936 spec.py:298] Evaluating on the training split.
I0405 22:57:08.088362 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 22:57:55.192142 139816912303936 spec.py:326] Evaluating on the test split.
I0405 22:57:56.601119 139816912303936 submission_runner.py:382] Time since start: 4108.14s, 	Step: 8167, 	{'train/accuracy': 0.42349609375, 'train/loss': 2.742060546875, 'validation/accuracy': 0.38916, 'validation/loss': 2.9104384375, 'validation/num_examples': 50000, 'test/accuracy': 0.2989, 'test/loss': 3.4634703125, 'test/num_examples': 10000}
I0405 22:57:56.601453 139816912303936 submission_runner.py:396] After eval at step 8167: RAM USED (GB) 100.81507328
I0405 22:57:56.609043 139773695993600 logging_writer.py:48] [8167] global_step=8167, preemption_count=0, score=3349.284041, test/accuracy=0.298900, test/loss=3.463470, test/num_examples=10000, total_duration=4108.140464, train/accuracy=0.423496, train/loss=2.742061, validation/accuracy=0.389160, validation/loss=2.910438, validation/num_examples=50000
I0405 22:57:57.024605 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_8167.
I0405 22:57:57.025303 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 8167: RAM USED (GB) 100.814163968
I0405 23:00:13.650778 139773687600896 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.824797, loss=5.094010
I0405 23:00:13.654639 139816912303936 submission.py:296] 8500) loss = 5.094, grad_norm = 0.825
I0405 23:03:40.926515 139773695993600 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.765835, loss=4.699123
I0405 23:03:40.933180 139816912303936 submission.py:296] 9000) loss = 4.699, grad_norm = 0.766
I0405 23:04:57.309564 139816912303936 submission_runner.py:373] Before eval at step 9187: RAM USED (GB) 100.520845312
I0405 23:04:57.309794 139816912303936 spec.py:298] Evaluating on the training split.
I0405 23:05:41.439310 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 23:06:29.441504 139816912303936 spec.py:326] Evaluating on the test split.
I0405 23:06:30.849890 139816912303936 submission_runner.py:382] Time since start: 4621.27s, 	Step: 9187, 	{'train/accuracy': 0.450703125, 'train/loss': 2.5782070922851563, 'validation/accuracy': 0.41628, 'validation/loss': 2.751583125, 'validation/num_examples': 50000, 'test/accuracy': 0.3295, 'test/loss': 3.307243359375, 'test/num_examples': 10000}
I0405 23:06:30.850275 139816912303936 submission_runner.py:396] After eval at step 9187: RAM USED (GB) 100.665307136
I0405 23:06:30.858126 139773687600896 logging_writer.py:48] [9187] global_step=9187, preemption_count=0, score=3767.201068, test/accuracy=0.329500, test/loss=3.307243, test/num_examples=10000, total_duration=4621.273322, train/accuracy=0.450703, train/loss=2.578207, validation/accuracy=0.416280, validation/loss=2.751583, validation/num_examples=50000
I0405 23:06:31.279510 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_9187.
I0405 23:06:31.280237 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 9187: RAM USED (GB) 100.664651776
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0405 23:08:40.072780 139773695993600 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.759361, loss=4.701298
I0405 23:08:40.077112 139816912303936 submission.py:296] 9500) loss = 4.701, grad_norm = 0.759
I0405 23:12:04.538071 139773687600896 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.749304, loss=4.523234
I0405 23:12:04.543668 139816912303936 submission.py:296] 10000) loss = 4.523, grad_norm = 0.749
I0405 23:13:31.582490 139816912303936 submission_runner.py:373] Before eval at step 10209: RAM USED (GB) 100.81783808
I0405 23:13:31.582741 139816912303936 spec.py:298] Evaluating on the training split.
I0405 23:14:15.262868 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 23:15:00.464031 139816912303936 spec.py:326] Evaluating on the test split.
I0405 23:15:01.868291 139816912303936 submission_runner.py:382] Time since start: 5135.55s, 	Step: 10209, 	{'train/accuracy': 0.4815234375, 'train/loss': 2.416207580566406, 'validation/accuracy': 0.44624, 'validation/loss': 2.60102359375, 'validation/num_examples': 50000, 'test/accuracy': 0.3511, 'test/loss': 3.154658203125, 'test/num_examples': 10000}
I0405 23:15:01.868659 139816912303936 submission_runner.py:396] After eval at step 10209: RAM USED (GB) 100.79705088
I0405 23:15:01.877263 139773695993600 logging_writer.py:48] [10209] global_step=10209, preemption_count=0, score=4185.141319, test/accuracy=0.351100, test/loss=3.154658, test/num_examples=10000, total_duration=5135.546975, train/accuracy=0.481523, train/loss=2.416208, validation/accuracy=0.446240, validation/loss=2.601024, validation/num_examples=50000
I0405 23:15:02.309262 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_10209.
I0405 23:15:02.309957 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 10209: RAM USED (GB) 100.778196992
I0405 23:17:02.224912 139773687600896 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.682303, loss=4.531244
I0405 23:17:02.229046 139816912303936 submission.py:296] 10500) loss = 4.531, grad_norm = 0.682
I0405 23:20:26.950635 139773695993600 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.696418, loss=4.556824
I0405 23:20:26.955217 139816912303936 submission.py:296] 11000) loss = 4.557, grad_norm = 0.696
I0405 23:22:02.502606 139816912303936 submission_runner.py:373] Before eval at step 11235: RAM USED (GB) 100.764024832
I0405 23:22:02.502854 139816912303936 spec.py:298] Evaluating on the training split.
I0405 23:22:46.611388 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 23:23:32.708003 139816912303936 spec.py:326] Evaluating on the test split.
I0405 23:23:34.118369 139816912303936 submission_runner.py:382] Time since start: 5646.47s, 	Step: 11235, 	{'train/accuracy': 0.51419921875, 'train/loss': 2.228268737792969, 'validation/accuracy': 0.47528, 'validation/loss': 2.42088359375, 'validation/num_examples': 50000, 'test/accuracy': 0.3713, 'test/loss': 3.0001427734375, 'test/num_examples': 10000}
I0405 23:23:34.118730 139816912303936 submission_runner.py:396] After eval at step 11235: RAM USED (GB) 100.722200576
I0405 23:23:34.126497 139773687600896 logging_writer.py:48] [11235] global_step=11235, preemption_count=0, score=4602.941558, test/accuracy=0.371300, test/loss=3.000143, test/num_examples=10000, total_duration=5646.466999, train/accuracy=0.514199, train/loss=2.228269, validation/accuracy=0.475280, validation/loss=2.420884, validation/num_examples=50000
I0405 23:23:34.550682 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_11235.
I0405 23:23:34.551442 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 11235: RAM USED (GB) 100.721291264
I0405 23:25:25.632680 139773695993600 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.815951, loss=4.141122
I0405 23:25:25.636254 139816912303936 submission.py:296] 11500) loss = 4.141, grad_norm = 0.816
I0405 23:28:51.039259 139773687600896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.715956, loss=3.882200
I0405 23:28:51.043635 139816912303936 submission.py:296] 12000) loss = 3.882, grad_norm = 0.716
I0405 23:30:34.639931 139816912303936 submission_runner.py:373] Before eval at step 12254: RAM USED (GB) 100.795101184
I0405 23:30:34.640142 139816912303936 spec.py:298] Evaluating on the training split.
I0405 23:31:18.729344 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 23:32:03.533185 139816912303936 spec.py:326] Evaluating on the test split.
I0405 23:32:04.933642 139816912303936 submission_runner.py:382] Time since start: 6158.60s, 	Step: 12254, 	{'train/accuracy': 0.5375, 'train/loss': 2.1095025634765623, 'validation/accuracy': 0.49486, 'validation/loss': 2.30860578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3911, 'test/loss': 2.9087640625, 'test/num_examples': 10000}
I0405 23:32:04.934044 139816912303936 submission_runner.py:396] After eval at step 12254: RAM USED (GB) 100.64381952
I0405 23:32:04.942693 139773695993600 logging_writer.py:48] [12254] global_step=12254, preemption_count=0, score=5020.631612, test/accuracy=0.391100, test/loss=2.908764, test/num_examples=10000, total_duration=6158.604473, train/accuracy=0.537500, train/loss=2.109503, validation/accuracy=0.494860, validation/loss=2.308606, validation/num_examples=50000
I0405 23:32:05.357794 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_12254.
I0405 23:32:05.358526 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 12254: RAM USED (GB) 100.643446784
I0405 23:33:46.249267 139773687600896 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.784926, loss=4.035329
I0405 23:33:46.253936 139816912303936 submission.py:296] 12500) loss = 4.035, grad_norm = 0.785
I0405 23:37:13.491655 139773695993600 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.832784, loss=4.783961
I0405 23:37:13.495722 139816912303936 submission.py:296] 13000) loss = 4.784, grad_norm = 0.833
I0405 23:39:05.770967 139816912303936 submission_runner.py:373] Before eval at step 13274: RAM USED (GB) 100.63325184
I0405 23:39:05.771186 139816912303936 spec.py:298] Evaluating on the training split.
I0405 23:39:48.907445 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 23:40:34.072483 139816912303936 spec.py:326] Evaluating on the test split.
I0405 23:40:35.478636 139816912303936 submission_runner.py:382] Time since start: 6669.74s, 	Step: 13274, 	{'train/accuracy': 0.55669921875, 'train/loss': 2.0007974243164064, 'validation/accuracy': 0.51202, 'validation/loss': 2.20923046875, 'validation/num_examples': 50000, 'test/accuracy': 0.4078, 'test/loss': 2.802915625, 'test/num_examples': 10000}
I0405 23:40:35.478994 139816912303936 submission_runner.py:396] After eval at step 13274: RAM USED (GB) 100.765806592
I0405 23:40:35.487276 139773687600896 logging_writer.py:48] [13274] global_step=13274, preemption_count=0, score=5438.645683, test/accuracy=0.407800, test/loss=2.802916, test/num_examples=10000, total_duration=6669.735416, train/accuracy=0.556699, train/loss=2.000797, validation/accuracy=0.512020, validation/loss=2.209230, validation/num_examples=50000
I0405 23:40:35.911031 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_13274.
I0405 23:40:35.911753 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 13274: RAM USED (GB) 100.765155328
I0405 23:42:08.777376 139773695993600 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.705541, loss=3.897128
I0405 23:42:08.781688 139816912303936 submission.py:296] 13500) loss = 3.897, grad_norm = 0.706
I0405 23:45:34.932958 139816912303936 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.747751424
I0405 23:45:34.933178 139816912303936 spec.py:298] Evaluating on the training split.
I0405 23:46:18.488548 139816912303936 spec.py:310] Evaluating on the validation split.
I0405 23:47:04.831124 139816912303936 spec.py:326] Evaluating on the test split.
I0405 23:47:06.236987 139816912303936 submission_runner.py:382] Time since start: 7058.90s, 	Step: 14000, 	{'train/accuracy': 0.571875, 'train/loss': 1.9680093383789063, 'validation/accuracy': 0.52582, 'validation/loss': 2.185435625, 'validation/num_examples': 50000, 'test/accuracy': 0.4182, 'test/loss': 2.786996484375, 'test/num_examples': 10000}
I0405 23:47:06.237333 139816912303936 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.594864128
I0405 23:47:06.246102 139773687600896 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5735.995229, test/accuracy=0.418200, test/loss=2.786996, test/num_examples=10000, total_duration=7058.897550, train/accuracy=0.571875, train/loss=1.968009, validation/accuracy=0.525820, validation/loss=2.185436, validation/num_examples=50000
I0405 23:47:06.672301 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:47:06.673071 139816912303936 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.595503104
I0405 23:47:06.681661 139773695993600 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5735.995229
I0405 23:47:07.677161 139816912303936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nadamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:47:08.051437 139816912303936 submission_runner.py:550] Tuning trial 1/1
I0405 23:47:08.051640 139816912303936 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 23:47:08.052267 139816912303936 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00203125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00278, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0022, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.582731246948242, 'total_duration': 6.584920883178711, 'global_step': 1, 'preemption_count': 0}), (1022, {'train/accuracy': 0.040390625, 'train/loss': 5.835340576171875, 'validation/accuracy': 0.03842, 'validation/loss': 5.86711375, 'validation/num_examples': 50000, 'test/accuracy': 0.0282, 'test/loss': 5.984708203125, 'test/num_examples': 10000, 'score': 424.44156408309937, 'total_duration': 533.0261051654816, 'global_step': 1022, 'preemption_count': 0}), (2043, {'train/accuracy': 0.093359375, 'train/loss': 5.129528198242188, 'validation/accuracy': 0.08426, 'validation/loss': 5.177661875, 'validation/num_examples': 50000, 'test/accuracy': 0.0647, 'test/loss': 5.392371875, 'test/num_examples': 10000, 'score': 842.1930320262909, 'total_duration': 1043.8573637008667, 'global_step': 2043, 'preemption_count': 0}), (3065, {'train/accuracy': 0.16287109375, 'train/loss': 4.476615295410157, 'validation/accuracy': 0.15128, 'validation/loss': 4.5514915625, 'validation/num_examples': 50000, 'test/accuracy': 0.1117, 'test/loss': 4.88459453125, 'test/num_examples': 10000, 'score': 1260.151222229004, 'total_duration': 1554.0212264060974, 'global_step': 3065, 'preemption_count': 0}), (4085, {'train/accuracy': 0.23255859375, 'train/loss': 3.941384582519531, 'validation/accuracy': 0.21422, 'validation/loss': 4.039335625, 'validation/num_examples': 50000, 'test/accuracy': 0.1623, 'test/loss': 4.434147265625, 'test/num_examples': 10000, 'score': 1677.995376586914, 'total_duration': 2064.3958024978638, 'global_step': 4085, 'preemption_count': 0}), (5103, {'train/accuracy': 0.282265625, 'train/loss': 3.5796585083007812, 'validation/accuracy': 0.2569, 'validation/loss': 3.708304375, 'validation/num_examples': 50000, 'test/accuracy': 0.198, 'test/loss': 4.149203515625, 'test/num_examples': 10000, 'score': 2095.70441699028, 'total_duration': 2574.4067721366882, 'global_step': 5103, 'preemption_count': 0}), (6128, {'train/accuracy': 0.34013671875, 'train/loss': 3.2015447998046875, 'validation/accuracy': 0.31212, 'validation/loss': 3.35121875, 'validation/num_examples': 50000, 'test/accuracy': 0.2384, 'test/loss': 3.839837890625, 'test/num_examples': 10000, 'score': 2513.470667362213, 'total_duration': 3085.606474876404, 'global_step': 6128, 'preemption_count': 0}), (7147, {'train/accuracy': 0.384296875, 'train/loss': 2.952892150878906, 'validation/accuracy': 0.34854, 'validation/loss': 3.1233821875, 'validation/num_examples': 50000, 'test/accuracy': 0.2713, 'test/loss': 3.6297921875, 'test/num_examples': 10000, 'score': 2931.3582785129547, 'total_duration': 3596.4786901474, 'global_step': 7147, 'preemption_count': 0}), (8167, {'train/accuracy': 0.42349609375, 'train/loss': 2.742060546875, 'validation/accuracy': 0.38916, 'validation/loss': 2.9104384375, 'validation/num_examples': 50000, 'test/accuracy': 0.2989, 'test/loss': 3.4634703125, 'test/num_examples': 10000, 'score': 3349.284040927887, 'total_duration': 4108.140463590622, 'global_step': 8167, 'preemption_count': 0}), (9187, {'train/accuracy': 0.450703125, 'train/loss': 2.5782070922851563, 'validation/accuracy': 0.41628, 'validation/loss': 2.751583125, 'validation/num_examples': 50000, 'test/accuracy': 0.3295, 'test/loss': 3.307243359375, 'test/num_examples': 10000, 'score': 3767.201068162918, 'total_duration': 4621.273321628571, 'global_step': 9187, 'preemption_count': 0}), (10209, {'train/accuracy': 0.4815234375, 'train/loss': 2.416207580566406, 'validation/accuracy': 0.44624, 'validation/loss': 2.60102359375, 'validation/num_examples': 50000, 'test/accuracy': 0.3511, 'test/loss': 3.154658203125, 'test/num_examples': 10000, 'score': 4185.141319274902, 'total_duration': 5135.546975374222, 'global_step': 10209, 'preemption_count': 0}), (11235, {'train/accuracy': 0.51419921875, 'train/loss': 2.228268737792969, 'validation/accuracy': 0.47528, 'validation/loss': 2.42088359375, 'validation/num_examples': 50000, 'test/accuracy': 0.3713, 'test/loss': 3.0001427734375, 'test/num_examples': 10000, 'score': 4602.941557645798, 'total_duration': 5646.4669988155365, 'global_step': 11235, 'preemption_count': 0}), (12254, {'train/accuracy': 0.5375, 'train/loss': 2.1095025634765623, 'validation/accuracy': 0.49486, 'validation/loss': 2.30860578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3911, 'test/loss': 2.9087640625, 'test/num_examples': 10000, 'score': 5020.631612062454, 'total_duration': 6158.6044726371765, 'global_step': 12254, 'preemption_count': 0}), (13274, {'train/accuracy': 0.55669921875, 'train/loss': 2.0007974243164064, 'validation/accuracy': 0.51202, 'validation/loss': 2.20923046875, 'validation/num_examples': 50000, 'test/accuracy': 0.4078, 'test/loss': 2.802915625, 'test/num_examples': 10000, 'score': 5438.6456825733185, 'total_duration': 6669.735415935516, 'global_step': 13274, 'preemption_count': 0}), (14000, {'train/accuracy': 0.571875, 'train/loss': 1.9680093383789063, 'validation/accuracy': 0.52582, 'validation/loss': 2.185435625, 'validation/num_examples': 50000, 'test/accuracy': 0.4182, 'test/loss': 2.786996484375, 'test/num_examples': 10000, 'score': 5735.995229005814, 'total_duration': 7058.89754986763, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 23:47:08.052384 139816912303936 submission_runner.py:553] Timing: 5735.995229005814
I0405 23:47:08.052454 139816912303936 submission_runner.py:554] ====================
I0405 23:47:08.052601 139816912303936 submission_runner.py:613] Final imagenet_vit score: 5735.995229005814
