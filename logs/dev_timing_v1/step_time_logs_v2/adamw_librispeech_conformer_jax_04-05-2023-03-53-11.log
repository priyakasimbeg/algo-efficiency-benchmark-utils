I0405 03:53:30.917344 140335240107840 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax.
I0405 03:53:30.974709 140335240107840 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 03:53:31.814141 140335240107840 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0405 03:53:31.814913 140335240107840 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 03:53:31.819449 140335240107840 submission_runner.py:511] Using RNG seed 4016649263
I0405 03:53:34.127704 140335240107840 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 03:53:34.127899 140335240107840 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1.
I0405 03:53:34.128085 140335240107840 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/hparams.json.
I0405 03:53:34.252931 140335240107840 submission_runner.py:230] Starting train once: RAM USED (GB) 4.410322944
I0405 03:53:34.253112 140335240107840 submission_runner.py:231] Initializing dataset.
I0405 03:53:34.253294 140335240107840 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.410322944
I0405 03:53:34.253365 140335240107840 submission_runner.py:240] Initializing model.
I0405 03:53:39.870225 140335240107840 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.868588032
I0405 03:53:39.870410 140335240107840 submission_runner.py:252] Initializing optimizer.
I0405 03:53:40.707321 140335240107840 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.868514304
I0405 03:53:40.707487 140335240107840 submission_runner.py:261] Initializing metrics bundle.
I0405 03:53:40.707532 140335240107840 submission_runner.py:276] Initializing checkpoint and logger.
I0405 03:53:40.708500 140335240107840 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0405 03:53:40.708770 140335240107840 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 03:53:40.708832 140335240107840 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 03:53:41.528406 140335240107840 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0405 03:53:41.529362 140335240107840 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/flags_0.json.
I0405 03:53:41.535200 140335240107840 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.867424768
I0405 03:53:41.535394 140335240107840 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.867424768
I0405 03:53:41.535454 140335240107840 submission_runner.py:313] Starting training loop.
I0405 03:53:41.735510 140335240107840 input_pipeline.py:20] Loading split = train-clean-100
I0405 03:53:41.767505 140335240107840 input_pipeline.py:20] Loading split = train-clean-360
I0405 03:53:42.090269 140335240107840 input_pipeline.py:20] Loading split = train-other-500
I0405 03:53:45.764025 140335240107840 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.564936704
2023-04-05 03:54:42.050699: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-05 03:54:42.330790: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0405 03:54:44.288806 140160444393216 logging_writer.py:48] [0] global_step=0, grad_norm=54.778385162353516, loss=32.19823455810547
I0405 03:54:44.307203 140335240107840 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.350928384
I0405 03:54:44.307542 140335240107840 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.350928384
I0405 03:54:44.307659 140335240107840 spec.py:298] Evaluating on the training split.
I0405 03:54:44.418749 140335240107840 input_pipeline.py:20] Loading split = train-clean-100
I0405 03:54:44.631442 140335240107840 input_pipeline.py:20] Loading split = train-clean-360
I0405 03:54:44.726190 140335240107840 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0405 03:55:30.943045 140335240107840 spec.py:310] Evaluating on the validation split.
I0405 03:55:31.003555 140335240107840 input_pipeline.py:20] Loading split = dev-clean
I0405 03:55:31.008448 140335240107840 input_pipeline.py:20] Loading split = dev-other
I0405 03:56:04.980638 140335240107840 spec.py:326] Evaluating on the test split.
I0405 03:56:05.042975 140335240107840 input_pipeline.py:20] Loading split = test-clean
I0405 03:56:30.225614 140335240107840 submission_runner.py:382] Time since start: 62.77s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.45655, dtype=float32), 'train/wer': 0.9792547321075318, 'validation/ctc_loss': DeviceArray(30.542128, dtype=float32), 'validation/wer': 0.9453347355015485, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.586128, dtype=float32), 'test/wer': 0.9475961245506063, 'test/num_examples': 2472}
I0405 03:56:30.226683 140335240107840 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.51258368
I0405 03:56:30.241292 140156946339584 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=62.571948, test/ctc_loss=30.58612823486328, test/num_examples=2472, test/wer=0.947596, total_duration=62.772048, train/ctc_loss=31.45655059814453, train/wer=0.979255, validation/ctc_loss=30.54212760925293, validation/num_examples=5348, validation/wer=0.945335
I0405 03:56:30.608133 140335240107840 checkpoints.py:356] Saving checkpoint at step: 1
I0405 03:56:31.974918 140335240107840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_1
I0405 03:56:32.006130 140335240107840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_1.
I0405 03:56:32.013417 140335240107840 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.788092928
I0405 03:56:32.070263 140335240107840 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.786085888
I0405 03:56:49.143095 140335240107840 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.153828864
I0405 03:58:04.606992 140161241396992 logging_writer.py:48] [100] global_step=100, grad_norm=1.8183706998825073, loss=6.293743133544922
I0405 03:59:20.976088 140161249789696 logging_writer.py:48] [200] global_step=200, grad_norm=0.5327675938606262, loss=5.8694963455200195
I0405 04:00:37.205586 140161241396992 logging_writer.py:48] [300] global_step=300, grad_norm=1.2211214303970337, loss=5.814055919647217
I0405 04:01:53.685894 140161249789696 logging_writer.py:48] [400] global_step=400, grad_norm=2.8982412815093994, loss=5.796505928039551
I0405 04:03:09.926761 140161241396992 logging_writer.py:48] [500] global_step=500, grad_norm=9.356207847595215, loss=5.875229835510254
I0405 04:04:26.426721 140161249789696 logging_writer.py:48] [600] global_step=600, grad_norm=0.41023993492126465, loss=5.804340362548828
I0405 04:05:42.758895 140161241396992 logging_writer.py:48] [700] global_step=700, grad_norm=0.5570100545883179, loss=5.781276702880859
I0405 04:07:02.867413 140161249789696 logging_writer.py:48] [800] global_step=800, grad_norm=1.376974105834961, loss=5.774534225463867
I0405 04:08:29.580410 140161241396992 logging_writer.py:48] [900] global_step=900, grad_norm=2.705718994140625, loss=5.7645463943481445
I0405 04:09:57.939781 140161249789696 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.6065988540649414, loss=5.675679683685303
I0405 04:11:19.321229 140161392465664 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1776279211044312, loss=5.5941853523254395
I0405 04:12:35.448291 140161384072960 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7648820281028748, loss=5.436532020568848
I0405 04:13:51.570438 140161392465664 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.5612951517105103, loss=5.067338943481445
I0405 04:15:07.767405 140161384072960 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9484562277793884, loss=4.266110420227051
I0405 04:16:23.962688 140161392465664 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.2865302562713623, loss=3.937856435775757
I0405 04:17:40.065428 140161384072960 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.449226975440979, loss=3.643372058868408
I0405 04:18:56.190031 140161392465664 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0708404779434204, loss=3.423503875732422
I0405 04:20:12.280797 140161384072960 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.285334825515747, loss=3.29771089553833
I0405 04:21:29.870294 140161392465664 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.4875562191009521, loss=3.16261625289917
I0405 04:22:48.845672 140161384072960 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.41801917552948, loss=3.0800468921661377
I0405 04:24:09.588490 140161392465664 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9778435826301575, loss=2.9504570960998535
I0405 04:25:25.420285 140161384072960 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0570005178451538, loss=2.900646448135376
I0405 04:26:41.407316 140161392465664 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3089525699615479, loss=2.8594799041748047
I0405 04:27:57.259228 140161384072960 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6703485250473022, loss=2.8101279735565186
I0405 04:29:13.159503 140161392465664 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.841931164264679, loss=2.70554780960083
I0405 04:30:28.951112 140161384072960 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7556898593902588, loss=2.636265754699707
I0405 04:31:44.971201 140161392465664 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0650702714920044, loss=2.580348491668701
I0405 04:33:00.824060 140161384072960 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8372378349304199, loss=2.573364019393921
I0405 04:34:16.674902 140161392465664 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2367912530899048, loss=2.4702324867248535
I0405 04:35:32.471423 140161384072960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8788470029830933, loss=2.473719596862793
I0405 04:36:32.617919 140335240107840 submission_runner.py:373] Before eval at step 3081: RAM USED (GB) 22.747676672
I0405 04:36:32.618120 140335240107840 spec.py:298] Evaluating on the training split.
I0405 04:37:06.991222 140335240107840 spec.py:310] Evaluating on the validation split.
I0405 04:37:43.891343 140335240107840 spec.py:326] Evaluating on the test split.
I0405 04:38:02.763613 140335240107840 submission_runner.py:382] Time since start: 2571.08s, 	Step: 3081, 	{'train/ctc_loss': DeviceArray(2.4590857, dtype=float32), 'train/wer': 0.5374915037795308, 'validation/ctc_loss': DeviceArray(2.8935754, dtype=float32), 'validation/wer': 0.5866916226881108, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.5527766, dtype=float32), 'test/wer': 0.5359413401580241, 'test/num_examples': 2472}
I0405 04:38:02.765051 140335240107840 submission_runner.py:396] After eval at step 3081: RAM USED (GB) 23.213408256
I0405 04:38:02.783075 140161392465664 logging_writer.py:48] [3081] global_step=3081, preemption_count=0, score=2455.550277, test/ctc_loss=2.552776575088501, test/num_examples=2472, test/wer=0.535941, total_duration=2571.079872, train/ctc_loss=2.459085702896118, train/wer=0.537492, validation/ctc_loss=2.893575429916382, validation/num_examples=5348, validation/wer=0.586692
I0405 04:38:03.125293 140335240107840 checkpoints.py:356] Saving checkpoint at step: 3081
I0405 04:38:04.551475 140335240107840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_3081
I0405 04:38:04.583451 140335240107840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_3081.
I0405 04:38:04.599219 140335240107840 submission_runner.py:416] After logging and checkpointing eval at step 3081: RAM USED (GB) 23.242768384
I0405 04:38:23.659860 140162047825664 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.2602221965789795, loss=2.4528613090515137
I0405 04:39:39.332714 140162039432960 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9011924862861633, loss=2.3596880435943604
I0405 04:40:54.997268 140162047825664 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8175850510597229, loss=2.3298656940460205
I0405 04:42:10.714434 140162039432960 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.235437035560608, loss=2.3126208782196045
I0405 04:43:26.481214 140162047825664 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0940303802490234, loss=2.2138519287109375
I0405 04:44:42.256320 140162039432960 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8466804027557373, loss=2.2563681602478027
I0405 04:45:58.015962 140162047825664 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7583494186401367, loss=2.2173197269439697
I0405 04:47:13.770893 140162039432960 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8247519135475159, loss=2.1447668075561523
I0405 04:48:29.539320 140162047825664 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8307009339332581, loss=2.1976566314697266
I0405 04:49:45.398022 140162039432960 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7500346899032593, loss=2.1259548664093018
I0405 04:51:06.469735 140162047825664 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8790242075920105, loss=2.099334716796875
I0405 04:52:26.619850 140162047825664 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7946193218231201, loss=2.0589396953582764
I0405 04:53:42.318274 140162039432960 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7251065969467163, loss=2.062023639678955
I0405 04:54:57.962554 140162047825664 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7334536910057068, loss=2.0636167526245117
I0405 04:56:13.630616 140162039432960 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7500779628753662, loss=1.9832319021224976
I0405 04:57:29.302584 140162047825664 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7395756244659424, loss=1.9564671516418457
I0405 04:58:44.920318 140162039432960 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6567309498786926, loss=1.991593599319458
I0405 05:00:00.465992 140162047825664 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7183508276939392, loss=1.9929955005645752
I0405 05:01:16.002563 140162039432960 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8260191679000854, loss=1.9518022537231445
I0405 05:02:38.419580 140162047825664 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8944544792175293, loss=1.9856014251708984
I0405 05:04:00.896697 140162039432960 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6824638247489929, loss=1.9513483047485352
I0405 05:05:23.830492 140162703185664 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6290332078933716, loss=1.9636566638946533
I0405 05:06:39.361826 140162694792960 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6622405648231506, loss=1.9151934385299683
I0405 05:07:54.839125 140162703185664 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6168450117111206, loss=1.8657585382461548
I0405 05:09:10.282075 140162694792960 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8180691599845886, loss=1.833849549293518
I0405 05:10:25.782129 140162703185664 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7899390459060669, loss=1.905064582824707
I0405 05:11:41.320081 140162694792960 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6822081804275513, loss=1.877811074256897
I0405 05:13:00.383365 140162703185664 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.696702241897583, loss=1.866379737854004
I0405 05:14:21.509059 140162694792960 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5558672547340393, loss=1.8678058385849
I0405 05:15:41.069617 140162703185664 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.580187201499939, loss=1.8245316743850708
I0405 05:17:02.413369 140162694792960 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5561466813087463, loss=1.7601162195205688
I0405 05:18:04.700733 140335240107840 submission_runner.py:373] Before eval at step 6179: RAM USED (GB) 23.153872896
I0405 05:18:04.700934 140335240107840 spec.py:298] Evaluating on the training split.
I0405 05:18:40.296533 140335240107840 spec.py:310] Evaluating on the validation split.
I0405 05:19:18.637555 140335240107840 spec.py:326] Evaluating on the test split.
I0405 05:19:38.406582 140335240107840 submission_runner.py:382] Time since start: 5063.16s, 	Step: 6179, 	{'train/ctc_loss': DeviceArray(0.5645077, dtype=float32), 'train/wer': 0.19523746104466877, 'validation/ctc_loss': DeviceArray(0.8975538, dtype=float32), 'validation/wer': 0.2651062721299771, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.60990775, dtype=float32), 'test/wer': 0.20252676050616455, 'test/num_examples': 2472}
I0405 05:19:38.407935 140335240107840 submission_runner.py:396] After eval at step 6179: RAM USED (GB) 22.799491072
I0405 05:19:38.426149 140162703185664 logging_writer.py:48] [6179] global_step=6179, preemption_count=0, score=4847.912945, test/ctc_loss=0.6099077463150024, test/num_examples=2472, test/wer=0.202527, total_duration=5063.163578, train/ctc_loss=0.5645077228546143, train/wer=0.195237, validation/ctc_loss=0.8975538015365601, validation/num_examples=5348, validation/wer=0.265106
I0405 05:19:38.759600 140335240107840 checkpoints.py:356] Saving checkpoint at step: 6179
I0405 05:19:40.189987 140335240107840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_6179
I0405 05:19:40.222818 140335240107840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_6179.
I0405 05:19:40.238062 140335240107840 submission_runner.py:416] After logging and checkpointing eval at step 6179: RAM USED (GB) 22.82225664
I0405 05:20:00.706596 140162047825664 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.553159236907959, loss=1.7739641666412354
I0405 05:21:16.124219 140162039432960 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.56804358959198, loss=1.7917190790176392
I0405 05:22:31.424901 140162047825664 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7998323440551758, loss=1.8003759384155273
I0405 05:23:46.773266 140162039432960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7195543646812439, loss=1.8286828994750977
I0405 05:25:02.203222 140162047825664 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5414209961891174, loss=1.7749747037887573
I0405 05:26:17.598654 140162039432960 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.498514324426651, loss=1.696745753288269
I0405 05:27:32.986222 140162047825664 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6073538661003113, loss=1.7312604188919067
I0405 05:28:48.385243 140162039432960 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.49269047379493713, loss=1.7012139558792114
I0405 05:30:07.054673 140162047825664 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5242239832878113, loss=1.7470444440841675
I0405 05:31:23.597946 140162039432960 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6583282947540283, loss=1.7132493257522583
I0405 05:32:43.985581 140162047825664 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.505772054195404, loss=1.745947241783142
I0405 05:34:03.317238 140162703185664 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.48718762397766113, loss=1.7406067848205566
I0405 05:35:18.639872 140162694792960 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.43767714500427246, loss=1.7054988145828247
I0405 05:36:34.046295 140162703185664 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6869955658912659, loss=1.650648832321167
I0405 05:37:49.401461 140162694792960 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6934061646461487, loss=1.626798152923584
I0405 05:39:04.733023 140162703185664 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5773336291313171, loss=1.6881725788116455
I0405 05:40:20.119349 140162694792960 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.45804721117019653, loss=1.6393024921417236
I0405 05:41:35.544085 140162703185664 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.46660467982292175, loss=1.671585202217102
I0405 05:42:53.516650 140162694792960 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4931853711605072, loss=1.6626883745193481
I0405 05:44:12.955307 140162703185664 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.46422767639160156, loss=1.6435173749923706
I0405 05:45:34.360853 140162694792960 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5207825899124146, loss=1.6331465244293213
I0405 05:46:56.541326 140162703185664 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3999027609825134, loss=1.591303825378418
I0405 05:48:11.917832 140162694792960 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5184699296951294, loss=1.6989548206329346
I0405 05:49:27.385354 140162703185664 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3718356192111969, loss=1.5887236595153809
I0405 05:50:42.830662 140162694792960 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.42711329460144043, loss=1.6773138046264648
I0405 05:51:58.165958 140162703185664 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.46508991718292236, loss=1.6136921644210815
I0405 05:53:13.492979 140162694792960 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5533848404884338, loss=1.6306220293045044
I0405 05:54:28.782310 140162703185664 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.41692277789115906, loss=1.6341331005096436
I0405 05:55:44.209487 140162694792960 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4345409572124481, loss=1.5328154563903809
I0405 05:57:02.190541 140162703185664 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.45092248916625977, loss=1.632516622543335
I0405 05:58:22.499143 140162694792960 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3921984136104584, loss=1.6227301359176636
I0405 05:59:40.333318 140335240107840 submission_runner.py:373] Before eval at step 9299: RAM USED (GB) 22.870167552
I0405 05:59:40.333761 140335240107840 spec.py:298] Evaluating on the training split.
I0405 06:00:16.417135 140335240107840 spec.py:310] Evaluating on the validation split.
I0405 06:00:54.851659 140335240107840 spec.py:326] Evaluating on the test split.
I0405 06:01:14.566060 140335240107840 submission_runner.py:382] Time since start: 7558.80s, 	Step: 9299, 	{'train/ctc_loss': DeviceArray(0.37607524, dtype=float32), 'train/wer': 0.1397125100811234, 'validation/ctc_loss': DeviceArray(0.70786303, dtype=float32), 'validation/wer': 0.21389497245511294, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45548412, dtype=float32), 'test/wer': 0.15473361363313226, 'test/num_examples': 2472}
I0405 06:01:14.567565 140335240107840 submission_runner.py:396] After eval at step 9299: RAM USED (GB) 22.6582528
I0405 06:01:14.586216 140162119505664 logging_writer.py:48] [9299] global_step=9299, preemption_count=0, score=7240.381781, test/ctc_loss=0.4554841220378876, test/num_examples=2472, test/wer=0.154734, total_duration=7558.795280, train/ctc_loss=0.37607523798942566, train/wer=0.139713, validation/ctc_loss=0.7078630328178406, validation/num_examples=5348, validation/wer=0.213895
I0405 06:01:14.908956 140335240107840 checkpoints.py:356] Saving checkpoint at step: 9299
I0405 06:01:16.300989 140335240107840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_9299
I0405 06:01:16.333768 140335240107840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_9299.
I0405 06:01:16.348626 140335240107840 submission_runner.py:416] After logging and checkpointing eval at step 9299: RAM USED (GB) 22.6818048
I0405 06:01:17.929894 140162111112960 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4195040166378021, loss=1.5759704113006592
I0405 06:02:33.147389 140160849745664 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7039445042610168, loss=1.6161481142044067
I0405 06:03:48.524646 140162111112960 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7514573931694031, loss=1.61561918258667
I0405 06:05:03.798325 140160849745664 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5096085667610168, loss=1.5074360370635986
I0405 06:06:19.176206 140162111112960 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4596042335033417, loss=1.5823694467544556
I0405 06:07:34.397771 140160849745664 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5787772536277771, loss=1.6164202690124512
I0405 06:08:49.624695 140162111112960 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.43106329441070557, loss=1.5461082458496094
I0405 06:10:03.727179 140335240107840 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 23.394729984
I0405 06:10:03.727391 140335240107840 spec.py:298] Evaluating on the training split.
I0405 06:10:40.034523 140335240107840 spec.py:310] Evaluating on the validation split.
I0405 06:11:19.535097 140335240107840 spec.py:326] Evaluating on the test split.
I0405 06:11:39.272665 140335240107840 submission_runner.py:382] Time since start: 8182.19s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(0.37331998, dtype=float32), 'train/wer': 0.13429819141471794, 'validation/ctc_loss': DeviceArray(0.69834214, dtype=float32), 'validation/wer': 0.20933149379154647, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.43828043, dtype=float32), 'test/wer': 0.148904190278878, 'test/num_examples': 2472}
I0405 06:11:39.273970 140335240107840 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 23.292481536
I0405 06:11:39.290820 140162119505664 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7766.075171, test/ctc_loss=0.4382804334163666, test/num_examples=2472, test/wer=0.148904, total_duration=8182.190292, train/ctc_loss=0.37331998348236084, train/wer=0.134298, validation/ctc_loss=0.6983421444892883, validation/num_examples=5348, validation/wer=0.209331
I0405 06:11:39.614049 140335240107840 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 06:11:40.976914 140335240107840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 06:11:41.009136 140335240107840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 06:11:41.024680 140335240107840 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 23.317291008
I0405 06:11:41.031799 140162111112960 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7766.075171
I0405 06:11:41.250749 140335240107840 checkpoints.py:356] Saving checkpoint at step: 10000
I0405 06:11:43.131137 140335240107840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000
I0405 06:11:43.163797 140335240107840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0405 06:11:44.635026 140335240107840 submission_runner.py:550] Tuning trial 1/1
I0405 06:11:44.635283 140335240107840 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 06:11:44.640359 140335240107840 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.45655, dtype=float32), 'train/wer': 0.9792547321075318, 'validation/ctc_loss': DeviceArray(30.542128, dtype=float32), 'validation/wer': 0.9453347355015485, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.586128, dtype=float32), 'test/wer': 0.9475961245506063, 'test/num_examples': 2472, 'score': 62.57194757461548, 'total_duration': 62.772048234939575, 'global_step': 1, 'preemption_count': 0}), (3081, {'train/ctc_loss': DeviceArray(2.4590857, dtype=float32), 'train/wer': 0.5374915037795308, 'validation/ctc_loss': DeviceArray(2.8935754, dtype=float32), 'validation/wer': 0.5866916226881108, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.5527766, dtype=float32), 'test/wer': 0.5359413401580241, 'test/num_examples': 2472, 'score': 2455.5502767562866, 'total_duration': 2571.0798716545105, 'global_step': 3081, 'preemption_count': 0}), (6179, {'train/ctc_loss': DeviceArray(0.5645077, dtype=float32), 'train/wer': 0.19523746104466877, 'validation/ctc_loss': DeviceArray(0.8975538, dtype=float32), 'validation/wer': 0.2651062721299771, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.60990775, dtype=float32), 'test/wer': 0.20252676050616455, 'test/num_examples': 2472, 'score': 4847.912944793701, 'total_duration': 5063.163577795029, 'global_step': 6179, 'preemption_count': 0}), (9299, {'train/ctc_loss': DeviceArray(0.37607524, dtype=float32), 'train/wer': 0.1397125100811234, 'validation/ctc_loss': DeviceArray(0.70786303, dtype=float32), 'validation/wer': 0.21389497245511294, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.45548412, dtype=float32), 'test/wer': 0.15473361363313226, 'test/num_examples': 2472, 'score': 7240.381781339645, 'total_duration': 7558.795279979706, 'global_step': 9299, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(0.37331998, dtype=float32), 'train/wer': 0.13429819141471794, 'validation/ctc_loss': DeviceArray(0.69834214, dtype=float32), 'validation/wer': 0.20933149379154647, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.43828043, dtype=float32), 'test/wer': 0.148904190278878, 'test/num_examples': 2472, 'score': 7766.075170755386, 'total_duration': 8182.190291881561, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0405 06:11:44.640494 140335240107840 submission_runner.py:553] Timing: 7766.075170755386
I0405 06:11:44.640602 140335240107840 submission_runner.py:554] ====================
I0405 06:11:44.641039 140335240107840 submission_runner.py:613] Final librispeech_conformer score: 7766.075170755386
