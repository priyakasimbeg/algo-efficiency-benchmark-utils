WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 00:46:52.508193 140219835778880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 00:46:52.508244 140413117577024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 00:46:53.486797 139913237714752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 00:46:53.486921 140425266091840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 00:46:53.487125 139830848083776 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 00:46:53.487309 140120589535040 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 00:46:53.487478 140667004000064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 00:46:53.493161 140343431890752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 00:46:53.493530 140343431890752 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.497407 139913237714752 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.497620 140425266091840 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.497969 140120589535040 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.497942 139830848083776 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.498112 140667004000064 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.502658 140219835778880 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:53.502725 140413117577024 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 00:46:57.351347 140343431890752 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_adamw/wmt_pytorch.
W0406 00:46:57.381120 139913237714752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.382239 140413117577024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.382411 140120589535040 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.382741 140425266091840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.383163 139830848083776 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.383203 140667004000064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.383875 140219835778880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 00:46:57.384373 140343431890752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 00:46:57.387768 140343431890752 submission_runner.py:511] Using RNG seed 1248559880
I0406 00:46:57.388855 140343431890752 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 00:46:57.388974 140343431890752 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1.
I0406 00:46:57.389174 140343431890752 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/hparams.json.
I0406 00:46:57.390157 140343431890752 submission_runner.py:230] Starting train once: RAM USED (GB) 15.09361664
I0406 00:46:57.390261 140343431890752 submission_runner.py:231] Initializing dataset.
I0406 00:46:57.390465 140343431890752 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 15.09361664
I0406 00:46:57.390542 140343431890752 submission_runner.py:240] Initializing model.
I0406 00:47:00.917724 140343431890752 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.430744064
I0406 00:47:00.917901 140343431890752 submission_runner.py:252] Initializing optimizer.
I0406 00:47:00.918998 140343431890752 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.430744064
I0406 00:47:00.919122 140343431890752 submission_runner.py:261] Initializing metrics bundle.
I0406 00:47:00.919171 140343431890752 submission_runner.py:276] Initializing checkpoint and logger.
I0406 00:47:00.920521 140343431890752 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 00:47:00.920632 140343431890752 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 00:47:01.566939 140343431890752 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0406 00:47:01.567740 140343431890752 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/flags_0.json.
I0406 00:47:01.603959 140343431890752 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 19.513593856
I0406 00:47:01.605090 140343431890752 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.513593856
I0406 00:47:01.605224 140343431890752 submission_runner.py:313] Starting training loop.
I0406 00:47:01.615854 140343431890752 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:47:01.619974 140343431890752 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:47:01.620117 140343431890752 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:47:01.675783 140343431890752 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:47:03.835010 140343431890752 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 19.776032768
I0406 00:47:05.464118 140295995905792 logging_writer.py:48] [0] global_step=0, grad_norm=5.338627, loss=11.211571
I0406 00:47:05.469331 140343431890752 submission.py:119] 0) loss = 11.212, grad_norm = 5.339
I0406 00:47:05.470428 140343431890752 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 19.877515264
I0406 00:47:05.471029 140343431890752 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 19.877515264
I0406 00:47:05.471142 140343431890752 spec.py:298] Evaluating on the training split.
I0406 00:47:05.473430 140343431890752 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:47:05.476045 140343431890752 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:47:05.476150 140343431890752 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:47:05.504335 140343431890752 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0406 00:47:09.548400 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 00:51:44.433208 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 00:51:44.436904 140343431890752 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:51:44.440517 140343431890752 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:51:44.440636 140343431890752 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:51:44.469869 140343431890752 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:51:48.286710 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 00:56:17.418038 140343431890752 spec.py:326] Evaluating on the test split.
I0406 00:56:17.420747 140343431890752 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:56:17.423458 140343431890752 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0406 00:56:17.423574 140343431890752 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0406 00:56:17.450929 140343431890752 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0406 00:56:21.355152 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:00:55.949220 140343431890752 submission_runner.py:382] Time since start: 3.87s, 	Step: 1, 	{'train/accuracy': 0.0006748178563667349, 'train/loss': 11.2061261165948, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.195090575442338, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.190892452501307, 'test/bleu': 0.0, 'test/num_examples': 3003}
I0406 01:00:55.949669 140343431890752 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.208062464
I0406 01:00:55.957905 140285726988032 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=3.864369, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.190892, test/num_examples=3003, total_duration=3.866407, train/accuracy=0.000675, train/bleu=0.000000, train/loss=11.206126, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.195091, validation/num_examples=3000
I0406 01:00:58.220311 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_1.
I0406 01:00:58.220891 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.208197632
I0406 01:00:58.225715 140343431890752 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.208197632
I0406 01:00:58.230581 140120589535040 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230587 139913237714752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230587 140425266091840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230592 140667004000064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230594 140413117577024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230602 140219835778880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230598 139830848083776 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.230853 140343431890752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 01:00:58.656432 140285718595328 logging_writer.py:48] [1] global_step=1, grad_norm=5.375254, loss=11.204609
I0406 01:00:58.660009 140343431890752 submission.py:119] 1) loss = 11.205, grad_norm = 5.375
I0406 01:00:58.660744 140343431890752 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.208033792
I0406 01:00:59.098198 140285726988032 logging_writer.py:48] [2] global_step=2, grad_norm=5.427863, loss=11.208863
I0406 01:00:59.101411 140343431890752 submission.py:119] 2) loss = 11.209, grad_norm = 5.428
I0406 01:00:59.538860 140285718595328 logging_writer.py:48] [3] global_step=3, grad_norm=5.331887, loss=11.181576
I0406 01:00:59.541871 140343431890752 submission.py:119] 3) loss = 11.182, grad_norm = 5.332
I0406 01:00:59.977609 140285726988032 logging_writer.py:48] [4] global_step=4, grad_norm=5.350310, loss=11.160646
I0406 01:00:59.981186 140343431890752 submission.py:119] 4) loss = 11.161, grad_norm = 5.350
I0406 01:01:00.419545 140285718595328 logging_writer.py:48] [5] global_step=5, grad_norm=5.210206, loss=11.132298
I0406 01:01:00.422553 140343431890752 submission.py:119] 5) loss = 11.132, grad_norm = 5.210
I0406 01:01:00.858649 140285726988032 logging_writer.py:48] [6] global_step=6, grad_norm=5.293788, loss=11.085536
I0406 01:01:00.861733 140343431890752 submission.py:119] 6) loss = 11.086, grad_norm = 5.294
I0406 01:01:01.300919 140285718595328 logging_writer.py:48] [7] global_step=7, grad_norm=5.107981, loss=11.051280
I0406 01:01:01.303869 140343431890752 submission.py:119] 7) loss = 11.051, grad_norm = 5.108
I0406 01:01:01.741893 140285726988032 logging_writer.py:48] [8] global_step=8, grad_norm=5.043324, loss=11.009167
I0406 01:01:01.744840 140343431890752 submission.py:119] 8) loss = 11.009, grad_norm = 5.043
I0406 01:01:02.184461 140285718595328 logging_writer.py:48] [9] global_step=9, grad_norm=4.924421, loss=10.941416
I0406 01:01:02.187617 140343431890752 submission.py:119] 9) loss = 10.941, grad_norm = 4.924
I0406 01:01:02.634068 140285726988032 logging_writer.py:48] [10] global_step=10, grad_norm=4.818386, loss=10.880572
I0406 01:01:02.637140 140343431890752 submission.py:119] 10) loss = 10.881, grad_norm = 4.818
I0406 01:01:03.078612 140285718595328 logging_writer.py:48] [11] global_step=11, grad_norm=4.700846, loss=10.810022
I0406 01:01:03.081670 140343431890752 submission.py:119] 11) loss = 10.810, grad_norm = 4.701
I0406 01:01:03.519145 140285726988032 logging_writer.py:48] [12] global_step=12, grad_norm=4.433897, loss=10.735787
I0406 01:01:03.523125 140343431890752 submission.py:119] 12) loss = 10.736, grad_norm = 4.434
I0406 01:01:03.963470 140285718595328 logging_writer.py:48] [13] global_step=13, grad_norm=4.296747, loss=10.667454
I0406 01:01:03.967139 140343431890752 submission.py:119] 13) loss = 10.667, grad_norm = 4.297
I0406 01:01:04.406697 140285726988032 logging_writer.py:48] [14] global_step=14, grad_norm=4.082240, loss=10.597798
I0406 01:01:04.410377 140343431890752 submission.py:119] 14) loss = 10.598, grad_norm = 4.082
I0406 01:01:04.845548 140285718595328 logging_writer.py:48] [15] global_step=15, grad_norm=3.888127, loss=10.513260
I0406 01:01:04.849192 140343431890752 submission.py:119] 15) loss = 10.513, grad_norm = 3.888
I0406 01:01:05.287889 140285726988032 logging_writer.py:48] [16] global_step=16, grad_norm=3.703603, loss=10.438103
I0406 01:01:05.291305 140343431890752 submission.py:119] 16) loss = 10.438, grad_norm = 3.704
I0406 01:01:05.737173 140285718595328 logging_writer.py:48] [17] global_step=17, grad_norm=3.459019, loss=10.363069
I0406 01:01:05.740171 140343431890752 submission.py:119] 17) loss = 10.363, grad_norm = 3.459
I0406 01:01:06.180369 140285726988032 logging_writer.py:48] [18] global_step=18, grad_norm=3.262907, loss=10.262075
I0406 01:01:06.184078 140343431890752 submission.py:119] 18) loss = 10.262, grad_norm = 3.263
I0406 01:01:06.625467 140285718595328 logging_writer.py:48] [19] global_step=19, grad_norm=3.065321, loss=10.211947
I0406 01:01:06.629186 140343431890752 submission.py:119] 19) loss = 10.212, grad_norm = 3.065
I0406 01:01:07.066854 140285726988032 logging_writer.py:48] [20] global_step=20, grad_norm=2.895070, loss=10.133845
I0406 01:01:07.070155 140343431890752 submission.py:119] 20) loss = 10.134, grad_norm = 2.895
I0406 01:01:07.507988 140285718595328 logging_writer.py:48] [21] global_step=21, grad_norm=2.707026, loss=10.050612
I0406 01:01:07.511624 140343431890752 submission.py:119] 21) loss = 10.051, grad_norm = 2.707
I0406 01:01:07.954939 140285726988032 logging_writer.py:48] [22] global_step=22, grad_norm=2.564716, loss=9.976111
I0406 01:01:07.958671 140343431890752 submission.py:119] 22) loss = 9.976, grad_norm = 2.565
I0406 01:01:08.395487 140285718595328 logging_writer.py:48] [23] global_step=23, grad_norm=2.341038, loss=9.914494
I0406 01:01:08.399039 140343431890752 submission.py:119] 23) loss = 9.914, grad_norm = 2.341
I0406 01:01:08.840210 140285726988032 logging_writer.py:48] [24] global_step=24, grad_norm=2.218998, loss=9.848759
I0406 01:01:08.843737 140343431890752 submission.py:119] 24) loss = 9.849, grad_norm = 2.219
I0406 01:01:09.282317 140285718595328 logging_writer.py:48] [25] global_step=25, grad_norm=2.071095, loss=9.785235
I0406 01:01:09.285906 140343431890752 submission.py:119] 25) loss = 9.785, grad_norm = 2.071
I0406 01:01:09.724927 140285726988032 logging_writer.py:48] [26] global_step=26, grad_norm=1.960045, loss=9.715176
I0406 01:01:09.728602 140343431890752 submission.py:119] 26) loss = 9.715, grad_norm = 1.960
I0406 01:01:10.167280 140285718595328 logging_writer.py:48] [27] global_step=27, grad_norm=1.801994, loss=9.669552
I0406 01:01:10.171110 140343431890752 submission.py:119] 27) loss = 9.670, grad_norm = 1.802
I0406 01:01:10.606596 140285726988032 logging_writer.py:48] [28] global_step=28, grad_norm=1.695196, loss=9.619301
I0406 01:01:10.610074 140343431890752 submission.py:119] 28) loss = 9.619, grad_norm = 1.695
I0406 01:01:11.049385 140285718595328 logging_writer.py:48] [29] global_step=29, grad_norm=1.590229, loss=9.557088
I0406 01:01:11.052942 140343431890752 submission.py:119] 29) loss = 9.557, grad_norm = 1.590
I0406 01:01:11.489842 140285726988032 logging_writer.py:48] [30] global_step=30, grad_norm=1.491265, loss=9.507103
I0406 01:01:11.493304 140343431890752 submission.py:119] 30) loss = 9.507, grad_norm = 1.491
I0406 01:01:11.929162 140285718595328 logging_writer.py:48] [31] global_step=31, grad_norm=1.403942, loss=9.458154
I0406 01:01:11.932915 140343431890752 submission.py:119] 31) loss = 9.458, grad_norm = 1.404
I0406 01:01:12.374042 140285726988032 logging_writer.py:48] [32] global_step=32, grad_norm=1.310644, loss=9.424911
I0406 01:01:12.377134 140343431890752 submission.py:119] 32) loss = 9.425, grad_norm = 1.311
I0406 01:01:12.813701 140285718595328 logging_writer.py:48] [33] global_step=33, grad_norm=1.216104, loss=9.386512
I0406 01:01:12.817507 140343431890752 submission.py:119] 33) loss = 9.387, grad_norm = 1.216
I0406 01:01:13.253949 140285726988032 logging_writer.py:48] [34] global_step=34, grad_norm=1.159537, loss=9.348467
I0406 01:01:13.257550 140343431890752 submission.py:119] 34) loss = 9.348, grad_norm = 1.160
I0406 01:01:13.697326 140285718595328 logging_writer.py:48] [35] global_step=35, grad_norm=1.083535, loss=9.306114
I0406 01:01:13.701140 140343431890752 submission.py:119] 35) loss = 9.306, grad_norm = 1.084
I0406 01:01:14.144207 140285726988032 logging_writer.py:48] [36] global_step=36, grad_norm=1.016077, loss=9.300535
I0406 01:01:14.147909 140343431890752 submission.py:119] 36) loss = 9.301, grad_norm = 1.016
I0406 01:01:14.585660 140285718595328 logging_writer.py:48] [37] global_step=37, grad_norm=0.960548, loss=9.258039
I0406 01:01:14.588966 140343431890752 submission.py:119] 37) loss = 9.258, grad_norm = 0.961
I0406 01:01:15.028349 140285726988032 logging_writer.py:48] [38] global_step=38, grad_norm=0.915394, loss=9.232227
I0406 01:01:15.031959 140343431890752 submission.py:119] 38) loss = 9.232, grad_norm = 0.915
I0406 01:01:15.470414 140285718595328 logging_writer.py:48] [39] global_step=39, grad_norm=0.862112, loss=9.220407
I0406 01:01:15.473977 140343431890752 submission.py:119] 39) loss = 9.220, grad_norm = 0.862
I0406 01:01:15.911943 140285726988032 logging_writer.py:48] [40] global_step=40, grad_norm=0.837562, loss=9.156290
I0406 01:01:15.915509 140343431890752 submission.py:119] 40) loss = 9.156, grad_norm = 0.838
I0406 01:01:16.352084 140285718595328 logging_writer.py:48] [41] global_step=41, grad_norm=0.789701, loss=9.137055
I0406 01:01:16.355414 140343431890752 submission.py:119] 41) loss = 9.137, grad_norm = 0.790
I0406 01:01:16.790831 140285726988032 logging_writer.py:48] [42] global_step=42, grad_norm=0.741745, loss=9.135869
I0406 01:01:16.794763 140343431890752 submission.py:119] 42) loss = 9.136, grad_norm = 0.742
I0406 01:01:17.244118 140285718595328 logging_writer.py:48] [43] global_step=43, grad_norm=0.721332, loss=9.095929
I0406 01:01:17.247666 140343431890752 submission.py:119] 43) loss = 9.096, grad_norm = 0.721
I0406 01:01:17.686356 140285726988032 logging_writer.py:48] [44] global_step=44, grad_norm=0.681706, loss=9.074215
I0406 01:01:17.690108 140343431890752 submission.py:119] 44) loss = 9.074, grad_norm = 0.682
I0406 01:01:18.131404 140285718595328 logging_writer.py:48] [45] global_step=45, grad_norm=0.662297, loss=9.014708
I0406 01:01:18.135208 140343431890752 submission.py:119] 45) loss = 9.015, grad_norm = 0.662
I0406 01:01:18.573347 140285726988032 logging_writer.py:48] [46] global_step=46, grad_norm=0.625012, loss=9.011804
I0406 01:01:18.576880 140343431890752 submission.py:119] 46) loss = 9.012, grad_norm = 0.625
I0406 01:01:19.014336 140285718595328 logging_writer.py:48] [47] global_step=47, grad_norm=0.594026, loss=9.027225
I0406 01:01:19.017906 140343431890752 submission.py:119] 47) loss = 9.027, grad_norm = 0.594
I0406 01:01:19.456928 140285726988032 logging_writer.py:48] [48] global_step=48, grad_norm=0.570255, loss=9.002551
I0406 01:01:19.460624 140343431890752 submission.py:119] 48) loss = 9.003, grad_norm = 0.570
I0406 01:01:19.903659 140285718595328 logging_writer.py:48] [49] global_step=49, grad_norm=0.559792, loss=8.949932
I0406 01:01:19.907708 140343431890752 submission.py:119] 49) loss = 8.950, grad_norm = 0.560
I0406 01:01:20.343561 140285726988032 logging_writer.py:48] [50] global_step=50, grad_norm=0.531295, loss=8.958336
I0406 01:01:20.347319 140343431890752 submission.py:119] 50) loss = 8.958, grad_norm = 0.531
I0406 01:01:20.787043 140285718595328 logging_writer.py:48] [51] global_step=51, grad_norm=0.502287, loss=8.930894
I0406 01:01:20.790656 140343431890752 submission.py:119] 51) loss = 8.931, grad_norm = 0.502
I0406 01:01:21.240830 140285726988032 logging_writer.py:48] [52] global_step=52, grad_norm=0.484440, loss=8.906995
I0406 01:01:21.244854 140343431890752 submission.py:119] 52) loss = 8.907, grad_norm = 0.484
I0406 01:01:21.683874 140285718595328 logging_writer.py:48] [53] global_step=53, grad_norm=0.455540, loss=8.933150
I0406 01:01:21.687696 140343431890752 submission.py:119] 53) loss = 8.933, grad_norm = 0.456
I0406 01:01:22.138202 140285726988032 logging_writer.py:48] [54] global_step=54, grad_norm=0.450362, loss=8.897213
I0406 01:01:22.142152 140343431890752 submission.py:119] 54) loss = 8.897, grad_norm = 0.450
I0406 01:01:22.591135 140285718595328 logging_writer.py:48] [55] global_step=55, grad_norm=0.433333, loss=8.914642
I0406 01:01:22.595137 140343431890752 submission.py:119] 55) loss = 8.915, grad_norm = 0.433
I0406 01:01:23.032643 140285726988032 logging_writer.py:48] [56] global_step=56, grad_norm=0.409720, loss=8.884683
I0406 01:01:23.036524 140343431890752 submission.py:119] 56) loss = 8.885, grad_norm = 0.410
I0406 01:01:23.478011 140285718595328 logging_writer.py:48] [57] global_step=57, grad_norm=0.406598, loss=8.862670
I0406 01:01:23.481685 140343431890752 submission.py:119] 57) loss = 8.863, grad_norm = 0.407
I0406 01:01:23.923949 140285726988032 logging_writer.py:48] [58] global_step=58, grad_norm=0.389655, loss=8.835712
I0406 01:01:23.927593 140343431890752 submission.py:119] 58) loss = 8.836, grad_norm = 0.390
I0406 01:01:24.365612 140285718595328 logging_writer.py:48] [59] global_step=59, grad_norm=0.367135, loss=8.867011
I0406 01:01:24.369538 140343431890752 submission.py:119] 59) loss = 8.867, grad_norm = 0.367
I0406 01:01:24.809120 140285726988032 logging_writer.py:48] [60] global_step=60, grad_norm=0.372722, loss=8.830909
I0406 01:01:24.812776 140343431890752 submission.py:119] 60) loss = 8.831, grad_norm = 0.373
I0406 01:01:25.250133 140285718595328 logging_writer.py:48] [61] global_step=61, grad_norm=0.347367, loss=8.817026
I0406 01:01:25.253827 140343431890752 submission.py:119] 61) loss = 8.817, grad_norm = 0.347
I0406 01:01:25.692980 140285726988032 logging_writer.py:48] [62] global_step=62, grad_norm=0.334408, loss=8.820782
I0406 01:01:25.696743 140343431890752 submission.py:119] 62) loss = 8.821, grad_norm = 0.334
I0406 01:01:26.139911 140285718595328 logging_writer.py:48] [63] global_step=63, grad_norm=0.328367, loss=8.805473
I0406 01:01:26.143667 140343431890752 submission.py:119] 63) loss = 8.805, grad_norm = 0.328
I0406 01:01:26.580033 140285726988032 logging_writer.py:48] [64] global_step=64, grad_norm=0.335377, loss=8.758781
I0406 01:01:26.583846 140343431890752 submission.py:119] 64) loss = 8.759, grad_norm = 0.335
I0406 01:01:27.026141 140285718595328 logging_writer.py:48] [65] global_step=65, grad_norm=0.312478, loss=8.816310
I0406 01:01:27.030003 140343431890752 submission.py:119] 65) loss = 8.816, grad_norm = 0.312
I0406 01:01:27.470601 140285726988032 logging_writer.py:48] [66] global_step=66, grad_norm=0.308792, loss=8.792073
I0406 01:01:27.474579 140343431890752 submission.py:119] 66) loss = 8.792, grad_norm = 0.309
I0406 01:01:27.935719 140285718595328 logging_writer.py:48] [67] global_step=67, grad_norm=0.299738, loss=8.731464
I0406 01:01:27.939892 140343431890752 submission.py:119] 67) loss = 8.731, grad_norm = 0.300
I0406 01:01:28.376230 140285726988032 logging_writer.py:48] [68] global_step=68, grad_norm=0.302777, loss=8.731587
I0406 01:01:28.380054 140343431890752 submission.py:119] 68) loss = 8.732, grad_norm = 0.303
I0406 01:01:28.817096 140285718595328 logging_writer.py:48] [69] global_step=69, grad_norm=0.290523, loss=8.738796
I0406 01:01:28.821040 140343431890752 submission.py:119] 69) loss = 8.739, grad_norm = 0.291
I0406 01:01:29.266734 140285726988032 logging_writer.py:48] [70] global_step=70, grad_norm=0.284356, loss=8.742510
I0406 01:01:29.270494 140343431890752 submission.py:119] 70) loss = 8.743, grad_norm = 0.284
I0406 01:01:29.708857 140285718595328 logging_writer.py:48] [71] global_step=71, grad_norm=0.309914, loss=8.739844
I0406 01:01:29.712538 140343431890752 submission.py:119] 71) loss = 8.740, grad_norm = 0.310
I0406 01:01:30.161692 140285726988032 logging_writer.py:48] [72] global_step=72, grad_norm=0.264962, loss=8.711350
I0406 01:01:30.165224 140343431890752 submission.py:119] 72) loss = 8.711, grad_norm = 0.265
I0406 01:01:30.609940 140285718595328 logging_writer.py:48] [73] global_step=73, grad_norm=0.271563, loss=8.716040
I0406 01:01:30.613734 140343431890752 submission.py:119] 73) loss = 8.716, grad_norm = 0.272
I0406 01:01:31.051468 140285726988032 logging_writer.py:48] [74] global_step=74, grad_norm=0.269139, loss=8.679313
I0406 01:01:31.055179 140343431890752 submission.py:119] 74) loss = 8.679, grad_norm = 0.269
I0406 01:01:31.493655 140285718595328 logging_writer.py:48] [75] global_step=75, grad_norm=0.272495, loss=8.692298
I0406 01:01:31.497236 140343431890752 submission.py:119] 75) loss = 8.692, grad_norm = 0.272
I0406 01:01:31.934546 140285726988032 logging_writer.py:48] [76] global_step=76, grad_norm=0.265683, loss=8.682875
I0406 01:01:31.938298 140343431890752 submission.py:119] 76) loss = 8.683, grad_norm = 0.266
I0406 01:01:32.375344 140285718595328 logging_writer.py:48] [77] global_step=77, grad_norm=0.253367, loss=8.669559
I0406 01:01:32.379063 140343431890752 submission.py:119] 77) loss = 8.670, grad_norm = 0.253
I0406 01:01:32.826980 140285726988032 logging_writer.py:48] [78] global_step=78, grad_norm=0.259892, loss=8.659615
I0406 01:01:32.830822 140343431890752 submission.py:119] 78) loss = 8.660, grad_norm = 0.260
I0406 01:01:33.268663 140285718595328 logging_writer.py:48] [79] global_step=79, grad_norm=0.244994, loss=8.648863
I0406 01:01:33.272583 140343431890752 submission.py:119] 79) loss = 8.649, grad_norm = 0.245
I0406 01:01:33.712902 140285726988032 logging_writer.py:48] [80] global_step=80, grad_norm=0.250062, loss=8.679880
I0406 01:01:33.716504 140343431890752 submission.py:119] 80) loss = 8.680, grad_norm = 0.250
I0406 01:01:34.152703 140285718595328 logging_writer.py:48] [81] global_step=81, grad_norm=0.240362, loss=8.676072
I0406 01:01:34.156322 140343431890752 submission.py:119] 81) loss = 8.676, grad_norm = 0.240
I0406 01:01:34.594222 140285726988032 logging_writer.py:48] [82] global_step=82, grad_norm=0.262317, loss=8.630658
I0406 01:01:34.597546 140343431890752 submission.py:119] 82) loss = 8.631, grad_norm = 0.262
I0406 01:01:35.046536 140285718595328 logging_writer.py:48] [83] global_step=83, grad_norm=0.248809, loss=8.637619
I0406 01:01:35.050427 140343431890752 submission.py:119] 83) loss = 8.638, grad_norm = 0.249
I0406 01:01:35.492789 140285726988032 logging_writer.py:48] [84] global_step=84, grad_norm=0.236784, loss=8.651961
I0406 01:01:35.496529 140343431890752 submission.py:119] 84) loss = 8.652, grad_norm = 0.237
I0406 01:01:35.942258 140285718595328 logging_writer.py:48] [85] global_step=85, grad_norm=0.230669, loss=8.648784
I0406 01:01:35.946149 140343431890752 submission.py:119] 85) loss = 8.649, grad_norm = 0.231
I0406 01:01:36.386315 140285726988032 logging_writer.py:48] [86] global_step=86, grad_norm=0.240414, loss=8.633609
I0406 01:01:36.389995 140343431890752 submission.py:119] 86) loss = 8.634, grad_norm = 0.240
I0406 01:01:36.826791 140285718595328 logging_writer.py:48] [87] global_step=87, grad_norm=0.227297, loss=8.597280
I0406 01:01:36.830605 140343431890752 submission.py:119] 87) loss = 8.597, grad_norm = 0.227
I0406 01:01:37.267960 140285726988032 logging_writer.py:48] [88] global_step=88, grad_norm=0.236677, loss=8.578136
I0406 01:01:37.271670 140343431890752 submission.py:119] 88) loss = 8.578, grad_norm = 0.237
I0406 01:01:37.708571 140285718595328 logging_writer.py:48] [89] global_step=89, grad_norm=0.232398, loss=8.584693
I0406 01:01:37.712199 140343431890752 submission.py:119] 89) loss = 8.585, grad_norm = 0.232
I0406 01:01:38.150567 140285726988032 logging_writer.py:48] [90] global_step=90, grad_norm=0.257587, loss=8.611166
I0406 01:01:38.154108 140343431890752 submission.py:119] 90) loss = 8.611, grad_norm = 0.258
I0406 01:01:38.592346 140285718595328 logging_writer.py:48] [91] global_step=91, grad_norm=0.222572, loss=8.617339
I0406 01:01:38.595870 140343431890752 submission.py:119] 91) loss = 8.617, grad_norm = 0.223
I0406 01:01:39.037235 140285726988032 logging_writer.py:48] [92] global_step=92, grad_norm=0.225501, loss=8.582191
I0406 01:01:39.040788 140343431890752 submission.py:119] 92) loss = 8.582, grad_norm = 0.226
I0406 01:01:39.475382 140285718595328 logging_writer.py:48] [93] global_step=93, grad_norm=0.239579, loss=8.586173
I0406 01:01:39.479220 140343431890752 submission.py:119] 93) loss = 8.586, grad_norm = 0.240
I0406 01:01:39.924923 140285726988032 logging_writer.py:48] [94] global_step=94, grad_norm=0.214979, loss=8.585462
I0406 01:01:39.928562 140343431890752 submission.py:119] 94) loss = 8.585, grad_norm = 0.215
I0406 01:01:40.365377 140285718595328 logging_writer.py:48] [95] global_step=95, grad_norm=0.239904, loss=8.558043
I0406 01:01:40.368951 140343431890752 submission.py:119] 95) loss = 8.558, grad_norm = 0.240
I0406 01:01:40.804514 140285726988032 logging_writer.py:48] [96] global_step=96, grad_norm=0.218064, loss=8.538167
I0406 01:01:40.808262 140343431890752 submission.py:119] 96) loss = 8.538, grad_norm = 0.218
I0406 01:01:41.246108 140285718595328 logging_writer.py:48] [97] global_step=97, grad_norm=0.210341, loss=8.552513
I0406 01:01:41.249640 140343431890752 submission.py:119] 97) loss = 8.553, grad_norm = 0.210
I0406 01:01:41.685500 140285726988032 logging_writer.py:48] [98] global_step=98, grad_norm=0.254410, loss=8.601377
I0406 01:01:41.689317 140343431890752 submission.py:119] 98) loss = 8.601, grad_norm = 0.254
I0406 01:01:42.128467 140285718595328 logging_writer.py:48] [99] global_step=99, grad_norm=0.216162, loss=8.570668
I0406 01:01:42.132191 140343431890752 submission.py:119] 99) loss = 8.571, grad_norm = 0.216
I0406 01:01:42.579663 140285726988032 logging_writer.py:48] [100] global_step=100, grad_norm=0.210884, loss=8.532011
I0406 01:01:42.583556 140343431890752 submission.py:119] 100) loss = 8.532, grad_norm = 0.211
I0406 01:04:36.577491 140285718595328 logging_writer.py:48] [500] global_step=500, grad_norm=0.917514, loss=6.886153
I0406 01:04:36.581048 140343431890752 submission.py:119] 500) loss = 6.886, grad_norm = 0.918
I0406 01:08:13.641198 140285726988032 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.460863, loss=7.267398
I0406 01:08:13.644850 140343431890752 submission.py:119] 1000) loss = 7.267, grad_norm = 0.461
I0406 01:11:50.608675 140285718595328 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.644131, loss=5.460595
I0406 01:11:50.612325 140343431890752 submission.py:119] 1500) loss = 5.461, grad_norm = 0.644
I0406 01:14:58.391163 140343431890752 submission_runner.py:373] Before eval at step 1933: RAM USED (GB) 20.57570304
I0406 01:14:58.391399 140343431890752 spec.py:298] Evaluating on the training split.
I0406 01:15:02.273039 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:18:10.028353 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 01:18:13.776968 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:21:37.020539 140343431890752 spec.py:326] Evaluating on the test split.
I0406 01:21:40.826628 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:24:38.968361 140343431890752 submission_runner.py:382] Time since start: 1676.79s, 	Step: 1933, 	{'train/accuracy': 0.41384003110633333, 'train/loss': 3.9237708995677134, 'train/bleu': 14.197196529256603, 'validation/accuracy': 0.40153252904489717, 'validation/loss': 4.037600277739892, 'validation/bleu': 9.782129503012326, 'validation/num_examples': 3000, 'test/accuracy': 0.391447330195805, 'test/loss': 4.214490732670966, 'test/bleu': 8.179868569229866, 'test/num_examples': 3003}
I0406 01:24:38.968777 140343431890752 submission_runner.py:396] After eval at step 1933: RAM USED (GB) 20.82742272
I0406 01:24:38.977043 140285726988032 logging_writer.py:48] [1933] global_step=1933, preemption_count=0, score=841.290560, test/accuracy=0.391447, test/bleu=8.179869, test/loss=4.214491, test/num_examples=3003, total_duration=1676.786222, train/accuracy=0.413840, train/bleu=14.197197, train/loss=3.923771, validation/accuracy=0.401533, validation/bleu=9.782130, validation/loss=4.037600, validation/num_examples=3000
I0406 01:24:41.149985 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_1933.
I0406 01:24:41.150645 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 1933: RAM USED (GB) 20.828008448
I0406 01:25:10.692276 140285718595328 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.536239, loss=4.759131
I0406 01:25:10.695500 140343431890752 submission.py:119] 2000) loss = 4.759, grad_norm = 0.536
I0406 01:28:47.262978 140285726988032 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.505142, loss=6.141721
I0406 01:28:47.266003 140343431890752 submission.py:119] 2500) loss = 6.142, grad_norm = 0.505
I0406 01:32:24.418967 140285718595328 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.227011, loss=5.541407
I0406 01:32:24.422100 140343431890752 submission.py:119] 3000) loss = 5.541, grad_norm = 0.227
I0406 01:36:01.647368 140285726988032 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.434913, loss=4.251079
I0406 01:36:01.651176 140343431890752 submission.py:119] 3500) loss = 4.251, grad_norm = 0.435
I0406 01:38:41.470875 140343431890752 submission_runner.py:373] Before eval at step 3869: RAM USED (GB) 21.006962688
I0406 01:38:41.471210 140343431890752 spec.py:298] Evaluating on the training split.
I0406 01:38:45.359578 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:41:25.549474 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 01:41:29.289715 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:43:59.864346 140343431890752 spec.py:326] Evaluating on the test split.
I0406 01:44:03.681912 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 01:46:33.507127 140343431890752 submission_runner.py:382] Time since start: 3099.87s, 	Step: 3869, 	{'train/accuracy': 0.49830128461776046, 'train/loss': 3.0529808308262507, 'train/bleu': 21.131413677203952, 'validation/accuracy': 0.4951085541406802, 'validation/loss': 3.0834143795489206, 'validation/bleu': 16.434988731960566, 'validation/num_examples': 3000, 'test/accuracy': 0.49262680843646506, 'test/loss': 3.161857242461217, 'test/bleu': 15.20499883000344, 'test/num_examples': 3003}
I0406 01:46:33.507523 140343431890752 submission_runner.py:396] After eval at step 3869: RAM USED (GB) 21.094731776
I0406 01:46:33.515939 140285718595328 logging_writer.py:48] [3869] global_step=3869, preemption_count=0, score=1678.850252, test/accuracy=0.492627, test/bleu=15.204999, test/loss=3.161857, test/num_examples=3003, total_duration=3099.865773, train/accuracy=0.498301, train/bleu=21.131414, train/loss=3.052981, validation/accuracy=0.495109, validation/bleu=16.434989, validation/loss=3.083414, validation/num_examples=3000
I0406 01:46:35.686917 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_3869.
I0406 01:46:35.687571 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 3869: RAM USED (GB) 21.095378944
I0406 01:47:33.029533 140285726988032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.386096, loss=4.095679
I0406 01:47:33.033039 140343431890752 submission.py:119] 4000) loss = 4.096, grad_norm = 0.386
I0406 01:51:09.954504 140285718595328 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.287068, loss=3.833055
I0406 01:51:09.957601 140343431890752 submission.py:119] 4500) loss = 3.833, grad_norm = 0.287
I0406 01:54:47.297184 140285726988032 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.235829, loss=3.674808
I0406 01:54:47.301127 140343431890752 submission.py:119] 5000) loss = 3.675, grad_norm = 0.236
I0406 01:58:24.674717 140285718595328 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.228632, loss=3.682985
I0406 01:58:24.678719 140343431890752 submission.py:119] 5500) loss = 3.683, grad_norm = 0.229
I0406 02:00:35.838838 140343431890752 submission_runner.py:373] Before eval at step 5803: RAM USED (GB) 21.123403776
I0406 02:00:35.839062 140343431890752 spec.py:298] Evaluating on the training split.
I0406 02:00:39.706475 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:02:58.744741 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 02:03:02.472856 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:05:15.909886 140343431890752 spec.py:326] Evaluating on the test split.
I0406 02:05:19.723274 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:07:26.903141 140343431890752 submission_runner.py:382] Time since start: 4414.23s, 	Step: 5803, 	{'train/accuracy': 0.557129840546697, 'train/loss': 2.533334460421412, 'train/bleu': 25.644124504970385, 'validation/accuracy': 0.5694907688683339, 'validation/loss': 2.4203504916244065, 'validation/bleu': 22.138721025116936, 'validation/num_examples': 3000, 'test/accuracy': 0.5682877229678693, 'test/loss': 2.427389097089071, 'test/bleu': 20.748086605090865, 'test/num_examples': 3003}
I0406 02:07:26.903580 140343431890752 submission_runner.py:396] After eval at step 5803: RAM USED (GB) 21.211889664
I0406 02:07:26.912340 140285726988032 logging_writer.py:48] [5803] global_step=5803, preemption_count=0, score=2516.310323, test/accuracy=0.568288, test/bleu=20.748087, test/loss=2.427389, test/num_examples=3003, total_duration=4414.233889, train/accuracy=0.557130, train/bleu=25.644125, train/loss=2.533334, validation/accuracy=0.569491, validation/bleu=22.138721, validation/loss=2.420350, validation/num_examples=3000
I0406 02:07:29.199428 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_5803.
I0406 02:07:29.200155 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 5803: RAM USED (GB) 21.210984448
I0406 02:08:55.206846 140285718595328 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.194387, loss=3.530438
I0406 02:08:55.210794 140343431890752 submission.py:119] 6000) loss = 3.530, grad_norm = 0.194
I0406 02:12:32.245119 140285726988032 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.182585, loss=3.442661
I0406 02:12:32.248867 140343431890752 submission.py:119] 6500) loss = 3.443, grad_norm = 0.183
I0406 02:16:09.012218 140285718595328 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.156386, loss=3.479758
I0406 02:16:09.015183 140343431890752 submission.py:119] 7000) loss = 3.480, grad_norm = 0.156
I0406 02:19:45.723131 140285726988032 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.171519, loss=3.371610
I0406 02:19:45.726640 140343431890752 submission.py:119] 7500) loss = 3.372, grad_norm = 0.172
I0406 02:21:29.469327 140343431890752 submission_runner.py:373] Before eval at step 7740: RAM USED (GB) 21.581504512
I0406 02:21:29.469530 140343431890752 spec.py:298] Evaluating on the training split.
I0406 02:21:33.342195 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:23:51.800198 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 02:23:55.517657 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:26:08.620622 140343431890752 spec.py:326] Evaluating on the test split.
I0406 02:26:12.418044 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:28:13.867687 140343431890752 submission_runner.py:382] Time since start: 5667.86s, 	Step: 7740, 	{'train/accuracy': 0.591425026423418, 'train/loss': 2.2330365964339873, 'train/bleu': 28.246088119762025, 'validation/accuracy': 0.601654040247486, 'validation/loss': 2.148388387930714, 'validation/bleu': 24.19240427755895, 'validation/num_examples': 3000, 'test/accuracy': 0.6064377433037011, 'test/loss': 2.1277654915461044, 'test/bleu': 23.01039636655288, 'test/num_examples': 3003}
I0406 02:28:13.868132 140343431890752 submission_runner.py:396] After eval at step 7740: RAM USED (GB) 21.628035072
I0406 02:28:13.876307 140285718595328 logging_writer.py:48] [7740] global_step=7740, preemption_count=0, score=3353.872901, test/accuracy=0.606438, test/bleu=23.010396, test/loss=2.127765, test/num_examples=3003, total_duration=5667.864413, train/accuracy=0.591425, train/bleu=28.246088, train/loss=2.233037, validation/accuracy=0.601654, validation/bleu=24.192404, validation/loss=2.148388, validation/num_examples=3000
I0406 02:28:16.217738 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_7740.
I0406 02:28:16.218341 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 7740: RAM USED (GB) 21.627547648
I0406 02:30:09.555173 140285726988032 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.137149, loss=3.313992
I0406 02:30:09.558440 140343431890752 submission.py:119] 8000) loss = 3.314, grad_norm = 0.137
I0406 02:33:46.308449 140285718595328 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.174446, loss=3.315895
I0406 02:33:46.311530 140343431890752 submission.py:119] 8500) loss = 3.316, grad_norm = 0.174
I0406 02:37:23.188710 140285726988032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.156010, loss=3.306187
I0406 02:37:23.191614 140343431890752 submission.py:119] 9000) loss = 3.306, grad_norm = 0.156
I0406 02:40:59.918443 140285718595328 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.130636, loss=3.200627
I0406 02:40:59.921502 140343431890752 submission.py:119] 9500) loss = 3.201, grad_norm = 0.131
I0406 02:42:16.225253 140343431890752 submission_runner.py:373] Before eval at step 9677: RAM USED (GB) 21.784395776
I0406 02:42:16.225512 140343431890752 spec.py:298] Evaluating on the training split.
I0406 02:42:20.078260 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:44:38.212782 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 02:44:41.929575 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:46:48.557881 140343431890752 spec.py:326] Evaluating on the test split.
I0406 02:46:52.351308 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:48:51.354209 140343431890752 submission_runner.py:382] Time since start: 6914.62s, 	Step: 9677, 	{'train/accuracy': 0.6060855355834789, 'train/loss': 2.1067673409590353, 'train/bleu': 28.733089828701047, 'validation/accuracy': 0.6214802048331701, 'validation/loss': 1.9910127819245886, 'validation/bleu': 25.656071879509376, 'validation/num_examples': 3000, 'test/accuracy': 0.6273197373772588, 'test/loss': 1.956236200685608, 'test/bleu': 24.53850332279205, 'test/num_examples': 3003}
I0406 02:48:51.354609 140343431890752 submission_runner.py:396] After eval at step 9677: RAM USED (GB) 21.833715712
I0406 02:48:51.362751 140285726988032 logging_writer.py:48] [9677] global_step=9677, preemption_count=0, score=4191.129731, test/accuracy=0.627320, test/bleu=24.538503, test/loss=1.956236, test/num_examples=3003, total_duration=6914.620267, train/accuracy=0.606086, train/bleu=28.733090, train/loss=2.106767, validation/accuracy=0.621480, validation/bleu=25.656072, validation/loss=1.991013, validation/num_examples=3000
I0406 02:48:53.507987 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_9677.
I0406 02:48:53.508602 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 9677: RAM USED (GB) 21.833351168
I0406 02:51:13.497848 140343431890752 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 21.857943552
I0406 02:51:13.498053 140343431890752 spec.py:298] Evaluating on the training split.
I0406 02:51:17.378460 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:53:47.366338 140343431890752 spec.py:310] Evaluating on the validation split.
I0406 02:53:51.102648 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:55:58.462940 140343431890752 spec.py:326] Evaluating on the test split.
I0406 02:56:02.258021 140343431890752 workload.py:130] Translating evaluation dataset.
I0406 02:58:10.718982 140343431890752 submission_runner.py:382] Time since start: 7451.89s, 	Step: 10000, 	{'train/accuracy': 0.6051503127465666, 'train/loss': 2.1092032967032965, 'train/bleu': 29.090309448024822, 'validation/accuracy': 0.6231292854397341, 'validation/loss': 1.9760747696866747, 'validation/bleu': 25.843048699872448, 'validation/num_examples': 3000, 'test/accuracy': 0.6289698448666551, 'test/loss': 1.938305081052815, 'test/bleu': 24.424768407934923, 'test/num_examples': 3003}
I0406 02:58:10.719364 140343431890752 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.92195584
I0406 02:58:10.727353 140285718595328 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4330.635096, test/accuracy=0.628970, test/bleu=24.424768, test/loss=1.938305, test/num_examples=3003, total_duration=7451.892874, train/accuracy=0.605150, train/bleu=29.090309, train/loss=2.109203, validation/accuracy=0.623129, validation/bleu=25.843049, validation/loss=1.976075, validation/num_examples=3000
I0406 02:58:12.864464 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0406 02:58:12.865033 140343431890752 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.922705408
I0406 02:58:12.872597 140285726988032 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4330.635096
I0406 02:58:17.010778 140343431890752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0406 02:58:17.034525 140343431890752 submission_runner.py:550] Tuning trial 1/1
I0406 02:58:17.034680 140343431890752 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0406 02:58:17.035343 140343431890752 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006748178563667349, 'train/loss': 11.2061261165948, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.195090575442338, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.190892452501307, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 3.8643693923950195, 'total_duration': 3.8664073944091797, 'global_step': 1, 'preemption_count': 0}), (1933, {'train/accuracy': 0.41384003110633333, 'train/loss': 3.9237708995677134, 'train/bleu': 14.197196529256603, 'validation/accuracy': 0.40153252904489717, 'validation/loss': 4.037600277739892, 'validation/bleu': 9.782129503012326, 'validation/num_examples': 3000, 'test/accuracy': 0.391447330195805, 'test/loss': 4.214490732670966, 'test/bleu': 8.179868569229866, 'test/num_examples': 3003, 'score': 841.2905595302582, 'total_duration': 1676.78622174263, 'global_step': 1933, 'preemption_count': 0}), (3869, {'train/accuracy': 0.49830128461776046, 'train/loss': 3.0529808308262507, 'train/bleu': 21.131413677203952, 'validation/accuracy': 0.4951085541406802, 'validation/loss': 3.0834143795489206, 'validation/bleu': 16.434988731960566, 'validation/num_examples': 3000, 'test/accuracy': 0.49262680843646506, 'test/loss': 3.161857242461217, 'test/bleu': 15.20499883000344, 'test/num_examples': 3003, 'score': 1678.8502523899078, 'total_duration': 3099.8657727241516, 'global_step': 3869, 'preemption_count': 0}), (5803, {'train/accuracy': 0.557129840546697, 'train/loss': 2.533334460421412, 'train/bleu': 25.644124504970385, 'validation/accuracy': 0.5694907688683339, 'validation/loss': 2.4203504916244065, 'validation/bleu': 22.138721025116936, 'validation/num_examples': 3000, 'test/accuracy': 0.5682877229678693, 'test/loss': 2.427389097089071, 'test/bleu': 20.748086605090865, 'test/num_examples': 3003, 'score': 2516.3103234767914, 'total_duration': 4414.233889341354, 'global_step': 5803, 'preemption_count': 0}), (7740, {'train/accuracy': 0.591425026423418, 'train/loss': 2.2330365964339873, 'train/bleu': 28.246088119762025, 'validation/accuracy': 0.601654040247486, 'validation/loss': 2.148388387930714, 'validation/bleu': 24.19240427755895, 'validation/num_examples': 3000, 'test/accuracy': 0.6064377433037011, 'test/loss': 2.1277654915461044, 'test/bleu': 23.01039636655288, 'test/num_examples': 3003, 'score': 3353.872900724411, 'total_duration': 5667.864413022995, 'global_step': 7740, 'preemption_count': 0}), (9677, {'train/accuracy': 0.6060855355834789, 'train/loss': 2.1067673409590353, 'train/bleu': 28.733089828701047, 'validation/accuracy': 0.6214802048331701, 'validation/loss': 1.9910127819245886, 'validation/bleu': 25.656071879509376, 'validation/num_examples': 3000, 'test/accuracy': 0.6273197373772588, 'test/loss': 1.956236200685608, 'test/bleu': 24.53850332279205, 'test/num_examples': 3003, 'score': 4191.1297307014465, 'total_duration': 6914.620266675949, 'global_step': 9677, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6051503127465666, 'train/loss': 2.1092032967032965, 'train/bleu': 29.090309448024822, 'validation/accuracy': 0.6231292854397341, 'validation/loss': 1.9760747696866747, 'validation/bleu': 25.843048699872448, 'validation/num_examples': 3000, 'test/accuracy': 0.6289698448666551, 'test/loss': 1.938305081052815, 'test/bleu': 24.424768407934923, 'test/num_examples': 3003, 'score': 4330.6350955963135, 'total_duration': 7451.892874479294, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0406 02:58:17.035587 140343431890752 submission_runner.py:553] Timing: 4330.6350955963135
I0406 02:58:17.035642 140343431890752 submission_runner.py:554] ====================
I0406 02:58:17.035727 140343431890752 submission_runner.py:613] Final wmt score: 4330.6350955963135
