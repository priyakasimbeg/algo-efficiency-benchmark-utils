WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 23:57:13.023874 140503842449216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 23:57:13.024749 139934569588544 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 23:57:13.025488 140374714279744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 23:57:14.014459 140484492064576 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 23:57:14.014553 140627098117952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 23:57:14.014762 139920019023680 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 23:57:14.014859 140613210093376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 23:57:14.020485 139868858238784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 23:57:14.020812 139868858238784 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.025099 140484492064576 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.025120 140627098117952 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.025301 139920019023680 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.025398 140613210093376 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.029882 140503842449216 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.029864 139934569588544 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:14.029893 140374714279744 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 23:57:15.200414 139868858238784 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch.
W0405 23:57:15.312289 139868858238784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.312755 140484492064576 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.313171 140613210093376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.313239 140627098117952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.313596 140503842449216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.313833 139934569588544 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.314358 140374714279744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 23:57:15.315774 139920019023680 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 23:57:15.317069 139868858238784 submission_runner.py:511] Using RNG seed 2427150849
I0405 23:57:15.318052 139868858238784 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 23:57:15.318168 139868858238784 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1.
I0405 23:57:15.318370 139868858238784 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/hparams.json.
I0405 23:57:15.319270 139868858238784 submission_runner.py:230] Starting train once: RAM USED (GB) 5.66499328
I0405 23:57:15.319365 139868858238784 submission_runner.py:231] Initializing dataset.
I0405 23:57:15.319531 139868858238784 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.66499328
I0405 23:57:15.319608 139868858238784 submission_runner.py:240] Initializing model.
I0405 23:57:19.268062 139868858238784 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.321202688
I0405 23:57:19.268245 139868858238784 submission_runner.py:252] Initializing optimizer.
I0405 23:57:19.386047 139868858238784 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.324360704
I0405 23:57:19.386243 139868858238784 submission_runner.py:261] Initializing metrics bundle.
I0405 23:57:19.386292 139868858238784 submission_runner.py:276] Initializing checkpoint and logger.
I0405 23:57:19.387512 139868858238784 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 23:57:19.387659 139868858238784 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 23:57:19.965677 139868858238784 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/meta_data_0.json.
I0405 23:57:19.966625 139868858238784 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/flags_0.json.
I0405 23:57:19.998121 139868858238784 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.375220736
I0405 23:57:19.999225 139868858238784 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.375220736
I0405 23:57:19.999349 139868858238784 submission_runner.py:313] Starting training loop.
I0405 23:57:20.237045 139868858238784 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:20.242276 139868858238784 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:57:20.343391 139868858238784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:21.696033 139868858238784 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 15.605477376
I0405 23:57:25.480991 139830738544384 logging_writer.py:48] [0] global_step=0, grad_norm=2.425196, loss=0.706771
I0405 23:57:25.486954 139868858238784 submission.py:139] 0) loss = 0.707, grad_norm = 2.425
I0405 23:57:25.487432 139868858238784 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 21.98581248
I0405 23:57:25.487999 139868858238784 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 21.98581248
I0405 23:57:25.488103 139868858238784 spec.py:298] Evaluating on the training split.
I0405 23:57:25.492908 139868858238784 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:57:25.496930 139868858238784 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:57:25.549673 139868858238784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0405 23:57:39.988168 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.241813 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.241809 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.242167 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.242267 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.242964 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.261988 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:40.262950 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.528802 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.698524 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.699478 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.704520 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.704515 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.704607 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.705176 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:57:53.705211 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0405 23:58:17.370819 139868858238784 spec.py:310] Evaluating on the validation split.
I0405 23:58:17.373493 139868858238784 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:58:17.377372 139868858238784 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:58:17.428598 139868858238784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
W0405 23:58:29.926859 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.125310 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.126524 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.130541 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.132408 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.132774 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.133000 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:30.133121 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.073212 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.248694 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.251137 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.254669 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.254807 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.255740 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.255863 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:58:35.256104 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0405 23:58:58.501663 139868858238784 spec.py:326] Evaluating on the test split.
I0405 23:58:58.504320 139868858238784 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0405 23:58:58.507912 139868858238784 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0405 23:58:58.560143 139868858238784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
W0405 23:59:11.247067 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.454306 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.454367 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.459645 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.460285 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.460393 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.460684 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:11.461413 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.556146 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.758145 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.759935 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.762825 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.763853 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.764103 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.764404 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0405 23:59:16.764777 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0405 23:59:40.779048 139868858238784 submission_runner.py:382] Time since start: 5.49s, 	Step: 1, 	{'train/accuracy': 0.5715422613054435, 'train/loss': 0.7069145441055298, 'train/mean_average_precision': 0.023202206958354867, 'validation/accuracy': 0.5636502837119542, 'validation/loss': 0.7099474668502808, 'validation/mean_average_precision': 0.026307287509230493, 'validation/num_examples': 43793, 'test/accuracy': 0.5584538195829415, 'test/loss': 0.714481770992279, 'test/mean_average_precision': 0.027913254581270124, 'test/num_examples': 43793}
I0405 23:59:40.779438 139868858238784 submission_runner.py:396] After eval at step 1: RAM USED (GB) 25.849012224
I0405 23:59:40.787544 139816913676032 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=5.487239, test/accuracy=0.558454, test/loss=0.714482, test/mean_average_precision=0.027913, test/num_examples=43793, total_duration=5.489253, train/accuracy=0.571542, train/loss=0.706915, train/mean_average_precision=0.023202, validation/accuracy=0.563650, validation/loss=0.709947, validation/mean_average_precision=0.026307, validation/num_examples=43793
I0405 23:59:41.070695 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_1.
I0405 23:59:41.071178 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 25.8453504
I0405 23:59:41.295852 139868858238784 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 25.877225472
I0405 23:59:41.298378 139868858238784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.304840 139934569588544 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.305998 139920019023680 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.306001 140627098117952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.306005 140613210093376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.306010 140374714279744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.306054 140484492064576 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.306054 140503842449216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 23:59:41.333755 139816922068736 logging_writer.py:48] [1] global_step=1, grad_norm=2.438380, loss=0.707247
I0405 23:59:41.337654 139868858238784 submission.py:139] 1) loss = 0.707, grad_norm = 2.438
I0405 23:59:41.338171 139868858238784 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 25.877209088
I0405 23:59:41.593931 139816913676032 logging_writer.py:48] [2] global_step=2, grad_norm=2.431724, loss=0.705684
I0405 23:59:41.597638 139868858238784 submission.py:139] 2) loss = 0.706, grad_norm = 2.432
I0405 23:59:41.853386 139816922068736 logging_writer.py:48] [3] global_step=3, grad_norm=2.397426, loss=0.700287
I0405 23:59:41.857226 139868858238784 submission.py:139] 3) loss = 0.700, grad_norm = 2.397
I0405 23:59:42.111572 139816913676032 logging_writer.py:48] [4] global_step=4, grad_norm=2.313443, loss=0.691965
I0405 23:59:42.115450 139868858238784 submission.py:139] 4) loss = 0.692, grad_norm = 2.313
I0405 23:59:42.371556 139816922068736 logging_writer.py:48] [5] global_step=5, grad_norm=2.236911, loss=0.679613
I0405 23:59:42.375419 139868858238784 submission.py:139] 5) loss = 0.680, grad_norm = 2.237
I0405 23:59:42.629308 139816913676032 logging_writer.py:48] [6] global_step=6, grad_norm=2.139257, loss=0.666677
I0405 23:59:42.633035 139868858238784 submission.py:139] 6) loss = 0.667, grad_norm = 2.139
I0405 23:59:42.887729 139816922068736 logging_writer.py:48] [7] global_step=7, grad_norm=1.981649, loss=0.647425
I0405 23:59:42.891497 139868858238784 submission.py:139] 7) loss = 0.647, grad_norm = 1.982
I0405 23:59:43.145725 139816913676032 logging_writer.py:48] [8] global_step=8, grad_norm=1.809677, loss=0.626635
I0405 23:59:43.149573 139868858238784 submission.py:139] 8) loss = 0.627, grad_norm = 1.810
I0405 23:59:43.400208 139816922068736 logging_writer.py:48] [9] global_step=9, grad_norm=1.679830, loss=0.611576
I0405 23:59:43.404117 139868858238784 submission.py:139] 9) loss = 0.612, grad_norm = 1.680
I0405 23:59:43.652500 139816913676032 logging_writer.py:48] [10] global_step=10, grad_norm=1.484675, loss=0.586214
I0405 23:59:43.656262 139868858238784 submission.py:139] 10) loss = 0.586, grad_norm = 1.485
I0405 23:59:43.911308 139816922068736 logging_writer.py:48] [11] global_step=11, grad_norm=1.316030, loss=0.568320
I0405 23:59:43.915209 139868858238784 submission.py:139] 11) loss = 0.568, grad_norm = 1.316
I0405 23:59:44.168262 139816913676032 logging_writer.py:48] [12] global_step=12, grad_norm=1.215622, loss=0.556078
I0405 23:59:44.171891 139868858238784 submission.py:139] 12) loss = 0.556, grad_norm = 1.216
I0405 23:59:44.421326 139816922068736 logging_writer.py:48] [13] global_step=13, grad_norm=1.153521, loss=0.541010
I0405 23:59:44.425241 139868858238784 submission.py:139] 13) loss = 0.541, grad_norm = 1.154
I0405 23:59:44.683796 139816913676032 logging_writer.py:48] [14] global_step=14, grad_norm=1.132124, loss=0.528380
I0405 23:59:44.687799 139868858238784 submission.py:139] 14) loss = 0.528, grad_norm = 1.132
I0405 23:59:44.950419 139816922068736 logging_writer.py:48] [15] global_step=15, grad_norm=1.086107, loss=0.519315
I0405 23:59:44.954388 139868858238784 submission.py:139] 15) loss = 0.519, grad_norm = 1.086
I0405 23:59:45.209071 139816913676032 logging_writer.py:48] [16] global_step=16, grad_norm=1.014469, loss=0.505279
I0405 23:59:45.212771 139868858238784 submission.py:139] 16) loss = 0.505, grad_norm = 1.014
I0405 23:59:45.467391 139816922068736 logging_writer.py:48] [17] global_step=17, grad_norm=0.966155, loss=0.495262
I0405 23:59:45.471151 139868858238784 submission.py:139] 17) loss = 0.495, grad_norm = 0.966
I0405 23:59:45.731022 139816913676032 logging_writer.py:48] [18] global_step=18, grad_norm=0.897992, loss=0.485735
I0405 23:59:45.735088 139868858238784 submission.py:139] 18) loss = 0.486, grad_norm = 0.898
I0405 23:59:45.989564 139816922068736 logging_writer.py:48] [19] global_step=19, grad_norm=0.792827, loss=0.472943
I0405 23:59:45.993258 139868858238784 submission.py:139] 19) loss = 0.473, grad_norm = 0.793
I0405 23:59:46.247164 139816913676032 logging_writer.py:48] [20] global_step=20, grad_norm=0.758825, loss=0.463024
I0405 23:59:46.250926 139868858238784 submission.py:139] 20) loss = 0.463, grad_norm = 0.759
I0405 23:59:46.504521 139816922068736 logging_writer.py:48] [21] global_step=21, grad_norm=0.717709, loss=0.448791
I0405 23:59:46.508403 139868858238784 submission.py:139] 21) loss = 0.449, grad_norm = 0.718
I0405 23:59:46.766177 139816913676032 logging_writer.py:48] [22] global_step=22, grad_norm=0.669061, loss=0.437790
I0405 23:59:46.769979 139868858238784 submission.py:139] 22) loss = 0.438, grad_norm = 0.669
I0405 23:59:47.021943 139816922068736 logging_writer.py:48] [23] global_step=23, grad_norm=0.625807, loss=0.434739
I0405 23:59:47.025890 139868858238784 submission.py:139] 23) loss = 0.435, grad_norm = 0.626
I0405 23:59:47.277197 139816913676032 logging_writer.py:48] [24] global_step=24, grad_norm=0.601727, loss=0.421489
I0405 23:59:47.280927 139868858238784 submission.py:139] 24) loss = 0.421, grad_norm = 0.602
I0405 23:59:47.530342 139816922068736 logging_writer.py:48] [25] global_step=25, grad_norm=0.573266, loss=0.412623
I0405 23:59:47.534035 139868858238784 submission.py:139] 25) loss = 0.413, grad_norm = 0.573
I0405 23:59:47.785903 139816913676032 logging_writer.py:48] [26] global_step=26, grad_norm=0.545173, loss=0.405232
I0405 23:59:47.790155 139868858238784 submission.py:139] 26) loss = 0.405, grad_norm = 0.545
I0405 23:59:48.068496 139816922068736 logging_writer.py:48] [27] global_step=27, grad_norm=0.532381, loss=0.394386
I0405 23:59:48.072275 139868858238784 submission.py:139] 27) loss = 0.394, grad_norm = 0.532
I0405 23:59:48.338762 139816913676032 logging_writer.py:48] [28] global_step=28, grad_norm=0.514660, loss=0.384186
I0405 23:59:48.342698 139868858238784 submission.py:139] 28) loss = 0.384, grad_norm = 0.515
I0405 23:59:48.597155 139816922068736 logging_writer.py:48] [29] global_step=29, grad_norm=0.506296, loss=0.378580
I0405 23:59:48.600890 139868858238784 submission.py:139] 29) loss = 0.379, grad_norm = 0.506
I0405 23:59:48.852303 139816913676032 logging_writer.py:48] [30] global_step=30, grad_norm=0.489016, loss=0.364855
I0405 23:59:48.856153 139868858238784 submission.py:139] 30) loss = 0.365, grad_norm = 0.489
I0405 23:59:49.107149 139816922068736 logging_writer.py:48] [31] global_step=31, grad_norm=0.465293, loss=0.359783
I0405 23:59:49.110759 139868858238784 submission.py:139] 31) loss = 0.360, grad_norm = 0.465
I0405 23:59:49.364368 139816913676032 logging_writer.py:48] [32] global_step=32, grad_norm=0.447246, loss=0.350866
I0405 23:59:49.368225 139868858238784 submission.py:139] 32) loss = 0.351, grad_norm = 0.447
I0405 23:59:49.628024 139816922068736 logging_writer.py:48] [33] global_step=33, grad_norm=0.426477, loss=0.343627
I0405 23:59:49.631649 139868858238784 submission.py:139] 33) loss = 0.344, grad_norm = 0.426
I0405 23:59:49.883974 139816913676032 logging_writer.py:48] [34] global_step=34, grad_norm=0.413306, loss=0.333696
I0405 23:59:49.887899 139868858238784 submission.py:139] 34) loss = 0.334, grad_norm = 0.413
I0405 23:59:50.145826 139816922068736 logging_writer.py:48] [35] global_step=35, grad_norm=0.396911, loss=0.325651
I0405 23:59:50.149579 139868858238784 submission.py:139] 35) loss = 0.326, grad_norm = 0.397
I0405 23:59:50.399274 139816913676032 logging_writer.py:48] [36] global_step=36, grad_norm=0.385279, loss=0.319153
I0405 23:59:50.402914 139868858238784 submission.py:139] 36) loss = 0.319, grad_norm = 0.385
I0405 23:59:50.654781 139816922068736 logging_writer.py:48] [37] global_step=37, grad_norm=0.373656, loss=0.312449
I0405 23:59:50.658563 139868858238784 submission.py:139] 37) loss = 0.312, grad_norm = 0.374
I0405 23:59:50.911309 139816913676032 logging_writer.py:48] [38] global_step=38, grad_norm=0.363105, loss=0.305477
I0405 23:59:50.915082 139868858238784 submission.py:139] 38) loss = 0.305, grad_norm = 0.363
I0405 23:59:51.171535 139816922068736 logging_writer.py:48] [39] global_step=39, grad_norm=0.350671, loss=0.296551
I0405 23:59:51.175335 139868858238784 submission.py:139] 39) loss = 0.297, grad_norm = 0.351
I0405 23:59:51.428264 139816913676032 logging_writer.py:48] [40] global_step=40, grad_norm=0.340300, loss=0.292047
I0405 23:59:51.431985 139868858238784 submission.py:139] 40) loss = 0.292, grad_norm = 0.340
I0405 23:59:51.678508 139816922068736 logging_writer.py:48] [41] global_step=41, grad_norm=0.327624, loss=0.285137
I0405 23:59:51.682298 139868858238784 submission.py:139] 41) loss = 0.285, grad_norm = 0.328
I0405 23:59:51.935606 139816913676032 logging_writer.py:48] [42] global_step=42, grad_norm=0.321392, loss=0.276780
I0405 23:59:51.939296 139868858238784 submission.py:139] 42) loss = 0.277, grad_norm = 0.321
I0405 23:59:52.193962 139816922068736 logging_writer.py:48] [43] global_step=43, grad_norm=0.312034, loss=0.272052
I0405 23:59:52.197797 139868858238784 submission.py:139] 43) loss = 0.272, grad_norm = 0.312
I0405 23:59:52.450777 139816913676032 logging_writer.py:48] [44] global_step=44, grad_norm=0.305028, loss=0.263126
I0405 23:59:52.454631 139868858238784 submission.py:139] 44) loss = 0.263, grad_norm = 0.305
I0405 23:59:52.712111 139816922068736 logging_writer.py:48] [45] global_step=45, grad_norm=0.297343, loss=0.258633
I0405 23:59:52.715936 139868858238784 submission.py:139] 45) loss = 0.259, grad_norm = 0.297
I0405 23:59:52.970229 139816913676032 logging_writer.py:48] [46] global_step=46, grad_norm=0.292609, loss=0.253321
I0405 23:59:52.973911 139868858238784 submission.py:139] 46) loss = 0.253, grad_norm = 0.293
I0405 23:59:53.243691 139816922068736 logging_writer.py:48] [47] global_step=47, grad_norm=0.285673, loss=0.243937
I0405 23:59:53.247331 139868858238784 submission.py:139] 47) loss = 0.244, grad_norm = 0.286
I0405 23:59:53.503098 139816913676032 logging_writer.py:48] [48] global_step=48, grad_norm=0.278048, loss=0.236958
I0405 23:59:53.507044 139868858238784 submission.py:139] 48) loss = 0.237, grad_norm = 0.278
I0405 23:59:53.762859 139816922068736 logging_writer.py:48] [49] global_step=49, grad_norm=0.270652, loss=0.233656
I0405 23:59:53.766784 139868858238784 submission.py:139] 49) loss = 0.234, grad_norm = 0.271
I0405 23:59:54.021282 139816913676032 logging_writer.py:48] [50] global_step=50, grad_norm=0.262229, loss=0.228390
I0405 23:59:54.025174 139868858238784 submission.py:139] 50) loss = 0.228, grad_norm = 0.262
I0405 23:59:54.276985 139816922068736 logging_writer.py:48] [51] global_step=51, grad_norm=0.255601, loss=0.219709
I0405 23:59:54.280655 139868858238784 submission.py:139] 51) loss = 0.220, grad_norm = 0.256
I0405 23:59:54.536222 139816913676032 logging_writer.py:48] [52] global_step=52, grad_norm=0.246865, loss=0.216328
I0405 23:59:54.539985 139868858238784 submission.py:139] 52) loss = 0.216, grad_norm = 0.247
I0405 23:59:54.793896 139816922068736 logging_writer.py:48] [53] global_step=53, grad_norm=0.238438, loss=0.212187
I0405 23:59:54.797755 139868858238784 submission.py:139] 53) loss = 0.212, grad_norm = 0.238
I0405 23:59:55.049673 139816913676032 logging_writer.py:48] [54] global_step=54, grad_norm=0.233203, loss=0.205967
I0405 23:59:55.053491 139868858238784 submission.py:139] 54) loss = 0.206, grad_norm = 0.233
I0405 23:59:55.306008 139816922068736 logging_writer.py:48] [55] global_step=55, grad_norm=0.225077, loss=0.199412
I0405 23:59:55.309670 139868858238784 submission.py:139] 55) loss = 0.199, grad_norm = 0.225
I0405 23:59:55.563503 139816913676032 logging_writer.py:48] [56] global_step=56, grad_norm=0.217744, loss=0.196665
I0405 23:59:55.567377 139868858238784 submission.py:139] 56) loss = 0.197, grad_norm = 0.218
I0405 23:59:55.817869 139816922068736 logging_writer.py:48] [57] global_step=57, grad_norm=0.210663, loss=0.195740
I0405 23:59:55.821616 139868858238784 submission.py:139] 57) loss = 0.196, grad_norm = 0.211
I0405 23:59:56.072844 139816913676032 logging_writer.py:48] [58] global_step=58, grad_norm=0.203708, loss=0.188532
I0405 23:59:56.076577 139868858238784 submission.py:139] 58) loss = 0.189, grad_norm = 0.204
I0405 23:59:56.326265 139816922068736 logging_writer.py:48] [59] global_step=59, grad_norm=0.197730, loss=0.181660
I0405 23:59:56.329934 139868858238784 submission.py:139] 59) loss = 0.182, grad_norm = 0.198
I0405 23:59:56.589996 139816913676032 logging_writer.py:48] [60] global_step=60, grad_norm=0.191223, loss=0.179870
I0405 23:59:56.593761 139868858238784 submission.py:139] 60) loss = 0.180, grad_norm = 0.191
I0405 23:59:56.851246 139816922068736 logging_writer.py:48] [61] global_step=61, grad_norm=0.188456, loss=0.173556
I0405 23:59:56.855002 139868858238784 submission.py:139] 61) loss = 0.174, grad_norm = 0.188
I0405 23:59:57.101902 139816913676032 logging_writer.py:48] [62] global_step=62, grad_norm=0.181339, loss=0.170011
I0405 23:59:57.105748 139868858238784 submission.py:139] 62) loss = 0.170, grad_norm = 0.181
I0405 23:59:57.355248 139816922068736 logging_writer.py:48] [63] global_step=63, grad_norm=0.173067, loss=0.165859
I0405 23:59:57.358941 139868858238784 submission.py:139] 63) loss = 0.166, grad_norm = 0.173
I0405 23:59:57.606698 139816913676032 logging_writer.py:48] [64] global_step=64, grad_norm=0.172506, loss=0.159501
I0405 23:59:57.610345 139868858238784 submission.py:139] 64) loss = 0.160, grad_norm = 0.173
I0405 23:59:57.862357 139816922068736 logging_writer.py:48] [65] global_step=65, grad_norm=0.164810, loss=0.155964
I0405 23:59:57.866280 139868858238784 submission.py:139] 65) loss = 0.156, grad_norm = 0.165
I0405 23:59:58.118775 139816913676032 logging_writer.py:48] [66] global_step=66, grad_norm=0.158855, loss=0.152383
I0405 23:59:58.122508 139868858238784 submission.py:139] 66) loss = 0.152, grad_norm = 0.159
I0405 23:59:58.376076 139816922068736 logging_writer.py:48] [67] global_step=67, grad_norm=0.156356, loss=0.149694
I0405 23:59:58.379787 139868858238784 submission.py:139] 67) loss = 0.150, grad_norm = 0.156
I0405 23:59:58.636680 139816913676032 logging_writer.py:48] [68] global_step=68, grad_norm=0.153941, loss=0.144919
I0405 23:59:58.640304 139868858238784 submission.py:139] 68) loss = 0.145, grad_norm = 0.154
I0405 23:59:58.891710 139816922068736 logging_writer.py:48] [69] global_step=69, grad_norm=0.147550, loss=0.141840
I0405 23:59:58.895493 139868858238784 submission.py:139] 69) loss = 0.142, grad_norm = 0.148
I0405 23:59:59.144423 139816913676032 logging_writer.py:48] [70] global_step=70, grad_norm=0.141610, loss=0.144376
I0405 23:59:59.148146 139868858238784 submission.py:139] 70) loss = 0.144, grad_norm = 0.142
I0405 23:59:59.397950 139816922068736 logging_writer.py:48] [71] global_step=71, grad_norm=0.136902, loss=0.139892
I0405 23:59:59.401748 139868858238784 submission.py:139] 71) loss = 0.140, grad_norm = 0.137
I0405 23:59:59.655228 139816913676032 logging_writer.py:48] [72] global_step=72, grad_norm=0.133963, loss=0.139708
I0405 23:59:59.658928 139868858238784 submission.py:139] 72) loss = 0.140, grad_norm = 0.134
I0405 23:59:59.910749 139816922068736 logging_writer.py:48] [73] global_step=73, grad_norm=0.130057, loss=0.133264
I0405 23:59:59.914399 139868858238784 submission.py:139] 73) loss = 0.133, grad_norm = 0.130
I0406 00:00:00.168455 139816913676032 logging_writer.py:48] [74] global_step=74, grad_norm=0.127555, loss=0.132439
I0406 00:00:00.172060 139868858238784 submission.py:139] 74) loss = 0.132, grad_norm = 0.128
I0406 00:00:00.420586 139816922068736 logging_writer.py:48] [75] global_step=75, grad_norm=0.123885, loss=0.126301
I0406 00:00:00.424233 139868858238784 submission.py:139] 75) loss = 0.126, grad_norm = 0.124
I0406 00:00:00.673959 139816913676032 logging_writer.py:48] [76] global_step=76, grad_norm=0.120554, loss=0.125953
I0406 00:00:00.677827 139868858238784 submission.py:139] 76) loss = 0.126, grad_norm = 0.121
I0406 00:00:00.927929 139816922068736 logging_writer.py:48] [77] global_step=77, grad_norm=0.117918, loss=0.121560
I0406 00:00:00.931436 139868858238784 submission.py:139] 77) loss = 0.122, grad_norm = 0.118
I0406 00:00:01.185051 139816913676032 logging_writer.py:48] [78] global_step=78, grad_norm=0.113871, loss=0.121877
I0406 00:00:01.188730 139868858238784 submission.py:139] 78) loss = 0.122, grad_norm = 0.114
I0406 00:00:01.438633 139816922068736 logging_writer.py:48] [79] global_step=79, grad_norm=0.110462, loss=0.120606
I0406 00:00:01.442417 139868858238784 submission.py:139] 79) loss = 0.121, grad_norm = 0.110
I0406 00:00:01.718161 139816913676032 logging_writer.py:48] [80] global_step=80, grad_norm=0.109482, loss=0.117866
I0406 00:00:01.722481 139868858238784 submission.py:139] 80) loss = 0.118, grad_norm = 0.109
I0406 00:00:01.987664 139816922068736 logging_writer.py:48] [81] global_step=81, grad_norm=0.105556, loss=0.117429
I0406 00:00:01.991317 139868858238784 submission.py:139] 81) loss = 0.117, grad_norm = 0.106
I0406 00:00:02.244772 139816913676032 logging_writer.py:48] [82] global_step=82, grad_norm=0.101421, loss=0.113964
I0406 00:00:02.248692 139868858238784 submission.py:139] 82) loss = 0.114, grad_norm = 0.101
I0406 00:00:02.506418 139816922068736 logging_writer.py:48] [83] global_step=83, grad_norm=0.101392, loss=0.106394
I0406 00:00:02.510200 139868858238784 submission.py:139] 83) loss = 0.106, grad_norm = 0.101
I0406 00:00:02.761795 139816913676032 logging_writer.py:48] [84] global_step=84, grad_norm=0.099993, loss=0.107798
I0406 00:00:02.765751 139868858238784 submission.py:139] 84) loss = 0.108, grad_norm = 0.100
I0406 00:00:03.016644 139816922068736 logging_writer.py:48] [85] global_step=85, grad_norm=0.092925, loss=0.108014
I0406 00:00:03.020388 139868858238784 submission.py:139] 85) loss = 0.108, grad_norm = 0.093
I0406 00:00:03.272804 139816913676032 logging_writer.py:48] [86] global_step=86, grad_norm=0.091432, loss=0.106533
I0406 00:00:03.276700 139868858238784 submission.py:139] 86) loss = 0.107, grad_norm = 0.091
I0406 00:00:03.531749 139816922068736 logging_writer.py:48] [87] global_step=87, grad_norm=0.091855, loss=0.104432
I0406 00:00:03.535497 139868858238784 submission.py:139] 87) loss = 0.104, grad_norm = 0.092
I0406 00:00:03.787288 139816913676032 logging_writer.py:48] [88] global_step=88, grad_norm=0.089048, loss=0.107414
I0406 00:00:03.790953 139868858238784 submission.py:139] 88) loss = 0.107, grad_norm = 0.089
I0406 00:00:04.041460 139816922068736 logging_writer.py:48] [89] global_step=89, grad_norm=0.085726, loss=0.102907
I0406 00:00:04.045158 139868858238784 submission.py:139] 89) loss = 0.103, grad_norm = 0.086
I0406 00:00:04.294150 139816913676032 logging_writer.py:48] [90] global_step=90, grad_norm=0.083207, loss=0.100832
I0406 00:00:04.297842 139868858238784 submission.py:139] 90) loss = 0.101, grad_norm = 0.083
I0406 00:00:04.551733 139816922068736 logging_writer.py:48] [91] global_step=91, grad_norm=0.081915, loss=0.098614
I0406 00:00:04.555813 139868858238784 submission.py:139] 91) loss = 0.099, grad_norm = 0.082
I0406 00:00:04.805433 139816913676032 logging_writer.py:48] [92] global_step=92, grad_norm=0.081255, loss=0.096478
I0406 00:00:04.809358 139868858238784 submission.py:139] 92) loss = 0.096, grad_norm = 0.081
I0406 00:00:05.064687 139816922068736 logging_writer.py:48] [93] global_step=93, grad_norm=0.075943, loss=0.099394
I0406 00:00:05.068657 139868858238784 submission.py:139] 93) loss = 0.099, grad_norm = 0.076
I0406 00:00:05.318580 139816913676032 logging_writer.py:48] [94] global_step=94, grad_norm=0.077529, loss=0.094141
I0406 00:00:05.322384 139868858238784 submission.py:139] 94) loss = 0.094, grad_norm = 0.078
I0406 00:00:05.575747 139816922068736 logging_writer.py:48] [95] global_step=95, grad_norm=0.075386, loss=0.092378
I0406 00:00:05.579504 139868858238784 submission.py:139] 95) loss = 0.092, grad_norm = 0.075
I0406 00:00:05.828642 139816913676032 logging_writer.py:48] [96] global_step=96, grad_norm=0.071366, loss=0.093560
I0406 00:00:05.832335 139868858238784 submission.py:139] 96) loss = 0.094, grad_norm = 0.071
I0406 00:00:06.084324 139816922068736 logging_writer.py:48] [97] global_step=97, grad_norm=0.072916, loss=0.088033
I0406 00:00:06.088069 139868858238784 submission.py:139] 97) loss = 0.088, grad_norm = 0.073
I0406 00:00:06.335360 139816913676032 logging_writer.py:48] [98] global_step=98, grad_norm=0.069666, loss=0.090107
I0406 00:00:06.339118 139868858238784 submission.py:139] 98) loss = 0.090, grad_norm = 0.070
I0406 00:00:06.589648 139816922068736 logging_writer.py:48] [99] global_step=99, grad_norm=0.069408, loss=0.092895
I0406 00:00:06.593525 139868858238784 submission.py:139] 99) loss = 0.093, grad_norm = 0.069
I0406 00:00:06.845446 139816913676032 logging_writer.py:48] [100] global_step=100, grad_norm=0.066661, loss=0.093013
I0406 00:00:06.849224 139868858238784 submission.py:139] 100) loss = 0.093, grad_norm = 0.067
I0406 00:01:46.513979 139816922068736 logging_writer.py:48] [500] global_step=500, grad_norm=0.016964, loss=0.063364
I0406 00:01:46.518114 139868858238784 submission.py:139] 500) loss = 0.063, grad_norm = 0.017
I0406 00:03:41.251326 139868858238784 submission_runner.py:373] Before eval at step 962: RAM USED (GB) 26.63501824
I0406 00:03:41.251525 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:03:54.743124 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.015048 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.015901 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.020920 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.021023 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.021157 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.021224 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:03:55.022533 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.318782 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.573948 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.575503 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.580289 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.580422 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.580897 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.581325 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:08.581866 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:04:32.475541 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:04:33.176896 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.479098 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.479468 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.486681 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.486842 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.487128 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.487407 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.499321 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.630345 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.899469 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.901644 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.902392 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.902867 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.903027 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.904701 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:33.905115 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:04:36.187441 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:04:36.557999 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.873161 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.874169 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.880195 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.880211 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.880248 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.880290 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:36.880655 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.012519 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.285940 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.290346 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.291354 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.291970 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.292119 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.292572 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:04:37.298449 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:04:39.588190 139868858238784 submission_runner.py:382] Time since start: 381.25s, 	Step: 962, 	{'train/accuracy': 0.9868119500716186, 'train/loss': 0.05468286573886871, 'train/mean_average_precision': 0.03396043962241302, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06553645431995392, 'validation/mean_average_precision': 0.03761652777552804, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06898437440395355, 'test/mean_average_precision': 0.0394538438272551, 'test/num_examples': 43793}
I0406 00:04:39.588615 139868858238784 submission_runner.py:396] After eval at step 962: RAM USED (GB) 27.903172608
I0406 00:04:39.596374 139816913676032 logging_writer.py:48] [962] global_step=962, preemption_count=0, score=244.660570, test/accuracy=0.983142, test/loss=0.068984, test/mean_average_precision=0.039454, test/num_examples=43793, total_duration=381.252482, train/accuracy=0.986812, train/loss=0.054683, train/mean_average_precision=0.033960, validation/accuracy=0.984118, validation/loss=0.065536, validation/mean_average_precision=0.037617, validation/num_examples=43793
I0406 00:04:39.662477 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_962.
I0406 00:04:39.662969 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 962: RAM USED (GB) 27.87012608
I0406 00:04:49.630990 139816922068736 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.022853, loss=0.055156
I0406 00:04:49.635262 139868858238784 submission.py:139] 1000) loss = 0.055, grad_norm = 0.023
I0406 00:06:59.036521 139816913676032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.028465, loss=0.051406
I0406 00:06:59.042349 139868858238784 submission.py:139] 1500) loss = 0.051, grad_norm = 0.028
I0406 00:08:40.021078 139868858238784 submission_runner.py:373] Before eval at step 1894: RAM USED (GB) 28.105940992
I0406 00:08:40.021295 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:08:54.304546 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.632198 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.632429 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.637467 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.637976 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.638319 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.638860 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:08:54.638991 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:08.786875 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.043248 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.047479 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.049449 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.049473 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.049445 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.049591 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:09.058494 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:34.499749 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:09:34.919991 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.258385 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.260219 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.264712 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.264806 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.265258 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.265521 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.265855 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.396537 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.681632 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.685932 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.686641 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.686779 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.687426 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.693769 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:35.696277 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:38.138071 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:09:39.368890 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.704885 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.705386 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.711135 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.711508 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.711588 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.711653 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.711891 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:39.844177 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.115604 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.116006 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.121079 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.121522 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.121715 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.121979 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:09:40.130162 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:09:42.486596 139868858238784 submission_runner.py:382] Time since start: 680.02s, 	Step: 1894, 	{'train/accuracy': 0.9867499846919077, 'train/loss': 0.051564209163188934, 'train/mean_average_precision': 0.05950298821177134, 'validation/accuracy': 0.9841443622549844, 'validation/loss': 0.061425067484378815, 'validation/mean_average_precision': 0.05454991043621724, 'validation/num_examples': 43793, 'test/accuracy': 0.9831640060500405, 'test/loss': 0.06479746848344803, 'test/mean_average_precision': 0.05624626895844242, 'test/num_examples': 43793}
I0406 00:09:42.486986 139868858238784 submission_runner.py:396] After eval at step 1894: RAM USED (GB) 28.694339584
I0406 00:09:42.494853 139816922068736 logging_writer.py:48] [1894] global_step=1894, preemption_count=0, score=483.834461, test/accuracy=0.983164, test/loss=0.064797, test/mean_average_precision=0.056246, test/num_examples=43793, total_duration=680.022185, train/accuracy=0.986750, train/loss=0.051564, train/mean_average_precision=0.059503, validation/accuracy=0.984144, validation/loss=0.061425, validation/mean_average_precision=0.054550, validation/num_examples=43793
I0406 00:09:42.554985 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_1894.
I0406 00:09:42.555419 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 1894: RAM USED (GB) 28.59546624
I0406 00:10:10.699157 139816913676032 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.064440, loss=0.052225
I0406 00:10:10.703400 139868858238784 submission.py:139] 2000) loss = 0.052, grad_norm = 0.064
I0406 00:12:17.325903 139816922068736 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.063597, loss=0.048595
I0406 00:12:17.330620 139868858238784 submission.py:139] 2500) loss = 0.049, grad_norm = 0.064
I0406 00:13:44.019855 139868858238784 submission_runner.py:373] Before eval at step 2843: RAM USED (GB) 28.814819328
I0406 00:13:44.020062 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:13:57.956226 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.282475 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.282588 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.287754 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.288058 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.288051 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.288920 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:13:58.289597 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:11.993561 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.250949 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.251048 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.255133 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.256754 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.256930 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.263590 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:12.265371 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:37.800231 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:14:38.259447 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.604717 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.606057 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.610329 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.611973 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.612008 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.612066 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.612483 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:38.747841 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.021774 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.026575 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.027846 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.028029 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.028646 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.033972 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:39.034881 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:41.544781 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:14:41.977335 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.310654 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.311818 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.315950 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.317981 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.318151 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.318258 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.318353 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.449914 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.725046 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.725081 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.730787 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.731412 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.731723 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.732043 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:14:42.732036 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:14:45.153952 139868858238784 submission_runner.py:382] Time since start: 984.02s, 	Step: 2843, 	{'train/accuracy': 0.9869432772696364, 'train/loss': 0.048594653606414795, 'train/mean_average_precision': 0.08598831242199148, 'validation/accuracy': 0.9842194612842178, 'validation/loss': 0.058249764144420624, 'validation/mean_average_precision': 0.08441856614445839, 'validation/num_examples': 43793, 'test/accuracy': 0.9832511933498413, 'test/loss': 0.061436474323272705, 'test/mean_average_precision': 0.08834793550405858, 'test/num_examples': 43793}
I0406 00:14:45.154377 139868858238784 submission_runner.py:396] After eval at step 2843: RAM USED (GB) 29.358022656
I0406 00:14:45.162484 139816913676032 logging_writer.py:48] [2843] global_step=2843, preemption_count=0, score=723.072412, test/accuracy=0.983251, test/loss=0.061436, test/mean_average_precision=0.088348, test/num_examples=43793, total_duration=984.021030, train/accuracy=0.986943, train/loss=0.048595, train/mean_average_precision=0.085988, validation/accuracy=0.984219, validation/loss=0.058250, validation/mean_average_precision=0.084419, validation/num_examples=43793
I0406 00:14:45.223365 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_2843.
I0406 00:14:45.223811 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 2843: RAM USED (GB) 29.290950656
I0406 00:15:26.778328 139816922068736 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.065699, loss=0.051054
I0406 00:15:26.782716 139868858238784 submission.py:139] 3000) loss = 0.051, grad_norm = 0.066
I0406 00:17:34.543067 139816913676032 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.042600, loss=0.055140
I0406 00:17:34.548766 139868858238784 submission.py:139] 3500) loss = 0.055, grad_norm = 0.043
I0406 00:18:45.398739 139868858238784 submission_runner.py:373] Before eval at step 3780: RAM USED (GB) 29.552893952
I0406 00:18:45.398981 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:18:59.813808 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.074457 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.074496 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.079560 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.080250 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.080754 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.081476 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:00.082043 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.248592 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.504410 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.505148 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.509001 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.510458 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.510455 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.511211 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:14.511950 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:40.767862 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:19:41.231282 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.520034 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.520357 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.526719 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.526778 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.526990 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.527339 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.527589 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.713019 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.980511 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.980608 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.986427 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.987249 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.987320 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.990495 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:41.993406 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:44.457974 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:19:44.903779 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.188193 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.188804 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.194620 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.195158 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.195232 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.195635 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.202781 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.342875 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.667358 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.667844 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.673570 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.673729 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.673883 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.680464 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:19:45.681204 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:19:48.140214 139868858238784 submission_runner.py:382] Time since start: 1285.40s, 	Step: 3780, 	{'train/accuracy': 0.9869897495652783, 'train/loss': 0.047168999910354614, 'train/mean_average_precision': 0.10856412142693239, 'validation/accuracy': 0.9843055207123123, 'validation/loss': 0.056511323899030685, 'validation/mean_average_precision': 0.10438536376488455, 'validation/num_examples': 43793, 'test/accuracy': 0.9833101606057454, 'test/loss': 0.05964108556509018, 'test/mean_average_precision': 0.11123258680349507, 'test/num_examples': 43793}
I0406 00:19:48.140604 139868858238784 submission_runner.py:396] After eval at step 3780: RAM USED (GB) 29.766598656
I0406 00:19:48.148854 139816922068736 logging_writer.py:48] [3780] global_step=3780, preemption_count=0, score=962.256053, test/accuracy=0.983310, test/loss=0.059641, test/mean_average_precision=0.111233, test/num_examples=43793, total_duration=1285.399865, train/accuracy=0.986990, train/loss=0.047169, train/mean_average_precision=0.108564, validation/accuracy=0.984306, validation/loss=0.056511, validation/mean_average_precision=0.104385, validation/num_examples=43793
I0406 00:19:48.210960 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_3780.
I0406 00:19:48.211359 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 3780: RAM USED (GB) 29.765545984
I0406 00:20:44.936399 139816913676032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.092485, loss=0.052104
I0406 00:20:44.941595 139868858238784 submission.py:139] 4000) loss = 0.052, grad_norm = 0.092
I0406 00:22:53.185603 139816922068736 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.071692, loss=0.044904
I0406 00:22:53.190980 139868858238784 submission.py:139] 4500) loss = 0.045, grad_norm = 0.072
I0406 00:23:48.376375 139868858238784 submission_runner.py:373] Before eval at step 4719: RAM USED (GB) 29.910470656
I0406 00:23:48.376572 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:24:02.862175 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.152288 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.152931 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.158548 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.159244 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.159245 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.159419 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:03.159764 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.055395 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.318398 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.323228 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.324155 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.324791 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.325214 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.329950 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:17.330660 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:43.721441 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:24:44.187002 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.473594 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.473707 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.478078 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.479322 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.479671 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.480224 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.480627 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.666746 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.953123 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.953474 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.953689 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.953702 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.954076 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.956322 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:44.958076 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:47.454369 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:24:47.912264 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.199826 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.200050 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.205361 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.206816 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.207127 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.207268 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.207880 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.387971 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.663266 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.664145 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.664493 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.664943 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.665812 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.665965 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:24:48.670409 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:24:51.118818 139868858238784 submission_runner.py:382] Time since start: 1588.38s, 	Step: 4719, 	{'train/accuracy': 0.9873607264098072, 'train/loss': 0.04485136270523071, 'train/mean_average_precision': 0.1433803044032309, 'validation/accuracy': 0.9846976594271203, 'validation/loss': 0.05486297979950905, 'validation/mean_average_precision': 0.12919340783654765, 'validation/num_examples': 43793, 'test/accuracy': 0.9836530130793586, 'test/loss': 0.057991817593574524, 'test/mean_average_precision': 0.13206443144163868, 'test/num_examples': 43793}
I0406 00:24:51.119190 139868858238784 submission_runner.py:396] After eval at step 4719: RAM USED (GB) 30.426513408
I0406 00:24:51.127397 139816913676032 logging_writer.py:48] [4719] global_step=4719, preemption_count=0, score=1201.422405, test/accuracy=0.983653, test/loss=0.057992, test/mean_average_precision=0.132064, test/num_examples=43793, total_duration=1588.377490, train/accuracy=0.987361, train/loss=0.044851, train/mean_average_precision=0.143380, validation/accuracy=0.984698, validation/loss=0.054863, validation/mean_average_precision=0.129193, validation/num_examples=43793
I0406 00:24:51.189144 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_4719.
I0406 00:24:51.189543 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 4719: RAM USED (GB) 30.42521088
I0406 00:26:03.857552 139816922068736 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.039513, loss=0.046543
I0406 00:26:03.862966 139868858238784 submission.py:139] 5000) loss = 0.047, grad_norm = 0.040
I0406 00:28:11.414677 139816913676032 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.084534, loss=0.046597
I0406 00:28:11.420723 139868858238784 submission.py:139] 5500) loss = 0.047, grad_norm = 0.085
I0406 00:28:51.251768 139868858238784 submission_runner.py:373] Before eval at step 5660: RAM USED (GB) 30.59591168
I0406 00:28:51.251969 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:29:05.557523 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.832124 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.832421 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.838453 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.838757 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.838959 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.839633 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:05.840199 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:19.791175 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.050457 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.057223 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.057624 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.057967 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.058378 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.058704 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:20.062659 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:45.757299 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:29:46.221384 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.507862 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.507869 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.513105 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.513496 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.514364 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.515000 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.515418 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.699333 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.979388 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.979758 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.980594 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.981667 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.984394 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.987117 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:46.989903 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:49.474814 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:29:49.902276 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.243654 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.244287 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.250047 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.250577 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.250797 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.251024 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.251343 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.382017 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.651745 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.656845 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.659018 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.659639 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.659948 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.665776 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:29:50.667818 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:29:53.111028 139868858238784 submission_runner.py:382] Time since start: 1891.25s, 	Step: 5660, 	{'train/accuracy': 0.9872167388007272, 'train/loss': 0.045800354331731796, 'train/mean_average_precision': 0.1506519918745005, 'validation/accuracy': 0.9845166098755629, 'validation/loss': 0.056680262088775635, 'validation/mean_average_precision': 0.13553070833003678, 'validation/num_examples': 43793, 'test/accuracy': 0.9835304454260153, 'test/loss': 0.060189321637153625, 'test/mean_average_precision': 0.14360346129308688, 'test/num_examples': 43793}
I0406 00:29:53.111412 139868858238784 submission_runner.py:396] After eval at step 5660: RAM USED (GB) 30.976765952
I0406 00:29:53.119816 139816922068736 logging_writer.py:48] [5660] global_step=5660, preemption_count=0, score=1440.464674, test/accuracy=0.983530, test/loss=0.060189, test/mean_average_precision=0.143603, test/num_examples=43793, total_duration=1891.252832, train/accuracy=0.987217, train/loss=0.045800, train/mean_average_precision=0.150652, validation/accuracy=0.984517, validation/loss=0.056680, validation/mean_average_precision=0.135531, validation/num_examples=43793
I0406 00:29:53.181843 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_5660.
I0406 00:29:53.182232 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 5660: RAM USED (GB) 30.95662592
I0406 00:31:19.365109 139868858238784 submission_runner.py:373] Before eval at step 6000: RAM USED (GB) 30.919589888
I0406 00:31:19.365584 139868858238784 spec.py:298] Evaluating on the training split.
W0406 00:31:33.523520 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.856167 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.856447 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.862222 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.862545 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.863490 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.863918 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:33.863993 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:47.735573 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.012677 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.018930 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.019570 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.020485 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.021307 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.026162 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:31:48.030098 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:32:14.063351 139868858238784 spec.py:310] Evaluating on the validation split.
W0406 00:32:14.801134 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.146540 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.147011 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.151539 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.152685 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.152744 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.154449 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.166783 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.297662 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.579026 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.584457 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.584452 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.584759 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.586776 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.590353 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:15.590781 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:32:18.064645 139868858238784 spec.py:326] Evaluating on the test split.
W0406 00:32:18.504451 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.846112 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.846496 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.851572 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.851978 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.852735 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.853476 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.853495 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:18.984500 139868858238784 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.268373 140503842449216 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.274042 139920019023680 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.275002 140613210093376 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.275071 139934569588544 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.275660 140374714279744 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.276528 140627098117952 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
W0406 00:32:19.282402 140484492064576 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0406 00:32:21.683370 139868858238784 submission_runner.py:382] Time since start: 2039.37s, 	Step: 6000, 	{'train/accuracy': 0.9878160222907877, 'train/loss': 0.04234765097498894, 'train/mean_average_precision': 0.17621149459917257, 'validation/accuracy': 0.9851003526000908, 'validation/loss': 0.05201447010040283, 'validation/mean_average_precision': 0.14908846302572384, 'validation/num_examples': 43793, 'test/accuracy': 0.9841424413033617, 'test/loss': 0.0549391470849514, 'test/mean_average_precision': 0.1538525383265131, 'test/num_examples': 43793}
I0406 00:32:21.683764 139868858238784 submission_runner.py:396] After eval at step 6000: RAM USED (GB) 31.71770368
I0406 00:32:21.692115 139816913676032 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1526.284501, test/accuracy=0.984142, test/loss=0.054939, test/mean_average_precision=0.153853, test/num_examples=43793, total_duration=2039.366135, train/accuracy=0.987816, train/loss=0.042348, train/mean_average_precision=0.176211, validation/accuracy=0.985100, validation/loss=0.052014, validation/mean_average_precision=0.149088, validation/num_examples=43793
I0406 00:32:21.756064 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:32:21.756515 139868858238784 submission_runner.py:416] After logging and checkpointing eval at step 6000: RAM USED (GB) 31.6516352
I0406 00:32:21.926928 139816922068736 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1526.284501
I0406 00:32:22.021540 139868858238784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/ogbg_pytorch/trial_1/checkpoint_6000.
I0406 00:32:22.173379 139868858238784 submission_runner.py:550] Tuning trial 1/1
I0406 00:32:22.173598 139868858238784 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 00:32:22.174669 139868858238784 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5715422613054435, 'train/loss': 0.7069145441055298, 'train/mean_average_precision': 0.023202206958354867, 'validation/accuracy': 0.5636502837119542, 'validation/loss': 0.7099474668502808, 'validation/mean_average_precision': 0.026307287509230493, 'validation/num_examples': 43793, 'test/accuracy': 0.5584538195829415, 'test/loss': 0.714481770992279, 'test/mean_average_precision': 0.027913254581270124, 'test/num_examples': 43793, 'score': 5.487239122390747, 'total_duration': 5.489253044128418, 'global_step': 1, 'preemption_count': 0}), (962, {'train/accuracy': 0.9868119500716186, 'train/loss': 0.05468286573886871, 'train/mean_average_precision': 0.03396043962241302, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06553645431995392, 'validation/mean_average_precision': 0.03761652777552804, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06898437440395355, 'test/mean_average_precision': 0.0394538438272551, 'test/num_examples': 43793, 'score': 244.66056990623474, 'total_duration': 381.25248169898987, 'global_step': 962, 'preemption_count': 0}), (1894, {'train/accuracy': 0.9867499846919077, 'train/loss': 0.051564209163188934, 'train/mean_average_precision': 0.05950298821177134, 'validation/accuracy': 0.9841443622549844, 'validation/loss': 0.061425067484378815, 'validation/mean_average_precision': 0.05454991043621724, 'validation/num_examples': 43793, 'test/accuracy': 0.9831640060500405, 'test/loss': 0.06479746848344803, 'test/mean_average_precision': 0.05624626895844242, 'test/num_examples': 43793, 'score': 483.8344612121582, 'total_duration': 680.0221853256226, 'global_step': 1894, 'preemption_count': 0}), (2843, {'train/accuracy': 0.9869432772696364, 'train/loss': 0.048594653606414795, 'train/mean_average_precision': 0.08598831242199148, 'validation/accuracy': 0.9842194612842178, 'validation/loss': 0.058249764144420624, 'validation/mean_average_precision': 0.08441856614445839, 'validation/num_examples': 43793, 'test/accuracy': 0.9832511933498413, 'test/loss': 0.061436474323272705, 'test/mean_average_precision': 0.08834793550405858, 'test/num_examples': 43793, 'score': 723.0724115371704, 'total_duration': 984.0210304260254, 'global_step': 2843, 'preemption_count': 0}), (3780, {'train/accuracy': 0.9869897495652783, 'train/loss': 0.047168999910354614, 'train/mean_average_precision': 0.10856412142693239, 'validation/accuracy': 0.9843055207123123, 'validation/loss': 0.056511323899030685, 'validation/mean_average_precision': 0.10438536376488455, 'validation/num_examples': 43793, 'test/accuracy': 0.9833101606057454, 'test/loss': 0.05964108556509018, 'test/mean_average_precision': 0.11123258680349507, 'test/num_examples': 43793, 'score': 962.2560532093048, 'total_duration': 1285.3998646736145, 'global_step': 3780, 'preemption_count': 0}), (4719, {'train/accuracy': 0.9873607264098072, 'train/loss': 0.04485136270523071, 'train/mean_average_precision': 0.1433803044032309, 'validation/accuracy': 0.9846976594271203, 'validation/loss': 0.05486297979950905, 'validation/mean_average_precision': 0.12919340783654765, 'validation/num_examples': 43793, 'test/accuracy': 0.9836530130793586, 'test/loss': 0.057991817593574524, 'test/mean_average_precision': 0.13206443144163868, 'test/num_examples': 43793, 'score': 1201.4224050045013, 'total_duration': 1588.3774900436401, 'global_step': 4719, 'preemption_count': 0}), (5660, {'train/accuracy': 0.9872167388007272, 'train/loss': 0.045800354331731796, 'train/mean_average_precision': 0.1506519918745005, 'validation/accuracy': 0.9845166098755629, 'validation/loss': 0.056680262088775635, 'validation/mean_average_precision': 0.13553070833003678, 'validation/num_examples': 43793, 'test/accuracy': 0.9835304454260153, 'test/loss': 0.060189321637153625, 'test/mean_average_precision': 0.14360346129308688, 'test/num_examples': 43793, 'score': 1440.4646742343903, 'total_duration': 1891.2528321743011, 'global_step': 5660, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9878160222907877, 'train/loss': 0.04234765097498894, 'train/mean_average_precision': 0.17621149459917257, 'validation/accuracy': 0.9851003526000908, 'validation/loss': 0.05201447010040283, 'validation/mean_average_precision': 0.14908846302572384, 'validation/num_examples': 43793, 'test/accuracy': 0.9841424413033617, 'test/loss': 0.0549391470849514, 'test/mean_average_precision': 0.1538525383265131, 'test/num_examples': 43793, 'score': 1526.2845005989075, 'total_duration': 2039.3661348819733, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0406 00:32:22.174797 139868858238784 submission_runner.py:553] Timing: 1526.2845005989075
I0406 00:32:22.174840 139868858238784 submission_runner.py:554] ====================
I0406 00:32:22.174933 139868858238784 submission_runner.py:613] Final ogbg score: 1526.2845005989075
