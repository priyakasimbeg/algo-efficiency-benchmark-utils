I0405 06:15:13.666352 140080644233024 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw_v2/criteo1tb_jax.
I0405 06:15:14.041673 140080644233024 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0405 06:15:14.945526 140080644233024 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0405 06:15:14.946329 140080644233024 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0405 06:15:14.951845 140080644233024 submission_runner.py:511] Using RNG seed 77125124
I0405 06:15:17.364230 140080644233024 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 06:15:17.364449 140080644233024 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1.
I0405 06:15:17.364621 140080644233024 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/hparams.json.
I0405 06:15:17.490084 140080644233024 submission_runner.py:230] Starting train once: RAM USED (GB) 4.134375424
I0405 06:15:17.490261 140080644233024 submission_runner.py:231] Initializing dataset.
I0405 06:15:17.490455 140080644233024 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.134375424
I0405 06:15:17.490524 140080644233024 submission_runner.py:240] Initializing model.
I0405 06:15:24.039923 140080644233024 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.955673088
I0405 06:15:24.040115 140080644233024 submission_runner.py:252] Initializing optimizer.
I0405 06:15:27.310465 140080644233024 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.956369408
I0405 06:15:27.310660 140080644233024 submission_runner.py:261] Initializing metrics bundle.
I0405 06:15:27.310709 140080644233024 submission_runner.py:276] Initializing checkpoint and logger.
I0405 06:15:27.311753 140080644233024 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1 with prefix checkpoint_
I0405 06:15:27.312070 140080644233024 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0405 06:15:27.312143 140080644233024 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0405 06:15:28.220934 140080644233024 submission_runner.py:297] Saving meta data to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/meta_data_0.json.
I0405 06:15:28.221981 140080644233024 submission_runner.py:300] Saving flags to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/flags_0.json.
I0405 06:15:28.272663 140080644233024 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.99746048
I0405 06:15:28.272884 140080644233024 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.99746048
I0405 06:15:28.272947 140080644233024 submission_runner.py:313] Starting training loop.
I0405 06:17:57.814812 140080644233024 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 46.1815808
I0405 06:18:18.848612 139873352673024 logging_writer.py:48] [0] global_step=0, grad_norm=5.278404712677002, loss=0.5323321223258972
I0405 06:18:18.871509 140080644233024 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 53.553000448
I0405 06:18:18.871762 140080644233024 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 53.553000448
I0405 06:18:18.871883 140080644233024 spec.py:298] Evaluating on the training split.
I0405 06:27:55.681874 140080644233024 spec.py:310] Evaluating on the validation split.
I0405 06:32:22.809900 140080644233024 spec.py:326] Evaluating on the test split.
I0405 06:37:20.966164 140080644233024 submission_runner.py:382] Time since start: 170.60s, 	Step: 1, 	{'train/loss': 0.5326289274483765, 'validation/loss': 0.5287150561797753, 'validation/num_examples': 89000000, 'test/loss': 0.5321396266220606, 'test/num_examples': 89274637}
I0405 06:37:20.966784 140080644233024 submission_runner.py:396] After eval at step 1: RAM USED (GB) 96.794759168
I0405 06:37:20.977830 139816593901312 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=170.404975, test/loss=0.532140, test/num_examples=89274637, total_duration=170.598861, train/loss=0.532629, validation/loss=0.528715, validation/num_examples=89000000
I0405 06:37:27.244045 140080644233024 checkpoints.py:356] Saving checkpoint at step: 1
I0405 06:38:03.437964 140080644233024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_1
I0405 06:38:03.934993 140080644233024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_1.
I0405 06:38:04.462598 140080644233024 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 96.891355136
I0405 06:38:04.468001 140080644233024 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 96.849936384
I0405 06:38:04.521154 140080644233024 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 96.84819968
I0405 06:39:42.993366 139816585508608 logging_writer.py:48] [100] global_step=100, grad_norm=0.20970427989959717, loss=0.12766031920909882
I0405 06:41:43.882559 139816477910784 logging_writer.py:48] [200] global_step=200, grad_norm=0.04756596311926842, loss=0.12553422152996063
I0405 06:43:50.890078 139816585508608 logging_writer.py:48] [300] global_step=300, grad_norm=0.08907075226306915, loss=0.1277007907629013
I0405 06:45:46.202560 139816477910784 logging_writer.py:48] [400] global_step=400, grad_norm=0.04433368891477585, loss=0.1242881715297699
I0405 06:47:05.711537 140080644233024 submission_runner.py:373] Before eval at step 470: RAM USED (GB) 103.551356928
I0405 06:47:05.711739 140080644233024 spec.py:298] Evaluating on the training split.
I0405 06:57:39.039670 140080644233024 spec.py:310] Evaluating on the validation split.
I0405 07:01:13.674055 140080644233024 spec.py:326] Evaluating on the test split.
I0405 07:05:03.616432 140080644233024 submission_runner.py:382] Time since start: 1897.44s, 	Step: 470, 	{'train/loss': 0.125945180369441, 'validation/loss': 0.1268641797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1292004917365276, 'test/num_examples': 89274637}
I0405 07:05:03.616860 140080644233024 submission_runner.py:396] After eval at step 470: RAM USED (GB) 109.831274496
I0405 07:05:03.625507 139856692872960 logging_writer.py:48] [470] global_step=470, preemption_count=0, score=709.591535, test/loss=0.129200, test/num_examples=89274637, total_duration=1897.438398, train/loss=0.125945, validation/loss=0.126864, validation/num_examples=89000000
I0405 07:05:09.818314 140080644233024 checkpoints.py:356] Saving checkpoint at step: 470
I0405 07:05:45.750148 140080644233024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_470
I0405 07:05:46.235180 140080644233024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_470.
I0405 07:05:46.763074 140080644233024 submission_runner.py:416] After logging and checkpointing eval at step 470: RAM USED (GB) 109.888278528
I0405 07:05:54.104434 139856684480256 logging_writer.py:48] [500] global_step=500, grad_norm=0.012295598164200783, loss=0.12479357421398163
I0405 07:07:48.837048 139856642516736 logging_writer.py:48] [600] global_step=600, grad_norm=0.020037194713950157, loss=0.12403851747512817
I0405 07:09:41.776824 139856684480256 logging_writer.py:48] [700] global_step=700, grad_norm=0.00993158295750618, loss=0.12340791523456573
I0405 07:11:36.031101 140080644233024 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 110.443032576
I0405 07:11:36.031284 140080644233024 spec.py:298] Evaluating on the training split.
I0405 07:22:17.351253 140080644233024 spec.py:310] Evaluating on the validation split.
I0405 07:26:50.452814 140080644233024 spec.py:326] Evaluating on the test split.
I0405 07:31:21.635181 140080644233024 submission_runner.py:382] Time since start: 3367.76s, 	Step: 800, 	{'train/loss': 0.12515609101217579, 'validation/loss': 0.1259584382022472, 'validation/num_examples': 89000000, 'test/loss': 0.1283992003238277, 'test/num_examples': 89274637}
I0405 07:31:21.635660 140080644233024 submission_runner.py:396] After eval at step 800: RAM USED (GB) 114.626818048
I0405 07:31:21.642523 139816008148736 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1056.558246, test/loss=0.128399, test/num_examples=89274637, total_duration=3367.757889, train/loss=0.125156, validation/loss=0.125958, validation/num_examples=89000000
I0405 07:31:27.850304 140080644233024 checkpoints.py:356] Saving checkpoint at step: 800
I0405 07:32:02.975213 140080644233024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_800
I0405 07:32:03.461195 140080644233024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_800.
I0405 07:32:03.960867 140080644233024 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 114.700972032
I0405 07:32:03.966656 139815999756032 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1056.558246
I0405 07:32:09.817433 140080644233024 checkpoints.py:356] Saving checkpoint at step: 800
I0405 07:32:52.242255 140080644233024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_800
I0405 07:32:52.752836 140080644233024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw_v2/criteo1tb_jax/trial_1/checkpoint_800.
I0405 07:34:26.496280 140080644233024 submission_runner.py:550] Tuning trial 1/1
I0405 07:34:26.496511 140080644233024 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0405 07:34:26.497737 140080644233024 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 0.5326289274483765, 'validation/loss': 0.5287150561797753, 'validation/num_examples': 89000000, 'test/loss': 0.5321396266220606, 'test/num_examples': 89274637, 'score': 170.40497517585754, 'total_duration': 170.59886074066162, 'global_step': 1, 'preemption_count': 0}), (470, {'train/loss': 0.125945180369441, 'validation/loss': 0.1268641797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1292004917365276, 'test/num_examples': 89274637, 'score': 709.5915353298187, 'total_duration': 1897.4383976459503, 'global_step': 470, 'preemption_count': 0}), (800, {'train/loss': 0.12515609101217579, 'validation/loss': 0.1259584382022472, 'validation/num_examples': 89000000, 'test/loss': 0.1283992003238277, 'test/num_examples': 89274637, 'score': 1056.5582463741302, 'total_duration': 3367.7578885555267, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0405 07:34:26.497873 140080644233024 submission_runner.py:553] Timing: 1056.5582463741302
I0405 07:34:26.497922 140080644233024 submission_runner.py:554] ====================
I0405 07:34:26.498020 140080644233024 submission_runner.py:613] Final criteo1tb score: 1056.5582463741302
