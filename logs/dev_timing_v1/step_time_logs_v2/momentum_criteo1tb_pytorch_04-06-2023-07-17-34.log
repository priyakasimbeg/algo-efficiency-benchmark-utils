WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 07:17:55.760473 140354122385216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 07:17:55.760502 140227345168192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 07:17:55.760527 140565975656256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 07:17:55.760556 139838806431552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 07:17:55.761118 140659560933184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 07:17:55.761245 139636941178688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 07:17:55.761589 139636941178688 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.761549 139858082907968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 07:17:55.761576 140541289908032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 07:17:55.761897 139858082907968 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.761919 140541289908032 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.771182 140354122385216 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.771214 140227345168192 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.771246 140565975656256 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.771267 139838806431552 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.771720 140659560933184 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:17:55.780162 140541289908032 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch.
W0406 07:17:56.113783 139838806431552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.113805 139636941178688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.114916 140354122385216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.115204 140565975656256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.115230 140659560933184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.116345 140227345168192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.133182 139858082907968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:17:56.136431 140541289908032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 07:17:56.139775 140541289908032 submission_runner.py:511] Using RNG seed 218293207
I0406 07:17:56.140839 140541289908032 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 07:17:56.140958 140541289908032 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1.
I0406 07:17:56.141175 140541289908032 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/hparams.json.
I0406 07:17:56.142118 140541289908032 submission_runner.py:230] Starting train once: RAM USED (GB) 5.654962176
I0406 07:17:56.142206 140541289908032 submission_runner.py:231] Initializing dataset.
I0406 07:17:56.142368 140541289908032 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.657059328
I0406 07:17:56.142424 140541289908032 submission_runner.py:240] Initializing model.
I0406 07:18:11.024691 140541289908032 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.189405696
I0406 07:18:11.024924 140541289908032 submission_runner.py:252] Initializing optimizer.
I0406 07:18:11.569421 140541289908032 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.193317376
I0406 07:18:11.569751 140541289908032 submission_runner.py:261] Initializing metrics bundle.
I0406 07:18:11.569864 140541289908032 submission_runner.py:276] Initializing checkpoint and logger.
I0406 07:18:11.574126 140541289908032 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 07:18:11.574373 140541289908032 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 07:18:12.240581 140541289908032 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0406 07:18:12.241625 140541289908032 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/flags_0.json.
I0406 07:18:12.285002 140541289908032 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.244447744
I0406 07:18:12.286220 140541289908032 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.244447744
I0406 07:18:12.286331 140541289908032 submission_runner.py:313] Starting training loop.
I0406 07:20:30.082745 140541289908032 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 54.041468928
I0406 07:20:33.131582 140458064860928 logging_writer.py:48] [0] global_step=0, grad_norm=12.458188, loss=1.257767
I0406 07:20:33.137156 140541289908032 submission.py:139] 0) loss = 1.258, grad_norm = 12.458
I0406 07:20:33.137580 140541289908032 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 58.175410176
I0406 07:20:33.138177 140541289908032 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 58.17567232
I0406 07:20:33.138278 140541289908032 spec.py:298] Evaluating on the training split.
I0406 07:30:08.661633 140541289908032 spec.py:310] Evaluating on the validation split.
I0406 07:35:25.801206 140541289908032 spec.py:326] Evaluating on the test split.
I0406 07:40:15.525985 140541289908032 submission_runner.py:382] Time since start: 140.85s, 	Step: 1, 	{'train/loss': 1.258120056107432, 'validation/loss': 1.2582778426966292, 'validation/num_examples': 89000000, 'test/loss': 1.2581616881847417, 'test/num_examples': 89274637}
I0406 07:40:15.532180 140541289908032 submission_runner.py:396] After eval at step 1: RAM USED (GB) 106.398068736
I0406 07:40:15.545561 140402749339392 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=140.850382, test/loss=1.258162, test/num_examples=89274637, total_duration=140.852459, train/loss=1.258120, validation/loss=1.258278, validation/num_examples=89000000
I0406 07:40:24.910403 140541289908032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_1.
I0406 07:40:24.910855 140541289908032 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 106.420727808
I0406 07:40:24.928533 140541289908032 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 106.378821632
I0406 07:40:24.931963 140541289908032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.931957 139838806431552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.931970 140659560933184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.931967 139636941178688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.931969 139858082907968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.931974 140354122385216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.931977 140565975656256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:24.932027 140227345168192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:26.693784 140402740946688 logging_writer.py:48] [1] global_step=1, grad_norm=12.459400, loss=1.257553
I0406 07:40:26.696864 140541289908032 submission.py:139] 1) loss = 1.258, grad_norm = 12.459
I0406 07:40:26.697241 140541289908032 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 106.495676416
I0406 07:40:28.442083 140402749339392 logging_writer.py:48] [2] global_step=2, grad_norm=11.158892, loss=1.026423
I0406 07:40:28.445369 140541289908032 submission.py:139] 2) loss = 1.026, grad_norm = 11.159
I0406 07:40:30.208477 140402740946688 logging_writer.py:48] [3] global_step=3, grad_norm=6.888098, loss=0.573080
I0406 07:40:30.211642 140541289908032 submission.py:139] 3) loss = 0.573, grad_norm = 6.888
I0406 07:40:31.981812 140402749339392 logging_writer.py:48] [4] global_step=4, grad_norm=2.489680, loss=0.264488
I0406 07:40:31.986942 140541289908032 submission.py:139] 4) loss = 0.264, grad_norm = 2.490
I0406 07:40:33.746044 140402740946688 logging_writer.py:48] [5] global_step=5, grad_norm=0.198559, loss=0.182921
I0406 07:40:33.749600 140541289908032 submission.py:139] 5) loss = 0.183, grad_norm = 0.199
I0406 07:40:35.500341 140402749339392 logging_writer.py:48] [6] global_step=6, grad_norm=1.079327, loss=0.210822
I0406 07:40:35.503321 140541289908032 submission.py:139] 6) loss = 0.211, grad_norm = 1.079
I0406 07:40:37.238375 140402740946688 logging_writer.py:48] [7] global_step=7, grad_norm=1.852789, loss=0.292996
I0406 07:40:37.241468 140541289908032 submission.py:139] 7) loss = 0.293, grad_norm = 1.853
I0406 07:40:38.982717 140402749339392 logging_writer.py:48] [8] global_step=8, grad_norm=2.428881, loss=0.378717
I0406 07:40:38.985759 140541289908032 submission.py:139] 8) loss = 0.379, grad_norm = 2.429
I0406 07:40:40.725434 140402740946688 logging_writer.py:48] [9] global_step=9, grad_norm=2.967094, loss=0.460737
I0406 07:40:40.728344 140541289908032 submission.py:139] 9) loss = 0.461, grad_norm = 2.967
I0406 07:40:42.491793 140402749339392 logging_writer.py:48] [10] global_step=10, grad_norm=3.204426, loss=0.491546
I0406 07:40:42.494758 140541289908032 submission.py:139] 10) loss = 0.492, grad_norm = 3.204
I0406 07:40:44.252454 140402740946688 logging_writer.py:48] [11] global_step=11, grad_norm=2.727858, loss=0.404478
I0406 07:40:44.255470 140541289908032 submission.py:139] 11) loss = 0.404, grad_norm = 2.728
I0406 07:40:46.017843 140402749339392 logging_writer.py:48] [12] global_step=12, grad_norm=2.018603, loss=0.290198
I0406 07:40:46.021159 140541289908032 submission.py:139] 12) loss = 0.290, grad_norm = 2.019
I0406 07:40:48.009443 140402740946688 logging_writer.py:48] [13] global_step=13, grad_norm=1.083783, loss=0.187380
I0406 07:40:48.012687 140541289908032 submission.py:139] 13) loss = 0.187, grad_norm = 1.084
I0406 07:40:50.008001 140402749339392 logging_writer.py:48] [14] global_step=14, grad_norm=0.464878, loss=0.159594
I0406 07:40:50.011476 140541289908032 submission.py:139] 14) loss = 0.160, grad_norm = 0.465
I0406 07:40:51.809466 140402740946688 logging_writer.py:48] [15] global_step=15, grad_norm=1.693211, loss=0.206651
I0406 07:40:51.812978 140541289908032 submission.py:139] 15) loss = 0.207, grad_norm = 1.693
I0406 07:40:53.677082 140402749339392 logging_writer.py:48] [16] global_step=16, grad_norm=1.765386, loss=0.208058
I0406 07:40:53.681125 140541289908032 submission.py:139] 16) loss = 0.208, grad_norm = 1.765
I0406 07:40:55.474904 140402740946688 logging_writer.py:48] [17] global_step=17, grad_norm=0.780386, loss=0.168005
I0406 07:40:55.478367 140541289908032 submission.py:139] 17) loss = 0.168, grad_norm = 0.780
I0406 07:40:57.367738 140402749339392 logging_writer.py:48] [18] global_step=18, grad_norm=0.488849, loss=0.160318
I0406 07:40:57.373749 140541289908032 submission.py:139] 18) loss = 0.160, grad_norm = 0.489
I0406 07:40:59.280384 140402740946688 logging_writer.py:48] [19] global_step=19, grad_norm=1.200356, loss=0.193588
I0406 07:40:59.284230 140541289908032 submission.py:139] 19) loss = 0.194, grad_norm = 1.200
I0406 07:41:01.080759 140402749339392 logging_writer.py:48] [20] global_step=20, grad_norm=1.411780, loss=0.209927
I0406 07:41:01.084536 140541289908032 submission.py:139] 20) loss = 0.210, grad_norm = 1.412
I0406 07:41:02.963918 140402740946688 logging_writer.py:48] [21] global_step=21, grad_norm=1.280257, loss=0.196724
I0406 07:41:02.967378 140541289908032 submission.py:139] 21) loss = 0.197, grad_norm = 1.280
I0406 07:41:04.775830 140402749339392 logging_writer.py:48] [22] global_step=22, grad_norm=0.774385, loss=0.162302
I0406 07:41:04.779582 140541289908032 submission.py:139] 22) loss = 0.162, grad_norm = 0.774
I0406 07:41:06.661866 140402740946688 logging_writer.py:48] [23] global_step=23, grad_norm=0.132616, loss=0.147620
I0406 07:41:06.665556 140541289908032 submission.py:139] 23) loss = 0.148, grad_norm = 0.133
I0406 07:41:08.531133 140402749339392 logging_writer.py:48] [24] global_step=24, grad_norm=0.894125, loss=0.161827
I0406 07:41:08.534705 140541289908032 submission.py:139] 24) loss = 0.162, grad_norm = 0.894
I0406 07:41:10.397419 140402740946688 logging_writer.py:48] [25] global_step=25, grad_norm=1.088102, loss=0.173233
I0406 07:41:10.400987 140541289908032 submission.py:139] 25) loss = 0.173, grad_norm = 1.088
I0406 07:41:12.332367 140402749339392 logging_writer.py:48] [26] global_step=26, grad_norm=0.771122, loss=0.157270
I0406 07:41:12.335880 140541289908032 submission.py:139] 26) loss = 0.157, grad_norm = 0.771
I0406 07:41:14.165253 140402740946688 logging_writer.py:48] [27] global_step=27, grad_norm=0.065329, loss=0.143330
I0406 07:41:14.168274 140541289908032 submission.py:139] 27) loss = 0.143, grad_norm = 0.065
I0406 07:41:15.910392 140402749339392 logging_writer.py:48] [28] global_step=28, grad_norm=0.611271, loss=0.151857
I0406 07:41:15.914056 140541289908032 submission.py:139] 28) loss = 0.152, grad_norm = 0.611
I0406 07:41:17.638707 140402740946688 logging_writer.py:48] [29] global_step=29, grad_norm=0.915278, loss=0.165235
I0406 07:41:17.641781 140541289908032 submission.py:139] 29) loss = 0.165, grad_norm = 0.915
I0406 07:41:19.352930 140402749339392 logging_writer.py:48] [30] global_step=30, grad_norm=0.860295, loss=0.163224
I0406 07:41:19.356062 140541289908032 submission.py:139] 30) loss = 0.163, grad_norm = 0.860
I0406 07:41:21.070580 140402740946688 logging_writer.py:48] [31] global_step=31, grad_norm=0.441147, loss=0.147160
I0406 07:41:21.074737 140541289908032 submission.py:139] 31) loss = 0.147, grad_norm = 0.441
I0406 07:41:22.779270 140402749339392 logging_writer.py:48] [32] global_step=32, grad_norm=0.164445, loss=0.141688
I0406 07:41:22.782233 140541289908032 submission.py:139] 32) loss = 0.142, grad_norm = 0.164
I0406 07:41:24.541008 140402740946688 logging_writer.py:48] [33] global_step=33, grad_norm=0.563078, loss=0.152257
I0406 07:41:24.544174 140541289908032 submission.py:139] 33) loss = 0.152, grad_norm = 0.563
I0406 07:41:26.300252 140402749339392 logging_writer.py:48] [34] global_step=34, grad_norm=0.653778, loss=0.156902
I0406 07:41:26.303350 140541289908032 submission.py:139] 34) loss = 0.157, grad_norm = 0.654
I0406 07:41:28.035205 140402740946688 logging_writer.py:48] [35] global_step=35, grad_norm=0.505526, loss=0.148845
I0406 07:41:28.038578 140541289908032 submission.py:139] 35) loss = 0.149, grad_norm = 0.506
I0406 07:41:29.736593 140402749339392 logging_writer.py:48] [36] global_step=36, grad_norm=0.163421, loss=0.142223
I0406 07:41:29.739854 140541289908032 submission.py:139] 36) loss = 0.142, grad_norm = 0.163
I0406 07:41:31.451826 140402740946688 logging_writer.py:48] [37] global_step=37, grad_norm=0.223602, loss=0.142352
I0406 07:41:31.455375 140541289908032 submission.py:139] 37) loss = 0.142, grad_norm = 0.224
I0406 07:41:33.214848 140402749339392 logging_writer.py:48] [38] global_step=38, grad_norm=0.477444, loss=0.149180
I0406 07:41:33.218328 140541289908032 submission.py:139] 38) loss = 0.149, grad_norm = 0.477
I0406 07:41:34.925410 140402740946688 logging_writer.py:48] [39] global_step=39, grad_norm=0.528771, loss=0.151395
I0406 07:41:34.928535 140541289908032 submission.py:139] 39) loss = 0.151, grad_norm = 0.529
I0406 07:41:36.649082 140402749339392 logging_writer.py:48] [40] global_step=40, grad_norm=0.374544, loss=0.146436
I0406 07:41:36.652432 140541289908032 submission.py:139] 40) loss = 0.146, grad_norm = 0.375
I0406 07:41:38.402496 140402740946688 logging_writer.py:48] [41] global_step=41, grad_norm=0.113317, loss=0.140710
I0406 07:41:38.405766 140541289908032 submission.py:139] 41) loss = 0.141, grad_norm = 0.113
I0406 07:41:40.128576 140402749339392 logging_writer.py:48] [42] global_step=42, grad_norm=0.152284, loss=0.140926
I0406 07:41:40.131895 140541289908032 submission.py:139] 42) loss = 0.141, grad_norm = 0.152
I0406 07:41:41.835987 140402740946688 logging_writer.py:48] [43] global_step=43, grad_norm=0.299921, loss=0.144424
I0406 07:41:41.839484 140541289908032 submission.py:139] 43) loss = 0.144, grad_norm = 0.300
I0406 07:41:43.535435 140402749339392 logging_writer.py:48] [44] global_step=44, grad_norm=0.323939, loss=0.146375
I0406 07:41:43.541131 140541289908032 submission.py:139] 44) loss = 0.146, grad_norm = 0.324
I0406 07:41:45.311831 140402740946688 logging_writer.py:48] [45] global_step=45, grad_norm=0.253792, loss=0.143991
I0406 07:41:45.317335 140541289908032 submission.py:139] 45) loss = 0.144, grad_norm = 0.254
I0406 07:41:47.113301 140402749339392 logging_writer.py:48] [46] global_step=46, grad_norm=0.106450, loss=0.141547
I0406 07:41:47.116415 140541289908032 submission.py:139] 46) loss = 0.142, grad_norm = 0.106
I0406 07:41:48.916988 140402740946688 logging_writer.py:48] [47] global_step=47, grad_norm=0.082656, loss=0.141397
I0406 07:41:48.920027 140541289908032 submission.py:139] 47) loss = 0.141, grad_norm = 0.083
I0406 07:41:50.715588 140402749339392 logging_writer.py:48] [48] global_step=48, grad_norm=0.221069, loss=0.142954
I0406 07:41:50.721744 140541289908032 submission.py:139] 48) loss = 0.143, grad_norm = 0.221
I0406 07:41:52.489669 140402740946688 logging_writer.py:48] [49] global_step=49, grad_norm=0.291898, loss=0.144757
I0406 07:41:52.494398 140541289908032 submission.py:139] 49) loss = 0.145, grad_norm = 0.292
I0406 07:41:54.262571 140402749339392 logging_writer.py:48] [50] global_step=50, grad_norm=0.242544, loss=0.142833
I0406 07:41:54.265680 140541289908032 submission.py:139] 50) loss = 0.143, grad_norm = 0.243
I0406 07:41:56.052942 140402740946688 logging_writer.py:48] [51] global_step=51, grad_norm=0.115817, loss=0.140066
I0406 07:41:56.056179 140541289908032 submission.py:139] 51) loss = 0.140, grad_norm = 0.116
I0406 07:41:57.878780 140402749339392 logging_writer.py:48] [52] global_step=52, grad_norm=0.042544, loss=0.139255
I0406 07:41:57.881965 140541289908032 submission.py:139] 52) loss = 0.139, grad_norm = 0.043
I0406 07:41:59.621098 140402740946688 logging_writer.py:48] [53] global_step=53, grad_norm=0.147746, loss=0.140987
I0406 07:41:59.624382 140541289908032 submission.py:139] 53) loss = 0.141, grad_norm = 0.148
I0406 07:42:01.387645 140402749339392 logging_writer.py:48] [54] global_step=54, grad_norm=0.194687, loss=0.144276
I0406 07:42:01.390774 140541289908032 submission.py:139] 54) loss = 0.144, grad_norm = 0.195
I0406 07:42:03.155278 140402740946688 logging_writer.py:48] [55] global_step=55, grad_norm=0.190507, loss=0.142199
I0406 07:42:03.158350 140541289908032 submission.py:139] 55) loss = 0.142, grad_norm = 0.191
I0406 07:42:04.952500 140402749339392 logging_writer.py:48] [56] global_step=56, grad_norm=0.125781, loss=0.139895
I0406 07:42:04.957319 140541289908032 submission.py:139] 56) loss = 0.140, grad_norm = 0.126
I0406 07:42:06.705615 140402740946688 logging_writer.py:48] [57] global_step=57, grad_norm=0.021588, loss=0.139976
I0406 07:42:06.708759 140541289908032 submission.py:139] 57) loss = 0.140, grad_norm = 0.022
I0406 07:42:08.477474 140402749339392 logging_writer.py:48] [58] global_step=58, grad_norm=0.100894, loss=0.139308
I0406 07:42:08.480893 140541289908032 submission.py:139] 58) loss = 0.139, grad_norm = 0.101
I0406 07:42:10.229259 140402740946688 logging_writer.py:48] [59] global_step=59, grad_norm=0.182254, loss=0.142211
I0406 07:42:10.232235 140541289908032 submission.py:139] 59) loss = 0.142, grad_norm = 0.182
I0406 07:42:11.959571 140402749339392 logging_writer.py:48] [60] global_step=60, grad_norm=0.179250, loss=0.140713
I0406 07:42:11.962782 140541289908032 submission.py:139] 60) loss = 0.141, grad_norm = 0.179
I0406 07:42:13.723405 140402740946688 logging_writer.py:48] [61] global_step=61, grad_norm=0.124247, loss=0.142290
I0406 07:42:13.726840 140541289908032 submission.py:139] 61) loss = 0.142, grad_norm = 0.124
I0406 07:42:15.495881 140402749339392 logging_writer.py:48] [62] global_step=62, grad_norm=0.019070, loss=0.139814
I0406 07:42:15.499065 140541289908032 submission.py:139] 62) loss = 0.140, grad_norm = 0.019
I0406 07:42:17.249575 140402740946688 logging_writer.py:48] [63] global_step=63, grad_norm=0.107531, loss=0.139477
I0406 07:42:17.252666 140541289908032 submission.py:139] 63) loss = 0.139, grad_norm = 0.108
I0406 07:42:18.993092 140402749339392 logging_writer.py:48] [64] global_step=64, grad_norm=0.143891, loss=0.143893
I0406 07:42:18.996192 140541289908032 submission.py:139] 64) loss = 0.144, grad_norm = 0.144
I0406 07:42:20.711547 140402740946688 logging_writer.py:48] [65] global_step=65, grad_norm=0.156152, loss=0.139842
I0406 07:42:20.714928 140541289908032 submission.py:139] 65) loss = 0.140, grad_norm = 0.156
I0406 07:42:22.405103 140402749339392 logging_writer.py:48] [66] global_step=66, grad_norm=0.088950, loss=0.139346
I0406 07:42:22.408363 140541289908032 submission.py:139] 66) loss = 0.139, grad_norm = 0.089
I0406 07:42:24.152513 140402740946688 logging_writer.py:48] [67] global_step=67, grad_norm=0.022834, loss=0.137592
I0406 07:42:24.155964 140541289908032 submission.py:139] 67) loss = 0.138, grad_norm = 0.023
I0406 07:42:25.865122 140402749339392 logging_writer.py:48] [68] global_step=68, grad_norm=0.128722, loss=0.139790
I0406 07:42:25.868160 140541289908032 submission.py:139] 68) loss = 0.140, grad_norm = 0.129
I0406 07:42:27.556094 140402740946688 logging_writer.py:48] [69] global_step=69, grad_norm=0.169513, loss=0.140575
I0406 07:42:27.559300 140541289908032 submission.py:139] 69) loss = 0.141, grad_norm = 0.170
I0406 07:42:29.249160 140402749339392 logging_writer.py:48] [70] global_step=70, grad_norm=0.124670, loss=0.140076
I0406 07:42:29.252270 140541289908032 submission.py:139] 70) loss = 0.140, grad_norm = 0.125
I0406 07:42:30.928050 140402740946688 logging_writer.py:48] [71] global_step=71, grad_norm=0.014386, loss=0.139093
I0406 07:42:30.931379 140541289908032 submission.py:139] 71) loss = 0.139, grad_norm = 0.014
I0406 07:42:32.646492 140402749339392 logging_writer.py:48] [72] global_step=72, grad_norm=0.106492, loss=0.138389
I0406 07:42:32.649615 140541289908032 submission.py:139] 72) loss = 0.138, grad_norm = 0.106
I0406 07:42:34.379037 140402740946688 logging_writer.py:48] [73] global_step=73, grad_norm=0.144287, loss=0.141023
I0406 07:42:34.382154 140541289908032 submission.py:139] 73) loss = 0.141, grad_norm = 0.144
I0406 07:42:36.153854 140402749339392 logging_writer.py:48] [74] global_step=74, grad_norm=0.123769, loss=0.138802
I0406 07:42:36.156863 140541289908032 submission.py:139] 74) loss = 0.139, grad_norm = 0.124
I0406 07:42:37.879354 140402740946688 logging_writer.py:48] [75] global_step=75, grad_norm=0.025609, loss=0.138098
I0406 07:42:37.882612 140541289908032 submission.py:139] 75) loss = 0.138, grad_norm = 0.026
I0406 07:42:39.572415 140402749339392 logging_writer.py:48] [76] global_step=76, grad_norm=0.073214, loss=0.135512
I0406 07:42:39.575558 140541289908032 submission.py:139] 76) loss = 0.136, grad_norm = 0.073
I0406 07:42:41.335466 140402740946688 logging_writer.py:48] [77] global_step=77, grad_norm=0.152260, loss=0.140294
I0406 07:42:41.338344 140541289908032 submission.py:139] 77) loss = 0.140, grad_norm = 0.152
I0406 07:42:43.126383 140402749339392 logging_writer.py:48] [78] global_step=78, grad_norm=0.119470, loss=0.138388
I0406 07:42:43.129422 140541289908032 submission.py:139] 78) loss = 0.138, grad_norm = 0.119
I0406 07:42:44.882003 140402740946688 logging_writer.py:48] [79] global_step=79, grad_norm=0.026529, loss=0.140209
I0406 07:42:44.885851 140541289908032 submission.py:139] 79) loss = 0.140, grad_norm = 0.027
I0406 07:42:46.646386 140402749339392 logging_writer.py:48] [80] global_step=80, grad_norm=0.094913, loss=0.138692
I0406 07:42:46.649473 140541289908032 submission.py:139] 80) loss = 0.139, grad_norm = 0.095
I0406 07:42:48.381250 140402740946688 logging_writer.py:48] [81] global_step=81, grad_norm=0.133853, loss=0.140035
I0406 07:42:48.384656 140541289908032 submission.py:139] 81) loss = 0.140, grad_norm = 0.134
I0406 07:42:50.140005 140402749339392 logging_writer.py:48] [82] global_step=82, grad_norm=0.090492, loss=0.141069
I0406 07:42:50.143003 140541289908032 submission.py:139] 82) loss = 0.141, grad_norm = 0.090
I0406 07:42:51.916186 140402740946688 logging_writer.py:48] [83] global_step=83, grad_norm=0.006077, loss=0.139231
I0406 07:42:51.919188 140541289908032 submission.py:139] 83) loss = 0.139, grad_norm = 0.006
I0406 07:42:53.629233 140402749339392 logging_writer.py:48] [84] global_step=84, grad_norm=0.088733, loss=0.139652
I0406 07:42:53.632482 140541289908032 submission.py:139] 84) loss = 0.140, grad_norm = 0.089
I0406 07:42:55.350405 140402740946688 logging_writer.py:48] [85] global_step=85, grad_norm=0.114100, loss=0.139131
I0406 07:42:55.353495 140541289908032 submission.py:139] 85) loss = 0.139, grad_norm = 0.114
I0406 07:42:57.101387 140402749339392 logging_writer.py:48] [86] global_step=86, grad_norm=0.055107, loss=0.137193
I0406 07:42:57.104448 140541289908032 submission.py:139] 86) loss = 0.137, grad_norm = 0.055
I0406 07:42:58.854918 140402740946688 logging_writer.py:48] [87] global_step=87, grad_norm=0.028375, loss=0.136433
I0406 07:42:58.857993 140541289908032 submission.py:139] 87) loss = 0.136, grad_norm = 0.028
I0406 07:43:00.572643 140402749339392 logging_writer.py:48] [88] global_step=88, grad_norm=0.076726, loss=0.138161
I0406 07:43:00.575673 140541289908032 submission.py:139] 88) loss = 0.138, grad_norm = 0.077
I0406 07:43:02.302541 140402740946688 logging_writer.py:48] [89] global_step=89, grad_norm=0.071175, loss=0.138922
I0406 07:43:02.305789 140541289908032 submission.py:139] 89) loss = 0.139, grad_norm = 0.071
I0406 07:43:04.021072 140402749339392 logging_writer.py:48] [90] global_step=90, grad_norm=0.033139, loss=0.138170
I0406 07:43:04.024207 140541289908032 submission.py:139] 90) loss = 0.138, grad_norm = 0.033
I0406 07:43:05.726243 140402740946688 logging_writer.py:48] [91] global_step=91, grad_norm=0.035613, loss=0.140113
I0406 07:43:05.729395 140541289908032 submission.py:139] 91) loss = 0.140, grad_norm = 0.036
I0406 07:43:07.429495 140402749339392 logging_writer.py:48] [92] global_step=92, grad_norm=0.060076, loss=0.138373
I0406 07:43:07.432624 140541289908032 submission.py:139] 92) loss = 0.138, grad_norm = 0.060
I0406 07:43:09.128855 140402740946688 logging_writer.py:48] [93] global_step=93, grad_norm=0.047017, loss=0.136265
I0406 07:43:09.132249 140541289908032 submission.py:139] 93) loss = 0.136, grad_norm = 0.047
I0406 07:43:10.842346 140402749339392 logging_writer.py:48] [94] global_step=94, grad_norm=0.024498, loss=0.138208
I0406 07:43:10.845681 140541289908032 submission.py:139] 94) loss = 0.138, grad_norm = 0.024
I0406 07:43:12.585788 140402740946688 logging_writer.py:48] [95] global_step=95, grad_norm=0.025867, loss=0.139202
I0406 07:43:12.588936 140541289908032 submission.py:139] 95) loss = 0.139, grad_norm = 0.026
I0406 07:43:14.309429 140402749339392 logging_writer.py:48] [96] global_step=96, grad_norm=0.056926, loss=0.139066
I0406 07:43:14.312597 140541289908032 submission.py:139] 96) loss = 0.139, grad_norm = 0.057
I0406 07:43:16.000298 140402740946688 logging_writer.py:48] [97] global_step=97, grad_norm=0.061675, loss=0.135612
I0406 07:43:16.003378 140541289908032 submission.py:139] 97) loss = 0.136, grad_norm = 0.062
I0406 07:43:17.695056 140402749339392 logging_writer.py:48] [98] global_step=98, grad_norm=0.006509, loss=0.138243
I0406 07:43:17.698195 140541289908032 submission.py:139] 98) loss = 0.138, grad_norm = 0.007
I0406 07:43:19.415760 140402740946688 logging_writer.py:48] [99] global_step=99, grad_norm=0.042812, loss=0.137382
I0406 07:43:19.419420 140541289908032 submission.py:139] 99) loss = 0.137, grad_norm = 0.043
I0406 07:43:21.126506 140402749339392 logging_writer.py:48] [100] global_step=100, grad_norm=0.060523, loss=0.138068
I0406 07:43:21.129662 140541289908032 submission.py:139] 100) loss = 0.138, grad_norm = 0.061
I0406 07:49:26.355882 140541289908032 submission_runner.py:373] Before eval at step 312: RAM USED (GB) 113.841197056
I0406 07:49:26.356085 140541289908032 spec.py:298] Evaluating on the training split.
I0406 07:59:40.048937 140541289908032 spec.py:310] Evaluating on the validation split.
I0406 08:04:17.872959 140541289908032 spec.py:326] Evaluating on the test split.
I0406 08:09:17.776865 140541289908032 submission_runner.py:382] Time since start: 1873.99s, 	Step: 312, 	{'train/loss': 0.13627465074776376, 'validation/loss': 0.1369444606741573, 'validation/num_examples': 89000000, 'test/loss': 0.14033484112626524, 'test/num_examples': 89274637}
I0406 08:09:17.777271 140541289908032 submission_runner.py:396] After eval at step 312: RAM USED (GB) 116.71590912
I0406 08:09:17.786376 140402740946688 logging_writer.py:48] [312] global_step=312, preemption_count=0, score=665.223445, test/loss=0.140335, test/num_examples=89274637, total_duration=1873.989602, train/loss=0.136275, validation/loss=0.136944, validation/num_examples=89000000
I0406 08:09:27.335260 140541289908032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_312.
I0406 08:09:27.335716 140541289908032 submission_runner.py:416] After logging and checkpointing eval at step 312: RAM USED (GB) 116.73997312
I0406 08:14:56.672959 140402749339392 logging_writer.py:48] [500] global_step=500, grad_norm=0.028782, loss=0.136166
I0406 08:14:56.679388 140541289908032 submission.py:139] 500) loss = 0.136, grad_norm = 0.029
I0406 08:18:27.817226 140541289908032 submission_runner.py:373] Before eval at step 625: RAM USED (GB) 118.865723392
I0406 08:18:27.817420 140541289908032 spec.py:298] Evaluating on the training split.
I0406 08:28:25.983362 140541289908032 spec.py:310] Evaluating on the validation split.
I0406 08:33:15.096624 140541289908032 spec.py:326] Evaluating on the test split.
I0406 08:38:03.996695 140541289908032 submission_runner.py:382] Time since start: 3615.45s, 	Step: 625, 	{'train/loss': 0.13454084136715877, 'validation/loss': 0.13457824719101125, 'validation/num_examples': 89000000, 'test/loss': 0.137992720149621, 'test/num_examples': 89274637}
I0406 08:38:03.997098 140541289908032 submission_runner.py:396] After eval at step 625: RAM USED (GB) 121.52397824
I0406 08:38:04.004822 140402740946688 logging_writer.py:48] [625] global_step=625, preemption_count=0, score=1180.561484, test/loss=0.137993, test/num_examples=89274637, total_duration=3615.452327, train/loss=0.134541, validation/loss=0.134578, validation/num_examples=89000000
I0406 08:38:13.161015 140541289908032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_625.
I0406 08:38:13.161453 140541289908032 submission_runner.py:416] After logging and checkpointing eval at step 625: RAM USED (GB) 121.480814592
I0406 08:43:12.080659 140541289908032 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 122.771673088
I0406 08:43:12.080850 140541289908032 spec.py:298] Evaluating on the training split.
I0406 08:53:20.508790 140541289908032 spec.py:310] Evaluating on the validation split.
I0406 08:57:27.393630 140541289908032 spec.py:326] Evaluating on the test split.
I0406 09:02:39.631678 140541289908032 submission_runner.py:382] Time since start: 5099.72s, 	Step: 800, 	{'train/loss': 0.13469144190998092, 'validation/loss': 0.13430706741573034, 'validation/num_examples': 89000000, 'test/loss': 0.13713196055896593, 'test/num_examples': 89274637}
I0406 09:02:39.632053 140541289908032 submission_runner.py:396] After eval at step 800: RAM USED (GB) 124.662956032
I0406 09:02:39.640257 140402749339392 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1465.387086, test/loss=0.137132, test/num_examples=89274637, total_duration=5099.716114, train/loss=0.134691, validation/loss=0.134307, validation/num_examples=89000000
I0406 09:02:49.019971 140541289908032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:02:49.020445 140541289908032 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 124.668481536
I0406 09:02:49.027154 140402740946688 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1465.387086
I0406 09:03:00.830121 140541289908032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:04:39.745338 140541289908032 submission_runner.py:550] Tuning trial 1/1
I0406 09:04:39.745579 140541289908032 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 09:04:39.751557 140541289908032 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 1.258120056107432, 'validation/loss': 1.2582778426966292, 'validation/num_examples': 89000000, 'test/loss': 1.2581616881847417, 'test/num_examples': 89274637, 'score': 140.85038208961487, 'total_duration': 140.85245895385742, 'global_step': 1, 'preemption_count': 0}), (312, {'train/loss': 0.13627465074776376, 'validation/loss': 0.1369444606741573, 'validation/num_examples': 89000000, 'test/loss': 0.14033484112626524, 'test/num_examples': 89274637, 'score': 665.2234447002411, 'total_duration': 1873.9896020889282, 'global_step': 312, 'preemption_count': 0}), (625, {'train/loss': 0.13454084136715877, 'validation/loss': 0.13457824719101125, 'validation/num_examples': 89000000, 'test/loss': 0.137992720149621, 'test/num_examples': 89274637, 'score': 1180.561484336853, 'total_duration': 3615.4523265361786, 'global_step': 625, 'preemption_count': 0}), (800, {'train/loss': 0.13469144190998092, 'validation/loss': 0.13430706741573034, 'validation/num_examples': 89000000, 'test/loss': 0.13713196055896593, 'test/num_examples': 89274637, 'score': 1465.3870861530304, 'total_duration': 5099.716114044189, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0406 09:04:39.752339 140541289908032 submission_runner.py:553] Timing: 1465.3870861530304
I0406 09:04:39.752411 140541289908032 submission_runner.py:554] ====================
I0406 09:04:39.752484 140541289908032 submission_runner.py:613] Final criteo1tb score: 1465.3870861530304
