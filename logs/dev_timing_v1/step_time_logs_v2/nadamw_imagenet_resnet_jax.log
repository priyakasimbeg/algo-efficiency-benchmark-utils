I0404 18:23:55.558835 140276792293184 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax.
I0404 18:23:55.610435 140276792293184 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0404 18:23:56.572008 140276792293184 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0404 18:23:56.572637 140276792293184 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0404 18:23:56.575984 140276792293184 submission_runner.py:511] Using RNG seed 4164590057
I0404 18:23:57.868589 140276792293184 submission_runner.py:520] --- Tuning run 1/1 ---
I0404 18:23:57.868782 140276792293184 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1.
I0404 18:23:57.868973 140276792293184 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/hparams.json.
I0404 18:23:57.988136 140276792293184 submission_runner.py:230] Starting train once: RAM USED (GB) 4.191817728
I0404 18:23:57.988319 140276792293184 submission_runner.py:231] Initializing dataset.
I0404 18:23:57.999000 140276792293184 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:23:58.006369 140276792293184 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:23:58.006477 140276792293184 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:23:58.215446 140276792293184 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:23:59.144589 140276792293184 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.264730624
I0404 18:23:59.144767 140276792293184 submission_runner.py:240] Initializing model.
I0404 18:24:10.106116 140276792293184 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.338464768
I0404 18:24:10.106296 140276792293184 submission_runner.py:252] Initializing optimizer.
I0404 18:24:11.197459 140276792293184 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.340320256
I0404 18:24:11.197628 140276792293184 submission_runner.py:261] Initializing metrics bundle.
I0404 18:24:11.197678 140276792293184 submission_runner.py:276] Initializing checkpoint and logger.
I0404 18:24:11.198544 140276792293184 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0404 18:24:11.977326 140276792293184 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0404 18:24:11.978259 140276792293184 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/flags_0.json.
I0404 18:24:11.981159 140276792293184 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.3383296
I0404 18:24:11.981346 140276792293184 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.3383296
I0404 18:24:11.981406 140276792293184 submission_runner.py:313] Starting training loop.
I0404 18:24:15.433825 140276792293184 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 12.989104128
I0404 18:24:56.414332 140100172240640 logging_writer.py:48] [0] global_step=0, grad_norm=0.6009634733200073, loss=6.921790599822998
I0404 18:24:56.427520 140276792293184 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 23.763275776
I0404 18:24:56.427751 140276792293184 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 23.763275776
I0404 18:24:56.427826 140276792293184 spec.py:298] Evaluating on the training split.
I0404 18:24:56.901279 140276792293184 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:24:56.907399 140276792293184 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:24:56.907504 140276792293184 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:24:56.966344 140276792293184 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:25:08.609786 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 18:25:09.328694 140276792293184 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:25:09.343000 140276792293184 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0404 18:25:09.343231 140276792293184 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0404 18:25:09.396266 140276792293184 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0404 18:25:27.074944 140276792293184 spec.py:326] Evaluating on the test split.
I0404 18:25:27.486460 140276792293184 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 18:25:27.491347 140276792293184 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0404 18:25:27.521245 140276792293184 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0404 18:25:36.194116 140276792293184 submission_runner.py:382] Time since start: 44.45s, 	Step: 1, 	{'train/accuracy': 0.0006178252515383065, 'train/loss': 6.9123969078063965, 'validation/accuracy': 0.000859999970998615, 'validation/loss': 6.9116291999816895, 'validation/num_examples': 50000, 'test/accuracy': 0.0005000000237487257, 'test/loss': 6.911300182342529, 'test/num_examples': 10000}
I0404 18:25:36.194834 140276792293184 submission_runner.py:396] After eval at step 1: RAM USED (GB) 65.987772416
I0404 18:25:36.201555 140071869069056 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=44.364161, test/accuracy=0.000500, test/loss=6.911300, test/num_examples=10000, total_duration=44.446372, train/accuracy=0.000618, train/loss=6.912397, validation/accuracy=0.000860, validation/loss=6.911629, validation/num_examples=50000
I0404 18:25:36.357836 140276792293184 checkpoints.py:356] Saving checkpoint at step: 1
I0404 18:25:36.993337 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1
I0404 18:25:36.994423 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1.
I0404 18:25:36.999110 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 66.071977984
I0404 18:25:37.004659 140276792293184 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 66.071461888
I0404 18:25:37.108589 140276792293184 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 66.522329088
I0404 18:26:10.868517 140071877461760 logging_writer.py:48] [100] global_step=100, grad_norm=0.6116118431091309, loss=6.873396396636963
I0404 18:26:44.576734 140072011679488 logging_writer.py:48] [200] global_step=200, grad_norm=0.6681728959083557, loss=6.746236324310303
I0404 18:27:18.332826 140071877461760 logging_writer.py:48] [300] global_step=300, grad_norm=0.7298542261123657, loss=6.572749137878418
I0404 18:27:51.907520 140072011679488 logging_writer.py:48] [400] global_step=400, grad_norm=0.7537122368812561, loss=6.439424514770508
I0404 18:28:25.875172 140071877461760 logging_writer.py:48] [500] global_step=500, grad_norm=1.4613356590270996, loss=6.253636360168457
I0404 18:28:59.664241 140072011679488 logging_writer.py:48] [600] global_step=600, grad_norm=2.9892737865448, loss=6.127536773681641
I0404 18:29:33.585916 140071877461760 logging_writer.py:48] [700] global_step=700, grad_norm=3.8520054817199707, loss=5.950847148895264
I0404 18:30:07.505066 140072011679488 logging_writer.py:48] [800] global_step=800, grad_norm=2.2411720752716064, loss=5.829283714294434
I0404 18:30:41.332127 140071877461760 logging_writer.py:48] [900] global_step=900, grad_norm=2.553318738937378, loss=5.73629903793335
I0404 18:31:14.943347 140072011679488 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.7432620525360107, loss=5.595364093780518
I0404 18:31:48.799322 140071877461760 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.867502212524414, loss=5.588876724243164
I0404 18:32:22.641837 140072011679488 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.86406946182251, loss=5.428216457366943
I0404 18:32:56.591024 140071877461760 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.634474754333496, loss=5.373363494873047
I0404 18:33:30.546128 140072011679488 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.8156044483184814, loss=5.281521320343018
I0404 18:34:04.398849 140071877461760 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4739935398101807, loss=5.211243629455566
I0404 18:34:07.190720 140276792293184 submission_runner.py:373] Before eval at step 1510: RAM USED (GB) 67.992907776
I0404 18:34:07.190901 140276792293184 spec.py:298] Evaluating on the training split.
I0404 18:34:14.146471 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 18:34:21.882928 140276792293184 spec.py:326] Evaluating on the test split.
I0404 18:34:23.867236 140276792293184 submission_runner.py:382] Time since start: 595.21s, 	Step: 1510, 	{'train/accuracy': 0.14164142310619354, 'train/loss': 4.5789666175842285, 'validation/accuracy': 0.12567999958992004, 'validation/loss': 4.692309379577637, 'validation/num_examples': 50000, 'test/accuracy': 0.09120000153779984, 'test/loss': 5.039073944091797, 'test/num_examples': 10000}
I0404 18:34:23.868000 140276792293184 submission_runner.py:396] After eval at step 1510: RAM USED (GB) 72.797618176
I0404 18:34:23.875647 140072028464896 logging_writer.py:48] [1510] global_step=1510, preemption_count=0, score=548.949982, test/accuracy=0.091200, test/loss=5.039074, test/num_examples=10000, total_duration=595.207983, train/accuracy=0.141641, train/loss=4.578967, validation/accuracy=0.125680, validation/loss=4.692309, validation/num_examples=50000
I0404 18:34:24.106876 140276792293184 checkpoints.py:356] Saving checkpoint at step: 1510
I0404 18:34:24.975909 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1510
I0404 18:34:24.977048 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_1510.
I0404 18:34:24.982719 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 1510: RAM USED (GB) 72.890359808
I0404 18:34:55.723548 140072036857600 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.403255939483643, loss=5.125779151916504
I0404 18:35:29.667715 140100818134784 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.5831050872802734, loss=5.1361260414123535
I0404 18:36:03.586250 140072036857600 logging_writer.py:48] [1800] global_step=1800, grad_norm=6.045754432678223, loss=5.00770378112793
I0404 18:36:37.239143 140100818134784 logging_writer.py:48] [1900] global_step=1900, grad_norm=5.526083946228027, loss=4.97927713394165
I0404 18:37:11.242135 140072036857600 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.113283634185791, loss=4.799079895019531
I0404 18:37:45.084120 140100818134784 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.6532793045043945, loss=4.855689525604248
I0404 18:38:19.017157 140072036857600 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.834685802459717, loss=4.81900691986084
I0404 18:38:52.966743 140100818134784 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.966529369354248, loss=4.557631492614746
I0404 18:39:26.793591 140072036857600 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.9040496349334717, loss=4.676608085632324
I0404 18:40:00.599477 140100818134784 logging_writer.py:48] [2500] global_step=2500, grad_norm=7.426338195800781, loss=4.6513800621032715
I0404 18:40:34.491322 140072036857600 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.528388738632202, loss=4.524910926818848
I0404 18:41:08.296448 140100818134784 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.07735276222229, loss=4.493198394775391
I0404 18:41:42.216487 140072036857600 logging_writer.py:48] [2800] global_step=2800, grad_norm=6.190765380859375, loss=4.432994842529297
I0404 18:42:15.976172 140100818134784 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.117146968841553, loss=4.4415130615234375
I0404 18:42:49.761628 140072036857600 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.972818374633789, loss=4.456485748291016
I0404 18:42:55.277022 140276792293184 submission_runner.py:373] Before eval at step 3018: RAM USED (GB) 73.888509952
I0404 18:42:55.277219 140276792293184 spec.py:298] Evaluating on the training split.
I0404 18:43:02.100407 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 18:43:10.080664 140276792293184 spec.py:326] Evaluating on the test split.
I0404 18:43:12.054939 140276792293184 submission_runner.py:382] Time since start: 1123.29s, 	Step: 3018, 	{'train/accuracy': 0.30285394191741943, 'train/loss': 3.3853204250335693, 'validation/accuracy': 0.2718600034713745, 'validation/loss': 3.5446014404296875, 'validation/num_examples': 50000, 'test/accuracy': 0.20750001072883606, 'test/loss': 4.086056709289551, 'test/num_examples': 10000}
I0404 18:43:12.055562 140276792293184 submission_runner.py:396] After eval at step 3018: RAM USED (GB) 79.109226496
I0404 18:43:12.063120 140100818134784 logging_writer.py:48] [3018] global_step=3018, preemption_count=0, score=1053.652828, test/accuracy=0.207500, test/loss=4.086057, test/num_examples=10000, total_duration=1123.294259, train/accuracy=0.302854, train/loss=3.385320, validation/accuracy=0.271860, validation/loss=3.544601, validation/num_examples=50000
I0404 18:43:12.285279 140276792293184 checkpoints.py:356] Saving checkpoint at step: 3018
I0404 18:43:13.023982 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_3018
I0404 18:43:13.024953 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_3018.
I0404 18:43:13.029785 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 3018: RAM USED (GB) 79.201878016
I0404 18:43:41.132356 140072036857600 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.541087627410889, loss=4.277346611022949
I0404 18:44:14.918901 140100784563968 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.66357421875, loss=4.172039031982422
I0404 18:44:48.680896 140072036857600 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.155982494354248, loss=4.200494766235352
I0404 18:45:22.275996 140100784563968 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.640000820159912, loss=4.194301605224609
I0404 18:45:56.083757 140072036857600 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.205400466918945, loss=4.079108238220215
I0404 18:46:29.985937 140100784563968 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.456025123596191, loss=4.0191121101379395
I0404 18:47:03.791941 140072036857600 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.760199546813965, loss=3.932199001312256
I0404 18:47:37.629162 140100784563968 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.624082088470459, loss=3.9959211349487305
I0404 18:48:11.425729 140072036857600 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.016302585601807, loss=3.914128065109253
I0404 18:48:45.179939 140100784563968 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.549508571624756, loss=4.002652645111084
I0404 18:49:19.137783 140072036857600 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.182730674743652, loss=3.9675211906433105
I0404 18:49:52.953912 140100784563968 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.223820686340332, loss=3.831875801086426
I0404 18:50:26.658657 140072036857600 logging_writer.py:48] [4300] global_step=4300, grad_norm=5.952178001403809, loss=3.9423255920410156
I0404 18:51:00.490866 140100784563968 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.6281912326812744, loss=3.894498586654663
I0404 18:51:34.213332 140072036857600 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.755239963531494, loss=3.7669355869293213
I0404 18:51:43.091093 140276792293184 submission_runner.py:373] Before eval at step 4528: RAM USED (GB) 80.04622336
I0404 18:51:43.091289 140276792293184 spec.py:298] Evaluating on the training split.
I0404 18:51:49.963911 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 18:51:58.315968 140276792293184 spec.py:326] Evaluating on the test split.
I0404 18:52:00.427440 140276792293184 submission_runner.py:382] Time since start: 1651.11s, 	Step: 4528, 	{'train/accuracy': 0.42364874482154846, 'train/loss': 2.6845128536224365, 'validation/accuracy': 0.397599995136261, 'validation/loss': 2.8345954418182373, 'validation/num_examples': 50000, 'test/accuracy': 0.2897000014781952, 'test/loss': 3.48028564453125, 'test/num_examples': 10000}
I0404 18:52:00.428075 140276792293184 submission_runner.py:396] After eval at step 4528: RAM USED (GB) 85.28863232
I0404 18:52:00.440439 140100784563968 logging_writer.py:48] [4528] global_step=4528, preemption_count=0, score=1558.104712, test/accuracy=0.289700, test/loss=3.480286, test/num_examples=10000, total_duration=1651.108411, train/accuracy=0.423649, train/loss=2.684513, validation/accuracy=0.397600, validation/loss=2.834595, validation/num_examples=50000
I0404 18:52:00.630952 140276792293184 checkpoints.py:356] Saving checkpoint at step: 4528
I0404 18:52:01.640737 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_4528
I0404 18:52:01.657324 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_4528.
I0404 18:52:01.665233 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 4528: RAM USED (GB) 85.434372096
I0404 18:52:26.250728 140072036857600 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.598593235015869, loss=3.7455270290374756
I0404 18:53:00.303900 140100776171264 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.020296335220337, loss=3.6942684650421143
I0404 18:53:34.152359 140072036857600 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.9233622550964355, loss=3.6882970333099365
I0404 18:54:07.911925 140100776171264 logging_writer.py:48] [4900] global_step=4900, grad_norm=6.610888481140137, loss=3.655400276184082
I0404 18:54:41.678433 140072036857600 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.552220582962036, loss=3.5641565322875977
I0404 18:55:15.487041 140100776171264 logging_writer.py:48] [5100] global_step=5100, grad_norm=4.454678058624268, loss=3.6752097606658936
I0404 18:55:49.278318 140072036857600 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.2932488918304443, loss=3.5715906620025635
I0404 18:56:23.022555 140100776171264 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.8652045726776123, loss=3.662705421447754
I0404 18:56:56.728192 140072036857600 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.030079364776611, loss=3.4392471313476562
I0404 18:57:30.615690 140100776171264 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.0343210697174072, loss=3.5556368827819824
I0404 18:58:04.410337 140072036857600 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.7247700691223145, loss=3.4741947650909424
I0404 18:58:38.230982 140100776171264 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.14633846282959, loss=3.439634323120117
I0404 18:59:11.961371 140072036857600 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.321722030639648, loss=3.3996987342834473
I0404 18:59:45.635984 140100776171264 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.4020965099334717, loss=3.5112600326538086
I0404 19:00:19.521677 140072036857600 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.598909616470337, loss=3.352325677871704
I0404 19:00:31.674868 140276792293184 submission_runner.py:373] Before eval at step 6038: RAM USED (GB) 85.952049152
I0404 19:00:31.675047 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:00:38.792254 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:00:47.445758 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:00:49.341287 140276792293184 submission_runner.py:382] Time since start: 2179.69s, 	Step: 6038, 	{'train/accuracy': 0.5026705861091614, 'train/loss': 2.2323079109191895, 'validation/accuracy': 0.4688799977302551, 'validation/loss': 2.405428409576416, 'validation/num_examples': 50000, 'test/accuracy': 0.35270002484321594, 'test/loss': 3.1103594303131104, 'test/num_examples': 10000}
I0404 19:00:49.341925 140276792293184 submission_runner.py:396] After eval at step 6038: RAM USED (GB) 91.257765888
I0404 19:00:49.352384 140100776171264 logging_writer.py:48] [6038] global_step=6038, preemption_count=0, score=2059.077163, test/accuracy=0.352700, test/loss=3.110359, test/num_examples=10000, total_duration=2179.692152, train/accuracy=0.502671, train/loss=2.232308, validation/accuracy=0.468880, validation/loss=2.405428, validation/num_examples=50000
I0404 19:00:49.590970 140276792293184 checkpoints.py:356] Saving checkpoint at step: 6038
I0404 19:00:50.491243 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_6038
I0404 19:00:50.504319 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_6038.
I0404 19:00:50.512838 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 6038: RAM USED (GB) 91.377856512
I0404 19:01:11.791929 140072036857600 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.8561298847198486, loss=3.4641013145446777
I0404 19:01:45.588952 140095562684160 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.4464802742004395, loss=3.4238228797912598
I0404 19:02:19.471030 140072036857600 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.454523801803589, loss=3.44514536857605
I0404 19:02:53.280173 140095562684160 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.966775417327881, loss=3.365133285522461
I0404 19:03:27.230330 140072036857600 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.76456618309021, loss=3.3934898376464844
I0404 19:04:00.980876 140095562684160 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.887084722518921, loss=3.2998039722442627
I0404 19:04:34.888256 140072036857600 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.1107301712036133, loss=3.309267044067383
I0404 19:05:08.620998 140095562684160 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.4557573795318604, loss=3.4518232345581055
I0404 19:05:42.576658 140072036857600 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.2739789485931396, loss=3.243940830230713
I0404 19:06:16.301269 140095562684160 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.4545350074768066, loss=3.2793612480163574
I0404 19:06:50.062350 140072036857600 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.456779956817627, loss=3.222069025039673
I0404 19:07:23.938033 140095562684160 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.332319974899292, loss=3.243427276611328
I0404 19:07:57.911400 140072036857600 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.5390055179595947, loss=3.1805837154388428
I0404 19:08:31.725158 140095562684160 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.899763584136963, loss=3.2503702640533447
I0404 19:09:05.719524 140072036857600 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.247976303100586, loss=3.225531578063965
I0404 19:09:20.564151 140276792293184 submission_runner.py:373] Before eval at step 7546: RAM USED (GB) 92.066766848
I0404 19:09:20.564350 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:09:27.881980 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:09:37.625591 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:09:39.785804 140276792293184 submission_runner.py:382] Time since start: 2708.58s, 	Step: 7546, 	{'train/accuracy': 0.5475525856018066, 'train/loss': 2.0313544273376465, 'validation/accuracy': 0.5117599964141846, 'validation/loss': 2.212968587875366, 'validation/num_examples': 50000, 'test/accuracy': 0.3947000205516815, 'test/loss': 2.8906760215759277, 'test/num_examples': 10000}
I0404 19:09:39.786407 140276792293184 submission_runner.py:396] After eval at step 7546: RAM USED (GB) 97.221525504
I0404 19:09:39.793553 140095562684160 logging_writer.py:48] [7546] global_step=7546, preemption_count=0, score=2557.174901, test/accuracy=0.394700, test/loss=2.890676, test/num_examples=10000, total_duration=2708.581546, train/accuracy=0.547553, train/loss=2.031354, validation/accuracy=0.511760, validation/loss=2.212969, validation/num_examples=50000
I0404 19:09:39.984461 140276792293184 checkpoints.py:356] Saving checkpoint at step: 7546
I0404 19:09:40.604868 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_7546
I0404 19:09:40.606069 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_7546.
I0404 19:09:40.614998 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 7546: RAM USED (GB) 97.233371136
I0404 19:09:59.065793 140072036857600 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.825579047203064, loss=3.2650537490844727
I0404 19:10:33.023766 140096179250944 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.3425655364990234, loss=3.136176109313965
I0404 19:11:06.795614 140072036857600 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.8409438133239746, loss=3.153322458267212
I0404 19:11:40.672492 140096179250944 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.2045981884002686, loss=3.085736036300659
I0404 19:12:14.516494 140072036857600 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.4293150901794434, loss=3.3113150596618652
I0404 19:12:48.286130 140096179250944 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.1714582443237305, loss=3.1218881607055664
I0404 19:13:22.191317 140072036857600 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.8450186252593994, loss=3.0847220420837402
I0404 19:13:56.008604 140096179250944 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.810991883277893, loss=3.023815631866455
I0404 19:14:29.968492 140072036857600 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.6002230644226074, loss=3.189215898513794
I0404 19:15:03.722516 140096179250944 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.3627827167510986, loss=3.0501925945281982
I0404 19:15:37.409943 140072036857600 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.8178397417068481, loss=3.0874853134155273
I0404 19:16:11.239902 140096179250944 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.314204692840576, loss=3.2288901805877686
I0404 19:16:45.050418 140072036857600 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.8106498718261719, loss=3.0796077251434326
I0404 19:17:18.963839 140096179250944 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.054239511489868, loss=3.0510411262512207
I0404 19:17:52.769543 140072036857600 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.109426975250244, loss=3.0509543418884277
I0404 19:18:10.778212 140276792293184 submission_runner.py:373] Before eval at step 9055: RAM USED (GB) 98.00892416
I0404 19:18:10.778420 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:18:18.011596 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:18:27.704696 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:18:29.686132 140276792293184 submission_runner.py:382] Time since start: 3238.80s, 	Step: 9055, 	{'train/accuracy': 0.6025390625, 'train/loss': 1.7748883962631226, 'validation/accuracy': 0.5511599779129028, 'validation/loss': 2.015714406967163, 'validation/num_examples': 50000, 'test/accuracy': 0.42320001125335693, 'test/loss': 2.716230630874634, 'test/num_examples': 10000}
I0404 19:18:29.686920 140276792293184 submission_runner.py:396] After eval at step 9055: RAM USED (GB) 103.23693568
I0404 19:18:29.696099 140096179250944 logging_writer.py:48] [9055] global_step=9055, preemption_count=0, score=3056.814972, test/accuracy=0.423200, test/loss=2.716231, test/num_examples=10000, total_duration=3238.795456, train/accuracy=0.602539, train/loss=1.774888, validation/accuracy=0.551160, validation/loss=2.015714, validation/num_examples=50000
I0404 19:18:29.991757 140276792293184 checkpoints.py:356] Saving checkpoint at step: 9055
I0404 19:18:31.094259 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_9055
I0404 19:18:31.108775 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_9055.
I0404 19:18:31.117965 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 9055: RAM USED (GB) 103.348707328
I0404 19:18:46.612138 140072036857600 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.2199273109436035, loss=3.1154732704162598
I0404 19:19:20.260901 140096787425024 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.350198984146118, loss=3.075697660446167
I0404 19:19:53.965215 140072036857600 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.1156247854232788, loss=3.093146800994873
I0404 19:20:27.647468 140096787425024 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.2102150917053223, loss=2.992853879928589
I0404 19:21:01.534280 140072036857600 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.534231185913086, loss=2.9152965545654297
I0404 19:21:35.078493 140096787425024 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.6545530557632446, loss=3.133004665374756
I0404 19:22:08.985409 140072036857600 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.9824568033218384, loss=2.942110776901245
I0404 19:22:42.765896 140096787425024 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.029047727584839, loss=3.001192331314087
I0404 19:23:16.703783 140072036857600 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.30191707611084, loss=3.0410542488098145
I0404 19:23:50.420123 140096787425024 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.6101624965667725, loss=2.9904251098632812
I0404 19:24:24.481834 140072036857600 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.6199537515640259, loss=2.97101092338562
I0404 19:24:58.114322 140096787425024 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.9883135557174683, loss=2.966597557067871
I0404 19:25:32.093090 140072036857600 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.456326961517334, loss=2.9847946166992188
I0404 19:26:05.771638 140096787425024 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4392362833023071, loss=2.890700578689575
I0404 19:26:39.423021 140072036857600 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.7091773748397827, loss=2.850440502166748
I0404 19:27:01.277490 140276792293184 submission_runner.py:373] Before eval at step 10567: RAM USED (GB) 103.846211584
I0404 19:27:01.277674 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:27:08.371814 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:27:18.135876 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:27:20.243410 140276792293184 submission_runner.py:382] Time since start: 3769.29s, 	Step: 10567, 	{'train/accuracy': 0.6473413705825806, 'train/loss': 1.5550014972686768, 'validation/accuracy': 0.5768600106239319, 'validation/loss': 1.8976210355758667, 'validation/num_examples': 50000, 'test/accuracy': 0.450300008058548, 'test/loss': 2.595371723175049, 'test/num_examples': 10000}
I0404 19:27:20.244142 140276792293184 submission_runner.py:396] After eval at step 10567: RAM USED (GB) 108.983119872
I0404 19:27:20.252337 140096787425024 logging_writer.py:48] [10567] global_step=10567, preemption_count=0, score=3555.471111, test/accuracy=0.450300, test/loss=2.595372, test/num_examples=10000, total_duration=3769.294739, train/accuracy=0.647341, train/loss=1.555001, validation/accuracy=0.576860, validation/loss=1.897621, validation/num_examples=50000
I0404 19:27:20.556330 140276792293184 checkpoints.py:356] Saving checkpoint at step: 10567
I0404 19:27:21.659224 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_10567
I0404 19:27:21.674652 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_10567.
I0404 19:27:21.683740 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 10567: RAM USED (GB) 109.128507392
I0404 19:27:33.101481 140072036857600 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.8889849185943604, loss=2.836221218109131
I0404 19:28:06.684405 140096770639616 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.1264382600784302, loss=2.7853035926818848
I0404 19:28:40.368308 140072036857600 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.3256421089172363, loss=2.8333821296691895
I0404 19:29:14.379980 140096770639616 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5923044681549072, loss=2.8960909843444824
I0404 19:29:48.036750 140072036857600 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.6828526258468628, loss=2.8258938789367676
I0404 19:30:21.776618 140096770639616 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.1388047933578491, loss=2.769866704940796
I0404 19:30:55.354625 140072036857600 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.6035219430923462, loss=2.8388452529907227
I0404 19:31:29.168717 140096770639616 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.7584190368652344, loss=2.8957951068878174
I0404 19:32:02.934954 140072036857600 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.4198265075683594, loss=2.916656732559204
I0404 19:32:36.774080 140096770639616 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.271106243133545, loss=2.9784183502197266
I0404 19:33:10.563375 140072036857600 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.1951841115951538, loss=2.89755916595459
I0404 19:33:44.127195 140096770639616 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.07295560836792, loss=2.8700876235961914
I0404 19:34:17.917252 140072036857600 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.1347428560256958, loss=2.963555335998535
I0404 19:34:51.701642 140096770639616 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4066572189331055, loss=2.7706878185272217
I0404 19:35:25.334910 140072036857600 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.516696572303772, loss=2.9118337631225586
I0404 19:35:51.696907 140276792293184 submission_runner.py:373] Before eval at step 12080: RAM USED (GB) 109.649424384
I0404 19:35:51.697095 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:35:58.813076 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:36:08.528550 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:36:10.572540 140276792293184 submission_runner.py:382] Time since start: 4299.71s, 	Step: 12080, 	{'train/accuracy': 0.6714165806770325, 'train/loss': 1.4444607496261597, 'validation/accuracy': 0.6060199737548828, 'validation/loss': 1.7523622512817383, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.4604697227478027, 'test/num_examples': 10000}
I0404 19:36:10.573149 140276792293184 submission_runner.py:396] After eval at step 12080: RAM USED (GB) 114.88929792
I0404 19:36:10.580618 140096770639616 logging_writer.py:48] [12080] global_step=12080, preemption_count=0, score=4059.822919, test/accuracy=0.470200, test/loss=2.460470, test/num_examples=10000, total_duration=4299.712961, train/accuracy=0.671417, train/loss=1.444461, validation/accuracy=0.606020, validation/loss=1.752362, validation/num_examples=50000
I0404 19:36:10.797036 140276792293184 checkpoints.py:356] Saving checkpoint at step: 12080
I0404 19:36:11.630041 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_12080
I0404 19:36:11.642383 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_12080.
I0404 19:36:11.647848 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 12080: RAM USED (GB) 115.05164288
I0404 19:36:18.720921 140072036857600 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.5018048286437988, loss=2.789323329925537
I0404 19:36:52.596786 140096762246912 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9962291121482849, loss=2.7991809844970703
I0404 19:37:26.523457 140072036857600 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9555439352989197, loss=2.8246264457702637
I0404 19:38:00.320650 140096762246912 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1814486980438232, loss=2.7830970287323
I0404 19:38:34.020593 140072036857600 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.6716865301132202, loss=2.783442258834839
I0404 19:39:07.815082 140096762246912 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.30185067653656, loss=2.821507215499878
I0404 19:39:41.514190 140072036857600 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.7283672094345093, loss=2.7824976444244385
I0404 19:40:15.299741 140096762246912 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.1579086780548096, loss=2.7513034343719482
I0404 19:40:49.234612 140072036857600 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.2433079481124878, loss=2.754035472869873
I0404 19:41:23.033302 140096762246912 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2011301517486572, loss=2.7763187885284424
I0404 19:41:56.666554 140072036857600 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.259843349456787, loss=2.772118330001831
I0404 19:42:30.485928 140096762246912 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.3699158430099487, loss=2.772947072982788
I0404 19:43:04.224641 140072036857600 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.2905131578445435, loss=2.7495832443237305
I0404 19:43:37.957005 140096762246912 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.13884699344635, loss=2.681917428970337
I0404 19:44:11.604800 140072036857600 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.0004518032073975, loss=2.718614339828491
I0404 19:44:41.941943 140276792293184 submission_runner.py:373] Before eval at step 13592: RAM USED (GB) 115.613970432
I0404 19:44:41.942170 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:44:48.889406 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:44:58.667785 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:45:00.550316 140276792293184 submission_runner.py:382] Time since start: 4829.96s, 	Step: 13592, 	{'train/accuracy': 0.6882373690605164, 'train/loss': 1.361229419708252, 'validation/accuracy': 0.619159996509552, 'validation/loss': 1.6771879196166992, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.3591628074645996, 'test/num_examples': 10000}
I0404 19:45:00.550960 140276792293184 submission_runner.py:396] After eval at step 13592: RAM USED (GB) 120.822562816
I0404 19:45:00.558994 140096762246912 logging_writer.py:48] [13592] global_step=13592, preemption_count=0, score=4564.517875, test/accuracy=0.493800, test/loss=2.359163, test/num_examples=10000, total_duration=4829.959332, train/accuracy=0.688237, train/loss=1.361229, validation/accuracy=0.619160, validation/loss=1.677188, validation/num_examples=50000
I0404 19:45:00.777819 140276792293184 checkpoints.py:356] Saving checkpoint at step: 13592
I0404 19:45:01.749012 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_13592
I0404 19:45:01.763443 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_13592.
I0404 19:45:01.768087 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 13592: RAM USED (GB) 121.062002688
I0404 19:45:04.803782 140072036857600 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.196927785873413, loss=2.7264857292175293
I0404 19:45:38.843509 140096753854208 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.164549708366394, loss=2.7811973094940186
I0404 19:46:12.458038 140072036857600 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.2786815166473389, loss=2.714308500289917
I0404 19:46:46.262673 140096753854208 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0545798540115356, loss=2.725132942199707
I0404 19:47:19.468113 140276792293184 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 121.401122816
I0404 19:47:19.468305 140276792293184 spec.py:298] Evaluating on the training split.
I0404 19:47:26.357710 140276792293184 spec.py:310] Evaluating on the validation split.
I0404 19:47:36.402176 140276792293184 spec.py:326] Evaluating on the test split.
I0404 19:47:38.520215 140276792293184 submission_runner.py:382] Time since start: 4987.49s, 	Step: 14000, 	{'train/accuracy': 0.6817601919174194, 'train/loss': 1.3943430185317993, 'validation/accuracy': 0.6193400025367737, 'validation/loss': 1.674507975578308, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.380953073501587, 'test/num_examples': 10000}
I0404 19:47:38.520973 140276792293184 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 126.54444544
I0404 19:47:38.529905 140072036857600 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4700.545746, test/accuracy=0.490000, test/loss=2.380953, test/num_examples=10000, total_duration=4987.485465, train/accuracy=0.681760, train/loss=1.394343, validation/accuracy=0.619340, validation/loss=1.674508, validation/num_examples=50000
I0404 19:47:38.879019 140276792293184 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 19:47:40.042784 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000
I0404 19:47:40.059857 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0404 19:47:40.065945 140276792293184 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 126.811418624
I0404 19:47:40.078304 140096753854208 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4700.545746
I0404 19:47:40.264980 140276792293184 checkpoints.py:356] Saving checkpoint at step: 14000
I0404 19:47:42.688069 140276792293184 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000
I0404 19:47:42.704369 140276792293184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw_v2/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0404 19:47:42.894378 140276792293184 submission_runner.py:550] Tuning trial 1/1
I0404 19:47:42.895318 140276792293184 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0404 19:47:42.899425 140276792293184 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006178252515383065, 'train/loss': 6.9123969078063965, 'validation/accuracy': 0.000859999970998615, 'validation/loss': 6.9116291999816895, 'validation/num_examples': 50000, 'test/accuracy': 0.0005000000237487257, 'test/loss': 6.911300182342529, 'test/num_examples': 10000, 'score': 44.364161014556885, 'total_duration': 44.44637203216553, 'global_step': 1, 'preemption_count': 0}), (1510, {'train/accuracy': 0.14164142310619354, 'train/loss': 4.5789666175842285, 'validation/accuracy': 0.12567999958992004, 'validation/loss': 4.692309379577637, 'validation/num_examples': 50000, 'test/accuracy': 0.09120000153779984, 'test/loss': 5.039073944091797, 'test/num_examples': 10000, 'score': 548.9499821662903, 'total_duration': 595.2079832553864, 'global_step': 1510, 'preemption_count': 0}), (3018, {'train/accuracy': 0.30285394191741943, 'train/loss': 3.3853204250335693, 'validation/accuracy': 0.2718600034713745, 'validation/loss': 3.5446014404296875, 'validation/num_examples': 50000, 'test/accuracy': 0.20750001072883606, 'test/loss': 4.086056709289551, 'test/num_examples': 10000, 'score': 1053.6528279781342, 'total_duration': 1123.294258594513, 'global_step': 3018, 'preemption_count': 0}), (4528, {'train/accuracy': 0.42364874482154846, 'train/loss': 2.6845128536224365, 'validation/accuracy': 0.397599995136261, 'validation/loss': 2.8345954418182373, 'validation/num_examples': 50000, 'test/accuracy': 0.2897000014781952, 'test/loss': 3.48028564453125, 'test/num_examples': 10000, 'score': 1558.1047122478485, 'total_duration': 1651.1084113121033, 'global_step': 4528, 'preemption_count': 0}), (6038, {'train/accuracy': 0.5026705861091614, 'train/loss': 2.2323079109191895, 'validation/accuracy': 0.4688799977302551, 'validation/loss': 2.405428409576416, 'validation/num_examples': 50000, 'test/accuracy': 0.35270002484321594, 'test/loss': 3.1103594303131104, 'test/num_examples': 10000, 'score': 2059.0771627426147, 'total_duration': 2179.692151784897, 'global_step': 6038, 'preemption_count': 0}), (7546, {'train/accuracy': 0.5475525856018066, 'train/loss': 2.0313544273376465, 'validation/accuracy': 0.5117599964141846, 'validation/loss': 2.212968587875366, 'validation/num_examples': 50000, 'test/accuracy': 0.3947000205516815, 'test/loss': 2.8906760215759277, 'test/num_examples': 10000, 'score': 2557.1749007701874, 'total_duration': 2708.581545829773, 'global_step': 7546, 'preemption_count': 0}), (9055, {'train/accuracy': 0.6025390625, 'train/loss': 1.7748883962631226, 'validation/accuracy': 0.5511599779129028, 'validation/loss': 2.015714406967163, 'validation/num_examples': 50000, 'test/accuracy': 0.42320001125335693, 'test/loss': 2.716230630874634, 'test/num_examples': 10000, 'score': 3056.8149724006653, 'total_duration': 3238.7954556941986, 'global_step': 9055, 'preemption_count': 0}), (10567, {'train/accuracy': 0.6473413705825806, 'train/loss': 1.5550014972686768, 'validation/accuracy': 0.5768600106239319, 'validation/loss': 1.8976210355758667, 'validation/num_examples': 50000, 'test/accuracy': 0.450300008058548, 'test/loss': 2.595371723175049, 'test/num_examples': 10000, 'score': 3555.471111059189, 'total_duration': 3769.2947385311127, 'global_step': 10567, 'preemption_count': 0}), (12080, {'train/accuracy': 0.6714165806770325, 'train/loss': 1.4444607496261597, 'validation/accuracy': 0.6060199737548828, 'validation/loss': 1.7523622512817383, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.4604697227478027, 'test/num_examples': 10000, 'score': 4059.822918653488, 'total_duration': 4299.712960720062, 'global_step': 12080, 'preemption_count': 0}), (13592, {'train/accuracy': 0.6882373690605164, 'train/loss': 1.361229419708252, 'validation/accuracy': 0.619159996509552, 'validation/loss': 1.6771879196166992, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.3591628074645996, 'test/num_examples': 10000, 'score': 4564.517874717712, 'total_duration': 4829.959332227707, 'global_step': 13592, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6817601919174194, 'train/loss': 1.3943430185317993, 'validation/accuracy': 0.6193400025367737, 'validation/loss': 1.674507975578308, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.380953073501587, 'test/num_examples': 10000, 'score': 4700.545746088028, 'total_duration': 4987.485465288162, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0404 19:47:42.899557 140276792293184 submission_runner.py:553] Timing: 4700.545746088028
I0404 19:47:42.899609 140276792293184 submission_runner.py:554] ====================
I0404 19:47:42.899720 140276792293184 submission_runner.py:613] Final imagenet_resnet score: 4700.545746088028
