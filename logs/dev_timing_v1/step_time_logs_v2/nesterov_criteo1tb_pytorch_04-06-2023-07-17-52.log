WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0406 07:18:13.799827 139864034211648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0406 07:18:13.799952 139659920078656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0406 07:18:13.800587 139790274516800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0406 07:18:13.800620 140155301730112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0406 07:18:13.800715 139712450488128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0406 07:18:13.801001 140047411513152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0406 07:18:13.801311 140529022433088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0406 07:18:13.811664 140177319524160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0406 07:18:13.811859 140529022433088 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.812017 140177319524160 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.820913 139864034211648 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.820955 139659920078656 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.821417 139790274516800 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.821547 140155301730112 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.821732 139712450488128 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.821874 140047411513152 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0406 07:18:13.828640 140177319524160 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch.
W0406 07:18:14.166743 139864034211648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.166763 139659920078656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.167052 140047411513152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.167186 140177319524160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.167576 140529022433088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.167801 139790274516800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.168397 139712450488128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0406 07:18:14.169188 140155301730112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0406 07:18:14.173187 140177319524160 submission_runner.py:511] Using RNG seed 2156611269
I0406 07:18:14.174255 140177319524160 submission_runner.py:520] --- Tuning run 1/1 ---
I0406 07:18:14.174412 140177319524160 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1.
I0406 07:18:14.174635 140177319524160 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/hparams.json.
I0406 07:18:14.175570 140177319524160 submission_runner.py:230] Starting train once: RAM USED (GB) 5.570359296
I0406 07:18:14.175681 140177319524160 submission_runner.py:231] Initializing dataset.
I0406 07:18:14.175860 140177319524160 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.570359296
I0406 07:18:14.175924 140177319524160 submission_runner.py:240] Initializing model.
I0406 07:18:29.130400 140177319524160 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.082524672
I0406 07:18:29.130574 140177319524160 submission_runner.py:252] Initializing optimizer.
I0406 07:18:29.689744 140177319524160 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.08777984
I0406 07:18:29.689921 140177319524160 submission_runner.py:261] Initializing metrics bundle.
I0406 07:18:29.689968 140177319524160 submission_runner.py:276] Initializing checkpoint and logger.
I0406 07:18:29.694148 140177319524160 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0406 07:18:29.694286 140177319524160 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0406 07:18:30.286253 140177319524160 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0406 07:18:30.287167 140177319524160 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/flags_0.json.
I0406 07:18:30.329753 140177319524160 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.146672128
I0406 07:18:30.330801 140177319524160 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.146672128
I0406 07:18:30.330918 140177319524160 submission_runner.py:313] Starting training loop.
I0406 07:21:10.938843 140177319524160 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 53.94704384
I0406 07:21:14.215649 140094195361536 logging_writer.py:48] [0] global_step=0, grad_norm=1.994133, loss=0.265284
I0406 07:21:14.222587 140177319524160 submission.py:139] 0) loss = 0.265, grad_norm = 1.994
I0406 07:21:14.223339 140177319524160 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 58.137530368
I0406 07:21:14.224190 140177319524160 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 58.138304512
I0406 07:21:14.224364 140177319524160 spec.py:298] Evaluating on the training split.
I0406 07:31:05.289018 140177319524160 spec.py:310] Evaluating on the validation split.
I0406 07:36:29.316641 140177319524160 spec.py:326] Evaluating on the test split.
I0406 07:40:12.920707 140177319524160 submission_runner.py:382] Time since start: 163.89s, 	Step: 1, 	{'train/loss': 0.2642074483909354, 'validation/loss': 0.2638746966292135, 'validation/num_examples': 89000000, 'test/loss': 0.2658376085024014, 'test/num_examples': 89274637}
I0406 07:40:12.921179 140177319524160 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.782771712
I0406 07:40:12.932022 140037198079744 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=163.891571, test/loss=0.265838, test/num_examples=89274637, total_duration=163.893592, train/loss=0.264207, validation/loss=0.263875, validation/num_examples=89000000
I0406 07:40:22.160414 140177319524160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_1.
I0406 07:40:22.160838 140177319524160 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 105.889558528
I0406 07:40:22.184514 140177319524160 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 105.84827904
I0406 07:40:22.187831 140177319524160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187826 139864034211648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187832 140155301730112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187834 139790274516800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187830 140529022433088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187839 139712450488128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187837 139659920078656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:22.187835 140047411513152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0406 07:40:23.994743 140037189687040 logging_writer.py:48] [1] global_step=1, grad_norm=2.004547, loss=0.263944
I0406 07:40:23.997839 140177319524160 submission.py:139] 1) loss = 0.264, grad_norm = 2.005
I0406 07:40:23.998181 140177319524160 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 106.010804224
I0406 07:40:25.773346 140037198079744 logging_writer.py:48] [2] global_step=2, grad_norm=1.883843, loss=0.254900
I0406 07:40:25.776485 140177319524160 submission.py:139] 2) loss = 0.255, grad_norm = 1.884
I0406 07:40:27.511337 140037189687040 logging_writer.py:48] [3] global_step=3, grad_norm=1.592087, loss=0.236148
I0406 07:40:27.514462 140177319524160 submission.py:139] 3) loss = 0.236, grad_norm = 1.592
I0406 07:40:29.267408 140037198079744 logging_writer.py:48] [4] global_step=4, grad_norm=1.186710, loss=0.209448
I0406 07:40:29.270790 140177319524160 submission.py:139] 4) loss = 0.209, grad_norm = 1.187
I0406 07:40:31.031723 140037189687040 logging_writer.py:48] [5] global_step=5, grad_norm=0.843622, loss=0.185558
I0406 07:40:31.034956 140177319524160 submission.py:139] 5) loss = 0.186, grad_norm = 0.844
I0406 07:40:32.791145 140037198079744 logging_writer.py:48] [6] global_step=6, grad_norm=0.565645, loss=0.167424
I0406 07:40:32.800999 140177319524160 submission.py:139] 6) loss = 0.167, grad_norm = 0.566
I0406 07:40:34.553702 140037189687040 logging_writer.py:48] [7] global_step=7, grad_norm=0.354754, loss=0.154166
I0406 07:40:34.567741 140177319524160 submission.py:139] 7) loss = 0.154, grad_norm = 0.355
I0406 07:40:36.331292 140037198079744 logging_writer.py:48] [8] global_step=8, grad_norm=0.158251, loss=0.149857
I0406 07:40:36.343057 140177319524160 submission.py:139] 8) loss = 0.150, grad_norm = 0.158
I0406 07:40:38.119835 140037189687040 logging_writer.py:48] [9] global_step=9, grad_norm=0.053470, loss=0.143959
I0406 07:40:38.133513 140177319524160 submission.py:139] 9) loss = 0.144, grad_norm = 0.053
I0406 07:40:39.914803 140037198079744 logging_writer.py:48] [10] global_step=10, grad_norm=0.189094, loss=0.148114
I0406 07:40:39.932282 140177319524160 submission.py:139] 10) loss = 0.148, grad_norm = 0.189
I0406 07:40:41.720696 140037189687040 logging_writer.py:48] [11] global_step=11, grad_norm=0.323465, loss=0.153575
I0406 07:40:41.733884 140177319524160 submission.py:139] 11) loss = 0.154, grad_norm = 0.323
I0406 07:40:43.499867 140037198079744 logging_writer.py:48] [12] global_step=12, grad_norm=0.398814, loss=0.155421
I0406 07:40:43.508274 140177319524160 submission.py:139] 12) loss = 0.155, grad_norm = 0.399
I0406 07:40:45.288930 140037189687040 logging_writer.py:48] [13] global_step=13, grad_norm=0.445546, loss=0.157421
I0406 07:40:45.296109 140177319524160 submission.py:139] 13) loss = 0.157, grad_norm = 0.446
I0406 07:40:47.231788 140037198079744 logging_writer.py:48] [14] global_step=14, grad_norm=0.459020, loss=0.160482
I0406 07:40:47.246898 140177319524160 submission.py:139] 14) loss = 0.160, grad_norm = 0.459
I0406 07:40:49.008153 140037189687040 logging_writer.py:48] [15] global_step=15, grad_norm=0.402810, loss=0.155903
I0406 07:40:49.015720 140177319524160 submission.py:139] 15) loss = 0.156, grad_norm = 0.403
I0406 07:40:50.730368 140037198079744 logging_writer.py:48] [16] global_step=16, grad_norm=0.321441, loss=0.150689
I0406 07:40:50.741332 140177319524160 submission.py:139] 16) loss = 0.151, grad_norm = 0.321
I0406 07:40:52.502074 140037189687040 logging_writer.py:48] [17] global_step=17, grad_norm=0.240984, loss=0.150227
I0406 07:40:52.518331 140177319524160 submission.py:139] 17) loss = 0.150, grad_norm = 0.241
I0406 07:40:54.263613 140037198079744 logging_writer.py:48] [18] global_step=18, grad_norm=0.151015, loss=0.148277
I0406 07:40:54.277609 140177319524160 submission.py:139] 18) loss = 0.148, grad_norm = 0.151
I0406 07:40:55.971876 140037189687040 logging_writer.py:48] [19] global_step=19, grad_norm=0.059481, loss=0.144180
I0406 07:40:55.991519 140177319524160 submission.py:139] 19) loss = 0.144, grad_norm = 0.059
I0406 07:40:57.741640 140037198079744 logging_writer.py:48] [20] global_step=20, grad_norm=0.056367, loss=0.145562
I0406 07:40:57.757507 140177319524160 submission.py:139] 20) loss = 0.146, grad_norm = 0.056
I0406 07:40:59.484950 140037189687040 logging_writer.py:48] [21] global_step=21, grad_norm=0.104161, loss=0.144761
I0406 07:40:59.504638 140177319524160 submission.py:139] 21) loss = 0.145, grad_norm = 0.104
I0406 07:41:01.285900 140037198079744 logging_writer.py:48] [22] global_step=22, grad_norm=0.134180, loss=0.145305
I0406 07:41:01.297804 140177319524160 submission.py:139] 22) loss = 0.145, grad_norm = 0.134
I0406 07:41:03.083200 140037189687040 logging_writer.py:48] [23] global_step=23, grad_norm=0.149924, loss=0.145544
I0406 07:41:03.102793 140177319524160 submission.py:139] 23) loss = 0.146, grad_norm = 0.150
I0406 07:41:04.846655 140037198079744 logging_writer.py:48] [24] global_step=24, grad_norm=0.151214, loss=0.145216
I0406 07:41:04.858381 140177319524160 submission.py:139] 24) loss = 0.145, grad_norm = 0.151
I0406 07:41:06.617878 140037189687040 logging_writer.py:48] [25] global_step=25, grad_norm=0.143134, loss=0.143421
I0406 07:41:06.628010 140177319524160 submission.py:139] 25) loss = 0.143, grad_norm = 0.143
I0406 07:41:08.405507 140037198079744 logging_writer.py:48] [26] global_step=26, grad_norm=0.115016, loss=0.144067
I0406 07:41:08.414629 140177319524160 submission.py:139] 26) loss = 0.144, grad_norm = 0.115
I0406 07:41:10.162827 140037189687040 logging_writer.py:48] [27] global_step=27, grad_norm=0.080966, loss=0.142730
I0406 07:41:10.169466 140177319524160 submission.py:139] 27) loss = 0.143, grad_norm = 0.081
I0406 07:41:12.013166 140037198079744 logging_writer.py:48] [28] global_step=28, grad_norm=0.037817, loss=0.142270
I0406 07:41:12.024960 140177319524160 submission.py:139] 28) loss = 0.142, grad_norm = 0.038
I0406 07:41:13.839875 140037189687040 logging_writer.py:48] [29] global_step=29, grad_norm=0.030268, loss=0.141955
I0406 07:41:13.852597 140177319524160 submission.py:139] 29) loss = 0.142, grad_norm = 0.030
I0406 07:41:15.666631 140037198079744 logging_writer.py:48] [30] global_step=30, grad_norm=0.057698, loss=0.141464
I0406 07:41:15.675649 140177319524160 submission.py:139] 30) loss = 0.141, grad_norm = 0.058
I0406 07:41:17.434018 140037189687040 logging_writer.py:48] [31] global_step=31, grad_norm=0.084955, loss=0.142128
I0406 07:41:17.447920 140177319524160 submission.py:139] 31) loss = 0.142, grad_norm = 0.085
I0406 07:41:19.149870 140037198079744 logging_writer.py:48] [32] global_step=32, grad_norm=0.089913, loss=0.140508
I0406 07:41:19.161928 140177319524160 submission.py:139] 32) loss = 0.141, grad_norm = 0.090
I0406 07:41:20.858846 140037189687040 logging_writer.py:48] [33] global_step=33, grad_norm=0.088798, loss=0.140755
I0406 07:41:20.872817 140177319524160 submission.py:139] 33) loss = 0.141, grad_norm = 0.089
I0406 07:41:22.578872 140037198079744 logging_writer.py:48] [34] global_step=34, grad_norm=0.076349, loss=0.141217
I0406 07:41:22.581898 140177319524160 submission.py:139] 34) loss = 0.141, grad_norm = 0.076
I0406 07:41:24.290719 140037189687040 logging_writer.py:48] [35] global_step=35, grad_norm=0.053778, loss=0.141681
I0406 07:41:24.293890 140177319524160 submission.py:139] 35) loss = 0.142, grad_norm = 0.054
I0406 07:41:26.030015 140037198079744 logging_writer.py:48] [36] global_step=36, grad_norm=0.019814, loss=0.138404
I0406 07:41:26.033023 140177319524160 submission.py:139] 36) loss = 0.138, grad_norm = 0.020
I0406 07:41:27.734252 140037189687040 logging_writer.py:48] [37] global_step=37, grad_norm=0.016497, loss=0.140268
I0406 07:41:27.737249 140177319524160 submission.py:139] 37) loss = 0.140, grad_norm = 0.016
I0406 07:41:29.419294 140037198079744 logging_writer.py:48] [38] global_step=38, grad_norm=0.027390, loss=0.138914
I0406 07:41:29.422380 140177319524160 submission.py:139] 38) loss = 0.139, grad_norm = 0.027
I0406 07:41:31.170771 140037189687040 logging_writer.py:48] [39] global_step=39, grad_norm=0.028134, loss=0.140393
I0406 07:41:31.173961 140177319524160 submission.py:139] 39) loss = 0.140, grad_norm = 0.028
I0406 07:41:32.872572 140037198079744 logging_writer.py:48] [40] global_step=40, grad_norm=0.035686, loss=0.138942
I0406 07:41:32.877682 140177319524160 submission.py:139] 40) loss = 0.139, grad_norm = 0.036
I0406 07:41:34.606745 140037189687040 logging_writer.py:48] [41] global_step=41, grad_norm=0.023673, loss=0.140850
I0406 07:41:34.611813 140177319524160 submission.py:139] 41) loss = 0.141, grad_norm = 0.024
I0406 07:41:36.312262 140037198079744 logging_writer.py:48] [42] global_step=42, grad_norm=0.019546, loss=0.140386
I0406 07:41:36.317294 140177319524160 submission.py:139] 42) loss = 0.140, grad_norm = 0.020
I0406 07:41:38.041321 140037189687040 logging_writer.py:48] [43] global_step=43, grad_norm=0.016621, loss=0.139871
I0406 07:41:38.046658 140177319524160 submission.py:139] 43) loss = 0.140, grad_norm = 0.017
I0406 07:41:39.808125 140037198079744 logging_writer.py:48] [44] global_step=44, grad_norm=0.011453, loss=0.138643
I0406 07:41:39.811149 140177319524160 submission.py:139] 44) loss = 0.139, grad_norm = 0.011
I0406 07:41:41.514508 140037189687040 logging_writer.py:48] [45] global_step=45, grad_norm=0.011211, loss=0.138310
I0406 07:41:41.519526 140177319524160 submission.py:139] 45) loss = 0.138, grad_norm = 0.011
I0406 07:41:43.271256 140037198079744 logging_writer.py:48] [46] global_step=46, grad_norm=0.014636, loss=0.138258
I0406 07:41:43.276325 140177319524160 submission.py:139] 46) loss = 0.138, grad_norm = 0.015
I0406 07:41:45.004698 140037189687040 logging_writer.py:48] [47] global_step=47, grad_norm=0.018188, loss=0.138563
I0406 07:41:45.010317 140177319524160 submission.py:139] 47) loss = 0.139, grad_norm = 0.018
I0406 07:41:46.711809 140037189687040 logging_writer.py:48] [48] global_step=48, grad_norm=0.010344, loss=0.136311
I0406 07:41:46.722874 140177319524160 submission.py:139] 48) loss = 0.136, grad_norm = 0.010
I0406 07:41:48.416893 140037198079744 logging_writer.py:48] [49] global_step=49, grad_norm=0.017469, loss=0.138841
I0406 07:41:48.424943 140177319524160 submission.py:139] 49) loss = 0.139, grad_norm = 0.017
I0406 07:41:50.156845 140037189687040 logging_writer.py:48] [50] global_step=50, grad_norm=0.022868, loss=0.140864
I0406 07:41:50.161951 140177319524160 submission.py:139] 50) loss = 0.141, grad_norm = 0.023
I0406 07:41:51.860507 140037198079744 logging_writer.py:48] [51] global_step=51, grad_norm=0.008791, loss=0.137940
I0406 07:41:51.863614 140177319524160 submission.py:139] 51) loss = 0.138, grad_norm = 0.009
I0406 07:41:53.556978 140037189687040 logging_writer.py:48] [52] global_step=52, grad_norm=0.013803, loss=0.138010
I0406 07:41:53.569486 140177319524160 submission.py:139] 52) loss = 0.138, grad_norm = 0.014
I0406 07:41:55.291311 140037198079744 logging_writer.py:48] [53] global_step=53, grad_norm=0.010765, loss=0.138437
I0406 07:41:55.300482 140177319524160 submission.py:139] 53) loss = 0.138, grad_norm = 0.011
I0406 07:41:57.013006 140037189687040 logging_writer.py:48] [54] global_step=54, grad_norm=0.007549, loss=0.140377
I0406 07:41:57.066596 140177319524160 submission.py:139] 54) loss = 0.140, grad_norm = 0.008
I0406 07:41:58.781475 140037198079744 logging_writer.py:48] [55] global_step=55, grad_norm=0.008588, loss=0.139144
I0406 07:41:58.791023 140177319524160 submission.py:139] 55) loss = 0.139, grad_norm = 0.009
I0406 07:42:00.503728 140037189687040 logging_writer.py:48] [56] global_step=56, grad_norm=0.008883, loss=0.137850
I0406 07:42:00.543830 140177319524160 submission.py:139] 56) loss = 0.138, grad_norm = 0.009
I0406 07:42:02.287234 140037198079744 logging_writer.py:48] [57] global_step=57, grad_norm=0.006917, loss=0.139504
I0406 07:42:02.294128 140177319524160 submission.py:139] 57) loss = 0.140, grad_norm = 0.007
I0406 07:42:04.023621 140037189687040 logging_writer.py:48] [58] global_step=58, grad_norm=0.012811, loss=0.141415
I0406 07:42:04.032606 140177319524160 submission.py:139] 58) loss = 0.141, grad_norm = 0.013
I0406 07:42:05.740177 140037198079744 logging_writer.py:48] [59] global_step=59, grad_norm=0.007062, loss=0.139035
I0406 07:42:05.745025 140177319524160 submission.py:139] 59) loss = 0.139, grad_norm = 0.007
I0406 07:42:07.466594 140037189687040 logging_writer.py:48] [60] global_step=60, grad_norm=0.010197, loss=0.141278
I0406 07:42:07.499633 140177319524160 submission.py:139] 60) loss = 0.141, grad_norm = 0.010
I0406 07:42:09.216144 140037198079744 logging_writer.py:48] [61] global_step=61, grad_norm=0.010820, loss=0.137917
I0406 07:42:09.221850 140177319524160 submission.py:139] 61) loss = 0.138, grad_norm = 0.011
I0406 07:42:10.928948 140037189687040 logging_writer.py:48] [62] global_step=62, grad_norm=0.006646, loss=0.138285
I0406 07:42:10.935121 140177319524160 submission.py:139] 62) loss = 0.138, grad_norm = 0.007
I0406 07:42:12.639020 140037198079744 logging_writer.py:48] [63] global_step=63, grad_norm=0.007052, loss=0.138291
I0406 07:42:12.668026 140177319524160 submission.py:139] 63) loss = 0.138, grad_norm = 0.007
I0406 07:42:14.380565 140037189687040 logging_writer.py:48] [64] global_step=64, grad_norm=0.006488, loss=0.140462
I0406 07:42:14.386053 140177319524160 submission.py:139] 64) loss = 0.140, grad_norm = 0.006
I0406 07:42:16.120397 140037198079744 logging_writer.py:48] [65] global_step=65, grad_norm=0.009625, loss=0.140575
I0406 07:42:16.135005 140177319524160 submission.py:139] 65) loss = 0.141, grad_norm = 0.010
I0406 07:42:17.850457 140037189687040 logging_writer.py:48] [66] global_step=66, grad_norm=0.005108, loss=0.139382
I0406 07:42:17.867267 140177319524160 submission.py:139] 66) loss = 0.139, grad_norm = 0.005
I0406 07:42:19.560230 140037198079744 logging_writer.py:48] [67] global_step=67, grad_norm=0.007247, loss=0.139607
I0406 07:42:19.564901 140177319524160 submission.py:139] 67) loss = 0.140, grad_norm = 0.007
I0406 07:42:21.261740 140037189687040 logging_writer.py:48] [68] global_step=68, grad_norm=0.006456, loss=0.140014
I0406 07:42:21.273674 140177319524160 submission.py:139] 68) loss = 0.140, grad_norm = 0.006
I0406 07:42:23.018438 140037198079744 logging_writer.py:48] [69] global_step=69, grad_norm=0.006644, loss=0.139258
I0406 07:42:23.027005 140177319524160 submission.py:139] 69) loss = 0.139, grad_norm = 0.007
I0406 07:42:24.739551 140037189687040 logging_writer.py:48] [70] global_step=70, grad_norm=0.005540, loss=0.140498
I0406 07:42:24.753793 140177319524160 submission.py:139] 70) loss = 0.140, grad_norm = 0.006
I0406 07:42:26.449320 140037198079744 logging_writer.py:48] [71] global_step=71, grad_norm=0.005583, loss=0.139777
I0406 07:42:26.486668 140177319524160 submission.py:139] 71) loss = 0.140, grad_norm = 0.006
I0406 07:42:28.206460 140037189687040 logging_writer.py:48] [72] global_step=72, grad_norm=0.010135, loss=0.138378
I0406 07:42:28.222421 140177319524160 submission.py:139] 72) loss = 0.138, grad_norm = 0.010
I0406 07:42:29.934548 140037198079744 logging_writer.py:48] [73] global_step=73, grad_norm=0.009717, loss=0.140772
I0406 07:42:29.939778 140177319524160 submission.py:139] 73) loss = 0.141, grad_norm = 0.010
I0406 07:42:31.640834 140037189687040 logging_writer.py:48] [74] global_step=74, grad_norm=0.005926, loss=0.138427
I0406 07:42:31.670475 140177319524160 submission.py:139] 74) loss = 0.138, grad_norm = 0.006
I0406 07:42:33.430783 140037198079744 logging_writer.py:48] [75] global_step=75, grad_norm=0.004723, loss=0.138437
I0406 07:42:33.436411 140177319524160 submission.py:139] 75) loss = 0.138, grad_norm = 0.005
I0406 07:42:35.196557 140037189687040 logging_writer.py:48] [76] global_step=76, grad_norm=0.008612, loss=0.140188
I0406 07:42:35.212363 140177319524160 submission.py:139] 76) loss = 0.140, grad_norm = 0.009
I0406 07:42:36.966722 140037198079744 logging_writer.py:48] [77] global_step=77, grad_norm=0.005206, loss=0.138812
I0406 07:42:36.993728 140177319524160 submission.py:139] 77) loss = 0.139, grad_norm = 0.005
I0406 07:42:38.713362 140037189687040 logging_writer.py:48] [78] global_step=78, grad_norm=0.009812, loss=0.137190
I0406 07:42:38.734512 140177319524160 submission.py:139] 78) loss = 0.137, grad_norm = 0.010
I0406 07:42:40.427261 140037198079744 logging_writer.py:48] [79] global_step=79, grad_norm=0.004764, loss=0.139162
I0406 07:42:40.438828 140177319524160 submission.py:139] 79) loss = 0.139, grad_norm = 0.005
I0406 07:42:42.140608 140037189687040 logging_writer.py:48] [80] global_step=80, grad_norm=0.004597, loss=0.137866
I0406 07:42:42.164529 140177319524160 submission.py:139] 80) loss = 0.138, grad_norm = 0.005
I0406 07:42:43.873918 140037198079744 logging_writer.py:48] [81] global_step=81, grad_norm=0.004946, loss=0.137770
I0406 07:42:43.881348 140177319524160 submission.py:139] 81) loss = 0.138, grad_norm = 0.005
I0406 07:42:45.574012 140037189687040 logging_writer.py:48] [82] global_step=82, grad_norm=0.012846, loss=0.139406
I0406 07:42:45.583824 140177319524160 submission.py:139] 82) loss = 0.139, grad_norm = 0.013
I0406 07:42:47.319074 140037198079744 logging_writer.py:48] [83] global_step=83, grad_norm=0.004474, loss=0.139284
I0406 07:42:47.358985 140177319524160 submission.py:139] 83) loss = 0.139, grad_norm = 0.004
I0406 07:42:49.072807 140037189687040 logging_writer.py:48] [84] global_step=84, grad_norm=0.006540, loss=0.137989
I0406 07:42:49.126257 140177319524160 submission.py:139] 84) loss = 0.138, grad_norm = 0.007
I0406 07:42:50.831074 140037198079744 logging_writer.py:48] [85] global_step=85, grad_norm=0.004609, loss=0.139790
I0406 07:42:50.859519 140177319524160 submission.py:139] 85) loss = 0.140, grad_norm = 0.005
I0406 07:42:52.584588 140037189687040 logging_writer.py:48] [86] global_step=86, grad_norm=0.005271, loss=0.139859
I0406 07:42:52.599445 140177319524160 submission.py:139] 86) loss = 0.140, grad_norm = 0.005
I0406 07:42:54.310445 140037198079744 logging_writer.py:48] [87] global_step=87, grad_norm=0.004543, loss=0.140562
I0406 07:42:54.319568 140177319524160 submission.py:139] 87) loss = 0.141, grad_norm = 0.005
I0406 07:42:56.097196 140037189687040 logging_writer.py:48] [88] global_step=88, grad_norm=0.009603, loss=0.139192
I0406 07:42:56.131045 140177319524160 submission.py:139] 88) loss = 0.139, grad_norm = 0.010
I0406 07:42:57.892978 140037198079744 logging_writer.py:48] [89] global_step=89, grad_norm=0.011089, loss=0.142283
I0406 07:42:57.902191 140177319524160 submission.py:139] 89) loss = 0.142, grad_norm = 0.011
I0406 07:42:59.658406 140037189687040 logging_writer.py:48] [90] global_step=90, grad_norm=0.010553, loss=0.137886
I0406 07:42:59.669388 140177319524160 submission.py:139] 90) loss = 0.138, grad_norm = 0.011
I0406 07:43:01.485155 140037198079744 logging_writer.py:48] [91] global_step=91, grad_norm=0.004692, loss=0.138807
I0406 07:43:01.495215 140177319524160 submission.py:139] 91) loss = 0.139, grad_norm = 0.005
I0406 07:43:03.240325 140037189687040 logging_writer.py:48] [92] global_step=92, grad_norm=0.004600, loss=0.139198
I0406 07:43:03.248454 140177319524160 submission.py:139] 92) loss = 0.139, grad_norm = 0.005
I0406 07:43:04.939844 140037198079744 logging_writer.py:48] [93] global_step=93, grad_norm=0.006304, loss=0.138673
I0406 07:43:05.021311 140177319524160 submission.py:139] 93) loss = 0.139, grad_norm = 0.006
I0406 07:43:06.780183 140037189687040 logging_writer.py:48] [94] global_step=94, grad_norm=0.004300, loss=0.138834
I0406 07:43:06.790946 140177319524160 submission.py:139] 94) loss = 0.139, grad_norm = 0.004
I0406 07:43:08.477570 140037198079744 logging_writer.py:48] [95] global_step=95, grad_norm=0.009773, loss=0.137370
I0406 07:43:08.493315 140177319524160 submission.py:139] 95) loss = 0.137, grad_norm = 0.010
I0406 07:43:10.175346 140037189687040 logging_writer.py:48] [96] global_step=96, grad_norm=0.006372, loss=0.137666
I0406 07:43:10.182657 140177319524160 submission.py:139] 96) loss = 0.138, grad_norm = 0.006
I0406 07:43:11.853777 140037198079744 logging_writer.py:48] [97] global_step=97, grad_norm=0.007539, loss=0.138810
I0406 07:43:11.865327 140177319524160 submission.py:139] 97) loss = 0.139, grad_norm = 0.008
I0406 07:43:13.604945 140037189687040 logging_writer.py:48] [98] global_step=98, grad_norm=0.005704, loss=0.138662
I0406 07:43:13.614989 140177319524160 submission.py:139] 98) loss = 0.139, grad_norm = 0.006
I0406 07:43:15.296842 140037198079744 logging_writer.py:48] [99] global_step=99, grad_norm=0.004232, loss=0.138138
I0406 07:43:15.304800 140177319524160 submission.py:139] 99) loss = 0.138, grad_norm = 0.004
I0406 07:43:17.015187 140037189687040 logging_writer.py:48] [100] global_step=100, grad_norm=0.006365, loss=0.137878
I0406 07:43:17.025902 140177319524160 submission.py:139] 100) loss = 0.138, grad_norm = 0.006
I0406 07:49:23.876144 140177319524160 submission_runner.py:373] Before eval at step 314: RAM USED (GB) 113.269415936
I0406 07:49:23.876387 140177319524160 spec.py:298] Evaluating on the training split.
I0406 07:59:52.126072 140177319524160 spec.py:310] Evaluating on the validation split.
I0406 08:05:04.739542 140177319524160 spec.py:326] Evaluating on the test split.
I0406 08:09:35.093310 140177319524160 submission_runner.py:382] Time since start: 1853.46s, 	Step: 314, 	{'train/loss': 0.13911844440204768, 'validation/loss': 0.1389366404494382, 'validation/num_examples': 89000000, 'test/loss': 0.14235464211408666, 'test/num_examples': 89274637}
I0406 08:09:35.093687 140177319524160 submission_runner.py:396] After eval at step 314: RAM USED (GB) 116.419940352
I0406 08:09:35.101688 140069973235456 logging_writer.py:48] [314] global_step=314, preemption_count=0, score=686.735082, test/loss=0.142355, test/num_examples=89274637, total_duration=1853.457654, train/loss=0.139118, validation/loss=0.138937, validation/num_examples=89000000
I0406 08:09:44.560945 140177319524160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_314.
I0406 08:09:44.561379 140177319524160 submission_runner.py:416] After logging and checkpointing eval at step 314: RAM USED (GB) 116.439281664
I0406 08:15:12.132893 140069964842752 logging_writer.py:48] [500] global_step=500, grad_norm=0.040361, loss=0.138163
I0406 08:15:12.136939 140177319524160 submission.py:139] 500) loss = 0.138, grad_norm = 0.040
I0406 08:18:45.803881 140177319524160 submission_runner.py:373] Before eval at step 624: RAM USED (GB) 118.977200128
I0406 08:18:45.804077 140177319524160 spec.py:298] Evaluating on the training split.
I0406 08:29:14.301416 140177319524160 spec.py:310] Evaluating on the validation split.
I0406 08:34:19.127926 140177319524160 spec.py:326] Evaluating on the test split.
I0406 08:38:19.660900 140177319524160 submission_runner.py:382] Time since start: 3615.39s, 	Step: 624, 	{'train/loss': 0.13458503459287618, 'validation/loss': 0.13513529213483147, 'validation/num_examples': 89000000, 'test/loss': 0.1384522235581871, 'test/num_examples': 89274637}
I0406 08:38:19.661377 140177319524160 submission_runner.py:396] After eval at step 624: RAM USED (GB) 121.144324096
I0406 08:38:19.670687 140036334941952 logging_writer.py:48] [624] global_step=624, preemption_count=0, score=1200.787793, test/loss=0.138452, test/num_examples=89274637, total_duration=3615.385914, train/loss=0.134585, validation/loss=0.135135, validation/num_examples=89000000
I0406 08:38:30.192672 140177319524160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_624.
I0406 08:38:30.193199 140177319524160 submission_runner.py:416] After logging and checkpointing eval at step 624: RAM USED (GB) 121.154162688
I0406 08:43:33.035777 140177319524160 submission_runner.py:373] Before eval at step 800: RAM USED (GB) 122.48772608
I0406 08:43:33.035996 140177319524160 spec.py:298] Evaluating on the training split.
I0406 08:53:10.764826 140177319524160 spec.py:310] Evaluating on the validation split.
I0406 08:58:03.820112 140177319524160 spec.py:326] Evaluating on the test split.
I0406 09:02:15.226930 140177319524160 submission_runner.py:382] Time since start: 5102.62s, 	Step: 800, 	{'train/loss': 0.13388142364704056, 'validation/loss': 0.13434016853932584, 'validation/num_examples': 89000000, 'test/loss': 0.13769369905138903, 'test/num_examples': 89274637}
I0406 09:02:15.227329 140177319524160 submission_runner.py:396] After eval at step 800: RAM USED (GB) 123.686612992
I0406 09:02:15.235743 140036326549248 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1488.158103, test/loss=0.137694, test/num_examples=89274637, total_duration=5102.617878, train/loss=0.133881, validation/loss=0.134340, validation/num_examples=89000000
I0406 09:02:24.440795 140177319524160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:02:24.441236 140177319524160 submission_runner.py:416] After logging and checkpointing eval at step 800: RAM USED (GB) 123.709452288
I0406 09:02:24.449098 140036334941952 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1488.158103
I0406 09:02:36.428649 140177319524160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_nesterov/criteo1tb_pytorch/trial_1/checkpoint_800.
I0406 09:04:15.754263 140177319524160 submission_runner.py:550] Tuning trial 1/1
I0406 09:04:15.754650 140177319524160 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0406 09:04:15.760306 140177319524160 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/loss': 0.2642074483909354, 'validation/loss': 0.2638746966292135, 'validation/num_examples': 89000000, 'test/loss': 0.2658376085024014, 'test/num_examples': 89274637, 'score': 163.89157128334045, 'total_duration': 163.8935923576355, 'global_step': 1, 'preemption_count': 0}), (314, {'train/loss': 0.13911844440204768, 'validation/loss': 0.1389366404494382, 'validation/num_examples': 89000000, 'test/loss': 0.14235464211408666, 'test/num_examples': 89274637, 'score': 686.7350823879242, 'total_duration': 1853.4576544761658, 'global_step': 314, 'preemption_count': 0}), (624, {'train/loss': 0.13458503459287618, 'validation/loss': 0.13513529213483147, 'validation/num_examples': 89000000, 'test/loss': 0.1384522235581871, 'test/num_examples': 89274637, 'score': 1200.7877931594849, 'total_duration': 3615.3859140872955, 'global_step': 624, 'preemption_count': 0}), (800, {'train/loss': 0.13388142364704056, 'validation/loss': 0.13434016853932584, 'validation/num_examples': 89000000, 'test/loss': 0.13769369905138903, 'test/num_examples': 89274637, 'score': 1488.1581025123596, 'total_duration': 5102.6178777217865, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0406 09:04:15.760938 140177319524160 submission_runner.py:553] Timing: 1488.1581025123596
I0406 09:04:15.761021 140177319524160 submission_runner.py:554] ====================
I0406 09:04:15.761106 140177319524160 submission_runner.py:613] Final criteo1tb score: 1488.1581025123596
