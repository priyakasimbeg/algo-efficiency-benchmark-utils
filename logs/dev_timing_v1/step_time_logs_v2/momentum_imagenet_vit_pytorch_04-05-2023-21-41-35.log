WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0405 21:41:57.528422 139883052345152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0405 21:41:57.528467 140481334884160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0405 21:41:57.529076 139718624053056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0405 21:41:57.529104 140583862613824 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0405 21:41:57.529610 140356270233408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0405 21:41:57.529748 139906727810880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0405 21:41:57.529803 139980397815616 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0405 21:41:57.539644 139718624053056 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.539736 140583862613824 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.539635 140541023364928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0405 21:41:57.539930 140541023364928 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.540296 139906727810880 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.540324 140356270233408 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.540411 139980397815616 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.549336 139883052345152 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:57.549363 140481334884160 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0405 21:41:59.579898 140541023364928 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch.
W0405 21:41:59.690799 140583862613824 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.690811 139718624053056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.690818 140356270233408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.690825 139906727810880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.690841 140541023364928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.690856 140481334884160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.690887 139883052345152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0405 21:41:59.695673 139980397815616 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0405 21:41:59.696394 140541023364928 submission_runner.py:511] Using RNG seed 226595559
I0405 21:41:59.697632 140541023364928 submission_runner.py:520] --- Tuning run 1/1 ---
I0405 21:41:59.697734 140541023364928 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1.
I0405 21:41:59.697928 140541023364928 logger_utils.py:84] Saving hparams to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/hparams.json.
I0405 21:41:59.698895 140541023364928 submission_runner.py:230] Starting train once: RAM USED (GB) 5.785063424
I0405 21:41:59.698990 140541023364928 submission_runner.py:231] Initializing dataset.
I0405 21:42:04.142632 140541023364928 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 8.018628608
I0405 21:42:04.142839 140541023364928 submission_runner.py:240] Initializing model.
I0405 21:42:08.465402 140541023364928 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.064084992
I0405 21:42:08.465620 140541023364928 submission_runner.py:252] Initializing optimizer.
I0405 21:42:09.029396 140541023364928 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.068504576
I0405 21:42:09.029610 140541023364928 submission_runner.py:261] Initializing metrics bundle.
I0405 21:42:09.029664 140541023364928 submission_runner.py:276] Initializing checkpoint and logger.
I0405 21:42:09.798029 140541023364928 submission_runner.py:297] Saving meta data to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0405 21:42:09.799056 140541023364928 submission_runner.py:300] Saving flags to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/flags_0.json.
I0405 21:42:09.845839 140541023364928 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.116681728
I0405 21:42:09.847084 140541023364928 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.116677632
I0405 21:42:09.847200 140541023364928 submission_runner.py:313] Starting training loop.
I0405 21:42:12.235866 140541023364928 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.481585664
I0405 21:42:16.331924 140512145164032 logging_writer.py:48] [0] global_step=0, grad_norm=0.293631, loss=6.907755
I0405 21:42:16.356971 140541023364928 submission.py:139] 0) loss = 6.908, grad_norm = 0.294
I0405 21:42:16.357831 140541023364928 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.94335232
I0405 21:42:16.358457 140541023364928 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.94335232
I0405 21:42:16.358592 140541023364928 spec.py:298] Evaluating on the training split.
I0405 21:43:06.905104 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 21:43:52.675131 140541023364928 spec.py:326] Evaluating on the test split.
I0405 21:43:52.692028 140541023364928 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:43:52.698314 140541023364928 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0405 21:43:52.781074 140541023364928 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0405 21:44:06.034469 140541023364928 submission_runner.py:382] Time since start: 6.51s, 	Step: 1, 	{'train/accuracy': 0.00109375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0405 21:44:06.036327 140541023364928 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.55389184
I0405 21:44:06.049979 140507103622912 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.509799, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=6.511882, train/accuracy=0.001094, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0405 21:44:06.356362 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0405 21:44:06.357076 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.556783616
I0405 21:44:06.376320 140541023364928 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.55778304
I0405 21:44:06.383552 140541023364928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383513 140356270233408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383528 140583862613824 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383537 139718624053056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383547 140481334884160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383556 139883052345152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383562 139906727810880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.383625 139980397815616 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0405 21:44:06.962448 140507095230208 logging_writer.py:48] [1] global_step=1, grad_norm=0.301411, loss=6.907754
I0405 21:44:06.966580 140541023364928 submission.py:139] 1) loss = 6.908, grad_norm = 0.301
I0405 21:44:06.967578 140541023364928 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.609204224
I0405 21:44:07.383539 140507103622912 logging_writer.py:48] [2] global_step=2, grad_norm=0.308907, loss=6.907755
I0405 21:44:07.387411 140541023364928 submission.py:139] 2) loss = 6.908, grad_norm = 0.309
I0405 21:44:07.775109 140507095230208 logging_writer.py:48] [3] global_step=3, grad_norm=0.302575, loss=6.907754
I0405 21:44:07.778928 140541023364928 submission.py:139] 3) loss = 6.908, grad_norm = 0.303
I0405 21:44:08.169882 140507103622912 logging_writer.py:48] [4] global_step=4, grad_norm=0.301557, loss=6.907754
I0405 21:44:08.173772 140541023364928 submission.py:139] 4) loss = 6.908, grad_norm = 0.302
I0405 21:44:08.560960 140507095230208 logging_writer.py:48] [5] global_step=5, grad_norm=0.302385, loss=6.907755
I0405 21:44:08.565011 140541023364928 submission.py:139] 5) loss = 6.908, grad_norm = 0.302
I0405 21:44:08.951637 140507103622912 logging_writer.py:48] [6] global_step=6, grad_norm=0.305994, loss=6.907748
I0405 21:44:08.956657 140541023364928 submission.py:139] 6) loss = 6.908, grad_norm = 0.306
I0405 21:44:09.348813 140507095230208 logging_writer.py:48] [7] global_step=7, grad_norm=0.315256, loss=6.907751
I0405 21:44:09.353571 140541023364928 submission.py:139] 7) loss = 6.908, grad_norm = 0.315
I0405 21:44:09.748147 140507103622912 logging_writer.py:48] [8] global_step=8, grad_norm=0.301855, loss=6.907748
I0405 21:44:09.753157 140541023364928 submission.py:139] 8) loss = 6.908, grad_norm = 0.302
I0405 21:44:10.147654 140507095230208 logging_writer.py:48] [9] global_step=9, grad_norm=0.300911, loss=6.907746
I0405 21:44:10.151631 140541023364928 submission.py:139] 9) loss = 6.908, grad_norm = 0.301
I0405 21:44:10.544363 140507103622912 logging_writer.py:48] [10] global_step=10, grad_norm=0.308184, loss=6.907738
I0405 21:44:10.551600 140541023364928 submission.py:139] 10) loss = 6.908, grad_norm = 0.308
I0405 21:44:10.949460 140507095230208 logging_writer.py:48] [11] global_step=11, grad_norm=0.304288, loss=6.907731
I0405 21:44:10.953578 140541023364928 submission.py:139] 11) loss = 6.908, grad_norm = 0.304
I0405 21:44:11.350905 140507103622912 logging_writer.py:48] [12] global_step=12, grad_norm=0.299043, loss=6.907739
I0405 21:44:11.354994 140541023364928 submission.py:139] 12) loss = 6.908, grad_norm = 0.299
I0405 21:44:11.756386 140507095230208 logging_writer.py:48] [13] global_step=13, grad_norm=0.298587, loss=6.907724
I0405 21:44:11.760388 140541023364928 submission.py:139] 13) loss = 6.908, grad_norm = 0.299
I0405 21:44:12.155618 140507103622912 logging_writer.py:48] [14] global_step=14, grad_norm=0.310744, loss=6.907728
I0405 21:44:12.165137 140541023364928 submission.py:139] 14) loss = 6.908, grad_norm = 0.311
I0405 21:44:12.559066 140507095230208 logging_writer.py:48] [15] global_step=15, grad_norm=0.299017, loss=6.907720
I0405 21:44:12.563817 140541023364928 submission.py:139] 15) loss = 6.908, grad_norm = 0.299
I0405 21:44:12.951978 140507103622912 logging_writer.py:48] [16] global_step=16, grad_norm=0.294277, loss=6.907712
I0405 21:44:12.957030 140541023364928 submission.py:139] 16) loss = 6.908, grad_norm = 0.294
I0405 21:44:13.347378 140507095230208 logging_writer.py:48] [17] global_step=17, grad_norm=0.301936, loss=6.907688
I0405 21:44:13.351148 140541023364928 submission.py:139] 17) loss = 6.908, grad_norm = 0.302
I0405 21:44:13.743154 140507103622912 logging_writer.py:48] [18] global_step=18, grad_norm=0.302677, loss=6.907697
I0405 21:44:13.747841 140541023364928 submission.py:139] 18) loss = 6.908, grad_norm = 0.303
I0405 21:44:14.143569 140507095230208 logging_writer.py:48] [19] global_step=19, grad_norm=0.301709, loss=6.907704
I0405 21:44:14.147879 140541023364928 submission.py:139] 19) loss = 6.908, grad_norm = 0.302
I0405 21:44:14.536478 140507103622912 logging_writer.py:48] [20] global_step=20, grad_norm=0.302111, loss=6.907613
I0405 21:44:14.540575 140541023364928 submission.py:139] 20) loss = 6.908, grad_norm = 0.302
I0405 21:44:14.929925 140507095230208 logging_writer.py:48] [21] global_step=21, grad_norm=0.299957, loss=6.907644
I0405 21:44:14.934034 140541023364928 submission.py:139] 21) loss = 6.908, grad_norm = 0.300
I0405 21:44:15.330059 140507103622912 logging_writer.py:48] [22] global_step=22, grad_norm=0.307815, loss=6.907629
I0405 21:44:15.336507 140541023364928 submission.py:139] 22) loss = 6.908, grad_norm = 0.308
I0405 21:44:15.729713 140507095230208 logging_writer.py:48] [23] global_step=23, grad_norm=0.295949, loss=6.907658
I0405 21:44:15.734660 140541023364928 submission.py:139] 23) loss = 6.908, grad_norm = 0.296
I0405 21:44:16.131745 140507103622912 logging_writer.py:48] [24] global_step=24, grad_norm=0.305441, loss=6.907505
I0405 21:44:16.135803 140541023364928 submission.py:139] 24) loss = 6.908, grad_norm = 0.305
I0405 21:44:16.527819 140507095230208 logging_writer.py:48] [25] global_step=25, grad_norm=0.291690, loss=6.907500
I0405 21:44:16.531956 140541023364928 submission.py:139] 25) loss = 6.908, grad_norm = 0.292
I0405 21:44:16.927003 140507103622912 logging_writer.py:48] [26] global_step=26, grad_norm=0.303123, loss=6.907597
I0405 21:44:16.931238 140541023364928 submission.py:139] 26) loss = 6.908, grad_norm = 0.303
I0405 21:44:17.325073 140507095230208 logging_writer.py:48] [27] global_step=27, grad_norm=0.297906, loss=6.907492
I0405 21:44:17.329207 140541023364928 submission.py:139] 27) loss = 6.907, grad_norm = 0.298
I0405 21:44:17.722398 140507103622912 logging_writer.py:48] [28] global_step=28, grad_norm=0.307796, loss=6.907472
I0405 21:44:17.727028 140541023364928 submission.py:139] 28) loss = 6.907, grad_norm = 0.308
I0405 21:44:18.119941 140507095230208 logging_writer.py:48] [29] global_step=29, grad_norm=0.303403, loss=6.907537
I0405 21:44:18.124162 140541023364928 submission.py:139] 29) loss = 6.908, grad_norm = 0.303
I0405 21:44:18.514676 140507103622912 logging_writer.py:48] [30] global_step=30, grad_norm=0.303083, loss=6.907378
I0405 21:44:18.522261 140541023364928 submission.py:139] 30) loss = 6.907, grad_norm = 0.303
I0405 21:44:18.914003 140507095230208 logging_writer.py:48] [31] global_step=31, grad_norm=0.297905, loss=6.907494
I0405 21:44:18.918632 140541023364928 submission.py:139] 31) loss = 6.907, grad_norm = 0.298
I0405 21:44:19.309465 140507103622912 logging_writer.py:48] [32] global_step=32, grad_norm=0.295394, loss=6.907542
I0405 21:44:19.313705 140541023364928 submission.py:139] 32) loss = 6.908, grad_norm = 0.295
I0405 21:44:19.706184 140507095230208 logging_writer.py:48] [33] global_step=33, grad_norm=0.307942, loss=6.907337
I0405 21:44:19.711512 140541023364928 submission.py:139] 33) loss = 6.907, grad_norm = 0.308
I0405 21:44:20.107447 140507103622912 logging_writer.py:48] [34] global_step=34, grad_norm=0.299193, loss=6.907406
I0405 21:44:20.112014 140541023364928 submission.py:139] 34) loss = 6.907, grad_norm = 0.299
I0405 21:44:20.511199 140507095230208 logging_writer.py:48] [35] global_step=35, grad_norm=0.295372, loss=6.907354
I0405 21:44:20.515385 140541023364928 submission.py:139] 35) loss = 6.907, grad_norm = 0.295
I0405 21:44:20.908264 140507103622912 logging_writer.py:48] [36] global_step=36, grad_norm=0.301781, loss=6.907255
I0405 21:44:20.912897 140541023364928 submission.py:139] 36) loss = 6.907, grad_norm = 0.302
I0405 21:44:21.308993 140507095230208 logging_writer.py:48] [37] global_step=37, grad_norm=0.298715, loss=6.907406
I0405 21:44:21.312644 140541023364928 submission.py:139] 37) loss = 6.907, grad_norm = 0.299
I0405 21:44:21.706652 140507103622912 logging_writer.py:48] [38] global_step=38, grad_norm=0.292931, loss=6.907279
I0405 21:44:21.711492 140541023364928 submission.py:139] 38) loss = 6.907, grad_norm = 0.293
I0405 21:44:22.107134 140507095230208 logging_writer.py:48] [39] global_step=39, grad_norm=0.306806, loss=6.907281
I0405 21:44:22.111267 140541023364928 submission.py:139] 39) loss = 6.907, grad_norm = 0.307
I0405 21:44:22.512915 140507103622912 logging_writer.py:48] [40] global_step=40, grad_norm=0.307746, loss=6.907107
I0405 21:44:22.519935 140541023364928 submission.py:139] 40) loss = 6.907, grad_norm = 0.308
I0405 21:44:22.912387 140507095230208 logging_writer.py:48] [41] global_step=41, grad_norm=0.294267, loss=6.907151
I0405 21:44:22.916401 140541023364928 submission.py:139] 41) loss = 6.907, grad_norm = 0.294
I0405 21:44:23.309215 140507103622912 logging_writer.py:48] [42] global_step=42, grad_norm=0.297235, loss=6.907375
I0405 21:44:23.313782 140541023364928 submission.py:139] 42) loss = 6.907, grad_norm = 0.297
I0405 21:44:23.703940 140507095230208 logging_writer.py:48] [43] global_step=43, grad_norm=0.306669, loss=6.906995
I0405 21:44:23.710891 140541023364928 submission.py:139] 43) loss = 6.907, grad_norm = 0.307
I0405 21:44:24.105788 140507103622912 logging_writer.py:48] [44] global_step=44, grad_norm=0.295701, loss=6.907106
I0405 21:44:24.109714 140541023364928 submission.py:139] 44) loss = 6.907, grad_norm = 0.296
I0405 21:44:24.505134 140507095230208 logging_writer.py:48] [45] global_step=45, grad_norm=0.295099, loss=6.906945
I0405 21:44:24.509140 140541023364928 submission.py:139] 45) loss = 6.907, grad_norm = 0.295
I0405 21:44:24.902058 140507103622912 logging_writer.py:48] [46] global_step=46, grad_norm=0.307543, loss=6.906744
I0405 21:44:24.907801 140541023364928 submission.py:139] 46) loss = 6.907, grad_norm = 0.308
I0405 21:44:25.303623 140507095230208 logging_writer.py:48] [47] global_step=47, grad_norm=0.304422, loss=6.906892
I0405 21:44:25.309052 140541023364928 submission.py:139] 47) loss = 6.907, grad_norm = 0.304
I0405 21:44:25.707214 140507103622912 logging_writer.py:48] [48] global_step=48, grad_norm=0.300643, loss=6.906771
I0405 21:44:25.712511 140541023364928 submission.py:139] 48) loss = 6.907, grad_norm = 0.301
I0405 21:44:26.116262 140507095230208 logging_writer.py:48] [49] global_step=49, grad_norm=0.303986, loss=6.906985
I0405 21:44:26.121349 140541023364928 submission.py:139] 49) loss = 6.907, grad_norm = 0.304
I0405 21:44:26.512791 140507103622912 logging_writer.py:48] [50] global_step=50, grad_norm=0.300575, loss=6.906613
I0405 21:44:26.516999 140541023364928 submission.py:139] 50) loss = 6.907, grad_norm = 0.301
I0405 21:44:26.908377 140507095230208 logging_writer.py:48] [51] global_step=51, grad_norm=0.302076, loss=6.906840
I0405 21:44:26.912317 140541023364928 submission.py:139] 51) loss = 6.907, grad_norm = 0.302
I0405 21:44:27.302170 140507103622912 logging_writer.py:48] [52] global_step=52, grad_norm=0.300204, loss=6.906904
I0405 21:44:27.306463 140541023364928 submission.py:139] 52) loss = 6.907, grad_norm = 0.300
I0405 21:44:27.697064 140507095230208 logging_writer.py:48] [53] global_step=53, grad_norm=0.294673, loss=6.906466
I0405 21:44:27.701088 140541023364928 submission.py:139] 53) loss = 6.906, grad_norm = 0.295
I0405 21:44:28.090406 140507103622912 logging_writer.py:48] [54] global_step=54, grad_norm=0.298644, loss=6.906888
I0405 21:44:28.095113 140541023364928 submission.py:139] 54) loss = 6.907, grad_norm = 0.299
I0405 21:44:28.494300 140507095230208 logging_writer.py:48] [55] global_step=55, grad_norm=0.292593, loss=6.906817
I0405 21:44:28.498237 140541023364928 submission.py:139] 55) loss = 6.907, grad_norm = 0.293
I0405 21:44:28.895364 140507103622912 logging_writer.py:48] [56] global_step=56, grad_norm=0.298963, loss=6.906573
I0405 21:44:28.902272 140541023364928 submission.py:139] 56) loss = 6.907, grad_norm = 0.299
I0405 21:44:29.293463 140507095230208 logging_writer.py:48] [57] global_step=57, grad_norm=0.306287, loss=6.906996
I0405 21:44:29.297583 140541023364928 submission.py:139] 57) loss = 6.907, grad_norm = 0.306
I0405 21:44:29.690676 140507103622912 logging_writer.py:48] [58] global_step=58, grad_norm=0.306541, loss=6.906178
I0405 21:44:29.695969 140541023364928 submission.py:139] 58) loss = 6.906, grad_norm = 0.307
I0405 21:44:30.090746 140507095230208 logging_writer.py:48] [59] global_step=59, grad_norm=0.300583, loss=6.906507
I0405 21:44:30.094280 140541023364928 submission.py:139] 59) loss = 6.907, grad_norm = 0.301
I0405 21:44:30.487348 140507103622912 logging_writer.py:48] [60] global_step=60, grad_norm=0.305126, loss=6.906207
I0405 21:44:30.491411 140541023364928 submission.py:139] 60) loss = 6.906, grad_norm = 0.305
I0405 21:44:30.883872 140507095230208 logging_writer.py:48] [61] global_step=61, grad_norm=0.303707, loss=6.905873
I0405 21:44:30.887740 140541023364928 submission.py:139] 61) loss = 6.906, grad_norm = 0.304
I0405 21:44:31.287106 140507103622912 logging_writer.py:48] [62] global_step=62, grad_norm=0.294478, loss=6.906687
I0405 21:44:31.292264 140541023364928 submission.py:139] 62) loss = 6.907, grad_norm = 0.294
I0405 21:44:31.687697 140507095230208 logging_writer.py:48] [63] global_step=63, grad_norm=0.303614, loss=6.905817
I0405 21:44:31.691338 140541023364928 submission.py:139] 63) loss = 6.906, grad_norm = 0.304
I0405 21:44:32.088952 140507103622912 logging_writer.py:48] [64] global_step=64, grad_norm=0.306035, loss=6.905902
I0405 21:44:32.096034 140541023364928 submission.py:139] 64) loss = 6.906, grad_norm = 0.306
I0405 21:44:32.487689 140507095230208 logging_writer.py:48] [65] global_step=65, grad_norm=0.303768, loss=6.905928
I0405 21:44:32.491335 140541023364928 submission.py:139] 65) loss = 6.906, grad_norm = 0.304
I0405 21:44:32.888365 140507103622912 logging_writer.py:48] [66] global_step=66, grad_norm=0.294432, loss=6.906258
I0405 21:44:32.895515 140541023364928 submission.py:139] 66) loss = 6.906, grad_norm = 0.294
I0405 21:44:33.288326 140507095230208 logging_writer.py:48] [67] global_step=67, grad_norm=0.305300, loss=6.906836
I0405 21:44:33.292688 140541023364928 submission.py:139] 67) loss = 6.907, grad_norm = 0.305
I0405 21:44:33.686299 140507103622912 logging_writer.py:48] [68] global_step=68, grad_norm=0.298913, loss=6.906421
I0405 21:44:33.694649 140541023364928 submission.py:139] 68) loss = 6.906, grad_norm = 0.299
I0405 21:44:34.091536 140507095230208 logging_writer.py:48] [69] global_step=69, grad_norm=0.295732, loss=6.905980
I0405 21:44:34.095111 140541023364928 submission.py:139] 69) loss = 6.906, grad_norm = 0.296
I0405 21:44:34.484720 140507103622912 logging_writer.py:48] [70] global_step=70, grad_norm=0.299523, loss=6.905667
I0405 21:44:34.494006 140541023364928 submission.py:139] 70) loss = 6.906, grad_norm = 0.300
I0405 21:44:34.884164 140507095230208 logging_writer.py:48] [71] global_step=71, grad_norm=0.296194, loss=6.905959
I0405 21:44:34.889443 140541023364928 submission.py:139] 71) loss = 6.906, grad_norm = 0.296
I0405 21:44:35.295412 140507103622912 logging_writer.py:48] [72] global_step=72, grad_norm=0.306026, loss=6.904526
I0405 21:44:35.299805 140541023364928 submission.py:139] 72) loss = 6.905, grad_norm = 0.306
I0405 21:44:35.693674 140507095230208 logging_writer.py:48] [73] global_step=73, grad_norm=0.306500, loss=6.905812
I0405 21:44:35.697869 140541023364928 submission.py:139] 73) loss = 6.906, grad_norm = 0.307
I0405 21:44:36.092642 140507103622912 logging_writer.py:48] [74] global_step=74, grad_norm=0.294313, loss=6.905829
I0405 21:44:36.096339 140541023364928 submission.py:139] 74) loss = 6.906, grad_norm = 0.294
I0405 21:44:36.491134 140507095230208 logging_writer.py:48] [75] global_step=75, grad_norm=0.295377, loss=6.904761
I0405 21:44:36.495846 140541023364928 submission.py:139] 75) loss = 6.905, grad_norm = 0.295
I0405 21:44:36.892694 140507103622912 logging_writer.py:48] [76] global_step=76, grad_norm=0.302460, loss=6.905365
I0405 21:44:36.897419 140541023364928 submission.py:139] 76) loss = 6.905, grad_norm = 0.302
I0405 21:44:37.288577 140507095230208 logging_writer.py:48] [77] global_step=77, grad_norm=0.300057, loss=6.905109
I0405 21:44:37.292561 140541023364928 submission.py:139] 77) loss = 6.905, grad_norm = 0.300
I0405 21:44:37.687163 140507103622912 logging_writer.py:48] [78] global_step=78, grad_norm=0.302713, loss=6.905261
I0405 21:44:37.692003 140541023364928 submission.py:139] 78) loss = 6.905, grad_norm = 0.303
I0405 21:44:38.088143 140507095230208 logging_writer.py:48] [79] global_step=79, grad_norm=0.311552, loss=6.906236
I0405 21:44:38.092572 140541023364928 submission.py:139] 79) loss = 6.906, grad_norm = 0.312
I0405 21:44:38.485275 140507103622912 logging_writer.py:48] [80] global_step=80, grad_norm=0.295406, loss=6.904430
I0405 21:44:38.489350 140541023364928 submission.py:139] 80) loss = 6.904, grad_norm = 0.295
I0405 21:44:38.881688 140507095230208 logging_writer.py:48] [81] global_step=81, grad_norm=0.296605, loss=6.905823
I0405 21:44:38.886545 140541023364928 submission.py:139] 81) loss = 6.906, grad_norm = 0.297
I0405 21:44:39.285849 140507103622912 logging_writer.py:48] [82] global_step=82, grad_norm=0.298736, loss=6.904396
I0405 21:44:39.290081 140541023364928 submission.py:139] 82) loss = 6.904, grad_norm = 0.299
I0405 21:44:39.680791 140507095230208 logging_writer.py:48] [83] global_step=83, grad_norm=0.303577, loss=6.904955
I0405 21:44:39.685818 140541023364928 submission.py:139] 83) loss = 6.905, grad_norm = 0.304
I0405 21:44:40.078547 140507103622912 logging_writer.py:48] [84] global_step=84, grad_norm=0.300584, loss=6.905252
I0405 21:44:40.085017 140541023364928 submission.py:139] 84) loss = 6.905, grad_norm = 0.301
I0405 21:44:40.477263 140507095230208 logging_writer.py:48] [85] global_step=85, grad_norm=0.296016, loss=6.905343
I0405 21:44:40.481112 140541023364928 submission.py:139] 85) loss = 6.905, grad_norm = 0.296
I0405 21:44:40.874996 140507103622912 logging_writer.py:48] [86] global_step=86, grad_norm=0.311411, loss=6.904270
I0405 21:44:40.878739 140541023364928 submission.py:139] 86) loss = 6.904, grad_norm = 0.311
I0405 21:44:41.270360 140507095230208 logging_writer.py:48] [87] global_step=87, grad_norm=0.299679, loss=6.905554
I0405 21:44:41.275570 140541023364928 submission.py:139] 87) loss = 6.906, grad_norm = 0.300
I0405 21:44:41.664771 140507103622912 logging_writer.py:48] [88] global_step=88, grad_norm=0.298203, loss=6.903772
I0405 21:44:41.671779 140541023364928 submission.py:139] 88) loss = 6.904, grad_norm = 0.298
I0405 21:44:42.072599 140507095230208 logging_writer.py:48] [89] global_step=89, grad_norm=0.305315, loss=6.904044
I0405 21:44:42.076849 140541023364928 submission.py:139] 89) loss = 6.904, grad_norm = 0.305
I0405 21:44:42.469280 140507103622912 logging_writer.py:48] [90] global_step=90, grad_norm=0.307630, loss=6.904824
I0405 21:44:42.472875 140541023364928 submission.py:139] 90) loss = 6.905, grad_norm = 0.308
I0405 21:44:42.864602 140507095230208 logging_writer.py:48] [91] global_step=91, grad_norm=0.298051, loss=6.903099
I0405 21:44:42.868157 140541023364928 submission.py:139] 91) loss = 6.903, grad_norm = 0.298
I0405 21:44:43.260879 140507103622912 logging_writer.py:48] [92] global_step=92, grad_norm=0.301239, loss=6.903465
I0405 21:44:43.264520 140541023364928 submission.py:139] 92) loss = 6.903, grad_norm = 0.301
I0405 21:44:43.659873 140507095230208 logging_writer.py:48] [93] global_step=93, grad_norm=0.305842, loss=6.903863
I0405 21:44:43.664771 140541023364928 submission.py:139] 93) loss = 6.904, grad_norm = 0.306
I0405 21:44:44.057439 140507103622912 logging_writer.py:48] [94] global_step=94, grad_norm=0.295455, loss=6.904602
I0405 21:44:44.061618 140541023364928 submission.py:139] 94) loss = 6.905, grad_norm = 0.295
I0405 21:44:44.465783 140507095230208 logging_writer.py:48] [95] global_step=95, grad_norm=0.297077, loss=6.903821
I0405 21:44:44.469633 140541023364928 submission.py:139] 95) loss = 6.904, grad_norm = 0.297
I0405 21:44:44.861154 140507103622912 logging_writer.py:48] [96] global_step=96, grad_norm=0.299408, loss=6.904411
I0405 21:44:44.864758 140541023364928 submission.py:139] 96) loss = 6.904, grad_norm = 0.299
I0405 21:44:45.257440 140507095230208 logging_writer.py:48] [97] global_step=97, grad_norm=0.309372, loss=6.901754
I0405 21:44:45.261381 140541023364928 submission.py:139] 97) loss = 6.902, grad_norm = 0.309
I0405 21:44:45.654725 140507103622912 logging_writer.py:48] [98] global_step=98, grad_norm=0.296165, loss=6.904821
I0405 21:44:45.659782 140541023364928 submission.py:139] 98) loss = 6.905, grad_norm = 0.296
I0405 21:44:46.050775 140507095230208 logging_writer.py:48] [99] global_step=99, grad_norm=0.302057, loss=6.903691
I0405 21:44:46.054648 140541023364928 submission.py:139] 99) loss = 6.904, grad_norm = 0.302
I0405 21:44:46.447332 140507103622912 logging_writer.py:48] [100] global_step=100, grad_norm=0.300507, loss=6.903783
I0405 21:44:46.451301 140541023364928 submission.py:139] 100) loss = 6.904, grad_norm = 0.301
I0405 21:47:20.702219 140507095230208 logging_writer.py:48] [500] global_step=500, grad_norm=0.682204, loss=6.770645
I0405 21:47:20.706144 140541023364928 submission.py:139] 500) loss = 6.771, grad_norm = 0.682
I0405 21:50:33.701579 140507103622912 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.016423, loss=6.629673
I0405 21:50:33.706508 140541023364928 submission.py:139] 1000) loss = 6.630, grad_norm = 1.016
I0405 21:51:06.479400 140541023364928 submission_runner.py:373] Before eval at step 1086: RAM USED (GB) 98.753335296
I0405 21:51:06.479618 140541023364928 spec.py:298] Evaluating on the training split.
I0405 21:51:50.570195 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 21:52:35.613646 140541023364928 spec.py:326] Evaluating on the test split.
I0405 21:52:37.049733 140541023364928 submission_runner.py:382] Time since start: 536.63s, 	Step: 1086, 	{'train/accuracy': 0.04373046875, 'train/loss': 5.959097290039063, 'validation/accuracy': 0.04164, 'validation/loss': 5.99495375, 'validation/num_examples': 50000, 'test/accuracy': 0.0324, 'test/loss': 6.1060328125, 'test/num_examples': 10000}
I0405 21:52:37.050116 140541023364928 submission_runner.py:396] After eval at step 1086: RAM USED (GB) 98.68181504
I0405 21:52:37.058131 140496080983808 logging_writer.py:48] [1086] global_step=1086, preemption_count=0, score=420.475207, test/accuracy=0.032400, test/loss=6.106033, test/num_examples=10000, total_duration=536.628176, train/accuracy=0.043730, train/loss=5.959097, validation/accuracy=0.041640, validation/loss=5.994954, validation/num_examples=50000
I0405 21:52:37.361999 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_1086.
I0405 21:52:37.362755 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 1086: RAM USED (GB) 98.682556416
I0405 21:55:19.808423 140496089376512 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.880545, loss=6.409390
I0405 21:55:19.814291 140541023364928 submission.py:139] 1500) loss = 6.409, grad_norm = 0.881
I0405 21:58:32.662892 140496080983808 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.775301, loss=6.372124
I0405 21:58:32.669050 140541023364928 submission.py:139] 2000) loss = 6.372, grad_norm = 0.775
I0405 21:59:37.378471 140541023364928 submission_runner.py:373] Before eval at step 2169: RAM USED (GB) 100.074192896
I0405 21:59:37.378847 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:00:21.307916 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:01:06.293576 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:01:07.717530 140541023364928 submission_runner.py:382] Time since start: 1047.53s, 	Step: 2169, 	{'train/accuracy': 0.0821875, 'train/loss': 5.4270751953125, 'validation/accuracy': 0.0746, 'validation/loss': 5.474475625, 'validation/num_examples': 50000, 'test/accuracy': 0.0577, 'test/loss': 5.672264453125, 'test/num_examples': 10000}
I0405 22:01:07.718019 140541023364928 submission_runner.py:396] After eval at step 2169: RAM USED (GB) 100.181225472
I0405 22:01:07.726627 140496089376512 logging_writer.py:48] [2169] global_step=2169, preemption_count=0, score=834.037340, test/accuracy=0.057700, test/loss=5.672264, test/num_examples=10000, total_duration=1047.527389, train/accuracy=0.082187, train/loss=5.427075, validation/accuracy=0.074600, validation/loss=5.474476, validation/num_examples=50000
I0405 22:01:08.025967 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_2169.
I0405 22:01:08.026768 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 2169: RAM USED (GB) 100.180103168
I0405 22:03:18.061821 140496080983808 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.694873, loss=6.170923
I0405 22:03:18.065444 140541023364928 submission.py:139] 2500) loss = 6.171, grad_norm = 0.695
I0405 22:06:32.856577 140496089376512 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.787036, loss=6.260319
I0405 22:06:32.863473 140541023364928 submission.py:139] 3000) loss = 6.260, grad_norm = 0.787
I0405 22:08:08.172044 140541023364928 submission_runner.py:373] Before eval at step 3248: RAM USED (GB) 100.860706816
I0405 22:08:08.172305 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:08:52.476897 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:09:37.798498 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:09:39.222631 140541023364928 submission_runner.py:382] Time since start: 1558.32s, 	Step: 3248, 	{'train/accuracy': 0.1059765625, 'train/loss': 5.097493591308594, 'validation/accuracy': 0.09698, 'validation/loss': 5.166584375, 'validation/num_examples': 50000, 'test/accuracy': 0.0735, 'test/loss': 5.419114453125, 'test/num_examples': 10000}
I0405 22:09:39.223167 140541023364928 submission_runner.py:396] After eval at step 3248: RAM USED (GB) 100.945936384
I0405 22:09:39.231723 140496080983808 logging_writer.py:48] [3248] global_step=3248, preemption_count=0, score=1247.783931, test/accuracy=0.073500, test/loss=5.419114, test/num_examples=10000, total_duration=1558.320977, train/accuracy=0.105977, train/loss=5.097494, validation/accuracy=0.096980, validation/loss=5.166584, validation/num_examples=50000
I0405 22:09:39.520996 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_3248.
I0405 22:09:39.521690 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 3248: RAM USED (GB) 100.945330176
I0405 22:11:16.855853 140496089376512 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.697730, loss=6.233016
I0405 22:11:16.861893 140541023364928 submission.py:139] 3500) loss = 6.233, grad_norm = 0.698
I0405 22:14:34.050810 140496080983808 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.695191, loss=6.309512
I0405 22:14:34.055797 140541023364928 submission.py:139] 4000) loss = 6.310, grad_norm = 0.695
I0405 22:16:39.892376 140541023364928 submission_runner.py:373] Before eval at step 4327: RAM USED (GB) 100.940877824
I0405 22:16:39.892804 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:17:23.996387 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:18:09.196939 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:18:10.623103 140541023364928 submission_runner.py:382] Time since start: 2070.04s, 	Step: 4327, 	{'train/accuracy': 0.1415625, 'train/loss': 4.7765499877929685, 'validation/accuracy': 0.1294, 'validation/loss': 4.857315, 'validation/num_examples': 50000, 'test/accuracy': 0.1016, 'test/loss': 5.1509296875, 'test/num_examples': 10000}
I0405 22:18:10.623453 140541023364928 submission_runner.py:396] After eval at step 4327: RAM USED (GB) 100.707721216
I0405 22:18:10.631374 140496089376512 logging_writer.py:48] [4327] global_step=4327, preemption_count=0, score=1661.731009, test/accuracy=0.101600, test/loss=5.150930, test/num_examples=10000, total_duration=2070.041860, train/accuracy=0.141563, train/loss=4.776550, validation/accuracy=0.129400, validation/loss=4.857315, validation/num_examples=50000
I0405 22:18:10.907263 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_4327.
I0405 22:18:10.907945 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 4327: RAM USED (GB) 100.706598912
I0405 22:19:17.999616 140496080983808 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.662173, loss=5.961004
I0405 22:19:18.004379 140541023364928 submission.py:139] 4500) loss = 5.961, grad_norm = 0.662
I0405 22:22:34.453195 140496089376512 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.729497, loss=5.882468
I0405 22:22:34.457842 140541023364928 submission.py:139] 5000) loss = 5.882, grad_norm = 0.729
I0405 22:25:11.055467 140541023364928 submission_runner.py:373] Before eval at step 5401: RAM USED (GB) 100.837666816
I0405 22:25:11.055742 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:25:55.028317 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:26:40.025898 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:26:41.446169 140541023364928 submission_runner.py:382] Time since start: 2581.20s, 	Step: 5401, 	{'train/accuracy': 0.17109375, 'train/loss': 4.534228210449219, 'validation/accuracy': 0.15758, 'validation/loss': 4.6162853125, 'validation/num_examples': 50000, 'test/accuracy': 0.1194, 'test/loss': 4.920837109375, 'test/num_examples': 10000}
I0405 22:26:41.446510 140541023364928 submission_runner.py:396] After eval at step 5401: RAM USED (GB) 100.977717248
I0405 22:26:41.454300 140496080983808 logging_writer.py:48] [5401] global_step=5401, preemption_count=0, score=2075.517383, test/accuracy=0.119400, test/loss=4.920837, test/num_examples=10000, total_duration=2581.204248, train/accuracy=0.171094, train/loss=4.534228, validation/accuracy=0.157580, validation/loss=4.616285, validation/num_examples=50000
I0405 22:26:41.732869 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_5401.
I0405 22:26:41.733526 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 5401: RAM USED (GB) 100.976615424
I0405 22:27:20.179343 140496089376512 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.607713, loss=6.167943
I0405 22:27:20.190517 140541023364928 submission.py:139] 5500) loss = 6.168, grad_norm = 0.608
I0405 22:30:33.532718 140496080983808 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.578772, loss=5.752823
I0405 22:30:33.536829 140541023364928 submission.py:139] 6000) loss = 5.753, grad_norm = 0.579
I0405 22:33:41.921171 140541023364928 submission_runner.py:373] Before eval at step 6477: RAM USED (GB) 101.030203392
I0405 22:33:41.921462 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:34:26.257699 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:35:11.412746 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:35:12.842170 140541023364928 submission_runner.py:382] Time since start: 3092.07s, 	Step: 6477, 	{'train/accuracy': 0.206171875, 'train/loss': 4.270375671386719, 'validation/accuracy': 0.19014, 'validation/loss': 4.37033, 'validation/num_examples': 50000, 'test/accuracy': 0.146, 'test/loss': 4.7223078125, 'test/num_examples': 10000}
I0405 22:35:12.842575 140541023364928 submission_runner.py:396] After eval at step 6477: RAM USED (GB) 101.174304768
I0405 22:35:12.850240 140496089376512 logging_writer.py:48] [6477] global_step=6477, preemption_count=0, score=2489.301556, test/accuracy=0.146000, test/loss=4.722308, test/num_examples=10000, total_duration=3092.069685, train/accuracy=0.206172, train/loss=4.270376, validation/accuracy=0.190140, validation/loss=4.370330, validation/num_examples=50000
I0405 22:35:13.136865 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_6477.
I0405 22:35:13.137511 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 6477: RAM USED (GB) 101.18750208
I0405 22:35:22.351878 140496080983808 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.641715, loss=5.772832
I0405 22:35:22.356253 140541023364928 submission.py:139] 6500) loss = 5.773, grad_norm = 0.642
I0405 22:38:35.322394 140496089376512 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.588860, loss=5.800579
I0405 22:38:35.327128 140541023364928 submission.py:139] 7000) loss = 5.801, grad_norm = 0.589
I0405 22:41:51.136941 140496080983808 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.614954, loss=5.540103
I0405 22:41:51.141860 140541023364928 submission.py:139] 7500) loss = 5.540, grad_norm = 0.615
I0405 22:42:13.510325 140541023364928 submission_runner.py:373] Before eval at step 7554: RAM USED (GB) 100.837871616
I0405 22:42:13.510558 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:42:57.995669 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:43:43.136968 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:43:44.562928 140541023364928 submission_runner.py:382] Time since start: 3603.66s, 	Step: 7554, 	{'train/accuracy': 0.23986328125, 'train/loss': 3.9661520385742186, 'validation/accuracy': 0.21982, 'validation/loss': 4.0910953125, 'validation/num_examples': 50000, 'test/accuracy': 0.1679, 'test/loss': 4.51940546875, 'test/num_examples': 10000}
I0405 22:43:44.563282 140541023364928 submission_runner.py:396] After eval at step 7554: RAM USED (GB) 100.935553024
I0405 22:43:44.571380 140496089376512 logging_writer.py:48] [7554] global_step=7554, preemption_count=0, score=2903.245206, test/accuracy=0.167900, test/loss=4.519405, test/num_examples=10000, total_duration=3603.659155, train/accuracy=0.239863, train/loss=3.966152, validation/accuracy=0.219820, validation/loss=4.091095, validation/num_examples=50000
I0405 22:43:44.852636 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_7554.
I0405 22:43:44.853311 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 7554: RAM USED (GB) 100.934905856
I0405 22:46:37.375266 140496080983808 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.651749, loss=5.503126
I0405 22:46:37.379793 140541023364928 submission.py:139] 8000) loss = 5.503, grad_norm = 0.652
I0405 22:49:50.476410 140496089376512 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.591548, loss=5.377906
I0405 22:49:50.481607 140541023364928 submission.py:139] 8500) loss = 5.378, grad_norm = 0.592
I0405 22:50:45.154655 140541023364928 submission_runner.py:373] Before eval at step 8638: RAM USED (GB) 100.810502144
I0405 22:50:45.154884 140541023364928 spec.py:298] Evaluating on the training split.
I0405 22:51:29.381161 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 22:52:17.047399 140541023364928 spec.py:326] Evaluating on the test split.
I0405 22:52:18.469885 140541023364928 submission_runner.py:382] Time since start: 4115.30s, 	Step: 8638, 	{'train/accuracy': 0.27287109375, 'train/loss': 3.7733950805664063, 'validation/accuracy': 0.2501, 'validation/loss': 3.908780625, 'validation/num_examples': 50000, 'test/accuracy': 0.1931, 'test/loss': 4.328880078125, 'test/num_examples': 10000}
I0405 22:52:18.470240 140541023364928 submission_runner.py:396] After eval at step 8638: RAM USED (GB) 100.697423872
I0405 22:52:18.479497 140496080983808 logging_writer.py:48] [8638] global_step=8638, preemption_count=0, score=3317.117199, test/accuracy=0.193100, test/loss=4.328880, test/num_examples=10000, total_duration=4115.303839, train/accuracy=0.272871, train/loss=3.773395, validation/accuracy=0.250100, validation/loss=3.908781, validation/num_examples=50000
I0405 22:52:18.761585 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_8638.
I0405 22:52:18.762308 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 8638: RAM USED (GB) 100.696817664
I0405 22:54:41.940389 140496089376512 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.575618, loss=5.428152
I0405 22:54:41.945115 140541023364928 submission.py:139] 9000) loss = 5.428, grad_norm = 0.576
I0405 22:57:55.096990 140496080983808 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.601461, loss=5.436399
I0405 22:57:55.101720 140541023364928 submission.py:139] 9500) loss = 5.436, grad_norm = 0.601
I0405 22:59:19.056999 140541023364928 submission_runner.py:373] Before eval at step 9718: RAM USED (GB) 100.959997952
I0405 22:59:19.057221 140541023364928 spec.py:298] Evaluating on the training split.
I0405 23:00:02.901887 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 23:00:50.045844 140541023364928 spec.py:326] Evaluating on the test split.
I0405 23:00:51.471462 140541023364928 submission_runner.py:382] Time since start: 4629.21s, 	Step: 9718, 	{'train/accuracy': 0.3128515625, 'train/loss': 3.5257394409179685, 'validation/accuracy': 0.28368, 'validation/loss': 3.669245625, 'validation/num_examples': 50000, 'test/accuracy': 0.2183, 'test/loss': 4.12694453125, 'test/num_examples': 10000}
I0405 23:00:51.471802 140541023364928 submission_runner.py:396] After eval at step 9718: RAM USED (GB) 101.027586048
I0405 23:00:51.479535 140496089376512 logging_writer.py:48] [9718] global_step=9718, preemption_count=0, score=3730.982774, test/accuracy=0.218300, test/loss=4.126945, test/num_examples=10000, total_duration=4629.206112, train/accuracy=0.312852, train/loss=3.525739, validation/accuracy=0.283680, validation/loss=3.669246, validation/num_examples=50000
I0405 23:00:51.757012 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_9718.
I0405 23:00:51.757624 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 9718: RAM USED (GB) 101.02697984
I0405 23:02:43.397393 140496080983808 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.604964, loss=5.344340
I0405 23:02:43.403467 140541023364928 submission.py:139] 10000) loss = 5.344, grad_norm = 0.605
I0405 23:05:58.449295 140496089376512 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.475286, loss=5.829261
I0405 23:05:58.454504 140541023364928 submission.py:139] 10500) loss = 5.829, grad_norm = 0.475
I0405 23:07:51.983958 140541023364928 submission_runner.py:373] Before eval at step 10795: RAM USED (GB) 100.951420928
I0405 23:07:51.984282 140541023364928 spec.py:298] Evaluating on the training split.
I0405 23:08:35.767612 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 23:09:22.684498 140541023364928 spec.py:326] Evaluating on the test split.
I0405 23:09:24.108771 140541023364928 submission_runner.py:382] Time since start: 5142.13s, 	Step: 10795, 	{'train/accuracy': 0.342578125, 'train/loss': 3.3336651611328123, 'validation/accuracy': 0.31332, 'validation/loss': 3.4860953125, 'validation/num_examples': 50000, 'test/accuracy': 0.2337, 'test/loss': 3.97425546875, 'test/num_examples': 10000}
I0405 23:09:24.109127 140541023364928 submission_runner.py:396] After eval at step 10795: RAM USED (GB) 100.99027968
I0405 23:09:24.116734 140496080983808 logging_writer.py:48] [10795] global_step=10795, preemption_count=0, score=4144.756203, test/accuracy=0.233700, test/loss=3.974255, test/num_examples=10000, total_duration=5142.133663, train/accuracy=0.342578, train/loss=3.333665, validation/accuracy=0.313320, validation/loss=3.486095, validation/num_examples=50000
I0405 23:09:24.394733 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_10795.
I0405 23:09:24.395357 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 10795: RAM USED (GB) 100.989157376
I0405 23:10:43.744710 140496089376512 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.601289, loss=5.371255
I0405 23:10:43.750082 140541023364928 submission.py:139] 11000) loss = 5.371, grad_norm = 0.601
I0405 23:14:01.625787 140496080983808 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.493887, loss=5.531490
I0405 23:14:01.632086 140541023364928 submission.py:139] 11500) loss = 5.531, grad_norm = 0.494
I0405 23:16:24.552416 140541023364928 submission_runner.py:373] Before eval at step 11871: RAM USED (GB) 100.836552704
I0405 23:16:24.552653 140541023364928 spec.py:298] Evaluating on the training split.
I0405 23:17:08.106136 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 23:17:54.884469 140541023364928 spec.py:326] Evaluating on the test split.
I0405 23:17:56.312394 140541023364928 submission_runner.py:382] Time since start: 5654.70s, 	Step: 11871, 	{'train/accuracy': 0.37103515625, 'train/loss': 3.144139709472656, 'validation/accuracy': 0.3394, 'validation/loss': 3.3146990625, 'validation/num_examples': 50000, 'test/accuracy': 0.2639, 'test/loss': 3.81489140625, 'test/num_examples': 10000}
I0405 23:17:56.312759 140541023364928 submission_runner.py:396] After eval at step 11871: RAM USED (GB) 100.96416768
I0405 23:17:56.320773 140496089376512 logging_writer.py:48] [11871] global_step=11871, preemption_count=0, score=4558.488272, test/accuracy=0.263900, test/loss=3.814891, test/num_examples=10000, total_duration=5654.701637, train/accuracy=0.371035, train/loss=3.144140, validation/accuracy=0.339400, validation/loss=3.314699, validation/num_examples=50000
I0405 23:17:56.600083 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_11871.
I0405 23:17:56.600744 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 11871: RAM USED (GB) 100.963561472
I0405 23:18:46.609088 140496080983808 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.529637, loss=5.204279
I0405 23:18:46.612939 140541023364928 submission.py:139] 12000) loss = 5.204, grad_norm = 0.530
I0405 23:22:01.826227 140496089376512 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.621804, loss=5.143241
I0405 23:22:01.831911 140541023364928 submission.py:139] 12500) loss = 5.143, grad_norm = 0.622
I0405 23:24:56.657069 140541023364928 submission_runner.py:373] Before eval at step 12948: RAM USED (GB) 100.852133888
I0405 23:24:56.657300 140541023364928 spec.py:298] Evaluating on the training split.
I0405 23:25:40.549255 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 23:26:25.796058 140541023364928 spec.py:326] Evaluating on the test split.
I0405 23:26:27.221601 140541023364928 submission_runner.py:382] Time since start: 6166.81s, 	Step: 12948, 	{'train/accuracy': 0.39484375, 'train/loss': 2.988192443847656, 'validation/accuracy': 0.35946, 'validation/loss': 3.176961875, 'validation/num_examples': 50000, 'test/accuracy': 0.272, 'test/loss': 3.701158203125, 'test/num_examples': 10000}
I0405 23:26:27.221935 140541023364928 submission_runner.py:396] After eval at step 12948: RAM USED (GB) 100.786778112
I0405 23:26:27.230121 140496080983808 logging_writer.py:48] [12948] global_step=12948, preemption_count=0, score=4972.077061, test/accuracy=0.272000, test/loss=3.701158, test/num_examples=10000, total_duration=6166.805884, train/accuracy=0.394844, train/loss=2.988192, validation/accuracy=0.359460, validation/loss=3.176962, validation/num_examples=50000
I0405 23:26:27.518384 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_12948.
I0405 23:26:27.519014 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 12948: RAM USED (GB) 100.785938432
I0405 23:26:48.075131 140496089376512 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.553430, loss=4.972935
I0405 23:26:48.078816 140541023364928 submission.py:139] 13000) loss = 4.973, grad_norm = 0.553
I0405 23:30:00.977600 140496080983808 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.593913, loss=5.140263
I0405 23:30:00.982317 140541023364928 submission.py:139] 13500) loss = 5.140, grad_norm = 0.594
I0405 23:33:18.363567 140541023364928 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.941062144
I0405 23:33:18.363850 140541023364928 spec.py:298] Evaluating on the training split.
I0405 23:34:02.333917 140541023364928 spec.py:310] Evaluating on the validation split.
I0405 23:34:47.769543 140541023364928 spec.py:326] Evaluating on the test split.
I0405 23:34:49.195074 140541023364928 submission_runner.py:382] Time since start: 6668.51s, 	Step: 14000, 	{'train/accuracy': 0.41447265625, 'train/loss': 2.9036099243164064, 'validation/accuracy': 0.37398, 'validation/loss': 3.0850028125, 'validation/num_examples': 50000, 'test/accuracy': 0.2962, 'test/loss': 3.61155078125, 'test/num_examples': 10000}
I0405 23:34:49.195417 140541023364928 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.965732352
I0405 23:34:49.203320 140496089376512 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5376.616328, test/accuracy=0.296200, test/loss=3.611551, test/num_examples=10000, total_duration=6668.512158, train/accuracy=0.414473, train/loss=2.903610, validation/accuracy=0.373980, validation/loss=3.085003, validation/num_examples=50000
I0405 23:34:49.481571 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:34:49.482172 140541023364928 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.965126144
I0405 23:34:49.490564 140496080983808 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5376.616328
I0405 23:34:50.284019 140541023364928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v2/timing_momentum/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0405 23:34:50.603575 140541023364928 submission_runner.py:550] Tuning trial 1/1
I0405 23:34:50.603785 140541023364928 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0405 23:34:50.604308 140541023364928 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00109375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.509799003601074, 'total_duration': 6.511882066726685, 'global_step': 1, 'preemption_count': 0}), (1086, {'train/accuracy': 0.04373046875, 'train/loss': 5.959097290039063, 'validation/accuracy': 0.04164, 'validation/loss': 5.99495375, 'validation/num_examples': 50000, 'test/accuracy': 0.0324, 'test/loss': 6.1060328125, 'test/num_examples': 10000, 'score': 420.4752070903778, 'total_duration': 536.6281764507294, 'global_step': 1086, 'preemption_count': 0}), (2169, {'train/accuracy': 0.0821875, 'train/loss': 5.4270751953125, 'validation/accuracy': 0.0746, 'validation/loss': 5.474475625, 'validation/num_examples': 50000, 'test/accuracy': 0.0577, 'test/loss': 5.672264453125, 'test/num_examples': 10000, 'score': 834.037339925766, 'total_duration': 1047.5273888111115, 'global_step': 2169, 'preemption_count': 0}), (3248, {'train/accuracy': 0.1059765625, 'train/loss': 5.097493591308594, 'validation/accuracy': 0.09698, 'validation/loss': 5.166584375, 'validation/num_examples': 50000, 'test/accuracy': 0.0735, 'test/loss': 5.419114453125, 'test/num_examples': 10000, 'score': 1247.7839312553406, 'total_duration': 1558.32097697258, 'global_step': 3248, 'preemption_count': 0}), (4327, {'train/accuracy': 0.1415625, 'train/loss': 4.7765499877929685, 'validation/accuracy': 0.1294, 'validation/loss': 4.857315, 'validation/num_examples': 50000, 'test/accuracy': 0.1016, 'test/loss': 5.1509296875, 'test/num_examples': 10000, 'score': 1661.7310092449188, 'total_duration': 2070.04185962677, 'global_step': 4327, 'preemption_count': 0}), (5401, {'train/accuracy': 0.17109375, 'train/loss': 4.534228210449219, 'validation/accuracy': 0.15758, 'validation/loss': 4.6162853125, 'validation/num_examples': 50000, 'test/accuracy': 0.1194, 'test/loss': 4.920837109375, 'test/num_examples': 10000, 'score': 2075.5173830986023, 'total_duration': 2581.2042479515076, 'global_step': 5401, 'preemption_count': 0}), (6477, {'train/accuracy': 0.206171875, 'train/loss': 4.270375671386719, 'validation/accuracy': 0.19014, 'validation/loss': 4.37033, 'validation/num_examples': 50000, 'test/accuracy': 0.146, 'test/loss': 4.7223078125, 'test/num_examples': 10000, 'score': 2489.3015563488007, 'total_duration': 3092.069685459137, 'global_step': 6477, 'preemption_count': 0}), (7554, {'train/accuracy': 0.23986328125, 'train/loss': 3.9661520385742186, 'validation/accuracy': 0.21982, 'validation/loss': 4.0910953125, 'validation/num_examples': 50000, 'test/accuracy': 0.1679, 'test/loss': 4.51940546875, 'test/num_examples': 10000, 'score': 2903.245205640793, 'total_duration': 3603.659154653549, 'global_step': 7554, 'preemption_count': 0}), (8638, {'train/accuracy': 0.27287109375, 'train/loss': 3.7733950805664063, 'validation/accuracy': 0.2501, 'validation/loss': 3.908780625, 'validation/num_examples': 50000, 'test/accuracy': 0.1931, 'test/loss': 4.328880078125, 'test/num_examples': 10000, 'score': 3317.117198944092, 'total_duration': 4115.303839206696, 'global_step': 8638, 'preemption_count': 0}), (9718, {'train/accuracy': 0.3128515625, 'train/loss': 3.5257394409179685, 'validation/accuracy': 0.28368, 'validation/loss': 3.669245625, 'validation/num_examples': 50000, 'test/accuracy': 0.2183, 'test/loss': 4.12694453125, 'test/num_examples': 10000, 'score': 3730.982773542404, 'total_duration': 4629.206111907959, 'global_step': 9718, 'preemption_count': 0}), (10795, {'train/accuracy': 0.342578125, 'train/loss': 3.3336651611328123, 'validation/accuracy': 0.31332, 'validation/loss': 3.4860953125, 'validation/num_examples': 50000, 'test/accuracy': 0.2337, 'test/loss': 3.97425546875, 'test/num_examples': 10000, 'score': 4144.75620341301, 'total_duration': 5142.133663415909, 'global_step': 10795, 'preemption_count': 0}), (11871, {'train/accuracy': 0.37103515625, 'train/loss': 3.144139709472656, 'validation/accuracy': 0.3394, 'validation/loss': 3.3146990625, 'validation/num_examples': 50000, 'test/accuracy': 0.2639, 'test/loss': 3.81489140625, 'test/num_examples': 10000, 'score': 4558.488271713257, 'total_duration': 5654.701637268066, 'global_step': 11871, 'preemption_count': 0}), (12948, {'train/accuracy': 0.39484375, 'train/loss': 2.988192443847656, 'validation/accuracy': 0.35946, 'validation/loss': 3.176961875, 'validation/num_examples': 50000, 'test/accuracy': 0.272, 'test/loss': 3.701158203125, 'test/num_examples': 10000, 'score': 4972.077060699463, 'total_duration': 6166.80588388443, 'global_step': 12948, 'preemption_count': 0}), (14000, {'train/accuracy': 0.41447265625, 'train/loss': 2.9036099243164064, 'validation/accuracy': 0.37398, 'validation/loss': 3.0850028125, 'validation/num_examples': 50000, 'test/accuracy': 0.2962, 'test/loss': 3.61155078125, 'test/num_examples': 10000, 'score': 5376.616327524185, 'total_duration': 6668.51215839386, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0405 23:34:50.604398 140541023364928 submission_runner.py:553] Timing: 5376.616327524185
I0405 23:34:50.604444 140541023364928 submission_runner.py:554] ====================
I0405 23:34:50.604535 140541023364928 submission_runner.py:613] Final imagenet_vit score: 5376.616327524185
