I0411 01:53:41.289449 140696552322880 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax.
I0411 01:53:41.344629 140696552322880 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0411 01:53:42.300790 140696552322880 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0411 01:53:42.301513 140696552322880 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0411 01:53:42.307763 140696552322880 submission_runner.py:511] Using RNG seed 3831601500
I0411 01:53:44.636840 140696552322880 submission_runner.py:520] --- Tuning run 1/1 ---
I0411 01:53:44.637051 140696552322880 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1.
I0411 01:53:44.637233 140696552322880 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/hparams.json.
I0411 01:53:44.764951 140696552322880 submission_runner.py:230] Starting train once: RAM USED (GB) 4.348522496
I0411 01:53:44.765112 140696552322880 submission_runner.py:231] Initializing dataset.
I0411 01:53:44.765265 140696552322880 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.348522496
I0411 01:53:44.765324 140696552322880 submission_runner.py:240] Initializing model.
I0411 01:53:50.394223 140696552322880 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.779016704
I0411 01:53:50.394416 140696552322880 submission_runner.py:252] Initializing optimizer.
I0411 01:53:51.084507 140696552322880 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.780614144
I0411 01:53:51.084671 140696552322880 submission_runner.py:261] Initializing metrics bundle.
I0411 01:53:51.084720 140696552322880 submission_runner.py:276] Initializing checkpoint and logger.
I0411 01:53:51.085732 140696552322880 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0411 01:53:51.086000 140696552322880 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0411 01:53:51.086067 140696552322880 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0411 01:53:51.811169 140696552322880 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0411 01:53:51.812042 140696552322880 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/flags_0.json.
I0411 01:53:51.818176 140696552322880 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.778889728
I0411 01:53:51.818372 140696552322880 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.778889728
I0411 01:53:51.818469 140696552322880 submission_runner.py:313] Starting training loop.
I0411 01:53:52.015773 140696552322880 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:53:52.046719 140696552322880 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:53:52.370229 140696552322880 input_pipeline.py:20] Loading split = train-other-500
I0411 01:53:55.730762 140696552322880 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.2955264
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0411 01:54:51.873093 140522354104064 logging_writer.py:48] [0] global_step=0, grad_norm=53.69330978393555, loss=32.00325012207031
I0411 01:54:51.891710 140696552322880 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.294399488
I0411 01:54:51.892066 140696552322880 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.294399488
I0411 01:54:51.892169 140696552322880 spec.py:298] Evaluating on the training split.
I0411 01:54:51.993726 140696552322880 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:54:52.019426 140696552322880 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:54:52.316584 140696552322880 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0411 01:55:48.813070 140696552322880 spec.py:310] Evaluating on the validation split.
I0411 01:55:48.888344 140696552322880 input_pipeline.py:20] Loading split = dev-clean
I0411 01:55:48.893742 140696552322880 input_pipeline.py:20] Loading split = dev-other
I0411 01:56:28.119241 140696552322880 spec.py:326] Evaluating on the test split.
I0411 01:56:28.192292 140696552322880 input_pipeline.py:20] Loading split = test-clean
I0411 01:56:55.765355 140696552322880 submission_runner.py:382] Time since start: 60.07s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.437061, dtype=float32), 'train/wer': 1.5108073214917852, 'validation/ctc_loss': DeviceArray(30.366978, dtype=float32), 'validation/wer': 1.081303244604386, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.472488, dtype=float32), 'test/wer': 1.134523591899742, 'test/num_examples': 2472}
I0411 01:56:55.766588 140696552322880 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.637401088
I0411 01:56:55.778402 140518931552000 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=59.876225, test/ctc_loss=30.472488403320312, test/num_examples=2472, test/wer=1.134524, total_duration=60.073627, train/ctc_loss=31.437061309814453, train/wer=1.510807, validation/ctc_loss=30.36697769165039, validation/num_examples=5348, validation/wer=1.081303
I0411 01:56:56.001321 140696552322880 checkpoints.py:356] Saving checkpoint at step: 1
I0411 01:56:56.925909 140696552322880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_1
I0411 01:56:56.947652 140696552322880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_1.
I0411 01:56:56.965909 140696552322880 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.598276096
I0411 01:56:57.024385 140696552322880 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.59558912
I0411 01:57:10.825582 140696552322880 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.9700864
I0411 01:58:25.789638 140523941742336 logging_writer.py:48] [100] global_step=100, grad_norm=4.881948471069336, loss=8.38684368133545
I0411 01:59:41.716135 140523950135040 logging_writer.py:48] [200] global_step=200, grad_norm=0.9115291833877563, loss=11.144303321838379
I0411 02:00:57.653046 140523941742336 logging_writer.py:48] [300] global_step=300, grad_norm=1.1160500049591064, loss=10.836541175842285
I0411 02:02:12.465816 140523950135040 logging_writer.py:48] [400] global_step=400, grad_norm=nan, loss=nan
I0411 02:03:25.228409 140523941742336 logging_writer.py:48] [500] global_step=500, grad_norm=nan, loss=nan
I0411 02:04:38.152686 140523950135040 logging_writer.py:48] [600] global_step=600, grad_norm=nan, loss=nan
I0411 02:05:51.379956 140523941742336 logging_writer.py:48] [700] global_step=700, grad_norm=nan, loss=nan
I0411 02:07:16.835886 140523950135040 logging_writer.py:48] [800] global_step=800, grad_norm=nan, loss=nan
I0411 02:08:42.694227 140523941742336 logging_writer.py:48] [900] global_step=900, grad_norm=nan, loss=nan
I0411 02:10:07.937509 140523950135040 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0411 02:11:26.868885 140521766909696 logging_writer.py:48] [1100] global_step=1100, grad_norm=nan, loss=nan
I0411 02:12:39.588484 140521758516992 logging_writer.py:48] [1200] global_step=1200, grad_norm=nan, loss=nan
I0411 02:13:52.260577 140521766909696 logging_writer.py:48] [1300] global_step=1300, grad_norm=nan, loss=nan
I0411 02:15:04.976457 140521758516992 logging_writer.py:48] [1400] global_step=1400, grad_norm=nan, loss=nan
I0411 02:16:17.723548 140521766909696 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0411 02:17:36.733896 140521758516992 logging_writer.py:48] [1600] global_step=1600, grad_norm=nan, loss=nan
I0411 02:18:59.813173 140521766909696 logging_writer.py:48] [1700] global_step=1700, grad_norm=nan, loss=nan
I0411 02:20:24.587330 140521758516992 logging_writer.py:48] [1800] global_step=1800, grad_norm=nan, loss=nan
I0411 02:21:48.831913 140521766909696 logging_writer.py:48] [1900] global_step=1900, grad_norm=nan, loss=nan
I0411 02:23:12.316002 140521758516992 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0411 02:24:33.011746 140524084418304 logging_writer.py:48] [2100] global_step=2100, grad_norm=nan, loss=nan
I0411 02:25:45.793119 140524076025600 logging_writer.py:48] [2200] global_step=2200, grad_norm=nan, loss=nan
I0411 02:26:58.586103 140524084418304 logging_writer.py:48] [2300] global_step=2300, grad_norm=nan, loss=nan
I0411 02:28:11.422586 140524076025600 logging_writer.py:48] [2400] global_step=2400, grad_norm=nan, loss=nan
I0411 02:29:26.501398 140524084418304 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0411 02:30:45.956671 140524076025600 logging_writer.py:48] [2600] global_step=2600, grad_norm=nan, loss=nan
I0411 02:32:09.673580 140524084418304 logging_writer.py:48] [2700] global_step=2700, grad_norm=nan, loss=nan
I0411 02:33:32.028665 140524076025600 logging_writer.py:48] [2800] global_step=2800, grad_norm=nan, loss=nan
I0411 02:34:52.737580 140524084418304 logging_writer.py:48] [2900] global_step=2900, grad_norm=nan, loss=nan
I0411 02:36:16.020256 140524076025600 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0411 02:36:57.177009 140696552322880 submission_runner.py:373] Before eval at step 3053: RAM USED (GB) 21.756944384
I0411 02:36:57.177240 140696552322880 spec.py:298] Evaluating on the training split.
I0411 02:37:23.541851 140696552322880 spec.py:310] Evaluating on the validation split.
I0411 02:37:59.114275 140696552322880 spec.py:326] Evaluating on the test split.
I0411 02:38:17.080101 140696552322880 submission_runner.py:382] Time since start: 2585.36s, 	Step: 3053, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 02:38:17.081418 140696552322880 submission_runner.py:396] After eval at step 3053: RAM USED (GB) 20.448354304
I0411 02:38:17.097378 140524084418304 logging_writer.py:48] [3053] global_step=3053, preemption_count=0, score=2454.207983, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2585.356818, train/ctc_loss=nan, train/wer=0.944636, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 02:38:17.301534 140696552322880 checkpoints.py:356] Saving checkpoint at step: 3053
I0411 02:38:18.242145 140696552322880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_3053
I0411 02:38:18.263382 140696552322880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_3053.
I0411 02:38:18.275720 140696552322880 submission_runner.py:416] After logging and checkpointing eval at step 3053: RAM USED (GB) 20.465836032
I0411 02:38:57.096439 140523756738304 logging_writer.py:48] [3100] global_step=3100, grad_norm=nan, loss=nan
I0411 02:40:09.638252 140523748345600 logging_writer.py:48] [3200] global_step=3200, grad_norm=nan, loss=nan
I0411 02:41:22.133255 140523756738304 logging_writer.py:48] [3300] global_step=3300, grad_norm=nan, loss=nan
I0411 02:42:34.647559 140523748345600 logging_writer.py:48] [3400] global_step=3400, grad_norm=nan, loss=nan
I0411 02:43:50.100091 140523756738304 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0411 02:45:10.153727 140523748345600 logging_writer.py:48] [3600] global_step=3600, grad_norm=nan, loss=nan
I0411 02:46:31.123656 140523756738304 logging_writer.py:48] [3700] global_step=3700, grad_norm=nan, loss=nan
I0411 02:47:50.779470 140523748345600 logging_writer.py:48] [3800] global_step=3800, grad_norm=nan, loss=nan
I0411 02:49:13.604002 140523756738304 logging_writer.py:48] [3900] global_step=3900, grad_norm=nan, loss=nan
I0411 02:50:35.069833 140523748345600 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0411 02:52:00.124780 140523756738304 logging_writer.py:48] [4100] global_step=4100, grad_norm=nan, loss=nan
I0411 02:53:18.230741 140523756738304 logging_writer.py:48] [4200] global_step=4200, grad_norm=nan, loss=nan
I0411 02:54:31.066375 140523748345600 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0411 02:55:43.737088 140523756738304 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0411 02:56:56.462740 140523748345600 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0411 02:58:14.099876 140523756738304 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0411 02:59:35.718688 140523748345600 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0411 03:00:56.889294 140523756738304 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0411 03:02:18.641012 140523748345600 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0411 03:03:39.891752 140523756738304 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0411 03:05:02.317587 140523748345600 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0411 03:06:23.405967 140524084418304 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0411 03:07:36.231874 140524076025600 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0411 03:08:48.975229 140524084418304 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0411 03:10:01.803646 140524076025600 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0411 03:11:14.481447 140524084418304 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0411 03:12:34.873166 140524076025600 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0411 03:13:56.652757 140524084418304 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0411 03:15:18.844882 140524076025600 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0411 03:16:42.932017 140524084418304 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0411 03:18:06.155860 140524076025600 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0411 03:18:18.297932 140696552322880 submission_runner.py:373] Before eval at step 6116: RAM USED (GB) 22.262792192
I0411 03:18:18.298137 140696552322880 spec.py:298] Evaluating on the training split.
I0411 03:18:44.605387 140696552322880 spec.py:310] Evaluating on the validation split.
I0411 03:19:19.928282 140696552322880 spec.py:326] Evaluating on the test split.
I0411 03:19:38.491214 140696552322880 submission_runner.py:382] Time since start: 5066.48s, 	Step: 6116, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 03:19:38.492567 140696552322880 submission_runner.py:396] After eval at step 6116: RAM USED (GB) 20.982915072
I0411 03:19:38.511795 140523869378304 logging_writer.py:48] [6116] global_step=6116, preemption_count=0, score=4848.278472, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5066.475954, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 03:19:38.707478 140696552322880 checkpoints.py:356] Saving checkpoint at step: 6116
I0411 03:19:39.670697 140696552322880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_6116
I0411 03:19:39.692108 140696552322880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_6116.
I0411 03:19:39.703066 140696552322880 submission_runner.py:416] After logging and checkpointing eval at step 6116: RAM USED (GB) 20.99693568
I0411 03:20:45.584272 140523869378304 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0411 03:21:58.222799 140523860985600 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0411 03:23:10.875620 140523869378304 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0411 03:24:23.592287 140523860985600 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0411 03:25:36.322346 140523869378304 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0411 03:26:54.069124 140523860985600 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0411 03:28:16.332684 140523869378304 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0411 03:29:38.259430 140523860985600 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0411 03:31:02.097033 140523869378304 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0411 03:32:24.899967 140523860985600 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0411 03:33:46.970457 140523869378304 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0411 03:35:03.707818 140523869378304 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0411 03:36:16.423572 140523860985600 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0411 03:37:29.223312 140523869378304 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0411 03:38:42.045793 140523860985600 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0411 03:39:54.776227 140523869378304 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0411 03:41:09.260597 140523860985600 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0411 03:42:28.904911 140523869378304 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0411 03:43:50.860801 140523860985600 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0411 03:45:11.498838 140523869378304 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0411 03:46:32.892541 140523860985600 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0411 03:47:53.014797 140523869378304 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0411 03:49:05.682150 140523860985600 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0411 03:50:18.380848 140523869378304 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0411 03:51:31.054996 140523860985600 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0411 03:52:44.272630 140523869378304 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0411 03:54:06.781547 140523860985600 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0411 03:55:32.025886 140523869378304 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0411 03:56:54.764016 140523860985600 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0411 03:58:18.395733 140523869378304 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0411 03:59:38.688700 140523860985600 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0411 03:59:40.089737 140696552322880 submission_runner.py:373] Before eval at step 9203: RAM USED (GB) 22.627786752
I0411 03:59:40.089998 140696552322880 spec.py:298] Evaluating on the training split.
I0411 04:00:06.853147 140696552322880 spec.py:310] Evaluating on the validation split.
I0411 04:00:42.717819 140696552322880 spec.py:326] Evaluating on the test split.
I0411 04:01:01.030028 140696552322880 submission_runner.py:382] Time since start: 7548.27s, 	Step: 9203, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:01:01.031407 140696552322880 submission_runner.py:396] After eval at step 9203: RAM USED (GB) 21.076000768
I0411 04:01:01.050755 140523577538304 logging_writer.py:48] [9203] global_step=9203, preemption_count=0, score=7242.724197, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7548.266648, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 04:01:01.275776 140696552322880 checkpoints.py:356] Saving checkpoint at step: 9203
I0411 04:01:02.254782 140696552322880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_9203
I0411 04:01:02.276815 140696552322880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_9203.
I0411 04:01:02.292730 140696552322880 submission_runner.py:416] After logging and checkpointing eval at step 9203: RAM USED (GB) 21.104840704
I0411 04:02:17.084443 140523577538304 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0411 04:03:29.683796 140523569145600 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0411 04:04:42.265052 140523577538304 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0411 04:05:54.887023 140523569145600 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0411 04:07:09.787023 140523577538304 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0411 04:08:30.063198 140523569145600 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0411 04:09:48.687524 140523577538304 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0411 04:11:05.051975 140696552322880 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.2240768
I0411 04:11:05.052196 140696552322880 spec.py:298] Evaluating on the training split.
I0411 04:11:31.604411 140696552322880 spec.py:310] Evaluating on the validation split.
I0411 04:12:06.281662 140696552322880 spec.py:326] Evaluating on the test split.
I0411 04:12:24.971300 140696552322880 submission_runner.py:382] Time since start: 8233.23s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:12:24.972630 140696552322880 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.036101632
I0411 04:12:25.014040 140523577538304 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7843.967965, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=8233.232389, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 04:12:25.266549 140696552322880 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:12:26.190333 140696552322880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:12:26.212053 140696552322880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:12:26.226900 140696552322880 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.050626048
I0411 04:12:26.234214 140523569145600 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7843.967965
I0411 04:12:26.414704 140696552322880 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:12:27.612173 140696552322880 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:12:27.633855 140696552322880 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:12:28.931457 140696552322880 submission_runner.py:550] Tuning trial 1/1
I0411 04:12:28.931698 140696552322880 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 04:12:28.936019 140696552322880 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.437061, dtype=float32), 'train/wer': 1.5108073214917852, 'validation/ctc_loss': DeviceArray(30.366978, dtype=float32), 'validation/wer': 1.081303244604386, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.472488, dtype=float32), 'test/wer': 1.134523591899742, 'test/num_examples': 2472, 'score': 59.876224756240845, 'total_duration': 60.07362747192383, 'global_step': 1, 'preemption_count': 0}), (3053, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2454.207982778549, 'total_duration': 2585.356817960739, 'global_step': 3053, 'preemption_count': 0}), (6116, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4848.2784724235535, 'total_duration': 5066.475953578949, 'global_step': 6116, 'preemption_count': 0}), (9203, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.724197387695, 'total_duration': 7548.266647577286, 'global_step': 9203, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7843.967965364456, 'total_duration': 8233.232388973236, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 04:12:28.936165 140696552322880 submission_runner.py:553] Timing: 7843.967965364456
I0411 04:12:28.936231 140696552322880 submission_runner.py:554] ====================
I0411 04:12:28.936635 140696552322880 submission_runner.py:613] Final librispeech_conformer score: 7843.967965364456
