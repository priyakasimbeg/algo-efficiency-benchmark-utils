I0410 23:23:54.568023 140428978612032 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax.
I0410 23:23:54.642787 140428978612032 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0410 23:23:55.502722 140428978612032 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0410 23:23:55.503416 140428978612032 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0410 23:23:55.507272 140428978612032 submission_runner.py:511] Using RNG seed 3968195110
I0410 23:23:58.040488 140428978612032 submission_runner.py:520] --- Tuning run 1/1 ---
I0410 23:23:58.040706 140428978612032 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1.
I0410 23:23:58.040888 140428978612032 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/hparams.json.
I0410 23:23:58.163021 140428978612032 submission_runner.py:230] Starting train once: RAM USED (GB) 4.31958016
I0410 23:23:58.163192 140428978612032 submission_runner.py:231] Initializing dataset.
I0410 23:23:58.163357 140428978612032 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.31958016
I0410 23:23:58.163417 140428978612032 submission_runner.py:240] Initializing model.
I0410 23:24:03.727136 140428978612032 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.772372992
I0410 23:24:03.727349 140428978612032 submission_runner.py:252] Initializing optimizer.
I0410 23:24:04.421222 140428978612032 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.7723648
I0410 23:24:04.421403 140428978612032 submission_runner.py:261] Initializing metrics bundle.
I0410 23:24:04.421452 140428978612032 submission_runner.py:276] Initializing checkpoint and logger.
I0410 23:24:04.422495 140428978612032 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0410 23:24:04.422748 140428978612032 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0410 23:24:04.422809 140428978612032 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0410 23:24:05.044053 140428978612032 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0410 23:24:05.044901 140428978612032 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/flags_0.json.
I0410 23:24:05.052256 140428978612032 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.773974528
I0410 23:24:05.052490 140428978612032 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.773974528
I0410 23:24:05.052554 140428978612032 submission_runner.py:313] Starting training loop.
I0410 23:24:05.242280 140428978612032 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:24:05.272056 140428978612032 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:24:05.581143 140428978612032 input_pipeline.py:20] Loading split = train-other-500
I0410 23:24:09.854228 140428978612032 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.668460544
2023-04-10 23:25:03.707649: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-10 23:25:04.055874: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0410 23:25:05.820460 140254463911680 logging_writer.py:48] [0] global_step=0, grad_norm=40.62179183959961, loss=31.898038864135742
I0410 23:25:05.847856 140428978612032 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.360607232
I0410 23:25:05.848309 140428978612032 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.360607232
I0410 23:25:05.848448 140428978612032 spec.py:298] Evaluating on the training split.
I0410 23:25:05.953657 140428978612032 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:25:05.981848 140428978612032 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:25:06.319554 140428978612032 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0410 23:25:48.678579 140428978612032 spec.py:310] Evaluating on the validation split.
I0410 23:25:48.740301 140428978612032 input_pipeline.py:20] Loading split = dev-clean
I0410 23:25:48.745100 140428978612032 input_pipeline.py:20] Loading split = dev-other
I0410 23:26:28.651119 140428978612032 spec.py:326] Evaluating on the test split.
I0410 23:26:28.709900 140428978612032 input_pipeline.py:20] Loading split = test-clean
I0410 23:26:56.429306 140428978612032 submission_runner.py:382] Time since start: 60.80s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.515316, dtype=float32), 'train/wer': 1.0080065801803302, 'validation/ctc_loss': DeviceArray(30.844193, dtype=float32), 'validation/wer': 1.14595413366265, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.919975, dtype=float32), 'test/wer': 1.1349095119127415, 'test/num_examples': 2472}
I0410 23:26:56.430304 140428978612032 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.425737728
I0410 23:26:56.448452 140251116861184 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=60.605886, test/ctc_loss=30.91997528076172, test/num_examples=2472, test/wer=1.134910, total_duration=60.795712, train/ctc_loss=31.515316009521484, train/wer=1.008007, validation/ctc_loss=30.844192504882812, validation/num_examples=5348, validation/wer=1.145954
I0410 23:26:56.617305 140428978612032 checkpoints.py:356] Saving checkpoint at step: 1
I0410 23:26:57.166903 140428978612032 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_1
I0410 23:26:57.167710 140428978612032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_1.
I0410 23:26:57.173931 140428978612032 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.43326208
I0410 23:26:57.208041 140428978612032 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.432254464
I0410 23:27:10.478945 140428978612032 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.762625536
I0410 23:28:25.853959 140255647840000 logging_writer.py:48] [100] global_step=100, grad_norm=19.732892990112305, loss=6.910181999206543
I0410 23:29:41.902234 140255656232704 logging_writer.py:48] [200] global_step=200, grad_norm=0.4172438681125641, loss=8.549888610839844
I0410 23:30:57.462464 140255647840000 logging_writer.py:48] [300] global_step=300, grad_norm=11.145583152770996, loss=9.727065086364746
I0410 23:32:12.414387 140255656232704 logging_writer.py:48] [400] global_step=400, grad_norm=0.0, loss=1896.985595703125
I0410 23:33:25.905386 140255647840000 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1876.4071044921875
I0410 23:34:39.521211 140255656232704 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1820.75537109375
I0410 23:35:52.942505 140255647840000 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1751.613037109375
I0410 23:37:11.056467 140255656232704 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1822.699951171875
I0410 23:38:31.840160 140255647840000 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1841.97900390625
I0410 23:39:52.946449 140255656232704 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1850.9017333984375
I0410 23:41:10.883745 140256454268672 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1872.01611328125
I0410 23:42:24.189196 140256445875968 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1862.211669921875
I0410 23:43:37.519738 140256454268672 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1836.5609130859375
I0410 23:44:50.830842 140256445875968 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1876.1319580078125
I0410 23:46:04.343319 140256454268672 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1782.3450927734375
I0410 23:47:17.742736 140256445875968 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1816.3631591796875
I0410 23:48:35.411112 140256454268672 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1746.3553466796875
I0410 23:49:58.981431 140256445875968 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1835.7706298828125
I0410 23:51:18.861575 140256454268672 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1826.471435546875
I0410 23:52:34.591879 140256445875968 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1942.1761474609375
I0410 23:53:54.338576 140254321301248 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1832.2232666015625
I0410 23:55:07.748843 140253935433472 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1786.9482421875
I0410 23:56:21.025752 140254321301248 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1912.5755615234375
I0410 23:57:34.376413 140253935433472 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1818.29833984375
I0410 23:58:47.697044 140254321301248 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1735.110595703125
I0411 00:00:06.148024 140253935433472 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1809.686279296875
I0411 00:01:27.382313 140254321301248 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1795.0931396484375
I0411 00:02:47.452107 140253935433472 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1853.1795654296875
I0411 00:04:06.531595 140254321301248 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1824.6488037109375
I0411 00:05:29.541373 140253935433472 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1861.80517578125
I0411 00:06:52.360013 140256454268672 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1869.4189453125
I0411 00:06:57.237416 140428978612032 submission_runner.py:373] Before eval at step 3108: RAM USED (GB) 24.526368768
I0411 00:06:57.237621 140428978612032 spec.py:298] Evaluating on the training split.
I0411 00:07:23.078361 140428978612032 spec.py:310] Evaluating on the validation split.
I0411 00:07:56.417542 140428978612032 spec.py:326] Evaluating on the test split.
I0411 00:08:13.676921 140428978612032 submission_runner.py:382] Time since start: 2572.18s, 	Step: 3108, 	{'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:08:13.678310 140428978612032 submission_runner.py:396] After eval at step 3108: RAM USED (GB) 21.95247104
I0411 00:08:13.698432 140256239228672 logging_writer.py:48] [3108] global_step=3108, preemption_count=0, score=2455.030807, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=2572.182220, train/ctc_loss=1767.6815185546875, train/wer=0.944636, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 00:08:13.888914 140428978612032 checkpoints.py:356] Saving checkpoint at step: 3108
I0411 00:08:14.822254 140428978612032 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_3108
I0411 00:08:14.842415 140428978612032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_3108.
I0411 00:08:14.851721 140428978612032 submission_runner.py:416] After logging and checkpointing eval at step 3108: RAM USED (GB) 21.923176448
I0411 00:09:23.089573 140256230835968 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1903.757080078125
I0411 00:10:36.615277 140256188872448 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1803.439453125
I0411 00:11:49.752923 140256230835968 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1777.271728515625
I0411 00:13:03.092795 140256188872448 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1803.058349609375
I0411 00:14:16.438767 140256230835968 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1897.6888427734375
I0411 00:15:30.002698 140256188872448 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1795.0931396484375
I0411 00:16:49.968721 140256230835968 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1829.9967041015625
I0411 00:18:12.883493 140256188872448 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1802.1697998046875
I0411 00:19:31.535317 140256230835968 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1909.43701171875
I0411 00:20:55.668750 140256188872448 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1810.83837890625
I0411 00:22:13.476814 140255583868672 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1816.749755859375
I0411 00:23:26.767430 140255575475968 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1730.419189453125
I0411 00:24:40.081228 140255583868672 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1816.4920654296875
I0411 00:25:53.407771 140255575475968 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1840.9193115234375
I0411 00:27:06.725595 140255583868672 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1776.038818359375
I0411 00:28:25.913122 140255575475968 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1859.7763671875
I0411 00:29:45.314195 140255583868672 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1763.802001953125
I0411 00:31:04.294211 140255575475968 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1815.9765625
I0411 00:32:24.308967 140255583868672 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1789.1964111328125
I0411 00:33:45.313827 140255575475968 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1796.8570556640625
I0411 00:35:06.142490 140256239228672 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1850.09912109375
I0411 00:36:19.481485 140256230835968 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1828.9505615234375
I0411 00:37:32.852283 140256239228672 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1796.3526611328125
I0411 00:38:46.251387 140256230835968 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1855.597412109375
I0411 00:39:59.639903 140256239228672 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1830.9127197265625
I0411 00:41:15.086174 140256230835968 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1831.1748046875
I0411 00:42:37.918728 140256239228672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1879.9898681640625
I0411 00:44:00.762894 140256230835968 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1823.4791259765625
I0411 00:45:24.029834 140256239228672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1863.83837890625
I0411 00:46:42.441427 140256230835968 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1820.6259765625
I0411 00:48:05.680241 140255583868672 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1893.6180419921875
I0411 00:48:14.908842 140428978612032 submission_runner.py:373] Before eval at step 6214: RAM USED (GB) 23.807352832
I0411 00:48:14.909064 140428978612032 spec.py:298] Evaluating on the training split.
I0411 00:48:40.470200 140428978612032 spec.py:310] Evaluating on the validation split.
I0411 00:49:14.734914 140428978612032 spec.py:326] Evaluating on the test split.
I0411 00:49:32.580687 140428978612032 submission_runner.py:382] Time since start: 5049.85s, 	Step: 6214, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:49:32.582047 140428978612032 submission_runner.py:396] After eval at step 6214: RAM USED (GB) 21.44892928
I0411 00:49:32.600397 140256024188672 logging_writer.py:48] [6214] global_step=6214, preemption_count=0, score=4849.199714, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5049.853523, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 00:49:32.783394 140428978612032 checkpoints.py:356] Saving checkpoint at step: 6214
I0411 00:49:33.719560 140428978612032 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_6214
I0411 00:49:33.739574 140428978612032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_6214.
I0411 00:49:33.748265 140428978612032 submission_runner.py:416] After logging and checkpointing eval at step 6214: RAM USED (GB) 21.419753472
I0411 00:50:37.634084 140256015795968 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1867.373291015625
I0411 00:51:51.097454 140255965439744 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1798.3717041015625
I0411 00:53:04.520948 140256015795968 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1901.6358642578125
I0411 00:54:18.039496 140255965439744 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1769.2877197265625
I0411 00:55:31.555468 140256015795968 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1771.1236572265625
I0411 00:56:45.114951 140255965439744 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1772.7181396484375
I0411 00:58:06.328505 140256015795968 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1874.0718994140625
I0411 00:59:25.807853 140255965439744 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1882.063232421875
I0411 01:00:49.269103 140256015795968 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1853.8505859375
I0411 01:02:12.610963 140255965439744 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1780.9810791015625
I0411 01:03:29.884750 140256024188672 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1851.972900390625
I0411 01:04:43.350081 140256015795968 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1847.295654296875
I0411 01:05:56.767099 140256024188672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1876.81982421875
I0411 01:07:10.122167 140256015795968 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1838.5394287109375
I0411 01:08:24.641871 140256024188672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1855.462890625
I0411 01:09:44.310951 140256015795968 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1817.394775390625
I0411 01:11:07.174268 140256024188672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1791.1993408203125
I0411 01:12:30.178113 140256015795968 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1778.1357421875
I0411 01:13:54.033842 140256024188672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1807.2589111328125
I0411 01:15:14.002336 140256015795968 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1789.571533203125
I0411 01:16:32.355360 140255696508672 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1892.6380615234375
I0411 01:17:46.002717 140255688115968 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1843.703369140625
I0411 01:18:59.780166 140255696508672 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1799.8890380859375
I0411 01:20:13.124086 140255688115968 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1791.4500732421875
I0411 01:21:26.600529 140255696508672 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1817.394775390625
I0411 01:22:42.332304 140255688115968 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1846.895751953125
I0411 01:24:06.554113 140255696508672 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1854.9251708984375
I0411 01:25:31.494945 140255688115968 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1815.2039794921875
I0411 01:26:55.646176 140255696508672 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1872.56396484375
I0411 01:28:18.403440 140255688115968 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1782.9656982421875
I0411 01:29:33.893097 140428978612032 submission_runner.py:373] Before eval at step 9296: RAM USED (GB) 23.997960192
I0411 01:29:33.893538 140428978612032 spec.py:298] Evaluating on the training split.
I0411 01:30:00.495386 140428978612032 spec.py:310] Evaluating on the validation split.
I0411 01:30:34.697745 140428978612032 spec.py:326] Evaluating on the test split.
I0411 01:30:51.914067 140428978612032 submission_runner.py:382] Time since start: 7528.84s, 	Step: 9296, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:30:51.915332 140428978612032 submission_runner.py:396] After eval at step 9296: RAM USED (GB) 21.5445504
I0411 01:30:51.934361 140255696508672 logging_writer.py:48] [9296] global_step=9296, preemption_count=0, score=7243.434037, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7528.837042, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 01:30:52.115759 140428978612032 checkpoints.py:356] Saving checkpoint at step: 9296
I0411 01:30:53.078272 140428978612032 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_9296
I0411 01:30:53.096493 140428978612032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_9296.
I0411 01:30:53.106029 140428978612032 submission_runner.py:416] After logging and checkpointing eval at step 9296: RAM USED (GB) 21.515194368
I0411 01:30:56.871134 140255688115968 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1812.24853515625
I0411 01:32:10.568686 140255629367040 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1882.61669921875
I0411 01:33:23.864344 140255688115968 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1836.9561767578125
I0411 01:34:37.311772 140255629367040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1832.0921630859375
I0411 01:35:51.286892 140255688115968 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1759.3170166015625
I0411 01:37:05.079590 140255629367040 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1784.58154296875
I0411 01:38:27.301938 140255688115968 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1765.5050048828125
I0411 01:39:49.757621 140428978612032 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 23.193624576
I0411 01:39:49.757841 140428978612032 spec.py:298] Evaluating on the training split.
I0411 01:40:15.979121 140428978612032 spec.py:310] Evaluating on the validation split.
I0411 01:40:52.009682 140428978612032 spec.py:326] Evaluating on the test split.
I0411 01:41:09.128914 140428978612032 submission_runner.py:382] Time since start: 8144.70s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:41:09.130178 140428978612032 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 22.568681472
I0411 01:41:09.145867 140256454268672 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7778.695571, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=8144.704097, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 01:41:09.325845 140428978612032 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:41:10.205291 140428978612032 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:41:10.223500 140428978612032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:41:10.227514 140428978612032 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.686613504
I0411 01:41:10.234615 140256445875968 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7778.695571
I0411 01:41:10.336441 140428978612032 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:41:11.556819 140428978612032 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:41:11.575483 140428978612032 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:41:12.808505 140428978612032 submission_runner.py:550] Tuning trial 1/1
I0411 01:41:12.808796 140428978612032 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 01:41:12.813558 140428978612032 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.515316, dtype=float32), 'train/wer': 1.0080065801803302, 'validation/ctc_loss': DeviceArray(30.844193, dtype=float32), 'validation/wer': 1.14595413366265, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.919975, dtype=float32), 'test/wer': 1.1349095119127415, 'test/num_examples': 2472, 'score': 60.60588550567627, 'total_duration': 60.795711517333984, 'global_step': 1, 'preemption_count': 0}), (3108, {'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2455.030806541443, 'total_duration': 2572.1822197437286, 'global_step': 3108, 'preemption_count': 0}), (6214, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4849.199714183807, 'total_duration': 5049.853522777557, 'global_step': 6214, 'preemption_count': 0}), (9296, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7243.43403673172, 'total_duration': 7528.837041854858, 'global_step': 9296, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7778.695570707321, 'total_duration': 8144.704097032547, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 01:41:12.813718 140428978612032 submission_runner.py:553] Timing: 7778.695570707321
I0411 01:41:12.813787 140428978612032 submission_runner.py:554] ====================
I0411 01:41:12.814196 140428978612032 submission_runner.py:613] Final librispeech_conformer score: 7778.695570707321
