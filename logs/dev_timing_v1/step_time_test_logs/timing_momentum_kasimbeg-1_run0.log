I0410 23:23:29.892093 139768555185984 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax.
I0410 23:23:29.963026 139768555185984 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0410 23:23:30.800164 139768555185984 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0410 23:23:30.800890 139768555185984 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0410 23:23:30.804827 139768555185984 submission_runner.py:511] Using RNG seed 3999033829
I0410 23:23:33.312094 139768555185984 submission_runner.py:520] --- Tuning run 1/1 ---
I0410 23:23:33.312288 139768555185984 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1.
I0410 23:23:33.312479 139768555185984 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/hparams.json.
I0410 23:23:33.435341 139768555185984 submission_runner.py:230] Starting train once: RAM USED (GB) 4.323131392
I0410 23:23:33.435517 139768555185984 submission_runner.py:231] Initializing dataset.
I0410 23:23:33.435677 139768555185984 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.323131392
I0410 23:23:33.435734 139768555185984 submission_runner.py:240] Initializing model.
I0410 23:23:38.866578 139768555185984 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.747944448
I0410 23:23:38.866796 139768555185984 submission_runner.py:252] Initializing optimizer.
I0410 23:23:39.526117 139768555185984 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.747780608
I0410 23:23:39.526297 139768555185984 submission_runner.py:261] Initializing metrics bundle.
I0410 23:23:39.526357 139768555185984 submission_runner.py:276] Initializing checkpoint and logger.
I0410 23:23:39.527277 139768555185984 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0410 23:23:39.527555 139768555185984 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0410 23:23:39.527618 139768555185984 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0410 23:23:40.138159 139768555185984 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0410 23:23:40.139047 139768555185984 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/flags_0.json.
I0410 23:23:40.145910 139768555185984 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.749926912
I0410 23:23:40.146107 139768555185984 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.749926912
I0410 23:23:40.146171 139768555185984 submission_runner.py:313] Starting training loop.
I0410 23:23:40.334516 139768555185984 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:23:40.366216 139768555185984 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:23:40.699590 139768555185984 input_pipeline.py:20] Loading split = train-other-500
I0410 23:23:44.689478 139768555185984 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.320480768
2023-04-10 23:24:35.123300: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-10 23:24:35.221235: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0410 23:24:36.951924 139592061785856 logging_writer.py:48] [0] global_step=0, grad_norm=39.27865219116211, loss=32.57898712158203
I0410 23:24:36.977096 139768555185984 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.199138816
I0410 23:24:36.977414 139768555185984 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.199138816
I0410 23:24:36.977498 139768555185984 spec.py:298] Evaluating on the training split.
I0410 23:24:37.075465 139768555185984 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:24:37.100883 139768555185984 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:24:37.382733 139768555185984 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0410 23:25:30.766238 139768555185984 spec.py:310] Evaluating on the validation split.
I0410 23:25:30.823751 139768555185984 input_pipeline.py:20] Loading split = dev-clean
I0410 23:25:30.828663 139768555185984 input_pipeline.py:20] Loading split = dev-other
I0410 23:26:13.454528 139768555185984 spec.py:326] Evaluating on the test split.
I0410 23:26:13.513135 139768555185984 input_pipeline.py:20] Loading split = test-clean
I0410 23:26:42.217506 139768555185984 submission_runner.py:382] Time since start: 56.83s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.965021, dtype=float32), 'train/wer': 1.60923932282448, 'validation/ctc_loss': DeviceArray(30.79176, dtype=float32), 'validation/wer': 1.683479821320032, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.878586, dtype=float32), 'test/wer': 1.6992058172364064, 'test/num_examples': 2472}
I0410 23:26:42.218388 139768555185984 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.016018944
I0410 23:26:42.236093 139590220375808 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=56.642873, test/ctc_loss=30.878585815429688, test/num_examples=2472, test/wer=1.699206, total_duration=56.831258, train/ctc_loss=31.96502113342285, train/wer=1.609239, validation/ctc_loss=30.791759490966797, validation/num_examples=5348, validation/wer=1.683480
I0410 23:26:42.417180 139768555185984 checkpoints.py:356] Saving checkpoint at step: 1
I0410 23:26:42.981457 139768555185984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_1
I0410 23:26:42.982644 139768555185984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_1.
I0410 23:26:42.990336 139768555185984 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.985286656
I0410 23:26:43.020955 139768555185984 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.984569856
I0410 23:26:56.089930 139768555185984 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.38550272
I0410 23:28:10.423981 139594701031168 logging_writer.py:48] [100] global_step=100, grad_norm=11.494039535522461, loss=6.154573440551758
I0410 23:29:25.923736 139594709423872 logging_writer.py:48] [200] global_step=200, grad_norm=48.2945442199707, loss=39.960472106933594
I0410 23:30:39.748346 139594701031168 logging_writer.py:48] [300] global_step=300, grad_norm=nan, loss=nan
I0410 23:31:52.417239 139594709423872 logging_writer.py:48] [400] global_step=400, grad_norm=nan, loss=nan
I0410 23:33:05.488343 139594701031168 logging_writer.py:48] [500] global_step=500, grad_norm=nan, loss=nan
I0410 23:34:19.692090 139594709423872 logging_writer.py:48] [600] global_step=600, grad_norm=nan, loss=nan
I0410 23:35:41.792117 139594701031168 logging_writer.py:48] [700] global_step=700, grad_norm=nan, loss=nan
I0410 23:36:58.170858 139594709423872 logging_writer.py:48] [800] global_step=800, grad_norm=nan, loss=nan
I0410 23:38:17.625500 139594701031168 logging_writer.py:48] [900] global_step=900, grad_norm=nan, loss=nan
I0410 23:39:39.495089 139594709423872 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0410 23:40:55.701891 139591185061632 logging_writer.py:48] [1100] global_step=1100, grad_norm=nan, loss=nan
I0410 23:42:08.547897 139591101183744 logging_writer.py:48] [1200] global_step=1200, grad_norm=nan, loss=nan
I0410 23:43:21.223047 139591185061632 logging_writer.py:48] [1300] global_step=1300, grad_norm=nan, loss=nan
I0410 23:44:33.322214 139591101183744 logging_writer.py:48] [1400] global_step=1400, grad_norm=nan, loss=nan
I0410 23:45:45.604929 139591185061632 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0410 23:47:02.494208 139591101183744 logging_writer.py:48] [1600] global_step=1600, grad_norm=nan, loss=nan
I0410 23:48:19.478568 139591185061632 logging_writer.py:48] [1700] global_step=1700, grad_norm=nan, loss=nan
I0410 23:49:38.491889 139591101183744 logging_writer.py:48] [1800] global_step=1800, grad_norm=nan, loss=nan
I0410 23:50:59.238674 139591185061632 logging_writer.py:48] [1900] global_step=1900, grad_norm=nan, loss=nan
I0410 23:52:17.042287 139591101183744 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0410 23:53:37.381387 139591185061632 logging_writer.py:48] [2100] global_step=2100, grad_norm=nan, loss=nan
I0410 23:54:50.361999 139591101183744 logging_writer.py:48] [2200] global_step=2200, grad_norm=nan, loss=nan
I0410 23:56:03.256209 139591185061632 logging_writer.py:48] [2300] global_step=2300, grad_norm=nan, loss=nan
I0410 23:57:16.196100 139591101183744 logging_writer.py:48] [2400] global_step=2400, grad_norm=nan, loss=nan
I0410 23:58:29.115467 139591185061632 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0410 23:59:42.010544 139591101183744 logging_writer.py:48] [2600] global_step=2600, grad_norm=nan, loss=nan
I0411 00:00:58.759999 139591185061632 logging_writer.py:48] [2700] global_step=2700, grad_norm=nan, loss=nan
I0411 00:02:20.434595 139591101183744 logging_writer.py:48] [2800] global_step=2800, grad_norm=nan, loss=nan
I0411 00:03:41.769938 139591185061632 logging_writer.py:48] [2900] global_step=2900, grad_norm=nan, loss=nan
I0411 00:05:02.260434 139591101183744 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0411 00:06:25.462904 139594843707136 logging_writer.py:48] [3100] global_step=3100, grad_norm=nan, loss=nan
I0411 00:06:43.406716 139768555185984 submission_runner.py:373] Before eval at step 3126: RAM USED (GB) 23.497879552
I0411 00:06:43.406973 139768555185984 spec.py:298] Evaluating on the training split.
I0411 00:07:09.240306 139768555185984 spec.py:310] Evaluating on the validation split.
I0411 00:07:42.957914 139768555185984 spec.py:326] Evaluating on the test split.
I0411 00:07:59.821381 139768555185984 submission_runner.py:382] Time since start: 2583.26s, 	Step: 3126, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:07:59.822740 139768555185984 submission_runner.py:396] After eval at step 3126: RAM USED (GB) 21.150302208
I0411 00:07:59.842263 139594843707136 logging_writer.py:48] [3126] global_step=3126, preemption_count=0, score=2451.592685, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2583.257480, train/ctc_loss=nan, train/wer=0.944636, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 00:08:00.044099 139768555185984 checkpoints.py:356] Saving checkpoint at step: 3126
I0411 00:08:00.901281 139768555185984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_3126
I0411 00:08:00.922277 139768555185984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_3126.
I0411 00:08:00.929990 139768555185984 submission_runner.py:416] After logging and checkpointing eval at step 3126: RAM USED (GB) 21.176930304
I0411 00:08:55.685920 139594835314432 logging_writer.py:48] [3200] global_step=3200, grad_norm=nan, loss=nan
I0411 00:10:08.676945 139591092791040 logging_writer.py:48] [3300] global_step=3300, grad_norm=nan, loss=nan
I0411 00:11:21.484260 139594835314432 logging_writer.py:48] [3400] global_step=3400, grad_norm=nan, loss=nan
I0411 00:12:34.298344 139591092791040 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0411 00:13:47.272559 139594835314432 logging_writer.py:48] [3600] global_step=3600, grad_norm=nan, loss=nan
I0411 00:15:04.104019 139591092791040 logging_writer.py:48] [3700] global_step=3700, grad_norm=nan, loss=nan
I0411 00:16:25.832064 139594835314432 logging_writer.py:48] [3800] global_step=3800, grad_norm=nan, loss=nan
I0411 00:17:46.537963 139591092791040 logging_writer.py:48] [3900] global_step=3900, grad_norm=nan, loss=nan
I0411 00:19:07.478422 139594835314432 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0411 00:20:30.184151 139591092791040 logging_writer.py:48] [4100] global_step=4100, grad_norm=nan, loss=nan
I0411 00:21:47.689041 139594843707136 logging_writer.py:48] [4200] global_step=4200, grad_norm=nan, loss=nan
I0411 00:23:00.544700 139594835314432 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0411 00:24:13.543473 139594843707136 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0411 00:25:26.384127 139594835314432 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0411 00:26:39.240620 139594843707136 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0411 00:27:52.227051 139594835314432 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0411 00:29:09.779148 139594843707136 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0411 00:30:26.106626 139594835314432 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0411 00:31:43.086038 139594843707136 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0411 00:33:01.488540 139594835314432 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0411 00:34:20.965807 139594843707136 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0411 00:35:33.792303 139594835314432 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0411 00:36:46.652328 139594843707136 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0411 00:37:59.470195 139594835314432 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0411 00:39:12.283479 139594843707136 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0411 00:40:26.965551 139594835314432 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0411 00:41:44.503146 139594843707136 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0411 00:43:01.466557 139594835314432 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0411 00:44:22.382569 139594843707136 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0411 00:45:43.633590 139594835314432 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0411 00:47:03.064635 139594843707136 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0411 00:48:01.556285 139768555185984 submission_runner.py:373] Before eval at step 6282: RAM USED (GB) 22.274195456
I0411 00:48:01.556490 139768555185984 spec.py:298] Evaluating on the training split.
I0411 00:48:27.454018 139768555185984 spec.py:310] Evaluating on the validation split.
I0411 00:49:01.009465 139768555185984 spec.py:326] Evaluating on the test split.
I0411 00:49:19.013596 139768555185984 submission_runner.py:382] Time since start: 5061.40s, 	Step: 6282, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:49:19.014976 139768555185984 submission_runner.py:396] After eval at step 6282: RAM USED (GB) 21.494022144
I0411 00:49:19.035184 139594843707136 logging_writer.py:48] [6282] global_step=6282, preemption_count=0, score=4846.409163, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5061.404119, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 00:49:19.255589 139768555185984 checkpoints.py:356] Saving checkpoint at step: 6282
I0411 00:49:20.209034 139768555185984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_6282
I0411 00:49:20.230454 139768555185984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_6282.
I0411 00:49:20.246103 139768555185984 submission_runner.py:416] After logging and checkpointing eval at step 6282: RAM USED (GB) 21.511815168
I0411 00:49:34.148271 139594835314432 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0411 00:50:47.056993 139591084398336 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0411 00:51:59.887367 139594835314432 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0411 00:53:12.918027 139591084398336 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0411 00:54:25.923735 139594835314432 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0411 00:55:38.894922 139591084398336 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0411 00:56:52.447339 139594835314432 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0411 00:58:10.360317 139591084398336 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0411 00:59:29.126556 139594835314432 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0411 01:00:47.449744 139591084398336 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0411 01:02:04.329029 139594516027136 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0411 01:03:17.198570 139594507634432 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0411 01:04:30.033028 139594516027136 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0411 01:05:42.920723 139594507634432 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0411 01:06:55.822665 139594516027136 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0411 01:08:14.845188 139594507634432 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0411 01:09:31.949869 139594516027136 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0411 01:10:48.978305 139594507634432 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0411 01:12:05.653320 139594516027136 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0411 01:13:27.499086 139594507634432 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0411 01:14:47.447692 139594843707136 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0411 01:16:00.352432 139594835314432 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0411 01:17:13.185936 139594843707136 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0411 01:18:26.047184 139594835314432 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0411 01:19:39.030886 139594843707136 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0411 01:20:56.846534 139594835314432 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0411 01:22:18.774649 139594843707136 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0411 01:23:37.304167 139594835314432 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0411 01:24:56.517544 139594843707136 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0411 01:26:16.310514 139594835314432 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0411 01:27:38.261907 139595499067136 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0411 01:28:50.688254 139595490674432 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0411 01:29:20.717514 139768555185984 submission_runner.py:373] Before eval at step 9443: RAM USED (GB) 23.199535104
I0411 01:29:20.717723 139768555185984 spec.py:298] Evaluating on the training split.
I0411 01:29:47.234973 139768555185984 spec.py:310] Evaluating on the validation split.
I0411 01:30:21.888571 139768555185984 spec.py:326] Evaluating on the test split.
I0411 01:30:40.219639 139768555185984 submission_runner.py:382] Time since start: 7540.57s, 	Step: 9443, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:30:40.221002 139768555185984 submission_runner.py:396] After eval at step 9443: RAM USED (GB) 21.421572096
I0411 01:30:40.241595 139595499067136 logging_writer.py:48] [9443] global_step=9443, preemption_count=0, score=7240.872418, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7540.567895, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 01:30:40.428370 139768555185984 checkpoints.py:356] Saving checkpoint at step: 9443
I0411 01:30:41.342099 139768555185984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_9443
I0411 01:30:41.363235 139768555185984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_9443.
I0411 01:30:41.371023 139768555185984 submission_runner.py:416] After logging and checkpointing eval at step 9443: RAM USED (GB) 21.441671168
I0411 01:31:23.620804 139595490674432 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0411 01:32:36.485488 139590966966016 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0411 01:33:49.408247 139595490674432 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0411 01:35:04.223668 139590966966016 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0411 01:36:28.376661 139595490674432 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0411 01:37:51.251094 139768555185984 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 23.548022784
I0411 01:37:51.251312 139768555185984 spec.py:298] Evaluating on the training split.
I0411 01:38:17.531968 139768555185984 spec.py:310] Evaluating on the validation split.
I0411 01:38:53.489722 139768555185984 spec.py:326] Evaluating on the test split.
I0411 01:39:11.686826 139768555185984 submission_runner.py:382] Time since start: 8051.10s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:39:11.688089 139768555185984 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 22.529384448
I0411 01:39:11.703677 139595499067136 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7669.690033, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=8051.103914, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 01:39:11.882386 139768555185984 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:39:12.808074 139768555185984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:39:12.829567 139768555185984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:39:12.839544 139768555185984 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.54569472
I0411 01:39:12.846538 139595490674432 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7669.690033
I0411 01:39:12.992424 139768555185984 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:39:14.182368 139768555185984 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:39:14.203644 139768555185984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-1_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:39:15.628620 139768555185984 submission_runner.py:550] Tuning trial 1/1
I0411 01:39:15.628869 139768555185984 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 01:39:15.633717 139768555185984 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.965021, dtype=float32), 'train/wer': 1.60923932282448, 'validation/ctc_loss': DeviceArray(30.79176, dtype=float32), 'validation/wer': 1.683479821320032, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.878586, dtype=float32), 'test/wer': 1.6992058172364064, 'test/num_examples': 2472, 'score': 56.64287281036377, 'total_duration': 56.83125829696655, 'global_step': 1, 'preemption_count': 0}), (3126, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2451.592684984207, 'total_duration': 2583.2574801445007, 'global_step': 3126, 'preemption_count': 0}), (6282, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4846.409163475037, 'total_duration': 5061.40411901474, 'global_step': 6282, 'preemption_count': 0}), (9443, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7240.872418165207, 'total_duration': 7540.5678951740265, 'global_step': 9443, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7669.690032720566, 'total_duration': 8051.1039135456085, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 01:39:15.633883 139768555185984 submission_runner.py:553] Timing: 7669.690032720566
I0411 01:39:15.633939 139768555185984 submission_runner.py:554] ====================
I0411 01:39:15.634324 139768555185984 submission_runner.py:613] Final librispeech_conformer score: 7669.690032720566
