I0411 01:53:56.651969 140088326670144 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax.
I0411 01:53:56.708274 140088326670144 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0411 01:53:57.664307 140088326670144 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0411 01:53:57.664970 140088326670144 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0411 01:53:57.670291 140088326670144 submission_runner.py:511] Using RNG seed 3355530536
I0411 01:54:00.109745 140088326670144 submission_runner.py:520] --- Tuning run 1/1 ---
I0411 01:54:00.109936 140088326670144 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1.
I0411 01:54:00.110098 140088326670144 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/hparams.json.
I0411 01:54:00.238085 140088326670144 submission_runner.py:230] Starting train once: RAM USED (GB) 4.3772928
I0411 01:54:00.238251 140088326670144 submission_runner.py:231] Initializing dataset.
I0411 01:54:00.238410 140088326670144 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.3772928
I0411 01:54:00.238476 140088326670144 submission_runner.py:240] Initializing model.
I0411 01:54:05.929270 140088326670144 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.800942592
I0411 01:54:05.929459 140088326670144 submission_runner.py:252] Initializing optimizer.
I0411 01:54:06.620833 140088326670144 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.801942016
I0411 01:54:06.621008 140088326670144 submission_runner.py:261] Initializing metrics bundle.
I0411 01:54:06.621055 140088326670144 submission_runner.py:276] Initializing checkpoint and logger.
I0411 01:54:06.622048 140088326670144 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0411 01:54:06.622337 140088326670144 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0411 01:54:06.622422 140088326670144 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0411 01:54:07.347936 140088326670144 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0411 01:54:07.348888 140088326670144 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/flags_0.json.
I0411 01:54:07.354588 140088326670144 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.801143296
I0411 01:54:07.354773 140088326670144 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.801143296
I0411 01:54:07.354836 140088326670144 submission_runner.py:313] Starting training loop.
I0411 01:54:07.552573 140088326670144 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:54:07.583764 140088326670144 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:54:07.902648 140088326670144 input_pipeline.py:20] Loading split = train-other-500
I0411 01:54:11.346740 140088326670144 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.984367104
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0411 01:55:05.248862 139912485533440 logging_writer.py:48] [0] global_step=0, grad_norm=36.54042053222656, loss=31.402957916259766
I0411 01:55:05.265331 140088326670144 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.321248768
I0411 01:55:05.265599 140088326670144 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.321248768
I0411 01:55:05.265678 140088326670144 spec.py:298] Evaluating on the training split.
I0411 01:55:05.368635 140088326670144 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:55:05.394649 140088326670144 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:55:05.692543 140088326670144 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0411 01:55:55.553409 140088326670144 spec.py:310] Evaluating on the validation split.
I0411 01:55:55.620262 140088326670144 input_pipeline.py:20] Loading split = dev-clean
I0411 01:55:55.625420 140088326670144 input_pipeline.py:20] Loading split = dev-other
I0411 01:56:33.221060 140088326670144 spec.py:326] Evaluating on the test split.
I0411 01:56:33.289245 140088326670144 input_pipeline.py:20] Loading split = test-clean
I0411 01:57:00.829162 140088326670144 submission_runner.py:382] Time since start: 57.91s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.737062, dtype=float32), 'train/wer': 1.195546925431564, 'validation/ctc_loss': DeviceArray(30.916302, dtype=float32), 'validation/wer': 1.1966251483371764, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.986362, dtype=float32), 'test/wer': 1.2233055064692382, 'test/num_examples': 2472}
I0411 01:57:00.830336 140088326670144 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.021184
I0411 01:57:00.842975 139909851502336 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=57.712987, test/ctc_loss=30.98636245727539, test/num_examples=2472, test/wer=1.223306, total_duration=57.910780, train/ctc_loss=31.737062454223633, train/wer=1.195547, validation/ctc_loss=30.916301727294922, validation/num_examples=5348, validation/wer=1.196625
I0411 01:57:01.067296 140088326670144 checkpoints.py:356] Saving checkpoint at step: 1
I0411 01:57:02.013380 140088326670144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_1
I0411 01:57:02.035620 140088326670144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_1.
I0411 01:57:02.054800 140088326670144 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.996247552
I0411 01:57:02.109478 140088326670144 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.992835584
I0411 01:57:16.122906 140088326670144 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.361127424
I0411 01:58:31.013020 139914431780608 logging_writer.py:48] [100] global_step=100, grad_norm=0.7441809773445129, loss=5.933339595794678
I0411 01:59:46.787508 139914440173312 logging_writer.py:48] [200] global_step=200, grad_norm=169.42359924316406, loss=65.0377426147461
I0411 02:01:00.558550 139914431780608 logging_writer.py:48] [300] global_step=300, grad_norm=0.0, loss=1802.669921875
I0411 02:02:13.982695 139914440173312 logging_writer.py:48] [400] global_step=400, grad_norm=0.0, loss=1820.3594970703125
I0411 02:03:27.334986 139914431780608 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1885.6597900390625
I0411 02:04:40.585808 139914440173312 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1907.2952880859375
I0411 02:05:59.958876 139914431780608 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1895.8541259765625
I0411 02:07:27.589702 139914440173312 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1858.5535888671875
I0411 02:08:53.314053 139914431780608 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1765.497802734375
I0411 02:10:17.434008 139914440173312 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1769.402587890625
I0411 02:11:36.526336 139914566063872 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1837.60791015625
I0411 02:12:49.903693 139914557671168 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1822.4329833984375
I0411 02:14:03.298680 139914566063872 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1836.81689453125
I0411 02:15:16.698599 139914557671168 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1818.8074951171875
I0411 02:16:30.128149 139914566063872 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1817.9034423828125
I0411 02:17:51.393079 139914557671168 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1820.22998046875
I0411 02:19:15.057291 139914566063872 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1846.8880615234375
I0411 02:20:39.500724 139914557671168 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1828.159423828125
I0411 02:22:02.975705 139914566063872 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1851.9652099609375
I0411 02:23:28.003196 139914557671168 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1826.8548583984375
I0411 02:24:50.035476 139915876783872 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1842.8995361328125
I0411 02:26:03.455912 139915868391168 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1902.6173095703125
I0411 02:27:16.845439 139915876783872 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1807.506591796875
I0411 02:28:30.180851 139915868391168 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1815.0677490234375
I0411 02:29:46.759105 139915876783872 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1846.7548828125
I0411 02:31:08.369312 139915868391168 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1806.1038818359375
I0411 02:32:29.531804 139915876783872 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1870.3671875
I0411 02:33:53.522016 139915868391168 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1820.87744140625
I0411 02:35:16.441170 139915876783872 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1890.67333984375
I0411 02:36:40.553151 139915868391168 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1805.7216796875
I0411 02:37:02.998912 140088326670144 submission_runner.py:373] Before eval at step 3027: RAM USED (GB) 22.921826304
I0411 02:37:02.999174 140088326670144 spec.py:298] Evaluating on the training split.
I0411 02:37:28.742487 140088326670144 spec.py:310] Evaluating on the validation split.
I0411 02:38:04.305303 140088326670144 spec.py:326] Evaluating on the test split.
I0411 02:38:22.835515 140088326670144 submission_runner.py:382] Time since start: 2575.64s, 	Step: 3027, 	{'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 02:38:22.837046 140088326670144 submission_runner.py:396] After eval at step 3027: RAM USED (GB) 20.821766144
I0411 02:38:22.853339 139915876783872 logging_writer.py:48] [3027] global_step=3027, preemption_count=0, score=2452.524406, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2575.642113, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 02:38:23.069195 140088326670144 checkpoints.py:356] Saving checkpoint at step: 3027
I0411 02:38:24.009772 140088326670144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_3027
I0411 02:38:24.031264 140088326670144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_3027.
I0411 02:38:24.044227 140088326670144 submission_runner.py:416] After logging and checkpointing eval at step 3027: RAM USED (GB) 20.84327424
I0411 02:39:22.363270 139915221423872 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1872.96728515625
I0411 02:40:35.725883 139915213031168 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1834.9735107421875
I0411 02:41:49.074089 139915221423872 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1881.9169921875
I0411 02:43:02.553299 139915213031168 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1786.3173828125
I0411 02:44:19.651640 139915221423872 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1847.021240234375
I0411 02:45:43.058531 139915213031168 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1838.0037841796875
I0411 02:47:08.209120 139915221423872 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1839.3243408203125
I0411 02:48:32.391297 139915213031168 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1869.8206787109375
I0411 02:49:55.052736 139915221423872 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1855.32080078125
I0411 02:51:20.071574 139915213031168 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1818.936767578125
I0411 02:52:45.628733 139915221423872 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1853.8428955078125
I0411 02:54:04.110317 139915876783872 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1804.575927734375
I0411 02:55:17.475382 139915868391168 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1806.6136474609375
I0411 02:56:30.882066 139915876783872 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1831.822265625
I0411 02:57:44.179343 139915868391168 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1869.6842041015625
I0411 02:59:01.117990 139915876783872 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1840.9117431640625
I0411 03:00:26.337929 139915868391168 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1809.0394287109375
I0411 03:01:51.242444 139915876783872 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1800.767822265625
I0411 03:03:14.239811 139915868391168 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1780.2305908203125
I0411 03:04:38.724827 139915876783872 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1795.21142578125
I0411 03:06:02.669658 139915868391168 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1808.0172119140625
I0411 03:07:24.893814 139915876783872 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1878.8782958984375
I0411 03:08:38.357858 139915868391168 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1807.63427734375
I0411 03:09:51.859267 139915876783872 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1800.3878173828125
I0411 03:11:05.366539 139915868391168 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1865.596435546875
I0411 03:12:18.896763 139915876783872 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1829.989013671875
I0411 03:13:38.112847 139915868391168 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1824.381103515625
I0411 03:15:03.288294 139915876783872 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1874.613037109375
I0411 03:16:26.485415 139915868391168 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1740.530029296875
I0411 03:17:48.027642 139915876783872 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1849.28955078125
I0411 03:18:24.554220 140088326670144 submission_runner.py:373] Before eval at step 6046: RAM USED (GB) 22.134902784
I0411 03:18:24.554443 140088326670144 spec.py:298] Evaluating on the training split.
I0411 03:18:51.455436 140088326670144 spec.py:310] Evaluating on the validation split.
I0411 03:19:27.858982 140088326670144 spec.py:326] Evaluating on the test split.
I0411 03:19:46.305354 140088326670144 submission_runner.py:382] Time since start: 5057.20s, 	Step: 6046, 	{'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 03:19:46.306793 140088326670144 submission_runner.py:396] After eval at step 6046: RAM USED (GB) 21.157101568
I0411 03:19:46.326085 139915876783872 logging_writer.py:48] [6046] global_step=6046, preemption_count=0, score=4847.085808, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5057.196611, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 03:19:46.517016 140088326670144 checkpoints.py:356] Saving checkpoint at step: 6046
I0411 03:19:47.485695 140088326670144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_6046
I0411 03:19:47.507436 140088326670144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_6046.
I0411 03:19:47.518033 140088326670144 submission_runner.py:416] After logging and checkpointing eval at step 6046: RAM USED (GB) 21.172178944
I0411 03:20:27.879483 139915868391168 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1976.8121337890625
I0411 03:21:45.287397 139915876783872 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1844.0941162109375
I0411 03:22:58.601934 139915868391168 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1822.1734619140625
I0411 03:24:12.023748 139915876783872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1804.0672607421875
I0411 03:25:25.368004 139915868391168 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1906.44287109375
I0411 03:26:38.703902 139915876783872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1809.8067626953125
I0411 03:27:58.860338 139915868391168 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1802.035400390625
I0411 03:29:21.278701 139915876783872 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1792.8228759765625
I0411 03:30:45.368227 139915868391168 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1850.89404296875
I0411 03:32:10.131012 139915876783872 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1758.4638671875
I0411 03:33:30.027742 139915868391168 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1854.3800048828125
I0411 03:34:52.205517 139915876783872 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1750.527587890625
I0411 03:36:09.665627 139915876783872 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1885.6597900390625
I0411 03:37:22.958184 139915868391168 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1813.0111083984375
I0411 03:38:36.375078 139915876783872 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1825.421875
I0411 03:39:49.780162 139915868391168 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1839.720947265625
I0411 03:41:03.299085 139915876783872 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1886.21533203125
I0411 03:42:26.529754 139915868391168 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1782.3377685546875
I0411 03:43:50.823714 139915876783872 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1776.031494140625
I0411 03:45:15.398632 139915868391168 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1827.6373291015625
I0411 03:46:36.959893 139915876783872 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1909.4290771484375
I0411 03:48:01.604221 139915868391168 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1784.822998046875
I0411 03:49:22.157460 139915876783872 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1783.7037353515625
I0411 03:50:35.517664 139915868391168 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1894.7314453125
I0411 03:51:48.881090 139915876783872 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1786.69140625
I0411 03:53:02.365457 139915868391168 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1813.396484375
I0411 03:54:21.626240 139915876783872 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1806.486328125
I0411 03:55:46.234025 139915868391168 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1831.5601806640625
I0411 03:57:09.030258 139915876783872 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1815.84033203125
I0411 03:58:34.961985 139915868391168 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1817.645263671875
I0411 03:59:48.320381 140088326670144 submission_runner.py:373] Before eval at step 9089: RAM USED (GB) 21.932388352
I0411 03:59:48.320599 140088326670144 spec.py:298] Evaluating on the training split.
I0411 04:00:15.092785 140088326670144 spec.py:310] Evaluating on the validation split.
I0411 04:00:50.706295 140088326670144 spec.py:326] Evaluating on the test split.
I0411 04:01:08.850806 140088326670144 submission_runner.py:382] Time since start: 7540.96s, 	Step: 9089, 	{'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:01:08.852386 140088326670144 submission_runner.py:396] After eval at step 9089: RAM USED (GB) 21.363183616
I0411 04:01:08.877634 139915876783872 logging_writer.py:48] [9089] global_step=9089, preemption_count=0, score=7241.906532, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7540.961054, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 04:01:09.094811 140088326670144 checkpoints.py:356] Saving checkpoint at step: 9089
I0411 04:01:10.059996 140088326670144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_9089
I0411 04:01:10.081624 140088326670144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_9089.
I0411 04:01:10.094598 140088326670144 submission_runner.py:416] After logging and checkpointing eval at step 9089: RAM USED (GB) 21.376679936
I0411 04:01:18.949037 139915868391168 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1778.4990234375
I0411 04:02:32.334483 139911223150336 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1761.004150390625
I0411 04:03:49.459060 139915876783872 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1778.3756103515625
I0411 04:05:02.811562 139915868391168 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1812.6258544921875
I0411 04:06:16.146655 139915876783872 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1838.3997802734375
I0411 04:07:29.565325 139915868391168 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1905.875244140625
I0411 04:08:48.795355 139915876783872 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1828.8125
I0411 04:10:09.309483 139915868391168 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1898.2437744140625
I0411 04:11:33.492254 139915876783872 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1771.9744873046875
I0411 04:12:55.068527 140088326670144 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.563209216
I0411 04:12:55.068761 140088326670144 spec.py:298] Evaluating on the training split.
I0411 04:13:21.466472 140088326670144 spec.py:310] Evaluating on the validation split.
I0411 04:13:57.779995 140088326670144 spec.py:326] Evaluating on the test split.
I0411 04:14:17.109521 140088326670144 submission_runner.py:382] Time since start: 8327.71s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:14:17.111001 140088326670144 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.64146176
I0411 04:14:17.145795 139915876783872 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7945.065702, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=8327.711832, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 04:14:17.368445 140088326670144 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:14:18.302106 140088326670144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:14:18.323539 140088326670144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:14:18.334215 140088326670144 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.65608448
I0411 04:14:18.341625 139915868391168 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7945.065702
I0411 04:14:18.491223 140088326670144 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:14:19.831948 140088326670144 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:14:19.853613 140088326670144 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:14:21.364933 140088326670144 submission_runner.py:550] Tuning trial 1/1
I0411 04:14:21.365223 140088326670144 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 04:14:21.369969 140088326670144 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.737062, dtype=float32), 'train/wer': 1.195546925431564, 'validation/ctc_loss': DeviceArray(30.916302, dtype=float32), 'validation/wer': 1.1966251483371764, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.986362, dtype=float32), 'test/wer': 1.2233055064692382, 'test/num_examples': 2472, 'score': 57.71298670768738, 'total_duration': 57.91078042984009, 'global_step': 1, 'preemption_count': 0}), (3027, {'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2452.5244059562683, 'total_duration': 2575.6421134471893, 'global_step': 3027, 'preemption_count': 0}), (6046, {'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4847.08580827713, 'total_duration': 5057.196610689163, 'global_step': 6046, 'preemption_count': 0}), (9089, {'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7241.906532049179, 'total_duration': 7540.961054086685, 'global_step': 9089, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7945.065702199936, 'total_duration': 8327.71183180809, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 04:14:21.370169 140088326670144 submission_runner.py:553] Timing: 7945.065702199936
I0411 04:14:21.370265 140088326670144 submission_runner.py:554] ====================
I0411 04:14:21.370703 140088326670144 submission_runner.py:613] Final librispeech_conformer score: 7945.065702199936
