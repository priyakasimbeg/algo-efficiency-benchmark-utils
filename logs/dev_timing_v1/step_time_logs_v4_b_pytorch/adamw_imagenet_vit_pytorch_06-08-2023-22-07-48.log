torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-08-2023-22-07-48.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 22:08:11.293063 140297801754432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 22:08:11.293074 140596234430272 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 22:08:11.293101 140504459724608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 22:08:11.293697 140292410058560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 22:08:11.294029 140042487408448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 22:08:11.294063 139798107739968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 22:08:11.294246 139979717265216 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 22:08:11.294593 139979717265216 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.294500 139825018017600 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 22:08:11.294846 139825018017600 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.303841 140596234430272 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.303864 140504459724608 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.303894 140297801754432 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.304374 140292410058560 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.304647 139798107739968 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:11.304664 140042487408448 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:08:13.593153 139825018017600 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_vit_pytorch.
W0608 22:08:13.632425 140042487408448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.633873 139825018017600 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.633943 140504459724608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.634350 139798107739968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.635425 140292410058560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.635615 140297801754432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.635793 140596234430272 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:08:13.637447 139979717265216 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 22:08:13.638982 139825018017600 submission_runner.py:541] Using RNG seed 3154951797
I0608 22:08:13.640293 139825018017600 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 22:08:13.640406 139825018017600 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_vit_pytorch/trial_1.
I0608 22:08:13.640656 139825018017600 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0608 22:08:13.642194 139825018017600 submission_runner.py:255] Initializing dataset.
I0608 22:08:20.120506 139825018017600 submission_runner.py:262] Initializing model.
I0608 22:08:24.530799 139825018017600 submission_runner.py:272] Initializing optimizer.
I0608 22:08:24.532361 139825018017600 submission_runner.py:279] Initializing metrics bundle.
I0608 22:08:24.532495 139825018017600 submission_runner.py:297] Initializing checkpoint and logger.
I0608 22:08:25.093912 139825018017600 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0608 22:08:25.095016 139825018017600 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0608 22:08:25.145108 139825018017600 submission_runner.py:332] Starting training loop.
I0608 22:08:31.744479 139795934205696 logging_writer.py:48] [0] global_step=0, grad_norm=0.343020, loss=6.907756
I0608 22:08:31.766442 139825018017600 submission.py:120] 0) loss = 6.908, grad_norm = 0.343
I0608 22:08:31.767969 139825018017600 spec.py:298] Evaluating on the training split.
I0608 22:09:31.320226 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 22:10:25.173950 139825018017600 spec.py:326] Evaluating on the test split.
I0608 22:10:25.193077 139825018017600 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 22:10:25.199562 139825018017600 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 22:10:25.283527 139825018017600 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 22:10:37.337081 139825018017600 submission_runner.py:419] Time since start: 132.19s, 	Step: 1, 	{'train/accuracy': 0.00220703125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.0029, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0027, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.622880697250366, 'total_duration': 132.19233965873718, 'accumulated_submission_time': 6.622880697250366, 'accumulated_eval_time': 125.56893801689148, 'accumulated_logging_time': 0}
I0608 22:10:37.355492 139790850717440 logging_writer.py:48] [1] accumulated_eval_time=125.568938, accumulated_logging_time=0, accumulated_submission_time=6.622881, global_step=1, preemption_count=0, score=6.622881, test/accuracy=0.002700, test/loss=6.907755, test/num_examples=10000, total_duration=132.192340, train/accuracy=0.002207, train/loss=6.907756, validation/accuracy=0.002900, validation/loss=6.907756, validation/num_examples=50000
I0608 22:10:37.375599 139825018017600 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.375761 140292410058560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.375803 140297801754432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.375807 139798107739968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.375857 140596234430272 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.375979 139979717265216 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.376008 140042487408448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.376482 140504459724608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:10:37.953868 139790842324736 logging_writer.py:48] [1] global_step=1, grad_norm=0.350624, loss=6.907756
I0608 22:10:37.957534 139825018017600 submission.py:120] 1) loss = 6.908, grad_norm = 0.351
I0608 22:10:38.366367 139790850717440 logging_writer.py:48] [2] global_step=2, grad_norm=0.349300, loss=6.907755
I0608 22:10:38.370132 139825018017600 submission.py:120] 2) loss = 6.908, grad_norm = 0.349
I0608 22:10:38.773486 139790842324736 logging_writer.py:48] [3] global_step=3, grad_norm=0.343533, loss=6.907753
I0608 22:10:38.777207 139825018017600 submission.py:120] 3) loss = 6.908, grad_norm = 0.344
I0608 22:10:39.180142 139790850717440 logging_writer.py:48] [4] global_step=4, grad_norm=0.344991, loss=6.907753
I0608 22:10:39.184718 139825018017600 submission.py:120] 4) loss = 6.908, grad_norm = 0.345
I0608 22:10:39.589607 139790842324736 logging_writer.py:48] [5] global_step=5, grad_norm=0.348972, loss=6.907755
I0608 22:10:39.594914 139825018017600 submission.py:120] 5) loss = 6.908, grad_norm = 0.349
I0608 22:10:39.998690 139790850717440 logging_writer.py:48] [6] global_step=6, grad_norm=0.355597, loss=6.907741
I0608 22:10:40.002659 139825018017600 submission.py:120] 6) loss = 6.908, grad_norm = 0.356
I0608 22:10:40.411660 139790842324736 logging_writer.py:48] [7] global_step=7, grad_norm=0.353213, loss=6.907743
I0608 22:10:40.416381 139825018017600 submission.py:120] 7) loss = 6.908, grad_norm = 0.353
I0608 22:10:40.823862 139790850717440 logging_writer.py:48] [8] global_step=8, grad_norm=0.353888, loss=6.907732
I0608 22:10:40.828461 139825018017600 submission.py:120] 8) loss = 6.908, grad_norm = 0.354
I0608 22:10:41.229944 139790842324736 logging_writer.py:48] [9] global_step=9, grad_norm=0.356829, loss=6.907734
I0608 22:10:41.234455 139825018017600 submission.py:120] 9) loss = 6.908, grad_norm = 0.357
I0608 22:10:41.638520 139790850717440 logging_writer.py:48] [10] global_step=10, grad_norm=0.358193, loss=6.907719
I0608 22:10:41.643020 139825018017600 submission.py:120] 10) loss = 6.908, grad_norm = 0.358
I0608 22:10:42.043941 139790842324736 logging_writer.py:48] [11] global_step=11, grad_norm=0.354949, loss=6.907698
I0608 22:10:42.047919 139825018017600 submission.py:120] 11) loss = 6.908, grad_norm = 0.355
I0608 22:10:42.450739 139790850717440 logging_writer.py:48] [12] global_step=12, grad_norm=0.344942, loss=6.907725
I0608 22:10:42.454561 139825018017600 submission.py:120] 12) loss = 6.908, grad_norm = 0.345
I0608 22:10:42.860826 139790842324736 logging_writer.py:48] [13] global_step=13, grad_norm=0.349741, loss=6.907685
I0608 22:10:42.866270 139825018017600 submission.py:120] 13) loss = 6.908, grad_norm = 0.350
I0608 22:10:43.267539 139790850717440 logging_writer.py:48] [14] global_step=14, grad_norm=0.356915, loss=6.907715
I0608 22:10:43.272295 139825018017600 submission.py:120] 14) loss = 6.908, grad_norm = 0.357
I0608 22:10:43.673950 139790842324736 logging_writer.py:48] [15] global_step=15, grad_norm=0.354523, loss=6.907691
I0608 22:10:43.678297 139825018017600 submission.py:120] 15) loss = 6.908, grad_norm = 0.355
I0608 22:10:44.087145 139790850717440 logging_writer.py:48] [16] global_step=16, grad_norm=0.345421, loss=6.907658
I0608 22:10:44.091007 139825018017600 submission.py:120] 16) loss = 6.908, grad_norm = 0.345
I0608 22:10:44.493301 139790842324736 logging_writer.py:48] [17] global_step=17, grad_norm=0.354006, loss=6.907612
I0608 22:10:44.498265 139825018017600 submission.py:120] 17) loss = 6.908, grad_norm = 0.354
I0608 22:10:44.903404 139790850717440 logging_writer.py:48] [18] global_step=18, grad_norm=0.348294, loss=6.907643
I0608 22:10:44.907397 139825018017600 submission.py:120] 18) loss = 6.908, grad_norm = 0.348
I0608 22:10:45.310491 139790842324736 logging_writer.py:48] [19] global_step=19, grad_norm=0.348787, loss=6.907659
I0608 22:10:45.314750 139825018017600 submission.py:120] 19) loss = 6.908, grad_norm = 0.349
I0608 22:10:45.716230 139790850717440 logging_writer.py:48] [20] global_step=20, grad_norm=0.353186, loss=6.907612
I0608 22:10:45.720960 139825018017600 submission.py:120] 20) loss = 6.908, grad_norm = 0.353
I0608 22:10:46.142183 139790842324736 logging_writer.py:48] [21] global_step=21, grad_norm=0.337916, loss=6.907579
I0608 22:10:46.148530 139825018017600 submission.py:120] 21) loss = 6.908, grad_norm = 0.338
I0608 22:10:46.552553 139790850717440 logging_writer.py:48] [22] global_step=22, grad_norm=0.350576, loss=6.907617
I0608 22:10:46.557770 139825018017600 submission.py:120] 22) loss = 6.908, grad_norm = 0.351
I0608 22:10:46.959213 139790842324736 logging_writer.py:48] [23] global_step=23, grad_norm=0.348328, loss=6.907438
I0608 22:10:46.965133 139825018017600 submission.py:120] 23) loss = 6.907, grad_norm = 0.348
I0608 22:10:47.370357 139790850717440 logging_writer.py:48] [24] global_step=24, grad_norm=0.347655, loss=6.907523
I0608 22:10:47.375321 139825018017600 submission.py:120] 24) loss = 6.908, grad_norm = 0.348
I0608 22:10:47.780913 139790842324736 logging_writer.py:48] [25] global_step=25, grad_norm=0.352519, loss=6.907482
I0608 22:10:47.786031 139825018017600 submission.py:120] 25) loss = 6.907, grad_norm = 0.353
I0608 22:10:48.195438 139790850717440 logging_writer.py:48] [26] global_step=26, grad_norm=0.343967, loss=6.907468
I0608 22:10:48.206187 139825018017600 submission.py:120] 26) loss = 6.907, grad_norm = 0.344
I0608 22:10:48.632829 139790842324736 logging_writer.py:48] [27] global_step=27, grad_norm=0.347681, loss=6.907427
I0608 22:10:48.638490 139825018017600 submission.py:120] 27) loss = 6.907, grad_norm = 0.348
I0608 22:10:49.064538 139790850717440 logging_writer.py:48] [28] global_step=28, grad_norm=0.343169, loss=6.907428
I0608 22:10:49.070436 139825018017600 submission.py:120] 28) loss = 6.907, grad_norm = 0.343
I0608 22:10:49.481895 139790842324736 logging_writer.py:48] [29] global_step=29, grad_norm=0.361419, loss=6.907422
I0608 22:10:49.486371 139825018017600 submission.py:120] 29) loss = 6.907, grad_norm = 0.361
I0608 22:10:49.893853 139790850717440 logging_writer.py:48] [30] global_step=30, grad_norm=0.363777, loss=6.907207
I0608 22:10:49.898780 139825018017600 submission.py:120] 30) loss = 6.907, grad_norm = 0.364
I0608 22:10:50.303380 139790842324736 logging_writer.py:48] [31] global_step=31, grad_norm=0.348038, loss=6.907288
I0608 22:10:50.308308 139825018017600 submission.py:120] 31) loss = 6.907, grad_norm = 0.348
I0608 22:10:50.711344 139790850717440 logging_writer.py:48] [32] global_step=32, grad_norm=0.355178, loss=6.907198
I0608 22:10:50.716652 139825018017600 submission.py:120] 32) loss = 6.907, grad_norm = 0.355
I0608 22:10:51.121045 139790842324736 logging_writer.py:48] [33] global_step=33, grad_norm=0.369071, loss=6.907066
I0608 22:10:51.126544 139825018017600 submission.py:120] 33) loss = 6.907, grad_norm = 0.369
I0608 22:10:51.532442 139790850717440 logging_writer.py:48] [34] global_step=34, grad_norm=0.359705, loss=6.907122
I0608 22:10:51.536403 139825018017600 submission.py:120] 34) loss = 6.907, grad_norm = 0.360
I0608 22:10:51.945958 139790842324736 logging_writer.py:48] [35] global_step=35, grad_norm=0.363644, loss=6.907001
I0608 22:10:51.951497 139825018017600 submission.py:120] 35) loss = 6.907, grad_norm = 0.364
I0608 22:10:52.364495 139790850717440 logging_writer.py:48] [36] global_step=36, grad_norm=0.373185, loss=6.906783
I0608 22:10:52.368971 139825018017600 submission.py:120] 36) loss = 6.907, grad_norm = 0.373
I0608 22:10:52.819467 139790842324736 logging_writer.py:48] [37] global_step=37, grad_norm=0.362615, loss=6.906908
I0608 22:10:52.824657 139825018017600 submission.py:120] 37) loss = 6.907, grad_norm = 0.363
I0608 22:10:53.249977 139790850717440 logging_writer.py:48] [38] global_step=38, grad_norm=0.373283, loss=6.906735
I0608 22:10:53.254201 139825018017600 submission.py:120] 38) loss = 6.907, grad_norm = 0.373
I0608 22:10:53.662980 139790842324736 logging_writer.py:48] [39] global_step=39, grad_norm=0.378647, loss=6.906746
I0608 22:10:53.667346 139825018017600 submission.py:120] 39) loss = 6.907, grad_norm = 0.379
I0608 22:10:54.074710 139790850717440 logging_writer.py:48] [40] global_step=40, grad_norm=0.365797, loss=6.906504
I0608 22:10:54.078692 139825018017600 submission.py:120] 40) loss = 6.907, grad_norm = 0.366
I0608 22:10:54.492320 139790842324736 logging_writer.py:48] [41] global_step=41, grad_norm=0.398726, loss=6.906396
I0608 22:10:54.497699 139825018017600 submission.py:120] 41) loss = 6.906, grad_norm = 0.399
I0608 22:10:54.902355 139790850717440 logging_writer.py:48] [42] global_step=42, grad_norm=0.378436, loss=6.906438
I0608 22:10:54.906792 139825018017600 submission.py:120] 42) loss = 6.906, grad_norm = 0.378
I0608 22:10:55.315222 139790842324736 logging_writer.py:48] [43] global_step=43, grad_norm=0.384597, loss=6.906181
I0608 22:10:55.319708 139825018017600 submission.py:120] 43) loss = 6.906, grad_norm = 0.385
I0608 22:10:55.726556 139790850717440 logging_writer.py:48] [44] global_step=44, grad_norm=0.394902, loss=6.906067
I0608 22:10:55.731739 139825018017600 submission.py:120] 44) loss = 6.906, grad_norm = 0.395
I0608 22:10:56.173316 139790842324736 logging_writer.py:48] [45] global_step=45, grad_norm=0.380660, loss=6.906074
I0608 22:10:56.178649 139825018017600 submission.py:120] 45) loss = 6.906, grad_norm = 0.381
I0608 22:10:56.584028 139790850717440 logging_writer.py:48] [46] global_step=46, grad_norm=0.387756, loss=6.905875
I0608 22:10:56.588380 139825018017600 submission.py:120] 46) loss = 6.906, grad_norm = 0.388
I0608 22:10:57.015677 139790842324736 logging_writer.py:48] [47] global_step=47, grad_norm=0.387927, loss=6.905984
I0608 22:10:57.019582 139825018017600 submission.py:120] 47) loss = 6.906, grad_norm = 0.388
I0608 22:10:57.445593 139790850717440 logging_writer.py:48] [48] global_step=48, grad_norm=0.389189, loss=6.905583
I0608 22:10:57.449844 139825018017600 submission.py:120] 48) loss = 6.906, grad_norm = 0.389
I0608 22:10:57.853159 139790842324736 logging_writer.py:48] [49] global_step=49, grad_norm=0.403147, loss=6.905661
I0608 22:10:57.857821 139825018017600 submission.py:120] 49) loss = 6.906, grad_norm = 0.403
I0608 22:10:58.264546 139790850717440 logging_writer.py:48] [50] global_step=50, grad_norm=0.381635, loss=6.905754
I0608 22:10:58.269266 139825018017600 submission.py:120] 50) loss = 6.906, grad_norm = 0.382
I0608 22:10:58.683057 139790842324736 logging_writer.py:48] [51] global_step=51, grad_norm=0.382265, loss=6.905339
I0608 22:10:58.688359 139825018017600 submission.py:120] 51) loss = 6.905, grad_norm = 0.382
I0608 22:10:59.100346 139790850717440 logging_writer.py:48] [52] global_step=52, grad_norm=0.383604, loss=6.905310
I0608 22:10:59.104620 139825018017600 submission.py:120] 52) loss = 6.905, grad_norm = 0.384
I0608 22:10:59.516812 139790842324736 logging_writer.py:48] [53] global_step=53, grad_norm=0.392231, loss=6.905097
I0608 22:10:59.522233 139825018017600 submission.py:120] 53) loss = 6.905, grad_norm = 0.392
I0608 22:10:59.934368 139790850717440 logging_writer.py:48] [54] global_step=54, grad_norm=0.410274, loss=6.904015
I0608 22:10:59.939500 139825018017600 submission.py:120] 54) loss = 6.904, grad_norm = 0.410
I0608 22:11:00.343761 139790842324736 logging_writer.py:48] [55] global_step=55, grad_norm=0.421827, loss=6.904417
I0608 22:11:00.348567 139825018017600 submission.py:120] 55) loss = 6.904, grad_norm = 0.422
I0608 22:11:00.753851 139790850717440 logging_writer.py:48] [56] global_step=56, grad_norm=0.414838, loss=6.904024
I0608 22:11:00.758561 139825018017600 submission.py:120] 56) loss = 6.904, grad_norm = 0.415
I0608 22:11:01.163528 139790842324736 logging_writer.py:48] [57] global_step=57, grad_norm=0.418404, loss=6.904787
I0608 22:11:01.168829 139825018017600 submission.py:120] 57) loss = 6.905, grad_norm = 0.418
I0608 22:11:01.577275 139790850717440 logging_writer.py:48] [58] global_step=58, grad_norm=0.395930, loss=6.904263
I0608 22:11:01.581115 139825018017600 submission.py:120] 58) loss = 6.904, grad_norm = 0.396
I0608 22:11:01.988308 139790842324736 logging_writer.py:48] [59] global_step=59, grad_norm=0.413201, loss=6.903104
I0608 22:11:01.992331 139825018017600 submission.py:120] 59) loss = 6.903, grad_norm = 0.413
I0608 22:11:02.407490 139790850717440 logging_writer.py:48] [60] global_step=60, grad_norm=0.396629, loss=6.903420
I0608 22:11:02.411907 139825018017600 submission.py:120] 60) loss = 6.903, grad_norm = 0.397
I0608 22:11:02.817131 139790842324736 logging_writer.py:48] [61] global_step=61, grad_norm=0.434248, loss=6.902946
I0608 22:11:02.822067 139825018017600 submission.py:120] 61) loss = 6.903, grad_norm = 0.434
I0608 22:11:03.239367 139790850717440 logging_writer.py:48] [62] global_step=62, grad_norm=0.414415, loss=6.902857
I0608 22:11:03.243459 139825018017600 submission.py:120] 62) loss = 6.903, grad_norm = 0.414
I0608 22:11:03.650523 139790842324736 logging_writer.py:48] [63] global_step=63, grad_norm=0.404850, loss=6.903293
I0608 22:11:03.655660 139825018017600 submission.py:120] 63) loss = 6.903, grad_norm = 0.405
I0608 22:11:04.064246 139790850717440 logging_writer.py:48] [64] global_step=64, grad_norm=0.420438, loss=6.902688
I0608 22:11:04.069014 139825018017600 submission.py:120] 64) loss = 6.903, grad_norm = 0.420
I0608 22:11:04.487867 139790842324736 logging_writer.py:48] [65] global_step=65, grad_norm=0.435764, loss=6.901823
I0608 22:11:04.492824 139825018017600 submission.py:120] 65) loss = 6.902, grad_norm = 0.436
I0608 22:11:04.898570 139790850717440 logging_writer.py:48] [66] global_step=66, grad_norm=0.426262, loss=6.902232
I0608 22:11:04.903601 139825018017600 submission.py:120] 66) loss = 6.902, grad_norm = 0.426
I0608 22:11:05.313229 139790842324736 logging_writer.py:48] [67] global_step=67, grad_norm=0.434475, loss=6.901722
I0608 22:11:05.318217 139825018017600 submission.py:120] 67) loss = 6.902, grad_norm = 0.434
I0608 22:11:05.725328 139790850717440 logging_writer.py:48] [68] global_step=68, grad_norm=0.431766, loss=6.901004
I0608 22:11:05.729895 139825018017600 submission.py:120] 68) loss = 6.901, grad_norm = 0.432
I0608 22:11:06.135999 139790842324736 logging_writer.py:48] [69] global_step=69, grad_norm=0.429773, loss=6.901560
I0608 22:11:06.141883 139825018017600 submission.py:120] 69) loss = 6.902, grad_norm = 0.430
I0608 22:11:06.552516 139790850717440 logging_writer.py:48] [70] global_step=70, grad_norm=0.451108, loss=6.900705
I0608 22:11:06.556991 139825018017600 submission.py:120] 70) loss = 6.901, grad_norm = 0.451
I0608 22:11:06.960972 139790842324736 logging_writer.py:48] [71] global_step=71, grad_norm=0.437618, loss=6.901210
I0608 22:11:06.965634 139825018017600 submission.py:120] 71) loss = 6.901, grad_norm = 0.438
I0608 22:11:07.372610 139790850717440 logging_writer.py:48] [72] global_step=72, grad_norm=0.431456, loss=6.899322
I0608 22:11:07.376561 139825018017600 submission.py:120] 72) loss = 6.899, grad_norm = 0.431
I0608 22:11:07.779546 139790842324736 logging_writer.py:48] [73] global_step=73, grad_norm=0.435898, loss=6.899941
I0608 22:11:07.784664 139825018017600 submission.py:120] 73) loss = 6.900, grad_norm = 0.436
I0608 22:11:08.190338 139790850717440 logging_writer.py:48] [74] global_step=74, grad_norm=0.418847, loss=6.900797
I0608 22:11:08.195554 139825018017600 submission.py:120] 74) loss = 6.901, grad_norm = 0.419
I0608 22:11:08.599879 139790842324736 logging_writer.py:48] [75] global_step=75, grad_norm=0.419008, loss=6.898684
I0608 22:11:08.604652 139825018017600 submission.py:120] 75) loss = 6.899, grad_norm = 0.419
I0608 22:11:09.012973 139790850717440 logging_writer.py:48] [76] global_step=76, grad_norm=0.444208, loss=6.899175
I0608 22:11:09.018225 139825018017600 submission.py:120] 76) loss = 6.899, grad_norm = 0.444
I0608 22:11:09.430075 139790842324736 logging_writer.py:48] [77] global_step=77, grad_norm=0.446829, loss=6.898336
I0608 22:11:09.434767 139825018017600 submission.py:120] 77) loss = 6.898, grad_norm = 0.447
I0608 22:11:09.848895 139790850717440 logging_writer.py:48] [78] global_step=78, grad_norm=0.429591, loss=6.899475
I0608 22:11:09.852995 139825018017600 submission.py:120] 78) loss = 6.899, grad_norm = 0.430
I0608 22:11:10.269753 139790842324736 logging_writer.py:48] [79] global_step=79, grad_norm=0.421632, loss=6.899461
I0608 22:11:10.277488 139825018017600 submission.py:120] 79) loss = 6.899, grad_norm = 0.422
I0608 22:11:10.690175 139790850717440 logging_writer.py:48] [80] global_step=80, grad_norm=0.432771, loss=6.899775
I0608 22:11:10.701157 139825018017600 submission.py:120] 80) loss = 6.900, grad_norm = 0.433
I0608 22:11:11.107009 139790842324736 logging_writer.py:48] [81] global_step=81, grad_norm=0.433874, loss=6.896309
I0608 22:11:11.111496 139825018017600 submission.py:120] 81) loss = 6.896, grad_norm = 0.434
I0608 22:11:11.526952 139790850717440 logging_writer.py:48] [82] global_step=82, grad_norm=0.437373, loss=6.896577
I0608 22:11:11.531509 139825018017600 submission.py:120] 82) loss = 6.897, grad_norm = 0.437
I0608 22:11:11.948483 139790842324736 logging_writer.py:48] [83] global_step=83, grad_norm=0.430279, loss=6.896647
I0608 22:11:11.953719 139825018017600 submission.py:120] 83) loss = 6.897, grad_norm = 0.430
I0608 22:11:12.357207 139790850717440 logging_writer.py:48] [84] global_step=84, grad_norm=0.445451, loss=6.897810
I0608 22:11:12.362523 139825018017600 submission.py:120] 84) loss = 6.898, grad_norm = 0.445
I0608 22:11:12.773746 139790842324736 logging_writer.py:48] [85] global_step=85, grad_norm=0.447051, loss=6.896294
I0608 22:11:12.778633 139825018017600 submission.py:120] 85) loss = 6.896, grad_norm = 0.447
I0608 22:11:13.186839 139790850717440 logging_writer.py:48] [86] global_step=86, grad_norm=0.491848, loss=6.891626
I0608 22:11:13.190855 139825018017600 submission.py:120] 86) loss = 6.892, grad_norm = 0.492
I0608 22:11:13.599686 139790842324736 logging_writer.py:48] [87] global_step=87, grad_norm=0.432312, loss=6.896786
I0608 22:11:13.604846 139825018017600 submission.py:120] 87) loss = 6.897, grad_norm = 0.432
I0608 22:11:14.016909 139790850717440 logging_writer.py:48] [88] global_step=88, grad_norm=0.472990, loss=6.892378
I0608 22:11:14.021886 139825018017600 submission.py:120] 88) loss = 6.892, grad_norm = 0.473
I0608 22:11:14.427907 139790842324736 logging_writer.py:48] [89] global_step=89, grad_norm=0.460696, loss=6.894660
I0608 22:11:14.432959 139825018017600 submission.py:120] 89) loss = 6.895, grad_norm = 0.461
I0608 22:11:14.839273 139790850717440 logging_writer.py:48] [90] global_step=90, grad_norm=0.473017, loss=6.893215
I0608 22:11:14.843915 139825018017600 submission.py:120] 90) loss = 6.893, grad_norm = 0.473
I0608 22:11:15.250787 139790842324736 logging_writer.py:48] [91] global_step=91, grad_norm=0.456558, loss=6.893160
I0608 22:11:15.255000 139825018017600 submission.py:120] 91) loss = 6.893, grad_norm = 0.457
I0608 22:11:15.675140 139790850717440 logging_writer.py:48] [92] global_step=92, grad_norm=0.440081, loss=6.891523
I0608 22:11:15.680181 139825018017600 submission.py:120] 92) loss = 6.892, grad_norm = 0.440
I0608 22:11:16.099819 139790842324736 logging_writer.py:48] [93] global_step=93, grad_norm=0.436213, loss=6.894774
I0608 22:11:16.103947 139825018017600 submission.py:120] 93) loss = 6.895, grad_norm = 0.436
I0608 22:11:16.521192 139790850717440 logging_writer.py:48] [94] global_step=94, grad_norm=0.431567, loss=6.892516
I0608 22:11:16.525005 139825018017600 submission.py:120] 94) loss = 6.893, grad_norm = 0.432
I0608 22:11:16.935058 139790842324736 logging_writer.py:48] [95] global_step=95, grad_norm=0.445859, loss=6.892904
I0608 22:11:16.939891 139825018017600 submission.py:120] 95) loss = 6.893, grad_norm = 0.446
I0608 22:11:17.350194 139790850717440 logging_writer.py:48] [96] global_step=96, grad_norm=0.463661, loss=6.888527
I0608 22:11:17.354775 139825018017600 submission.py:120] 96) loss = 6.889, grad_norm = 0.464
I0608 22:11:17.760992 139790842324736 logging_writer.py:48] [97] global_step=97, grad_norm=0.471211, loss=6.891649
I0608 22:11:17.766722 139825018017600 submission.py:120] 97) loss = 6.892, grad_norm = 0.471
I0608 22:11:18.173804 139790850717440 logging_writer.py:48] [98] global_step=98, grad_norm=0.455821, loss=6.890367
I0608 22:11:18.177684 139825018017600 submission.py:120] 98) loss = 6.890, grad_norm = 0.456
I0608 22:11:18.584201 139790842324736 logging_writer.py:48] [99] global_step=99, grad_norm=0.436411, loss=6.891903
I0608 22:11:18.589152 139825018017600 submission.py:120] 99) loss = 6.892, grad_norm = 0.436
I0608 22:11:19.002466 139790850717440 logging_writer.py:48] [100] global_step=100, grad_norm=0.446173, loss=6.892946
I0608 22:11:19.007312 139825018017600 submission.py:120] 100) loss = 6.893, grad_norm = 0.446
I0608 22:13:59.974590 139790842324736 logging_writer.py:48] [500] global_step=500, grad_norm=0.578909, loss=6.690518
I0608 22:13:59.979565 139825018017600 submission.py:120] 500) loss = 6.691, grad_norm = 0.579
I0608 22:17:20.487173 139790850717440 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.884851, loss=6.343771
I0608 22:17:20.492129 139825018017600 submission.py:120] 1000) loss = 6.344, grad_norm = 0.885
I0608 22:17:37.364667 139825018017600 spec.py:298] Evaluating on the training split.
I0608 22:18:20.041223 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 22:19:04.073564 139825018017600 spec.py:326] Evaluating on the test split.
I0608 22:19:05.541365 139825018017600 submission_runner.py:419] Time since start: 640.40s, 	Step: 1043, 	{'train/accuracy': 0.03525390625, 'train/loss': 5.9252862548828125, 'validation/accuracy': 0.03476, 'validation/loss': 5.952164375, 'validation/num_examples': 50000, 'test/accuracy': 0.0247, 'test/loss': 6.0632984375, 'test/num_examples': 10000, 'score': 426.0211238861084, 'total_duration': 640.3967559337616, 'accumulated_submission_time': 426.0211238861084, 'accumulated_eval_time': 213.74573588371277, 'accumulated_logging_time': 0.028234481811523438}
I0608 22:19:05.551538 139780998383360 logging_writer.py:48] [1043] accumulated_eval_time=213.745736, accumulated_logging_time=0.028234, accumulated_submission_time=426.021124, global_step=1043, preemption_count=0, score=426.021124, test/accuracy=0.024700, test/loss=6.063298, test/num_examples=10000, total_duration=640.396756, train/accuracy=0.035254, train/loss=5.925286, validation/accuracy=0.034760, validation/loss=5.952164, validation/num_examples=50000
I0608 22:22:11.635725 139781006776064 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.056097, loss=6.179373
I0608 22:22:11.641261 139825018017600 submission.py:120] 1500) loss = 6.179, grad_norm = 1.056
I0608 22:25:33.664107 139780998383360 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.922162, loss=6.178374
I0608 22:25:33.670174 139825018017600 submission.py:120] 2000) loss = 6.178, grad_norm = 0.922
I0608 22:26:05.825462 139825018017600 spec.py:298] Evaluating on the training split.
I0608 22:26:49.420392 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 22:27:41.937163 139825018017600 spec.py:326] Evaluating on the test split.
I0608 22:27:43.366256 139825018017600 submission_runner.py:419] Time since start: 1158.22s, 	Step: 2081, 	{'train/accuracy': 0.08458984375, 'train/loss': 5.2411346435546875, 'validation/accuracy': 0.0777, 'validation/loss': 5.29019375, 'validation/num_examples': 50000, 'test/accuracy': 0.061, 'test/loss': 5.504282421875, 'test/num_examples': 10000, 'score': 845.6689677238464, 'total_duration': 1158.2216303348541, 'accumulated_submission_time': 845.6689677238464, 'accumulated_eval_time': 311.28668761253357, 'accumulated_logging_time': 0.04751253128051758}
I0608 22:27:43.376698 139781006776064 logging_writer.py:48] [2081] accumulated_eval_time=311.286688, accumulated_logging_time=0.047513, accumulated_submission_time=845.668968, global_step=2081, preemption_count=0, score=845.668968, test/accuracy=0.061000, test/loss=5.504282, test/num_examples=10000, total_duration=1158.221630, train/accuracy=0.084590, train/loss=5.241135, validation/accuracy=0.077700, validation/loss=5.290194, validation/num_examples=50000
I0608 22:30:32.911861 139780998383360 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.950116, loss=5.875902
I0608 22:30:32.916288 139825018017600 submission.py:120] 2500) loss = 5.876, grad_norm = 0.950
I0608 22:33:57.190489 139781006776064 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.060385, loss=6.020647
I0608 22:33:57.194979 139825018017600 submission.py:120] 3000) loss = 6.021, grad_norm = 1.060
I0608 22:34:43.554119 139825018017600 spec.py:298] Evaluating on the training split.
I0608 22:35:28.108234 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 22:36:21.650009 139825018017600 spec.py:326] Evaluating on the test split.
I0608 22:36:23.076240 139825018017600 submission_runner.py:419] Time since start: 1677.93s, 	Step: 3116, 	{'train/accuracy': 0.13943359375, 'train/loss': 4.692435913085937, 'validation/accuracy': 0.13024, 'validation/loss': 4.758155, 'validation/num_examples': 50000, 'test/accuracy': 0.0977, 'test/loss': 5.05568203125, 'test/num_examples': 10000, 'score': 1265.218917608261, 'total_duration': 1677.9315848350525, 'accumulated_submission_time': 1265.218917608261, 'accumulated_eval_time': 410.8088412284851, 'accumulated_logging_time': 0.06659698486328125}
I0608 22:36:23.091513 139780998383360 logging_writer.py:48] [3116] accumulated_eval_time=410.808841, accumulated_logging_time=0.066597, accumulated_submission_time=1265.218918, global_step=3116, preemption_count=0, score=1265.218918, test/accuracy=0.097700, test/loss=5.055682, test/num_examples=10000, total_duration=1677.931585, train/accuracy=0.139434, train/loss=4.692436, validation/accuracy=0.130240, validation/loss=4.758155, validation/num_examples=50000
I0608 22:38:58.748155 139781006776064 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.124974, loss=5.811513
I0608 22:38:58.753498 139825018017600 submission.py:120] 3500) loss = 5.812, grad_norm = 1.125
I0608 22:42:22.612285 139780998383360 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.713057, loss=5.960130
I0608 22:42:22.617399 139825018017600 submission.py:120] 4000) loss = 5.960, grad_norm = 0.713
I0608 22:43:23.397066 139825018017600 spec.py:298] Evaluating on the training split.
I0608 22:44:07.550649 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 22:44:53.071803 139825018017600 spec.py:326] Evaluating on the test split.
I0608 22:44:54.496357 139825018017600 submission_runner.py:419] Time since start: 2189.35s, 	Step: 4151, 	{'train/accuracy': 0.18830078125, 'train/loss': 4.266922302246094, 'validation/accuracy': 0.17488, 'validation/loss': 4.3579521875, 'validation/num_examples': 50000, 'test/accuracy': 0.1331, 'test/loss': 4.7072484375, 'test/num_examples': 10000, 'score': 1684.8896028995514, 'total_duration': 2189.3517332077026, 'accumulated_submission_time': 1684.8896028995514, 'accumulated_eval_time': 501.90835332870483, 'accumulated_logging_time': 0.09024524688720703}
I0608 22:44:54.506610 139781006776064 logging_writer.py:48] [4151] accumulated_eval_time=501.908353, accumulated_logging_time=0.090245, accumulated_submission_time=1684.889603, global_step=4151, preemption_count=0, score=1684.889603, test/accuracy=0.133100, test/loss=4.707248, test/num_examples=10000, total_duration=2189.351733, train/accuracy=0.188301, train/loss=4.266922, validation/accuracy=0.174880, validation/loss=4.357952, validation/num_examples=50000
I0608 22:47:16.194090 139780998383360 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.830370, loss=5.440645
I0608 22:47:16.198974 139825018017600 submission.py:120] 4500) loss = 5.441, grad_norm = 0.830
I0608 22:50:37.647046 139781006776064 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.744339, loss=5.460612
I0608 22:50:37.651355 139825018017600 submission.py:120] 5000) loss = 5.461, grad_norm = 0.744
I0608 22:51:54.548285 139825018017600 spec.py:298] Evaluating on the training split.
I0608 22:52:38.920102 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 22:53:24.228623 139825018017600 spec.py:326] Evaluating on the test split.
I0608 22:53:25.655988 139825018017600 submission_runner.py:419] Time since start: 2700.51s, 	Step: 5186, 	{'train/accuracy': 0.2512890625, 'train/loss': 3.8073284912109373, 'validation/accuracy': 0.2286, 'validation/loss': 3.92833, 'validation/num_examples': 50000, 'test/accuracy': 0.1762, 'test/loss': 4.34499375, 'test/num_examples': 10000, 'score': 2104.3070023059845, 'total_duration': 2700.5095207691193, 'accumulated_submission_time': 2104.3070023059845, 'accumulated_eval_time': 593.0142815113068, 'accumulated_logging_time': 0.10902810096740723}
I0608 22:53:25.667580 139780998383360 logging_writer.py:48] [5186] accumulated_eval_time=593.014282, accumulated_logging_time=0.109028, accumulated_submission_time=2104.307002, global_step=5186, preemption_count=0, score=2104.307002, test/accuracy=0.176200, test/loss=4.344994, test/num_examples=10000, total_duration=2700.509521, train/accuracy=0.251289, train/loss=3.807328, validation/accuracy=0.228600, validation/loss=3.928330, validation/num_examples=50000
I0608 22:55:32.821196 139781006776064 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.967079, loss=5.396366
I0608 22:55:32.826711 139825018017600 submission.py:120] 5500) loss = 5.396, grad_norm = 0.967
I0608 22:58:54.635815 139780998383360 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.832779, loss=5.257918
I0608 22:58:54.642817 139825018017600 submission.py:120] 6000) loss = 5.258, grad_norm = 0.833
I0608 23:00:25.729286 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:01:12.630695 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:01:59.178158 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:02:00.604668 139825018017600 submission_runner.py:419] Time since start: 3215.46s, 	Step: 6227, 	{'train/accuracy': 0.29087890625, 'train/loss': 3.54396728515625, 'validation/accuracy': 0.2656, 'validation/loss': 3.681008125, 'validation/num_examples': 50000, 'test/accuracy': 0.2034, 'test/loss': 4.134701953125, 'test/num_examples': 10000, 'score': 2523.7379727363586, 'total_duration': 3215.46004486084, 'accumulated_submission_time': 2523.7379727363586, 'accumulated_eval_time': 687.8897821903229, 'accumulated_logging_time': 0.13066864013671875}
I0608 23:02:00.615945 139781006776064 logging_writer.py:48] [6227] accumulated_eval_time=687.889782, accumulated_logging_time=0.130669, accumulated_submission_time=2523.737973, global_step=6227, preemption_count=0, score=2523.737973, test/accuracy=0.203400, test/loss=4.134702, test/num_examples=10000, total_duration=3215.460045, train/accuracy=0.290879, train/loss=3.543967, validation/accuracy=0.265600, validation/loss=3.681008, validation/num_examples=50000
I0608 23:03:53.137777 139780998383360 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.830931, loss=5.392386
I0608 23:03:53.143301 139825018017600 submission.py:120] 6500) loss = 5.392, grad_norm = 0.831
I0608 23:07:15.471259 139781006776064 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.768119, loss=5.437310
I0608 23:07:15.475996 139825018017600 submission.py:120] 7000) loss = 5.437, grad_norm = 0.768
I0608 23:09:00.931992 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:09:44.664394 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:10:29.879995 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:10:31.307069 139825018017600 submission_runner.py:419] Time since start: 3726.16s, 	Step: 7262, 	{'train/accuracy': 0.32767578125, 'train/loss': 3.2986932373046876, 'validation/accuracy': 0.29908, 'validation/loss': 3.453194375, 'validation/num_examples': 50000, 'test/accuracy': 0.236, 'test/loss': 3.92604453125, 'test/num_examples': 10000, 'score': 2943.4246406555176, 'total_duration': 3726.1624205112457, 'accumulated_submission_time': 2943.4246406555176, 'accumulated_eval_time': 778.2649435997009, 'accumulated_logging_time': 0.1525413990020752}
I0608 23:10:31.318423 139780998383360 logging_writer.py:48] [7262] accumulated_eval_time=778.264944, accumulated_logging_time=0.152541, accumulated_submission_time=2943.424641, global_step=7262, preemption_count=0, score=2943.424641, test/accuracy=0.236000, test/loss=3.926045, test/num_examples=10000, total_duration=3726.162421, train/accuracy=0.327676, train/loss=3.298693, validation/accuracy=0.299080, validation/loss=3.453194, validation/num_examples=50000
I0608 23:12:07.432037 139781006776064 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.785860, loss=5.023702
I0608 23:12:07.436055 139825018017600 submission.py:120] 7500) loss = 5.024, grad_norm = 0.786
I0608 23:15:31.815651 139780998383360 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.659900, loss=4.920557
I0608 23:15:31.820827 139825018017600 submission.py:120] 8000) loss = 4.921, grad_norm = 0.660
I0608 23:17:31.595490 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:18:16.178997 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:19:01.891595 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:19:03.320498 139825018017600 submission_runner.py:419] Time since start: 4238.18s, 	Step: 8297, 	{'train/accuracy': 0.36814453125, 'train/loss': 3.04942138671875, 'validation/accuracy': 0.33464, 'validation/loss': 3.2181196875, 'validation/num_examples': 50000, 'test/accuracy': 0.2613, 'test/loss': 3.714704296875, 'test/num_examples': 10000, 'score': 3363.0720682144165, 'total_duration': 4238.1758189201355, 'accumulated_submission_time': 3363.0720682144165, 'accumulated_eval_time': 869.9900667667389, 'accumulated_logging_time': 0.1786205768585205}
I0608 23:19:03.330542 139781006776064 logging_writer.py:48] [8297] accumulated_eval_time=869.990067, accumulated_logging_time=0.178621, accumulated_submission_time=3363.072068, global_step=8297, preemption_count=0, score=3363.072068, test/accuracy=0.261300, test/loss=3.714704, test/num_examples=10000, total_duration=4238.175819, train/accuracy=0.368145, train/loss=3.049421, validation/accuracy=0.334640, validation/loss=3.218120, validation/num_examples=50000
I0608 23:20:25.616489 139780998383360 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.644648, loss=4.944653
I0608 23:20:25.621467 139825018017600 submission.py:120] 8500) loss = 4.945, grad_norm = 0.645
I0608 23:23:49.704270 139781006776064 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.677804, loss=4.831908
I0608 23:23:49.713990 139825018017600 submission.py:120] 9000) loss = 4.832, grad_norm = 0.678
I0608 23:26:03.610557 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:26:48.298877 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:27:34.382393 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:27:35.810314 139825018017600 submission_runner.py:419] Time since start: 4750.67s, 	Step: 9332, 	{'train/accuracy': 0.41015625, 'train/loss': 2.7904595947265625, 'validation/accuracy': 0.37416, 'validation/loss': 2.9712965625, 'validation/num_examples': 50000, 'test/accuracy': 0.2896, 'test/loss': 3.518267578125, 'test/num_examples': 10000, 'score': 3782.7250921726227, 'total_duration': 4750.665675878525, 'accumulated_submission_time': 3782.7250921726227, 'accumulated_eval_time': 962.189864397049, 'accumulated_logging_time': 0.19685101509094238}
I0608 23:27:35.820410 139780998383360 logging_writer.py:48] [9332] accumulated_eval_time=962.189864, accumulated_logging_time=0.196851, accumulated_submission_time=3782.725092, global_step=9332, preemption_count=0, score=3782.725092, test/accuracy=0.289600, test/loss=3.518268, test/num_examples=10000, total_duration=4750.665676, train/accuracy=0.410156, train/loss=2.790460, validation/accuracy=0.374160, validation/loss=2.971297, validation/num_examples=50000
I0608 23:28:44.011878 139781006776064 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.759267, loss=4.466084
I0608 23:28:44.017916 139825018017600 submission.py:120] 9500) loss = 4.466, grad_norm = 0.759
I0608 23:32:05.840973 139780998383360 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.602555, loss=4.544788
I0608 23:32:05.849505 139825018017600 submission.py:120] 10000) loss = 4.545, grad_norm = 0.603
I0608 23:34:35.827656 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:35:20.814997 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:36:07.069410 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:36:08.495440 139825018017600 submission_runner.py:419] Time since start: 5263.35s, 	Step: 10368, 	{'train/accuracy': 0.4385546875, 'train/loss': 2.6523980712890625, 'validation/accuracy': 0.4005, 'validation/loss': 2.84266625, 'validation/num_examples': 50000, 'test/accuracy': 0.3097, 'test/loss': 3.382415234375, 'test/num_examples': 10000, 'score': 4202.115720510483, 'total_duration': 5263.350814342499, 'accumulated_submission_time': 4202.115720510483, 'accumulated_eval_time': 1054.8578073978424, 'accumulated_logging_time': 0.215057373046875}
I0608 23:36:08.506031 139781006776064 logging_writer.py:48] [10368] accumulated_eval_time=1054.857807, accumulated_logging_time=0.215057, accumulated_submission_time=4202.115721, global_step=10368, preemption_count=0, score=4202.115721, test/accuracy=0.309700, test/loss=3.382415, test/num_examples=10000, total_duration=5263.350814, train/accuracy=0.438555, train/loss=2.652398, validation/accuracy=0.400500, validation/loss=2.842666, validation/num_examples=50000
I0608 23:37:02.197308 139780998383360 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.664734, loss=4.327261
I0608 23:37:02.203135 139825018017600 submission.py:120] 10500) loss = 4.327, grad_norm = 0.665
I0608 23:40:24.300840 139781006776064 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.582751, loss=4.858854
I0608 23:40:24.307451 139825018017600 submission.py:120] 11000) loss = 4.859, grad_norm = 0.583
I0608 23:43:08.778743 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:43:53.637789 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:44:39.707632 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:44:41.132850 139825018017600 submission_runner.py:419] Time since start: 5775.99s, 	Step: 11404, 	{'train/accuracy': 0.473125, 'train/loss': 2.4494837951660156, 'validation/accuracy': 0.43388, 'validation/loss': 2.651764375, 'validation/num_examples': 50000, 'test/accuracy': 0.3374, 'test/loss': 3.2343755859375, 'test/num_examples': 10000, 'score': 4621.759845256805, 'total_duration': 5775.9882254600525, 'accumulated_submission_time': 4621.759845256805, 'accumulated_eval_time': 1147.2120311260223, 'accumulated_logging_time': 0.23401141166687012}
I0608 23:44:41.143465 139780998383360 logging_writer.py:48] [11404] accumulated_eval_time=1147.212031, accumulated_logging_time=0.234011, accumulated_submission_time=4621.759845, global_step=11404, preemption_count=0, score=4621.759845, test/accuracy=0.337400, test/loss=3.234376, test/num_examples=10000, total_duration=5775.988225, train/accuracy=0.473125, train/loss=2.449484, validation/accuracy=0.433880, validation/loss=2.651764, validation/num_examples=50000
I0608 23:45:20.333014 139781006776064 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.620172, loss=4.321692
I0608 23:45:20.337545 139825018017600 submission.py:120] 11500) loss = 4.322, grad_norm = 0.620
I0608 23:48:42.604152 139780998383360 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.692055, loss=4.408984
I0608 23:48:42.609107 139825018017600 submission.py:120] 12000) loss = 4.409, grad_norm = 0.692
I0608 23:51:41.197134 139825018017600 spec.py:298] Evaluating on the training split.
I0608 23:52:26.111361 139825018017600 spec.py:310] Evaluating on the validation split.
I0608 23:53:11.624521 139825018017600 spec.py:326] Evaluating on the test split.
I0608 23:53:13.053385 139825018017600 submission_runner.py:419] Time since start: 6287.91s, 	Step: 12444, 	{'train/accuracy': 0.50552734375, 'train/loss': 2.2489689636230468, 'validation/accuracy': 0.4613, 'validation/loss': 2.4632825, 'validation/num_examples': 50000, 'test/accuracy': 0.3555, 'test/loss': 3.07796484375, 'test/num_examples': 10000, 'score': 5041.180806398392, 'total_duration': 6287.908731460571, 'accumulated_submission_time': 5041.180806398392, 'accumulated_eval_time': 1239.0683307647705, 'accumulated_logging_time': 0.25696420669555664}
I0608 23:53:13.064294 139781006776064 logging_writer.py:48] [12444] accumulated_eval_time=1239.068331, accumulated_logging_time=0.256964, accumulated_submission_time=5041.180806, global_step=12444, preemption_count=0, score=5041.180806, test/accuracy=0.355500, test/loss=3.077965, test/num_examples=10000, total_duration=6287.908731, train/accuracy=0.505527, train/loss=2.248969, validation/accuracy=0.461300, validation/loss=2.463283, validation/num_examples=50000
I0608 23:53:36.073667 139780998383360 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.728549, loss=4.242561
I0608 23:53:36.080046 139825018017600 submission.py:120] 12500) loss = 4.243, grad_norm = 0.729
I0608 23:56:59.976162 139781006776064 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.622648, loss=4.581789
I0608 23:56:59.981278 139825018017600 submission.py:120] 13000) loss = 4.582, grad_norm = 0.623
I0609 00:00:13.231526 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:00:58.036010 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:01:43.730439 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:01:45.157096 139825018017600 submission_runner.py:419] Time since start: 6800.01s, 	Step: 13479, 	{'train/accuracy': 0.52431640625, 'train/loss': 2.19983642578125, 'validation/accuracy': 0.47698, 'validation/loss': 2.4166659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3724, 'test/loss': 3.023970703125, 'test/num_examples': 10000, 'score': 5460.721163988113, 'total_duration': 6800.012439727783, 'accumulated_submission_time': 5460.721163988113, 'accumulated_eval_time': 1330.9939250946045, 'accumulated_logging_time': 0.28195977210998535}
I0609 00:01:45.168370 139780998383360 logging_writer.py:48] [13479] accumulated_eval_time=1330.993925, accumulated_logging_time=0.281960, accumulated_submission_time=5460.721164, global_step=13479, preemption_count=0, score=5460.721164, test/accuracy=0.372400, test/loss=3.023971, test/num_examples=10000, total_duration=6800.012440, train/accuracy=0.524316, train/loss=2.199836, validation/accuracy=0.476980, validation/loss=2.416666, validation/num_examples=50000
I0609 00:01:53.973115 139781006776064 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.587003, loss=4.216387
I0609 00:01:53.977909 139825018017600 submission.py:120] 13500) loss = 4.216, grad_norm = 0.587
I0609 00:05:17.176495 139780998383360 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.730963, loss=4.353352
I0609 00:05:17.181742 139825018017600 submission.py:120] 14000) loss = 4.353, grad_norm = 0.731
I0609 00:08:39.227544 139781006776064 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.768182, loss=4.220268
I0609 00:08:39.233891 139825018017600 submission.py:120] 14500) loss = 4.220, grad_norm = 0.768
I0609 00:08:45.279653 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:09:30.078422 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:10:16.021679 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:10:17.450512 139825018017600 submission_runner.py:419] Time since start: 7312.31s, 	Step: 14516, 	{'train/accuracy': 0.5487109375, 'train/loss': 2.0630027770996096, 'validation/accuracy': 0.504, 'validation/loss': 2.2928159375, 'validation/num_examples': 50000, 'test/accuracy': 0.3934, 'test/loss': 2.8893359375, 'test/num_examples': 10000, 'score': 5880.205817222595, 'total_duration': 7312.305900812149, 'accumulated_submission_time': 5880.205817222595, 'accumulated_eval_time': 1423.1648411750793, 'accumulated_logging_time': 0.3029608726501465}
I0609 00:10:17.461681 139780998383360 logging_writer.py:48] [14516] accumulated_eval_time=1423.164841, accumulated_logging_time=0.302961, accumulated_submission_time=5880.205817, global_step=14516, preemption_count=0, score=5880.205817, test/accuracy=0.393400, test/loss=2.889336, test/num_examples=10000, total_duration=7312.305901, train/accuracy=0.548711, train/loss=2.063003, validation/accuracy=0.504000, validation/loss=2.292816, validation/num_examples=50000
I0609 00:13:32.512442 139781006776064 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.728520, loss=4.341441
I0609 00:13:32.519553 139825018017600 submission.py:120] 15000) loss = 4.341, grad_norm = 0.729
I0609 00:16:56.682720 139780998383360 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.584279, loss=4.649786
I0609 00:16:56.688531 139825018017600 submission.py:120] 15500) loss = 4.650, grad_norm = 0.584
I0609 00:17:17.799451 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:18:03.122142 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:18:49.370270 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:18:50.795076 139825018017600 submission_runner.py:419] Time since start: 7825.65s, 	Step: 15553, 	{'train/accuracy': 0.56396484375, 'train/loss': 1.9813763427734374, 'validation/accuracy': 0.51804, 'validation/loss': 2.20847796875, 'validation/num_examples': 50000, 'test/accuracy': 0.4026, 'test/loss': 2.8403578125, 'test/num_examples': 10000, 'score': 6299.921359062195, 'total_duration': 7825.650443553925, 'accumulated_submission_time': 6299.921359062195, 'accumulated_eval_time': 1516.1604175567627, 'accumulated_logging_time': 0.3220236301422119}
I0609 00:18:50.805681 139781006776064 logging_writer.py:48] [15553] accumulated_eval_time=1516.160418, accumulated_logging_time=0.322024, accumulated_submission_time=6299.921359, global_step=15553, preemption_count=0, score=6299.921359, test/accuracy=0.402600, test/loss=2.840358, test/num_examples=10000, total_duration=7825.650444, train/accuracy=0.563965, train/loss=1.981376, validation/accuracy=0.518040, validation/loss=2.208478, validation/num_examples=50000
I0609 00:21:51.459768 139780998383360 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.820534, loss=3.909390
I0609 00:21:51.466618 139825018017600 submission.py:120] 16000) loss = 3.909, grad_norm = 0.821
I0609 00:25:15.200464 139781006776064 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.656749, loss=3.626240
I0609 00:25:15.206256 139825018017600 submission.py:120] 16500) loss = 3.626, grad_norm = 0.657
I0609 00:25:51.138002 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:26:36.397626 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:27:22.488910 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:27:23.913801 139825018017600 submission_runner.py:419] Time since start: 8338.77s, 	Step: 16590, 	{'train/accuracy': 0.57830078125, 'train/loss': 1.9079759216308594, 'validation/accuracy': 0.52944, 'validation/loss': 2.1546125, 'validation/num_examples': 50000, 'test/accuracy': 0.4177, 'test/loss': 2.7754787109375, 'test/num_examples': 10000, 'score': 6719.635491371155, 'total_duration': 8338.769189596176, 'accumulated_submission_time': 6719.635491371155, 'accumulated_eval_time': 1608.9363205432892, 'accumulated_logging_time': 0.34078454971313477}
I0609 00:27:23.925259 139780998383360 logging_writer.py:48] [16590] accumulated_eval_time=1608.936321, accumulated_logging_time=0.340785, accumulated_submission_time=6719.635491, global_step=16590, preemption_count=0, score=6719.635491, test/accuracy=0.417700, test/loss=2.775479, test/num_examples=10000, total_duration=8338.769190, train/accuracy=0.578301, train/loss=1.907976, validation/accuracy=0.529440, validation/loss=2.154612, validation/num_examples=50000
I0609 00:30:10.001052 139781006776064 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.689640, loss=4.379780
I0609 00:30:10.007387 139825018017600 submission.py:120] 17000) loss = 4.380, grad_norm = 0.690
I0609 00:33:31.410471 139780998383360 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.752414, loss=3.742535
I0609 00:33:31.416390 139825018017600 submission.py:120] 17500) loss = 3.743, grad_norm = 0.752
I0609 00:34:23.919885 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:35:08.933364 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:35:54.709543 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:35:56.135069 139825018017600 submission_runner.py:419] Time since start: 8850.99s, 	Step: 17626, 	{'train/accuracy': 0.5969921875, 'train/loss': 1.8343675231933594, 'validation/accuracy': 0.54342, 'validation/loss': 2.0753528125, 'validation/num_examples': 50000, 'test/accuracy': 0.425, 'test/loss': 2.69705390625, 'test/num_examples': 10000, 'score': 7139.009434938431, 'total_duration': 8850.99041056633, 'accumulated_submission_time': 7139.009434938431, 'accumulated_eval_time': 1701.1516015529633, 'accumulated_logging_time': 0.36162590980529785}
I0609 00:35:56.146500 139781006776064 logging_writer.py:48] [17626] accumulated_eval_time=1701.151602, accumulated_logging_time=0.361626, accumulated_submission_time=7139.009435, global_step=17626, preemption_count=0, score=7139.009435, test/accuracy=0.425000, test/loss=2.697054, test/num_examples=10000, total_duration=8850.990411, train/accuracy=0.596992, train/loss=1.834368, validation/accuracy=0.543420, validation/loss=2.075353, validation/num_examples=50000
I0609 00:38:27.972051 139780998383360 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.750475, loss=4.162517
I0609 00:38:27.978757 139825018017600 submission.py:120] 18000) loss = 4.163, grad_norm = 0.750
I0609 00:41:49.798053 139781006776064 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.730500, loss=3.596441
I0609 00:41:49.803475 139825018017600 submission.py:120] 18500) loss = 3.596, grad_norm = 0.730
I0609 00:42:56.248393 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:43:41.242283 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:44:27.408222 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:44:28.834601 139825018017600 submission_runner.py:419] Time since start: 9363.69s, 	Step: 18666, 	{'train/accuracy': 0.60541015625, 'train/loss': 1.7813375854492188, 'validation/accuracy': 0.55394, 'validation/loss': 2.0304234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4347, 'test/loss': 2.6676984375, 'test/num_examples': 10000, 'score': 7558.478419780731, 'total_duration': 9363.689949035645, 'accumulated_submission_time': 7558.478419780731, 'accumulated_eval_time': 1793.7379922866821, 'accumulated_logging_time': 0.3815150260925293}
I0609 00:44:28.850697 139780998383360 logging_writer.py:48] [18666] accumulated_eval_time=1793.737992, accumulated_logging_time=0.381515, accumulated_submission_time=7558.478420, global_step=18666, preemption_count=0, score=7558.478420, test/accuracy=0.434700, test/loss=2.667698, test/num_examples=10000, total_duration=9363.689949, train/accuracy=0.605410, train/loss=1.781338, validation/accuracy=0.553940, validation/loss=2.030423, validation/num_examples=50000
I0609 00:46:45.901333 139781006776064 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.755571, loss=3.943470
I0609 00:46:45.906878 139825018017600 submission.py:120] 19000) loss = 3.943, grad_norm = 0.756
I0609 00:50:08.409132 139780998383360 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.785929, loss=3.685159
I0609 00:50:08.414397 139825018017600 submission.py:120] 19500) loss = 3.685, grad_norm = 0.786
I0609 00:51:28.941416 139825018017600 spec.py:298] Evaluating on the training split.
I0609 00:52:13.834323 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 00:52:59.985028 139825018017600 spec.py:326] Evaluating on the test split.
I0609 00:53:01.412876 139825018017600 submission_runner.py:419] Time since start: 9876.27s, 	Step: 19701, 	{'train/accuracy': 0.6208984375, 'train/loss': 1.6827462768554688, 'validation/accuracy': 0.56624, 'validation/loss': 1.938425625, 'validation/num_examples': 50000, 'test/accuracy': 0.4461, 'test/loss': 2.5860849609375, 'test/num_examples': 10000, 'score': 7977.937218427658, 'total_duration': 9876.268121480942, 'accumulated_submission_time': 7977.937218427658, 'accumulated_eval_time': 1886.2094955444336, 'accumulated_logging_time': 0.4095165729522705}
I0609 00:53:01.426958 139781006776064 logging_writer.py:48] [19701] accumulated_eval_time=1886.209496, accumulated_logging_time=0.409517, accumulated_submission_time=7977.937218, global_step=19701, preemption_count=0, score=7977.937218, test/accuracy=0.446100, test/loss=2.586085, test/num_examples=10000, total_duration=9876.268121, train/accuracy=0.620898, train/loss=1.682746, validation/accuracy=0.566240, validation/loss=1.938426, validation/num_examples=50000
I0609 00:55:02.140978 139780998383360 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.759216, loss=3.928205
I0609 00:55:02.147374 139825018017600 submission.py:120] 20000) loss = 3.928, grad_norm = 0.759
I0609 00:58:26.322857 139781006776064 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.756409, loss=4.213838
I0609 00:58:26.328464 139825018017600 submission.py:120] 20500) loss = 4.214, grad_norm = 0.756
I0609 01:00:01.775523 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:00:46.815868 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:01:33.438646 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:01:34.865986 139825018017600 submission_runner.py:419] Time since start: 10389.72s, 	Step: 20737, 	{'train/accuracy': 0.63453125, 'train/loss': 1.65840087890625, 'validation/accuracy': 0.57816, 'validation/loss': 1.906141875, 'validation/num_examples': 50000, 'test/accuracy': 0.453, 'test/loss': 2.53839375, 'test/num_examples': 10000, 'score': 8397.652898788452, 'total_duration': 10389.721275091171, 'accumulated_submission_time': 8397.652898788452, 'accumulated_eval_time': 1979.299936056137, 'accumulated_logging_time': 0.4332573413848877}
I0609 01:01:34.877940 139780998383360 logging_writer.py:48] [20737] accumulated_eval_time=1979.299936, accumulated_logging_time=0.433257, accumulated_submission_time=8397.652899, global_step=20737, preemption_count=0, score=8397.652899, test/accuracy=0.453000, test/loss=2.538394, test/num_examples=10000, total_duration=10389.721275, train/accuracy=0.634531, train/loss=1.658401, validation/accuracy=0.578160, validation/loss=1.906142, validation/num_examples=50000
I0609 01:03:21.221125 139781006776064 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.717090, loss=3.989672
I0609 01:03:21.228437 139825018017600 submission.py:120] 21000) loss = 3.990, grad_norm = 0.717
I0609 01:06:44.925457 139780998383360 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.758753, loss=3.706154
I0609 01:06:44.932043 139825018017600 submission.py:120] 21500) loss = 3.706, grad_norm = 0.759
I0609 01:08:34.993256 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:09:20.225466 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:10:06.487195 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:10:07.915424 139825018017600 submission_runner.py:419] Time since start: 10902.77s, 	Step: 21773, 	{'train/accuracy': 0.636640625, 'train/loss': 1.6145965576171875, 'validation/accuracy': 0.58014, 'validation/loss': 1.87282859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4616, 'test/loss': 2.5033615234375, 'test/num_examples': 10000, 'score': 8817.136900186539, 'total_duration': 10902.770802021027, 'accumulated_submission_time': 8817.136900186539, 'accumulated_eval_time': 2072.2221744060516, 'accumulated_logging_time': 0.45603394508361816}
I0609 01:10:07.926217 139781006776064 logging_writer.py:48] [21773] accumulated_eval_time=2072.222174, accumulated_logging_time=0.456034, accumulated_submission_time=8817.136900, global_step=21773, preemption_count=0, score=8817.136900, test/accuracy=0.461600, test/loss=2.503362, test/num_examples=10000, total_duration=10902.770802, train/accuracy=0.636641, train/loss=1.614597, validation/accuracy=0.580140, validation/loss=1.872829, validation/num_examples=50000
I0609 01:11:40.095934 139780998383360 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.763659, loss=3.945917
I0609 01:11:40.101444 139825018017600 submission.py:120] 22000) loss = 3.946, grad_norm = 0.764
I0609 01:15:01.348185 139781006776064 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.812722, loss=3.732624
I0609 01:15:01.355067 139825018017600 submission.py:120] 22500) loss = 3.733, grad_norm = 0.813
I0609 01:17:08.082669 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:17:53.249932 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:18:39.117318 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:18:40.544926 139825018017600 submission_runner.py:419] Time since start: 11415.40s, 	Step: 22809, 	{'train/accuracy': 0.64603515625, 'train/loss': 1.5680531311035155, 'validation/accuracy': 0.5878, 'validation/loss': 1.835575625, 'validation/num_examples': 50000, 'test/accuracy': 0.4681, 'test/loss': 2.47385703125, 'test/num_examples': 10000, 'score': 9236.670559883118, 'total_duration': 11415.400270462036, 'accumulated_submission_time': 9236.670559883118, 'accumulated_eval_time': 2164.684612751007, 'accumulated_logging_time': 0.4760558605194092}
I0609 01:18:40.560615 139780998383360 logging_writer.py:48] [22809] accumulated_eval_time=2164.684613, accumulated_logging_time=0.476056, accumulated_submission_time=9236.670560, global_step=22809, preemption_count=0, score=9236.670560, test/accuracy=0.468100, test/loss=2.473857, test/num_examples=10000, total_duration=11415.400270, train/accuracy=0.646035, train/loss=1.568053, validation/accuracy=0.587800, validation/loss=1.835576, validation/num_examples=50000
I0609 01:19:57.903101 139781006776064 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.845582, loss=3.486396
I0609 01:19:57.908368 139825018017600 submission.py:120] 23000) loss = 3.486, grad_norm = 0.846
I0609 01:23:19.943850 139780998383360 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.936350, loss=3.566505
I0609 01:23:19.948332 139825018017600 submission.py:120] 23500) loss = 3.567, grad_norm = 0.936
I0609 01:25:40.906437 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:26:26.169029 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:27:12.376079 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:27:13.800690 139825018017600 submission_runner.py:419] Time since start: 11928.66s, 	Step: 23846, 	{'train/accuracy': 0.65853515625, 'train/loss': 1.5434065246582032, 'validation/accuracy': 0.59948, 'validation/loss': 1.8011696875, 'validation/num_examples': 50000, 'test/accuracy': 0.4828, 'test/loss': 2.42939375, 'test/num_examples': 10000, 'score': 9656.3903799057, 'total_duration': 11928.656091690063, 'accumulated_submission_time': 9656.3903799057, 'accumulated_eval_time': 2257.578985929489, 'accumulated_logging_time': 0.5000166893005371}
I0609 01:27:13.811626 139781006776064 logging_writer.py:48] [23846] accumulated_eval_time=2257.578986, accumulated_logging_time=0.500017, accumulated_submission_time=9656.390380, global_step=23846, preemption_count=0, score=9656.390380, test/accuracy=0.482800, test/loss=2.429394, test/num_examples=10000, total_duration=11928.656092, train/accuracy=0.658535, train/loss=1.543407, validation/accuracy=0.599480, validation/loss=1.801170, validation/num_examples=50000
I0609 01:28:16.219629 139780998383360 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.763194, loss=3.541663
I0609 01:28:16.225064 139825018017600 submission.py:120] 24000) loss = 3.542, grad_norm = 0.763
I0609 01:31:38.512509 139781006776064 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.888386, loss=3.939923
I0609 01:31:38.517263 139825018017600 submission.py:120] 24500) loss = 3.940, grad_norm = 0.888
I0609 01:34:14.168411 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:34:59.122177 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:35:45.305597 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:35:46.731025 139825018017600 submission_runner.py:419] Time since start: 12441.59s, 	Step: 24887, 	{'train/accuracy': 0.66169921875, 'train/loss': 1.5394711303710937, 'validation/accuracy': 0.60306, 'validation/loss': 1.80728203125, 'validation/num_examples': 50000, 'test/accuracy': 0.4785, 'test/loss': 2.41697109375, 'test/num_examples': 10000, 'score': 10076.113626003265, 'total_duration': 12441.586392879486, 'accumulated_submission_time': 10076.113626003265, 'accumulated_eval_time': 2350.141647338867, 'accumulated_logging_time': 0.5214667320251465}
I0609 01:35:46.742841 139780998383360 logging_writer.py:48] [24887] accumulated_eval_time=2350.141647, accumulated_logging_time=0.521467, accumulated_submission_time=10076.113626, global_step=24887, preemption_count=0, score=10076.113626, test/accuracy=0.478500, test/loss=2.416971, test/num_examples=10000, total_duration=12441.586393, train/accuracy=0.661699, train/loss=1.539471, validation/accuracy=0.603060, validation/loss=1.807282, validation/num_examples=50000
I0609 01:36:32.391804 139781006776064 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.761061, loss=3.755133
I0609 01:36:32.396925 139825018017600 submission.py:120] 25000) loss = 3.755, grad_norm = 0.761
I0609 01:39:56.799760 139780998383360 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.736143, loss=3.406894
I0609 01:39:56.804641 139825018017600 submission.py:120] 25500) loss = 3.407, grad_norm = 0.736
I0609 01:42:46.982815 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:43:32.038425 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:44:18.195788 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:44:19.620211 139825018017600 submission_runner.py:419] Time since start: 12954.48s, 	Step: 25922, 	{'train/accuracy': 0.66701171875, 'train/loss': 1.4902020263671876, 'validation/accuracy': 0.6111, 'validation/loss': 1.75186375, 'validation/num_examples': 50000, 'test/accuracy': 0.4896, 'test/loss': 2.3821765625, 'test/num_examples': 10000, 'score': 10495.722432136536, 'total_duration': 12954.47559094429, 'accumulated_submission_time': 10495.722432136536, 'accumulated_eval_time': 2442.7791635990143, 'accumulated_logging_time': 0.5430014133453369}
I0609 01:44:19.632154 139781006776064 logging_writer.py:48] [25922] accumulated_eval_time=2442.779164, accumulated_logging_time=0.543001, accumulated_submission_time=10495.722432, global_step=25922, preemption_count=0, score=10495.722432, test/accuracy=0.489600, test/loss=2.382177, test/num_examples=10000, total_duration=12954.475591, train/accuracy=0.667012, train/loss=1.490202, validation/accuracy=0.611100, validation/loss=1.751864, validation/num_examples=50000
I0609 01:44:51.489503 139780998383360 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.808857, loss=3.576150
I0609 01:44:51.494345 139825018017600 submission.py:120] 26000) loss = 3.576, grad_norm = 0.809
I0609 01:48:15.325129 139781006776064 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.772101, loss=4.107265
I0609 01:48:15.331084 139825018017600 submission.py:120] 26500) loss = 4.107, grad_norm = 0.772
I0609 01:51:19.946462 139825018017600 spec.py:298] Evaluating on the training split.
I0609 01:52:04.998669 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 01:52:50.949831 139825018017600 spec.py:326] Evaluating on the test split.
I0609 01:52:52.375231 139825018017600 submission_runner.py:419] Time since start: 13467.23s, 	Step: 26958, 	{'train/accuracy': 0.6801953125, 'train/loss': 1.4653579711914062, 'validation/accuracy': 0.61668, 'validation/loss': 1.733103125, 'validation/num_examples': 50000, 'test/accuracy': 0.492, 'test/loss': 2.3624765625, 'test/num_examples': 10000, 'score': 10915.415491819382, 'total_duration': 13467.230561733246, 'accumulated_submission_time': 10915.415491819382, 'accumulated_eval_time': 2535.207940340042, 'accumulated_logging_time': 0.5634706020355225}
I0609 01:52:52.387357 139780998383360 logging_writer.py:48] [26958] accumulated_eval_time=2535.207940, accumulated_logging_time=0.563471, accumulated_submission_time=10915.415492, global_step=26958, preemption_count=0, score=10915.415492, test/accuracy=0.492000, test/loss=2.362477, test/num_examples=10000, total_duration=13467.230562, train/accuracy=0.680195, train/loss=1.465358, validation/accuracy=0.616680, validation/loss=1.733103, validation/num_examples=50000
I0609 01:53:09.675806 139781006776064 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.893156, loss=3.570073
I0609 01:53:09.681261 139825018017600 submission.py:120] 27000) loss = 3.570, grad_norm = 0.893
I0609 01:56:31.240282 139780998383360 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.971207, loss=3.889412
I0609 01:56:31.246902 139825018017600 submission.py:120] 27500) loss = 3.889, grad_norm = 0.971
I0609 01:59:52.432716 139825018017600 spec.py:298] Evaluating on the training split.
I0609 02:00:37.762603 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 02:01:24.031286 139825018017600 spec.py:326] Evaluating on the test split.
I0609 02:01:25.455177 139825018017600 submission_runner.py:419] Time since start: 13980.31s, 	Step: 27994, 	{'train/accuracy': 0.688671875, 'train/loss': 1.3888761901855469, 'validation/accuracy': 0.62694, 'validation/loss': 1.6659071875, 'validation/num_examples': 50000, 'test/accuracy': 0.5018, 'test/loss': 2.300290234375, 'test/num_examples': 10000, 'score': 11334.837124347687, 'total_duration': 13980.310522794724, 'accumulated_submission_time': 11334.837124347687, 'accumulated_eval_time': 2628.2305829524994, 'accumulated_logging_time': 0.5837960243225098}
I0609 02:01:25.466434 139781006776064 logging_writer.py:48] [27994] accumulated_eval_time=2628.230583, accumulated_logging_time=0.583796, accumulated_submission_time=11334.837124, global_step=27994, preemption_count=0, score=11334.837124, test/accuracy=0.501800, test/loss=2.300290, test/num_examples=10000, total_duration=13980.310523, train/accuracy=0.688672, train/loss=1.388876, validation/accuracy=0.626940, validation/loss=1.665907, validation/num_examples=50000
I0609 02:01:27.859513 139825018017600 spec.py:298] Evaluating on the training split.
I0609 02:02:12.570944 139825018017600 spec.py:310] Evaluating on the validation split.
I0609 02:02:58.766023 139825018017600 spec.py:326] Evaluating on the test split.
I0609 02:03:00.192423 139825018017600 submission_runner.py:419] Time since start: 14075.05s, 	Step: 28000, 	{'train/accuracy': 0.6823046875, 'train/loss': 1.4159164428710938, 'validation/accuracy': 0.62064, 'validation/loss': 1.69644453125, 'validation/num_examples': 50000, 'test/accuracy': 0.4952, 'test/loss': 2.325923828125, 'test/num_examples': 10000, 'score': 11337.217742204666, 'total_duration': 14075.047792673111, 'accumulated_submission_time': 11337.217742204666, 'accumulated_eval_time': 2720.5636336803436, 'accumulated_logging_time': 0.6031069755554199}
I0609 02:03:00.203213 139780998383360 logging_writer.py:48] [28000] accumulated_eval_time=2720.563634, accumulated_logging_time=0.603107, accumulated_submission_time=11337.217742, global_step=28000, preemption_count=0, score=11337.217742, test/accuracy=0.495200, test/loss=2.325924, test/num_examples=10000, total_duration=14075.047793, train/accuracy=0.682305, train/loss=1.415916, validation/accuracy=0.620640, validation/loss=1.696445, validation/num_examples=50000
I0609 02:03:00.221599 139781006776064 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11337.217742
I0609 02:03:00.906814 139825018017600 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0609 02:03:01.164026 139825018017600 submission_runner.py:581] Tuning trial 1/1
I0609 02:03:01.164229 139825018017600 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 02:03:01.165186 139825018017600 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00220703125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.0029, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0027, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.622880697250366, 'total_duration': 132.19233965873718, 'accumulated_submission_time': 6.622880697250366, 'accumulated_eval_time': 125.56893801689148, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1043, {'train/accuracy': 0.03525390625, 'train/loss': 5.9252862548828125, 'validation/accuracy': 0.03476, 'validation/loss': 5.952164375, 'validation/num_examples': 50000, 'test/accuracy': 0.0247, 'test/loss': 6.0632984375, 'test/num_examples': 10000, 'score': 426.0211238861084, 'total_duration': 640.3967559337616, 'accumulated_submission_time': 426.0211238861084, 'accumulated_eval_time': 213.74573588371277, 'accumulated_logging_time': 0.028234481811523438, 'global_step': 1043, 'preemption_count': 0}), (2081, {'train/accuracy': 0.08458984375, 'train/loss': 5.2411346435546875, 'validation/accuracy': 0.0777, 'validation/loss': 5.29019375, 'validation/num_examples': 50000, 'test/accuracy': 0.061, 'test/loss': 5.504282421875, 'test/num_examples': 10000, 'score': 845.6689677238464, 'total_duration': 1158.2216303348541, 'accumulated_submission_time': 845.6689677238464, 'accumulated_eval_time': 311.28668761253357, 'accumulated_logging_time': 0.04751253128051758, 'global_step': 2081, 'preemption_count': 0}), (3116, {'train/accuracy': 0.13943359375, 'train/loss': 4.692435913085937, 'validation/accuracy': 0.13024, 'validation/loss': 4.758155, 'validation/num_examples': 50000, 'test/accuracy': 0.0977, 'test/loss': 5.05568203125, 'test/num_examples': 10000, 'score': 1265.218917608261, 'total_duration': 1677.9315848350525, 'accumulated_submission_time': 1265.218917608261, 'accumulated_eval_time': 410.8088412284851, 'accumulated_logging_time': 0.06659698486328125, 'global_step': 3116, 'preemption_count': 0}), (4151, {'train/accuracy': 0.18830078125, 'train/loss': 4.266922302246094, 'validation/accuracy': 0.17488, 'validation/loss': 4.3579521875, 'validation/num_examples': 50000, 'test/accuracy': 0.1331, 'test/loss': 4.7072484375, 'test/num_examples': 10000, 'score': 1684.8896028995514, 'total_duration': 2189.3517332077026, 'accumulated_submission_time': 1684.8896028995514, 'accumulated_eval_time': 501.90835332870483, 'accumulated_logging_time': 0.09024524688720703, 'global_step': 4151, 'preemption_count': 0}), (5186, {'train/accuracy': 0.2512890625, 'train/loss': 3.8073284912109373, 'validation/accuracy': 0.2286, 'validation/loss': 3.92833, 'validation/num_examples': 50000, 'test/accuracy': 0.1762, 'test/loss': 4.34499375, 'test/num_examples': 10000, 'score': 2104.3070023059845, 'total_duration': 2700.5095207691193, 'accumulated_submission_time': 2104.3070023059845, 'accumulated_eval_time': 593.0142815113068, 'accumulated_logging_time': 0.10902810096740723, 'global_step': 5186, 'preemption_count': 0}), (6227, {'train/accuracy': 0.29087890625, 'train/loss': 3.54396728515625, 'validation/accuracy': 0.2656, 'validation/loss': 3.681008125, 'validation/num_examples': 50000, 'test/accuracy': 0.2034, 'test/loss': 4.134701953125, 'test/num_examples': 10000, 'score': 2523.7379727363586, 'total_duration': 3215.46004486084, 'accumulated_submission_time': 2523.7379727363586, 'accumulated_eval_time': 687.8897821903229, 'accumulated_logging_time': 0.13066864013671875, 'global_step': 6227, 'preemption_count': 0}), (7262, {'train/accuracy': 0.32767578125, 'train/loss': 3.2986932373046876, 'validation/accuracy': 0.29908, 'validation/loss': 3.453194375, 'validation/num_examples': 50000, 'test/accuracy': 0.236, 'test/loss': 3.92604453125, 'test/num_examples': 10000, 'score': 2943.4246406555176, 'total_duration': 3726.1624205112457, 'accumulated_submission_time': 2943.4246406555176, 'accumulated_eval_time': 778.2649435997009, 'accumulated_logging_time': 0.1525413990020752, 'global_step': 7262, 'preemption_count': 0}), (8297, {'train/accuracy': 0.36814453125, 'train/loss': 3.04942138671875, 'validation/accuracy': 0.33464, 'validation/loss': 3.2181196875, 'validation/num_examples': 50000, 'test/accuracy': 0.2613, 'test/loss': 3.714704296875, 'test/num_examples': 10000, 'score': 3363.0720682144165, 'total_duration': 4238.1758189201355, 'accumulated_submission_time': 3363.0720682144165, 'accumulated_eval_time': 869.9900667667389, 'accumulated_logging_time': 0.1786205768585205, 'global_step': 8297, 'preemption_count': 0}), (9332, {'train/accuracy': 0.41015625, 'train/loss': 2.7904595947265625, 'validation/accuracy': 0.37416, 'validation/loss': 2.9712965625, 'validation/num_examples': 50000, 'test/accuracy': 0.2896, 'test/loss': 3.518267578125, 'test/num_examples': 10000, 'score': 3782.7250921726227, 'total_duration': 4750.665675878525, 'accumulated_submission_time': 3782.7250921726227, 'accumulated_eval_time': 962.189864397049, 'accumulated_logging_time': 0.19685101509094238, 'global_step': 9332, 'preemption_count': 0}), (10368, {'train/accuracy': 0.4385546875, 'train/loss': 2.6523980712890625, 'validation/accuracy': 0.4005, 'validation/loss': 2.84266625, 'validation/num_examples': 50000, 'test/accuracy': 0.3097, 'test/loss': 3.382415234375, 'test/num_examples': 10000, 'score': 4202.115720510483, 'total_duration': 5263.350814342499, 'accumulated_submission_time': 4202.115720510483, 'accumulated_eval_time': 1054.8578073978424, 'accumulated_logging_time': 0.215057373046875, 'global_step': 10368, 'preemption_count': 0}), (11404, {'train/accuracy': 0.473125, 'train/loss': 2.4494837951660156, 'validation/accuracy': 0.43388, 'validation/loss': 2.651764375, 'validation/num_examples': 50000, 'test/accuracy': 0.3374, 'test/loss': 3.2343755859375, 'test/num_examples': 10000, 'score': 4621.759845256805, 'total_duration': 5775.9882254600525, 'accumulated_submission_time': 4621.759845256805, 'accumulated_eval_time': 1147.2120311260223, 'accumulated_logging_time': 0.23401141166687012, 'global_step': 11404, 'preemption_count': 0}), (12444, {'train/accuracy': 0.50552734375, 'train/loss': 2.2489689636230468, 'validation/accuracy': 0.4613, 'validation/loss': 2.4632825, 'validation/num_examples': 50000, 'test/accuracy': 0.3555, 'test/loss': 3.07796484375, 'test/num_examples': 10000, 'score': 5041.180806398392, 'total_duration': 6287.908731460571, 'accumulated_submission_time': 5041.180806398392, 'accumulated_eval_time': 1239.0683307647705, 'accumulated_logging_time': 0.25696420669555664, 'global_step': 12444, 'preemption_count': 0}), (13479, {'train/accuracy': 0.52431640625, 'train/loss': 2.19983642578125, 'validation/accuracy': 0.47698, 'validation/loss': 2.4166659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3724, 'test/loss': 3.023970703125, 'test/num_examples': 10000, 'score': 5460.721163988113, 'total_duration': 6800.012439727783, 'accumulated_submission_time': 5460.721163988113, 'accumulated_eval_time': 1330.9939250946045, 'accumulated_logging_time': 0.28195977210998535, 'global_step': 13479, 'preemption_count': 0}), (14516, {'train/accuracy': 0.5487109375, 'train/loss': 2.0630027770996096, 'validation/accuracy': 0.504, 'validation/loss': 2.2928159375, 'validation/num_examples': 50000, 'test/accuracy': 0.3934, 'test/loss': 2.8893359375, 'test/num_examples': 10000, 'score': 5880.205817222595, 'total_duration': 7312.305900812149, 'accumulated_submission_time': 5880.205817222595, 'accumulated_eval_time': 1423.1648411750793, 'accumulated_logging_time': 0.3029608726501465, 'global_step': 14516, 'preemption_count': 0}), (15553, {'train/accuracy': 0.56396484375, 'train/loss': 1.9813763427734374, 'validation/accuracy': 0.51804, 'validation/loss': 2.20847796875, 'validation/num_examples': 50000, 'test/accuracy': 0.4026, 'test/loss': 2.8403578125, 'test/num_examples': 10000, 'score': 6299.921359062195, 'total_duration': 7825.650443553925, 'accumulated_submission_time': 6299.921359062195, 'accumulated_eval_time': 1516.1604175567627, 'accumulated_logging_time': 0.3220236301422119, 'global_step': 15553, 'preemption_count': 0}), (16590, {'train/accuracy': 0.57830078125, 'train/loss': 1.9079759216308594, 'validation/accuracy': 0.52944, 'validation/loss': 2.1546125, 'validation/num_examples': 50000, 'test/accuracy': 0.4177, 'test/loss': 2.7754787109375, 'test/num_examples': 10000, 'score': 6719.635491371155, 'total_duration': 8338.769189596176, 'accumulated_submission_time': 6719.635491371155, 'accumulated_eval_time': 1608.9363205432892, 'accumulated_logging_time': 0.34078454971313477, 'global_step': 16590, 'preemption_count': 0}), (17626, {'train/accuracy': 0.5969921875, 'train/loss': 1.8343675231933594, 'validation/accuracy': 0.54342, 'validation/loss': 2.0753528125, 'validation/num_examples': 50000, 'test/accuracy': 0.425, 'test/loss': 2.69705390625, 'test/num_examples': 10000, 'score': 7139.009434938431, 'total_duration': 8850.99041056633, 'accumulated_submission_time': 7139.009434938431, 'accumulated_eval_time': 1701.1516015529633, 'accumulated_logging_time': 0.36162590980529785, 'global_step': 17626, 'preemption_count': 0}), (18666, {'train/accuracy': 0.60541015625, 'train/loss': 1.7813375854492188, 'validation/accuracy': 0.55394, 'validation/loss': 2.0304234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4347, 'test/loss': 2.6676984375, 'test/num_examples': 10000, 'score': 7558.478419780731, 'total_duration': 9363.689949035645, 'accumulated_submission_time': 7558.478419780731, 'accumulated_eval_time': 1793.7379922866821, 'accumulated_logging_time': 0.3815150260925293, 'global_step': 18666, 'preemption_count': 0}), (19701, {'train/accuracy': 0.6208984375, 'train/loss': 1.6827462768554688, 'validation/accuracy': 0.56624, 'validation/loss': 1.938425625, 'validation/num_examples': 50000, 'test/accuracy': 0.4461, 'test/loss': 2.5860849609375, 'test/num_examples': 10000, 'score': 7977.937218427658, 'total_duration': 9876.268121480942, 'accumulated_submission_time': 7977.937218427658, 'accumulated_eval_time': 1886.2094955444336, 'accumulated_logging_time': 0.4095165729522705, 'global_step': 19701, 'preemption_count': 0}), (20737, {'train/accuracy': 0.63453125, 'train/loss': 1.65840087890625, 'validation/accuracy': 0.57816, 'validation/loss': 1.906141875, 'validation/num_examples': 50000, 'test/accuracy': 0.453, 'test/loss': 2.53839375, 'test/num_examples': 10000, 'score': 8397.652898788452, 'total_duration': 10389.721275091171, 'accumulated_submission_time': 8397.652898788452, 'accumulated_eval_time': 1979.299936056137, 'accumulated_logging_time': 0.4332573413848877, 'global_step': 20737, 'preemption_count': 0}), (21773, {'train/accuracy': 0.636640625, 'train/loss': 1.6145965576171875, 'validation/accuracy': 0.58014, 'validation/loss': 1.87282859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4616, 'test/loss': 2.5033615234375, 'test/num_examples': 10000, 'score': 8817.136900186539, 'total_duration': 10902.770802021027, 'accumulated_submission_time': 8817.136900186539, 'accumulated_eval_time': 2072.2221744060516, 'accumulated_logging_time': 0.45603394508361816, 'global_step': 21773, 'preemption_count': 0}), (22809, {'train/accuracy': 0.64603515625, 'train/loss': 1.5680531311035155, 'validation/accuracy': 0.5878, 'validation/loss': 1.835575625, 'validation/num_examples': 50000, 'test/accuracy': 0.4681, 'test/loss': 2.47385703125, 'test/num_examples': 10000, 'score': 9236.670559883118, 'total_duration': 11415.400270462036, 'accumulated_submission_time': 9236.670559883118, 'accumulated_eval_time': 2164.684612751007, 'accumulated_logging_time': 0.4760558605194092, 'global_step': 22809, 'preemption_count': 0}), (23846, {'train/accuracy': 0.65853515625, 'train/loss': 1.5434065246582032, 'validation/accuracy': 0.59948, 'validation/loss': 1.8011696875, 'validation/num_examples': 50000, 'test/accuracy': 0.4828, 'test/loss': 2.42939375, 'test/num_examples': 10000, 'score': 9656.3903799057, 'total_duration': 11928.656091690063, 'accumulated_submission_time': 9656.3903799057, 'accumulated_eval_time': 2257.578985929489, 'accumulated_logging_time': 0.5000166893005371, 'global_step': 23846, 'preemption_count': 0}), (24887, {'train/accuracy': 0.66169921875, 'train/loss': 1.5394711303710937, 'validation/accuracy': 0.60306, 'validation/loss': 1.80728203125, 'validation/num_examples': 50000, 'test/accuracy': 0.4785, 'test/loss': 2.41697109375, 'test/num_examples': 10000, 'score': 10076.113626003265, 'total_duration': 12441.586392879486, 'accumulated_submission_time': 10076.113626003265, 'accumulated_eval_time': 2350.141647338867, 'accumulated_logging_time': 0.5214667320251465, 'global_step': 24887, 'preemption_count': 0}), (25922, {'train/accuracy': 0.66701171875, 'train/loss': 1.4902020263671876, 'validation/accuracy': 0.6111, 'validation/loss': 1.75186375, 'validation/num_examples': 50000, 'test/accuracy': 0.4896, 'test/loss': 2.3821765625, 'test/num_examples': 10000, 'score': 10495.722432136536, 'total_duration': 12954.47559094429, 'accumulated_submission_time': 10495.722432136536, 'accumulated_eval_time': 2442.7791635990143, 'accumulated_logging_time': 0.5430014133453369, 'global_step': 25922, 'preemption_count': 0}), (26958, {'train/accuracy': 0.6801953125, 'train/loss': 1.4653579711914062, 'validation/accuracy': 0.61668, 'validation/loss': 1.733103125, 'validation/num_examples': 50000, 'test/accuracy': 0.492, 'test/loss': 2.3624765625, 'test/num_examples': 10000, 'score': 10915.415491819382, 'total_duration': 13467.230561733246, 'accumulated_submission_time': 10915.415491819382, 'accumulated_eval_time': 2535.207940340042, 'accumulated_logging_time': 0.5634706020355225, 'global_step': 26958, 'preemption_count': 0}), (27994, {'train/accuracy': 0.688671875, 'train/loss': 1.3888761901855469, 'validation/accuracy': 0.62694, 'validation/loss': 1.6659071875, 'validation/num_examples': 50000, 'test/accuracy': 0.5018, 'test/loss': 2.300290234375, 'test/num_examples': 10000, 'score': 11334.837124347687, 'total_duration': 13980.310522794724, 'accumulated_submission_time': 11334.837124347687, 'accumulated_eval_time': 2628.2305829524994, 'accumulated_logging_time': 0.5837960243225098, 'global_step': 27994, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6823046875, 'train/loss': 1.4159164428710938, 'validation/accuracy': 0.62064, 'validation/loss': 1.69644453125, 'validation/num_examples': 50000, 'test/accuracy': 0.4952, 'test/loss': 2.325923828125, 'test/num_examples': 10000, 'score': 11337.217742204666, 'total_duration': 14075.047792673111, 'accumulated_submission_time': 11337.217742204666, 'accumulated_eval_time': 2720.5636336803436, 'accumulated_logging_time': 0.6031069755554199, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0609 02:03:01.165304 139825018017600 submission_runner.py:584] Timing: 11337.217742204666
I0609 02:03:01.165357 139825018017600 submission_runner.py:586] Total number of evals: 29
I0609 02:03:01.165402 139825018017600 submission_runner.py:587] ====================
I0609 02:03:01.165510 139825018017600 submission_runner.py:655] Final imagenet_vit score: 11337.217742204666
