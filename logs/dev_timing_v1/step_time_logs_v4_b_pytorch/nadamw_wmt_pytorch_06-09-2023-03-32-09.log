torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_pytorch_06-09-2023-03-32-09.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 03:32:32.882529 139738549851968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 03:32:32.882567 139628558374720 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 03:32:32.883252 139887094957888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 03:32:32.883559 140301638661952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 03:32:32.883680 139809805489984 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 03:32:32.883665 139981650913088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 03:32:32.883862 140044272662336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 03:32:32.893976 139726197217088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 03:32:32.894204 140301638661952 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.894301 139809805489984 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.894348 139726197217088 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.894329 139981650913088 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.894424 140044272662336 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.903512 139738549851968 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.903540 139628558374720 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:32.904102 139887094957888 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:32:37.468357 139726197217088 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nadamw/wmt_pytorch.
W0609 03:32:37.511886 140301638661952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.512177 139887094957888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.514018 139628558374720 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.514183 139726197217088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.514680 139809805489984 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.515466 139981650913088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.516978 139738549851968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:32:37.519160 140044272662336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 03:32:37.519474 139726197217088 submission_runner.py:541] Using RNG seed 3666083068
I0609 03:32:37.521037 139726197217088 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 03:32:37.521167 139726197217088 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nadamw/wmt_pytorch/trial_1.
I0609 03:32:37.521875 139726197217088 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nadamw/wmt_pytorch/trial_1/hparams.json.
I0609 03:32:37.523020 139726197217088 submission_runner.py:255] Initializing dataset.
I0609 03:32:37.523151 139726197217088 submission_runner.py:262] Initializing model.
I0609 03:32:41.187131 139726197217088 submission_runner.py:272] Initializing optimizer.
I0609 03:32:41.188529 139726197217088 submission_runner.py:279] Initializing metrics bundle.
I0609 03:32:41.188630 139726197217088 submission_runner.py:297] Initializing checkpoint and logger.
I0609 03:32:41.192037 139726197217088 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 03:32:41.192173 139726197217088 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 03:32:41.770076 139726197217088 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nadamw/wmt_pytorch/trial_1/meta_data_0.json.
I0609 03:32:41.771173 139726197217088 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nadamw/wmt_pytorch/trial_1/flags_0.json.
I0609 03:32:41.818636 139726197217088 submission_runner.py:332] Starting training loop.
I0609 03:32:41.833735 139726197217088 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:32:41.837325 139726197217088 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:32:41.837458 139726197217088 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:32:41.907629 139726197217088 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:32:46.259096 139686194444032 logging_writer.py:48] [0] global_step=0, grad_norm=5.560221, loss=11.044103
I0609 03:32:46.265906 139726197217088 submission.py:296] 0) loss = 11.044, grad_norm = 5.560
I0609 03:32:46.267143 139726197217088 spec.py:298] Evaluating on the training split.
I0609 03:32:46.269566 139726197217088 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:32:46.272437 139726197217088 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:32:46.272564 139726197217088 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:32:46.302373 139726197217088 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:32:50.456388 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 03:37:24.252414 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 03:37:24.257179 139726197217088 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 03:37:24.260218 139726197217088 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:37:24.260346 139726197217088 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:37:24.289662 139726197217088 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 03:37:28.121522 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 03:41:56.533782 139726197217088 spec.py:326] Evaluating on the test split.
I0609 03:41:56.536751 139726197217088 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 03:41:56.539957 139726197217088 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:41:56.540082 139726197217088 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:41:56.571010 139726197217088 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 03:42:00.487540 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 03:46:34.157396 139726197217088 submission_runner.py:419] Time since start: 832.34s, 	Step: 1, 	{'train/accuracy': 0.0006492027334851936, 'train/loss': 11.039222665148063, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.024672663699148, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.017222996920575, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.448609113693237, 'total_duration': 832.3391466140747, 'accumulated_submission_time': 4.448609113693237, 'accumulated_eval_time': 827.8901655673981, 'accumulated_logging_time': 0}
I0609 03:46:34.174920 139668233185024 logging_writer.py:48] [1] accumulated_eval_time=827.890166, accumulated_logging_time=0, accumulated_submission_time=4.448609, global_step=1, preemption_count=0, score=4.448609, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.017223, test/num_examples=3003, total_duration=832.339147, train/accuracy=0.000649, train/bleu=0.000000, train/loss=11.039223, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.024673, validation/num_examples=3000
I0609 03:46:34.194000 139726197217088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194075 139887094957888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194113 140301638661952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194084 139738549851968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194126 140044272662336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194135 139628558374720 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194154 139981650913088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.194397 139809805489984 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 03:46:34.656177 139668224792320 logging_writer.py:48] [1] global_step=1, grad_norm=5.534537, loss=11.050697
I0609 03:46:34.659809 139726197217088 submission.py:296] 1) loss = 11.051, grad_norm = 5.535
I0609 03:46:35.108692 139668233185024 logging_writer.py:48] [2] global_step=2, grad_norm=5.503384, loss=11.041845
I0609 03:46:35.112093 139726197217088 submission.py:296] 2) loss = 11.042, grad_norm = 5.503
I0609 03:46:35.562808 139668224792320 logging_writer.py:48] [3] global_step=3, grad_norm=5.466779, loss=11.019983
I0609 03:46:35.566156 139726197217088 submission.py:296] 3) loss = 11.020, grad_norm = 5.467
I0609 03:46:36.022933 139668233185024 logging_writer.py:48] [4] global_step=4, grad_norm=5.419604, loss=10.973800
I0609 03:46:36.026237 139726197217088 submission.py:296] 4) loss = 10.974, grad_norm = 5.420
I0609 03:46:36.478216 139668224792320 logging_writer.py:48] [5] global_step=5, grad_norm=5.250264, loss=10.950346
I0609 03:46:36.481651 139726197217088 submission.py:296] 5) loss = 10.950, grad_norm = 5.250
I0609 03:46:36.933406 139668233185024 logging_writer.py:48] [6] global_step=6, grad_norm=5.162783, loss=10.903480
I0609 03:46:36.936844 139726197217088 submission.py:296] 6) loss = 10.903, grad_norm = 5.163
I0609 03:46:37.391929 139668224792320 logging_writer.py:48] [7] global_step=7, grad_norm=5.023171, loss=10.854340
I0609 03:46:37.395302 139726197217088 submission.py:296] 7) loss = 10.854, grad_norm = 5.023
I0609 03:46:37.845276 139668233185024 logging_writer.py:48] [8] global_step=8, grad_norm=4.877066, loss=10.810002
I0609 03:46:37.848880 139726197217088 submission.py:296] 8) loss = 10.810, grad_norm = 4.877
I0609 03:46:38.316612 139668224792320 logging_writer.py:48] [9] global_step=9, grad_norm=4.714844, loss=10.738915
I0609 03:46:38.319974 139726197217088 submission.py:296] 9) loss = 10.739, grad_norm = 4.715
I0609 03:46:38.766889 139668233185024 logging_writer.py:48] [10] global_step=10, grad_norm=4.418875, loss=10.692177
I0609 03:46:38.770205 139726197217088 submission.py:296] 10) loss = 10.692, grad_norm = 4.419
I0609 03:46:39.226091 139668224792320 logging_writer.py:48] [11] global_step=11, grad_norm=4.233587, loss=10.621426
I0609 03:46:39.229418 139726197217088 submission.py:296] 11) loss = 10.621, grad_norm = 4.234
I0609 03:46:39.682863 139668233185024 logging_writer.py:48] [12] global_step=12, grad_norm=3.981733, loss=10.557254
I0609 03:46:39.686273 139726197217088 submission.py:296] 12) loss = 10.557, grad_norm = 3.982
I0609 03:46:40.134817 139668224792320 logging_writer.py:48] [13] global_step=13, grad_norm=3.883165, loss=10.474509
I0609 03:46:40.137986 139726197217088 submission.py:296] 13) loss = 10.475, grad_norm = 3.883
I0609 03:46:40.589675 139668233185024 logging_writer.py:48] [14] global_step=14, grad_norm=3.596298, loss=10.407454
I0609 03:46:40.593039 139726197217088 submission.py:296] 14) loss = 10.407, grad_norm = 3.596
I0609 03:46:41.045505 139668224792320 logging_writer.py:48] [15] global_step=15, grad_norm=3.384809, loss=10.338846
I0609 03:46:41.049063 139726197217088 submission.py:296] 15) loss = 10.339, grad_norm = 3.385
I0609 03:46:41.513030 139668233185024 logging_writer.py:48] [16] global_step=16, grad_norm=3.128619, loss=10.275002
I0609 03:46:41.516855 139726197217088 submission.py:296] 16) loss = 10.275, grad_norm = 3.129
I0609 03:46:41.963813 139668224792320 logging_writer.py:48] [17] global_step=17, grad_norm=2.942926, loss=10.203676
I0609 03:46:41.967336 139726197217088 submission.py:296] 17) loss = 10.204, grad_norm = 2.943
I0609 03:46:42.420965 139668233185024 logging_writer.py:48] [18] global_step=18, grad_norm=2.732383, loss=10.156369
I0609 03:46:42.424266 139726197217088 submission.py:296] 18) loss = 10.156, grad_norm = 2.732
I0609 03:46:42.877091 139668224792320 logging_writer.py:48] [19] global_step=19, grad_norm=2.560577, loss=10.100521
I0609 03:46:42.880509 139726197217088 submission.py:296] 19) loss = 10.101, grad_norm = 2.561
I0609 03:46:43.330507 139668233185024 logging_writer.py:48] [20] global_step=20, grad_norm=2.408106, loss=10.026031
I0609 03:46:43.334197 139726197217088 submission.py:296] 20) loss = 10.026, grad_norm = 2.408
I0609 03:46:43.784606 139668224792320 logging_writer.py:48] [21] global_step=21, grad_norm=2.247669, loss=9.967356
I0609 03:46:43.788264 139726197217088 submission.py:296] 21) loss = 9.967, grad_norm = 2.248
I0609 03:46:44.257887 139668233185024 logging_writer.py:48] [22] global_step=22, grad_norm=2.081256, loss=9.919741
I0609 03:46:44.261369 139726197217088 submission.py:296] 22) loss = 9.920, grad_norm = 2.081
I0609 03:46:44.714529 139668224792320 logging_writer.py:48] [23] global_step=23, grad_norm=1.955409, loss=9.858617
I0609 03:46:44.717869 139726197217088 submission.py:296] 23) loss = 9.859, grad_norm = 1.955
I0609 03:46:45.170175 139668233185024 logging_writer.py:48] [24] global_step=24, grad_norm=1.818172, loss=9.822613
I0609 03:46:45.173464 139726197217088 submission.py:296] 24) loss = 9.823, grad_norm = 1.818
I0609 03:46:45.624283 139668224792320 logging_writer.py:48] [25] global_step=25, grad_norm=1.705340, loss=9.767015
I0609 03:46:45.627785 139726197217088 submission.py:296] 25) loss = 9.767, grad_norm = 1.705
I0609 03:46:46.084697 139668233185024 logging_writer.py:48] [26] global_step=26, grad_norm=1.622303, loss=9.723024
I0609 03:46:46.088062 139726197217088 submission.py:296] 26) loss = 9.723, grad_norm = 1.622
I0609 03:46:46.540068 139668224792320 logging_writer.py:48] [27] global_step=27, grad_norm=1.509250, loss=9.673933
I0609 03:46:46.543272 139726197217088 submission.py:296] 27) loss = 9.674, grad_norm = 1.509
I0609 03:46:46.997025 139668233185024 logging_writer.py:48] [28] global_step=28, grad_norm=1.439555, loss=9.638404
I0609 03:46:47.000389 139726197217088 submission.py:296] 28) loss = 9.638, grad_norm = 1.440
I0609 03:46:47.454533 139668224792320 logging_writer.py:48] [29] global_step=29, grad_norm=1.385450, loss=9.600045
I0609 03:46:47.457952 139726197217088 submission.py:296] 29) loss = 9.600, grad_norm = 1.385
I0609 03:46:47.908552 139668233185024 logging_writer.py:48] [30] global_step=30, grad_norm=1.313789, loss=9.560941
I0609 03:46:47.912204 139726197217088 submission.py:296] 30) loss = 9.561, grad_norm = 1.314
I0609 03:46:48.362539 139668224792320 logging_writer.py:48] [31] global_step=31, grad_norm=1.264626, loss=9.524016
I0609 03:46:48.365695 139726197217088 submission.py:296] 31) loss = 9.524, grad_norm = 1.265
I0609 03:46:48.813197 139668233185024 logging_writer.py:48] [32] global_step=32, grad_norm=1.194720, loss=9.480326
I0609 03:46:48.816567 139726197217088 submission.py:296] 32) loss = 9.480, grad_norm = 1.195
I0609 03:46:49.270292 139668224792320 logging_writer.py:48] [33] global_step=33, grad_norm=1.140023, loss=9.465508
I0609 03:46:49.273574 139726197217088 submission.py:296] 33) loss = 9.466, grad_norm = 1.140
I0609 03:46:49.726405 139668233185024 logging_writer.py:48] [34] global_step=34, grad_norm=1.083594, loss=9.427192
I0609 03:46:49.729490 139726197217088 submission.py:296] 34) loss = 9.427, grad_norm = 1.084
I0609 03:46:50.178123 139668224792320 logging_writer.py:48] [35] global_step=35, grad_norm=1.023367, loss=9.404318
I0609 03:46:50.181315 139726197217088 submission.py:296] 35) loss = 9.404, grad_norm = 1.023
I0609 03:46:50.633553 139668233185024 logging_writer.py:48] [36] global_step=36, grad_norm=0.961035, loss=9.409516
I0609 03:46:50.636862 139726197217088 submission.py:296] 36) loss = 9.410, grad_norm = 0.961
I0609 03:46:51.091738 139668224792320 logging_writer.py:48] [37] global_step=37, grad_norm=0.918880, loss=9.367010
I0609 03:46:51.095014 139726197217088 submission.py:296] 37) loss = 9.367, grad_norm = 0.919
I0609 03:46:51.543593 139668233185024 logging_writer.py:48] [38] global_step=38, grad_norm=0.879729, loss=9.356385
I0609 03:46:51.547197 139726197217088 submission.py:296] 38) loss = 9.356, grad_norm = 0.880
I0609 03:46:51.999967 139668224792320 logging_writer.py:48] [39] global_step=39, grad_norm=0.829920, loss=9.321141
I0609 03:46:52.003552 139726197217088 submission.py:296] 39) loss = 9.321, grad_norm = 0.830
I0609 03:46:52.455353 139668233185024 logging_writer.py:48] [40] global_step=40, grad_norm=0.809727, loss=9.276811
I0609 03:46:52.458751 139726197217088 submission.py:296] 40) loss = 9.277, grad_norm = 0.810
I0609 03:46:52.905133 139668224792320 logging_writer.py:48] [41] global_step=41, grad_norm=0.770716, loss=9.272001
I0609 03:46:52.908861 139726197217088 submission.py:296] 41) loss = 9.272, grad_norm = 0.771
I0609 03:46:53.358490 139668233185024 logging_writer.py:48] [42] global_step=42, grad_norm=0.732209, loss=9.225374
I0609 03:46:53.361912 139726197217088 submission.py:296] 42) loss = 9.225, grad_norm = 0.732
I0609 03:46:53.817050 139668224792320 logging_writer.py:48] [43] global_step=43, grad_norm=0.706895, loss=9.207103
I0609 03:46:53.820534 139726197217088 submission.py:296] 43) loss = 9.207, grad_norm = 0.707
I0609 03:46:54.293120 139668233185024 logging_writer.py:48] [44] global_step=44, grad_norm=0.682095, loss=9.205619
I0609 03:46:54.296404 139726197217088 submission.py:296] 44) loss = 9.206, grad_norm = 0.682
I0609 03:46:54.746486 139668224792320 logging_writer.py:48] [45] global_step=45, grad_norm=0.663234, loss=9.153528
I0609 03:46:54.749736 139726197217088 submission.py:296] 45) loss = 9.154, grad_norm = 0.663
I0609 03:46:55.205175 139668233185024 logging_writer.py:48] [46] global_step=46, grad_norm=0.645796, loss=9.148548
I0609 03:46:55.208422 139726197217088 submission.py:296] 46) loss = 9.149, grad_norm = 0.646
I0609 03:46:55.660752 139668224792320 logging_writer.py:48] [47] global_step=47, grad_norm=0.607811, loss=9.130987
I0609 03:46:55.664258 139726197217088 submission.py:296] 47) loss = 9.131, grad_norm = 0.608
I0609 03:46:56.113216 139668233185024 logging_writer.py:48] [48] global_step=48, grad_norm=0.594192, loss=9.142017
I0609 03:46:56.116579 139726197217088 submission.py:296] 48) loss = 9.142, grad_norm = 0.594
I0609 03:46:56.573398 139668224792320 logging_writer.py:48] [49] global_step=49, grad_norm=0.562458, loss=9.106485
I0609 03:46:56.577030 139726197217088 submission.py:296] 49) loss = 9.106, grad_norm = 0.562
I0609 03:46:57.032111 139668233185024 logging_writer.py:48] [50] global_step=50, grad_norm=0.557289, loss=9.070780
I0609 03:46:57.035784 139726197217088 submission.py:296] 50) loss = 9.071, grad_norm = 0.557
I0609 03:46:57.499728 139668224792320 logging_writer.py:48] [51] global_step=51, grad_norm=0.532672, loss=9.037155
I0609 03:46:57.503225 139726197217088 submission.py:296] 51) loss = 9.037, grad_norm = 0.533
I0609 03:46:57.951812 139668233185024 logging_writer.py:48] [52] global_step=52, grad_norm=0.507954, loss=9.053214
I0609 03:46:57.954990 139726197217088 submission.py:296] 52) loss = 9.053, grad_norm = 0.508
I0609 03:46:58.406924 139668224792320 logging_writer.py:48] [53] global_step=53, grad_norm=0.499148, loss=9.070470
I0609 03:46:58.410093 139726197217088 submission.py:296] 53) loss = 9.070, grad_norm = 0.499
I0609 03:46:58.861704 139668233185024 logging_writer.py:48] [54] global_step=54, grad_norm=0.469649, loss=9.049574
I0609 03:46:58.865125 139726197217088 submission.py:296] 54) loss = 9.050, grad_norm = 0.470
I0609 03:46:59.313215 139668224792320 logging_writer.py:48] [55] global_step=55, grad_norm=0.451783, loss=9.026563
I0609 03:46:59.316610 139726197217088 submission.py:296] 55) loss = 9.027, grad_norm = 0.452
I0609 03:46:59.772830 139668233185024 logging_writer.py:48] [56] global_step=56, grad_norm=0.438021, loss=9.054983
I0609 03:46:59.777005 139726197217088 submission.py:296] 56) loss = 9.055, grad_norm = 0.438
I0609 03:47:00.231780 139668224792320 logging_writer.py:48] [57] global_step=57, grad_norm=0.421867, loss=8.975219
I0609 03:47:00.235564 139726197217088 submission.py:296] 57) loss = 8.975, grad_norm = 0.422
I0609 03:47:00.687311 139668233185024 logging_writer.py:48] [58] global_step=58, grad_norm=0.423607, loss=9.004833
I0609 03:47:00.690857 139726197217088 submission.py:296] 58) loss = 9.005, grad_norm = 0.424
I0609 03:47:01.138474 139668224792320 logging_writer.py:48] [59] global_step=59, grad_norm=0.397962, loss=8.971686
I0609 03:47:01.141874 139726197217088 submission.py:296] 59) loss = 8.972, grad_norm = 0.398
I0609 03:47:01.603155 139668233185024 logging_writer.py:48] [60] global_step=60, grad_norm=0.388654, loss=8.968870
I0609 03:47:01.607412 139726197217088 submission.py:296] 60) loss = 8.969, grad_norm = 0.389
I0609 03:47:02.056312 139668224792320 logging_writer.py:48] [61] global_step=61, grad_norm=0.378874, loss=8.939479
I0609 03:47:02.060284 139726197217088 submission.py:296] 61) loss = 8.939, grad_norm = 0.379
I0609 03:47:02.508748 139668233185024 logging_writer.py:48] [62] global_step=62, grad_norm=0.363037, loss=8.928896
I0609 03:47:02.512227 139726197217088 submission.py:296] 62) loss = 8.929, grad_norm = 0.363
I0609 03:47:02.960949 139668224792320 logging_writer.py:48] [63] global_step=63, grad_norm=0.354266, loss=8.934284
I0609 03:47:02.964184 139726197217088 submission.py:296] 63) loss = 8.934, grad_norm = 0.354
I0609 03:47:03.418622 139668233185024 logging_writer.py:48] [64] global_step=64, grad_norm=0.330245, loss=8.936604
I0609 03:47:03.421675 139726197217088 submission.py:296] 64) loss = 8.937, grad_norm = 0.330
I0609 03:47:03.868867 139668224792320 logging_writer.py:48] [65] global_step=65, grad_norm=0.328946, loss=8.929504
I0609 03:47:03.872492 139726197217088 submission.py:296] 65) loss = 8.930, grad_norm = 0.329
I0609 03:47:04.335991 139668233185024 logging_writer.py:48] [66] global_step=66, grad_norm=0.320749, loss=8.877491
I0609 03:47:04.339293 139726197217088 submission.py:296] 66) loss = 8.877, grad_norm = 0.321
I0609 03:47:04.790323 139668224792320 logging_writer.py:48] [67] global_step=67, grad_norm=0.306467, loss=8.865807
I0609 03:47:04.794444 139726197217088 submission.py:296] 67) loss = 8.866, grad_norm = 0.306
I0609 03:47:05.244498 139668233185024 logging_writer.py:48] [68] global_step=68, grad_norm=0.304950, loss=8.861927
I0609 03:47:05.248352 139726197217088 submission.py:296] 68) loss = 8.862, grad_norm = 0.305
I0609 03:47:05.695637 139668224792320 logging_writer.py:48] [69] global_step=69, grad_norm=0.297659, loss=8.865663
I0609 03:47:05.699321 139726197217088 submission.py:296] 69) loss = 8.866, grad_norm = 0.298
I0609 03:47:06.147673 139668233185024 logging_writer.py:48] [70] global_step=70, grad_norm=0.290037, loss=8.861485
I0609 03:47:06.151103 139726197217088 submission.py:296] 70) loss = 8.861, grad_norm = 0.290
I0609 03:47:06.605832 139668224792320 logging_writer.py:48] [71] global_step=71, grad_norm=0.278937, loss=8.837589
I0609 03:47:06.609135 139726197217088 submission.py:296] 71) loss = 8.838, grad_norm = 0.279
I0609 03:47:07.060299 139668233185024 logging_writer.py:48] [72] global_step=72, grad_norm=0.271250, loss=8.853615
I0609 03:47:07.063693 139726197217088 submission.py:296] 72) loss = 8.854, grad_norm = 0.271
I0609 03:47:07.512805 139668224792320 logging_writer.py:48] [73] global_step=73, grad_norm=0.266554, loss=8.828505
I0609 03:47:07.515988 139726197217088 submission.py:296] 73) loss = 8.829, grad_norm = 0.267
I0609 03:47:07.979827 139668233185024 logging_writer.py:48] [74] global_step=74, grad_norm=0.258557, loss=8.815013
I0609 03:47:07.983834 139726197217088 submission.py:296] 74) loss = 8.815, grad_norm = 0.259
I0609 03:47:08.433350 139668224792320 logging_writer.py:48] [75] global_step=75, grad_norm=0.253272, loss=8.809788
I0609 03:47:08.437294 139726197217088 submission.py:296] 75) loss = 8.810, grad_norm = 0.253
I0609 03:47:08.889147 139668233185024 logging_writer.py:48] [76] global_step=76, grad_norm=0.261335, loss=8.784612
I0609 03:47:08.892864 139726197217088 submission.py:296] 76) loss = 8.785, grad_norm = 0.261
I0609 03:47:09.340947 139668224792320 logging_writer.py:48] [77] global_step=77, grad_norm=0.236523, loss=8.809137
I0609 03:47:09.344770 139726197217088 submission.py:296] 77) loss = 8.809, grad_norm = 0.237
I0609 03:47:09.799634 139668233185024 logging_writer.py:48] [78] global_step=78, grad_norm=0.228808, loss=8.792060
I0609 03:47:09.803244 139726197217088 submission.py:296] 78) loss = 8.792, grad_norm = 0.229
I0609 03:47:10.250999 139668224792320 logging_writer.py:48] [79] global_step=79, grad_norm=0.234330, loss=8.804273
I0609 03:47:10.254864 139726197217088 submission.py:296] 79) loss = 8.804, grad_norm = 0.234
I0609 03:47:10.702190 139668233185024 logging_writer.py:48] [80] global_step=80, grad_norm=0.217740, loss=8.831993
I0609 03:47:10.705947 139726197217088 submission.py:296] 80) loss = 8.832, grad_norm = 0.218
I0609 03:47:11.154327 139668224792320 logging_writer.py:48] [81] global_step=81, grad_norm=0.229572, loss=8.746012
I0609 03:47:11.157749 139726197217088 submission.py:296] 81) loss = 8.746, grad_norm = 0.230
I0609 03:47:11.607409 139668233185024 logging_writer.py:48] [82] global_step=82, grad_norm=0.215069, loss=8.772823
I0609 03:47:11.610749 139726197217088 submission.py:296] 82) loss = 8.773, grad_norm = 0.215
I0609 03:47:12.056340 139668224792320 logging_writer.py:48] [83] global_step=83, grad_norm=0.204038, loss=8.769003
I0609 03:47:12.060659 139726197217088 submission.py:296] 83) loss = 8.769, grad_norm = 0.204
I0609 03:47:12.513287 139668233185024 logging_writer.py:48] [84] global_step=84, grad_norm=0.204207, loss=8.769602
I0609 03:47:12.517577 139726197217088 submission.py:296] 84) loss = 8.770, grad_norm = 0.204
I0609 03:47:12.969039 139668224792320 logging_writer.py:48] [85] global_step=85, grad_norm=0.208220, loss=8.758141
I0609 03:47:12.973002 139726197217088 submission.py:296] 85) loss = 8.758, grad_norm = 0.208
I0609 03:47:13.423207 139668233185024 logging_writer.py:48] [86] global_step=86, grad_norm=0.204325, loss=8.756136
I0609 03:47:13.427048 139726197217088 submission.py:296] 86) loss = 8.756, grad_norm = 0.204
I0609 03:47:13.877149 139668224792320 logging_writer.py:48] [87] global_step=87, grad_norm=0.198616, loss=8.723284
I0609 03:47:13.881084 139726197217088 submission.py:296] 87) loss = 8.723, grad_norm = 0.199
I0609 03:47:14.334469 139668233185024 logging_writer.py:48] [88] global_step=88, grad_norm=0.198336, loss=8.711479
I0609 03:47:14.337770 139726197217088 submission.py:296] 88) loss = 8.711, grad_norm = 0.198
I0609 03:47:14.789572 139668224792320 logging_writer.py:48] [89] global_step=89, grad_norm=0.193703, loss=8.717615
I0609 03:47:14.792778 139726197217088 submission.py:296] 89) loss = 8.718, grad_norm = 0.194
I0609 03:47:15.240494 139668233185024 logging_writer.py:48] [90] global_step=90, grad_norm=0.190901, loss=8.738930
I0609 03:47:15.244470 139726197217088 submission.py:296] 90) loss = 8.739, grad_norm = 0.191
I0609 03:47:15.697842 139668224792320 logging_writer.py:48] [91] global_step=91, grad_norm=0.202624, loss=8.762340
I0609 03:47:15.701038 139726197217088 submission.py:296] 91) loss = 8.762, grad_norm = 0.203
I0609 03:47:16.153534 139668233185024 logging_writer.py:48] [92] global_step=92, grad_norm=0.182152, loss=8.721395
I0609 03:47:16.156671 139726197217088 submission.py:296] 92) loss = 8.721, grad_norm = 0.182
I0609 03:47:16.605497 139668224792320 logging_writer.py:48] [93] global_step=93, grad_norm=0.193878, loss=8.690388
I0609 03:47:16.609067 139726197217088 submission.py:296] 93) loss = 8.690, grad_norm = 0.194
I0609 03:47:17.060626 139668233185024 logging_writer.py:48] [94] global_step=94, grad_norm=0.193214, loss=8.717894
I0609 03:47:17.064300 139726197217088 submission.py:296] 94) loss = 8.718, grad_norm = 0.193
I0609 03:47:17.518339 139668224792320 logging_writer.py:48] [95] global_step=95, grad_norm=0.179143, loss=8.709694
I0609 03:47:17.522283 139726197217088 submission.py:296] 95) loss = 8.710, grad_norm = 0.179
I0609 03:47:17.972950 139668233185024 logging_writer.py:48] [96] global_step=96, grad_norm=0.178841, loss=8.678846
I0609 03:47:17.977036 139726197217088 submission.py:296] 96) loss = 8.679, grad_norm = 0.179
I0609 03:47:18.428239 139668224792320 logging_writer.py:48] [97] global_step=97, grad_norm=0.181552, loss=8.696379
I0609 03:47:18.431993 139726197217088 submission.py:296] 97) loss = 8.696, grad_norm = 0.182
I0609 03:47:18.879572 139668233185024 logging_writer.py:48] [98] global_step=98, grad_norm=0.187289, loss=8.713015
I0609 03:47:18.883823 139726197217088 submission.py:296] 98) loss = 8.713, grad_norm = 0.187
I0609 03:47:19.347990 139668224792320 logging_writer.py:48] [99] global_step=99, grad_norm=0.174221, loss=8.730592
I0609 03:47:19.351410 139726197217088 submission.py:296] 99) loss = 8.731, grad_norm = 0.174
I0609 03:47:19.799603 139668233185024 logging_writer.py:48] [100] global_step=100, grad_norm=0.177604, loss=8.693222
I0609 03:47:19.802999 139726197217088 submission.py:296] 100) loss = 8.693, grad_norm = 0.178
I0609 03:50:18.159359 139668224792320 logging_writer.py:48] [500] global_step=500, grad_norm=0.635115, loss=6.968908
I0609 03:50:18.163778 139726197217088 submission.py:296] 500) loss = 6.969, grad_norm = 0.635
I0609 03:54:01.261496 139668233185024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.799958, loss=5.742962
I0609 03:54:01.265333 139726197217088 submission.py:296] 1000) loss = 5.743, grad_norm = 0.800
I0609 03:57:44.121610 139668224792320 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.041340, loss=4.988699
I0609 03:57:44.125321 139726197217088 submission.py:296] 1500) loss = 4.989, grad_norm = 1.041
I0609 04:00:34.165963 139726197217088 spec.py:298] Evaluating on the training split.
I0609 04:00:38.060992 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:03:30.843531 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 04:03:34.579551 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:05:56.633515 139726197217088 spec.py:326] Evaluating on the test split.
I0609 04:06:00.442078 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:08:12.202178 139726197217088 submission_runner.py:419] Time since start: 2130.38s, 	Step: 1882, 	{'train/accuracy': 0.4645883881650817, 'train/loss': 3.4762316301185563, 'train/bleu': 17.01688757131775, 'validation/accuracy': 0.45680772712055645, 'validation/loss': 3.558908135051022, 'validation/bleu': 13.265436520096918, 'validation/num_examples': 3000, 'test/accuracy': 0.44730695485445354, 'test/loss': 3.6861385886932774, 'test/bleu': 11.501741281065073, 'test/num_examples': 3003, 'score': 843.7392175197601, 'total_duration': 2130.3839128017426, 'accumulated_submission_time': 843.7392175197601, 'accumulated_eval_time': 1285.926302433014, 'accumulated_logging_time': 0.026691675186157227}
I0609 04:08:12.216298 139668233185024 logging_writer.py:48] [1882] accumulated_eval_time=1285.926302, accumulated_logging_time=0.026692, accumulated_submission_time=843.739218, global_step=1882, preemption_count=0, score=843.739218, test/accuracy=0.447307, test/bleu=11.501741, test/loss=3.686139, test/num_examples=3003, total_duration=2130.383913, train/accuracy=0.464588, train/bleu=17.016888, train/loss=3.476232, validation/accuracy=0.456808, validation/bleu=13.265437, validation/loss=3.558908, validation/num_examples=3000
I0609 04:09:05.306261 139668224792320 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.619827, loss=4.297148
I0609 04:09:05.309965 139726197217088 submission.py:296] 2000) loss = 4.297, grad_norm = 0.620
I0609 04:12:48.450904 139668233185024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.551068, loss=4.168973
I0609 04:12:48.454481 139726197217088 submission.py:296] 2500) loss = 4.169, grad_norm = 0.551
I0609 04:16:31.773684 139668224792320 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.458088, loss=3.909447
I0609 04:16:31.777635 139726197217088 submission.py:296] 3000) loss = 3.909, grad_norm = 0.458
I0609 04:20:14.806449 139668233185024 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.663820, loss=3.697320
I0609 04:20:14.809980 139726197217088 submission.py:296] 3500) loss = 3.697, grad_norm = 0.664
I0609 04:22:12.346578 139726197217088 spec.py:298] Evaluating on the training split.
I0609 04:22:16.223220 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:24:48.090230 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 04:24:51.823622 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:27:10.514781 139726197217088 spec.py:326] Evaluating on the test split.
I0609 04:27:14.329599 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:29:27.504803 139726197217088 submission_runner.py:419] Time since start: 3405.69s, 	Step: 3764, 	{'train/accuracy': 0.5476749594436693, 'train/loss': 2.643866670353606, 'train/bleu': 25.120871441361505, 'validation/accuracy': 0.5546738416138671, 'validation/loss': 2.5749764029584257, 'validation/bleu': 21.30386749298869, 'validation/num_examples': 3000, 'test/accuracy': 0.5551682063796409, 'test/loss': 2.5954954317006567, 'test/bleu': 19.86985380051431, 'test/num_examples': 3003, 'score': 1683.1617560386658, 'total_duration': 3405.6865434646606, 'accumulated_submission_time': 1683.1617560386658, 'accumulated_eval_time': 1721.0844223499298, 'accumulated_logging_time': 0.051592111587524414}
I0609 04:29:27.515480 139668224792320 logging_writer.py:48] [3764] accumulated_eval_time=1721.084422, accumulated_logging_time=0.051592, accumulated_submission_time=1683.161756, global_step=3764, preemption_count=0, score=1683.161756, test/accuracy=0.555168, test/bleu=19.869854, test/loss=2.595495, test/num_examples=3003, total_duration=3405.686543, train/accuracy=0.547675, train/bleu=25.120871, train/loss=2.643867, validation/accuracy=0.554674, validation/bleu=21.303867, validation/loss=2.574976, validation/num_examples=3000
I0609 04:31:13.442743 139668233185024 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.524285, loss=3.685910
I0609 04:31:13.446270 139726197217088 submission.py:296] 4000) loss = 3.686, grad_norm = 0.524
I0609 04:34:56.447040 139668224792320 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.357940, loss=3.591203
I0609 04:34:56.450938 139726197217088 submission.py:296] 4500) loss = 3.591, grad_norm = 0.358
I0609 04:38:39.420059 139668233185024 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.341091, loss=3.407851
I0609 04:38:39.423738 139726197217088 submission.py:296] 5000) loss = 3.408, grad_norm = 0.341
I0609 04:42:22.529432 139668224792320 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.281688, loss=3.489743
I0609 04:42:22.533678 139726197217088 submission.py:296] 5500) loss = 3.490, grad_norm = 0.282
I0609 04:43:27.676149 139726197217088 spec.py:298] Evaluating on the training split.
I0609 04:43:31.566311 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:46:02.288533 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 04:46:06.027899 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:48:16.116696 139726197217088 spec.py:326] Evaluating on the test split.
I0609 04:48:19.917637 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 04:50:27.208794 139726197217088 submission_runner.py:419] Time since start: 4665.39s, 	Step: 5647, 	{'train/accuracy': 0.5756558982169646, 'train/loss': 2.375983954685478, 'train/bleu': 27.344970445081287, 'validation/accuracy': 0.5916107673804416, 'validation/loss': 2.2607318105169187, 'validation/bleu': 23.77760843648078, 'validation/num_examples': 3000, 'test/accuracy': 0.5922142815641159, 'test/loss': 2.255566207657893, 'test/bleu': 22.204193210583583, 'test/num_examples': 3003, 'score': 2522.6290516853333, 'total_duration': 4665.390494585037, 'accumulated_submission_time': 2522.6290516853333, 'accumulated_eval_time': 2140.6169533729553, 'accumulated_logging_time': 0.07140898704528809}
I0609 04:50:27.219559 139668233185024 logging_writer.py:48] [5647] accumulated_eval_time=2140.616953, accumulated_logging_time=0.071409, accumulated_submission_time=2522.629052, global_step=5647, preemption_count=0, score=2522.629052, test/accuracy=0.592214, test/bleu=22.204193, test/loss=2.255566, test/num_examples=3003, total_duration=4665.390495, train/accuracy=0.575656, train/bleu=27.344970, train/loss=2.375984, validation/accuracy=0.591611, validation/bleu=23.777608, validation/loss=2.260732, validation/num_examples=3000
I0609 04:53:05.438409 139668224792320 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.234586, loss=3.394795
I0609 04:53:05.442172 139726197217088 submission.py:296] 6000) loss = 3.395, grad_norm = 0.235
I0609 04:56:48.482205 139668233185024 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.216279, loss=3.419304
I0609 04:56:48.486016 139726197217088 submission.py:296] 6500) loss = 3.419, grad_norm = 0.216
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0609 05:00:31.509911 139668224792320 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.197088, loss=3.354825
I0609 05:00:31.513585 139726197217088 submission.py:296] 7000) loss = 3.355, grad_norm = 0.197
I0609 05:04:14.606126 139668233185024 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.190415, loss=3.281391
I0609 05:04:14.610357 139726197217088 submission.py:296] 7500) loss = 3.281, grad_norm = 0.190
I0609 05:04:27.531726 139726197217088 spec.py:298] Evaluating on the training split.
I0609 05:04:31.410102 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:06:49.072438 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 05:06:52.784729 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:09:04.101249 139726197217088 spec.py:326] Evaluating on the test split.
I0609 05:09:07.920446 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:11:11.578449 139726197217088 submission_runner.py:419] Time since start: 5909.76s, 	Step: 7530, 	{'train/accuracy': 0.602557796360059, 'train/loss': 2.152053615346778, 'train/bleu': 28.825764219095248, 'validation/accuracy': 0.610829375953181, 'validation/loss': 2.0860369834224004, 'validation/bleu': 25.005126186599863, 'validation/num_examples': 3000, 'test/accuracy': 0.613584335599326, 'test/loss': 2.060310448259834, 'test/bleu': 23.547994607245737, 'test/num_examples': 3003, 'score': 3362.2392139434814, 'total_duration': 5909.7601499557495, 'accumulated_submission_time': 3362.2392139434814, 'accumulated_eval_time': 2544.6635942459106, 'accumulated_logging_time': 0.09342408180236816}
I0609 05:11:11.589537 139668224792320 logging_writer.py:48] [7530] accumulated_eval_time=2544.663594, accumulated_logging_time=0.093424, accumulated_submission_time=3362.239214, global_step=7530, preemption_count=0, score=3362.239214, test/accuracy=0.613584, test/bleu=23.547995, test/loss=2.060310, test/num_examples=3003, total_duration=5909.760150, train/accuracy=0.602558, train/bleu=28.825764, train/loss=2.152054, validation/accuracy=0.610829, validation/bleu=25.005126, validation/loss=2.086037, validation/num_examples=3000
I0609 05:14:41.659916 139668233185024 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.168460, loss=3.365668
I0609 05:14:41.663388 139726197217088 submission.py:296] 8000) loss = 3.366, grad_norm = 0.168
I0609 05:18:24.638198 139668224792320 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.174038, loss=3.260803
I0609 05:18:24.641753 139726197217088 submission.py:296] 8500) loss = 3.261, grad_norm = 0.174
I0609 05:22:07.524037 139668233185024 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.166005, loss=3.282831
I0609 05:22:07.527954 139726197217088 submission.py:296] 9000) loss = 3.283, grad_norm = 0.166
I0609 05:25:11.618834 139726197217088 spec.py:298] Evaluating on the training split.
I0609 05:25:15.492256 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:28:05.712499 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 05:28:09.435520 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:30:31.941132 139726197217088 spec.py:326] Evaluating on the test split.
I0609 05:30:35.741989 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:32:34.294471 139726197217088 submission_runner.py:419] Time since start: 7192.48s, 	Step: 9414, 	{'train/accuracy': 0.6078900860840317, 'train/loss': 2.1025819864887976, 'train/bleu': 29.110805198319753, 'validation/accuracy': 0.6267002269035722, 'validation/loss': 1.9638508047017396, 'validation/bleu': 25.503471341900973, 'validation/num_examples': 3000, 'test/accuracy': 0.629260356748591, 'test/loss': 1.931768926849108, 'test/bleu': 24.520757914283436, 'test/num_examples': 3003, 'score': 4201.573984861374, 'total_duration': 7192.476225614548, 'accumulated_submission_time': 4201.573984861374, 'accumulated_eval_time': 2987.339191675186, 'accumulated_logging_time': 0.1149282455444336}
I0609 05:32:34.305139 139668224792320 logging_writer.py:48] [9414] accumulated_eval_time=2987.339192, accumulated_logging_time=0.114928, accumulated_submission_time=4201.573985, global_step=9414, preemption_count=0, score=4201.573985, test/accuracy=0.629260, test/bleu=24.520758, test/loss=1.931769, test/num_examples=3003, total_duration=7192.476226, train/accuracy=0.607890, train/bleu=29.110805, train/loss=2.102582, validation/accuracy=0.626700, validation/bleu=25.503471, validation/loss=1.963851, validation/num_examples=3000
I0609 05:33:13.092739 139668233185024 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.160239, loss=3.144616
I0609 05:33:13.096184 139726197217088 submission.py:296] 9500) loss = 3.145, grad_norm = 0.160
I0609 05:36:56.334853 139668224792320 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.170328, loss=3.149345
I0609 05:36:56.338365 139726197217088 submission.py:296] 10000) loss = 3.149, grad_norm = 0.170
I0609 05:40:39.447087 139668233185024 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.139202, loss=3.169533
I0609 05:40:39.451138 139726197217088 submission.py:296] 10500) loss = 3.170, grad_norm = 0.139
I0609 05:44:22.401681 139668224792320 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.161104, loss=3.186442
I0609 05:44:22.405225 139726197217088 submission.py:296] 11000) loss = 3.186, grad_norm = 0.161
I0609 05:46:34.410301 139726197217088 spec.py:298] Evaluating on the training split.
I0609 05:46:38.281802 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:49:06.338776 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 05:49:10.078771 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:51:17.806187 139726197217088 spec.py:326] Evaluating on the test split.
I0609 05:51:21.619946 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 05:53:18.333719 139726197217088 submission_runner.py:419] Time since start: 8436.52s, 	Step: 11297, 	{'train/accuracy': 0.616138961677976, 'train/loss': 2.028408960876978, 'train/bleu': 30.04789766866057, 'validation/accuracy': 0.6363219302922468, 'validation/loss': 1.8807113364992374, 'validation/bleu': 26.6587106864602, 'validation/num_examples': 3000, 'test/accuracy': 0.6435535413398408, 'test/loss': 1.8344136743942827, 'test/bleu': 25.93232069239458, 'test/num_examples': 3003, 'score': 5040.992859840393, 'total_duration': 8436.515478372574, 'accumulated_submission_time': 5040.992859840393, 'accumulated_eval_time': 3391.262586593628, 'accumulated_logging_time': 0.13532805442810059}
I0609 05:53:18.346755 139668233185024 logging_writer.py:48] [11297] accumulated_eval_time=3391.262587, accumulated_logging_time=0.135328, accumulated_submission_time=5040.992860, global_step=11297, preemption_count=0, score=5040.992860, test/accuracy=0.643554, test/bleu=25.932321, test/loss=1.834414, test/num_examples=3003, total_duration=8436.515478, train/accuracy=0.616139, train/bleu=30.047898, train/loss=2.028409, validation/accuracy=0.636322, validation/bleu=26.658711, validation/loss=1.880711, validation/num_examples=3000
I0609 05:54:49.437238 139668224792320 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.159173, loss=3.141521
I0609 05:54:49.440892 139726197217088 submission.py:296] 11500) loss = 3.142, grad_norm = 0.159
I0609 05:58:32.212374 139668233185024 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.168675, loss=3.170614
I0609 05:58:32.215940 139726197217088 submission.py:296] 12000) loss = 3.171, grad_norm = 0.169
I0609 06:02:15.271924 139668224792320 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.172066, loss=3.062493
I0609 06:02:15.276302 139726197217088 submission.py:296] 12500) loss = 3.062, grad_norm = 0.172
I0609 06:05:58.140579 139668233185024 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.145906, loss=3.083731
I0609 06:05:58.144438 139726197217088 submission.py:296] 13000) loss = 3.084, grad_norm = 0.146
I0609 06:07:18.433730 139726197217088 spec.py:298] Evaluating on the training split.
I0609 06:07:22.307679 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:10:20.967368 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 06:10:24.707026 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:12:35.695518 139726197217088 spec.py:326] Evaluating on the test split.
I0609 06:12:39.506415 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:14:36.809125 139726197217088 submission_runner.py:419] Time since start: 9714.99s, 	Step: 13181, 	{'train/accuracy': 0.6357882161032039, 'train/loss': 1.879510287073095, 'train/bleu': 31.044697743354995, 'validation/accuracy': 0.6417651362041388, 'validation/loss': 1.829293034184325, 'validation/bleu': 27.142555555337918, 'validation/num_examples': 3000, 'test/accuracy': 0.6523037592237523, 'test/loss': 1.7721255664981699, 'test/bleu': 26.3076047627652, 'test/num_examples': 3003, 'score': 5880.397294044495, 'total_duration': 9714.990818023682, 'accumulated_submission_time': 5880.397294044495, 'accumulated_eval_time': 3829.637838602066, 'accumulated_logging_time': 0.15727782249450684}
I0609 06:14:36.820569 139668224792320 logging_writer.py:48] [13181] accumulated_eval_time=3829.637839, accumulated_logging_time=0.157278, accumulated_submission_time=5880.397294, global_step=13181, preemption_count=0, score=5880.397294, test/accuracy=0.652304, test/bleu=26.307605, test/loss=1.772126, test/num_examples=3003, total_duration=9714.990818, train/accuracy=0.635788, train/bleu=31.044698, train/loss=1.879510, validation/accuracy=0.641765, validation/bleu=27.142556, validation/loss=1.829293, validation/num_examples=3000
I0609 06:16:59.443999 139668233185024 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.155865, loss=3.145932
I0609 06:16:59.447709 139726197217088 submission.py:296] 13500) loss = 3.146, grad_norm = 0.156
I0609 06:20:42.313646 139668224792320 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.144573, loss=3.043835
I0609 06:20:42.318619 139726197217088 submission.py:296] 14000) loss = 3.044, grad_norm = 0.145
I0609 06:24:25.077826 139668233185024 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.248018, loss=3.109452
I0609 06:24:25.082075 139726197217088 submission.py:296] 14500) loss = 3.109, grad_norm = 0.248
I0609 06:28:08.190537 139668224792320 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.155046, loss=3.004433
I0609 06:28:08.194225 139726197217088 submission.py:296] 15000) loss = 3.004, grad_norm = 0.155
I0609 06:28:37.138886 139726197217088 spec.py:298] Evaluating on the training split.
I0609 06:28:41.033468 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:30:55.035560 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 06:30:58.773228 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:33:05.341728 139726197217088 spec.py:326] Evaluating on the test split.
I0609 06:33:09.151274 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:35:12.635470 139726197217088 submission_runner.py:419] Time since start: 10950.82s, 	Step: 15066, 	{'train/accuracy': 0.6320160596501291, 'train/loss': 1.9006086894178378, 'train/bleu': 31.13618389009489, 'validation/accuracy': 0.6492293957917447, 'validation/loss': 1.7852161628498098, 'validation/bleu': 27.402033333848323, 'validation/num_examples': 3000, 'test/accuracy': 0.6593806286677125, 'test/loss': 1.7252233310092382, 'test/bleu': 26.704391696208766, 'test/num_examples': 3003, 'score': 6720.030926942825, 'total_duration': 10950.817243337631, 'accumulated_submission_time': 6720.030926942825, 'accumulated_eval_time': 4225.134322166443, 'accumulated_logging_time': 0.17809152603149414}
I0609 06:35:12.646351 139668233185024 logging_writer.py:48] [15066] accumulated_eval_time=4225.134322, accumulated_logging_time=0.178092, accumulated_submission_time=6720.030927, global_step=15066, preemption_count=0, score=6720.030927, test/accuracy=0.659381, test/bleu=26.704392, test/loss=1.725223, test/num_examples=3003, total_duration=10950.817243, train/accuracy=0.632016, train/bleu=31.136184, train/loss=1.900609, validation/accuracy=0.649229, validation/bleu=27.402033, validation/loss=1.785216, validation/num_examples=3000
I0609 06:38:26.450064 139668224792320 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.152775, loss=3.063988
I0609 06:38:26.454244 139726197217088 submission.py:296] 15500) loss = 3.064, grad_norm = 0.153
I0609 06:42:09.173784 139668233185024 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.162148, loss=3.039632
I0609 06:42:09.177323 139726197217088 submission.py:296] 16000) loss = 3.040, grad_norm = 0.162
I0609 06:45:52.240773 139668224792320 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.182163, loss=3.075519
I0609 06:45:52.244225 139726197217088 submission.py:296] 16500) loss = 3.076, grad_norm = 0.182
I0609 06:49:12.794534 139726197217088 spec.py:298] Evaluating on the training split.
I0609 06:49:16.661156 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:52:31.992265 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 06:52:35.710561 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:54:48.829448 139726197217088 spec.py:326] Evaluating on the test split.
I0609 06:54:52.634315 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 06:57:06.366848 139726197217088 submission_runner.py:419] Time since start: 12264.55s, 	Step: 16951, 	{'train/accuracy': 0.6338624949489119, 'train/loss': 1.897482032557871, 'train/bleu': 30.743680885303018, 'validation/accuracy': 0.6536682744169322, 'validation/loss': 1.752701260678727, 'validation/bleu': 27.726988063128722, 'validation/num_examples': 3000, 'test/accuracy': 0.6633315902620417, 'test/loss': 1.6888481567021092, 'test/bleu': 27.061127124681565, 'test/num_examples': 3003, 'score': 7559.490671634674, 'total_duration': 12264.548608064651, 'accumulated_submission_time': 7559.490671634674, 'accumulated_eval_time': 4698.706595897675, 'accumulated_logging_time': 0.1993236541748047}
I0609 06:57:06.377894 139668233185024 logging_writer.py:48] [16951] accumulated_eval_time=4698.706596, accumulated_logging_time=0.199324, accumulated_submission_time=7559.490672, global_step=16951, preemption_count=0, score=7559.490672, test/accuracy=0.663332, test/bleu=27.061127, test/loss=1.688848, test/num_examples=3003, total_duration=12264.548608, train/accuracy=0.633862, train/bleu=30.743681, train/loss=1.897482, validation/accuracy=0.653668, validation/bleu=27.726988, validation/loss=1.752701, validation/num_examples=3000
I0609 06:57:28.689848 139668224792320 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.173594, loss=2.984776
I0609 06:57:28.693552 139726197217088 submission.py:296] 17000) loss = 2.985, grad_norm = 0.174
I0609 07:01:11.679712 139668233185024 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.164971, loss=3.020260
I0609 07:01:11.683231 139726197217088 submission.py:296] 17500) loss = 3.020, grad_norm = 0.165
I0609 07:04:54.589948 139668224792320 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.171111, loss=2.903705
I0609 07:04:54.593479 139726197217088 submission.py:296] 18000) loss = 2.904, grad_norm = 0.171
I0609 07:08:37.588040 139668233185024 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.220166, loss=2.993178
I0609 07:08:37.592458 139726197217088 submission.py:296] 18500) loss = 2.993, grad_norm = 0.220
I0609 07:11:06.461642 139726197217088 spec.py:298] Evaluating on the training split.
I0609 07:11:10.371464 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 07:15:06.634085 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 07:15:10.347037 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 07:17:34.579069 139726197217088 spec.py:326] Evaluating on the test split.
I0609 07:17:38.376169 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 07:19:46.357358 139726197217088 submission_runner.py:419] Time since start: 13624.54s, 	Step: 18835, 	{'train/accuracy': 0.6834847576442374, 'train/loss': 1.585972809920214, 'train/bleu': 34.234715122596, 'validation/accuracy': 0.6579583638144598, 'validation/loss': 1.7184789633730517, 'validation/bleu': 28.60348997237404, 'validation/num_examples': 3000, 'test/accuracy': 0.6673755156585904, 'test/loss': 1.6571685622566963, 'test/bleu': 27.59825309218052, 'test/num_examples': 3003, 'score': 8398.888617038727, 'total_duration': 13624.539085626602, 'accumulated_submission_time': 8398.888617038727, 'accumulated_eval_time': 5218.602213382721, 'accumulated_logging_time': 0.2195441722869873}
I0609 07:19:46.368120 139668224792320 logging_writer.py:48] [18835] accumulated_eval_time=5218.602213, accumulated_logging_time=0.219544, accumulated_submission_time=8398.888617, global_step=18835, preemption_count=0, score=8398.888617, test/accuracy=0.667376, test/bleu=27.598253, test/loss=1.657169, test/num_examples=3003, total_duration=13624.539086, train/accuracy=0.683485, train/bleu=34.234715, train/loss=1.585973, validation/accuracy=0.657958, validation/bleu=28.603490, validation/loss=1.718479, validation/num_examples=3000
I0609 07:21:00.324707 139668233185024 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.178442, loss=2.884705
I0609 07:21:00.328035 139726197217088 submission.py:296] 19000) loss = 2.885, grad_norm = 0.178
I0609 07:24:43.268389 139668224792320 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.176136, loss=2.911505
I0609 07:24:43.272917 139726197217088 submission.py:296] 19500) loss = 2.912, grad_norm = 0.176
I0609 07:28:25.719640 139726197217088 spec.py:298] Evaluating on the training split.
I0609 07:28:29.595342 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 07:31:25.907812 139726197217088 spec.py:310] Evaluating on the validation split.
I0609 07:31:29.622536 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 07:33:41.069148 139726197217088 spec.py:326] Evaluating on the test split.
I0609 07:33:44.866566 139726197217088 workload.py:130] Translating evaluation dataset.
I0609 07:35:52.280609 139726197217088 submission_runner.py:419] Time since start: 14590.46s, 	Step: 20000, 	{'train/accuracy': 0.6499681282156354, 'train/loss': 1.7739541715043483, 'train/bleu': 32.038508242884866, 'validation/accuracy': 0.6618144846313126, 'validation/loss': 1.697380340913318, 'validation/bleu': 28.226938120374303, 'validation/num_examples': 3000, 'test/accuracy': 0.6702341525768404, 'test/loss': 1.6397721660565918, 'test/bleu': 27.70530786994273, 'test/num_examples': 3003, 'score': 8917.815506219864, 'total_duration': 14590.462373971939, 'accumulated_submission_time': 8917.815506219864, 'accumulated_eval_time': 5665.163120269775, 'accumulated_logging_time': 0.23902559280395508}
I0609 07:35:52.291989 139668233185024 logging_writer.py:48] [20000] accumulated_eval_time=5665.163120, accumulated_logging_time=0.239026, accumulated_submission_time=8917.815506, global_step=20000, preemption_count=0, score=8917.815506, test/accuracy=0.670234, test/bleu=27.705308, test/loss=1.639772, test/num_examples=3003, total_duration=14590.462374, train/accuracy=0.649968, train/bleu=32.038508, train/loss=1.773954, validation/accuracy=0.661814, validation/bleu=28.226938, validation/loss=1.697380, validation/num_examples=3000
I0609 07:35:52.309992 139668224792320 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=8917.815506
I0609 07:35:54.639124 139726197217088 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nadamw/wmt_pytorch/trial_1/checkpoint_20000.
I0609 07:35:54.662591 139726197217088 submission_runner.py:581] Tuning trial 1/1
I0609 07:35:54.662771 139726197217088 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 07:35:54.663793 139726197217088 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006492027334851936, 'train/loss': 11.039222665148063, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.024672663699148, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.017222996920575, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.448609113693237, 'total_duration': 832.3391466140747, 'accumulated_submission_time': 4.448609113693237, 'accumulated_eval_time': 827.8901655673981, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1882, {'train/accuracy': 0.4645883881650817, 'train/loss': 3.4762316301185563, 'train/bleu': 17.01688757131775, 'validation/accuracy': 0.45680772712055645, 'validation/loss': 3.558908135051022, 'validation/bleu': 13.265436520096918, 'validation/num_examples': 3000, 'test/accuracy': 0.44730695485445354, 'test/loss': 3.6861385886932774, 'test/bleu': 11.501741281065073, 'test/num_examples': 3003, 'score': 843.7392175197601, 'total_duration': 2130.3839128017426, 'accumulated_submission_time': 843.7392175197601, 'accumulated_eval_time': 1285.926302433014, 'accumulated_logging_time': 0.026691675186157227, 'global_step': 1882, 'preemption_count': 0}), (3764, {'train/accuracy': 0.5476749594436693, 'train/loss': 2.643866670353606, 'train/bleu': 25.120871441361505, 'validation/accuracy': 0.5546738416138671, 'validation/loss': 2.5749764029584257, 'validation/bleu': 21.30386749298869, 'validation/num_examples': 3000, 'test/accuracy': 0.5551682063796409, 'test/loss': 2.5954954317006567, 'test/bleu': 19.86985380051431, 'test/num_examples': 3003, 'score': 1683.1617560386658, 'total_duration': 3405.6865434646606, 'accumulated_submission_time': 1683.1617560386658, 'accumulated_eval_time': 1721.0844223499298, 'accumulated_logging_time': 0.051592111587524414, 'global_step': 3764, 'preemption_count': 0}), (5647, {'train/accuracy': 0.5756558982169646, 'train/loss': 2.375983954685478, 'train/bleu': 27.344970445081287, 'validation/accuracy': 0.5916107673804416, 'validation/loss': 2.2607318105169187, 'validation/bleu': 23.77760843648078, 'validation/num_examples': 3000, 'test/accuracy': 0.5922142815641159, 'test/loss': 2.255566207657893, 'test/bleu': 22.204193210583583, 'test/num_examples': 3003, 'score': 2522.6290516853333, 'total_duration': 4665.390494585037, 'accumulated_submission_time': 2522.6290516853333, 'accumulated_eval_time': 2140.6169533729553, 'accumulated_logging_time': 0.07140898704528809, 'global_step': 5647, 'preemption_count': 0}), (7530, {'train/accuracy': 0.602557796360059, 'train/loss': 2.152053615346778, 'train/bleu': 28.825764219095248, 'validation/accuracy': 0.610829375953181, 'validation/loss': 2.0860369834224004, 'validation/bleu': 25.005126186599863, 'validation/num_examples': 3000, 'test/accuracy': 0.613584335599326, 'test/loss': 2.060310448259834, 'test/bleu': 23.547994607245737, 'test/num_examples': 3003, 'score': 3362.2392139434814, 'total_duration': 5909.7601499557495, 'accumulated_submission_time': 3362.2392139434814, 'accumulated_eval_time': 2544.6635942459106, 'accumulated_logging_time': 0.09342408180236816, 'global_step': 7530, 'preemption_count': 0}), (9414, {'train/accuracy': 0.6078900860840317, 'train/loss': 2.1025819864887976, 'train/bleu': 29.110805198319753, 'validation/accuracy': 0.6267002269035722, 'validation/loss': 1.9638508047017396, 'validation/bleu': 25.503471341900973, 'validation/num_examples': 3000, 'test/accuracy': 0.629260356748591, 'test/loss': 1.931768926849108, 'test/bleu': 24.520757914283436, 'test/num_examples': 3003, 'score': 4201.573984861374, 'total_duration': 7192.476225614548, 'accumulated_submission_time': 4201.573984861374, 'accumulated_eval_time': 2987.339191675186, 'accumulated_logging_time': 0.1149282455444336, 'global_step': 9414, 'preemption_count': 0}), (11297, {'train/accuracy': 0.616138961677976, 'train/loss': 2.028408960876978, 'train/bleu': 30.04789766866057, 'validation/accuracy': 0.6363219302922468, 'validation/loss': 1.8807113364992374, 'validation/bleu': 26.6587106864602, 'validation/num_examples': 3000, 'test/accuracy': 0.6435535413398408, 'test/loss': 1.8344136743942827, 'test/bleu': 25.93232069239458, 'test/num_examples': 3003, 'score': 5040.992859840393, 'total_duration': 8436.515478372574, 'accumulated_submission_time': 5040.992859840393, 'accumulated_eval_time': 3391.262586593628, 'accumulated_logging_time': 0.13532805442810059, 'global_step': 11297, 'preemption_count': 0}), (13181, {'train/accuracy': 0.6357882161032039, 'train/loss': 1.879510287073095, 'train/bleu': 31.044697743354995, 'validation/accuracy': 0.6417651362041388, 'validation/loss': 1.829293034184325, 'validation/bleu': 27.142555555337918, 'validation/num_examples': 3000, 'test/accuracy': 0.6523037592237523, 'test/loss': 1.7721255664981699, 'test/bleu': 26.3076047627652, 'test/num_examples': 3003, 'score': 5880.397294044495, 'total_duration': 9714.990818023682, 'accumulated_submission_time': 5880.397294044495, 'accumulated_eval_time': 3829.637838602066, 'accumulated_logging_time': 0.15727782249450684, 'global_step': 13181, 'preemption_count': 0}), (15066, {'train/accuracy': 0.6320160596501291, 'train/loss': 1.9006086894178378, 'train/bleu': 31.13618389009489, 'validation/accuracy': 0.6492293957917447, 'validation/loss': 1.7852161628498098, 'validation/bleu': 27.402033333848323, 'validation/num_examples': 3000, 'test/accuracy': 0.6593806286677125, 'test/loss': 1.7252233310092382, 'test/bleu': 26.704391696208766, 'test/num_examples': 3003, 'score': 6720.030926942825, 'total_duration': 10950.817243337631, 'accumulated_submission_time': 6720.030926942825, 'accumulated_eval_time': 4225.134322166443, 'accumulated_logging_time': 0.17809152603149414, 'global_step': 15066, 'preemption_count': 0}), (16951, {'train/accuracy': 0.6338624949489119, 'train/loss': 1.897482032557871, 'train/bleu': 30.743680885303018, 'validation/accuracy': 0.6536682744169322, 'validation/loss': 1.752701260678727, 'validation/bleu': 27.726988063128722, 'validation/num_examples': 3000, 'test/accuracy': 0.6633315902620417, 'test/loss': 1.6888481567021092, 'test/bleu': 27.061127124681565, 'test/num_examples': 3003, 'score': 7559.490671634674, 'total_duration': 12264.548608064651, 'accumulated_submission_time': 7559.490671634674, 'accumulated_eval_time': 4698.706595897675, 'accumulated_logging_time': 0.1993236541748047, 'global_step': 16951, 'preemption_count': 0}), (18835, {'train/accuracy': 0.6834847576442374, 'train/loss': 1.585972809920214, 'train/bleu': 34.234715122596, 'validation/accuracy': 0.6579583638144598, 'validation/loss': 1.7184789633730517, 'validation/bleu': 28.60348997237404, 'validation/num_examples': 3000, 'test/accuracy': 0.6673755156585904, 'test/loss': 1.6571685622566963, 'test/bleu': 27.59825309218052, 'test/num_examples': 3003, 'score': 8398.888617038727, 'total_duration': 13624.539085626602, 'accumulated_submission_time': 8398.888617038727, 'accumulated_eval_time': 5218.602213382721, 'accumulated_logging_time': 0.2195441722869873, 'global_step': 18835, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6499681282156354, 'train/loss': 1.7739541715043483, 'train/bleu': 32.038508242884866, 'validation/accuracy': 0.6618144846313126, 'validation/loss': 1.697380340913318, 'validation/bleu': 28.226938120374303, 'validation/num_examples': 3000, 'test/accuracy': 0.6702341525768404, 'test/loss': 1.6397721660565918, 'test/bleu': 27.70530786994273, 'test/num_examples': 3003, 'score': 8917.815506219864, 'total_duration': 14590.462373971939, 'accumulated_submission_time': 8917.815506219864, 'accumulated_eval_time': 5665.163120269775, 'accumulated_logging_time': 0.23902559280395508, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0609 07:35:54.663895 139726197217088 submission_runner.py:584] Timing: 8917.815506219864
I0609 07:35:54.663944 139726197217088 submission_runner.py:586] Total number of evals: 12
I0609 07:35:54.664016 139726197217088 submission_runner.py:587] ====================
I0609 07:35:54.664119 139726197217088 submission_runner.py:655] Final wmt score: 8917.815506219864
