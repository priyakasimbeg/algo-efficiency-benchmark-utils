torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-08-2023-18-07-25.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 18:07:49.157210 140699089037120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 18:07:49.157271 140309142755136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 18:07:49.157354 139787938056000 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 18:07:49.158047 139761053755200 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 18:07:49.158082 140067012392768 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 18:07:50.135716 140685410367296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 18:07:50.136036 139641603352384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 18:07:50.140416 139749874411328 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 18:07:50.140746 139749874411328 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.146352 140685410367296 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.146785 139641603352384 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.149343 140309142755136 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.149366 140699089037120 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.149381 139761053755200 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.149417 139787938056000 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:50.149450 140067012392768 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:52.452114 139749874411328 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_resnet_pytorch.
W0608 18:07:52.492569 139787938056000 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.494800 139749874411328 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.495153 140699089037120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.495640 140309142755136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.496002 140685410367296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.496379 140067012392768 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.496552 139761053755200 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:52.497630 139641603352384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 18:07:52.499864 139749874411328 submission_runner.py:541] Using RNG seed 2827777270
I0608 18:07:52.501177 139749874411328 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 18:07:52.501294 139749874411328 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_resnet_pytorch/trial_1.
I0608 18:07:52.501538 139749874411328 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0608 18:07:52.502526 139749874411328 submission_runner.py:255] Initializing dataset.
I0608 18:07:58.961268 139749874411328 submission_runner.py:262] Initializing model.
I0608 18:08:03.478170 139749874411328 submission_runner.py:272] Initializing optimizer.
I0608 18:08:03.479504 139749874411328 submission_runner.py:279] Initializing metrics bundle.
I0608 18:08:03.479615 139749874411328 submission_runner.py:297] Initializing checkpoint and logger.
I0608 18:08:03.974426 139749874411328 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0608 18:08:03.976434 139749874411328 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0608 18:08:04.023766 139749874411328 submission_runner.py:332] Starting training loop.
I0608 18:08:12.120949 139720721954560 logging_writer.py:48] [0] global_step=0, grad_norm=0.600556, loss=6.916485
I0608 18:08:12.140548 139749874411328 submission.py:120] 0) loss = 6.916, grad_norm = 0.601
I0608 18:08:12.141660 139749874411328 spec.py:298] Evaluating on the training split.
I0608 18:09:11.842790 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 18:10:06.863018 139749874411328 spec.py:326] Evaluating on the test split.
I0608 18:10:06.883175 139749874411328 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 18:10:06.889117 139749874411328 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 18:10:06.966816 139749874411328 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 18:10:18.764155 139749874411328 submission_runner.py:419] Time since start: 134.74s, 	Step: 1, 	{'train/accuracy': 0.001295440051020408, 'train/loss': 6.921907386001275, 'validation/accuracy': 0.00146, 'validation/loss': 6.9212575, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.92266796875, 'test/num_examples': 10000, 'score': 8.117964267730713, 'total_duration': 134.7407829761505, 'accumulated_submission_time': 8.117964267730713, 'accumulated_eval_time': 126.62242221832275, 'accumulated_logging_time': 0}
I0608 18:10:18.782712 139696948631296 logging_writer.py:48] [1] accumulated_eval_time=126.622422, accumulated_logging_time=0, accumulated_submission_time=8.117964, global_step=1, preemption_count=0, score=8.117964, test/accuracy=0.001000, test/loss=6.922668, test/num_examples=10000, total_duration=134.740783, train/accuracy=0.001295, train/loss=6.921907, validation/accuracy=0.001460, validation/loss=6.921258, validation/num_examples=50000
I0608 18:10:18.802339 139749874411328 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.802477 140067012392768 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.803102 140685410367296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.803211 139787938056000 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.803244 139641603352384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.803336 140309142755136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.803362 140699089037120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:18.803340 139761053755200 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:19.184037 139696940238592 logging_writer.py:48] [1] global_step=1, grad_norm=0.623069, loss=6.916236
I0608 18:10:19.188122 139749874411328 submission.py:120] 1) loss = 6.916, grad_norm = 0.623
I0608 18:10:19.573999 139696948631296 logging_writer.py:48] [2] global_step=2, grad_norm=0.619624, loss=6.925660
I0608 18:10:19.577983 139749874411328 submission.py:120] 2) loss = 6.926, grad_norm = 0.620
I0608 18:10:19.962773 139696940238592 logging_writer.py:48] [3] global_step=3, grad_norm=0.614372, loss=6.921824
I0608 18:10:19.966486 139749874411328 submission.py:120] 3) loss = 6.922, grad_norm = 0.614
I0608 18:10:20.352830 139696948631296 logging_writer.py:48] [4] global_step=4, grad_norm=0.606604, loss=6.921118
I0608 18:10:20.358143 139749874411328 submission.py:120] 4) loss = 6.921, grad_norm = 0.607
I0608 18:10:20.746608 139696940238592 logging_writer.py:48] [5] global_step=5, grad_norm=0.608035, loss=6.928672
I0608 18:10:20.751704 139749874411328 submission.py:120] 5) loss = 6.929, grad_norm = 0.608
I0608 18:10:21.139888 139696948631296 logging_writer.py:48] [6] global_step=6, grad_norm=0.624537, loss=6.926511
I0608 18:10:21.143672 139749874411328 submission.py:120] 6) loss = 6.927, grad_norm = 0.625
I0608 18:10:21.530297 139696940238592 logging_writer.py:48] [7] global_step=7, grad_norm=0.628498, loss=6.923068
I0608 18:10:21.534307 139749874411328 submission.py:120] 7) loss = 6.923, grad_norm = 0.628
I0608 18:10:21.924757 139696948631296 logging_writer.py:48] [8] global_step=8, grad_norm=0.624326, loss=6.929257
I0608 18:10:21.928883 139749874411328 submission.py:120] 8) loss = 6.929, grad_norm = 0.624
I0608 18:10:22.316131 139696940238592 logging_writer.py:48] [9] global_step=9, grad_norm=0.634174, loss=6.931417
I0608 18:10:22.320289 139749874411328 submission.py:120] 9) loss = 6.931, grad_norm = 0.634
I0608 18:10:22.712990 139696948631296 logging_writer.py:48] [10] global_step=10, grad_norm=0.616414, loss=6.913127
I0608 18:10:22.717287 139749874411328 submission.py:120] 10) loss = 6.913, grad_norm = 0.616
I0608 18:10:23.104877 139696940238592 logging_writer.py:48] [11] global_step=11, grad_norm=0.613027, loss=6.923913
I0608 18:10:23.110261 139749874411328 submission.py:120] 11) loss = 6.924, grad_norm = 0.613
I0608 18:10:23.496872 139696948631296 logging_writer.py:48] [12] global_step=12, grad_norm=0.607717, loss=6.929062
I0608 18:10:23.500423 139749874411328 submission.py:120] 12) loss = 6.929, grad_norm = 0.608
I0608 18:10:23.885047 139696940238592 logging_writer.py:48] [13] global_step=13, grad_norm=0.623122, loss=6.927884
I0608 18:10:23.888855 139749874411328 submission.py:120] 13) loss = 6.928, grad_norm = 0.623
I0608 18:10:24.274330 139696948631296 logging_writer.py:48] [14] global_step=14, grad_norm=0.631617, loss=6.931379
I0608 18:10:24.278762 139749874411328 submission.py:120] 14) loss = 6.931, grad_norm = 0.632
I0608 18:10:24.665975 139696940238592 logging_writer.py:48] [15] global_step=15, grad_norm=0.603617, loss=6.923186
I0608 18:10:24.671065 139749874411328 submission.py:120] 15) loss = 6.923, grad_norm = 0.604
I0608 18:10:25.059587 139696948631296 logging_writer.py:48] [16] global_step=16, grad_norm=0.609225, loss=6.931607
I0608 18:10:25.064226 139749874411328 submission.py:120] 16) loss = 6.932, grad_norm = 0.609
I0608 18:10:25.452570 139696940238592 logging_writer.py:48] [17] global_step=17, grad_norm=0.612046, loss=6.922493
I0608 18:10:25.456421 139749874411328 submission.py:120] 17) loss = 6.922, grad_norm = 0.612
I0608 18:10:25.887092 139696948631296 logging_writer.py:48] [18] global_step=18, grad_norm=0.625340, loss=6.925363
I0608 18:10:25.891893 139749874411328 submission.py:120] 18) loss = 6.925, grad_norm = 0.625
I0608 18:10:26.282474 139696940238592 logging_writer.py:48] [19] global_step=19, grad_norm=0.614398, loss=6.918351
I0608 18:10:26.286851 139749874411328 submission.py:120] 19) loss = 6.918, grad_norm = 0.614
I0608 18:10:26.673717 139696948631296 logging_writer.py:48] [20] global_step=20, grad_norm=0.618577, loss=6.919936
I0608 18:10:26.677773 139749874411328 submission.py:120] 20) loss = 6.920, grad_norm = 0.619
I0608 18:10:27.075491 139696940238592 logging_writer.py:48] [21] global_step=21, grad_norm=0.616238, loss=6.926769
I0608 18:10:27.079437 139749874411328 submission.py:120] 21) loss = 6.927, grad_norm = 0.616
I0608 18:10:27.463263 139696948631296 logging_writer.py:48] [22] global_step=22, grad_norm=0.624441, loss=6.925282
I0608 18:10:27.467418 139749874411328 submission.py:120] 22) loss = 6.925, grad_norm = 0.624
I0608 18:10:27.854085 139696940238592 logging_writer.py:48] [23] global_step=23, grad_norm=0.609451, loss=6.927896
I0608 18:10:27.858578 139749874411328 submission.py:120] 23) loss = 6.928, grad_norm = 0.609
I0608 18:10:28.246396 139696948631296 logging_writer.py:48] [24] global_step=24, grad_norm=0.622916, loss=6.928030
I0608 18:10:28.250417 139749874411328 submission.py:120] 24) loss = 6.928, grad_norm = 0.623
I0608 18:10:28.637832 139696940238592 logging_writer.py:48] [25] global_step=25, grad_norm=0.609952, loss=6.921788
I0608 18:10:28.642344 139749874411328 submission.py:120] 25) loss = 6.922, grad_norm = 0.610
I0608 18:10:29.028925 139696948631296 logging_writer.py:48] [26] global_step=26, grad_norm=0.624391, loss=6.926296
I0608 18:10:29.032871 139749874411328 submission.py:120] 26) loss = 6.926, grad_norm = 0.624
I0608 18:10:29.419865 139696940238592 logging_writer.py:48] [27] global_step=27, grad_norm=0.609470, loss=6.932443
I0608 18:10:29.424827 139749874411328 submission.py:120] 27) loss = 6.932, grad_norm = 0.609
I0608 18:10:29.812998 139696948631296 logging_writer.py:48] [28] global_step=28, grad_norm=0.620352, loss=6.916518
I0608 18:10:29.816831 139749874411328 submission.py:120] 28) loss = 6.917, grad_norm = 0.620
I0608 18:10:30.209432 139696940238592 logging_writer.py:48] [29] global_step=29, grad_norm=0.620772, loss=6.915487
I0608 18:10:30.219805 139749874411328 submission.py:120] 29) loss = 6.915, grad_norm = 0.621
I0608 18:10:30.602771 139696948631296 logging_writer.py:48] [30] global_step=30, grad_norm=0.617371, loss=6.910187
I0608 18:10:30.606685 139749874411328 submission.py:120] 30) loss = 6.910, grad_norm = 0.617
I0608 18:10:31.003064 139696940238592 logging_writer.py:48] [31] global_step=31, grad_norm=0.604257, loss=6.929566
I0608 18:10:31.007958 139749874411328 submission.py:120] 31) loss = 6.930, grad_norm = 0.604
I0608 18:10:31.403001 139696948631296 logging_writer.py:48] [32] global_step=32, grad_norm=0.594842, loss=6.922740
I0608 18:10:31.406710 139749874411328 submission.py:120] 32) loss = 6.923, grad_norm = 0.595
I0608 18:10:31.799463 139696940238592 logging_writer.py:48] [33] global_step=33, grad_norm=0.628897, loss=6.917336
I0608 18:10:31.803992 139749874411328 submission.py:120] 33) loss = 6.917, grad_norm = 0.629
I0608 18:10:32.197236 139696948631296 logging_writer.py:48] [34] global_step=34, grad_norm=0.608767, loss=6.923016
I0608 18:10:32.203340 139749874411328 submission.py:120] 34) loss = 6.923, grad_norm = 0.609
I0608 18:10:32.656100 139696940238592 logging_writer.py:48] [35] global_step=35, grad_norm=0.607109, loss=6.916986
I0608 18:10:32.660021 139749874411328 submission.py:120] 35) loss = 6.917, grad_norm = 0.607
I0608 18:10:33.047132 139696948631296 logging_writer.py:48] [36] global_step=36, grad_norm=0.623656, loss=6.917238
I0608 18:10:33.051784 139749874411328 submission.py:120] 36) loss = 6.917, grad_norm = 0.624
I0608 18:10:33.437521 139696940238592 logging_writer.py:48] [37] global_step=37, grad_norm=0.616247, loss=6.922879
I0608 18:10:33.441214 139749874411328 submission.py:120] 37) loss = 6.923, grad_norm = 0.616
I0608 18:10:33.840042 139696948631296 logging_writer.py:48] [38] global_step=38, grad_norm=0.605194, loss=6.917375
I0608 18:10:33.843811 139749874411328 submission.py:120] 38) loss = 6.917, grad_norm = 0.605
I0608 18:10:34.229391 139696940238592 logging_writer.py:48] [39] global_step=39, grad_norm=0.617386, loss=6.916272
I0608 18:10:34.233998 139749874411328 submission.py:120] 39) loss = 6.916, grad_norm = 0.617
I0608 18:10:34.619322 139696948631296 logging_writer.py:48] [40] global_step=40, grad_norm=0.619623, loss=6.914884
I0608 18:10:34.624024 139749874411328 submission.py:120] 40) loss = 6.915, grad_norm = 0.620
I0608 18:10:35.012097 139696940238592 logging_writer.py:48] [41] global_step=41, grad_norm=0.594527, loss=6.917760
I0608 18:10:35.016532 139749874411328 submission.py:120] 41) loss = 6.918, grad_norm = 0.595
I0608 18:10:35.400997 139696948631296 logging_writer.py:48] [42] global_step=42, grad_norm=0.603814, loss=6.918113
I0608 18:10:35.405515 139749874411328 submission.py:120] 42) loss = 6.918, grad_norm = 0.604
I0608 18:10:35.805095 139696940238592 logging_writer.py:48] [43] global_step=43, grad_norm=0.616600, loss=6.915601
I0608 18:10:35.814966 139749874411328 submission.py:120] 43) loss = 6.916, grad_norm = 0.617
I0608 18:10:36.204875 139696948631296 logging_writer.py:48] [44] global_step=44, grad_norm=0.604579, loss=6.909734
I0608 18:10:36.208519 139749874411328 submission.py:120] 44) loss = 6.910, grad_norm = 0.605
I0608 18:10:36.599437 139696940238592 logging_writer.py:48] [45] global_step=45, grad_norm=0.600054, loss=6.913972
I0608 18:10:36.603609 139749874411328 submission.py:120] 45) loss = 6.914, grad_norm = 0.600
I0608 18:10:36.992435 139696948631296 logging_writer.py:48] [46] global_step=46, grad_norm=0.626679, loss=6.911308
I0608 18:10:36.996374 139749874411328 submission.py:120] 46) loss = 6.911, grad_norm = 0.627
I0608 18:10:37.388029 139696940238592 logging_writer.py:48] [47] global_step=47, grad_norm=0.610596, loss=6.906086
I0608 18:10:37.393113 139749874411328 submission.py:120] 47) loss = 6.906, grad_norm = 0.611
I0608 18:10:37.779027 139696948631296 logging_writer.py:48] [48] global_step=48, grad_norm=0.603372, loss=6.912621
I0608 18:10:37.782860 139749874411328 submission.py:120] 48) loss = 6.913, grad_norm = 0.603
I0608 18:10:38.169508 139696940238592 logging_writer.py:48] [49] global_step=49, grad_norm=0.626626, loss=6.913852
I0608 18:10:38.174242 139749874411328 submission.py:120] 49) loss = 6.914, grad_norm = 0.627
I0608 18:10:38.565022 139696948631296 logging_writer.py:48] [50] global_step=50, grad_norm=0.602719, loss=6.909889
I0608 18:10:38.569665 139749874411328 submission.py:120] 50) loss = 6.910, grad_norm = 0.603
I0608 18:10:38.959346 139696940238592 logging_writer.py:48] [51] global_step=51, grad_norm=0.604983, loss=6.908742
I0608 18:10:38.964175 139749874411328 submission.py:120] 51) loss = 6.909, grad_norm = 0.605
I0608 18:10:39.431601 139696948631296 logging_writer.py:48] [52] global_step=52, grad_norm=0.608322, loss=6.908765
I0608 18:10:39.436146 139749874411328 submission.py:120] 52) loss = 6.909, grad_norm = 0.608
I0608 18:10:39.825281 139696940238592 logging_writer.py:48] [53] global_step=53, grad_norm=0.600229, loss=6.912056
I0608 18:10:39.829580 139749874411328 submission.py:120] 53) loss = 6.912, grad_norm = 0.600
I0608 18:10:40.220133 139696948631296 logging_writer.py:48] [54] global_step=54, grad_norm=0.617453, loss=6.909305
I0608 18:10:40.225004 139749874411328 submission.py:120] 54) loss = 6.909, grad_norm = 0.617
I0608 18:10:40.619823 139696940238592 logging_writer.py:48] [55] global_step=55, grad_norm=0.603849, loss=6.914901
I0608 18:10:40.624024 139749874411328 submission.py:120] 55) loss = 6.915, grad_norm = 0.604
I0608 18:10:41.013632 139696948631296 logging_writer.py:48] [56] global_step=56, grad_norm=0.615173, loss=6.904072
I0608 18:10:41.020268 139749874411328 submission.py:120] 56) loss = 6.904, grad_norm = 0.615
I0608 18:10:41.410486 139696940238592 logging_writer.py:48] [57] global_step=57, grad_norm=0.605876, loss=6.903859
I0608 18:10:41.414533 139749874411328 submission.py:120] 57) loss = 6.904, grad_norm = 0.606
I0608 18:10:41.806652 139696948631296 logging_writer.py:48] [58] global_step=58, grad_norm=0.623259, loss=6.896547
I0608 18:10:41.810897 139749874411328 submission.py:120] 58) loss = 6.897, grad_norm = 0.623
I0608 18:10:42.213969 139696940238592 logging_writer.py:48] [59] global_step=59, grad_norm=0.614374, loss=6.901168
I0608 18:10:42.218727 139749874411328 submission.py:120] 59) loss = 6.901, grad_norm = 0.614
I0608 18:10:42.609413 139696948631296 logging_writer.py:48] [60] global_step=60, grad_norm=0.618545, loss=6.912837
I0608 18:10:42.613902 139749874411328 submission.py:120] 60) loss = 6.913, grad_norm = 0.619
I0608 18:10:43.001299 139696940238592 logging_writer.py:48] [61] global_step=61, grad_norm=0.588620, loss=6.898157
I0608 18:10:43.005944 139749874411328 submission.py:120] 61) loss = 6.898, grad_norm = 0.589
I0608 18:10:43.396377 139696948631296 logging_writer.py:48] [62] global_step=62, grad_norm=0.597819, loss=6.903992
I0608 18:10:43.400771 139749874411328 submission.py:120] 62) loss = 6.904, grad_norm = 0.598
I0608 18:10:43.788300 139696940238592 logging_writer.py:48] [63] global_step=63, grad_norm=0.620941, loss=6.901403
I0608 18:10:43.792858 139749874411328 submission.py:120] 63) loss = 6.901, grad_norm = 0.621
I0608 18:10:44.184092 139696948631296 logging_writer.py:48] [64] global_step=64, grad_norm=0.637670, loss=6.903140
I0608 18:10:44.189563 139749874411328 submission.py:120] 64) loss = 6.903, grad_norm = 0.638
I0608 18:10:44.584527 139696940238592 logging_writer.py:48] [65] global_step=65, grad_norm=0.608186, loss=6.894938
I0608 18:10:44.588787 139749874411328 submission.py:120] 65) loss = 6.895, grad_norm = 0.608
I0608 18:10:44.976219 139696948631296 logging_writer.py:48] [66] global_step=66, grad_norm=0.609707, loss=6.901390
I0608 18:10:44.981004 139749874411328 submission.py:120] 66) loss = 6.901, grad_norm = 0.610
I0608 18:10:45.368613 139696940238592 logging_writer.py:48] [67] global_step=67, grad_norm=0.615276, loss=6.910664
I0608 18:10:45.372319 139749874411328 submission.py:120] 67) loss = 6.911, grad_norm = 0.615
I0608 18:10:45.759299 139696948631296 logging_writer.py:48] [68] global_step=68, grad_norm=0.611368, loss=6.891131
I0608 18:10:45.763308 139749874411328 submission.py:120] 68) loss = 6.891, grad_norm = 0.611
I0608 18:10:46.173070 139696940238592 logging_writer.py:48] [69] global_step=69, grad_norm=0.581543, loss=6.899295
I0608 18:10:46.178234 139749874411328 submission.py:120] 69) loss = 6.899, grad_norm = 0.582
I0608 18:10:46.587639 139696948631296 logging_writer.py:48] [70] global_step=70, grad_norm=0.609380, loss=6.889300
I0608 18:10:46.595539 139749874411328 submission.py:120] 70) loss = 6.889, grad_norm = 0.609
I0608 18:10:46.987671 139696940238592 logging_writer.py:48] [71] global_step=71, grad_norm=0.604441, loss=6.895822
I0608 18:10:46.996591 139749874411328 submission.py:120] 71) loss = 6.896, grad_norm = 0.604
I0608 18:10:47.383998 139696948631296 logging_writer.py:48] [72] global_step=72, grad_norm=0.614673, loss=6.890812
I0608 18:10:47.388367 139749874411328 submission.py:120] 72) loss = 6.891, grad_norm = 0.615
I0608 18:10:47.775081 139696940238592 logging_writer.py:48] [73] global_step=73, grad_norm=0.620893, loss=6.891823
I0608 18:10:47.779531 139749874411328 submission.py:120] 73) loss = 6.892, grad_norm = 0.621
I0608 18:10:48.171185 139696948631296 logging_writer.py:48] [74] global_step=74, grad_norm=0.588296, loss=6.894537
I0608 18:10:48.179621 139749874411328 submission.py:120] 74) loss = 6.895, grad_norm = 0.588
I0608 18:10:48.569847 139696940238592 logging_writer.py:48] [75] global_step=75, grad_norm=0.603450, loss=6.886138
I0608 18:10:48.574465 139749874411328 submission.py:120] 75) loss = 6.886, grad_norm = 0.603
I0608 18:10:48.960690 139696948631296 logging_writer.py:48] [76] global_step=76, grad_norm=0.596842, loss=6.891122
I0608 18:10:48.965392 139749874411328 submission.py:120] 76) loss = 6.891, grad_norm = 0.597
I0608 18:10:49.355767 139696940238592 logging_writer.py:48] [77] global_step=77, grad_norm=0.611152, loss=6.894283
I0608 18:10:49.364000 139749874411328 submission.py:120] 77) loss = 6.894, grad_norm = 0.611
I0608 18:10:49.752674 139696948631296 logging_writer.py:48] [78] global_step=78, grad_norm=0.616889, loss=6.895517
I0608 18:10:49.756587 139749874411328 submission.py:120] 78) loss = 6.896, grad_norm = 0.617
I0608 18:10:50.142702 139696940238592 logging_writer.py:48] [79] global_step=79, grad_norm=0.621733, loss=6.890681
I0608 18:10:50.147567 139749874411328 submission.py:120] 79) loss = 6.891, grad_norm = 0.622
I0608 18:10:50.539337 139696948631296 logging_writer.py:48] [80] global_step=80, grad_norm=0.593934, loss=6.884632
I0608 18:10:50.543067 139749874411328 submission.py:120] 80) loss = 6.885, grad_norm = 0.594
I0608 18:10:50.931731 139696940238592 logging_writer.py:48] [81] global_step=81, grad_norm=0.600158, loss=6.888403
I0608 18:10:50.936711 139749874411328 submission.py:120] 81) loss = 6.888, grad_norm = 0.600
I0608 18:10:51.324591 139696948631296 logging_writer.py:48] [82] global_step=82, grad_norm=0.606754, loss=6.880701
I0608 18:10:51.328263 139749874411328 submission.py:120] 82) loss = 6.881, grad_norm = 0.607
I0608 18:10:51.724926 139696940238592 logging_writer.py:48] [83] global_step=83, grad_norm=0.609527, loss=6.886081
I0608 18:10:51.729019 139749874411328 submission.py:120] 83) loss = 6.886, grad_norm = 0.610
I0608 18:10:52.116832 139696948631296 logging_writer.py:48] [84] global_step=84, grad_norm=0.607346, loss=6.897110
I0608 18:10:52.121896 139749874411328 submission.py:120] 84) loss = 6.897, grad_norm = 0.607
I0608 18:10:52.513506 139696940238592 logging_writer.py:48] [85] global_step=85, grad_norm=0.595926, loss=6.879611
I0608 18:10:52.517221 139749874411328 submission.py:120] 85) loss = 6.880, grad_norm = 0.596
I0608 18:10:52.970072 139696948631296 logging_writer.py:48] [86] global_step=86, grad_norm=0.620675, loss=6.883752
I0608 18:10:52.973806 139749874411328 submission.py:120] 86) loss = 6.884, grad_norm = 0.621
I0608 18:10:53.364081 139696940238592 logging_writer.py:48] [87] global_step=87, grad_norm=0.618197, loss=6.875360
I0608 18:10:53.369004 139749874411328 submission.py:120] 87) loss = 6.875, grad_norm = 0.618
I0608 18:10:53.757140 139696948631296 logging_writer.py:48] [88] global_step=88, grad_norm=0.608719, loss=6.885276
I0608 18:10:53.760825 139749874411328 submission.py:120] 88) loss = 6.885, grad_norm = 0.609
I0608 18:10:54.158182 139696940238592 logging_writer.py:48] [89] global_step=89, grad_norm=0.603490, loss=6.874819
I0608 18:10:54.162476 139749874411328 submission.py:120] 89) loss = 6.875, grad_norm = 0.603
I0608 18:10:54.553193 139696948631296 logging_writer.py:48] [90] global_step=90, grad_norm=0.624709, loss=6.890054
I0608 18:10:54.557352 139749874411328 submission.py:120] 90) loss = 6.890, grad_norm = 0.625
I0608 18:10:54.948530 139696940238592 logging_writer.py:48] [91] global_step=91, grad_norm=0.597664, loss=6.876843
I0608 18:10:54.952655 139749874411328 submission.py:120] 91) loss = 6.877, grad_norm = 0.598
I0608 18:10:55.346095 139696948631296 logging_writer.py:48] [92] global_step=92, grad_norm=0.615383, loss=6.877867
I0608 18:10:55.350259 139749874411328 submission.py:120] 92) loss = 6.878, grad_norm = 0.615
I0608 18:10:55.739444 139696940238592 logging_writer.py:48] [93] global_step=93, grad_norm=0.603787, loss=6.873332
I0608 18:10:55.744648 139749874411328 submission.py:120] 93) loss = 6.873, grad_norm = 0.604
I0608 18:10:56.140306 139696948631296 logging_writer.py:48] [94] global_step=94, grad_norm=0.592599, loss=6.874634
I0608 18:10:56.145478 139749874411328 submission.py:120] 94) loss = 6.875, grad_norm = 0.593
I0608 18:10:56.536358 139696940238592 logging_writer.py:48] [95] global_step=95, grad_norm=0.609771, loss=6.882474
I0608 18:10:56.540324 139749874411328 submission.py:120] 95) loss = 6.882, grad_norm = 0.610
I0608 18:10:56.926982 139696948631296 logging_writer.py:48] [96] global_step=96, grad_norm=0.606799, loss=6.876371
I0608 18:10:56.931877 139749874411328 submission.py:120] 96) loss = 6.876, grad_norm = 0.607
I0608 18:10:57.330359 139696940238592 logging_writer.py:48] [97] global_step=97, grad_norm=0.609875, loss=6.868237
I0608 18:10:57.334488 139749874411328 submission.py:120] 97) loss = 6.868, grad_norm = 0.610
I0608 18:10:57.726903 139696948631296 logging_writer.py:48] [98] global_step=98, grad_norm=0.626254, loss=6.868192
I0608 18:10:57.730689 139749874411328 submission.py:120] 98) loss = 6.868, grad_norm = 0.626
I0608 18:10:58.126711 139696940238592 logging_writer.py:48] [99] global_step=99, grad_norm=0.621129, loss=6.871443
I0608 18:10:58.130631 139749874411328 submission.py:120] 99) loss = 6.871, grad_norm = 0.621
I0608 18:10:58.516863 139696948631296 logging_writer.py:48] [100] global_step=100, grad_norm=0.617000, loss=6.877405
I0608 18:10:58.521347 139749874411328 submission.py:120] 100) loss = 6.877, grad_norm = 0.617
I0608 18:13:29.812953 139696940238592 logging_writer.py:48] [500] global_step=500, grad_norm=1.187792, loss=6.294072
I0608 18:13:29.817250 139749874411328 submission.py:120] 500) loss = 6.294, grad_norm = 1.188
I0608 18:16:39.146556 139696948631296 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.813397, loss=5.663226
I0608 18:16:39.152138 139749874411328 submission.py:120] 1000) loss = 5.663, grad_norm = 1.813
I0608 18:18:49.051548 139749874411328 spec.py:298] Evaluating on the training split.
I0608 18:19:30.971767 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 18:20:24.873071 139749874411328 spec.py:326] Evaluating on the test split.
I0608 18:20:26.270343 139749874411328 submission_runner.py:419] Time since start: 742.25s, 	Step: 1341, 	{'train/accuracy': 0.11136798469387756, 'train/loss': 4.8504180908203125, 'validation/accuracy': 0.10356, 'validation/loss': 4.926635625, 'validation/num_examples': 50000, 'test/accuracy': 0.0707, 'test/loss': 5.302338671875, 'test/num_examples': 10000, 'score': 517.7223064899445, 'total_duration': 742.2470140457153, 'accumulated_submission_time': 517.7223064899445, 'accumulated_eval_time': 223.84112906455994, 'accumulated_logging_time': 0.026972293853759766}
I0608 18:20:26.281648 139696957024000 logging_writer.py:48] [1341] accumulated_eval_time=223.841129, accumulated_logging_time=0.026972, accumulated_submission_time=517.722306, global_step=1341, preemption_count=0, score=517.722306, test/accuracy=0.070700, test/loss=5.302339, test/num_examples=10000, total_duration=742.247014, train/accuracy=0.111368, train/loss=4.850418, validation/accuracy=0.103560, validation/loss=4.926636, validation/num_examples=50000
I0608 18:21:26.369414 139696965416704 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.596087, loss=5.326440
I0608 18:21:26.373419 139749874411328 submission.py:120] 1500) loss = 5.326, grad_norm = 3.596
I0608 18:24:34.614906 139696957024000 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.355453, loss=4.911460
I0608 18:24:34.619572 139749874411328 submission.py:120] 2000) loss = 4.911, grad_norm = 4.355
I0608 18:27:43.569082 139696965416704 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.431961, loss=4.649415
I0608 18:27:43.574438 139749874411328 submission.py:120] 2500) loss = 4.649, grad_norm = 3.432
I0608 18:28:56.345676 139749874411328 spec.py:298] Evaluating on the training split.
I0608 18:29:41.637617 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 18:30:36.475657 139749874411328 spec.py:326] Evaluating on the test split.
I0608 18:30:37.839871 139749874411328 submission_runner.py:419] Time since start: 1353.82s, 	Step: 2690, 	{'train/accuracy': 0.2392578125, 'train/loss': 3.7761504504145407, 'validation/accuracy': 0.2175, 'validation/loss': 3.9146465625, 'validation/num_examples': 50000, 'test/accuracy': 0.1504, 'test/loss': 4.516253125, 'test/num_examples': 10000, 'score': 1027.1857087612152, 'total_duration': 1353.8151302337646, 'accumulated_submission_time': 1027.1857087612152, 'accumulated_eval_time': 325.3338165283203, 'accumulated_logging_time': 0.04780006408691406}
I0608 18:30:37.850279 139696957024000 logging_writer.py:48] [2690] accumulated_eval_time=325.333817, accumulated_logging_time=0.047800, accumulated_submission_time=1027.185709, global_step=2690, preemption_count=0, score=1027.185709, test/accuracy=0.150400, test/loss=4.516253, test/num_examples=10000, total_duration=1353.815130, train/accuracy=0.239258, train/loss=3.776150, validation/accuracy=0.217500, validation/loss=3.914647, validation/num_examples=50000
I0608 18:32:34.700249 139696965416704 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.139321, loss=4.315691
I0608 18:32:34.704568 139749874411328 submission.py:120] 3000) loss = 4.316, grad_norm = 3.139
I0608 18:35:43.088635 139696957024000 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.271605, loss=4.017745
I0608 18:35:43.092888 139749874411328 submission.py:120] 3500) loss = 4.018, grad_norm = 2.272
I0608 18:38:52.916503 139696965416704 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.073329, loss=3.998965
I0608 18:38:52.921184 139749874411328 submission.py:120] 4000) loss = 3.999, grad_norm = 2.073
I0608 18:39:07.956342 139749874411328 spec.py:298] Evaluating on the training split.
I0608 18:39:50.935392 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 18:40:36.860477 139749874411328 spec.py:326] Evaluating on the test split.
I0608 18:40:38.227201 139749874411328 submission_runner.py:419] Time since start: 1954.20s, 	Step: 4041, 	{'train/accuracy': 0.3389469068877551, 'train/loss': 3.164668491908482, 'validation/accuracy': 0.31148, 'validation/loss': 3.3307875, 'validation/num_examples': 50000, 'test/accuracy': 0.2135, 'test/loss': 4.062483984375, 'test/num_examples': 10000, 'score': 1536.701803445816, 'total_duration': 1954.2038989067078, 'accumulated_submission_time': 1536.701803445816, 'accumulated_eval_time': 415.60464882850647, 'accumulated_logging_time': 0.06724739074707031}
I0608 18:40:38.238135 139696957024000 logging_writer.py:48] [4041] accumulated_eval_time=415.604649, accumulated_logging_time=0.067247, accumulated_submission_time=1536.701803, global_step=4041, preemption_count=0, score=1536.701803, test/accuracy=0.213500, test/loss=4.062484, test/num_examples=10000, total_duration=1954.203899, train/accuracy=0.338947, train/loss=3.164668, validation/accuracy=0.311480, validation/loss=3.330788, validation/num_examples=50000
I0608 18:43:31.267226 139696965416704 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.044780, loss=3.673183
I0608 18:43:31.275129 139749874411328 submission.py:120] 4500) loss = 3.673, grad_norm = 2.045
I0608 18:46:40.009598 139696957024000 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.586748, loss=3.670119
I0608 18:46:40.013772 139749874411328 submission.py:120] 5000) loss = 3.670, grad_norm = 1.587
I0608 18:49:08.601414 139749874411328 spec.py:298] Evaluating on the training split.
I0608 18:49:53.767938 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 18:50:48.292450 139749874411328 spec.py:326] Evaluating on the test split.
I0608 18:50:49.662094 139749874411328 submission_runner.py:419] Time since start: 2565.64s, 	Step: 5392, 	{'train/accuracy': 0.4586854272959184, 'train/loss': 2.484075585190131, 'validation/accuracy': 0.41804, 'validation/loss': 2.676306875, 'validation/num_examples': 50000, 'test/accuracy': 0.3146, 'test/loss': 3.359944921875, 'test/num_examples': 10000, 'score': 2046.4686601161957, 'total_duration': 2565.637288570404, 'accumulated_submission_time': 2046.4686601161957, 'accumulated_eval_time': 516.6638307571411, 'accumulated_logging_time': 0.08736729621887207}
I0608 18:50:49.672011 139696965416704 logging_writer.py:48] [5392] accumulated_eval_time=516.663831, accumulated_logging_time=0.087367, accumulated_submission_time=2046.468660, global_step=5392, preemption_count=0, score=2046.468660, test/accuracy=0.314600, test/loss=3.359945, test/num_examples=10000, total_duration=2565.637289, train/accuracy=0.458685, train/loss=2.484076, validation/accuracy=0.418040, validation/loss=2.676307, validation/num_examples=50000
I0608 18:51:30.584879 139696957024000 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.861412, loss=3.636236
I0608 18:51:30.588583 139749874411328 submission.py:120] 5500) loss = 3.636, grad_norm = 1.861
I0608 18:54:38.834551 139696965416704 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.294405, loss=3.368469
I0608 18:54:38.839595 139749874411328 submission.py:120] 6000) loss = 3.368, grad_norm = 1.294
I0608 18:57:48.728188 139696957024000 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.233830, loss=3.334262
I0608 18:57:48.733817 139749874411328 submission.py:120] 6500) loss = 3.334, grad_norm = 2.234
I0608 18:59:20.042469 139749874411328 spec.py:298] Evaluating on the training split.
I0608 19:00:03.678707 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 19:00:50.821349 139749874411328 spec.py:326] Evaluating on the test split.
I0608 19:00:52.189638 139749874411328 submission_runner.py:419] Time since start: 3168.17s, 	Step: 6744, 	{'train/accuracy': 0.5137715242346939, 'train/loss': 2.18264677086655, 'validation/accuracy': 0.47054, 'validation/loss': 2.402321875, 'validation/num_examples': 50000, 'test/accuracy': 0.351, 'test/loss': 3.11070546875, 'test/num_examples': 10000, 'score': 2556.2421708106995, 'total_duration': 3168.1663432121277, 'accumulated_submission_time': 2556.2421708106995, 'accumulated_eval_time': 608.8109827041626, 'accumulated_logging_time': 0.10605764389038086}
I0608 19:00:52.199778 139696965416704 logging_writer.py:48] [6744] accumulated_eval_time=608.810983, accumulated_logging_time=0.106058, accumulated_submission_time=2556.242171, global_step=6744, preemption_count=0, score=2556.242171, test/accuracy=0.351000, test/loss=3.110705, test/num_examples=10000, total_duration=3168.166343, train/accuracy=0.513772, train/loss=2.182647, validation/accuracy=0.470540, validation/loss=2.402322, validation/num_examples=50000
I0608 19:02:28.709738 139696957024000 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.692541, loss=3.295462
I0608 19:02:28.713778 139749874411328 submission.py:120] 7000) loss = 3.295, grad_norm = 1.693
I0608 19:05:37.349957 139696965416704 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.036813, loss=3.170754
I0608 19:05:37.354517 139749874411328 submission.py:120] 7500) loss = 3.171, grad_norm = 1.037
I0608 19:08:46.657281 139696957024000 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.453714, loss=3.159349
I0608 19:08:46.661468 139749874411328 submission.py:120] 8000) loss = 3.159, grad_norm = 1.454
I0608 19:09:22.305850 139749874411328 spec.py:298] Evaluating on the training split.
I0608 19:10:07.393545 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 19:10:58.613939 139749874411328 spec.py:326] Evaluating on the test split.
I0608 19:10:59.974239 139749874411328 submission_runner.py:419] Time since start: 3775.95s, 	Step: 8096, 	{'train/accuracy': 0.5440250318877551, 'train/loss': 2.057747510014748, 'validation/accuracy': 0.49942, 'validation/loss': 2.2861575, 'validation/num_examples': 50000, 'test/accuracy': 0.378, 'test/loss': 2.988608984375, 'test/num_examples': 10000, 'score': 3065.7530615329742, 'total_duration': 3775.9509284496307, 'accumulated_submission_time': 3065.7530615329742, 'accumulated_eval_time': 706.4793224334717, 'accumulated_logging_time': 0.12501025199890137}
I0608 19:10:59.984620 139696965416704 logging_writer.py:48] [8096] accumulated_eval_time=706.479322, accumulated_logging_time=0.125010, accumulated_submission_time=3065.753062, global_step=8096, preemption_count=0, score=3065.753062, test/accuracy=0.378000, test/loss=2.988609, test/num_examples=10000, total_duration=3775.950928, train/accuracy=0.544025, train/loss=2.057748, validation/accuracy=0.499420, validation/loss=2.286157, validation/num_examples=50000
I0608 19:13:32.267893 139696957024000 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.428043, loss=3.144108
I0608 19:13:32.273258 139749874411328 submission.py:120] 8500) loss = 3.144, grad_norm = 1.428
I0608 19:16:42.047449 139696965416704 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.972636, loss=3.047974
I0608 19:16:42.052854 139749874411328 submission.py:120] 9000) loss = 3.048, grad_norm = 0.973
I0608 19:19:30.315898 139749874411328 spec.py:298] Evaluating on the training split.
I0608 19:20:14.973887 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 19:21:02.221161 139749874411328 spec.py:326] Evaluating on the test split.
I0608 19:21:03.587467 139749874411328 submission_runner.py:419] Time since start: 4379.56s, 	Step: 9449, 	{'train/accuracy': 0.5942083864795918, 'train/loss': 1.8202193902463328, 'validation/accuracy': 0.5436, 'validation/loss': 2.080743125, 'validation/num_examples': 50000, 'test/accuracy': 0.4151, 'test/loss': 2.805365234375, 'test/num_examples': 10000, 'score': 3575.4831614494324, 'total_duration': 4379.564140319824, 'accumulated_submission_time': 3575.4831614494324, 'accumulated_eval_time': 799.7509381771088, 'accumulated_logging_time': 0.1434004306793213}
I0608 19:21:03.599050 139696957024000 logging_writer.py:48] [9449] accumulated_eval_time=799.750938, accumulated_logging_time=0.143400, accumulated_submission_time=3575.483161, global_step=9449, preemption_count=0, score=3575.483161, test/accuracy=0.415100, test/loss=2.805365, test/num_examples=10000, total_duration=4379.564140, train/accuracy=0.594208, train/loss=1.820219, validation/accuracy=0.543600, validation/loss=2.080743, validation/num_examples=50000
I0608 19:21:23.060395 139696965416704 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.958469, loss=3.038445
I0608 19:21:23.064900 139749874411328 submission.py:120] 9500) loss = 3.038, grad_norm = 0.958
I0608 19:24:31.762492 139696957024000 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.022126, loss=2.934424
I0608 19:24:31.767910 139749874411328 submission.py:120] 10000) loss = 2.934, grad_norm = 1.022
I0608 19:27:41.062727 139696965416704 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.906173, loss=3.015378
I0608 19:27:41.066842 139749874411328 submission.py:120] 10500) loss = 3.015, grad_norm = 0.906
I0608 19:29:33.786916 139749874411328 spec.py:298] Evaluating on the training split.
I0608 19:30:18.138928 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 19:31:07.560215 139749874411328 spec.py:326] Evaluating on the test split.
I0608 19:31:08.922576 139749874411328 submission_runner.py:419] Time since start: 4984.90s, 	Step: 10801, 	{'train/accuracy': 0.6136997767857143, 'train/loss': 1.7404437940947863, 'validation/accuracy': 0.55408, 'validation/loss': 2.019565625, 'validation/num_examples': 50000, 'test/accuracy': 0.42, 'test/loss': 2.7894474609375, 'test/num_examples': 10000, 'score': 4085.0703444480896, 'total_duration': 4984.899262905121, 'accumulated_submission_time': 4085.0703444480896, 'accumulated_eval_time': 894.8866424560547, 'accumulated_logging_time': 0.16315817832946777}
I0608 19:31:08.933092 139696957024000 logging_writer.py:48] [10801] accumulated_eval_time=894.886642, accumulated_logging_time=0.163158, accumulated_submission_time=4085.070344, global_step=10801, preemption_count=0, score=4085.070344, test/accuracy=0.420000, test/loss=2.789447, test/num_examples=10000, total_duration=4984.899263, train/accuracy=0.613700, train/loss=1.740444, validation/accuracy=0.554080, validation/loss=2.019566, validation/num_examples=50000
I0608 19:32:24.204193 139696965416704 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.733471, loss=2.803749
I0608 19:32:24.208138 139749874411328 submission.py:120] 11000) loss = 2.804, grad_norm = 0.733
I0608 19:35:34.113675 139696957024000 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.770345, loss=2.733743
I0608 19:35:34.118819 139749874411328 submission.py:120] 11500) loss = 2.734, grad_norm = 0.770
I0608 19:38:41.763520 139696965416704 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.724593, loss=2.737762
I0608 19:38:41.767784 139749874411328 submission.py:120] 12000) loss = 2.738, grad_norm = 0.725
I0608 19:39:39.178156 139749874411328 spec.py:298] Evaluating on the training split.
I0608 19:40:23.618882 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 19:41:10.639957 139749874411328 spec.py:326] Evaluating on the test split.
I0608 19:41:12.002180 139749874411328 submission_runner.py:419] Time since start: 5587.98s, 	Step: 12154, 	{'train/accuracy': 0.6123046875, 'train/loss': 1.752645375777264, 'validation/accuracy': 0.55518, 'validation/loss': 2.036, 'validation/num_examples': 50000, 'test/accuracy': 0.4214, 'test/loss': 2.7879666015625, 'test/num_examples': 10000, 'score': 4594.71954035759, 'total_duration': 5587.978865623474, 'accumulated_submission_time': 4594.71954035759, 'accumulated_eval_time': 987.710599899292, 'accumulated_logging_time': 0.18181633949279785}
I0608 19:41:12.012447 139696957024000 logging_writer.py:48] [12154] accumulated_eval_time=987.710600, accumulated_logging_time=0.181816, accumulated_submission_time=4594.719540, global_step=12154, preemption_count=0, score=4594.719540, test/accuracy=0.421400, test/loss=2.787967, test/num_examples=10000, total_duration=5587.978866, train/accuracy=0.612305, train/loss=1.752645, validation/accuracy=0.555180, validation/loss=2.036000, validation/num_examples=50000
I0608 19:43:22.846488 139696965416704 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.864584, loss=2.873998
I0608 19:43:22.852098 139749874411328 submission.py:120] 12500) loss = 2.874, grad_norm = 0.865
I0608 19:46:32.026102 139696957024000 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.579611, loss=2.744293
I0608 19:46:32.030793 139749874411328 submission.py:120] 13000) loss = 2.744, grad_norm = 0.580
I0608 19:49:40.040306 139696965416704 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.608216, loss=2.670455
I0608 19:49:40.045280 139749874411328 submission.py:120] 13500) loss = 2.670, grad_norm = 0.608
I0608 19:49:42.308929 139749874411328 spec.py:298] Evaluating on the training split.
I0608 19:50:26.456135 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 19:51:20.272301 139749874411328 spec.py:326] Evaluating on the test split.
I0608 19:51:21.636313 139749874411328 submission_runner.py:419] Time since start: 6197.61s, 	Step: 13507, 	{'train/accuracy': 0.6807238520408163, 'train/loss': 1.3944417213907048, 'validation/accuracy': 0.61206, 'validation/loss': 1.722970625, 'validation/num_examples': 50000, 'test/accuracy': 0.4748, 'test/loss': 2.4524080078125, 'test/num_examples': 10000, 'score': 5104.418829679489, 'total_duration': 6197.6118516922, 'accumulated_submission_time': 5104.418829679489, 'accumulated_eval_time': 1087.0367939472198, 'accumulated_logging_time': 0.1999657154083252}
I0608 19:51:21.647036 139696957024000 logging_writer.py:48] [13507] accumulated_eval_time=1087.036794, accumulated_logging_time=0.199966, accumulated_submission_time=5104.418830, global_step=13507, preemption_count=0, score=5104.418830, test/accuracy=0.474800, test/loss=2.452408, test/num_examples=10000, total_duration=6197.611852, train/accuracy=0.680724, train/loss=1.394442, validation/accuracy=0.612060, validation/loss=1.722971, validation/num_examples=50000
I0608 19:54:29.247855 139696965416704 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.557080, loss=2.612105
I0608 19:54:29.252592 139749874411328 submission.py:120] 14000) loss = 2.612, grad_norm = 0.557
I0608 19:57:37.060403 139696957024000 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.660888, loss=2.667637
I0608 19:57:37.065764 139749874411328 submission.py:120] 14500) loss = 2.668, grad_norm = 0.661
I0608 19:59:51.648011 139749874411328 spec.py:298] Evaluating on the training split.
I0608 20:00:35.883352 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 20:01:23.293562 139749874411328 spec.py:326] Evaluating on the test split.
I0608 20:01:24.653644 139749874411328 submission_runner.py:419] Time since start: 6800.63s, 	Step: 14858, 	{'train/accuracy': 0.6696029974489796, 'train/loss': 1.4393989407286352, 'validation/accuracy': 0.6027, 'validation/loss': 1.77343484375, 'validation/num_examples': 50000, 'test/accuracy': 0.4652, 'test/loss': 2.514530078125, 'test/num_examples': 10000, 'score': 5613.818248987198, 'total_duration': 6800.630295753479, 'accumulated_submission_time': 5613.818248987198, 'accumulated_eval_time': 1180.0423018932343, 'accumulated_logging_time': 0.2188100814819336}
I0608 20:01:24.666800 139696965416704 logging_writer.py:48] [14858] accumulated_eval_time=1180.042302, accumulated_logging_time=0.218810, accumulated_submission_time=5613.818249, global_step=14858, preemption_count=0, score=5613.818249, test/accuracy=0.465200, test/loss=2.514530, test/num_examples=10000, total_duration=6800.630296, train/accuracy=0.669603, train/loss=1.439399, validation/accuracy=0.602700, validation/loss=1.773435, validation/num_examples=50000
I0608 20:02:18.620787 139696957024000 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.530809, loss=2.602373
I0608 20:02:18.625131 139749874411328 submission.py:120] 15000) loss = 2.602, grad_norm = 0.531
I0608 20:05:27.862800 139696965416704 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.573331, loss=2.537876
I0608 20:05:27.867756 139749874411328 submission.py:120] 15500) loss = 2.538, grad_norm = 0.573
I0608 20:08:36.083221 139696957024000 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.525715, loss=2.555907
I0608 20:08:36.093522 139749874411328 submission.py:120] 16000) loss = 2.556, grad_norm = 0.526
I0608 20:09:54.746072 139749874411328 spec.py:298] Evaluating on the training split.
I0608 20:10:38.460640 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 20:11:32.298376 139749874411328 spec.py:326] Evaluating on the test split.
I0608 20:11:33.658455 139749874411328 submission_runner.py:419] Time since start: 7409.63s, 	Step: 16209, 	{'train/accuracy': 0.6984016262755102, 'train/loss': 1.350320932816486, 'validation/accuracy': 0.6224, 'validation/loss': 1.70775578125, 'validation/num_examples': 50000, 'test/accuracy': 0.4838, 'test/loss': 2.446226171875, 'test/num_examples': 10000, 'score': 6123.29189157486, 'total_duration': 7409.633941411972, 'accumulated_submission_time': 6123.29189157486, 'accumulated_eval_time': 1278.953506231308, 'accumulated_logging_time': 0.24216747283935547}
I0608 20:11:33.669072 139696965416704 logging_writer.py:48] [16209] accumulated_eval_time=1278.953506, accumulated_logging_time=0.242167, accumulated_submission_time=6123.291892, global_step=16209, preemption_count=0, score=6123.291892, test/accuracy=0.483800, test/loss=2.446226, test/num_examples=10000, total_duration=7409.633941, train/accuracy=0.698402, train/loss=1.350321, validation/accuracy=0.622400, validation/loss=1.707756, validation/num_examples=50000
I0608 20:13:24.848657 139696957024000 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.581389, loss=2.585119
I0608 20:13:24.853018 139749874411328 submission.py:120] 16500) loss = 2.585, grad_norm = 0.581
I0608 20:16:32.542929 139696965416704 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.530922, loss=2.538393
I0608 20:16:32.547015 139749874411328 submission.py:120] 17000) loss = 2.538, grad_norm = 0.531
I0608 20:19:41.046814 139696957024000 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.561250, loss=2.604643
I0608 20:19:41.052476 139749874411328 submission.py:120] 17500) loss = 2.605, grad_norm = 0.561
I0608 20:20:04.008124 139749874411328 spec.py:298] Evaluating on the training split.
I0608 20:20:47.641120 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 20:21:37.724609 139749874411328 spec.py:326] Evaluating on the test split.
I0608 20:21:39.085005 139749874411328 submission_runner.py:419] Time since start: 8015.06s, 	Step: 17558, 	{'train/accuracy': 0.7170758928571429, 'train/loss': 1.224493377062739, 'validation/accuracy': 0.63682, 'validation/loss': 1.59553125, 'validation/num_examples': 50000, 'test/accuracy': 0.4928, 'test/loss': 2.335598046875, 'test/num_examples': 10000, 'score': 6633.0330176353455, 'total_duration': 8015.061676263809, 'accumulated_submission_time': 6633.0330176353455, 'accumulated_eval_time': 1374.0302698612213, 'accumulated_logging_time': 0.26075196266174316}
I0608 20:21:39.096138 139696965416704 logging_writer.py:48] [17558] accumulated_eval_time=1374.030270, accumulated_logging_time=0.260752, accumulated_submission_time=6633.033018, global_step=17558, preemption_count=0, score=6633.033018, test/accuracy=0.492800, test/loss=2.335598, test/num_examples=10000, total_duration=8015.061676, train/accuracy=0.717076, train/loss=1.224493, validation/accuracy=0.636820, validation/loss=1.595531, validation/num_examples=50000
I0608 20:24:25.340964 139696957024000 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.593735, loss=2.513041
I0608 20:24:25.345458 139749874411328 submission.py:120] 18000) loss = 2.513, grad_norm = 0.594
I0608 20:27:33.296399 139696965416704 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.490454, loss=2.480175
I0608 20:27:33.300908 139749874411328 submission.py:120] 18500) loss = 2.480, grad_norm = 0.490
I0608 20:30:09.419956 139749874411328 spec.py:298] Evaluating on the training split.
I0608 20:30:53.265942 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 20:31:48.050902 139749874411328 spec.py:326] Evaluating on the test split.
I0608 20:31:49.413773 139749874411328 submission_runner.py:419] Time since start: 8625.39s, 	Step: 18911, 	{'train/accuracy': 0.7066924426020408, 'train/loss': 1.2610742997150033, 'validation/accuracy': 0.62406, 'validation/loss': 1.66098421875, 'validation/num_examples': 50000, 'test/accuracy': 0.4804, 'test/loss': 2.4285322265625, 'test/num_examples': 10000, 'score': 7142.763200044632, 'total_duration': 8625.389287948608, 'accumulated_submission_time': 7142.763200044632, 'accumulated_eval_time': 1474.022938966751, 'accumulated_logging_time': 0.2799344062805176}
I0608 20:31:49.424866 139696957024000 logging_writer.py:48] [18911] accumulated_eval_time=1474.022939, accumulated_logging_time=0.279934, accumulated_submission_time=7142.763200, global_step=18911, preemption_count=0, score=7142.763200, test/accuracy=0.480400, test/loss=2.428532, test/num_examples=10000, total_duration=8625.389288, train/accuracy=0.706692, train/loss=1.261074, validation/accuracy=0.624060, validation/loss=1.660984, validation/num_examples=50000
I0608 20:32:23.121780 139696965416704 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.529198, loss=2.468485
I0608 20:32:23.125890 139749874411328 submission.py:120] 19000) loss = 2.468, grad_norm = 0.529
I0608 20:35:30.937624 139696957024000 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.492510, loss=2.428813
I0608 20:35:30.943512 139749874411328 submission.py:120] 19500) loss = 2.429, grad_norm = 0.493
I0608 20:38:39.411438 139696965416704 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.541139, loss=2.399881
I0608 20:38:39.417332 139749874411328 submission.py:120] 20000) loss = 2.400, grad_norm = 0.541
I0608 20:40:19.730330 139749874411328 spec.py:298] Evaluating on the training split.
I0608 20:41:04.265714 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 20:41:56.015389 139749874411328 spec.py:326] Evaluating on the test split.
I0608 20:41:57.373875 139749874411328 submission_runner.py:419] Time since start: 9233.35s, 	Step: 20264, 	{'train/accuracy': 0.7203244579081632, 'train/loss': 1.23456993881537, 'validation/accuracy': 0.6348, 'validation/loss': 1.6247740625, 'validation/num_examples': 50000, 'test/accuracy': 0.499, 'test/loss': 2.3388466796875, 'test/num_examples': 10000, 'score': 7652.462999820709, 'total_duration': 9233.35052204132, 'accumulated_submission_time': 7652.462999820709, 'accumulated_eval_time': 1571.6663863658905, 'accumulated_logging_time': 0.30271267890930176}
I0608 20:41:57.390029 139696957024000 logging_writer.py:48] [20264] accumulated_eval_time=1571.666386, accumulated_logging_time=0.302713, accumulated_submission_time=7652.463000, global_step=20264, preemption_count=0, score=7652.463000, test/accuracy=0.499000, test/loss=2.338847, test/num_examples=10000, total_duration=9233.350522, train/accuracy=0.720324, train/loss=1.234570, validation/accuracy=0.634800, validation/loss=1.624774, validation/num_examples=50000
I0608 20:43:26.201353 139696965416704 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.495699, loss=2.499787
I0608 20:43:26.205426 139749874411328 submission.py:120] 20500) loss = 2.500, grad_norm = 0.496
I0608 20:46:34.277575 139696957024000 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.432483, loss=2.427334
I0608 20:46:34.282853 139749874411328 submission.py:120] 21000) loss = 2.427, grad_norm = 0.432
I0608 20:49:43.935302 139696965416704 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.674031, loss=2.456546
I0608 20:49:43.939637 139749874411328 submission.py:120] 21500) loss = 2.457, grad_norm = 0.674
I0608 20:50:27.570434 139749874411328 spec.py:298] Evaluating on the training split.
I0608 20:51:12.045365 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 20:52:01.936506 139749874411328 spec.py:326] Evaluating on the test split.
I0608 20:52:03.301158 139749874411328 submission_runner.py:419] Time since start: 9839.28s, 	Step: 21617, 	{'train/accuracy': 0.7498007015306123, 'train/loss': 1.1180796720543686, 'validation/accuracy': 0.6571, 'validation/loss': 1.54331125, 'validation/num_examples': 50000, 'test/accuracy': 0.5227, 'test/loss': 2.24592109375, 'test/num_examples': 10000, 'score': 8162.046283721924, 'total_duration': 9839.276325941086, 'accumulated_submission_time': 8162.046283721924, 'accumulated_eval_time': 1667.3956122398376, 'accumulated_logging_time': 0.3283679485321045}
I0608 20:52:03.312261 139696957024000 logging_writer.py:48] [21617] accumulated_eval_time=1667.395612, accumulated_logging_time=0.328368, accumulated_submission_time=8162.046284, global_step=21617, preemption_count=0, score=8162.046284, test/accuracy=0.522700, test/loss=2.245921, test/num_examples=10000, total_duration=9839.276326, train/accuracy=0.749801, train/loss=1.118080, validation/accuracy=0.657100, validation/loss=1.543311, validation/num_examples=50000
I0608 20:54:27.494175 139696965416704 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.502264, loss=2.485273
I0608 20:54:27.498220 139749874411328 submission.py:120] 22000) loss = 2.485, grad_norm = 0.502
I0608 20:57:36.114328 139696957024000 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.475507, loss=2.423801
I0608 20:57:36.120026 139749874411328 submission.py:120] 22500) loss = 2.424, grad_norm = 0.476
I0608 21:00:33.616267 139749874411328 spec.py:298] Evaluating on the training split.
I0608 21:01:17.938994 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 21:02:09.710314 139749874411328 spec.py:326] Evaluating on the test split.
I0608 21:02:11.073350 139749874411328 submission_runner.py:419] Time since start: 10447.05s, 	Step: 22970, 	{'train/accuracy': 0.7476682079081632, 'train/loss': 1.1125486724230709, 'validation/accuracy': 0.65498, 'validation/loss': 1.5393734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5195, 'test/loss': 2.277894140625, 'test/num_examples': 10000, 'score': 8671.750516653061, 'total_duration': 10447.05002951622, 'accumulated_submission_time': 8671.750516653061, 'accumulated_eval_time': 1764.8526494503021, 'accumulated_logging_time': 0.3478724956512451}
I0608 21:02:11.084866 139696965416704 logging_writer.py:48] [22970] accumulated_eval_time=1764.852649, accumulated_logging_time=0.347872, accumulated_submission_time=8671.750517, global_step=22970, preemption_count=0, score=8671.750517, test/accuracy=0.519500, test/loss=2.277894, test/num_examples=10000, total_duration=10447.050030, train/accuracy=0.747668, train/loss=1.112549, validation/accuracy=0.654980, validation/loss=1.539373, validation/num_examples=50000
I0608 21:02:22.678193 139696957024000 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.431398, loss=2.373549
I0608 21:02:22.682086 139749874411328 submission.py:120] 23000) loss = 2.374, grad_norm = 0.431
I0608 21:05:30.585429 139696965416704 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.422632, loss=2.349421
I0608 21:05:30.589833 139749874411328 submission.py:120] 23500) loss = 2.349, grad_norm = 0.423
I0608 21:08:40.182487 139696957024000 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.479414, loss=2.353814
I0608 21:08:40.187687 139749874411328 submission.py:120] 24000) loss = 2.354, grad_norm = 0.479
I0608 21:10:41.391322 139749874411328 spec.py:298] Evaluating on the training split.
I0608 21:11:26.435211 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 21:12:16.461910 139749874411328 spec.py:326] Evaluating on the test split.
I0608 21:12:17.829952 139749874411328 submission_runner.py:419] Time since start: 11053.81s, 	Step: 24324, 	{'train/accuracy': 0.7506377551020408, 'train/loss': 1.1101288308902664, 'validation/accuracy': 0.6506, 'validation/loss': 1.5604803125, 'validation/num_examples': 50000, 'test/accuracy': 0.516, 'test/loss': 2.269962890625, 'test/num_examples': 10000, 'score': 9181.463483333588, 'total_duration': 11053.805245876312, 'accumulated_submission_time': 9181.463483333588, 'accumulated_eval_time': 1861.289921760559, 'accumulated_logging_time': 0.36819028854370117}
I0608 21:12:17.841262 139696965416704 logging_writer.py:48] [24324] accumulated_eval_time=1861.289922, accumulated_logging_time=0.368190, accumulated_submission_time=9181.463483, global_step=24324, preemption_count=0, score=9181.463483, test/accuracy=0.516000, test/loss=2.269963, test/num_examples=10000, total_duration=11053.805246, train/accuracy=0.750638, train/loss=1.110129, validation/accuracy=0.650600, validation/loss=1.560480, validation/num_examples=50000
I0608 21:13:24.250895 139696957024000 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.502745, loss=2.395580
I0608 21:13:24.254930 139749874411328 submission.py:120] 24500) loss = 2.396, grad_norm = 0.503
I0608 21:16:32.617106 139696965416704 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.458513, loss=2.368933
I0608 21:16:32.625955 139749874411328 submission.py:120] 25000) loss = 2.369, grad_norm = 0.459
I0608 21:19:41.735258 139696957024000 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.465768, loss=2.410894
I0608 21:19:41.739295 139749874411328 submission.py:120] 25500) loss = 2.411, grad_norm = 0.466
I0608 21:20:47.863034 139749874411328 spec.py:298] Evaluating on the training split.
I0608 21:21:32.623834 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 21:22:23.424354 139749874411328 spec.py:326] Evaluating on the test split.
I0608 21:22:24.781545 139749874411328 submission_runner.py:419] Time since start: 11660.76s, 	Step: 25677, 	{'train/accuracy': 0.7821667729591837, 'train/loss': 0.9467768766442124, 'validation/accuracy': 0.67862, 'validation/loss': 1.41338015625, 'validation/num_examples': 50000, 'test/accuracy': 0.5355, 'test/loss': 2.1521478515625, 'test/num_examples': 10000, 'score': 9690.88958621025, 'total_duration': 11660.758251428604, 'accumulated_submission_time': 9690.88958621025, 'accumulated_eval_time': 1958.2085580825806, 'accumulated_logging_time': 0.38829994201660156}
I0608 21:22:24.793326 139696965416704 logging_writer.py:48] [25677] accumulated_eval_time=1958.208558, accumulated_logging_time=0.388300, accumulated_submission_time=9690.889586, global_step=25677, preemption_count=0, score=9690.889586, test/accuracy=0.535500, test/loss=2.152148, test/num_examples=10000, total_duration=11660.758251, train/accuracy=0.782167, train/loss=0.946777, validation/accuracy=0.678620, validation/loss=1.413380, validation/num_examples=50000
I0608 21:24:26.654692 139696957024000 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.419380, loss=2.344019
I0608 21:24:26.659651 139749874411328 submission.py:120] 26000) loss = 2.344, grad_norm = 0.419
I0608 21:27:36.329926 139696965416704 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.430350, loss=2.410059
I0608 21:27:36.335632 139749874411328 submission.py:120] 26500) loss = 2.410, grad_norm = 0.430
I0608 21:30:44.173347 139696957024000 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.440194, loss=2.360071
I0608 21:30:44.178440 139749874411328 submission.py:120] 27000) loss = 2.360, grad_norm = 0.440
I0608 21:30:55.110146 139749874411328 spec.py:298] Evaluating on the training split.
I0608 21:31:39.592432 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 21:32:27.074914 139749874411328 spec.py:326] Evaluating on the test split.
I0608 21:32:28.439369 139749874411328 submission_runner.py:419] Time since start: 12264.42s, 	Step: 27030, 	{'train/accuracy': 0.7674585459183674, 'train/loss': 1.0300382497359295, 'validation/accuracy': 0.66706, 'validation/loss': 1.482361875, 'validation/num_examples': 50000, 'test/accuracy': 0.529, 'test/loss': 2.216233984375, 'test/num_examples': 10000, 'score': 10200.608146429062, 'total_duration': 12264.416088819504, 'accumulated_submission_time': 10200.608146429062, 'accumulated_eval_time': 2051.5377221107483, 'accumulated_logging_time': 0.4082300662994385}
I0608 21:32:28.450353 139696965416704 logging_writer.py:48] [27030] accumulated_eval_time=2051.537722, accumulated_logging_time=0.408230, accumulated_submission_time=10200.608146, global_step=27030, preemption_count=0, score=10200.608146, test/accuracy=0.529000, test/loss=2.216234, test/num_examples=10000, total_duration=12264.416089, train/accuracy=0.767459, train/loss=1.030038, validation/accuracy=0.667060, validation/loss=1.482362, validation/num_examples=50000
I0608 21:35:25.988005 139696957024000 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.434664, loss=2.324535
I0608 21:35:25.994111 139749874411328 submission.py:120] 27500) loss = 2.325, grad_norm = 0.435
I0608 21:38:34.946570 139749874411328 spec.py:298] Evaluating on the training split.
I0608 21:39:18.991879 139749874411328 spec.py:310] Evaluating on the validation split.
I0608 21:40:05.418603 139749874411328 spec.py:326] Evaluating on the test split.
I0608 21:40:06.776354 139749874411328 submission_runner.py:419] Time since start: 12722.75s, 	Step: 28000, 	{'train/accuracy': 0.7611208545918368, 'train/loss': 1.0457381423638792, 'validation/accuracy': 0.65686, 'validation/loss': 1.53159734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5164, 'test/loss': 2.2855771484375, 'test/num_examples': 10000, 'score': 10566.667551994324, 'total_duration': 12722.753021240234, 'accumulated_submission_time': 10566.667551994324, 'accumulated_eval_time': 2143.367429494858, 'accumulated_logging_time': 0.42850208282470703}
I0608 21:40:06.789009 139696965416704 logging_writer.py:48] [28000] accumulated_eval_time=2143.367429, accumulated_logging_time=0.428502, accumulated_submission_time=10566.667552, global_step=28000, preemption_count=0, score=10566.667552, test/accuracy=0.516400, test/loss=2.285577, test/num_examples=10000, total_duration=12722.753021, train/accuracy=0.761121, train/loss=1.045738, validation/accuracy=0.656860, validation/loss=1.531597, validation/num_examples=50000
I0608 21:40:06.809234 139696957024000 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10566.667552
I0608 21:40:07.528820 139749874411328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/adamw/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0608 21:40:07.795760 139749874411328 submission_runner.py:581] Tuning trial 1/1
I0608 21:40:07.795999 139749874411328 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0608 21:40:07.797087 139749874411328 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.001295440051020408, 'train/loss': 6.921907386001275, 'validation/accuracy': 0.00146, 'validation/loss': 6.9212575, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.92266796875, 'test/num_examples': 10000, 'score': 8.117964267730713, 'total_duration': 134.7407829761505, 'accumulated_submission_time': 8.117964267730713, 'accumulated_eval_time': 126.62242221832275, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1341, {'train/accuracy': 0.11136798469387756, 'train/loss': 4.8504180908203125, 'validation/accuracy': 0.10356, 'validation/loss': 4.926635625, 'validation/num_examples': 50000, 'test/accuracy': 0.0707, 'test/loss': 5.302338671875, 'test/num_examples': 10000, 'score': 517.7223064899445, 'total_duration': 742.2470140457153, 'accumulated_submission_time': 517.7223064899445, 'accumulated_eval_time': 223.84112906455994, 'accumulated_logging_time': 0.026972293853759766, 'global_step': 1341, 'preemption_count': 0}), (2690, {'train/accuracy': 0.2392578125, 'train/loss': 3.7761504504145407, 'validation/accuracy': 0.2175, 'validation/loss': 3.9146465625, 'validation/num_examples': 50000, 'test/accuracy': 0.1504, 'test/loss': 4.516253125, 'test/num_examples': 10000, 'score': 1027.1857087612152, 'total_duration': 1353.8151302337646, 'accumulated_submission_time': 1027.1857087612152, 'accumulated_eval_time': 325.3338165283203, 'accumulated_logging_time': 0.04780006408691406, 'global_step': 2690, 'preemption_count': 0}), (4041, {'train/accuracy': 0.3389469068877551, 'train/loss': 3.164668491908482, 'validation/accuracy': 0.31148, 'validation/loss': 3.3307875, 'validation/num_examples': 50000, 'test/accuracy': 0.2135, 'test/loss': 4.062483984375, 'test/num_examples': 10000, 'score': 1536.701803445816, 'total_duration': 1954.2038989067078, 'accumulated_submission_time': 1536.701803445816, 'accumulated_eval_time': 415.60464882850647, 'accumulated_logging_time': 0.06724739074707031, 'global_step': 4041, 'preemption_count': 0}), (5392, {'train/accuracy': 0.4586854272959184, 'train/loss': 2.484075585190131, 'validation/accuracy': 0.41804, 'validation/loss': 2.676306875, 'validation/num_examples': 50000, 'test/accuracy': 0.3146, 'test/loss': 3.359944921875, 'test/num_examples': 10000, 'score': 2046.4686601161957, 'total_duration': 2565.637288570404, 'accumulated_submission_time': 2046.4686601161957, 'accumulated_eval_time': 516.6638307571411, 'accumulated_logging_time': 0.08736729621887207, 'global_step': 5392, 'preemption_count': 0}), (6744, {'train/accuracy': 0.5137715242346939, 'train/loss': 2.18264677086655, 'validation/accuracy': 0.47054, 'validation/loss': 2.402321875, 'validation/num_examples': 50000, 'test/accuracy': 0.351, 'test/loss': 3.11070546875, 'test/num_examples': 10000, 'score': 2556.2421708106995, 'total_duration': 3168.1663432121277, 'accumulated_submission_time': 2556.2421708106995, 'accumulated_eval_time': 608.8109827041626, 'accumulated_logging_time': 0.10605764389038086, 'global_step': 6744, 'preemption_count': 0}), (8096, {'train/accuracy': 0.5440250318877551, 'train/loss': 2.057747510014748, 'validation/accuracy': 0.49942, 'validation/loss': 2.2861575, 'validation/num_examples': 50000, 'test/accuracy': 0.378, 'test/loss': 2.988608984375, 'test/num_examples': 10000, 'score': 3065.7530615329742, 'total_duration': 3775.9509284496307, 'accumulated_submission_time': 3065.7530615329742, 'accumulated_eval_time': 706.4793224334717, 'accumulated_logging_time': 0.12501025199890137, 'global_step': 8096, 'preemption_count': 0}), (9449, {'train/accuracy': 0.5942083864795918, 'train/loss': 1.8202193902463328, 'validation/accuracy': 0.5436, 'validation/loss': 2.080743125, 'validation/num_examples': 50000, 'test/accuracy': 0.4151, 'test/loss': 2.805365234375, 'test/num_examples': 10000, 'score': 3575.4831614494324, 'total_duration': 4379.564140319824, 'accumulated_submission_time': 3575.4831614494324, 'accumulated_eval_time': 799.7509381771088, 'accumulated_logging_time': 0.1434004306793213, 'global_step': 9449, 'preemption_count': 0}), (10801, {'train/accuracy': 0.6136997767857143, 'train/loss': 1.7404437940947863, 'validation/accuracy': 0.55408, 'validation/loss': 2.019565625, 'validation/num_examples': 50000, 'test/accuracy': 0.42, 'test/loss': 2.7894474609375, 'test/num_examples': 10000, 'score': 4085.0703444480896, 'total_duration': 4984.899262905121, 'accumulated_submission_time': 4085.0703444480896, 'accumulated_eval_time': 894.8866424560547, 'accumulated_logging_time': 0.16315817832946777, 'global_step': 10801, 'preemption_count': 0}), (12154, {'train/accuracy': 0.6123046875, 'train/loss': 1.752645375777264, 'validation/accuracy': 0.55518, 'validation/loss': 2.036, 'validation/num_examples': 50000, 'test/accuracy': 0.4214, 'test/loss': 2.7879666015625, 'test/num_examples': 10000, 'score': 4594.71954035759, 'total_duration': 5587.978865623474, 'accumulated_submission_time': 4594.71954035759, 'accumulated_eval_time': 987.710599899292, 'accumulated_logging_time': 0.18181633949279785, 'global_step': 12154, 'preemption_count': 0}), (13507, {'train/accuracy': 0.6807238520408163, 'train/loss': 1.3944417213907048, 'validation/accuracy': 0.61206, 'validation/loss': 1.722970625, 'validation/num_examples': 50000, 'test/accuracy': 0.4748, 'test/loss': 2.4524080078125, 'test/num_examples': 10000, 'score': 5104.418829679489, 'total_duration': 6197.6118516922, 'accumulated_submission_time': 5104.418829679489, 'accumulated_eval_time': 1087.0367939472198, 'accumulated_logging_time': 0.1999657154083252, 'global_step': 13507, 'preemption_count': 0}), (14858, {'train/accuracy': 0.6696029974489796, 'train/loss': 1.4393989407286352, 'validation/accuracy': 0.6027, 'validation/loss': 1.77343484375, 'validation/num_examples': 50000, 'test/accuracy': 0.4652, 'test/loss': 2.514530078125, 'test/num_examples': 10000, 'score': 5613.818248987198, 'total_duration': 6800.630295753479, 'accumulated_submission_time': 5613.818248987198, 'accumulated_eval_time': 1180.0423018932343, 'accumulated_logging_time': 0.2188100814819336, 'global_step': 14858, 'preemption_count': 0}), (16209, {'train/accuracy': 0.6984016262755102, 'train/loss': 1.350320932816486, 'validation/accuracy': 0.6224, 'validation/loss': 1.70775578125, 'validation/num_examples': 50000, 'test/accuracy': 0.4838, 'test/loss': 2.446226171875, 'test/num_examples': 10000, 'score': 6123.29189157486, 'total_duration': 7409.633941411972, 'accumulated_submission_time': 6123.29189157486, 'accumulated_eval_time': 1278.953506231308, 'accumulated_logging_time': 0.24216747283935547, 'global_step': 16209, 'preemption_count': 0}), (17558, {'train/accuracy': 0.7170758928571429, 'train/loss': 1.224493377062739, 'validation/accuracy': 0.63682, 'validation/loss': 1.59553125, 'validation/num_examples': 50000, 'test/accuracy': 0.4928, 'test/loss': 2.335598046875, 'test/num_examples': 10000, 'score': 6633.0330176353455, 'total_duration': 8015.061676263809, 'accumulated_submission_time': 6633.0330176353455, 'accumulated_eval_time': 1374.0302698612213, 'accumulated_logging_time': 0.26075196266174316, 'global_step': 17558, 'preemption_count': 0}), (18911, {'train/accuracy': 0.7066924426020408, 'train/loss': 1.2610742997150033, 'validation/accuracy': 0.62406, 'validation/loss': 1.66098421875, 'validation/num_examples': 50000, 'test/accuracy': 0.4804, 'test/loss': 2.4285322265625, 'test/num_examples': 10000, 'score': 7142.763200044632, 'total_duration': 8625.389287948608, 'accumulated_submission_time': 7142.763200044632, 'accumulated_eval_time': 1474.022938966751, 'accumulated_logging_time': 0.2799344062805176, 'global_step': 18911, 'preemption_count': 0}), (20264, {'train/accuracy': 0.7203244579081632, 'train/loss': 1.23456993881537, 'validation/accuracy': 0.6348, 'validation/loss': 1.6247740625, 'validation/num_examples': 50000, 'test/accuracy': 0.499, 'test/loss': 2.3388466796875, 'test/num_examples': 10000, 'score': 7652.462999820709, 'total_duration': 9233.35052204132, 'accumulated_submission_time': 7652.462999820709, 'accumulated_eval_time': 1571.6663863658905, 'accumulated_logging_time': 0.30271267890930176, 'global_step': 20264, 'preemption_count': 0}), (21617, {'train/accuracy': 0.7498007015306123, 'train/loss': 1.1180796720543686, 'validation/accuracy': 0.6571, 'validation/loss': 1.54331125, 'validation/num_examples': 50000, 'test/accuracy': 0.5227, 'test/loss': 2.24592109375, 'test/num_examples': 10000, 'score': 8162.046283721924, 'total_duration': 9839.276325941086, 'accumulated_submission_time': 8162.046283721924, 'accumulated_eval_time': 1667.3956122398376, 'accumulated_logging_time': 0.3283679485321045, 'global_step': 21617, 'preemption_count': 0}), (22970, {'train/accuracy': 0.7476682079081632, 'train/loss': 1.1125486724230709, 'validation/accuracy': 0.65498, 'validation/loss': 1.5393734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5195, 'test/loss': 2.277894140625, 'test/num_examples': 10000, 'score': 8671.750516653061, 'total_duration': 10447.05002951622, 'accumulated_submission_time': 8671.750516653061, 'accumulated_eval_time': 1764.8526494503021, 'accumulated_logging_time': 0.3478724956512451, 'global_step': 22970, 'preemption_count': 0}), (24324, {'train/accuracy': 0.7506377551020408, 'train/loss': 1.1101288308902664, 'validation/accuracy': 0.6506, 'validation/loss': 1.5604803125, 'validation/num_examples': 50000, 'test/accuracy': 0.516, 'test/loss': 2.269962890625, 'test/num_examples': 10000, 'score': 9181.463483333588, 'total_duration': 11053.805245876312, 'accumulated_submission_time': 9181.463483333588, 'accumulated_eval_time': 1861.289921760559, 'accumulated_logging_time': 0.36819028854370117, 'global_step': 24324, 'preemption_count': 0}), (25677, {'train/accuracy': 0.7821667729591837, 'train/loss': 0.9467768766442124, 'validation/accuracy': 0.67862, 'validation/loss': 1.41338015625, 'validation/num_examples': 50000, 'test/accuracy': 0.5355, 'test/loss': 2.1521478515625, 'test/num_examples': 10000, 'score': 9690.88958621025, 'total_duration': 11660.758251428604, 'accumulated_submission_time': 9690.88958621025, 'accumulated_eval_time': 1958.2085580825806, 'accumulated_logging_time': 0.38829994201660156, 'global_step': 25677, 'preemption_count': 0}), (27030, {'train/accuracy': 0.7674585459183674, 'train/loss': 1.0300382497359295, 'validation/accuracy': 0.66706, 'validation/loss': 1.482361875, 'validation/num_examples': 50000, 'test/accuracy': 0.529, 'test/loss': 2.216233984375, 'test/num_examples': 10000, 'score': 10200.608146429062, 'total_duration': 12264.416088819504, 'accumulated_submission_time': 10200.608146429062, 'accumulated_eval_time': 2051.5377221107483, 'accumulated_logging_time': 0.4082300662994385, 'global_step': 27030, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7611208545918368, 'train/loss': 1.0457381423638792, 'validation/accuracy': 0.65686, 'validation/loss': 1.53159734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5164, 'test/loss': 2.2855771484375, 'test/num_examples': 10000, 'score': 10566.667551994324, 'total_duration': 12722.753021240234, 'accumulated_submission_time': 10566.667551994324, 'accumulated_eval_time': 2143.367429494858, 'accumulated_logging_time': 0.42850208282470703, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0608 21:40:07.797225 139749874411328 submission_runner.py:584] Timing: 10566.667551994324
I0608 21:40:07.797275 139749874411328 submission_runner.py:586] Total number of evals: 22
I0608 21:40:07.797317 139749874411328 submission_runner.py:587] ====================
I0608 21:40:07.797431 139749874411328 submission_runner.py:655] Final imagenet_resnet score: 10566.667551994324
