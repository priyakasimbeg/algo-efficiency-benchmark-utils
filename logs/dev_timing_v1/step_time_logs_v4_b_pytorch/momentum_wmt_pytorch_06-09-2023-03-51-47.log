torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=20000 2>&1 | tee -a /logs/wmt_pytorch_06-09-2023-03-51-47.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 03:52:10.300023 139749357221696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 03:52:10.300051 139733055211328 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 03:52:10.300069 140143528748864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 03:52:10.300097 140574815799104 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 03:52:10.300690 140034190636864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 03:52:10.300884 140222489282368 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 03:52:10.300999 140447348516672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 03:52:10.301427 140597298014016 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 03:52:10.301748 140597298014016 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.310635 139749357221696 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.310667 139733055211328 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.310697 140574815799104 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.310729 140143528748864 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.311284 140034190636864 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.311534 140222489282368 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:10.311613 140447348516672 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 03:52:14.808640 140597298014016 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/momentum/wmt_pytorch.
W0609 03:52:14.929261 140574815799104 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.930713 139733055211328 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.931398 139749357221696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.931859 140034190636864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.932117 140597298014016 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.932345 140143528748864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.932491 140222489282368 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 03:52:14.932678 140447348516672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 03:52:14.937470 140597298014016 submission_runner.py:541] Using RNG seed 869900999
I0609 03:52:14.938803 140597298014016 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 03:52:14.938920 140597298014016 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/momentum/wmt_pytorch/trial_1.
I0609 03:52:14.939136 140597298014016 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/momentum/wmt_pytorch/trial_1/hparams.json.
I0609 03:52:14.940105 140597298014016 submission_runner.py:255] Initializing dataset.
I0609 03:52:14.940217 140597298014016 submission_runner.py:262] Initializing model.
I0609 03:52:18.541939 140597298014016 submission_runner.py:272] Initializing optimizer.
I0609 03:52:19.051702 140597298014016 submission_runner.py:279] Initializing metrics bundle.
I0609 03:52:19.051895 140597298014016 submission_runner.py:297] Initializing checkpoint and logger.
I0609 03:52:19.055751 140597298014016 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 03:52:19.055873 140597298014016 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 03:52:19.547991 140597298014016 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/momentum/wmt_pytorch/trial_1/meta_data_0.json.
I0609 03:52:19.550822 140597298014016 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/momentum/wmt_pytorch/trial_1/flags_0.json.
I0609 03:52:19.609438 140597298014016 submission_runner.py:332] Starting training loop.
I0609 03:52:19.622038 140597298014016 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:52:19.625323 140597298014016 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:52:19.625437 140597298014016 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:52:19.702106 140597298014016 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:52:23.984220 140549046662912 logging_writer.py:48] [0] global_step=0, grad_norm=5.195028, loss=11.049775
I0609 03:52:23.994322 140597298014016 spec.py:298] Evaluating on the training split.
I0609 03:52:23.997008 140597298014016 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:52:23.999891 140597298014016 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:52:24.000020 140597298014016 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:52:24.029492 140597298014016 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0609 03:52:28.190848 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 03:57:00.555620 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 03:57:00.559026 140597298014016 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 03:57:00.562267 140597298014016 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 03:57:00.562389 140597298014016 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 03:57:00.592658 140597298014016 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 03:57:04.418477 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:01:31.787370 140597298014016 spec.py:326] Evaluating on the test split.
I0609 04:01:31.790308 140597298014016 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 04:01:31.793613 140597298014016 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0609 04:01:31.793751 140597298014016 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0609 04:01:31.822474 140597298014016 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0609 04:01:35.701222 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:06:08.589211 140597298014016 submission_runner.py:419] Time since start: 828.98s, 	Step: 1, 	{'train/accuracy': 0.0005054101863125732, 'train/loss': 11.057106325667945, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.077843114158535, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.06855063622102, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.384934902191162, 'total_duration': 828.9801795482635, 'accumulated_submission_time': 4.384934902191162, 'accumulated_eval_time': 824.5947737693787, 'accumulated_logging_time': 0}
I0609 04:06:08.605899 140539314632448 logging_writer.py:48] [1] accumulated_eval_time=824.594774, accumulated_logging_time=0, accumulated_submission_time=4.384935, global_step=1, preemption_count=0, score=4.384935, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.068551, test/num_examples=3003, total_duration=828.980180, train/accuracy=0.000505, train/bleu=0.000000, train/loss=11.057106, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.077843, validation/num_examples=3000
I0609 04:06:08.627485 140597298014016 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627480 140447348516672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627483 139749357221696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627527 140143528748864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627543 140034190636864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627555 140574815799104 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627549 140222489282368 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:08.627798 139733055211328 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 04:06:09.052487 140539306239744 logging_writer.py:48] [1] global_step=1, grad_norm=5.170760, loss=11.046172
I0609 04:06:09.487170 140539314632448 logging_writer.py:48] [2] global_step=2, grad_norm=5.217235, loss=11.047877
I0609 04:06:09.930976 140539306239744 logging_writer.py:48] [3] global_step=3, grad_norm=5.120288, loss=11.039691
I0609 04:06:10.363729 140539314632448 logging_writer.py:48] [4] global_step=4, grad_norm=5.048456, loss=11.013751
I0609 04:06:10.797772 140539306239744 logging_writer.py:48] [5] global_step=5, grad_norm=5.040646, loss=10.984947
I0609 04:06:11.228869 140539314632448 logging_writer.py:48] [6] global_step=6, grad_norm=4.951230, loss=10.944436
I0609 04:06:11.662611 140539306239744 logging_writer.py:48] [7] global_step=7, grad_norm=4.779940, loss=10.883094
I0609 04:06:12.097770 140539314632448 logging_writer.py:48] [8] global_step=8, grad_norm=4.596697, loss=10.822065
I0609 04:06:12.528278 140539306239744 logging_writer.py:48] [9] global_step=9, grad_norm=4.301549, loss=10.749100
I0609 04:06:12.963464 140539314632448 logging_writer.py:48] [10] global_step=10, grad_norm=4.006505, loss=10.657370
I0609 04:06:13.397331 140539306239744 logging_writer.py:48] [11] global_step=11, grad_norm=3.710452, loss=10.567279
I0609 04:06:13.832192 140539314632448 logging_writer.py:48] [12] global_step=12, grad_norm=3.362191, loss=10.479724
I0609 04:06:14.272759 140539306239744 logging_writer.py:48] [13] global_step=13, grad_norm=3.047750, loss=10.386894
I0609 04:06:14.709858 140539314632448 logging_writer.py:48] [14] global_step=14, grad_norm=2.761963, loss=10.297519
I0609 04:06:15.148252 140539306239744 logging_writer.py:48] [15] global_step=15, grad_norm=2.456906, loss=10.230711
I0609 04:06:15.582622 140539314632448 logging_writer.py:48] [16] global_step=16, grad_norm=2.221228, loss=10.150598
I0609 04:06:16.017074 140539306239744 logging_writer.py:48] [17] global_step=17, grad_norm=2.039031, loss=10.078069
I0609 04:06:16.450404 140539314632448 logging_writer.py:48] [18] global_step=18, grad_norm=1.845778, loss=10.020720
I0609 04:06:16.884366 140539306239744 logging_writer.py:48] [19] global_step=19, grad_norm=1.701874, loss=9.965751
I0609 04:06:17.316488 140539314632448 logging_writer.py:48] [20] global_step=20, grad_norm=1.549028, loss=9.912926
I0609 04:06:17.748385 140539306239744 logging_writer.py:48] [21] global_step=21, grad_norm=1.468602, loss=9.868331
I0609 04:06:18.182596 140539314632448 logging_writer.py:48] [22] global_step=22, grad_norm=1.377783, loss=9.836527
I0609 04:06:18.613819 140539306239744 logging_writer.py:48] [23] global_step=23, grad_norm=1.319012, loss=9.788974
I0609 04:06:19.044645 140539314632448 logging_writer.py:48] [24] global_step=24, grad_norm=1.277875, loss=9.762894
I0609 04:06:19.477173 140539306239744 logging_writer.py:48] [25] global_step=25, grad_norm=1.229547, loss=9.710849
I0609 04:06:19.907704 140539314632448 logging_writer.py:48] [26] global_step=26, grad_norm=1.191433, loss=9.690475
I0609 04:06:20.339070 140539306239744 logging_writer.py:48] [27] global_step=27, grad_norm=1.130239, loss=9.659146
I0609 04:06:20.771641 140539314632448 logging_writer.py:48] [28] global_step=28, grad_norm=1.081051, loss=9.638121
I0609 04:06:21.205591 140539306239744 logging_writer.py:48] [29] global_step=29, grad_norm=1.037714, loss=9.604767
I0609 04:06:21.639082 140539314632448 logging_writer.py:48] [30] global_step=30, grad_norm=0.991514, loss=9.560327
I0609 04:06:22.077789 140539306239744 logging_writer.py:48] [31] global_step=31, grad_norm=0.939227, loss=9.558841
I0609 04:06:22.517010 140539314632448 logging_writer.py:48] [32] global_step=32, grad_norm=0.877269, loss=9.526727
I0609 04:06:22.951966 140539306239744 logging_writer.py:48] [33] global_step=33, grad_norm=0.849650, loss=9.499820
I0609 04:06:23.385975 140539314632448 logging_writer.py:48] [34] global_step=34, grad_norm=0.793521, loss=9.484698
I0609 04:06:23.817120 140539306239744 logging_writer.py:48] [35] global_step=35, grad_norm=0.750566, loss=9.487633
I0609 04:06:24.253801 140539314632448 logging_writer.py:48] [36] global_step=36, grad_norm=0.745896, loss=9.447821
I0609 04:06:24.687867 140539306239744 logging_writer.py:48] [37] global_step=37, grad_norm=0.688832, loss=9.449218
I0609 04:06:25.123084 140539314632448 logging_writer.py:48] [38] global_step=38, grad_norm=0.667276, loss=9.429238
I0609 04:06:25.554870 140539306239744 logging_writer.py:48] [39] global_step=39, grad_norm=0.640878, loss=9.423944
I0609 04:06:25.986270 140539314632448 logging_writer.py:48] [40] global_step=40, grad_norm=0.626252, loss=9.400682
I0609 04:06:26.418550 140539306239744 logging_writer.py:48] [41] global_step=41, grad_norm=0.609067, loss=9.374231
I0609 04:06:26.852224 140539314632448 logging_writer.py:48] [42] global_step=42, grad_norm=0.588284, loss=9.378751
I0609 04:06:27.288396 140539306239744 logging_writer.py:48] [43] global_step=43, grad_norm=0.585181, loss=9.336913
I0609 04:06:27.721365 140539314632448 logging_writer.py:48] [44] global_step=44, grad_norm=0.573189, loss=9.336412
I0609 04:06:28.155200 140539306239744 logging_writer.py:48] [45] global_step=45, grad_norm=0.556436, loss=9.328701
I0609 04:06:28.596189 140539314632448 logging_writer.py:48] [46] global_step=46, grad_norm=0.532723, loss=9.302214
I0609 04:06:29.031072 140539306239744 logging_writer.py:48] [47] global_step=47, grad_norm=0.525066, loss=9.280672
I0609 04:06:29.463628 140539314632448 logging_writer.py:48] [48] global_step=48, grad_norm=0.502795, loss=9.300271
I0609 04:06:29.897294 140539306239744 logging_writer.py:48] [49] global_step=49, grad_norm=0.504233, loss=9.262524
I0609 04:06:30.329308 140539314632448 logging_writer.py:48] [50] global_step=50, grad_norm=0.478313, loss=9.274396
I0609 04:06:30.762093 140539306239744 logging_writer.py:48] [51] global_step=51, grad_norm=0.448000, loss=9.271445
I0609 04:06:31.195776 140539314632448 logging_writer.py:48] [52] global_step=52, grad_norm=0.441612, loss=9.248988
I0609 04:06:31.630563 140539306239744 logging_writer.py:48] [53] global_step=53, grad_norm=0.413421, loss=9.259421
I0609 04:06:32.064599 140539314632448 logging_writer.py:48] [54] global_step=54, grad_norm=0.406289, loss=9.227421
I0609 04:06:32.496788 140539306239744 logging_writer.py:48] [55] global_step=55, grad_norm=0.386017, loss=9.229799
I0609 04:06:32.932381 140539314632448 logging_writer.py:48] [56] global_step=56, grad_norm=0.386023, loss=9.262423
I0609 04:06:33.363827 140539306239744 logging_writer.py:48] [57] global_step=57, grad_norm=0.386731, loss=9.206230
I0609 04:06:33.797568 140539314632448 logging_writer.py:48] [58] global_step=58, grad_norm=0.365622, loss=9.208782
I0609 04:06:34.228276 140539306239744 logging_writer.py:48] [59] global_step=59, grad_norm=0.352358, loss=9.225792
I0609 04:06:34.661604 140539314632448 logging_writer.py:48] [60] global_step=60, grad_norm=0.349783, loss=9.192149
I0609 04:06:35.093195 140539306239744 logging_writer.py:48] [61] global_step=61, grad_norm=0.344306, loss=9.222268
I0609 04:06:35.525004 140539314632448 logging_writer.py:48] [62] global_step=62, grad_norm=0.343787, loss=9.160879
I0609 04:06:35.957859 140539306239744 logging_writer.py:48] [63] global_step=63, grad_norm=0.338589, loss=9.192187
I0609 04:06:36.390812 140539314632448 logging_writer.py:48] [64] global_step=64, grad_norm=0.322941, loss=9.166024
I0609 04:06:36.829819 140539306239744 logging_writer.py:48] [65] global_step=65, grad_norm=0.324347, loss=9.169725
I0609 04:06:37.263774 140539314632448 logging_writer.py:48] [66] global_step=66, grad_norm=0.316005, loss=9.154150
I0609 04:06:37.697646 140539306239744 logging_writer.py:48] [67] global_step=67, grad_norm=0.316084, loss=9.141191
I0609 04:06:38.135799 140539314632448 logging_writer.py:48] [68] global_step=68, grad_norm=0.291129, loss=9.131829
I0609 04:06:38.567327 140539306239744 logging_writer.py:48] [69] global_step=69, grad_norm=0.284340, loss=9.164853
I0609 04:06:39.001531 140539314632448 logging_writer.py:48] [70] global_step=70, grad_norm=0.289244, loss=9.109898
I0609 04:06:39.434859 140539306239744 logging_writer.py:48] [71] global_step=71, grad_norm=0.275465, loss=9.158054
I0609 04:06:39.874325 140539314632448 logging_writer.py:48] [72] global_step=72, grad_norm=0.265332, loss=9.112876
I0609 04:06:40.309048 140539306239744 logging_writer.py:48] [73] global_step=73, grad_norm=0.264849, loss=9.118261
I0609 04:06:40.742784 140539314632448 logging_writer.py:48] [74] global_step=74, grad_norm=0.260104, loss=9.100568
I0609 04:06:41.177098 140539306239744 logging_writer.py:48] [75] global_step=75, grad_norm=0.250613, loss=9.125089
I0609 04:06:41.611649 140539314632448 logging_writer.py:48] [76] global_step=76, grad_norm=0.247021, loss=9.082911
I0609 04:06:42.047704 140539306239744 logging_writer.py:48] [77] global_step=77, grad_norm=0.244113, loss=9.106780
I0609 04:06:42.482605 140539314632448 logging_writer.py:48] [78] global_step=78, grad_norm=0.237584, loss=9.065018
I0609 04:06:42.914357 140539306239744 logging_writer.py:48] [79] global_step=79, grad_norm=0.229237, loss=9.116041
I0609 04:06:43.347698 140539314632448 logging_writer.py:48] [80] global_step=80, grad_norm=0.226034, loss=9.080443
I0609 04:06:43.780028 140539306239744 logging_writer.py:48] [81] global_step=81, grad_norm=0.218987, loss=9.100365
I0609 04:06:44.213551 140539314632448 logging_writer.py:48] [82] global_step=82, grad_norm=0.220969, loss=9.063735
I0609 04:06:44.644987 140539306239744 logging_writer.py:48] [83] global_step=83, grad_norm=0.204477, loss=9.073951
I0609 04:06:45.079362 140539314632448 logging_writer.py:48] [84] global_step=84, grad_norm=0.212511, loss=9.097179
I0609 04:06:45.514931 140539306239744 logging_writer.py:48] [85] global_step=85, grad_norm=0.207336, loss=9.057100
I0609 04:06:45.950859 140539314632448 logging_writer.py:48] [86] global_step=86, grad_norm=0.210549, loss=9.075508
I0609 04:06:46.383316 140539306239744 logging_writer.py:48] [87] global_step=87, grad_norm=0.204427, loss=9.057129
I0609 04:06:46.814582 140539314632448 logging_writer.py:48] [88] global_step=88, grad_norm=0.207310, loss=9.040604
I0609 04:06:47.248894 140539306239744 logging_writer.py:48] [89] global_step=89, grad_norm=0.197224, loss=9.044387
I0609 04:06:47.680704 140539314632448 logging_writer.py:48] [90] global_step=90, grad_norm=0.198415, loss=9.069152
I0609 04:06:48.113444 140539306239744 logging_writer.py:48] [91] global_step=91, grad_norm=0.195705, loss=9.066261
I0609 04:06:48.546133 140539314632448 logging_writer.py:48] [92] global_step=92, grad_norm=0.194860, loss=9.054385
I0609 04:06:48.977821 140539306239744 logging_writer.py:48] [93] global_step=93, grad_norm=0.178346, loss=9.060322
I0609 04:06:49.409912 140539314632448 logging_writer.py:48] [94] global_step=94, grad_norm=0.187611, loss=9.071651
I0609 04:06:49.849911 140539306239744 logging_writer.py:48] [95] global_step=95, grad_norm=0.187970, loss=9.022648
I0609 04:06:50.287097 140539314632448 logging_writer.py:48] [96] global_step=96, grad_norm=0.181832, loss=9.010930
I0609 04:06:50.722692 140539306239744 logging_writer.py:48] [97] global_step=97, grad_norm=0.176297, loss=9.050800
I0609 04:06:51.154863 140539314632448 logging_writer.py:48] [98] global_step=98, grad_norm=0.174331, loss=9.046826
I0609 04:06:51.587876 140539306239744 logging_writer.py:48] [99] global_step=99, grad_norm=0.166036, loss=9.053481
I0609 04:06:52.020289 140539314632448 logging_writer.py:48] [100] global_step=100, grad_norm=0.166575, loss=9.032369
I0609 04:09:40.908683 140539306239744 logging_writer.py:48] [500] global_step=500, grad_norm=0.379422, loss=8.414151
I0609 04:13:12.168887 140539314632448 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.740867, loss=7.772114
I0609 04:16:43.497637 140539306239744 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.822037, loss=7.443101
I0609 04:20:08.953012 140597298014016 spec.py:298] Evaluating on the training split.
I0609 04:20:12.815107 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:24:39.319854 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 04:24:43.029485 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:29:02.577296 140597298014016 spec.py:326] Evaluating on the test split.
I0609 04:29:06.362727 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:33:35.581648 140597298014016 submission_runner.py:419] Time since start: 2475.97s, 	Step: 1987, 	{'train/accuracy': 0.28377701934015925, 'train/loss': 5.853253697383391, 'train/bleu': 5.475088416993237, 'validation/accuracy': 0.26292296437737905, 'validation/loss': 6.113344068889412, 'validation/bleu': 2.977993949912662, 'validation/num_examples': 3000, 'test/accuracy': 0.24125268723490792, 'test/loss': 6.408942682005694, 'test/bleu': 1.9630391815225328, 'test/num_examples': 3003, 'score': 844.0200328826904, 'total_duration': 2475.9726145267487, 'accumulated_submission_time': 844.0200328826904, 'accumulated_eval_time': 1631.223284959793, 'accumulated_logging_time': 0.02764153480529785}
I0609 04:33:35.594783 140539314632448 logging_writer.py:48] [1987] accumulated_eval_time=1631.223285, accumulated_logging_time=0.027642, accumulated_submission_time=844.020033, global_step=1987, preemption_count=0, score=844.020033, test/accuracy=0.241253, test/bleu=1.963039, test/loss=6.408943, test/num_examples=3003, total_duration=2475.972615, train/accuracy=0.283777, train/bleu=5.475088, train/loss=5.853254, validation/accuracy=0.262923, validation/bleu=2.977994, validation/loss=6.113344, validation/num_examples=3000
I0609 04:33:41.487421 140539306239744 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.636136, loss=7.042663
I0609 04:37:12.718767 140539314632448 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.605657, loss=6.755192
I0609 04:40:44.151425 140539306239744 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.762524, loss=6.483130
I0609 04:44:15.516308 140539314632448 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.817785, loss=6.112442
I0609 04:47:35.747802 140597298014016 spec.py:298] Evaluating on the training split.
I0609 04:47:39.609974 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:51:05.169325 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 04:51:08.888605 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:54:16.584488 140597298014016 spec.py:326] Evaluating on the test split.
I0609 04:54:20.361652 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 04:57:14.546618 140597298014016 submission_runner.py:419] Time since start: 3894.94s, 	Step: 3975, 	{'train/accuracy': 0.4132028506154738, 'train/loss': 4.337728458417158, 'train/bleu': 14.19307087851006, 'validation/accuracy': 0.4022144796716718, 'validation/loss': 4.422075516732589, 'validation/bleu': 9.588013079384943, 'validation/num_examples': 3000, 'test/accuracy': 0.38685724246121667, 'test/loss': 4.6264060775085705, 'test/bleu': 7.8895629184947875, 'test/num_examples': 3003, 'score': 1683.4681777954102, 'total_duration': 3894.937638759613, 'accumulated_submission_time': 1683.4681777954102, 'accumulated_eval_time': 2210.0220777988434, 'accumulated_logging_time': 0.051088809967041016}
I0609 04:57:14.557187 140539306239744 logging_writer.py:48] [3975] accumulated_eval_time=2210.022078, accumulated_logging_time=0.051089, accumulated_submission_time=1683.468178, global_step=3975, preemption_count=0, score=1683.468178, test/accuracy=0.386857, test/bleu=7.889563, test/loss=4.626406, test/num_examples=3003, total_duration=3894.937639, train/accuracy=0.413203, train/bleu=14.193071, train/loss=4.337728, validation/accuracy=0.402214, validation/bleu=9.588013, validation/loss=4.422076, validation/num_examples=3000
I0609 04:57:25.512339 140539314632448 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.688316, loss=5.917872
I0609 05:00:56.591152 140539306239744 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.606074, loss=5.630756
I0609 05:04:27.895314 140539314632448 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.577960, loss=5.389393
I0609 05:07:59.386936 140539306239744 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.519425, loss=5.311126
I0609 05:11:14.762252 140597298014016 spec.py:298] Evaluating on the training split.
I0609 05:11:18.629166 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:13:45.314896 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 05:13:49.028834 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:16:06.981719 140597298014016 spec.py:326] Evaluating on the test split.
I0609 05:16:10.776197 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:18:22.190311 140597298014016 submission_runner.py:419] Time since start: 5162.58s, 	Step: 5963, 	{'train/accuracy': 0.5042889441812377, 'train/loss': 3.4751372604156017, 'train/bleu': 21.528439040005907, 'validation/accuracy': 0.5080036205378731, 'validation/loss': 3.427724470248354, 'validation/bleu': 17.453508311593463, 'validation/num_examples': 3000, 'test/accuracy': 0.5010167915867759, 'test/loss': 3.5172102870257396, 'test/bleu': 15.81214656059185, 'test/num_examples': 3003, 'score': 2522.978393793106, 'total_duration': 5162.581321239471, 'accumulated_submission_time': 2522.978393793106, 'accumulated_eval_time': 2637.4500370025635, 'accumulated_logging_time': 0.07077336311340332}
I0609 05:18:22.200805 140539314632448 logging_writer.py:48] [5963] accumulated_eval_time=2637.450037, accumulated_logging_time=0.070773, accumulated_submission_time=2522.978394, global_step=5963, preemption_count=0, score=2522.978394, test/accuracy=0.501017, test/bleu=15.812147, test/loss=3.517210, test/num_examples=3003, total_duration=5162.581321, train/accuracy=0.504289, train/bleu=21.528439, train/loss=3.475137, validation/accuracy=0.508004, validation/bleu=17.453508, validation/loss=3.427724, validation/num_examples=3000
I0609 05:18:38.250494 140539306239744 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.460738, loss=5.181481
I0609 05:22:09.544348 140539314632448 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.458800, loss=5.040076
I0609 05:25:40.886234 140539306239744 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.422327, loss=5.119991
I0609 05:29:12.355999 140539314632448 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.404602, loss=4.931723
I0609 05:32:22.274502 140597298014016 spec.py:298] Evaluating on the training split.
I0609 05:32:26.144156 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:34:50.535623 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 05:34:54.250070 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:37:07.072234 140597298014016 spec.py:326] Evaluating on the test split.
I0609 05:37:10.860359 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:39:15.568517 140597298014016 submission_runner.py:419] Time since start: 6415.96s, 	Step: 7950, 	{'train/accuracy': 0.5554622815087397, 'train/loss': 2.9908113284843605, 'train/bleu': 25.410440450104414, 'validation/accuracy': 0.5519212409021587, 'validation/loss': 2.979251070972462, 'validation/bleu': 20.852007604762793, 'validation/num_examples': 3000, 'test/accuracy': 0.5506594619719947, 'test/loss': 3.0220847132647726, 'test/bleu': 19.640039835208068, 'test/num_examples': 3003, 'score': 3362.3598408699036, 'total_duration': 6415.959545373917, 'accumulated_submission_time': 3362.3598408699036, 'accumulated_eval_time': 3050.7440316677094, 'accumulated_logging_time': 0.09055447578430176}
I0609 05:39:15.579087 140539306239744 logging_writer.py:48] [7950] accumulated_eval_time=3050.744032, accumulated_logging_time=0.090554, accumulated_submission_time=3362.359841, global_step=7950, preemption_count=0, score=3362.359841, test/accuracy=0.550659, test/bleu=19.640040, test/loss=3.022085, test/num_examples=3003, total_duration=6415.959545, train/accuracy=0.555462, train/bleu=25.410440, train/loss=2.990811, validation/accuracy=0.551921, validation/bleu=20.852008, validation/loss=2.979251, validation/num_examples=3000
I0609 05:39:37.106086 140539314632448 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.404310, loss=5.005776
I0609 05:43:08.503624 140539306239744 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.374031, loss=4.824400
I0609 05:46:40.090410 140539314632448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.363963, loss=4.895858
I0609 05:50:11.466419 140539306239744 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.390031, loss=4.824469
I0609 05:53:15.726005 140597298014016 spec.py:298] Evaluating on the training split.
I0609 05:53:19.599421 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:55:41.805546 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 05:55:45.508355 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 05:58:02.767466 140597298014016 spec.py:326] Evaluating on the test split.
I0609 05:58:06.561583 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:00:13.231347 140597298014016 submission_runner.py:419] Time since start: 7673.62s, 	Step: 9937, 	{'train/accuracy': 0.5693350430481773, 'train/loss': 2.827032158763052, 'train/bleu': 26.53665208366083, 'validation/accuracy': 0.5738180555727765, 'validation/loss': 2.7521531816096516, 'validation/bleu': 22.533927230700936, 'validation/num_examples': 3000, 'test/accuracy': 0.576642844692348, 'test/loss': 2.77235743129394, 'test/bleu': 21.317588483227176, 'test/num_examples': 3003, 'score': 4201.8130440711975, 'total_duration': 7673.622352600098, 'accumulated_submission_time': 4201.8130440711975, 'accumulated_eval_time': 3468.2493073940277, 'accumulated_logging_time': 0.1112356185913086}
I0609 06:00:13.242158 140539314632448 logging_writer.py:48] [9937] accumulated_eval_time=3468.249307, accumulated_logging_time=0.111236, accumulated_submission_time=4201.813044, global_step=9937, preemption_count=0, score=4201.813044, test/accuracy=0.576643, test/bleu=21.317588, test/loss=2.772357, test/num_examples=3003, total_duration=7673.622353, train/accuracy=0.569335, train/bleu=26.536652, train/loss=2.827032, validation/accuracy=0.573818, validation/bleu=22.533927, validation/loss=2.752153, validation/num_examples=3000
I0609 06:00:40.211954 140539306239744 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.374531, loss=4.684844
I0609 06:04:11.517881 140539314632448 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.371783, loss=4.819367
I0609 06:07:42.869829 140539306239744 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.347906, loss=4.707995
I0609 06:11:14.262665 140539314632448 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.337864, loss=4.680563
I0609 06:14:13.468720 140597298014016 spec.py:298] Evaluating on the training split.
I0609 06:14:17.324016 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:16:28.050450 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 06:16:31.769773 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:18:41.643440 140597298014016 spec.py:326] Evaluating on the test split.
I0609 06:18:45.421675 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:20:52.047203 140597298014016 submission_runner.py:419] Time since start: 8912.44s, 	Step: 11925, 	{'train/accuracy': 0.5829252029155457, 'train/loss': 2.673851163171691, 'train/bleu': 27.409821336063192, 'validation/accuracy': 0.5913379871297318, 'validation/loss': 2.57349587264882, 'validation/bleu': 23.654784478773465, 'validation/num_examples': 3000, 'test/accuracy': 0.5952239846609726, 'test/loss': 2.5781912730230667, 'test/bleu': 22.373584719748354, 'test/num_examples': 3003, 'score': 5041.362347364426, 'total_duration': 8912.438205242157, 'accumulated_submission_time': 5041.362347364426, 'accumulated_eval_time': 3866.8277263641357, 'accumulated_logging_time': 0.13179993629455566}
I0609 06:20:52.058162 140539306239744 logging_writer.py:48] [11925] accumulated_eval_time=3866.827726, accumulated_logging_time=0.131800, accumulated_submission_time=5041.362347, global_step=11925, preemption_count=0, score=5041.362347, test/accuracy=0.595224, test/bleu=22.373585, test/loss=2.578191, test/num_examples=3003, total_duration=8912.438205, train/accuracy=0.582925, train/bleu=27.409821, train/loss=2.673851, validation/accuracy=0.591338, validation/bleu=23.654784, validation/loss=2.573496, validation/num_examples=3000
I0609 06:21:24.096190 140539314632448 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.353490, loss=4.714546
I0609 06:24:55.275678 140539306239744 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.342281, loss=4.592610
I0609 06:28:26.488833 140539314632448 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.337326, loss=4.540957
I0609 06:31:57.660609 140539306239744 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.335808, loss=4.653960
I0609 06:34:52.218327 140597298014016 spec.py:298] Evaluating on the training split.
I0609 06:34:56.083214 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:37:12.879365 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 06:37:16.600852 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:39:25.595873 140597298014016 spec.py:326] Evaluating on the test split.
I0609 06:39:29.403731 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:41:31.317941 140597298014016 submission_runner.py:419] Time since start: 10151.71s, 	Step: 13914, 	{'train/accuracy': 0.5973670712942384, 'train/loss': 2.5496058799224937, 'train/bleu': 28.261151359988734, 'validation/accuracy': 0.6039354750716048, 'validation/loss': 2.4738454498394318, 'validation/bleu': 24.554349113263367, 'validation/num_examples': 3000, 'test/accuracy': 0.605508105281506, 'test/loss': 2.4695338373714484, 'test/bleu': 23.121756294400917, 'test/num_examples': 3003, 'score': 5880.8427810668945, 'total_duration': 10151.708930253983, 'accumulated_submission_time': 5880.8427810668945, 'accumulated_eval_time': 4265.927235126495, 'accumulated_logging_time': 0.1519918441772461}
I0609 06:41:31.329861 140539314632448 logging_writer.py:48] [13914] accumulated_eval_time=4265.927235, accumulated_logging_time=0.151992, accumulated_submission_time=5880.842781, global_step=13914, preemption_count=0, score=5880.842781, test/accuracy=0.605508, test/bleu=23.121756, test/loss=2.469534, test/num_examples=3003, total_duration=10151.708930, train/accuracy=0.597367, train/bleu=28.261151, train/loss=2.549606, validation/accuracy=0.603935, validation/bleu=24.554349, validation/loss=2.473845, validation/num_examples=3000
I0609 06:42:08.025976 140539306239744 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.320581, loss=4.486164
I0609 06:45:39.309447 140539314632448 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.309948, loss=4.529046
I0609 06:49:10.536452 140539306239744 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.332048, loss=4.556671
I0609 06:52:41.709433 140539314632448 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.327051, loss=4.563967
I0609 06:55:31.606061 140597298014016 spec.py:298] Evaluating on the training split.
I0609 06:55:35.476001 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 06:57:53.970515 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 06:57:57.676140 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:00:06.762490 140597298014016 spec.py:326] Evaluating on the test split.
I0609 07:00:10.537232 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:02:17.486858 140597298014016 submission_runner.py:419] Time since start: 11397.88s, 	Step: 15903, 	{'train/accuracy': 0.6038362819690164, 'train/loss': 2.502930135094876, 'train/bleu': 29.297429574853844, 'validation/accuracy': 0.6105441966001661, 'validation/loss': 2.4036473509317924, 'validation/bleu': 25.20660522357201, 'validation/num_examples': 3000, 'test/accuracy': 0.6156992620998198, 'test/loss': 2.388951833129975, 'test/bleu': 23.88116230941021, 'test/num_examples': 3003, 'score': 6720.4412932395935, 'total_duration': 11397.877881765366, 'accumulated_submission_time': 6720.4412932395935, 'accumulated_eval_time': 4671.80801820755, 'accumulated_logging_time': 0.17331242561340332}
I0609 07:02:17.497750 140539306239744 logging_writer.py:48] [15903] accumulated_eval_time=4671.808018, accumulated_logging_time=0.173312, accumulated_submission_time=6720.441293, global_step=15903, preemption_count=0, score=6720.441293, test/accuracy=0.615699, test/bleu=23.881162, test/loss=2.388952, test/num_examples=3003, total_duration=11397.877882, train/accuracy=0.603836, train/bleu=29.297430, train/loss=2.502930, validation/accuracy=0.610544, validation/bleu=25.206605, validation/loss=2.403647, validation/num_examples=3000
I0609 07:02:58.808991 140539314632448 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.327567, loss=4.423497
I0609 07:06:29.735106 140539306239744 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.324096, loss=4.498250
I0609 07:10:01.013045 140539314632448 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.332707, loss=4.518769
I0609 07:13:32.127882 140539306239744 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.324841, loss=4.464849
I0609 07:16:17.737467 140597298014016 spec.py:298] Evaluating on the training split.
I0609 07:16:21.599851 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:18:42.919872 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 07:18:46.622961 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:20:57.492561 140597298014016 spec.py:326] Evaluating on the test split.
I0609 07:21:01.273997 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:23:03.804230 140597298014016 submission_runner.py:419] Time since start: 12644.20s, 	Step: 17893, 	{'train/accuracy': 0.6063646591509195, 'train/loss': 2.4605317443689296, 'train/bleu': 29.288719117737084, 'validation/accuracy': 0.616173389046633, 'validation/loss': 2.344571634263679, 'validation/bleu': 25.21503432452438, 'validation/num_examples': 3000, 'test/accuracy': 0.6206495845680088, 'test/loss': 2.3243563346115854, 'test/bleu': 24.273966061154656, 'test/num_examples': 3003, 'score': 7560.00834608078, 'total_duration': 12644.195238113403, 'accumulated_submission_time': 7560.00834608078, 'accumulated_eval_time': 5077.874714612961, 'accumulated_logging_time': 0.19314289093017578}
I0609 07:23:03.815156 140539314632448 logging_writer.py:48] [17893] accumulated_eval_time=5077.874715, accumulated_logging_time=0.193143, accumulated_submission_time=7560.008346, global_step=17893, preemption_count=0, score=7560.008346, test/accuracy=0.620650, test/bleu=24.273966, test/loss=2.324356, test/num_examples=3003, total_duration=12644.195238, train/accuracy=0.606365, train/bleu=29.288719, train/loss=2.460532, validation/accuracy=0.616173, validation/bleu=25.215034, validation/loss=2.344572, validation/num_examples=3000
I0609 07:23:49.345770 140539306239744 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.305324, loss=4.391582
I0609 07:27:20.598001 140539314632448 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.317303, loss=4.413204
I0609 07:30:52.040205 140539306239744 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.307996, loss=4.331775
I0609 07:34:23.435455 140539314632448 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.318575, loss=4.397049
I0609 07:37:04.095627 140597298014016 spec.py:298] Evaluating on the training split.
I0609 07:37:07.961436 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:39:19.087292 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 07:39:22.800162 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:41:33.257836 140597298014016 spec.py:326] Evaluating on the test split.
I0609 07:41:37.055686 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:43:41.874958 140597298014016 submission_runner.py:419] Time since start: 13882.27s, 	Step: 19881, 	{'train/accuracy': 0.619751067920922, 'train/loss': 2.354302555526131, 'train/bleu': 30.465042245261987, 'validation/accuracy': 0.6223605410968246, 'validation/loss': 2.2990525923423144, 'validation/bleu': 25.7026757135051, 'validation/num_examples': 3000, 'test/accuracy': 0.6268084364650515, 'test/loss': 2.2681709735053164, 'test/bleu': 24.517497598280414, 'test/num_examples': 3003, 'score': 8399.618220567703, 'total_duration': 13882.265967607498, 'accumulated_submission_time': 8399.618220567703, 'accumulated_eval_time': 5475.653971910477, 'accumulated_logging_time': 0.2131192684173584}
I0609 07:43:41.886014 140539306239744 logging_writer.py:48] [19881] accumulated_eval_time=5475.653972, accumulated_logging_time=0.213119, accumulated_submission_time=8399.618221, global_step=19881, preemption_count=0, score=8399.618221, test/accuracy=0.626808, test/bleu=24.517498, test/loss=2.268171, test/num_examples=3003, total_duration=13882.265968, train/accuracy=0.619751, train/bleu=30.465042, train/loss=2.354303, validation/accuracy=0.622361, validation/bleu=25.702676, validation/loss=2.299053, validation/num_examples=3000
I0609 07:44:32.067739 140597298014016 spec.py:298] Evaluating on the training split.
I0609 07:44:35.949340 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:46:52.580534 140597298014016 spec.py:310] Evaluating on the validation split.
I0609 07:46:56.306200 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:49:11.128638 140597298014016 spec.py:326] Evaluating on the test split.
I0609 07:49:14.913419 140597298014016 workload.py:130] Translating evaluation dataset.
I0609 07:51:17.461332 140597298014016 submission_runner.py:419] Time since start: 14337.85s, 	Step: 20000, 	{'train/accuracy': 0.6185311015045656, 'train/loss': 2.354307637391785, 'train/bleu': 30.33387963325935, 'validation/accuracy': 0.623290473769699, 'validation/loss': 2.281792654461817, 'validation/bleu': 26.042052676915787, 'validation/num_examples': 3000, 'test/accuracy': 0.6283655801522282, 'test/loss': 2.2585341496717217, 'test/bleu': 24.80435382135081, 'test/num_examples': 3003, 'score': 8449.75104689598, 'total_duration': 14337.85234951973, 'accumulated_submission_time': 8449.75104689598, 'accumulated_eval_time': 5881.04748249054, 'accumulated_logging_time': 0.23356890678405762}
I0609 07:51:17.472622 140539314632448 logging_writer.py:48] [20000] accumulated_eval_time=5881.047482, accumulated_logging_time=0.233569, accumulated_submission_time=8449.751047, global_step=20000, preemption_count=0, score=8449.751047, test/accuracy=0.628366, test/bleu=24.804354, test/loss=2.258534, test/num_examples=3003, total_duration=14337.852350, train/accuracy=0.618531, train/bleu=30.333880, train/loss=2.354308, validation/accuracy=0.623290, validation/bleu=26.042053, validation/loss=2.281793, validation/num_examples=3000
I0609 07:51:17.491546 140539306239744 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=8449.751047
I0609 07:51:19.051530 140597298014016 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/momentum/wmt_pytorch/trial_1/checkpoint_20000.
I0609 07:51:19.075047 140597298014016 submission_runner.py:581] Tuning trial 1/1
I0609 07:51:19.075243 140597298014016 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 07:51:19.076296 140597298014016 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005054101863125732, 'train/loss': 11.057106325667945, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.077843114158535, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.06855063622102, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.384934902191162, 'total_duration': 828.9801795482635, 'accumulated_submission_time': 4.384934902191162, 'accumulated_eval_time': 824.5947737693787, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1987, {'train/accuracy': 0.28377701934015925, 'train/loss': 5.853253697383391, 'train/bleu': 5.475088416993237, 'validation/accuracy': 0.26292296437737905, 'validation/loss': 6.113344068889412, 'validation/bleu': 2.977993949912662, 'validation/num_examples': 3000, 'test/accuracy': 0.24125268723490792, 'test/loss': 6.408942682005694, 'test/bleu': 1.9630391815225328, 'test/num_examples': 3003, 'score': 844.0200328826904, 'total_duration': 2475.9726145267487, 'accumulated_submission_time': 844.0200328826904, 'accumulated_eval_time': 1631.223284959793, 'accumulated_logging_time': 0.02764153480529785, 'global_step': 1987, 'preemption_count': 0}), (3975, {'train/accuracy': 0.4132028506154738, 'train/loss': 4.337728458417158, 'train/bleu': 14.19307087851006, 'validation/accuracy': 0.4022144796716718, 'validation/loss': 4.422075516732589, 'validation/bleu': 9.588013079384943, 'validation/num_examples': 3000, 'test/accuracy': 0.38685724246121667, 'test/loss': 4.6264060775085705, 'test/bleu': 7.8895629184947875, 'test/num_examples': 3003, 'score': 1683.4681777954102, 'total_duration': 3894.937638759613, 'accumulated_submission_time': 1683.4681777954102, 'accumulated_eval_time': 2210.0220777988434, 'accumulated_logging_time': 0.051088809967041016, 'global_step': 3975, 'preemption_count': 0}), (5963, {'train/accuracy': 0.5042889441812377, 'train/loss': 3.4751372604156017, 'train/bleu': 21.528439040005907, 'validation/accuracy': 0.5080036205378731, 'validation/loss': 3.427724470248354, 'validation/bleu': 17.453508311593463, 'validation/num_examples': 3000, 'test/accuracy': 0.5010167915867759, 'test/loss': 3.5172102870257396, 'test/bleu': 15.81214656059185, 'test/num_examples': 3003, 'score': 2522.978393793106, 'total_duration': 5162.581321239471, 'accumulated_submission_time': 2522.978393793106, 'accumulated_eval_time': 2637.4500370025635, 'accumulated_logging_time': 0.07077336311340332, 'global_step': 5963, 'preemption_count': 0}), (7950, {'train/accuracy': 0.5554622815087397, 'train/loss': 2.9908113284843605, 'train/bleu': 25.410440450104414, 'validation/accuracy': 0.5519212409021587, 'validation/loss': 2.979251070972462, 'validation/bleu': 20.852007604762793, 'validation/num_examples': 3000, 'test/accuracy': 0.5506594619719947, 'test/loss': 3.0220847132647726, 'test/bleu': 19.640039835208068, 'test/num_examples': 3003, 'score': 3362.3598408699036, 'total_duration': 6415.959545373917, 'accumulated_submission_time': 3362.3598408699036, 'accumulated_eval_time': 3050.7440316677094, 'accumulated_logging_time': 0.09055447578430176, 'global_step': 7950, 'preemption_count': 0}), (9937, {'train/accuracy': 0.5693350430481773, 'train/loss': 2.827032158763052, 'train/bleu': 26.53665208366083, 'validation/accuracy': 0.5738180555727765, 'validation/loss': 2.7521531816096516, 'validation/bleu': 22.533927230700936, 'validation/num_examples': 3000, 'test/accuracy': 0.576642844692348, 'test/loss': 2.77235743129394, 'test/bleu': 21.317588483227176, 'test/num_examples': 3003, 'score': 4201.8130440711975, 'total_duration': 7673.622352600098, 'accumulated_submission_time': 4201.8130440711975, 'accumulated_eval_time': 3468.2493073940277, 'accumulated_logging_time': 0.1112356185913086, 'global_step': 9937, 'preemption_count': 0}), (11925, {'train/accuracy': 0.5829252029155457, 'train/loss': 2.673851163171691, 'train/bleu': 27.409821336063192, 'validation/accuracy': 0.5913379871297318, 'validation/loss': 2.57349587264882, 'validation/bleu': 23.654784478773465, 'validation/num_examples': 3000, 'test/accuracy': 0.5952239846609726, 'test/loss': 2.5781912730230667, 'test/bleu': 22.373584719748354, 'test/num_examples': 3003, 'score': 5041.362347364426, 'total_duration': 8912.438205242157, 'accumulated_submission_time': 5041.362347364426, 'accumulated_eval_time': 3866.8277263641357, 'accumulated_logging_time': 0.13179993629455566, 'global_step': 11925, 'preemption_count': 0}), (13914, {'train/accuracy': 0.5973670712942384, 'train/loss': 2.5496058799224937, 'train/bleu': 28.261151359988734, 'validation/accuracy': 0.6039354750716048, 'validation/loss': 2.4738454498394318, 'validation/bleu': 24.554349113263367, 'validation/num_examples': 3000, 'test/accuracy': 0.605508105281506, 'test/loss': 2.4695338373714484, 'test/bleu': 23.121756294400917, 'test/num_examples': 3003, 'score': 5880.8427810668945, 'total_duration': 10151.708930253983, 'accumulated_submission_time': 5880.8427810668945, 'accumulated_eval_time': 4265.927235126495, 'accumulated_logging_time': 0.1519918441772461, 'global_step': 13914, 'preemption_count': 0}), (15903, {'train/accuracy': 0.6038362819690164, 'train/loss': 2.502930135094876, 'train/bleu': 29.297429574853844, 'validation/accuracy': 0.6105441966001661, 'validation/loss': 2.4036473509317924, 'validation/bleu': 25.20660522357201, 'validation/num_examples': 3000, 'test/accuracy': 0.6156992620998198, 'test/loss': 2.388951833129975, 'test/bleu': 23.88116230941021, 'test/num_examples': 3003, 'score': 6720.4412932395935, 'total_duration': 11397.877881765366, 'accumulated_submission_time': 6720.4412932395935, 'accumulated_eval_time': 4671.80801820755, 'accumulated_logging_time': 0.17331242561340332, 'global_step': 15903, 'preemption_count': 0}), (17893, {'train/accuracy': 0.6063646591509195, 'train/loss': 2.4605317443689296, 'train/bleu': 29.288719117737084, 'validation/accuracy': 0.616173389046633, 'validation/loss': 2.344571634263679, 'validation/bleu': 25.21503432452438, 'validation/num_examples': 3000, 'test/accuracy': 0.6206495845680088, 'test/loss': 2.3243563346115854, 'test/bleu': 24.273966061154656, 'test/num_examples': 3003, 'score': 7560.00834608078, 'total_duration': 12644.195238113403, 'accumulated_submission_time': 7560.00834608078, 'accumulated_eval_time': 5077.874714612961, 'accumulated_logging_time': 0.19314289093017578, 'global_step': 17893, 'preemption_count': 0}), (19881, {'train/accuracy': 0.619751067920922, 'train/loss': 2.354302555526131, 'train/bleu': 30.465042245261987, 'validation/accuracy': 0.6223605410968246, 'validation/loss': 2.2990525923423144, 'validation/bleu': 25.7026757135051, 'validation/num_examples': 3000, 'test/accuracy': 0.6268084364650515, 'test/loss': 2.2681709735053164, 'test/bleu': 24.517497598280414, 'test/num_examples': 3003, 'score': 8399.618220567703, 'total_duration': 13882.265967607498, 'accumulated_submission_time': 8399.618220567703, 'accumulated_eval_time': 5475.653971910477, 'accumulated_logging_time': 0.2131192684173584, 'global_step': 19881, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6185311015045656, 'train/loss': 2.354307637391785, 'train/bleu': 30.33387963325935, 'validation/accuracy': 0.623290473769699, 'validation/loss': 2.281792654461817, 'validation/bleu': 26.042052676915787, 'validation/num_examples': 3000, 'test/accuracy': 0.6283655801522282, 'test/loss': 2.2585341496717217, 'test/bleu': 24.80435382135081, 'test/num_examples': 3003, 'score': 8449.75104689598, 'total_duration': 14337.85234951973, 'accumulated_submission_time': 8449.75104689598, 'accumulated_eval_time': 5881.04748249054, 'accumulated_logging_time': 0.23356890678405762, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0609 07:51:19.076451 140597298014016 submission_runner.py:584] Timing: 8449.75104689598
I0609 07:51:19.076512 140597298014016 submission_runner.py:586] Total number of evals: 12
I0609 07:51:19.076572 140597298014016 submission_runner.py:587] ====================
I0609 07:51:19.076684 140597298014016 submission_runner.py:655] Final wmt score: 8449.75104689598
