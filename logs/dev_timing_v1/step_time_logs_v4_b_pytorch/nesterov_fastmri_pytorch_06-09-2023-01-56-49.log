torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=5428 2>&1 | tee -a /logs/fastmri_pytorch_06-09-2023-01-56-49.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 01:57:12.632438 139716664227648 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 01:57:12.632475 140555416033088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 01:57:12.632490 140326600681280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 01:57:12.633312 139694376003392 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 01:57:12.633684 140185963456320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 01:57:12.633713 140347316012864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 01:57:12.633949 140366527158080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 01:57:12.633970 139672502753088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 01:57:12.634144 140347316012864 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.634181 140185963456320 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.634324 140366527158080 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.634343 139672502753088 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.643139 139716664227648 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.643186 140326600681280 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.643178 140555416033088 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:12.643950 139694376003392 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:57:13.193249 139672502753088 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nesterov/fastmri_pytorch.
W0609 01:57:13.324008 139672502753088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.324039 139694376003392 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.324373 139716664227648 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.324485 140185963456320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.324622 140326600681280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.325272 140366527158080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.326223 140555416033088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:57:13.329117 140347316012864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 01:57:13.330694 139672502753088 submission_runner.py:541] Using RNG seed 585488389
I0609 01:57:13.332039 139672502753088 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 01:57:13.332155 139672502753088 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nesterov/fastmri_pytorch/trial_1.
I0609 01:57:13.332508 139672502753088 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nesterov/fastmri_pytorch/trial_1/hparams.json.
I0609 01:57:13.333465 139672502753088 submission_runner.py:255] Initializing dataset.
I0609 01:57:13.333585 139672502753088 submission_runner.py:262] Initializing model.
I0609 01:57:17.512541 139672502753088 submission_runner.py:272] Initializing optimizer.
I0609 01:57:17.975201 139672502753088 submission_runner.py:279] Initializing metrics bundle.
I0609 01:57:17.975437 139672502753088 submission_runner.py:297] Initializing checkpoint and logger.
I0609 01:57:17.978435 139672502753088 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 01:57:17.978572 139672502753088 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 01:57:18.470449 139672502753088 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nesterov/fastmri_pytorch/trial_1/meta_data_0.json.
I0609 01:57:18.471395 139672502753088 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nesterov/fastmri_pytorch/trial_1/flags_0.json.
I0609 01:57:18.524802 139672502753088 submission_runner.py:332] Starting training loop.
I0609 01:58:04.764717 139630141757184 logging_writer.py:48] [0] global_step=0, grad_norm=5.454565, loss=0.881558
I0609 01:58:04.776236 139672502753088 submission.py:139] 0) loss = 0.882, grad_norm = 5.455
I0609 01:58:04.777392 139672502753088 spec.py:298] Evaluating on the training split.
I0609 01:59:38.753016 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:00:40.414374 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:01:42.575233 139672502753088 submission_runner.py:419] Time since start: 264.05s, 	Step: 1, 	{'train/ssim': 0.23230159282684326, 'train/loss': 0.8644339697701591, 'validation/ssim': 0.23160025318083496, 'validation/loss': 0.8679018551939716, 'validation/num_examples': 3554, 'test/ssim': 0.252667786854667, 'test/loss': 0.8683983175090757, 'test/num_examples': 3581, 'score': 46.25284767150879, 'total_duration': 264.05111956596375, 'accumulated_submission_time': 46.25284767150879, 'accumulated_eval_time': 217.79783082008362, 'accumulated_logging_time': 0}
I0609 02:01:42.593168 139606141945600 logging_writer.py:48] [1] accumulated_eval_time=217.797831, accumulated_logging_time=0, accumulated_submission_time=46.252848, global_step=1, preemption_count=0, score=46.252848, test/loss=0.868398, test/num_examples=3581, test/ssim=0.252668, total_duration=264.051120, train/loss=0.864434, train/ssim=0.232302, validation/loss=0.867902, validation/num_examples=3554, validation/ssim=0.231600
I0609 02:01:42.618434 139672502753088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618603 140326600681280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618600 140555416033088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618603 139716664227648 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618605 140185963456320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618614 140366527158080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618731 140347316012864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.618747 139694376003392 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:01:42.680528 139606133552896 logging_writer.py:48] [1] global_step=1, grad_norm=5.169803, loss=0.866452
I0609 02:01:42.687080 139672502753088 submission.py:139] 1) loss = 0.866, grad_norm = 5.170
I0609 02:01:42.765150 139606141945600 logging_writer.py:48] [2] global_step=2, grad_norm=5.231922, loss=0.835753
I0609 02:01:42.769812 139672502753088 submission.py:139] 2) loss = 0.836, grad_norm = 5.232
I0609 02:01:42.850182 139606133552896 logging_writer.py:48] [3] global_step=3, grad_norm=4.464948, loss=0.779946
I0609 02:01:42.853678 139672502753088 submission.py:139] 3) loss = 0.780, grad_norm = 4.465
I0609 02:01:42.920997 139606141945600 logging_writer.py:48] [4] global_step=4, grad_norm=4.033279, loss=0.748555
I0609 02:01:42.924518 139672502753088 submission.py:139] 4) loss = 0.749, grad_norm = 4.033
I0609 02:01:43.000431 139606133552896 logging_writer.py:48] [5] global_step=5, grad_norm=2.436872, loss=0.693546
I0609 02:01:43.004898 139672502753088 submission.py:139] 5) loss = 0.694, grad_norm = 2.437
I0609 02:01:43.081395 139606141945600 logging_writer.py:48] [6] global_step=6, grad_norm=1.505359, loss=0.603284
I0609 02:01:43.087455 139672502753088 submission.py:139] 6) loss = 0.603, grad_norm = 1.505
I0609 02:01:43.174674 139606133552896 logging_writer.py:48] [7] global_step=7, grad_norm=1.151068, loss=0.545084
I0609 02:01:43.178450 139672502753088 submission.py:139] 7) loss = 0.545, grad_norm = 1.151
I0609 02:01:43.250427 139606141945600 logging_writer.py:48] [8] global_step=8, grad_norm=1.346988, loss=0.628006
I0609 02:01:43.255781 139672502753088 submission.py:139] 8) loss = 0.628, grad_norm = 1.347
I0609 02:01:43.337446 139606133552896 logging_writer.py:48] [9] global_step=9, grad_norm=1.647393, loss=0.575286
I0609 02:01:43.343734 139672502753088 submission.py:139] 9) loss = 0.575, grad_norm = 1.647
I0609 02:01:43.420169 139606141945600 logging_writer.py:48] [10] global_step=10, grad_norm=1.659180, loss=0.574906
I0609 02:01:43.423962 139672502753088 submission.py:139] 10) loss = 0.575, grad_norm = 1.659
I0609 02:01:43.494030 139606133552896 logging_writer.py:48] [11] global_step=11, grad_norm=1.711969, loss=0.572032
I0609 02:01:43.499176 139672502753088 submission.py:139] 11) loss = 0.572, grad_norm = 1.712
I0609 02:01:43.574108 139606141945600 logging_writer.py:48] [12] global_step=12, grad_norm=1.497393, loss=0.615514
I0609 02:01:43.579384 139672502753088 submission.py:139] 12) loss = 0.616, grad_norm = 1.497
I0609 02:01:43.655232 139606133552896 logging_writer.py:48] [13] global_step=13, grad_norm=1.290467, loss=0.481873
I0609 02:01:43.658922 139672502753088 submission.py:139] 13) loss = 0.482, grad_norm = 1.290
I0609 02:01:43.723326 139606141945600 logging_writer.py:48] [14] global_step=14, grad_norm=0.887278, loss=0.468892
I0609 02:01:43.726471 139672502753088 submission.py:139] 14) loss = 0.469, grad_norm = 0.887
I0609 02:01:43.977724 139606133552896 logging_writer.py:48] [15] global_step=15, grad_norm=0.722304, loss=0.497549
I0609 02:01:43.984606 139672502753088 submission.py:139] 15) loss = 0.498, grad_norm = 0.722
I0609 02:01:44.229796 139606141945600 logging_writer.py:48] [16] global_step=16, grad_norm=0.761001, loss=0.450627
I0609 02:01:44.233186 139672502753088 submission.py:139] 16) loss = 0.451, grad_norm = 0.761
I0609 02:01:44.497238 139606133552896 logging_writer.py:48] [17] global_step=17, grad_norm=0.948680, loss=0.469664
I0609 02:01:44.503099 139672502753088 submission.py:139] 17) loss = 0.470, grad_norm = 0.949
I0609 02:01:44.713205 139606141945600 logging_writer.py:48] [18] global_step=18, grad_norm=0.859839, loss=0.434922
I0609 02:01:44.719299 139672502753088 submission.py:139] 18) loss = 0.435, grad_norm = 0.860
I0609 02:01:44.996393 139606133552896 logging_writer.py:48] [19] global_step=19, grad_norm=0.954690, loss=0.404674
I0609 02:01:45.001826 139672502753088 submission.py:139] 19) loss = 0.405, grad_norm = 0.955
I0609 02:01:45.260540 139606141945600 logging_writer.py:48] [20] global_step=20, grad_norm=0.906945, loss=0.400892
I0609 02:01:45.267389 139672502753088 submission.py:139] 20) loss = 0.401, grad_norm = 0.907
I0609 02:01:45.548832 139606133552896 logging_writer.py:48] [21] global_step=21, grad_norm=0.659768, loss=0.346096
I0609 02:01:45.555222 139672502753088 submission.py:139] 21) loss = 0.346, grad_norm = 0.660
I0609 02:01:45.800738 139606141945600 logging_writer.py:48] [22] global_step=22, grad_norm=0.445059, loss=0.336825
I0609 02:01:45.805881 139672502753088 submission.py:139] 22) loss = 0.337, grad_norm = 0.445
I0609 02:01:46.049087 139606133552896 logging_writer.py:48] [23] global_step=23, grad_norm=0.357774, loss=0.387570
I0609 02:01:46.054153 139672502753088 submission.py:139] 23) loss = 0.388, grad_norm = 0.358
I0609 02:01:46.300534 139606141945600 logging_writer.py:48] [24] global_step=24, grad_norm=0.326869, loss=0.380679
I0609 02:01:46.306446 139672502753088 submission.py:139] 24) loss = 0.381, grad_norm = 0.327
I0609 02:01:46.577428 139606133552896 logging_writer.py:48] [25] global_step=25, grad_norm=0.346258, loss=0.330585
I0609 02:01:46.582798 139672502753088 submission.py:139] 25) loss = 0.331, grad_norm = 0.346
I0609 02:01:46.870329 139606141945600 logging_writer.py:48] [26] global_step=26, grad_norm=0.402491, loss=0.404545
I0609 02:01:46.879087 139672502753088 submission.py:139] 26) loss = 0.405, grad_norm = 0.402
I0609 02:01:47.153059 139606133552896 logging_writer.py:48] [27] global_step=27, grad_norm=0.319356, loss=0.373355
I0609 02:01:47.158384 139672502753088 submission.py:139] 27) loss = 0.373, grad_norm = 0.319
I0609 02:01:47.383336 139606141945600 logging_writer.py:48] [28] global_step=28, grad_norm=0.291302, loss=0.333987
I0609 02:01:47.387352 139672502753088 submission.py:139] 28) loss = 0.334, grad_norm = 0.291
I0609 02:01:47.657834 139606133552896 logging_writer.py:48] [29] global_step=29, grad_norm=0.309914, loss=0.433376
I0609 02:01:47.661780 139672502753088 submission.py:139] 29) loss = 0.433, grad_norm = 0.310
I0609 02:01:47.910904 139606141945600 logging_writer.py:48] [30] global_step=30, grad_norm=0.256671, loss=0.422952
I0609 02:01:47.914912 139672502753088 submission.py:139] 30) loss = 0.423, grad_norm = 0.257
I0609 02:01:48.172294 139606133552896 logging_writer.py:48] [31] global_step=31, grad_norm=0.196676, loss=0.405154
I0609 02:01:48.175797 139672502753088 submission.py:139] 31) loss = 0.405, grad_norm = 0.197
I0609 02:01:48.448545 139606141945600 logging_writer.py:48] [32] global_step=32, grad_norm=0.182279, loss=0.330484
I0609 02:01:48.452199 139672502753088 submission.py:139] 32) loss = 0.330, grad_norm = 0.182
I0609 02:01:48.690782 139606133552896 logging_writer.py:48] [33] global_step=33, grad_norm=0.217648, loss=0.330777
I0609 02:01:48.694062 139672502753088 submission.py:139] 33) loss = 0.331, grad_norm = 0.218
I0609 02:01:48.946384 139606141945600 logging_writer.py:48] [34] global_step=34, grad_norm=0.118862, loss=0.352035
I0609 02:01:48.950506 139672502753088 submission.py:139] 34) loss = 0.352, grad_norm = 0.119
I0609 02:01:49.241398 139606133552896 logging_writer.py:48] [35] global_step=35, grad_norm=0.191832, loss=0.308326
I0609 02:01:49.246388 139672502753088 submission.py:139] 35) loss = 0.308, grad_norm = 0.192
I0609 02:01:49.491189 139606141945600 logging_writer.py:48] [36] global_step=36, grad_norm=0.135415, loss=0.325868
I0609 02:01:49.494788 139672502753088 submission.py:139] 36) loss = 0.326, grad_norm = 0.135
I0609 02:01:49.719192 139606133552896 logging_writer.py:48] [37] global_step=37, grad_norm=0.110449, loss=0.340027
I0609 02:01:49.725110 139672502753088 submission.py:139] 37) loss = 0.340, grad_norm = 0.110
I0609 02:01:50.007035 139606141945600 logging_writer.py:48] [38] global_step=38, grad_norm=0.102991, loss=0.331743
I0609 02:01:50.014754 139672502753088 submission.py:139] 38) loss = 0.332, grad_norm = 0.103
I0609 02:01:50.331251 139606133552896 logging_writer.py:48] [39] global_step=39, grad_norm=0.140869, loss=0.330062
I0609 02:01:50.338428 139672502753088 submission.py:139] 39) loss = 0.330, grad_norm = 0.141
I0609 02:01:50.544836 139606141945600 logging_writer.py:48] [40] global_step=40, grad_norm=0.145492, loss=0.358832
I0609 02:01:50.550336 139672502753088 submission.py:139] 40) loss = 0.359, grad_norm = 0.145
I0609 02:01:50.800666 139606133552896 logging_writer.py:48] [41] global_step=41, grad_norm=0.112793, loss=0.351564
I0609 02:01:50.806154 139672502753088 submission.py:139] 41) loss = 0.352, grad_norm = 0.113
I0609 02:01:51.094434 139606141945600 logging_writer.py:48] [42] global_step=42, grad_norm=0.139102, loss=0.348745
I0609 02:01:51.101473 139672502753088 submission.py:139] 42) loss = 0.349, grad_norm = 0.139
I0609 02:01:51.351965 139606133552896 logging_writer.py:48] [43] global_step=43, grad_norm=0.127830, loss=0.400445
I0609 02:01:51.357049 139672502753088 submission.py:139] 43) loss = 0.400, grad_norm = 0.128
I0609 02:01:51.587450 139606141945600 logging_writer.py:48] [44] global_step=44, grad_norm=0.160085, loss=0.359295
I0609 02:01:51.591934 139672502753088 submission.py:139] 44) loss = 0.359, grad_norm = 0.160
I0609 02:01:51.870632 139606133552896 logging_writer.py:48] [45] global_step=45, grad_norm=0.080909, loss=0.437210
I0609 02:01:51.873967 139672502753088 submission.py:139] 45) loss = 0.437, grad_norm = 0.081
I0609 02:01:52.157464 139606141945600 logging_writer.py:48] [46] global_step=46, grad_norm=0.106842, loss=0.413435
I0609 02:01:52.162695 139672502753088 submission.py:139] 46) loss = 0.413, grad_norm = 0.107
I0609 02:01:52.426114 139606133552896 logging_writer.py:48] [47] global_step=47, grad_norm=0.105937, loss=0.371926
I0609 02:01:52.429822 139672502753088 submission.py:139] 47) loss = 0.372, grad_norm = 0.106
I0609 02:01:52.684547 139606141945600 logging_writer.py:48] [48] global_step=48, grad_norm=0.093567, loss=0.350654
I0609 02:01:52.688423 139672502753088 submission.py:139] 48) loss = 0.351, grad_norm = 0.094
I0609 02:01:52.955347 139606133552896 logging_writer.py:48] [49] global_step=49, grad_norm=0.197639, loss=0.333117
I0609 02:01:52.961318 139672502753088 submission.py:139] 49) loss = 0.333, grad_norm = 0.198
I0609 02:01:53.240599 139606141945600 logging_writer.py:48] [50] global_step=50, grad_norm=0.127187, loss=0.365529
I0609 02:01:53.243913 139672502753088 submission.py:139] 50) loss = 0.366, grad_norm = 0.127
I0609 02:01:53.489284 139606133552896 logging_writer.py:48] [51] global_step=51, grad_norm=0.083939, loss=0.353995
I0609 02:01:53.494987 139672502753088 submission.py:139] 51) loss = 0.354, grad_norm = 0.084
I0609 02:01:53.745197 139606141945600 logging_writer.py:48] [52] global_step=52, grad_norm=0.088325, loss=0.400859
I0609 02:01:53.750310 139672502753088 submission.py:139] 52) loss = 0.401, grad_norm = 0.088
I0609 02:01:54.056530 139606133552896 logging_writer.py:48] [53] global_step=53, grad_norm=0.089357, loss=0.340260
I0609 02:01:54.062623 139672502753088 submission.py:139] 53) loss = 0.340, grad_norm = 0.089
I0609 02:01:54.282678 139606141945600 logging_writer.py:48] [54] global_step=54, grad_norm=0.228248, loss=0.485127
I0609 02:01:54.289560 139672502753088 submission.py:139] 54) loss = 0.485, grad_norm = 0.228
I0609 02:01:54.570667 139606133552896 logging_writer.py:48] [55] global_step=55, grad_norm=0.264682, loss=0.326806
I0609 02:01:54.576776 139672502753088 submission.py:139] 55) loss = 0.327, grad_norm = 0.265
I0609 02:01:54.801484 139606141945600 logging_writer.py:48] [56] global_step=56, grad_norm=0.058777, loss=0.311134
I0609 02:01:54.808184 139672502753088 submission.py:139] 56) loss = 0.311, grad_norm = 0.059
I0609 02:01:55.096074 139606133552896 logging_writer.py:48] [57] global_step=57, grad_norm=0.107766, loss=0.343997
I0609 02:01:55.102163 139672502753088 submission.py:139] 57) loss = 0.344, grad_norm = 0.108
I0609 02:01:55.339647 139606141945600 logging_writer.py:48] [58] global_step=58, grad_norm=0.113821, loss=0.469622
I0609 02:01:55.345239 139672502753088 submission.py:139] 58) loss = 0.470, grad_norm = 0.114
I0609 02:01:55.630152 139606133552896 logging_writer.py:48] [59] global_step=59, grad_norm=0.087079, loss=0.395946
I0609 02:01:55.636992 139672502753088 submission.py:139] 59) loss = 0.396, grad_norm = 0.087
I0609 02:01:55.885820 139606141945600 logging_writer.py:48] [60] global_step=60, grad_norm=0.145480, loss=0.360976
I0609 02:01:55.891266 139672502753088 submission.py:139] 60) loss = 0.361, grad_norm = 0.145
I0609 02:01:56.163582 139606133552896 logging_writer.py:48] [61] global_step=61, grad_norm=0.180990, loss=0.317567
I0609 02:01:56.168403 139672502753088 submission.py:139] 61) loss = 0.318, grad_norm = 0.181
I0609 02:01:56.446540 139606141945600 logging_writer.py:48] [62] global_step=62, grad_norm=0.101275, loss=0.448691
I0609 02:01:56.450039 139672502753088 submission.py:139] 62) loss = 0.449, grad_norm = 0.101
I0609 02:01:56.680614 139606133552896 logging_writer.py:48] [63] global_step=63, grad_norm=0.085028, loss=0.293859
I0609 02:01:56.684052 139672502753088 submission.py:139] 63) loss = 0.294, grad_norm = 0.085
I0609 02:01:56.976976 139606141945600 logging_writer.py:48] [64] global_step=64, grad_norm=0.082566, loss=0.351407
I0609 02:01:56.980362 139672502753088 submission.py:139] 64) loss = 0.351, grad_norm = 0.083
I0609 02:01:57.253779 139606133552896 logging_writer.py:48] [65] global_step=65, grad_norm=0.066164, loss=0.361377
I0609 02:01:57.257246 139672502753088 submission.py:139] 65) loss = 0.361, grad_norm = 0.066
I0609 02:01:57.502071 139606141945600 logging_writer.py:48] [66] global_step=66, grad_norm=0.105585, loss=0.319445
I0609 02:01:57.507753 139672502753088 submission.py:139] 66) loss = 0.319, grad_norm = 0.106
I0609 02:01:57.809062 139606133552896 logging_writer.py:48] [67] global_step=67, grad_norm=0.068778, loss=0.274160
I0609 02:01:57.814272 139672502753088 submission.py:139] 67) loss = 0.274, grad_norm = 0.069
I0609 02:01:58.024455 139606141945600 logging_writer.py:48] [68] global_step=68, grad_norm=0.087096, loss=0.271233
I0609 02:01:58.030207 139672502753088 submission.py:139] 68) loss = 0.271, grad_norm = 0.087
I0609 02:01:58.330819 139606133552896 logging_writer.py:48] [69] global_step=69, grad_norm=0.168039, loss=0.373742
I0609 02:01:58.334927 139672502753088 submission.py:139] 69) loss = 0.374, grad_norm = 0.168
I0609 02:01:58.574547 139606141945600 logging_writer.py:48] [70] global_step=70, grad_norm=0.065009, loss=0.352738
I0609 02:01:58.578553 139672502753088 submission.py:139] 70) loss = 0.353, grad_norm = 0.065
I0609 02:01:58.905494 139606133552896 logging_writer.py:48] [71] global_step=71, grad_norm=0.126010, loss=0.339445
I0609 02:01:58.909409 139672502753088 submission.py:139] 71) loss = 0.339, grad_norm = 0.126
I0609 02:01:59.167784 139606141945600 logging_writer.py:48] [72] global_step=72, grad_norm=0.119619, loss=0.341223
I0609 02:01:59.171699 139672502753088 submission.py:139] 72) loss = 0.341, grad_norm = 0.120
I0609 02:01:59.401654 139606133552896 logging_writer.py:48] [73] global_step=73, grad_norm=0.184415, loss=0.315504
I0609 02:01:59.407294 139672502753088 submission.py:139] 73) loss = 0.316, grad_norm = 0.184
I0609 02:01:59.678767 139606141945600 logging_writer.py:48] [74] global_step=74, grad_norm=0.075156, loss=0.278427
I0609 02:01:59.684264 139672502753088 submission.py:139] 74) loss = 0.278, grad_norm = 0.075
I0609 02:01:59.913120 139606133552896 logging_writer.py:48] [75] global_step=75, grad_norm=0.154185, loss=0.258395
I0609 02:01:59.919212 139672502753088 submission.py:139] 75) loss = 0.258, grad_norm = 0.154
I0609 02:02:00.219579 139606141945600 logging_writer.py:48] [76] global_step=76, grad_norm=0.189121, loss=0.325856
I0609 02:02:00.224173 139672502753088 submission.py:139] 76) loss = 0.326, grad_norm = 0.189
I0609 02:02:00.498405 139606133552896 logging_writer.py:48] [77] global_step=77, grad_norm=0.120140, loss=0.308527
I0609 02:02:00.504889 139672502753088 submission.py:139] 77) loss = 0.309, grad_norm = 0.120
I0609 02:02:00.727371 139606141945600 logging_writer.py:48] [78] global_step=78, grad_norm=0.160380, loss=0.269980
I0609 02:02:00.732779 139672502753088 submission.py:139] 78) loss = 0.270, grad_norm = 0.160
I0609 02:02:00.989807 139606133552896 logging_writer.py:48] [79] global_step=79, grad_norm=0.154505, loss=0.291440
I0609 02:02:00.995048 139672502753088 submission.py:139] 79) loss = 0.291, grad_norm = 0.155
I0609 02:02:01.272633 139606141945600 logging_writer.py:48] [80] global_step=80, grad_norm=0.149075, loss=0.255048
I0609 02:02:01.275801 139672502753088 submission.py:139] 80) loss = 0.255, grad_norm = 0.149
I0609 02:02:01.520989 139606133552896 logging_writer.py:48] [81] global_step=81, grad_norm=0.072680, loss=0.247555
I0609 02:02:01.524890 139672502753088 submission.py:139] 81) loss = 0.248, grad_norm = 0.073
I0609 02:02:01.913728 139606141945600 logging_writer.py:48] [82] global_step=82, grad_norm=0.214518, loss=0.334993
I0609 02:02:01.917756 139672502753088 submission.py:139] 82) loss = 0.335, grad_norm = 0.215
I0609 02:02:02.239806 139606133552896 logging_writer.py:48] [83] global_step=83, grad_norm=0.064299, loss=0.277383
I0609 02:02:02.243399 139672502753088 submission.py:139] 83) loss = 0.277, grad_norm = 0.064
I0609 02:02:02.476859 139606141945600 logging_writer.py:48] [84] global_step=84, grad_norm=0.096987, loss=0.408073
I0609 02:02:02.482778 139672502753088 submission.py:139] 84) loss = 0.408, grad_norm = 0.097
I0609 02:02:02.728408 139606133552896 logging_writer.py:48] [85] global_step=85, grad_norm=0.043115, loss=0.282708
I0609 02:02:02.734997 139672502753088 submission.py:139] 85) loss = 0.283, grad_norm = 0.043
I0609 02:02:03.020592 139606141945600 logging_writer.py:48] [86] global_step=86, grad_norm=0.047233, loss=0.285438
I0609 02:02:03.026107 139672502753088 submission.py:139] 86) loss = 0.285, grad_norm = 0.047
I0609 02:02:03.266784 139606133552896 logging_writer.py:48] [87] global_step=87, grad_norm=0.213134, loss=0.242843
I0609 02:02:03.271028 139672502753088 submission.py:139] 87) loss = 0.243, grad_norm = 0.213
I0609 02:02:03.553929 139606141945600 logging_writer.py:48] [88] global_step=88, grad_norm=0.312315, loss=0.292571
I0609 02:02:03.557736 139672502753088 submission.py:139] 88) loss = 0.293, grad_norm = 0.312
I0609 02:02:03.881248 139606133552896 logging_writer.py:48] [89] global_step=89, grad_norm=0.233602, loss=0.251655
I0609 02:02:03.884729 139672502753088 submission.py:139] 89) loss = 0.252, grad_norm = 0.234
I0609 02:02:04.121131 139606141945600 logging_writer.py:48] [90] global_step=90, grad_norm=0.093478, loss=0.267330
I0609 02:02:04.124454 139672502753088 submission.py:139] 90) loss = 0.267, grad_norm = 0.093
I0609 02:02:04.436504 139606133552896 logging_writer.py:48] [91] global_step=91, grad_norm=0.211332, loss=0.331604
I0609 02:02:04.439866 139672502753088 submission.py:139] 91) loss = 0.332, grad_norm = 0.211
I0609 02:02:04.693787 139606141945600 logging_writer.py:48] [92] global_step=92, grad_norm=0.205754, loss=0.498269
I0609 02:02:04.699746 139672502753088 submission.py:139] 92) loss = 0.498, grad_norm = 0.206
I0609 02:02:04.904510 139606133552896 logging_writer.py:48] [93] global_step=93, grad_norm=0.310123, loss=0.286338
I0609 02:02:04.910300 139672502753088 submission.py:139] 93) loss = 0.286, grad_norm = 0.310
I0609 02:02:05.151911 139606141945600 logging_writer.py:48] [94] global_step=94, grad_norm=0.051146, loss=0.266114
I0609 02:02:05.155390 139672502753088 submission.py:139] 94) loss = 0.266, grad_norm = 0.051
I0609 02:02:05.452574 139606133552896 logging_writer.py:48] [95] global_step=95, grad_norm=0.155789, loss=0.239964
I0609 02:02:05.456011 139672502753088 submission.py:139] 95) loss = 0.240, grad_norm = 0.156
I0609 02:02:05.672843 139606141945600 logging_writer.py:48] [96] global_step=96, grad_norm=0.094042, loss=0.304517
I0609 02:02:05.676149 139672502753088 submission.py:139] 96) loss = 0.305, grad_norm = 0.094
I0609 02:02:05.996043 139606133552896 logging_writer.py:48] [97] global_step=97, grad_norm=0.188809, loss=0.393168
I0609 02:02:06.001136 139672502753088 submission.py:139] 97) loss = 0.393, grad_norm = 0.189
I0609 02:02:06.257656 139606141945600 logging_writer.py:48] [98] global_step=98, grad_norm=0.072339, loss=0.295524
I0609 02:02:06.261376 139672502753088 submission.py:139] 98) loss = 0.296, grad_norm = 0.072
I0609 02:02:06.553604 139606133552896 logging_writer.py:48] [99] global_step=99, grad_norm=0.104584, loss=0.315182
I0609 02:02:06.560103 139672502753088 submission.py:139] 99) loss = 0.315, grad_norm = 0.105
I0609 02:02:06.810037 139606141945600 logging_writer.py:48] [100] global_step=100, grad_norm=0.140826, loss=0.233963
I0609 02:02:06.814373 139672502753088 submission.py:139] 100) loss = 0.234, grad_norm = 0.141
I0609 02:03:02.673418 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:03:04.996268 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:03:07.389669 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:03:09.714979 139672502753088 submission_runner.py:419] Time since start: 351.19s, 	Step: 311, 	{'train/ssim': 0.6965204647609166, 'train/loss': 0.3033086231776646, 'validation/ssim': 0.6784282592017797, 'validation/loss': 0.32579755809914884, 'validation/num_examples': 3554, 'test/ssim': 0.6959481110156032, 'test/loss': 0.3277748446662943, 'test/num_examples': 3581, 'score': 126.08786129951477, 'total_duration': 351.1908903121948, 'accumulated_submission_time': 126.08786129951477, 'accumulated_eval_time': 224.84088587760925, 'accumulated_logging_time': 0.02626323699951172}
I0609 02:03:09.729456 139606133552896 logging_writer.py:48] [311] accumulated_eval_time=224.840886, accumulated_logging_time=0.026263, accumulated_submission_time=126.087861, global_step=311, preemption_count=0, score=126.087861, test/loss=0.327775, test/num_examples=3581, test/ssim=0.695948, total_duration=351.190890, train/loss=0.303309, train/ssim=0.696520, validation/loss=0.325798, validation/num_examples=3554, validation/ssim=0.678428
I0609 02:04:16.161809 139606141945600 logging_writer.py:48] [500] global_step=500, grad_norm=0.792490, loss=0.327914
I0609 02:04:16.168468 139672502753088 submission.py:139] 500) loss = 0.328, grad_norm = 0.792
I0609 02:04:30.066027 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:04:32.297887 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:04:34.569028 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:04:36.802035 139672502753088 submission_runner.py:419] Time since start: 438.28s, 	Step: 538, 	{'train/ssim': 0.7086457524980817, 'train/loss': 0.29624911717006136, 'validation/ssim': 0.6904097660646454, 'validation/loss': 0.31810482714239235, 'validation/num_examples': 3554, 'test/ssim': 0.7076781101647585, 'test/loss': 0.3200758588077702, 'test/num_examples': 3581, 'score': 206.2127537727356, 'total_duration': 438.277939081192, 'accumulated_submission_time': 206.2127537727356, 'accumulated_eval_time': 231.576895236969, 'accumulated_logging_time': 0.05191516876220703}
I0609 02:04:36.813653 139606133552896 logging_writer.py:48] [538] accumulated_eval_time=231.576895, accumulated_logging_time=0.051915, accumulated_submission_time=206.212754, global_step=538, preemption_count=0, score=206.212754, test/loss=0.320076, test/num_examples=3581, test/ssim=0.707678, total_duration=438.277939, train/loss=0.296249, train/ssim=0.708646, validation/loss=0.318105, validation/num_examples=3554, validation/ssim=0.690410
I0609 02:05:57.086885 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:05:59.334578 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:06:01.691971 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:06:04.003621 139672502753088 submission_runner.py:419] Time since start: 525.48s, 	Step: 766, 	{'train/ssim': 0.7089924131120954, 'train/loss': 0.30248955317905973, 'validation/ssim': 0.6897930259039111, 'validation/loss': 0.3235404938207829, 'validation/num_examples': 3554, 'test/ssim': 0.7069152814987084, 'test/loss': 0.3259843891807456, 'test/num_examples': 3581, 'score': 286.2685968875885, 'total_duration': 525.4795181751251, 'accumulated_submission_time': 286.2685968875885, 'accumulated_eval_time': 238.49534440040588, 'accumulated_logging_time': 0.07453680038452148}
I0609 02:06:04.018524 139606141945600 logging_writer.py:48] [766] accumulated_eval_time=238.495344, accumulated_logging_time=0.074537, accumulated_submission_time=286.268597, global_step=766, preemption_count=0, score=286.268597, test/loss=0.325984, test/num_examples=3581, test/ssim=0.706915, total_duration=525.479518, train/loss=0.302490, train/ssim=0.708992, validation/loss=0.323540, validation/num_examples=3554, validation/ssim=0.689793
I0609 02:07:24.268331 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:07:26.578924 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:07:28.864990 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:07:31.063501 139672502753088 submission_runner.py:419] Time since start: 612.54s, 	Step: 999, 	{'train/ssim': 0.6815761838640485, 'train/loss': 0.3159088066646031, 'validation/ssim': 0.6704485565340109, 'validation/loss': 0.33679807166001335, 'validation/num_examples': 3554, 'test/ssim': 0.6852959848855068, 'test/loss': 0.3380982229344108, 'test/num_examples': 3581, 'score': 366.2849199771881, 'total_duration': 612.5394117832184, 'accumulated_submission_time': 366.2849199771881, 'accumulated_eval_time': 245.2913463115692, 'accumulated_logging_time': 0.10164260864257812}
I0609 02:07:31.073844 139606133552896 logging_writer.py:48] [999] accumulated_eval_time=245.291346, accumulated_logging_time=0.101643, accumulated_submission_time=366.284920, global_step=999, preemption_count=0, score=366.284920, test/loss=0.338098, test/num_examples=3581, test/ssim=0.685296, total_duration=612.539412, train/loss=0.315909, train/ssim=0.681576, validation/loss=0.336798, validation/num_examples=3554, validation/ssim=0.670449
I0609 02:07:31.215166 139606141945600 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.484781, loss=0.310868
I0609 02:07:31.218545 139672502753088 submission.py:139] 1000) loss = 0.311, grad_norm = 0.485
I0609 02:08:51.230472 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:08:53.402615 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:08:55.621675 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:08:57.821971 139672502753088 submission_runner.py:419] Time since start: 699.30s, 	Step: 1308, 	{'train/ssim': 0.720123427254813, 'train/loss': 0.2879981313432966, 'validation/ssim': 0.7001497679771033, 'validation/loss': 0.31017716063766176, 'validation/num_examples': 3554, 'test/ssim': 0.7167619003726263, 'test/loss': 0.3125237254782184, 'test/num_examples': 3581, 'score': 446.3002552986145, 'total_duration': 699.2978775501251, 'accumulated_submission_time': 446.3002552986145, 'accumulated_eval_time': 251.8828341960907, 'accumulated_logging_time': 0.12072324752807617}
I0609 02:08:57.832616 139606133552896 logging_writer.py:48] [1308] accumulated_eval_time=251.882834, accumulated_logging_time=0.120723, accumulated_submission_time=446.300255, global_step=1308, preemption_count=0, score=446.300255, test/loss=0.312524, test/num_examples=3581, test/ssim=0.716762, total_duration=699.297878, train/loss=0.287998, train/ssim=0.720123, validation/loss=0.310177, validation/num_examples=3554, validation/ssim=0.700150
I0609 02:09:46.188997 139606141945600 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.610706, loss=0.389344
I0609 02:09:46.193243 139672502753088 submission.py:139] 1500) loss = 0.389, grad_norm = 0.611
I0609 02:10:18.000218 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:10:20.151211 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:10:22.358392 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:10:24.526846 139672502753088 submission_runner.py:419] Time since start: 786.00s, 	Step: 1621, 	{'train/ssim': 0.6625005858285087, 'train/loss': 0.3107748712812151, 'validation/ssim': 0.6572780834535031, 'validation/loss': 0.3309654188612303, 'validation/num_examples': 3554, 'test/ssim': 0.6718838629660011, 'test/loss': 0.3317626680418354, 'test/num_examples': 3581, 'score': 526.3265244960785, 'total_duration': 786.0027675628662, 'accumulated_submission_time': 526.3265244960785, 'accumulated_eval_time': 258.4094932079315, 'accumulated_logging_time': 0.13926029205322266}
I0609 02:10:24.537097 139606133552896 logging_writer.py:48] [1621] accumulated_eval_time=258.409493, accumulated_logging_time=0.139260, accumulated_submission_time=526.326524, global_step=1621, preemption_count=0, score=526.326524, test/loss=0.331763, test/num_examples=3581, test/ssim=0.671884, total_duration=786.002768, train/loss=0.310775, train/ssim=0.662501, validation/loss=0.330965, validation/num_examples=3554, validation/ssim=0.657278
I0609 02:11:44.592659 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:11:46.761892 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:11:48.975783 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:11:51.162724 139672502753088 submission_runner.py:419] Time since start: 872.64s, 	Step: 1933, 	{'train/ssim': 0.6366872106279645, 'train/loss': 0.3387303352355957, 'validation/ssim': 0.631342847627673, 'validation/loss': 0.35779626059545583, 'validation/num_examples': 3554, 'test/ssim': 0.6503120173092363, 'test/loss': 0.3583553157615715, 'test/num_examples': 3581, 'score': 606.2427084445953, 'total_duration': 872.6386241912842, 'accumulated_submission_time': 606.2427084445953, 'accumulated_eval_time': 264.97969222068787, 'accumulated_logging_time': 0.15848541259765625}
I0609 02:11:51.173057 139606141945600 logging_writer.py:48] [1933] accumulated_eval_time=264.979692, accumulated_logging_time=0.158485, accumulated_submission_time=606.242708, global_step=1933, preemption_count=0, score=606.242708, test/loss=0.358355, test/num_examples=3581, test/ssim=0.650312, total_duration=872.638624, train/loss=0.338730, train/ssim=0.636687, validation/loss=0.357796, validation/num_examples=3554, validation/ssim=0.631343
I0609 02:12:07.074320 139606133552896 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.532918, loss=0.285853
I0609 02:12:07.078142 139672502753088 submission.py:139] 2000) loss = 0.286, grad_norm = 0.533
I0609 02:13:11.425207 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:13:13.622483 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:13:15.846115 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:13:18.043014 139672502753088 submission_runner.py:419] Time since start: 959.52s, 	Step: 2241, 	{'train/ssim': 0.6576857566833496, 'train/loss': 0.3173941544124058, 'validation/ssim': 0.6520114055026027, 'validation/loss': 0.3373012252919246, 'validation/num_examples': 3554, 'test/ssim': 0.6686650382443801, 'test/loss': 0.3374894394351264, 'test/num_examples': 3581, 'score': 686.3546783924103, 'total_duration': 959.5189249515533, 'accumulated_submission_time': 686.3546783924103, 'accumulated_eval_time': 271.59761095046997, 'accumulated_logging_time': 0.17799043655395508}
I0609 02:13:18.053751 139606141945600 logging_writer.py:48] [2241] accumulated_eval_time=271.597611, accumulated_logging_time=0.177990, accumulated_submission_time=686.354678, global_step=2241, preemption_count=0, score=686.354678, test/loss=0.337489, test/num_examples=3581, test/ssim=0.668665, total_duration=959.518925, train/loss=0.317394, train/ssim=0.657686, validation/loss=0.337301, validation/num_examples=3554, validation/ssim=0.652011
I0609 02:14:24.500450 139606133552896 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.740435, loss=0.332084
I0609 02:14:24.504391 139672502753088 submission.py:139] 2500) loss = 0.332, grad_norm = 0.740
I0609 02:14:38.050032 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:14:40.189987 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:14:42.390486 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:14:44.556066 139672502753088 submission_runner.py:419] Time since start: 1046.03s, 	Step: 2553, 	{'train/ssim': 0.6913807732718331, 'train/loss': 0.3032665933881487, 'validation/ssim': 0.6757587181696679, 'validation/loss': 0.3247719476514139, 'validation/num_examples': 3554, 'test/ssim': 0.694359253918249, 'test/loss': 0.32525367172228425, 'test/num_examples': 3581, 'score': 766.2095983028412, 'total_duration': 1046.0319752693176, 'accumulated_submission_time': 766.2095983028412, 'accumulated_eval_time': 278.1036808490753, 'accumulated_logging_time': 0.1972494125366211}
I0609 02:14:44.566274 139606141945600 logging_writer.py:48] [2553] accumulated_eval_time=278.103681, accumulated_logging_time=0.197249, accumulated_submission_time=766.209598, global_step=2553, preemption_count=0, score=766.209598, test/loss=0.325254, test/num_examples=3581, test/ssim=0.694359, total_duration=1046.031975, train/loss=0.303267, train/ssim=0.691381, validation/loss=0.324772, validation/num_examples=3554, validation/ssim=0.675759
I0609 02:16:04.655973 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:16:06.831689 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:16:09.040127 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:16:11.228331 139672502753088 submission_runner.py:419] Time since start: 1132.70s, 	Step: 2864, 	{'train/ssim': 0.6514045170375279, 'train/loss': 0.318314722606114, 'validation/ssim': 0.6499981040289111, 'validation/loss': 0.3368402157999789, 'validation/num_examples': 3554, 'test/ssim': 0.6639444180222005, 'test/loss': 0.3376435186902227, 'test/num_examples': 3581, 'score': 846.1581611633301, 'total_duration': 1132.704252243042, 'accumulated_submission_time': 846.1581611633301, 'accumulated_eval_time': 284.6760981082916, 'accumulated_logging_time': 0.2155625820159912}
I0609 02:16:11.239093 139606133552896 logging_writer.py:48] [2864] accumulated_eval_time=284.676098, accumulated_logging_time=0.215563, accumulated_submission_time=846.158161, global_step=2864, preemption_count=0, score=846.158161, test/loss=0.337644, test/num_examples=3581, test/ssim=0.663944, total_duration=1132.704252, train/loss=0.318315, train/ssim=0.651405, validation/loss=0.336840, validation/num_examples=3554, validation/ssim=0.649998
I0609 02:16:44.981522 139606141945600 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.347400, loss=0.304973
I0609 02:16:44.985568 139672502753088 submission.py:139] 3000) loss = 0.305, grad_norm = 0.347
I0609 02:17:31.410083 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:17:33.620644 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:17:35.870649 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:17:38.087298 139672502753088 submission_runner.py:419] Time since start: 1219.56s, 	Step: 3174, 	{'train/ssim': 0.6790946551731655, 'train/loss': 0.3176948683602469, 'validation/ssim': 0.6677547663064505, 'validation/loss': 0.33762563556248243, 'validation/num_examples': 3554, 'test/ssim': 0.6859770697343619, 'test/loss': 0.33794796157236107, 'test/num_examples': 3581, 'score': 926.1877493858337, 'total_duration': 1219.5632240772247, 'accumulated_submission_time': 926.1877493858337, 'accumulated_eval_time': 291.3532955646515, 'accumulated_logging_time': 0.23450803756713867}
I0609 02:17:38.097886 139606133552896 logging_writer.py:48] [3174] accumulated_eval_time=291.353296, accumulated_logging_time=0.234508, accumulated_submission_time=926.187749, global_step=3174, preemption_count=0, score=926.187749, test/loss=0.337948, test/num_examples=3581, test/ssim=0.685977, total_duration=1219.563224, train/loss=0.317695, train/ssim=0.679095, validation/loss=0.337626, validation/num_examples=3554, validation/ssim=0.667755
I0609 02:18:58.099705 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:19:00.269898 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:19:02.502247 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:19:04.700729 139672502753088 submission_runner.py:419] Time since start: 1306.18s, 	Step: 3485, 	{'train/ssim': 0.7054952212742397, 'train/loss': 0.31322063718523296, 'validation/ssim': 0.6898453024980655, 'validation/loss': 0.3319183847645259, 'validation/num_examples': 3554, 'test/ssim': 0.7051178719936819, 'test/loss': 0.33572867488568137, 'test/num_examples': 3581, 'score': 1006.0501532554626, 'total_duration': 1306.1766304969788, 'accumulated_submission_time': 1006.0501532554626, 'accumulated_eval_time': 297.9543762207031, 'accumulated_logging_time': 0.25336146354675293}
I0609 02:19:04.711992 139606141945600 logging_writer.py:48] [3485] accumulated_eval_time=297.954376, accumulated_logging_time=0.253361, accumulated_submission_time=1006.050153, global_step=3485, preemption_count=0, score=1006.050153, test/loss=0.335729, test/num_examples=3581, test/ssim=0.705118, total_duration=1306.176630, train/loss=0.313221, train/ssim=0.705495, validation/loss=0.331918, validation/num_examples=3554, validation/ssim=0.689845
I0609 02:19:06.324981 139606133552896 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.527271, loss=0.346861
I0609 02:19:06.328340 139672502753088 submission.py:139] 3500) loss = 0.347, grad_norm = 0.527
I0609 02:20:24.840836 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:20:27.044730 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:20:29.295267 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:20:31.494932 139672502753088 submission_runner.py:419] Time since start: 1392.97s, 	Step: 3798, 	{'train/ssim': 0.6388454437255859, 'train/loss': 0.32320901325770784, 'validation/ssim': 0.6372992881040729, 'validation/loss': 0.3416518263422376, 'validation/num_examples': 3554, 'test/ssim': 0.6548182901075119, 'test/loss': 0.3423464470687308, 'test/num_examples': 3581, 'score': 1086.035945892334, 'total_duration': 1392.9708111286163, 'accumulated_submission_time': 1086.035945892334, 'accumulated_eval_time': 304.6084363460541, 'accumulated_logging_time': 0.27286553382873535}
I0609 02:20:31.507234 139606141945600 logging_writer.py:48] [3798] accumulated_eval_time=304.608436, accumulated_logging_time=0.272866, accumulated_submission_time=1086.035946, global_step=3798, preemption_count=0, score=1086.035946, test/loss=0.342346, test/num_examples=3581, test/ssim=0.654818, total_duration=1392.970811, train/loss=0.323209, train/ssim=0.638845, validation/loss=0.341652, validation/num_examples=3554, validation/ssim=0.637299
I0609 02:21:23.008548 139606133552896 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.515628, loss=0.361156
I0609 02:21:23.012407 139672502753088 submission.py:139] 4000) loss = 0.361, grad_norm = 0.516
I0609 02:21:51.617500 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:21:53.795530 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:21:56.087691 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:21:58.352886 139672502753088 submission_runner.py:419] Time since start: 1479.83s, 	Step: 4109, 	{'train/ssim': 0.7164004870823452, 'train/loss': 0.28491086619240896, 'validation/ssim': 0.6999127715909891, 'validation/loss': 0.30613297187148286, 'validation/num_examples': 3554, 'test/ssim': 0.7161894209456158, 'test/loss': 0.3074166799383901, 'test/num_examples': 3581, 'score': 1166.0067369937897, 'total_duration': 1479.8287830352783, 'accumulated_submission_time': 1166.0067369937897, 'accumulated_eval_time': 311.34376287460327, 'accumulated_logging_time': 0.29354166984558105}
I0609 02:21:58.363394 139606141945600 logging_writer.py:48] [4109] accumulated_eval_time=311.343763, accumulated_logging_time=0.293542, accumulated_submission_time=1166.006737, global_step=4109, preemption_count=0, score=1166.006737, test/loss=0.307417, test/num_examples=3581, test/ssim=0.716189, total_duration=1479.828783, train/loss=0.284911, train/ssim=0.716400, validation/loss=0.306133, validation/num_examples=3554, validation/ssim=0.699913
I0609 02:23:18.405842 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:23:20.574336 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:23:22.788694 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:23:24.977076 139672502753088 submission_runner.py:419] Time since start: 1566.45s, 	Step: 4418, 	{'train/ssim': 0.6858346121651786, 'train/loss': 0.30887276785714285, 'validation/ssim': 0.6744327749103123, 'validation/loss': 0.32854829615398146, 'validation/num_examples': 3554, 'test/ssim': 0.6916541403413851, 'test/loss': 0.32940106257417623, 'test/num_examples': 3581, 'score': 1245.9085433483124, 'total_duration': 1566.4529979228973, 'accumulated_submission_time': 1245.9085433483124, 'accumulated_eval_time': 317.915061712265, 'accumulated_logging_time': 0.31268787384033203}
I0609 02:23:24.987792 139606133552896 logging_writer.py:48] [4418] accumulated_eval_time=317.915062, accumulated_logging_time=0.312688, accumulated_submission_time=1245.908543, global_step=4418, preemption_count=0, score=1245.908543, test/loss=0.329401, test/num_examples=3581, test/ssim=0.691654, total_duration=1566.452998, train/loss=0.308873, train/ssim=0.685835, validation/loss=0.328548, validation/num_examples=3554, validation/ssim=0.674433
I0609 02:23:44.394003 139606141945600 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.523019, loss=0.307833
I0609 02:23:44.397672 139672502753088 submission.py:139] 4500) loss = 0.308, grad_norm = 0.523
I0609 02:24:45.068474 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:24:47.212932 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:24:49.400271 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:24:51.578047 139672502753088 submission_runner.py:419] Time since start: 1653.05s, 	Step: 4731, 	{'train/ssim': 0.7358489717755999, 'train/loss': 0.2720906734466553, 'validation/ssim': 0.714820599918226, 'validation/loss': 0.2945460811651484, 'validation/num_examples': 3554, 'test/ssim': 0.7319285486770455, 'test/loss': 0.2961689624930187, 'test/num_examples': 3581, 'score': 1325.8456196784973, 'total_duration': 1653.0539779663086, 'accumulated_submission_time': 1325.8456196784973, 'accumulated_eval_time': 324.42472791671753, 'accumulated_logging_time': 0.33221888542175293}
I0609 02:24:51.589743 139606133552896 logging_writer.py:48] [4731] accumulated_eval_time=324.424728, accumulated_logging_time=0.332219, accumulated_submission_time=1325.845620, global_step=4731, preemption_count=0, score=1325.845620, test/loss=0.296169, test/num_examples=3581, test/ssim=0.731929, total_duration=1653.053978, train/loss=0.272091, train/ssim=0.735849, validation/loss=0.294546, validation/num_examples=3554, validation/ssim=0.714821
I0609 02:26:00.739223 139606141945600 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.396087, loss=0.289885
I0609 02:26:00.744081 139672502753088 submission.py:139] 5000) loss = 0.290, grad_norm = 0.396
I0609 02:26:11.777884 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:26:13.963563 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:26:16.223143 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:26:18.454342 139672502753088 submission_runner.py:419] Time since start: 1739.93s, 	Step: 5042, 	{'train/ssim': 0.6380365235464913, 'train/loss': 0.324563775743757, 'validation/ssim': 0.6384862621781795, 'validation/loss': 0.3427757731165588, 'validation/num_examples': 3554, 'test/ssim': 0.653278315676487, 'test/loss': 0.3436874138246998, 'test/num_examples': 3581, 'score': 1405.893534898758, 'total_duration': 1739.9302320480347, 'accumulated_submission_time': 1405.893534898758, 'accumulated_eval_time': 331.1012051105499, 'accumulated_logging_time': 0.35309433937072754}
I0609 02:26:18.467565 139606133552896 logging_writer.py:48] [5042] accumulated_eval_time=331.101205, accumulated_logging_time=0.353094, accumulated_submission_time=1405.893535, global_step=5042, preemption_count=0, score=1405.893535, test/loss=0.343687, test/num_examples=3581, test/ssim=0.653278, total_duration=1739.930232, train/loss=0.324564, train/ssim=0.638037, validation/loss=0.342776, validation/num_examples=3554, validation/ssim=0.638486
I0609 02:27:38.531588 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:27:40.824609 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:27:43.117107 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:27:45.392014 139672502753088 submission_runner.py:419] Time since start: 1826.87s, 	Step: 5351, 	{'train/ssim': 0.7145969527108329, 'train/loss': 0.2977220671517508, 'validation/ssim': 0.6972302472785945, 'validation/loss': 0.31872342205745285, 'validation/num_examples': 3554, 'test/ssim': 0.7130567715241902, 'test/loss': 0.32086807161494696, 'test/num_examples': 3581, 'score': 1485.8165066242218, 'total_duration': 1826.8679521083832, 'accumulated_submission_time': 1485.8165066242218, 'accumulated_eval_time': 337.96172976493835, 'accumulated_logging_time': 0.3751826286315918}
I0609 02:27:45.403545 139606141945600 logging_writer.py:48] [5351] accumulated_eval_time=337.961730, accumulated_logging_time=0.375183, accumulated_submission_time=1485.816507, global_step=5351, preemption_count=0, score=1485.816507, test/loss=0.320868, test/num_examples=3581, test/ssim=0.713057, total_duration=1826.867952, train/loss=0.297722, train/ssim=0.714597, validation/loss=0.318723, validation/num_examples=3554, validation/ssim=0.697230
I0609 02:28:03.287427 139672502753088 spec.py:298] Evaluating on the training split.
I0609 02:28:05.319686 139672502753088 spec.py:310] Evaluating on the validation split.
I0609 02:28:07.399439 139672502753088 spec.py:326] Evaluating on the test split.
I0609 02:28:09.439249 139672502753088 submission_runner.py:419] Time since start: 1850.92s, 	Step: 5428, 	{'train/ssim': 0.6989173889160156, 'train/loss': 0.2920323780604771, 'validation/ssim': 0.6877501170556064, 'validation/loss': 0.3121958889851224, 'validation/num_examples': 3554, 'test/ssim': 0.7031946083714395, 'test/loss': 0.3132322855216769, 'test/num_examples': 3581, 'score': 1503.6578857898712, 'total_duration': 1850.915161371231, 'accumulated_submission_time': 1503.6578857898712, 'accumulated_eval_time': 344.1135013103485, 'accumulated_logging_time': 0.3956015110015869}
I0609 02:28:09.450211 139606133552896 logging_writer.py:48] [5428] accumulated_eval_time=344.113501, accumulated_logging_time=0.395602, accumulated_submission_time=1503.657886, global_step=5428, preemption_count=0, score=1503.657886, test/loss=0.313232, test/num_examples=3581, test/ssim=0.703195, total_duration=1850.915161, train/loss=0.292032, train/ssim=0.698917, validation/loss=0.312196, validation/num_examples=3554, validation/ssim=0.687750
I0609 02:28:09.467020 139606141945600 logging_writer.py:48] [5428] global_step=5428, preemption_count=0, score=1503.657886
I0609 02:28:09.567531 139672502753088 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nesterov/fastmri_pytorch/trial_1/checkpoint_5428.
I0609 02:28:10.300286 139672502753088 submission_runner.py:581] Tuning trial 1/1
I0609 02:28:10.300528 139672502753088 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 02:28:10.307220 139672502753088 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ssim': 0.23230159282684326, 'train/loss': 0.8644339697701591, 'validation/ssim': 0.23160025318083496, 'validation/loss': 0.8679018551939716, 'validation/num_examples': 3554, 'test/ssim': 0.252667786854667, 'test/loss': 0.8683983175090757, 'test/num_examples': 3581, 'score': 46.25284767150879, 'total_duration': 264.05111956596375, 'accumulated_submission_time': 46.25284767150879, 'accumulated_eval_time': 217.79783082008362, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (311, {'train/ssim': 0.6965204647609166, 'train/loss': 0.3033086231776646, 'validation/ssim': 0.6784282592017797, 'validation/loss': 0.32579755809914884, 'validation/num_examples': 3554, 'test/ssim': 0.6959481110156032, 'test/loss': 0.3277748446662943, 'test/num_examples': 3581, 'score': 126.08786129951477, 'total_duration': 351.1908903121948, 'accumulated_submission_time': 126.08786129951477, 'accumulated_eval_time': 224.84088587760925, 'accumulated_logging_time': 0.02626323699951172, 'global_step': 311, 'preemption_count': 0}), (538, {'train/ssim': 0.7086457524980817, 'train/loss': 0.29624911717006136, 'validation/ssim': 0.6904097660646454, 'validation/loss': 0.31810482714239235, 'validation/num_examples': 3554, 'test/ssim': 0.7076781101647585, 'test/loss': 0.3200758588077702, 'test/num_examples': 3581, 'score': 206.2127537727356, 'total_duration': 438.277939081192, 'accumulated_submission_time': 206.2127537727356, 'accumulated_eval_time': 231.576895236969, 'accumulated_logging_time': 0.05191516876220703, 'global_step': 538, 'preemption_count': 0}), (766, {'train/ssim': 0.7089924131120954, 'train/loss': 0.30248955317905973, 'validation/ssim': 0.6897930259039111, 'validation/loss': 0.3235404938207829, 'validation/num_examples': 3554, 'test/ssim': 0.7069152814987084, 'test/loss': 0.3259843891807456, 'test/num_examples': 3581, 'score': 286.2685968875885, 'total_duration': 525.4795181751251, 'accumulated_submission_time': 286.2685968875885, 'accumulated_eval_time': 238.49534440040588, 'accumulated_logging_time': 0.07453680038452148, 'global_step': 766, 'preemption_count': 0}), (999, {'train/ssim': 0.6815761838640485, 'train/loss': 0.3159088066646031, 'validation/ssim': 0.6704485565340109, 'validation/loss': 0.33679807166001335, 'validation/num_examples': 3554, 'test/ssim': 0.6852959848855068, 'test/loss': 0.3380982229344108, 'test/num_examples': 3581, 'score': 366.2849199771881, 'total_duration': 612.5394117832184, 'accumulated_submission_time': 366.2849199771881, 'accumulated_eval_time': 245.2913463115692, 'accumulated_logging_time': 0.10164260864257812, 'global_step': 999, 'preemption_count': 0}), (1308, {'train/ssim': 0.720123427254813, 'train/loss': 0.2879981313432966, 'validation/ssim': 0.7001497679771033, 'validation/loss': 0.31017716063766176, 'validation/num_examples': 3554, 'test/ssim': 0.7167619003726263, 'test/loss': 0.3125237254782184, 'test/num_examples': 3581, 'score': 446.3002552986145, 'total_duration': 699.2978775501251, 'accumulated_submission_time': 446.3002552986145, 'accumulated_eval_time': 251.8828341960907, 'accumulated_logging_time': 0.12072324752807617, 'global_step': 1308, 'preemption_count': 0}), (1621, {'train/ssim': 0.6625005858285087, 'train/loss': 0.3107748712812151, 'validation/ssim': 0.6572780834535031, 'validation/loss': 0.3309654188612303, 'validation/num_examples': 3554, 'test/ssim': 0.6718838629660011, 'test/loss': 0.3317626680418354, 'test/num_examples': 3581, 'score': 526.3265244960785, 'total_duration': 786.0027675628662, 'accumulated_submission_time': 526.3265244960785, 'accumulated_eval_time': 258.4094932079315, 'accumulated_logging_time': 0.13926029205322266, 'global_step': 1621, 'preemption_count': 0}), (1933, {'train/ssim': 0.6366872106279645, 'train/loss': 0.3387303352355957, 'validation/ssim': 0.631342847627673, 'validation/loss': 0.35779626059545583, 'validation/num_examples': 3554, 'test/ssim': 0.6503120173092363, 'test/loss': 0.3583553157615715, 'test/num_examples': 3581, 'score': 606.2427084445953, 'total_duration': 872.6386241912842, 'accumulated_submission_time': 606.2427084445953, 'accumulated_eval_time': 264.97969222068787, 'accumulated_logging_time': 0.15848541259765625, 'global_step': 1933, 'preemption_count': 0}), (2241, {'train/ssim': 0.6576857566833496, 'train/loss': 0.3173941544124058, 'validation/ssim': 0.6520114055026027, 'validation/loss': 0.3373012252919246, 'validation/num_examples': 3554, 'test/ssim': 0.6686650382443801, 'test/loss': 0.3374894394351264, 'test/num_examples': 3581, 'score': 686.3546783924103, 'total_duration': 959.5189249515533, 'accumulated_submission_time': 686.3546783924103, 'accumulated_eval_time': 271.59761095046997, 'accumulated_logging_time': 0.17799043655395508, 'global_step': 2241, 'preemption_count': 0}), (2553, {'train/ssim': 0.6913807732718331, 'train/loss': 0.3032665933881487, 'validation/ssim': 0.6757587181696679, 'validation/loss': 0.3247719476514139, 'validation/num_examples': 3554, 'test/ssim': 0.694359253918249, 'test/loss': 0.32525367172228425, 'test/num_examples': 3581, 'score': 766.2095983028412, 'total_duration': 1046.0319752693176, 'accumulated_submission_time': 766.2095983028412, 'accumulated_eval_time': 278.1036808490753, 'accumulated_logging_time': 0.1972494125366211, 'global_step': 2553, 'preemption_count': 0}), (2864, {'train/ssim': 0.6514045170375279, 'train/loss': 0.318314722606114, 'validation/ssim': 0.6499981040289111, 'validation/loss': 0.3368402157999789, 'validation/num_examples': 3554, 'test/ssim': 0.6639444180222005, 'test/loss': 0.3376435186902227, 'test/num_examples': 3581, 'score': 846.1581611633301, 'total_duration': 1132.704252243042, 'accumulated_submission_time': 846.1581611633301, 'accumulated_eval_time': 284.6760981082916, 'accumulated_logging_time': 0.2155625820159912, 'global_step': 2864, 'preemption_count': 0}), (3174, {'train/ssim': 0.6790946551731655, 'train/loss': 0.3176948683602469, 'validation/ssim': 0.6677547663064505, 'validation/loss': 0.33762563556248243, 'validation/num_examples': 3554, 'test/ssim': 0.6859770697343619, 'test/loss': 0.33794796157236107, 'test/num_examples': 3581, 'score': 926.1877493858337, 'total_duration': 1219.5632240772247, 'accumulated_submission_time': 926.1877493858337, 'accumulated_eval_time': 291.3532955646515, 'accumulated_logging_time': 0.23450803756713867, 'global_step': 3174, 'preemption_count': 0}), (3485, {'train/ssim': 0.7054952212742397, 'train/loss': 0.31322063718523296, 'validation/ssim': 0.6898453024980655, 'validation/loss': 0.3319183847645259, 'validation/num_examples': 3554, 'test/ssim': 0.7051178719936819, 'test/loss': 0.33572867488568137, 'test/num_examples': 3581, 'score': 1006.0501532554626, 'total_duration': 1306.1766304969788, 'accumulated_submission_time': 1006.0501532554626, 'accumulated_eval_time': 297.9543762207031, 'accumulated_logging_time': 0.25336146354675293, 'global_step': 3485, 'preemption_count': 0}), (3798, {'train/ssim': 0.6388454437255859, 'train/loss': 0.32320901325770784, 'validation/ssim': 0.6372992881040729, 'validation/loss': 0.3416518263422376, 'validation/num_examples': 3554, 'test/ssim': 0.6548182901075119, 'test/loss': 0.3423464470687308, 'test/num_examples': 3581, 'score': 1086.035945892334, 'total_duration': 1392.9708111286163, 'accumulated_submission_time': 1086.035945892334, 'accumulated_eval_time': 304.6084363460541, 'accumulated_logging_time': 0.27286553382873535, 'global_step': 3798, 'preemption_count': 0}), (4109, {'train/ssim': 0.7164004870823452, 'train/loss': 0.28491086619240896, 'validation/ssim': 0.6999127715909891, 'validation/loss': 0.30613297187148286, 'validation/num_examples': 3554, 'test/ssim': 0.7161894209456158, 'test/loss': 0.3074166799383901, 'test/num_examples': 3581, 'score': 1166.0067369937897, 'total_duration': 1479.8287830352783, 'accumulated_submission_time': 1166.0067369937897, 'accumulated_eval_time': 311.34376287460327, 'accumulated_logging_time': 0.29354166984558105, 'global_step': 4109, 'preemption_count': 0}), (4418, {'train/ssim': 0.6858346121651786, 'train/loss': 0.30887276785714285, 'validation/ssim': 0.6744327749103123, 'validation/loss': 0.32854829615398146, 'validation/num_examples': 3554, 'test/ssim': 0.6916541403413851, 'test/loss': 0.32940106257417623, 'test/num_examples': 3581, 'score': 1245.9085433483124, 'total_duration': 1566.4529979228973, 'accumulated_submission_time': 1245.9085433483124, 'accumulated_eval_time': 317.915061712265, 'accumulated_logging_time': 0.31268787384033203, 'global_step': 4418, 'preemption_count': 0}), (4731, {'train/ssim': 0.7358489717755999, 'train/loss': 0.2720906734466553, 'validation/ssim': 0.714820599918226, 'validation/loss': 0.2945460811651484, 'validation/num_examples': 3554, 'test/ssim': 0.7319285486770455, 'test/loss': 0.2961689624930187, 'test/num_examples': 3581, 'score': 1325.8456196784973, 'total_duration': 1653.0539779663086, 'accumulated_submission_time': 1325.8456196784973, 'accumulated_eval_time': 324.42472791671753, 'accumulated_logging_time': 0.33221888542175293, 'global_step': 4731, 'preemption_count': 0}), (5042, {'train/ssim': 0.6380365235464913, 'train/loss': 0.324563775743757, 'validation/ssim': 0.6384862621781795, 'validation/loss': 0.3427757731165588, 'validation/num_examples': 3554, 'test/ssim': 0.653278315676487, 'test/loss': 0.3436874138246998, 'test/num_examples': 3581, 'score': 1405.893534898758, 'total_duration': 1739.9302320480347, 'accumulated_submission_time': 1405.893534898758, 'accumulated_eval_time': 331.1012051105499, 'accumulated_logging_time': 0.35309433937072754, 'global_step': 5042, 'preemption_count': 0}), (5351, {'train/ssim': 0.7145969527108329, 'train/loss': 0.2977220671517508, 'validation/ssim': 0.6972302472785945, 'validation/loss': 0.31872342205745285, 'validation/num_examples': 3554, 'test/ssim': 0.7130567715241902, 'test/loss': 0.32086807161494696, 'test/num_examples': 3581, 'score': 1485.8165066242218, 'total_duration': 1826.8679521083832, 'accumulated_submission_time': 1485.8165066242218, 'accumulated_eval_time': 337.96172976493835, 'accumulated_logging_time': 0.3751826286315918, 'global_step': 5351, 'preemption_count': 0}), (5428, {'train/ssim': 0.6989173889160156, 'train/loss': 0.2920323780604771, 'validation/ssim': 0.6877501170556064, 'validation/loss': 0.3121958889851224, 'validation/num_examples': 3554, 'test/ssim': 0.7031946083714395, 'test/loss': 0.3132322855216769, 'test/num_examples': 3581, 'score': 1503.6578857898712, 'total_duration': 1850.915161371231, 'accumulated_submission_time': 1503.6578857898712, 'accumulated_eval_time': 344.1135013103485, 'accumulated_logging_time': 0.3956015110015869, 'global_step': 5428, 'preemption_count': 0})], 'global_step': 5428}
I0609 02:28:10.307410 139672502753088 submission_runner.py:584] Timing: 1503.6578857898712
I0609 02:28:10.307461 139672502753088 submission_runner.py:586] Total number of evals: 20
I0609 02:28:10.307503 139672502753088 submission_runner.py:587] ====================
I0609 02:28:10.307618 139672502753088 submission_runner.py:655] Final fastmri score: 1503.6578857898712
