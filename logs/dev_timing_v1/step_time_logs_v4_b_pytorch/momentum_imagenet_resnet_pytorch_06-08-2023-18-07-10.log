torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-08-2023-18-07-10.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 18:07:33.260199 140387896768320 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 18:07:33.260250 140624911374144 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 18:07:33.260286 139794017748800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 18:07:33.260770 139860972365632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 18:07:33.261024 139941585884992 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 18:07:33.261213 140346347251520 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 18:07:33.261337 140619027248960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 18:07:33.262046 140476240373568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 18:07:33.262351 140476240373568 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.270806 140387896768320 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.270838 140624911374144 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.270861 139794017748800 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.271336 139860972365632 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.271555 139941585884992 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.271782 140346347251520 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:33.272000 140619027248960 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 18:07:35.558878 140476240373568 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_resnet_pytorch.
W0608 18:07:35.680366 139860972365632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.680711 140346347251520 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.680747 140387896768320 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.680846 140476240373568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.680851 140619027248960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.681317 139941585884992 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.681604 139794017748800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 18:07:35.683370 140624911374144 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 18:07:35.686922 140476240373568 submission_runner.py:541] Using RNG seed 704595407
I0608 18:07:35.688198 140476240373568 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 18:07:35.688313 140476240373568 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_resnet_pytorch/trial_1.
I0608 18:07:35.688523 140476240373568 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_resnet_pytorch/trial_1/hparams.json.
I0608 18:07:35.689517 140476240373568 submission_runner.py:255] Initializing dataset.
I0608 18:07:42.153548 140476240373568 submission_runner.py:262] Initializing model.
I0608 18:07:46.753003 140476240373568 submission_runner.py:272] Initializing optimizer.
I0608 18:07:47.256584 140476240373568 submission_runner.py:279] Initializing metrics bundle.
I0608 18:07:47.256817 140476240373568 submission_runner.py:297] Initializing checkpoint and logger.
I0608 18:07:47.749609 140476240373568 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0608 18:07:47.750716 140476240373568 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0608 18:07:47.806848 140476240373568 submission_runner.py:332] Starting training loop.
I0608 18:07:55.791286 140447812941568 logging_writer.py:48] [0] global_step=0, grad_norm=0.527141, loss=6.927131
I0608 18:07:55.812784 140476240373568 spec.py:298] Evaluating on the training split.
I0608 18:08:55.986317 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 18:09:51.169092 140476240373568 spec.py:326] Evaluating on the test split.
I0608 18:09:51.188476 140476240373568 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 18:09:51.195388 140476240373568 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 18:09:51.277256 140476240373568 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 18:10:02.839028 140476240373568 submission_runner.py:419] Time since start: 135.03s, 	Step: 1, 	{'train/accuracy': 0.0012157206632653062, 'train/loss': 6.920588279257015, 'validation/accuracy': 0.0013, 'validation/loss': 6.922508125, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92375, 'test/num_examples': 10000, 'score': 8.005820274353027, 'total_duration': 135.03247570991516, 'accumulated_submission_time': 8.005820274353027, 'accumulated_eval_time': 127.02614879608154, 'accumulated_logging_time': 0}
I0608 18:10:02.857373 140417534224128 logging_writer.py:48] [1] accumulated_eval_time=127.026149, accumulated_logging_time=0, accumulated_submission_time=8.005820, global_step=1, preemption_count=0, score=8.005820, test/accuracy=0.000600, test/loss=6.923750, test/num_examples=10000, total_duration=135.032476, train/accuracy=0.001216, train/loss=6.920588, validation/accuracy=0.001300, validation/loss=6.922508, validation/num_examples=50000
I0608 18:10:02.901577 140476240373568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.901612 139794017748800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.901607 140624911374144 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.901662 140619027248960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.901665 140346347251520 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.901648 140387896768320 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.902166 139941585884992 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:02.902361 139860972365632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 18:10:03.267851 140417525831424 logging_writer.py:48] [1] global_step=1, grad_norm=0.548891, loss=6.931953
I0608 18:10:03.644387 140417534224128 logging_writer.py:48] [2] global_step=2, grad_norm=0.541512, loss=6.929745
I0608 18:10:04.020119 140417525831424 logging_writer.py:48] [3] global_step=3, grad_norm=0.542641, loss=6.926888
I0608 18:10:04.401688 140417534224128 logging_writer.py:48] [4] global_step=4, grad_norm=0.535776, loss=6.930738
I0608 18:10:04.787826 140417525831424 logging_writer.py:48] [5] global_step=5, grad_norm=0.533057, loss=6.928463
I0608 18:10:05.166066 140417534224128 logging_writer.py:48] [6] global_step=6, grad_norm=0.541531, loss=6.923308
I0608 18:10:05.547577 140417525831424 logging_writer.py:48] [7] global_step=7, grad_norm=0.545530, loss=6.922638
I0608 18:10:05.937406 140417534224128 logging_writer.py:48] [8] global_step=8, grad_norm=0.545943, loss=6.931208
I0608 18:10:06.314508 140417525831424 logging_writer.py:48] [9] global_step=9, grad_norm=0.558388, loss=6.929671
I0608 18:10:06.694198 140417534224128 logging_writer.py:48] [10] global_step=10, grad_norm=0.545549, loss=6.932037
I0608 18:10:07.078705 140417525831424 logging_writer.py:48] [11] global_step=11, grad_norm=0.533327, loss=6.922845
I0608 18:10:07.470180 140417534224128 logging_writer.py:48] [12] global_step=12, grad_norm=0.535664, loss=6.929900
I0608 18:10:07.855052 140417525831424 logging_writer.py:48] [13] global_step=13, grad_norm=0.535104, loss=6.922084
I0608 18:10:08.237648 140417534224128 logging_writer.py:48] [14] global_step=14, grad_norm=0.545522, loss=6.918715
I0608 18:10:08.615666 140417525831424 logging_writer.py:48] [15] global_step=15, grad_norm=0.535751, loss=6.933476
I0608 18:10:08.995805 140417534224128 logging_writer.py:48] [16] global_step=16, grad_norm=0.529509, loss=6.930622
I0608 18:10:09.376716 140417525831424 logging_writer.py:48] [17] global_step=17, grad_norm=0.544643, loss=6.935440
I0608 18:10:09.755796 140417534224128 logging_writer.py:48] [18] global_step=18, grad_norm=0.540590, loss=6.924495
I0608 18:10:10.138538 140417525831424 logging_writer.py:48] [19] global_step=19, grad_norm=0.538005, loss=6.928731
I0608 18:10:10.520713 140417534224128 logging_writer.py:48] [20] global_step=20, grad_norm=0.545762, loss=6.922399
I0608 18:10:10.904918 140417525831424 logging_writer.py:48] [21] global_step=21, grad_norm=0.538042, loss=6.925829
I0608 18:10:11.284330 140417534224128 logging_writer.py:48] [22] global_step=22, grad_norm=0.536763, loss=6.922315
I0608 18:10:11.662467 140417525831424 logging_writer.py:48] [23] global_step=23, grad_norm=0.530672, loss=6.932462
I0608 18:10:12.047742 140417534224128 logging_writer.py:48] [24] global_step=24, grad_norm=0.541106, loss=6.922785
I0608 18:10:12.425126 140417525831424 logging_writer.py:48] [25] global_step=25, grad_norm=0.527642, loss=6.922396
I0608 18:10:12.806527 140417534224128 logging_writer.py:48] [26] global_step=26, grad_norm=0.547596, loss=6.925035
I0608 18:10:13.194859 140417525831424 logging_writer.py:48] [27] global_step=27, grad_norm=0.530055, loss=6.925740
I0608 18:10:13.576806 140417534224128 logging_writer.py:48] [28] global_step=28, grad_norm=0.544454, loss=6.923210
I0608 18:10:13.956896 140417525831424 logging_writer.py:48] [29] global_step=29, grad_norm=0.542034, loss=6.923315
I0608 18:10:14.335959 140417534224128 logging_writer.py:48] [30] global_step=30, grad_norm=0.536256, loss=6.921544
I0608 18:10:14.715727 140417525831424 logging_writer.py:48] [31] global_step=31, grad_norm=0.521402, loss=6.922552
I0608 18:10:15.096009 140417534224128 logging_writer.py:48] [32] global_step=32, grad_norm=0.519889, loss=6.925638
I0608 18:10:15.474261 140417525831424 logging_writer.py:48] [33] global_step=33, grad_norm=0.554643, loss=6.932780
I0608 18:10:15.853212 140417534224128 logging_writer.py:48] [34] global_step=34, grad_norm=0.533886, loss=6.917533
I0608 18:10:16.232027 140417525831424 logging_writer.py:48] [35] global_step=35, grad_norm=0.526912, loss=6.916708
I0608 18:10:16.609417 140417534224128 logging_writer.py:48] [36] global_step=36, grad_norm=0.545150, loss=6.909304
I0608 18:10:16.988606 140417525831424 logging_writer.py:48] [37] global_step=37, grad_norm=0.536246, loss=6.919662
I0608 18:10:17.368503 140417534224128 logging_writer.py:48] [38] global_step=38, grad_norm=0.525224, loss=6.913800
I0608 18:10:17.749133 140417525831424 logging_writer.py:48] [39] global_step=39, grad_norm=0.533980, loss=6.925619
I0608 18:10:18.127854 140417534224128 logging_writer.py:48] [40] global_step=40, grad_norm=0.539508, loss=6.924917
I0608 18:10:18.508741 140417525831424 logging_writer.py:48] [41] global_step=41, grad_norm=0.520853, loss=6.923532
I0608 18:10:18.894856 140417534224128 logging_writer.py:48] [42] global_step=42, grad_norm=0.530593, loss=6.926773
I0608 18:10:19.276011 140417525831424 logging_writer.py:48] [43] global_step=43, grad_norm=0.538463, loss=6.918275
I0608 18:10:19.657377 140417534224128 logging_writer.py:48] [44] global_step=44, grad_norm=0.527787, loss=6.910870
I0608 18:10:20.036982 140417525831424 logging_writer.py:48] [45] global_step=45, grad_norm=0.517706, loss=6.908758
I0608 18:10:20.417425 140417534224128 logging_writer.py:48] [46] global_step=46, grad_norm=0.546997, loss=6.919691
I0608 18:10:20.799527 140417525831424 logging_writer.py:48] [47] global_step=47, grad_norm=0.528197, loss=6.909141
I0608 18:10:21.189646 140417534224128 logging_writer.py:48] [48] global_step=48, grad_norm=0.522583, loss=6.910880
I0608 18:10:21.569972 140417525831424 logging_writer.py:48] [49] global_step=49, grad_norm=0.539073, loss=6.911802
I0608 18:10:21.952291 140417534224128 logging_writer.py:48] [50] global_step=50, grad_norm=0.522527, loss=6.909677
I0608 18:10:22.333239 140417525831424 logging_writer.py:48] [51] global_step=51, grad_norm=0.527320, loss=6.914811
I0608 18:10:22.725242 140417534224128 logging_writer.py:48] [52] global_step=52, grad_norm=0.523702, loss=6.914845
I0608 18:10:23.104977 140417525831424 logging_writer.py:48] [53] global_step=53, grad_norm=0.523641, loss=6.914239
I0608 18:10:23.487176 140417534224128 logging_writer.py:48] [54] global_step=54, grad_norm=0.537960, loss=6.909160
I0608 18:10:23.865003 140417525831424 logging_writer.py:48] [55] global_step=55, grad_norm=0.520631, loss=6.914612
I0608 18:10:24.251479 140417534224128 logging_writer.py:48] [56] global_step=56, grad_norm=0.535184, loss=6.912219
I0608 18:10:24.632349 140417525831424 logging_writer.py:48] [57] global_step=57, grad_norm=0.531231, loss=6.908321
I0608 18:10:25.020497 140417534224128 logging_writer.py:48] [58] global_step=58, grad_norm=0.547057, loss=6.905670
I0608 18:10:25.408866 140417525831424 logging_writer.py:48] [59] global_step=59, grad_norm=0.543149, loss=6.913838
I0608 18:10:25.794744 140417534224128 logging_writer.py:48] [60] global_step=60, grad_norm=0.531646, loss=6.909480
I0608 18:10:26.176913 140417525831424 logging_writer.py:48] [61] global_step=61, grad_norm=0.518004, loss=6.912360
I0608 18:10:26.559309 140417534224128 logging_writer.py:48] [62] global_step=62, grad_norm=0.514872, loss=6.916375
I0608 18:10:26.940613 140417525831424 logging_writer.py:48] [63] global_step=63, grad_norm=0.538548, loss=6.909551
I0608 18:10:27.324041 140417534224128 logging_writer.py:48] [64] global_step=64, grad_norm=0.548874, loss=6.906322
I0608 18:10:27.705472 140417525831424 logging_writer.py:48] [65] global_step=65, grad_norm=0.528620, loss=6.905690
I0608 18:10:28.084889 140417534224128 logging_writer.py:48] [66] global_step=66, grad_norm=0.526612, loss=6.906138
I0608 18:10:28.463763 140417525831424 logging_writer.py:48] [67] global_step=67, grad_norm=0.535406, loss=6.908978
I0608 18:10:28.853711 140417534224128 logging_writer.py:48] [68] global_step=68, grad_norm=0.533381, loss=6.908364
I0608 18:10:29.234040 140417525831424 logging_writer.py:48] [69] global_step=69, grad_norm=0.510916, loss=6.907981
I0608 18:10:29.617424 140417534224128 logging_writer.py:48] [70] global_step=70, grad_norm=0.528521, loss=6.905629
I0608 18:10:29.996211 140417525831424 logging_writer.py:48] [71] global_step=71, grad_norm=0.527454, loss=6.906414
I0608 18:10:30.377391 140417534224128 logging_writer.py:48] [72] global_step=72, grad_norm=0.528616, loss=6.906094
I0608 18:10:30.757915 140417525831424 logging_writer.py:48] [73] global_step=73, grad_norm=0.542645, loss=6.903172
I0608 18:10:31.140215 140417534224128 logging_writer.py:48] [74] global_step=74, grad_norm=0.516049, loss=6.897998
I0608 18:10:31.524248 140417525831424 logging_writer.py:48] [75] global_step=75, grad_norm=0.516139, loss=6.896420
I0608 18:10:31.904961 140417534224128 logging_writer.py:48] [76] global_step=76, grad_norm=0.520625, loss=6.901728
I0608 18:10:32.289152 140417525831424 logging_writer.py:48] [77] global_step=77, grad_norm=0.526509, loss=6.897312
I0608 18:10:32.672319 140417534224128 logging_writer.py:48] [78] global_step=78, grad_norm=0.536541, loss=6.901673
I0608 18:10:33.055680 140417525831424 logging_writer.py:48] [79] global_step=79, grad_norm=0.542188, loss=6.901734
I0608 18:10:33.442796 140417534224128 logging_writer.py:48] [80] global_step=80, grad_norm=0.513949, loss=6.896145
I0608 18:10:33.824533 140417525831424 logging_writer.py:48] [81] global_step=81, grad_norm=0.510155, loss=6.895980
I0608 18:10:34.209159 140417534224128 logging_writer.py:48] [82] global_step=82, grad_norm=0.519400, loss=6.899729
I0608 18:10:34.600424 140417525831424 logging_writer.py:48] [83] global_step=83, grad_norm=0.529527, loss=6.897480
I0608 18:10:34.984494 140417534224128 logging_writer.py:48] [84] global_step=84, grad_norm=0.523770, loss=6.898602
I0608 18:10:35.363986 140417525831424 logging_writer.py:48] [85] global_step=85, grad_norm=0.513818, loss=6.890585
I0608 18:10:35.748607 140417534224128 logging_writer.py:48] [86] global_step=86, grad_norm=0.539141, loss=6.891252
I0608 18:10:36.130185 140417525831424 logging_writer.py:48] [87] global_step=87, grad_norm=0.527066, loss=6.895222
I0608 18:10:36.511908 140417534224128 logging_writer.py:48] [88] global_step=88, grad_norm=0.523846, loss=6.896639
I0608 18:10:36.896700 140417525831424 logging_writer.py:48] [89] global_step=89, grad_norm=0.522723, loss=6.889954
I0608 18:10:37.279185 140417534224128 logging_writer.py:48] [90] global_step=90, grad_norm=0.534912, loss=6.898445
I0608 18:10:37.658373 140417525831424 logging_writer.py:48] [91] global_step=91, grad_norm=0.516984, loss=6.894018
I0608 18:10:38.040651 140417534224128 logging_writer.py:48] [92] global_step=92, grad_norm=0.531080, loss=6.890714
I0608 18:10:38.426703 140417525831424 logging_writer.py:48] [93] global_step=93, grad_norm=0.529212, loss=6.894990
I0608 18:10:38.807782 140417534224128 logging_writer.py:48] [94] global_step=94, grad_norm=0.516336, loss=6.894250
I0608 18:10:39.187946 140417525831424 logging_writer.py:48] [95] global_step=95, grad_norm=0.521391, loss=6.894935
I0608 18:10:39.567446 140417534224128 logging_writer.py:48] [96] global_step=96, grad_norm=0.525029, loss=6.894037
I0608 18:10:39.947716 140417525831424 logging_writer.py:48] [97] global_step=97, grad_norm=0.529399, loss=6.882586
I0608 18:10:40.324047 140417534224128 logging_writer.py:48] [98] global_step=98, grad_norm=0.530466, loss=6.889139
I0608 18:10:40.711880 140417525831424 logging_writer.py:48] [99] global_step=99, grad_norm=0.528228, loss=6.885005
I0608 18:10:41.100628 140417534224128 logging_writer.py:48] [100] global_step=100, grad_norm=0.526184, loss=6.891942
I0608 18:13:10.105889 140417525831424 logging_writer.py:48] [500] global_step=500, grad_norm=0.629553, loss=6.568393
I0608 18:16:16.060038 140417534224128 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.796624, loss=6.271816
I0608 18:18:33.082710 140476240373568 spec.py:298] Evaluating on the training split.
I0608 18:19:15.600003 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 18:20:09.409950 140476240373568 spec.py:326] Evaluating on the test split.
I0608 18:20:10.810935 140476240373568 submission_runner.py:419] Time since start: 743.00s, 	Step: 1366, 	{'train/accuracy': 0.06353635204081633, 'train/loss': 5.57986948441486, 'validation/accuracy': 0.0593, 'validation/loss': 5.640805625, 'validation/num_examples': 50000, 'test/accuracy': 0.0384, 'test/loss': 5.870466015625, 'test/num_examples': 10000, 'score': 517.1433017253876, 'total_duration': 743.0044584274292, 'accumulated_submission_time': 517.1433017253876, 'accumulated_eval_time': 224.7544801235199, 'accumulated_logging_time': 0.026584863662719727}
I0608 18:20:10.820807 140423771166464 logging_writer.py:48] [1366] accumulated_eval_time=224.754480, accumulated_logging_time=0.026585, accumulated_submission_time=517.143302, global_step=1366, preemption_count=0, score=517.143302, test/accuracy=0.038400, test/loss=5.870466, test/num_examples=10000, total_duration=743.004458, train/accuracy=0.063536, train/loss=5.579869, validation/accuracy=0.059300, validation/loss=5.640806, validation/num_examples=50000
I0608 18:21:00.810391 140423779559168 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.853546, loss=6.044702
I0608 18:24:06.878062 140423771166464 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.741644, loss=5.796567
I0608 18:27:12.846004 140423779559168 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.818956, loss=5.555170
I0608 18:28:41.175233 140476240373568 spec.py:298] Evaluating on the training split.
I0608 18:29:26.989657 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 18:30:21.510066 140476240373568 spec.py:326] Evaluating on the test split.
I0608 18:30:22.870228 140476240373568 submission_runner.py:419] Time since start: 1355.06s, 	Step: 2735, 	{'train/accuracy': 0.15361926020408162, 'train/loss': 4.597867070412149, 'validation/accuracy': 0.13774, 'validation/loss': 4.6978903125, 'validation/num_examples': 50000, 'test/accuracy': 0.0934, 'test/loss': 5.1442328125, 'test/num_examples': 10000, 'score': 1026.578690290451, 'total_duration': 1355.0625894069672, 'accumulated_submission_time': 1026.578690290451, 'accumulated_eval_time': 326.4483983516693, 'accumulated_logging_time': 0.04493093490600586}
I0608 18:30:22.880920 140423771166464 logging_writer.py:48] [2735] accumulated_eval_time=326.448398, accumulated_logging_time=0.044931, accumulated_submission_time=1026.578690, global_step=2735, preemption_count=0, score=1026.578690, test/accuracy=0.093400, test/loss=5.144233, test/num_examples=10000, total_duration=1355.062589, train/accuracy=0.153619, train/loss=4.597867, validation/accuracy=0.137740, validation/loss=4.697890, validation/num_examples=50000
I0608 18:32:01.525257 140423779559168 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.908416, loss=5.302412
I0608 18:35:07.643140 140423771166464 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.808056, loss=5.001308
I0608 18:38:15.236365 140423779559168 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.754690, loss=4.857155
I0608 18:38:53.135308 140476240373568 spec.py:298] Evaluating on the training split.
I0608 18:39:36.331997 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 18:40:21.394464 140476240373568 spec.py:326] Evaluating on the test split.
I0608 18:40:22.756188 140476240373568 submission_runner.py:419] Time since start: 1954.95s, 	Step: 4103, 	{'train/accuracy': 0.2780811543367347, 'train/loss': 3.5910252162388394, 'validation/accuracy': 0.2565, 'validation/loss': 3.7313278125, 'validation/num_examples': 50000, 'test/accuracy': 0.1862, 'test/loss': 4.28628046875, 'test/num_examples': 10000, 'score': 1535.906127691269, 'total_duration': 1954.9497203826904, 'accumulated_submission_time': 1535.906127691269, 'accumulated_eval_time': 416.0694487094879, 'accumulated_logging_time': 0.06348371505737305}
I0608 18:40:22.767158 140423771166464 logging_writer.py:48] [4103] accumulated_eval_time=416.069449, accumulated_logging_time=0.063484, accumulated_submission_time=1535.906128, global_step=4103, preemption_count=0, score=1535.906128, test/accuracy=0.186200, test/loss=4.286280, test/num_examples=10000, total_duration=1954.949720, train/accuracy=0.278081, train/loss=3.591025, validation/accuracy=0.256500, validation/loss=3.731328, validation/num_examples=50000
I0608 18:42:50.486332 140423779559168 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.774448, loss=4.534354
I0608 18:45:56.306214 140423771166464 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.671730, loss=4.530273
I0608 18:48:52.980230 140476240373568 spec.py:298] Evaluating on the training split.
I0608 18:49:38.524727 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 18:50:32.294829 140476240373568 spec.py:326] Evaluating on the test split.
I0608 18:50:33.655117 140476240373568 submission_runner.py:419] Time since start: 2565.85s, 	Step: 5473, 	{'train/accuracy': 0.3940927933673469, 'train/loss': 2.866691122249681, 'validation/accuracy': 0.35906, 'validation/loss': 3.0543703125, 'validation/num_examples': 50000, 'test/accuracy': 0.2687, 'test/loss': 3.6696234375, 'test/num_examples': 10000, 'score': 2045.2199194431305, 'total_duration': 2565.8486864566803, 'accumulated_submission_time': 2045.2199194431305, 'accumulated_eval_time': 516.7443010807037, 'accumulated_logging_time': 0.08250665664672852}
I0608 18:50:33.669418 140423779559168 logging_writer.py:48] [5473] accumulated_eval_time=516.744301, accumulated_logging_time=0.082507, accumulated_submission_time=2045.219919, global_step=5473, preemption_count=0, score=2045.219919, test/accuracy=0.268700, test/loss=3.669623, test/num_examples=10000, total_duration=2565.848686, train/accuracy=0.394093, train/loss=2.866691, validation/accuracy=0.359060, validation/loss=3.054370, validation/num_examples=50000
I0608 18:50:44.055901 140423771166464 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.680272, loss=4.461308
I0608 18:53:49.856825 140423779559168 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.666641, loss=4.325202
I0608 18:56:56.999860 140423771166464 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.631473, loss=4.122245
I0608 18:59:03.705561 140476240373568 spec.py:298] Evaluating on the training split.
I0608 18:59:47.748825 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 19:00:33.148741 140476240373568 spec.py:326] Evaluating on the test split.
I0608 19:00:34.510249 140476240373568 submission_runner.py:419] Time since start: 3166.70s, 	Step: 6842, 	{'train/accuracy': 0.4734733737244898, 'train/loss': 2.505651512924506, 'validation/accuracy': 0.43396, 'validation/loss': 2.693298125, 'validation/num_examples': 50000, 'test/accuracy': 0.3199, 'test/loss': 3.38362421875, 'test/num_examples': 10000, 'score': 2554.3588993549347, 'total_duration': 3166.7037839889526, 'accumulated_submission_time': 2554.3588993549347, 'accumulated_eval_time': 607.549102306366, 'accumulated_logging_time': 0.10478734970092773}
I0608 19:00:34.520664 140423779559168 logging_writer.py:48] [6842] accumulated_eval_time=607.549102, accumulated_logging_time=0.104787, accumulated_submission_time=2554.358899, global_step=6842, preemption_count=0, score=2554.358899, test/accuracy=0.319900, test/loss=3.383624, test/num_examples=10000, total_duration=3166.703784, train/accuracy=0.473473, train/loss=2.505652, validation/accuracy=0.433960, validation/loss=2.693298, validation/num_examples=50000
I0608 19:01:33.431125 140423771166464 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.591143, loss=4.126053
I0608 19:04:39.192737 140423779559168 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.603851, loss=4.124660
I0608 19:07:46.116749 140423771166464 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.595652, loss=3.962927
I0608 19:09:04.788744 140476240373568 spec.py:298] Evaluating on the training split.
I0608 19:09:48.673261 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 19:10:39.091776 140476240373568 spec.py:326] Evaluating on the test split.
I0608 19:10:40.454285 140476240373568 submission_runner.py:419] Time since start: 3772.65s, 	Step: 8213, 	{'train/accuracy': 0.5301339285714286, 'train/loss': 2.2454398018973216, 'validation/accuracy': 0.48484, 'validation/loss': 2.44819890625, 'validation/num_examples': 50000, 'test/accuracy': 0.3595, 'test/loss': 3.1488232421875, 'test/num_examples': 10000, 'score': 3063.7238051891327, 'total_duration': 3772.6478641033173, 'accumulated_submission_time': 3063.7238051891327, 'accumulated_eval_time': 703.2149121761322, 'accumulated_logging_time': 0.12306666374206543}
I0608 19:10:40.465025 140423779559168 logging_writer.py:48] [8213] accumulated_eval_time=703.214912, accumulated_logging_time=0.123067, accumulated_submission_time=3063.723805, global_step=8213, preemption_count=0, score=3063.723805, test/accuracy=0.359500, test/loss=3.148823, test/num_examples=10000, total_duration=3772.647864, train/accuracy=0.530134, train/loss=2.245440, validation/accuracy=0.484840, validation/loss=2.448199, validation/num_examples=50000
I0608 19:12:27.461076 140423771166464 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.563805, loss=3.889082
I0608 19:15:34.436508 140423779559168 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.525959, loss=3.861312
I0608 19:18:40.016135 140423771166464 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.509939, loss=3.754621
I0608 19:19:10.516034 140476240373568 spec.py:298] Evaluating on the training split.
I0608 19:19:54.679763 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 19:20:39.968982 140476240373568 spec.py:326] Evaluating on the test split.
I0608 19:20:41.327988 140476240373568 submission_runner.py:419] Time since start: 4373.52s, 	Step: 9583, 	{'train/accuracy': 0.5597696109693877, 'train/loss': 2.1285434645049426, 'validation/accuracy': 0.5114, 'validation/loss': 2.3616871875, 'validation/num_examples': 50000, 'test/accuracy': 0.3841, 'test/loss': 3.0491427734375, 'test/num_examples': 10000, 'score': 3572.8534750938416, 'total_duration': 4373.521535873413, 'accumulated_submission_time': 3572.8534750938416, 'accumulated_eval_time': 794.0268387794495, 'accumulated_logging_time': 0.14242124557495117}
I0608 19:20:41.339414 140423779559168 logging_writer.py:48] [9583] accumulated_eval_time=794.026839, accumulated_logging_time=0.142421, accumulated_submission_time=3572.853475, global_step=9583, preemption_count=0, score=3572.853475, test/accuracy=0.384100, test/loss=3.049143, test/num_examples=10000, total_duration=4373.521536, train/accuracy=0.559770, train/loss=2.128543, validation/accuracy=0.511400, validation/loss=2.361687, validation/num_examples=50000
I0608 19:23:16.384336 140423771166464 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.495884, loss=3.706973
I0608 19:26:23.276581 140423779559168 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.498917, loss=3.723395
I0608 19:29:11.618827 140476240373568 spec.py:298] Evaluating on the training split.
I0608 19:29:55.281100 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 19:30:49.636868 140476240373568 spec.py:326] Evaluating on the test split.
I0608 19:30:50.997621 140476240373568 submission_runner.py:419] Time since start: 4983.19s, 	Step: 10954, 	{'train/accuracy': 0.6045121173469388, 'train/loss': 1.9520126654177297, 'validation/accuracy': 0.55198, 'validation/loss': 2.1955259375, 'validation/num_examples': 50000, 'test/accuracy': 0.4238, 'test/loss': 2.8554158203125, 'test/num_examples': 10000, 'score': 4082.223005056381, 'total_duration': 4983.190026760101, 'accumulated_submission_time': 4082.223005056381, 'accumulated_eval_time': 893.4048180580139, 'accumulated_logging_time': 0.1631162166595459}
I0608 19:30:51.008114 140423771166464 logging_writer.py:48] [10954] accumulated_eval_time=893.404818, accumulated_logging_time=0.163116, accumulated_submission_time=4082.223005, global_step=10954, preemption_count=0, score=4082.223005, test/accuracy=0.423800, test/loss=2.855416, test/num_examples=10000, total_duration=4983.190027, train/accuracy=0.604512, train/loss=1.952013, validation/accuracy=0.551980, validation/loss=2.195526, validation/num_examples=50000
I0608 19:31:08.395795 140423779559168 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.503518, loss=3.605519
I0608 19:34:15.105609 140423771166464 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.467404, loss=3.510334
I0608 19:37:20.840512 140423779559168 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.482564, loss=3.533747
I0608 19:39:21.251316 140476240373568 spec.py:298] Evaluating on the training split.
I0608 19:40:05.566172 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 19:40:50.930992 140476240373568 spec.py:326] Evaluating on the test split.
I0608 19:40:52.290131 140476240373568 submission_runner.py:419] Time since start: 5584.48s, 	Step: 12325, 	{'train/accuracy': 0.6241828762755102, 'train/loss': 1.8086317412707271, 'validation/accuracy': 0.56704, 'validation/loss': 2.067254375, 'validation/num_examples': 50000, 'test/accuracy': 0.4432, 'test/loss': 2.73801875, 'test/num_examples': 10000, 'score': 4591.561492681503, 'total_duration': 5584.483610868454, 'accumulated_submission_time': 4591.561492681503, 'accumulated_eval_time': 984.4435458183289, 'accumulated_logging_time': 0.18165946006774902}
I0608 19:40:52.300987 140423771166464 logging_writer.py:48] [12325] accumulated_eval_time=984.443546, accumulated_logging_time=0.181659, accumulated_submission_time=4591.561493, global_step=12325, preemption_count=0, score=4591.561493, test/accuracy=0.443200, test/loss=2.738019, test/num_examples=10000, total_duration=5584.483611, train/accuracy=0.624183, train/loss=1.808632, validation/accuracy=0.567040, validation/loss=2.067254, validation/num_examples=50000
I0608 19:41:57.400688 140423779559168 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.479916, loss=3.558367
I0608 19:45:04.206334 140423771166464 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.482694, loss=3.545704
I0608 19:48:10.066523 140423779559168 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.459761, loss=3.448437
I0608 19:49:22.511710 140476240373568 spec.py:298] Evaluating on the training split.
I0608 19:50:06.807283 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 19:51:02.108447 140476240373568 spec.py:326] Evaluating on the test split.
I0608 19:51:03.467716 140476240373568 submission_runner.py:419] Time since start: 6195.66s, 	Step: 13696, 	{'train/accuracy': 0.6643415178571429, 'train/loss': 1.5976434824418049, 'validation/accuracy': 0.59848, 'validation/loss': 1.8848571875, 'validation/num_examples': 50000, 'test/accuracy': 0.4717, 'test/loss': 2.5554984375, 'test/num_examples': 10000, 'score': 5100.866478443146, 'total_duration': 6195.659802436829, 'accumulated_submission_time': 5100.866478443146, 'accumulated_eval_time': 1085.3982436656952, 'accumulated_logging_time': 0.20180726051330566}
I0608 19:51:03.478813 140423771166464 logging_writer.py:48] [13696] accumulated_eval_time=1085.398244, accumulated_logging_time=0.201807, accumulated_submission_time=5100.866478, global_step=13696, preemption_count=0, score=5100.866478, test/accuracy=0.471700, test/loss=2.555498, test/num_examples=10000, total_duration=6195.659802, train/accuracy=0.664342, train/loss=1.597643, validation/accuracy=0.598480, validation/loss=1.884857, validation/num_examples=50000
I0608 19:52:57.636226 140423779559168 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.450491, loss=3.397527
I0608 19:56:03.326752 140423771166464 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.459521, loss=3.411148
I0608 19:59:09.085475 140423779559168 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.459154, loss=3.359224
I0608 19:59:33.709962 140476240373568 spec.py:298] Evaluating on the training split.
I0608 20:00:17.487420 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 20:01:05.059402 140476240373568 spec.py:326] Evaluating on the test split.
I0608 20:01:06.417359 140476240373568 submission_runner.py:419] Time since start: 6798.61s, 	Step: 15063, 	{'train/accuracy': 0.6731305803571429, 'train/loss': 1.590261965381856, 'validation/accuracy': 0.60782, 'validation/loss': 1.891286875, 'validation/num_examples': 50000, 'test/accuracy': 0.4743, 'test/loss': 2.6013884765625, 'test/num_examples': 10000, 'score': 5610.198609828949, 'total_duration': 6798.610905647278, 'accumulated_submission_time': 5610.198609828949, 'accumulated_eval_time': 1178.1057260036469, 'accumulated_logging_time': 0.22146821022033691}
I0608 20:01:06.429404 140423771166464 logging_writer.py:48] [15063] accumulated_eval_time=1178.105726, accumulated_logging_time=0.221468, accumulated_submission_time=5610.198610, global_step=15063, preemption_count=0, score=5610.198610, test/accuracy=0.474300, test/loss=2.601388, test/num_examples=10000, total_duration=6798.610906, train/accuracy=0.673131, train/loss=1.590262, validation/accuracy=0.607820, validation/loss=1.891287, validation/num_examples=50000
I0608 20:03:48.874499 140423779559168 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.456501, loss=3.336493
I0608 20:06:54.669446 140423771166464 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.448289, loss=3.335743
I0608 20:09:36.778662 140476240373568 spec.py:298] Evaluating on the training split.
I0608 20:10:20.701747 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 20:11:10.964256 140476240373568 spec.py:326] Evaluating on the test split.
I0608 20:11:12.324071 140476240373568 submission_runner.py:419] Time since start: 7404.52s, 	Step: 16434, 	{'train/accuracy': 0.6963089923469388, 'train/loss': 1.4172269860092475, 'validation/accuracy': 0.6221, 'validation/loss': 1.74039015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4872, 'test/loss': 2.4283359375, 'test/num_examples': 10000, 'score': 6119.646467208862, 'total_duration': 7404.517594337463, 'accumulated_submission_time': 6119.646467208862, 'accumulated_eval_time': 1273.651251077652, 'accumulated_logging_time': 0.24190974235534668}
I0608 20:11:12.335563 140423779559168 logging_writer.py:48] [16434] accumulated_eval_time=1273.651251, accumulated_logging_time=0.241910, accumulated_submission_time=6119.646467, global_step=16434, preemption_count=0, score=6119.646467, test/accuracy=0.487200, test/loss=2.428336, test/num_examples=10000, total_duration=7404.517594, train/accuracy=0.696309, train/loss=1.417227, validation/accuracy=0.622100, validation/loss=1.740390, validation/num_examples=50000
I0608 20:11:37.064471 140423771166464 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.475622, loss=3.382886
I0608 20:14:42.662186 140423779559168 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.457640, loss=3.309926
I0608 20:17:48.270770 140423771166464 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.450293, loss=3.311526
I0608 20:19:42.410138 140476240373568 spec.py:298] Evaluating on the training split.
I0608 20:20:26.495373 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 20:21:18.672648 140476240373568 spec.py:326] Evaluating on the test split.
I0608 20:21:20.029450 140476240373568 submission_runner.py:419] Time since start: 8012.22s, 	Step: 17805, 	{'train/accuracy': 0.7056560905612245, 'train/loss': 1.5359775776765785, 'validation/accuracy': 0.63284, 'validation/loss': 1.8530328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4952, 'test/loss': 2.5269599609375, 'test/num_examples': 10000, 'score': 6628.824547767639, 'total_duration': 8012.222981452942, 'accumulated_submission_time': 6628.824547767639, 'accumulated_eval_time': 1371.2705116271973, 'accumulated_logging_time': 0.2622663974761963}
I0608 20:21:20.040266 140423779559168 logging_writer.py:48] [17805] accumulated_eval_time=1371.270512, accumulated_logging_time=0.262266, accumulated_submission_time=6628.824548, global_step=17805, preemption_count=0, score=6628.824548, test/accuracy=0.495200, test/loss=2.526960, test/num_examples=10000, total_duration=8012.222981, train/accuracy=0.705656, train/loss=1.535978, validation/accuracy=0.632840, validation/loss=1.853033, validation/num_examples=50000
I0608 20:22:32.656654 140423771166464 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.446613, loss=3.301206
I0608 20:25:38.374276 140423779559168 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.454189, loss=3.308501
I0608 20:28:45.209130 140423771166464 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.448707, loss=3.254702
I0608 20:29:50.084929 140476240373568 spec.py:298] Evaluating on the training split.
I0608 20:30:35.114737 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 20:31:23.222109 140476240373568 spec.py:326] Evaluating on the test split.
I0608 20:31:24.578825 140476240373568 submission_runner.py:419] Time since start: 8616.77s, 	Step: 19176, 	{'train/accuracy': 0.7154017857142857, 'train/loss': 1.4138118198939733, 'validation/accuracy': 0.63626, 'validation/loss': 1.7656065625, 'validation/num_examples': 50000, 'test/accuracy': 0.4999, 'test/loss': 2.4738638671875, 'test/num_examples': 10000, 'score': 7137.965769529343, 'total_duration': 8616.772367477417, 'accumulated_submission_time': 7137.965769529343, 'accumulated_eval_time': 1465.7643563747406, 'accumulated_logging_time': 0.2820303440093994}
I0608 20:31:24.589854 140423779559168 logging_writer.py:48] [19176] accumulated_eval_time=1465.764356, accumulated_logging_time=0.282030, accumulated_submission_time=7137.965770, global_step=19176, preemption_count=0, score=7137.965770, test/accuracy=0.499900, test/loss=2.473864, test/num_examples=10000, total_duration=8616.772367, train/accuracy=0.715402, train/loss=1.413812, validation/accuracy=0.636260, validation/loss=1.765607, validation/num_examples=50000
I0608 20:33:25.091813 140423771166464 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.432010, loss=3.192442
I0608 20:36:30.706665 140423779559168 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.447759, loss=3.220550
I0608 20:39:37.866607 140423771166464 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.449247, loss=3.271382
I0608 20:39:54.937813 140476240373568 spec.py:298] Evaluating on the training split.
I0608 20:40:38.947610 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 20:41:31.424619 140476240373568 spec.py:326] Evaluating on the test split.
I0608 20:41:32.782432 140476240373568 submission_runner.py:419] Time since start: 9224.98s, 	Step: 20547, 	{'train/accuracy': 0.7210220025510204, 'train/loss': 1.3331628916214924, 'validation/accuracy': 0.63704, 'validation/loss': 1.7039775, 'validation/num_examples': 50000, 'test/accuracy': 0.5022, 'test/loss': 2.421622265625, 'test/num_examples': 10000, 'score': 7647.411053419113, 'total_duration': 9224.97600722313, 'accumulated_submission_time': 7647.411053419113, 'accumulated_eval_time': 1563.6089434623718, 'accumulated_logging_time': 0.3015017509460449}
I0608 20:41:32.795413 140423779559168 logging_writer.py:48] [20547] accumulated_eval_time=1563.608943, accumulated_logging_time=0.301502, accumulated_submission_time=7647.411053, global_step=20547, preemption_count=0, score=7647.411053, test/accuracy=0.502200, test/loss=2.421622, test/num_examples=10000, total_duration=9224.976007, train/accuracy=0.721022, train/loss=1.333163, validation/accuracy=0.637040, validation/loss=1.703977, validation/num_examples=50000
I0608 20:44:21.268815 140423771166464 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.459632, loss=3.214012
I0608 20:47:28.319145 140423779559168 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.448968, loss=3.256552
I0608 20:50:03.030010 140476240373568 spec.py:298] Evaluating on the training split.
I0608 20:50:47.800648 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 20:51:36.434384 140476240373568 spec.py:326] Evaluating on the test split.
I0608 20:51:37.791552 140476240373568 submission_runner.py:419] Time since start: 9829.99s, 	Step: 21918, 	{'train/accuracy': 0.7386001275510204, 'train/loss': 1.2786855892259248, 'validation/accuracy': 0.6514, 'validation/loss': 1.657015, 'validation/num_examples': 50000, 'test/accuracy': 0.5155, 'test/loss': 2.362696484375, 'test/num_examples': 10000, 'score': 8156.740550041199, 'total_duration': 9829.985138893127, 'accumulated_submission_time': 8156.740550041199, 'accumulated_eval_time': 1658.3708853721619, 'accumulated_logging_time': 0.32253003120422363}
I0608 20:51:37.802887 140423771166464 logging_writer.py:48] [21918] accumulated_eval_time=1658.370885, accumulated_logging_time=0.322530, accumulated_submission_time=8156.740550, global_step=21918, preemption_count=0, score=8156.740550, test/accuracy=0.515500, test/loss=2.362696, test/num_examples=10000, total_duration=9829.985139, train/accuracy=0.738600, train/loss=1.278686, validation/accuracy=0.651400, validation/loss=1.657015, validation/num_examples=50000
I0608 20:52:08.529287 140423779559168 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.438451, loss=3.201506
I0608 20:55:13.995729 140423771166464 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.430959, loss=3.191872
I0608 20:58:20.943607 140423779559168 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.432049, loss=3.164744
I0608 21:00:07.880359 140476240373568 spec.py:298] Evaluating on the training split.
I0608 21:00:52.312707 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 21:01:39.568014 140476240373568 spec.py:326] Evaluating on the test split.
I0608 21:01:40.926805 140476240373568 submission_runner.py:419] Time since start: 10433.12s, 	Step: 23289, 	{'train/accuracy': 0.7411710778061225, 'train/loss': 1.2924671562350527, 'validation/accuracy': 0.65628, 'validation/loss': 1.67214859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5252, 'test/loss': 2.32922265625, 'test/num_examples': 10000, 'score': 8665.9090051651, 'total_duration': 10433.12037730217, 'accumulated_submission_time': 8665.9090051651, 'accumulated_eval_time': 1751.4173650741577, 'accumulated_logging_time': 0.342113733291626}
I0608 21:01:40.937704 140423771166464 logging_writer.py:48] [23289] accumulated_eval_time=1751.417365, accumulated_logging_time=0.342114, accumulated_submission_time=8665.909005, global_step=23289, preemption_count=0, score=8665.909005, test/accuracy=0.525200, test/loss=2.329223, test/num_examples=10000, total_duration=10433.120377, train/accuracy=0.741171, train/loss=1.292467, validation/accuracy=0.656280, validation/loss=1.672149, validation/num_examples=50000
I0608 21:02:59.515871 140423779559168 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.430579, loss=3.143257
I0608 21:06:06.257851 140423771166464 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.456389, loss=3.121894
I0608 21:09:11.831138 140423779559168 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.432508, loss=3.212009
I0608 21:10:10.934303 140476240373568 spec.py:298] Evaluating on the training split.
I0608 21:10:56.059177 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 21:11:44.489950 140476240373568 spec.py:326] Evaluating on the test split.
I0608 21:11:45.847871 140476240373568 submission_runner.py:419] Time since start: 11038.04s, 	Step: 24660, 	{'train/accuracy': 0.7581114477040817, 'train/loss': 1.1910621487364477, 'validation/accuracy': 0.6677, 'validation/loss': 1.5869803125, 'validation/num_examples': 50000, 'test/accuracy': 0.5286, 'test/loss': 2.305275390625, 'test/num_examples': 10000, 'score': 9175.009410142899, 'total_duration': 11038.04137635231, 'accumulated_submission_time': 9175.009410142899, 'accumulated_eval_time': 1846.3311083316803, 'accumulated_logging_time': 0.3610191345214844}
I0608 21:11:45.859689 140423771166464 logging_writer.py:48] [24660] accumulated_eval_time=1846.331108, accumulated_logging_time=0.361019, accumulated_submission_time=9175.009410, global_step=24660, preemption_count=0, score=9175.009410, test/accuracy=0.528600, test/loss=2.305275, test/num_examples=10000, total_duration=11038.041376, train/accuracy=0.758111, train/loss=1.191062, validation/accuracy=0.667700, validation/loss=1.586980, validation/num_examples=50000
I0608 21:13:52.145952 140423779559168 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.433720, loss=3.154424
I0608 21:16:59.183912 140423771166464 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.446937, loss=3.192010
I0608 21:20:04.839510 140423779559168 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.447363, loss=3.147486
I0608 21:20:16.001505 140476240373568 spec.py:298] Evaluating on the training split.
I0608 21:21:00.199170 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 21:21:53.973128 140476240373568 spec.py:326] Evaluating on the test split.
I0608 21:21:55.331093 140476240373568 submission_runner.py:419] Time since start: 11647.52s, 	Step: 26031, 	{'train/accuracy': 0.7624561543367347, 'train/loss': 1.2241501321597976, 'validation/accuracy': 0.66708, 'validation/loss': 1.63809359375, 'validation/num_examples': 50000, 'test/accuracy': 0.5267, 'test/loss': 2.3390865234375, 'test/num_examples': 10000, 'score': 9684.243774414062, 'total_duration': 11647.523039579391, 'accumulated_submission_time': 9684.243774414062, 'accumulated_eval_time': 1945.659033536911, 'accumulated_logging_time': 0.3807854652404785}
I0608 21:21:55.341714 140423771166464 logging_writer.py:48] [26031] accumulated_eval_time=1945.659034, accumulated_logging_time=0.380785, accumulated_submission_time=9684.243774, global_step=26031, preemption_count=0, score=9684.243774, test/accuracy=0.526700, test/loss=2.339087, test/num_examples=10000, total_duration=11647.523040, train/accuracy=0.762456, train/loss=1.224150, validation/accuracy=0.667080, validation/loss=1.638094, validation/num_examples=50000
I0608 21:24:51.053039 140423779559168 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.456717, loss=3.128463
I0608 21:27:56.549341 140423771166464 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.440308, loss=3.132137
I0608 21:30:25.407600 140476240373568 spec.py:298] Evaluating on the training split.
I0608 21:31:09.932454 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 21:31:58.257743 140476240373568 spec.py:326] Evaluating on the test split.
I0608 21:31:59.616445 140476240373568 submission_runner.py:419] Time since start: 12251.81s, 	Step: 27402, 	{'train/accuracy': 0.7621572066326531, 'train/loss': 1.174157123176419, 'validation/accuracy': 0.66488, 'validation/loss': 1.59800859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5389, 'test/loss': 2.2809947265625, 'test/num_examples': 10000, 'score': 10193.39817905426, 'total_duration': 12251.810008764267, 'accumulated_submission_time': 10193.39817905426, 'accumulated_eval_time': 2039.867930173874, 'accumulated_logging_time': 0.39975643157958984}
I0608 21:31:59.627348 140423779559168 logging_writer.py:48] [27402] accumulated_eval_time=2039.867930, accumulated_logging_time=0.399756, accumulated_submission_time=10193.398179, global_step=27402, preemption_count=0, score=10193.398179, test/accuracy=0.538900, test/loss=2.280995, test/num_examples=10000, total_duration=12251.810009, train/accuracy=0.762157, train/loss=1.174157, validation/accuracy=0.664880, validation/loss=1.598009, validation/num_examples=50000
I0608 21:32:36.175024 140423771166464 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.435119, loss=3.043337
I0608 21:35:42.643074 140476240373568 spec.py:298] Evaluating on the training split.
I0608 21:36:26.710638 140476240373568 spec.py:310] Evaluating on the validation split.
I0608 21:37:12.760294 140476240373568 spec.py:326] Evaluating on the test split.
I0608 21:37:14.119943 140476240373568 submission_runner.py:419] Time since start: 12566.31s, 	Step: 28000, 	{'train/accuracy': 0.7790377869897959, 'train/loss': 1.1142694901446908, 'validation/accuracy': 0.67686, 'validation/loss': 1.551265, 'validation/num_examples': 50000, 'test/accuracy': 0.5366, 'test/loss': 2.2747939453125, 'test/num_examples': 10000, 'score': 10416.01683473587, 'total_duration': 12566.313518762589, 'accumulated_submission_time': 10416.01683473587, 'accumulated_eval_time': 2131.345008611679, 'accumulated_logging_time': 0.41965293884277344}
I0608 21:37:14.132063 140423779559168 logging_writer.py:48] [28000] accumulated_eval_time=2131.345009, accumulated_logging_time=0.419653, accumulated_submission_time=10416.016835, global_step=28000, preemption_count=0, score=10416.016835, test/accuracy=0.536600, test/loss=2.274794, test/num_examples=10000, total_duration=12566.313519, train/accuracy=0.779038, train/loss=1.114269, validation/accuracy=0.676860, validation/loss=1.551265, validation/num_examples=50000
I0608 21:37:14.150207 140423771166464 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10416.016835
I0608 21:37:14.689352 140476240373568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/momentum/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0608 21:37:14.942417 140476240373568 submission_runner.py:581] Tuning trial 1/1
I0608 21:37:14.942643 140476240373568 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0608 21:37:14.943682 140476240373568 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012157206632653062, 'train/loss': 6.920588279257015, 'validation/accuracy': 0.0013, 'validation/loss': 6.922508125, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.92375, 'test/num_examples': 10000, 'score': 8.005820274353027, 'total_duration': 135.03247570991516, 'accumulated_submission_time': 8.005820274353027, 'accumulated_eval_time': 127.02614879608154, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1366, {'train/accuracy': 0.06353635204081633, 'train/loss': 5.57986948441486, 'validation/accuracy': 0.0593, 'validation/loss': 5.640805625, 'validation/num_examples': 50000, 'test/accuracy': 0.0384, 'test/loss': 5.870466015625, 'test/num_examples': 10000, 'score': 517.1433017253876, 'total_duration': 743.0044584274292, 'accumulated_submission_time': 517.1433017253876, 'accumulated_eval_time': 224.7544801235199, 'accumulated_logging_time': 0.026584863662719727, 'global_step': 1366, 'preemption_count': 0}), (2735, {'train/accuracy': 0.15361926020408162, 'train/loss': 4.597867070412149, 'validation/accuracy': 0.13774, 'validation/loss': 4.6978903125, 'validation/num_examples': 50000, 'test/accuracy': 0.0934, 'test/loss': 5.1442328125, 'test/num_examples': 10000, 'score': 1026.578690290451, 'total_duration': 1355.0625894069672, 'accumulated_submission_time': 1026.578690290451, 'accumulated_eval_time': 326.4483983516693, 'accumulated_logging_time': 0.04493093490600586, 'global_step': 2735, 'preemption_count': 0}), (4103, {'train/accuracy': 0.2780811543367347, 'train/loss': 3.5910252162388394, 'validation/accuracy': 0.2565, 'validation/loss': 3.7313278125, 'validation/num_examples': 50000, 'test/accuracy': 0.1862, 'test/loss': 4.28628046875, 'test/num_examples': 10000, 'score': 1535.906127691269, 'total_duration': 1954.9497203826904, 'accumulated_submission_time': 1535.906127691269, 'accumulated_eval_time': 416.0694487094879, 'accumulated_logging_time': 0.06348371505737305, 'global_step': 4103, 'preemption_count': 0}), (5473, {'train/accuracy': 0.3940927933673469, 'train/loss': 2.866691122249681, 'validation/accuracy': 0.35906, 'validation/loss': 3.0543703125, 'validation/num_examples': 50000, 'test/accuracy': 0.2687, 'test/loss': 3.6696234375, 'test/num_examples': 10000, 'score': 2045.2199194431305, 'total_duration': 2565.8486864566803, 'accumulated_submission_time': 2045.2199194431305, 'accumulated_eval_time': 516.7443010807037, 'accumulated_logging_time': 0.08250665664672852, 'global_step': 5473, 'preemption_count': 0}), (6842, {'train/accuracy': 0.4734733737244898, 'train/loss': 2.505651512924506, 'validation/accuracy': 0.43396, 'validation/loss': 2.693298125, 'validation/num_examples': 50000, 'test/accuracy': 0.3199, 'test/loss': 3.38362421875, 'test/num_examples': 10000, 'score': 2554.3588993549347, 'total_duration': 3166.7037839889526, 'accumulated_submission_time': 2554.3588993549347, 'accumulated_eval_time': 607.549102306366, 'accumulated_logging_time': 0.10478734970092773, 'global_step': 6842, 'preemption_count': 0}), (8213, {'train/accuracy': 0.5301339285714286, 'train/loss': 2.2454398018973216, 'validation/accuracy': 0.48484, 'validation/loss': 2.44819890625, 'validation/num_examples': 50000, 'test/accuracy': 0.3595, 'test/loss': 3.1488232421875, 'test/num_examples': 10000, 'score': 3063.7238051891327, 'total_duration': 3772.6478641033173, 'accumulated_submission_time': 3063.7238051891327, 'accumulated_eval_time': 703.2149121761322, 'accumulated_logging_time': 0.12306666374206543, 'global_step': 8213, 'preemption_count': 0}), (9583, {'train/accuracy': 0.5597696109693877, 'train/loss': 2.1285434645049426, 'validation/accuracy': 0.5114, 'validation/loss': 2.3616871875, 'validation/num_examples': 50000, 'test/accuracy': 0.3841, 'test/loss': 3.0491427734375, 'test/num_examples': 10000, 'score': 3572.8534750938416, 'total_duration': 4373.521535873413, 'accumulated_submission_time': 3572.8534750938416, 'accumulated_eval_time': 794.0268387794495, 'accumulated_logging_time': 0.14242124557495117, 'global_step': 9583, 'preemption_count': 0}), (10954, {'train/accuracy': 0.6045121173469388, 'train/loss': 1.9520126654177297, 'validation/accuracy': 0.55198, 'validation/loss': 2.1955259375, 'validation/num_examples': 50000, 'test/accuracy': 0.4238, 'test/loss': 2.8554158203125, 'test/num_examples': 10000, 'score': 4082.223005056381, 'total_duration': 4983.190026760101, 'accumulated_submission_time': 4082.223005056381, 'accumulated_eval_time': 893.4048180580139, 'accumulated_logging_time': 0.1631162166595459, 'global_step': 10954, 'preemption_count': 0}), (12325, {'train/accuracy': 0.6241828762755102, 'train/loss': 1.8086317412707271, 'validation/accuracy': 0.56704, 'validation/loss': 2.067254375, 'validation/num_examples': 50000, 'test/accuracy': 0.4432, 'test/loss': 2.73801875, 'test/num_examples': 10000, 'score': 4591.561492681503, 'total_duration': 5584.483610868454, 'accumulated_submission_time': 4591.561492681503, 'accumulated_eval_time': 984.4435458183289, 'accumulated_logging_time': 0.18165946006774902, 'global_step': 12325, 'preemption_count': 0}), (13696, {'train/accuracy': 0.6643415178571429, 'train/loss': 1.5976434824418049, 'validation/accuracy': 0.59848, 'validation/loss': 1.8848571875, 'validation/num_examples': 50000, 'test/accuracy': 0.4717, 'test/loss': 2.5554984375, 'test/num_examples': 10000, 'score': 5100.866478443146, 'total_duration': 6195.659802436829, 'accumulated_submission_time': 5100.866478443146, 'accumulated_eval_time': 1085.3982436656952, 'accumulated_logging_time': 0.20180726051330566, 'global_step': 13696, 'preemption_count': 0}), (15063, {'train/accuracy': 0.6731305803571429, 'train/loss': 1.590261965381856, 'validation/accuracy': 0.60782, 'validation/loss': 1.891286875, 'validation/num_examples': 50000, 'test/accuracy': 0.4743, 'test/loss': 2.6013884765625, 'test/num_examples': 10000, 'score': 5610.198609828949, 'total_duration': 6798.610905647278, 'accumulated_submission_time': 5610.198609828949, 'accumulated_eval_time': 1178.1057260036469, 'accumulated_logging_time': 0.22146821022033691, 'global_step': 15063, 'preemption_count': 0}), (16434, {'train/accuracy': 0.6963089923469388, 'train/loss': 1.4172269860092475, 'validation/accuracy': 0.6221, 'validation/loss': 1.74039015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4872, 'test/loss': 2.4283359375, 'test/num_examples': 10000, 'score': 6119.646467208862, 'total_duration': 7404.517594337463, 'accumulated_submission_time': 6119.646467208862, 'accumulated_eval_time': 1273.651251077652, 'accumulated_logging_time': 0.24190974235534668, 'global_step': 16434, 'preemption_count': 0}), (17805, {'train/accuracy': 0.7056560905612245, 'train/loss': 1.5359775776765785, 'validation/accuracy': 0.63284, 'validation/loss': 1.8530328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4952, 'test/loss': 2.5269599609375, 'test/num_examples': 10000, 'score': 6628.824547767639, 'total_duration': 8012.222981452942, 'accumulated_submission_time': 6628.824547767639, 'accumulated_eval_time': 1371.2705116271973, 'accumulated_logging_time': 0.2622663974761963, 'global_step': 17805, 'preemption_count': 0}), (19176, {'train/accuracy': 0.7154017857142857, 'train/loss': 1.4138118198939733, 'validation/accuracy': 0.63626, 'validation/loss': 1.7656065625, 'validation/num_examples': 50000, 'test/accuracy': 0.4999, 'test/loss': 2.4738638671875, 'test/num_examples': 10000, 'score': 7137.965769529343, 'total_duration': 8616.772367477417, 'accumulated_submission_time': 7137.965769529343, 'accumulated_eval_time': 1465.7643563747406, 'accumulated_logging_time': 0.2820303440093994, 'global_step': 19176, 'preemption_count': 0}), (20547, {'train/accuracy': 0.7210220025510204, 'train/loss': 1.3331628916214924, 'validation/accuracy': 0.63704, 'validation/loss': 1.7039775, 'validation/num_examples': 50000, 'test/accuracy': 0.5022, 'test/loss': 2.421622265625, 'test/num_examples': 10000, 'score': 7647.411053419113, 'total_duration': 9224.97600722313, 'accumulated_submission_time': 7647.411053419113, 'accumulated_eval_time': 1563.6089434623718, 'accumulated_logging_time': 0.3015017509460449, 'global_step': 20547, 'preemption_count': 0}), (21918, {'train/accuracy': 0.7386001275510204, 'train/loss': 1.2786855892259248, 'validation/accuracy': 0.6514, 'validation/loss': 1.657015, 'validation/num_examples': 50000, 'test/accuracy': 0.5155, 'test/loss': 2.362696484375, 'test/num_examples': 10000, 'score': 8156.740550041199, 'total_duration': 9829.985138893127, 'accumulated_submission_time': 8156.740550041199, 'accumulated_eval_time': 1658.3708853721619, 'accumulated_logging_time': 0.32253003120422363, 'global_step': 21918, 'preemption_count': 0}), (23289, {'train/accuracy': 0.7411710778061225, 'train/loss': 1.2924671562350527, 'validation/accuracy': 0.65628, 'validation/loss': 1.67214859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5252, 'test/loss': 2.32922265625, 'test/num_examples': 10000, 'score': 8665.9090051651, 'total_duration': 10433.12037730217, 'accumulated_submission_time': 8665.9090051651, 'accumulated_eval_time': 1751.4173650741577, 'accumulated_logging_time': 0.342113733291626, 'global_step': 23289, 'preemption_count': 0}), (24660, {'train/accuracy': 0.7581114477040817, 'train/loss': 1.1910621487364477, 'validation/accuracy': 0.6677, 'validation/loss': 1.5869803125, 'validation/num_examples': 50000, 'test/accuracy': 0.5286, 'test/loss': 2.305275390625, 'test/num_examples': 10000, 'score': 9175.009410142899, 'total_duration': 11038.04137635231, 'accumulated_submission_time': 9175.009410142899, 'accumulated_eval_time': 1846.3311083316803, 'accumulated_logging_time': 0.3610191345214844, 'global_step': 24660, 'preemption_count': 0}), (26031, {'train/accuracy': 0.7624561543367347, 'train/loss': 1.2241501321597976, 'validation/accuracy': 0.66708, 'validation/loss': 1.63809359375, 'validation/num_examples': 50000, 'test/accuracy': 0.5267, 'test/loss': 2.3390865234375, 'test/num_examples': 10000, 'score': 9684.243774414062, 'total_duration': 11647.523039579391, 'accumulated_submission_time': 9684.243774414062, 'accumulated_eval_time': 1945.659033536911, 'accumulated_logging_time': 0.3807854652404785, 'global_step': 26031, 'preemption_count': 0}), (27402, {'train/accuracy': 0.7621572066326531, 'train/loss': 1.174157123176419, 'validation/accuracy': 0.66488, 'validation/loss': 1.59800859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5389, 'test/loss': 2.2809947265625, 'test/num_examples': 10000, 'score': 10193.39817905426, 'total_duration': 12251.810008764267, 'accumulated_submission_time': 10193.39817905426, 'accumulated_eval_time': 2039.867930173874, 'accumulated_logging_time': 0.39975643157958984, 'global_step': 27402, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7790377869897959, 'train/loss': 1.1142694901446908, 'validation/accuracy': 0.67686, 'validation/loss': 1.551265, 'validation/num_examples': 50000, 'test/accuracy': 0.5366, 'test/loss': 2.2747939453125, 'test/num_examples': 10000, 'score': 10416.01683473587, 'total_duration': 12566.313518762589, 'accumulated_submission_time': 10416.01683473587, 'accumulated_eval_time': 2131.345008611679, 'accumulated_logging_time': 0.41965293884277344, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0608 21:37:14.943815 140476240373568 submission_runner.py:584] Timing: 10416.01683473587
I0608 21:37:14.943873 140476240373568 submission_runner.py:586] Total number of evals: 22
I0608 21:37:14.943924 140476240373568 submission_runner.py:587] ====================
I0608 21:37:14.944053 140476240373568 submission_runner.py:655] Final imagenet_resnet score: 10416.01683473587
