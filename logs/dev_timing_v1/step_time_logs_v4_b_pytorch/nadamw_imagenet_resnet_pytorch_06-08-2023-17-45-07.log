torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-08-2023-17-45-07.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 17:45:30.894415 140508368914240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 17:45:30.894466 140610813785920 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 17:45:30.894508 139864351430464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 17:45:30.895552 140139145631552 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 17:45:30.895590 140719506016064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 17:45:30.895611 139657727321920 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 17:45:30.895636 140392771966784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 17:45:30.906270 140139145631552 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.906305 139657727321920 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.906327 140719506016064 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.906351 140392771966784 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.906284 140625997584192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 17:45:30.906593 140625997584192 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.915361 140610813785920 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.915389 140508368914240 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:30.915423 139864351430464 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 17:45:33.252156 140625997584192 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nadamw/imagenet_resnet_pytorch.
W0608 17:45:33.300642 139864351430464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.300847 140719506016064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.301032 140625997584192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.301166 140392771966784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.301327 140508368914240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.301808 140610813785920 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.302153 139657727321920 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 17:45:33.303056 140139145631552 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 17:45:33.306519 140625997584192 submission_runner.py:541] Using RNG seed 3238367716
I0608 17:45:33.307775 140625997584192 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 17:45:33.307884 140625997584192 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nadamw/imagenet_resnet_pytorch/trial_1.
I0608 17:45:33.308511 140625997584192 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0608 17:45:33.309674 140625997584192 submission_runner.py:255] Initializing dataset.
I0608 17:45:45.151355 140625997584192 submission_runner.py:262] Initializing model.
I0608 17:45:49.675982 140625997584192 submission_runner.py:272] Initializing optimizer.
I0608 17:45:49.677338 140625997584192 submission_runner.py:279] Initializing metrics bundle.
I0608 17:45:49.677438 140625997584192 submission_runner.py:297] Initializing checkpoint and logger.
I0608 17:45:50.207496 140625997584192 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0608 17:45:50.208376 140625997584192 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0608 17:45:50.256116 140625997584192 submission_runner.py:332] Starting training loop.
I0608 17:45:58.465150 140596853335808 logging_writer.py:48] [0] global_step=0, grad_norm=0.582672, loss=6.926615
I0608 17:45:58.562002 140625997584192 submission.py:296] 0) loss = 6.927, grad_norm = 0.583
I0608 17:45:58.563377 140625997584192 spec.py:298] Evaluating on the training split.
I0608 17:46:58.425842 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 17:47:53.699454 140625997584192 spec.py:326] Evaluating on the test split.
I0608 17:47:53.719490 140625997584192 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 17:47:53.726509 140625997584192 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 17:47:53.807616 140625997584192 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 17:48:05.648190 140625997584192 submission_runner.py:419] Time since start: 135.39s, 	Step: 1, 	{'train/accuracy': 0.0008370535714285714, 'train/loss': 6.924691336495536, 'validation/accuracy': 0.00084, 'validation/loss': 6.925395, 'validation/num_examples': 50000, 'test/accuracy': 0.0013, 'test/loss': 6.9280890625, 'test/num_examples': 10000, 'score': 8.307104110717773, 'total_duration': 135.3924424648285, 'accumulated_submission_time': 8.307104110717773, 'accumulated_eval_time': 127.0849061012268, 'accumulated_logging_time': 0}
I0608 17:48:05.665313 140573432325888 logging_writer.py:48] [1] accumulated_eval_time=127.084906, accumulated_logging_time=0, accumulated_submission_time=8.307104, global_step=1, preemption_count=0, score=8.307104, test/accuracy=0.001300, test/loss=6.928089, test/num_examples=10000, total_duration=135.392442, train/accuracy=0.000837, train/loss=6.924691, validation/accuracy=0.000840, validation/loss=6.925395, validation/num_examples=50000
I0608 17:48:05.684778 140625997584192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.685902 140719506016064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.685942 140139145631552 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.685919 139864351430464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.685976 140610813785920 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.686002 140508368914240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.686243 139657727321920 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:05.686626 140392771966784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 17:48:06.083020 140573423933184 logging_writer.py:48] [1] global_step=1, grad_norm=0.608818, loss=6.921320
I0608 17:48:06.086581 140625997584192 submission.py:296] 1) loss = 6.921, grad_norm = 0.609
I0608 17:48:06.475462 140573432325888 logging_writer.py:48] [2] global_step=2, grad_norm=0.605481, loss=6.928836
I0608 17:48:06.480158 140625997584192 submission.py:296] 2) loss = 6.929, grad_norm = 0.605
I0608 17:48:06.872841 140573423933184 logging_writer.py:48] [3] global_step=3, grad_norm=0.609076, loss=6.928844
I0608 17:48:06.876723 140625997584192 submission.py:296] 3) loss = 6.929, grad_norm = 0.609
I0608 17:48:07.274974 140573432325888 logging_writer.py:48] [4] global_step=4, grad_norm=0.593440, loss=6.926389
I0608 17:48:07.279002 140625997584192 submission.py:296] 4) loss = 6.926, grad_norm = 0.593
I0608 17:48:07.678713 140573423933184 logging_writer.py:48] [5] global_step=5, grad_norm=0.598487, loss=6.925918
I0608 17:48:07.683868 140625997584192 submission.py:296] 5) loss = 6.926, grad_norm = 0.598
I0608 17:48:08.083776 140573432325888 logging_writer.py:48] [6] global_step=6, grad_norm=0.602209, loss=6.919436
I0608 17:48:08.087878 140625997584192 submission.py:296] 6) loss = 6.919, grad_norm = 0.602
I0608 17:48:08.482307 140573423933184 logging_writer.py:48] [7] global_step=7, grad_norm=0.608278, loss=6.923409
I0608 17:48:08.486593 140625997584192 submission.py:296] 7) loss = 6.923, grad_norm = 0.608
I0608 17:48:08.880940 140573432325888 logging_writer.py:48] [8] global_step=8, grad_norm=0.596234, loss=6.911128
I0608 17:48:08.888593 140625997584192 submission.py:296] 8) loss = 6.911, grad_norm = 0.596
I0608 17:48:09.283102 140573423933184 logging_writer.py:48] [9] global_step=9, grad_norm=0.622759, loss=6.929622
I0608 17:48:09.287552 140625997584192 submission.py:296] 9) loss = 6.930, grad_norm = 0.623
I0608 17:48:09.689870 140573432325888 logging_writer.py:48] [10] global_step=10, grad_norm=0.604714, loss=6.925156
I0608 17:48:09.693986 140625997584192 submission.py:296] 10) loss = 6.925, grad_norm = 0.605
I0608 17:48:10.091501 140573423933184 logging_writer.py:48] [11] global_step=11, grad_norm=0.590178, loss=6.918732
I0608 17:48:10.096925 140625997584192 submission.py:296] 11) loss = 6.919, grad_norm = 0.590
I0608 17:48:10.493009 140573432325888 logging_writer.py:48] [12] global_step=12, grad_norm=0.589622, loss=6.919784
I0608 17:48:10.497063 140625997584192 submission.py:296] 12) loss = 6.920, grad_norm = 0.590
I0608 17:48:10.890467 140573423933184 logging_writer.py:48] [13] global_step=13, grad_norm=0.599082, loss=6.921170
I0608 17:48:10.894168 140625997584192 submission.py:296] 13) loss = 6.921, grad_norm = 0.599
I0608 17:48:11.283583 140573432325888 logging_writer.py:48] [14] global_step=14, grad_norm=0.613727, loss=6.925803
I0608 17:48:11.288381 140625997584192 submission.py:296] 14) loss = 6.926, grad_norm = 0.614
I0608 17:48:11.680647 140573423933184 logging_writer.py:48] [15] global_step=15, grad_norm=0.600013, loss=6.932995
I0608 17:48:11.684695 140625997584192 submission.py:296] 15) loss = 6.933, grad_norm = 0.600
I0608 17:48:12.080300 140573432325888 logging_writer.py:48] [16] global_step=16, grad_norm=0.588978, loss=6.934944
I0608 17:48:12.084214 140625997584192 submission.py:296] 16) loss = 6.935, grad_norm = 0.589
I0608 17:48:12.484826 140573423933184 logging_writer.py:48] [17] global_step=17, grad_norm=0.601795, loss=6.928483
I0608 17:48:12.489874 140625997584192 submission.py:296] 17) loss = 6.928, grad_norm = 0.602
I0608 17:48:12.883219 140573432325888 logging_writer.py:48] [18] global_step=18, grad_norm=0.601599, loss=6.920993
I0608 17:48:12.887133 140625997584192 submission.py:296] 18) loss = 6.921, grad_norm = 0.602
I0608 17:48:13.280349 140573423933184 logging_writer.py:48] [19] global_step=19, grad_norm=0.600223, loss=6.926502
I0608 17:48:13.285100 140625997584192 submission.py:296] 19) loss = 6.927, grad_norm = 0.600
I0608 17:48:13.674298 140573432325888 logging_writer.py:48] [20] global_step=20, grad_norm=0.604541, loss=6.923582
I0608 17:48:13.679329 140625997584192 submission.py:296] 20) loss = 6.924, grad_norm = 0.605
I0608 17:48:14.072867 140573423933184 logging_writer.py:48] [21] global_step=21, grad_norm=0.598374, loss=6.927163
I0608 17:48:14.077838 140625997584192 submission.py:296] 21) loss = 6.927, grad_norm = 0.598
I0608 17:48:14.473752 140573432325888 logging_writer.py:48] [22] global_step=22, grad_norm=0.599522, loss=6.923433
I0608 17:48:14.478044 140625997584192 submission.py:296] 22) loss = 6.923, grad_norm = 0.600
I0608 17:48:14.881940 140573423933184 logging_writer.py:48] [23] global_step=23, grad_norm=0.597761, loss=6.937544
I0608 17:48:14.885855 140625997584192 submission.py:296] 23) loss = 6.938, grad_norm = 0.598
I0608 17:48:15.278725 140573432325888 logging_writer.py:48] [24] global_step=24, grad_norm=0.605796, loss=6.923220
I0608 17:48:15.283065 140625997584192 submission.py:296] 24) loss = 6.923, grad_norm = 0.606
I0608 17:48:15.680843 140573423933184 logging_writer.py:48] [25] global_step=25, grad_norm=0.587180, loss=6.911268
I0608 17:48:15.686822 140625997584192 submission.py:296] 25) loss = 6.911, grad_norm = 0.587
I0608 17:48:16.088168 140573432325888 logging_writer.py:48] [26] global_step=26, grad_norm=0.609049, loss=6.917093
I0608 17:48:16.091915 140625997584192 submission.py:296] 26) loss = 6.917, grad_norm = 0.609
I0608 17:48:16.507526 140573423933184 logging_writer.py:48] [27] global_step=27, grad_norm=0.589051, loss=6.929067
I0608 17:48:16.512088 140625997584192 submission.py:296] 27) loss = 6.929, grad_norm = 0.589
I0608 17:48:16.904458 140573432325888 logging_writer.py:48] [28] global_step=28, grad_norm=0.602236, loss=6.907793
I0608 17:48:16.909101 140625997584192 submission.py:296] 28) loss = 6.908, grad_norm = 0.602
I0608 17:48:17.302213 140573423933184 logging_writer.py:48] [29] global_step=29, grad_norm=0.606581, loss=6.916027
I0608 17:48:17.306381 140625997584192 submission.py:296] 29) loss = 6.916, grad_norm = 0.607
I0608 17:48:17.700310 140573432325888 logging_writer.py:48] [30] global_step=30, grad_norm=0.602170, loss=6.925256
I0608 17:48:17.704133 140625997584192 submission.py:296] 30) loss = 6.925, grad_norm = 0.602
I0608 17:48:18.097648 140573423933184 logging_writer.py:48] [31] global_step=31, grad_norm=0.578349, loss=6.911429
I0608 17:48:18.101538 140625997584192 submission.py:296] 31) loss = 6.911, grad_norm = 0.578
I0608 17:48:18.499610 140573432325888 logging_writer.py:48] [32] global_step=32, grad_norm=0.578006, loss=6.921292
I0608 17:48:18.510646 140625997584192 submission.py:296] 32) loss = 6.921, grad_norm = 0.578
I0608 17:48:18.907768 140573423933184 logging_writer.py:48] [33] global_step=33, grad_norm=0.612974, loss=6.918919
I0608 17:48:18.911872 140625997584192 submission.py:296] 33) loss = 6.919, grad_norm = 0.613
I0608 17:48:19.304127 140573432325888 logging_writer.py:48] [34] global_step=34, grad_norm=0.591176, loss=6.913289
I0608 17:48:19.309128 140625997584192 submission.py:296] 34) loss = 6.913, grad_norm = 0.591
I0608 17:48:19.702952 140573423933184 logging_writer.py:48] [35] global_step=35, grad_norm=0.591495, loss=6.921303
I0608 17:48:19.706941 140625997584192 submission.py:296] 35) loss = 6.921, grad_norm = 0.591
I0608 17:48:20.113237 140573432325888 logging_writer.py:48] [36] global_step=36, grad_norm=0.600408, loss=6.911808
I0608 17:48:20.116989 140625997584192 submission.py:296] 36) loss = 6.912, grad_norm = 0.600
I0608 17:48:20.520874 140573423933184 logging_writer.py:48] [37] global_step=37, grad_norm=0.601723, loss=6.916991
I0608 17:48:20.525443 140625997584192 submission.py:296] 37) loss = 6.917, grad_norm = 0.602
I0608 17:48:20.919513 140573432325888 logging_writer.py:48] [38] global_step=38, grad_norm=0.587414, loss=6.914286
I0608 17:48:20.923715 140625997584192 submission.py:296] 38) loss = 6.914, grad_norm = 0.587
I0608 17:48:21.315848 140573423933184 logging_writer.py:48] [39] global_step=39, grad_norm=0.594496, loss=6.920796
I0608 17:48:21.320417 140625997584192 submission.py:296] 39) loss = 6.921, grad_norm = 0.594
I0608 17:48:21.714631 140573432325888 logging_writer.py:48] [40] global_step=40, grad_norm=0.604675, loss=6.920948
I0608 17:48:21.718539 140625997584192 submission.py:296] 40) loss = 6.921, grad_norm = 0.605
I0608 17:48:22.135411 140573423933184 logging_writer.py:48] [41] global_step=41, grad_norm=0.574392, loss=6.910802
I0608 17:48:22.140747 140625997584192 submission.py:296] 41) loss = 6.911, grad_norm = 0.574
I0608 17:48:22.537844 140573432325888 logging_writer.py:48] [42] global_step=42, grad_norm=0.590105, loss=6.917914
I0608 17:48:22.542533 140625997584192 submission.py:296] 42) loss = 6.918, grad_norm = 0.590
I0608 17:48:22.941662 140573423933184 logging_writer.py:48] [43] global_step=43, grad_norm=0.595977, loss=6.911728
I0608 17:48:22.945944 140625997584192 submission.py:296] 43) loss = 6.912, grad_norm = 0.596
I0608 17:48:23.374864 140573432325888 logging_writer.py:48] [44] global_step=44, grad_norm=0.594664, loss=6.914319
I0608 17:48:23.379765 140625997584192 submission.py:296] 44) loss = 6.914, grad_norm = 0.595
I0608 17:48:23.772579 140573423933184 logging_writer.py:48] [45] global_step=45, grad_norm=0.586453, loss=6.924180
I0608 17:48:23.776696 140625997584192 submission.py:296] 45) loss = 6.924, grad_norm = 0.586
I0608 17:48:24.178456 140573432325888 logging_writer.py:48] [46] global_step=46, grad_norm=0.607687, loss=6.907166
I0608 17:48:24.182315 140625997584192 submission.py:296] 46) loss = 6.907, grad_norm = 0.608
I0608 17:48:24.573385 140573423933184 logging_writer.py:48] [47] global_step=47, grad_norm=0.593840, loss=6.913017
I0608 17:48:24.577289 140625997584192 submission.py:296] 47) loss = 6.913, grad_norm = 0.594
I0608 17:48:24.968825 140573432325888 logging_writer.py:48] [48] global_step=48, grad_norm=0.581395, loss=6.903510
I0608 17:48:24.973325 140625997584192 submission.py:296] 48) loss = 6.904, grad_norm = 0.581
I0608 17:48:25.382209 140573423933184 logging_writer.py:48] [49] global_step=49, grad_norm=0.606652, loss=6.915833
I0608 17:48:25.386368 140625997584192 submission.py:296] 49) loss = 6.916, grad_norm = 0.607
I0608 17:48:25.779427 140573432325888 logging_writer.py:48] [50] global_step=50, grad_norm=0.587631, loss=6.908443
I0608 17:48:25.783969 140625997584192 submission.py:296] 50) loss = 6.908, grad_norm = 0.588
I0608 17:48:26.196695 140573423933184 logging_writer.py:48] [51] global_step=51, grad_norm=0.590323, loss=6.908141
I0608 17:48:26.201521 140625997584192 submission.py:296] 51) loss = 6.908, grad_norm = 0.590
I0608 17:48:26.601771 140573432325888 logging_writer.py:48] [52] global_step=52, grad_norm=0.581499, loss=6.908042
I0608 17:48:26.606343 140625997584192 submission.py:296] 52) loss = 6.908, grad_norm = 0.581
I0608 17:48:27.005753 140573423933184 logging_writer.py:48] [53] global_step=53, grad_norm=0.588350, loss=6.904012
I0608 17:48:27.011018 140625997584192 submission.py:296] 53) loss = 6.904, grad_norm = 0.588
I0608 17:48:27.421347 140573432325888 logging_writer.py:48] [54] global_step=54, grad_norm=0.598745, loss=6.916114
I0608 17:48:27.426785 140625997584192 submission.py:296] 54) loss = 6.916, grad_norm = 0.599
I0608 17:48:27.822281 140573423933184 logging_writer.py:48] [55] global_step=55, grad_norm=0.582582, loss=6.909467
I0608 17:48:27.830730 140625997584192 submission.py:296] 55) loss = 6.909, grad_norm = 0.583
I0608 17:48:28.223946 140573432325888 logging_writer.py:48] [56] global_step=56, grad_norm=0.593439, loss=6.915064
I0608 17:48:28.228622 140625997584192 submission.py:296] 56) loss = 6.915, grad_norm = 0.593
I0608 17:48:28.618805 140573423933184 logging_writer.py:48] [57] global_step=57, grad_norm=0.598098, loss=6.907464
I0608 17:48:28.622781 140625997584192 submission.py:296] 57) loss = 6.907, grad_norm = 0.598
I0608 17:48:29.027211 140573432325888 logging_writer.py:48] [58] global_step=58, grad_norm=0.605935, loss=6.900010
I0608 17:48:29.031434 140625997584192 submission.py:296] 58) loss = 6.900, grad_norm = 0.606
I0608 17:48:29.428545 140573423933184 logging_writer.py:48] [59] global_step=59, grad_norm=0.603385, loss=6.902407
I0608 17:48:29.433240 140625997584192 submission.py:296] 59) loss = 6.902, grad_norm = 0.603
I0608 17:48:29.832858 140573432325888 logging_writer.py:48] [60] global_step=60, grad_norm=0.601766, loss=6.907642
I0608 17:48:29.836833 140625997584192 submission.py:296] 60) loss = 6.908, grad_norm = 0.602
I0608 17:48:30.254398 140573423933184 logging_writer.py:48] [61] global_step=61, grad_norm=0.573155, loss=6.899550
I0608 17:48:30.259335 140625997584192 submission.py:296] 61) loss = 6.900, grad_norm = 0.573
I0608 17:48:30.651555 140573432325888 logging_writer.py:48] [62] global_step=62, grad_norm=0.574910, loss=6.906859
I0608 17:48:30.656383 140625997584192 submission.py:296] 62) loss = 6.907, grad_norm = 0.575
I0608 17:48:31.050299 140573423933184 logging_writer.py:48] [63] global_step=63, grad_norm=0.602826, loss=6.903613
I0608 17:48:31.055224 140625997584192 submission.py:296] 63) loss = 6.904, grad_norm = 0.603
I0608 17:48:31.450362 140573432325888 logging_writer.py:48] [64] global_step=64, grad_norm=0.616652, loss=6.903548
I0608 17:48:31.454135 140625997584192 submission.py:296] 64) loss = 6.904, grad_norm = 0.617
I0608 17:48:31.853310 140573423933184 logging_writer.py:48] [65] global_step=65, grad_norm=0.587108, loss=6.904669
I0608 17:48:31.857433 140625997584192 submission.py:296] 65) loss = 6.905, grad_norm = 0.587
I0608 17:48:32.252737 140573432325888 logging_writer.py:48] [66] global_step=66, grad_norm=0.590648, loss=6.901139
I0608 17:48:32.256747 140625997584192 submission.py:296] 66) loss = 6.901, grad_norm = 0.591
I0608 17:48:32.650945 140573423933184 logging_writer.py:48] [67] global_step=67, grad_norm=0.598388, loss=6.901602
I0608 17:48:32.654830 140625997584192 submission.py:296] 67) loss = 6.902, grad_norm = 0.598
I0608 17:48:33.050860 140573432325888 logging_writer.py:48] [68] global_step=68, grad_norm=0.599070, loss=6.902584
I0608 17:48:33.054833 140625997584192 submission.py:296] 68) loss = 6.903, grad_norm = 0.599
I0608 17:48:33.450625 140573423933184 logging_writer.py:48] [69] global_step=69, grad_norm=0.570375, loss=6.902424
I0608 17:48:33.455488 140625997584192 submission.py:296] 69) loss = 6.902, grad_norm = 0.570
I0608 17:48:33.854830 140573432325888 logging_writer.py:48] [70] global_step=70, grad_norm=0.594230, loss=6.892878
I0608 17:48:33.860654 140625997584192 submission.py:296] 70) loss = 6.893, grad_norm = 0.594
I0608 17:48:34.260525 140573423933184 logging_writer.py:48] [71] global_step=71, grad_norm=0.593272, loss=6.900838
I0608 17:48:34.264949 140625997584192 submission.py:296] 71) loss = 6.901, grad_norm = 0.593
I0608 17:48:34.660695 140573432325888 logging_writer.py:48] [72] global_step=72, grad_norm=0.593957, loss=6.892613
I0608 17:48:34.665564 140625997584192 submission.py:296] 72) loss = 6.893, grad_norm = 0.594
I0608 17:48:35.061482 140573423933184 logging_writer.py:48] [73] global_step=73, grad_norm=0.604375, loss=6.897663
I0608 17:48:35.065195 140625997584192 submission.py:296] 73) loss = 6.898, grad_norm = 0.604
I0608 17:48:35.462929 140573432325888 logging_writer.py:48] [74] global_step=74, grad_norm=0.579255, loss=6.901228
I0608 17:48:35.466685 140625997584192 submission.py:296] 74) loss = 6.901, grad_norm = 0.579
I0608 17:48:35.862731 140573423933184 logging_writer.py:48] [75] global_step=75, grad_norm=0.576739, loss=6.897437
I0608 17:48:35.868763 140625997584192 submission.py:296] 75) loss = 6.897, grad_norm = 0.577
I0608 17:48:36.274660 140573432325888 logging_writer.py:48] [76] global_step=76, grad_norm=0.586690, loss=6.891044
I0608 17:48:36.284124 140625997584192 submission.py:296] 76) loss = 6.891, grad_norm = 0.587
I0608 17:48:36.679894 140573423933184 logging_writer.py:48] [77] global_step=77, grad_norm=0.591133, loss=6.892230
I0608 17:48:36.684243 140625997584192 submission.py:296] 77) loss = 6.892, grad_norm = 0.591
I0608 17:48:37.146026 140573432325888 logging_writer.py:48] [78] global_step=78, grad_norm=0.598986, loss=6.896007
I0608 17:48:37.151907 140625997584192 submission.py:296] 78) loss = 6.896, grad_norm = 0.599
I0608 17:48:37.549118 140573423933184 logging_writer.py:48] [79] global_step=79, grad_norm=0.613719, loss=6.900176
I0608 17:48:37.552769 140625997584192 submission.py:296] 79) loss = 6.900, grad_norm = 0.614
I0608 17:48:37.962033 140573432325888 logging_writer.py:48] [80] global_step=80, grad_norm=0.575693, loss=6.889378
I0608 17:48:37.967072 140625997584192 submission.py:296] 80) loss = 6.889, grad_norm = 0.576
I0608 17:48:38.361818 140573423933184 logging_writer.py:48] [81] global_step=81, grad_norm=0.577009, loss=6.885460
I0608 17:48:38.365906 140625997584192 submission.py:296] 81) loss = 6.885, grad_norm = 0.577
I0608 17:48:38.766167 140573432325888 logging_writer.py:48] [82] global_step=82, grad_norm=0.579847, loss=6.887501
I0608 17:48:38.769925 140625997584192 submission.py:296] 82) loss = 6.888, grad_norm = 0.580
I0608 17:48:39.166709 140573423933184 logging_writer.py:48] [83] global_step=83, grad_norm=0.587632, loss=6.888517
I0608 17:48:39.170551 140625997584192 submission.py:296] 83) loss = 6.889, grad_norm = 0.588
I0608 17:48:39.566277 140573432325888 logging_writer.py:48] [84] global_step=84, grad_norm=0.585243, loss=6.896818
I0608 17:48:39.571201 140625997584192 submission.py:296] 84) loss = 6.897, grad_norm = 0.585
I0608 17:48:39.966777 140573423933184 logging_writer.py:48] [85] global_step=85, grad_norm=0.583231, loss=6.883617
I0608 17:48:39.970708 140625997584192 submission.py:296] 85) loss = 6.884, grad_norm = 0.583
I0608 17:48:40.364228 140573432325888 logging_writer.py:48] [86] global_step=86, grad_norm=0.597983, loss=6.885863
I0608 17:48:40.368046 140625997584192 submission.py:296] 86) loss = 6.886, grad_norm = 0.598
I0608 17:48:40.764241 140573423933184 logging_writer.py:48] [87] global_step=87, grad_norm=0.591696, loss=6.883423
I0608 17:48:40.767990 140625997584192 submission.py:296] 87) loss = 6.883, grad_norm = 0.592
I0608 17:48:41.159923 140573432325888 logging_writer.py:48] [88] global_step=88, grad_norm=0.589157, loss=6.884846
I0608 17:48:41.164997 140625997584192 submission.py:296] 88) loss = 6.885, grad_norm = 0.589
I0608 17:48:41.574365 140573423933184 logging_writer.py:48] [89] global_step=89, grad_norm=0.581238, loss=6.873506
I0608 17:48:41.578985 140625997584192 submission.py:296] 89) loss = 6.874, grad_norm = 0.581
I0608 17:48:41.977672 140573432325888 logging_writer.py:48] [90] global_step=90, grad_norm=0.600919, loss=6.884759
I0608 17:48:41.983659 140625997584192 submission.py:296] 90) loss = 6.885, grad_norm = 0.601
I0608 17:48:42.376862 140573423933184 logging_writer.py:48] [91] global_step=91, grad_norm=0.577347, loss=6.879287
I0608 17:48:42.380941 140625997584192 submission.py:296] 91) loss = 6.879, grad_norm = 0.577
I0608 17:48:42.773838 140573432325888 logging_writer.py:48] [92] global_step=92, grad_norm=0.599995, loss=6.880608
I0608 17:48:42.778978 140625997584192 submission.py:296] 92) loss = 6.881, grad_norm = 0.600
I0608 17:48:43.174213 140573423933184 logging_writer.py:48] [93] global_step=93, grad_norm=0.586943, loss=6.885883
I0608 17:48:43.178052 140625997584192 submission.py:296] 93) loss = 6.886, grad_norm = 0.587
I0608 17:48:43.572291 140573432325888 logging_writer.py:48] [94] global_step=94, grad_norm=0.579948, loss=6.880270
I0608 17:48:43.576054 140625997584192 submission.py:296] 94) loss = 6.880, grad_norm = 0.580
I0608 17:48:43.990408 140573423933184 logging_writer.py:48] [95] global_step=95, grad_norm=0.586591, loss=6.883789
I0608 17:48:43.994150 140625997584192 submission.py:296] 95) loss = 6.884, grad_norm = 0.587
I0608 17:48:44.391019 140573432325888 logging_writer.py:48] [96] global_step=96, grad_norm=0.591018, loss=6.880867
I0608 17:48:44.394719 140625997584192 submission.py:296] 96) loss = 6.881, grad_norm = 0.591
I0608 17:48:44.790528 140573423933184 logging_writer.py:48] [97] global_step=97, grad_norm=0.596323, loss=6.866161
I0608 17:48:44.794261 140625997584192 submission.py:296] 97) loss = 6.866, grad_norm = 0.596
I0608 17:48:45.189628 140573432325888 logging_writer.py:48] [98] global_step=98, grad_norm=0.591439, loss=6.873968
I0608 17:48:45.194034 140625997584192 submission.py:296] 98) loss = 6.874, grad_norm = 0.591
I0608 17:48:45.602855 140573423933184 logging_writer.py:48] [99] global_step=99, grad_norm=0.598733, loss=6.884261
I0608 17:48:45.606697 140625997584192 submission.py:296] 99) loss = 6.884, grad_norm = 0.599
I0608 17:48:46.001498 140573432325888 logging_writer.py:48] [100] global_step=100, grad_norm=0.589383, loss=6.875962
I0608 17:48:46.005559 140625997584192 submission.py:296] 100) loss = 6.876, grad_norm = 0.589
I0608 17:51:21.392678 140573423933184 logging_writer.py:48] [500] global_step=500, grad_norm=1.002736, loss=6.317269
I0608 17:51:21.397992 140625997584192 submission.py:296] 500) loss = 6.317, grad_norm = 1.003
I0608 17:54:35.349069 140573432325888 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.683265, loss=5.652728
I0608 17:54:35.353646 140625997584192 submission.py:296] 1000) loss = 5.653, grad_norm = 2.683
I0608 17:56:35.870046 140625997584192 spec.py:298] Evaluating on the training split.
I0608 17:57:19.170833 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 17:58:14.509579 140625997584192 spec.py:326] Evaluating on the test split.
I0608 17:58:15.926438 140625997584192 submission_runner.py:419] Time since start: 745.67s, 	Step: 1308, 	{'train/accuracy': 0.11706792091836735, 'train/loss': 4.777226039341518, 'validation/accuracy': 0.1059, 'validation/loss': 4.8630571875, 'validation/num_examples': 50000, 'test/accuracy': 0.0696, 'test/loss': 5.30064296875, 'test/num_examples': 10000, 'score': 517.8533375263214, 'total_duration': 745.6706850528717, 'accumulated_submission_time': 517.8533375263214, 'accumulated_eval_time': 227.1411907672882, 'accumulated_logging_time': 0.025414466857910156}
I0608 17:58:15.936963 140573440718592 logging_writer.py:48] [1308] accumulated_eval_time=227.141191, accumulated_logging_time=0.025414, accumulated_submission_time=517.853338, global_step=1308, preemption_count=0, score=517.853338, test/accuracy=0.069600, test/loss=5.300643, test/num_examples=10000, total_duration=745.670685, train/accuracy=0.117068, train/loss=4.777226, validation/accuracy=0.105900, validation/loss=4.863057, validation/num_examples=50000
I0608 17:59:30.118887 140573449111296 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.871328, loss=5.216165
I0608 17:59:30.123037 140625997584192 submission.py:296] 1500) loss = 5.216, grad_norm = 3.871
I0608 18:02:42.434957 140573440718592 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.591706, loss=4.852729
I0608 18:02:42.439693 140625997584192 submission.py:296] 2000) loss = 4.853, grad_norm = 3.592
I0608 18:05:56.180946 140573449111296 logging_writer.py:48] [2500] global_step=2500, grad_norm=6.352222, loss=4.585700
I0608 18:05:56.186045 140625997584192 submission.py:296] 2500) loss = 4.586, grad_norm = 6.352
I0608 18:06:46.172913 140625997584192 spec.py:298] Evaluating on the training split.
I0608 18:07:31.835052 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 18:08:26.939678 140625997584192 spec.py:326] Evaluating on the test split.
I0608 18:08:28.322427 140625997584192 submission_runner.py:419] Time since start: 1358.06s, 	Step: 2627, 	{'train/accuracy': 0.2649473852040816, 'train/loss': 3.6080783143335458, 'validation/accuracy': 0.24226, 'validation/loss': 3.743725625, 'validation/num_examples': 50000, 'test/accuracy': 0.1731, 'test/loss': 4.344927734375, 'test/num_examples': 10000, 'score': 1027.491409778595, 'total_duration': 1358.0648384094238, 'accumulated_submission_time': 1027.491409778595, 'accumulated_eval_time': 329.28884100914, 'accumulated_logging_time': 0.04461359977722168}
I0608 18:08:28.336357 140573440718592 logging_writer.py:48] [2627] accumulated_eval_time=329.288841, accumulated_logging_time=0.044614, accumulated_submission_time=1027.491410, global_step=2627, preemption_count=0, score=1027.491410, test/accuracy=0.173100, test/loss=4.344928, test/num_examples=10000, total_duration=1358.064838, train/accuracy=0.264947, train/loss=3.608078, validation/accuracy=0.242260, validation/loss=3.743726, validation/num_examples=50000
I0608 18:10:52.213729 140573449111296 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.898188, loss=4.316885
I0608 18:10:52.218091 140625997584192 submission.py:296] 3000) loss = 4.317, grad_norm = 4.898
I0608 18:14:04.914686 140573440718592 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.613809, loss=3.938090
I0608 18:14:04.919329 140625997584192 submission.py:296] 3500) loss = 3.938, grad_norm = 5.614
I0608 18:16:58.639122 140625997584192 spec.py:298] Evaluating on the training split.
I0608 18:17:43.209034 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 18:18:32.693359 140625997584192 spec.py:326] Evaluating on the test split.
I0608 18:18:34.071056 140625997584192 submission_runner.py:419] Time since start: 1963.82s, 	Step: 3947, 	{'train/accuracy': 0.39255819515306123, 'train/loss': 2.831575043347417, 'validation/accuracy': 0.36038, 'validation/loss': 3.011588125, 'validation/num_examples': 50000, 'test/accuracy': 0.259, 'test/loss': 3.712580078125, 'test/num_examples': 10000, 'score': 1537.1995556354523, 'total_duration': 1963.8153643608093, 'accumulated_submission_time': 1537.1995556354523, 'accumulated_eval_time': 424.7207202911377, 'accumulated_logging_time': 0.0685720443725586}
I0608 18:18:34.084648 140573449111296 logging_writer.py:48] [3947] accumulated_eval_time=424.720720, accumulated_logging_time=0.068572, accumulated_submission_time=1537.199556, global_step=3947, preemption_count=0, score=1537.199556, test/accuracy=0.259000, test/loss=3.712580, test/num_examples=10000, total_duration=1963.815364, train/accuracy=0.392558, train/loss=2.831575, validation/accuracy=0.360380, validation/loss=3.011588, validation/num_examples=50000
I0608 18:18:54.769032 140573440718592 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.026897, loss=3.951002
I0608 18:18:54.772928 140625997584192 submission.py:296] 4000) loss = 3.951, grad_norm = 4.027
I0608 18:22:07.298221 140573449111296 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.697484, loss=3.608724
I0608 18:22:07.303053 140625997584192 submission.py:296] 4500) loss = 3.609, grad_norm = 4.697
I0608 18:25:21.037891 140573440718592 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.623992, loss=3.606608
I0608 18:25:21.041563 140625997584192 submission.py:296] 5000) loss = 3.607, grad_norm = 3.624
I0608 18:27:04.162135 140625997584192 spec.py:298] Evaluating on the training split.
I0608 18:27:48.528863 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 18:28:42.468532 140625997584192 spec.py:326] Evaluating on the test split.
I0608 18:28:43.844809 140625997584192 submission_runner.py:419] Time since start: 2573.59s, 	Step: 5265, 	{'train/accuracy': 0.46994579081632654, 'train/loss': 2.43906916404257, 'validation/accuracy': 0.42826, 'validation/loss': 2.640971875, 'validation/num_examples': 50000, 'test/accuracy': 0.3114, 'test/loss': 3.366155078125, 'test/num_examples': 10000, 'score': 2046.674776315689, 'total_duration': 2573.589118719101, 'accumulated_submission_time': 2046.674776315689, 'accumulated_eval_time': 524.4036891460419, 'accumulated_logging_time': 0.0917043685913086}
I0608 18:28:43.856371 140573449111296 logging_writer.py:48] [5265] accumulated_eval_time=524.403689, accumulated_logging_time=0.091704, accumulated_submission_time=2046.674776, global_step=5265, preemption_count=0, score=2046.674776, test/accuracy=0.311400, test/loss=3.366155, test/num_examples=10000, total_duration=2573.589119, train/accuracy=0.469946, train/loss=2.439069, validation/accuracy=0.428260, validation/loss=2.640972, validation/num_examples=50000
I0608 18:30:14.597109 140573440718592 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.537632, loss=3.604793
I0608 18:30:14.604091 140625997584192 submission.py:296] 5500) loss = 3.605, grad_norm = 3.538
I0608 18:33:27.262130 140573449111296 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.648026, loss=3.350372
I0608 18:33:27.268217 140625997584192 submission.py:296] 6000) loss = 3.350, grad_norm = 3.648
I0608 18:36:41.836372 140573440718592 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.694656, loss=3.303544
I0608 18:36:41.845386 140625997584192 submission.py:296] 6500) loss = 3.304, grad_norm = 2.695
I0608 18:37:14.189442 140625997584192 spec.py:298] Evaluating on the training split.
I0608 18:37:58.162596 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 18:38:52.303782 140625997584192 spec.py:326] Evaluating on the test split.
I0608 18:38:53.679826 140625997584192 submission_runner.py:419] Time since start: 3183.42s, 	Step: 6585, 	{'train/accuracy': 0.5301937181122449, 'train/loss': 2.1486297918825734, 'validation/accuracy': 0.48468, 'validation/loss': 2.37033296875, 'validation/num_examples': 50000, 'test/accuracy': 0.3597, 'test/loss': 3.150373828125, 'test/num_examples': 10000, 'score': 2556.406012058258, 'total_duration': 3183.4241621494293, 'accumulated_submission_time': 2556.406012058258, 'accumulated_eval_time': 623.8940417766571, 'accumulated_logging_time': 0.11789536476135254}
I0608 18:38:53.691463 140573449111296 logging_writer.py:48] [6585] accumulated_eval_time=623.894042, accumulated_logging_time=0.117895, accumulated_submission_time=2556.406012, global_step=6585, preemption_count=0, score=2556.406012, test/accuracy=0.359700, test/loss=3.150374, test/num_examples=10000, total_duration=3183.424162, train/accuracy=0.530194, train/loss=2.148630, validation/accuracy=0.484680, validation/loss=2.370333, validation/num_examples=50000
I0608 18:41:33.799765 140573440718592 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.864898, loss=3.240452
I0608 18:41:33.804453 140625997584192 submission.py:296] 7000) loss = 3.240, grad_norm = 2.865
I0608 18:44:47.244620 140573449111296 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.535732, loss=3.215429
I0608 18:44:47.250355 140625997584192 submission.py:296] 7500) loss = 3.215, grad_norm = 2.536
I0608 18:47:23.685134 140625997584192 spec.py:298] Evaluating on the training split.
I0608 18:48:08.665662 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 18:48:54.901383 140625997584192 spec.py:326] Evaluating on the test split.
I0608 18:48:56.275103 140625997584192 submission_runner.py:419] Time since start: 3786.02s, 	Step: 7904, 	{'train/accuracy': 0.5787826849489796, 'train/loss': 1.8444100204779177, 'validation/accuracy': 0.52654, 'validation/loss': 2.0915446875, 'validation/num_examples': 50000, 'test/accuracy': 0.4007, 'test/loss': 2.8277609375, 'test/num_examples': 10000, 'score': 3065.798003911972, 'total_duration': 3786.0194430351257, 'accumulated_submission_time': 3065.798003911972, 'accumulated_eval_time': 716.483968257904, 'accumulated_logging_time': 0.13777661323547363}
I0608 18:48:56.285418 140573440718592 logging_writer.py:48] [7904] accumulated_eval_time=716.483968, accumulated_logging_time=0.137777, accumulated_submission_time=3065.798004, global_step=7904, preemption_count=0, score=3065.798004, test/accuracy=0.400700, test/loss=2.827761, test/num_examples=10000, total_duration=3786.019443, train/accuracy=0.578783, train/loss=1.844410, validation/accuracy=0.526540, validation/loss=2.091545, validation/num_examples=50000
I0608 18:49:33.544533 140573449111296 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.249412, loss=3.054305
I0608 18:49:33.549016 140625997584192 submission.py:296] 8000) loss = 3.054, grad_norm = 2.249
I0608 18:52:45.840123 140573440718592 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.082315, loss=3.059741
I0608 18:52:45.845623 140625997584192 submission.py:296] 8500) loss = 3.060, grad_norm = 2.082
I0608 18:56:00.242344 140573449111296 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.570963, loss=3.054757
I0608 18:56:00.247248 140625997584192 submission.py:296] 9000) loss = 3.055, grad_norm = 2.571
I0608 18:57:26.314360 140625997584192 spec.py:298] Evaluating on the training split.
I0608 18:58:10.941449 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 18:58:57.226634 140625997584192 spec.py:326] Evaluating on the test split.
I0608 18:58:58.602522 140625997584192 submission_runner.py:419] Time since start: 4388.35s, 	Step: 9225, 	{'train/accuracy': 0.6096540178571429, 'train/loss': 1.708965067960778, 'validation/accuracy': 0.55734, 'validation/loss': 1.97201953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4208, 'test/loss': 2.7369892578125, 'test/num_examples': 10000, 'score': 3575.225672006607, 'total_duration': 4388.34682393074, 'accumulated_submission_time': 3575.225672006607, 'accumulated_eval_time': 808.7721037864685, 'accumulated_logging_time': 0.1563129425048828}
I0608 18:58:58.613278 140573440718592 logging_writer.py:48] [9225] accumulated_eval_time=808.772104, accumulated_logging_time=0.156313, accumulated_submission_time=3575.225672, global_step=9225, preemption_count=0, score=3575.225672, test/accuracy=0.420800, test/loss=2.736989, test/num_examples=10000, total_duration=4388.346824, train/accuracy=0.609654, train/loss=1.708965, validation/accuracy=0.557340, validation/loss=1.972020, validation/num_examples=50000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0608 19:00:44.529715 140573449111296 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.712358, loss=2.968624
I0608 19:00:44.534165 140625997584192 submission.py:296] 9500) loss = 2.969, grad_norm = 1.712
I0608 19:03:57.995063 140573440718592 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.362905, loss=2.890821
I0608 19:03:57.999224 140625997584192 submission.py:296] 10000) loss = 2.891, grad_norm = 2.363
I0608 19:07:12.058937 140573449111296 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.119979, loss=2.854715
I0608 19:07:12.063370 140625997584192 submission.py:296] 10500) loss = 2.855, grad_norm = 2.120
I0608 19:07:28.963114 140625997584192 spec.py:298] Evaluating on the training split.
I0608 19:08:13.912520 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 19:09:00.393673 140625997584192 spec.py:326] Evaluating on the test split.
I0608 19:09:01.775308 140625997584192 submission_runner.py:419] Time since start: 4991.52s, 	Step: 10545, 	{'train/accuracy': 0.6447305484693877, 'train/loss': 1.5726687372947226, 'validation/accuracy': 0.58188, 'validation/loss': 1.85750703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4407, 'test/loss': 2.6162478515625, 'test/num_examples': 10000, 'score': 4084.9771118164062, 'total_duration': 4991.519228935242, 'accumulated_submission_time': 4084.9771118164062, 'accumulated_eval_time': 901.5841298103333, 'accumulated_logging_time': 0.17508339881896973}
I0608 19:09:01.791560 140573440718592 logging_writer.py:48] [10545] accumulated_eval_time=901.584130, accumulated_logging_time=0.175083, accumulated_submission_time=4084.977112, global_step=10545, preemption_count=0, score=4084.977112, test/accuracy=0.440700, test/loss=2.616248, test/num_examples=10000, total_duration=4991.519229, train/accuracy=0.644731, train/loss=1.572669, validation/accuracy=0.581880, validation/loss=1.857507, validation/num_examples=50000
I0608 19:11:57.264827 140573449111296 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.298992, loss=2.875757
I0608 19:11:57.269418 140625997584192 submission.py:296] 11000) loss = 2.876, grad_norm = 1.299
I0608 19:15:11.647460 140573440718592 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.725162, loss=2.721012
I0608 19:15:11.657401 140625997584192 submission.py:296] 11500) loss = 2.721, grad_norm = 1.725
I0608 19:17:31.879656 140625997584192 spec.py:298] Evaluating on the training split.
I0608 19:18:16.778510 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 19:19:07.108042 140625997584192 spec.py:326] Evaluating on the test split.
I0608 19:19:08.482448 140625997584192 submission_runner.py:419] Time since start: 5598.23s, 	Step: 11866, 	{'train/accuracy': 0.6716757015306123, 'train/loss': 1.4374045547173948, 'validation/accuracy': 0.60616, 'validation/loss': 1.7493134375, 'validation/num_examples': 50000, 'test/accuracy': 0.4653, 'test/loss': 2.53922421875, 'test/num_examples': 10000, 'score': 4594.464698791504, 'total_duration': 5598.2267343997955, 'accumulated_submission_time': 4594.464698791504, 'accumulated_eval_time': 998.186891078949, 'accumulated_logging_time': 0.2006237506866455}
I0608 19:19:08.493772 140573449111296 logging_writer.py:48] [11866] accumulated_eval_time=998.186891, accumulated_logging_time=0.200624, accumulated_submission_time=4594.464699, global_step=11866, preemption_count=0, score=4594.464699, test/accuracy=0.465300, test/loss=2.539224, test/num_examples=10000, total_duration=5598.226734, train/accuracy=0.671676, train/loss=1.437405, validation/accuracy=0.606160, validation/loss=1.749313, validation/num_examples=50000
I0608 19:20:00.439692 140573440718592 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.754916, loss=2.714463
I0608 19:20:00.443775 140625997584192 submission.py:296] 12000) loss = 2.714, grad_norm = 1.755
I0608 19:23:13.710676 140573449111296 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.621463, loss=2.852656
I0608 19:23:13.716646 140625997584192 submission.py:296] 12500) loss = 2.853, grad_norm = 1.621
I0608 19:26:27.324303 140573440718592 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.848112, loss=2.720933
I0608 19:26:27.329005 140625997584192 submission.py:296] 13000) loss = 2.721, grad_norm = 1.848
I0608 19:27:38.789597 140625997584192 spec.py:298] Evaluating on the training split.
I0608 19:28:23.827986 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 19:29:10.437943 140625997584192 spec.py:326] Evaluating on the test split.
I0608 19:29:11.810650 140625997584192 submission_runner.py:419] Time since start: 6201.55s, 	Step: 13187, 	{'train/accuracy': 0.6864237882653061, 'train/loss': 1.3833456234056123, 'validation/accuracy': 0.6167, 'validation/loss': 1.70272578125, 'validation/num_examples': 50000, 'test/accuracy': 0.4763, 'test/loss': 2.4389048828125, 'test/num_examples': 10000, 'score': 5104.159840345383, 'total_duration': 6201.554953813553, 'accumulated_submission_time': 5104.159840345383, 'accumulated_eval_time': 1091.2078711986542, 'accumulated_logging_time': 0.22185373306274414}
I0608 19:29:11.821235 140573449111296 logging_writer.py:48] [13187] accumulated_eval_time=1091.207871, accumulated_logging_time=0.221854, accumulated_submission_time=5104.159840, global_step=13187, preemption_count=0, score=5104.159840, test/accuracy=0.476300, test/loss=2.438905, test/num_examples=10000, total_duration=6201.554954, train/accuracy=0.686424, train/loss=1.383346, validation/accuracy=0.616700, validation/loss=1.702726, validation/num_examples=50000
I0608 19:31:12.726531 140573440718592 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.559721, loss=2.645367
I0608 19:31:12.730997 140625997584192 submission.py:296] 13500) loss = 2.645, grad_norm = 1.560
I0608 19:34:27.825625 140573449111296 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.140903, loss=2.583359
I0608 19:34:27.837182 140625997584192 submission.py:296] 14000) loss = 2.583, grad_norm = 1.141
I0608 19:37:39.985180 140573440718592 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.400664, loss=2.608934
I0608 19:37:39.989886 140625997584192 submission.py:296] 14500) loss = 2.609, grad_norm = 1.401
I0608 19:37:41.912065 140625997584192 spec.py:298] Evaluating on the training split.
I0608 19:38:27.347980 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 19:39:14.423369 140625997584192 spec.py:326] Evaluating on the test split.
I0608 19:39:15.796858 140625997584192 submission_runner.py:419] Time since start: 6805.54s, 	Step: 14506, 	{'train/accuracy': 0.7080676020408163, 'train/loss': 1.2585560545629384, 'validation/accuracy': 0.63484, 'validation/loss': 1.6114834375, 'validation/num_examples': 50000, 'test/accuracy': 0.4861, 'test/loss': 2.3946166015625, 'test/num_examples': 10000, 'score': 5613.642600774765, 'total_duration': 6805.541194438934, 'accumulated_submission_time': 5613.642600774765, 'accumulated_eval_time': 1185.092606306076, 'accumulated_logging_time': 0.24149608612060547}
I0608 19:39:15.807665 140573449111296 logging_writer.py:48] [14506] accumulated_eval_time=1185.092606, accumulated_logging_time=0.241496, accumulated_submission_time=5613.642601, global_step=14506, preemption_count=0, score=5613.642601, test/accuracy=0.486100, test/loss=2.394617, test/num_examples=10000, total_duration=6805.541194, train/accuracy=0.708068, train/loss=1.258556, validation/accuracy=0.634840, validation/loss=1.611483, validation/num_examples=50000
I0608 19:42:27.305459 140573440718592 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.308353, loss=2.590868
I0608 19:42:27.312003 140625997584192 submission.py:296] 15000) loss = 2.591, grad_norm = 1.308
I0608 19:45:41.141053 140573449111296 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.891107, loss=2.519998
I0608 19:45:41.145952 140625997584192 submission.py:296] 15500) loss = 2.520, grad_norm = 0.891
I0608 19:47:45.838440 140625997584192 spec.py:298] Evaluating on the training split.
I0608 19:48:30.425329 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 19:49:16.538458 140625997584192 spec.py:326] Evaluating on the test split.
I0608 19:49:17.911297 140625997584192 submission_runner.py:419] Time since start: 7407.66s, 	Step: 15825, 	{'train/accuracy': 0.7253467793367347, 'train/loss': 1.2164442879813058, 'validation/accuracy': 0.6428, 'validation/loss': 1.5761675, 'validation/num_examples': 50000, 'test/accuracy': 0.4924, 'test/loss': 2.371537109375, 'test/num_examples': 10000, 'score': 6123.071384429932, 'total_duration': 7407.655592918396, 'accumulated_submission_time': 6123.071384429932, 'accumulated_eval_time': 1277.1653707027435, 'accumulated_logging_time': 0.26083898544311523}
I0608 19:49:17.922381 140573440718592 logging_writer.py:48] [15825] accumulated_eval_time=1277.165371, accumulated_logging_time=0.260839, accumulated_submission_time=6123.071384, global_step=15825, preemption_count=0, score=6123.071384, test/accuracy=0.492400, test/loss=2.371537, test/num_examples=10000, total_duration=7407.655593, train/accuracy=0.725347, train/loss=1.216444, validation/accuracy=0.642800, validation/loss=1.576167, validation/num_examples=50000
I0608 19:50:25.792421 140573449111296 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.138988, loss=2.569730
I0608 19:50:25.797281 140625997584192 submission.py:296] 16000) loss = 2.570, grad_norm = 1.139
I0608 19:53:40.430918 140573440718592 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.129193, loss=2.551569
I0608 19:53:40.435213 140625997584192 submission.py:296] 16500) loss = 2.552, grad_norm = 1.129
I0608 19:56:52.735733 140573449111296 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.961378, loss=2.546414
I0608 19:56:52.740572 140625997584192 submission.py:296] 17000) loss = 2.546, grad_norm = 0.961
I0608 19:57:47.925992 140625997584192 spec.py:298] Evaluating on the training split.
I0608 19:58:32.950672 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 19:59:21.609858 140625997584192 spec.py:326] Evaluating on the test split.
I0608 19:59:22.984416 140625997584192 submission_runner.py:419] Time since start: 8012.73s, 	Step: 17144, 	{'train/accuracy': 0.7315848214285714, 'train/loss': 1.206716420699139, 'validation/accuracy': 0.64668, 'validation/loss': 1.58421609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5112, 'test/loss': 2.27880546875, 'test/num_examples': 10000, 'score': 6632.469932317734, 'total_duration': 8012.728769779205, 'accumulated_submission_time': 6632.469932317734, 'accumulated_eval_time': 1372.223869562149, 'accumulated_logging_time': 0.2815859317779541}
I0608 19:59:22.995406 140573440718592 logging_writer.py:48] [17144] accumulated_eval_time=1372.223870, accumulated_logging_time=0.281586, accumulated_submission_time=6632.469932, global_step=17144, preemption_count=0, score=6632.469932, test/accuracy=0.511200, test/loss=2.278805, test/num_examples=10000, total_duration=8012.728770, train/accuracy=0.731585, train/loss=1.206716, validation/accuracy=0.646680, validation/loss=1.584216, validation/num_examples=50000
I0608 20:01:41.319030 140573449111296 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.074314, loss=2.472232
I0608 20:01:41.324609 140625997584192 submission.py:296] 17500) loss = 2.472, grad_norm = 1.074
I0608 20:04:55.187169 140573440718592 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.146658, loss=2.547289
I0608 20:04:55.191872 140625997584192 submission.py:296] 18000) loss = 2.547, grad_norm = 1.147
I0608 20:07:53.160812 140625997584192 spec.py:298] Evaluating on the training split.
I0608 20:08:38.643945 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 20:09:26.163772 140625997584192 spec.py:326] Evaluating on the test split.
I0608 20:09:27.538918 140625997584192 submission_runner.py:419] Time since start: 8617.28s, 	Step: 18463, 	{'train/accuracy': 0.7461535395408163, 'train/loss': 1.1019312021683674, 'validation/accuracy': 0.65644, 'validation/loss': 1.50135375, 'validation/num_examples': 50000, 'test/accuracy': 0.5109, 'test/loss': 2.2571041015625, 'test/num_examples': 10000, 'score': 7142.026972293854, 'total_duration': 8617.283230543137, 'accumulated_submission_time': 7142.026972293854, 'accumulated_eval_time': 1466.6019623279572, 'accumulated_logging_time': 0.30153417587280273}
I0608 20:09:27.549934 140573449111296 logging_writer.py:48] [18463] accumulated_eval_time=1466.601962, accumulated_logging_time=0.301534, accumulated_submission_time=7142.026972, global_step=18463, preemption_count=0, score=7142.026972, test/accuracy=0.510900, test/loss=2.257104, test/num_examples=10000, total_duration=8617.283231, train/accuracy=0.746154, train/loss=1.101931, validation/accuracy=0.656440, validation/loss=1.501354, validation/num_examples=50000
I0608 20:09:42.286561 140573440718592 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.636837, loss=2.481130
I0608 20:09:42.291072 140625997584192 submission.py:296] 18500) loss = 2.481, grad_norm = 0.637
I0608 20:12:56.906052 140573449111296 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.217164, loss=2.494906
I0608 20:12:56.911303 140625997584192 submission.py:296] 19000) loss = 2.495, grad_norm = 1.217
I0608 20:16:09.206419 140573440718592 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.770814, loss=2.422573
I0608 20:16:09.210934 140625997584192 submission.py:296] 19500) loss = 2.423, grad_norm = 0.771
I0608 20:17:57.887409 140625997584192 spec.py:298] Evaluating on the training split.
I0608 20:18:42.633866 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 20:19:37.332872 140625997584192 spec.py:326] Evaluating on the test split.
I0608 20:19:38.705347 140625997584192 submission_runner.py:419] Time since start: 9228.45s, 	Step: 19783, 	{'train/accuracy': 0.7535076530612245, 'train/loss': 1.07894009959941, 'validation/accuracy': 0.66218, 'validation/loss': 1.48525796875, 'validation/num_examples': 50000, 'test/accuracy': 0.5274, 'test/loss': 2.1923275390625, 'test/num_examples': 10000, 'score': 7651.762818336487, 'total_duration': 9228.449648618698, 'accumulated_submission_time': 7651.762818336487, 'accumulated_eval_time': 1567.419855594635, 'accumulated_logging_time': 0.32286977767944336}
I0608 20:19:38.716178 140573449111296 logging_writer.py:48] [19783] accumulated_eval_time=1567.419856, accumulated_logging_time=0.322870, accumulated_submission_time=7651.762818, global_step=19783, preemption_count=0, score=7651.762818, test/accuracy=0.527400, test/loss=2.192328, test/num_examples=10000, total_duration=9228.449649, train/accuracy=0.753508, train/loss=1.078940, validation/accuracy=0.662180, validation/loss=1.485258, validation/num_examples=50000
I0608 20:21:03.128742 140573440718592 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.847825, loss=2.402847
I0608 20:21:03.133942 140625997584192 submission.py:296] 20000) loss = 2.403, grad_norm = 0.848
I0608 20:24:17.360125 140573449111296 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.937030, loss=2.484660
I0608 20:24:17.365477 140625997584192 submission.py:296] 20500) loss = 2.485, grad_norm = 0.937
I0608 20:27:29.813444 140573440718592 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.108591, loss=2.435268
I0608 20:27:29.818204 140625997584192 submission.py:296] 21000) loss = 2.435, grad_norm = 1.109
I0608 20:28:08.984624 140625997584192 spec.py:298] Evaluating on the training split.
I0608 20:28:54.039291 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 20:29:41.308892 140625997584192 spec.py:326] Evaluating on the test split.
I0608 20:29:42.686131 140625997584192 submission_runner.py:419] Time since start: 9832.43s, 	Step: 21102, 	{'train/accuracy': 0.7625956632653061, 'train/loss': 1.0334506910674426, 'validation/accuracy': 0.66856, 'validation/loss': 1.4563609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5246, 'test/loss': 2.186473828125, 'test/num_examples': 10000, 'score': 8161.425109386444, 'total_duration': 9832.428794145584, 'accumulated_submission_time': 8161.425109386444, 'accumulated_eval_time': 1661.1196460723877, 'accumulated_logging_time': 0.3441891670227051}
I0608 20:29:42.697157 140573449111296 logging_writer.py:48] [21102] accumulated_eval_time=1661.119646, accumulated_logging_time=0.344189, accumulated_submission_time=8161.425109, global_step=21102, preemption_count=0, score=8161.425109, test/accuracy=0.524600, test/loss=2.186474, test/num_examples=10000, total_duration=9832.428794, train/accuracy=0.762596, train/loss=1.033451, validation/accuracy=0.668560, validation/loss=1.456361, validation/num_examples=50000
I0608 20:32:18.205189 140573440718592 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.143417, loss=2.444278
I0608 20:32:18.210092 140625997584192 submission.py:296] 21500) loss = 2.444, grad_norm = 1.143
I0608 20:35:30.509334 140573449111296 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.812401, loss=2.470037
I0608 20:35:30.514616 140625997584192 submission.py:296] 22000) loss = 2.470, grad_norm = 0.812
I0608 20:38:13.018536 140625997584192 spec.py:298] Evaluating on the training split.
I0608 20:38:58.013786 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 20:39:53.676108 140625997584192 spec.py:326] Evaluating on the test split.
I0608 20:39:55.049260 140625997584192 submission_runner.py:419] Time since start: 10444.79s, 	Step: 22422, 	{'train/accuracy': 0.7684351084183674, 'train/loss': 1.0292472060845823, 'validation/accuracy': 0.66902, 'validation/loss': 1.4651525, 'validation/num_examples': 50000, 'test/accuracy': 0.5259, 'test/loss': 2.1977400390625, 'test/num_examples': 10000, 'score': 8671.146996974945, 'total_duration': 10444.79356598854, 'accumulated_submission_time': 8671.146996974945, 'accumulated_eval_time': 1763.1503958702087, 'accumulated_logging_time': 0.36335325241088867}
I0608 20:39:55.060452 140573440718592 logging_writer.py:48] [22422] accumulated_eval_time=1763.150396, accumulated_logging_time=0.363353, accumulated_submission_time=8671.146997, global_step=22422, preemption_count=0, score=8671.146997, test/accuracy=0.525900, test/loss=2.197740, test/num_examples=10000, total_duration=10444.793566, train/accuracy=0.768435, train/loss=1.029247, validation/accuracy=0.669020, validation/loss=1.465153, validation/num_examples=50000
I0608 20:40:25.584557 140573449111296 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.832670, loss=2.483867
I0608 20:40:25.589543 140625997584192 submission.py:296] 22500) loss = 2.484, grad_norm = 0.833
I0608 20:43:39.333308 140573440718592 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.637235, loss=2.464622
I0608 20:43:39.337587 140625997584192 submission.py:296] 23000) loss = 2.465, grad_norm = 0.637
I0608 20:46:51.899630 140573449111296 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.738297, loss=2.324486
I0608 20:46:51.904055 140625997584192 submission.py:296] 23500) loss = 2.324, grad_norm = 0.738
I0608 20:48:25.365010 140625997584192 spec.py:298] Evaluating on the training split.
I0608 20:49:10.211328 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 20:49:57.347939 140625997584192 spec.py:326] Evaluating on the test split.
I0608 20:49:58.724763 140625997584192 submission_runner.py:419] Time since start: 11048.47s, 	Step: 23742, 	{'train/accuracy': 0.7783601721938775, 'train/loss': 0.9754880009865274, 'validation/accuracy': 0.67568, 'validation/loss': 1.424681875, 'validation/num_examples': 50000, 'test/accuracy': 0.5317, 'test/loss': 2.1681126953125, 'test/num_examples': 10000, 'score': 9180.848364591599, 'total_duration': 11048.467314958572, 'accumulated_submission_time': 9180.848364591599, 'accumulated_eval_time': 1856.5083038806915, 'accumulated_logging_time': 0.38370418548583984}
I0608 20:49:58.735482 140573440718592 logging_writer.py:48] [23742] accumulated_eval_time=1856.508304, accumulated_logging_time=0.383704, accumulated_submission_time=9180.848365, global_step=23742, preemption_count=0, score=9180.848365, test/accuracy=0.531700, test/loss=2.168113, test/num_examples=10000, total_duration=11048.467315, train/accuracy=0.778360, train/loss=0.975488, validation/accuracy=0.675680, validation/loss=1.424682, validation/num_examples=50000
I0608 20:51:39.764223 140573449111296 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.661076, loss=2.333622
I0608 20:51:39.768720 140625997584192 submission.py:296] 24000) loss = 2.334, grad_norm = 0.661
I0608 20:54:52.040109 140573440718592 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.605337, loss=2.351157
I0608 20:54:52.044460 140625997584192 submission.py:296] 24500) loss = 2.351, grad_norm = 0.605
I0608 20:58:05.659299 140573449111296 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.905528, loss=2.352697
I0608 20:58:05.667658 140625997584192 submission.py:296] 25000) loss = 2.353, grad_norm = 0.906
I0608 20:58:29.077905 140625997584192 spec.py:298] Evaluating on the training split.
I0608 20:59:14.077889 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 21:00:06.458091 140625997584192 spec.py:326] Evaluating on the test split.
I0608 21:00:07.830406 140625997584192 submission_runner.py:419] Time since start: 11657.57s, 	Step: 25057, 	{'train/accuracy': 0.7817283163265306, 'train/loss': 0.9640533291563695, 'validation/accuracy': 0.67582, 'validation/loss': 1.42929765625, 'validation/num_examples': 50000, 'test/accuracy': 0.5318, 'test/loss': 2.1743380859375, 'test/num_examples': 10000, 'score': 9690.590930700302, 'total_duration': 11657.574691772461, 'accumulated_submission_time': 9690.590930700302, 'accumulated_eval_time': 1955.2607171535492, 'accumulated_logging_time': 0.40319108963012695}
I0608 21:00:07.841384 140573440718592 logging_writer.py:48] [25057] accumulated_eval_time=1955.260717, accumulated_logging_time=0.403191, accumulated_submission_time=9690.590931, global_step=25057, preemption_count=0, score=9690.590931, test/accuracy=0.531800, test/loss=2.174338, test/num_examples=10000, total_duration=11657.574692, train/accuracy=0.781728, train/loss=0.964053, validation/accuracy=0.675820, validation/loss=1.429298, validation/num_examples=50000
I0608 21:02:58.474308 140573449111296 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.622536, loss=2.279442
I0608 21:02:58.479328 140625997584192 submission.py:296] 25500) loss = 2.279, grad_norm = 0.623
I0608 21:06:11.355801 140573440718592 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.647824, loss=2.278352
I0608 21:06:11.366420 140625997584192 submission.py:296] 26000) loss = 2.278, grad_norm = 0.648
I0608 21:08:38.084082 140625997584192 spec.py:298] Evaluating on the training split.
I0608 21:09:22.816301 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 21:10:18.327756 140625997584192 spec.py:326] Evaluating on the test split.
I0608 21:10:19.700961 140625997584192 submission_runner.py:419] Time since start: 12269.44s, 	Step: 26376, 	{'train/accuracy': 0.7901387117346939, 'train/loss': 0.9398671364297673, 'validation/accuracy': 0.67954, 'validation/loss': 1.42300546875, 'validation/num_examples': 50000, 'test/accuracy': 0.5326, 'test/loss': 2.1928619140625, 'test/num_examples': 10000, 'score': 10200.2275826931, 'total_duration': 12269.443665504456, 'accumulated_submission_time': 10200.2275826931, 'accumulated_eval_time': 2056.876007795334, 'accumulated_logging_time': 0.4232809543609619}
I0608 21:10:19.712255 140573449111296 logging_writer.py:48] [26376] accumulated_eval_time=2056.876008, accumulated_logging_time=0.423281, accumulated_submission_time=10200.227583, global_step=26376, preemption_count=0, score=10200.227583, test/accuracy=0.532600, test/loss=2.192862, test/num_examples=10000, total_duration=12269.443666, train/accuracy=0.790139, train/loss=0.939867, validation/accuracy=0.679540, validation/loss=1.423005, validation/num_examples=50000
I0608 21:11:07.712880 140573440718592 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.870847, loss=2.381796
I0608 21:11:07.717180 140625997584192 submission.py:296] 26500) loss = 2.382, grad_norm = 0.871
I0608 21:14:19.875014 140573449111296 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.844387, loss=2.339862
I0608 21:14:19.881444 140625997584192 submission.py:296] 27000) loss = 2.340, grad_norm = 0.844
I0608 21:17:33.158725 140573440718592 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.692168, loss=2.254169
I0608 21:17:33.163839 140625997584192 submission.py:296] 27500) loss = 2.254, grad_norm = 0.692
I0608 21:18:49.994265 140625997584192 spec.py:298] Evaluating on the training split.
I0608 21:19:34.918826 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 21:20:24.934451 140625997584192 spec.py:326] Evaluating on the test split.
I0608 21:20:26.308387 140625997584192 submission_runner.py:419] Time since start: 12876.05s, 	Step: 27697, 	{'train/accuracy': 0.7938456632653061, 'train/loss': 0.8834201267787388, 'validation/accuracy': 0.68148, 'validation/loss': 1.3832715625, 'validation/num_examples': 50000, 'test/accuracy': 0.5453, 'test/loss': 2.105448046875, 'test/num_examples': 10000, 'score': 10709.906783819199, 'total_duration': 12876.052739858627, 'accumulated_submission_time': 10709.906783819199, 'accumulated_eval_time': 2153.1901354789734, 'accumulated_logging_time': 0.4440603256225586}
I0608 21:20:26.320322 140573449111296 logging_writer.py:48] [27697] accumulated_eval_time=2153.190135, accumulated_logging_time=0.444060, accumulated_submission_time=10709.906784, global_step=27697, preemption_count=0, score=10709.906784, test/accuracy=0.545300, test/loss=2.105448, test/num_examples=10000, total_duration=12876.052740, train/accuracy=0.793846, train/loss=0.883420, validation/accuracy=0.681480, validation/loss=1.383272, validation/num_examples=50000
I0608 21:22:22.624313 140625997584192 spec.py:298] Evaluating on the training split.
I0608 21:23:06.861112 140625997584192 spec.py:310] Evaluating on the validation split.
I0608 21:23:53.226447 140625997584192 spec.py:326] Evaluating on the test split.
I0608 21:23:54.598755 140625997584192 submission_runner.py:419] Time since start: 13084.34s, 	Step: 28000, 	{'train/accuracy': 0.8028539540816326, 'train/loss': 0.8710322477379624, 'validation/accuracy': 0.68866, 'validation/loss': 1.3759490625, 'validation/num_examples': 50000, 'test/accuracy': 0.5468, 'test/loss': 2.12481640625, 'test/num_examples': 10000, 'score': 10826.070421218872, 'total_duration': 13084.34306716919, 'accumulated_submission_time': 10826.070421218872, 'accumulated_eval_time': 2245.1645262241364, 'accumulated_logging_time': 0.46427059173583984}
I0608 21:23:54.609926 140573440718592 logging_writer.py:48] [28000] accumulated_eval_time=2245.164526, accumulated_logging_time=0.464271, accumulated_submission_time=10826.070421, global_step=28000, preemption_count=0, score=10826.070421, test/accuracy=0.546800, test/loss=2.124816, test/num_examples=10000, total_duration=13084.343067, train/accuracy=0.802854, train/loss=0.871032, validation/accuracy=0.688660, validation/loss=1.375949, validation/num_examples=50000
I0608 21:23:54.627608 140573449111296 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10826.070421
I0608 21:23:55.409947 140625997584192 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0608 21:23:55.706074 140625997584192 submission_runner.py:581] Tuning trial 1/1
I0608 21:23:55.706291 140625997584192 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0608 21:23:55.707309 140625997584192 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008370535714285714, 'train/loss': 6.924691336495536, 'validation/accuracy': 0.00084, 'validation/loss': 6.925395, 'validation/num_examples': 50000, 'test/accuracy': 0.0013, 'test/loss': 6.9280890625, 'test/num_examples': 10000, 'score': 8.307104110717773, 'total_duration': 135.3924424648285, 'accumulated_submission_time': 8.307104110717773, 'accumulated_eval_time': 127.0849061012268, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1308, {'train/accuracy': 0.11706792091836735, 'train/loss': 4.777226039341518, 'validation/accuracy': 0.1059, 'validation/loss': 4.8630571875, 'validation/num_examples': 50000, 'test/accuracy': 0.0696, 'test/loss': 5.30064296875, 'test/num_examples': 10000, 'score': 517.8533375263214, 'total_duration': 745.6706850528717, 'accumulated_submission_time': 517.8533375263214, 'accumulated_eval_time': 227.1411907672882, 'accumulated_logging_time': 0.025414466857910156, 'global_step': 1308, 'preemption_count': 0}), (2627, {'train/accuracy': 0.2649473852040816, 'train/loss': 3.6080783143335458, 'validation/accuracy': 0.24226, 'validation/loss': 3.743725625, 'validation/num_examples': 50000, 'test/accuracy': 0.1731, 'test/loss': 4.344927734375, 'test/num_examples': 10000, 'score': 1027.491409778595, 'total_duration': 1358.0648384094238, 'accumulated_submission_time': 1027.491409778595, 'accumulated_eval_time': 329.28884100914, 'accumulated_logging_time': 0.04461359977722168, 'global_step': 2627, 'preemption_count': 0}), (3947, {'train/accuracy': 0.39255819515306123, 'train/loss': 2.831575043347417, 'validation/accuracy': 0.36038, 'validation/loss': 3.011588125, 'validation/num_examples': 50000, 'test/accuracy': 0.259, 'test/loss': 3.712580078125, 'test/num_examples': 10000, 'score': 1537.1995556354523, 'total_duration': 1963.8153643608093, 'accumulated_submission_time': 1537.1995556354523, 'accumulated_eval_time': 424.7207202911377, 'accumulated_logging_time': 0.0685720443725586, 'global_step': 3947, 'preemption_count': 0}), (5265, {'train/accuracy': 0.46994579081632654, 'train/loss': 2.43906916404257, 'validation/accuracy': 0.42826, 'validation/loss': 2.640971875, 'validation/num_examples': 50000, 'test/accuracy': 0.3114, 'test/loss': 3.366155078125, 'test/num_examples': 10000, 'score': 2046.674776315689, 'total_duration': 2573.589118719101, 'accumulated_submission_time': 2046.674776315689, 'accumulated_eval_time': 524.4036891460419, 'accumulated_logging_time': 0.0917043685913086, 'global_step': 5265, 'preemption_count': 0}), (6585, {'train/accuracy': 0.5301937181122449, 'train/loss': 2.1486297918825734, 'validation/accuracy': 0.48468, 'validation/loss': 2.37033296875, 'validation/num_examples': 50000, 'test/accuracy': 0.3597, 'test/loss': 3.150373828125, 'test/num_examples': 10000, 'score': 2556.406012058258, 'total_duration': 3183.4241621494293, 'accumulated_submission_time': 2556.406012058258, 'accumulated_eval_time': 623.8940417766571, 'accumulated_logging_time': 0.11789536476135254, 'global_step': 6585, 'preemption_count': 0}), (7904, {'train/accuracy': 0.5787826849489796, 'train/loss': 1.8444100204779177, 'validation/accuracy': 0.52654, 'validation/loss': 2.0915446875, 'validation/num_examples': 50000, 'test/accuracy': 0.4007, 'test/loss': 2.8277609375, 'test/num_examples': 10000, 'score': 3065.798003911972, 'total_duration': 3786.0194430351257, 'accumulated_submission_time': 3065.798003911972, 'accumulated_eval_time': 716.483968257904, 'accumulated_logging_time': 0.13777661323547363, 'global_step': 7904, 'preemption_count': 0}), (9225, {'train/accuracy': 0.6096540178571429, 'train/loss': 1.708965067960778, 'validation/accuracy': 0.55734, 'validation/loss': 1.97201953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4208, 'test/loss': 2.7369892578125, 'test/num_examples': 10000, 'score': 3575.225672006607, 'total_duration': 4388.34682393074, 'accumulated_submission_time': 3575.225672006607, 'accumulated_eval_time': 808.7721037864685, 'accumulated_logging_time': 0.1563129425048828, 'global_step': 9225, 'preemption_count': 0}), (10545, {'train/accuracy': 0.6447305484693877, 'train/loss': 1.5726687372947226, 'validation/accuracy': 0.58188, 'validation/loss': 1.85750703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4407, 'test/loss': 2.6162478515625, 'test/num_examples': 10000, 'score': 4084.9771118164062, 'total_duration': 4991.519228935242, 'accumulated_submission_time': 4084.9771118164062, 'accumulated_eval_time': 901.5841298103333, 'accumulated_logging_time': 0.17508339881896973, 'global_step': 10545, 'preemption_count': 0}), (11866, {'train/accuracy': 0.6716757015306123, 'train/loss': 1.4374045547173948, 'validation/accuracy': 0.60616, 'validation/loss': 1.7493134375, 'validation/num_examples': 50000, 'test/accuracy': 0.4653, 'test/loss': 2.53922421875, 'test/num_examples': 10000, 'score': 4594.464698791504, 'total_duration': 5598.2267343997955, 'accumulated_submission_time': 4594.464698791504, 'accumulated_eval_time': 998.186891078949, 'accumulated_logging_time': 0.2006237506866455, 'global_step': 11866, 'preemption_count': 0}), (13187, {'train/accuracy': 0.6864237882653061, 'train/loss': 1.3833456234056123, 'validation/accuracy': 0.6167, 'validation/loss': 1.70272578125, 'validation/num_examples': 50000, 'test/accuracy': 0.4763, 'test/loss': 2.4389048828125, 'test/num_examples': 10000, 'score': 5104.159840345383, 'total_duration': 6201.554953813553, 'accumulated_submission_time': 5104.159840345383, 'accumulated_eval_time': 1091.2078711986542, 'accumulated_logging_time': 0.22185373306274414, 'global_step': 13187, 'preemption_count': 0}), (14506, {'train/accuracy': 0.7080676020408163, 'train/loss': 1.2585560545629384, 'validation/accuracy': 0.63484, 'validation/loss': 1.6114834375, 'validation/num_examples': 50000, 'test/accuracy': 0.4861, 'test/loss': 2.3946166015625, 'test/num_examples': 10000, 'score': 5613.642600774765, 'total_duration': 6805.541194438934, 'accumulated_submission_time': 5613.642600774765, 'accumulated_eval_time': 1185.092606306076, 'accumulated_logging_time': 0.24149608612060547, 'global_step': 14506, 'preemption_count': 0}), (15825, {'train/accuracy': 0.7253467793367347, 'train/loss': 1.2164442879813058, 'validation/accuracy': 0.6428, 'validation/loss': 1.5761675, 'validation/num_examples': 50000, 'test/accuracy': 0.4924, 'test/loss': 2.371537109375, 'test/num_examples': 10000, 'score': 6123.071384429932, 'total_duration': 7407.655592918396, 'accumulated_submission_time': 6123.071384429932, 'accumulated_eval_time': 1277.1653707027435, 'accumulated_logging_time': 0.26083898544311523, 'global_step': 15825, 'preemption_count': 0}), (17144, {'train/accuracy': 0.7315848214285714, 'train/loss': 1.206716420699139, 'validation/accuracy': 0.64668, 'validation/loss': 1.58421609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5112, 'test/loss': 2.27880546875, 'test/num_examples': 10000, 'score': 6632.469932317734, 'total_duration': 8012.728769779205, 'accumulated_submission_time': 6632.469932317734, 'accumulated_eval_time': 1372.223869562149, 'accumulated_logging_time': 0.2815859317779541, 'global_step': 17144, 'preemption_count': 0}), (18463, {'train/accuracy': 0.7461535395408163, 'train/loss': 1.1019312021683674, 'validation/accuracy': 0.65644, 'validation/loss': 1.50135375, 'validation/num_examples': 50000, 'test/accuracy': 0.5109, 'test/loss': 2.2571041015625, 'test/num_examples': 10000, 'score': 7142.026972293854, 'total_duration': 8617.283230543137, 'accumulated_submission_time': 7142.026972293854, 'accumulated_eval_time': 1466.6019623279572, 'accumulated_logging_time': 0.30153417587280273, 'global_step': 18463, 'preemption_count': 0}), (19783, {'train/accuracy': 0.7535076530612245, 'train/loss': 1.07894009959941, 'validation/accuracy': 0.66218, 'validation/loss': 1.48525796875, 'validation/num_examples': 50000, 'test/accuracy': 0.5274, 'test/loss': 2.1923275390625, 'test/num_examples': 10000, 'score': 7651.762818336487, 'total_duration': 9228.449648618698, 'accumulated_submission_time': 7651.762818336487, 'accumulated_eval_time': 1567.419855594635, 'accumulated_logging_time': 0.32286977767944336, 'global_step': 19783, 'preemption_count': 0}), (21102, {'train/accuracy': 0.7625956632653061, 'train/loss': 1.0334506910674426, 'validation/accuracy': 0.66856, 'validation/loss': 1.4563609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5246, 'test/loss': 2.186473828125, 'test/num_examples': 10000, 'score': 8161.425109386444, 'total_duration': 9832.428794145584, 'accumulated_submission_time': 8161.425109386444, 'accumulated_eval_time': 1661.1196460723877, 'accumulated_logging_time': 0.3441891670227051, 'global_step': 21102, 'preemption_count': 0}), (22422, {'train/accuracy': 0.7684351084183674, 'train/loss': 1.0292472060845823, 'validation/accuracy': 0.66902, 'validation/loss': 1.4651525, 'validation/num_examples': 50000, 'test/accuracy': 0.5259, 'test/loss': 2.1977400390625, 'test/num_examples': 10000, 'score': 8671.146996974945, 'total_duration': 10444.79356598854, 'accumulated_submission_time': 8671.146996974945, 'accumulated_eval_time': 1763.1503958702087, 'accumulated_logging_time': 0.36335325241088867, 'global_step': 22422, 'preemption_count': 0}), (23742, {'train/accuracy': 0.7783601721938775, 'train/loss': 0.9754880009865274, 'validation/accuracy': 0.67568, 'validation/loss': 1.424681875, 'validation/num_examples': 50000, 'test/accuracy': 0.5317, 'test/loss': 2.1681126953125, 'test/num_examples': 10000, 'score': 9180.848364591599, 'total_duration': 11048.467314958572, 'accumulated_submission_time': 9180.848364591599, 'accumulated_eval_time': 1856.5083038806915, 'accumulated_logging_time': 0.38370418548583984, 'global_step': 23742, 'preemption_count': 0}), (25057, {'train/accuracy': 0.7817283163265306, 'train/loss': 0.9640533291563695, 'validation/accuracy': 0.67582, 'validation/loss': 1.42929765625, 'validation/num_examples': 50000, 'test/accuracy': 0.5318, 'test/loss': 2.1743380859375, 'test/num_examples': 10000, 'score': 9690.590930700302, 'total_duration': 11657.574691772461, 'accumulated_submission_time': 9690.590930700302, 'accumulated_eval_time': 1955.2607171535492, 'accumulated_logging_time': 0.40319108963012695, 'global_step': 25057, 'preemption_count': 0}), (26376, {'train/accuracy': 0.7901387117346939, 'train/loss': 0.9398671364297673, 'validation/accuracy': 0.67954, 'validation/loss': 1.42300546875, 'validation/num_examples': 50000, 'test/accuracy': 0.5326, 'test/loss': 2.1928619140625, 'test/num_examples': 10000, 'score': 10200.2275826931, 'total_duration': 12269.443665504456, 'accumulated_submission_time': 10200.2275826931, 'accumulated_eval_time': 2056.876007795334, 'accumulated_logging_time': 0.4232809543609619, 'global_step': 26376, 'preemption_count': 0}), (27697, {'train/accuracy': 0.7938456632653061, 'train/loss': 0.8834201267787388, 'validation/accuracy': 0.68148, 'validation/loss': 1.3832715625, 'validation/num_examples': 50000, 'test/accuracy': 0.5453, 'test/loss': 2.105448046875, 'test/num_examples': 10000, 'score': 10709.906783819199, 'total_duration': 12876.052739858627, 'accumulated_submission_time': 10709.906783819199, 'accumulated_eval_time': 2153.1901354789734, 'accumulated_logging_time': 0.4440603256225586, 'global_step': 27697, 'preemption_count': 0}), (28000, {'train/accuracy': 0.8028539540816326, 'train/loss': 0.8710322477379624, 'validation/accuracy': 0.68866, 'validation/loss': 1.3759490625, 'validation/num_examples': 50000, 'test/accuracy': 0.5468, 'test/loss': 2.12481640625, 'test/num_examples': 10000, 'score': 10826.070421218872, 'total_duration': 13084.34306716919, 'accumulated_submission_time': 10826.070421218872, 'accumulated_eval_time': 2245.1645262241364, 'accumulated_logging_time': 0.46427059173583984, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0608 21:23:55.707435 140625997584192 submission_runner.py:584] Timing: 10826.070421218872
I0608 21:23:55.707485 140625997584192 submission_runner.py:586] Total number of evals: 23
I0608 21:23:55.707554 140625997584192 submission_runner.py:587] ====================
I0608 21:23:55.707739 140625997584192 submission_runner.py:655] Final imagenet_resnet score: 10826.070421218872
