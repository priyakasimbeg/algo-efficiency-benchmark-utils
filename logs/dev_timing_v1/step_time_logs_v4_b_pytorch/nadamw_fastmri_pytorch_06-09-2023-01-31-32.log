torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=5428 2>&1 | tee -a /logs/fastmri_pytorch_06-09-2023-01-31-32.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 01:31:55.334444 140319510558528 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 01:31:55.334475 140047807579968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 01:31:55.334507 140698923284288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 01:31:55.335406 139646661175104 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 01:31:55.335413 140149424535360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 01:31:55.335502 140097639335744 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 01:31:55.336163 140387061000000 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 01:31:55.345591 140061952972608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 01:31:55.345881 140061952972608 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.346021 140097639335744 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.346045 140149424535360 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.346114 139646661175104 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.346685 140387061000000 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.355421 140319510558528 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.355441 140047807579968 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.355461 140698923284288 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 01:31:55.928092 140061952972608 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nadamw/fastmri_pytorch.
W0609 01:31:55.971892 140061952972608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.972462 140319510558528 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.973713 139646661175104 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.973959 140698923284288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.975470 140149424535360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.976639 140387061000000 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.976957 140047807579968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 01:31:55.978129 140097639335744 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 01:31:55.977930 140061952972608 submission_runner.py:541] Using RNG seed 2204337716
I0609 01:31:55.979418 140061952972608 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 01:31:55.979572 140061952972608 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nadamw/fastmri_pytorch/trial_1.
I0609 01:31:55.980217 140061952972608 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nadamw/fastmri_pytorch/trial_1/hparams.json.
I0609 01:31:55.981159 140061952972608 submission_runner.py:255] Initializing dataset.
I0609 01:31:55.981284 140061952972608 submission_runner.py:262] Initializing model.
I0609 01:32:00.123944 140061952972608 submission_runner.py:272] Initializing optimizer.
I0609 01:32:00.124945 140061952972608 submission_runner.py:279] Initializing metrics bundle.
I0609 01:32:00.125071 140061952972608 submission_runner.py:297] Initializing checkpoint and logger.
I0609 01:32:00.128051 140061952972608 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 01:32:00.128161 140061952972608 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 01:32:00.637727 140061952972608 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nadamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0609 01:32:00.638631 140061952972608 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nadamw/fastmri_pytorch/trial_1/flags_0.json.
I0609 01:32:00.687293 140061952972608 submission_runner.py:332] Starting training loop.
I0609 01:32:45.213337 140019842930432 logging_writer.py:48] [0] global_step=0, grad_norm=6.614142, loss=1.194369
I0609 01:32:45.223649 140061952972608 submission.py:296] 0) loss = 1.194, grad_norm = 6.614
I0609 01:32:45.225531 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:34:19.123439 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:35:21.274244 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:36:20.296520 140061952972608 submission_runner.py:419] Time since start: 259.61s, 	Step: 1, 	{'train/ssim': 0.16000606332506453, 'train/loss': 1.189906392778669, 'validation/ssim': 0.15333123494214265, 'validation/loss': 1.1928664225168824, 'validation/num_examples': 3554, 'test/ssim': 0.1770178041988097, 'test/loss': 1.1892374140428652, 'test/num_examples': 3581, 'score': 44.537604570388794, 'total_duration': 259.60966753959656, 'accumulated_submission_time': 44.537604570388794, 'accumulated_eval_time': 215.07096552848816, 'accumulated_logging_time': 0}
I0609 01:36:20.313489 139994526119680 logging_writer.py:48] [1] accumulated_eval_time=215.070966, accumulated_logging_time=0, accumulated_submission_time=44.537605, global_step=1, preemption_count=0, score=44.537605, test/loss=1.189237, test/num_examples=3581, test/ssim=0.177018, total_duration=259.609668, train/loss=1.189906, train/ssim=0.160006, validation/loss=1.192866, validation/num_examples=3554, validation/ssim=0.153331
I0609 01:36:20.339319 140061952972608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339297 140149424535360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339302 140047807579968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339305 140698923284288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339319 140387061000000 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339407 139646661175104 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339499 140319510558528 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.339510 140097639335744 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 01:36:20.400015 139994517726976 logging_writer.py:48] [1] global_step=1, grad_norm=6.461233, loss=1.196901
I0609 01:36:20.405641 140061952972608 submission.py:296] 1) loss = 1.197, grad_norm = 6.461
I0609 01:36:20.485551 139994526119680 logging_writer.py:48] [2] global_step=2, grad_norm=6.167920, loss=1.157598
I0609 01:36:20.490981 140061952972608 submission.py:296] 2) loss = 1.158, grad_norm = 6.168
I0609 01:36:20.574790 139994517726976 logging_writer.py:48] [3] global_step=3, grad_norm=6.095213, loss=1.126303
I0609 01:36:20.578750 140061952972608 submission.py:296] 3) loss = 1.126, grad_norm = 6.095
I0609 01:36:20.649011 139994526119680 logging_writer.py:48] [4] global_step=4, grad_norm=5.330944, loss=1.132356
I0609 01:36:20.653154 140061952972608 submission.py:296] 4) loss = 1.132, grad_norm = 5.331
I0609 01:36:20.734300 139994517726976 logging_writer.py:48] [5] global_step=5, grad_norm=5.120922, loss=1.166113
I0609 01:36:20.740906 140061952972608 submission.py:296] 5) loss = 1.166, grad_norm = 5.121
I0609 01:36:20.818408 139994526119680 logging_writer.py:48] [6] global_step=6, grad_norm=5.746346, loss=1.168600
I0609 01:36:20.824839 140061952972608 submission.py:296] 6) loss = 1.169, grad_norm = 5.746
I0609 01:36:20.896215 139994517726976 logging_writer.py:48] [7] global_step=7, grad_norm=5.404254, loss=1.172584
I0609 01:36:20.899927 140061952972608 submission.py:296] 7) loss = 1.173, grad_norm = 5.404
I0609 01:36:20.972621 139994526119680 logging_writer.py:48] [8] global_step=8, grad_norm=5.037091, loss=1.104666
I0609 01:36:20.979165 140061952972608 submission.py:296] 8) loss = 1.105, grad_norm = 5.037
I0609 01:36:21.062925 139994517726976 logging_writer.py:48] [9] global_step=9, grad_norm=5.913233, loss=1.164363
I0609 01:36:21.068207 140061952972608 submission.py:296] 9) loss = 1.164, grad_norm = 5.913
I0609 01:36:21.146826 139994526119680 logging_writer.py:48] [10] global_step=10, grad_norm=5.301663, loss=1.024122
I0609 01:36:21.151726 140061952972608 submission.py:296] 10) loss = 1.024, grad_norm = 5.302
I0609 01:36:21.217324 139994517726976 logging_writer.py:48] [11] global_step=11, grad_norm=5.239674, loss=1.096804
I0609 01:36:21.221094 140061952972608 submission.py:296] 11) loss = 1.097, grad_norm = 5.240
I0609 01:36:21.303627 139994526119680 logging_writer.py:48] [12] global_step=12, grad_norm=4.669345, loss=1.030781
I0609 01:36:21.309913 140061952972608 submission.py:296] 12) loss = 1.031, grad_norm = 4.669
I0609 01:36:21.387901 139994517726976 logging_writer.py:48] [13] global_step=13, grad_norm=4.311398, loss=1.061768
I0609 01:36:21.393792 140061952972608 submission.py:296] 13) loss = 1.062, grad_norm = 4.311
I0609 01:36:21.466578 139994526119680 logging_writer.py:48] [14] global_step=14, grad_norm=4.019361, loss=1.034772
I0609 01:36:21.469992 140061952972608 submission.py:296] 14) loss = 1.035, grad_norm = 4.019
I0609 01:36:21.703413 139994517726976 logging_writer.py:48] [15] global_step=15, grad_norm=3.994797, loss=0.978028
I0609 01:36:21.708928 140061952972608 submission.py:296] 15) loss = 0.978, grad_norm = 3.995
I0609 01:36:22.026899 139994526119680 logging_writer.py:48] [16] global_step=16, grad_norm=3.697347, loss=0.985067
I0609 01:36:22.030603 140061952972608 submission.py:296] 16) loss = 0.985, grad_norm = 3.697
I0609 01:36:22.310641 139994517726976 logging_writer.py:48] [17] global_step=17, grad_norm=3.506236, loss=0.956343
I0609 01:36:22.316045 140061952972608 submission.py:296] 17) loss = 0.956, grad_norm = 3.506
I0609 01:36:22.537479 139994526119680 logging_writer.py:48] [18] global_step=18, grad_norm=3.219229, loss=0.938578
I0609 01:36:22.542060 140061952972608 submission.py:296] 18) loss = 0.939, grad_norm = 3.219
I0609 01:36:22.793812 139994517726976 logging_writer.py:48] [19] global_step=19, grad_norm=2.759927, loss=0.980847
I0609 01:36:22.800511 140061952972608 submission.py:296] 19) loss = 0.981, grad_norm = 2.760
I0609 01:36:23.106779 139994526119680 logging_writer.py:48] [20] global_step=20, grad_norm=2.897691, loss=0.943384
I0609 01:36:23.112785 140061952972608 submission.py:296] 20) loss = 0.943, grad_norm = 2.898
I0609 01:36:23.362640 139994517726976 logging_writer.py:48] [21] global_step=21, grad_norm=2.845302, loss=0.900257
I0609 01:36:23.368954 140061952972608 submission.py:296] 21) loss = 0.900, grad_norm = 2.845
I0609 01:36:23.672770 139994526119680 logging_writer.py:48] [22] global_step=22, grad_norm=2.695425, loss=0.923210
I0609 01:36:23.678585 140061952972608 submission.py:296] 22) loss = 0.923, grad_norm = 2.695
I0609 01:36:23.915450 139994517726976 logging_writer.py:48] [23] global_step=23, grad_norm=2.421289, loss=0.922545
I0609 01:36:23.921027 140061952972608 submission.py:296] 23) loss = 0.923, grad_norm = 2.421
I0609 01:36:24.196604 139994526119680 logging_writer.py:48] [24] global_step=24, grad_norm=2.390386, loss=0.889857
I0609 01:36:24.202989 140061952972608 submission.py:296] 24) loss = 0.890, grad_norm = 2.390
I0609 01:36:24.496686 139994517726976 logging_writer.py:48] [25] global_step=25, grad_norm=1.961744, loss=0.823187
I0609 01:36:24.501954 140061952972608 submission.py:296] 25) loss = 0.823, grad_norm = 1.962
I0609 01:36:24.744492 139994526119680 logging_writer.py:48] [26] global_step=26, grad_norm=1.935077, loss=0.845594
I0609 01:36:24.750387 140061952972608 submission.py:296] 26) loss = 0.846, grad_norm = 1.935
I0609 01:36:25.005428 139994517726976 logging_writer.py:48] [27] global_step=27, grad_norm=2.001923, loss=0.908802
I0609 01:36:25.008763 140061952972608 submission.py:296] 27) loss = 0.909, grad_norm = 2.002
I0609 01:36:25.270189 139994526119680 logging_writer.py:48] [28] global_step=28, grad_norm=1.848773, loss=0.852402
I0609 01:36:25.273956 140061952972608 submission.py:296] 28) loss = 0.852, grad_norm = 1.849
I0609 01:36:25.547677 139994517726976 logging_writer.py:48] [29] global_step=29, grad_norm=1.783243, loss=0.833591
I0609 01:36:25.551771 140061952972608 submission.py:296] 29) loss = 0.834, grad_norm = 1.783
I0609 01:36:25.866455 139994526119680 logging_writer.py:48] [30] global_step=30, grad_norm=1.559423, loss=0.812650
I0609 01:36:25.870274 140061952972608 submission.py:296] 30) loss = 0.813, grad_norm = 1.559
I0609 01:36:26.126624 139994517726976 logging_writer.py:48] [31] global_step=31, grad_norm=1.460990, loss=0.817040
I0609 01:36:26.130613 140061952972608 submission.py:296] 31) loss = 0.817, grad_norm = 1.461
I0609 01:36:26.370125 139994526119680 logging_writer.py:48] [32] global_step=32, grad_norm=1.690531, loss=0.768791
I0609 01:36:26.374151 140061952972608 submission.py:296] 32) loss = 0.769, grad_norm = 1.691
I0609 01:36:26.603854 139994517726976 logging_writer.py:48] [33] global_step=33, grad_norm=1.620543, loss=0.824899
I0609 01:36:26.610079 140061952972608 submission.py:296] 33) loss = 0.825, grad_norm = 1.621
I0609 01:36:26.904607 139994526119680 logging_writer.py:48] [34] global_step=34, grad_norm=1.485296, loss=0.814549
I0609 01:36:26.908190 140061952972608 submission.py:296] 34) loss = 0.815, grad_norm = 1.485
I0609 01:36:27.174030 139994517726976 logging_writer.py:48] [35] global_step=35, grad_norm=1.604027, loss=0.771271
I0609 01:36:27.179430 140061952972608 submission.py:296] 35) loss = 0.771, grad_norm = 1.604
I0609 01:36:27.428004 139994526119680 logging_writer.py:48] [36] global_step=36, grad_norm=1.544613, loss=0.750292
I0609 01:36:27.433917 140061952972608 submission.py:296] 36) loss = 0.750, grad_norm = 1.545
I0609 01:36:27.702696 139994517726976 logging_writer.py:48] [37] global_step=37, grad_norm=1.532202, loss=0.728985
I0609 01:36:27.708384 140061952972608 submission.py:296] 37) loss = 0.729, grad_norm = 1.532
I0609 01:36:27.918458 139994526119680 logging_writer.py:48] [38] global_step=38, grad_norm=1.657512, loss=0.734909
I0609 01:36:27.926605 140061952972608 submission.py:296] 38) loss = 0.735, grad_norm = 1.658
I0609 01:36:28.232359 139994517726976 logging_writer.py:48] [39] global_step=39, grad_norm=1.594000, loss=0.737035
I0609 01:36:28.238371 140061952972608 submission.py:296] 39) loss = 0.737, grad_norm = 1.594
I0609 01:36:28.461558 139994526119680 logging_writer.py:48] [40] global_step=40, grad_norm=1.567940, loss=0.726064
I0609 01:36:28.467467 140061952972608 submission.py:296] 40) loss = 0.726, grad_norm = 1.568
I0609 01:36:28.766968 139994517726976 logging_writer.py:48] [41] global_step=41, grad_norm=1.435611, loss=0.724523
I0609 01:36:28.773080 140061952972608 submission.py:296] 41) loss = 0.725, grad_norm = 1.436
I0609 01:36:29.009304 139994526119680 logging_writer.py:48] [42] global_step=42, grad_norm=1.375313, loss=0.728702
I0609 01:36:29.013915 140061952972608 submission.py:296] 42) loss = 0.729, grad_norm = 1.375
I0609 01:36:29.288855 139994517726976 logging_writer.py:48] [43] global_step=43, grad_norm=1.409713, loss=0.692547
I0609 01:36:29.294596 140061952972608 submission.py:296] 43) loss = 0.693, grad_norm = 1.410
I0609 01:36:29.558011 139994526119680 logging_writer.py:48] [44] global_step=44, grad_norm=1.324504, loss=0.733445
I0609 01:36:29.561849 140061952972608 submission.py:296] 44) loss = 0.733, grad_norm = 1.325
I0609 01:36:29.820037 139994517726976 logging_writer.py:48] [45] global_step=45, grad_norm=1.256696, loss=0.833925
I0609 01:36:29.823477 140061952972608 submission.py:296] 45) loss = 0.834, grad_norm = 1.257
I0609 01:36:30.093011 139994526119680 logging_writer.py:48] [46] global_step=46, grad_norm=1.346711, loss=0.709672
I0609 01:36:30.097252 140061952972608 submission.py:296] 46) loss = 0.710, grad_norm = 1.347
I0609 01:36:30.354629 139994517726976 logging_writer.py:48] [47] global_step=47, grad_norm=1.258361, loss=0.757063
I0609 01:36:30.358345 140061952972608 submission.py:296] 47) loss = 0.757, grad_norm = 1.258
I0609 01:36:30.612915 139994526119680 logging_writer.py:48] [48] global_step=48, grad_norm=1.383843, loss=0.679402
I0609 01:36:30.616793 140061952972608 submission.py:296] 48) loss = 0.679, grad_norm = 1.384
I0609 01:36:30.878380 139994517726976 logging_writer.py:48] [49] global_step=49, grad_norm=1.291787, loss=0.715511
I0609 01:36:30.882257 140061952972608 submission.py:296] 49) loss = 0.716, grad_norm = 1.292
I0609 01:36:31.146985 139994526119680 logging_writer.py:48] [50] global_step=50, grad_norm=1.369105, loss=0.683321
I0609 01:36:31.153220 140061952972608 submission.py:296] 50) loss = 0.683, grad_norm = 1.369
I0609 01:36:31.439672 139994517726976 logging_writer.py:48] [51] global_step=51, grad_norm=1.343017, loss=0.666299
I0609 01:36:31.445698 140061952972608 submission.py:296] 51) loss = 0.666, grad_norm = 1.343
I0609 01:36:31.685313 139994526119680 logging_writer.py:48] [52] global_step=52, grad_norm=1.269601, loss=0.669661
I0609 01:36:31.691709 140061952972608 submission.py:296] 52) loss = 0.670, grad_norm = 1.270
I0609 01:36:32.017077 139994517726976 logging_writer.py:48] [53] global_step=53, grad_norm=1.283652, loss=0.647521
I0609 01:36:32.020836 140061952972608 submission.py:296] 53) loss = 0.648, grad_norm = 1.284
I0609 01:36:32.228342 139994526119680 logging_writer.py:48] [54] global_step=54, grad_norm=1.432334, loss=0.606521
I0609 01:36:32.233606 140061952972608 submission.py:296] 54) loss = 0.607, grad_norm = 1.432
I0609 01:36:32.519066 139994517726976 logging_writer.py:48] [55] global_step=55, grad_norm=1.238678, loss=0.620496
I0609 01:36:32.523931 140061952972608 submission.py:296] 55) loss = 0.620, grad_norm = 1.239
I0609 01:36:32.813834 139994526119680 logging_writer.py:48] [56] global_step=56, grad_norm=1.296460, loss=0.617083
I0609 01:36:32.820662 140061952972608 submission.py:296] 56) loss = 0.617, grad_norm = 1.296
I0609 01:36:33.082533 139994517726976 logging_writer.py:48] [57] global_step=57, grad_norm=1.243945, loss=0.645888
I0609 01:36:33.087890 140061952972608 submission.py:296] 57) loss = 0.646, grad_norm = 1.244
I0609 01:36:33.371703 139994526119680 logging_writer.py:48] [58] global_step=58, grad_norm=1.185703, loss=0.653621
I0609 01:36:33.378370 140061952972608 submission.py:296] 58) loss = 0.654, grad_norm = 1.186
I0609 01:36:33.669273 139994517726976 logging_writer.py:48] [59] global_step=59, grad_norm=1.220754, loss=0.593299
I0609 01:36:33.674733 140061952972608 submission.py:296] 59) loss = 0.593, grad_norm = 1.221
I0609 01:36:33.911448 139994526119680 logging_writer.py:48] [60] global_step=60, grad_norm=1.101390, loss=0.645252
I0609 01:36:33.920417 140061952972608 submission.py:296] 60) loss = 0.645, grad_norm = 1.101
I0609 01:36:34.205517 139994517726976 logging_writer.py:48] [61] global_step=61, grad_norm=1.161352, loss=0.613385
I0609 01:36:34.210483 140061952972608 submission.py:296] 61) loss = 0.613, grad_norm = 1.161
I0609 01:36:34.479943 139994526119680 logging_writer.py:48] [62] global_step=62, grad_norm=1.134031, loss=0.587586
I0609 01:36:34.483553 140061952972608 submission.py:296] 62) loss = 0.588, grad_norm = 1.134
I0609 01:36:34.759472 139994517726976 logging_writer.py:48] [63] global_step=63, grad_norm=1.259718, loss=0.523510
I0609 01:36:34.763509 140061952972608 submission.py:296] 63) loss = 0.524, grad_norm = 1.260
I0609 01:36:35.059648 139994526119680 logging_writer.py:48] [64] global_step=64, grad_norm=1.148948, loss=0.585013
I0609 01:36:35.063766 140061952972608 submission.py:296] 64) loss = 0.585, grad_norm = 1.149
I0609 01:36:35.332722 139994517726976 logging_writer.py:48] [65] global_step=65, grad_norm=1.196055, loss=0.533510
I0609 01:36:35.336716 140061952972608 submission.py:296] 65) loss = 0.534, grad_norm = 1.196
I0609 01:36:35.616538 139994526119680 logging_writer.py:48] [66] global_step=66, grad_norm=1.166347, loss=0.498426
I0609 01:36:35.623307 140061952972608 submission.py:296] 66) loss = 0.498, grad_norm = 1.166
I0609 01:36:35.880972 139994517726976 logging_writer.py:48] [67] global_step=67, grad_norm=0.927796, loss=0.555205
I0609 01:36:35.887672 140061952972608 submission.py:296] 67) loss = 0.555, grad_norm = 0.928
I0609 01:36:36.140026 139994526119680 logging_writer.py:48] [68] global_step=68, grad_norm=1.197171, loss=0.523411
I0609 01:36:36.145558 140061952972608 submission.py:296] 68) loss = 0.523, grad_norm = 1.197
I0609 01:36:36.372785 139994517726976 logging_writer.py:48] [69] global_step=69, grad_norm=0.948583, loss=0.521127
I0609 01:36:36.376367 140061952972608 submission.py:296] 69) loss = 0.521, grad_norm = 0.949
I0609 01:36:36.637336 139994526119680 logging_writer.py:48] [70] global_step=70, grad_norm=0.966928, loss=0.550382
I0609 01:36:36.640797 140061952972608 submission.py:296] 70) loss = 0.550, grad_norm = 0.967
I0609 01:36:36.979941 139994517726976 logging_writer.py:48] [71] global_step=71, grad_norm=0.850733, loss=0.543278
I0609 01:36:36.983470 140061952972608 submission.py:296] 71) loss = 0.543, grad_norm = 0.851
I0609 01:36:37.261503 139994526119680 logging_writer.py:48] [72] global_step=72, grad_norm=0.956172, loss=0.568991
I0609 01:36:37.265571 140061952972608 submission.py:296] 72) loss = 0.569, grad_norm = 0.956
I0609 01:36:37.508551 139994517726976 logging_writer.py:48] [73] global_step=73, grad_norm=0.935198, loss=0.481176
I0609 01:36:37.514341 140061952972608 submission.py:296] 73) loss = 0.481, grad_norm = 0.935
I0609 01:36:37.800971 139994526119680 logging_writer.py:48] [74] global_step=74, grad_norm=0.794504, loss=0.472461
I0609 01:36:37.808263 140061952972608 submission.py:296] 74) loss = 0.472, grad_norm = 0.795
I0609 01:36:38.078988 139994517726976 logging_writer.py:48] [75] global_step=75, grad_norm=1.006537, loss=0.455872
I0609 01:36:38.085720 140061952972608 submission.py:296] 75) loss = 0.456, grad_norm = 1.007
I0609 01:36:38.353073 139994526119680 logging_writer.py:48] [76] global_step=76, grad_norm=1.066649, loss=0.460624
I0609 01:36:38.358676 140061952972608 submission.py:296] 76) loss = 0.461, grad_norm = 1.067
I0609 01:36:38.646726 139994517726976 logging_writer.py:48] [77] global_step=77, grad_norm=0.916298, loss=0.459962
I0609 01:36:38.652564 140061952972608 submission.py:296] 77) loss = 0.460, grad_norm = 0.916
I0609 01:36:38.905688 139994526119680 logging_writer.py:48] [78] global_step=78, grad_norm=0.946016, loss=0.461163
I0609 01:36:38.911316 140061952972608 submission.py:296] 78) loss = 0.461, grad_norm = 0.946
I0609 01:36:39.116827 139994517726976 logging_writer.py:48] [79] global_step=79, grad_norm=1.046851, loss=0.406919
I0609 01:36:39.121073 140061952972608 submission.py:296] 79) loss = 0.407, grad_norm = 1.047
I0609 01:36:39.405752 139994526119680 logging_writer.py:48] [80] global_step=80, grad_norm=0.851596, loss=0.407599
I0609 01:36:39.409266 140061952972608 submission.py:296] 80) loss = 0.408, grad_norm = 0.852
I0609 01:36:39.678538 139994517726976 logging_writer.py:48] [81] global_step=81, grad_norm=1.051183, loss=0.405466
I0609 01:36:39.682497 140061952972608 submission.py:296] 81) loss = 0.405, grad_norm = 1.051
I0609 01:36:39.936877 139994526119680 logging_writer.py:48] [82] global_step=82, grad_norm=0.805892, loss=0.439203
I0609 01:36:39.943878 140061952972608 submission.py:296] 82) loss = 0.439, grad_norm = 0.806
I0609 01:36:40.191015 139994517726976 logging_writer.py:48] [83] global_step=83, grad_norm=0.835717, loss=0.450236
I0609 01:36:40.195730 140061952972608 submission.py:296] 83) loss = 0.450, grad_norm = 0.836
I0609 01:36:40.478183 139994526119680 logging_writer.py:48] [84] global_step=84, grad_norm=0.711177, loss=0.547212
I0609 01:36:40.486491 140061952972608 submission.py:296] 84) loss = 0.547, grad_norm = 0.711
I0609 01:36:40.764022 139994517726976 logging_writer.py:48] [85] global_step=85, grad_norm=0.741270, loss=0.393795
I0609 01:36:40.770137 140061952972608 submission.py:296] 85) loss = 0.394, grad_norm = 0.741
I0609 01:36:41.066521 139994526119680 logging_writer.py:48] [86] global_step=86, grad_norm=0.746420, loss=0.390219
I0609 01:36:41.073355 140061952972608 submission.py:296] 86) loss = 0.390, grad_norm = 0.746
I0609 01:36:41.281509 139994517726976 logging_writer.py:48] [87] global_step=87, grad_norm=0.754814, loss=0.372289
I0609 01:36:41.284815 140061952972608 submission.py:296] 87) loss = 0.372, grad_norm = 0.755
I0609 01:36:41.586387 139994526119680 logging_writer.py:48] [88] global_step=88, grad_norm=0.658907, loss=0.435318
I0609 01:36:41.590197 140061952972608 submission.py:296] 88) loss = 0.435, grad_norm = 0.659
I0609 01:36:41.924656 139994517726976 logging_writer.py:48] [89] global_step=89, grad_norm=0.706277, loss=0.412741
I0609 01:36:41.928218 140061952972608 submission.py:296] 89) loss = 0.413, grad_norm = 0.706
I0609 01:36:42.204542 139994526119680 logging_writer.py:48] [90] global_step=90, grad_norm=0.689721, loss=0.347181
I0609 01:36:42.208456 140061952972608 submission.py:296] 90) loss = 0.347, grad_norm = 0.690
I0609 01:36:42.406570 139994517726976 logging_writer.py:48] [91] global_step=91, grad_norm=0.571707, loss=0.484462
I0609 01:36:42.410684 140061952972608 submission.py:296] 91) loss = 0.484, grad_norm = 0.572
I0609 01:36:42.670562 139994526119680 logging_writer.py:48] [92] global_step=92, grad_norm=0.665287, loss=0.329471
I0609 01:36:42.676213 140061952972608 submission.py:296] 92) loss = 0.329, grad_norm = 0.665
I0609 01:36:42.939206 139994517726976 logging_writer.py:48] [93] global_step=93, grad_norm=0.642608, loss=0.401325
I0609 01:36:42.944476 140061952972608 submission.py:296] 93) loss = 0.401, grad_norm = 0.643
I0609 01:36:43.230958 139994526119680 logging_writer.py:48] [94] global_step=94, grad_norm=0.643703, loss=0.338156
I0609 01:36:43.234553 140061952972608 submission.py:296] 94) loss = 0.338, grad_norm = 0.644
I0609 01:36:43.523065 139994517726976 logging_writer.py:48] [95] global_step=95, grad_norm=0.579679, loss=0.341321
I0609 01:36:43.526704 140061952972608 submission.py:296] 95) loss = 0.341, grad_norm = 0.580
I0609 01:36:43.750619 139994526119680 logging_writer.py:48] [96] global_step=96, grad_norm=0.571462, loss=0.318468
I0609 01:36:43.754025 140061952972608 submission.py:296] 96) loss = 0.318, grad_norm = 0.571
I0609 01:36:44.079781 139994517726976 logging_writer.py:48] [97] global_step=97, grad_norm=0.448440, loss=0.354342
I0609 01:36:44.085439 140061952972608 submission.py:296] 97) loss = 0.354, grad_norm = 0.448
I0609 01:36:44.347650 139994526119680 logging_writer.py:48] [98] global_step=98, grad_norm=0.483164, loss=0.404339
I0609 01:36:44.351339 140061952972608 submission.py:296] 98) loss = 0.404, grad_norm = 0.483
I0609 01:36:44.574647 139994517726976 logging_writer.py:48] [99] global_step=99, grad_norm=0.469826, loss=0.340184
I0609 01:36:44.581094 140061952972608 submission.py:296] 99) loss = 0.340, grad_norm = 0.470
I0609 01:36:44.822159 139994526119680 logging_writer.py:48] [100] global_step=100, grad_norm=0.493310, loss=0.330740
I0609 01:36:44.825685 140061952972608 submission.py:296] 100) loss = 0.331, grad_norm = 0.493
I0609 01:37:40.299808 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:37:42.612662 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:37:44.971511 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:37:47.272967 140061952972608 submission_runner.py:419] Time since start: 346.59s, 	Step: 305, 	{'train/ssim': 0.7003750801086426, 'train/loss': 0.29859161376953125, 'validation/ssim': 0.6777707831404756, 'validation/loss': 0.32285749771384353, 'validation/num_examples': 3554, 'test/ssim': 0.6965263172821837, 'test/loss': 0.32393401017304874, 'test/num_examples': 3581, 'score': 124.28650259971619, 'total_duration': 346.5861577987671, 'accumulated_submission_time': 124.28650259971619, 'accumulated_eval_time': 222.04425930976868, 'accumulated_logging_time': 0.025522232055664062}
I0609 01:37:47.284506 139994517726976 logging_writer.py:48] [305] accumulated_eval_time=222.044259, accumulated_logging_time=0.025522, accumulated_submission_time=124.286503, global_step=305, preemption_count=0, score=124.286503, test/loss=0.323934, test/num_examples=3581, test/ssim=0.696526, total_duration=346.586158, train/loss=0.298592, train/ssim=0.700375, validation/loss=0.322857, validation/num_examples=3554, validation/ssim=0.677771
I0609 01:38:50.212494 139994526119680 logging_writer.py:48] [500] global_step=500, grad_norm=0.097136, loss=0.294092
I0609 01:38:50.217232 140061952972608 submission.py:296] 500) loss = 0.294, grad_norm = 0.097
I0609 01:39:07.349479 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:39:09.573061 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:39:11.844669 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:39:14.126593 140061952972608 submission_runner.py:419] Time since start: 433.44s, 	Step: 549, 	{'train/ssim': 0.7177085876464844, 'train/loss': 0.28425002098083496, 'validation/ssim': 0.6954724897342783, 'validation/loss': 0.3078782956923537, 'validation/num_examples': 3554, 'test/ssim': 0.7133747474736456, 'test/loss': 0.30945086698896956, 'test/num_examples': 3581, 'score': 204.1059124469757, 'total_duration': 433.439120054245, 'accumulated_submission_time': 204.1059124469757, 'accumulated_eval_time': 228.8207812309265, 'accumulated_logging_time': 0.05111813545227051}
I0609 01:39:14.137917 139994517726976 logging_writer.py:48] [549] accumulated_eval_time=228.820781, accumulated_logging_time=0.051118, accumulated_submission_time=204.105912, global_step=549, preemption_count=0, score=204.105912, test/loss=0.309451, test/num_examples=3581, test/ssim=0.713375, total_duration=433.439120, train/loss=0.284250, train/ssim=0.717709, validation/loss=0.307878, validation/num_examples=3554, validation/ssim=0.695472
I0609 01:40:34.364526 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:40:36.626745 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:40:38.955691 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:40:41.261173 140061952972608 submission_runner.py:419] Time since start: 520.57s, 	Step: 782, 	{'train/ssim': 0.7236897604806083, 'train/loss': 0.2809121438435146, 'validation/ssim': 0.7022136973744373, 'validation/loss': 0.30430679450091447, 'validation/num_examples': 3554, 'test/ssim': 0.7195530528963627, 'test/loss': 0.3060543738873569, 'test/num_examples': 3581, 'score': 284.11075139045715, 'total_duration': 520.5742552280426, 'accumulated_submission_time': 284.11075139045715, 'accumulated_eval_time': 235.71733331680298, 'accumulated_logging_time': 0.07307028770446777}
I0609 01:40:41.275603 139994526119680 logging_writer.py:48] [782] accumulated_eval_time=235.717333, accumulated_logging_time=0.073070, accumulated_submission_time=284.110751, global_step=782, preemption_count=0, score=284.110751, test/loss=0.306054, test/num_examples=3581, test/ssim=0.719553, total_duration=520.574255, train/loss=0.280912, train/ssim=0.723690, validation/loss=0.304307, validation/num_examples=3554, validation/ssim=0.702214
I0609 01:41:52.889318 139994517726976 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.315327, loss=0.246883
I0609 01:41:52.894998 140061952972608 submission.py:296] 1000) loss = 0.247, grad_norm = 0.315
I0609 01:42:01.395443 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:42:03.582368 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:42:05.803792 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:42:07.988963 140061952972608 submission_runner.py:419] Time since start: 607.30s, 	Step: 1033, 	{'train/ssim': 0.7291106496538434, 'train/loss': 0.27550610474177767, 'validation/ssim': 0.7077094718319148, 'validation/loss': 0.29858171745304585, 'validation/num_examples': 3554, 'test/ssim': 0.7248534474483385, 'test/loss': 0.30049869182621824, 'test/num_examples': 3581, 'score': 363.99790930747986, 'total_duration': 607.3021352291107, 'accumulated_submission_time': 363.99790930747986, 'accumulated_eval_time': 242.31082820892334, 'accumulated_logging_time': 0.10203933715820312}
I0609 01:42:07.999770 139994526119680 logging_writer.py:48] [1033] accumulated_eval_time=242.310828, accumulated_logging_time=0.102039, accumulated_submission_time=363.997909, global_step=1033, preemption_count=0, score=363.997909, test/loss=0.300499, test/num_examples=3581, test/ssim=0.724853, total_duration=607.302135, train/loss=0.275506, train/ssim=0.729111, validation/loss=0.298582, validation/num_examples=3554, validation/ssim=0.707709
I0609 01:43:28.116990 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:43:30.282772 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:43:32.484289 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:43:34.658467 140061952972608 submission_runner.py:419] Time since start: 693.97s, 	Step: 1339, 	{'train/ssim': 0.7316130229404995, 'train/loss': 0.2740303448268345, 'validation/ssim': 0.7098902507517938, 'validation/loss': 0.29711017597909045, 'validation/num_examples': 3554, 'test/ssim': 0.7271709085276459, 'test/loss': 0.2988418966856325, 'test/num_examples': 3581, 'score': 443.98345398902893, 'total_duration': 693.971649646759, 'accumulated_submission_time': 443.98345398902893, 'accumulated_eval_time': 248.8523383140564, 'accumulated_logging_time': 0.12193799018859863}
I0609 01:43:34.668937 139994517726976 logging_writer.py:48] [1339] accumulated_eval_time=248.852338, accumulated_logging_time=0.121938, accumulated_submission_time=443.983454, global_step=1339, preemption_count=0, score=443.983454, test/loss=0.298842, test/num_examples=3581, test/ssim=0.727171, total_duration=693.971650, train/loss=0.274030, train/ssim=0.731613, validation/loss=0.297110, validation/num_examples=3554, validation/ssim=0.709890
I0609 01:44:15.394190 139994526119680 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.266833, loss=0.274612
I0609 01:44:15.397795 140061952972608 submission.py:296] 1500) loss = 0.275, grad_norm = 0.267
I0609 01:44:54.845934 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:44:56.999229 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:44:59.192983 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:45:01.398627 140061952972608 submission_runner.py:419] Time since start: 780.71s, 	Step: 1648, 	{'train/ssim': 0.7162786892482212, 'train/loss': 0.28711908204214914, 'validation/ssim': 0.6955876905863112, 'validation/loss': 0.3098321763330051, 'validation/num_examples': 3554, 'test/ssim': 0.7133718158771991, 'test/loss': 0.3112932049139556, 'test/num_examples': 3581, 'score': 524.0290811061859, 'total_duration': 780.7117528915405, 'accumulated_submission_time': 524.0290811061859, 'accumulated_eval_time': 255.4049620628357, 'accumulated_logging_time': 0.14195704460144043}
I0609 01:45:01.411152 139994517726976 logging_writer.py:48] [1648] accumulated_eval_time=255.404962, accumulated_logging_time=0.141957, accumulated_submission_time=524.029081, global_step=1648, preemption_count=0, score=524.029081, test/loss=0.311293, test/num_examples=3581, test/ssim=0.713372, total_duration=780.711753, train/loss=0.287119, train/ssim=0.716279, validation/loss=0.309832, validation/num_examples=3554, validation/ssim=0.695588
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0609 01:46:21.483583 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:46:23.655789 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:46:25.866074 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:46:28.043321 140061952972608 submission_runner.py:419] Time since start: 867.36s, 	Step: 1957, 	{'train/ssim': 0.7274180821010044, 'train/loss': 0.2781473057610648, 'validation/ssim': 0.7059704679146737, 'validation/loss': 0.30120842739914533, 'validation/num_examples': 3554, 'test/ssim': 0.7231955273928372, 'test/loss': 0.30289009049497345, 'test/num_examples': 3581, 'score': 603.9677288532257, 'total_duration': 867.356481552124, 'accumulated_submission_time': 603.9677288532257, 'accumulated_eval_time': 261.9647102355957, 'accumulated_logging_time': 0.1637587547302246}
I0609 01:46:28.054155 139994526119680 logging_writer.py:48] [1957] accumulated_eval_time=261.964710, accumulated_logging_time=0.163759, accumulated_submission_time=603.967729, global_step=1957, preemption_count=0, score=603.967729, test/loss=0.302890, test/num_examples=3581, test/ssim=0.723196, total_duration=867.356482, train/loss=0.278147, train/ssim=0.727418, validation/loss=0.301208, validation/num_examples=3554, validation/ssim=0.705970
I0609 01:46:37.350546 139994517726976 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.106570, loss=0.329744
I0609 01:46:37.354004 140061952972608 submission.py:296] 2000) loss = 0.330, grad_norm = 0.107
I0609 01:47:48.094787 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:47:50.284346 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:47:52.490952 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:47:54.658979 140061952972608 submission_runner.py:419] Time since start: 953.97s, 	Step: 2264, 	{'train/ssim': 0.7308120046343122, 'train/loss': 0.2745599235807146, 'validation/ssim': 0.7085329827527083, 'validation/loss': 0.29787268609093626, 'validation/num_examples': 3554, 'test/ssim': 0.725552667289165, 'test/loss': 0.2997477599876082, 'test/num_examples': 3581, 'score': 683.8763139247894, 'total_duration': 953.972142457962, 'accumulated_submission_time': 683.8763139247894, 'accumulated_eval_time': 268.52898025512695, 'accumulated_logging_time': 0.182936429977417}
I0609 01:47:54.669469 139994526119680 logging_writer.py:48] [2264] accumulated_eval_time=268.528980, accumulated_logging_time=0.182936, accumulated_submission_time=683.876314, global_step=2264, preemption_count=0, score=683.876314, test/loss=0.299748, test/num_examples=3581, test/ssim=0.725553, total_duration=953.972142, train/loss=0.274560, train/ssim=0.730812, validation/loss=0.297873, validation/num_examples=3554, validation/ssim=0.708533
I0609 01:48:55.362835 139994517726976 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.328150, loss=0.210974
I0609 01:48:55.366491 140061952972608 submission.py:296] 2500) loss = 0.211, grad_norm = 0.328
I0609 01:49:14.910399 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:49:17.064074 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:49:19.247575 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:49:21.404213 140061952972608 submission_runner.py:419] Time since start: 1040.72s, 	Step: 2573, 	{'train/ssim': 0.7323831149509975, 'train/loss': 0.27328996998923166, 'validation/ssim': 0.7105063726610861, 'validation/loss': 0.2966380379611881, 'validation/num_examples': 3554, 'test/ssim': 0.7275953764224379, 'test/loss': 0.2985018655861666, 'test/num_examples': 3581, 'score': 763.9867715835571, 'total_duration': 1040.717381477356, 'accumulated_submission_time': 763.9867715835571, 'accumulated_eval_time': 275.02283692359924, 'accumulated_logging_time': 0.20191192626953125}
I0609 01:49:21.414925 139994526119680 logging_writer.py:48] [2573] accumulated_eval_time=275.022837, accumulated_logging_time=0.201912, accumulated_submission_time=763.986772, global_step=2573, preemption_count=0, score=763.986772, test/loss=0.298502, test/num_examples=3581, test/ssim=0.727595, total_duration=1040.717381, train/loss=0.273290, train/ssim=0.732383, validation/loss=0.296638, validation/num_examples=3554, validation/ssim=0.710506
I0609 01:50:41.554780 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:50:43.696619 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:50:45.882323 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:50:48.054419 140061952972608 submission_runner.py:419] Time since start: 1127.37s, 	Step: 2881, 	{'train/ssim': 0.7363921574183873, 'train/loss': 0.27080399649483816, 'validation/ssim': 0.7145126420054868, 'validation/loss': 0.2942574607836241, 'validation/num_examples': 3554, 'test/ssim': 0.7315413734161198, 'test/loss': 0.29592618540124965, 'test/num_examples': 3581, 'score': 843.9946985244751, 'total_duration': 1127.3676090240479, 'accumulated_submission_time': 843.9946985244751, 'accumulated_eval_time': 281.5225658416748, 'accumulated_logging_time': 0.22075438499450684}
I0609 01:50:48.065018 139994517726976 logging_writer.py:48] [2881] accumulated_eval_time=281.522566, accumulated_logging_time=0.220754, accumulated_submission_time=843.994699, global_step=2881, preemption_count=0, score=843.994699, test/loss=0.295926, test/num_examples=3581, test/ssim=0.731541, total_duration=1127.367609, train/loss=0.270804, train/ssim=0.736392, validation/loss=0.294257, validation/num_examples=3554, validation/ssim=0.714513
I0609 01:51:17.845450 139994526119680 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.130709, loss=0.298295
I0609 01:51:17.849617 140061952972608 submission.py:296] 3000) loss = 0.298, grad_norm = 0.131
I0609 01:52:08.319444 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:52:10.482117 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:52:12.695355 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:52:14.872299 140061952972608 submission_runner.py:419] Time since start: 1214.19s, 	Step: 3187, 	{'train/ssim': 0.7371068681989398, 'train/loss': 0.2698768207005092, 'validation/ssim': 0.7155264369812183, 'validation/loss': 0.2930267282463421, 'validation/num_examples': 3554, 'test/ssim': 0.7325885669374825, 'test/loss': 0.29478818056670625, 'test/num_examples': 3581, 'score': 924.1189267635345, 'total_duration': 1214.1854603290558, 'accumulated_submission_time': 924.1189267635345, 'accumulated_eval_time': 288.0754635334015, 'accumulated_logging_time': 0.24020028114318848}
I0609 01:52:14.883993 139994517726976 logging_writer.py:48] [3187] accumulated_eval_time=288.075464, accumulated_logging_time=0.240200, accumulated_submission_time=924.118927, global_step=3187, preemption_count=0, score=924.118927, test/loss=0.294788, test/num_examples=3581, test/ssim=0.732589, total_duration=1214.185460, train/loss=0.269877, train/ssim=0.737107, validation/loss=0.293027, validation/num_examples=3554, validation/ssim=0.715526
I0609 01:53:34.930618 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:53:37.082197 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:53:39.291578 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:53:41.463481 140061952972608 submission_runner.py:419] Time since start: 1300.78s, 	Step: 3494, 	{'train/ssim': 0.7374491010393415, 'train/loss': 0.2693626199449812, 'validation/ssim': 0.7158530111318233, 'validation/loss': 0.2926224261505522, 'validation/num_examples': 3554, 'test/ssim': 0.7329413811609885, 'test/loss': 0.2942697652323024, 'test/num_examples': 3581, 'score': 1004.0350694656372, 'total_duration': 1300.776643037796, 'accumulated_submission_time': 1004.0350694656372, 'accumulated_eval_time': 294.60836124420166, 'accumulated_logging_time': 0.26085758209228516}
I0609 01:53:41.474611 139994526119680 logging_writer.py:48] [3494] accumulated_eval_time=294.608361, accumulated_logging_time=0.260858, accumulated_submission_time=1004.035069, global_step=3494, preemption_count=0, score=1004.035069, test/loss=0.294270, test/num_examples=3581, test/ssim=0.732941, total_duration=1300.776643, train/loss=0.269363, train/ssim=0.737449, validation/loss=0.292622, validation/num_examples=3554, validation/ssim=0.715853
I0609 01:53:41.908876 139994517726976 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.103984, loss=0.319819
I0609 01:53:41.912548 140061952972608 submission.py:296] 3500) loss = 0.320, grad_norm = 0.104
I0609 01:55:01.613533 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:55:03.787750 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:55:05.973838 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:55:08.135963 140061952972608 submission_runner.py:419] Time since start: 1387.45s, 	Step: 3804, 	{'train/ssim': 0.7388351304190499, 'train/loss': 0.2685789040156773, 'validation/ssim': 0.7163669841639702, 'validation/loss': 0.2923207194468381, 'validation/num_examples': 3554, 'test/ssim': 0.7334065505227241, 'test/loss': 0.294034419396642, 'test/num_examples': 3581, 'score': 1084.0432493686676, 'total_duration': 1387.449114561081, 'accumulated_submission_time': 1084.0432493686676, 'accumulated_eval_time': 301.13075613975525, 'accumulated_logging_time': 0.28010106086730957}
I0609 01:55:08.147119 139994526119680 logging_writer.py:48] [3804] accumulated_eval_time=301.130756, accumulated_logging_time=0.280101, accumulated_submission_time=1084.043249, global_step=3804, preemption_count=0, score=1084.043249, test/loss=0.294034, test/num_examples=3581, test/ssim=0.733407, total_duration=1387.449115, train/loss=0.268579, train/ssim=0.738835, validation/loss=0.292321, validation/num_examples=3554, validation/ssim=0.716367
I0609 01:55:58.402688 139994517726976 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.080752, loss=0.217961
I0609 01:55:58.406452 140061952972608 submission.py:296] 4000) loss = 0.218, grad_norm = 0.081
I0609 01:56:28.234218 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:56:30.392763 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:56:32.579849 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:56:34.738188 140061952972608 submission_runner.py:419] Time since start: 1474.05s, 	Step: 4112, 	{'train/ssim': 0.7386702128819057, 'train/loss': 0.2682734898158482, 'validation/ssim': 0.7166385339362338, 'validation/loss': 0.29162116794017306, 'validation/num_examples': 3554, 'test/ssim': 0.7337660460590617, 'test/loss': 0.2932386954913956, 'test/num_examples': 3581, 'score': 1163.9945034980774, 'total_duration': 1474.051326751709, 'accumulated_submission_time': 1163.9945034980774, 'accumulated_eval_time': 307.63476371765137, 'accumulated_logging_time': 0.3011801242828369}
I0609 01:56:34.749051 139994526119680 logging_writer.py:48] [4112] accumulated_eval_time=307.634764, accumulated_logging_time=0.301180, accumulated_submission_time=1163.994503, global_step=4112, preemption_count=0, score=1163.994503, test/loss=0.293239, test/num_examples=3581, test/ssim=0.733766, total_duration=1474.051327, train/loss=0.268273, train/ssim=0.738670, validation/loss=0.291621, validation/num_examples=3554, validation/ssim=0.716639
I0609 01:57:54.744005 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:57:56.881840 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:57:59.087476 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:58:01.255557 140061952972608 submission_runner.py:419] Time since start: 1560.57s, 	Step: 4416, 	{'train/ssim': 0.7397494316101074, 'train/loss': 0.2672057832990374, 'validation/ssim': 0.7171830073728546, 'validation/loss': 0.29091114050629574, 'validation/num_examples': 3554, 'test/ssim': 0.7342979603724519, 'test/loss': 0.29254857723488553, 'test/num_examples': 3581, 'score': 1243.8596181869507, 'total_duration': 1560.5687363147736, 'accumulated_submission_time': 1243.8596181869507, 'accumulated_eval_time': 314.1463129520416, 'accumulated_logging_time': 0.32124972343444824}
I0609 01:58:01.266407 139994517726976 logging_writer.py:48] [4416] accumulated_eval_time=314.146313, accumulated_logging_time=0.321250, accumulated_submission_time=1243.859618, global_step=4416, preemption_count=0, score=1243.859618, test/loss=0.292549, test/num_examples=3581, test/ssim=0.734298, total_duration=1560.568736, train/loss=0.267206, train/ssim=0.739749, validation/loss=0.290911, validation/num_examples=3554, validation/ssim=0.717183
I0609 01:58:21.495919 139994526119680 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.185190, loss=0.225318
I0609 01:58:21.501160 140061952972608 submission.py:296] 4500) loss = 0.225, grad_norm = 0.185
I0609 01:59:21.333943 140061952972608 spec.py:298] Evaluating on the training split.
I0609 01:59:23.479953 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 01:59:25.689739 140061952972608 spec.py:326] Evaluating on the test split.
I0609 01:59:27.850911 140061952972608 submission_runner.py:419] Time since start: 1647.16s, 	Step: 4723, 	{'train/ssim': 0.7400279726300921, 'train/loss': 0.2679309674671718, 'validation/ssim': 0.7177244582468697, 'validation/loss': 0.29154141350415025, 'validation/num_examples': 3554, 'test/ssim': 0.7347385861360305, 'test/loss': 0.29324554724588103, 'test/num_examples': 3581, 'score': 1323.793673992157, 'total_duration': 1647.1640620231628, 'accumulated_submission_time': 1323.793673992157, 'accumulated_eval_time': 320.6633400917053, 'accumulated_logging_time': 0.34044361114501953}
I0609 01:59:27.861588 139994517726976 logging_writer.py:48] [4723] accumulated_eval_time=320.663340, accumulated_logging_time=0.340444, accumulated_submission_time=1323.793674, global_step=4723, preemption_count=0, score=1323.793674, test/loss=0.293246, test/num_examples=3581, test/ssim=0.734739, total_duration=1647.164062, train/loss=0.267931, train/ssim=0.740028, validation/loss=0.291541, validation/num_examples=3554, validation/ssim=0.717724
I0609 02:00:39.636703 139994526119680 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.309262, loss=0.247802
I0609 02:00:39.641090 140061952972608 submission.py:296] 5000) loss = 0.248, grad_norm = 0.309
I0609 02:00:47.966282 140061952972608 spec.py:298] Evaluating on the training split.
I0609 02:00:50.103427 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 02:00:52.293380 140061952972608 spec.py:326] Evaluating on the test split.
I0609 02:00:54.456035 140061952972608 submission_runner.py:419] Time since start: 1733.77s, 	Step: 5032, 	{'train/ssim': 0.7407213619777134, 'train/loss': 0.2672462122780936, 'validation/ssim': 0.718178666972953, 'validation/loss': 0.291126635481148, 'validation/num_examples': 3554, 'test/ssim': 0.7353198603523806, 'test/loss': 0.2925675644351264, 'test/num_examples': 3581, 'score': 1403.7649915218353, 'total_duration': 1733.7691860198975, 'accumulated_submission_time': 1403.7649915218353, 'accumulated_eval_time': 327.15305733680725, 'accumulated_logging_time': 0.35953664779663086}
I0609 02:00:54.466578 139994517726976 logging_writer.py:48] [5032] accumulated_eval_time=327.153057, accumulated_logging_time=0.359537, accumulated_submission_time=1403.764992, global_step=5032, preemption_count=0, score=1403.764992, test/loss=0.292568, test/num_examples=3581, test/ssim=0.735320, total_duration=1733.769186, train/loss=0.267246, train/ssim=0.740721, validation/loss=0.291127, validation/num_examples=3554, validation/ssim=0.718179
I0609 02:02:14.717528 140061952972608 spec.py:298] Evaluating on the training split.
I0609 02:02:16.892975 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 02:02:19.081336 140061952972608 spec.py:326] Evaluating on the test split.
I0609 02:02:21.245228 140061952972608 submission_runner.py:419] Time since start: 1820.56s, 	Step: 5338, 	{'train/ssim': 0.7419865471976144, 'train/loss': 0.2669118813105992, 'validation/ssim': 0.7195082510463562, 'validation/loss': 0.29073270627066333, 'validation/num_examples': 3554, 'test/ssim': 0.7366646450013963, 'test/loss': 0.29217830978602344, 'test/num_examples': 3581, 'score': 1483.8854320049286, 'total_duration': 1820.5584189891815, 'accumulated_submission_time': 1483.8854320049286, 'accumulated_eval_time': 333.680757522583, 'accumulated_logging_time': 0.3789939880371094}
I0609 02:02:21.257271 139994526119680 logging_writer.py:48] [5338] accumulated_eval_time=333.680758, accumulated_logging_time=0.378994, accumulated_submission_time=1483.885432, global_step=5338, preemption_count=0, score=1483.885432, test/loss=0.292178, test/num_examples=3581, test/ssim=0.736665, total_duration=1820.558419, train/loss=0.266912, train/ssim=0.741987, validation/loss=0.290733, validation/num_examples=3554, validation/ssim=0.719508
I0609 02:02:42.832950 140061952972608 spec.py:298] Evaluating on the training split.
I0609 02:02:44.902819 140061952972608 spec.py:310] Evaluating on the validation split.
I0609 02:02:47.010365 140061952972608 spec.py:326] Evaluating on the test split.
I0609 02:02:49.061296 140061952972608 submission_runner.py:419] Time since start: 1848.37s, 	Step: 5428, 	{'train/ssim': 0.7392055647713798, 'train/loss': 0.2670864888599941, 'validation/ssim': 0.7176242328186551, 'validation/loss': 0.2904604008577483, 'validation/num_examples': 3554, 'test/ssim': 0.7347197693774434, 'test/loss': 0.29203517288510544, 'test/num_examples': 3581, 'score': 1505.4162628650665, 'total_duration': 1848.374490737915, 'accumulated_submission_time': 1505.4162628650665, 'accumulated_eval_time': 339.9091775417328, 'accumulated_logging_time': 0.3991847038269043}
I0609 02:02:49.072697 139994517726976 logging_writer.py:48] [5428] accumulated_eval_time=339.909178, accumulated_logging_time=0.399185, accumulated_submission_time=1505.416263, global_step=5428, preemption_count=0, score=1505.416263, test/loss=0.292035, test/num_examples=3581, test/ssim=0.734720, total_duration=1848.374491, train/loss=0.267086, train/ssim=0.739206, validation/loss=0.290460, validation/num_examples=3554, validation/ssim=0.717624
I0609 02:02:49.089381 139994526119680 logging_writer.py:48] [5428] global_step=5428, preemption_count=0, score=1505.416263
I0609 02:02:49.231876 140061952972608 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nadamw/fastmri_pytorch/trial_1/checkpoint_5428.
I0609 02:02:50.005625 140061952972608 submission_runner.py:581] Tuning trial 1/1
I0609 02:02:50.005840 140061952972608 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 02:02:50.013866 140061952972608 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ssim': 0.16000606332506453, 'train/loss': 1.189906392778669, 'validation/ssim': 0.15333123494214265, 'validation/loss': 1.1928664225168824, 'validation/num_examples': 3554, 'test/ssim': 0.1770178041988097, 'test/loss': 1.1892374140428652, 'test/num_examples': 3581, 'score': 44.537604570388794, 'total_duration': 259.60966753959656, 'accumulated_submission_time': 44.537604570388794, 'accumulated_eval_time': 215.07096552848816, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (305, {'train/ssim': 0.7003750801086426, 'train/loss': 0.29859161376953125, 'validation/ssim': 0.6777707831404756, 'validation/loss': 0.32285749771384353, 'validation/num_examples': 3554, 'test/ssim': 0.6965263172821837, 'test/loss': 0.32393401017304874, 'test/num_examples': 3581, 'score': 124.28650259971619, 'total_duration': 346.5861577987671, 'accumulated_submission_time': 124.28650259971619, 'accumulated_eval_time': 222.04425930976868, 'accumulated_logging_time': 0.025522232055664062, 'global_step': 305, 'preemption_count': 0}), (549, {'train/ssim': 0.7177085876464844, 'train/loss': 0.28425002098083496, 'validation/ssim': 0.6954724897342783, 'validation/loss': 0.3078782956923537, 'validation/num_examples': 3554, 'test/ssim': 0.7133747474736456, 'test/loss': 0.30945086698896956, 'test/num_examples': 3581, 'score': 204.1059124469757, 'total_duration': 433.439120054245, 'accumulated_submission_time': 204.1059124469757, 'accumulated_eval_time': 228.8207812309265, 'accumulated_logging_time': 0.05111813545227051, 'global_step': 549, 'preemption_count': 0}), (782, {'train/ssim': 0.7236897604806083, 'train/loss': 0.2809121438435146, 'validation/ssim': 0.7022136973744373, 'validation/loss': 0.30430679450091447, 'validation/num_examples': 3554, 'test/ssim': 0.7195530528963627, 'test/loss': 0.3060543738873569, 'test/num_examples': 3581, 'score': 284.11075139045715, 'total_duration': 520.5742552280426, 'accumulated_submission_time': 284.11075139045715, 'accumulated_eval_time': 235.71733331680298, 'accumulated_logging_time': 0.07307028770446777, 'global_step': 782, 'preemption_count': 0}), (1033, {'train/ssim': 0.7291106496538434, 'train/loss': 0.27550610474177767, 'validation/ssim': 0.7077094718319148, 'validation/loss': 0.29858171745304585, 'validation/num_examples': 3554, 'test/ssim': 0.7248534474483385, 'test/loss': 0.30049869182621824, 'test/num_examples': 3581, 'score': 363.99790930747986, 'total_duration': 607.3021352291107, 'accumulated_submission_time': 363.99790930747986, 'accumulated_eval_time': 242.31082820892334, 'accumulated_logging_time': 0.10203933715820312, 'global_step': 1033, 'preemption_count': 0}), (1339, {'train/ssim': 0.7316130229404995, 'train/loss': 0.2740303448268345, 'validation/ssim': 0.7098902507517938, 'validation/loss': 0.29711017597909045, 'validation/num_examples': 3554, 'test/ssim': 0.7271709085276459, 'test/loss': 0.2988418966856325, 'test/num_examples': 3581, 'score': 443.98345398902893, 'total_duration': 693.971649646759, 'accumulated_submission_time': 443.98345398902893, 'accumulated_eval_time': 248.8523383140564, 'accumulated_logging_time': 0.12193799018859863, 'global_step': 1339, 'preemption_count': 0}), (1648, {'train/ssim': 0.7162786892482212, 'train/loss': 0.28711908204214914, 'validation/ssim': 0.6955876905863112, 'validation/loss': 0.3098321763330051, 'validation/num_examples': 3554, 'test/ssim': 0.7133718158771991, 'test/loss': 0.3112932049139556, 'test/num_examples': 3581, 'score': 524.0290811061859, 'total_duration': 780.7117528915405, 'accumulated_submission_time': 524.0290811061859, 'accumulated_eval_time': 255.4049620628357, 'accumulated_logging_time': 0.14195704460144043, 'global_step': 1648, 'preemption_count': 0}), (1957, {'train/ssim': 0.7274180821010044, 'train/loss': 0.2781473057610648, 'validation/ssim': 0.7059704679146737, 'validation/loss': 0.30120842739914533, 'validation/num_examples': 3554, 'test/ssim': 0.7231955273928372, 'test/loss': 0.30289009049497345, 'test/num_examples': 3581, 'score': 603.9677288532257, 'total_duration': 867.356481552124, 'accumulated_submission_time': 603.9677288532257, 'accumulated_eval_time': 261.9647102355957, 'accumulated_logging_time': 0.1637587547302246, 'global_step': 1957, 'preemption_count': 0}), (2264, {'train/ssim': 0.7308120046343122, 'train/loss': 0.2745599235807146, 'validation/ssim': 0.7085329827527083, 'validation/loss': 0.29787268609093626, 'validation/num_examples': 3554, 'test/ssim': 0.725552667289165, 'test/loss': 0.2997477599876082, 'test/num_examples': 3581, 'score': 683.8763139247894, 'total_duration': 953.972142457962, 'accumulated_submission_time': 683.8763139247894, 'accumulated_eval_time': 268.52898025512695, 'accumulated_logging_time': 0.182936429977417, 'global_step': 2264, 'preemption_count': 0}), (2573, {'train/ssim': 0.7323831149509975, 'train/loss': 0.27328996998923166, 'validation/ssim': 0.7105063726610861, 'validation/loss': 0.2966380379611881, 'validation/num_examples': 3554, 'test/ssim': 0.7275953764224379, 'test/loss': 0.2985018655861666, 'test/num_examples': 3581, 'score': 763.9867715835571, 'total_duration': 1040.717381477356, 'accumulated_submission_time': 763.9867715835571, 'accumulated_eval_time': 275.02283692359924, 'accumulated_logging_time': 0.20191192626953125, 'global_step': 2573, 'preemption_count': 0}), (2881, {'train/ssim': 0.7363921574183873, 'train/loss': 0.27080399649483816, 'validation/ssim': 0.7145126420054868, 'validation/loss': 0.2942574607836241, 'validation/num_examples': 3554, 'test/ssim': 0.7315413734161198, 'test/loss': 0.29592618540124965, 'test/num_examples': 3581, 'score': 843.9946985244751, 'total_duration': 1127.3676090240479, 'accumulated_submission_time': 843.9946985244751, 'accumulated_eval_time': 281.5225658416748, 'accumulated_logging_time': 0.22075438499450684, 'global_step': 2881, 'preemption_count': 0}), (3187, {'train/ssim': 0.7371068681989398, 'train/loss': 0.2698768207005092, 'validation/ssim': 0.7155264369812183, 'validation/loss': 0.2930267282463421, 'validation/num_examples': 3554, 'test/ssim': 0.7325885669374825, 'test/loss': 0.29478818056670625, 'test/num_examples': 3581, 'score': 924.1189267635345, 'total_duration': 1214.1854603290558, 'accumulated_submission_time': 924.1189267635345, 'accumulated_eval_time': 288.0754635334015, 'accumulated_logging_time': 0.24020028114318848, 'global_step': 3187, 'preemption_count': 0}), (3494, {'train/ssim': 0.7374491010393415, 'train/loss': 0.2693626199449812, 'validation/ssim': 0.7158530111318233, 'validation/loss': 0.2926224261505522, 'validation/num_examples': 3554, 'test/ssim': 0.7329413811609885, 'test/loss': 0.2942697652323024, 'test/num_examples': 3581, 'score': 1004.0350694656372, 'total_duration': 1300.776643037796, 'accumulated_submission_time': 1004.0350694656372, 'accumulated_eval_time': 294.60836124420166, 'accumulated_logging_time': 0.26085758209228516, 'global_step': 3494, 'preemption_count': 0}), (3804, {'train/ssim': 0.7388351304190499, 'train/loss': 0.2685789040156773, 'validation/ssim': 0.7163669841639702, 'validation/loss': 0.2923207194468381, 'validation/num_examples': 3554, 'test/ssim': 0.7334065505227241, 'test/loss': 0.294034419396642, 'test/num_examples': 3581, 'score': 1084.0432493686676, 'total_duration': 1387.449114561081, 'accumulated_submission_time': 1084.0432493686676, 'accumulated_eval_time': 301.13075613975525, 'accumulated_logging_time': 0.28010106086730957, 'global_step': 3804, 'preemption_count': 0}), (4112, {'train/ssim': 0.7386702128819057, 'train/loss': 0.2682734898158482, 'validation/ssim': 0.7166385339362338, 'validation/loss': 0.29162116794017306, 'validation/num_examples': 3554, 'test/ssim': 0.7337660460590617, 'test/loss': 0.2932386954913956, 'test/num_examples': 3581, 'score': 1163.9945034980774, 'total_duration': 1474.051326751709, 'accumulated_submission_time': 1163.9945034980774, 'accumulated_eval_time': 307.63476371765137, 'accumulated_logging_time': 0.3011801242828369, 'global_step': 4112, 'preemption_count': 0}), (4416, {'train/ssim': 0.7397494316101074, 'train/loss': 0.2672057832990374, 'validation/ssim': 0.7171830073728546, 'validation/loss': 0.29091114050629574, 'validation/num_examples': 3554, 'test/ssim': 0.7342979603724519, 'test/loss': 0.29254857723488553, 'test/num_examples': 3581, 'score': 1243.8596181869507, 'total_duration': 1560.5687363147736, 'accumulated_submission_time': 1243.8596181869507, 'accumulated_eval_time': 314.1463129520416, 'accumulated_logging_time': 0.32124972343444824, 'global_step': 4416, 'preemption_count': 0}), (4723, {'train/ssim': 0.7400279726300921, 'train/loss': 0.2679309674671718, 'validation/ssim': 0.7177244582468697, 'validation/loss': 0.29154141350415025, 'validation/num_examples': 3554, 'test/ssim': 0.7347385861360305, 'test/loss': 0.29324554724588103, 'test/num_examples': 3581, 'score': 1323.793673992157, 'total_duration': 1647.1640620231628, 'accumulated_submission_time': 1323.793673992157, 'accumulated_eval_time': 320.6633400917053, 'accumulated_logging_time': 0.34044361114501953, 'global_step': 4723, 'preemption_count': 0}), (5032, {'train/ssim': 0.7407213619777134, 'train/loss': 0.2672462122780936, 'validation/ssim': 0.718178666972953, 'validation/loss': 0.291126635481148, 'validation/num_examples': 3554, 'test/ssim': 0.7353198603523806, 'test/loss': 0.2925675644351264, 'test/num_examples': 3581, 'score': 1403.7649915218353, 'total_duration': 1733.7691860198975, 'accumulated_submission_time': 1403.7649915218353, 'accumulated_eval_time': 327.15305733680725, 'accumulated_logging_time': 0.35953664779663086, 'global_step': 5032, 'preemption_count': 0}), (5338, {'train/ssim': 0.7419865471976144, 'train/loss': 0.2669118813105992, 'validation/ssim': 0.7195082510463562, 'validation/loss': 0.29073270627066333, 'validation/num_examples': 3554, 'test/ssim': 0.7366646450013963, 'test/loss': 0.29217830978602344, 'test/num_examples': 3581, 'score': 1483.8854320049286, 'total_duration': 1820.5584189891815, 'accumulated_submission_time': 1483.8854320049286, 'accumulated_eval_time': 333.680757522583, 'accumulated_logging_time': 0.3789939880371094, 'global_step': 5338, 'preemption_count': 0}), (5428, {'train/ssim': 0.7392055647713798, 'train/loss': 0.2670864888599941, 'validation/ssim': 0.7176242328186551, 'validation/loss': 0.2904604008577483, 'validation/num_examples': 3554, 'test/ssim': 0.7347197693774434, 'test/loss': 0.29203517288510544, 'test/num_examples': 3581, 'score': 1505.4162628650665, 'total_duration': 1848.374490737915, 'accumulated_submission_time': 1505.4162628650665, 'accumulated_eval_time': 339.9091775417328, 'accumulated_logging_time': 0.3991847038269043, 'global_step': 5428, 'preemption_count': 0})], 'global_step': 5428}
I0609 02:02:50.014123 140061952972608 submission_runner.py:584] Timing: 1505.4162628650665
I0609 02:02:50.014194 140061952972608 submission_runner.py:586] Total number of evals: 20
I0609 02:02:50.014245 140061952972608 submission_runner.py:587] ====================
I0609 02:02:50.014383 140061952972608 submission_runner.py:655] Final fastmri score: 1505.4162628650665
