torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-08-2023-22-02-33.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0608 22:02:56.392496 140700882048832 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0608 22:02:56.392527 139672597780288 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0608 22:02:56.392550 140592146372416 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0608 22:02:57.373430 139650140641088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0608 22:02:57.373466 140382002411328 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0608 22:02:57.373482 140495855994688 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0608 22:02:57.373667 140593529726784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0608 22:02:57.375331 140500248893248 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0608 22:02:57.375714 140500248893248 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384106 139650140641088 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384132 140382002411328 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384168 140495855994688 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384250 140593529726784 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384544 140700882048832 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384581 139672597780288 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:57.384603 140592146372416 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0608 22:02:59.721083 140500248893248 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_vit_pytorch.
W0608 22:02:59.850944 140382002411328 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.851288 140593529726784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.851513 139672597780288 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.851969 140592146372416 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.852133 140500248893248 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.853058 140495855994688 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.853355 139650140641088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0608 22:02:59.854216 140700882048832 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0608 22:02:59.857678 140500248893248 submission_runner.py:541] Using RNG seed 2183264328
I0608 22:02:59.859185 140500248893248 submission_runner.py:550] --- Tuning run 1/1 ---
I0608 22:02:59.859306 140500248893248 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_vit_pytorch/trial_1.
I0608 22:02:59.859637 140500248893248 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_vit_pytorch/trial_1/hparams.json.
I0608 22:02:59.860625 140500248893248 submission_runner.py:255] Initializing dataset.
I0608 22:03:06.251322 140500248893248 submission_runner.py:262] Initializing model.
I0608 22:03:10.755409 140500248893248 submission_runner.py:272] Initializing optimizer.
I0608 22:03:11.218358 140500248893248 submission_runner.py:279] Initializing metrics bundle.
I0608 22:03:11.218557 140500248893248 submission_runner.py:297] Initializing checkpoint and logger.
I0608 22:03:11.727749 140500248893248 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0608 22:03:11.729520 140500248893248 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_vit_pytorch/trial_1/flags_0.json.
I0608 22:03:11.777611 140500248893248 submission_runner.py:332] Starting training loop.
I0608 22:03:18.500117 140471686911744 logging_writer.py:48] [0] global_step=0, grad_norm=0.306687, loss=6.907755
I0608 22:03:18.533395 140500248893248 submission.py:139] 0) loss = 6.908, grad_norm = 0.307
I0608 22:03:18.534901 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:04:20.864264 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:05:17.342546 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:05:17.362673 140500248893248 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 22:05:17.369368 140500248893248 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0608 22:05:17.451520 140500248893248 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0608 22:05:30.222684 140500248893248 submission_runner.py:419] Time since start: 138.45s, 	Step: 1, 	{'train/accuracy': 0.0009375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.757206201553345, 'total_duration': 138.44540071487427, 'accumulated_submission_time': 6.757206201553345, 'accumulated_eval_time': 131.68766379356384, 'accumulated_logging_time': 0}
I0608 22:05:30.243131 140465965889280 logging_writer.py:48] [1] accumulated_eval_time=131.687664, accumulated_logging_time=0, accumulated_submission_time=6.757206, global_step=1, preemption_count=0, score=6.757206, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=138.445401, train/accuracy=0.000937, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0608 22:05:30.262672 140500248893248 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.263030 139650140641088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.263130 140700882048832 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.263083 140593529726784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.263617 140592146372416 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.263695 140382002411328 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.264191 139672597780288 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.264381 140495855994688 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0608 22:05:30.839871 140465957496576 logging_writer.py:48] [1] global_step=1, grad_norm=0.312288, loss=6.907755
I0608 22:05:30.843613 140500248893248 submission.py:139] 1) loss = 6.908, grad_norm = 0.312
I0608 22:05:31.242517 140465965889280 logging_writer.py:48] [2] global_step=2, grad_norm=0.315540, loss=6.907755
I0608 22:05:31.246306 140500248893248 submission.py:139] 2) loss = 6.908, grad_norm = 0.316
I0608 22:05:31.640394 140465957496576 logging_writer.py:48] [3] global_step=3, grad_norm=0.314164, loss=6.907754
I0608 22:05:31.644163 140500248893248 submission.py:139] 3) loss = 6.908, grad_norm = 0.314
I0608 22:05:32.038894 140465965889280 logging_writer.py:48] [4] global_step=4, grad_norm=0.306667, loss=6.907754
I0608 22:05:32.042929 140500248893248 submission.py:139] 4) loss = 6.908, grad_norm = 0.307
I0608 22:05:32.435089 140465957496576 logging_writer.py:48] [5] global_step=5, grad_norm=0.311989, loss=6.907754
I0608 22:05:32.438865 140500248893248 submission.py:139] 5) loss = 6.908, grad_norm = 0.312
I0608 22:05:32.843080 140465965889280 logging_writer.py:48] [6] global_step=6, grad_norm=0.318645, loss=6.907746
I0608 22:05:32.849000 140500248893248 submission.py:139] 6) loss = 6.908, grad_norm = 0.319
I0608 22:05:33.247026 140465957496576 logging_writer.py:48] [7] global_step=7, grad_norm=0.318601, loss=6.907748
I0608 22:05:33.252564 140500248893248 submission.py:139] 7) loss = 6.908, grad_norm = 0.319
I0608 22:05:33.658002 140465965889280 logging_writer.py:48] [8] global_step=8, grad_norm=0.319076, loss=6.907742
I0608 22:05:33.662393 140500248893248 submission.py:139] 8) loss = 6.908, grad_norm = 0.319
I0608 22:05:34.065083 140465957496576 logging_writer.py:48] [9] global_step=9, grad_norm=0.322182, loss=6.907745
I0608 22:05:34.070178 140500248893248 submission.py:139] 9) loss = 6.908, grad_norm = 0.322
I0608 22:05:34.468441 140465965889280 logging_writer.py:48] [10] global_step=10, grad_norm=0.314834, loss=6.907738
I0608 22:05:34.474041 140500248893248 submission.py:139] 10) loss = 6.908, grad_norm = 0.315
I0608 22:05:34.866552 140465957496576 logging_writer.py:48] [11] global_step=11, grad_norm=0.307047, loss=6.907739
I0608 22:05:34.871003 140500248893248 submission.py:139] 11) loss = 6.908, grad_norm = 0.307
I0608 22:05:35.264392 140465965889280 logging_writer.py:48] [12] global_step=12, grad_norm=0.310072, loss=6.907742
I0608 22:05:35.269636 140500248893248 submission.py:139] 12) loss = 6.908, grad_norm = 0.310
I0608 22:05:35.662583 140465957496576 logging_writer.py:48] [13] global_step=13, grad_norm=0.308181, loss=6.907728
I0608 22:05:35.666597 140500248893248 submission.py:139] 13) loss = 6.908, grad_norm = 0.308
I0608 22:05:36.061533 140465965889280 logging_writer.py:48] [14] global_step=14, grad_norm=0.318126, loss=6.907728
I0608 22:05:36.066387 140500248893248 submission.py:139] 14) loss = 6.908, grad_norm = 0.318
I0608 22:05:36.462682 140465957496576 logging_writer.py:48] [15] global_step=15, grad_norm=0.310222, loss=6.907737
I0608 22:05:36.467527 140500248893248 submission.py:139] 15) loss = 6.908, grad_norm = 0.310
I0608 22:05:36.864582 140465965889280 logging_writer.py:48] [16] global_step=16, grad_norm=0.309497, loss=6.907712
I0608 22:05:36.868896 140500248893248 submission.py:139] 16) loss = 6.908, grad_norm = 0.309
I0608 22:05:37.272421 140465957496576 logging_writer.py:48] [17] global_step=17, grad_norm=0.310316, loss=6.907674
I0608 22:05:37.277251 140500248893248 submission.py:139] 17) loss = 6.908, grad_norm = 0.310
I0608 22:05:37.677729 140465965889280 logging_writer.py:48] [18] global_step=18, grad_norm=0.314374, loss=6.907677
I0608 22:05:37.682494 140500248893248 submission.py:139] 18) loss = 6.908, grad_norm = 0.314
I0608 22:05:38.079657 140465957496576 logging_writer.py:48] [19] global_step=19, grad_norm=0.314632, loss=6.907706
I0608 22:05:38.085080 140500248893248 submission.py:139] 19) loss = 6.908, grad_norm = 0.315
I0608 22:05:38.481115 140465965889280 logging_writer.py:48] [20] global_step=20, grad_norm=0.307823, loss=6.907640
I0608 22:05:38.485384 140500248893248 submission.py:139] 20) loss = 6.908, grad_norm = 0.308
I0608 22:05:38.881090 140465957496576 logging_writer.py:48] [21] global_step=21, grad_norm=0.313344, loss=6.907595
I0608 22:05:38.885307 140500248893248 submission.py:139] 21) loss = 6.908, grad_norm = 0.313
I0608 22:05:39.281376 140465965889280 logging_writer.py:48] [22] global_step=22, grad_norm=0.314297, loss=6.907656
I0608 22:05:39.286440 140500248893248 submission.py:139] 22) loss = 6.908, grad_norm = 0.314
I0608 22:05:39.688090 140465957496576 logging_writer.py:48] [23] global_step=23, grad_norm=0.309246, loss=6.907605
I0608 22:05:39.693143 140500248893248 submission.py:139] 23) loss = 6.908, grad_norm = 0.309
I0608 22:05:40.089895 140465965889280 logging_writer.py:48] [24] global_step=24, grad_norm=0.317113, loss=6.907509
I0608 22:05:40.094722 140500248893248 submission.py:139] 24) loss = 6.908, grad_norm = 0.317
I0608 22:05:40.489748 140465957496576 logging_writer.py:48] [25] global_step=25, grad_norm=0.311846, loss=6.907540
I0608 22:05:40.494431 140500248893248 submission.py:139] 25) loss = 6.908, grad_norm = 0.312
I0608 22:05:40.901753 140465965889280 logging_writer.py:48] [26] global_step=26, grad_norm=0.315970, loss=6.907491
I0608 22:05:40.907045 140500248893248 submission.py:139] 26) loss = 6.907, grad_norm = 0.316
I0608 22:05:41.308923 140465957496576 logging_writer.py:48] [27] global_step=27, grad_norm=0.307321, loss=6.907585
I0608 22:05:41.314385 140500248893248 submission.py:139] 27) loss = 6.908, grad_norm = 0.307
I0608 22:05:41.708604 140465965889280 logging_writer.py:48] [28] global_step=28, grad_norm=0.318489, loss=6.907510
I0608 22:05:41.713970 140500248893248 submission.py:139] 28) loss = 6.908, grad_norm = 0.318
I0608 22:05:42.120959 140465957496576 logging_writer.py:48] [29] global_step=29, grad_norm=0.310633, loss=6.907627
I0608 22:05:42.125952 140500248893248 submission.py:139] 29) loss = 6.908, grad_norm = 0.311
I0608 22:05:42.528622 140465965889280 logging_writer.py:48] [30] global_step=30, grad_norm=0.320288, loss=6.907328
I0608 22:05:42.534320 140500248893248 submission.py:139] 30) loss = 6.907, grad_norm = 0.320
I0608 22:05:42.938401 140465957496576 logging_writer.py:48] [31] global_step=31, grad_norm=0.306823, loss=6.907537
I0608 22:05:42.943683 140500248893248 submission.py:139] 31) loss = 6.908, grad_norm = 0.307
I0608 22:05:43.340485 140465965889280 logging_writer.py:48] [32] global_step=32, grad_norm=0.302985, loss=6.907445
I0608 22:05:43.344890 140500248893248 submission.py:139] 32) loss = 6.907, grad_norm = 0.303
I0608 22:05:43.742594 140465957496576 logging_writer.py:48] [33] global_step=33, grad_norm=0.317315, loss=6.907276
I0608 22:05:43.747648 140500248893248 submission.py:139] 33) loss = 6.907, grad_norm = 0.317
I0608 22:05:44.159207 140465965889280 logging_writer.py:48] [34] global_step=34, grad_norm=0.321809, loss=6.907382
I0608 22:05:44.165440 140500248893248 submission.py:139] 34) loss = 6.907, grad_norm = 0.322
I0608 22:05:44.563663 140465957496576 logging_writer.py:48] [35] global_step=35, grad_norm=0.307066, loss=6.907245
I0608 22:05:44.567644 140500248893248 submission.py:139] 35) loss = 6.907, grad_norm = 0.307
I0608 22:05:44.964430 140465965889280 logging_writer.py:48] [36] global_step=36, grad_norm=0.311908, loss=6.907326
I0608 22:05:44.969435 140500248893248 submission.py:139] 36) loss = 6.907, grad_norm = 0.312
I0608 22:05:45.367551 140465957496576 logging_writer.py:48] [37] global_step=37, grad_norm=0.312768, loss=6.907503
I0608 22:05:45.373163 140500248893248 submission.py:139] 37) loss = 6.908, grad_norm = 0.313
I0608 22:05:45.770413 140465965889280 logging_writer.py:48] [38] global_step=38, grad_norm=0.307539, loss=6.907331
I0608 22:05:45.775170 140500248893248 submission.py:139] 38) loss = 6.907, grad_norm = 0.308
I0608 22:05:46.174463 140465957496576 logging_writer.py:48] [39] global_step=39, grad_norm=0.310854, loss=6.907513
I0608 22:05:46.178817 140500248893248 submission.py:139] 39) loss = 6.908, grad_norm = 0.311
I0608 22:05:46.574577 140465965889280 logging_writer.py:48] [40] global_step=40, grad_norm=0.317239, loss=6.907146
I0608 22:05:46.579147 140500248893248 submission.py:139] 40) loss = 6.907, grad_norm = 0.317
I0608 22:05:47.001780 140465957496576 logging_writer.py:48] [41] global_step=41, grad_norm=0.303035, loss=6.907310
I0608 22:05:47.006401 140500248893248 submission.py:139] 41) loss = 6.907, grad_norm = 0.303
I0608 22:05:47.412806 140465965889280 logging_writer.py:48] [42] global_step=42, grad_norm=0.308557, loss=6.907386
I0608 22:05:47.418094 140500248893248 submission.py:139] 42) loss = 6.907, grad_norm = 0.309
I0608 22:05:47.821697 140465957496576 logging_writer.py:48] [43] global_step=43, grad_norm=0.308961, loss=6.907158
I0608 22:05:47.827042 140500248893248 submission.py:139] 43) loss = 6.907, grad_norm = 0.309
I0608 22:05:48.230677 140465965889280 logging_writer.py:48] [44] global_step=44, grad_norm=0.311605, loss=6.907050
I0608 22:05:48.236528 140500248893248 submission.py:139] 44) loss = 6.907, grad_norm = 0.312
I0608 22:05:48.633295 140465957496576 logging_writer.py:48] [45] global_step=45, grad_norm=0.299295, loss=6.907056
I0608 22:05:48.638700 140500248893248 submission.py:139] 45) loss = 6.907, grad_norm = 0.299
I0608 22:05:49.036310 140465965889280 logging_writer.py:48] [46] global_step=46, grad_norm=0.318750, loss=6.906686
I0608 22:05:49.040792 140500248893248 submission.py:139] 46) loss = 6.907, grad_norm = 0.319
I0608 22:05:49.439346 140465957496576 logging_writer.py:48] [47] global_step=47, grad_norm=0.310461, loss=6.907032
I0608 22:05:49.445192 140500248893248 submission.py:139] 47) loss = 6.907, grad_norm = 0.310
I0608 22:05:49.845776 140465965889280 logging_writer.py:48] [48] global_step=48, grad_norm=0.310275, loss=6.907027
I0608 22:05:49.850514 140500248893248 submission.py:139] 48) loss = 6.907, grad_norm = 0.310
I0608 22:05:50.254997 140465957496576 logging_writer.py:48] [49] global_step=49, grad_norm=0.317233, loss=6.906997
I0608 22:05:50.260062 140500248893248 submission.py:139] 49) loss = 6.907, grad_norm = 0.317
I0608 22:05:50.659235 140465965889280 logging_writer.py:48] [50] global_step=50, grad_norm=0.305361, loss=6.906715
I0608 22:05:50.663600 140500248893248 submission.py:139] 50) loss = 6.907, grad_norm = 0.305
I0608 22:05:51.062341 140465957496576 logging_writer.py:48] [51] global_step=51, grad_norm=0.308807, loss=6.907000
I0608 22:05:51.067202 140500248893248 submission.py:139] 51) loss = 6.907, grad_norm = 0.309
I0608 22:05:51.469295 140465965889280 logging_writer.py:48] [52] global_step=52, grad_norm=0.312144, loss=6.906823
I0608 22:05:51.473649 140500248893248 submission.py:139] 52) loss = 6.907, grad_norm = 0.312
I0608 22:05:51.871080 140465957496576 logging_writer.py:48] [53] global_step=53, grad_norm=0.310139, loss=6.906526
I0608 22:05:51.875599 140500248893248 submission.py:139] 53) loss = 6.907, grad_norm = 0.310
I0608 22:05:52.271550 140465965889280 logging_writer.py:48] [54] global_step=54, grad_norm=0.313218, loss=6.906963
I0608 22:05:52.276266 140500248893248 submission.py:139] 54) loss = 6.907, grad_norm = 0.313
I0608 22:05:52.671817 140465957496576 logging_writer.py:48] [55] global_step=55, grad_norm=0.311933, loss=6.906921
I0608 22:05:52.678038 140500248893248 submission.py:139] 55) loss = 6.907, grad_norm = 0.312
I0608 22:05:53.080699 140465965889280 logging_writer.py:48] [56] global_step=56, grad_norm=0.304661, loss=6.906480
I0608 22:05:53.087107 140500248893248 submission.py:139] 56) loss = 6.906, grad_norm = 0.305
I0608 22:05:53.486453 140465957496576 logging_writer.py:48] [57] global_step=57, grad_norm=0.313087, loss=6.907114
I0608 22:05:53.492300 140500248893248 submission.py:139] 57) loss = 6.907, grad_norm = 0.313
I0608 22:05:53.895710 140465965889280 logging_writer.py:48] [58] global_step=58, grad_norm=0.317144, loss=6.906365
I0608 22:05:53.901179 140500248893248 submission.py:139] 58) loss = 6.906, grad_norm = 0.317
I0608 22:05:54.298045 140465957496576 logging_writer.py:48] [59] global_step=59, grad_norm=0.317771, loss=6.906824
I0608 22:05:54.303987 140500248893248 submission.py:139] 59) loss = 6.907, grad_norm = 0.318
I0608 22:05:54.699875 140465965889280 logging_writer.py:48] [60] global_step=60, grad_norm=0.313926, loss=6.905766
I0608 22:05:54.704503 140500248893248 submission.py:139] 60) loss = 6.906, grad_norm = 0.314
I0608 22:05:55.104779 140465957496576 logging_writer.py:48] [61] global_step=61, grad_norm=0.310774, loss=6.905876
I0608 22:05:55.109236 140500248893248 submission.py:139] 61) loss = 6.906, grad_norm = 0.311
I0608 22:05:55.508278 140465965889280 logging_writer.py:48] [62] global_step=62, grad_norm=0.300729, loss=6.906896
I0608 22:05:55.513649 140500248893248 submission.py:139] 62) loss = 6.907, grad_norm = 0.301
I0608 22:05:55.913844 140465957496576 logging_writer.py:48] [63] global_step=63, grad_norm=0.313787, loss=6.906112
I0608 22:05:55.919857 140500248893248 submission.py:139] 63) loss = 6.906, grad_norm = 0.314
I0608 22:05:56.320704 140465965889280 logging_writer.py:48] [64] global_step=64, grad_norm=0.316231, loss=6.905742
I0608 22:05:56.325221 140500248893248 submission.py:139] 64) loss = 6.906, grad_norm = 0.316
I0608 22:05:56.721762 140465957496576 logging_writer.py:48] [65] global_step=65, grad_norm=0.309759, loss=6.906183
I0608 22:05:56.726842 140500248893248 submission.py:139] 65) loss = 6.906, grad_norm = 0.310
I0608 22:05:57.137084 140465965889280 logging_writer.py:48] [66] global_step=66, grad_norm=0.307575, loss=6.906452
I0608 22:05:57.143507 140500248893248 submission.py:139] 66) loss = 6.906, grad_norm = 0.308
I0608 22:05:57.549792 140465957496576 logging_writer.py:48] [67] global_step=67, grad_norm=0.313974, loss=6.906627
I0608 22:05:57.554939 140500248893248 submission.py:139] 67) loss = 6.907, grad_norm = 0.314
I0608 22:05:57.952551 140465965889280 logging_writer.py:48] [68] global_step=68, grad_norm=0.309702, loss=6.906217
I0608 22:05:57.957534 140500248893248 submission.py:139] 68) loss = 6.906, grad_norm = 0.310
I0608 22:05:58.354925 140465957496576 logging_writer.py:48] [69] global_step=69, grad_norm=0.306150, loss=6.906649
I0608 22:05:58.360451 140500248893248 submission.py:139] 69) loss = 6.907, grad_norm = 0.306
I0608 22:05:58.802656 140465965889280 logging_writer.py:48] [70] global_step=70, grad_norm=0.308036, loss=6.905387
I0608 22:05:58.806747 140500248893248 submission.py:139] 70) loss = 6.905, grad_norm = 0.308
I0608 22:05:59.213179 140465957496576 logging_writer.py:48] [71] global_step=71, grad_norm=0.314267, loss=6.905912
I0608 22:05:59.218035 140500248893248 submission.py:139] 71) loss = 6.906, grad_norm = 0.314
I0608 22:05:59.618921 140465965889280 logging_writer.py:48] [72] global_step=72, grad_norm=0.311551, loss=6.905353
I0608 22:05:59.623108 140500248893248 submission.py:139] 72) loss = 6.905, grad_norm = 0.312
I0608 22:06:00.024262 140465957496576 logging_writer.py:48] [73] global_step=73, grad_norm=0.319438, loss=6.905645
I0608 22:06:00.029497 140500248893248 submission.py:139] 73) loss = 6.906, grad_norm = 0.319
I0608 22:06:00.435781 140465965889280 logging_writer.py:48] [74] global_step=74, grad_norm=0.302533, loss=6.905412
I0608 22:06:00.440544 140500248893248 submission.py:139] 74) loss = 6.905, grad_norm = 0.303
I0608 22:06:00.861073 140465957496576 logging_writer.py:48] [75] global_step=75, grad_norm=0.310038, loss=6.905871
I0608 22:06:00.869608 140500248893248 submission.py:139] 75) loss = 6.906, grad_norm = 0.310
I0608 22:06:01.275846 140465965889280 logging_writer.py:48] [76] global_step=76, grad_norm=0.314650, loss=6.906185
I0608 22:06:01.281292 140500248893248 submission.py:139] 76) loss = 6.906, grad_norm = 0.315
I0608 22:06:01.706293 140465957496576 logging_writer.py:48] [77] global_step=77, grad_norm=0.314716, loss=6.904474
I0608 22:06:01.713168 140500248893248 submission.py:139] 77) loss = 6.904, grad_norm = 0.315
I0608 22:06:02.117738 140465965889280 logging_writer.py:48] [78] global_step=78, grad_norm=0.312803, loss=6.904892
I0608 22:06:02.122045 140500248893248 submission.py:139] 78) loss = 6.905, grad_norm = 0.313
I0608 22:06:02.620789 140465957496576 logging_writer.py:48] [79] global_step=79, grad_norm=0.320036, loss=6.906803
I0608 22:06:02.624992 140500248893248 submission.py:139] 79) loss = 6.907, grad_norm = 0.320
I0608 22:06:03.025906 140465965889280 logging_writer.py:48] [80] global_step=80, grad_norm=0.304271, loss=6.904973
I0608 22:06:03.030647 140500248893248 submission.py:139] 80) loss = 6.905, grad_norm = 0.304
I0608 22:06:03.427981 140465957496576 logging_writer.py:48] [81] global_step=81, grad_norm=0.301211, loss=6.905645
I0608 22:06:03.433867 140500248893248 submission.py:139] 81) loss = 6.906, grad_norm = 0.301
I0608 22:06:03.831378 140465965889280 logging_writer.py:48] [82] global_step=82, grad_norm=0.308542, loss=6.904473
I0608 22:06:03.837839 140500248893248 submission.py:139] 82) loss = 6.904, grad_norm = 0.309
I0608 22:06:04.340247 140465957496576 logging_writer.py:48] [83] global_step=83, grad_norm=0.316527, loss=6.904656
I0608 22:06:04.344613 140500248893248 submission.py:139] 83) loss = 6.905, grad_norm = 0.317
I0608 22:06:04.740610 140465965889280 logging_writer.py:48] [84] global_step=84, grad_norm=0.312175, loss=6.906061
I0608 22:06:04.745002 140500248893248 submission.py:139] 84) loss = 6.906, grad_norm = 0.312
I0608 22:06:05.151690 140465957496576 logging_writer.py:48] [85] global_step=85, grad_norm=0.310928, loss=6.905225
I0608 22:06:05.160431 140500248893248 submission.py:139] 85) loss = 6.905, grad_norm = 0.311
I0608 22:06:05.563329 140465965889280 logging_writer.py:48] [86] global_step=86, grad_norm=0.317253, loss=6.903498
I0608 22:06:05.568266 140500248893248 submission.py:139] 86) loss = 6.903, grad_norm = 0.317
I0608 22:06:06.089585 140465957496576 logging_writer.py:48] [87] global_step=87, grad_norm=0.306873, loss=6.905419
I0608 22:06:06.094286 140500248893248 submission.py:139] 87) loss = 6.905, grad_norm = 0.307
I0608 22:06:06.491250 140465965889280 logging_writer.py:48] [88] global_step=88, grad_norm=0.315856, loss=6.904335
I0608 22:06:06.495502 140500248893248 submission.py:139] 88) loss = 6.904, grad_norm = 0.316
I0608 22:06:06.894772 140465957496576 logging_writer.py:48] [89] global_step=89, grad_norm=0.313702, loss=6.903805
I0608 22:06:06.900949 140500248893248 submission.py:139] 89) loss = 6.904, grad_norm = 0.314
I0608 22:06:07.299299 140465965889280 logging_writer.py:48] [90] global_step=90, grad_norm=0.316829, loss=6.906798
I0608 22:06:07.304637 140500248893248 submission.py:139] 90) loss = 6.907, grad_norm = 0.317
I0608 22:06:07.707398 140465957496576 logging_writer.py:48] [91] global_step=91, grad_norm=0.306570, loss=6.902748
I0608 22:06:07.711905 140500248893248 submission.py:139] 91) loss = 6.903, grad_norm = 0.307
I0608 22:06:08.127127 140465965889280 logging_writer.py:48] [92] global_step=92, grad_norm=0.313826, loss=6.902814
I0608 22:06:08.132234 140500248893248 submission.py:139] 92) loss = 6.903, grad_norm = 0.314
I0608 22:06:08.531847 140465957496576 logging_writer.py:48] [93] global_step=93, grad_norm=0.310717, loss=6.903891
I0608 22:06:08.535878 140500248893248 submission.py:139] 93) loss = 6.904, grad_norm = 0.311
I0608 22:06:08.938459 140465965889280 logging_writer.py:48] [94] global_step=94, grad_norm=0.312216, loss=6.904356
I0608 22:06:08.943837 140500248893248 submission.py:139] 94) loss = 6.904, grad_norm = 0.312
I0608 22:06:09.344558 140465957496576 logging_writer.py:48] [95] global_step=95, grad_norm=0.314161, loss=6.904239
I0608 22:06:09.349871 140500248893248 submission.py:139] 95) loss = 6.904, grad_norm = 0.314
I0608 22:06:09.748828 140465965889280 logging_writer.py:48] [96] global_step=96, grad_norm=0.316918, loss=6.904940
I0608 22:06:09.753951 140500248893248 submission.py:139] 96) loss = 6.905, grad_norm = 0.317
I0608 22:06:10.152161 140465957496576 logging_writer.py:48] [97] global_step=97, grad_norm=0.318899, loss=6.902605
I0608 22:06:10.157161 140500248893248 submission.py:139] 97) loss = 6.903, grad_norm = 0.319
I0608 22:06:10.553430 140465965889280 logging_writer.py:48] [98] global_step=98, grad_norm=0.310272, loss=6.904494
I0608 22:06:10.559415 140500248893248 submission.py:139] 98) loss = 6.904, grad_norm = 0.310
I0608 22:06:10.960530 140465957496576 logging_writer.py:48] [99] global_step=99, grad_norm=0.308130, loss=6.903712
I0608 22:06:10.966586 140500248893248 submission.py:139] 99) loss = 6.904, grad_norm = 0.308
I0608 22:06:11.367083 140465965889280 logging_writer.py:48] [100] global_step=100, grad_norm=0.312372, loss=6.904427
I0608 22:06:11.371547 140500248893248 submission.py:139] 100) loss = 6.904, grad_norm = 0.312
I0608 22:08:55.256053 140465957496576 logging_writer.py:48] [500] global_step=500, grad_norm=1.026672, loss=6.755631
I0608 22:08:55.262148 140500248893248 submission.py:139] 500) loss = 6.756, grad_norm = 1.027
I0608 22:12:20.604817 140465965889280 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.982857, loss=6.489025
I0608 22:12:20.611396 140500248893248 submission.py:139] 1000) loss = 6.489, grad_norm = 0.983
I0608 22:12:30.371920 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:13:12.617335 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:13:56.154360 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:13:57.637067 140500248893248 submission_runner.py:419] Time since start: 645.86s, 	Step: 1026, 	{'train/accuracy': 0.04685546875, 'train/loss': 5.93833740234375, 'validation/accuracy': 0.04318, 'validation/loss': 5.968809375, 'validation/num_examples': 50000, 'test/accuracy': 0.0319, 'test/loss': 6.090401953125, 'test/num_examples': 10000, 'score': 426.3121557235718, 'total_duration': 645.859879732132, 'accumulated_submission_time': 426.3121557235718, 'accumulated_eval_time': 218.95281267166138, 'accumulated_logging_time': 0.029004573822021484}
I0608 22:13:57.647587 140457719858944 logging_writer.py:48] [1026] accumulated_eval_time=218.952813, accumulated_logging_time=0.029005, accumulated_submission_time=426.312156, global_step=1026, preemption_count=0, score=426.312156, test/accuracy=0.031900, test/loss=6.090402, test/num_examples=10000, total_duration=645.859880, train/accuracy=0.046855, train/loss=5.938337, validation/accuracy=0.043180, validation/loss=5.968809, validation/num_examples=50000
I0608 22:17:08.973385 140457728251648 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.987967, loss=6.409653
I0608 22:17:08.979289 140500248893248 submission.py:139] 1500) loss = 6.410, grad_norm = 0.988
I0608 22:20:24.386344 140457719858944 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.859332, loss=6.380476
I0608 22:20:24.391031 140500248893248 submission.py:139] 2000) loss = 6.380, grad_norm = 0.859
I0608 22:20:57.935346 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:21:41.411086 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:22:26.166858 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:22:27.605335 140500248893248 submission_runner.py:419] Time since start: 1155.83s, 	Step: 2087, 	{'train/accuracy': 0.0915625, 'train/loss': 5.324912719726562, 'validation/accuracy': 0.08322, 'validation/loss': 5.370950625, 'validation/num_examples': 50000, 'test/accuracy': 0.0641, 'test/loss': 5.57846796875, 'test/num_examples': 10000, 'score': 845.9935002326965, 'total_duration': 1155.8281183242798, 'accumulated_submission_time': 845.9935002326965, 'accumulated_eval_time': 308.62283635139465, 'accumulated_logging_time': 0.047644615173339844}
I0608 22:22:27.616379 140457728251648 logging_writer.py:48] [2087] accumulated_eval_time=308.622836, accumulated_logging_time=0.047645, accumulated_submission_time=845.993500, global_step=2087, preemption_count=0, score=845.993500, test/accuracy=0.064100, test/loss=5.578468, test/num_examples=10000, total_duration=1155.828118, train/accuracy=0.091563, train/loss=5.324913, validation/accuracy=0.083220, validation/loss=5.370951, validation/num_examples=50000
I0608 22:25:14.670443 140457719858944 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.721220, loss=6.186563
I0608 22:25:14.674879 140500248893248 submission.py:139] 2500) loss = 6.187, grad_norm = 0.721
I0608 22:28:32.722999 140457728251648 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.147455, loss=6.131059
I0608 22:28:32.730619 140500248893248 submission.py:139] 3000) loss = 6.131, grad_norm = 1.147
I0608 22:29:27.855900 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:30:12.309655 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:31:08.005642 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:31:09.445029 140500248893248 submission_runner.py:419] Time since start: 1677.67s, 	Step: 3141, 	{'train/accuracy': 0.12275390625, 'train/loss': 4.94825439453125, 'validation/accuracy': 0.11064, 'validation/loss': 5.02022375, 'validation/num_examples': 50000, 'test/accuracy': 0.0865, 'test/loss': 5.280218359375, 'test/num_examples': 10000, 'score': 1265.6203184127808, 'total_duration': 1677.6677758693695, 'accumulated_submission_time': 1265.6203184127808, 'accumulated_eval_time': 410.2120027542114, 'accumulated_logging_time': 0.06779003143310547}
I0608 22:31:09.456328 140457719858944 logging_writer.py:48] [3141] accumulated_eval_time=410.212003, accumulated_logging_time=0.067790, accumulated_submission_time=1265.620318, global_step=3141, preemption_count=0, score=1265.620318, test/accuracy=0.086500, test/loss=5.280218, test/num_examples=10000, total_duration=1677.667776, train/accuracy=0.122754, train/loss=4.948254, validation/accuracy=0.110640, validation/loss=5.020224, validation/num_examples=50000
I0608 22:33:29.722186 140457728251648 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.951183, loss=6.075945
I0608 22:33:29.728755 140500248893248 submission.py:139] 3500) loss = 6.076, grad_norm = 0.951
I0608 22:36:47.300411 140457719858944 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.694360, loss=6.075933
I0608 22:36:47.305806 140500248893248 submission.py:139] 4000) loss = 6.076, grad_norm = 0.694
I0608 22:38:09.751435 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:38:52.714710 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:39:38.395678 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:39:39.832666 140500248893248 submission_runner.py:419] Time since start: 2188.06s, 	Step: 4212, 	{'train/accuracy': 0.1624609375, 'train/loss': 4.612516479492188, 'validation/accuracy': 0.14962, 'validation/loss': 4.7011196875, 'validation/num_examples': 50000, 'test/accuracy': 0.1116, 'test/loss': 5.025137109375, 'test/num_examples': 10000, 'score': 1685.2962164878845, 'total_duration': 2188.055465698242, 'accumulated_submission_time': 1685.2962164878845, 'accumulated_eval_time': 500.29323840141296, 'accumulated_logging_time': 0.08804702758789062}
I0608 22:39:39.847709 140457728251648 logging_writer.py:48] [4212] accumulated_eval_time=500.293238, accumulated_logging_time=0.088047, accumulated_submission_time=1685.296216, global_step=4212, preemption_count=0, score=1685.296216, test/accuracy=0.111600, test/loss=5.025137, test/num_examples=10000, total_duration=2188.055466, train/accuracy=0.162461, train/loss=4.612516, validation/accuracy=0.149620, validation/loss=4.701120, validation/num_examples=50000
I0608 22:41:32.901946 140457719858944 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.982297, loss=5.735254
I0608 22:41:32.907567 140500248893248 submission.py:139] 4500) loss = 5.735, grad_norm = 0.982
I0608 22:44:52.356881 140457728251648 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.905864, loss=5.696550
I0608 22:44:52.361236 140500248893248 submission.py:139] 5000) loss = 5.697, grad_norm = 0.906
I0608 22:46:40.211922 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:47:24.141084 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:48:09.045224 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:48:10.487664 140500248893248 submission_runner.py:419] Time since start: 2698.71s, 	Step: 5271, 	{'train/accuracy': 0.20306640625, 'train/loss': 4.2773974609375, 'validation/accuracy': 0.18656, 'validation/loss': 4.381340625, 'validation/num_examples': 50000, 'test/accuracy': 0.1448, 'test/loss': 4.71303359375, 'test/num_examples': 10000, 'score': 2105.0590381622314, 'total_duration': 2698.708825111389, 'accumulated_submission_time': 2105.0590381622314, 'accumulated_eval_time': 590.5673751831055, 'accumulated_logging_time': 0.11194086074829102}
I0608 22:48:10.497571 140457719858944 logging_writer.py:48] [5271] accumulated_eval_time=590.567375, accumulated_logging_time=0.111941, accumulated_submission_time=2105.059038, global_step=5271, preemption_count=0, score=2105.059038, test/accuracy=0.144800, test/loss=4.713034, test/num_examples=10000, total_duration=2698.708825, train/accuracy=0.203066, train/loss=4.277397, validation/accuracy=0.186560, validation/loss=4.381341, validation/num_examples=50000
I0608 22:49:40.409595 140457728251648 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.714227, loss=5.698158
I0608 22:49:40.421624 140500248893248 submission.py:139] 5500) loss = 5.698, grad_norm = 0.714
I0608 22:52:56.272404 140457719858944 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.800236, loss=5.649240
I0608 22:52:56.276982 140500248893248 submission.py:139] 6000) loss = 5.649, grad_norm = 0.800
I0608 22:55:10.757020 140500248893248 spec.py:298] Evaluating on the training split.
I0608 22:55:55.532609 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 22:56:40.662812 140500248893248 spec.py:326] Evaluating on the test split.
I0608 22:56:42.109935 140500248893248 submission_runner.py:419] Time since start: 3210.33s, 	Step: 6332, 	{'train/accuracy': 0.249921875, 'train/loss': 3.9210516357421876, 'validation/accuracy': 0.23062, 'validation/loss': 4.0371303125, 'validation/num_examples': 50000, 'test/accuracy': 0.178, 'test/loss': 4.429851171875, 'test/num_examples': 10000, 'score': 2524.7024869918823, 'total_duration': 3210.332690477371, 'accumulated_submission_time': 2524.7024869918823, 'accumulated_eval_time': 681.9203765392303, 'accumulated_logging_time': 0.12985467910766602}
I0608 22:56:42.120401 140457728251648 logging_writer.py:48] [6332] accumulated_eval_time=681.920377, accumulated_logging_time=0.129855, accumulated_submission_time=2524.702487, global_step=6332, preemption_count=0, score=2524.702487, test/accuracy=0.178000, test/loss=4.429851, test/num_examples=10000, total_duration=3210.332690, train/accuracy=0.249922, train/loss=3.921052, validation/accuracy=0.230620, validation/loss=4.037130, validation/num_examples=50000
I0608 22:57:48.008257 140457719858944 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.642019, loss=5.627879
I0608 22:57:48.014382 140500248893248 submission.py:139] 6500) loss = 5.628, grad_norm = 0.642
I0608 23:01:03.788787 140457728251648 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.538126, loss=5.714147
I0608 23:01:03.794963 140500248893248 submission.py:139] 7000) loss = 5.714, grad_norm = 0.538
I0608 23:03:42.196206 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:04:28.673595 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:05:15.408270 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:05:16.866003 140500248893248 submission_runner.py:419] Time since start: 3725.09s, 	Step: 7398, 	{'train/accuracy': 0.28734375, 'train/loss': 3.6703445434570314, 'validation/accuracy': 0.26384, 'validation/loss': 3.80526375, 'validation/num_examples': 50000, 'test/accuracy': 0.2019, 'test/loss': 4.239192578125, 'test/num_examples': 10000, 'score': 2944.158174753189, 'total_duration': 3725.0888187885284, 'accumulated_submission_time': 2944.158174753189, 'accumulated_eval_time': 776.5902438163757, 'accumulated_logging_time': 0.1495072841644287}
I0608 23:05:16.875826 140457719858944 logging_writer.py:48] [7398] accumulated_eval_time=776.590244, accumulated_logging_time=0.149507, accumulated_submission_time=2944.158175, global_step=7398, preemption_count=0, score=2944.158175, test/accuracy=0.201900, test/loss=4.239193, test/num_examples=10000, total_duration=3725.088819, train/accuracy=0.287344, train/loss=3.670345, validation/accuracy=0.263840, validation/loss=3.805264, validation/num_examples=50000
I0608 23:05:58.771953 140457728251648 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.740149, loss=5.433865
I0608 23:05:58.775519 140500248893248 submission.py:139] 7500) loss = 5.434, grad_norm = 0.740
I0608 23:09:16.768498 140457719858944 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.612597, loss=5.451093
I0608 23:09:16.773986 140500248893248 submission.py:139] 8000) loss = 5.451, grad_norm = 0.613
I0608 23:12:17.095146 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:13:01.305966 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:13:46.703401 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:13:48.147778 140500248893248 submission_runner.py:419] Time since start: 4236.37s, 	Step: 8460, 	{'train/accuracy': 0.32244140625, 'train/loss': 3.464699401855469, 'validation/accuracy': 0.29052, 'validation/loss': 3.6158715625, 'validation/num_examples': 50000, 'test/accuracy': 0.2236, 'test/loss': 4.071866015625, 'test/num_examples': 10000, 'score': 3363.7643988132477, 'total_duration': 4236.370578527451, 'accumulated_submission_time': 3363.7643988132477, 'accumulated_eval_time': 867.6428635120392, 'accumulated_logging_time': 0.16844677925109863}
I0608 23:13:48.159827 140457728251648 logging_writer.py:48] [8460] accumulated_eval_time=867.642864, accumulated_logging_time=0.168447, accumulated_submission_time=3363.764399, global_step=8460, preemption_count=0, score=3363.764399, test/accuracy=0.223600, test/loss=4.071866, test/num_examples=10000, total_duration=4236.370579, train/accuracy=0.322441, train/loss=3.464699, validation/accuracy=0.290520, validation/loss=3.615872, validation/num_examples=50000
I0608 23:14:04.172853 140457719858944 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.659287, loss=5.276649
I0608 23:14:04.178260 140500248893248 submission.py:139] 8500) loss = 5.277, grad_norm = 0.659
I0608 23:17:26.026713 140457728251648 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.556462, loss=5.643410
I0608 23:17:26.033044 140500248893248 submission.py:139] 9000) loss = 5.643, grad_norm = 0.556
I0608 23:20:41.554172 140457719858944 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.558416, loss=5.700247
I0608 23:20:41.561561 140500248893248 submission.py:139] 9500) loss = 5.700, grad_norm = 0.558
I0608 23:20:48.208826 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:21:31.963886 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:22:16.732633 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:22:18.168564 140500248893248 submission_runner.py:419] Time since start: 4746.39s, 	Step: 9518, 	{'train/accuracy': 0.34849609375, 'train/loss': 3.303388977050781, 'validation/accuracy': 0.31728, 'validation/loss': 3.450475, 'validation/num_examples': 50000, 'test/accuracy': 0.2441, 'test/loss': 3.933622265625, 'test/num_examples': 10000, 'score': 3783.209609270096, 'total_duration': 4746.391388654709, 'accumulated_submission_time': 3783.209609270096, 'accumulated_eval_time': 957.6026136875153, 'accumulated_logging_time': 0.19009900093078613}
I0608 23:22:18.179368 140457728251648 logging_writer.py:48] [9518] accumulated_eval_time=957.602614, accumulated_logging_time=0.190099, accumulated_submission_time=3783.209609, global_step=9518, preemption_count=0, score=3783.209609, test/accuracy=0.244100, test/loss=3.933622, test/num_examples=10000, total_duration=4746.391389, train/accuracy=0.348496, train/loss=3.303389, validation/accuracy=0.317280, validation/loss=3.450475, validation/num_examples=50000
I0608 23:25:31.708680 140457719858944 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.606309, loss=4.921822
I0608 23:25:31.716217 140500248893248 submission.py:139] 10000) loss = 4.922, grad_norm = 0.606
I0608 23:28:49.556156 140457728251648 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.617557, loss=5.219040
I0608 23:28:49.561435 140500248893248 submission.py:139] 10500) loss = 5.219, grad_norm = 0.618
I0608 23:29:18.481023 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:30:03.266456 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:30:48.705290 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:30:50.144088 140500248893248 submission_runner.py:419] Time since start: 5258.37s, 	Step: 10575, 	{'train/accuracy': 0.39119140625, 'train/loss': 3.0521197509765625, 'validation/accuracy': 0.3559, 'validation/loss': 3.21490625, 'validation/num_examples': 50000, 'test/accuracy': 0.2762, 'test/loss': 3.71592578125, 'test/num_examples': 10000, 'score': 4202.900686502457, 'total_duration': 5258.366930484772, 'accumulated_submission_time': 4202.900686502457, 'accumulated_eval_time': 1049.2656977176666, 'accumulated_logging_time': 0.21040701866149902}
I0608 23:30:50.155808 140457719858944 logging_writer.py:48] [10575] accumulated_eval_time=1049.265698, accumulated_logging_time=0.210407, accumulated_submission_time=4202.900687, global_step=10575, preemption_count=0, score=4202.900687, test/accuracy=0.276200, test/loss=3.715926, test/num_examples=10000, total_duration=5258.366930, train/accuracy=0.391191, train/loss=3.052120, validation/accuracy=0.355900, validation/loss=3.214906, validation/num_examples=50000
I0608 23:33:37.696853 140457728251648 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.638868, loss=4.921352
I0608 23:33:37.703737 140500248893248 submission.py:139] 11000) loss = 4.921, grad_norm = 0.639
I0608 23:36:58.414186 140457719858944 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.664905, loss=4.968808
I0608 23:36:58.421341 140500248893248 submission.py:139] 11500) loss = 4.969, grad_norm = 0.665
I0608 23:37:50.382859 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:38:34.830698 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:39:20.309017 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:39:21.752602 140500248893248 submission_runner.py:419] Time since start: 5769.98s, 	Step: 11634, 	{'train/accuracy': 0.42107421875, 'train/loss': 2.9030419921875, 'validation/accuracy': 0.37886, 'validation/loss': 3.08210875, 'validation/num_examples': 50000, 'test/accuracy': 0.295, 'test/loss': 3.628368359375, 'test/num_examples': 10000, 'score': 4622.517159700394, 'total_duration': 5769.975457191467, 'accumulated_submission_time': 4622.517159700394, 'accumulated_eval_time': 1140.6355004310608, 'accumulated_logging_time': 0.23031330108642578}
I0608 23:39:21.762939 140457728251648 logging_writer.py:48] [11634] accumulated_eval_time=1140.635500, accumulated_logging_time=0.230313, accumulated_submission_time=4622.517160, global_step=11634, preemption_count=0, score=4622.517160, test/accuracy=0.295000, test/loss=3.628368, test/num_examples=10000, total_duration=5769.975457, train/accuracy=0.421074, train/loss=2.903042, validation/accuracy=0.378860, validation/loss=3.082109, validation/num_examples=50000
I0608 23:41:45.320008 140457719858944 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.597567, loss=5.045690
I0608 23:41:45.325188 140500248893248 submission.py:139] 12000) loss = 5.046, grad_norm = 0.598
I0608 23:45:05.084308 140457728251648 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.619929, loss=5.079322
I0608 23:45:05.091094 140500248893248 submission.py:139] 12500) loss = 5.079, grad_norm = 0.620
I0608 23:46:21.845041 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:47:07.070815 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:47:52.175241 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:47:53.618158 140500248893248 submission_runner.py:419] Time since start: 6281.84s, 	Step: 12691, 	{'train/accuracy': 0.44375, 'train/loss': 2.7511138916015625, 'validation/accuracy': 0.40214, 'validation/loss': 2.9306284375, 'validation/num_examples': 50000, 'test/accuracy': 0.3113, 'test/loss': 3.47606875, 'test/num_examples': 10000, 'score': 5042.001644611359, 'total_duration': 6281.840917825699, 'accumulated_submission_time': 5042.001644611359, 'accumulated_eval_time': 1232.4086577892303, 'accumulated_logging_time': 0.2499232292175293}
I0608 23:47:53.629080 140457719858944 logging_writer.py:48] [12691] accumulated_eval_time=1232.408658, accumulated_logging_time=0.249923, accumulated_submission_time=5042.001645, global_step=12691, preemption_count=0, score=5042.001645, test/accuracy=0.311300, test/loss=3.476069, test/num_examples=10000, total_duration=6281.840918, train/accuracy=0.443750, train/loss=2.751114, validation/accuracy=0.402140, validation/loss=2.930628, validation/num_examples=50000
I0608 23:49:55.302838 140457728251648 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.578850, loss=5.067801
I0608 23:49:55.309439 140500248893248 submission.py:139] 13000) loss = 5.068, grad_norm = 0.579
I0608 23:53:11.492085 140457719858944 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.602149, loss=4.813140
I0608 23:53:11.497357 140500248893248 submission.py:139] 13500) loss = 4.813, grad_norm = 0.602
I0608 23:54:53.914634 140500248893248 spec.py:298] Evaluating on the training split.
I0608 23:55:40.338822 140500248893248 spec.py:310] Evaluating on the validation split.
I0608 23:56:33.013607 140500248893248 spec.py:326] Evaluating on the test split.
I0608 23:56:34.461879 140500248893248 submission_runner.py:419] Time since start: 6802.68s, 	Step: 13754, 	{'train/accuracy': 0.46400390625, 'train/loss': 2.6383697509765627, 'validation/accuracy': 0.41986, 'validation/loss': 2.8324784375, 'validation/num_examples': 50000, 'test/accuracy': 0.3286, 'test/loss': 3.4041234375, 'test/num_examples': 10000, 'score': 5461.681895971298, 'total_duration': 6802.684654474258, 'accumulated_submission_time': 5461.681895971298, 'accumulated_eval_time': 1332.95583486557, 'accumulated_logging_time': 0.27028441429138184}
I0608 23:56:34.472297 140457728251648 logging_writer.py:48] [13754] accumulated_eval_time=1332.955835, accumulated_logging_time=0.270284, accumulated_submission_time=5461.681896, global_step=13754, preemption_count=0, score=5461.681896, test/accuracy=0.328600, test/loss=3.404123, test/num_examples=10000, total_duration=6802.684654, train/accuracy=0.464004, train/loss=2.638370, validation/accuracy=0.419860, validation/loss=2.832478, validation/num_examples=50000
I0608 23:58:13.665080 140457719858944 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.601775, loss=4.961322
I0608 23:58:13.670056 140500248893248 submission.py:139] 14000) loss = 4.961, grad_norm = 0.602
I0609 00:01:29.438602 140457728251648 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.570822, loss=4.873045
I0609 00:01:29.444419 140500248893248 submission.py:139] 14500) loss = 4.873, grad_norm = 0.571
I0609 00:03:34.750203 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:04:19.437111 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:05:04.894081 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:05:06.343824 140500248893248 submission_runner.py:419] Time since start: 7314.57s, 	Step: 14817, 	{'train/accuracy': 0.4814453125, 'train/loss': 2.5606634521484377, 'validation/accuracy': 0.44104, 'validation/loss': 2.7477015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3451, 'test/loss': 3.298730078125, 'test/num_examples': 10000, 'score': 5881.3451561927795, 'total_duration': 7314.566672563553, 'accumulated_submission_time': 5881.3451561927795, 'accumulated_eval_time': 1424.5496530532837, 'accumulated_logging_time': 0.2914128303527832}
I0609 00:05:06.354387 140457719858944 logging_writer.py:48] [14817] accumulated_eval_time=1424.549653, accumulated_logging_time=0.291413, accumulated_submission_time=5881.345156, global_step=14817, preemption_count=0, score=5881.345156, test/accuracy=0.345100, test/loss=3.298730, test/num_examples=10000, total_duration=7314.566673, train/accuracy=0.481445, train/loss=2.560663, validation/accuracy=0.441040, validation/loss=2.747702, validation/num_examples=50000
I0609 00:06:21.200615 140457728251648 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.667443, loss=4.920470
I0609 00:06:21.206308 140500248893248 submission.py:139] 15000) loss = 4.920, grad_norm = 0.667
I0609 00:09:39.451195 140457719858944 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.597557, loss=4.851290
I0609 00:09:39.456385 140500248893248 submission.py:139] 15500) loss = 4.851, grad_norm = 0.598
I0609 00:12:06.642238 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:12:50.857746 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:13:35.843722 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:13:37.286304 140500248893248 submission_runner.py:419] Time since start: 7825.51s, 	Step: 15877, 	{'train/accuracy': 0.505703125, 'train/loss': 2.4262493896484374, 'validation/accuracy': 0.46244, 'validation/loss': 2.6334903125, 'validation/num_examples': 50000, 'test/accuracy': 0.3609, 'test/loss': 3.1954896484375, 'test/num_examples': 10000, 'score': 6301.022573709488, 'total_duration': 7825.509122371674, 'accumulated_submission_time': 6301.022573709488, 'accumulated_eval_time': 1515.1939339637756, 'accumulated_logging_time': 0.3109152317047119}
I0609 00:13:37.297944 140457728251648 logging_writer.py:48] [15877] accumulated_eval_time=1515.193934, accumulated_logging_time=0.310915, accumulated_submission_time=6301.022574, global_step=15877, preemption_count=0, score=6301.022574, test/accuracy=0.360900, test/loss=3.195490, test/num_examples=10000, total_duration=7825.509122, train/accuracy=0.505703, train/loss=2.426249, validation/accuracy=0.462440, validation/loss=2.633490, validation/num_examples=50000
I0609 00:14:26.216062 140457719858944 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.603562, loss=4.774345
I0609 00:14:26.221519 140500248893248 submission.py:139] 16000) loss = 4.774, grad_norm = 0.604
I0609 00:17:49.050617 140457728251648 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.705739, loss=4.929498
I0609 00:17:49.058897 140500248893248 submission.py:139] 16500) loss = 4.929, grad_norm = 0.706
I0609 00:20:37.370056 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:21:21.985298 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:22:07.931754 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:22:09.376437 140500248893248 submission_runner.py:419] Time since start: 8337.60s, 	Step: 16931, 	{'train/accuracy': 0.52326171875, 'train/loss': 2.369455108642578, 'validation/accuracy': 0.47516, 'validation/loss': 2.58387703125, 'validation/num_examples': 50000, 'test/accuracy': 0.3704, 'test/loss': 3.153058984375, 'test/num_examples': 10000, 'score': 6720.496959686279, 'total_duration': 8337.599304914474, 'accumulated_submission_time': 6720.496959686279, 'accumulated_eval_time': 1607.2003800868988, 'accumulated_logging_time': 0.33048105239868164}
I0609 00:22:09.387933 140457719858944 logging_writer.py:48] [16931] accumulated_eval_time=1607.200380, accumulated_logging_time=0.330481, accumulated_submission_time=6720.496960, global_step=16931, preemption_count=0, score=6720.496960, test/accuracy=0.370400, test/loss=3.153059, test/num_examples=10000, total_duration=8337.599305, train/accuracy=0.523262, train/loss=2.369455, validation/accuracy=0.475160, validation/loss=2.583877, validation/num_examples=50000
I0609 00:22:36.595519 140457728251648 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.595410, loss=4.748103
I0609 00:22:36.603676 140500248893248 submission.py:139] 17000) loss = 4.748, grad_norm = 0.595
I0609 00:25:57.290033 140457719858944 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.581674, loss=5.025943
I0609 00:25:57.295822 140500248893248 submission.py:139] 17500) loss = 5.026, grad_norm = 0.582
I0609 00:29:09.380109 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:29:54.144737 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:30:39.698397 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:30:41.138410 140500248893248 submission_runner.py:419] Time since start: 8849.36s, 	Step: 17986, 	{'train/accuracy': 0.53310546875, 'train/loss': 2.2464427185058593, 'validation/accuracy': 0.48896, 'validation/loss': 2.466663125, 'validation/num_examples': 50000, 'test/accuracy': 0.3802, 'test/loss': 3.057661328125, 'test/num_examples': 10000, 'score': 7139.891441345215, 'total_duration': 8849.361192941666, 'accumulated_submission_time': 7139.891441345215, 'accumulated_eval_time': 1698.958985567093, 'accumulated_logging_time': 0.35022449493408203}
I0609 00:30:41.148848 140457728251648 logging_writer.py:48] [17986] accumulated_eval_time=1698.958986, accumulated_logging_time=0.350224, accumulated_submission_time=7139.891441, global_step=17986, preemption_count=0, score=7139.891441, test/accuracy=0.380200, test/loss=3.057661, test/num_examples=10000, total_duration=8849.361193, train/accuracy=0.533105, train/loss=2.246443, validation/accuracy=0.488960, validation/loss=2.466663, validation/num_examples=50000
I0609 00:30:47.002222 140457719858944 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.586930, loss=5.205265
I0609 00:30:47.008097 140500248893248 submission.py:139] 18000) loss = 5.205, grad_norm = 0.587
I0609 00:34:03.508337 140457728251648 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.605297, loss=4.505325
I0609 00:34:03.513316 140500248893248 submission.py:139] 18500) loss = 4.505, grad_norm = 0.605
I0609 00:37:25.480338 140457719858944 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.685734, loss=4.970649
I0609 00:37:25.486476 140500248893248 submission.py:139] 19000) loss = 4.971, grad_norm = 0.686
I0609 00:37:41.499107 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:38:27.806719 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:39:13.940657 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:39:15.380206 140500248893248 submission_runner.py:419] Time since start: 9363.60s, 	Step: 19042, 	{'train/accuracy': 0.55056640625, 'train/loss': 2.213994903564453, 'validation/accuracy': 0.50336, 'validation/loss': 2.4398609375, 'validation/num_examples': 50000, 'test/accuracy': 0.3862, 'test/loss': 3.0356419921875, 'test/num_examples': 10000, 'score': 7559.646770238876, 'total_duration': 9363.602983236313, 'accumulated_submission_time': 7559.646770238876, 'accumulated_eval_time': 1792.840077638626, 'accumulated_logging_time': 0.3684706687927246}
I0609 00:39:15.391881 140457728251648 logging_writer.py:48] [19042] accumulated_eval_time=1792.840078, accumulated_logging_time=0.368471, accumulated_submission_time=7559.646770, global_step=19042, preemption_count=0, score=7559.646770, test/accuracy=0.386200, test/loss=3.035642, test/num_examples=10000, total_duration=9363.602983, train/accuracy=0.550566, train/loss=2.213995, validation/accuracy=0.503360, validation/loss=2.439861, validation/num_examples=50000
I0609 00:42:14.844973 140457719858944 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.675663, loss=4.327742
I0609 00:42:14.851794 140500248893248 submission.py:139] 19500) loss = 4.328, grad_norm = 0.676
I0609 00:45:36.010523 140457728251648 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.606289, loss=4.632703
I0609 00:45:36.016442 140500248893248 submission.py:139] 20000) loss = 4.633, grad_norm = 0.606
I0609 00:46:15.517335 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:47:00.329031 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:47:46.326784 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:47:47.767128 140500248893248 submission_runner.py:419] Time since start: 9875.99s, 	Step: 20096, 	{'train/accuracy': 0.560703125, 'train/loss': 2.165033874511719, 'validation/accuracy': 0.50744, 'validation/loss': 2.404935625, 'validation/num_examples': 50000, 'test/accuracy': 0.3952, 'test/loss': 3.013013671875, 'test/num_examples': 10000, 'score': 7979.180966615677, 'total_duration': 9875.98996925354, 'accumulated_submission_time': 7979.180966615677, 'accumulated_eval_time': 1885.090029001236, 'accumulated_logging_time': 0.3890049457550049}
I0609 00:47:47.778919 140457719858944 logging_writer.py:48] [20096] accumulated_eval_time=1885.090029, accumulated_logging_time=0.389005, accumulated_submission_time=7979.180967, global_step=20096, preemption_count=0, score=7979.180967, test/accuracy=0.395200, test/loss=3.013014, test/num_examples=10000, total_duration=9875.989969, train/accuracy=0.560703, train/loss=2.165034, validation/accuracy=0.507440, validation/loss=2.404936, validation/num_examples=50000
I0609 00:50:26.198143 140457728251648 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.595815, loss=4.587275
I0609 00:50:26.204174 140500248893248 submission.py:139] 20500) loss = 4.587, grad_norm = 0.596
I0609 00:53:42.425566 140457719858944 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.609161, loss=4.401187
I0609 00:53:42.431693 140500248893248 submission.py:139] 21000) loss = 4.401, grad_norm = 0.609
I0609 00:54:47.866744 140500248893248 spec.py:298] Evaluating on the training split.
I0609 00:55:33.362685 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 00:56:19.722127 140500248893248 spec.py:326] Evaluating on the test split.
I0609 00:56:21.165050 140500248893248 submission_runner.py:419] Time since start: 10389.39s, 	Step: 21161, 	{'train/accuracy': 0.57642578125, 'train/loss': 2.077220764160156, 'validation/accuracy': 0.52282, 'validation/loss': 2.322801875, 'validation/num_examples': 50000, 'test/accuracy': 0.4178, 'test/loss': 2.9127734375, 'test/num_examples': 10000, 'score': 8398.65545129776, 'total_duration': 10389.387785434723, 'accumulated_submission_time': 8398.65545129776, 'accumulated_eval_time': 1978.3882493972778, 'accumulated_logging_time': 0.4092214107513428}
I0609 00:56:21.175783 140457728251648 logging_writer.py:48] [21161] accumulated_eval_time=1978.388249, accumulated_logging_time=0.409221, accumulated_submission_time=8398.655451, global_step=21161, preemption_count=0, score=8398.655451, test/accuracy=0.417800, test/loss=2.912773, test/num_examples=10000, total_duration=10389.387785, train/accuracy=0.576426, train/loss=2.077221, validation/accuracy=0.522820, validation/loss=2.322802, validation/num_examples=50000
I0609 00:58:38.304910 140457719858944 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.610084, loss=4.553626
I0609 00:58:38.314504 140500248893248 submission.py:139] 21500) loss = 4.554, grad_norm = 0.610
I0609 01:01:54.056863 140457728251648 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.637804, loss=4.803778
I0609 01:01:54.064294 140500248893248 submission.py:139] 22000) loss = 4.804, grad_norm = 0.638
I0609 01:03:21.203309 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:04:05.792994 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:04:51.470746 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:04:52.912718 140500248893248 submission_runner.py:419] Time since start: 10901.14s, 	Step: 22222, 	{'train/accuracy': 0.58388671875, 'train/loss': 2.0474525451660157, 'validation/accuracy': 0.52936, 'validation/loss': 2.2956990625, 'validation/num_examples': 50000, 'test/accuracy': 0.4161, 'test/loss': 2.888051171875, 'test/num_examples': 10000, 'score': 8818.064839363098, 'total_duration': 10901.135578155518, 'accumulated_submission_time': 8818.064839363098, 'accumulated_eval_time': 2070.0977988243103, 'accumulated_logging_time': 0.4292612075805664}
I0609 01:04:52.924702 140457719858944 logging_writer.py:48] [22222] accumulated_eval_time=2070.097799, accumulated_logging_time=0.429261, accumulated_submission_time=8818.064839, global_step=22222, preemption_count=0, score=8818.064839, test/accuracy=0.416100, test/loss=2.888051, test/num_examples=10000, total_duration=10901.135578, train/accuracy=0.583887, train/loss=2.047453, validation/accuracy=0.529360, validation/loss=2.295699, validation/num_examples=50000
I0609 01:06:46.070816 140457728251648 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.615939, loss=4.380283
I0609 01:06:46.076799 140500248893248 submission.py:139] 22500) loss = 4.380, grad_norm = 0.616
I0609 01:10:04.407676 140457719858944 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.626674, loss=4.185850
I0609 01:10:04.415357 140500248893248 submission.py:139] 23000) loss = 4.186, grad_norm = 0.627
I0609 01:11:53.199519 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:12:37.545279 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:13:23.750794 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:13:25.190259 140500248893248 submission_runner.py:419] Time since start: 11413.41s, 	Step: 23279, 	{'train/accuracy': 0.5920703125, 'train/loss': 1.9773826599121094, 'validation/accuracy': 0.5372, 'validation/loss': 2.2296446875, 'validation/num_examples': 50000, 'test/accuracy': 0.4219, 'test/loss': 2.8562646484375, 'test/num_examples': 10000, 'score': 9237.738062143326, 'total_duration': 11413.413077831268, 'accumulated_submission_time': 9237.738062143326, 'accumulated_eval_time': 2162.088733434677, 'accumulated_logging_time': 0.4504718780517578}
I0609 01:13:25.201762 140457728251648 logging_writer.py:48] [23279] accumulated_eval_time=2162.088733, accumulated_logging_time=0.450472, accumulated_submission_time=9237.738062, global_step=23279, preemption_count=0, score=9237.738062, test/accuracy=0.421900, test/loss=2.856265, test/num_examples=10000, total_duration=11413.413078, train/accuracy=0.592070, train/loss=1.977383, validation/accuracy=0.537200, validation/loss=2.229645, validation/num_examples=50000
I0609 01:14:52.248956 140457719858944 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.701335, loss=4.328035
I0609 01:14:52.253893 140500248893248 submission.py:139] 23500) loss = 4.328, grad_norm = 0.701
I0609 01:18:15.663128 140457728251648 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.681565, loss=4.307000
I0609 01:18:15.668688 140500248893248 submission.py:139] 24000) loss = 4.307, grad_norm = 0.682
I0609 01:20:25.257073 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:21:10.755909 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:21:56.532958 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:21:57.977407 140500248893248 submission_runner.py:419] Time since start: 11926.20s, 	Step: 24332, 	{'train/accuracy': 0.602734375, 'train/loss': 1.9051229858398437, 'validation/accuracy': 0.54442, 'validation/loss': 2.159108125, 'validation/num_examples': 50000, 'test/accuracy': 0.43, 'test/loss': 2.784556640625, 'test/num_examples': 10000, 'score': 9657.19081234932, 'total_duration': 11926.20014333725, 'accumulated_submission_time': 9657.19081234932, 'accumulated_eval_time': 2254.8090534210205, 'accumulated_logging_time': 0.47185540199279785}
I0609 01:21:57.988130 140457719858944 logging_writer.py:48] [24332] accumulated_eval_time=2254.809053, accumulated_logging_time=0.471855, accumulated_submission_time=9657.190812, global_step=24332, preemption_count=0, score=9657.190812, test/accuracy=0.430000, test/loss=2.784557, test/num_examples=10000, total_duration=11926.200143, train/accuracy=0.602734, train/loss=1.905123, validation/accuracy=0.544420, validation/loss=2.159108, validation/num_examples=50000
I0609 01:23:03.888714 140457728251648 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.697918, loss=4.281939
I0609 01:23:03.893487 140500248893248 submission.py:139] 24500) loss = 4.282, grad_norm = 0.698
I0609 01:26:23.464219 140457719858944 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.658989, loss=4.536756
I0609 01:26:23.471799 140500248893248 submission.py:139] 25000) loss = 4.537, grad_norm = 0.659
I0609 01:28:58.133261 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:29:44.160240 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:30:30.135128 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:30:31.574452 140500248893248 submission_runner.py:419] Time since start: 12439.80s, 	Step: 25388, 	{'train/accuracy': 0.61564453125, 'train/loss': 1.8430711364746093, 'validation/accuracy': 0.55892, 'validation/loss': 2.1079784375, 'validation/num_examples': 50000, 'test/accuracy': 0.4351, 'test/loss': 2.7411392578125, 'test/num_examples': 10000, 'score': 10076.726477146149, 'total_duration': 12439.797152280807, 'accumulated_submission_time': 10076.726477146149, 'accumulated_eval_time': 2348.250182390213, 'accumulated_logging_time': 0.49073338508605957}
I0609 01:30:31.588421 140457728251648 logging_writer.py:48] [25388] accumulated_eval_time=2348.250182, accumulated_logging_time=0.490733, accumulated_submission_time=10076.726477, global_step=25388, preemption_count=0, score=10076.726477, test/accuracy=0.435100, test/loss=2.741139, test/num_examples=10000, total_duration=12439.797152, train/accuracy=0.615645, train/loss=1.843071, validation/accuracy=0.558920, validation/loss=2.107978, validation/num_examples=50000
I0609 01:31:15.988867 140457719858944 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.639481, loss=4.436830
I0609 01:31:15.994439 140500248893248 submission.py:139] 25500) loss = 4.437, grad_norm = 0.639
I0609 01:34:32.658283 140457728251648 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.580418, loss=4.612617
I0609 01:34:32.663486 140500248893248 submission.py:139] 26000) loss = 4.613, grad_norm = 0.580
I0609 01:37:31.856480 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:38:16.455530 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:39:03.182079 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:39:04.630104 140500248893248 submission_runner.py:419] Time since start: 12952.85s, 	Step: 26443, 	{'train/accuracy': 0.62353515625, 'train/loss': 1.794798583984375, 'validation/accuracy': 0.5662, 'validation/loss': 2.06603, 'validation/num_examples': 50000, 'test/accuracy': 0.4456, 'test/loss': 2.6827669921875, 'test/num_examples': 10000, 'score': 10496.385911464691, 'total_duration': 12952.852885723114, 'accumulated_submission_time': 10496.385911464691, 'accumulated_eval_time': 2441.023733854294, 'accumulated_logging_time': 0.5131669044494629}
I0609 01:39:04.641164 140457719858944 logging_writer.py:48] [26443] accumulated_eval_time=2441.023734, accumulated_logging_time=0.513167, accumulated_submission_time=10496.385911, global_step=26443, preemption_count=0, score=10496.385911, test/accuracy=0.445600, test/loss=2.682767, test/num_examples=10000, total_duration=12952.852886, train/accuracy=0.623535, train/loss=1.794799, validation/accuracy=0.566200, validation/loss=2.066030, validation/num_examples=50000
I0609 01:39:27.315788 140457728251648 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.594355, loss=4.570690
I0609 01:39:27.321579 140500248893248 submission.py:139] 26500) loss = 4.571, grad_norm = 0.594
I0609 01:42:43.045713 140457719858944 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.570955, loss=4.617651
I0609 01:42:43.053271 140500248893248 submission.py:139] 27000) loss = 4.618, grad_norm = 0.571
I0609 01:46:03.560643 140457728251648 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.705170, loss=4.198485
I0609 01:46:03.569089 140500248893248 submission.py:139] 27500) loss = 4.198, grad_norm = 0.705
I0609 01:46:04.968169 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:46:52.584592 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:47:38.813423 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:47:40.272913 140500248893248 submission_runner.py:419] Time since start: 13468.50s, 	Step: 27504, 	{'train/accuracy': 0.6291015625, 'train/loss': 1.8338388061523438, 'validation/accuracy': 0.56824, 'validation/loss': 2.1021384375, 'validation/num_examples': 50000, 'test/accuracy': 0.449, 'test/loss': 2.7070771484375, 'test/num_examples': 10000, 'score': 10916.09705209732, 'total_duration': 13468.495706319809, 'accumulated_submission_time': 10916.09705209732, 'accumulated_eval_time': 2536.328562259674, 'accumulated_logging_time': 0.5369217395782471}
I0609 01:47:40.286333 140457719858944 logging_writer.py:48] [27504] accumulated_eval_time=2536.328562, accumulated_logging_time=0.536922, accumulated_submission_time=10916.097052, global_step=27504, preemption_count=0, score=10916.097052, test/accuracy=0.449000, test/loss=2.707077, test/num_examples=10000, total_duration=13468.495706, train/accuracy=0.629102, train/loss=1.833839, validation/accuracy=0.568240, validation/loss=2.102138, validation/num_examples=50000
I0609 01:50:57.074091 140500248893248 spec.py:298] Evaluating on the training split.
I0609 01:51:41.727712 140500248893248 spec.py:310] Evaluating on the validation split.
I0609 01:52:27.422495 140500248893248 spec.py:326] Evaluating on the test split.
I0609 01:52:28.871337 140500248893248 submission_runner.py:419] Time since start: 13757.09s, 	Step: 28000, 	{'train/accuracy': 0.63400390625, 'train/loss': 1.8023179626464845, 'validation/accuracy': 0.57366, 'validation/loss': 2.079451875, 'validation/num_examples': 50000, 'test/accuracy': 0.4529, 'test/loss': 2.691754296875, 'test/num_examples': 10000, 'score': 11112.588920354843, 'total_duration': 13757.09417629242, 'accumulated_submission_time': 11112.588920354843, 'accumulated_eval_time': 2628.1263518333435, 'accumulated_logging_time': 0.5592715740203857}
I0609 01:52:28.883134 140457728251648 logging_writer.py:48] [28000] accumulated_eval_time=2628.126352, accumulated_logging_time=0.559272, accumulated_submission_time=11112.588920, global_step=28000, preemption_count=0, score=11112.588920, test/accuracy=0.452900, test/loss=2.691754, test/num_examples=10000, total_duration=13757.094176, train/accuracy=0.634004, train/loss=1.802318, validation/accuracy=0.573660, validation/loss=2.079452, validation/num_examples=50000
I0609 01:52:28.900209 140457719858944 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11112.588920
I0609 01:52:29.405810 140500248893248 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nesterov/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0609 01:52:29.665817 140500248893248 submission_runner.py:581] Tuning trial 1/1
I0609 01:52:29.666018 140500248893248 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 01:52:29.666837 140500248893248 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009375, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.757206201553345, 'total_duration': 138.44540071487427, 'accumulated_submission_time': 6.757206201553345, 'accumulated_eval_time': 131.68766379356384, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1026, {'train/accuracy': 0.04685546875, 'train/loss': 5.93833740234375, 'validation/accuracy': 0.04318, 'validation/loss': 5.968809375, 'validation/num_examples': 50000, 'test/accuracy': 0.0319, 'test/loss': 6.090401953125, 'test/num_examples': 10000, 'score': 426.3121557235718, 'total_duration': 645.859879732132, 'accumulated_submission_time': 426.3121557235718, 'accumulated_eval_time': 218.95281267166138, 'accumulated_logging_time': 0.029004573822021484, 'global_step': 1026, 'preemption_count': 0}), (2087, {'train/accuracy': 0.0915625, 'train/loss': 5.324912719726562, 'validation/accuracy': 0.08322, 'validation/loss': 5.370950625, 'validation/num_examples': 50000, 'test/accuracy': 0.0641, 'test/loss': 5.57846796875, 'test/num_examples': 10000, 'score': 845.9935002326965, 'total_duration': 1155.8281183242798, 'accumulated_submission_time': 845.9935002326965, 'accumulated_eval_time': 308.62283635139465, 'accumulated_logging_time': 0.047644615173339844, 'global_step': 2087, 'preemption_count': 0}), (3141, {'train/accuracy': 0.12275390625, 'train/loss': 4.94825439453125, 'validation/accuracy': 0.11064, 'validation/loss': 5.02022375, 'validation/num_examples': 50000, 'test/accuracy': 0.0865, 'test/loss': 5.280218359375, 'test/num_examples': 10000, 'score': 1265.6203184127808, 'total_duration': 1677.6677758693695, 'accumulated_submission_time': 1265.6203184127808, 'accumulated_eval_time': 410.2120027542114, 'accumulated_logging_time': 0.06779003143310547, 'global_step': 3141, 'preemption_count': 0}), (4212, {'train/accuracy': 0.1624609375, 'train/loss': 4.612516479492188, 'validation/accuracy': 0.14962, 'validation/loss': 4.7011196875, 'validation/num_examples': 50000, 'test/accuracy': 0.1116, 'test/loss': 5.025137109375, 'test/num_examples': 10000, 'score': 1685.2962164878845, 'total_duration': 2188.055465698242, 'accumulated_submission_time': 1685.2962164878845, 'accumulated_eval_time': 500.29323840141296, 'accumulated_logging_time': 0.08804702758789062, 'global_step': 4212, 'preemption_count': 0}), (5271, {'train/accuracy': 0.20306640625, 'train/loss': 4.2773974609375, 'validation/accuracy': 0.18656, 'validation/loss': 4.381340625, 'validation/num_examples': 50000, 'test/accuracy': 0.1448, 'test/loss': 4.71303359375, 'test/num_examples': 10000, 'score': 2105.0590381622314, 'total_duration': 2698.708825111389, 'accumulated_submission_time': 2105.0590381622314, 'accumulated_eval_time': 590.5673751831055, 'accumulated_logging_time': 0.11194086074829102, 'global_step': 5271, 'preemption_count': 0}), (6332, {'train/accuracy': 0.249921875, 'train/loss': 3.9210516357421876, 'validation/accuracy': 0.23062, 'validation/loss': 4.0371303125, 'validation/num_examples': 50000, 'test/accuracy': 0.178, 'test/loss': 4.429851171875, 'test/num_examples': 10000, 'score': 2524.7024869918823, 'total_duration': 3210.332690477371, 'accumulated_submission_time': 2524.7024869918823, 'accumulated_eval_time': 681.9203765392303, 'accumulated_logging_time': 0.12985467910766602, 'global_step': 6332, 'preemption_count': 0}), (7398, {'train/accuracy': 0.28734375, 'train/loss': 3.6703445434570314, 'validation/accuracy': 0.26384, 'validation/loss': 3.80526375, 'validation/num_examples': 50000, 'test/accuracy': 0.2019, 'test/loss': 4.239192578125, 'test/num_examples': 10000, 'score': 2944.158174753189, 'total_duration': 3725.0888187885284, 'accumulated_submission_time': 2944.158174753189, 'accumulated_eval_time': 776.5902438163757, 'accumulated_logging_time': 0.1495072841644287, 'global_step': 7398, 'preemption_count': 0}), (8460, {'train/accuracy': 0.32244140625, 'train/loss': 3.464699401855469, 'validation/accuracy': 0.29052, 'validation/loss': 3.6158715625, 'validation/num_examples': 50000, 'test/accuracy': 0.2236, 'test/loss': 4.071866015625, 'test/num_examples': 10000, 'score': 3363.7643988132477, 'total_duration': 4236.370578527451, 'accumulated_submission_time': 3363.7643988132477, 'accumulated_eval_time': 867.6428635120392, 'accumulated_logging_time': 0.16844677925109863, 'global_step': 8460, 'preemption_count': 0}), (9518, {'train/accuracy': 0.34849609375, 'train/loss': 3.303388977050781, 'validation/accuracy': 0.31728, 'validation/loss': 3.450475, 'validation/num_examples': 50000, 'test/accuracy': 0.2441, 'test/loss': 3.933622265625, 'test/num_examples': 10000, 'score': 3783.209609270096, 'total_duration': 4746.391388654709, 'accumulated_submission_time': 3783.209609270096, 'accumulated_eval_time': 957.6026136875153, 'accumulated_logging_time': 0.19009900093078613, 'global_step': 9518, 'preemption_count': 0}), (10575, {'train/accuracy': 0.39119140625, 'train/loss': 3.0521197509765625, 'validation/accuracy': 0.3559, 'validation/loss': 3.21490625, 'validation/num_examples': 50000, 'test/accuracy': 0.2762, 'test/loss': 3.71592578125, 'test/num_examples': 10000, 'score': 4202.900686502457, 'total_duration': 5258.366930484772, 'accumulated_submission_time': 4202.900686502457, 'accumulated_eval_time': 1049.2656977176666, 'accumulated_logging_time': 0.21040701866149902, 'global_step': 10575, 'preemption_count': 0}), (11634, {'train/accuracy': 0.42107421875, 'train/loss': 2.9030419921875, 'validation/accuracy': 0.37886, 'validation/loss': 3.08210875, 'validation/num_examples': 50000, 'test/accuracy': 0.295, 'test/loss': 3.628368359375, 'test/num_examples': 10000, 'score': 4622.517159700394, 'total_duration': 5769.975457191467, 'accumulated_submission_time': 4622.517159700394, 'accumulated_eval_time': 1140.6355004310608, 'accumulated_logging_time': 0.23031330108642578, 'global_step': 11634, 'preemption_count': 0}), (12691, {'train/accuracy': 0.44375, 'train/loss': 2.7511138916015625, 'validation/accuracy': 0.40214, 'validation/loss': 2.9306284375, 'validation/num_examples': 50000, 'test/accuracy': 0.3113, 'test/loss': 3.47606875, 'test/num_examples': 10000, 'score': 5042.001644611359, 'total_duration': 6281.840917825699, 'accumulated_submission_time': 5042.001644611359, 'accumulated_eval_time': 1232.4086577892303, 'accumulated_logging_time': 0.2499232292175293, 'global_step': 12691, 'preemption_count': 0}), (13754, {'train/accuracy': 0.46400390625, 'train/loss': 2.6383697509765627, 'validation/accuracy': 0.41986, 'validation/loss': 2.8324784375, 'validation/num_examples': 50000, 'test/accuracy': 0.3286, 'test/loss': 3.4041234375, 'test/num_examples': 10000, 'score': 5461.681895971298, 'total_duration': 6802.684654474258, 'accumulated_submission_time': 5461.681895971298, 'accumulated_eval_time': 1332.95583486557, 'accumulated_logging_time': 0.27028441429138184, 'global_step': 13754, 'preemption_count': 0}), (14817, {'train/accuracy': 0.4814453125, 'train/loss': 2.5606634521484377, 'validation/accuracy': 0.44104, 'validation/loss': 2.7477015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3451, 'test/loss': 3.298730078125, 'test/num_examples': 10000, 'score': 5881.3451561927795, 'total_duration': 7314.566672563553, 'accumulated_submission_time': 5881.3451561927795, 'accumulated_eval_time': 1424.5496530532837, 'accumulated_logging_time': 0.2914128303527832, 'global_step': 14817, 'preemption_count': 0}), (15877, {'train/accuracy': 0.505703125, 'train/loss': 2.4262493896484374, 'validation/accuracy': 0.46244, 'validation/loss': 2.6334903125, 'validation/num_examples': 50000, 'test/accuracy': 0.3609, 'test/loss': 3.1954896484375, 'test/num_examples': 10000, 'score': 6301.022573709488, 'total_duration': 7825.509122371674, 'accumulated_submission_time': 6301.022573709488, 'accumulated_eval_time': 1515.1939339637756, 'accumulated_logging_time': 0.3109152317047119, 'global_step': 15877, 'preemption_count': 0}), (16931, {'train/accuracy': 0.52326171875, 'train/loss': 2.369455108642578, 'validation/accuracy': 0.47516, 'validation/loss': 2.58387703125, 'validation/num_examples': 50000, 'test/accuracy': 0.3704, 'test/loss': 3.153058984375, 'test/num_examples': 10000, 'score': 6720.496959686279, 'total_duration': 8337.599304914474, 'accumulated_submission_time': 6720.496959686279, 'accumulated_eval_time': 1607.2003800868988, 'accumulated_logging_time': 0.33048105239868164, 'global_step': 16931, 'preemption_count': 0}), (17986, {'train/accuracy': 0.53310546875, 'train/loss': 2.2464427185058593, 'validation/accuracy': 0.48896, 'validation/loss': 2.466663125, 'validation/num_examples': 50000, 'test/accuracy': 0.3802, 'test/loss': 3.057661328125, 'test/num_examples': 10000, 'score': 7139.891441345215, 'total_duration': 8849.361192941666, 'accumulated_submission_time': 7139.891441345215, 'accumulated_eval_time': 1698.958985567093, 'accumulated_logging_time': 0.35022449493408203, 'global_step': 17986, 'preemption_count': 0}), (19042, {'train/accuracy': 0.55056640625, 'train/loss': 2.213994903564453, 'validation/accuracy': 0.50336, 'validation/loss': 2.4398609375, 'validation/num_examples': 50000, 'test/accuracy': 0.3862, 'test/loss': 3.0356419921875, 'test/num_examples': 10000, 'score': 7559.646770238876, 'total_duration': 9363.602983236313, 'accumulated_submission_time': 7559.646770238876, 'accumulated_eval_time': 1792.840077638626, 'accumulated_logging_time': 0.3684706687927246, 'global_step': 19042, 'preemption_count': 0}), (20096, {'train/accuracy': 0.560703125, 'train/loss': 2.165033874511719, 'validation/accuracy': 0.50744, 'validation/loss': 2.404935625, 'validation/num_examples': 50000, 'test/accuracy': 0.3952, 'test/loss': 3.013013671875, 'test/num_examples': 10000, 'score': 7979.180966615677, 'total_duration': 9875.98996925354, 'accumulated_submission_time': 7979.180966615677, 'accumulated_eval_time': 1885.090029001236, 'accumulated_logging_time': 0.3890049457550049, 'global_step': 20096, 'preemption_count': 0}), (21161, {'train/accuracy': 0.57642578125, 'train/loss': 2.077220764160156, 'validation/accuracy': 0.52282, 'validation/loss': 2.322801875, 'validation/num_examples': 50000, 'test/accuracy': 0.4178, 'test/loss': 2.9127734375, 'test/num_examples': 10000, 'score': 8398.65545129776, 'total_duration': 10389.387785434723, 'accumulated_submission_time': 8398.65545129776, 'accumulated_eval_time': 1978.3882493972778, 'accumulated_logging_time': 0.4092214107513428, 'global_step': 21161, 'preemption_count': 0}), (22222, {'train/accuracy': 0.58388671875, 'train/loss': 2.0474525451660157, 'validation/accuracy': 0.52936, 'validation/loss': 2.2956990625, 'validation/num_examples': 50000, 'test/accuracy': 0.4161, 'test/loss': 2.888051171875, 'test/num_examples': 10000, 'score': 8818.064839363098, 'total_duration': 10901.135578155518, 'accumulated_submission_time': 8818.064839363098, 'accumulated_eval_time': 2070.0977988243103, 'accumulated_logging_time': 0.4292612075805664, 'global_step': 22222, 'preemption_count': 0}), (23279, {'train/accuracy': 0.5920703125, 'train/loss': 1.9773826599121094, 'validation/accuracy': 0.5372, 'validation/loss': 2.2296446875, 'validation/num_examples': 50000, 'test/accuracy': 0.4219, 'test/loss': 2.8562646484375, 'test/num_examples': 10000, 'score': 9237.738062143326, 'total_duration': 11413.413077831268, 'accumulated_submission_time': 9237.738062143326, 'accumulated_eval_time': 2162.088733434677, 'accumulated_logging_time': 0.4504718780517578, 'global_step': 23279, 'preemption_count': 0}), (24332, {'train/accuracy': 0.602734375, 'train/loss': 1.9051229858398437, 'validation/accuracy': 0.54442, 'validation/loss': 2.159108125, 'validation/num_examples': 50000, 'test/accuracy': 0.43, 'test/loss': 2.784556640625, 'test/num_examples': 10000, 'score': 9657.19081234932, 'total_duration': 11926.20014333725, 'accumulated_submission_time': 9657.19081234932, 'accumulated_eval_time': 2254.8090534210205, 'accumulated_logging_time': 0.47185540199279785, 'global_step': 24332, 'preemption_count': 0}), (25388, {'train/accuracy': 0.61564453125, 'train/loss': 1.8430711364746093, 'validation/accuracy': 0.55892, 'validation/loss': 2.1079784375, 'validation/num_examples': 50000, 'test/accuracy': 0.4351, 'test/loss': 2.7411392578125, 'test/num_examples': 10000, 'score': 10076.726477146149, 'total_duration': 12439.797152280807, 'accumulated_submission_time': 10076.726477146149, 'accumulated_eval_time': 2348.250182390213, 'accumulated_logging_time': 0.49073338508605957, 'global_step': 25388, 'preemption_count': 0}), (26443, {'train/accuracy': 0.62353515625, 'train/loss': 1.794798583984375, 'validation/accuracy': 0.5662, 'validation/loss': 2.06603, 'validation/num_examples': 50000, 'test/accuracy': 0.4456, 'test/loss': 2.6827669921875, 'test/num_examples': 10000, 'score': 10496.385911464691, 'total_duration': 12952.852885723114, 'accumulated_submission_time': 10496.385911464691, 'accumulated_eval_time': 2441.023733854294, 'accumulated_logging_time': 0.5131669044494629, 'global_step': 26443, 'preemption_count': 0}), (27504, {'train/accuracy': 0.6291015625, 'train/loss': 1.8338388061523438, 'validation/accuracy': 0.56824, 'validation/loss': 2.1021384375, 'validation/num_examples': 50000, 'test/accuracy': 0.449, 'test/loss': 2.7070771484375, 'test/num_examples': 10000, 'score': 10916.09705209732, 'total_duration': 13468.495706319809, 'accumulated_submission_time': 10916.09705209732, 'accumulated_eval_time': 2536.328562259674, 'accumulated_logging_time': 0.5369217395782471, 'global_step': 27504, 'preemption_count': 0}), (28000, {'train/accuracy': 0.63400390625, 'train/loss': 1.8023179626464845, 'validation/accuracy': 0.57366, 'validation/loss': 2.079451875, 'validation/num_examples': 50000, 'test/accuracy': 0.4529, 'test/loss': 2.691754296875, 'test/num_examples': 10000, 'score': 11112.588920354843, 'total_duration': 13757.09417629242, 'accumulated_submission_time': 11112.588920354843, 'accumulated_eval_time': 2628.1263518333435, 'accumulated_logging_time': 0.5592715740203857, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0609 01:52:29.666947 140500248893248 submission_runner.py:584] Timing: 11112.588920354843
I0609 01:52:29.666996 140500248893248 submission_runner.py:586] Total number of evals: 28
I0609 01:52:29.667040 140500248893248 submission_runner.py:587] ====================
I0609 01:52:29.667155 140500248893248 submission_runner.py:655] Final imagenet_vit score: 11112.588920354843
