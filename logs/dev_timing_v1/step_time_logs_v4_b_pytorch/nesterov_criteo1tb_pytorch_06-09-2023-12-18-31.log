torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_pytorch_06-09-2023-12-18-31.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 12:18:54.964428 139971802834752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 12:18:54.964457 139703197075264 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 12:18:54.964481 140588468139840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 12:18:54.965290 140142761670464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 12:18:54.965299 139948846614336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 12:18:54.965324 140047401502528 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 12:18:54.965545 140301309593408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 12:18:54.975435 139656796374848 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 12:18:54.975855 139656796374848 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.975909 140047401502528 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.975966 140142761670464 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.976115 139948846614336 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.976252 140301309593408 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.985519 140588468139840 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.985555 139971802834752 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.985590 139703197075264 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:54.997163 139656796374848 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nesterov/criteo1tb_pytorch.
W0609 12:18:55.129528 140142761670464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.130707 139703197075264 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.130814 139971802834752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.130899 140047401502528 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.133295 139948846614336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.133392 139656796374848 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.133559 140301309593408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:55.133665 140588468139840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 12:18:55.139153 139656796374848 submission_runner.py:541] Using RNG seed 2875583876
I0609 12:18:55.140859 139656796374848 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 12:18:55.140971 139656796374848 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nesterov/criteo1tb_pytorch/trial_1.
I0609 12:18:55.141297 139656796374848 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nesterov/criteo1tb_pytorch/trial_1/hparams.json.
I0609 12:18:55.142363 139656796374848 submission_runner.py:255] Initializing dataset.
I0609 12:18:55.142502 139656796374848 submission_runner.py:262] Initializing model.
I0609 12:19:08.524660 139656796374848 submission_runner.py:272] Initializing optimizer.
I0609 12:19:09.000966 139656796374848 submission_runner.py:279] Initializing metrics bundle.
I0609 12:19:09.001182 139656796374848 submission_runner.py:297] Initializing checkpoint and logger.
I0609 12:19:09.004321 139656796374848 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 12:19:09.004466 139656796374848 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 12:19:09.491338 139656796374848 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nesterov/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0609 12:19:09.492401 139656796374848 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nesterov/criteo1tb_pytorch/trial_1/flags_0.json.
I0609 12:19:09.544688 139656796374848 submission_runner.py:332] Starting training loop.
I0609 12:19:15.401237 139619203012352 logging_writer.py:48] [0] global_step=0, grad_norm=6.396466, loss=0.946037
I0609 12:19:15.408665 139656796374848 submission.py:139] 0) loss = 0.946, grad_norm = 6.396
I0609 12:19:15.409701 139656796374848 spec.py:298] Evaluating on the training split.
I0609 12:24:09.929920 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 12:28:59.765965 139656796374848 spec.py:326] Evaluating on the test split.
I0609 12:33:45.897728 139656796374848 submission_runner.py:419] Time since start: 876.35s, 	Step: 1, 	{'train/loss': 0.9444523489238411, 'validation/loss': 0.9428141123595506, 'validation/num_examples': 89000000, 'test/loss': 0.9445281530520253, 'test/num_examples': 89274637, 'score': 5.865057468414307, 'total_duration': 876.353316783905, 'accumulated_submission_time': 5.865057468414307, 'accumulated_eval_time': 870.4878296852112, 'accumulated_logging_time': 0}
I0609 12:33:45.915650 139593441421056 logging_writer.py:48] [1] accumulated_eval_time=870.487830, accumulated_logging_time=0, accumulated_submission_time=5.865057, global_step=1, preemption_count=0, score=5.865057, test/loss=0.944528, test/num_examples=89274637, total_duration=876.353317, train/loss=0.944452, validation/loss=0.942814, validation/num_examples=89000000
I0609 12:33:45.940716 139656796374848 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940745 140047401502528 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940795 139703197075264 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940808 140142761670464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940827 140588468139840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940899 139948846614336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940872 140301309593408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:45.940910 139971802834752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:33:47.136350 139593433028352 logging_writer.py:48] [1] global_step=1, grad_norm=6.375136, loss=0.945792
I0609 12:33:47.140217 139656796374848 submission.py:139] 1) loss = 0.946, grad_norm = 6.375
I0609 12:33:48.310146 139593441421056 logging_writer.py:48] [2] global_step=2, grad_norm=5.775269, loss=0.856536
I0609 12:33:48.313453 139656796374848 submission.py:139] 2) loss = 0.857, grad_norm = 5.775
I0609 12:33:49.492755 139593433028352 logging_writer.py:48] [3] global_step=3, grad_norm=4.476564, loss=0.689832
I0609 12:33:49.496214 139656796374848 submission.py:139] 3) loss = 0.690, grad_norm = 4.477
I0609 12:33:50.688501 139593441421056 logging_writer.py:48] [4] global_step=4, grad_norm=3.688229, loss=0.515975
I0609 12:33:50.691808 139656796374848 submission.py:139] 4) loss = 0.516, grad_norm = 3.688
I0609 12:33:51.875535 139593433028352 logging_writer.py:48] [5] global_step=5, grad_norm=2.759452, loss=0.348830
I0609 12:33:51.878863 139656796374848 submission.py:139] 5) loss = 0.349, grad_norm = 2.759
I0609 12:33:53.052516 139593441421056 logging_writer.py:48] [6] global_step=6, grad_norm=1.313918, loss=0.227083
I0609 12:33:53.055757 139656796374848 submission.py:139] 6) loss = 0.227, grad_norm = 1.314
I0609 12:33:54.222438 139593433028352 logging_writer.py:48] [7] global_step=7, grad_norm=0.238973, loss=0.188998
I0609 12:33:54.226033 139656796374848 submission.py:139] 7) loss = 0.189, grad_norm = 0.239
I0609 12:33:55.396141 139593441421056 logging_writer.py:48] [8] global_step=8, grad_norm=0.945752, loss=0.214881
I0609 12:33:55.399765 139656796374848 submission.py:139] 8) loss = 0.215, grad_norm = 0.946
I0609 12:33:56.557788 139593433028352 logging_writer.py:48] [9] global_step=9, grad_norm=1.529871, loss=0.269170
I0609 12:33:56.561221 139656796374848 submission.py:139] 9) loss = 0.269, grad_norm = 1.530
I0609 12:33:57.736814 139593441421056 logging_writer.py:48] [10] global_step=10, grad_norm=1.864347, loss=0.311308
I0609 12:33:57.740565 139656796374848 submission.py:139] 10) loss = 0.311, grad_norm = 1.864
I0609 12:33:58.907028 139593433028352 logging_writer.py:48] [11] global_step=11, grad_norm=2.048338, loss=0.336524
I0609 12:33:58.910561 139656796374848 submission.py:139] 11) loss = 0.337, grad_norm = 2.048
I0609 12:34:00.111247 139593441421056 logging_writer.py:48] [12] global_step=12, grad_norm=1.953493, loss=0.319161
I0609 12:34:00.114607 139656796374848 submission.py:139] 12) loss = 0.319, grad_norm = 1.953
I0609 12:34:01.224964 139593433028352 logging_writer.py:48] [13] global_step=13, grad_norm=1.535163, loss=0.258043
I0609 12:34:01.228208 139656796374848 submission.py:139] 13) loss = 0.258, grad_norm = 1.535
I0609 12:34:02.449359 139593441421056 logging_writer.py:48] [14] global_step=14, grad_norm=1.056435, loss=0.206124
I0609 12:34:02.452616 139656796374848 submission.py:139] 14) loss = 0.206, grad_norm = 1.056
I0609 12:34:03.556605 139593433028352 logging_writer.py:48] [15] global_step=15, grad_norm=0.401916, loss=0.169209
I0609 12:34:03.560407 139656796374848 submission.py:139] 15) loss = 0.169, grad_norm = 0.402
I0609 12:34:04.713795 139593441421056 logging_writer.py:48] [16] global_step=16, grad_norm=0.427522, loss=0.166280
I0609 12:34:04.717117 139656796374848 submission.py:139] 16) loss = 0.166, grad_norm = 0.428
I0609 12:34:05.814292 139593433028352 logging_writer.py:48] [17] global_step=17, grad_norm=0.893628, loss=0.177238
I0609 12:34:05.817371 139656796374848 submission.py:139] 17) loss = 0.177, grad_norm = 0.894
I0609 12:34:06.921401 139593441421056 logging_writer.py:48] [18] global_step=18, grad_norm=0.894742, loss=0.175495
I0609 12:34:06.924914 139656796374848 submission.py:139] 18) loss = 0.175, grad_norm = 0.895
I0609 12:34:08.033357 139593433028352 logging_writer.py:48] [19] global_step=19, grad_norm=0.572788, loss=0.165013
I0609 12:34:08.036560 139656796374848 submission.py:139] 19) loss = 0.165, grad_norm = 0.573
I0609 12:34:09.142420 139593441421056 logging_writer.py:48] [20] global_step=20, grad_norm=0.187204, loss=0.152594
I0609 12:34:09.145786 139656796374848 submission.py:139] 20) loss = 0.153, grad_norm = 0.187
I0609 12:34:10.257649 139593433028352 logging_writer.py:48] [21] global_step=21, grad_norm=0.191096, loss=0.151892
I0609 12:34:10.261164 139656796374848 submission.py:139] 21) loss = 0.152, grad_norm = 0.191
I0609 12:34:11.368357 139593441421056 logging_writer.py:48] [22] global_step=22, grad_norm=0.358312, loss=0.150834
I0609 12:34:11.371718 139656796374848 submission.py:139] 22) loss = 0.151, grad_norm = 0.358
I0609 12:34:12.477970 139593433028352 logging_writer.py:48] [23] global_step=23, grad_norm=0.440327, loss=0.155887
I0609 12:34:12.481756 139656796374848 submission.py:139] 23) loss = 0.156, grad_norm = 0.440
I0609 12:34:13.589111 139593441421056 logging_writer.py:48] [24] global_step=24, grad_norm=0.377230, loss=0.155574
I0609 12:34:13.592618 139656796374848 submission.py:139] 24) loss = 0.156, grad_norm = 0.377
I0609 12:34:14.711588 139593433028352 logging_writer.py:48] [25] global_step=25, grad_norm=0.187365, loss=0.147986
I0609 12:34:14.714831 139656796374848 submission.py:139] 25) loss = 0.148, grad_norm = 0.187
I0609 12:34:15.822408 139593441421056 logging_writer.py:48] [26] global_step=26, grad_norm=0.058680, loss=0.147987
I0609 12:34:15.825819 139656796374848 submission.py:139] 26) loss = 0.148, grad_norm = 0.059
I0609 12:34:16.938553 139593433028352 logging_writer.py:48] [27] global_step=27, grad_norm=0.073330, loss=0.149378
I0609 12:34:16.942152 139656796374848 submission.py:139] 27) loss = 0.149, grad_norm = 0.073
I0609 12:34:18.052577 139593441421056 logging_writer.py:48] [28] global_step=28, grad_norm=0.130883, loss=0.146188
I0609 12:34:18.055872 139656796374848 submission.py:139] 28) loss = 0.146, grad_norm = 0.131
I0609 12:34:19.168164 139593433028352 logging_writer.py:48] [29] global_step=29, grad_norm=0.094401, loss=0.147146
I0609 12:34:19.171673 139656796374848 submission.py:139] 29) loss = 0.147, grad_norm = 0.094
I0609 12:34:20.277033 139593441421056 logging_writer.py:48] [30] global_step=30, grad_norm=0.073896, loss=0.143157
I0609 12:34:20.280409 139656796374848 submission.py:139] 30) loss = 0.143, grad_norm = 0.074
I0609 12:34:21.388059 139593433028352 logging_writer.py:48] [31] global_step=31, grad_norm=0.044017, loss=0.142595
I0609 12:34:21.391517 139656796374848 submission.py:139] 31) loss = 0.143, grad_norm = 0.044
I0609 12:34:22.494517 139593441421056 logging_writer.py:48] [32] global_step=32, grad_norm=0.072558, loss=0.145777
I0609 12:34:22.498042 139656796374848 submission.py:139] 32) loss = 0.146, grad_norm = 0.073
I0609 12:34:23.612201 139593433028352 logging_writer.py:48] [33] global_step=33, grad_norm=0.051473, loss=0.140493
I0609 12:34:23.615603 139656796374848 submission.py:139] 33) loss = 0.140, grad_norm = 0.051
I0609 12:34:24.722228 139593441421056 logging_writer.py:48] [34] global_step=34, grad_norm=0.088940, loss=0.145820
I0609 12:34:24.725532 139656796374848 submission.py:139] 34) loss = 0.146, grad_norm = 0.089
I0609 12:34:25.832933 139593433028352 logging_writer.py:48] [35] global_step=35, grad_norm=0.046811, loss=0.144134
I0609 12:34:25.836282 139656796374848 submission.py:139] 35) loss = 0.144, grad_norm = 0.047
I0609 12:34:26.939594 139593441421056 logging_writer.py:48] [36] global_step=36, grad_norm=0.029278, loss=0.144654
I0609 12:34:26.943068 139656796374848 submission.py:139] 36) loss = 0.145, grad_norm = 0.029
I0609 12:34:28.048364 139593433028352 logging_writer.py:48] [37] global_step=37, grad_norm=0.025968, loss=0.143943
I0609 12:34:28.051750 139656796374848 submission.py:139] 37) loss = 0.144, grad_norm = 0.026
I0609 12:34:29.157291 139593441421056 logging_writer.py:48] [38] global_step=38, grad_norm=0.039806, loss=0.146698
I0609 12:34:29.160575 139656796374848 submission.py:139] 38) loss = 0.147, grad_norm = 0.040
I0609 12:34:30.267923 139593433028352 logging_writer.py:48] [39] global_step=39, grad_norm=0.031930, loss=0.145906
I0609 12:34:30.271390 139656796374848 submission.py:139] 39) loss = 0.146, grad_norm = 0.032
I0609 12:34:31.386461 139593441421056 logging_writer.py:48] [40] global_step=40, grad_norm=0.027221, loss=0.146281
I0609 12:34:31.390215 139656796374848 submission.py:139] 40) loss = 0.146, grad_norm = 0.027
I0609 12:34:32.535880 139593433028352 logging_writer.py:48] [41] global_step=41, grad_norm=0.027668, loss=0.145607
I0609 12:34:32.539170 139656796374848 submission.py:139] 41) loss = 0.146, grad_norm = 0.028
I0609 12:34:33.647035 139593441421056 logging_writer.py:48] [42] global_step=42, grad_norm=0.028393, loss=0.145448
I0609 12:34:33.650411 139656796374848 submission.py:139] 42) loss = 0.145, grad_norm = 0.028
I0609 12:34:34.760796 139593433028352 logging_writer.py:48] [43] global_step=43, grad_norm=0.022518, loss=0.144355
I0609 12:34:34.764129 139656796374848 submission.py:139] 43) loss = 0.144, grad_norm = 0.023
I0609 12:34:35.879749 139593441421056 logging_writer.py:48] [44] global_step=44, grad_norm=0.022041, loss=0.144822
I0609 12:34:35.883230 139656796374848 submission.py:139] 44) loss = 0.145, grad_norm = 0.022
I0609 12:34:36.991638 139593433028352 logging_writer.py:48] [45] global_step=45, grad_norm=0.016572, loss=0.143040
I0609 12:34:36.995037 139656796374848 submission.py:139] 45) loss = 0.143, grad_norm = 0.017
I0609 12:34:38.100226 139593441421056 logging_writer.py:48] [46] global_step=46, grad_norm=0.020102, loss=0.144505
I0609 12:34:38.103687 139656796374848 submission.py:139] 46) loss = 0.145, grad_norm = 0.020
I0609 12:34:39.211825 139593433028352 logging_writer.py:48] [47] global_step=47, grad_norm=0.019304, loss=0.145467
I0609 12:34:39.215142 139656796374848 submission.py:139] 47) loss = 0.145, grad_norm = 0.019
I0609 12:34:40.323643 139593441421056 logging_writer.py:48] [48] global_step=48, grad_norm=0.021616, loss=0.142361
I0609 12:34:40.326915 139656796374848 submission.py:139] 48) loss = 0.142, grad_norm = 0.022
I0609 12:34:41.434975 139593433028352 logging_writer.py:48] [49] global_step=49, grad_norm=0.015603, loss=0.142426
I0609 12:34:41.438566 139656796374848 submission.py:139] 49) loss = 0.142, grad_norm = 0.016
I0609 12:34:42.542705 139593441421056 logging_writer.py:48] [50] global_step=50, grad_norm=0.014540, loss=0.142235
I0609 12:34:42.546202 139656796374848 submission.py:139] 50) loss = 0.142, grad_norm = 0.015
I0609 12:34:43.662420 139593433028352 logging_writer.py:48] [51] global_step=51, grad_norm=0.015700, loss=0.141281
I0609 12:34:43.666214 139656796374848 submission.py:139] 51) loss = 0.141, grad_norm = 0.016
I0609 12:34:44.769066 139593441421056 logging_writer.py:48] [52] global_step=52, grad_norm=0.016572, loss=0.141863
I0609 12:34:44.772403 139656796374848 submission.py:139] 52) loss = 0.142, grad_norm = 0.017
I0609 12:34:45.875157 139593433028352 logging_writer.py:48] [53] global_step=53, grad_norm=0.013931, loss=0.140769
I0609 12:34:45.878839 139656796374848 submission.py:139] 53) loss = 0.141, grad_norm = 0.014
I0609 12:34:46.990893 139593441421056 logging_writer.py:48] [54] global_step=54, grad_norm=0.013719, loss=0.139088
I0609 12:34:46.994404 139656796374848 submission.py:139] 54) loss = 0.139, grad_norm = 0.014
I0609 12:34:48.097221 139593433028352 logging_writer.py:48] [55] global_step=55, grad_norm=0.013721, loss=0.139154
I0609 12:34:48.100510 139656796374848 submission.py:139] 55) loss = 0.139, grad_norm = 0.014
I0609 12:34:49.206342 139593441421056 logging_writer.py:48] [56] global_step=56, grad_norm=0.020577, loss=0.141053
I0609 12:34:49.209877 139656796374848 submission.py:139] 56) loss = 0.141, grad_norm = 0.021
I0609 12:34:50.323401 139593433028352 logging_writer.py:48] [57] global_step=57, grad_norm=0.012964, loss=0.133574
I0609 12:34:50.326941 139656796374848 submission.py:139] 57) loss = 0.134, grad_norm = 0.013
I0609 12:34:51.440756 139593441421056 logging_writer.py:48] [58] global_step=58, grad_norm=0.017131, loss=0.130381
I0609 12:34:51.444154 139656796374848 submission.py:139] 58) loss = 0.130, grad_norm = 0.017
I0609 12:34:52.557856 139593433028352 logging_writer.py:48] [59] global_step=59, grad_norm=0.016544, loss=0.128547
I0609 12:34:52.561131 139656796374848 submission.py:139] 59) loss = 0.129, grad_norm = 0.017
I0609 12:34:53.671227 139593441421056 logging_writer.py:48] [60] global_step=60, grad_norm=0.011842, loss=0.129037
I0609 12:34:53.674766 139656796374848 submission.py:139] 60) loss = 0.129, grad_norm = 0.012
I0609 12:34:54.786002 139593433028352 logging_writer.py:48] [61] global_step=61, grad_norm=0.011176, loss=0.127180
I0609 12:34:54.789441 139656796374848 submission.py:139] 61) loss = 0.127, grad_norm = 0.011
I0609 12:34:55.910152 139593441421056 logging_writer.py:48] [62] global_step=62, grad_norm=0.030447, loss=0.130975
I0609 12:34:55.913807 139656796374848 submission.py:139] 62) loss = 0.131, grad_norm = 0.030
I0609 12:34:57.028863 139593433028352 logging_writer.py:48] [63] global_step=63, grad_norm=0.010804, loss=0.128975
I0609 12:34:57.032100 139656796374848 submission.py:139] 63) loss = 0.129, grad_norm = 0.011
I0609 12:34:58.138244 139593441421056 logging_writer.py:48] [64] global_step=64, grad_norm=0.016510, loss=0.128525
I0609 12:34:58.141623 139656796374848 submission.py:139] 64) loss = 0.129, grad_norm = 0.017
I0609 12:34:59.265038 139593433028352 logging_writer.py:48] [65] global_step=65, grad_norm=0.010847, loss=0.129381
I0609 12:34:59.268238 139656796374848 submission.py:139] 65) loss = 0.129, grad_norm = 0.011
I0609 12:35:00.378660 139593441421056 logging_writer.py:48] [66] global_step=66, grad_norm=0.010048, loss=0.130192
I0609 12:35:00.382078 139656796374848 submission.py:139] 66) loss = 0.130, grad_norm = 0.010
I0609 12:35:01.517496 139593433028352 logging_writer.py:48] [67] global_step=67, grad_norm=0.012044, loss=0.130877
I0609 12:35:01.521247 139656796374848 submission.py:139] 67) loss = 0.131, grad_norm = 0.012
I0609 12:35:02.636095 139593441421056 logging_writer.py:48] [68] global_step=68, grad_norm=0.013005, loss=0.129804
I0609 12:35:02.639356 139656796374848 submission.py:139] 68) loss = 0.130, grad_norm = 0.013
I0609 12:35:03.743800 139593433028352 logging_writer.py:48] [69] global_step=69, grad_norm=0.010725, loss=0.130740
I0609 12:35:03.747668 139656796374848 submission.py:139] 69) loss = 0.131, grad_norm = 0.011
I0609 12:35:04.857891 139593441421056 logging_writer.py:48] [70] global_step=70, grad_norm=0.011980, loss=0.128249
I0609 12:35:04.861074 139656796374848 submission.py:139] 70) loss = 0.128, grad_norm = 0.012
I0609 12:35:05.967635 139593433028352 logging_writer.py:48] [71] global_step=71, grad_norm=0.011518, loss=0.129698
I0609 12:35:05.971247 139656796374848 submission.py:139] 71) loss = 0.130, grad_norm = 0.012
I0609 12:35:07.072135 139593441421056 logging_writer.py:48] [72] global_step=72, grad_norm=0.011373, loss=0.129450
I0609 12:35:07.075396 139656796374848 submission.py:139] 72) loss = 0.129, grad_norm = 0.011
I0609 12:35:08.189867 139593433028352 logging_writer.py:48] [73] global_step=73, grad_norm=0.009567, loss=0.129758
I0609 12:35:08.193121 139656796374848 submission.py:139] 73) loss = 0.130, grad_norm = 0.010
I0609 12:35:09.303087 139593441421056 logging_writer.py:48] [74] global_step=74, grad_norm=0.008187, loss=0.130288
I0609 12:35:09.306479 139656796374848 submission.py:139] 74) loss = 0.130, grad_norm = 0.008
I0609 12:35:10.412998 139593433028352 logging_writer.py:48] [75] global_step=75, grad_norm=0.010170, loss=0.128916
I0609 12:35:10.416368 139656796374848 submission.py:139] 75) loss = 0.129, grad_norm = 0.010
I0609 12:35:11.519585 139593441421056 logging_writer.py:48] [76] global_step=76, grad_norm=0.009675, loss=0.130506
I0609 12:35:11.523065 139656796374848 submission.py:139] 76) loss = 0.131, grad_norm = 0.010
I0609 12:35:12.629060 139593433028352 logging_writer.py:48] [77] global_step=77, grad_norm=0.015225, loss=0.133437
I0609 12:35:12.632354 139656796374848 submission.py:139] 77) loss = 0.133, grad_norm = 0.015
I0609 12:35:13.741414 139593441421056 logging_writer.py:48] [78] global_step=78, grad_norm=0.009818, loss=0.132450
I0609 12:35:13.744838 139656796374848 submission.py:139] 78) loss = 0.132, grad_norm = 0.010
I0609 12:35:14.853132 139593433028352 logging_writer.py:48] [79] global_step=79, grad_norm=0.012544, loss=0.131890
I0609 12:35:14.856529 139656796374848 submission.py:139] 79) loss = 0.132, grad_norm = 0.013
I0609 12:35:15.961412 139593441421056 logging_writer.py:48] [80] global_step=80, grad_norm=0.017546, loss=0.135113
I0609 12:35:15.964849 139656796374848 submission.py:139] 80) loss = 0.135, grad_norm = 0.018
I0609 12:35:17.082732 139593433028352 logging_writer.py:48] [81] global_step=81, grad_norm=0.011282, loss=0.134136
I0609 12:35:17.086493 139656796374848 submission.py:139] 81) loss = 0.134, grad_norm = 0.011
I0609 12:35:18.207582 139593441421056 logging_writer.py:48] [82] global_step=82, grad_norm=0.009489, loss=0.135346
I0609 12:35:18.210877 139656796374848 submission.py:139] 82) loss = 0.135, grad_norm = 0.009
I0609 12:35:19.316013 139593433028352 logging_writer.py:48] [83] global_step=83, grad_norm=0.015718, loss=0.133942
I0609 12:35:19.319566 139656796374848 submission.py:139] 83) loss = 0.134, grad_norm = 0.016
I0609 12:35:20.427948 139593441421056 logging_writer.py:48] [84] global_step=84, grad_norm=0.007281, loss=0.134183
I0609 12:35:20.431183 139656796374848 submission.py:139] 84) loss = 0.134, grad_norm = 0.007
I0609 12:35:21.541855 139593433028352 logging_writer.py:48] [85] global_step=85, grad_norm=0.010494, loss=0.131747
I0609 12:35:21.545306 139656796374848 submission.py:139] 85) loss = 0.132, grad_norm = 0.010
I0609 12:35:22.657918 139593441421056 logging_writer.py:48] [86] global_step=86, grad_norm=0.006608, loss=0.131385
I0609 12:35:22.661215 139656796374848 submission.py:139] 86) loss = 0.131, grad_norm = 0.007
I0609 12:35:23.775723 139593433028352 logging_writer.py:48] [87] global_step=87, grad_norm=0.008476, loss=0.131616
I0609 12:35:23.779566 139656796374848 submission.py:139] 87) loss = 0.132, grad_norm = 0.008
I0609 12:35:24.922849 139593441421056 logging_writer.py:48] [88] global_step=88, grad_norm=0.009196, loss=0.133059
I0609 12:35:24.926484 139656796374848 submission.py:139] 88) loss = 0.133, grad_norm = 0.009
I0609 12:35:26.032971 139593433028352 logging_writer.py:48] [89] global_step=89, grad_norm=0.010183, loss=0.134046
I0609 12:35:26.036384 139656796374848 submission.py:139] 89) loss = 0.134, grad_norm = 0.010
I0609 12:35:27.146905 139593441421056 logging_writer.py:48] [90] global_step=90, grad_norm=0.007010, loss=0.135289
I0609 12:35:27.150077 139656796374848 submission.py:139] 90) loss = 0.135, grad_norm = 0.007
I0609 12:35:28.258789 139593433028352 logging_writer.py:48] [91] global_step=91, grad_norm=0.016822, loss=0.132920
I0609 12:35:28.262146 139656796374848 submission.py:139] 91) loss = 0.133, grad_norm = 0.017
I0609 12:35:29.363709 139593441421056 logging_writer.py:48] [92] global_step=92, grad_norm=0.008269, loss=0.132505
I0609 12:35:29.367196 139656796374848 submission.py:139] 92) loss = 0.133, grad_norm = 0.008
I0609 12:35:30.470715 139593433028352 logging_writer.py:48] [93] global_step=93, grad_norm=0.011322, loss=0.133745
I0609 12:35:30.474117 139656796374848 submission.py:139] 93) loss = 0.134, grad_norm = 0.011
I0609 12:35:31.582174 139593441421056 logging_writer.py:48] [94] global_step=94, grad_norm=0.011060, loss=0.134358
I0609 12:35:31.585594 139656796374848 submission.py:139] 94) loss = 0.134, grad_norm = 0.011
I0609 12:35:32.688402 139593433028352 logging_writer.py:48] [95] global_step=95, grad_norm=0.009069, loss=0.139469
I0609 12:35:32.691607 139656796374848 submission.py:139] 95) loss = 0.139, grad_norm = 0.009
I0609 12:35:33.795332 139593441421056 logging_writer.py:48] [96] global_step=96, grad_norm=0.009788, loss=0.141236
I0609 12:35:33.799109 139656796374848 submission.py:139] 96) loss = 0.141, grad_norm = 0.010
I0609 12:35:34.898972 139593433028352 logging_writer.py:48] [97] global_step=97, grad_norm=0.023006, loss=0.138464
I0609 12:35:34.902333 139656796374848 submission.py:139] 97) loss = 0.138, grad_norm = 0.023
I0609 12:35:36.010928 139593441421056 logging_writer.py:48] [98] global_step=98, grad_norm=0.012357, loss=0.139439
I0609 12:35:36.014306 139656796374848 submission.py:139] 98) loss = 0.139, grad_norm = 0.012
I0609 12:35:37.116275 139593433028352 logging_writer.py:48] [99] global_step=99, grad_norm=0.019208, loss=0.141631
I0609 12:35:37.119806 139656796374848 submission.py:139] 99) loss = 0.142, grad_norm = 0.019
I0609 12:35:38.228033 139593441421056 logging_writer.py:48] [100] global_step=100, grad_norm=0.020694, loss=0.138327
I0609 12:35:38.231342 139656796374848 submission.py:139] 100) loss = 0.138, grad_norm = 0.021
I0609 12:35:45.933388 139656796374848 spec.py:298] Evaluating on the training split.
I0609 12:40:38.147811 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 12:45:00.961679 139656796374848 spec.py:326] Evaluating on the test split.
I0609 12:49:26.943848 139656796374848 submission_runner.py:419] Time since start: 1817.40s, 	Step: 108, 	{'train/loss': 0.1372356288480443, 'validation/loss': 0.13782044943820224, 'validation/num_examples': 89000000, 'test/loss': 0.14144200888769787, 'test/num_examples': 89274637, 'score': 125.83706045150757, 'total_duration': 1817.3995718955994, 'accumulated_submission_time': 125.83706045150757, 'accumulated_eval_time': 1691.4981713294983, 'accumulated_logging_time': 0.025094985961914062}
I0609 12:49:26.956421 139593433028352 logging_writer.py:48] [108] accumulated_eval_time=1691.498171, accumulated_logging_time=0.025095, accumulated_submission_time=125.837060, global_step=108, preemption_count=0, score=125.837060, test/loss=0.141442, test/num_examples=89274637, total_duration=1817.399572, train/loss=0.137236, validation/loss=0.137820, validation/num_examples=89000000
I0609 12:51:27.616665 139656796374848 spec.py:298] Evaluating on the training split.
I0609 12:56:13.325802 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 13:00:36.146806 139656796374848 spec.py:326] Evaluating on the test split.
I0609 13:04:49.390034 139656796374848 submission_runner.py:419] Time since start: 2739.85s, 	Step: 215, 	{'train/loss': 0.1347713400284274, 'validation/loss': 0.1369538202247191, 'validation/num_examples': 89000000, 'test/loss': 0.1408018494659351, 'test/num_examples': 89274637, 'score': 246.44711184501648, 'total_duration': 2739.845708847046, 'accumulated_submission_time': 246.44711184501648, 'accumulated_eval_time': 2493.271426677704, 'accumulated_logging_time': 0.04493880271911621}
I0609 13:04:49.401158 139593441421056 logging_writer.py:48] [215] accumulated_eval_time=2493.271427, accumulated_logging_time=0.044939, accumulated_submission_time=246.447112, global_step=215, preemption_count=0, score=246.447112, test/loss=0.140802, test/num_examples=89274637, total_duration=2739.845709, train/loss=0.134771, validation/loss=0.136954, validation/num_examples=89000000
I0609 13:06:49.664283 139656796374848 spec.py:298] Evaluating on the training split.
I0609 13:11:19.693119 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 13:15:47.280682 139656796374848 spec.py:326] Evaluating on the test split.
I0609 13:20:28.075188 139656796374848 submission_runner.py:419] Time since start: 3678.53s, 	Step: 319, 	{'train/loss': 0.1370171440972222, 'validation/loss': 0.13672304494382023, 'validation/num_examples': 89000000, 'test/loss': 0.1405237973692349, 'test/num_examples': 89274637, 'score': 366.6559627056122, 'total_duration': 3678.5308294296265, 'accumulated_submission_time': 366.6559627056122, 'accumulated_eval_time': 3311.6822168827057, 'accumulated_logging_time': 0.06325840950012207}
I0609 13:20:28.086570 139593433028352 logging_writer.py:48] [319] accumulated_eval_time=3311.682217, accumulated_logging_time=0.063258, accumulated_submission_time=366.655963, global_step=319, preemption_count=0, score=366.655963, test/loss=0.140524, test/num_examples=89274637, total_duration=3678.530829, train/loss=0.137017, validation/loss=0.136723, validation/num_examples=89000000
I0609 13:22:28.331036 139656796374848 spec.py:298] Evaluating on the training split.
I0609 13:27:21.124542 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 13:31:49.886280 139656796374848 spec.py:326] Evaluating on the test split.
I0609 13:36:18.435650 139656796374848 submission_runner.py:419] Time since start: 4628.89s, 	Step: 426, 	{'train/loss': 0.13413097614571837, 'validation/loss': 0.13648077528089889, 'validation/num_examples': 89000000, 'test/loss': 0.1402082318184055, 'test/num_examples': 89274637, 'score': 486.8451509475708, 'total_duration': 4628.891350984573, 'accumulated_submission_time': 486.8451509475708, 'accumulated_eval_time': 4141.786771297455, 'accumulated_logging_time': 0.08203887939453125}
I0609 13:36:18.445773 139593441421056 logging_writer.py:48] [426] accumulated_eval_time=4141.786771, accumulated_logging_time=0.082039, accumulated_submission_time=486.845151, global_step=426, preemption_count=0, score=486.845151, test/loss=0.140208, test/num_examples=89274637, total_duration=4628.891351, train/loss=0.134131, validation/loss=0.136481, validation/num_examples=89000000
I0609 13:37:41.901046 139593433028352 logging_writer.py:48] [500] global_step=500, grad_norm=0.046574, loss=0.142576
I0609 13:37:41.904423 139656796374848 submission.py:139] 500) loss = 0.143, grad_norm = 0.047
I0609 13:38:18.556158 139656796374848 spec.py:298] Evaluating on the training split.
I0609 13:43:08.241164 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 13:48:01.170782 139656796374848 spec.py:326] Evaluating on the test split.
I0609 13:52:30.310189 139656796374848 submission_runner.py:419] Time since start: 5600.77s, 	Step: 534, 	{'train/loss': 0.13682136226875805, 'validation/loss': 0.13627429213483147, 'validation/num_examples': 89000000, 'test/loss': 0.13988296586408971, 'test/num_examples': 89274637, 'score': 606.9008159637451, 'total_duration': 5600.765918493271, 'accumulated_submission_time': 606.9008159637451, 'accumulated_eval_time': 4993.540714025497, 'accumulated_logging_time': 0.0990135669708252}
I0609 13:52:30.320796 139593441421056 logging_writer.py:48] [534] accumulated_eval_time=4993.540714, accumulated_logging_time=0.099014, accumulated_submission_time=606.900816, global_step=534, preemption_count=0, score=606.900816, test/loss=0.139883, test/num_examples=89274637, total_duration=5600.765918, train/loss=0.136821, validation/loss=0.136274, validation/num_examples=89000000
I0609 13:54:30.472713 139656796374848 spec.py:298] Evaluating on the training split.
I0609 13:59:12.815865 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 14:03:35.440883 139656796374848 spec.py:326] Evaluating on the test split.
I0609 14:08:00.385414 139656796374848 submission_runner.py:419] Time since start: 6530.84s, 	Step: 628, 	{'train/loss': 0.13644671913803808, 'validation/loss': 0.13695044943820225, 'validation/num_examples': 89000000, 'test/loss': 0.14062231359170915, 'test/num_examples': 89274637, 'score': 727.0038509368896, 'total_duration': 6530.8410987854, 'accumulated_submission_time': 727.0038509368896, 'accumulated_eval_time': 5803.4533104896545, 'accumulated_logging_time': 0.11631178855895996}
I0609 14:08:00.396747 139593433028352 logging_writer.py:48] [628] accumulated_eval_time=5803.453310, accumulated_logging_time=0.116312, accumulated_submission_time=727.003851, global_step=628, preemption_count=0, score=727.003851, test/loss=0.140622, test/num_examples=89274637, total_duration=6530.841099, train/loss=0.136447, validation/loss=0.136950, validation/num_examples=89000000
I0609 14:10:01.560550 139656796374848 spec.py:298] Evaluating on the training split.
I0609 14:14:59.540310 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 14:19:22.133865 139656796374848 spec.py:326] Evaluating on the test split.
I0609 14:23:46.022722 139656796374848 submission_runner.py:419] Time since start: 7476.48s, 	Step: 709, 	{'train/loss': 0.13843897355664206, 'validation/loss': 0.13619726966292134, 'validation/num_examples': 89000000, 'test/loss': 0.13930962273193, 'test/num_examples': 89274637, 'score': 848.1260616779327, 'total_duration': 7476.478439331055, 'accumulated_submission_time': 848.1260616779327, 'accumulated_eval_time': 6627.91548871994, 'accumulated_logging_time': 0.13513636589050293}
I0609 14:23:46.033454 139593441421056 logging_writer.py:48] [709] accumulated_eval_time=6627.915489, accumulated_logging_time=0.135136, accumulated_submission_time=848.126062, global_step=709, preemption_count=0, score=848.126062, test/loss=0.139310, test/num_examples=89274637, total_duration=7476.478439, train/loss=0.138439, validation/loss=0.136197, validation/num_examples=89000000
I0609 14:25:47.093295 139656796374848 spec.py:298] Evaluating on the training split.
I0609 14:30:19.641610 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 14:34:44.103843 139656796374848 spec.py:326] Evaluating on the test split.
I0609 14:39:07.803621 139656796374848 submission_runner.py:419] Time since start: 8398.26s, 	Step: 810, 	{'train/loss': 0.1332302023331149, 'validation/loss': 0.13425558426966291, 'validation/num_examples': 89000000, 'test/loss': 0.13763140812322766, 'test/num_examples': 89274637, 'score': 969.136387348175, 'total_duration': 8398.259331226349, 'accumulated_submission_time': 969.136387348175, 'accumulated_eval_time': 7428.625693082809, 'accumulated_logging_time': 0.15247869491577148}
I0609 14:39:07.814306 139593433028352 logging_writer.py:48] [810] accumulated_eval_time=7428.625693, accumulated_logging_time=0.152479, accumulated_submission_time=969.136387, global_step=810, preemption_count=0, score=969.136387, test/loss=0.137631, test/num_examples=89274637, total_duration=8398.259331, train/loss=0.133230, validation/loss=0.134256, validation/num_examples=89000000
I0609 14:41:08.870541 139656796374848 spec.py:298] Evaluating on the training split.
I0609 14:45:59.812668 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 14:50:25.486122 139656796374848 spec.py:326] Evaluating on the test split.
I0609 14:54:54.207106 139656796374848 submission_runner.py:419] Time since start: 9344.66s, 	Step: 915, 	{'train/loss': 0.13341490217128862, 'validation/loss': 0.13318931460674158, 'validation/num_examples': 89000000, 'test/loss': 0.1362244687704527, 'test/num_examples': 89274637, 'score': 1090.1467115879059, 'total_duration': 9344.662842035294, 'accumulated_submission_time': 1090.1467115879059, 'accumulated_eval_time': 8253.962165594101, 'accumulated_logging_time': 0.1697540283203125}
I0609 14:54:54.217023 139593441421056 logging_writer.py:48] [915] accumulated_eval_time=8253.962166, accumulated_logging_time=0.169754, accumulated_submission_time=1090.146712, global_step=915, preemption_count=0, score=1090.146712, test/loss=0.136224, test/num_examples=89274637, total_duration=9344.662842, train/loss=0.133415, validation/loss=0.133189, validation/num_examples=89000000
I0609 14:56:32.613197 139593433028352 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.007643, loss=0.124558
I0609 14:56:32.616799 139656796374848 submission.py:139] 1000) loss = 0.125, grad_norm = 0.008
I0609 14:56:54.214290 139656796374848 spec.py:298] Evaluating on the training split.
I0609 15:01:30.864473 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 15:05:55.333059 139656796374848 spec.py:326] Evaluating on the test split.
I0609 15:10:18.843805 139656796374848 submission_runner.py:419] Time since start: 10269.30s, 	Step: 1020, 	{'train/loss': 0.1313670581539735, 'validation/loss': 0.1328051797752809, 'validation/num_examples': 89000000, 'test/loss': 0.13547257548636127, 'test/num_examples': 89274637, 'score': 1210.0998532772064, 'total_duration': 10269.299475431442, 'accumulated_submission_time': 1210.0998532772064, 'accumulated_eval_time': 9058.591496944427, 'accumulated_logging_time': 0.1864635944366455}
I0609 15:10:18.853568 139593441421056 logging_writer.py:48] [1020] accumulated_eval_time=9058.591497, accumulated_logging_time=0.186464, accumulated_submission_time=1210.099853, global_step=1020, preemption_count=0, score=1210.099853, test/loss=0.135473, test/num_examples=89274637, total_duration=10269.299475, train/loss=0.131367, validation/loss=0.132805, validation/num_examples=89000000
I0609 15:12:19.899473 139656796374848 spec.py:298] Evaluating on the training split.
I0609 15:17:16.995600 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 15:21:44.935898 139656796374848 spec.py:326] Evaluating on the test split.
I0609 15:26:09.539736 139656796374848 submission_runner.py:419] Time since start: 11220.00s, 	Step: 1129, 	{'train/loss': 0.13158857445229144, 'validation/loss': 0.1320264831460674, 'validation/num_examples': 89000000, 'test/loss': 0.13477616268548928, 'test/num_examples': 89274637, 'score': 1331.100422859192, 'total_duration': 11219.995468854904, 'accumulated_submission_time': 1331.100422859192, 'accumulated_eval_time': 9888.231664419174, 'accumulated_logging_time': 0.20269250869750977}
I0609 15:26:09.549700 139593433028352 logging_writer.py:48] [1129] accumulated_eval_time=9888.231664, accumulated_logging_time=0.202693, accumulated_submission_time=1331.100423, global_step=1129, preemption_count=0, score=1331.100423, test/loss=0.134776, test/num_examples=89274637, total_duration=11219.995469, train/loss=0.131589, validation/loss=0.132026, validation/num_examples=89000000
I0609 15:28:10.987149 139656796374848 spec.py:298] Evaluating on the training split.
I0609 15:32:52.578819 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 15:37:18.033309 139656796374848 spec.py:326] Evaluating on the test split.
I0609 15:42:17.232471 139656796374848 submission_runner.py:419] Time since start: 12187.69s, 	Step: 1210, 	{'train/loss': 0.1305000243422036, 'validation/loss': 0.1318183820224719, 'validation/num_examples': 89000000, 'test/loss': 0.1344937980537518, 'test/num_examples': 89274637, 'score': 1452.4960985183716, 'total_duration': 12187.688198566437, 'accumulated_submission_time': 1452.4960985183716, 'accumulated_eval_time': 10734.476928472519, 'accumulated_logging_time': 0.2193443775177002}
I0609 15:42:17.242518 139593441421056 logging_writer.py:48] [1210] accumulated_eval_time=10734.476928, accumulated_logging_time=0.219344, accumulated_submission_time=1452.496099, global_step=1210, preemption_count=0, score=1452.496099, test/loss=0.134494, test/num_examples=89274637, total_duration=12187.688199, train/loss=0.130500, validation/loss=0.131818, validation/num_examples=89000000
I0609 15:44:18.666330 139656796374848 spec.py:298] Evaluating on the training split.
I0609 15:49:10.199892 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 15:53:34.147795 139656796374848 spec.py:326] Evaluating on the test split.
I0609 15:57:59.098818 139656796374848 submission_runner.py:419] Time since start: 13129.55s, 	Step: 1291, 	{'train/loss': 0.130221784421852, 'validation/loss': 0.13308323595505617, 'validation/num_examples': 89000000, 'test/loss': 0.13559795264135321, 'test/num_examples': 89274637, 'score': 1573.877433538437, 'total_duration': 13129.554527759552, 'accumulated_submission_time': 1573.877433538437, 'accumulated_eval_time': 11554.909344911575, 'accumulated_logging_time': 0.23651695251464844}
I0609 15:57:59.109098 139593433028352 logging_writer.py:48] [1291] accumulated_eval_time=11554.909345, accumulated_logging_time=0.236517, accumulated_submission_time=1573.877434, global_step=1291, preemption_count=0, score=1573.877434, test/loss=0.135598, test/num_examples=89274637, total_duration=13129.554528, train/loss=0.130222, validation/loss=0.133083, validation/num_examples=89000000
I0609 16:00:00.313303 139656796374848 spec.py:298] Evaluating on the training split.
I0609 16:04:56.973136 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 16:09:18.876987 139656796374848 spec.py:326] Evaluating on the test split.
I0609 16:13:44.044807 139656796374848 submission_runner.py:419] Time since start: 14074.50s, 	Step: 1385, 	{'train/loss': 0.12940647779272443, 'validation/loss': 0.1306829213483146, 'validation/num_examples': 89000000, 'test/loss': 0.13312999525273903, 'test/num_examples': 89274637, 'score': 1695.040522813797, 'total_duration': 14074.500462055206, 'accumulated_submission_time': 1695.040522813797, 'accumulated_eval_time': 12378.64066362381, 'accumulated_logging_time': 0.25345730781555176}
I0609 16:13:44.055767 139593441421056 logging_writer.py:48] [1385] accumulated_eval_time=12378.640664, accumulated_logging_time=0.253457, accumulated_submission_time=1695.040523, global_step=1385, preemption_count=0, score=1695.040523, test/loss=0.133130, test/num_examples=89274637, total_duration=14074.500462, train/loss=0.129406, validation/loss=0.130683, validation/num_examples=89000000
I0609 16:15:44.814539 139656796374848 spec.py:298] Evaluating on the training split.
I0609 16:20:20.523481 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 16:24:44.568384 139656796374848 spec.py:326] Evaluating on the test split.
I0609 16:29:08.009102 139656796374848 submission_runner.py:419] Time since start: 14998.46s, 	Step: 1493, 	{'train/loss': 0.12977711254397764, 'validation/loss': 0.13163278651685392, 'validation/num_examples': 89000000, 'test/loss': 0.13458967074825517, 'test/num_examples': 89274637, 'score': 1815.7507348060608, 'total_duration': 14998.464824199677, 'accumulated_submission_time': 1815.7507348060608, 'accumulated_eval_time': 13181.83512210846, 'accumulated_logging_time': 0.271512508392334}
I0609 16:29:08.019401 139593433028352 logging_writer.py:48] [1493] accumulated_eval_time=13181.835122, accumulated_logging_time=0.271513, accumulated_submission_time=1815.750735, global_step=1493, preemption_count=0, score=1815.750735, test/loss=0.134590, test/num_examples=89274637, total_duration=14998.464824, train/loss=0.129777, validation/loss=0.131633, validation/num_examples=89000000
I0609 16:29:17.294811 139593441421056 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.104035, loss=0.132184
I0609 16:29:17.298099 139656796374848 submission.py:139] 1500) loss = 0.132, grad_norm = 0.104
I0609 16:31:09.122108 139656796374848 spec.py:298] Evaluating on the training split.
I0609 16:35:43.077892 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 16:40:06.475394 139656796374848 spec.py:326] Evaluating on the test split.
I0609 16:44:31.847198 139656796374848 submission_runner.py:419] Time since start: 15922.30s, 	Step: 1598, 	{'train/loss': 0.13151980323595014, 'validation/loss': 0.1298583820224719, 'validation/num_examples': 89000000, 'test/loss': 0.13280916504874726, 'test/num_examples': 89274637, 'score': 1936.8022985458374, 'total_duration': 15922.302869796753, 'accumulated_submission_time': 1936.8022985458374, 'accumulated_eval_time': 13984.560068368912, 'accumulated_logging_time': 0.28871607780456543}
I0609 16:44:31.856910 139593433028352 logging_writer.py:48] [1598] accumulated_eval_time=13984.560068, accumulated_logging_time=0.288716, accumulated_submission_time=1936.802299, global_step=1598, preemption_count=0, score=1936.802299, test/loss=0.132809, test/num_examples=89274637, total_duration=15922.302870, train/loss=0.131520, validation/loss=0.129858, validation/num_examples=89000000
I0609 16:44:34.843018 139656796374848 spec.py:298] Evaluating on the training split.
I0609 16:49:35.443650 139656796374848 spec.py:310] Evaluating on the validation split.
I0609 16:53:59.225386 139656796374848 spec.py:326] Evaluating on the test split.
I0609 16:58:27.307320 139656796374848 submission_runner.py:419] Time since start: 16757.76s, 	Step: 1600, 	{'train/loss': 0.12766917651852236, 'validation/loss': 0.129674797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1327401196825925, 'test/num_examples': 89274637, 'score': 1939.7800393104553, 'total_duration': 16757.763016462326, 'accumulated_submission_time': 1939.7800393104553, 'accumulated_eval_time': 14817.024225234985, 'accumulated_logging_time': 0.3050243854522705}
I0609 16:58:27.318204 139593441421056 logging_writer.py:48] [1600] accumulated_eval_time=14817.024225, accumulated_logging_time=0.305024, accumulated_submission_time=1939.780039, global_step=1600, preemption_count=0, score=1939.780039, test/loss=0.132740, test/num_examples=89274637, total_duration=16757.763016, train/loss=0.127669, validation/loss=0.129675, validation/num_examples=89000000
I0609 16:58:27.333155 139593433028352 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1939.780039
I0609 16:58:36.007742 139656796374848 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nesterov/criteo1tb_pytorch/trial_1/checkpoint_1600.
I0609 16:58:36.076507 139656796374848 submission_runner.py:581] Tuning trial 1/1
I0609 16:58:36.076744 139656796374848 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 16:58:36.078076 139656796374848 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/loss': 0.9444523489238411, 'validation/loss': 0.9428141123595506, 'validation/num_examples': 89000000, 'test/loss': 0.9445281530520253, 'test/num_examples': 89274637, 'score': 5.865057468414307, 'total_duration': 876.353316783905, 'accumulated_submission_time': 5.865057468414307, 'accumulated_eval_time': 870.4878296852112, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (108, {'train/loss': 0.1372356288480443, 'validation/loss': 0.13782044943820224, 'validation/num_examples': 89000000, 'test/loss': 0.14144200888769787, 'test/num_examples': 89274637, 'score': 125.83706045150757, 'total_duration': 1817.3995718955994, 'accumulated_submission_time': 125.83706045150757, 'accumulated_eval_time': 1691.4981713294983, 'accumulated_logging_time': 0.025094985961914062, 'global_step': 108, 'preemption_count': 0}), (215, {'train/loss': 0.1347713400284274, 'validation/loss': 0.1369538202247191, 'validation/num_examples': 89000000, 'test/loss': 0.1408018494659351, 'test/num_examples': 89274637, 'score': 246.44711184501648, 'total_duration': 2739.845708847046, 'accumulated_submission_time': 246.44711184501648, 'accumulated_eval_time': 2493.271426677704, 'accumulated_logging_time': 0.04493880271911621, 'global_step': 215, 'preemption_count': 0}), (319, {'train/loss': 0.1370171440972222, 'validation/loss': 0.13672304494382023, 'validation/num_examples': 89000000, 'test/loss': 0.1405237973692349, 'test/num_examples': 89274637, 'score': 366.6559627056122, 'total_duration': 3678.5308294296265, 'accumulated_submission_time': 366.6559627056122, 'accumulated_eval_time': 3311.6822168827057, 'accumulated_logging_time': 0.06325840950012207, 'global_step': 319, 'preemption_count': 0}), (426, {'train/loss': 0.13413097614571837, 'validation/loss': 0.13648077528089889, 'validation/num_examples': 89000000, 'test/loss': 0.1402082318184055, 'test/num_examples': 89274637, 'score': 486.8451509475708, 'total_duration': 4628.891350984573, 'accumulated_submission_time': 486.8451509475708, 'accumulated_eval_time': 4141.786771297455, 'accumulated_logging_time': 0.08203887939453125, 'global_step': 426, 'preemption_count': 0}), (534, {'train/loss': 0.13682136226875805, 'validation/loss': 0.13627429213483147, 'validation/num_examples': 89000000, 'test/loss': 0.13988296586408971, 'test/num_examples': 89274637, 'score': 606.9008159637451, 'total_duration': 5600.765918493271, 'accumulated_submission_time': 606.9008159637451, 'accumulated_eval_time': 4993.540714025497, 'accumulated_logging_time': 0.0990135669708252, 'global_step': 534, 'preemption_count': 0}), (628, {'train/loss': 0.13644671913803808, 'validation/loss': 0.13695044943820225, 'validation/num_examples': 89000000, 'test/loss': 0.14062231359170915, 'test/num_examples': 89274637, 'score': 727.0038509368896, 'total_duration': 6530.8410987854, 'accumulated_submission_time': 727.0038509368896, 'accumulated_eval_time': 5803.4533104896545, 'accumulated_logging_time': 0.11631178855895996, 'global_step': 628, 'preemption_count': 0}), (709, {'train/loss': 0.13843897355664206, 'validation/loss': 0.13619726966292134, 'validation/num_examples': 89000000, 'test/loss': 0.13930962273193, 'test/num_examples': 89274637, 'score': 848.1260616779327, 'total_duration': 7476.478439331055, 'accumulated_submission_time': 848.1260616779327, 'accumulated_eval_time': 6627.91548871994, 'accumulated_logging_time': 0.13513636589050293, 'global_step': 709, 'preemption_count': 0}), (810, {'train/loss': 0.1332302023331149, 'validation/loss': 0.13425558426966291, 'validation/num_examples': 89000000, 'test/loss': 0.13763140812322766, 'test/num_examples': 89274637, 'score': 969.136387348175, 'total_duration': 8398.259331226349, 'accumulated_submission_time': 969.136387348175, 'accumulated_eval_time': 7428.625693082809, 'accumulated_logging_time': 0.15247869491577148, 'global_step': 810, 'preemption_count': 0}), (915, {'train/loss': 0.13341490217128862, 'validation/loss': 0.13318931460674158, 'validation/num_examples': 89000000, 'test/loss': 0.1362244687704527, 'test/num_examples': 89274637, 'score': 1090.1467115879059, 'total_duration': 9344.662842035294, 'accumulated_submission_time': 1090.1467115879059, 'accumulated_eval_time': 8253.962165594101, 'accumulated_logging_time': 0.1697540283203125, 'global_step': 915, 'preemption_count': 0}), (1020, {'train/loss': 0.1313670581539735, 'validation/loss': 0.1328051797752809, 'validation/num_examples': 89000000, 'test/loss': 0.13547257548636127, 'test/num_examples': 89274637, 'score': 1210.0998532772064, 'total_duration': 10269.299475431442, 'accumulated_submission_time': 1210.0998532772064, 'accumulated_eval_time': 9058.591496944427, 'accumulated_logging_time': 0.1864635944366455, 'global_step': 1020, 'preemption_count': 0}), (1129, {'train/loss': 0.13158857445229144, 'validation/loss': 0.1320264831460674, 'validation/num_examples': 89000000, 'test/loss': 0.13477616268548928, 'test/num_examples': 89274637, 'score': 1331.100422859192, 'total_duration': 11219.995468854904, 'accumulated_submission_time': 1331.100422859192, 'accumulated_eval_time': 9888.231664419174, 'accumulated_logging_time': 0.20269250869750977, 'global_step': 1129, 'preemption_count': 0}), (1210, {'train/loss': 0.1305000243422036, 'validation/loss': 0.1318183820224719, 'validation/num_examples': 89000000, 'test/loss': 0.1344937980537518, 'test/num_examples': 89274637, 'score': 1452.4960985183716, 'total_duration': 12187.688198566437, 'accumulated_submission_time': 1452.4960985183716, 'accumulated_eval_time': 10734.476928472519, 'accumulated_logging_time': 0.2193443775177002, 'global_step': 1210, 'preemption_count': 0}), (1291, {'train/loss': 0.130221784421852, 'validation/loss': 0.13308323595505617, 'validation/num_examples': 89000000, 'test/loss': 0.13559795264135321, 'test/num_examples': 89274637, 'score': 1573.877433538437, 'total_duration': 13129.554527759552, 'accumulated_submission_time': 1573.877433538437, 'accumulated_eval_time': 11554.909344911575, 'accumulated_logging_time': 0.23651695251464844, 'global_step': 1291, 'preemption_count': 0}), (1385, {'train/loss': 0.12940647779272443, 'validation/loss': 0.1306829213483146, 'validation/num_examples': 89000000, 'test/loss': 0.13312999525273903, 'test/num_examples': 89274637, 'score': 1695.040522813797, 'total_duration': 14074.500462055206, 'accumulated_submission_time': 1695.040522813797, 'accumulated_eval_time': 12378.64066362381, 'accumulated_logging_time': 0.25345730781555176, 'global_step': 1385, 'preemption_count': 0}), (1493, {'train/loss': 0.12977711254397764, 'validation/loss': 0.13163278651685392, 'validation/num_examples': 89000000, 'test/loss': 0.13458967074825517, 'test/num_examples': 89274637, 'score': 1815.7507348060608, 'total_duration': 14998.464824199677, 'accumulated_submission_time': 1815.7507348060608, 'accumulated_eval_time': 13181.83512210846, 'accumulated_logging_time': 0.271512508392334, 'global_step': 1493, 'preemption_count': 0}), (1598, {'train/loss': 0.13151980323595014, 'validation/loss': 0.1298583820224719, 'validation/num_examples': 89000000, 'test/loss': 0.13280916504874726, 'test/num_examples': 89274637, 'score': 1936.8022985458374, 'total_duration': 15922.302869796753, 'accumulated_submission_time': 1936.8022985458374, 'accumulated_eval_time': 13984.560068368912, 'accumulated_logging_time': 0.28871607780456543, 'global_step': 1598, 'preemption_count': 0}), (1600, {'train/loss': 0.12766917651852236, 'validation/loss': 0.129674797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1327401196825925, 'test/num_examples': 89274637, 'score': 1939.7800393104553, 'total_duration': 16757.763016462326, 'accumulated_submission_time': 1939.7800393104553, 'accumulated_eval_time': 14817.024225234985, 'accumulated_logging_time': 0.3050243854522705, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0609 16:58:36.078259 139656796374848 submission_runner.py:584] Timing: 1939.7800393104553
I0609 16:58:36.078312 139656796374848 submission_runner.py:586] Total number of evals: 18
I0609 16:58:36.078361 139656796374848 submission_runner.py:587] ====================
I0609 16:58:36.078466 139656796374848 submission_runner.py:655] Final criteo1tb score: 1939.7800393104553
