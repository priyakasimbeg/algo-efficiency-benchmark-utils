torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-09-2023-02-26-31.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 02:26:54.247966 140030387128128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 02:26:54.248000 140547624421184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 02:26:54.248020 140087519385408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 02:26:54.248046 140005492746048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 02:26:54.248591 139925233645376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 02:26:54.248950 139925233645376 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.248875 139811366827840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 02:26:54.248882 140321551468352 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 02:26:54.248921 140273019930432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 02:26:54.249180 139811366827840 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.249214 140321551468352 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.249241 140273019930432 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.258719 140030387128128 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.258741 140547624421184 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.258774 140087519385408 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:54.258793 140005492746048 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:26:55.369428 140321551468352 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/momentum/ogbg_pytorch.
W0609 02:26:55.491697 140321551468352 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.492526 140273019930432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.492568 139925233645376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.492723 139811366827840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.492841 140005492746048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.493057 140030387128128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.493842 140547624421184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:26:55.494163 140087519385408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 02:26:55.498113 140321551468352 submission_runner.py:541] Using RNG seed 4294646812
I0609 02:26:55.499390 140321551468352 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 02:26:55.499500 140321551468352 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/momentum/ogbg_pytorch/trial_1.
I0609 02:26:55.499711 140321551468352 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/momentum/ogbg_pytorch/trial_1/hparams.json.
I0609 02:26:55.500614 140321551468352 submission_runner.py:255] Initializing dataset.
I0609 02:26:55.500724 140321551468352 submission_runner.py:262] Initializing model.
I0609 02:26:59.522824 140321551468352 submission_runner.py:272] Initializing optimizer.
I0609 02:27:00.012030 140321551468352 submission_runner.py:279] Initializing metrics bundle.
I0609 02:27:00.012250 140321551468352 submission_runner.py:297] Initializing checkpoint and logger.
I0609 02:27:00.015742 140321551468352 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 02:27:00.015867 140321551468352 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 02:27:00.491226 140321551468352 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/momentum/ogbg_pytorch/trial_1/meta_data_0.json.
I0609 02:27:00.492258 140321551468352 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/momentum/ogbg_pytorch/trial_1/flags_0.json.
I0609 02:27:00.549237 140321551468352 submission_runner.py:332] Starting training loop.
I0609 02:27:00.806601 140321551468352 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:27:00.813545 140321551468352 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:27:00.965794 140321551468352 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:27:05.579727 140283723360000 logging_writer.py:48] [0] global_step=0, grad_norm=2.746258, loss=0.763513
I0609 02:27:05.589530 140321551468352 spec.py:298] Evaluating on the training split.
I0609 02:27:05.595330 140321551468352 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:27:05.600018 140321551468352 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:27:05.654388 140321551468352 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:28:04.522948 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 02:28:04.526547 140321551468352 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:28:04.531643 140321551468352 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:28:04.586951 140321551468352 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:28:50.707030 140321551468352 spec.py:326] Evaluating on the test split.
I0609 02:28:50.711298 140321551468352 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:28:50.716264 140321551468352 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:28:50.773241 140321551468352 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:29:38.177056 140321551468352 submission_runner.py:419] Time since start: 157.63s, 	Step: 1, 	{'train/accuracy': 0.4739316640944997, 'train/loss': 0.7644702862996201, 'train/mean_average_precision': 0.02343682593869213, 'validation/accuracy': 0.46567974364033005, 'validation/loss': 0.7720541492416622, 'validation/mean_average_precision': 0.027659589852904683, 'validation/num_examples': 43793, 'test/accuracy': 0.4638676033474869, 'test/loss': 0.7739508672187967, 'test/mean_average_precision': 0.028388931286938906, 'test/num_examples': 43793, 'score': 5.0405449867248535, 'total_duration': 157.62819838523865, 'accumulated_submission_time': 5.0405449867248535, 'accumulated_eval_time': 152.58722686767578, 'accumulated_logging_time': 0}
I0609 02:29:38.194509 140269319689984 logging_writer.py:48] [1] accumulated_eval_time=152.587227, accumulated_logging_time=0, accumulated_submission_time=5.040545, global_step=1, preemption_count=0, score=5.040545, test/accuracy=0.463868, test/loss=0.773951, test/mean_average_precision=0.028389, test/num_examples=43793, total_duration=157.628198, train/accuracy=0.473932, train/loss=0.764470, train/mean_average_precision=0.023437, validation/accuracy=0.465680, validation/loss=0.772054, validation/mean_average_precision=0.027660, validation/num_examples=43793
I0609 02:29:38.473747 140321551468352 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.479603 139925233645376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.479598 140087519385408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.480100 140273019930432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.480111 140005492746048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.480114 140547624421184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.480103 139811366827840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.480144 140030387128128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:29:38.512919 140269328082688 logging_writer.py:48] [1] global_step=1, grad_norm=2.746106, loss=0.764618
I0609 02:29:38.818405 140269319689984 logging_writer.py:48] [2] global_step=2, grad_norm=2.712923, loss=0.762353
I0609 02:29:39.125653 140269328082688 logging_writer.py:48] [3] global_step=3, grad_norm=2.666529, loss=0.758407
I0609 02:29:39.451363 140269319689984 logging_writer.py:48] [4] global_step=4, grad_norm=2.552254, loss=0.746109
I0609 02:29:39.776330 140269328082688 logging_writer.py:48] [5] global_step=5, grad_norm=2.420443, loss=0.739009
I0609 02:29:40.084072 140269319689984 logging_writer.py:48] [6] global_step=6, grad_norm=2.254871, loss=0.719523
I0609 02:29:40.395845 140269328082688 logging_writer.py:48] [7] global_step=7, grad_norm=2.005312, loss=0.700882
I0609 02:29:40.714282 140269319689984 logging_writer.py:48] [8] global_step=8, grad_norm=1.841054, loss=0.683936
I0609 02:29:41.021705 140269328082688 logging_writer.py:48] [9] global_step=9, grad_norm=1.641865, loss=0.664740
I0609 02:29:41.326734 140269319689984 logging_writer.py:48] [10] global_step=10, grad_norm=1.590087, loss=0.649001
I0609 02:29:41.638822 140269328082688 logging_writer.py:48] [11] global_step=11, grad_norm=1.608799, loss=0.637216
I0609 02:29:41.942336 140269319689984 logging_writer.py:48] [12] global_step=12, grad_norm=1.590110, loss=0.623898
I0609 02:29:42.243109 140269328082688 logging_writer.py:48] [13] global_step=13, grad_norm=1.541859, loss=0.610493
I0609 02:29:42.549766 140269319689984 logging_writer.py:48] [14] global_step=14, grad_norm=1.430448, loss=0.594844
I0609 02:29:42.853211 140269328082688 logging_writer.py:48] [15] global_step=15, grad_norm=1.390881, loss=0.582081
I0609 02:29:43.172273 140269319689984 logging_writer.py:48] [16] global_step=16, grad_norm=1.289696, loss=0.570348
I0609 02:29:43.477915 140269328082688 logging_writer.py:48] [17] global_step=17, grad_norm=1.063798, loss=0.555244
I0609 02:29:43.782753 140269319689984 logging_writer.py:48] [18] global_step=18, grad_norm=0.985104, loss=0.540802
I0609 02:29:44.090690 140269328082688 logging_writer.py:48] [19] global_step=19, grad_norm=0.889713, loss=0.531236
I0609 02:29:44.415931 140269319689984 logging_writer.py:48] [20] global_step=20, grad_norm=0.843881, loss=0.519323
I0609 02:29:44.733047 140269328082688 logging_writer.py:48] [21] global_step=21, grad_norm=0.803754, loss=0.506886
I0609 02:29:45.033423 140269319689984 logging_writer.py:48] [22] global_step=22, grad_norm=0.773349, loss=0.493832
I0609 02:29:45.362869 140269328082688 logging_writer.py:48] [23] global_step=23, grad_norm=0.728222, loss=0.487448
I0609 02:29:45.668444 140269319689984 logging_writer.py:48] [24] global_step=24, grad_norm=0.675994, loss=0.471475
I0609 02:29:45.973476 140269328082688 logging_writer.py:48] [25] global_step=25, grad_norm=0.634935, loss=0.461053
I0609 02:29:46.282438 140269319689984 logging_writer.py:48] [26] global_step=26, grad_norm=0.607854, loss=0.452908
I0609 02:29:46.589778 140269328082688 logging_writer.py:48] [27] global_step=27, grad_norm=0.597582, loss=0.442402
I0609 02:29:46.912409 140269319689984 logging_writer.py:48] [28] global_step=28, grad_norm=0.579618, loss=0.431913
I0609 02:29:47.217802 140269328082688 logging_writer.py:48] [29] global_step=29, grad_norm=0.554100, loss=0.426462
I0609 02:29:47.523246 140269319689984 logging_writer.py:48] [30] global_step=30, grad_norm=0.540982, loss=0.413008
I0609 02:29:47.829881 140269328082688 logging_writer.py:48] [31] global_step=31, grad_norm=0.520330, loss=0.404243
I0609 02:29:48.136277 140269319689984 logging_writer.py:48] [32] global_step=32, grad_norm=0.506239, loss=0.394431
I0609 02:29:48.450994 140269328082688 logging_writer.py:48] [33] global_step=33, grad_norm=0.480270, loss=0.384635
I0609 02:29:48.760913 140269319689984 logging_writer.py:48] [34] global_step=34, grad_norm=0.459252, loss=0.377069
I0609 02:29:49.067014 140269328082688 logging_writer.py:48] [35] global_step=35, grad_norm=0.444029, loss=0.366851
I0609 02:29:49.369639 140269319689984 logging_writer.py:48] [36] global_step=36, grad_norm=0.428370, loss=0.355202
I0609 02:29:49.674098 140269328082688 logging_writer.py:48] [37] global_step=37, grad_norm=0.416651, loss=0.347823
I0609 02:29:49.978913 140269319689984 logging_writer.py:48] [38] global_step=38, grad_norm=0.402449, loss=0.340054
I0609 02:29:50.285531 140269328082688 logging_writer.py:48] [39] global_step=39, grad_norm=0.399576, loss=0.330943
I0609 02:29:50.592005 140269319689984 logging_writer.py:48] [40] global_step=40, grad_norm=0.386517, loss=0.322313
I0609 02:29:50.901173 140269328082688 logging_writer.py:48] [41] global_step=41, grad_norm=0.381963, loss=0.313871
I0609 02:29:51.211503 140269319689984 logging_writer.py:48] [42] global_step=42, grad_norm=0.368687, loss=0.306830
I0609 02:29:51.520616 140269328082688 logging_writer.py:48] [43] global_step=43, grad_norm=0.356486, loss=0.299046
I0609 02:29:51.828495 140269319689984 logging_writer.py:48] [44] global_step=44, grad_norm=0.344242, loss=0.292059
I0609 02:29:52.135520 140269328082688 logging_writer.py:48] [45] global_step=45, grad_norm=0.335056, loss=0.282908
I0609 02:29:52.441122 140269319689984 logging_writer.py:48] [46] global_step=46, grad_norm=0.324485, loss=0.275942
I0609 02:29:52.741650 140269328082688 logging_writer.py:48] [47] global_step=47, grad_norm=0.312577, loss=0.271082
I0609 02:29:53.043124 140269319689984 logging_writer.py:48] [48] global_step=48, grad_norm=0.302467, loss=0.259363
I0609 02:29:53.346535 140269328082688 logging_writer.py:48] [49] global_step=49, grad_norm=0.293677, loss=0.249603
I0609 02:29:53.653763 140269319689984 logging_writer.py:48] [50] global_step=50, grad_norm=0.286329, loss=0.247196
I0609 02:29:53.961663 140269328082688 logging_writer.py:48] [51] global_step=51, grad_norm=0.277584, loss=0.241629
I0609 02:29:54.291256 140269319689984 logging_writer.py:48] [52] global_step=52, grad_norm=0.269138, loss=0.231446
I0609 02:29:54.600177 140269328082688 logging_writer.py:48] [53] global_step=53, grad_norm=0.259244, loss=0.228406
I0609 02:29:54.903158 140269319689984 logging_writer.py:48] [54] global_step=54, grad_norm=0.251338, loss=0.219172
I0609 02:29:55.206670 140269328082688 logging_writer.py:48] [55] global_step=55, grad_norm=0.241491, loss=0.212707
I0609 02:29:55.509250 140269319689984 logging_writer.py:48] [56] global_step=56, grad_norm=0.234973, loss=0.209913
I0609 02:29:55.809484 140269328082688 logging_writer.py:48] [57] global_step=57, grad_norm=0.228281, loss=0.200033
I0609 02:29:56.115001 140269319689984 logging_writer.py:48] [58] global_step=58, grad_norm=0.220926, loss=0.193418
I0609 02:29:56.421094 140269328082688 logging_writer.py:48] [59] global_step=59, grad_norm=0.212133, loss=0.192170
I0609 02:29:56.721047 140269319689984 logging_writer.py:48] [60] global_step=60, grad_norm=0.207217, loss=0.186635
I0609 02:29:57.023736 140269328082688 logging_writer.py:48] [61] global_step=61, grad_norm=0.201501, loss=0.183904
I0609 02:29:57.325250 140269319689984 logging_writer.py:48] [62] global_step=62, grad_norm=0.190659, loss=0.177841
I0609 02:29:57.628413 140269328082688 logging_writer.py:48] [63] global_step=63, grad_norm=0.187270, loss=0.169887
I0609 02:29:57.939292 140269319689984 logging_writer.py:48] [64] global_step=64, grad_norm=0.179425, loss=0.169884
I0609 02:29:58.253839 140269328082688 logging_writer.py:48] [65] global_step=65, grad_norm=0.173633, loss=0.164054
I0609 02:29:58.575323 140269319689984 logging_writer.py:48] [66] global_step=66, grad_norm=0.168751, loss=0.156529
I0609 02:29:58.875548 140269328082688 logging_writer.py:48] [67] global_step=67, grad_norm=0.163567, loss=0.153049
I0609 02:29:59.181831 140269319689984 logging_writer.py:48] [68] global_step=68, grad_norm=0.158009, loss=0.152155
I0609 02:29:59.488660 140269328082688 logging_writer.py:48] [69] global_step=69, grad_norm=0.152623, loss=0.151260
I0609 02:29:59.797058 140269319689984 logging_writer.py:48] [70] global_step=70, grad_norm=0.148894, loss=0.141956
I0609 02:30:00.107884 140269328082688 logging_writer.py:48] [71] global_step=71, grad_norm=0.144941, loss=0.142609
I0609 02:30:00.422632 140269319689984 logging_writer.py:48] [72] global_step=72, grad_norm=0.140473, loss=0.137966
I0609 02:30:00.753956 140269328082688 logging_writer.py:48] [73] global_step=73, grad_norm=0.134143, loss=0.134618
I0609 02:30:01.084325 140269319689984 logging_writer.py:48] [74] global_step=74, grad_norm=0.130554, loss=0.133722
I0609 02:30:01.412146 140269328082688 logging_writer.py:48] [75] global_step=75, grad_norm=0.127973, loss=0.126419
I0609 02:30:01.717368 140269319689984 logging_writer.py:48] [76] global_step=76, grad_norm=0.123435, loss=0.127657
I0609 02:30:02.042718 140269328082688 logging_writer.py:48] [77] global_step=77, grad_norm=0.117744, loss=0.126072
I0609 02:30:02.393771 140269319689984 logging_writer.py:48] [78] global_step=78, grad_norm=0.118189, loss=0.120565
I0609 02:30:02.696983 140269328082688 logging_writer.py:48] [79] global_step=79, grad_norm=0.114000, loss=0.121927
I0609 02:30:02.999664 140269319689984 logging_writer.py:48] [80] global_step=80, grad_norm=0.109576, loss=0.117635
I0609 02:30:03.297502 140269328082688 logging_writer.py:48] [81] global_step=81, grad_norm=0.106887, loss=0.115281
I0609 02:30:03.601655 140269319689984 logging_writer.py:48] [82] global_step=82, grad_norm=0.103220, loss=0.113218
I0609 02:30:03.904427 140269328082688 logging_writer.py:48] [83] global_step=83, grad_norm=0.098960, loss=0.112249
I0609 02:30:04.231755 140269319689984 logging_writer.py:48] [84] global_step=84, grad_norm=0.098198, loss=0.111009
I0609 02:30:04.545212 140269328082688 logging_writer.py:48] [85] global_step=85, grad_norm=0.097419, loss=0.109533
I0609 02:30:04.848881 140269319689984 logging_writer.py:48] [86] global_step=86, grad_norm=0.093395, loss=0.110112
I0609 02:30:05.146965 140269328082688 logging_writer.py:48] [87] global_step=87, grad_norm=0.091347, loss=0.105656
I0609 02:30:05.448454 140269319689984 logging_writer.py:48] [88] global_step=88, grad_norm=0.088263, loss=0.102131
I0609 02:30:05.776452 140269328082688 logging_writer.py:48] [89] global_step=89, grad_norm=0.086140, loss=0.103855
I0609 02:30:06.079783 140269319689984 logging_writer.py:48] [90] global_step=90, grad_norm=0.085148, loss=0.098788
I0609 02:30:06.380180 140269328082688 logging_writer.py:48] [91] global_step=91, grad_norm=0.080753, loss=0.099650
I0609 02:30:06.682559 140269319689984 logging_writer.py:48] [92] global_step=92, grad_norm=0.079259, loss=0.099705
I0609 02:30:06.986053 140269328082688 logging_writer.py:48] [93] global_step=93, grad_norm=0.079197, loss=0.094942
I0609 02:30:07.287820 140269319689984 logging_writer.py:48] [94] global_step=94, grad_norm=0.075895, loss=0.097364
I0609 02:30:07.588408 140269328082688 logging_writer.py:48] [95] global_step=95, grad_norm=0.072933, loss=0.095880
I0609 02:30:07.890011 140269319689984 logging_writer.py:48] [96] global_step=96, grad_norm=0.072879, loss=0.091054
I0609 02:30:08.188961 140269328082688 logging_writer.py:48] [97] global_step=97, grad_norm=0.070806, loss=0.092459
I0609 02:30:08.496989 140269319689984 logging_writer.py:48] [98] global_step=98, grad_norm=0.067499, loss=0.090891
I0609 02:30:08.804136 140269328082688 logging_writer.py:48] [99] global_step=99, grad_norm=0.067714, loss=0.089478
I0609 02:30:09.108601 140269319689984 logging_writer.py:48] [100] global_step=100, grad_norm=0.064843, loss=0.090849
I0609 02:32:07.121690 140269328082688 logging_writer.py:48] [500] global_step=500, grad_norm=0.014105, loss=0.060179
I0609 02:33:38.441083 140321551468352 spec.py:298] Evaluating on the training split.
I0609 02:34:36.380972 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 02:34:39.582514 140321551468352 spec.py:326] Evaluating on the test split.
I0609 02:34:42.913410 140321551468352 submission_runner.py:419] Time since start: 462.36s, 	Step: 810, 	{'train/accuracy': 0.9867415713965898, 'train/loss': 0.055606636558546134, 'train/mean_average_precision': 0.031000198643842035, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06552511448542551, 'validation/mean_average_precision': 0.03298735965654078, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06890200126442644, 'test/mean_average_precision': 0.03399196387155398, 'test/num_examples': 43793, 'score': 245.039794921875, 'total_duration': 462.3645806312561, 'accumulated_submission_time': 245.039794921875, 'accumulated_eval_time': 217.0593101978302, 'accumulated_logging_time': 0.028801441192626953}
I0609 02:34:42.923762 140269319689984 logging_writer.py:48] [810] accumulated_eval_time=217.059310, accumulated_logging_time=0.028801, accumulated_submission_time=245.039795, global_step=810, preemption_count=0, score=245.039795, test/accuracy=0.983142, test/loss=0.068902, test/mean_average_precision=0.033992, test/num_examples=43793, total_duration=462.364581, train/accuracy=0.986742, train/loss=0.055607, train/mean_average_precision=0.031000, validation/accuracy=0.984118, validation/loss=0.065525, validation/mean_average_precision=0.032987, validation/num_examples=43793
I0609 02:35:39.830507 140269328082688 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.033947, loss=0.057552
I0609 02:38:06.824301 140269319689984 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.068020, loss=0.052624
I0609 02:38:43.157483 140321551468352 spec.py:298] Evaluating on the training split.
I0609 02:39:42.320841 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 02:39:45.561499 140321551468352 spec.py:326] Evaluating on the test split.
I0609 02:39:48.786893 140321551468352 submission_runner.py:419] Time since start: 768.24s, 	Step: 1625, 	{'train/accuracy': 0.9867320674615835, 'train/loss': 0.053626494796168, 'train/mean_average_precision': 0.04145945411675076, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06366210039197634, 'validation/mean_average_precision': 0.04355950496058079, 'validation/num_examples': 43793, 'test/accuracy': 0.983141682731734, 'test/loss': 0.06700037307319226, 'test/mean_average_precision': 0.04418998863328308, 'test/num_examples': 43793, 'score': 485.03745794296265, 'total_duration': 768.2381312847137, 'accumulated_submission_time': 485.03745794296265, 'accumulated_eval_time': 282.6884641647339, 'accumulated_logging_time': 0.05013275146484375}
I0609 02:39:48.796980 140269328082688 logging_writer.py:48] [1625] accumulated_eval_time=282.688464, accumulated_logging_time=0.050133, accumulated_submission_time=485.037458, global_step=1625, preemption_count=0, score=485.037458, test/accuracy=0.983142, test/loss=0.067000, test/mean_average_precision=0.044190, test/num_examples=43793, total_duration=768.238131, train/accuracy=0.986732, train/loss=0.053626, train/mean_average_precision=0.041459, validation/accuracy=0.984118, validation/loss=0.063662, validation/mean_average_precision=0.043560, validation/num_examples=43793
I0609 02:41:40.226873 140269319689984 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.023939, loss=0.055503
I0609 02:43:48.894592 140321551468352 spec.py:298] Evaluating on the training split.
I0609 02:44:47.931338 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 02:44:51.204106 140321551468352 spec.py:326] Evaluating on the test split.
I0609 02:44:54.396358 140321551468352 submission_runner.py:419] Time since start: 1073.85s, 	Step: 2438, 	{'train/accuracy': 0.9867478646153847, 'train/loss': 0.05118090153846154, 'train/mean_average_precision': 0.0631280601012357, 'validation/accuracy': 0.9841849563248403, 'validation/loss': 0.06013387315327428, 'validation/mean_average_precision': 0.05774673621832885, 'validation/num_examples': 43793, 'test/accuracy': 0.9831926472886224, 'test/loss': 0.063225027419774, 'test/mean_average_precision': 0.059830496437903016, 'test/num_examples': 43793, 'score': 724.9007804393768, 'total_duration': 1073.8475813865662, 'accumulated_submission_time': 724.9007804393768, 'accumulated_eval_time': 348.18996238708496, 'accumulated_logging_time': 0.0716543197631836}
I0609 02:44:54.407054 140269328082688 logging_writer.py:48] [2438] accumulated_eval_time=348.189962, accumulated_logging_time=0.071654, accumulated_submission_time=724.900780, global_step=2438, preemption_count=0, score=724.900780, test/accuracy=0.983193, test/loss=0.063225, test/mean_average_precision=0.059830, test/num_examples=43793, total_duration=1073.847581, train/accuracy=0.986748, train/loss=0.051181, train/mean_average_precision=0.063128, validation/accuracy=0.984185, validation/loss=0.060134, validation/mean_average_precision=0.057747, validation/num_examples=43793
I0609 02:45:13.090365 140269319689984 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.067217, loss=0.051204
I0609 02:47:41.170449 140269328082688 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.041912, loss=0.050393
I0609 02:48:54.505479 140321551468352 spec.py:298] Evaluating on the training split.
I0609 02:49:54.762982 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 02:49:58.072962 140321551468352 spec.py:326] Evaluating on the test split.
I0609 02:50:01.296125 140321551468352 submission_runner.py:419] Time since start: 1380.75s, 	Step: 3247, 	{'train/accuracy': 0.9870868881917129, 'train/loss': 0.04891866527868349, 'train/mean_average_precision': 0.07996652897628079, 'validation/accuracy': 0.9842872533808771, 'validation/loss': 0.05850110354978903, 'validation/mean_average_precision': 0.07720556521124354, 'validation/num_examples': 43793, 'test/accuracy': 0.9832907856502341, 'test/loss': 0.06146471004536688, 'test/mean_average_precision': 0.07934115425193121, 'test/num_examples': 43793, 'score': 964.7633464336395, 'total_duration': 1380.7474224567413, 'accumulated_submission_time': 964.7633464336395, 'accumulated_eval_time': 414.9804263114929, 'accumulated_logging_time': 0.09351038932800293}
I0609 02:50:01.306300 140269319689984 logging_writer.py:48] [3247] accumulated_eval_time=414.980426, accumulated_logging_time=0.093510, accumulated_submission_time=964.763346, global_step=3247, preemption_count=0, score=964.763346, test/accuracy=0.983291, test/loss=0.061465, test/mean_average_precision=0.079341, test/num_examples=43793, total_duration=1380.747422, train/accuracy=0.987087, train/loss=0.048919, train/mean_average_precision=0.079967, validation/accuracy=0.984287, validation/loss=0.058501, validation/mean_average_precision=0.077206, validation/num_examples=43793
I0609 02:51:16.491430 140269328082688 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.047219, loss=0.051080
I0609 02:53:45.189935 140269319689984 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.050930, loss=0.050259
I0609 02:54:01.656431 140321551468352 spec.py:298] Evaluating on the training split.
I0609 02:55:02.585809 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 02:55:05.852582 140321551468352 spec.py:326] Evaluating on the test split.
I0609 02:55:09.070031 140321551468352 submission_runner.py:419] Time since start: 1688.52s, 	Step: 4056, 	{'train/accuracy': 0.9870369833961228, 'train/loss': 0.04740558014060969, 'train/mean_average_precision': 0.09843753387593629, 'validation/accuracy': 0.9843018672460252, 'validation/loss': 0.056977455880335176, 'validation/mean_average_precision': 0.09735136523081689, 'validation/num_examples': 43793, 'test/accuracy': 0.9833779729500349, 'test/loss': 0.059749203836746624, 'test/mean_average_precision': 0.09826581316901449, 'test/num_examples': 43793, 'score': 1204.8736226558685, 'total_duration': 1688.5212664604187, 'accumulated_submission_time': 1204.8736226558685, 'accumulated_eval_time': 482.39381194114685, 'accumulated_logging_time': 0.11806416511535645}
I0609 02:55:09.080371 140269328082688 logging_writer.py:48] [4056] accumulated_eval_time=482.393812, accumulated_logging_time=0.118064, accumulated_submission_time=1204.873623, global_step=4056, preemption_count=0, score=1204.873623, test/accuracy=0.983378, test/loss=0.059749, test/mean_average_precision=0.098266, test/num_examples=43793, total_duration=1688.521266, train/accuracy=0.987037, train/loss=0.047406, train/mean_average_precision=0.098438, validation/accuracy=0.984302, validation/loss=0.056977, validation/mean_average_precision=0.097351, validation/num_examples=43793
I0609 02:57:20.679665 140269319689984 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.055396, loss=0.047729
I0609 02:59:09.141426 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:00:09.796489 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:00:13.122903 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:00:16.402618 140321551468352 submission_runner.py:419] Time since start: 1995.85s, 	Step: 4867, 	{'train/accuracy': 0.9871022275767868, 'train/loss': 0.046965417373688506, 'train/mean_average_precision': 0.1207065657952333, 'validation/accuracy': 0.984534065325601, 'validation/loss': 0.05700649332592898, 'validation/mean_average_precision': 0.11515708091208979, 'validation/num_examples': 43793, 'test/accuracy': 0.9835603502486523, 'test/loss': 0.06012042113992972, 'test/mean_average_precision': 0.11921976679134427, 'test/num_examples': 43793, 'score': 1444.6982975006104, 'total_duration': 1995.8538448810577, 'accumulated_submission_time': 1444.6982975006104, 'accumulated_eval_time': 549.6549000740051, 'accumulated_logging_time': 0.1417684555053711}
I0609 03:00:16.413491 140269328082688 logging_writer.py:48] [4867] accumulated_eval_time=549.654900, accumulated_logging_time=0.141768, accumulated_submission_time=1444.698298, global_step=4867, preemption_count=0, score=1444.698298, test/accuracy=0.983560, test/loss=0.060120, test/mean_average_precision=0.119220, test/num_examples=43793, total_duration=1995.853845, train/accuracy=0.987102, train/loss=0.046965, train/mean_average_precision=0.120707, validation/accuracy=0.984534, validation/loss=0.057006, validation/mean_average_precision=0.115157, validation/num_examples=43793
I0609 03:00:56.089222 140269319689984 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.038503, loss=0.047422
I0609 03:03:26.128288 140269328082688 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.056481, loss=0.048151
I0609 03:04:16.546753 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:05:18.703390 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:05:22.041608 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:05:25.325889 140321551468352 submission_runner.py:419] Time since start: 2304.78s, 	Step: 5669, 	{'train/accuracy': 0.9874120766774296, 'train/loss': 0.044321106432941126, 'train/mean_average_precision': 0.13773427593172666, 'validation/accuracy': 0.9848157881704009, 'validation/loss': 0.053594365127014786, 'validation/mean_average_precision': 0.13171776192380769, 'validation/num_examples': 43793, 'test/accuracy': 0.9838467626344717, 'test/loss': 0.05653255318951781, 'test/mean_average_precision': 0.13556548129351734, 'test/num_examples': 43793, 'score': 1684.5935509204865, 'total_duration': 2304.7771594524384, 'accumulated_submission_time': 1684.5935509204865, 'accumulated_eval_time': 618.433872461319, 'accumulated_logging_time': 0.16843557357788086}
I0609 03:05:25.336739 140269319689984 logging_writer.py:48] [5669] accumulated_eval_time=618.433872, accumulated_logging_time=0.168436, accumulated_submission_time=1684.593551, global_step=5669, preemption_count=0, score=1684.593551, test/accuracy=0.983847, test/loss=0.056533, test/mean_average_precision=0.135565, test/num_examples=43793, total_duration=2304.777159, train/accuracy=0.987412, train/loss=0.044321, train/mean_average_precision=0.137734, validation/accuracy=0.984816, validation/loss=0.053594, validation/mean_average_precision=0.131718, validation/num_examples=43793
I0609 03:07:06.152110 140269328082688 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.055489, loss=0.047678
I0609 03:09:25.525467 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:10:27.052247 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:10:30.315620 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:10:33.561910 140321551468352 submission_runner.py:419] Time since start: 2613.01s, 	Step: 6464, 	{'train/accuracy': 0.988020956831878, 'train/loss': 0.042493709260853524, 'train/mean_average_precision': 0.1566877482453046, 'validation/accuracy': 0.9850796496244643, 'validation/loss': 0.05234756584256645, 'validation/mean_average_precision': 0.145486986600105, 'validation/num_examples': 43793, 'test/accuracy': 0.9841045337817091, 'test/loss': 0.05525583312519296, 'test/mean_average_precision': 0.14553212833647447, 'test/num_examples': 43793, 'score': 1924.5521306991577, 'total_duration': 2613.013090133667, 'accumulated_submission_time': 1924.5521306991577, 'accumulated_eval_time': 686.4700605869293, 'accumulated_logging_time': 0.19159936904907227}
I0609 03:10:33.574349 140269319689984 logging_writer.py:48] [6464] accumulated_eval_time=686.470061, accumulated_logging_time=0.191599, accumulated_submission_time=1924.552131, global_step=6464, preemption_count=0, score=1924.552131, test/accuracy=0.984105, test/loss=0.055256, test/mean_average_precision=0.145532, test/num_examples=43793, total_duration=2613.013090, train/accuracy=0.988021, train/loss=0.042494, train/mean_average_precision=0.156688, validation/accuracy=0.985080, validation/loss=0.052348, validation/mean_average_precision=0.145487, validation/num_examples=43793
I0609 03:10:44.690035 140269328082688 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.056152, loss=0.047420
I0609 03:13:14.652570 140269319689984 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.044688, loss=0.043388
I0609 03:14:33.748505 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:15:37.177707 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:15:40.437022 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:15:43.661133 140321551468352 submission_runner.py:419] Time since start: 2923.11s, 	Step: 7266, 	{'train/accuracy': 0.987968393270871, 'train/loss': 0.042440894400216395, 'train/mean_average_precision': 0.16584594353447504, 'validation/accuracy': 0.9851929070793622, 'validation/loss': 0.05183884286197935, 'validation/mean_average_precision': 0.15168674084563966, 'validation/num_examples': 43793, 'test/accuracy': 0.9842287862137925, 'test/loss': 0.054699217767760835, 'test/mean_average_precision': 0.15594345823397723, 'test/num_examples': 43793, 'score': 2164.4888598918915, 'total_duration': 2923.112371444702, 'accumulated_submission_time': 2164.4888598918915, 'accumulated_eval_time': 756.3824317455292, 'accumulated_logging_time': 0.2209324836730957}
I0609 03:15:43.671954 140269328082688 logging_writer.py:48] [7266] accumulated_eval_time=756.382432, accumulated_logging_time=0.220932, accumulated_submission_time=2164.488860, global_step=7266, preemption_count=0, score=2164.488860, test/accuracy=0.984229, test/loss=0.054699, test/mean_average_precision=0.155943, test/num_examples=43793, total_duration=2923.112371, train/accuracy=0.987968, train/loss=0.042441, train/mean_average_precision=0.165846, validation/accuracy=0.985193, validation/loss=0.051839, validation/mean_average_precision=0.151687, validation/num_examples=43793
I0609 03:16:54.367159 140269319689984 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.046093, loss=0.048510
I0609 03:19:24.850787 140269328082688 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.076184, loss=0.041137
I0609 03:19:43.720654 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:20:45.890696 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:20:49.188073 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:20:52.457232 140321551468352 submission_runner.py:419] Time since start: 3231.91s, 	Step: 8065, 	{'train/accuracy': 0.9880283510173158, 'train/loss': 0.04179700453576153, 'train/mean_average_precision': 0.18147678786709137, 'validation/accuracy': 0.9852245704538498, 'validation/loss': 0.051595804897187396, 'validation/mean_average_precision': 0.1668060428100778, 'validation/num_examples': 43793, 'test/accuracy': 0.984265851346075, 'test/loss': 0.05434619739120436, 'test/mean_average_precision': 0.1734413097230173, 'test/num_examples': 43793, 'score': 2404.3078150749207, 'total_duration': 3231.9084548950195, 'accumulated_submission_time': 2404.3078150749207, 'accumulated_eval_time': 825.1187763214111, 'accumulated_logging_time': 0.24302124977111816}
I0609 03:20:52.472628 140269319689984 logging_writer.py:48] [8065] accumulated_eval_time=825.118776, accumulated_logging_time=0.243021, accumulated_submission_time=2404.307815, global_step=8065, preemption_count=0, score=2404.307815, test/accuracy=0.984266, test/loss=0.054346, test/mean_average_precision=0.173441, test/num_examples=43793, total_duration=3231.908455, train/accuracy=0.988028, train/loss=0.041797, train/mean_average_precision=0.181477, validation/accuracy=0.985225, validation/loss=0.051596, validation/mean_average_precision=0.166806, validation/num_examples=43793
I0609 03:23:03.055239 140269328082688 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.071666, loss=0.044556
I0609 03:24:52.639436 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:25:54.806578 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:25:58.124547 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:26:01.327807 140321551468352 submission_runner.py:419] Time since start: 3540.78s, 	Step: 8870, 	{'train/accuracy': 0.9878202411111221, 'train/loss': 0.04156612566167556, 'train/mean_average_precision': 0.19091372344952137, 'validation/accuracy': 0.985115778346636, 'validation/loss': 0.05088759060596392, 'validation/mean_average_precision': 0.1699887213283211, 'validation/num_examples': 43793, 'test/accuracy': 0.9841736097100537, 'test/loss': 0.05386088239654722, 'test/mean_average_precision': 0.16775012767614125, 'test/num_examples': 43793, 'score': 2644.2433822155, 'total_duration': 3540.7790310382843, 'accumulated_submission_time': 2644.2433822155, 'accumulated_eval_time': 893.8068926334381, 'accumulated_logging_time': 0.26973724365234375}
I0609 03:26:01.338599 140269319689984 logging_writer.py:48] [8870] accumulated_eval_time=893.806893, accumulated_logging_time=0.269737, accumulated_submission_time=2644.243382, global_step=8870, preemption_count=0, score=2644.243382, test/accuracy=0.984174, test/loss=0.053861, test/mean_average_precision=0.167750, test/num_examples=43793, total_duration=3540.779031, train/accuracy=0.987820, train/loss=0.041566, train/mean_average_precision=0.190914, validation/accuracy=0.985116, validation/loss=0.050888, validation/mean_average_precision=0.169989, validation/num_examples=43793
I0609 03:26:40.497671 140269328082688 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.051823, loss=0.042106
I0609 03:29:09.118687 140269319689984 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.064292, loss=0.043702
I0609 03:30:01.332725 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:31:04.270988 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:31:07.590362 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:31:10.862047 140321551468352 submission_runner.py:419] Time since start: 3850.31s, 	Step: 9676, 	{'train/accuracy': 0.9882628439131164, 'train/loss': 0.040386987550715105, 'train/mean_average_precision': 0.210961898217981, 'validation/accuracy': 0.9854498675415501, 'validation/loss': 0.04978688557323292, 'validation/mean_average_precision': 0.18104681565476385, 'validation/num_examples': 43793, 'test/accuracy': 0.9845392066966585, 'test/loss': 0.05235059013587319, 'test/mean_average_precision': 0.18245284220669938, 'test/num_examples': 43793, 'score': 2884.0057315826416, 'total_duration': 3850.313273668289, 'accumulated_submission_time': 2884.0057315826416, 'accumulated_eval_time': 963.3360526561737, 'accumulated_logging_time': 0.2935457229614258}
I0609 03:31:10.872948 140269328082688 logging_writer.py:48] [9676] accumulated_eval_time=963.336053, accumulated_logging_time=0.293546, accumulated_submission_time=2884.005732, global_step=9676, preemption_count=0, score=2884.005732, test/accuracy=0.984539, test/loss=0.052351, test/mean_average_precision=0.182453, test/num_examples=43793, total_duration=3850.313274, train/accuracy=0.988263, train/loss=0.040387, train/mean_average_precision=0.210962, validation/accuracy=0.985450, validation/loss=0.049787, validation/mean_average_precision=0.181047, validation/num_examples=43793
I0609 03:32:49.338325 140269319689984 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.051980, loss=0.045568
I0609 03:35:10.993625 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:36:14.193974 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:36:17.501768 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:36:20.766566 140321551468352 submission_runner.py:419] Time since start: 4160.22s, 	Step: 10478, 	{'train/accuracy': 0.9884220678419076, 'train/loss': 0.040165381850757895, 'train/mean_average_precision': 0.21542770154275964, 'validation/accuracy': 0.9853889764367663, 'validation/loss': 0.050279282126349856, 'validation/mean_average_precision': 0.18691448781972173, 'validation/num_examples': 43793, 'test/accuracy': 0.9844966660334706, 'test/loss': 0.053078250022428615, 'test/mean_average_precision': 0.1865348480134821, 'test/num_examples': 43793, 'score': 3123.8959336280823, 'total_duration': 4160.217764139175, 'accumulated_submission_time': 3123.8959336280823, 'accumulated_eval_time': 1033.1087353229523, 'accumulated_logging_time': 0.3154017925262451}
I0609 03:36:20.777609 140269328082688 logging_writer.py:48] [10478] accumulated_eval_time=1033.108735, accumulated_logging_time=0.315402, accumulated_submission_time=3123.895934, global_step=10478, preemption_count=0, score=3123.895934, test/accuracy=0.984497, test/loss=0.053078, test/mean_average_precision=0.186535, test/num_examples=43793, total_duration=4160.217764, train/accuracy=0.988422, train/loss=0.040165, train/mean_average_precision=0.215428, validation/accuracy=0.985389, validation/loss=0.050279, validation/mean_average_precision=0.186914, validation/num_examples=43793
I0609 03:36:27.604009 140269319689984 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.035686, loss=0.045089
I0609 03:38:56.748684 140269328082688 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.057997, loss=0.043974
I0609 03:40:20.935408 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:41:24.147418 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:41:27.481675 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:41:30.748885 140321551468352 submission_runner.py:419] Time since start: 4470.20s, 	Step: 11284, 	{'train/accuracy': 0.9887422171578968, 'train/loss': 0.03862924690383424, 'train/mean_average_precision': 0.23136382798199412, 'validation/accuracy': 0.9857734022783016, 'validation/loss': 0.04859289980794945, 'validation/mean_average_precision': 0.20001616596405883, 'validation/num_examples': 43793, 'test/accuracy': 0.9848273038612181, 'test/loss': 0.05134470154460515, 'test/mean_average_precision': 0.19948189466971802, 'test/num_examples': 43793, 'score': 3363.82128572464, 'total_duration': 4470.200117111206, 'accumulated_submission_time': 3363.82128572464, 'accumulated_eval_time': 1102.9219598770142, 'accumulated_logging_time': 0.33776330947875977}
I0609 03:41:30.759729 140269319689984 logging_writer.py:48] [11284] accumulated_eval_time=1102.921960, accumulated_logging_time=0.337763, accumulated_submission_time=3363.821286, global_step=11284, preemption_count=0, score=3363.821286, test/accuracy=0.984827, test/loss=0.051345, test/mean_average_precision=0.199482, test/num_examples=43793, total_duration=4470.200117, train/accuracy=0.988742, train/loss=0.038629, train/mean_average_precision=0.231364, validation/accuracy=0.985773, validation/loss=0.048593, validation/mean_average_precision=0.200016, validation/num_examples=43793
I0609 03:42:34.681886 140269328082688 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.047407, loss=0.039749
I0609 03:45:01.936177 140321551468352 spec.py:298] Evaluating on the training split.
I0609 03:46:06.222372 140321551468352 spec.py:310] Evaluating on the validation split.
I0609 03:46:09.575894 140321551468352 spec.py:326] Evaluating on the test split.
I0609 03:46:12.873938 140321551468352 submission_runner.py:419] Time since start: 4752.33s, 	Step: 12000, 	{'train/accuracy': 0.98876296297752, 'train/loss': 0.03837907263983901, 'train/mean_average_precision': 0.231671907104484, 'validation/accuracy': 0.9857047983002452, 'validation/loss': 0.04894342325833173, 'validation/mean_average_precision': 0.19742444338557977, 'validation/num_examples': 43793, 'test/accuracy': 0.984796556649211, 'test/loss': 0.051622476148797974, 'test/mean_average_precision': 0.19714546284128223, 'test/num_examples': 43793, 'score': 3574.7923941612244, 'total_duration': 4752.32505440712, 'accumulated_submission_time': 3574.7923941612244, 'accumulated_eval_time': 1173.859403371811, 'accumulated_logging_time': 0.35981321334838867}
I0609 03:46:12.886404 140269319689984 logging_writer.py:48] [12000] accumulated_eval_time=1173.859403, accumulated_logging_time=0.359813, accumulated_submission_time=3574.792394, global_step=12000, preemption_count=0, score=3574.792394, test/accuracy=0.984797, test/loss=0.051622, test/mean_average_precision=0.197145, test/num_examples=43793, total_duration=4752.325054, train/accuracy=0.988763, train/loss=0.038379, train/mean_average_precision=0.231672, validation/accuracy=0.985705, validation/loss=0.048943, validation/mean_average_precision=0.197424, validation/num_examples=43793
I0609 03:46:12.907883 140269328082688 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3574.792394
I0609 03:46:12.971806 140321551468352 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/momentum/ogbg_pytorch/trial_1/checkpoint_12000.
I0609 03:46:13.142122 140321551468352 submission_runner.py:581] Tuning trial 1/1
I0609 03:46:13.142350 140321551468352 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 03:46:13.143807 140321551468352 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4739316640944997, 'train/loss': 0.7644702862996201, 'train/mean_average_precision': 0.02343682593869213, 'validation/accuracy': 0.46567974364033005, 'validation/loss': 0.7720541492416622, 'validation/mean_average_precision': 0.027659589852904683, 'validation/num_examples': 43793, 'test/accuracy': 0.4638676033474869, 'test/loss': 0.7739508672187967, 'test/mean_average_precision': 0.028388931286938906, 'test/num_examples': 43793, 'score': 5.0405449867248535, 'total_duration': 157.62819838523865, 'accumulated_submission_time': 5.0405449867248535, 'accumulated_eval_time': 152.58722686767578, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (810, {'train/accuracy': 0.9867415713965898, 'train/loss': 0.055606636558546134, 'train/mean_average_precision': 0.031000198643842035, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06552511448542551, 'validation/mean_average_precision': 0.03298735965654078, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06890200126442644, 'test/mean_average_precision': 0.03399196387155398, 'test/num_examples': 43793, 'score': 245.039794921875, 'total_duration': 462.3645806312561, 'accumulated_submission_time': 245.039794921875, 'accumulated_eval_time': 217.0593101978302, 'accumulated_logging_time': 0.028801441192626953, 'global_step': 810, 'preemption_count': 0}), (1625, {'train/accuracy': 0.9867320674615835, 'train/loss': 0.053626494796168, 'train/mean_average_precision': 0.04145945411675076, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06366210039197634, 'validation/mean_average_precision': 0.04355950496058079, 'validation/num_examples': 43793, 'test/accuracy': 0.983141682731734, 'test/loss': 0.06700037307319226, 'test/mean_average_precision': 0.04418998863328308, 'test/num_examples': 43793, 'score': 485.03745794296265, 'total_duration': 768.2381312847137, 'accumulated_submission_time': 485.03745794296265, 'accumulated_eval_time': 282.6884641647339, 'accumulated_logging_time': 0.05013275146484375, 'global_step': 1625, 'preemption_count': 0}), (2438, {'train/accuracy': 0.9867478646153847, 'train/loss': 0.05118090153846154, 'train/mean_average_precision': 0.0631280601012357, 'validation/accuracy': 0.9841849563248403, 'validation/loss': 0.06013387315327428, 'validation/mean_average_precision': 0.05774673621832885, 'validation/num_examples': 43793, 'test/accuracy': 0.9831926472886224, 'test/loss': 0.063225027419774, 'test/mean_average_precision': 0.059830496437903016, 'test/num_examples': 43793, 'score': 724.9007804393768, 'total_duration': 1073.8475813865662, 'accumulated_submission_time': 724.9007804393768, 'accumulated_eval_time': 348.18996238708496, 'accumulated_logging_time': 0.0716543197631836, 'global_step': 2438, 'preemption_count': 0}), (3247, {'train/accuracy': 0.9870868881917129, 'train/loss': 0.04891866527868349, 'train/mean_average_precision': 0.07996652897628079, 'validation/accuracy': 0.9842872533808771, 'validation/loss': 0.05850110354978903, 'validation/mean_average_precision': 0.07720556521124354, 'validation/num_examples': 43793, 'test/accuracy': 0.9832907856502341, 'test/loss': 0.06146471004536688, 'test/mean_average_precision': 0.07934115425193121, 'test/num_examples': 43793, 'score': 964.7633464336395, 'total_duration': 1380.7474224567413, 'accumulated_submission_time': 964.7633464336395, 'accumulated_eval_time': 414.9804263114929, 'accumulated_logging_time': 0.09351038932800293, 'global_step': 3247, 'preemption_count': 0}), (4056, {'train/accuracy': 0.9870369833961228, 'train/loss': 0.04740558014060969, 'train/mean_average_precision': 0.09843753387593629, 'validation/accuracy': 0.9843018672460252, 'validation/loss': 0.056977455880335176, 'validation/mean_average_precision': 0.09735136523081689, 'validation/num_examples': 43793, 'test/accuracy': 0.9833779729500349, 'test/loss': 0.059749203836746624, 'test/mean_average_precision': 0.09826581316901449, 'test/num_examples': 43793, 'score': 1204.8736226558685, 'total_duration': 1688.5212664604187, 'accumulated_submission_time': 1204.8736226558685, 'accumulated_eval_time': 482.39381194114685, 'accumulated_logging_time': 0.11806416511535645, 'global_step': 4056, 'preemption_count': 0}), (4867, {'train/accuracy': 0.9871022275767868, 'train/loss': 0.046965417373688506, 'train/mean_average_precision': 0.1207065657952333, 'validation/accuracy': 0.984534065325601, 'validation/loss': 0.05700649332592898, 'validation/mean_average_precision': 0.11515708091208979, 'validation/num_examples': 43793, 'test/accuracy': 0.9835603502486523, 'test/loss': 0.06012042113992972, 'test/mean_average_precision': 0.11921976679134427, 'test/num_examples': 43793, 'score': 1444.6982975006104, 'total_duration': 1995.8538448810577, 'accumulated_submission_time': 1444.6982975006104, 'accumulated_eval_time': 549.6549000740051, 'accumulated_logging_time': 0.1417684555053711, 'global_step': 4867, 'preemption_count': 0}), (5669, {'train/accuracy': 0.9874120766774296, 'train/loss': 0.044321106432941126, 'train/mean_average_precision': 0.13773427593172666, 'validation/accuracy': 0.9848157881704009, 'validation/loss': 0.053594365127014786, 'validation/mean_average_precision': 0.13171776192380769, 'validation/num_examples': 43793, 'test/accuracy': 0.9838467626344717, 'test/loss': 0.05653255318951781, 'test/mean_average_precision': 0.13556548129351734, 'test/num_examples': 43793, 'score': 1684.5935509204865, 'total_duration': 2304.7771594524384, 'accumulated_submission_time': 1684.5935509204865, 'accumulated_eval_time': 618.433872461319, 'accumulated_logging_time': 0.16843557357788086, 'global_step': 5669, 'preemption_count': 0}), (6464, {'train/accuracy': 0.988020956831878, 'train/loss': 0.042493709260853524, 'train/mean_average_precision': 0.1566877482453046, 'validation/accuracy': 0.9850796496244643, 'validation/loss': 0.05234756584256645, 'validation/mean_average_precision': 0.145486986600105, 'validation/num_examples': 43793, 'test/accuracy': 0.9841045337817091, 'test/loss': 0.05525583312519296, 'test/mean_average_precision': 0.14553212833647447, 'test/num_examples': 43793, 'score': 1924.5521306991577, 'total_duration': 2613.013090133667, 'accumulated_submission_time': 1924.5521306991577, 'accumulated_eval_time': 686.4700605869293, 'accumulated_logging_time': 0.19159936904907227, 'global_step': 6464, 'preemption_count': 0}), (7266, {'train/accuracy': 0.987968393270871, 'train/loss': 0.042440894400216395, 'train/mean_average_precision': 0.16584594353447504, 'validation/accuracy': 0.9851929070793622, 'validation/loss': 0.05183884286197935, 'validation/mean_average_precision': 0.15168674084563966, 'validation/num_examples': 43793, 'test/accuracy': 0.9842287862137925, 'test/loss': 0.054699217767760835, 'test/mean_average_precision': 0.15594345823397723, 'test/num_examples': 43793, 'score': 2164.4888598918915, 'total_duration': 2923.112371444702, 'accumulated_submission_time': 2164.4888598918915, 'accumulated_eval_time': 756.3824317455292, 'accumulated_logging_time': 0.2209324836730957, 'global_step': 7266, 'preemption_count': 0}), (8065, {'train/accuracy': 0.9880283510173158, 'train/loss': 0.04179700453576153, 'train/mean_average_precision': 0.18147678786709137, 'validation/accuracy': 0.9852245704538498, 'validation/loss': 0.051595804897187396, 'validation/mean_average_precision': 0.1668060428100778, 'validation/num_examples': 43793, 'test/accuracy': 0.984265851346075, 'test/loss': 0.05434619739120436, 'test/mean_average_precision': 0.1734413097230173, 'test/num_examples': 43793, 'score': 2404.3078150749207, 'total_duration': 3231.9084548950195, 'accumulated_submission_time': 2404.3078150749207, 'accumulated_eval_time': 825.1187763214111, 'accumulated_logging_time': 0.24302124977111816, 'global_step': 8065, 'preemption_count': 0}), (8870, {'train/accuracy': 0.9878202411111221, 'train/loss': 0.04156612566167556, 'train/mean_average_precision': 0.19091372344952137, 'validation/accuracy': 0.985115778346636, 'validation/loss': 0.05088759060596392, 'validation/mean_average_precision': 0.1699887213283211, 'validation/num_examples': 43793, 'test/accuracy': 0.9841736097100537, 'test/loss': 0.05386088239654722, 'test/mean_average_precision': 0.16775012767614125, 'test/num_examples': 43793, 'score': 2644.2433822155, 'total_duration': 3540.7790310382843, 'accumulated_submission_time': 2644.2433822155, 'accumulated_eval_time': 893.8068926334381, 'accumulated_logging_time': 0.26973724365234375, 'global_step': 8870, 'preemption_count': 0}), (9676, {'train/accuracy': 0.9882628439131164, 'train/loss': 0.040386987550715105, 'train/mean_average_precision': 0.210961898217981, 'validation/accuracy': 0.9854498675415501, 'validation/loss': 0.04978688557323292, 'validation/mean_average_precision': 0.18104681565476385, 'validation/num_examples': 43793, 'test/accuracy': 0.9845392066966585, 'test/loss': 0.05235059013587319, 'test/mean_average_precision': 0.18245284220669938, 'test/num_examples': 43793, 'score': 2884.0057315826416, 'total_duration': 3850.313273668289, 'accumulated_submission_time': 2884.0057315826416, 'accumulated_eval_time': 963.3360526561737, 'accumulated_logging_time': 0.2935457229614258, 'global_step': 9676, 'preemption_count': 0}), (10478, {'train/accuracy': 0.9884220678419076, 'train/loss': 0.040165381850757895, 'train/mean_average_precision': 0.21542770154275964, 'validation/accuracy': 0.9853889764367663, 'validation/loss': 0.050279282126349856, 'validation/mean_average_precision': 0.18691448781972173, 'validation/num_examples': 43793, 'test/accuracy': 0.9844966660334706, 'test/loss': 0.053078250022428615, 'test/mean_average_precision': 0.1865348480134821, 'test/num_examples': 43793, 'score': 3123.8959336280823, 'total_duration': 4160.217764139175, 'accumulated_submission_time': 3123.8959336280823, 'accumulated_eval_time': 1033.1087353229523, 'accumulated_logging_time': 0.3154017925262451, 'global_step': 10478, 'preemption_count': 0}), (11284, {'train/accuracy': 0.9887422171578968, 'train/loss': 0.03862924690383424, 'train/mean_average_precision': 0.23136382798199412, 'validation/accuracy': 0.9857734022783016, 'validation/loss': 0.04859289980794945, 'validation/mean_average_precision': 0.20001616596405883, 'validation/num_examples': 43793, 'test/accuracy': 0.9848273038612181, 'test/loss': 0.05134470154460515, 'test/mean_average_precision': 0.19948189466971802, 'test/num_examples': 43793, 'score': 3363.82128572464, 'total_duration': 4470.200117111206, 'accumulated_submission_time': 3363.82128572464, 'accumulated_eval_time': 1102.9219598770142, 'accumulated_logging_time': 0.33776330947875977, 'global_step': 11284, 'preemption_count': 0}), (12000, {'train/accuracy': 0.98876296297752, 'train/loss': 0.03837907263983901, 'train/mean_average_precision': 0.231671907104484, 'validation/accuracy': 0.9857047983002452, 'validation/loss': 0.04894342325833173, 'validation/mean_average_precision': 0.19742444338557977, 'validation/num_examples': 43793, 'test/accuracy': 0.984796556649211, 'test/loss': 0.051622476148797974, 'test/mean_average_precision': 0.19714546284128223, 'test/num_examples': 43793, 'score': 3574.7923941612244, 'total_duration': 4752.32505440712, 'accumulated_submission_time': 3574.7923941612244, 'accumulated_eval_time': 1173.859403371811, 'accumulated_logging_time': 0.35981321334838867, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0609 03:46:13.143934 140321551468352 submission_runner.py:584] Timing: 3574.7923941612244
I0609 03:46:13.143991 140321551468352 submission_runner.py:586] Total number of evals: 16
I0609 03:46:13.144047 140321551468352 submission_runner.py:587] ====================
I0609 03:46:13.144185 140321551468352 submission_runner.py:655] Final ogbg score: 3574.7923941612244
