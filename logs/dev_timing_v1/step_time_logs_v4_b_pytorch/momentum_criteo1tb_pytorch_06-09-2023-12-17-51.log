torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_pytorch_06-09-2023-12-17-51.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 12:18:14.720128 140066302445376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 12:18:14.720168 140672609740608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 12:18:14.721072 140272822896448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 12:18:14.721166 139674610620224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 12:18:14.721322 139999379097408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 12:18:14.721360 139704872204096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 12:18:14.721568 139872871233344 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 12:18:14.731815 139674610620224 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.731808 140567472187200 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 12:18:14.731963 139999379097408 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.731976 139704872204096 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.732103 140567472187200 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.732087 139872871233344 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.741010 140066302445376 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.741041 140672609740608 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.741960 140272822896448 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 12:18:14.752479 140567472187200 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/momentum/criteo1tb_pytorch.
W0609 12:18:14.881717 140567472187200 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.883366 140672609740608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.883688 139872871233344 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.883899 140066302445376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.884240 139674610620224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.885813 140272822896448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.886147 139704872204096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 12:18:14.886620 139999379097408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 12:18:14.888314 140567472187200 submission_runner.py:541] Using RNG seed 3188097513
I0609 12:18:14.890064 140567472187200 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 12:18:14.890200 140567472187200 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/momentum/criteo1tb_pytorch/trial_1.
I0609 12:18:14.890462 140567472187200 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/momentum/criteo1tb_pytorch/trial_1/hparams.json.
I0609 12:18:14.891547 140567472187200 submission_runner.py:255] Initializing dataset.
I0609 12:18:14.891685 140567472187200 submission_runner.py:262] Initializing model.
I0609 12:18:28.223400 140567472187200 submission_runner.py:272] Initializing optimizer.
I0609 12:18:28.704802 140567472187200 submission_runner.py:279] Initializing metrics bundle.
I0609 12:18:28.705003 140567472187200 submission_runner.py:297] Initializing checkpoint and logger.
I0609 12:18:28.709011 140567472187200 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 12:18:28.709141 140567472187200 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 12:18:29.173618 140567472187200 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/momentum/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0609 12:18:29.174637 140567472187200 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/momentum/criteo1tb_pytorch/trial_1/flags_0.json.
I0609 12:18:29.227720 140567472187200 submission_runner.py:332] Starting training loop.
I0609 12:18:34.869785 140528612013824 logging_writer.py:48] [0] global_step=0, grad_norm=3.185164, loss=0.326934
I0609 12:18:34.877806 140567472187200 spec.py:298] Evaluating on the training split.
I0609 12:23:21.459674 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 12:28:02.377391 140567472187200 spec.py:326] Evaluating on the test split.
I0609 12:32:44.101644 140567472187200 submission_runner.py:419] Time since start: 854.87s, 	Step: 1, 	{'train/loss': 0.3292841065710656, 'validation/loss': 0.32778004494382024, 'validation/num_examples': 89000000, 'test/loss': 0.3298541779565007, 'test/num_examples': 89274637, 'score': 5.6501243114471436, 'total_duration': 854.8743205070496, 'accumulated_submission_time': 5.6501243114471436, 'accumulated_eval_time': 849.2237873077393, 'accumulated_logging_time': 0}
I0609 12:32:44.118709 140502775084800 logging_writer.py:48] [1] accumulated_eval_time=849.223787, accumulated_logging_time=0, accumulated_submission_time=5.650124, global_step=1, preemption_count=0, score=5.650124, test/loss=0.329854, test/num_examples=89274637, total_duration=854.874321, train/loss=0.329284, validation/loss=0.327780, validation/num_examples=89000000
I0609 12:32:44.142869 140567472187200 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142891 139674610620224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142889 140066302445376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142901 140272822896448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142904 140672609740608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142910 139999379097408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142900 139704872204096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:44.142924 139872871233344 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:32:45.306775 140502766692096 logging_writer.py:48] [1] global_step=1, grad_norm=3.195343, loss=0.326048
I0609 12:32:46.477076 140502775084800 logging_writer.py:48] [2] global_step=2, grad_norm=2.972187, loss=0.311445
I0609 12:32:47.624525 140502766692096 logging_writer.py:48] [3] global_step=3, grad_norm=2.365359, loss=0.274680
I0609 12:32:48.830407 140502775084800 logging_writer.py:48] [4] global_step=4, grad_norm=1.530418, loss=0.226458
I0609 12:32:49.983127 140502766692096 logging_writer.py:48] [5] global_step=5, grad_norm=0.939526, loss=0.189209
I0609 12:32:51.135159 140502775084800 logging_writer.py:48] [6] global_step=6, grad_norm=0.598418, loss=0.165845
I0609 12:32:52.240435 140502766692096 logging_writer.py:48] [7] global_step=7, grad_norm=0.343281, loss=0.152014
I0609 12:32:53.348010 140502775084800 logging_writer.py:48] [8] global_step=8, grad_norm=0.125041, loss=0.144451
I0609 12:32:54.452985 140502766692096 logging_writer.py:48] [9] global_step=9, grad_norm=0.112963, loss=0.142304
I0609 12:32:55.597565 140502775084800 logging_writer.py:48] [10] global_step=10, grad_norm=0.310990, loss=0.147560
I0609 12:32:56.717715 140502766692096 logging_writer.py:48] [11] global_step=11, grad_norm=0.479897, loss=0.156373
I0609 12:32:57.826478 140502775084800 logging_writer.py:48] [12] global_step=12, grad_norm=0.588120, loss=0.162192
I0609 12:32:58.928249 140502766692096 logging_writer.py:48] [13] global_step=13, grad_norm=0.666274, loss=0.169781
I0609 12:33:00.037214 140502775084800 logging_writer.py:48] [14] global_step=14, grad_norm=0.656461, loss=0.166477
I0609 12:33:01.180390 140502766692096 logging_writer.py:48] [15] global_step=15, grad_norm=0.625083, loss=0.164974
I0609 12:33:02.348604 140502775084800 logging_writer.py:48] [16] global_step=16, grad_norm=0.493974, loss=0.155210
I0609 12:33:03.454254 140502766692096 logging_writer.py:48] [17] global_step=17, grad_norm=0.315981, loss=0.144797
I0609 12:33:04.549834 140502775084800 logging_writer.py:48] [18] global_step=18, grad_norm=0.118633, loss=0.139097
I0609 12:33:05.696953 140502766692096 logging_writer.py:48] [19] global_step=19, grad_norm=0.084231, loss=0.141723
I0609 12:33:06.804612 140502775084800 logging_writer.py:48] [20] global_step=20, grad_norm=0.214435, loss=0.142998
I0609 12:33:07.905056 140502766692096 logging_writer.py:48] [21] global_step=21, grad_norm=0.290854, loss=0.146998
I0609 12:33:09.037441 140502775084800 logging_writer.py:48] [22] global_step=22, grad_norm=0.317080, loss=0.151143
I0609 12:33:10.168892 140502766692096 logging_writer.py:48] [23] global_step=23, grad_norm=0.316494, loss=0.151257
I0609 12:33:11.270539 140502775084800 logging_writer.py:48] [24] global_step=24, grad_norm=0.292059, loss=0.147728
I0609 12:33:12.377544 140502766692096 logging_writer.py:48] [25] global_step=25, grad_norm=0.221696, loss=0.144228
I0609 12:33:13.473695 140502775084800 logging_writer.py:48] [26] global_step=26, grad_norm=0.115135, loss=0.139819
I0609 12:33:14.582792 140502766692096 logging_writer.py:48] [27] global_step=27, grad_norm=0.052406, loss=0.141064
I0609 12:33:15.692789 140502775084800 logging_writer.py:48] [28] global_step=28, grad_norm=0.189330, loss=0.142491
I0609 12:33:16.811006 140502766692096 logging_writer.py:48] [29] global_step=29, grad_norm=0.281786, loss=0.142230
I0609 12:33:17.914656 140502775084800 logging_writer.py:48] [30] global_step=30, grad_norm=0.338329, loss=0.144467
I0609 12:33:19.033539 140502766692096 logging_writer.py:48] [31] global_step=31, grad_norm=0.326179, loss=0.144290
I0609 12:33:20.132928 140502775084800 logging_writer.py:48] [32] global_step=32, grad_norm=0.236514, loss=0.138105
I0609 12:33:21.243353 140502766692096 logging_writer.py:48] [33] global_step=33, grad_norm=0.147594, loss=0.139713
I0609 12:33:22.351870 140502775084800 logging_writer.py:48] [34] global_step=34, grad_norm=0.017724, loss=0.133361
I0609 12:33:23.457328 140502766692096 logging_writer.py:48] [35] global_step=35, grad_norm=0.096841, loss=0.135513
I0609 12:33:24.561199 140502775084800 logging_writer.py:48] [36] global_step=36, grad_norm=0.156868, loss=0.137280
I0609 12:33:25.666310 140502766692096 logging_writer.py:48] [37] global_step=37, grad_norm=0.181989, loss=0.140265
I0609 12:33:26.807523 140502775084800 logging_writer.py:48] [38] global_step=38, grad_norm=0.189821, loss=0.137756
I0609 12:33:27.933450 140502766692096 logging_writer.py:48] [39] global_step=39, grad_norm=0.174143, loss=0.135665
I0609 12:33:29.075211 140502775084800 logging_writer.py:48] [40] global_step=40, grad_norm=0.131315, loss=0.134050
I0609 12:33:30.223392 140502766692096 logging_writer.py:48] [41] global_step=41, grad_norm=0.064954, loss=0.133838
I0609 12:33:31.342569 140502775084800 logging_writer.py:48] [42] global_step=42, grad_norm=0.019496, loss=0.132824
I0609 12:33:32.445125 140502766692096 logging_writer.py:48] [43] global_step=43, grad_norm=0.078944, loss=0.132202
I0609 12:33:33.555898 140502775084800 logging_writer.py:48] [44] global_step=44, grad_norm=0.141486, loss=0.134863
I0609 12:33:34.669262 140502766692096 logging_writer.py:48] [45] global_step=45, grad_norm=0.168875, loss=0.134784
I0609 12:33:35.783108 140502775084800 logging_writer.py:48] [46] global_step=46, grad_norm=0.144910, loss=0.132068
I0609 12:33:36.895743 140502766692096 logging_writer.py:48] [47] global_step=47, grad_norm=0.123884, loss=0.135372
I0609 12:33:38.044241 140502775084800 logging_writer.py:48] [48] global_step=48, grad_norm=0.068427, loss=0.133755
I0609 12:33:39.173665 140502766692096 logging_writer.py:48] [49] global_step=49, grad_norm=0.013294, loss=0.133927
I0609 12:33:40.269741 140502775084800 logging_writer.py:48] [50] global_step=50, grad_norm=0.055674, loss=0.133386
I0609 12:33:41.372984 140502766692096 logging_writer.py:48] [51] global_step=51, grad_norm=0.084715, loss=0.134508
I0609 12:33:42.489537 140502775084800 logging_writer.py:48] [52] global_step=52, grad_norm=0.112089, loss=0.132570
I0609 12:33:43.588705 140502766692096 logging_writer.py:48] [53] global_step=53, grad_norm=0.109464, loss=0.133631
I0609 12:33:44.735306 140502775084800 logging_writer.py:48] [54] global_step=54, grad_norm=0.088258, loss=0.134525
I0609 12:33:45.833248 140502766692096 logging_writer.py:48] [55] global_step=55, grad_norm=0.060714, loss=0.132996
I0609 12:33:46.937187 140502775084800 logging_writer.py:48] [56] global_step=56, grad_norm=0.021469, loss=0.132824
I0609 12:33:48.040603 140502766692096 logging_writer.py:48] [57] global_step=57, grad_norm=0.016584, loss=0.129266
I0609 12:33:49.148738 140502775084800 logging_writer.py:48] [58] global_step=58, grad_norm=0.062973, loss=0.131224
I0609 12:33:50.265514 140502766692096 logging_writer.py:48] [59] global_step=59, grad_norm=0.118607, loss=0.139536
I0609 12:33:51.400443 140502775084800 logging_writer.py:48] [60] global_step=60, grad_norm=0.111484, loss=0.138496
I0609 12:33:52.531758 140502766692096 logging_writer.py:48] [61] global_step=61, grad_norm=0.100523, loss=0.141754
I0609 12:33:53.650408 140502775084800 logging_writer.py:48] [62] global_step=62, grad_norm=0.049377, loss=0.139949
I0609 12:33:54.745138 140502766692096 logging_writer.py:48] [63] global_step=63, grad_norm=0.011489, loss=0.140378
I0609 12:33:55.849617 140502775084800 logging_writer.py:48] [64] global_step=64, grad_norm=0.050134, loss=0.138296
I0609 12:33:56.966192 140502766692096 logging_writer.py:48] [65] global_step=65, grad_norm=0.066883, loss=0.141269
I0609 12:33:58.096489 140502775084800 logging_writer.py:48] [66] global_step=66, grad_norm=0.083675, loss=0.140659
I0609 12:33:59.209530 140502766692096 logging_writer.py:48] [67] global_step=67, grad_norm=0.083883, loss=0.139134
I0609 12:34:00.321390 140502775084800 logging_writer.py:48] [68] global_step=68, grad_norm=0.052828, loss=0.142744
I0609 12:34:01.436487 140502766692096 logging_writer.py:48] [69] global_step=69, grad_norm=0.038917, loss=0.137989
I0609 12:34:02.632003 140502775084800 logging_writer.py:48] [70] global_step=70, grad_norm=0.009592, loss=0.140090
I0609 12:34:03.792967 140502766692096 logging_writer.py:48] [71] global_step=71, grad_norm=0.042950, loss=0.140841
I0609 12:34:04.907294 140502775084800 logging_writer.py:48] [72] global_step=72, grad_norm=0.062292, loss=0.139273
I0609 12:34:06.048953 140502766692096 logging_writer.py:48] [73] global_step=73, grad_norm=0.078954, loss=0.141544
I0609 12:34:07.159694 140502775084800 logging_writer.py:48] [74] global_step=74, grad_norm=0.060833, loss=0.138730
I0609 12:34:08.283217 140502766692096 logging_writer.py:48] [75] global_step=75, grad_norm=0.035887, loss=0.138792
I0609 12:34:09.406546 140502775084800 logging_writer.py:48] [76] global_step=76, grad_norm=0.007811, loss=0.136443
I0609 12:34:10.532317 140502766692096 logging_writer.py:48] [77] global_step=77, grad_norm=0.035246, loss=0.135423
I0609 12:34:11.678896 140502775084800 logging_writer.py:48] [78] global_step=78, grad_norm=0.055369, loss=0.135231
I0609 12:34:12.782727 140502766692096 logging_writer.py:48] [79] global_step=79, grad_norm=0.056972, loss=0.135762
I0609 12:34:13.908713 140502775084800 logging_writer.py:48] [80] global_step=80, grad_norm=0.045202, loss=0.136658
I0609 12:34:15.023676 140502766692096 logging_writer.py:48] [81] global_step=81, grad_norm=0.016295, loss=0.138476
I0609 12:34:16.125818 140502775084800 logging_writer.py:48] [82] global_step=82, grad_norm=0.006336, loss=0.136004
I0609 12:34:17.235072 140502766692096 logging_writer.py:48] [83] global_step=83, grad_norm=0.028868, loss=0.137365
I0609 12:34:18.352046 140502775084800 logging_writer.py:48] [84] global_step=84, grad_norm=0.044587, loss=0.137659
I0609 12:34:19.454286 140502766692096 logging_writer.py:48] [85] global_step=85, grad_norm=0.038860, loss=0.135754
I0609 12:34:20.572249 140502775084800 logging_writer.py:48] [86] global_step=86, grad_norm=0.032692, loss=0.135899
I0609 12:34:21.675307 140502766692096 logging_writer.py:48] [87] global_step=87, grad_norm=0.022245, loss=0.137441
I0609 12:34:22.779129 140502775084800 logging_writer.py:48] [88] global_step=88, grad_norm=0.006263, loss=0.137133
I0609 12:34:23.889415 140502766692096 logging_writer.py:48] [89] global_step=89, grad_norm=0.026789, loss=0.135485
I0609 12:34:25.015496 140502775084800 logging_writer.py:48] [90] global_step=90, grad_norm=0.039969, loss=0.135029
I0609 12:34:26.162687 140502766692096 logging_writer.py:48] [91] global_step=91, grad_norm=0.035706, loss=0.135208
I0609 12:34:27.290242 140502775084800 logging_writer.py:48] [92] global_step=92, grad_norm=0.023154, loss=0.136249
I0609 12:34:28.457489 140502766692096 logging_writer.py:48] [93] global_step=93, grad_norm=0.011303, loss=0.135148
I0609 12:34:29.562412 140502775084800 logging_writer.py:48] [94] global_step=94, grad_norm=0.011106, loss=0.134876
I0609 12:34:30.666713 140502766692096 logging_writer.py:48] [95] global_step=95, grad_norm=0.025919, loss=0.134145
I0609 12:34:31.767380 140502775084800 logging_writer.py:48] [96] global_step=96, grad_norm=0.027061, loss=0.133196
I0609 12:34:32.875655 140502766692096 logging_writer.py:48] [97] global_step=97, grad_norm=0.024142, loss=0.133518
I0609 12:34:33.981030 140502775084800 logging_writer.py:48] [98] global_step=98, grad_norm=0.021682, loss=0.134504
I0609 12:34:35.085433 140502766692096 logging_writer.py:48] [99] global_step=99, grad_norm=0.008248, loss=0.133300
I0609 12:34:36.198251 140502775084800 logging_writer.py:48] [100] global_step=100, grad_norm=0.015513, loss=0.133164
I0609 12:34:45.150289 140567472187200 spec.py:298] Evaluating on the training split.
I0609 12:39:19.354103 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 12:43:52.171333 140567472187200 spec.py:326] Evaluating on the test split.
I0609 12:48:10.424175 140567472187200 submission_runner.py:419] Time since start: 1781.20s, 	Step: 109, 	{'train/loss': 0.13678408546251264, 'validation/loss': 0.13845903370786516, 'validation/num_examples': 89000000, 'test/loss': 0.14208774660153478, 'test/num_examples': 89274637, 'score': 126.63603210449219, 'total_duration': 1781.196810245514, 'accumulated_submission_time': 126.63603210449219, 'accumulated_eval_time': 1654.4975144863129, 'accumulated_logging_time': 0.024366378784179688}
I0609 12:48:10.434911 140502766692096 logging_writer.py:48] [109] accumulated_eval_time=1654.497514, accumulated_logging_time=0.024366, accumulated_submission_time=126.636032, global_step=109, preemption_count=0, score=126.636032, test/loss=0.142088, test/num_examples=89274637, total_duration=1781.196810, train/loss=0.136784, validation/loss=0.138459, validation/num_examples=89000000
I0609 12:50:11.295162 140567472187200 spec.py:298] Evaluating on the training split.
I0609 12:54:53.128861 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 12:59:24.560734 140567472187200 spec.py:326] Evaluating on the test split.
I0609 13:04:06.476787 140567472187200 submission_runner.py:419] Time since start: 2737.25s, 	Step: 218, 	{'train/loss': 0.13780526785047023, 'validation/loss': 0.13739034831460675, 'validation/num_examples': 89000000, 'test/loss': 0.14115231854709193, 'test/num_examples': 89274637, 'score': 247.44577646255493, 'total_duration': 2737.2494509220123, 'accumulated_submission_time': 247.44577646255493, 'accumulated_eval_time': 2489.6790239810944, 'accumulated_logging_time': 0.04213571548461914}
I0609 13:04:06.486396 140502775084800 logging_writer.py:48] [218] accumulated_eval_time=2489.679024, accumulated_logging_time=0.042136, accumulated_submission_time=247.445776, global_step=218, preemption_count=0, score=247.445776, test/loss=0.141152, test/num_examples=89274637, total_duration=2737.249451, train/loss=0.137805, validation/loss=0.137390, validation/num_examples=89000000
I0609 13:06:07.071334 140567472187200 spec.py:298] Evaluating on the training split.
I0609 13:10:41.425479 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 13:15:15.269209 140567472187200 spec.py:326] Evaluating on the test split.
I0609 13:19:43.207905 140567472187200 submission_runner.py:419] Time since start: 3673.98s, 	Step: 326, 	{'train/loss': 0.13801794774923543, 'validation/loss': 0.13672240449438203, 'validation/num_examples': 89000000, 'test/loss': 0.14050819383337285, 'test/num_examples': 89274637, 'score': 367.9774475097656, 'total_duration': 3673.980553627014, 'accumulated_submission_time': 367.9774475097656, 'accumulated_eval_time': 3305.8154628276825, 'accumulated_logging_time': 0.05950331687927246}
I0609 13:19:43.217614 140502766692096 logging_writer.py:48] [326] accumulated_eval_time=3305.815463, accumulated_logging_time=0.059503, accumulated_submission_time=367.977448, global_step=326, preemption_count=0, score=367.977448, test/loss=0.140508, test/num_examples=89274637, total_duration=3673.980554, train/loss=0.138018, validation/loss=0.136722, validation/num_examples=89000000
I0609 13:21:43.488202 140567472187200 spec.py:298] Evaluating on the training split.
I0609 13:26:16.152077 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 13:30:48.232620 140567472187200 spec.py:326] Evaluating on the test split.
I0609 13:35:28.569674 140567472187200 submission_runner.py:419] Time since start: 4619.34s, 	Step: 434, 	{'train/loss': 0.13573192515032884, 'validation/loss': 0.1364168202247191, 'validation/num_examples': 89000000, 'test/loss': 0.14026599738512519, 'test/num_examples': 89274637, 'score': 488.1965391635895, 'total_duration': 4619.342343568802, 'accumulated_submission_time': 488.1965391635895, 'accumulated_eval_time': 4130.896869421005, 'accumulated_logging_time': 0.07708144187927246}
I0609 13:35:28.579521 140502775084800 logging_writer.py:48] [434] accumulated_eval_time=4130.896869, accumulated_logging_time=0.077081, accumulated_submission_time=488.196539, global_step=434, preemption_count=0, score=488.196539, test/loss=0.140266, test/num_examples=89274637, total_duration=4619.342344, train/loss=0.135732, validation/loss=0.136417, validation/num_examples=89000000
I0609 13:36:43.237928 140502766692096 logging_writer.py:48] [500] global_step=500, grad_norm=0.010935, loss=0.134962
I0609 13:37:28.738530 140567472187200 spec.py:298] Evaluating on the training split.
I0609 13:42:11.110169 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 13:46:45.386389 140567472187200 spec.py:326] Evaluating on the test split.
I0609 13:51:14.273802 140567472187200 submission_runner.py:419] Time since start: 5565.05s, 	Step: 542, 	{'train/loss': 0.1351669008368688, 'validation/loss': 0.13537720224719102, 'validation/num_examples': 89000000, 'test/loss': 0.1390642226862261, 'test/num_examples': 89274637, 'score': 608.3043968677521, 'total_duration': 5565.046504020691, 'accumulated_submission_time': 608.3043968677521, 'accumulated_eval_time': 4956.432088136673, 'accumulated_logging_time': 0.09420967102050781}
I0609 13:51:14.283839 140502775084800 logging_writer.py:48] [542] accumulated_eval_time=4956.432088, accumulated_logging_time=0.094210, accumulated_submission_time=608.304397, global_step=542, preemption_count=0, score=608.304397, test/loss=0.139064, test/num_examples=89274637, total_duration=5565.046504, train/loss=0.135167, validation/loss=0.135377, validation/num_examples=89000000
I0609 13:53:15.067321 140567472187200 spec.py:298] Evaluating on the training split.
I0609 13:57:45.754747 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 14:02:22.690786 140567472187200 spec.py:326] Evaluating on the test split.
I0609 14:07:03.612243 140567472187200 submission_runner.py:419] Time since start: 6514.38s, 	Step: 628, 	{'train/loss': 0.13258675076314155, 'validation/loss': 0.13504939325842696, 'validation/num_examples': 89000000, 'test/loss': 0.13875011331605863, 'test/num_examples': 89274637, 'score': 729.0434608459473, 'total_duration': 6514.384889125824, 'accumulated_submission_time': 729.0434608459473, 'accumulated_eval_time': 5784.97686958313, 'accumulated_logging_time': 0.11249256134033203}
I0609 14:07:03.623589 140502766692096 logging_writer.py:48] [628] accumulated_eval_time=5784.976870, accumulated_logging_time=0.112493, accumulated_submission_time=729.043461, global_step=628, preemption_count=0, score=729.043461, test/loss=0.138750, test/num_examples=89274637, total_duration=6514.384889, train/loss=0.132587, validation/loss=0.135049, validation/num_examples=89000000
I0609 14:09:04.115529 140567472187200 spec.py:298] Evaluating on the training split.
I0609 14:13:44.838996 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 14:18:20.242866 140567472187200 spec.py:326] Evaluating on the test split.
I0609 14:22:59.483518 140567472187200 submission_runner.py:419] Time since start: 7470.26s, 	Step: 721, 	{'train/loss': 0.13326464969503082, 'validation/loss': 0.1344388651685393, 'validation/num_examples': 89000000, 'test/loss': 0.13815556595318332, 'test/num_examples': 89274637, 'score': 849.4898755550385, 'total_duration': 7470.256201028824, 'accumulated_submission_time': 849.4898755550385, 'accumulated_eval_time': 6620.344753026962, 'accumulated_logging_time': 0.13146352767944336}
I0609 14:22:59.494432 140502775084800 logging_writer.py:48] [721] accumulated_eval_time=6620.344753, accumulated_logging_time=0.131464, accumulated_submission_time=849.489876, global_step=721, preemption_count=0, score=849.489876, test/loss=0.138156, test/num_examples=89274637, total_duration=7470.256201, train/loss=0.133265, validation/loss=0.134439, validation/num_examples=89000000
I0609 14:25:00.082599 140567472187200 spec.py:298] Evaluating on the training split.
I0609 14:29:33.037770 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 14:34:07.616805 140567472187200 spec.py:326] Evaluating on the test split.
I0609 14:38:47.097830 140567472187200 submission_runner.py:419] Time since start: 8417.87s, 	Step: 824, 	{'train/loss': 0.132687151125023, 'validation/loss': 0.13429255056179776, 'validation/num_examples': 89000000, 'test/loss': 0.13759126234251728, 'test/num_examples': 89274637, 'score': 970.0306107997894, 'total_duration': 8417.870494365692, 'accumulated_submission_time': 970.0306107997894, 'accumulated_eval_time': 7447.35987329483, 'accumulated_logging_time': 0.14932799339294434}
I0609 14:38:47.108034 140502766692096 logging_writer.py:48] [824] accumulated_eval_time=7447.359873, accumulated_logging_time=0.149328, accumulated_submission_time=970.030611, global_step=824, preemption_count=0, score=970.030611, test/loss=0.137591, test/num_examples=89274637, total_duration=8417.870494, train/loss=0.132687, validation/loss=0.134293, validation/num_examples=89000000
I0609 14:40:47.984226 140567472187200 spec.py:298] Evaluating on the training split.
I0609 14:45:22.566200 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 14:49:58.092919 140567472187200 spec.py:326] Evaluating on the test split.
I0609 14:54:27.025980 140567472187200 submission_runner.py:419] Time since start: 9357.80s, 	Step: 928, 	{'train/loss': 0.1322659119864963, 'validation/loss': 0.13236962921348314, 'validation/num_examples': 89000000, 'test/loss': 0.13548131257033283, 'test/num_examples': 89274637, 'score': 1090.85812997818, 'total_duration': 9357.798676490784, 'accumulated_submission_time': 1090.85812997818, 'accumulated_eval_time': 8266.40153336525, 'accumulated_logging_time': 0.16714191436767578}
I0609 14:54:27.036088 140502775084800 logging_writer.py:48] [928] accumulated_eval_time=8266.401533, accumulated_logging_time=0.167142, accumulated_submission_time=1090.858130, global_step=928, preemption_count=0, score=1090.858130, test/loss=0.135481, test/num_examples=89274637, total_duration=9357.798676, train/loss=0.132266, validation/loss=0.132370, validation/num_examples=89000000
I0609 14:55:49.159578 140502766692096 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.012628, loss=0.128296
I0609 14:56:27.986103 140567472187200 spec.py:298] Evaluating on the training split.
I0609 15:01:08.430765 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 15:05:43.004468 140567472187200 spec.py:326] Evaluating on the test split.
I0609 15:10:21.787790 140567472187200 submission_runner.py:419] Time since start: 10312.56s, 	Step: 1036, 	{'train/loss': 0.12846889874793047, 'validation/loss': 0.1311619775280899, 'validation/num_examples': 89000000, 'test/loss': 0.1341715900788261, 'test/num_examples': 89274637, 'score': 1211.7582137584686, 'total_duration': 10312.560465335846, 'accumulated_submission_time': 1211.7582137584686, 'accumulated_eval_time': 9100.203099012375, 'accumulated_logging_time': 0.1853163242340088}
I0609 15:10:21.798012 140502775084800 logging_writer.py:48] [1036] accumulated_eval_time=9100.203099, accumulated_logging_time=0.185316, accumulated_submission_time=1211.758214, global_step=1036, preemption_count=0, score=1211.758214, test/loss=0.134172, test/num_examples=89274637, total_duration=10312.560465, train/loss=0.128469, validation/loss=0.131162, validation/num_examples=89000000
I0609 15:12:22.097016 140567472187200 spec.py:298] Evaluating on the training split.
I0609 15:16:54.987207 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 15:21:30.374932 140567472187200 spec.py:326] Evaluating on the test split.
I0609 15:26:13.422187 140567472187200 submission_runner.py:419] Time since start: 11264.19s, 	Step: 1145, 	{'train/loss': 0.13010045515429544, 'validation/loss': 0.13116803370786517, 'validation/num_examples': 89000000, 'test/loss': 0.13413492793031462, 'test/num_examples': 89274637, 'score': 1332.007820367813, 'total_duration': 11264.19484782219, 'accumulated_submission_time': 1332.007820367813, 'accumulated_eval_time': 9931.528153181076, 'accumulated_logging_time': 0.20362210273742676}
I0609 15:26:13.432302 140502766692096 logging_writer.py:48] [1145] accumulated_eval_time=9931.528153, accumulated_logging_time=0.203622, accumulated_submission_time=1332.007820, global_step=1145, preemption_count=0, score=1332.007820, test/loss=0.134135, test/num_examples=89274637, total_duration=11264.194848, train/loss=0.130100, validation/loss=0.131168, validation/num_examples=89000000
I0609 15:28:14.118775 140567472187200 spec.py:298] Evaluating on the training split.
I0609 15:32:57.017544 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 15:37:33.361568 140567472187200 spec.py:326] Evaluating on the test split.
I0609 15:42:00.505286 140567472187200 submission_runner.py:419] Time since start: 12211.28s, 	Step: 1229, 	{'train/loss': 0.12959004360757564, 'validation/loss': 0.13028903370786518, 'validation/num_examples': 89000000, 'test/loss': 0.13305039817748013, 'test/num_examples': 89274637, 'score': 1452.6532094478607, 'total_duration': 12211.277970552444, 'accumulated_submission_time': 1452.6532094478607, 'accumulated_eval_time': 10757.914537668228, 'accumulated_logging_time': 0.22098135948181152}
I0609 15:42:00.515581 140502775084800 logging_writer.py:48] [1229] accumulated_eval_time=10757.914538, accumulated_logging_time=0.220981, accumulated_submission_time=1452.653209, global_step=1229, preemption_count=0, score=1452.653209, test/loss=0.133050, test/num_examples=89274637, total_duration=12211.277971, train/loss=0.129590, validation/loss=0.130289, validation/num_examples=89000000
I0609 15:44:00.758253 140567472187200 spec.py:298] Evaluating on the training split.
I0609 15:48:34.384998 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 15:53:06.094564 140567472187200 spec.py:326] Evaluating on the test split.
I0609 15:57:46.642578 140567472187200 submission_runner.py:419] Time since start: 13157.42s, 	Step: 1319, 	{'train/loss': 0.1314644157404054, 'validation/loss': 0.13333578651685393, 'validation/num_examples': 89000000, 'test/loss': 0.13546764687489013, 'test/num_examples': 89274637, 'score': 1572.853505373001, 'total_duration': 13157.415254116058, 'accumulated_submission_time': 1572.853505373001, 'accumulated_eval_time': 11583.79873752594, 'accumulated_logging_time': 0.23812127113342285}
I0609 15:57:46.654305 140502766692096 logging_writer.py:48] [1319] accumulated_eval_time=11583.798738, accumulated_logging_time=0.238121, accumulated_submission_time=1572.853505, global_step=1319, preemption_count=0, score=1572.853505, test/loss=0.135468, test/num_examples=89274637, total_duration=13157.415254, train/loss=0.131464, validation/loss=0.133336, validation/num_examples=89000000
I0609 15:59:47.267235 140567472187200 spec.py:298] Evaluating on the training split.
I0609 16:04:30.897161 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 16:09:03.736013 140567472187200 spec.py:326] Evaluating on the test split.
I0609 16:13:31.523709 140567472187200 submission_runner.py:419] Time since start: 14102.30s, 	Step: 1413, 	{'train/loss': 0.12903183466200446, 'validation/loss': 0.12982780898876403, 'validation/num_examples': 89000000, 'test/loss': 0.13230120442830812, 'test/num_examples': 89274637, 'score': 1693.4214088916779, 'total_duration': 14102.29636836052, 'accumulated_submission_time': 1693.4214088916779, 'accumulated_eval_time': 12408.055095672607, 'accumulated_logging_time': 0.2570230960845947}
I0609 16:13:31.533985 140502775084800 logging_writer.py:48] [1413] accumulated_eval_time=12408.055096, accumulated_logging_time=0.257023, accumulated_submission_time=1693.421409, global_step=1413, preemption_count=0, score=1693.421409, test/loss=0.132301, test/num_examples=89274637, total_duration=14102.296368, train/loss=0.129032, validation/loss=0.129828, validation/num_examples=89000000
I0609 16:15:09.485237 140502766692096 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.030845, loss=0.130899
I0609 16:15:31.725204 140567472187200 spec.py:298] Evaluating on the training split.
I0609 16:20:07.700006 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 16:24:42.447952 140567472187200 spec.py:326] Evaluating on the test split.
I0609 16:29:21.847784 140567472187200 submission_runner.py:419] Time since start: 15052.62s, 	Step: 1521, 	{'train/loss': 0.12988049954154018, 'validation/loss': 0.1293346179775281, 'validation/num_examples': 89000000, 'test/loss': 0.13215938363322607, 'test/num_examples': 89274637, 'score': 1813.5632507801056, 'total_duration': 15052.620465993881, 'accumulated_submission_time': 1813.5632507801056, 'accumulated_eval_time': 13238.177546262741, 'accumulated_logging_time': 0.27461791038513184}
I0609 16:29:21.857908 140502775084800 logging_writer.py:48] [1521] accumulated_eval_time=13238.177546, accumulated_logging_time=0.274618, accumulated_submission_time=1813.563251, global_step=1521, preemption_count=0, score=1813.563251, test/loss=0.132159, test/num_examples=89274637, total_duration=15052.620466, train/loss=0.129880, validation/loss=0.129335, validation/num_examples=89000000
I0609 16:31:00.817435 140567472187200 spec.py:298] Evaluating on the training split.
I0609 16:35:43.971133 140567472187200 spec.py:310] Evaluating on the validation split.
I0609 16:40:18.409076 140567472187200 spec.py:326] Evaluating on the test split.
I0609 16:44:51.452601 140567472187200 submission_runner.py:419] Time since start: 15982.23s, 	Step: 1600, 	{'train/loss': 0.1295308835195916, 'validation/loss': 0.12993847191011235, 'validation/num_examples': 89000000, 'test/loss': 0.1330266736340804, 'test/num_examples': 89274637, 'score': 1912.485206604004, 'total_duration': 15982.225271701813, 'accumulated_submission_time': 1912.485206604004, 'accumulated_eval_time': 14068.812608242035, 'accumulated_logging_time': 0.29181337356567383}
I0609 16:44:51.462495 140502766692096 logging_writer.py:48] [1600] accumulated_eval_time=14068.812608, accumulated_logging_time=0.291813, accumulated_submission_time=1912.485207, global_step=1600, preemption_count=0, score=1912.485207, test/loss=0.133027, test/num_examples=89274637, total_duration=15982.225272, train/loss=0.129531, validation/loss=0.129938, validation/num_examples=89000000
I0609 16:44:51.476868 140502775084800 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1912.485207
I0609 16:44:58.774581 140567472187200 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/momentum/criteo1tb_pytorch/trial_1/checkpoint_1600.
I0609 16:44:58.869341 140567472187200 submission_runner.py:581] Tuning trial 1/1
I0609 16:44:58.869617 140567472187200 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 16:44:58.870871 140567472187200 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/loss': 0.3292841065710656, 'validation/loss': 0.32778004494382024, 'validation/num_examples': 89000000, 'test/loss': 0.3298541779565007, 'test/num_examples': 89274637, 'score': 5.6501243114471436, 'total_duration': 854.8743205070496, 'accumulated_submission_time': 5.6501243114471436, 'accumulated_eval_time': 849.2237873077393, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (109, {'train/loss': 0.13678408546251264, 'validation/loss': 0.13845903370786516, 'validation/num_examples': 89000000, 'test/loss': 0.14208774660153478, 'test/num_examples': 89274637, 'score': 126.63603210449219, 'total_duration': 1781.196810245514, 'accumulated_submission_time': 126.63603210449219, 'accumulated_eval_time': 1654.4975144863129, 'accumulated_logging_time': 0.024366378784179688, 'global_step': 109, 'preemption_count': 0}), (218, {'train/loss': 0.13780526785047023, 'validation/loss': 0.13739034831460675, 'validation/num_examples': 89000000, 'test/loss': 0.14115231854709193, 'test/num_examples': 89274637, 'score': 247.44577646255493, 'total_duration': 2737.2494509220123, 'accumulated_submission_time': 247.44577646255493, 'accumulated_eval_time': 2489.6790239810944, 'accumulated_logging_time': 0.04213571548461914, 'global_step': 218, 'preemption_count': 0}), (326, {'train/loss': 0.13801794774923543, 'validation/loss': 0.13672240449438203, 'validation/num_examples': 89000000, 'test/loss': 0.14050819383337285, 'test/num_examples': 89274637, 'score': 367.9774475097656, 'total_duration': 3673.980553627014, 'accumulated_submission_time': 367.9774475097656, 'accumulated_eval_time': 3305.8154628276825, 'accumulated_logging_time': 0.05950331687927246, 'global_step': 326, 'preemption_count': 0}), (434, {'train/loss': 0.13573192515032884, 'validation/loss': 0.1364168202247191, 'validation/num_examples': 89000000, 'test/loss': 0.14026599738512519, 'test/num_examples': 89274637, 'score': 488.1965391635895, 'total_duration': 4619.342343568802, 'accumulated_submission_time': 488.1965391635895, 'accumulated_eval_time': 4130.896869421005, 'accumulated_logging_time': 0.07708144187927246, 'global_step': 434, 'preemption_count': 0}), (542, {'train/loss': 0.1351669008368688, 'validation/loss': 0.13537720224719102, 'validation/num_examples': 89000000, 'test/loss': 0.1390642226862261, 'test/num_examples': 89274637, 'score': 608.3043968677521, 'total_duration': 5565.046504020691, 'accumulated_submission_time': 608.3043968677521, 'accumulated_eval_time': 4956.432088136673, 'accumulated_logging_time': 0.09420967102050781, 'global_step': 542, 'preemption_count': 0}), (628, {'train/loss': 0.13258675076314155, 'validation/loss': 0.13504939325842696, 'validation/num_examples': 89000000, 'test/loss': 0.13875011331605863, 'test/num_examples': 89274637, 'score': 729.0434608459473, 'total_duration': 6514.384889125824, 'accumulated_submission_time': 729.0434608459473, 'accumulated_eval_time': 5784.97686958313, 'accumulated_logging_time': 0.11249256134033203, 'global_step': 628, 'preemption_count': 0}), (721, {'train/loss': 0.13326464969503082, 'validation/loss': 0.1344388651685393, 'validation/num_examples': 89000000, 'test/loss': 0.13815556595318332, 'test/num_examples': 89274637, 'score': 849.4898755550385, 'total_duration': 7470.256201028824, 'accumulated_submission_time': 849.4898755550385, 'accumulated_eval_time': 6620.344753026962, 'accumulated_logging_time': 0.13146352767944336, 'global_step': 721, 'preemption_count': 0}), (824, {'train/loss': 0.132687151125023, 'validation/loss': 0.13429255056179776, 'validation/num_examples': 89000000, 'test/loss': 0.13759126234251728, 'test/num_examples': 89274637, 'score': 970.0306107997894, 'total_duration': 8417.870494365692, 'accumulated_submission_time': 970.0306107997894, 'accumulated_eval_time': 7447.35987329483, 'accumulated_logging_time': 0.14932799339294434, 'global_step': 824, 'preemption_count': 0}), (928, {'train/loss': 0.1322659119864963, 'validation/loss': 0.13236962921348314, 'validation/num_examples': 89000000, 'test/loss': 0.13548131257033283, 'test/num_examples': 89274637, 'score': 1090.85812997818, 'total_duration': 9357.798676490784, 'accumulated_submission_time': 1090.85812997818, 'accumulated_eval_time': 8266.40153336525, 'accumulated_logging_time': 0.16714191436767578, 'global_step': 928, 'preemption_count': 0}), (1036, {'train/loss': 0.12846889874793047, 'validation/loss': 0.1311619775280899, 'validation/num_examples': 89000000, 'test/loss': 0.1341715900788261, 'test/num_examples': 89274637, 'score': 1211.7582137584686, 'total_duration': 10312.560465335846, 'accumulated_submission_time': 1211.7582137584686, 'accumulated_eval_time': 9100.203099012375, 'accumulated_logging_time': 0.1853163242340088, 'global_step': 1036, 'preemption_count': 0}), (1145, {'train/loss': 0.13010045515429544, 'validation/loss': 0.13116803370786517, 'validation/num_examples': 89000000, 'test/loss': 0.13413492793031462, 'test/num_examples': 89274637, 'score': 1332.007820367813, 'total_duration': 11264.19484782219, 'accumulated_submission_time': 1332.007820367813, 'accumulated_eval_time': 9931.528153181076, 'accumulated_logging_time': 0.20362210273742676, 'global_step': 1145, 'preemption_count': 0}), (1229, {'train/loss': 0.12959004360757564, 'validation/loss': 0.13028903370786518, 'validation/num_examples': 89000000, 'test/loss': 0.13305039817748013, 'test/num_examples': 89274637, 'score': 1452.6532094478607, 'total_duration': 12211.277970552444, 'accumulated_submission_time': 1452.6532094478607, 'accumulated_eval_time': 10757.914537668228, 'accumulated_logging_time': 0.22098135948181152, 'global_step': 1229, 'preemption_count': 0}), (1319, {'train/loss': 0.1314644157404054, 'validation/loss': 0.13333578651685393, 'validation/num_examples': 89000000, 'test/loss': 0.13546764687489013, 'test/num_examples': 89274637, 'score': 1572.853505373001, 'total_duration': 13157.415254116058, 'accumulated_submission_time': 1572.853505373001, 'accumulated_eval_time': 11583.79873752594, 'accumulated_logging_time': 0.23812127113342285, 'global_step': 1319, 'preemption_count': 0}), (1413, {'train/loss': 0.12903183466200446, 'validation/loss': 0.12982780898876403, 'validation/num_examples': 89000000, 'test/loss': 0.13230120442830812, 'test/num_examples': 89274637, 'score': 1693.4214088916779, 'total_duration': 14102.29636836052, 'accumulated_submission_time': 1693.4214088916779, 'accumulated_eval_time': 12408.055095672607, 'accumulated_logging_time': 0.2570230960845947, 'global_step': 1413, 'preemption_count': 0}), (1521, {'train/loss': 0.12988049954154018, 'validation/loss': 0.1293346179775281, 'validation/num_examples': 89000000, 'test/loss': 0.13215938363322607, 'test/num_examples': 89274637, 'score': 1813.5632507801056, 'total_duration': 15052.620465993881, 'accumulated_submission_time': 1813.5632507801056, 'accumulated_eval_time': 13238.177546262741, 'accumulated_logging_time': 0.27461791038513184, 'global_step': 1521, 'preemption_count': 0}), (1600, {'train/loss': 0.1295308835195916, 'validation/loss': 0.12993847191011235, 'validation/num_examples': 89000000, 'test/loss': 0.1330266736340804, 'test/num_examples': 89274637, 'score': 1912.485206604004, 'total_duration': 15982.225271701813, 'accumulated_submission_time': 1912.485206604004, 'accumulated_eval_time': 14068.812608242035, 'accumulated_logging_time': 0.29181337356567383, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0609 16:44:58.870997 140567472187200 submission_runner.py:584] Timing: 1912.485206604004
I0609 16:44:58.871047 140567472187200 submission_runner.py:586] Total number of evals: 17
I0609 16:44:58.871090 140567472187200 submission_runner.py:587] ====================
I0609 16:44:58.871177 140567472187200 submission_runner.py:655] Final criteo1tb score: 1912.485206604004
