torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-09-2023-02-32-10.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 02:32:34.052627 140227946252096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 02:32:34.052651 140574337095488 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 02:32:34.052675 139943829493568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 02:32:34.053195 139659236935488 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 02:32:34.053525 140149854992192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 02:32:34.053595 139970131085120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 02:32:34.053692 140635815528256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 02:32:34.054461 140558051338048 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 02:32:34.054846 140558051338048 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.063364 140227946252096 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.063385 140574337095488 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.063409 139943829493568 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.063830 139659236935488 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.064179 140149854992192 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.064304 139970131085120 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:34.064324 140635815528256 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 02:32:35.274493 140635815528256 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nesterov/ogbg_pytorch.
W0609 02:32:35.406793 139970131085120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.410176 140635815528256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.410380 140558051338048 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.410566 139659236935488 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.410913 139943829493568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.411063 140149854992192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.412734 140574337095488 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 02:32:35.412770 140227946252096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 02:32:35.415661 140635815528256 submission_runner.py:541] Using RNG seed 464845062
I0609 02:32:35.416949 140635815528256 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 02:32:35.417066 140635815528256 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nesterov/ogbg_pytorch/trial_1.
I0609 02:32:35.417407 140635815528256 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nesterov/ogbg_pytorch/trial_1/hparams.json.
I0609 02:32:35.418326 140635815528256 submission_runner.py:255] Initializing dataset.
I0609 02:32:35.418440 140635815528256 submission_runner.py:262] Initializing model.
I0609 02:32:39.668759 140635815528256 submission_runner.py:272] Initializing optimizer.
I0609 02:32:40.156660 140635815528256 submission_runner.py:279] Initializing metrics bundle.
I0609 02:32:40.156868 140635815528256 submission_runner.py:297] Initializing checkpoint and logger.
I0609 02:32:40.160411 140635815528256 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 02:32:40.160548 140635815528256 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 02:32:40.652825 140635815528256 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nesterov/ogbg_pytorch/trial_1/meta_data_0.json.
I0609 02:32:40.653835 140635815528256 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nesterov/ogbg_pytorch/trial_1/flags_0.json.
I0609 02:32:40.704572 140635815528256 submission_runner.py:332] Starting training loop.
I0609 02:32:40.942316 140635815528256 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:32:40.948214 140635815528256 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:32:41.080883 140635815528256 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:32:45.766064 140597180471040 logging_writer.py:48] [0] global_step=0, grad_norm=2.988379, loss=0.788133
I0609 02:32:45.775548 140635815528256 submission.py:139] 0) loss = 0.788, grad_norm = 2.988
I0609 02:32:45.776619 140635815528256 spec.py:298] Evaluating on the training split.
I0609 02:32:45.782223 140635815528256 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:32:45.786247 140635815528256 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:32:45.840312 140635815528256 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:33:41.894026 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 02:33:41.897449 140635815528256 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:33:41.902355 140635815528256 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:33:41.959288 140635815528256 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:34:28.524204 140635815528256 spec.py:326] Evaluating on the test split.
I0609 02:34:28.527850 140635815528256 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:34:28.532902 140635815528256 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0609 02:34:28.592128 140635815528256 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0609 02:35:14.149460 140635815528256 submission_runner.py:419] Time since start: 153.45s, 	Step: 1, 	{'train/accuracy': 0.43729867992554, 'train/loss': 0.7881166007644227, 'train/mean_average_precision': 0.023672840894495825, 'validation/accuracy': 0.43877358819914153, 'validation/loss': 0.787225472454082, 'validation/mean_average_precision': 0.026926596716653393, 'validation/num_examples': 43793, 'test/accuracy': 0.4385786532636902, 'test/loss': 0.7869883695511624, 'test/mean_average_precision': 0.028913975699431736, 'test/num_examples': 43793, 'score': 5.072153329849243, 'total_duration': 153.4451286792755, 'accumulated_submission_time': 5.072153329849243, 'accumulated_eval_time': 148.3725233078003, 'accumulated_logging_time': 0}
I0609 02:35:14.167125 140583582107392 logging_writer.py:48] [1] accumulated_eval_time=148.372523, accumulated_logging_time=0, accumulated_submission_time=5.072153, global_step=1, preemption_count=0, score=5.072153, test/accuracy=0.438579, test/loss=0.786988, test/mean_average_precision=0.028914, test/num_examples=43793, total_duration=153.445129, train/accuracy=0.437299, train/loss=0.788117, train/mean_average_precision=0.023673, validation/accuracy=0.438774, validation/loss=0.787225, validation/mean_average_precision=0.026927, validation/num_examples=43793
I0609 02:35:14.449041 140635815528256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456385 139659236935488 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456394 140149854992192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456390 139970131085120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456396 140574337095488 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456491 140227946252096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456527 140558051338048 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.456584 139943829493568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 02:35:14.494539 140583590500096 logging_writer.py:48] [1] global_step=1, grad_norm=2.983773, loss=0.787648
I0609 02:35:14.498617 140635815528256 submission.py:139] 1) loss = 0.788, grad_norm = 2.984
I0609 02:35:14.809517 140583582107392 logging_writer.py:48] [2] global_step=2, grad_norm=2.986831, loss=0.786651
I0609 02:35:14.813307 140635815528256 submission.py:139] 2) loss = 0.787, grad_norm = 2.987
I0609 02:35:15.120586 140583590500096 logging_writer.py:48] [3] global_step=3, grad_norm=2.964463, loss=0.777407
I0609 02:35:15.125439 140635815528256 submission.py:139] 3) loss = 0.777, grad_norm = 2.964
I0609 02:35:15.432466 140583582107392 logging_writer.py:48] [4] global_step=4, grad_norm=2.926391, loss=0.767093
I0609 02:35:15.436514 140635815528256 submission.py:139] 4) loss = 0.767, grad_norm = 2.926
I0609 02:35:15.738437 140583590500096 logging_writer.py:48] [5] global_step=5, grad_norm=2.750655, loss=0.748541
I0609 02:35:15.742505 140635815528256 submission.py:139] 5) loss = 0.749, grad_norm = 2.751
I0609 02:35:16.046157 140583582107392 logging_writer.py:48] [6] global_step=6, grad_norm=2.571512, loss=0.726467
I0609 02:35:16.050129 140635815528256 submission.py:139] 6) loss = 0.726, grad_norm = 2.572
I0609 02:35:16.353269 140583590500096 logging_writer.py:48] [7] global_step=7, grad_norm=2.244700, loss=0.702461
I0609 02:35:16.357380 140635815528256 submission.py:139] 7) loss = 0.702, grad_norm = 2.245
I0609 02:35:16.657061 140583582107392 logging_writer.py:48] [8] global_step=8, grad_norm=2.027318, loss=0.678626
I0609 02:35:16.660780 140635815528256 submission.py:139] 8) loss = 0.679, grad_norm = 2.027
I0609 02:35:16.961632 140583590500096 logging_writer.py:48] [9] global_step=9, grad_norm=1.942182, loss=0.656867
I0609 02:35:16.965854 140635815528256 submission.py:139] 9) loss = 0.657, grad_norm = 1.942
I0609 02:35:17.266172 140583582107392 logging_writer.py:48] [10] global_step=10, grad_norm=1.786013, loss=0.637417
I0609 02:35:17.270479 140635815528256 submission.py:139] 10) loss = 0.637, grad_norm = 1.786
I0609 02:35:17.579130 140583590500096 logging_writer.py:48] [11] global_step=11, grad_norm=1.628746, loss=0.618861
I0609 02:35:17.583580 140635815528256 submission.py:139] 11) loss = 0.619, grad_norm = 1.629
I0609 02:35:17.884088 140583582107392 logging_writer.py:48] [12] global_step=12, grad_norm=1.500292, loss=0.601497
I0609 02:35:17.888413 140635815528256 submission.py:139] 12) loss = 0.601, grad_norm = 1.500
I0609 02:35:18.198222 140583590500096 logging_writer.py:48] [13] global_step=13, grad_norm=1.313244, loss=0.583375
I0609 02:35:18.202572 140635815528256 submission.py:139] 13) loss = 0.583, grad_norm = 1.313
I0609 02:35:18.503859 140583582107392 logging_writer.py:48] [14] global_step=14, grad_norm=1.195815, loss=0.570076
I0609 02:35:18.508138 140635815528256 submission.py:139] 14) loss = 0.570, grad_norm = 1.196
I0609 02:35:18.799981 140583590500096 logging_writer.py:48] [15] global_step=15, grad_norm=1.114659, loss=0.555374
I0609 02:35:18.804118 140635815528256 submission.py:139] 15) loss = 0.555, grad_norm = 1.115
I0609 02:35:19.099102 140583582107392 logging_writer.py:48] [16] global_step=16, grad_norm=0.997185, loss=0.541608
I0609 02:35:19.103383 140635815528256 submission.py:139] 16) loss = 0.542, grad_norm = 0.997
I0609 02:35:19.399375 140583590500096 logging_writer.py:48] [17] global_step=17, grad_norm=0.922670, loss=0.530114
I0609 02:35:19.403556 140635815528256 submission.py:139] 17) loss = 0.530, grad_norm = 0.923
I0609 02:35:19.704434 140583582107392 logging_writer.py:48] [18] global_step=18, grad_norm=0.877323, loss=0.518454
I0609 02:35:19.708351 140635815528256 submission.py:139] 18) loss = 0.518, grad_norm = 0.877
I0609 02:35:20.002927 140583590500096 logging_writer.py:48] [19] global_step=19, grad_norm=0.852858, loss=0.506951
I0609 02:35:20.006963 140635815528256 submission.py:139] 19) loss = 0.507, grad_norm = 0.853
I0609 02:35:20.303797 140583582107392 logging_writer.py:48] [20] global_step=20, grad_norm=0.845779, loss=0.495042
I0609 02:35:20.307874 140635815528256 submission.py:139] 20) loss = 0.495, grad_norm = 0.846
I0609 02:35:20.606321 140583590500096 logging_writer.py:48] [21] global_step=21, grad_norm=0.795682, loss=0.482301
I0609 02:35:20.610354 140635815528256 submission.py:139] 21) loss = 0.482, grad_norm = 0.796
I0609 02:35:20.909786 140583582107392 logging_writer.py:48] [22] global_step=22, grad_norm=0.809485, loss=0.472022
I0609 02:35:20.913743 140635815528256 submission.py:139] 22) loss = 0.472, grad_norm = 0.809
I0609 02:35:21.227241 140583590500096 logging_writer.py:48] [23] global_step=23, grad_norm=0.740644, loss=0.458419
I0609 02:35:21.231380 140635815528256 submission.py:139] 23) loss = 0.458, grad_norm = 0.741
I0609 02:35:21.532733 140583582107392 logging_writer.py:48] [24] global_step=24, grad_norm=0.672280, loss=0.446049
I0609 02:35:21.536839 140635815528256 submission.py:139] 24) loss = 0.446, grad_norm = 0.672
I0609 02:35:21.839433 140583590500096 logging_writer.py:48] [25] global_step=25, grad_norm=0.639680, loss=0.433277
I0609 02:35:21.843625 140635815528256 submission.py:139] 25) loss = 0.433, grad_norm = 0.640
I0609 02:35:22.142404 140583582107392 logging_writer.py:48] [26] global_step=26, grad_norm=0.584572, loss=0.423482
I0609 02:35:22.146590 140635815528256 submission.py:139] 26) loss = 0.423, grad_norm = 0.585
I0609 02:35:22.440746 140583590500096 logging_writer.py:48] [27] global_step=27, grad_norm=0.558934, loss=0.412601
I0609 02:35:22.444826 140635815528256 submission.py:139] 27) loss = 0.413, grad_norm = 0.559
I0609 02:35:22.745565 140583582107392 logging_writer.py:48] [28] global_step=28, grad_norm=0.543386, loss=0.402535
I0609 02:35:22.749588 140635815528256 submission.py:139] 28) loss = 0.403, grad_norm = 0.543
I0609 02:35:23.046521 140583590500096 logging_writer.py:48] [29] global_step=29, grad_norm=0.518807, loss=0.393314
I0609 02:35:23.050583 140635815528256 submission.py:139] 29) loss = 0.393, grad_norm = 0.519
I0609 02:35:23.351789 140583582107392 logging_writer.py:48] [30] global_step=30, grad_norm=0.495893, loss=0.385761
I0609 02:35:23.355862 140635815528256 submission.py:139] 30) loss = 0.386, grad_norm = 0.496
I0609 02:35:23.654973 140583590500096 logging_writer.py:48] [31] global_step=31, grad_norm=0.481741, loss=0.376634
I0609 02:35:23.659401 140635815528256 submission.py:139] 31) loss = 0.377, grad_norm = 0.482
I0609 02:35:23.950465 140583582107392 logging_writer.py:48] [32] global_step=32, grad_norm=0.458913, loss=0.367836
I0609 02:35:23.954456 140635815528256 submission.py:139] 32) loss = 0.368, grad_norm = 0.459
I0609 02:35:24.250945 140583590500096 logging_writer.py:48] [33] global_step=33, grad_norm=0.440516, loss=0.359057
I0609 02:35:24.254880 140635815528256 submission.py:139] 33) loss = 0.359, grad_norm = 0.441
I0609 02:35:24.549946 140583582107392 logging_writer.py:48] [34] global_step=34, grad_norm=0.428382, loss=0.349891
I0609 02:35:24.554030 140635815528256 submission.py:139] 34) loss = 0.350, grad_norm = 0.428
I0609 02:35:24.847038 140583590500096 logging_writer.py:48] [35] global_step=35, grad_norm=0.419227, loss=0.341793
I0609 02:35:24.851006 140635815528256 submission.py:139] 35) loss = 0.342, grad_norm = 0.419
I0609 02:35:25.138828 140583582107392 logging_writer.py:48] [36] global_step=36, grad_norm=0.407049, loss=0.333219
I0609 02:35:25.142986 140635815528256 submission.py:139] 36) loss = 0.333, grad_norm = 0.407
I0609 02:35:25.436432 140583590500096 logging_writer.py:48] [37] global_step=37, grad_norm=0.393952, loss=0.326917
I0609 02:35:25.440312 140635815528256 submission.py:139] 37) loss = 0.327, grad_norm = 0.394
I0609 02:35:25.730123 140583582107392 logging_writer.py:48] [38] global_step=38, grad_norm=0.384426, loss=0.318631
I0609 02:35:25.734182 140635815528256 submission.py:139] 38) loss = 0.319, grad_norm = 0.384
I0609 02:35:26.031554 140583590500096 logging_writer.py:48] [39] global_step=39, grad_norm=0.372932, loss=0.310677
I0609 02:35:26.035446 140635815528256 submission.py:139] 39) loss = 0.311, grad_norm = 0.373
I0609 02:35:26.333706 140583582107392 logging_writer.py:48] [40] global_step=40, grad_norm=0.364328, loss=0.301342
I0609 02:35:26.337939 140635815528256 submission.py:139] 40) loss = 0.301, grad_norm = 0.364
I0609 02:35:26.635439 140583590500096 logging_writer.py:48] [41] global_step=41, grad_norm=0.354995, loss=0.295167
I0609 02:35:26.639729 140635815528256 submission.py:139] 41) loss = 0.295, grad_norm = 0.355
I0609 02:35:26.940323 140583582107392 logging_writer.py:48] [42] global_step=42, grad_norm=0.341777, loss=0.285804
I0609 02:35:26.944271 140635815528256 submission.py:139] 42) loss = 0.286, grad_norm = 0.342
I0609 02:35:27.241155 140583590500096 logging_writer.py:48] [43] global_step=43, grad_norm=0.334237, loss=0.280362
I0609 02:35:27.245436 140635815528256 submission.py:139] 43) loss = 0.280, grad_norm = 0.334
I0609 02:35:27.541601 140583582107392 logging_writer.py:48] [44] global_step=44, grad_norm=0.324856, loss=0.271830
I0609 02:35:27.545671 140635815528256 submission.py:139] 44) loss = 0.272, grad_norm = 0.325
I0609 02:35:27.843856 140583590500096 logging_writer.py:48] [45] global_step=45, grad_norm=0.315533, loss=0.265806
I0609 02:35:27.847909 140635815528256 submission.py:139] 45) loss = 0.266, grad_norm = 0.316
I0609 02:35:28.148197 140583582107392 logging_writer.py:48] [46] global_step=46, grad_norm=0.305706, loss=0.258410
I0609 02:35:28.152202 140635815528256 submission.py:139] 46) loss = 0.258, grad_norm = 0.306
I0609 02:35:28.454094 140583590500096 logging_writer.py:48] [47] global_step=47, grad_norm=0.294427, loss=0.252201
I0609 02:35:28.458241 140635815528256 submission.py:139] 47) loss = 0.252, grad_norm = 0.294
I0609 02:35:28.768895 140583582107392 logging_writer.py:48] [48] global_step=48, grad_norm=0.285033, loss=0.242485
I0609 02:35:28.772990 140635815528256 submission.py:139] 48) loss = 0.242, grad_norm = 0.285
I0609 02:35:29.081736 140583590500096 logging_writer.py:48] [49] global_step=49, grad_norm=0.279055, loss=0.239375
I0609 02:35:29.085829 140635815528256 submission.py:139] 49) loss = 0.239, grad_norm = 0.279
I0609 02:35:29.391311 140583582107392 logging_writer.py:48] [50] global_step=50, grad_norm=0.268538, loss=0.234761
I0609 02:35:29.395232 140635815528256 submission.py:139] 50) loss = 0.235, grad_norm = 0.269
I0609 02:35:29.684202 140583590500096 logging_writer.py:48] [51] global_step=51, grad_norm=0.260463, loss=0.225531
I0609 02:35:29.688257 140635815528256 submission.py:139] 51) loss = 0.226, grad_norm = 0.260
I0609 02:35:29.973494 140583582107392 logging_writer.py:48] [52] global_step=52, grad_norm=0.251199, loss=0.220141
I0609 02:35:29.977661 140635815528256 submission.py:139] 52) loss = 0.220, grad_norm = 0.251
I0609 02:35:30.263681 140583590500096 logging_writer.py:48] [53] global_step=53, grad_norm=0.241933, loss=0.213918
I0609 02:35:30.267520 140635815528256 submission.py:139] 53) loss = 0.214, grad_norm = 0.242
I0609 02:35:30.559320 140583582107392 logging_writer.py:48] [54] global_step=54, grad_norm=0.238431, loss=0.206969
I0609 02:35:30.563650 140635815528256 submission.py:139] 54) loss = 0.207, grad_norm = 0.238
I0609 02:35:30.858113 140583590500096 logging_writer.py:48] [55] global_step=55, grad_norm=0.231105, loss=0.204497
I0609 02:35:30.862285 140635815528256 submission.py:139] 55) loss = 0.204, grad_norm = 0.231
I0609 02:35:31.159883 140583582107392 logging_writer.py:48] [56] global_step=56, grad_norm=0.224694, loss=0.198248
I0609 02:35:31.163895 140635815528256 submission.py:139] 56) loss = 0.198, grad_norm = 0.225
I0609 02:35:31.458409 140583590500096 logging_writer.py:48] [57] global_step=57, grad_norm=0.217574, loss=0.194620
I0609 02:35:31.462614 140635815528256 submission.py:139] 57) loss = 0.195, grad_norm = 0.218
I0609 02:35:31.760074 140583582107392 logging_writer.py:48] [58] global_step=58, grad_norm=0.210183, loss=0.186236
I0609 02:35:31.763905 140635815528256 submission.py:139] 58) loss = 0.186, grad_norm = 0.210
I0609 02:35:32.062546 140583590500096 logging_writer.py:48] [59] global_step=59, grad_norm=0.202056, loss=0.183839
I0609 02:35:32.066453 140635815528256 submission.py:139] 59) loss = 0.184, grad_norm = 0.202
I0609 02:35:32.358788 140583582107392 logging_writer.py:48] [60] global_step=60, grad_norm=0.196037, loss=0.178878
I0609 02:35:32.362940 140635815528256 submission.py:139] 60) loss = 0.179, grad_norm = 0.196
I0609 02:35:32.665130 140583590500096 logging_writer.py:48] [61] global_step=61, grad_norm=0.191613, loss=0.174872
I0609 02:35:32.669199 140635815528256 submission.py:139] 61) loss = 0.175, grad_norm = 0.192
I0609 02:35:32.961674 140583582107392 logging_writer.py:48] [62] global_step=62, grad_norm=0.183865, loss=0.170672
I0609 02:35:32.965618 140635815528256 submission.py:139] 62) loss = 0.171, grad_norm = 0.184
I0609 02:35:33.261272 140583590500096 logging_writer.py:48] [63] global_step=63, grad_norm=0.180964, loss=0.163608
I0609 02:35:33.265466 140635815528256 submission.py:139] 63) loss = 0.164, grad_norm = 0.181
I0609 02:35:33.561381 140583582107392 logging_writer.py:48] [64] global_step=64, grad_norm=0.176703, loss=0.162224
I0609 02:35:33.565949 140635815528256 submission.py:139] 64) loss = 0.162, grad_norm = 0.177
I0609 02:35:33.862708 140583590500096 logging_writer.py:48] [65] global_step=65, grad_norm=0.170876, loss=0.157683
I0609 02:35:33.866950 140635815528256 submission.py:139] 65) loss = 0.158, grad_norm = 0.171
I0609 02:35:34.161244 140583582107392 logging_writer.py:48] [66] global_step=66, grad_norm=0.162382, loss=0.154928
I0609 02:35:34.165292 140635815528256 submission.py:139] 66) loss = 0.155, grad_norm = 0.162
I0609 02:35:34.463139 140583590500096 logging_writer.py:48] [67] global_step=67, grad_norm=0.156518, loss=0.152327
I0609 02:35:34.467824 140635815528256 submission.py:139] 67) loss = 0.152, grad_norm = 0.157
I0609 02:35:34.763930 140583582107392 logging_writer.py:48] [68] global_step=68, grad_norm=0.154038, loss=0.148273
I0609 02:35:34.768393 140635815528256 submission.py:139] 68) loss = 0.148, grad_norm = 0.154
I0609 02:35:35.066173 140583590500096 logging_writer.py:48] [69] global_step=69, grad_norm=0.149929, loss=0.142990
I0609 02:35:35.070215 140635815528256 submission.py:139] 69) loss = 0.143, grad_norm = 0.150
I0609 02:35:35.364696 140583582107392 logging_writer.py:48] [70] global_step=70, grad_norm=0.144725, loss=0.139453
I0609 02:35:35.368801 140635815528256 submission.py:139] 70) loss = 0.139, grad_norm = 0.145
I0609 02:35:35.664604 140583590500096 logging_writer.py:48] [71] global_step=71, grad_norm=0.141002, loss=0.136561
I0609 02:35:35.668624 140635815528256 submission.py:139] 71) loss = 0.137, grad_norm = 0.141
I0609 02:35:35.963474 140583582107392 logging_writer.py:48] [72] global_step=72, grad_norm=0.134490, loss=0.135663
I0609 02:35:35.967496 140635815528256 submission.py:139] 72) loss = 0.136, grad_norm = 0.134
I0609 02:35:36.267238 140583590500096 logging_writer.py:48] [73] global_step=73, grad_norm=0.132480, loss=0.132835
I0609 02:35:36.271114 140635815528256 submission.py:139] 73) loss = 0.133, grad_norm = 0.132
I0609 02:35:36.570601 140583582107392 logging_writer.py:48] [74] global_step=74, grad_norm=0.129547, loss=0.128055
I0609 02:35:36.574855 140635815528256 submission.py:139] 74) loss = 0.128, grad_norm = 0.130
I0609 02:35:36.867531 140583590500096 logging_writer.py:48] [75] global_step=75, grad_norm=0.125299, loss=0.124960
I0609 02:35:36.871559 140635815528256 submission.py:139] 75) loss = 0.125, grad_norm = 0.125
I0609 02:35:37.161221 140583582107392 logging_writer.py:48] [76] global_step=76, grad_norm=0.120487, loss=0.123218
I0609 02:35:37.165151 140635815528256 submission.py:139] 76) loss = 0.123, grad_norm = 0.120
I0609 02:35:37.466629 140583590500096 logging_writer.py:48] [77] global_step=77, grad_norm=0.118396, loss=0.119613
I0609 02:35:37.470788 140635815528256 submission.py:139] 77) loss = 0.120, grad_norm = 0.118
I0609 02:35:37.772201 140583582107392 logging_writer.py:48] [78] global_step=78, grad_norm=0.113297, loss=0.121351
I0609 02:35:37.776212 140635815528256 submission.py:139] 78) loss = 0.121, grad_norm = 0.113
I0609 02:35:38.070743 140583590500096 logging_writer.py:48] [79] global_step=79, grad_norm=0.110962, loss=0.119579
I0609 02:35:38.074972 140635815528256 submission.py:139] 79) loss = 0.120, grad_norm = 0.111
I0609 02:35:38.374414 140583582107392 logging_writer.py:48] [80] global_step=80, grad_norm=0.108275, loss=0.117737
I0609 02:35:38.378369 140635815528256 submission.py:139] 80) loss = 0.118, grad_norm = 0.108
I0609 02:35:38.678059 140583590500096 logging_writer.py:48] [81] global_step=81, grad_norm=0.105646, loss=0.114892
I0609 02:35:38.682055 140635815528256 submission.py:139] 81) loss = 0.115, grad_norm = 0.106
I0609 02:35:38.983270 140583582107392 logging_writer.py:48] [82] global_step=82, grad_norm=0.102949, loss=0.112082
I0609 02:35:38.987520 140635815528256 submission.py:139] 82) loss = 0.112, grad_norm = 0.103
I0609 02:35:39.285484 140583590500096 logging_writer.py:48] [83] global_step=83, grad_norm=0.098460, loss=0.110196
I0609 02:35:39.289459 140635815528256 submission.py:139] 83) loss = 0.110, grad_norm = 0.098
I0609 02:35:39.585582 140583582107392 logging_writer.py:48] [84] global_step=84, grad_norm=0.097583, loss=0.108847
I0609 02:35:39.589710 140635815528256 submission.py:139] 84) loss = 0.109, grad_norm = 0.098
I0609 02:35:39.891734 140583590500096 logging_writer.py:48] [85] global_step=85, grad_norm=0.094335, loss=0.111205
I0609 02:35:39.895639 140635815528256 submission.py:139] 85) loss = 0.111, grad_norm = 0.094
I0609 02:35:40.193642 140583582107392 logging_writer.py:48] [86] global_step=86, grad_norm=0.091475, loss=0.105352
I0609 02:35:40.197650 140635815528256 submission.py:139] 86) loss = 0.105, grad_norm = 0.091
I0609 02:35:40.487163 140583590500096 logging_writer.py:48] [87] global_step=87, grad_norm=0.090206, loss=0.103136
I0609 02:35:40.490979 140635815528256 submission.py:139] 87) loss = 0.103, grad_norm = 0.090
I0609 02:35:40.784122 140583582107392 logging_writer.py:48] [88] global_step=88, grad_norm=0.086842, loss=0.101636
I0609 02:35:40.788317 140635815528256 submission.py:139] 88) loss = 0.102, grad_norm = 0.087
I0609 02:35:41.086372 140583590500096 logging_writer.py:48] [89] global_step=89, grad_norm=0.086331, loss=0.101835
I0609 02:35:41.090395 140635815528256 submission.py:139] 89) loss = 0.102, grad_norm = 0.086
I0609 02:35:41.387835 140583582107392 logging_writer.py:48] [90] global_step=90, grad_norm=0.086128, loss=0.097076
I0609 02:35:41.391867 140635815528256 submission.py:139] 90) loss = 0.097, grad_norm = 0.086
I0609 02:35:41.691000 140583590500096 logging_writer.py:48] [91] global_step=91, grad_norm=0.082650, loss=0.095965
I0609 02:35:41.695190 140635815528256 submission.py:139] 91) loss = 0.096, grad_norm = 0.083
I0609 02:35:41.991658 140583582107392 logging_writer.py:48] [92] global_step=92, grad_norm=0.080227, loss=0.096032
I0609 02:35:41.995676 140635815528256 submission.py:139] 92) loss = 0.096, grad_norm = 0.080
I0609 02:35:42.291528 140583590500096 logging_writer.py:48] [93] global_step=93, grad_norm=0.077489, loss=0.094318
I0609 02:35:42.295513 140635815528256 submission.py:139] 93) loss = 0.094, grad_norm = 0.077
I0609 02:35:42.591583 140583582107392 logging_writer.py:48] [94] global_step=94, grad_norm=0.077188, loss=0.095716
I0609 02:35:42.595697 140635815528256 submission.py:139] 94) loss = 0.096, grad_norm = 0.077
I0609 02:35:42.891408 140583590500096 logging_writer.py:48] [95] global_step=95, grad_norm=0.075282, loss=0.092999
I0609 02:35:42.895445 140635815528256 submission.py:139] 95) loss = 0.093, grad_norm = 0.075
I0609 02:35:43.189116 140583582107392 logging_writer.py:48] [96] global_step=96, grad_norm=0.073246, loss=0.089188
I0609 02:35:43.193106 140635815528256 submission.py:139] 96) loss = 0.089, grad_norm = 0.073
I0609 02:35:43.490827 140583590500096 logging_writer.py:48] [97] global_step=97, grad_norm=0.070572, loss=0.088630
I0609 02:35:43.494882 140635815528256 submission.py:139] 97) loss = 0.089, grad_norm = 0.071
I0609 02:35:43.783984 140583582107392 logging_writer.py:48] [98] global_step=98, grad_norm=0.069643, loss=0.087382
I0609 02:35:43.788211 140635815528256 submission.py:139] 98) loss = 0.087, grad_norm = 0.070
I0609 02:35:44.082536 140583590500096 logging_writer.py:48] [99] global_step=99, grad_norm=0.069672, loss=0.088208
I0609 02:35:44.086554 140635815528256 submission.py:139] 99) loss = 0.088, grad_norm = 0.070
I0609 02:35:44.383209 140583582107392 logging_writer.py:48] [100] global_step=100, grad_norm=0.067976, loss=0.086379
I0609 02:35:44.387953 140635815528256 submission.py:139] 100) loss = 0.086, grad_norm = 0.068
I0609 02:37:39.965226 140583590500096 logging_writer.py:48] [500] global_step=500, grad_norm=0.011446, loss=0.053516
I0609 02:37:39.970002 140635815528256 submission.py:139] 500) loss = 0.054, grad_norm = 0.011
I0609 02:39:14.246376 140635815528256 spec.py:298] Evaluating on the training split.
I0609 02:40:11.675884 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 02:40:14.943003 140635815528256 spec.py:326] Evaluating on the test split.
I0609 02:40:18.290147 140635815528256 submission_runner.py:419] Time since start: 457.59s, 	Step: 824, 	{'train/accuracy': 0.9866066898374427, 'train/loss': 0.05550375551396227, 'train/mean_average_precision': 0.03166504563987395, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06509399277790903, 'validation/mean_average_precision': 0.033657318850088054, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06842116804446469, 'test/mean_average_precision': 0.03507978339420782, 'test/num_examples': 43793, 'score': 244.91187000274658, 'total_duration': 457.5859205722809, 'accumulated_submission_time': 244.91187000274658, 'accumulated_eval_time': 212.41606378555298, 'accumulated_logging_time': 0.027385234832763672}
I0609 02:40:18.300263 140583582107392 logging_writer.py:48] [824] accumulated_eval_time=212.416064, accumulated_logging_time=0.027385, accumulated_submission_time=244.911870, global_step=824, preemption_count=0, score=244.911870, test/accuracy=0.983142, test/loss=0.068421, test/mean_average_precision=0.035080, test/num_examples=43793, total_duration=457.585921, train/accuracy=0.986607, train/loss=0.055504, train/mean_average_precision=0.031665, validation/accuracy=0.984118, validation/loss=0.065094, validation/mean_average_precision=0.033657, validation/num_examples=43793
I0609 02:41:10.449080 140583590500096 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.044252, loss=0.058405
I0609 02:41:10.454267 140635815528256 submission.py:139] 1000) loss = 0.058, grad_norm = 0.044
I0609 02:43:36.653493 140583582107392 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.046925, loss=0.055063
I0609 02:43:36.659388 140635815528256 submission.py:139] 1500) loss = 0.055, grad_norm = 0.047
I0609 02:44:18.357882 140635815528256 spec.py:298] Evaluating on the training split.
I0609 02:45:16.253644 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 02:45:19.483855 140635815528256 spec.py:326] Evaluating on the test split.
I0609 02:45:22.684609 140635815528256 submission_runner.py:419] Time since start: 761.98s, 	Step: 1643, 	{'train/accuracy': 0.9869027821970401, 'train/loss': 0.05170757714753029, 'train/mean_average_precision': 0.05029313156150437, 'validation/accuracy': 0.9841277186863434, 'validation/loss': 0.061741322205686906, 'validation/mean_average_precision': 0.049983595792444345, 'validation/num_examples': 43793, 'test/accuracy': 0.9831374707848837, 'test/loss': 0.06500098822802974, 'test/mean_average_precision': 0.050819504018569744, 'test/num_examples': 43793, 'score': 484.73305320739746, 'total_duration': 761.98033618927, 'accumulated_submission_time': 484.73305320739746, 'accumulated_eval_time': 276.7425534725189, 'accumulated_logging_time': 0.04808640480041504}
I0609 02:45:22.694521 140583590500096 logging_writer.py:48] [1643] accumulated_eval_time=276.742553, accumulated_logging_time=0.048086, accumulated_submission_time=484.733053, global_step=1643, preemption_count=0, score=484.733053, test/accuracy=0.983137, test/loss=0.065001, test/mean_average_precision=0.050820, test/num_examples=43793, total_duration=761.980336, train/accuracy=0.986903, train/loss=0.051708, train/mean_average_precision=0.050293, validation/accuracy=0.984128, validation/loss=0.061741, validation/mean_average_precision=0.049984, validation/num_examples=43793
I0609 02:47:08.415278 140583582107392 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.076172, loss=0.057930
I0609 02:47:08.421298 140635815528256 submission.py:139] 2000) loss = 0.058, grad_norm = 0.076
I0609 02:49:22.766683 140635815528256 spec.py:298] Evaluating on the training split.
I0609 02:50:22.191726 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 02:50:25.500283 140635815528256 spec.py:326] Evaluating on the test split.
I0609 02:50:28.766007 140635815528256 submission_runner.py:419] Time since start: 1068.06s, 	Step: 2456, 	{'train/accuracy': 0.9869383602108072, 'train/loss': 0.049312567244506285, 'train/mean_average_precision': 0.07925902382473374, 'validation/accuracy': 0.9842101246481509, 'validation/loss': 0.05886341830889976, 'validation/mean_average_precision': 0.07315315091798476, 'validation/num_examples': 43793, 'test/accuracy': 0.9832200249431492, 'test/loss': 0.062094823875336484, 'test/mean_average_precision': 0.07412854962568939, 'test/num_examples': 43793, 'score': 724.5709307193756, 'total_duration': 1068.0616962909698, 'accumulated_submission_time': 724.5709307193756, 'accumulated_eval_time': 342.74162793159485, 'accumulated_logging_time': 0.0692605972290039}
I0609 02:50:28.775834 140583590500096 logging_writer.py:48] [2456] accumulated_eval_time=342.741628, accumulated_logging_time=0.069261, accumulated_submission_time=724.570931, global_step=2456, preemption_count=0, score=724.570931, test/accuracy=0.983220, test/loss=0.062095, test/mean_average_precision=0.074129, test/num_examples=43793, total_duration=1068.061696, train/accuracy=0.986938, train/loss=0.049313, train/mean_average_precision=0.079259, validation/accuracy=0.984210, validation/loss=0.058863, validation/mean_average_precision=0.073153, validation/num_examples=43793
I0609 02:50:42.149922 140583582107392 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.072455, loss=0.053735
I0609 02:50:42.155805 140635815528256 submission.py:139] 2500) loss = 0.054, grad_norm = 0.072
I0609 02:53:10.030027 140583590500096 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.101620, loss=0.053386
I0609 02:53:10.035472 140635815528256 submission.py:139] 3000) loss = 0.053, grad_norm = 0.102
I0609 02:54:28.878080 140635815528256 spec.py:298] Evaluating on the training split.
I0609 02:55:29.171962 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 02:55:32.518428 140635815528256 spec.py:326] Evaluating on the test split.
I0609 02:55:35.833827 140635815528256 submission_runner.py:419] Time since start: 1375.13s, 	Step: 3268, 	{'train/accuracy': 0.9867974467188497, 'train/loss': 0.048872075733018744, 'train/mean_average_precision': 0.10058265161248903, 'validation/accuracy': 0.9842068771225624, 'validation/loss': 0.05820365050291993, 'validation/mean_average_precision': 0.09998723467853952, 'validation/num_examples': 43793, 'test/accuracy': 0.983231397199645, 'test/loss': 0.061601249516152604, 'test/mean_average_precision': 0.09962067374647257, 'test/num_examples': 43793, 'score': 964.4392058849335, 'total_duration': 1375.129564523697, 'accumulated_submission_time': 964.4392058849335, 'accumulated_eval_time': 409.6970989704132, 'accumulated_logging_time': 0.0915534496307373}
I0609 02:55:35.844568 140583582107392 logging_writer.py:48] [3268] accumulated_eval_time=409.697099, accumulated_logging_time=0.091553, accumulated_submission_time=964.439206, global_step=3268, preemption_count=0, score=964.439206, test/accuracy=0.983231, test/loss=0.061601, test/mean_average_precision=0.099621, test/num_examples=43793, total_duration=1375.129565, train/accuracy=0.986797, train/loss=0.048872, train/mean_average_precision=0.100583, validation/accuracy=0.984207, validation/loss=0.058204, validation/mean_average_precision=0.099987, validation/num_examples=43793
I0609 02:56:45.349636 140583590500096 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.064866, loss=0.050582
I0609 02:56:45.355000 140635815528256 submission.py:139] 3500) loss = 0.051, grad_norm = 0.065
I0609 02:59:12.271355 140583582107392 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.071966, loss=0.049210
I0609 02:59:12.280502 140635815528256 submission.py:139] 4000) loss = 0.049, grad_norm = 0.072
I0609 02:59:36.007526 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:00:36.442153 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:00:39.722387 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:00:42.982056 140635815528256 submission_runner.py:419] Time since start: 1682.28s, 	Step: 4083, 	{'train/accuracy': 0.987096625494039, 'train/loss': 0.047572719624393425, 'train/mean_average_precision': 0.11693619891408194, 'validation/accuracy': 0.9844601841184633, 'validation/loss': 0.057887270470980516, 'validation/mean_average_precision': 0.10797637509745102, 'validation/num_examples': 43793, 'test/accuracy': 0.9834655814445209, 'test/loss': 0.06099816453886132, 'test/mean_average_precision': 0.11095792589988143, 'test/num_examples': 43793, 'score': 1204.3712077140808, 'total_duration': 1682.2778522968292, 'accumulated_submission_time': 1204.3712077140808, 'accumulated_eval_time': 476.67142701148987, 'accumulated_logging_time': 0.11356830596923828}
I0609 03:00:42.998094 140583590500096 logging_writer.py:48] [4083] accumulated_eval_time=476.671427, accumulated_logging_time=0.113568, accumulated_submission_time=1204.371208, global_step=4083, preemption_count=0, score=1204.371208, test/accuracy=0.983466, test/loss=0.060998, test/mean_average_precision=0.110958, test/num_examples=43793, total_duration=1682.277852, train/accuracy=0.987097, train/loss=0.047573, train/mean_average_precision=0.116936, validation/accuracy=0.984460, validation/loss=0.057887, validation/mean_average_precision=0.107976, validation/num_examples=43793
I0609 03:02:46.462317 140583582107392 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.036737, loss=0.048305
I0609 03:02:46.468535 140635815528256 submission.py:139] 4500) loss = 0.048, grad_norm = 0.037
I0609 03:04:43.136704 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:05:43.430148 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:05:46.749115 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:05:50.032133 140635815528256 submission_runner.py:419] Time since start: 1989.33s, 	Step: 4897, 	{'train/accuracy': 0.987517747651637, 'train/loss': 0.045769049597496655, 'train/mean_average_precision': 0.1398303224456469, 'validation/accuracy': 0.98489210502173, 'validation/loss': 0.05394878307097386, 'validation/mean_average_precision': 0.12687142203750482, 'validation/num_examples': 43793, 'test/accuracy': 0.9838989907754152, 'test/loss': 0.056438732073427714, 'test/mean_average_precision': 0.13005831818251035, 'test/num_examples': 43793, 'score': 1444.278552532196, 'total_duration': 1989.32785654068, 'accumulated_submission_time': 1444.278552532196, 'accumulated_eval_time': 543.5665919780731, 'accumulated_logging_time': 0.14059877395629883}
I0609 03:05:50.042425 140583590500096 logging_writer.py:48] [4897] accumulated_eval_time=543.566592, accumulated_logging_time=0.140599, accumulated_submission_time=1444.278553, global_step=4897, preemption_count=0, score=1444.278553, test/accuracy=0.983899, test/loss=0.056439, test/mean_average_precision=0.130058, test/num_examples=43793, total_duration=1989.327857, train/accuracy=0.987518, train/loss=0.045769, train/mean_average_precision=0.139830, validation/accuracy=0.984892, validation/loss=0.053949, validation/mean_average_precision=0.126871, validation/num_examples=43793
I0609 03:06:20.567718 140583582107392 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.035404, loss=0.046210
I0609 03:06:20.572825 140635815528256 submission.py:139] 5000) loss = 0.046, grad_norm = 0.035
I0609 03:08:47.858053 140583590500096 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.052958, loss=0.046074
I0609 03:08:47.866974 140635815528256 submission.py:139] 5500) loss = 0.046, grad_norm = 0.053
I0609 03:09:50.152603 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:10:52.197129 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:10:55.560155 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:10:58.858918 140635815528256 submission_runner.py:419] Time since start: 2298.15s, 	Step: 5711, 	{'train/accuracy': 0.9879461541396158, 'train/loss': 0.04216060998406743, 'train/mean_average_precision': 0.16246426727287855, 'validation/accuracy': 0.9850004911882453, 'validation/loss': 0.0521461367993768, 'validation/mean_average_precision': 0.1508553281663315, 'validation/num_examples': 43793, 'test/accuracy': 0.9840388274108447, 'test/loss': 0.055038565769971265, 'test/mean_average_precision': 0.1508395025617807, 'test/num_examples': 43793, 'score': 1684.1535289287567, 'total_duration': 2298.154627799988, 'accumulated_submission_time': 1684.1535289287567, 'accumulated_eval_time': 612.2726137638092, 'accumulated_logging_time': 0.16208744049072266}
I0609 03:10:58.869313 140583582107392 logging_writer.py:48] [5711] accumulated_eval_time=612.272614, accumulated_logging_time=0.162087, accumulated_submission_time=1684.153529, global_step=5711, preemption_count=0, score=1684.153529, test/accuracy=0.984039, test/loss=0.055039, test/mean_average_precision=0.150840, test/num_examples=43793, total_duration=2298.154628, train/accuracy=0.987946, train/loss=0.042161, train/mean_average_precision=0.162464, validation/accuracy=0.985000, validation/loss=0.052146, validation/mean_average_precision=0.150855, validation/num_examples=43793
I0609 03:12:24.307728 140583590500096 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.056169, loss=0.045300
I0609 03:12:24.313055 140635815528256 submission.py:139] 6000) loss = 0.045, grad_norm = 0.056
I0609 03:14:50.145540 140583582107392 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.048372, loss=0.044060
I0609 03:14:50.156143 140635815528256 submission.py:139] 6500) loss = 0.044, grad_norm = 0.048
I0609 03:14:59.040875 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:15:58.301765 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:16:01.546282 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:16:04.847614 140635815528256 submission_runner.py:419] Time since start: 2604.14s, 	Step: 6532, 	{'train/accuracy': 0.9881259533577385, 'train/loss': 0.04183423629807536, 'train/mean_average_precision': 0.18218145188014587, 'validation/accuracy': 0.9850500159534694, 'validation/loss': 0.051893974683102397, 'validation/mean_average_precision': 0.1565374937653063, 'validation/num_examples': 43793, 'test/accuracy': 0.9840750501537571, 'test/loss': 0.05473537469900375, 'test/mean_average_precision': 0.162956688819232, 'test/num_examples': 43793, 'score': 1924.089158296585, 'total_duration': 2604.143358230591, 'accumulated_submission_time': 1924.089158296585, 'accumulated_eval_time': 678.0791051387787, 'accumulated_logging_time': 0.18374156951904297}
I0609 03:16:04.859082 140583590500096 logging_writer.py:48] [6532] accumulated_eval_time=678.079105, accumulated_logging_time=0.183742, accumulated_submission_time=1924.089158, global_step=6532, preemption_count=0, score=1924.089158, test/accuracy=0.984075, test/loss=0.054735, test/mean_average_precision=0.162957, test/num_examples=43793, total_duration=2604.143358, train/accuracy=0.988126, train/loss=0.041834, train/mean_average_precision=0.182181, validation/accuracy=0.985050, validation/loss=0.051894, validation/mean_average_precision=0.156537, validation/num_examples=43793
I0609 03:18:19.641834 140583582107392 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.041877, loss=0.042629
I0609 03:18:19.648327 140635815528256 submission.py:139] 7000) loss = 0.043, grad_norm = 0.042
I0609 03:20:04.978363 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:21:04.907209 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:21:08.136600 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:21:11.411337 140635815528256 submission_runner.py:419] Time since start: 2910.71s, 	Step: 7370, 	{'train/accuracy': 0.9883786426517096, 'train/loss': 0.04082393595483168, 'train/mean_average_precision': 0.18935074246773792, 'validation/accuracy': 0.9853589368250728, 'validation/loss': 0.050747874063190354, 'validation/mean_average_precision': 0.16272739072306866, 'validation/num_examples': 43793, 'test/accuracy': 0.9843736771854423, 'test/loss': 0.05335864722165244, 'test/mean_average_precision': 0.1696080602406446, 'test/num_examples': 43793, 'score': 2163.9583241939545, 'total_duration': 2910.707056760788, 'accumulated_submission_time': 2163.9583241939545, 'accumulated_eval_time': 744.5118305683136, 'accumulated_logging_time': 0.20802998542785645}
I0609 03:21:11.422991 140583590500096 logging_writer.py:48] [7370] accumulated_eval_time=744.511831, accumulated_logging_time=0.208030, accumulated_submission_time=2163.958324, global_step=7370, preemption_count=0, score=2163.958324, test/accuracy=0.984374, test/loss=0.053359, test/mean_average_precision=0.169608, test/num_examples=43793, total_duration=2910.707057, train/accuracy=0.988379, train/loss=0.040824, train/mean_average_precision=0.189351, validation/accuracy=0.985359, validation/loss=0.050748, validation/mean_average_precision=0.162727, validation/num_examples=43793
I0609 03:21:48.947840 140583582107392 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.073918, loss=0.043422
I0609 03:21:48.953879 140635815528256 submission.py:139] 7500) loss = 0.043, grad_norm = 0.074
I0609 03:24:13.254324 140583590500096 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.045645, loss=0.042993
I0609 03:24:13.261038 140635815528256 submission.py:139] 8000) loss = 0.043, grad_norm = 0.046
I0609 03:25:11.436838 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:26:12.494022 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:26:15.810431 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:26:19.104456 140635815528256 submission_runner.py:419] Time since start: 3218.40s, 	Step: 8203, 	{'train/accuracy': 0.9882727297066174, 'train/loss': 0.04134865466228485, 'train/mean_average_precision': 0.21098399365438508, 'validation/accuracy': 0.9852513625399547, 'validation/loss': 0.05072033986674591, 'validation/mean_average_precision': 0.17546493869083238, 'validation/num_examples': 43793, 'test/accuracy': 0.9843185006817036, 'test/loss': 0.053308600737554016, 'test/mean_average_precision': 0.1740767798523853, 'test/num_examples': 43793, 'score': 2403.730702638626, 'total_duration': 3218.4001619815826, 'accumulated_submission_time': 2403.730702638626, 'accumulated_eval_time': 812.1792085170746, 'accumulated_logging_time': 0.23122525215148926}
I0609 03:26:19.116092 140583582107392 logging_writer.py:48] [8203] accumulated_eval_time=812.179209, accumulated_logging_time=0.231225, accumulated_submission_time=2403.730703, global_step=8203, preemption_count=0, score=2403.730703, test/accuracy=0.984319, test/loss=0.053309, test/mean_average_precision=0.174077, test/num_examples=43793, total_duration=3218.400162, train/accuracy=0.988273, train/loss=0.041349, train/mean_average_precision=0.210984, validation/accuracy=0.985251, validation/loss=0.050720, validation/mean_average_precision=0.175465, validation/num_examples=43793
I0609 03:27:46.084282 140583590500096 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.050596, loss=0.041696
I0609 03:27:46.090049 140635815528256 submission.py:139] 8500) loss = 0.042, grad_norm = 0.051
I0609 03:30:11.243552 140583582107392 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.056151, loss=0.039884
I0609 03:30:11.249385 140635815528256 submission.py:139] 9000) loss = 0.040, grad_norm = 0.056
I0609 03:30:19.347419 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:31:20.467262 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:31:23.763634 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:31:27.021729 140635815528256 submission_runner.py:419] Time since start: 3526.32s, 	Step: 9029, 	{'train/accuracy': 0.988422037283976, 'train/loss': 0.039330197589939324, 'train/mean_average_precision': 0.2237597580724335, 'validation/accuracy': 0.9854762536869563, 'validation/loss': 0.050256314762764195, 'validation/mean_average_precision': 0.1803516535336957, 'validation/num_examples': 43793, 'test/accuracy': 0.9845101442633916, 'test/loss': 0.053334402202595485, 'test/mean_average_precision': 0.1852482140164883, 'test/num_examples': 43793, 'score': 2643.726198196411, 'total_duration': 3526.317481279373, 'accumulated_submission_time': 2643.726198196411, 'accumulated_eval_time': 879.853349685669, 'accumulated_logging_time': 0.2541654109954834}
I0609 03:31:27.033136 140583590500096 logging_writer.py:48] [9029] accumulated_eval_time=879.853350, accumulated_logging_time=0.254165, accumulated_submission_time=2643.726198, global_step=9029, preemption_count=0, score=2643.726198, test/accuracy=0.984510, test/loss=0.053334, test/mean_average_precision=0.185248, test/num_examples=43793, total_duration=3526.317481, train/accuracy=0.988422, train/loss=0.039330, train/mean_average_precision=0.223760, validation/accuracy=0.985476, validation/loss=0.050256, validation/mean_average_precision=0.180352, validation/num_examples=43793
I0609 03:33:42.382487 140583582107392 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.055272, loss=0.039881
I0609 03:33:42.389923 140635815528256 submission.py:139] 9500) loss = 0.040, grad_norm = 0.055
I0609 03:35:27.259297 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:36:27.757828 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:36:31.074977 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:36:34.437498 140635815528256 submission_runner.py:419] Time since start: 3833.73s, 	Step: 9860, 	{'train/accuracy': 0.9884953777670042, 'train/loss': 0.03912512737604656, 'train/mean_average_precision': 0.24604326719995503, 'validation/accuracy': 0.9856788180955374, 'validation/loss': 0.04933739504910665, 'validation/mean_average_precision': 0.19908113079108275, 'validation/num_examples': 43793, 'test/accuracy': 0.9847047362078747, 'test/loss': 0.05224883871360404, 'test/mean_average_precision': 0.1949264223465612, 'test/num_examples': 43793, 'score': 2883.7120513916016, 'total_duration': 3833.7332005500793, 'accumulated_submission_time': 2883.7120513916016, 'accumulated_eval_time': 947.0312414169312, 'accumulated_logging_time': 0.27878475189208984}
I0609 03:36:34.450992 140583590500096 logging_writer.py:48] [9860] accumulated_eval_time=947.031241, accumulated_logging_time=0.278785, accumulated_submission_time=2883.712051, global_step=9860, preemption_count=0, score=2883.712051, test/accuracy=0.984705, test/loss=0.052249, test/mean_average_precision=0.194926, test/num_examples=43793, total_duration=3833.733201, train/accuracy=0.988495, train/loss=0.039125, train/mean_average_precision=0.246043, validation/accuracy=0.985679, validation/loss=0.049337, validation/mean_average_precision=0.199081, validation/num_examples=43793
I0609 03:37:15.041547 140583582107392 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.057871, loss=0.039209
I0609 03:37:15.047592 140635815528256 submission.py:139] 10000) loss = 0.039, grad_norm = 0.058
I0609 03:39:37.304954 140583590500096 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.049461, loss=0.039801
I0609 03:39:37.311300 140635815528256 submission.py:139] 10500) loss = 0.040, grad_norm = 0.049
I0609 03:40:34.482182 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:41:34.675459 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:41:37.945336 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:41:41.191312 140635815528256 submission_runner.py:419] Time since start: 4140.49s, 	Step: 10698, 	{'train/accuracy': 0.9890398277149989, 'train/loss': 0.03704014189925391, 'train/mean_average_precision': 0.26159569324900955, 'validation/accuracy': 0.9859106102344145, 'validation/loss': 0.047913516820558784, 'validation/mean_average_precision': 0.20440331729120265, 'validation/num_examples': 43793, 'test/accuracy': 0.9850252653631815, 'test/loss': 0.050656058106333966, 'test/mean_average_precision': 0.20372237335659943, 'test/num_examples': 43793, 'score': 3123.499500274658, 'total_duration': 4140.487038135529, 'accumulated_submission_time': 3123.499500274658, 'accumulated_eval_time': 1013.7401068210602, 'accumulated_logging_time': 0.3036341667175293}
I0609 03:41:41.202362 140583582107392 logging_writer.py:48] [10698] accumulated_eval_time=1013.740107, accumulated_logging_time=0.303634, accumulated_submission_time=3123.499500, global_step=10698, preemption_count=0, score=3123.499500, test/accuracy=0.985025, test/loss=0.050656, test/mean_average_precision=0.203722, test/num_examples=43793, total_duration=4140.487038, train/accuracy=0.989040, train/loss=0.037040, train/mean_average_precision=0.261596, validation/accuracy=0.985911, validation/loss=0.047914, validation/mean_average_precision=0.204403, validation/num_examples=43793
I0609 03:43:09.067392 140583590500096 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.039466, loss=0.038869
I0609 03:43:09.073286 140635815528256 submission.py:139] 11000) loss = 0.039, grad_norm = 0.039
I0609 03:45:34.783718 140583582107392 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.051760, loss=0.038821
I0609 03:45:34.790372 140635815528256 submission.py:139] 11500) loss = 0.039, grad_norm = 0.052
I0609 03:45:41.453910 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:46:43.213580 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:46:46.542944 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:46:49.835838 140635815528256 submission_runner.py:419] Time since start: 4449.13s, 	Step: 11524, 	{'train/accuracy': 0.9891557045311907, 'train/loss': 0.03655007838195577, 'train/mean_average_precision': 0.26458042910110613, 'validation/accuracy': 0.986016966697437, 'validation/loss': 0.047561521835144234, 'validation/mean_average_precision': 0.21392976131764194, 'validation/num_examples': 43793, 'test/accuracy': 0.9850972896543213, 'test/loss': 0.050225860384912974, 'test/mean_average_precision': 0.20780360007753257, 'test/num_examples': 43793, 'score': 3363.5137791633606, 'total_duration': 4449.131601572037, 'accumulated_submission_time': 3363.5137791633606, 'accumulated_eval_time': 1082.121802330017, 'accumulated_logging_time': 0.3258850574493408}
I0609 03:46:49.846378 140583590500096 logging_writer.py:48] [11524] accumulated_eval_time=1082.121802, accumulated_logging_time=0.325885, accumulated_submission_time=3363.513779, global_step=11524, preemption_count=0, score=3363.513779, test/accuracy=0.985097, test/loss=0.050226, test/mean_average_precision=0.207804, test/num_examples=43793, total_duration=4449.131602, train/accuracy=0.989156, train/loss=0.036550, train/mean_average_precision=0.264580, validation/accuracy=0.986017, validation/loss=0.047562, validation/mean_average_precision=0.213930, validation/num_examples=43793
I0609 03:49:08.029019 140635815528256 spec.py:298] Evaluating on the training split.
I0609 03:50:09.141677 140635815528256 spec.py:310] Evaluating on the validation split.
I0609 03:50:12.365326 140635815528256 spec.py:326] Evaluating on the test split.
I0609 03:50:15.741141 140635815528256 submission_runner.py:419] Time since start: 4655.04s, 	Step: 12000, 	{'train/accuracy': 0.9893203505462634, 'train/loss': 0.036372910018147166, 'train/mean_average_precision': 0.2755039950437364, 'validation/accuracy': 0.9860758280987281, 'validation/loss': 0.047102884959653556, 'validation/mean_average_precision': 0.21609638533612416, 'validation/num_examples': 43793, 'test/accuracy': 0.9852009035468383, 'test/loss': 0.04972211154161888, 'test/mean_average_precision': 0.2133556851110297, 'test/num_examples': 43793, 'score': 3501.554164648056, 'total_duration': 4655.036751508713, 'accumulated_submission_time': 3501.554164648056, 'accumulated_eval_time': 1149.8335273265839, 'accumulated_logging_time': 0.3476064205169678}
I0609 03:50:15.754352 140583582107392 logging_writer.py:48] [12000] accumulated_eval_time=1149.833527, accumulated_logging_time=0.347606, accumulated_submission_time=3501.554165, global_step=12000, preemption_count=0, score=3501.554165, test/accuracy=0.985201, test/loss=0.049722, test/mean_average_precision=0.213356, test/num_examples=43793, total_duration=4655.036752, train/accuracy=0.989320, train/loss=0.036373, train/mean_average_precision=0.275504, validation/accuracy=0.986076, validation/loss=0.047103, validation/mean_average_precision=0.216096, validation/num_examples=43793
I0609 03:50:15.776658 140583590500096 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3501.554165
I0609 03:50:15.838815 140635815528256 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nesterov/ogbg_pytorch/trial_1/checkpoint_12000.
I0609 03:50:16.025832 140635815528256 submission_runner.py:581] Tuning trial 1/1
I0609 03:50:16.026105 140635815528256 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0609 03:50:16.027608 140635815528256 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.43729867992554, 'train/loss': 0.7881166007644227, 'train/mean_average_precision': 0.023672840894495825, 'validation/accuracy': 0.43877358819914153, 'validation/loss': 0.787225472454082, 'validation/mean_average_precision': 0.026926596716653393, 'validation/num_examples': 43793, 'test/accuracy': 0.4385786532636902, 'test/loss': 0.7869883695511624, 'test/mean_average_precision': 0.028913975699431736, 'test/num_examples': 43793, 'score': 5.072153329849243, 'total_duration': 153.4451286792755, 'accumulated_submission_time': 5.072153329849243, 'accumulated_eval_time': 148.3725233078003, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (824, {'train/accuracy': 0.9866066898374427, 'train/loss': 0.05550375551396227, 'train/mean_average_precision': 0.03166504563987395, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06509399277790903, 'validation/mean_average_precision': 0.033657318850088054, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06842116804446469, 'test/mean_average_precision': 0.03507978339420782, 'test/num_examples': 43793, 'score': 244.91187000274658, 'total_duration': 457.5859205722809, 'accumulated_submission_time': 244.91187000274658, 'accumulated_eval_time': 212.41606378555298, 'accumulated_logging_time': 0.027385234832763672, 'global_step': 824, 'preemption_count': 0}), (1643, {'train/accuracy': 0.9869027821970401, 'train/loss': 0.05170757714753029, 'train/mean_average_precision': 0.05029313156150437, 'validation/accuracy': 0.9841277186863434, 'validation/loss': 0.061741322205686906, 'validation/mean_average_precision': 0.049983595792444345, 'validation/num_examples': 43793, 'test/accuracy': 0.9831374707848837, 'test/loss': 0.06500098822802974, 'test/mean_average_precision': 0.050819504018569744, 'test/num_examples': 43793, 'score': 484.73305320739746, 'total_duration': 761.98033618927, 'accumulated_submission_time': 484.73305320739746, 'accumulated_eval_time': 276.7425534725189, 'accumulated_logging_time': 0.04808640480041504, 'global_step': 1643, 'preemption_count': 0}), (2456, {'train/accuracy': 0.9869383602108072, 'train/loss': 0.049312567244506285, 'train/mean_average_precision': 0.07925902382473374, 'validation/accuracy': 0.9842101246481509, 'validation/loss': 0.05886341830889976, 'validation/mean_average_precision': 0.07315315091798476, 'validation/num_examples': 43793, 'test/accuracy': 0.9832200249431492, 'test/loss': 0.062094823875336484, 'test/mean_average_precision': 0.07412854962568939, 'test/num_examples': 43793, 'score': 724.5709307193756, 'total_duration': 1068.0616962909698, 'accumulated_submission_time': 724.5709307193756, 'accumulated_eval_time': 342.74162793159485, 'accumulated_logging_time': 0.0692605972290039, 'global_step': 2456, 'preemption_count': 0}), (3268, {'train/accuracy': 0.9867974467188497, 'train/loss': 0.048872075733018744, 'train/mean_average_precision': 0.10058265161248903, 'validation/accuracy': 0.9842068771225624, 'validation/loss': 0.05820365050291993, 'validation/mean_average_precision': 0.09998723467853952, 'validation/num_examples': 43793, 'test/accuracy': 0.983231397199645, 'test/loss': 0.061601249516152604, 'test/mean_average_precision': 0.09962067374647257, 'test/num_examples': 43793, 'score': 964.4392058849335, 'total_duration': 1375.129564523697, 'accumulated_submission_time': 964.4392058849335, 'accumulated_eval_time': 409.6970989704132, 'accumulated_logging_time': 0.0915534496307373, 'global_step': 3268, 'preemption_count': 0}), (4083, {'train/accuracy': 0.987096625494039, 'train/loss': 0.047572719624393425, 'train/mean_average_precision': 0.11693619891408194, 'validation/accuracy': 0.9844601841184633, 'validation/loss': 0.057887270470980516, 'validation/mean_average_precision': 0.10797637509745102, 'validation/num_examples': 43793, 'test/accuracy': 0.9834655814445209, 'test/loss': 0.06099816453886132, 'test/mean_average_precision': 0.11095792589988143, 'test/num_examples': 43793, 'score': 1204.3712077140808, 'total_duration': 1682.2778522968292, 'accumulated_submission_time': 1204.3712077140808, 'accumulated_eval_time': 476.67142701148987, 'accumulated_logging_time': 0.11356830596923828, 'global_step': 4083, 'preemption_count': 0}), (4897, {'train/accuracy': 0.987517747651637, 'train/loss': 0.045769049597496655, 'train/mean_average_precision': 0.1398303224456469, 'validation/accuracy': 0.98489210502173, 'validation/loss': 0.05394878307097386, 'validation/mean_average_precision': 0.12687142203750482, 'validation/num_examples': 43793, 'test/accuracy': 0.9838989907754152, 'test/loss': 0.056438732073427714, 'test/mean_average_precision': 0.13005831818251035, 'test/num_examples': 43793, 'score': 1444.278552532196, 'total_duration': 1989.32785654068, 'accumulated_submission_time': 1444.278552532196, 'accumulated_eval_time': 543.5665919780731, 'accumulated_logging_time': 0.14059877395629883, 'global_step': 4897, 'preemption_count': 0}), (5711, {'train/accuracy': 0.9879461541396158, 'train/loss': 0.04216060998406743, 'train/mean_average_precision': 0.16246426727287855, 'validation/accuracy': 0.9850004911882453, 'validation/loss': 0.0521461367993768, 'validation/mean_average_precision': 0.1508553281663315, 'validation/num_examples': 43793, 'test/accuracy': 0.9840388274108447, 'test/loss': 0.055038565769971265, 'test/mean_average_precision': 0.1508395025617807, 'test/num_examples': 43793, 'score': 1684.1535289287567, 'total_duration': 2298.154627799988, 'accumulated_submission_time': 1684.1535289287567, 'accumulated_eval_time': 612.2726137638092, 'accumulated_logging_time': 0.16208744049072266, 'global_step': 5711, 'preemption_count': 0}), (6532, {'train/accuracy': 0.9881259533577385, 'train/loss': 0.04183423629807536, 'train/mean_average_precision': 0.18218145188014587, 'validation/accuracy': 0.9850500159534694, 'validation/loss': 0.051893974683102397, 'validation/mean_average_precision': 0.1565374937653063, 'validation/num_examples': 43793, 'test/accuracy': 0.9840750501537571, 'test/loss': 0.05473537469900375, 'test/mean_average_precision': 0.162956688819232, 'test/num_examples': 43793, 'score': 1924.089158296585, 'total_duration': 2604.143358230591, 'accumulated_submission_time': 1924.089158296585, 'accumulated_eval_time': 678.0791051387787, 'accumulated_logging_time': 0.18374156951904297, 'global_step': 6532, 'preemption_count': 0}), (7370, {'train/accuracy': 0.9883786426517096, 'train/loss': 0.04082393595483168, 'train/mean_average_precision': 0.18935074246773792, 'validation/accuracy': 0.9853589368250728, 'validation/loss': 0.050747874063190354, 'validation/mean_average_precision': 0.16272739072306866, 'validation/num_examples': 43793, 'test/accuracy': 0.9843736771854423, 'test/loss': 0.05335864722165244, 'test/mean_average_precision': 0.1696080602406446, 'test/num_examples': 43793, 'score': 2163.9583241939545, 'total_duration': 2910.707056760788, 'accumulated_submission_time': 2163.9583241939545, 'accumulated_eval_time': 744.5118305683136, 'accumulated_logging_time': 0.20802998542785645, 'global_step': 7370, 'preemption_count': 0}), (8203, {'train/accuracy': 0.9882727297066174, 'train/loss': 0.04134865466228485, 'train/mean_average_precision': 0.21098399365438508, 'validation/accuracy': 0.9852513625399547, 'validation/loss': 0.05072033986674591, 'validation/mean_average_precision': 0.17546493869083238, 'validation/num_examples': 43793, 'test/accuracy': 0.9843185006817036, 'test/loss': 0.053308600737554016, 'test/mean_average_precision': 0.1740767798523853, 'test/num_examples': 43793, 'score': 2403.730702638626, 'total_duration': 3218.4001619815826, 'accumulated_submission_time': 2403.730702638626, 'accumulated_eval_time': 812.1792085170746, 'accumulated_logging_time': 0.23122525215148926, 'global_step': 8203, 'preemption_count': 0}), (9029, {'train/accuracy': 0.988422037283976, 'train/loss': 0.039330197589939324, 'train/mean_average_precision': 0.2237597580724335, 'validation/accuracy': 0.9854762536869563, 'validation/loss': 0.050256314762764195, 'validation/mean_average_precision': 0.1803516535336957, 'validation/num_examples': 43793, 'test/accuracy': 0.9845101442633916, 'test/loss': 0.053334402202595485, 'test/mean_average_precision': 0.1852482140164883, 'test/num_examples': 43793, 'score': 2643.726198196411, 'total_duration': 3526.317481279373, 'accumulated_submission_time': 2643.726198196411, 'accumulated_eval_time': 879.853349685669, 'accumulated_logging_time': 0.2541654109954834, 'global_step': 9029, 'preemption_count': 0}), (9860, {'train/accuracy': 0.9884953777670042, 'train/loss': 0.03912512737604656, 'train/mean_average_precision': 0.24604326719995503, 'validation/accuracy': 0.9856788180955374, 'validation/loss': 0.04933739504910665, 'validation/mean_average_precision': 0.19908113079108275, 'validation/num_examples': 43793, 'test/accuracy': 0.9847047362078747, 'test/loss': 0.05224883871360404, 'test/mean_average_precision': 0.1949264223465612, 'test/num_examples': 43793, 'score': 2883.7120513916016, 'total_duration': 3833.7332005500793, 'accumulated_submission_time': 2883.7120513916016, 'accumulated_eval_time': 947.0312414169312, 'accumulated_logging_time': 0.27878475189208984, 'global_step': 9860, 'preemption_count': 0}), (10698, {'train/accuracy': 0.9890398277149989, 'train/loss': 0.03704014189925391, 'train/mean_average_precision': 0.26159569324900955, 'validation/accuracy': 0.9859106102344145, 'validation/loss': 0.047913516820558784, 'validation/mean_average_precision': 0.20440331729120265, 'validation/num_examples': 43793, 'test/accuracy': 0.9850252653631815, 'test/loss': 0.050656058106333966, 'test/mean_average_precision': 0.20372237335659943, 'test/num_examples': 43793, 'score': 3123.499500274658, 'total_duration': 4140.487038135529, 'accumulated_submission_time': 3123.499500274658, 'accumulated_eval_time': 1013.7401068210602, 'accumulated_logging_time': 0.3036341667175293, 'global_step': 10698, 'preemption_count': 0}), (11524, {'train/accuracy': 0.9891557045311907, 'train/loss': 0.03655007838195577, 'train/mean_average_precision': 0.26458042910110613, 'validation/accuracy': 0.986016966697437, 'validation/loss': 0.047561521835144234, 'validation/mean_average_precision': 0.21392976131764194, 'validation/num_examples': 43793, 'test/accuracy': 0.9850972896543213, 'test/loss': 0.050225860384912974, 'test/mean_average_precision': 0.20780360007753257, 'test/num_examples': 43793, 'score': 3363.5137791633606, 'total_duration': 4449.131601572037, 'accumulated_submission_time': 3363.5137791633606, 'accumulated_eval_time': 1082.121802330017, 'accumulated_logging_time': 0.3258850574493408, 'global_step': 11524, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9893203505462634, 'train/loss': 0.036372910018147166, 'train/mean_average_precision': 0.2755039950437364, 'validation/accuracy': 0.9860758280987281, 'validation/loss': 0.047102884959653556, 'validation/mean_average_precision': 0.21609638533612416, 'validation/num_examples': 43793, 'test/accuracy': 0.9852009035468383, 'test/loss': 0.04972211154161888, 'test/mean_average_precision': 0.2133556851110297, 'test/num_examples': 43793, 'score': 3501.554164648056, 'total_duration': 4655.036751508713, 'accumulated_submission_time': 3501.554164648056, 'accumulated_eval_time': 1149.8335273265839, 'accumulated_logging_time': 0.3476064205169678, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0609 03:50:16.027749 140635815528256 submission_runner.py:584] Timing: 3501.554164648056
I0609 03:50:16.027797 140635815528256 submission_runner.py:586] Total number of evals: 16
I0609 03:50:16.027839 140635815528256 submission_runner.py:587] ====================
I0609 03:50:16.027956 140635815528256 submission_runner.py:655] Final ogbg score: 3501.554164648056
