torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_b_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_pytorch_06-09-2023-11-58-14.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0609 11:58:38.086260 140629191505728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0609 11:58:38.086312 139936999319360 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0609 11:58:38.086346 139755037853504 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0609 11:58:38.087157 140444940285760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0609 11:58:38.087210 139798524856128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0609 11:58:38.087239 140137780438848 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0609 11:58:38.087471 140444940285760 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.087522 139798524856128 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.087382 140093511223104 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0609 11:58:38.087556 140137780438848 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.087409 139771734505280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0609 11:58:38.087732 140093511223104 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.087762 139771734505280 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.096953 140629191505728 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.096979 139936999319360 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.097023 139755037853504 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0609 11:58:38.107862 139771734505280 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_b_pytorch/nadamw/criteo1tb_pytorch.
W0609 11:58:38.151496 140444940285760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.151498 140629191505728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.152052 139798524856128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.153127 139755037853504 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.153422 139936999319360 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.153627 140093511223104 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.153710 139771734505280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0609 11:58:38.154994 140137780438848 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0609 11:58:38.158905 139771734505280 submission_runner.py:541] Using RNG seed 3871139798
I0609 11:58:38.160668 139771734505280 submission_runner.py:550] --- Tuning run 1/1 ---
I0609 11:58:38.160785 139771734505280 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_b_pytorch/nadamw/criteo1tb_pytorch/trial_1.
I0609 11:58:38.161073 139771734505280 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_b_pytorch/nadamw/criteo1tb_pytorch/trial_1/hparams.json.
I0609 11:58:38.162059 139771734505280 submission_runner.py:255] Initializing dataset.
I0609 11:58:38.162171 139771734505280 submission_runner.py:262] Initializing model.
I0609 11:58:51.517928 139771734505280 submission_runner.py:272] Initializing optimizer.
I0609 11:58:51.518572 139771734505280 submission_runner.py:279] Initializing metrics bundle.
I0609 11:58:51.518673 139771734505280 submission_runner.py:297] Initializing checkpoint and logger.
I0609 11:58:51.521946 139771734505280 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0609 11:58:51.522084 139771734505280 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0609 11:58:52.025973 139771734505280 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_b_pytorch/nadamw/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0609 11:58:52.026953 139771734505280 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_b_pytorch/nadamw/criteo1tb_pytorch/trial_1/flags_0.json.
I0609 11:58:52.076053 139771734505280 submission_runner.py:332] Starting training loop.
I0609 11:58:58.012683 139733297522432 logging_writer.py:48] [0] global_step=0, grad_norm=3.206899, loss=0.307595
I0609 11:58:58.019551 139771734505280 submission.py:296] 0) loss = 0.308, grad_norm = 3.207
I0609 11:58:58.020735 139771734505280 spec.py:298] Evaluating on the training split.
I0609 12:03:44.935351 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 12:08:30.074521 139771734505280 spec.py:326] Evaluating on the test split.
I0609 12:13:10.723441 139771734505280 submission_runner.py:419] Time since start: 858.65s, 	Step: 1, 	{'train/loss': 0.31335947739893766, 'validation/loss': 0.3096135056179775, 'validation/num_examples': 89000000, 'test/loss': 0.3119057431731702, 'test/num_examples': 89274637, 'score': 5.944623708724976, 'total_duration': 858.6477429866791, 'accumulated_submission_time': 5.944623708724976, 'accumulated_eval_time': 852.7026681900024, 'accumulated_logging_time': 0}
I0609 12:13:10.739211 139705278392064 logging_writer.py:48] [1] accumulated_eval_time=852.702668, accumulated_logging_time=0, accumulated_submission_time=5.944624, global_step=1, preemption_count=0, score=5.944624, test/loss=0.311906, test/num_examples=89274637, total_duration=858.647743, train/loss=0.313359, validation/loss=0.309614, validation/num_examples=89000000
I0609 12:13:10.764921 139771734505280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764933 140137780438848 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764928 139798524856128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764950 139755037853504 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764929 139936999319360 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764930 140629191505728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764949 140444940285760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:10.764971 140093511223104 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0609 12:13:11.965848 139705269999360 logging_writer.py:48] [1] global_step=1, grad_norm=3.207202, loss=0.307472
I0609 12:13:11.969222 139771734505280 submission.py:296] 1) loss = 0.307, grad_norm = 3.207
I0609 12:13:13.162145 139705278392064 logging_writer.py:48] [2] global_step=2, grad_norm=3.096473, loss=0.301244
I0609 12:13:13.166179 139771734505280 submission.py:296] 2) loss = 0.301, grad_norm = 3.096
I0609 12:13:14.342590 139705269999360 logging_writer.py:48] [3] global_step=3, grad_norm=2.893215, loss=0.291098
I0609 12:13:14.346123 139771734505280 submission.py:296] 3) loss = 0.291, grad_norm = 2.893
I0609 12:13:15.536844 139705278392064 logging_writer.py:48] [4] global_step=4, grad_norm=2.626965, loss=0.276572
I0609 12:13:15.540238 139771734505280 submission.py:296] 4) loss = 0.277, grad_norm = 2.627
I0609 12:13:16.801552 139705269999360 logging_writer.py:48] [5] global_step=5, grad_norm=2.297608, loss=0.261162
I0609 12:13:16.805032 139771734505280 submission.py:296] 5) loss = 0.261, grad_norm = 2.298
I0609 12:13:17.974536 139705278392064 logging_writer.py:48] [6] global_step=6, grad_norm=1.996434, loss=0.242665
I0609 12:13:17.978469 139771734505280 submission.py:296] 6) loss = 0.243, grad_norm = 1.996
I0609 12:13:19.137949 139705269999360 logging_writer.py:48] [7] global_step=7, grad_norm=1.686482, loss=0.226004
I0609 12:13:19.142137 139771734505280 submission.py:296] 7) loss = 0.226, grad_norm = 1.686
I0609 12:13:20.304928 139705278392064 logging_writer.py:48] [8] global_step=8, grad_norm=1.406589, loss=0.210418
I0609 12:13:20.308595 139771734505280 submission.py:296] 8) loss = 0.210, grad_norm = 1.407
I0609 12:13:21.514612 139705269999360 logging_writer.py:48] [9] global_step=9, grad_norm=1.169070, loss=0.197897
I0609 12:13:21.518158 139771734505280 submission.py:296] 9) loss = 0.198, grad_norm = 1.169
I0609 12:13:22.685220 139705278392064 logging_writer.py:48] [10] global_step=10, grad_norm=0.982837, loss=0.186472
I0609 12:13:22.688823 139771734505280 submission.py:296] 10) loss = 0.186, grad_norm = 0.983
I0609 12:13:23.921475 139705269999360 logging_writer.py:48] [11] global_step=11, grad_norm=0.843089, loss=0.175194
I0609 12:13:23.924972 139771734505280 submission.py:296] 11) loss = 0.175, grad_norm = 0.843
I0609 12:13:25.134323 139705278392064 logging_writer.py:48] [12] global_step=12, grad_norm=0.692536, loss=0.166462
I0609 12:13:25.138516 139771734505280 submission.py:296] 12) loss = 0.166, grad_norm = 0.693
I0609 12:13:26.321467 139705269999360 logging_writer.py:48] [13] global_step=13, grad_norm=0.568902, loss=0.157919
I0609 12:13:26.325380 139771734505280 submission.py:296] 13) loss = 0.158, grad_norm = 0.569
I0609 12:13:27.484978 139705278392064 logging_writer.py:48] [14] global_step=14, grad_norm=0.451195, loss=0.152782
I0609 12:13:27.488809 139771734505280 submission.py:296] 14) loss = 0.153, grad_norm = 0.451
I0609 12:13:28.654465 139705269999360 logging_writer.py:48] [15] global_step=15, grad_norm=0.354576, loss=0.147283
I0609 12:13:28.658431 139771734505280 submission.py:296] 15) loss = 0.147, grad_norm = 0.355
I0609 12:13:29.825074 139705278392064 logging_writer.py:48] [16] global_step=16, grad_norm=0.265350, loss=0.142703
I0609 12:13:29.828556 139771734505280 submission.py:296] 16) loss = 0.143, grad_norm = 0.265
I0609 12:13:30.978508 139705269999360 logging_writer.py:48] [17] global_step=17, grad_norm=0.170308, loss=0.140512
I0609 12:13:30.982011 139771734505280 submission.py:296] 17) loss = 0.141, grad_norm = 0.170
I0609 12:13:32.189079 139705278392064 logging_writer.py:48] [18] global_step=18, grad_norm=0.083337, loss=0.140925
I0609 12:13:32.192630 139771734505280 submission.py:296] 18) loss = 0.141, grad_norm = 0.083
I0609 12:13:33.374236 139705269999360 logging_writer.py:48] [19] global_step=19, grad_norm=0.055984, loss=0.144442
I0609 12:13:33.377688 139771734505280 submission.py:296] 19) loss = 0.144, grad_norm = 0.056
I0609 12:13:34.541603 139705278392064 logging_writer.py:48] [20] global_step=20, grad_norm=0.108853, loss=0.145369
I0609 12:13:34.545594 139771734505280 submission.py:296] 20) loss = 0.145, grad_norm = 0.109
I0609 12:13:35.712729 139705269999360 logging_writer.py:48] [21] global_step=21, grad_norm=0.152640, loss=0.143104
I0609 12:13:35.716689 139771734505280 submission.py:296] 21) loss = 0.143, grad_norm = 0.153
I0609 12:13:36.887673 139705278392064 logging_writer.py:48] [22] global_step=22, grad_norm=0.200571, loss=0.144546
I0609 12:13:36.891067 139771734505280 submission.py:296] 22) loss = 0.145, grad_norm = 0.201
I0609 12:13:38.050160 139705269999360 logging_writer.py:48] [23] global_step=23, grad_norm=0.255095, loss=0.148309
I0609 12:13:38.053664 139771734505280 submission.py:296] 23) loss = 0.148, grad_norm = 0.255
I0609 12:13:39.243747 139705278392064 logging_writer.py:48] [24] global_step=24, grad_norm=0.269265, loss=0.146511
I0609 12:13:39.247046 139771734505280 submission.py:296] 24) loss = 0.147, grad_norm = 0.269
I0609 12:13:40.407572 139705269999360 logging_writer.py:48] [25] global_step=25, grad_norm=0.277488, loss=0.146045
I0609 12:13:40.411078 139771734505280 submission.py:296] 25) loss = 0.146, grad_norm = 0.277
I0609 12:13:41.574948 139705278392064 logging_writer.py:48] [26] global_step=26, grad_norm=0.303068, loss=0.150009
I0609 12:13:41.578643 139771734505280 submission.py:296] 26) loss = 0.150, grad_norm = 0.303
I0609 12:13:42.739751 139705269999360 logging_writer.py:48] [27] global_step=27, grad_norm=0.288715, loss=0.147179
I0609 12:13:42.743464 139771734505280 submission.py:296] 27) loss = 0.147, grad_norm = 0.289
I0609 12:13:43.897385 139705278392064 logging_writer.py:48] [28] global_step=28, grad_norm=0.281192, loss=0.147235
I0609 12:13:43.900669 139771734505280 submission.py:296] 28) loss = 0.147, grad_norm = 0.281
I0609 12:13:45.059702 139705269999360 logging_writer.py:48] [29] global_step=29, grad_norm=0.256878, loss=0.145029
I0609 12:13:45.063273 139771734505280 submission.py:296] 29) loss = 0.145, grad_norm = 0.257
I0609 12:13:46.232434 139705278392064 logging_writer.py:48] [30] global_step=30, grad_norm=0.226885, loss=0.142564
I0609 12:13:46.235755 139771734505280 submission.py:296] 30) loss = 0.143, grad_norm = 0.227
I0609 12:13:47.403169 139705269999360 logging_writer.py:48] [31] global_step=31, grad_norm=0.208017, loss=0.144008
I0609 12:13:47.406453 139771734505280 submission.py:296] 31) loss = 0.144, grad_norm = 0.208
I0609 12:13:48.599110 139705278392064 logging_writer.py:48] [32] global_step=32, grad_norm=0.176226, loss=0.143973
I0609 12:13:48.602581 139771734505280 submission.py:296] 32) loss = 0.144, grad_norm = 0.176
I0609 12:13:49.772807 139705269999360 logging_writer.py:48] [33] global_step=33, grad_norm=0.148434, loss=0.146436
I0609 12:13:49.776639 139771734505280 submission.py:296] 33) loss = 0.146, grad_norm = 0.148
I0609 12:13:50.954598 139705278392064 logging_writer.py:48] [34] global_step=34, grad_norm=0.092016, loss=0.141376
I0609 12:13:50.958457 139771734505280 submission.py:296] 34) loss = 0.141, grad_norm = 0.092
I0609 12:13:52.124889 139705269999360 logging_writer.py:48] [35] global_step=35, grad_norm=0.052079, loss=0.140207
I0609 12:13:52.128327 139771734505280 submission.py:296] 35) loss = 0.140, grad_norm = 0.052
I0609 12:13:53.330694 139705278392064 logging_writer.py:48] [36] global_step=36, grad_norm=0.030111, loss=0.137047
I0609 12:13:53.334021 139771734505280 submission.py:296] 36) loss = 0.137, grad_norm = 0.030
I0609 12:13:54.501698 139705269999360 logging_writer.py:48] [37] global_step=37, grad_norm=0.050657, loss=0.136184
I0609 12:13:54.505213 139771734505280 submission.py:296] 37) loss = 0.136, grad_norm = 0.051
I0609 12:13:55.671661 139705278392064 logging_writer.py:48] [38] global_step=38, grad_norm=0.056834, loss=0.138536
I0609 12:13:55.675033 139771734505280 submission.py:296] 38) loss = 0.139, grad_norm = 0.057
I0609 12:13:56.837353 139705269999360 logging_writer.py:48] [39] global_step=39, grad_norm=0.070795, loss=0.139610
I0609 12:13:56.840808 139771734505280 submission.py:296] 39) loss = 0.140, grad_norm = 0.071
I0609 12:13:58.007113 139705278392064 logging_writer.py:48] [40] global_step=40, grad_norm=0.086056, loss=0.137484
I0609 12:13:58.010437 139771734505280 submission.py:296] 40) loss = 0.137, grad_norm = 0.086
I0609 12:13:59.169342 139705269999360 logging_writer.py:48] [41] global_step=41, grad_norm=0.085533, loss=0.136816
I0609 12:13:59.172793 139771734505280 submission.py:296] 41) loss = 0.137, grad_norm = 0.086
I0609 12:14:00.338609 139705278392064 logging_writer.py:48] [42] global_step=42, grad_norm=0.069818, loss=0.139833
I0609 12:14:00.342170 139771734505280 submission.py:296] 42) loss = 0.140, grad_norm = 0.070
I0609 12:14:01.573610 139705269999360 logging_writer.py:48] [43] global_step=43, grad_norm=0.062102, loss=0.138581
I0609 12:14:01.578774 139771734505280 submission.py:296] 43) loss = 0.139, grad_norm = 0.062
I0609 12:14:02.733001 139705278392064 logging_writer.py:48] [44] global_step=44, grad_norm=0.050883, loss=0.136795
I0609 12:14:02.736370 139771734505280 submission.py:296] 44) loss = 0.137, grad_norm = 0.051
I0609 12:14:03.899661 139705269999360 logging_writer.py:48] [45] global_step=45, grad_norm=0.022819, loss=0.139046
I0609 12:14:03.903143 139771734505280 submission.py:296] 45) loss = 0.139, grad_norm = 0.023
I0609 12:14:05.067163 139705278392064 logging_writer.py:48] [46] global_step=46, grad_norm=0.016751, loss=0.136295
I0609 12:14:05.070550 139771734505280 submission.py:296] 46) loss = 0.136, grad_norm = 0.017
I0609 12:14:06.233919 139705269999360 logging_writer.py:48] [47] global_step=47, grad_norm=0.038584, loss=0.141043
I0609 12:14:06.237591 139771734505280 submission.py:296] 47) loss = 0.141, grad_norm = 0.039
I0609 12:14:07.403640 139705278392064 logging_writer.py:48] [48] global_step=48, grad_norm=0.035613, loss=0.137523
I0609 12:14:07.406929 139771734505280 submission.py:296] 48) loss = 0.138, grad_norm = 0.036
I0609 12:14:08.561009 139705269999360 logging_writer.py:48] [49] global_step=49, grad_norm=0.041680, loss=0.137998
I0609 12:14:08.564466 139771734505280 submission.py:296] 49) loss = 0.138, grad_norm = 0.042
I0609 12:14:09.745189 139705278392064 logging_writer.py:48] [50] global_step=50, grad_norm=0.030030, loss=0.136356
I0609 12:14:09.749177 139771734505280 submission.py:296] 50) loss = 0.136, grad_norm = 0.030
I0609 12:14:10.907947 139705269999360 logging_writer.py:48] [51] global_step=51, grad_norm=0.031213, loss=0.137867
I0609 12:14:10.912083 139771734505280 submission.py:296] 51) loss = 0.138, grad_norm = 0.031
I0609 12:14:12.074675 139705278392064 logging_writer.py:48] [52] global_step=52, grad_norm=0.026735, loss=0.138451
I0609 12:14:12.078092 139771734505280 submission.py:296] 52) loss = 0.138, grad_norm = 0.027
I0609 12:14:13.244661 139705269999360 logging_writer.py:48] [53] global_step=53, grad_norm=0.012417, loss=0.136215
I0609 12:14:13.247886 139771734505280 submission.py:296] 53) loss = 0.136, grad_norm = 0.012
I0609 12:14:14.410590 139705278392064 logging_writer.py:48] [54] global_step=54, grad_norm=0.009636, loss=0.136944
I0609 12:14:14.413836 139771734505280 submission.py:296] 54) loss = 0.137, grad_norm = 0.010
I0609 12:14:15.575422 139705269999360 logging_writer.py:48] [55] global_step=55, grad_norm=0.019668, loss=0.136212
I0609 12:14:15.578710 139771734505280 submission.py:296] 55) loss = 0.136, grad_norm = 0.020
I0609 12:14:16.733008 139705278392064 logging_writer.py:48] [56] global_step=56, grad_norm=0.013842, loss=0.139056
I0609 12:14:16.736329 139771734505280 submission.py:296] 56) loss = 0.139, grad_norm = 0.014
I0609 12:14:17.900046 139705269999360 logging_writer.py:48] [57] global_step=57, grad_norm=0.031052, loss=0.135423
I0609 12:14:17.903376 139771734505280 submission.py:296] 57) loss = 0.135, grad_norm = 0.031
I0609 12:14:19.079456 139705278392064 logging_writer.py:48] [58] global_step=58, grad_norm=0.026261, loss=0.136397
I0609 12:14:19.082803 139771734505280 submission.py:296] 58) loss = 0.136, grad_norm = 0.026
I0609 12:14:20.245448 139705269999360 logging_writer.py:48] [59] global_step=59, grad_norm=0.012583, loss=0.137597
I0609 12:14:20.248794 139771734505280 submission.py:296] 59) loss = 0.138, grad_norm = 0.013
I0609 12:14:21.408476 139705278392064 logging_writer.py:48] [60] global_step=60, grad_norm=0.013748, loss=0.133267
I0609 12:14:21.411828 139771734505280 submission.py:296] 60) loss = 0.133, grad_norm = 0.014
I0609 12:14:22.588221 139705269999360 logging_writer.py:48] [61] global_step=61, grad_norm=0.013515, loss=0.134386
I0609 12:14:22.591595 139771734505280 submission.py:296] 61) loss = 0.134, grad_norm = 0.014
I0609 12:14:23.753035 139705278392064 logging_writer.py:48] [62] global_step=62, grad_norm=0.021394, loss=0.136376
I0609 12:14:23.756460 139771734505280 submission.py:296] 62) loss = 0.136, grad_norm = 0.021
I0609 12:14:24.910371 139705269999360 logging_writer.py:48] [63] global_step=63, grad_norm=0.009765, loss=0.132593
I0609 12:14:24.913781 139771734505280 submission.py:296] 63) loss = 0.133, grad_norm = 0.010
I0609 12:14:26.063506 139705278392064 logging_writer.py:48] [64] global_step=64, grad_norm=0.013604, loss=0.132674
I0609 12:14:26.066969 139771734505280 submission.py:296] 64) loss = 0.133, grad_norm = 0.014
I0609 12:14:27.227054 139705269999360 logging_writer.py:48] [65] global_step=65, grad_norm=0.013577, loss=0.133939
I0609 12:14:27.230423 139771734505280 submission.py:296] 65) loss = 0.134, grad_norm = 0.014
I0609 12:14:28.394397 139705278392064 logging_writer.py:48] [66] global_step=66, grad_norm=0.012710, loss=0.135158
I0609 12:14:28.397626 139771734505280 submission.py:296] 66) loss = 0.135, grad_norm = 0.013
I0609 12:14:29.561041 139705269999360 logging_writer.py:48] [67] global_step=67, grad_norm=0.007393, loss=0.135032
I0609 12:14:29.564881 139771734505280 submission.py:296] 67) loss = 0.135, grad_norm = 0.007
I0609 12:14:30.728839 139705278392064 logging_writer.py:48] [68] global_step=68, grad_norm=0.010554, loss=0.136053
I0609 12:14:30.732110 139771734505280 submission.py:296] 68) loss = 0.136, grad_norm = 0.011
I0609 12:14:31.932761 139705269999360 logging_writer.py:48] [69] global_step=69, grad_norm=0.011897, loss=0.133709
I0609 12:14:31.936752 139771734505280 submission.py:296] 69) loss = 0.134, grad_norm = 0.012
I0609 12:14:33.103959 139705278392064 logging_writer.py:48] [70] global_step=70, grad_norm=0.007036, loss=0.133231
I0609 12:14:33.107226 139771734505280 submission.py:296] 70) loss = 0.133, grad_norm = 0.007
I0609 12:14:34.262475 139705269999360 logging_writer.py:48] [71] global_step=71, grad_norm=0.023806, loss=0.136793
I0609 12:14:34.265900 139771734505280 submission.py:296] 71) loss = 0.137, grad_norm = 0.024
I0609 12:14:35.428245 139705278392064 logging_writer.py:48] [72] global_step=72, grad_norm=0.007703, loss=0.135166
I0609 12:14:35.431626 139771734505280 submission.py:296] 72) loss = 0.135, grad_norm = 0.008
I0609 12:14:36.596939 139705269999360 logging_writer.py:48] [73] global_step=73, grad_norm=0.007741, loss=0.135999
I0609 12:14:36.600070 139771734505280 submission.py:296] 73) loss = 0.136, grad_norm = 0.008
I0609 12:14:37.747108 139705278392064 logging_writer.py:48] [74] global_step=74, grad_norm=0.008210, loss=0.134751
I0609 12:14:37.750504 139771734505280 submission.py:296] 74) loss = 0.135, grad_norm = 0.008
I0609 12:14:38.915349 139705269999360 logging_writer.py:48] [75] global_step=75, grad_norm=0.006173, loss=0.132285
I0609 12:14:38.918679 139771734505280 submission.py:296] 75) loss = 0.132, grad_norm = 0.006
I0609 12:14:40.079795 139705278392064 logging_writer.py:48] [76] global_step=76, grad_norm=0.030255, loss=0.138801
I0609 12:14:40.083110 139771734505280 submission.py:296] 76) loss = 0.139, grad_norm = 0.030
I0609 12:14:41.242656 139705269999360 logging_writer.py:48] [77] global_step=77, grad_norm=0.010361, loss=0.139877
I0609 12:14:41.245973 139771734505280 submission.py:296] 77) loss = 0.140, grad_norm = 0.010
I0609 12:14:42.405143 139705278392064 logging_writer.py:48] [78] global_step=78, grad_norm=0.009874, loss=0.139891
I0609 12:14:42.408541 139771734505280 submission.py:296] 78) loss = 0.140, grad_norm = 0.010
I0609 12:14:43.567911 139705269999360 logging_writer.py:48] [79] global_step=79, grad_norm=0.008858, loss=0.139376
I0609 12:14:43.571243 139771734505280 submission.py:296] 79) loss = 0.139, grad_norm = 0.009
I0609 12:14:44.733434 139705278392064 logging_writer.py:48] [80] global_step=80, grad_norm=0.010747, loss=0.139318
I0609 12:14:44.736740 139771734505280 submission.py:296] 80) loss = 0.139, grad_norm = 0.011
I0609 12:14:45.927182 139705269999360 logging_writer.py:48] [81] global_step=81, grad_norm=0.008206, loss=0.139916
I0609 12:14:45.930623 139771734505280 submission.py:296] 81) loss = 0.140, grad_norm = 0.008
I0609 12:14:47.086785 139705278392064 logging_writer.py:48] [82] global_step=82, grad_norm=0.012208, loss=0.138955
I0609 12:14:47.090329 139771734505280 submission.py:296] 82) loss = 0.139, grad_norm = 0.012
I0609 12:14:48.257754 139705269999360 logging_writer.py:48] [83] global_step=83, grad_norm=0.015185, loss=0.138939
I0609 12:14:48.261021 139771734505280 submission.py:296] 83) loss = 0.139, grad_norm = 0.015
I0609 12:14:49.420166 139705278392064 logging_writer.py:48] [84] global_step=84, grad_norm=0.012610, loss=0.138893
I0609 12:14:49.423522 139771734505280 submission.py:296] 84) loss = 0.139, grad_norm = 0.013
I0609 12:14:50.581170 139705269999360 logging_writer.py:48] [85] global_step=85, grad_norm=0.013638, loss=0.138811
I0609 12:14:50.584518 139771734505280 submission.py:296] 85) loss = 0.139, grad_norm = 0.014
I0609 12:14:51.745089 139705278392064 logging_writer.py:48] [86] global_step=86, grad_norm=0.017682, loss=0.140794
I0609 12:14:51.748378 139771734505280 submission.py:296] 86) loss = 0.141, grad_norm = 0.018
I0609 12:14:52.920771 139705269999360 logging_writer.py:48] [87] global_step=87, grad_norm=0.015787, loss=0.138095
I0609 12:14:52.924049 139771734505280 submission.py:296] 87) loss = 0.138, grad_norm = 0.016
I0609 12:14:54.113344 139705278392064 logging_writer.py:48] [88] global_step=88, grad_norm=0.042926, loss=0.136583
I0609 12:14:54.116738 139771734505280 submission.py:296] 88) loss = 0.137, grad_norm = 0.043
I0609 12:14:55.285892 139705269999360 logging_writer.py:48] [89] global_step=89, grad_norm=0.113087, loss=0.137746
I0609 12:14:55.289170 139771734505280 submission.py:296] 89) loss = 0.138, grad_norm = 0.113
I0609 12:14:56.452142 139705278392064 logging_writer.py:48] [90] global_step=90, grad_norm=0.141420, loss=0.139573
I0609 12:14:56.455386 139771734505280 submission.py:296] 90) loss = 0.140, grad_norm = 0.141
I0609 12:14:57.625486 139705269999360 logging_writer.py:48] [91] global_step=91, grad_norm=0.064031, loss=0.134659
I0609 12:14:57.628912 139771734505280 submission.py:296] 91) loss = 0.135, grad_norm = 0.064
I0609 12:14:58.792006 139705278392064 logging_writer.py:48] [92] global_step=92, grad_norm=0.011702, loss=0.134964
I0609 12:14:58.795691 139771734505280 submission.py:296] 92) loss = 0.135, grad_norm = 0.012
I0609 12:14:59.954453 139705269999360 logging_writer.py:48] [93] global_step=93, grad_norm=0.022504, loss=0.137481
I0609 12:14:59.957989 139771734505280 submission.py:296] 93) loss = 0.137, grad_norm = 0.023
I0609 12:15:01.140377 139705278392064 logging_writer.py:48] [94] global_step=94, grad_norm=0.045437, loss=0.135348
I0609 12:15:01.143932 139771734505280 submission.py:296] 94) loss = 0.135, grad_norm = 0.045
I0609 12:15:02.421160 139705269999360 logging_writer.py:48] [95] global_step=95, grad_norm=0.060163, loss=0.131223
I0609 12:15:02.424687 139771734505280 submission.py:296] 95) loss = 0.131, grad_norm = 0.060
I0609 12:15:03.587105 139705278392064 logging_writer.py:48] [96] global_step=96, grad_norm=0.076107, loss=0.129056
I0609 12:15:03.590487 139771734505280 submission.py:296] 96) loss = 0.129, grad_norm = 0.076
I0609 12:15:04.784740 139705269999360 logging_writer.py:48] [97] global_step=97, grad_norm=0.082697, loss=0.128897
I0609 12:15:04.788524 139771734505280 submission.py:296] 97) loss = 0.129, grad_norm = 0.083
I0609 12:15:05.980766 139705278392064 logging_writer.py:48] [98] global_step=98, grad_norm=0.138469, loss=0.126923
I0609 12:15:05.984158 139771734505280 submission.py:296] 98) loss = 0.127, grad_norm = 0.138
I0609 12:15:07.150807 139705269999360 logging_writer.py:48] [99] global_step=99, grad_norm=0.160332, loss=0.126564
I0609 12:15:07.154237 139771734505280 submission.py:296] 99) loss = 0.127, grad_norm = 0.160
I0609 12:15:08.325803 139705278392064 logging_writer.py:48] [100] global_step=100, grad_norm=0.124923, loss=0.127065
I0609 12:15:08.329469 139771734505280 submission.py:296] 100) loss = 0.127, grad_norm = 0.125
I0609 12:15:11.823219 139771734505280 spec.py:298] Evaluating on the training split.
I0609 12:19:45.299788 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 12:24:20.815010 139771734505280 spec.py:326] Evaluating on the test split.
I0609 12:28:35.203781 139771734505280 submission_runner.py:419] Time since start: 1783.13s, 	Step: 104, 	{'train/loss': 0.13273625096650227, 'validation/loss': 0.13200874157303372, 'validation/num_examples': 89000000, 'test/loss': 0.13510234715376104, 'test/num_examples': 89274637, 'score': 126.98406910896301, 'total_duration': 1783.1280653476715, 'accumulated_submission_time': 126.98406910896301, 'accumulated_eval_time': 1656.083083152771, 'accumulated_logging_time': 0.024466991424560547}
I0609 12:28:35.214153 139705269999360 logging_writer.py:48] [104] accumulated_eval_time=1656.083083, accumulated_logging_time=0.024467, accumulated_submission_time=126.984069, global_step=104, preemption_count=0, score=126.984069, test/loss=0.135102, test/num_examples=89274637, total_duration=1783.128065, train/loss=0.132736, validation/loss=0.132009, validation/num_examples=89000000
I0609 12:30:35.272626 139771734505280 spec.py:298] Evaluating on the training split.
I0609 12:35:19.647268 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 12:39:56.025385 139771734505280 spec.py:326] Evaluating on the test split.
I0609 12:44:18.847945 139771734505280 submission_runner.py:419] Time since start: 2726.77s, 	Step: 206, 	{'train/loss': 0.1286713948927003, 'validation/loss': 0.128670202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13183275110936604, 'test/num_examples': 89274637, 'score': 246.99238228797913, 'total_duration': 2726.7722606658936, 'accumulated_submission_time': 246.99238228797913, 'accumulated_eval_time': 2479.658311843872, 'accumulated_logging_time': 0.0419316291809082}
I0609 12:44:18.858082 139705278392064 logging_writer.py:48] [206] accumulated_eval_time=2479.658312, accumulated_logging_time=0.041932, accumulated_submission_time=246.992382, global_step=206, preemption_count=0, score=246.992382, test/loss=0.131833, test/num_examples=89274637, total_duration=2726.772261, train/loss=0.128671, validation/loss=0.128670, validation/num_examples=89000000
I0609 12:46:19.807608 139771734505280 spec.py:298] Evaluating on the training split.
I0609 12:50:50.788790 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 12:55:30.176751 139771734505280 spec.py:326] Evaluating on the test split.
I0609 13:00:04.815988 139771734505280 submission_runner.py:419] Time since start: 3672.74s, 	Step: 309, 	{'train/loss': 0.1279878146162447, 'validation/loss': 0.1270848651685393, 'validation/num_examples': 89000000, 'test/loss': 0.13010868921259236, 'test/num_examples': 89274637, 'score': 367.89371609687805, 'total_duration': 3672.740253686905, 'accumulated_submission_time': 367.89371609687805, 'accumulated_eval_time': 3304.666560649872, 'accumulated_logging_time': 0.05884289741516113}
I0609 13:00:04.826066 139705269999360 logging_writer.py:48] [309] accumulated_eval_time=3304.666561, accumulated_logging_time=0.058843, accumulated_submission_time=367.893716, global_step=309, preemption_count=0, score=367.893716, test/loss=0.130109, test/num_examples=89274637, total_duration=3672.740254, train/loss=0.127988, validation/loss=0.127085, validation/num_examples=89000000
I0609 13:02:05.219763 139771734505280 spec.py:298] Evaluating on the training split.
I0609 13:06:47.064661 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 13:11:25.880067 139771734505280 spec.py:326] Evaluating on the test split.
I0609 13:15:49.462647 139771734505280 submission_runner.py:419] Time since start: 4617.39s, 	Step: 411, 	{'train/loss': 0.12663425111525017, 'validation/loss': 0.12702922471910114, 'validation/num_examples': 89000000, 'test/loss': 0.12985831574985848, 'test/num_examples': 89274637, 'score': 488.23723673820496, 'total_duration': 4617.3869433403015, 'accumulated_submission_time': 488.23723673820496, 'accumulated_eval_time': 4128.90936923027, 'accumulated_logging_time': 0.07611560821533203}
I0609 13:15:49.472914 139705278392064 logging_writer.py:48] [411] accumulated_eval_time=4128.909369, accumulated_logging_time=0.076116, accumulated_submission_time=488.237237, global_step=411, preemption_count=0, score=488.237237, test/loss=0.129858, test/num_examples=89274637, total_duration=4617.386943, train/loss=0.126634, validation/loss=0.127029, validation/num_examples=89000000
I0609 13:17:35.434367 139705269999360 logging_writer.py:48] [500] global_step=500, grad_norm=0.043666, loss=0.123524
I0609 13:17:35.438061 139771734505280 submission.py:296] 500) loss = 0.124, grad_norm = 0.044
I0609 13:17:50.577058 139771734505280 spec.py:298] Evaluating on the training split.
I0609 13:22:35.061124 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 13:27:13.542521 139771734505280 spec.py:326] Evaluating on the test split.
I0609 13:31:38.022524 139771734505280 submission_runner.py:419] Time since start: 5565.95s, 	Step: 514, 	{'train/loss': 0.1249267937419518, 'validation/loss': 0.1268436966292135, 'validation/num_examples': 89000000, 'test/loss': 0.12976934311141472, 'test/num_examples': 89274637, 'score': 609.2934358119965, 'total_duration': 5565.946829795837, 'accumulated_submission_time': 609.2934358119965, 'accumulated_eval_time': 4956.3547031879425, 'accumulated_logging_time': 0.09395384788513184}
I0609 13:31:38.032676 139705278392064 logging_writer.py:48] [514] accumulated_eval_time=4956.354703, accumulated_logging_time=0.093954, accumulated_submission_time=609.293436, global_step=514, preemption_count=0, score=609.293436, test/loss=0.129769, test/num_examples=89274637, total_duration=5565.946830, train/loss=0.124927, validation/loss=0.126844, validation/num_examples=89000000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0609 13:33:39.399845 139771734505280 spec.py:298] Evaluating on the training split.
I0609 13:38:17.108077 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 13:42:56.061011 139771734505280 spec.py:326] Evaluating on the test split.
I0609 13:47:24.684576 139771734505280 submission_runner.py:419] Time since start: 6512.61s, 	Step: 594, 	{'train/loss': 0.12643232759611847, 'validation/loss': 0.12655056179775281, 'validation/num_examples': 89000000, 'test/loss': 0.12935857694946437, 'test/num_examples': 89274637, 'score': 730.6188862323761, 'total_duration': 6512.6088235378265, 'accumulated_submission_time': 730.6188862323761, 'accumulated_eval_time': 5781.639311552048, 'accumulated_logging_time': 0.11118412017822266}
I0609 13:47:24.695391 139705269999360 logging_writer.py:48] [594] accumulated_eval_time=5781.639312, accumulated_logging_time=0.111184, accumulated_submission_time=730.618886, global_step=594, preemption_count=0, score=730.618886, test/loss=0.129359, test/num_examples=89274637, total_duration=6512.608824, train/loss=0.126432, validation/loss=0.126551, validation/num_examples=89000000
I0609 13:49:25.039433 139771734505280 spec.py:298] Evaluating on the training split.
I0609 13:54:00.502314 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 13:58:39.839127 139771734505280 spec.py:326] Evaluating on the test split.
I0609 14:03:05.988686 139771734505280 submission_runner.py:419] Time since start: 7453.91s, 	Step: 668, 	{'train/loss': 0.1240516421196652, 'validation/loss': 0.12663110112359552, 'validation/num_examples': 89000000, 'test/loss': 0.12933795519101354, 'test/num_examples': 89274637, 'score': 850.9250977039337, 'total_duration': 7453.913006305695, 'accumulated_submission_time': 850.9250977039337, 'accumulated_eval_time': 6602.588476657867, 'accumulated_logging_time': 0.1290287971496582}
I0609 14:03:05.999513 139705278392064 logging_writer.py:48] [668] accumulated_eval_time=6602.588477, accumulated_logging_time=0.129029, accumulated_submission_time=850.925098, global_step=668, preemption_count=0, score=850.925098, test/loss=0.129338, test/num_examples=89274637, total_duration=7453.913006, train/loss=0.124052, validation/loss=0.126631, validation/num_examples=89000000
I0609 14:05:06.681118 139771734505280 spec.py:298] Evaluating on the training split.
I0609 14:09:40.169707 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 14:14:19.136652 139771734505280 spec.py:326] Evaluating on the test split.
I0609 14:18:46.778964 139771734505280 submission_runner.py:419] Time since start: 8394.70s, 	Step: 762, 	{'train/loss': 0.12394655970109571, 'validation/loss': 0.1262403033707865, 'validation/num_examples': 89000000, 'test/loss': 0.12877229621219294, 'test/num_examples': 89274637, 'score': 971.5617439746857, 'total_duration': 8394.703275203705, 'accumulated_submission_time': 971.5617439746857, 'accumulated_eval_time': 7422.6862325668335, 'accumulated_logging_time': 0.14650821685791016}
I0609 14:18:46.789031 139705269999360 logging_writer.py:48] [762] accumulated_eval_time=7422.686233, accumulated_logging_time=0.146508, accumulated_submission_time=971.561744, global_step=762, preemption_count=0, score=971.561744, test/loss=0.128772, test/num_examples=89274637, total_duration=8394.703275, train/loss=0.123947, validation/loss=0.126240, validation/num_examples=89000000
I0609 14:20:46.970078 139771734505280 spec.py:298] Evaluating on the training split.
I0609 14:25:25.117110 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 14:30:06.498067 139771734505280 spec.py:326] Evaluating on the test split.
I0609 14:34:44.107378 139771734505280 submission_runner.py:419] Time since start: 9352.03s, 	Step: 859, 	{'train/loss': 0.12283129828918322, 'validation/loss': 0.12618611235955057, 'validation/num_examples': 89000000, 'test/loss': 0.1287918202344525, 'test/num_examples': 89274637, 'score': 1091.6952495574951, 'total_duration': 9352.031713724136, 'accumulated_submission_time': 1091.6952495574951, 'accumulated_eval_time': 8259.823496341705, 'accumulated_logging_time': 0.16450119018554688}
I0609 14:34:44.118852 139705278392064 logging_writer.py:48] [859] accumulated_eval_time=8259.823496, accumulated_logging_time=0.164501, accumulated_submission_time=1091.695250, global_step=859, preemption_count=0, score=1091.695250, test/loss=0.128792, test/num_examples=89274637, total_duration=9352.031714, train/loss=0.122831, validation/loss=0.126186, validation/num_examples=89000000
I0609 14:36:44.463655 139771734505280 spec.py:298] Evaluating on the training split.
I0609 14:41:25.449711 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 14:46:04.522911 139771734505280 spec.py:326] Evaluating on the test split.
I0609 14:50:30.904076 139771734505280 submission_runner.py:419] Time since start: 10298.83s, 	Step: 956, 	{'train/loss': 0.12427265316006715, 'validation/loss': 0.12611098876404495, 'validation/num_examples': 89000000, 'test/loss': 0.12891428502812058, 'test/num_examples': 89274637, 'score': 1211.9942626953125, 'total_duration': 10298.828360080719, 'accumulated_submission_time': 1211.9942626953125, 'accumulated_eval_time': 9086.263801813126, 'accumulated_logging_time': 0.18286800384521484}
I0609 14:50:30.914490 139705269999360 logging_writer.py:48] [956] accumulated_eval_time=9086.263802, accumulated_logging_time=0.182868, accumulated_submission_time=1211.994263, global_step=956, preemption_count=0, score=1211.994263, test/loss=0.128914, test/num_examples=89274637, total_duration=10298.828360, train/loss=0.124273, validation/loss=0.126111, validation/num_examples=89000000
I0609 14:51:28.306157 139705278392064 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.013746, loss=0.120380
I0609 14:51:28.310034 139771734505280 submission.py:296] 1000) loss = 0.120, grad_norm = 0.014
I0609 14:52:31.323625 139771734505280 spec.py:298] Evaluating on the training split.
I0609 14:57:09.668062 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 15:01:48.630175 139771734505280 spec.py:326] Evaluating on the test split.
I0609 15:06:14.158897 139771734505280 submission_runner.py:419] Time since start: 11242.08s, 	Step: 1052, 	{'train/loss': 0.12530346913228937, 'validation/loss': 0.1260410561797753, 'validation/num_examples': 89000000, 'test/loss': 0.12870891874922996, 'test/num_examples': 89274637, 'score': 1332.3588399887085, 'total_duration': 11242.083186388016, 'accumulated_submission_time': 1332.3588399887085, 'accumulated_eval_time': 9909.098937511444, 'accumulated_logging_time': 0.20011663436889648}
I0609 15:06:14.169458 139705269999360 logging_writer.py:48] [1052] accumulated_eval_time=9909.098938, accumulated_logging_time=0.200117, accumulated_submission_time=1332.358840, global_step=1052, preemption_count=0, score=1332.358840, test/loss=0.128709, test/num_examples=89274637, total_duration=11242.083186, train/loss=0.125303, validation/loss=0.126041, validation/num_examples=89000000
I0609 15:08:14.231844 139771734505280 spec.py:298] Evaluating on the training split.
I0609 15:12:54.269684 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 15:17:35.444112 139771734505280 spec.py:326] Evaluating on the test split.
I0609 15:22:14.175544 139771734505280 submission_runner.py:419] Time since start: 12202.10s, 	Step: 1153, 	{'train/loss': 0.1253333691729615, 'validation/loss': 0.125925202247191, 'validation/num_examples': 89000000, 'test/loss': 0.1286031664290049, 'test/num_examples': 89274637, 'score': 1452.3731768131256, 'total_duration': 12202.099821329117, 'accumulated_submission_time': 1452.3731768131256, 'accumulated_eval_time': 10749.042487859726, 'accumulated_logging_time': 0.21769237518310547}
I0609 15:22:14.185675 139705278392064 logging_writer.py:48] [1153] accumulated_eval_time=10749.042488, accumulated_logging_time=0.217692, accumulated_submission_time=1452.373177, global_step=1153, preemption_count=0, score=1452.373177, test/loss=0.128603, test/num_examples=89274637, total_duration=12202.099821, train/loss=0.125333, validation/loss=0.125925, validation/num_examples=89000000
I0609 15:24:15.273906 139771734505280 spec.py:298] Evaluating on the training split.
I0609 15:28:51.961252 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 15:33:30.454883 139771734505280 spec.py:326] Evaluating on the test split.
I0609 15:38:07.055428 139771734505280 submission_runner.py:419] Time since start: 13154.98s, 	Step: 1227, 	{'train/loss': 0.12304330451072709, 'validation/loss': 0.12631329213483147, 'validation/num_examples': 89000000, 'test/loss': 0.1283604211126616, 'test/num_examples': 89274637, 'score': 1573.4229826927185, 'total_duration': 13154.979719877243, 'accumulated_submission_time': 1573.4229826927185, 'accumulated_eval_time': 11580.82388472557, 'accumulated_logging_time': 0.23485946655273438}
I0609 15:38:07.065440 139705269999360 logging_writer.py:48] [1227] accumulated_eval_time=11580.823885, accumulated_logging_time=0.234859, accumulated_submission_time=1573.422983, global_step=1227, preemption_count=0, score=1573.422983, test/loss=0.128360, test/num_examples=89274637, total_duration=13154.979720, train/loss=0.123043, validation/loss=0.126313, validation/num_examples=89000000
I0609 15:40:07.710020 139771734505280 spec.py:298] Evaluating on the training split.
I0609 15:44:50.821133 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 15:49:28.710946 139771734505280 spec.py:326] Evaluating on the test split.
I0609 15:53:51.853363 139771734505280 submission_runner.py:419] Time since start: 14099.78s, 	Step: 1319, 	{'train/loss': 0.12559535101637234, 'validation/loss': 0.12633303370786517, 'validation/num_examples': 89000000, 'test/loss': 0.12858583787912797, 'test/num_examples': 89274637, 'score': 1694.0234179496765, 'total_duration': 14099.777683496475, 'accumulated_submission_time': 1694.0234179496765, 'accumulated_eval_time': 12404.967156410217, 'accumulated_logging_time': 0.2518792152404785}
I0609 15:53:51.863890 139705278392064 logging_writer.py:48] [1319] accumulated_eval_time=12404.967156, accumulated_logging_time=0.251879, accumulated_submission_time=1694.023418, global_step=1319, preemption_count=0, score=1694.023418, test/loss=0.128586, test/num_examples=89274637, total_duration=14099.777683, train/loss=0.125595, validation/loss=0.126333, validation/num_examples=89000000
I0609 15:55:51.886626 139771734505280 spec.py:298] Evaluating on the training split.
I0609 16:00:27.716671 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 16:05:09.513018 139771734505280 spec.py:326] Evaluating on the test split.
I0609 16:09:50.050159 139771734505280 submission_runner.py:419] Time since start: 15057.97s, 	Step: 1414, 	{'train/loss': 0.12406731634301417, 'validation/loss': 0.12529501123595504, 'validation/num_examples': 89000000, 'test/loss': 0.12802200472682965, 'test/num_examples': 89274637, 'score': 1814.00048828125, 'total_duration': 15057.974485635757, 'accumulated_submission_time': 1814.00048828125, 'accumulated_eval_time': 13243.130616664886, 'accumulated_logging_time': 0.2701282501220703}
I0609 16:09:50.060382 139705269999360 logging_writer.py:48] [1414] accumulated_eval_time=13243.130617, accumulated_logging_time=0.270128, accumulated_submission_time=1814.000488, global_step=1414, preemption_count=0, score=1814.000488, test/loss=0.128022, test/num_examples=89274637, total_duration=15057.974486, train/loss=0.124067, validation/loss=0.125295, validation/num_examples=89000000
I0609 16:11:50.556838 139771734505280 spec.py:298] Evaluating on the training split.
I0609 16:16:33.780619 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 16:21:14.983973 139771734505280 spec.py:326] Evaluating on the test split.
I0609 16:25:41.165697 139771734505280 submission_runner.py:419] Time since start: 16009.09s, 	Step: 1489, 	{'train/loss': 0.12358201050074158, 'validation/loss': 0.125270797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1279746676539273, 'test/num_examples': 89274637, 'score': 1934.4587030410767, 'total_duration': 16009.089979887009, 'accumulated_submission_time': 1934.4587030410767, 'accumulated_eval_time': 14073.739357233047, 'accumulated_logging_time': 0.2871439456939697}
I0609 16:25:41.176897 139705278392064 logging_writer.py:48] [1489] accumulated_eval_time=14073.739357, accumulated_logging_time=0.287144, accumulated_submission_time=1934.458703, global_step=1489, preemption_count=0, score=1934.458703, test/loss=0.127975, test/num_examples=89274637, total_duration=16009.089980, train/loss=0.123582, validation/loss=0.125271, validation/num_examples=89000000
I0609 16:25:57.596466 139705269999360 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.007511, loss=0.122279
I0609 16:25:57.600030 139771734505280 submission.py:296] 1500) loss = 0.122, grad_norm = 0.008
I0609 16:27:41.631228 139771734505280 spec.py:298] Evaluating on the training split.
I0609 16:32:20.578126 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 16:37:00.421707 139771734505280 spec.py:326] Evaluating on the test split.
I0609 16:41:38.680213 139771734505280 submission_runner.py:419] Time since start: 16966.60s, 	Step: 1582, 	{'train/loss': 0.12508142512014808, 'validation/loss': 0.1254138202247191, 'validation/num_examples': 89000000, 'test/loss': 0.12817599023113363, 'test/num_examples': 89274637, 'score': 2054.8685047626495, 'total_duration': 16966.604539871216, 'accumulated_submission_time': 2054.8685047626495, 'accumulated_eval_time': 14910.788256168365, 'accumulated_logging_time': 0.30548906326293945}
I0609 16:41:38.691346 139705278392064 logging_writer.py:48] [1582] accumulated_eval_time=14910.788256, accumulated_logging_time=0.305489, accumulated_submission_time=2054.868505, global_step=1582, preemption_count=0, score=2054.868505, test/loss=0.128176, test/num_examples=89274637, total_duration=16966.604540, train/loss=0.125081, validation/loss=0.125414, validation/num_examples=89000000
I0609 16:42:02.132226 139771734505280 spec.py:298] Evaluating on the training split.
I0609 16:46:44.622083 139771734505280 spec.py:310] Evaluating on the validation split.
I0609 16:51:26.257176 139771734505280 spec.py:326] Evaluating on the test split.
I0609 16:55:51.962327 139771734505280 submission_runner.py:419] Time since start: 17819.89s, 	Step: 1600, 	{'train/loss': 0.12318070098001173, 'validation/loss': 0.12539240449438202, 'validation/num_examples': 89000000, 'test/loss': 0.12815927775769057, 'test/num_examples': 89274637, 'score': 2078.292751312256, 'total_duration': 17819.88662457466, 'accumulated_submission_time': 2078.292751312256, 'accumulated_eval_time': 15740.618307352066, 'accumulated_logging_time': 0.32465028762817383}
I0609 16:55:51.973036 139705269999360 logging_writer.py:48] [1600] accumulated_eval_time=15740.618307, accumulated_logging_time=0.324650, accumulated_submission_time=2078.292751, global_step=1600, preemption_count=0, score=2078.292751, test/loss=0.128159, test/num_examples=89274637, total_duration=17819.886625, train/loss=0.123181, validation/loss=0.125392, validation/num_examples=89000000
I0609 16:55:51.988268 139705278392064 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=2078.292751
I0609 16:56:03.239644 139771734505280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_b_pytorch/nadamw/criteo1tb_pytorch/trial_1/checkpoint_1600.
I0609 16:56:03.335979 139771734505280 submission_runner.py:581] Tuning trial 1/1
I0609 16:56:03.336192 139771734505280 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0609 16:56:03.337304 139771734505280 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/loss': 0.31335947739893766, 'validation/loss': 0.3096135056179775, 'validation/num_examples': 89000000, 'test/loss': 0.3119057431731702, 'test/num_examples': 89274637, 'score': 5.944623708724976, 'total_duration': 858.6477429866791, 'accumulated_submission_time': 5.944623708724976, 'accumulated_eval_time': 852.7026681900024, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (104, {'train/loss': 0.13273625096650227, 'validation/loss': 0.13200874157303372, 'validation/num_examples': 89000000, 'test/loss': 0.13510234715376104, 'test/num_examples': 89274637, 'score': 126.98406910896301, 'total_duration': 1783.1280653476715, 'accumulated_submission_time': 126.98406910896301, 'accumulated_eval_time': 1656.083083152771, 'accumulated_logging_time': 0.024466991424560547, 'global_step': 104, 'preemption_count': 0}), (206, {'train/loss': 0.1286713948927003, 'validation/loss': 0.128670202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13183275110936604, 'test/num_examples': 89274637, 'score': 246.99238228797913, 'total_duration': 2726.7722606658936, 'accumulated_submission_time': 246.99238228797913, 'accumulated_eval_time': 2479.658311843872, 'accumulated_logging_time': 0.0419316291809082, 'global_step': 206, 'preemption_count': 0}), (309, {'train/loss': 0.1279878146162447, 'validation/loss': 0.1270848651685393, 'validation/num_examples': 89000000, 'test/loss': 0.13010868921259236, 'test/num_examples': 89274637, 'score': 367.89371609687805, 'total_duration': 3672.740253686905, 'accumulated_submission_time': 367.89371609687805, 'accumulated_eval_time': 3304.666560649872, 'accumulated_logging_time': 0.05884289741516113, 'global_step': 309, 'preemption_count': 0}), (411, {'train/loss': 0.12663425111525017, 'validation/loss': 0.12702922471910114, 'validation/num_examples': 89000000, 'test/loss': 0.12985831574985848, 'test/num_examples': 89274637, 'score': 488.23723673820496, 'total_duration': 4617.3869433403015, 'accumulated_submission_time': 488.23723673820496, 'accumulated_eval_time': 4128.90936923027, 'accumulated_logging_time': 0.07611560821533203, 'global_step': 411, 'preemption_count': 0}), (514, {'train/loss': 0.1249267937419518, 'validation/loss': 0.1268436966292135, 'validation/num_examples': 89000000, 'test/loss': 0.12976934311141472, 'test/num_examples': 89274637, 'score': 609.2934358119965, 'total_duration': 5565.946829795837, 'accumulated_submission_time': 609.2934358119965, 'accumulated_eval_time': 4956.3547031879425, 'accumulated_logging_time': 0.09395384788513184, 'global_step': 514, 'preemption_count': 0}), (594, {'train/loss': 0.12643232759611847, 'validation/loss': 0.12655056179775281, 'validation/num_examples': 89000000, 'test/loss': 0.12935857694946437, 'test/num_examples': 89274637, 'score': 730.6188862323761, 'total_duration': 6512.6088235378265, 'accumulated_submission_time': 730.6188862323761, 'accumulated_eval_time': 5781.639311552048, 'accumulated_logging_time': 0.11118412017822266, 'global_step': 594, 'preemption_count': 0}), (668, {'train/loss': 0.1240516421196652, 'validation/loss': 0.12663110112359552, 'validation/num_examples': 89000000, 'test/loss': 0.12933795519101354, 'test/num_examples': 89274637, 'score': 850.9250977039337, 'total_duration': 7453.913006305695, 'accumulated_submission_time': 850.9250977039337, 'accumulated_eval_time': 6602.588476657867, 'accumulated_logging_time': 0.1290287971496582, 'global_step': 668, 'preemption_count': 0}), (762, {'train/loss': 0.12394655970109571, 'validation/loss': 0.1262403033707865, 'validation/num_examples': 89000000, 'test/loss': 0.12877229621219294, 'test/num_examples': 89274637, 'score': 971.5617439746857, 'total_duration': 8394.703275203705, 'accumulated_submission_time': 971.5617439746857, 'accumulated_eval_time': 7422.6862325668335, 'accumulated_logging_time': 0.14650821685791016, 'global_step': 762, 'preemption_count': 0}), (859, {'train/loss': 0.12283129828918322, 'validation/loss': 0.12618611235955057, 'validation/num_examples': 89000000, 'test/loss': 0.1287918202344525, 'test/num_examples': 89274637, 'score': 1091.6952495574951, 'total_duration': 9352.031713724136, 'accumulated_submission_time': 1091.6952495574951, 'accumulated_eval_time': 8259.823496341705, 'accumulated_logging_time': 0.16450119018554688, 'global_step': 859, 'preemption_count': 0}), (956, {'train/loss': 0.12427265316006715, 'validation/loss': 0.12611098876404495, 'validation/num_examples': 89000000, 'test/loss': 0.12891428502812058, 'test/num_examples': 89274637, 'score': 1211.9942626953125, 'total_duration': 10298.828360080719, 'accumulated_submission_time': 1211.9942626953125, 'accumulated_eval_time': 9086.263801813126, 'accumulated_logging_time': 0.18286800384521484, 'global_step': 956, 'preemption_count': 0}), (1052, {'train/loss': 0.12530346913228937, 'validation/loss': 0.1260410561797753, 'validation/num_examples': 89000000, 'test/loss': 0.12870891874922996, 'test/num_examples': 89274637, 'score': 1332.3588399887085, 'total_duration': 11242.083186388016, 'accumulated_submission_time': 1332.3588399887085, 'accumulated_eval_time': 9909.098937511444, 'accumulated_logging_time': 0.20011663436889648, 'global_step': 1052, 'preemption_count': 0}), (1153, {'train/loss': 0.1253333691729615, 'validation/loss': 0.125925202247191, 'validation/num_examples': 89000000, 'test/loss': 0.1286031664290049, 'test/num_examples': 89274637, 'score': 1452.3731768131256, 'total_duration': 12202.099821329117, 'accumulated_submission_time': 1452.3731768131256, 'accumulated_eval_time': 10749.042487859726, 'accumulated_logging_time': 0.21769237518310547, 'global_step': 1153, 'preemption_count': 0}), (1227, {'train/loss': 0.12304330451072709, 'validation/loss': 0.12631329213483147, 'validation/num_examples': 89000000, 'test/loss': 0.1283604211126616, 'test/num_examples': 89274637, 'score': 1573.4229826927185, 'total_duration': 13154.979719877243, 'accumulated_submission_time': 1573.4229826927185, 'accumulated_eval_time': 11580.82388472557, 'accumulated_logging_time': 0.23485946655273438, 'global_step': 1227, 'preemption_count': 0}), (1319, {'train/loss': 0.12559535101637234, 'validation/loss': 0.12633303370786517, 'validation/num_examples': 89000000, 'test/loss': 0.12858583787912797, 'test/num_examples': 89274637, 'score': 1694.0234179496765, 'total_duration': 14099.777683496475, 'accumulated_submission_time': 1694.0234179496765, 'accumulated_eval_time': 12404.967156410217, 'accumulated_logging_time': 0.2518792152404785, 'global_step': 1319, 'preemption_count': 0}), (1414, {'train/loss': 0.12406731634301417, 'validation/loss': 0.12529501123595504, 'validation/num_examples': 89000000, 'test/loss': 0.12802200472682965, 'test/num_examples': 89274637, 'score': 1814.00048828125, 'total_duration': 15057.974485635757, 'accumulated_submission_time': 1814.00048828125, 'accumulated_eval_time': 13243.130616664886, 'accumulated_logging_time': 0.2701282501220703, 'global_step': 1414, 'preemption_count': 0}), (1489, {'train/loss': 0.12358201050074158, 'validation/loss': 0.125270797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1279746676539273, 'test/num_examples': 89274637, 'score': 1934.4587030410767, 'total_duration': 16009.089979887009, 'accumulated_submission_time': 1934.4587030410767, 'accumulated_eval_time': 14073.739357233047, 'accumulated_logging_time': 0.2871439456939697, 'global_step': 1489, 'preemption_count': 0}), (1582, {'train/loss': 0.12508142512014808, 'validation/loss': 0.1254138202247191, 'validation/num_examples': 89000000, 'test/loss': 0.12817599023113363, 'test/num_examples': 89274637, 'score': 2054.8685047626495, 'total_duration': 16966.604539871216, 'accumulated_submission_time': 2054.8685047626495, 'accumulated_eval_time': 14910.788256168365, 'accumulated_logging_time': 0.30548906326293945, 'global_step': 1582, 'preemption_count': 0}), (1600, {'train/loss': 0.12318070098001173, 'validation/loss': 0.12539240449438202, 'validation/num_examples': 89000000, 'test/loss': 0.12815927775769057, 'test/num_examples': 89274637, 'score': 2078.292751312256, 'total_duration': 17819.88662457466, 'accumulated_submission_time': 2078.292751312256, 'accumulated_eval_time': 15740.618307352066, 'accumulated_logging_time': 0.32465028762817383, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0609 16:56:03.337412 139771734505280 submission_runner.py:584] Timing: 2078.292751312256
I0609 16:56:03.337461 139771734505280 submission_runner.py:586] Total number of evals: 19
I0609 16:56:03.337517 139771734505280 submission_runner.py:587] ====================
I0609 16:56:03.337609 139771734505280 submission_runner.py:655] Final criteo1tb score: 2078.292751312256
