torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch_redo/momentum --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_06-10-2023-14-38-24.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0610 14:38:47.400493 139927688292160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0610 14:38:47.400515 139993697101632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0610 14:38:47.400532 140473328658240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0610 14:38:47.400486 140294591952704 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0610 14:38:47.401122 139799763814208 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0610 14:38:48.380842 140362527119168 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0610 14:38:48.381116 139685612828480 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0610 14:38:48.381521 139685612828480 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.381543 140642935117632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0610 14:38:48.381936 140642935117632 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.390497 139927688292160 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.390534 140473328658240 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.390492 140294591952704 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.390562 139993697101632 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.390655 139799763814208 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.391537 140362527119168 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0610 14:38:48.745913 139685612828480 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch_redo/momentum/librispeech_deepspeech_pytorch.
W0610 14:38:49.070422 140294591952704 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0610 14:38:49.070878 139685612828480 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0610 14:38:49.070890 140642935117632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0610 14:38:49.071295 140362527119168 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0610 14:38:49.071458 140473328658240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0610 14:38:49.072730 139993697101632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0610 14:38:49.073298 139799763814208 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0610 14:38:49.076985 139685612828480 submission_runner.py:541] Using RNG seed 1556369896
I0610 14:38:49.078385 139685612828480 submission_runner.py:550] --- Tuning run 1/1 ---
I0610 14:38:49.078516 139685612828480 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch_redo/momentum/librispeech_deepspeech_pytorch/trial_1.
W0610 14:38:49.078745 139927688292160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0610 14:38:49.078847 139685612828480 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch_redo/momentum/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0610 14:38:49.079870 139685612828480 submission_runner.py:255] Initializing dataset.
I0610 14:38:49.080004 139685612828480 input_pipeline.py:20] Loading split = train-clean-100
I0610 14:38:49.117312 139685612828480 input_pipeline.py:20] Loading split = train-clean-360
I0610 14:38:49.465419 139685612828480 input_pipeline.py:20] Loading split = train-other-500
I0610 14:38:49.913069 139685612828480 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0610 14:38:57.811742 139685612828480 submission_runner.py:272] Initializing optimizer.
I0610 14:38:58.284039 139685612828480 submission_runner.py:279] Initializing metrics bundle.
I0610 14:38:58.284248 139685612828480 submission_runner.py:297] Initializing checkpoint and logger.
I0610 14:38:58.285830 139685612828480 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0610 14:38:58.285963 139685612828480 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0610 14:38:58.854356 139685612828480 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch_redo/momentum/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0610 14:38:58.855354 139685612828480 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch_redo/momentum/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0610 14:38:58.862584 139685612828480 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0610 14:39:08.162579 139659058112256 logging_writer.py:48] [0] global_step=0, grad_norm=31.179598, loss=33.406502
I0610 14:39:08.184236 139685612828480 spec.py:298] Evaluating on the training split.
I0610 14:39:08.185447 139685612828480 input_pipeline.py:20] Loading split = train-clean-100
I0610 14:39:08.217676 139685612828480 input_pipeline.py:20] Loading split = train-clean-360
I0610 14:39:08.646673 139685612828480 input_pipeline.py:20] Loading split = train-other-500
I0610 14:39:28.082571 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 14:39:28.083947 139685612828480 input_pipeline.py:20] Loading split = dev-clean
I0610 14:39:28.088920 139685612828480 input_pipeline.py:20] Loading split = dev-other
I0610 14:39:40.862658 139685612828480 spec.py:326] Evaluating on the test split.
I0610 14:39:40.864030 139685612828480 input_pipeline.py:20] Loading split = test-clean
I0610 14:39:48.432513 139685612828480 submission_runner.py:419] Time since start: 49.57s, 	Step: 1, 	{'train/ctc_loss': 31.235646294097524, 'train/wer': 3.2527722735059257, 'validation/ctc_loss': 30.284317234277676, 'validation/wer': 2.9414956790421476, 'validation/num_examples': 5348, 'test/ctc_loss': 30.277553390541325, 'test/wer': 3.2171510978408793, 'test/num_examples': 2472, 'score': 9.321647644042969, 'total_duration': 49.570231676101685, 'accumulated_submission_time': 9.321647644042969, 'accumulated_eval_time': 40.248082399368286, 'accumulated_logging_time': 0}
I0610 14:39:48.456238 139656322606848 logging_writer.py:48] [1] accumulated_eval_time=40.248082, accumulated_logging_time=0, accumulated_submission_time=9.321648, global_step=1, preemption_count=0, score=9.321648, test/ctc_loss=30.277553, test/num_examples=2472, test/wer=3.217151, total_duration=49.570232, train/ctc_loss=31.235646, train/wer=3.252772, validation/ctc_loss=30.284317, validation/num_examples=5348, validation/wer=2.941496
I0610 14:39:48.497781 139685612828480 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.498604 139993697101632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.499894 139799763814208 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.499821 140642935117632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.499841 140294591952704 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.499869 140362527119168 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.500230 140473328658240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:48.500979 139927688292160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0610 14:39:49.704871 139656314214144 logging_writer.py:48] [1] global_step=1, grad_norm=31.128302, loss=32.743141
I0610 14:39:50.772827 139656322606848 logging_writer.py:48] [2] global_step=2, grad_norm=46.644863, loss=33.208633
I0610 14:39:51.722717 139656314214144 logging_writer.py:48] [3] global_step=3, grad_norm=61.939415, loss=32.624092
I0610 14:39:52.660928 139656322606848 logging_writer.py:48] [4] global_step=4, grad_norm=60.205208, loss=30.840961
I0610 14:39:53.621521 139656314214144 logging_writer.py:48] [5] global_step=5, grad_norm=46.992134, loss=29.426147
I0610 14:39:54.574275 139656322606848 logging_writer.py:48] [6] global_step=6, grad_norm=37.382034, loss=28.475561
I0610 14:39:55.533254 139656314214144 logging_writer.py:48] [7] global_step=7, grad_norm=30.147999, loss=26.657906
I0610 14:39:56.483016 139656322606848 logging_writer.py:48] [8] global_step=8, grad_norm=24.944563, loss=25.942778
I0610 14:39:57.428854 139656314214144 logging_writer.py:48] [9] global_step=9, grad_norm=22.184326, loss=24.809958
I0610 14:39:58.359344 139656322606848 logging_writer.py:48] [10] global_step=10, grad_norm=22.171289, loss=23.628345
I0610 14:39:59.305517 139656314214144 logging_writer.py:48] [11] global_step=11, grad_norm=22.059978, loss=22.729610
I0610 14:40:00.253664 139656322606848 logging_writer.py:48] [12] global_step=12, grad_norm=22.120287, loss=21.801193
I0610 14:40:01.220130 139656314214144 logging_writer.py:48] [13] global_step=13, grad_norm=21.467165, loss=20.048878
I0610 14:40:02.168178 139656322606848 logging_writer.py:48] [14] global_step=14, grad_norm=21.590670, loss=18.409925
I0610 14:40:03.125448 139656314214144 logging_writer.py:48] [15] global_step=15, grad_norm=22.162521, loss=16.493248
I0610 14:40:04.067376 139656322606848 logging_writer.py:48] [16] global_step=16, grad_norm=19.510487, loss=15.281030
I0610 14:40:05.007999 139656314214144 logging_writer.py:48] [17] global_step=17, grad_norm=15.728477, loss=13.871098
I0610 14:40:05.948750 139656322606848 logging_writer.py:48] [18] global_step=18, grad_norm=12.039256, loss=12.646954
I0610 14:40:06.879868 139656314214144 logging_writer.py:48] [19] global_step=19, grad_norm=12.442776, loss=12.211138
I0610 14:40:07.860700 139656322606848 logging_writer.py:48] [20] global_step=20, grad_norm=17.303917, loss=12.004130
I0610 14:40:08.811623 139656314214144 logging_writer.py:48] [21] global_step=21, grad_norm=16.799305, loss=11.700946
I0610 14:40:09.782766 139656322606848 logging_writer.py:48] [22] global_step=22, grad_norm=16.193689, loss=11.334270
I0610 14:40:10.742137 139656314214144 logging_writer.py:48] [23] global_step=23, grad_norm=15.679601, loss=10.854254
I0610 14:40:11.712370 139656322606848 logging_writer.py:48] [24] global_step=24, grad_norm=11.999806, loss=10.412351
I0610 14:40:12.674412 139656314214144 logging_writer.py:48] [25] global_step=25, grad_norm=9.320663, loss=9.975750
I0610 14:40:13.640766 139656322606848 logging_writer.py:48] [26] global_step=26, grad_norm=8.656791, loss=9.355159
I0610 14:40:14.619678 139656314214144 logging_writer.py:48] [27] global_step=27, grad_norm=10.465695, loss=9.767663
I0610 14:40:15.561343 139656322606848 logging_writer.py:48] [28] global_step=28, grad_norm=12.303594, loss=9.534627
I0610 14:40:16.506981 139656314214144 logging_writer.py:48] [29] global_step=29, grad_norm=10.950014, loss=9.223537
I0610 14:40:17.480943 139656322606848 logging_writer.py:48] [30] global_step=30, grad_norm=13.746318, loss=8.813824
I0610 14:40:18.457008 139656314214144 logging_writer.py:48] [31] global_step=31, grad_norm=7.891852, loss=8.680239
I0610 14:40:19.412811 139656322606848 logging_writer.py:48] [32] global_step=32, grad_norm=7.192157, loss=8.371098
I0610 14:40:20.370038 139656314214144 logging_writer.py:48] [33] global_step=33, grad_norm=6.258017, loss=8.236287
I0610 14:40:21.310156 139656322606848 logging_writer.py:48] [34] global_step=34, grad_norm=6.562913, loss=8.038960
I0610 14:40:22.248248 139656314214144 logging_writer.py:48] [35] global_step=35, grad_norm=10.007316, loss=7.927860
I0610 14:40:23.211590 139656322606848 logging_writer.py:48] [36] global_step=36, grad_norm=4.547969, loss=7.657449
I0610 14:40:24.150519 139656314214144 logging_writer.py:48] [37] global_step=37, grad_norm=18.557568, loss=7.705590
I0610 14:40:25.127053 139656322606848 logging_writer.py:48] [38] global_step=38, grad_norm=24.018414, loss=7.744582
I0610 14:40:26.067162 139656314214144 logging_writer.py:48] [39] global_step=39, grad_norm=15.719958, loss=7.607433
I0610 14:40:27.020380 139656322606848 logging_writer.py:48] [40] global_step=40, grad_norm=11.014715, loss=7.544489
I0610 14:40:27.981384 139656314214144 logging_writer.py:48] [41] global_step=41, grad_norm=9.808838, loss=7.559305
I0610 14:40:28.965277 139656322606848 logging_writer.py:48] [42] global_step=42, grad_norm=11.477372, loss=7.662409
I0610 14:40:29.912446 139656314214144 logging_writer.py:48] [43] global_step=43, grad_norm=5.906554, loss=7.442098
I0610 14:40:30.881660 139656322606848 logging_writer.py:48] [44] global_step=44, grad_norm=20.093939, loss=7.456289
I0610 14:40:31.825185 139656314214144 logging_writer.py:48] [45] global_step=45, grad_norm=8.812977, loss=7.207660
I0610 14:40:32.810069 139656322606848 logging_writer.py:48] [46] global_step=46, grad_norm=12.844401, loss=7.324807
I0610 14:40:33.771639 139656314214144 logging_writer.py:48] [47] global_step=47, grad_norm=4.492455, loss=6.985723
I0610 14:40:34.739775 139656322606848 logging_writer.py:48] [48] global_step=48, grad_norm=20.891666, loss=7.292671
I0610 14:40:35.681520 139656314214144 logging_writer.py:48] [49] global_step=49, grad_norm=3.238860, loss=6.890449
I0610 14:40:36.649776 139656322606848 logging_writer.py:48] [50] global_step=50, grad_norm=9.657330, loss=6.997277
I0610 14:40:37.583001 139656314214144 logging_writer.py:48] [51] global_step=51, grad_norm=7.972250, loss=6.883360
I0610 14:40:38.512189 139656322606848 logging_writer.py:48] [52] global_step=52, grad_norm=6.411563, loss=6.781392
I0610 14:40:39.445713 139656314214144 logging_writer.py:48] [53] global_step=53, grad_norm=10.152585, loss=6.750147
I0610 14:40:40.398432 139656322606848 logging_writer.py:48] [54] global_step=54, grad_norm=5.602330, loss=6.689681
I0610 14:40:41.330833 139656314214144 logging_writer.py:48] [55] global_step=55, grad_norm=7.335948, loss=6.656345
I0610 14:40:42.268203 139656322606848 logging_writer.py:48] [56] global_step=56, grad_norm=3.022682, loss=6.496494
I0610 14:40:43.218740 139656314214144 logging_writer.py:48] [57] global_step=57, grad_norm=10.948034, loss=6.560424
I0610 14:40:44.163912 139656322606848 logging_writer.py:48] [58] global_step=58, grad_norm=3.547370, loss=6.479761
I0610 14:40:45.098752 139656314214144 logging_writer.py:48] [59] global_step=59, grad_norm=11.244419, loss=6.584396
I0610 14:40:46.033569 139656322606848 logging_writer.py:48] [60] global_step=60, grad_norm=6.644123, loss=6.470879
I0610 14:40:47.001639 139656314214144 logging_writer.py:48] [61] global_step=61, grad_norm=8.056583, loss=6.423886
I0610 14:40:47.936088 139656322606848 logging_writer.py:48] [62] global_step=62, grad_norm=11.024187, loss=6.445729
I0610 14:40:48.869623 139656314214144 logging_writer.py:48] [63] global_step=63, grad_norm=1.413586, loss=6.312077
I0610 14:40:49.800990 139656322606848 logging_writer.py:48] [64] global_step=64, grad_norm=8.153719, loss=6.345768
I0610 14:40:50.756836 139656314214144 logging_writer.py:48] [65] global_step=65, grad_norm=11.222965, loss=6.425597
I0610 14:40:51.692150 139656322606848 logging_writer.py:48] [66] global_step=66, grad_norm=7.492295, loss=6.335919
I0610 14:40:52.626283 139656314214144 logging_writer.py:48] [67] global_step=67, grad_norm=2.088429, loss=6.232665
I0610 14:40:53.562121 139656322606848 logging_writer.py:48] [68] global_step=68, grad_norm=8.437517, loss=6.287530
I0610 14:40:54.508231 139656314214144 logging_writer.py:48] [69] global_step=69, grad_norm=4.206658, loss=6.240170
I0610 14:40:55.442796 139656322606848 logging_writer.py:48] [70] global_step=70, grad_norm=7.524801, loss=6.227288
I0610 14:40:56.371333 139656314214144 logging_writer.py:48] [71] global_step=71, grad_norm=14.456286, loss=6.367416
I0610 14:40:57.313837 139656322606848 logging_writer.py:48] [72] global_step=72, grad_norm=6.654590, loss=6.187077
I0610 14:40:58.269059 139656314214144 logging_writer.py:48] [73] global_step=73, grad_norm=6.904365, loss=6.217547
I0610 14:40:59.204969 139656322606848 logging_writer.py:48] [74] global_step=74, grad_norm=9.265903, loss=6.264408
I0610 14:41:00.147830 139656314214144 logging_writer.py:48] [75] global_step=75, grad_norm=2.806163, loss=6.131440
I0610 14:41:01.099691 139656322606848 logging_writer.py:48] [76] global_step=76, grad_norm=11.816592, loss=6.262573
I0610 14:41:02.086681 139656314214144 logging_writer.py:48] [77] global_step=77, grad_norm=5.827684, loss=6.156247
I0610 14:41:03.028340 139656322606848 logging_writer.py:48] [78] global_step=78, grad_norm=8.713516, loss=6.196458
I0610 14:41:03.983035 139656314214144 logging_writer.py:48] [79] global_step=79, grad_norm=12.383026, loss=6.262161
I0610 14:41:04.936157 139656322606848 logging_writer.py:48] [80] global_step=80, grad_norm=3.059193, loss=6.102474
I0610 14:41:05.878727 139656314214144 logging_writer.py:48] [81] global_step=81, grad_norm=7.314435, loss=6.143416
I0610 14:41:06.825331 139656322606848 logging_writer.py:48] [82] global_step=82, grad_norm=7.078266, loss=6.126102
I0610 14:41:07.767518 139656314214144 logging_writer.py:48] [83] global_step=83, grad_norm=1.643857, loss=6.091653
I0610 14:41:08.728065 139656322606848 logging_writer.py:48] [84] global_step=84, grad_norm=8.060552, loss=6.139714
I0610 14:41:09.675742 139656314214144 logging_writer.py:48] [85] global_step=85, grad_norm=4.745428, loss=6.063267
I0610 14:41:10.631322 139656322606848 logging_writer.py:48] [86] global_step=86, grad_norm=3.470217, loss=6.054544
I0610 14:41:11.572539 139656314214144 logging_writer.py:48] [87] global_step=87, grad_norm=5.370687, loss=6.065159
I0610 14:41:12.511731 139656322606848 logging_writer.py:48] [88] global_step=88, grad_norm=0.892545, loss=6.042253
I0610 14:41:13.452597 139656314214144 logging_writer.py:48] [89] global_step=89, grad_norm=4.982124, loss=6.070097
I0610 14:41:14.390447 139656322606848 logging_writer.py:48] [90] global_step=90, grad_norm=6.386151, loss=6.060107
I0610 14:41:15.323440 139656314214144 logging_writer.py:48] [91] global_step=91, grad_norm=6.220421, loss=6.054978
I0610 14:41:16.261621 139656322606848 logging_writer.py:48] [92] global_step=92, grad_norm=5.085453, loss=6.026812
I0610 14:41:17.222807 139656314214144 logging_writer.py:48] [93] global_step=93, grad_norm=2.425704, loss=5.999394
I0610 14:41:18.167307 139656322606848 logging_writer.py:48] [94] global_step=94, grad_norm=0.982099, loss=6.000541
I0610 14:41:19.106053 139656314214144 logging_writer.py:48] [95] global_step=95, grad_norm=4.151453, loss=6.007310
I0610 14:41:20.045200 139656322606848 logging_writer.py:48] [96] global_step=96, grad_norm=6.628490, loss=6.053624
I0610 14:41:20.994306 139656314214144 logging_writer.py:48] [97] global_step=97, grad_norm=5.929487, loss=6.011274
I0610 14:41:21.944929 139656322606848 logging_writer.py:48] [98] global_step=98, grad_norm=1.581186, loss=5.983393
I0610 14:41:22.878947 139656314214144 logging_writer.py:48] [99] global_step=99, grad_norm=1.817910, loss=5.992608
I0610 14:41:23.813781 139656322606848 logging_writer.py:48] [100] global_step=100, grad_norm=0.792102, loss=5.977890
I0610 14:47:37.592392 139656314214144 logging_writer.py:48] [500] global_step=500, grad_norm=2.088197, loss=5.795285
I0610 14:55:24.669654 139656322606848 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.527082, loss=4.877117
I0610 15:03:11.032604 139656322606848 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.965377, loss=3.480565
I0610 15:10:56.076499 139656314214144 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.117677, loss=3.046695
I0610 15:18:41.837382 139656314214144 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.010329, loss=2.770979
I0610 15:19:49.204616 139685612828480 spec.py:298] Evaluating on the training split.
I0610 15:20:01.127083 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 15:20:11.674554 139685612828480 spec.py:326] Evaluating on the test split.
I0610 15:20:17.016294 139685612828480 submission_runner.py:419] Time since start: 2478.15s, 	Step: 2574, 	{'train/ctc_loss': 3.1110699539822773, 'train/wer': 0.6703129111422572, 'validation/ctc_loss': 3.2085593731163353, 'validation/wer': 0.6613817409356443, 'validation/num_examples': 5348, 'test/ctc_loss': 2.7826922301438803, 'test/wer': 0.6172689050027421, 'test/num_examples': 2472, 'score': 2408.890144586563, 'total_duration': 2478.1539788246155, 'accumulated_submission_time': 2408.890144586563, 'accumulated_eval_time': 68.05943059921265, 'accumulated_logging_time': 0.03310799598693848}
I0610 15:20:17.039611 139656314214144 logging_writer.py:48] [2574] accumulated_eval_time=68.059431, accumulated_logging_time=0.033108, accumulated_submission_time=2408.890145, global_step=2574, preemption_count=0, score=2408.890145, test/ctc_loss=2.782692, test/num_examples=2472, test/wer=0.617269, total_duration=2478.153979, train/ctc_loss=3.111070, train/wer=0.670313, validation/ctc_loss=3.208559, validation/num_examples=5348, validation/wer=0.661382
I0610 15:26:52.791975 139656096118528 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.015187, loss=2.729718
I0610 15:34:37.601615 139656314214144 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.915760, loss=2.521577
I0610 15:42:21.484953 139656096118528 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.016177, loss=2.460631
I0610 15:50:07.673088 139656096118528 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.732339, loss=2.364599
I0610 15:57:53.065008 139656087725824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.722023, loss=2.311668
I0610 16:00:17.252388 139685612828480 spec.py:298] Evaluating on the training split.
I0610 16:00:29.553551 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 16:00:39.585504 139685612828480 spec.py:326] Evaluating on the test split.
I0610 16:00:45.138230 139685612828480 submission_runner.py:419] Time since start: 4906.28s, 	Step: 5155, 	{'train/ctc_loss': 1.2689410137986088, 'train/wer': 0.38199646652597213, 'validation/ctc_loss': 1.5377968145657526, 'validation/wer': 0.4128711437261623, 'validation/num_examples': 5348, 'test/ctc_loss': 1.121823585392606, 'test/wer': 0.34570308532894606, 'test/num_examples': 2472, 'score': 4807.8518679142, 'total_duration': 4906.275943994522, 'accumulated_submission_time': 4807.8518679142, 'accumulated_eval_time': 95.94519758224487, 'accumulated_logging_time': 0.06686162948608398}
I0610 16:00:45.158046 139656096118528 logging_writer.py:48] [5155] accumulated_eval_time=95.945198, accumulated_logging_time=0.066862, accumulated_submission_time=4807.851868, global_step=5155, preemption_count=0, score=4807.851868, test/ctc_loss=1.121824, test/num_examples=2472, test/wer=0.345703, total_duration=4906.275944, train/ctc_loss=1.268941, train/wer=0.381996, validation/ctc_loss=1.537797, validation/num_examples=5348, validation/wer=0.412871
I0610 16:06:06.043189 139656087725824 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.860521, loss=2.456440
I0610 16:13:42.270698 139656096118528 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0610 16:21:13.876797 139656096118528 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0610 16:28:48.985627 139656087725824 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0610 16:36:23.695564 139656087725824 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0610 16:40:45.861999 139685612828480 spec.py:298] Evaluating on the training split.
I0610 16:40:56.205955 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 16:41:05.822158 139685612828480 spec.py:326] Evaluating on the test split.
I0610 16:41:10.952929 139685612828480 submission_runner.py:419] Time since start: 7332.09s, 	Step: 7791, 	{'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7207.339407444, 'total_duration': 7332.090581655502, 'accumulated_submission_time': 7207.339407444, 'accumulated_eval_time': 121.0358076095581, 'accumulated_logging_time': 0.09575724601745605}
I0610 16:41:10.975801 139656087725824 logging_writer.py:48] [7791] accumulated_eval_time=121.035808, accumulated_logging_time=0.095757, accumulated_submission_time=7207.339407, global_step=7791, preemption_count=0, score=7207.339407, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7332.090582, train/ctc_loss=nan, train/wer=0.942262, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0610 16:44:21.993509 139656079333120 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0610 16:51:54.257047 139656087725824 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0610 16:59:27.155243 139656079333120 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0610 17:06:59.809181 139656087725824 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0610 17:14:31.480360 139656079333120 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0610 17:21:11.433499 139685612828480 spec.py:298] Evaluating on the training split.
I0610 17:21:21.797125 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 17:21:31.405001 139685612828480 spec.py:326] Evaluating on the test split.
I0610 17:21:36.998122 139685612828480 submission_runner.py:419] Time since start: 9758.14s, 	Step: 10442, 	{'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9606.555097103119, 'total_duration': 9758.13576889038, 'accumulated_submission_time': 9606.555097103119, 'accumulated_eval_time': 146.60024213790894, 'accumulated_logging_time': 0.12931227684020996}
I0610 17:21:37.017031 139656087725824 logging_writer.py:48] [10442] accumulated_eval_time=146.600242, accumulated_logging_time=0.129312, accumulated_submission_time=9606.555097, global_step=10442, preemption_count=0, score=9606.555097, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9758.135769, train/ctc_loss=nan, train/wer=0.942262, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0610 17:22:29.990601 139656079333120 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0610 17:30:02.944880 139656087725824 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0610 17:37:34.934078 139656087725824 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0610 17:45:04.616168 139656079333120 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0610 17:52:39.078541 139656087725824 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0610 18:00:10.591626 139656079333120 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0610 18:01:37.752319 139685612828480 spec.py:298] Evaluating on the training split.
I0610 18:01:48.094958 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 18:01:57.711444 139685612828480 spec.py:326] Evaluating on the test split.
I0610 18:02:02.853468 139685612828480 submission_runner.py:419] Time since start: 12183.99s, 	Step: 13096, 	{'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12006.039948701859, 'total_duration': 12183.991151332855, 'accumulated_submission_time': 12006.039948701859, 'accumulated_eval_time': 171.70113825798035, 'accumulated_logging_time': 0.15705204010009766}
I0610 18:02:02.873013 139656087725824 logging_writer.py:48] [13096] accumulated_eval_time=171.701138, accumulated_logging_time=0.157052, accumulated_submission_time=12006.039949, global_step=13096, preemption_count=0, score=12006.039949, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12183.991151, train/ctc_loss=nan, train/wer=0.942262, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0610 18:08:11.625706 139656087725824 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0610 18:15:41.223354 139656079333120 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0610 18:23:14.560677 139656079333120 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0610 18:30:47.727856 139656070940416 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0610 18:38:22.465064 139656079333120 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0610 18:42:02.926871 139685612828480 spec.py:298] Evaluating on the training split.
I0610 18:42:13.254149 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 18:42:23.664234 139685612828480 spec.py:326] Evaluating on the test split.
I0610 18:42:28.783055 139685612828480 submission_runner.py:419] Time since start: 14609.92s, 	Step: 15745, 	{'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14404.823634147644, 'total_duration': 14609.920681715012, 'accumulated_submission_time': 14404.823634147644, 'accumulated_eval_time': 197.5569612979889, 'accumulated_logging_time': 0.1853187084197998}
I0610 18:42:28.803661 139656079333120 logging_writer.py:48] [15745] accumulated_eval_time=197.556961, accumulated_logging_time=0.185319, accumulated_submission_time=14404.823634, global_step=15745, preemption_count=0, score=14404.823634, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=14609.920682, train/ctc_loss=nan, train/wer=0.942262, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0610 18:46:19.384007 139685612828480 spec.py:298] Evaluating on the training split.
I0610 18:46:29.107081 139685612828480 spec.py:310] Evaluating on the validation split.
I0610 18:46:37.985722 139685612828480 spec.py:326] Evaluating on the test split.
I0610 18:46:42.802212 139685612828480 submission_runner.py:419] Time since start: 14863.94s, 	Step: 16000, 	{'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14635.241894960403, 'total_duration': 14863.9398291111, 'accumulated_submission_time': 14635.241894960403, 'accumulated_eval_time': 220.97477746009827, 'accumulated_logging_time': 0.21483373641967773}
I0610 18:46:42.818885 139656079333120 logging_writer.py:48] [16000] accumulated_eval_time=220.974777, accumulated_logging_time=0.214834, accumulated_submission_time=14635.241895, global_step=16000, preemption_count=0, score=14635.241895, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=14863.939829, train/ctc_loss=nan, train/wer=0.942262, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0610 18:46:42.838437 139656070940416 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=14635.241895
I0610 18:46:43.113239 139685612828480 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch_redo/momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_16000.
I0610 18:46:43.208883 139685612828480 submission_runner.py:581] Tuning trial 1/1
I0610 18:46:43.209146 139685612828480 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0610 18:46:43.209751 139685612828480 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.235646294097524, 'train/wer': 3.2527722735059257, 'validation/ctc_loss': 30.284317234277676, 'validation/wer': 2.9414956790421476, 'validation/num_examples': 5348, 'test/ctc_loss': 30.277553390541325, 'test/wer': 3.2171510978408793, 'test/num_examples': 2472, 'score': 9.321647644042969, 'total_duration': 49.570231676101685, 'accumulated_submission_time': 9.321647644042969, 'accumulated_eval_time': 40.248082399368286, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2574, {'train/ctc_loss': 3.1110699539822773, 'train/wer': 0.6703129111422572, 'validation/ctc_loss': 3.2085593731163353, 'validation/wer': 0.6613817409356443, 'validation/num_examples': 5348, 'test/ctc_loss': 2.7826922301438803, 'test/wer': 0.6172689050027421, 'test/num_examples': 2472, 'score': 2408.890144586563, 'total_duration': 2478.1539788246155, 'accumulated_submission_time': 2408.890144586563, 'accumulated_eval_time': 68.05943059921265, 'accumulated_logging_time': 0.03310799598693848, 'global_step': 2574, 'preemption_count': 0}), (5155, {'train/ctc_loss': 1.2689410137986088, 'train/wer': 0.38199646652597213, 'validation/ctc_loss': 1.5377968145657526, 'validation/wer': 0.4128711437261623, 'validation/num_examples': 5348, 'test/ctc_loss': 1.121823585392606, 'test/wer': 0.34570308532894606, 'test/num_examples': 2472, 'score': 4807.8518679142, 'total_duration': 4906.275943994522, 'accumulated_submission_time': 4807.8518679142, 'accumulated_eval_time': 95.94519758224487, 'accumulated_logging_time': 0.06686162948608398, 'global_step': 5155, 'preemption_count': 0}), (7791, {'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7207.339407444, 'total_duration': 7332.090581655502, 'accumulated_submission_time': 7207.339407444, 'accumulated_eval_time': 121.0358076095581, 'accumulated_logging_time': 0.09575724601745605, 'global_step': 7791, 'preemption_count': 0}), (10442, {'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9606.555097103119, 'total_duration': 9758.13576889038, 'accumulated_submission_time': 9606.555097103119, 'accumulated_eval_time': 146.60024213790894, 'accumulated_logging_time': 0.12931227684020996, 'global_step': 10442, 'preemption_count': 0}), (13096, {'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12006.039948701859, 'total_duration': 12183.991151332855, 'accumulated_submission_time': 12006.039948701859, 'accumulated_eval_time': 171.70113825798035, 'accumulated_logging_time': 0.15705204010009766, 'global_step': 13096, 'preemption_count': 0}), (15745, {'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14404.823634147644, 'total_duration': 14609.920681715012, 'accumulated_submission_time': 14404.823634147644, 'accumulated_eval_time': 197.5569612979889, 'accumulated_logging_time': 0.1853187084197998, 'global_step': 15745, 'preemption_count': 0}), (16000, {'train/ctc_loss': nan, 'train/wer': 0.9422615307782772, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14635.241894960403, 'total_duration': 14863.9398291111, 'accumulated_submission_time': 14635.241894960403, 'accumulated_eval_time': 220.97477746009827, 'accumulated_logging_time': 0.21483373641967773, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0610 18:46:43.209890 139685612828480 submission_runner.py:584] Timing: 14635.241894960403
I0610 18:46:43.209966 139685612828480 submission_runner.py:586] Total number of evals: 8
I0610 18:46:43.210040 139685612828480 submission_runner.py:587] ====================
I0610 18:46:43.210227 139685612828480 submission_runner.py:655] Final librispeech_deepspeech score: 14635.241894960403
