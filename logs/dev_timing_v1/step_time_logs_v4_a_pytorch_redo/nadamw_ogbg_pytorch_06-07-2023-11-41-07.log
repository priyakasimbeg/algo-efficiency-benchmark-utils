torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-07-2023-11-41-07.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 11:41:31.745034 140278920976192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 11:41:31.745084 140003910977344 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 11:41:31.745126 140581147498304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 11:41:32.732266 140216481490752 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 11:41:32.732305 140039118866240 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 11:41:32.732326 139757491668800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 11:41:32.732354 140491303962432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 11:41:32.736819 139733503477568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 11:41:32.737161 139733503477568 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.742861 140216481490752 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.742883 140039118866240 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.742933 139757491668800 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.742956 140491303962432 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.746394 140278920976192 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.746458 140003910977344 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:32.746490 140581147498304 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:41:34.008119 139733503477568 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch because --overwrite was set.
I0607 11:41:34.022212 139733503477568 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch.
W0607 11:41:34.049769 140216481490752 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.051519 140278920976192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.052121 140491303962432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.052984 139757491668800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.053931 140003910977344 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.054127 140039118866240 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.054376 140581147498304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:41:34.058741 139733503477568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 11:41:34.064352 139733503477568 submission_runner.py:541] Using RNG seed 3313851156
I0607 11:41:34.065851 139733503477568 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 11:41:34.065975 139733503477568 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch/trial_1.
I0607 11:41:34.066227 139733503477568 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch/trial_1/hparams.json.
I0607 11:41:34.067223 139733503477568 submission_runner.py:255] Initializing dataset.
I0607 11:41:34.067347 139733503477568 submission_runner.py:262] Initializing model.
I0607 11:41:38.282203 139733503477568 submission_runner.py:272] Initializing optimizer.
I0607 11:41:38.283390 139733503477568 submission_runner.py:279] Initializing metrics bundle.
I0607 11:41:38.283562 139733503477568 submission_runner.py:297] Initializing checkpoint and logger.
I0607 11:41:38.287150 139733503477568 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 11:41:38.287298 139733503477568 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 11:41:38.792981 139733503477568 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0607 11:41:38.793908 139733503477568 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch/trial_1/flags_0.json.
I0607 11:41:38.846567 139733503477568 submission_runner.py:332] Starting training loop.
I0607 11:41:39.453488 139733503477568 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:41:39.460682 139733503477568 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:41:39.596936 139733503477568 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:41:44.253821 139694239115008 logging_writer.py:48] [0] global_step=0, grad_norm=3.004812, loss=0.791974
I0607 11:41:44.262923 139733503477568 submission.py:296] 0) loss = 0.792, grad_norm = 3.005
I0607 11:41:44.263837 139733503477568 spec.py:298] Evaluating on the training split.
I0607 11:41:44.269506 139733503477568 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:41:44.274195 139733503477568 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:41:44.327404 139733503477568 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:42:41.160468 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 11:42:41.163943 139733503477568 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:42:41.168601 139733503477568 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:42:41.223528 139733503477568 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:43:26.311627 139733503477568 spec.py:326] Evaluating on the test split.
I0607 11:43:26.314878 139733503477568 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:43:26.319082 139733503477568 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:43:26.375514 139733503477568 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:44:11.950647 139733503477568 submission_runner.py:419] Time since start: 153.10s, 	Step: 1, 	{'train/accuracy': 0.44477107552266776, 'train/loss': 0.7929020778204073, 'train/mean_average_precision': 0.021953884342576904, 'validation/accuracy': 0.4484065609759464, 'validation/loss': 0.7904324547152853, 'validation/mean_average_precision': 0.025981803290386633, 'validation/num_examples': 43793, 'test/accuracy': 0.4508013018285325, 'test/loss': 0.7888642653796081, 'test/mean_average_precision': 0.028103970856366876, 'test/num_examples': 43793, 'score': 5.417672872543335, 'total_duration': 153.10452890396118, 'accumulated_submission_time': 5.417672872543335, 'accumulated_eval_time': 147.68651366233826, 'accumulated_logging_time': 0}
I0607 11:44:11.969867 139681286661888 logging_writer.py:48] [1] accumulated_eval_time=147.686514, accumulated_logging_time=0, accumulated_submission_time=5.417673, global_step=1, preemption_count=0, score=5.417673, test/accuracy=0.450801, test/loss=0.788864, test/mean_average_precision=0.028104, test/num_examples=43793, total_duration=153.104529, train/accuracy=0.444771, train/loss=0.792902, train/mean_average_precision=0.021954, validation/accuracy=0.448407, validation/loss=0.790432, validation/mean_average_precision=0.025982, validation/num_examples=43793
I0607 11:44:12.241252 139733503477568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.247494 139757491668800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.247492 140491303962432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.247503 140216481490752 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.247508 140278920976192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.248171 140581147498304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.248173 140003910977344 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.248244 140039118866240 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:44:12.285328 139681295054592 logging_writer.py:48] [1] global_step=1, grad_norm=3.008957, loss=0.792091
I0607 11:44:12.289311 139733503477568 submission.py:296] 1) loss = 0.792, grad_norm = 3.009
I0607 11:44:12.580992 139681286661888 logging_writer.py:48] [2] global_step=2, grad_norm=2.963644, loss=0.793159
I0607 11:44:12.585181 139733503477568 submission.py:296] 2) loss = 0.793, grad_norm = 2.964
I0607 11:44:12.874493 139681295054592 logging_writer.py:48] [3] global_step=3, grad_norm=3.001304, loss=0.789348
I0607 11:44:12.878706 139733503477568 submission.py:296] 3) loss = 0.789, grad_norm = 3.001
I0607 11:44:13.167641 139681286661888 logging_writer.py:48] [4] global_step=4, grad_norm=2.971316, loss=0.785022
I0607 11:44:13.171843 139733503477568 submission.py:296] 4) loss = 0.785, grad_norm = 2.971
I0607 11:44:13.464934 139681295054592 logging_writer.py:48] [5] global_step=5, grad_norm=2.913332, loss=0.780355
I0607 11:44:13.468949 139733503477568 submission.py:296] 5) loss = 0.780, grad_norm = 2.913
I0607 11:44:13.761975 139681286661888 logging_writer.py:48] [6] global_step=6, grad_norm=2.905997, loss=0.776303
I0607 11:44:13.765954 139733503477568 submission.py:296] 6) loss = 0.776, grad_norm = 2.906
I0607 11:44:14.075907 139681295054592 logging_writer.py:48] [7] global_step=7, grad_norm=2.870203, loss=0.770386
I0607 11:44:14.079961 139733503477568 submission.py:296] 7) loss = 0.770, grad_norm = 2.870
I0607 11:44:14.369908 139681286661888 logging_writer.py:48] [8] global_step=8, grad_norm=2.895084, loss=0.765129
I0607 11:44:14.373987 139733503477568 submission.py:296] 8) loss = 0.765, grad_norm = 2.895
I0607 11:44:14.666347 139681295054592 logging_writer.py:48] [9] global_step=9, grad_norm=2.788455, loss=0.759165
I0607 11:44:14.670281 139733503477568 submission.py:296] 9) loss = 0.759, grad_norm = 2.788
I0607 11:44:14.967677 139681286661888 logging_writer.py:48] [10] global_step=10, grad_norm=2.694507, loss=0.749689
I0607 11:44:14.971656 139733503477568 submission.py:296] 10) loss = 0.750, grad_norm = 2.695
I0607 11:44:15.250720 139681295054592 logging_writer.py:48] [11] global_step=11, grad_norm=2.570620, loss=0.740384
I0607 11:44:15.255069 139733503477568 submission.py:296] 11) loss = 0.740, grad_norm = 2.571
I0607 11:44:15.538660 139681286661888 logging_writer.py:48] [12] global_step=12, grad_norm=2.444062, loss=0.732195
I0607 11:44:15.542763 139733503477568 submission.py:296] 12) loss = 0.732, grad_norm = 2.444
I0607 11:44:15.830683 139681295054592 logging_writer.py:48] [13] global_step=13, grad_norm=2.319235, loss=0.726280
I0607 11:44:15.834767 139733503477568 submission.py:296] 13) loss = 0.726, grad_norm = 2.319
I0607 11:44:16.121467 139681286661888 logging_writer.py:48] [14] global_step=14, grad_norm=2.181878, loss=0.716229
I0607 11:44:16.125527 139733503477568 submission.py:296] 14) loss = 0.716, grad_norm = 2.182
I0607 11:44:16.407361 139681295054592 logging_writer.py:48] [15] global_step=15, grad_norm=2.085429, loss=0.709919
I0607 11:44:16.411339 139733503477568 submission.py:296] 15) loss = 0.710, grad_norm = 2.085
I0607 11:44:16.697082 139681286661888 logging_writer.py:48] [16] global_step=16, grad_norm=1.960648, loss=0.699461
I0607 11:44:16.701056 139733503477568 submission.py:296] 16) loss = 0.699, grad_norm = 1.961
I0607 11:44:16.988498 139681295054592 logging_writer.py:48] [17] global_step=17, grad_norm=1.856064, loss=0.692069
I0607 11:44:16.992363 139733503477568 submission.py:296] 17) loss = 0.692, grad_norm = 1.856
I0607 11:44:17.484010 139681286661888 logging_writer.py:48] [18] global_step=18, grad_norm=1.768618, loss=0.684081
I0607 11:44:17.487993 139733503477568 submission.py:296] 18) loss = 0.684, grad_norm = 1.769
I0607 11:44:17.777815 139681295054592 logging_writer.py:48] [19] global_step=19, grad_norm=1.717904, loss=0.678464
I0607 11:44:17.781775 139733503477568 submission.py:296] 19) loss = 0.678, grad_norm = 1.718
I0607 11:44:18.071625 139681286661888 logging_writer.py:48] [20] global_step=20, grad_norm=1.668476, loss=0.670712
I0607 11:44:18.075605 139733503477568 submission.py:296] 20) loss = 0.671, grad_norm = 1.668
I0607 11:44:18.364703 139681295054592 logging_writer.py:48] [21] global_step=21, grad_norm=1.635631, loss=0.664638
I0607 11:44:18.368827 139733503477568 submission.py:296] 21) loss = 0.665, grad_norm = 1.636
I0607 11:44:18.658849 139681286661888 logging_writer.py:48] [22] global_step=22, grad_norm=1.573143, loss=0.659161
I0607 11:44:18.663020 139733503477568 submission.py:296] 22) loss = 0.659, grad_norm = 1.573
I0607 11:44:18.955188 139681295054592 logging_writer.py:48] [23] global_step=23, grad_norm=1.566079, loss=0.652495
I0607 11:44:18.959366 139733503477568 submission.py:296] 23) loss = 0.652, grad_norm = 1.566
I0607 11:44:19.269007 139681286661888 logging_writer.py:48] [24] global_step=24, grad_norm=1.550217, loss=0.645714
I0607 11:44:19.272928 139733503477568 submission.py:296] 24) loss = 0.646, grad_norm = 1.550
I0607 11:44:19.568065 139681295054592 logging_writer.py:48] [25] global_step=25, grad_norm=1.499757, loss=0.640786
I0607 11:44:19.571969 139733503477568 submission.py:296] 25) loss = 0.641, grad_norm = 1.500
I0607 11:44:19.866424 139681286661888 logging_writer.py:48] [26] global_step=26, grad_norm=1.429869, loss=0.631094
I0607 11:44:19.870752 139733503477568 submission.py:296] 26) loss = 0.631, grad_norm = 1.430
I0607 11:44:20.162111 139681295054592 logging_writer.py:48] [27] global_step=27, grad_norm=1.414476, loss=0.624553
I0607 11:44:20.166351 139733503477568 submission.py:296] 27) loss = 0.625, grad_norm = 1.414
I0607 11:44:20.462480 139681286661888 logging_writer.py:48] [28] global_step=28, grad_norm=1.351751, loss=0.620978
I0607 11:44:20.466642 139733503477568 submission.py:296] 28) loss = 0.621, grad_norm = 1.352
I0607 11:44:20.760073 139681295054592 logging_writer.py:48] [29] global_step=29, grad_norm=1.325973, loss=0.614988
I0607 11:44:20.763895 139733503477568 submission.py:296] 29) loss = 0.615, grad_norm = 1.326
I0607 11:44:21.059991 139681286661888 logging_writer.py:48] [30] global_step=30, grad_norm=1.272881, loss=0.607946
I0607 11:44:21.064021 139733503477568 submission.py:296] 30) loss = 0.608, grad_norm = 1.273
I0607 11:44:21.361615 139681295054592 logging_writer.py:48] [31] global_step=31, grad_norm=1.244825, loss=0.603283
I0607 11:44:21.365865 139733503477568 submission.py:296] 31) loss = 0.603, grad_norm = 1.245
I0607 11:44:21.688441 139681286661888 logging_writer.py:48] [32] global_step=32, grad_norm=1.225626, loss=0.596837
I0607 11:44:21.692286 139733503477568 submission.py:296] 32) loss = 0.597, grad_norm = 1.226
I0607 11:44:22.009567 139681295054592 logging_writer.py:48] [33] global_step=33, grad_norm=1.222264, loss=0.590823
I0607 11:44:22.013489 139733503477568 submission.py:296] 33) loss = 0.591, grad_norm = 1.222
I0607 11:44:22.326310 139681286661888 logging_writer.py:48] [34] global_step=34, grad_norm=1.184619, loss=0.585967
I0607 11:44:22.330678 139733503477568 submission.py:296] 34) loss = 0.586, grad_norm = 1.185
I0607 11:44:22.641928 139681295054592 logging_writer.py:48] [35] global_step=35, grad_norm=1.171747, loss=0.579526
I0607 11:44:22.645952 139733503477568 submission.py:296] 35) loss = 0.580, grad_norm = 1.172
I0607 11:44:22.959490 139681286661888 logging_writer.py:48] [36] global_step=36, grad_norm=1.140554, loss=0.573754
I0607 11:44:22.963191 139733503477568 submission.py:296] 36) loss = 0.574, grad_norm = 1.141
I0607 11:44:23.281445 139681295054592 logging_writer.py:48] [37] global_step=37, grad_norm=1.097781, loss=0.569753
I0607 11:44:23.285627 139733503477568 submission.py:296] 37) loss = 0.570, grad_norm = 1.098
I0607 11:44:23.598525 139681286661888 logging_writer.py:48] [38] global_step=38, grad_norm=1.066466, loss=0.563452
I0607 11:44:23.602599 139733503477568 submission.py:296] 38) loss = 0.563, grad_norm = 1.066
I0607 11:44:23.914293 139681295054592 logging_writer.py:48] [39] global_step=39, grad_norm=1.035433, loss=0.556473
I0607 11:44:23.918330 139733503477568 submission.py:296] 39) loss = 0.556, grad_norm = 1.035
I0607 11:44:24.240949 139681286661888 logging_writer.py:48] [40] global_step=40, grad_norm=1.017941, loss=0.551604
I0607 11:44:24.244963 139733503477568 submission.py:296] 40) loss = 0.552, grad_norm = 1.018
I0607 11:44:24.563378 139681295054592 logging_writer.py:48] [41] global_step=41, grad_norm=1.008869, loss=0.546495
I0607 11:44:24.567459 139733503477568 submission.py:296] 41) loss = 0.546, grad_norm = 1.009
I0607 11:44:24.878070 139681286661888 logging_writer.py:48] [42] global_step=42, grad_norm=1.003728, loss=0.540461
I0607 11:44:24.881976 139733503477568 submission.py:296] 42) loss = 0.540, grad_norm = 1.004
I0607 11:44:25.198222 139681295054592 logging_writer.py:48] [43] global_step=43, grad_norm=1.003712, loss=0.534636
I0607 11:44:25.202414 139733503477568 submission.py:296] 43) loss = 0.535, grad_norm = 1.004
I0607 11:44:25.517118 139681286661888 logging_writer.py:48] [44] global_step=44, grad_norm=0.975579, loss=0.530369
I0607 11:44:25.521243 139733503477568 submission.py:296] 44) loss = 0.530, grad_norm = 0.976
I0607 11:44:25.835040 139681295054592 logging_writer.py:48] [45] global_step=45, grad_norm=0.953993, loss=0.524385
I0607 11:44:25.838973 139733503477568 submission.py:296] 45) loss = 0.524, grad_norm = 0.954
I0607 11:44:26.150755 139681286661888 logging_writer.py:48] [46] global_step=46, grad_norm=0.946759, loss=0.518855
I0607 11:44:26.154742 139733503477568 submission.py:296] 46) loss = 0.519, grad_norm = 0.947
I0607 11:44:26.467634 139681295054592 logging_writer.py:48] [47] global_step=47, grad_norm=0.965796, loss=0.511497
I0607 11:44:26.471606 139733503477568 submission.py:296] 47) loss = 0.511, grad_norm = 0.966
I0607 11:44:26.786486 139681286661888 logging_writer.py:48] [48] global_step=48, grad_norm=0.943452, loss=0.505193
I0607 11:44:26.790608 139733503477568 submission.py:296] 48) loss = 0.505, grad_norm = 0.943
I0607 11:44:27.126325 139681295054592 logging_writer.py:48] [49] global_step=49, grad_norm=0.906054, loss=0.500623
I0607 11:44:27.130044 139733503477568 submission.py:296] 49) loss = 0.501, grad_norm = 0.906
I0607 11:44:27.446497 139681286661888 logging_writer.py:48] [50] global_step=50, grad_norm=0.859796, loss=0.496254
I0607 11:44:27.450444 139733503477568 submission.py:296] 50) loss = 0.496, grad_norm = 0.860
I0607 11:44:27.764809 139681295054592 logging_writer.py:48] [51] global_step=51, grad_norm=0.827779, loss=0.490291
I0607 11:44:27.768693 139733503477568 submission.py:296] 51) loss = 0.490, grad_norm = 0.828
I0607 11:44:28.084009 139681286661888 logging_writer.py:48] [52] global_step=52, grad_norm=0.799239, loss=0.485507
I0607 11:44:28.088394 139733503477568 submission.py:296] 52) loss = 0.486, grad_norm = 0.799
I0607 11:44:28.402711 139681295054592 logging_writer.py:48] [53] global_step=53, grad_norm=0.798961, loss=0.480141
I0607 11:44:28.406857 139733503477568 submission.py:296] 53) loss = 0.480, grad_norm = 0.799
I0607 11:44:28.733432 139681286661888 logging_writer.py:48] [54] global_step=54, grad_norm=0.789374, loss=0.476427
I0607 11:44:28.737583 139733503477568 submission.py:296] 54) loss = 0.476, grad_norm = 0.789
I0607 11:44:29.063381 139681295054592 logging_writer.py:48] [55] global_step=55, grad_norm=0.760208, loss=0.472815
I0607 11:44:29.068102 139733503477568 submission.py:296] 55) loss = 0.473, grad_norm = 0.760
I0607 11:44:29.375640 139681286661888 logging_writer.py:48] [56] global_step=56, grad_norm=0.735037, loss=0.466963
I0607 11:44:29.379574 139733503477568 submission.py:296] 56) loss = 0.467, grad_norm = 0.735
I0607 11:44:29.692997 139681295054592 logging_writer.py:48] [57] global_step=57, grad_norm=0.699902, loss=0.464638
I0607 11:44:29.697020 139733503477568 submission.py:296] 57) loss = 0.465, grad_norm = 0.700
I0607 11:44:30.013728 139681286661888 logging_writer.py:48] [58] global_step=58, grad_norm=0.678371, loss=0.460051
I0607 11:44:30.017758 139733503477568 submission.py:296] 58) loss = 0.460, grad_norm = 0.678
I0607 11:44:30.330992 139681295054592 logging_writer.py:48] [59] global_step=59, grad_norm=0.668857, loss=0.455575
I0607 11:44:30.334731 139733503477568 submission.py:296] 59) loss = 0.456, grad_norm = 0.669
I0607 11:44:30.647531 139681286661888 logging_writer.py:48] [60] global_step=60, grad_norm=0.651857, loss=0.451130
I0607 11:44:30.651724 139733503477568 submission.py:296] 60) loss = 0.451, grad_norm = 0.652
I0607 11:44:30.965761 139681295054592 logging_writer.py:48] [61] global_step=61, grad_norm=0.644382, loss=0.449630
I0607 11:44:30.970026 139733503477568 submission.py:296] 61) loss = 0.450, grad_norm = 0.644
I0607 11:44:31.283653 139681286661888 logging_writer.py:48] [62] global_step=62, grad_norm=0.634195, loss=0.443983
I0607 11:44:31.287968 139733503477568 submission.py:296] 62) loss = 0.444, grad_norm = 0.634
I0607 11:44:31.604324 139681295054592 logging_writer.py:48] [63] global_step=63, grad_norm=0.615139, loss=0.443261
I0607 11:44:31.608396 139733503477568 submission.py:296] 63) loss = 0.443, grad_norm = 0.615
I0607 11:44:31.933584 139681286661888 logging_writer.py:48] [64] global_step=64, grad_norm=0.609172, loss=0.437846
I0607 11:44:31.937684 139733503477568 submission.py:296] 64) loss = 0.438, grad_norm = 0.609
I0607 11:44:32.256801 139681295054592 logging_writer.py:48] [65] global_step=65, grad_norm=0.604911, loss=0.433274
I0607 11:44:32.260665 139733503477568 submission.py:296] 65) loss = 0.433, grad_norm = 0.605
I0607 11:44:32.578561 139681286661888 logging_writer.py:48] [66] global_step=66, grad_norm=0.590768, loss=0.429273
I0607 11:44:32.582534 139733503477568 submission.py:296] 66) loss = 0.429, grad_norm = 0.591
I0607 11:44:32.898541 139681295054592 logging_writer.py:48] [67] global_step=67, grad_norm=0.573579, loss=0.425235
I0607 11:44:32.902589 139733503477568 submission.py:296] 67) loss = 0.425, grad_norm = 0.574
I0607 11:44:33.218885 139681286661888 logging_writer.py:48] [68] global_step=68, grad_norm=0.557426, loss=0.423560
I0607 11:44:33.223242 139733503477568 submission.py:296] 68) loss = 0.424, grad_norm = 0.557
I0607 11:44:33.539854 139681295054592 logging_writer.py:48] [69] global_step=69, grad_norm=0.547836, loss=0.420855
I0607 11:44:33.543940 139733503477568 submission.py:296] 69) loss = 0.421, grad_norm = 0.548
I0607 11:44:33.857922 139681286661888 logging_writer.py:48] [70] global_step=70, grad_norm=0.542326, loss=0.417179
I0607 11:44:33.862066 139733503477568 submission.py:296] 70) loss = 0.417, grad_norm = 0.542
I0607 11:44:34.180455 139681295054592 logging_writer.py:48] [71] global_step=71, grad_norm=0.537295, loss=0.413221
I0607 11:44:34.184403 139733503477568 submission.py:296] 71) loss = 0.413, grad_norm = 0.537
I0607 11:44:34.499558 139681286661888 logging_writer.py:48] [72] global_step=72, grad_norm=0.519602, loss=0.410218
I0607 11:44:34.503658 139733503477568 submission.py:296] 72) loss = 0.410, grad_norm = 0.520
I0607 11:44:34.819751 139681295054592 logging_writer.py:48] [73] global_step=73, grad_norm=0.507768, loss=0.408283
I0607 11:44:34.823857 139733503477568 submission.py:296] 73) loss = 0.408, grad_norm = 0.508
I0607 11:44:35.160128 139681286661888 logging_writer.py:48] [74] global_step=74, grad_norm=0.500595, loss=0.405685
I0607 11:44:35.164625 139733503477568 submission.py:296] 74) loss = 0.406, grad_norm = 0.501
I0607 11:44:35.479657 139681295054592 logging_writer.py:48] [75] global_step=75, grad_norm=0.495901, loss=0.401595
I0607 11:44:35.483830 139733503477568 submission.py:296] 75) loss = 0.402, grad_norm = 0.496
I0607 11:44:35.794483 139681286661888 logging_writer.py:48] [76] global_step=76, grad_norm=0.486286, loss=0.400826
I0607 11:44:35.798404 139733503477568 submission.py:296] 76) loss = 0.401, grad_norm = 0.486
I0607 11:44:36.119628 139681295054592 logging_writer.py:48] [77] global_step=77, grad_norm=0.484374, loss=0.397926
I0607 11:44:36.123707 139733503477568 submission.py:296] 77) loss = 0.398, grad_norm = 0.484
I0607 11:44:36.442786 139681286661888 logging_writer.py:48] [78] global_step=78, grad_norm=0.479854, loss=0.394374
I0607 11:44:36.447018 139733503477568 submission.py:296] 78) loss = 0.394, grad_norm = 0.480
I0607 11:44:36.763162 139681295054592 logging_writer.py:48] [79] global_step=79, grad_norm=0.468884, loss=0.390847
I0607 11:44:36.767493 139733503477568 submission.py:296] 79) loss = 0.391, grad_norm = 0.469
I0607 11:44:37.086312 139681286661888 logging_writer.py:48] [80] global_step=80, grad_norm=0.465120, loss=0.389530
I0607 11:44:37.090603 139733503477568 submission.py:296] 80) loss = 0.390, grad_norm = 0.465
I0607 11:44:37.402680 139681295054592 logging_writer.py:48] [81] global_step=81, grad_norm=0.455118, loss=0.386643
I0607 11:44:37.406710 139733503477568 submission.py:296] 81) loss = 0.387, grad_norm = 0.455
I0607 11:44:37.721354 139681286661888 logging_writer.py:48] [82] global_step=82, grad_norm=0.450267, loss=0.383323
I0607 11:44:37.725276 139733503477568 submission.py:296] 82) loss = 0.383, grad_norm = 0.450
I0607 11:44:38.039199 139681295054592 logging_writer.py:48] [83] global_step=83, grad_norm=0.444235, loss=0.381663
I0607 11:44:38.043276 139733503477568 submission.py:296] 83) loss = 0.382, grad_norm = 0.444
I0607 11:44:38.360122 139681286661888 logging_writer.py:48] [84] global_step=84, grad_norm=0.439261, loss=0.379623
I0607 11:44:38.364259 139733503477568 submission.py:296] 84) loss = 0.380, grad_norm = 0.439
I0607 11:44:38.677083 139681295054592 logging_writer.py:48] [85] global_step=85, grad_norm=0.439110, loss=0.377368
I0607 11:44:38.680991 139733503477568 submission.py:296] 85) loss = 0.377, grad_norm = 0.439
I0607 11:44:38.998075 139681286661888 logging_writer.py:48] [86] global_step=86, grad_norm=0.438272, loss=0.372963
I0607 11:44:39.002020 139733503477568 submission.py:296] 86) loss = 0.373, grad_norm = 0.438
I0607 11:44:39.320098 139681295054592 logging_writer.py:48] [87] global_step=87, grad_norm=0.430248, loss=0.370076
I0607 11:44:39.324120 139733503477568 submission.py:296] 87) loss = 0.370, grad_norm = 0.430
I0607 11:44:39.644902 139681286661888 logging_writer.py:48] [88] global_step=88, grad_norm=0.424862, loss=0.369291
I0607 11:44:39.648954 139733503477568 submission.py:296] 88) loss = 0.369, grad_norm = 0.425
I0607 11:44:39.974827 139681295054592 logging_writer.py:48] [89] global_step=89, grad_norm=0.422288, loss=0.366383
I0607 11:44:39.978880 139733503477568 submission.py:296] 89) loss = 0.366, grad_norm = 0.422
I0607 11:44:40.297585 139681286661888 logging_writer.py:48] [90] global_step=90, grad_norm=0.418637, loss=0.366086
I0607 11:44:40.301546 139733503477568 submission.py:296] 90) loss = 0.366, grad_norm = 0.419
I0607 11:44:40.620912 139681295054592 logging_writer.py:48] [91] global_step=91, grad_norm=0.413483, loss=0.364399
I0607 11:44:40.624989 139733503477568 submission.py:296] 91) loss = 0.364, grad_norm = 0.413
I0607 11:44:40.941015 139681286661888 logging_writer.py:48] [92] global_step=92, grad_norm=0.412882, loss=0.361090
I0607 11:44:40.944931 139733503477568 submission.py:296] 92) loss = 0.361, grad_norm = 0.413
I0607 11:44:41.259485 139681295054592 logging_writer.py:48] [93] global_step=93, grad_norm=0.407367, loss=0.358938
I0607 11:44:41.263650 139733503477568 submission.py:296] 93) loss = 0.359, grad_norm = 0.407
I0607 11:44:41.576675 139681286661888 logging_writer.py:48] [94] global_step=94, grad_norm=0.405690, loss=0.358757
I0607 11:44:41.580577 139733503477568 submission.py:296] 94) loss = 0.359, grad_norm = 0.406
I0607 11:44:41.904918 139681295054592 logging_writer.py:48] [95] global_step=95, grad_norm=0.403179, loss=0.357402
I0607 11:44:41.909175 139733503477568 submission.py:296] 95) loss = 0.357, grad_norm = 0.403
I0607 11:44:42.224849 139681286661888 logging_writer.py:48] [96] global_step=96, grad_norm=0.403148, loss=0.352410
I0607 11:44:42.228873 139733503477568 submission.py:296] 96) loss = 0.352, grad_norm = 0.403
I0607 11:44:42.534628 139681295054592 logging_writer.py:48] [97] global_step=97, grad_norm=0.401012, loss=0.351131
I0607 11:44:42.538847 139733503477568 submission.py:296] 97) loss = 0.351, grad_norm = 0.401
I0607 11:44:42.834977 139681286661888 logging_writer.py:48] [98] global_step=98, grad_norm=0.395853, loss=0.348630
I0607 11:44:42.838952 139733503477568 submission.py:296] 98) loss = 0.349, grad_norm = 0.396
I0607 11:44:43.139832 139681295054592 logging_writer.py:48] [99] global_step=99, grad_norm=0.394500, loss=0.347153
I0607 11:44:43.143883 139733503477568 submission.py:296] 99) loss = 0.347, grad_norm = 0.394
I0607 11:44:43.458955 139681286661888 logging_writer.py:48] [100] global_step=100, grad_norm=0.390475, loss=0.344973
I0607 11:44:43.463003 139733503477568 submission.py:296] 100) loss = 0.345, grad_norm = 0.390
I0607 11:46:40.965053 139681295054592 logging_writer.py:48] [500] global_step=500, grad_norm=0.025555, loss=0.059742
I0607 11:46:40.969728 139733503477568 submission.py:296] 500) loss = 0.060, grad_norm = 0.026
I0607 11:48:12.098056 139733503477568 spec.py:298] Evaluating on the training split.
I0607 11:49:09.378875 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 11:49:12.578394 139733503477568 spec.py:326] Evaluating on the test split.
I0607 11:49:15.767522 139733503477568 submission_runner.py:419] Time since start: 456.92s, 	Step: 813, 	{'train/accuracy': 0.9867493930933282, 'train/loss': 0.05322541966426859, 'train/mean_average_precision': 0.04918263127260883, 'validation/accuracy': 0.9842214909877105, 'validation/loss': 0.06241768283772033, 'validation/mean_average_precision': 0.0486176301186144, 'validation/num_examples': 43793, 'test/accuracy': 0.9832377151199204, 'test/loss': 0.06559185855945521, 'test/mean_average_precision': 0.05141372191930102, 'test/num_examples': 43793, 'score': 245.31859254837036, 'total_duration': 456.92142271995544, 'accumulated_submission_time': 245.31859254837036, 'accumulated_eval_time': 211.35571837425232, 'accumulated_logging_time': 0.028687477111816406}
I0607 11:49:15.779088 139681286661888 logging_writer.py:48] [813] accumulated_eval_time=211.355718, accumulated_logging_time=0.028687, accumulated_submission_time=245.318593, global_step=813, preemption_count=0, score=245.318593, test/accuracy=0.983238, test/loss=0.065592, test/mean_average_precision=0.051414, test/num_examples=43793, total_duration=456.921423, train/accuracy=0.986749, train/loss=0.053225, train/mean_average_precision=0.049183, validation/accuracy=0.984221, validation/loss=0.062418, validation/mean_average_precision=0.048618, validation/num_examples=43793
I0607 11:50:12.425441 139681295054592 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.050514, loss=0.052846
I0607 11:50:12.430634 139733503477568 submission.py:296] 1000) loss = 0.053, grad_norm = 0.051
I0607 11:52:39.480182 139681286661888 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.090419, loss=0.046599
I0607 11:52:39.485664 139733503477568 submission.py:296] 1500) loss = 0.047, grad_norm = 0.090
I0607 11:53:15.917654 139733503477568 spec.py:298] Evaluating on the training split.
I0607 11:54:12.824325 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 11:54:16.024549 139733503477568 spec.py:326] Evaluating on the test split.
I0607 11:54:19.153958 139733503477568 submission_runner.py:419] Time since start: 760.31s, 	Step: 1623, 	{'train/accuracy': 0.9872332799501433, 'train/loss': 0.04791965270893952, 'train/mean_average_precision': 0.10138192203982103, 'validation/accuracy': 0.9844808870940898, 'validation/loss': 0.05738474125745815, 'validation/mean_average_precision': 0.1016398966430054, 'validation/num_examples': 43793, 'test/accuracy': 0.9834466276836946, 'test/loss': 0.06059150765163324, 'test/mean_average_precision': 0.10035739237710407, 'test/num_examples': 43793, 'score': 485.233585357666, 'total_duration': 760.3078620433807, 'accumulated_submission_time': 485.233585357666, 'accumulated_eval_time': 274.5917761325836, 'accumulated_logging_time': 0.051976919174194336}
I0607 11:54:19.164266 139681295054592 logging_writer.py:48] [1623] accumulated_eval_time=274.591776, accumulated_logging_time=0.051977, accumulated_submission_time=485.233585, global_step=1623, preemption_count=0, score=485.233585, test/accuracy=0.983447, test/loss=0.060592, test/mean_average_precision=0.100357, test/num_examples=43793, total_duration=760.307862, train/accuracy=0.987233, train/loss=0.047920, train/mean_average_precision=0.101382, validation/accuracy=0.984481, validation/loss=0.057385, validation/mean_average_precision=0.101640, validation/num_examples=43793
I0607 11:56:10.344649 139681286661888 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.030833, loss=0.041492
I0607 11:56:10.349280 139733503477568 submission.py:296] 2000) loss = 0.041, grad_norm = 0.031
I0607 11:58:19.402593 139733503477568 spec.py:298] Evaluating on the training split.
I0607 11:59:17.105459 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 11:59:20.281792 139733503477568 spec.py:326] Evaluating on the test split.
I0607 11:59:23.469479 139733503477568 submission_runner.py:419] Time since start: 1064.62s, 	Step: 2441, 	{'train/accuracy': 0.9879263319628766, 'train/loss': 0.04322425189204725, 'train/mean_average_precision': 0.15634587801903174, 'validation/accuracy': 0.9850962931931052, 'validation/loss': 0.05192526383100851, 'validation/mean_average_precision': 0.14340183208574017, 'validation/num_examples': 43793, 'test/accuracy': 0.9841196967903701, 'test/loss': 0.054466000954427156, 'test/mean_average_precision': 0.14838453014016115, 'test/num_examples': 43793, 'score': 725.2487759590149, 'total_duration': 1064.6234338283539, 'accumulated_submission_time': 725.2487759590149, 'accumulated_eval_time': 338.6585023403168, 'accumulated_logging_time': 0.07243609428405762}
I0607 11:59:23.479517 139681295054592 logging_writer.py:48] [2441] accumulated_eval_time=338.658502, accumulated_logging_time=0.072436, accumulated_submission_time=725.248776, global_step=2441, preemption_count=0, score=725.248776, test/accuracy=0.984120, test/loss=0.054466, test/mean_average_precision=0.148385, test/num_examples=43793, total_duration=1064.623434, train/accuracy=0.987926, train/loss=0.043224, train/mean_average_precision=0.156346, validation/accuracy=0.985096, validation/loss=0.051925, validation/mean_average_precision=0.143402, validation/num_examples=43793
I0607 11:59:40.977210 139681286661888 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.054224, loss=0.044796
I0607 11:59:40.982332 139733503477568 submission.py:296] 2500) loss = 0.045, grad_norm = 0.054
I0607 12:02:08.250848 139681295054592 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.033537, loss=0.040526
I0607 12:02:08.255844 139733503477568 submission.py:296] 3000) loss = 0.041, grad_norm = 0.034
I0607 12:03:23.713217 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:04:21.959391 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:04:25.184342 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:04:28.309896 139733503477568 submission_runner.py:419] Time since start: 1369.46s, 	Step: 3257, 	{'train/accuracy': 0.988314631749966, 'train/loss': 0.04062472718869871, 'train/mean_average_precision': 0.18688978162589243, 'validation/accuracy': 0.9853824813855893, 'validation/loss': 0.05030036567138126, 'validation/mean_average_precision': 0.1663081485155946, 'validation/num_examples': 43793, 'test/accuracy': 0.9844945600600455, 'test/loss': 0.05307111603745095, 'test/mean_average_precision': 0.17156174086658935, 'test/num_examples': 43793, 'score': 965.2607612609863, 'total_duration': 1369.463802099228, 'accumulated_submission_time': 965.2607612609863, 'accumulated_eval_time': 403.25494408607483, 'accumulated_logging_time': 0.09304094314575195}
I0607 12:04:28.320414 139681286661888 logging_writer.py:48] [3257] accumulated_eval_time=403.254944, accumulated_logging_time=0.093041, accumulated_submission_time=965.260761, global_step=3257, preemption_count=0, score=965.260761, test/accuracy=0.984495, test/loss=0.053071, test/mean_average_precision=0.171562, test/num_examples=43793, total_duration=1369.463802, train/accuracy=0.988315, train/loss=0.040625, train/mean_average_precision=0.186890, validation/accuracy=0.985382, validation/loss=0.050300, validation/mean_average_precision=0.166308, validation/num_examples=43793
I0607 12:05:41.212868 139681295054592 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.047188, loss=0.044880
I0607 12:05:41.218989 139733503477568 submission.py:296] 3500) loss = 0.045, grad_norm = 0.047
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0607 12:08:07.313475 139681286661888 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.038083, loss=0.044145
I0607 12:08:07.319598 139733503477568 submission.py:296] 4000) loss = 0.044, grad_norm = 0.038
I0607 12:08:28.577489 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:09:25.307857 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:09:28.542717 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:09:31.861710 139733503477568 submission_runner.py:419] Time since start: 1673.02s, 	Step: 4073, 	{'train/accuracy': 0.9885471225437423, 'train/loss': 0.03954627428266029, 'train/mean_average_precision': 0.21038666520087274, 'validation/accuracy': 0.9855667784627351, 'validation/loss': 0.04901214775003308, 'validation/mean_average_precision': 0.185379927423137, 'validation/num_examples': 43793, 'test/accuracy': 0.9846179701027589, 'test/loss': 0.05162437810604756, 'test/mean_average_precision': 0.1895384285288494, 'test/num_examples': 43793, 'score': 1205.2896554470062, 'total_duration': 1673.0155284404755, 'accumulated_submission_time': 1205.2896554470062, 'accumulated_eval_time': 466.538831949234, 'accumulated_logging_time': 0.11581254005432129}
I0607 12:09:31.872653 139681295054592 logging_writer.py:48] [4073] accumulated_eval_time=466.538832, accumulated_logging_time=0.115813, accumulated_submission_time=1205.289655, global_step=4073, preemption_count=0, score=1205.289655, test/accuracy=0.984618, test/loss=0.051624, test/mean_average_precision=0.189538, test/num_examples=43793, total_duration=1673.015528, train/accuracy=0.988547, train/loss=0.039546, train/mean_average_precision=0.210387, validation/accuracy=0.985567, validation/loss=0.049012, validation/mean_average_precision=0.185380, validation/num_examples=43793
I0607 12:11:39.086735 139681286661888 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016999, loss=0.042999
I0607 12:11:39.094474 139733503477568 submission.py:296] 4500) loss = 0.043, grad_norm = 0.017
I0607 12:13:31.900612 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:14:30.680491 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:14:33.838357 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:14:36.949655 139733503477568 submission_runner.py:419] Time since start: 1978.10s, 	Step: 4880, 	{'train/accuracy': 0.9888096199772624, 'train/loss': 0.03813204482020539, 'train/mean_average_precision': 0.239242876368897, 'validation/accuracy': 0.985903709242539, 'validation/loss': 0.0479558107670899, 'validation/mean_average_precision': 0.2098113949236059, 'validation/num_examples': 43793, 'test/accuracy': 0.9849679828860176, 'test/loss': 0.050703185842888486, 'test/mean_average_precision': 0.21055854572513355, 'test/num_examples': 43793, 'score': 1445.0920367240906, 'total_duration': 1978.1036145687103, 'accumulated_submission_time': 1445.0920367240906, 'accumulated_eval_time': 531.5877513885498, 'accumulated_logging_time': 0.1385793685913086}
I0607 12:14:36.961749 139681295054592 logging_writer.py:48] [4880] accumulated_eval_time=531.587751, accumulated_logging_time=0.138579, accumulated_submission_time=1445.092037, global_step=4880, preemption_count=0, score=1445.092037, test/accuracy=0.984968, test/loss=0.050703, test/mean_average_precision=0.210559, test/num_examples=43793, total_duration=1978.103615, train/accuracy=0.988810, train/loss=0.038132, train/mean_average_precision=0.239243, validation/accuracy=0.985904, validation/loss=0.047956, validation/mean_average_precision=0.209811, validation/num_examples=43793
I0607 12:15:13.321708 139681286661888 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.014809, loss=0.040491
I0607 12:15:13.327434 139733503477568 submission.py:296] 5000) loss = 0.040, grad_norm = 0.015
I0607 12:17:39.467023 139681295054592 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.012303, loss=0.041910
I0607 12:17:39.472203 139733503477568 submission.py:296] 5500) loss = 0.042, grad_norm = 0.012
I0607 12:18:36.979958 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:19:37.185756 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:19:40.349617 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:19:43.496365 139733503477568 submission_runner.py:419] Time since start: 2284.65s, 	Step: 5697, 	{'train/accuracy': 0.9890326782467711, 'train/loss': 0.037578720205301556, 'train/mean_average_precision': 0.2630152224190091, 'validation/accuracy': 0.9861192637534738, 'validation/loss': 0.04713733600503204, 'validation/mean_average_precision': 0.21642623130511981, 'validation/num_examples': 43793, 'test/accuracy': 0.9851465694324697, 'test/loss': 0.049879055920544146, 'test/mean_average_precision': 0.21117851361082635, 'test/num_examples': 43793, 'score': 1684.8837049007416, 'total_duration': 2284.6502628326416, 'accumulated_submission_time': 1684.8837049007416, 'accumulated_eval_time': 598.1038835048676, 'accumulated_logging_time': 0.16190862655639648}
I0607 12:19:43.506953 139681286661888 logging_writer.py:48] [5697] accumulated_eval_time=598.103884, accumulated_logging_time=0.161909, accumulated_submission_time=1684.883705, global_step=5697, preemption_count=0, score=1684.883705, test/accuracy=0.985147, test/loss=0.049879, test/mean_average_precision=0.211179, test/num_examples=43793, total_duration=2284.650263, train/accuracy=0.989033, train/loss=0.037579, train/mean_average_precision=0.263015, validation/accuracy=0.986119, validation/loss=0.047137, validation/mean_average_precision=0.216426, validation/num_examples=43793
I0607 12:21:13.593452 139681295054592 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.019130, loss=0.042580
I0607 12:21:13.599258 139733503477568 submission.py:296] 6000) loss = 0.043, grad_norm = 0.019
I0607 12:23:41.473724 139681286661888 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.013629, loss=0.041653
I0607 12:23:41.479501 139733503477568 submission.py:296] 6500) loss = 0.042, grad_norm = 0.014
I0607 12:23:43.582715 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:24:43.487210 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:24:46.631523 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:24:49.823759 139733503477568 submission_runner.py:419] Time since start: 2590.98s, 	Step: 6508, 	{'train/accuracy': 0.989510108065382, 'train/loss': 0.03576481837540777, 'train/mean_average_precision': 0.29053396778689905, 'validation/accuracy': 0.986102620184833, 'validation/loss': 0.04651261229842, 'validation/mean_average_precision': 0.22704528828996823, 'validation/num_examples': 43793, 'test/accuracy': 0.985268715891128, 'test/loss': 0.049126163839888735, 'test/mean_average_precision': 0.22799319414026498, 'test/num_examples': 43793, 'score': 1924.7372615337372, 'total_duration': 2590.9776751995087, 'accumulated_submission_time': 1924.7372615337372, 'accumulated_eval_time': 664.3446640968323, 'accumulated_logging_time': 0.18303608894348145}
I0607 12:24:49.833734 139681295054592 logging_writer.py:48] [6508] accumulated_eval_time=664.344664, accumulated_logging_time=0.183036, accumulated_submission_time=1924.737262, global_step=6508, preemption_count=0, score=1924.737262, test/accuracy=0.985269, test/loss=0.049126, test/mean_average_precision=0.227993, test/num_examples=43793, total_duration=2590.977675, train/accuracy=0.989510, train/loss=0.035765, train/mean_average_precision=0.290534, validation/accuracy=0.986103, validation/loss=0.046513, validation/mean_average_precision=0.227045, validation/num_examples=43793
I0607 12:27:15.901111 139681286661888 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.012186, loss=0.041504
I0607 12:27:15.907797 139733503477568 submission.py:296] 7000) loss = 0.042, grad_norm = 0.012
I0607 12:28:49.873725 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:29:48.845858 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:29:52.125649 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:29:55.358557 139733503477568 submission_runner.py:419] Time since start: 2896.51s, 	Step: 7323, 	{'train/accuracy': 0.9895325093570669, 'train/loss': 0.035677623487825445, 'train/mean_average_precision': 0.2853906852061793, 'validation/accuracy': 0.9863798776819487, 'validation/loss': 0.04620286051694924, 'validation/mean_average_precision': 0.22921092714955893, 'validation/num_examples': 43793, 'test/accuracy': 0.9854894219060828, 'test/loss': 0.048817842749280915, 'test/mean_average_precision': 0.2304124624687853, 'test/num_examples': 43793, 'score': 2164.55100941658, 'total_duration': 2896.512417078018, 'accumulated_submission_time': 2164.55100941658, 'accumulated_eval_time': 729.8292510509491, 'accumulated_logging_time': 0.20536541938781738}
I0607 12:29:55.369715 139681295054592 logging_writer.py:48] [7323] accumulated_eval_time=729.829251, accumulated_logging_time=0.205365, accumulated_submission_time=2164.551009, global_step=7323, preemption_count=0, score=2164.551009, test/accuracy=0.985489, test/loss=0.048818, test/mean_average_precision=0.230412, test/num_examples=43793, total_duration=2896.512417, train/accuracy=0.989533, train/loss=0.035678, train/mean_average_precision=0.285391, validation/accuracy=0.986380, validation/loss=0.046203, validation/mean_average_precision=0.229211, validation/num_examples=43793
I0607 12:30:46.938127 139681286661888 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.013434, loss=0.042010
I0607 12:30:46.943888 139733503477568 submission.py:296] 7500) loss = 0.042, grad_norm = 0.013
I0607 12:33:14.892992 139681295054592 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.012225, loss=0.038714
I0607 12:33:14.899852 139733503477568 submission.py:296] 8000) loss = 0.039, grad_norm = 0.012
I0607 12:33:55.435627 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:34:55.517459 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:34:58.772292 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:35:02.052915 139733503477568 submission_runner.py:419] Time since start: 3203.21s, 	Step: 8139, 	{'train/accuracy': 0.9897939017337514, 'train/loss': 0.03441838044501415, 'train/mean_average_precision': 0.32015491452784983, 'validation/accuracy': 0.9864294024471729, 'validation/loss': 0.04567599706139528, 'validation/mean_average_precision': 0.2312404282729669, 'validation/num_examples': 43793, 'test/accuracy': 0.9854607806675009, 'test/loss': 0.0485900455269335, 'test/mean_average_precision': 0.22641564436727493, 'test/num_examples': 43793, 'score': 2404.3951449394226, 'total_duration': 3203.2067246437073, 'accumulated_submission_time': 2404.3951449394226, 'accumulated_eval_time': 796.4461789131165, 'accumulated_logging_time': 0.22836637496948242}
I0607 12:35:02.064967 139681286661888 logging_writer.py:48] [8139] accumulated_eval_time=796.446179, accumulated_logging_time=0.228366, accumulated_submission_time=2404.395145, global_step=8139, preemption_count=0, score=2404.395145, test/accuracy=0.985461, test/loss=0.048590, test/mean_average_precision=0.226416, test/num_examples=43793, total_duration=3203.206725, train/accuracy=0.989794, train/loss=0.034418, train/mean_average_precision=0.320155, validation/accuracy=0.986429, validation/loss=0.045676, validation/mean_average_precision=0.231240, validation/num_examples=43793
I0607 12:36:48.455532 139681295054592 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.020690, loss=0.040124
I0607 12:36:48.460861 139733503477568 submission.py:296] 8500) loss = 0.040, grad_norm = 0.021
I0607 12:39:02.339254 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:40:01.055500 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:40:04.335774 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:40:07.547584 139733503477568 submission_runner.py:419] Time since start: 3508.70s, 	Step: 8960, 	{'train/accuracy': 0.9898971536345628, 'train/loss': 0.03382295020249023, 'train/mean_average_precision': 0.333422563556964, 'validation/accuracy': 0.986505719298502, 'validation/loss': 0.0452257834553997, 'validation/mean_average_precision': 0.23999795240400223, 'validation/num_examples': 43793, 'test/accuracy': 0.9856364188511578, 'test/loss': 0.04797617401700532, 'test/mean_average_precision': 0.2431258285630593, 'test/num_examples': 43793, 'score': 2644.444247484207, 'total_duration': 3508.7015001773834, 'accumulated_submission_time': 2644.444247484207, 'accumulated_eval_time': 861.6542859077454, 'accumulated_logging_time': 0.252488374710083}
I0607 12:40:07.558297 139681286661888 logging_writer.py:48] [8960] accumulated_eval_time=861.654286, accumulated_logging_time=0.252488, accumulated_submission_time=2644.444247, global_step=8960, preemption_count=0, score=2644.444247, test/accuracy=0.985636, test/loss=0.047976, test/mean_average_precision=0.243126, test/num_examples=43793, total_duration=3508.701500, train/accuracy=0.989897, train/loss=0.033823, train/mean_average_precision=0.333423, validation/accuracy=0.986506, validation/loss=0.045226, validation/mean_average_precision=0.239998, validation/num_examples=43793
I0607 12:40:19.796877 139681295054592 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.011864, loss=0.036469
I0607 12:40:19.805434 139733503477568 submission.py:296] 9000) loss = 0.036, grad_norm = 0.012
I0607 12:42:46.486463 139681286661888 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.010200, loss=0.034922
I0607 12:42:46.491951 139733503477568 submission.py:296] 9500) loss = 0.035, grad_norm = 0.010
I0607 12:44:07.746220 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:45:06.815826 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:45:10.149995 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:45:13.346861 139733503477568 submission_runner.py:419] Time since start: 3814.50s, 	Step: 9773, 	{'train/accuracy': 0.9901306386549167, 'train/loss': 0.03318173943386965, 'train/mean_average_precision': 0.3635692951333488, 'validation/accuracy': 0.9866636302302414, 'validation/loss': 0.04495200499692703, 'validation/mean_average_precision': 0.24990092234032926, 'validation/num_examples': 43793, 'test/accuracy': 0.9857455082745802, 'test/loss': 0.047524367033892276, 'test/mean_average_precision': 0.244853141440761, 'test/num_examples': 43793, 'score': 2884.4118287563324, 'total_duration': 3814.5008177757263, 'accumulated_submission_time': 2884.4118287563324, 'accumulated_eval_time': 927.2547194957733, 'accumulated_logging_time': 0.2777531147003174}
I0607 12:45:13.357614 139681295054592 logging_writer.py:48] [9773] accumulated_eval_time=927.254719, accumulated_logging_time=0.277753, accumulated_submission_time=2884.411829, global_step=9773, preemption_count=0, score=2884.411829, test/accuracy=0.985746, test/loss=0.047524, test/mean_average_precision=0.244853, test/num_examples=43793, total_duration=3814.500818, train/accuracy=0.990131, train/loss=0.033182, train/mean_average_precision=0.363569, validation/accuracy=0.986664, validation/loss=0.044952, validation/mean_average_precision=0.249901, validation/num_examples=43793
I0607 12:46:19.885154 139681286661888 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.010599, loss=0.034195
I0607 12:46:19.890602 139733503477568 submission.py:296] 10000) loss = 0.034, grad_norm = 0.011
I0607 12:48:47.032000 139681295054592 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.010090, loss=0.038046
I0607 12:48:47.037985 139733503477568 submission.py:296] 10500) loss = 0.038, grad_norm = 0.010
I0607 12:49:13.632190 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:50:14.662677 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:50:17.976212 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:50:21.280386 139733503477568 submission_runner.py:419] Time since start: 4122.43s, 	Step: 10591, 	{'train/accuracy': 0.9904838850055864, 'train/loss': 0.03184294806789017, 'train/mean_average_precision': 0.37081049942645633, 'validation/accuracy': 0.98666687775583, 'validation/loss': 0.044916713527446056, 'validation/mean_average_precision': 0.25609675340521787, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301684062709, 'test/loss': 0.04772775141742541, 'test/mean_average_precision': 0.24988323492725328, 'test/num_examples': 43793, 'score': 3124.459648132324, 'total_duration': 4122.434123277664, 'accumulated_submission_time': 3124.459648132324, 'accumulated_eval_time': 994.9024879932404, 'accumulated_logging_time': 0.30339550971984863}
I0607 12:50:21.291300 139681286661888 logging_writer.py:48] [10591] accumulated_eval_time=994.902488, accumulated_logging_time=0.303396, accumulated_submission_time=3124.459648, global_step=10591, preemption_count=0, score=3124.459648, test/accuracy=0.985830, test/loss=0.047728, test/mean_average_precision=0.249883, test/num_examples=43793, total_duration=4122.434123, train/accuracy=0.990484, train/loss=0.031843, train/mean_average_precision=0.370810, validation/accuracy=0.986667, validation/loss=0.044917, validation/mean_average_precision=0.256097, validation/num_examples=43793
I0607 12:52:23.307572 139681295054592 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.022960, loss=0.033043
I0607 12:52:23.313375 139733503477568 submission.py:296] 11000) loss = 0.033, grad_norm = 0.023
I0607 12:54:21.369704 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:55:22.082209 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:55:25.410785 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:55:28.700983 139733503477568 submission_runner.py:419] Time since start: 4429.85s, 	Step: 11394, 	{'train/accuracy': 0.9906182973788277, 'train/loss': 0.031342219437030965, 'train/mean_average_precision': 0.39362995947960544, 'validation/accuracy': 0.9867781055072351, 'validation/loss': 0.044866522765763285, 'validation/mean_average_precision': 0.2600111644741225, 'validation/num_examples': 43793, 'test/accuracy': 0.9858251140700506, 'test/loss': 0.04798938900024809, 'test/mean_average_precision': 0.25282005468097823, 'test/num_examples': 43793, 'score': 3364.313054561615, 'total_duration': 4429.854870796204, 'accumulated_submission_time': 3364.313054561615, 'accumulated_eval_time': 1062.233479499817, 'accumulated_logging_time': 0.3255183696746826}
I0607 12:55:28.712011 139681286661888 logging_writer.py:48] [11394] accumulated_eval_time=1062.233479, accumulated_logging_time=0.325518, accumulated_submission_time=3364.313055, global_step=11394, preemption_count=0, score=3364.313055, test/accuracy=0.985825, test/loss=0.047989, test/mean_average_precision=0.252820, test/num_examples=43793, total_duration=4429.854871, train/accuracy=0.990618, train/loss=0.031342, train/mean_average_precision=0.393630, validation/accuracy=0.986778, validation/loss=0.044867, validation/mean_average_precision=0.260011, validation/num_examples=43793
I0607 12:56:01.055937 139681295054592 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.009903, loss=0.033284
I0607 12:56:01.061216 139733503477568 submission.py:296] 11500) loss = 0.033, grad_norm = 0.010
I0607 12:58:28.772583 139733503477568 spec.py:298] Evaluating on the training split.
I0607 12:59:28.883818 139733503477568 spec.py:310] Evaluating on the validation split.
I0607 12:59:32.211908 139733503477568 spec.py:326] Evaluating on the test split.
I0607 12:59:35.478712 139733503477568 submission_runner.py:419] Time since start: 4676.63s, 	Step: 12000, 	{'train/accuracy': 0.9907005446815403, 'train/loss': 0.030828178246080615, 'train/mean_average_precision': 0.4142997589157889, 'validation/accuracy': 0.9867042243000973, 'validation/loss': 0.04492740435631201, 'validation/mean_average_precision': 0.2593250248067667, 'validation/num_examples': 43793, 'test/accuracy': 0.9858638639810732, 'test/loss': 0.04792074742892234, 'test/mean_average_precision': 0.2532031884079153, 'test/num_examples': 43793, 'score': 3544.2031922340393, 'total_duration': 4676.632670879364, 'accumulated_submission_time': 3544.2031922340393, 'accumulated_eval_time': 1128.9394226074219, 'accumulated_logging_time': 0.34755945205688477}
I0607 12:59:35.489620 139681286661888 logging_writer.py:48] [12000] accumulated_eval_time=1128.939423, accumulated_logging_time=0.347559, accumulated_submission_time=3544.203192, global_step=12000, preemption_count=0, score=3544.203192, test/accuracy=0.985864, test/loss=0.047921, test/mean_average_precision=0.253203, test/num_examples=43793, total_duration=4676.632671, train/accuracy=0.990701, train/loss=0.030828, train/mean_average_precision=0.414300, validation/accuracy=0.986704, validation/loss=0.044927, validation/mean_average_precision=0.259325, validation/num_examples=43793
I0607 12:59:35.510016 139681295054592 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3544.203192
I0607 12:59:35.601634 139733503477568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nadamw/ogbg_pytorch/trial_1/checkpoint_12000.
I0607 12:59:35.770554 139733503477568 submission_runner.py:581] Tuning trial 1/1
I0607 12:59:35.770775 139733503477568 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 12:59:35.772561 139733503477568 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.44477107552266776, 'train/loss': 0.7929020778204073, 'train/mean_average_precision': 0.021953884342576904, 'validation/accuracy': 0.4484065609759464, 'validation/loss': 0.7904324547152853, 'validation/mean_average_precision': 0.025981803290386633, 'validation/num_examples': 43793, 'test/accuracy': 0.4508013018285325, 'test/loss': 0.7888642653796081, 'test/mean_average_precision': 0.028103970856366876, 'test/num_examples': 43793, 'score': 5.417672872543335, 'total_duration': 153.10452890396118, 'accumulated_submission_time': 5.417672872543335, 'accumulated_eval_time': 147.68651366233826, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (813, {'train/accuracy': 0.9867493930933282, 'train/loss': 0.05322541966426859, 'train/mean_average_precision': 0.04918263127260883, 'validation/accuracy': 0.9842214909877105, 'validation/loss': 0.06241768283772033, 'validation/mean_average_precision': 0.0486176301186144, 'validation/num_examples': 43793, 'test/accuracy': 0.9832377151199204, 'test/loss': 0.06559185855945521, 'test/mean_average_precision': 0.05141372191930102, 'test/num_examples': 43793, 'score': 245.31859254837036, 'total_duration': 456.92142271995544, 'accumulated_submission_time': 245.31859254837036, 'accumulated_eval_time': 211.35571837425232, 'accumulated_logging_time': 0.028687477111816406, 'global_step': 813, 'preemption_count': 0}), (1623, {'train/accuracy': 0.9872332799501433, 'train/loss': 0.04791965270893952, 'train/mean_average_precision': 0.10138192203982103, 'validation/accuracy': 0.9844808870940898, 'validation/loss': 0.05738474125745815, 'validation/mean_average_precision': 0.1016398966430054, 'validation/num_examples': 43793, 'test/accuracy': 0.9834466276836946, 'test/loss': 0.06059150765163324, 'test/mean_average_precision': 0.10035739237710407, 'test/num_examples': 43793, 'score': 485.233585357666, 'total_duration': 760.3078620433807, 'accumulated_submission_time': 485.233585357666, 'accumulated_eval_time': 274.5917761325836, 'accumulated_logging_time': 0.051976919174194336, 'global_step': 1623, 'preemption_count': 0}), (2441, {'train/accuracy': 0.9879263319628766, 'train/loss': 0.04322425189204725, 'train/mean_average_precision': 0.15634587801903174, 'validation/accuracy': 0.9850962931931052, 'validation/loss': 0.05192526383100851, 'validation/mean_average_precision': 0.14340183208574017, 'validation/num_examples': 43793, 'test/accuracy': 0.9841196967903701, 'test/loss': 0.054466000954427156, 'test/mean_average_precision': 0.14838453014016115, 'test/num_examples': 43793, 'score': 725.2487759590149, 'total_duration': 1064.6234338283539, 'accumulated_submission_time': 725.2487759590149, 'accumulated_eval_time': 338.6585023403168, 'accumulated_logging_time': 0.07243609428405762, 'global_step': 2441, 'preemption_count': 0}), (3257, {'train/accuracy': 0.988314631749966, 'train/loss': 0.04062472718869871, 'train/mean_average_precision': 0.18688978162589243, 'validation/accuracy': 0.9853824813855893, 'validation/loss': 0.05030036567138126, 'validation/mean_average_precision': 0.1663081485155946, 'validation/num_examples': 43793, 'test/accuracy': 0.9844945600600455, 'test/loss': 0.05307111603745095, 'test/mean_average_precision': 0.17156174086658935, 'test/num_examples': 43793, 'score': 965.2607612609863, 'total_duration': 1369.463802099228, 'accumulated_submission_time': 965.2607612609863, 'accumulated_eval_time': 403.25494408607483, 'accumulated_logging_time': 0.09304094314575195, 'global_step': 3257, 'preemption_count': 0}), (4073, {'train/accuracy': 0.9885471225437423, 'train/loss': 0.03954627428266029, 'train/mean_average_precision': 0.21038666520087274, 'validation/accuracy': 0.9855667784627351, 'validation/loss': 0.04901214775003308, 'validation/mean_average_precision': 0.185379927423137, 'validation/num_examples': 43793, 'test/accuracy': 0.9846179701027589, 'test/loss': 0.05162437810604756, 'test/mean_average_precision': 0.1895384285288494, 'test/num_examples': 43793, 'score': 1205.2896554470062, 'total_duration': 1673.0155284404755, 'accumulated_submission_time': 1205.2896554470062, 'accumulated_eval_time': 466.538831949234, 'accumulated_logging_time': 0.11581254005432129, 'global_step': 4073, 'preemption_count': 0}), (4880, {'train/accuracy': 0.9888096199772624, 'train/loss': 0.03813204482020539, 'train/mean_average_precision': 0.239242876368897, 'validation/accuracy': 0.985903709242539, 'validation/loss': 0.0479558107670899, 'validation/mean_average_precision': 0.2098113949236059, 'validation/num_examples': 43793, 'test/accuracy': 0.9849679828860176, 'test/loss': 0.050703185842888486, 'test/mean_average_precision': 0.21055854572513355, 'test/num_examples': 43793, 'score': 1445.0920367240906, 'total_duration': 1978.1036145687103, 'accumulated_submission_time': 1445.0920367240906, 'accumulated_eval_time': 531.5877513885498, 'accumulated_logging_time': 0.1385793685913086, 'global_step': 4880, 'preemption_count': 0}), (5697, {'train/accuracy': 0.9890326782467711, 'train/loss': 0.037578720205301556, 'train/mean_average_precision': 0.2630152224190091, 'validation/accuracy': 0.9861192637534738, 'validation/loss': 0.04713733600503204, 'validation/mean_average_precision': 0.21642623130511981, 'validation/num_examples': 43793, 'test/accuracy': 0.9851465694324697, 'test/loss': 0.049879055920544146, 'test/mean_average_precision': 0.21117851361082635, 'test/num_examples': 43793, 'score': 1684.8837049007416, 'total_duration': 2284.6502628326416, 'accumulated_submission_time': 1684.8837049007416, 'accumulated_eval_time': 598.1038835048676, 'accumulated_logging_time': 0.16190862655639648, 'global_step': 5697, 'preemption_count': 0}), (6508, {'train/accuracy': 0.989510108065382, 'train/loss': 0.03576481837540777, 'train/mean_average_precision': 0.29053396778689905, 'validation/accuracy': 0.986102620184833, 'validation/loss': 0.04651261229842, 'validation/mean_average_precision': 0.22704528828996823, 'validation/num_examples': 43793, 'test/accuracy': 0.985268715891128, 'test/loss': 0.049126163839888735, 'test/mean_average_precision': 0.22799319414026498, 'test/num_examples': 43793, 'score': 1924.7372615337372, 'total_duration': 2590.9776751995087, 'accumulated_submission_time': 1924.7372615337372, 'accumulated_eval_time': 664.3446640968323, 'accumulated_logging_time': 0.18303608894348145, 'global_step': 6508, 'preemption_count': 0}), (7323, {'train/accuracy': 0.9895325093570669, 'train/loss': 0.035677623487825445, 'train/mean_average_precision': 0.2853906852061793, 'validation/accuracy': 0.9863798776819487, 'validation/loss': 0.04620286051694924, 'validation/mean_average_precision': 0.22921092714955893, 'validation/num_examples': 43793, 'test/accuracy': 0.9854894219060828, 'test/loss': 0.048817842749280915, 'test/mean_average_precision': 0.2304124624687853, 'test/num_examples': 43793, 'score': 2164.55100941658, 'total_duration': 2896.512417078018, 'accumulated_submission_time': 2164.55100941658, 'accumulated_eval_time': 729.8292510509491, 'accumulated_logging_time': 0.20536541938781738, 'global_step': 7323, 'preemption_count': 0}), (8139, {'train/accuracy': 0.9897939017337514, 'train/loss': 0.03441838044501415, 'train/mean_average_precision': 0.32015491452784983, 'validation/accuracy': 0.9864294024471729, 'validation/loss': 0.04567599706139528, 'validation/mean_average_precision': 0.2312404282729669, 'validation/num_examples': 43793, 'test/accuracy': 0.9854607806675009, 'test/loss': 0.0485900455269335, 'test/mean_average_precision': 0.22641564436727493, 'test/num_examples': 43793, 'score': 2404.3951449394226, 'total_duration': 3203.2067246437073, 'accumulated_submission_time': 2404.3951449394226, 'accumulated_eval_time': 796.4461789131165, 'accumulated_logging_time': 0.22836637496948242, 'global_step': 8139, 'preemption_count': 0}), (8960, {'train/accuracy': 0.9898971536345628, 'train/loss': 0.03382295020249023, 'train/mean_average_precision': 0.333422563556964, 'validation/accuracy': 0.986505719298502, 'validation/loss': 0.0452257834553997, 'validation/mean_average_precision': 0.23999795240400223, 'validation/num_examples': 43793, 'test/accuracy': 0.9856364188511578, 'test/loss': 0.04797617401700532, 'test/mean_average_precision': 0.2431258285630593, 'test/num_examples': 43793, 'score': 2644.444247484207, 'total_duration': 3508.7015001773834, 'accumulated_submission_time': 2644.444247484207, 'accumulated_eval_time': 861.6542859077454, 'accumulated_logging_time': 0.252488374710083, 'global_step': 8960, 'preemption_count': 0}), (9773, {'train/accuracy': 0.9901306386549167, 'train/loss': 0.03318173943386965, 'train/mean_average_precision': 0.3635692951333488, 'validation/accuracy': 0.9866636302302414, 'validation/loss': 0.04495200499692703, 'validation/mean_average_precision': 0.24990092234032926, 'validation/num_examples': 43793, 'test/accuracy': 0.9857455082745802, 'test/loss': 0.047524367033892276, 'test/mean_average_precision': 0.244853141440761, 'test/num_examples': 43793, 'score': 2884.4118287563324, 'total_duration': 3814.5008177757263, 'accumulated_submission_time': 2884.4118287563324, 'accumulated_eval_time': 927.2547194957733, 'accumulated_logging_time': 0.2777531147003174, 'global_step': 9773, 'preemption_count': 0}), (10591, {'train/accuracy': 0.9904838850055864, 'train/loss': 0.03184294806789017, 'train/mean_average_precision': 0.37081049942645633, 'validation/accuracy': 0.98666687775583, 'validation/loss': 0.044916713527446056, 'validation/mean_average_precision': 0.25609675340521787, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301684062709, 'test/loss': 0.04772775141742541, 'test/mean_average_precision': 0.24988323492725328, 'test/num_examples': 43793, 'score': 3124.459648132324, 'total_duration': 4122.434123277664, 'accumulated_submission_time': 3124.459648132324, 'accumulated_eval_time': 994.9024879932404, 'accumulated_logging_time': 0.30339550971984863, 'global_step': 10591, 'preemption_count': 0}), (11394, {'train/accuracy': 0.9906182973788277, 'train/loss': 0.031342219437030965, 'train/mean_average_precision': 0.39362995947960544, 'validation/accuracy': 0.9867781055072351, 'validation/loss': 0.044866522765763285, 'validation/mean_average_precision': 0.2600111644741225, 'validation/num_examples': 43793, 'test/accuracy': 0.9858251140700506, 'test/loss': 0.04798938900024809, 'test/mean_average_precision': 0.25282005468097823, 'test/num_examples': 43793, 'score': 3364.313054561615, 'total_duration': 4429.854870796204, 'accumulated_submission_time': 3364.313054561615, 'accumulated_eval_time': 1062.233479499817, 'accumulated_logging_time': 0.3255183696746826, 'global_step': 11394, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9907005446815403, 'train/loss': 0.030828178246080615, 'train/mean_average_precision': 0.4142997589157889, 'validation/accuracy': 0.9867042243000973, 'validation/loss': 0.04492740435631201, 'validation/mean_average_precision': 0.2593250248067667, 'validation/num_examples': 43793, 'test/accuracy': 0.9858638639810732, 'test/loss': 0.04792074742892234, 'test/mean_average_precision': 0.2532031884079153, 'test/num_examples': 43793, 'score': 3544.2031922340393, 'total_duration': 4676.632670879364, 'accumulated_submission_time': 3544.2031922340393, 'accumulated_eval_time': 1128.9394226074219, 'accumulated_logging_time': 0.34755945205688477, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0607 12:59:35.772685 139733503477568 submission_runner.py:584] Timing: 3544.2031922340393
I0607 12:59:35.772804 139733503477568 submission_runner.py:586] Total number of evals: 16
I0607 12:59:35.772879 139733503477568 submission_runner.py:587] ====================
I0607 12:59:35.773032 139733503477568 submission_runner.py:655] Final ogbg score: 3544.2031922340393
