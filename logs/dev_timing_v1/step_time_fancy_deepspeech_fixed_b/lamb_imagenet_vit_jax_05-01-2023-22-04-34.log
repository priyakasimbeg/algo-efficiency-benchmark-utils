python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_05-01-2023-22-04-34.log
I0501 22:04:56.814450 139985718949696 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax.
I0501 22:04:56.888446 139985718949696 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 22:04:57.813864 139985718949696 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0501 22:04:57.814551 139985718949696 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 22:04:57.818497 139985718949696 submission_runner.py:538] Using RNG seed 2330778721
I0501 22:05:00.525130 139985718949696 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 22:05:00.525345 139985718949696 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1.
I0501 22:05:00.525605 139985718949696 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1/hparams.json.
I0501 22:05:00.647859 139985718949696 submission_runner.py:241] Initializing dataset.
I0501 22:05:00.660253 139985718949696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:05:00.668487 139985718949696 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 22:05:00.668597 139985718949696 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 22:05:00.927179 139985718949696 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:05:07.643299 139985718949696 submission_runner.py:248] Initializing model.
I0501 22:05:18.798953 139985718949696 submission_runner.py:258] Initializing optimizer.
I0501 22:05:19.449318 139985718949696 submission_runner.py:265] Initializing metrics bundle.
I0501 22:05:19.449506 139985718949696 submission_runner.py:282] Initializing checkpoint and logger.
I0501 22:05:19.450499 139985718949696 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0501 22:05:20.334069 139985718949696 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1/meta_data_0.json.
I0501 22:05:20.335111 139985718949696 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1/flags_0.json.
I0501 22:05:20.340442 139985718949696 submission_runner.py:318] Starting training loop.
I0501 22:06:31.103449 139807120406272 logging_writer.py:48] [0] global_step=0, grad_norm=0.3046501874923706, loss=6.907756805419922
I0501 22:06:31.119418 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:06:31.125844 139985718949696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:06:31.131892 139985718949696 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 22:06:31.132020 139985718949696 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 22:06:31.191916 139985718949696 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:06:51.197196 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:06:51.206871 139985718949696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:06:51.235537 139985718949696 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 22:06:51.235867 139985718949696 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 22:06:51.298522 139985718949696 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 22:07:11.575049 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:07:11.581594 139985718949696 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 22:07:11.586469 139985718949696 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0501 22:07:11.617969 139985718949696 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 22:07:22.623643 139985718949696 submission_runner.py:415] Time since start: 122.28s, 	Step: 1, 	{'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 70.77880144119263, 'total_duration': 122.28314208984375, 'accumulated_submission_time': 70.77880144119263, 'accumulated_eval_time': 51.5041766166687, 'accumulated_logging_time': 0}
I0501 22:07:22.639417 139758068020992 logging_writer.py:48] [1] accumulated_eval_time=51.504177, accumulated_logging_time=0, accumulated_submission_time=70.778801, global_step=1, preemption_count=0, score=70.778801, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=122.283142, train/accuracy=0.000918, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0501 22:08:43.483828 139803487885056 logging_writer.py:48] [100] global_step=100, grad_norm=0.31559208035469055, loss=6.907756328582764
I0501 22:09:27.470095 139803496277760 logging_writer.py:48] [200] global_step=200, grad_norm=0.3597773611545563, loss=6.907756328582764
I0501 22:10:11.743162 139803487885056 logging_writer.py:48] [300] global_step=300, grad_norm=0.37109872698783875, loss=6.907756328582764
I0501 22:10:55.632684 139803496277760 logging_writer.py:48] [400] global_step=400, grad_norm=0.3890971541404724, loss=6.9077558517456055
I0501 22:11:39.778195 139803487885056 logging_writer.py:48] [500] global_step=500, grad_norm=0.4165504574775696, loss=6.907756328582764
I0501 22:12:23.978373 139803496277760 logging_writer.py:48] [600] global_step=600, grad_norm=0.4199768900871277, loss=6.907755374908447
I0501 22:13:08.187070 139803487885056 logging_writer.py:48] [700] global_step=700, grad_norm=0.42748919129371643, loss=6.907756328582764
I0501 22:13:52.332835 139803496277760 logging_writer.py:48] [800] global_step=800, grad_norm=0.44788479804992676, loss=6.9077558517456055
I0501 22:14:22.934865 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:14:29.441266 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:14:36.408252 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:14:38.231625 139985718949696 submission_runner.py:415] Time since start: 557.89s, 	Step: 870, 	{'train/accuracy': 0.002070312388241291, 'train/loss': 6.907751083374023, 'validation/accuracy': 0.00203999993391335, 'validation/loss': 6.907751560211182, 'validation/num_examples': 50000, 'test/accuracy': 0.002199999988079071, 'test/loss': 6.9077534675598145, 'test/num_examples': 10000, 'score': 491.0510485172272, 'total_duration': 557.8910610675812, 'accumulated_submission_time': 491.0510485172272, 'accumulated_eval_time': 66.80093789100647, 'accumulated_logging_time': 0.02406930923461914}
I0501 22:14:38.246110 139758302918400 logging_writer.py:48] [870] accumulated_eval_time=66.800938, accumulated_logging_time=0.024069, accumulated_submission_time=491.051049, global_step=870, preemption_count=0, score=491.051049, test/accuracy=0.002200, test/loss=6.907753, test/num_examples=10000, total_duration=557.891061, train/accuracy=0.002070, train/loss=6.907751, validation/accuracy=0.002040, validation/loss=6.907752, validation/num_examples=50000
I0501 22:14:52.407863 139758386779904 logging_writer.py:48] [900] global_step=900, grad_norm=0.4741179049015045, loss=6.907756328582764
I0501 22:15:36.563341 139758302918400 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4700469374656677, loss=6.9077558517456055
I0501 22:16:20.717284 139758386779904 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.46772605180740356, loss=6.9077558517456055
I0501 22:17:05.287760 139758302918400 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.49385735392570496, loss=6.907754421234131
I0501 22:17:49.276625 139758386779904 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.49096956849098206, loss=6.907754421234131
I0501 22:18:33.379929 139758302918400 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.43582025170326233, loss=6.907754421234131
I0501 22:19:17.519981 139758386779904 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5205501317977905, loss=6.9077534675598145
I0501 22:20:01.522144 139758302918400 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5049485564231873, loss=6.907753944396973
I0501 22:20:45.505611 139758386779904 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5278018712997437, loss=6.907753944396973
I0501 22:21:29.545195 139758302918400 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5089797973632812, loss=6.90775203704834
I0501 22:21:38.370585 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:21:44.861214 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:21:51.709916 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:21:53.433775 139985718949696 submission_runner.py:415] Time since start: 993.09s, 	Step: 1821, 	{'train/accuracy': 0.0032031249720603228, 'train/loss': 6.907746315002441, 'validation/accuracy': 0.002859999891370535, 'validation/loss': 6.907747268676758, 'validation/num_examples': 50000, 'test/accuracy': 0.0026000002399086952, 'test/loss': 6.907749652862549, 'test/num_examples': 10000, 'score': 911.1428608894348, 'total_duration': 993.0932540893555, 'accumulated_submission_time': 911.1428608894348, 'accumulated_eval_time': 81.86409640312195, 'accumulated_logging_time': 0.05510687828063965}
I0501 22:21:53.444711 139758386779904 logging_writer.py:48] [1821] accumulated_eval_time=81.864096, accumulated_logging_time=0.055107, accumulated_submission_time=911.142861, global_step=1821, preemption_count=0, score=911.142861, test/accuracy=0.002600, test/loss=6.907750, test/num_examples=10000, total_duration=993.093254, train/accuracy=0.003203, train/loss=6.907746, validation/accuracy=0.002860, validation/loss=6.907747, validation/num_examples=50000
I0501 22:22:29.241960 139758302918400 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5436565279960632, loss=6.907751560211182
I0501 22:23:13.313680 139758386779904 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5463343858718872, loss=6.907750129699707
I0501 22:23:57.322281 139758302918400 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5343528985977173, loss=6.907749652862549
I0501 22:24:41.643510 139758386779904 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5598821640014648, loss=6.907749176025391
I0501 22:25:25.716168 139758302918400 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.561265766620636, loss=6.907749176025391
I0501 22:26:09.843393 139758386779904 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5192380547523499, loss=6.907748699188232
I0501 22:26:54.530456 139758302918400 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5320353507995605, loss=6.907749652862549
I0501 22:27:39.075050 139758386779904 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5640490055084229, loss=6.907747268676758
I0501 22:28:23.051939 139758302918400 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5657068490982056, loss=6.907745361328125
I0501 22:28:53.808377 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:29:00.334257 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:29:07.356529 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:29:09.085254 139985718949696 submission_runner.py:415] Time since start: 1428.74s, 	Step: 2771, 	{'train/accuracy': 0.0036328125279396772, 'train/loss': 6.907740116119385, 'validation/accuracy': 0.0031799999997019768, 'validation/loss': 6.907739639282227, 'validation/num_examples': 50000, 'test/accuracy': 0.002400000113993883, 'test/loss': 6.907741546630859, 'test/num_examples': 10000, 'score': 1331.477290391922, 'total_duration': 1428.7446813583374, 'accumulated_submission_time': 1331.477290391922, 'accumulated_eval_time': 97.14090657234192, 'accumulated_logging_time': 0.07891082763671875}
I0501 22:29:09.098905 139758386779904 logging_writer.py:48] [2771] accumulated_eval_time=97.140907, accumulated_logging_time=0.078911, accumulated_submission_time=1331.477290, global_step=2771, preemption_count=0, score=1331.477290, test/accuracy=0.002400, test/loss=6.907742, test/num_examples=10000, total_duration=1428.744681, train/accuracy=0.003633, train/loss=6.907740, validation/accuracy=0.003180, validation/loss=6.907740, validation/num_examples=50000
I0501 22:29:22.845470 139758302918400 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.452022910118103, loss=6.907746315002441
I0501 22:30:07.128882 139758386779904 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5835844874382019, loss=6.907742023468018
I0501 22:30:51.204057 139758302918400 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5795219540596008, loss=6.907741069793701
I0501 22:31:35.394613 139758386779904 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5761141180992126, loss=6.907740592956543
I0501 22:32:19.754006 139758302918400 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.585054874420166, loss=6.907736301422119
I0501 22:33:03.849514 139758386779904 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5628513693809509, loss=6.9077372550964355
I0501 22:33:47.936865 139758302918400 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5728985667228699, loss=6.907733917236328
I0501 22:34:32.179710 139758386779904 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.46309876441955566, loss=6.907741546630859
I0501 22:35:16.537094 139758302918400 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5182440280914307, loss=6.9077348709106445
I0501 22:36:01.047980 139758386779904 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5799105763435364, loss=6.9077301025390625
I0501 22:36:09.509546 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:36:16.021820 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:36:23.079762 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:36:24.822810 139985718949696 submission_runner.py:415] Time since start: 1864.48s, 	Step: 3720, 	{'train/accuracy': 0.0047851563431322575, 'train/loss': 6.907717704772949, 'validation/accuracy': 0.004299999680370092, 'validation/loss': 6.907718658447266, 'validation/num_examples': 50000, 'test/accuracy': 0.0034000002779066563, 'test/loss': 6.9077229499816895, 'test/num_examples': 10000, 'score': 1751.861798286438, 'total_duration': 1864.4822659492493, 'accumulated_submission_time': 1751.861798286438, 'accumulated_eval_time': 112.45414781570435, 'accumulated_logging_time': 0.10265111923217773}
I0501 22:36:24.837280 139758302918400 logging_writer.py:48] [3720] accumulated_eval_time=112.454148, accumulated_logging_time=0.102651, accumulated_submission_time=1751.861798, global_step=3720, preemption_count=0, score=1751.861798, test/accuracy=0.003400, test/loss=6.907723, test/num_examples=10000, total_duration=1864.482266, train/accuracy=0.004785, train/loss=6.907718, validation/accuracy=0.004300, validation/loss=6.907719, validation/num_examples=50000
I0501 22:37:01.073664 139758386779904 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.574672520160675, loss=6.907729148864746
I0501 22:37:45.186957 139758302918400 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5883174538612366, loss=6.9077253341674805
I0501 22:38:29.213696 139758386779904 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5857832431793213, loss=6.907724380493164
I0501 22:39:13.336456 139758302918400 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5016170740127563, loss=6.907724380493164
I0501 22:39:57.596919 139758386779904 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.46909067034721375, loss=6.907729148864746
I0501 22:40:41.769950 139758302918400 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.4811606705188751, loss=6.907726287841797
I0501 22:41:25.823184 139758386779904 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5821498036384583, loss=6.907712459564209
I0501 22:42:09.827429 139758302918400 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6066311001777649, loss=6.907703399658203
I0501 22:42:53.884556 139758386779904 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5903993844985962, loss=6.907699108123779
I0501 22:43:25.176890 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:43:31.680854 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:43:38.486013 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:43:40.222277 139985718949696 submission_runner.py:415] Time since start: 2299.88s, 	Step: 4672, 	{'train/accuracy': 0.005156249739229679, 'train/loss': 6.907676696777344, 'validation/accuracy': 0.00443999981507659, 'validation/loss': 6.907678604125977, 'validation/num_examples': 50000, 'test/accuracy': 0.004399999976158142, 'test/loss': 6.907683849334717, 'test/num_examples': 10000, 'score': 2172.172273874283, 'total_duration': 2299.881747484207, 'accumulated_submission_time': 2172.172273874283, 'accumulated_eval_time': 127.49950432777405, 'accumulated_logging_time': 0.13039588928222656}
I0501 22:43:40.233202 139758302918400 logging_writer.py:48] [4672] accumulated_eval_time=127.499504, accumulated_logging_time=0.130396, accumulated_submission_time=2172.172274, global_step=4672, preemption_count=0, score=2172.172274, test/accuracy=0.004400, test/loss=6.907684, test/num_examples=10000, total_duration=2299.881747, train/accuracy=0.005156, train/loss=6.907677, validation/accuracy=0.004440, validation/loss=6.907679, validation/num_examples=50000
I0501 22:43:53.533064 139758386779904 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5839576125144958, loss=6.9076948165893555
I0501 22:44:38.489960 139758302918400 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6030203700065613, loss=6.907684326171875
I0501 22:45:23.617905 139758386779904 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.4785453975200653, loss=6.907703876495361
I0501 22:46:08.499752 139758302918400 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5990419983863831, loss=6.907676696777344
I0501 22:46:53.105906 139758386779904 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5969861745834351, loss=6.907661437988281
I0501 22:47:37.620355 139758302918400 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4668879806995392, loss=6.907690048217773
I0501 22:48:22.339817 139758386779904 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5449246168136597, loss=6.907650470733643
I0501 22:49:06.912803 139758302918400 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5900672674179077, loss=6.9076385498046875
I0501 22:49:51.502146 139758386779904 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5985373258590698, loss=6.907622814178467
I0501 22:50:36.159670 139758302918400 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.46527278423309326, loss=6.907666206359863
I0501 22:50:40.618336 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:50:47.135102 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:50:54.242328 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:50:55.983743 139985718949696 submission_runner.py:415] Time since start: 2735.64s, 	Step: 5611, 	{'train/accuracy': 0.005546874832361937, 'train/loss': 6.907561302185059, 'validation/accuracy': 0.005799999926239252, 'validation/loss': 6.907567501068115, 'validation/num_examples': 50000, 'test/accuracy': 0.00570000009611249, 'test/loss': 6.9075775146484375, 'test/num_examples': 10000, 'score': 2592.5282533168793, 'total_duration': 2735.6432070732117, 'accumulated_submission_time': 2592.5282533168793, 'accumulated_eval_time': 142.86485958099365, 'accumulated_logging_time': 0.1527261734008789}
I0501 22:50:55.997210 139758386779904 logging_writer.py:48] [5611] accumulated_eval_time=142.864860, accumulated_logging_time=0.152726, accumulated_submission_time=2592.528253, global_step=5611, preemption_count=0, score=2592.528253, test/accuracy=0.005700, test/loss=6.907578, test/num_examples=10000, total_duration=2735.643207, train/accuracy=0.005547, train/loss=6.907561, validation/accuracy=0.005800, validation/loss=6.907568, validation/num_examples=50000
I0501 22:51:36.620023 139758302918400 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6076682209968567, loss=6.907606601715088
I0501 22:52:21.281338 139758386779904 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5951766967773438, loss=6.907584190368652
I0501 22:53:06.026211 139758302918400 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5979047417640686, loss=6.907559871673584
I0501 22:53:50.739145 139758386779904 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6088320016860962, loss=6.907531261444092
I0501 22:54:35.556309 139758302918400 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6035650968551636, loss=6.907508850097656
I0501 22:55:20.346199 139758386779904 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6110878586769104, loss=6.907495021820068
I0501 22:56:05.605841 139758302918400 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6131564378738403, loss=6.907443046569824
I0501 22:56:50.232032 139758386779904 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5922581553459167, loss=6.907427787780762
I0501 22:57:34.950282 139758302918400 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6192973256111145, loss=6.907355308532715
I0501 22:57:56.396272 139985718949696 spec.py:298] Evaluating on the training split.
I0501 22:58:02.946397 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 22:58:09.980008 139985718949696 spec.py:326] Evaluating on the test split.
I0501 22:58:11.728425 139985718949696 submission_runner.py:415] Time since start: 3171.39s, 	Step: 6549, 	{'train/accuracy': 0.0044140624813735485, 'train/loss': 6.907231330871582, 'validation/accuracy': 0.0041600000113248825, 'validation/loss': 6.9072442054748535, 'validation/num_examples': 50000, 'test/accuracy': 0.005100000184029341, 'test/loss': 6.907261848449707, 'test/num_examples': 10000, 'score': 3012.8977262973785, 'total_duration': 3171.387901544571, 'accumulated_submission_time': 3012.8977262973785, 'accumulated_eval_time': 158.19697952270508, 'accumulated_logging_time': 0.17829203605651855}
I0501 22:58:11.741756 139758386779904 logging_writer.py:48] [6549] accumulated_eval_time=158.196980, accumulated_logging_time=0.178292, accumulated_submission_time=3012.897726, global_step=6549, preemption_count=0, score=3012.897726, test/accuracy=0.005100, test/loss=6.907262, test/num_examples=10000, total_duration=3171.387902, train/accuracy=0.004414, train/loss=6.907231, validation/accuracy=0.004160, validation/loss=6.907244, validation/num_examples=50000
I0501 22:58:35.446717 139758302918400 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4844450354576111, loss=6.9074506759643555
I0501 22:59:20.553997 139758386779904 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.45441457629203796, loss=6.907506942749023
I0501 23:00:05.712887 139758302918400 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5411442518234253, loss=6.907292366027832
I0501 23:00:50.744190 139758386779904 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.46868008375167847, loss=6.907400131225586
I0501 23:01:35.932415 139758302918400 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6171631217002869, loss=6.9070024490356445
I0501 23:02:21.329114 139758386779904 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6164249777793884, loss=6.906900405883789
I0501 23:03:06.755830 139758302918400 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6117315888404846, loss=6.906917095184326
I0501 23:03:52.118500 139758386779904 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6098306775093079, loss=6.9068603515625
I0501 23:04:37.265066 139758302918400 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5506601929664612, loss=6.906787872314453
I0501 23:05:12.127515 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:05:18.655501 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:05:25.620299 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:05:27.363077 139985718949696 submission_runner.py:415] Time since start: 3607.02s, 	Step: 7478, 	{'train/accuracy': 0.004062499850988388, 'train/loss': 6.906137466430664, 'validation/accuracy': 0.003399999812245369, 'validation/loss': 6.906174182891846, 'validation/num_examples': 50000, 'test/accuracy': 0.0034000002779066563, 'test/loss': 6.906256675720215, 'test/num_examples': 10000, 'score': 3433.2519192695618, 'total_duration': 3607.0225265026093, 'accumulated_submission_time': 3433.2519192695618, 'accumulated_eval_time': 173.43251752853394, 'accumulated_logging_time': 0.20505642890930176}
I0501 23:05:27.378672 139758386779904 logging_writer.py:48] [7478] accumulated_eval_time=173.432518, accumulated_logging_time=0.205056, accumulated_submission_time=3433.251919, global_step=7478, preemption_count=0, score=3433.251919, test/accuracy=0.003400, test/loss=6.906257, test/num_examples=10000, total_duration=3607.022527, train/accuracy=0.004062, train/loss=6.906137, validation/accuracy=0.003400, validation/loss=6.906174, validation/num_examples=50000
I0501 23:05:38.115938 139758302918400 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6228543519973755, loss=6.906364440917969
I0501 23:06:23.572261 139758386779904 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5801999568939209, loss=6.906415939331055
I0501 23:07:08.858057 139758302918400 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6255035400390625, loss=6.906105995178223
I0501 23:07:54.092902 139758386779904 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6333412528038025, loss=6.905643463134766
I0501 23:08:39.488353 139758302918400 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6035024523735046, loss=6.905638217926025
I0501 23:09:25.142737 139758386779904 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4933096170425415, loss=6.906079292297363
I0501 23:10:10.531757 139758302918400 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5302833318710327, loss=6.905468463897705
I0501 23:10:55.695362 139758386779904 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4862659275531769, loss=6.905411720275879
I0501 23:11:41.620212 139758302918400 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.47763410210609436, loss=6.905506134033203
I0501 23:12:27.163698 139758386779904 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.48570215702056885, loss=6.904938220977783
I0501 23:12:27.600317 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:12:34.143804 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:12:41.097633 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:12:42.824600 139985718949696 submission_runner.py:415] Time since start: 4042.48s, 	Step: 8402, 	{'train/accuracy': 0.0027343749534338713, 'train/loss': 6.902018070220947, 'validation/accuracy': 0.0025400000158697367, 'validation/loss': 6.902092933654785, 'validation/num_examples': 50000, 'test/accuracy': 0.0027000000700354576, 'test/loss': 6.90219259262085, 'test/num_examples': 10000, 'score': 3853.4424431324005, 'total_duration': 4042.4840536117554, 'accumulated_submission_time': 3853.4424431324005, 'accumulated_eval_time': 188.6567726135254, 'accumulated_logging_time': 0.23351550102233887}
I0501 23:12:42.841220 139758302918400 logging_writer.py:48] [8402] accumulated_eval_time=188.656773, accumulated_logging_time=0.233516, accumulated_submission_time=3853.442443, global_step=8402, preemption_count=0, score=3853.442443, test/accuracy=0.002700, test/loss=6.902193, test/num_examples=10000, total_duration=4042.484054, train/accuracy=0.002734, train/loss=6.902018, validation/accuracy=0.002540, validation/loss=6.902093, validation/num_examples=50000
I0501 23:13:28.209820 139758386779904 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5916590094566345, loss=6.902863502502441
I0501 23:14:13.739606 139758302918400 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6107622981071472, loss=6.901617527008057
I0501 23:14:59.194945 139758386779904 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4956035614013672, loss=6.903565883636475
I0501 23:15:44.927481 139758302918400 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6244974136352539, loss=6.899170398712158
I0501 23:16:30.771011 139758386779904 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5762251615524292, loss=6.899116516113281
I0501 23:17:16.605407 139758302918400 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5601016879081726, loss=6.898044109344482
I0501 23:18:02.634373 139758386779904 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6033373475074768, loss=6.894527912139893
I0501 23:18:48.300620 139758302918400 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6225118637084961, loss=6.89231014251709
I0501 23:19:34.363729 139758386779904 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4872463047504425, loss=6.896875381469727
I0501 23:19:43.040769 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:19:49.562950 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:19:56.429397 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:19:58.174788 139985718949696 submission_runner.py:415] Time since start: 4477.83s, 	Step: 9320, 	{'train/accuracy': 0.0023242186289280653, 'train/loss': 6.884580135345459, 'validation/accuracy': 0.002099999925121665, 'validation/loss': 6.884843349456787, 'validation/num_examples': 50000, 'test/accuracy': 0.002199999988079071, 'test/loss': 6.885444641113281, 'test/num_examples': 10000, 'score': 4273.608917713165, 'total_duration': 4477.834264278412, 'accumulated_submission_time': 4273.608917713165, 'accumulated_eval_time': 203.79075837135315, 'accumulated_logging_time': 0.2649216651916504}
I0501 23:19:58.186858 139758302918400 logging_writer.py:48] [9320] accumulated_eval_time=203.790758, accumulated_logging_time=0.264922, accumulated_submission_time=4273.608918, global_step=9320, preemption_count=0, score=4273.608918, test/accuracy=0.002200, test/loss=6.885445, test/num_examples=10000, total_duration=4477.834264, train/accuracy=0.002324, train/loss=6.884580, validation/accuracy=0.002100, validation/loss=6.884843, validation/num_examples=50000
I0501 23:20:35.521122 139758386779904 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.482697457075119, loss=6.897486209869385
I0501 23:21:21.236181 139758302918400 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6128275990486145, loss=6.8862996101379395
I0501 23:22:06.979013 139758386779904 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5128507614135742, loss=6.888528347015381
I0501 23:22:52.708852 139758302918400 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5092619061470032, loss=6.888089179992676
I0501 23:23:38.483423 139758386779904 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4992930293083191, loss=6.882781028747559
I0501 23:24:24.363317 139758302918400 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5626010298728943, loss=6.870474815368652
I0501 23:25:10.300498 139758386779904 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5512858033180237, loss=6.865833759307861
I0501 23:25:55.959801 139758302918400 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.598204493522644, loss=6.851181507110596
I0501 23:26:41.610087 139758386779904 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.58887779712677, loss=6.844275951385498
I0501 23:26:58.589628 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:27:05.312568 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:27:12.236963 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:27:13.975283 139985718949696 submission_runner.py:415] Time since start: 4913.63s, 	Step: 10238, 	{'train/accuracy': 0.0022656249348074198, 'train/loss': 6.823245048522949, 'validation/accuracy': 0.00215999991632998, 'validation/loss': 6.824596881866455, 'validation/num_examples': 50000, 'test/accuracy': 0.0019000000320374966, 'test/loss': 6.825493335723877, 'test/num_examples': 10000, 'score': 4693.982902050018, 'total_duration': 4913.63467669487, 'accumulated_submission_time': 4693.982902050018, 'accumulated_eval_time': 219.17631793022156, 'accumulated_logging_time': 0.2870972156524658}
I0501 23:27:13.994457 139758302918400 logging_writer.py:48] [10238] accumulated_eval_time=219.176318, accumulated_logging_time=0.287097, accumulated_submission_time=4693.982902, global_step=10238, preemption_count=0, score=4693.982902, test/accuracy=0.001900, test/loss=6.825493, test/num_examples=10000, total_duration=4913.634677, train/accuracy=0.002266, train/loss=6.823245, validation/accuracy=0.002160, validation/loss=6.824597, validation/num_examples=50000
I0501 23:27:43.890255 139758386779904 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.47460174560546875, loss=6.859787940979004
I0501 23:28:29.711919 139758302918400 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5471071004867554, loss=6.8408660888671875
I0501 23:29:15.669316 139758386779904 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5969439148902893, loss=6.824009895324707
I0501 23:30:01.301409 139758302918400 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5981343984603882, loss=6.801311016082764
I0501 23:30:47.088299 139758386779904 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5001455545425415, loss=6.820263862609863
I0501 23:31:32.992630 139758302918400 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5778264403343201, loss=6.774361610412598
I0501 23:32:19.084597 139758386779904 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5923910737037659, loss=6.774960041046143
I0501 23:33:05.066950 139758302918400 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5213175415992737, loss=6.802567005157471
I0501 23:33:50.832687 139758386779904 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6153690218925476, loss=6.766698360443115
I0501 23:34:14.338771 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:34:21.068677 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:34:28.020933 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:34:29.762556 139985718949696 submission_runner.py:415] Time since start: 5349.42s, 	Step: 11152, 	{'train/accuracy': 0.00341796875, 'train/loss': 6.678938865661621, 'validation/accuracy': 0.0035800000187009573, 'validation/loss': 6.683973789215088, 'validation/num_examples': 50000, 'test/accuracy': 0.0035000001080334187, 'test/loss': 6.696055889129639, 'test/num_examples': 10000, 'score': 5114.257411003113, 'total_duration': 5349.4219622612, 'accumulated_submission_time': 5114.257411003113, 'accumulated_eval_time': 234.60003757476807, 'accumulated_logging_time': 0.35730838775634766}
I0501 23:34:29.781398 139758302918400 logging_writer.py:48] [11152] accumulated_eval_time=234.600038, accumulated_logging_time=0.357308, accumulated_submission_time=5114.257411, global_step=11152, preemption_count=0, score=5114.257411, test/accuracy=0.003500, test/loss=6.696056, test/num_examples=10000, total_duration=5349.421962, train/accuracy=0.003418, train/loss=6.678939, validation/accuracy=0.003580, validation/loss=6.683974, validation/num_examples=50000
I0501 23:34:53.058336 139758386779904 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5942953824996948, loss=6.741551876068115
I0501 23:35:38.706540 139758302918400 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6102376580238342, loss=6.717689514160156
I0501 23:36:24.938771 139758386779904 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5461023449897766, loss=6.832773685455322
I0501 23:37:10.738358 139758302918400 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5027785897254944, loss=6.762839317321777
I0501 23:37:56.542619 139758386779904 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5188860893249512, loss=6.7981367111206055
I0501 23:38:42.125199 139758302918400 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5270879864692688, loss=6.725924968719482
I0501 23:39:28.026165 139758386779904 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.635388195514679, loss=6.6660332679748535
I0501 23:40:13.646281 139758302918400 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6375299096107483, loss=6.666351318359375
I0501 23:40:59.168468 139758386779904 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5830684304237366, loss=6.612127780914307
I0501 23:41:30.144920 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:41:36.912139 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:41:43.850593 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:41:45.607141 139985718949696 submission_runner.py:415] Time since start: 5785.27s, 	Step: 12069, 	{'train/accuracy': 0.006269531324505806, 'train/loss': 6.516449928283691, 'validation/accuracy': 0.0050999997183680534, 'validation/loss': 6.528395652770996, 'validation/num_examples': 50000, 'test/accuracy': 0.0052000004798173904, 'test/loss': 6.551764965057373, 'test/num_examples': 10000, 'score': 5534.556992530823, 'total_duration': 5785.266585350037, 'accumulated_submission_time': 5534.556992530823, 'accumulated_eval_time': 250.06222891807556, 'accumulated_logging_time': 0.421358585357666}
I0501 23:41:45.623440 139758302918400 logging_writer.py:48] [12069] accumulated_eval_time=250.062229, accumulated_logging_time=0.421359, accumulated_submission_time=5534.556993, global_step=12069, preemption_count=0, score=5534.556993, test/accuracy=0.005200, test/loss=6.551765, test/num_examples=10000, total_duration=5785.266585, train/accuracy=0.006270, train/loss=6.516450, validation/accuracy=0.005100, validation/loss=6.528396, validation/num_examples=50000
I0501 23:42:00.904630 139758386779904 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6594022512435913, loss=6.645814895629883
I0501 23:42:46.874722 139758302918400 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8963364958763123, loss=6.614465713500977
I0501 23:43:32.647926 139758386779904 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0426359176635742, loss=6.708131790161133
I0501 23:44:18.987983 139758302918400 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8244582414627075, loss=6.664201736450195
I0501 23:45:04.753441 139758386779904 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.60493403673172, loss=6.59822416305542
I0501 23:45:50.308422 139758302918400 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5876622200012207, loss=6.607749938964844
I0501 23:46:36.039055 139758386779904 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5781495571136475, loss=6.616520881652832
I0501 23:47:21.937198 139758302918400 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4866775870323181, loss=6.78744649887085
I0501 23:48:07.470132 139758386779904 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7301765084266663, loss=6.56843376159668
I0501 23:48:45.675717 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:48:53.168745 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:49:00.430336 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:49:02.184233 139985718949696 submission_runner.py:415] Time since start: 6221.84s, 	Step: 12985, 	{'train/accuracy': 0.00830078125, 'train/loss': 6.399616718292236, 'validation/accuracy': 0.007400000002235174, 'validation/loss': 6.413402557373047, 'validation/num_examples': 50000, 'test/accuracy': 0.007300000172108412, 'test/loss': 6.45481014251709, 'test/num_examples': 10000, 'score': 5954.575271606445, 'total_duration': 6221.843695640564, 'accumulated_submission_time': 5954.575271606445, 'accumulated_eval_time': 266.5707266330719, 'accumulated_logging_time': 0.4531431198120117}
I0501 23:49:02.199473 139758302918400 logging_writer.py:48] [12985] accumulated_eval_time=266.570727, accumulated_logging_time=0.453143, accumulated_submission_time=5954.575272, global_step=12985, preemption_count=0, score=5954.575272, test/accuracy=0.007300, test/loss=6.454810, test/num_examples=10000, total_duration=6221.843696, train/accuracy=0.008301, train/loss=6.399617, validation/accuracy=0.007400, validation/loss=6.413403, validation/num_examples=50000
I0501 23:49:09.948383 139758386779904 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5782673954963684, loss=6.566432952880859
I0501 23:49:55.734571 139758302918400 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6362142562866211, loss=6.562215805053711
I0501 23:50:41.430848 139758386779904 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6240382194519043, loss=6.821329593658447
I0501 23:51:26.953524 139758302918400 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5094603300094604, loss=6.694060325622559
I0501 23:52:13.165291 139758386779904 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.592800498008728, loss=6.543664932250977
I0501 23:52:59.111780 139758302918400 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5164660811424255, loss=6.659818172454834
I0501 23:53:44.797883 139758386779904 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6249129176139832, loss=6.49363374710083
I0501 23:54:30.776093 139758302918400 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.49285322427749634, loss=6.738426685333252
I0501 23:55:16.554826 139758386779904 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6134940981864929, loss=6.451905727386475
I0501 23:56:02.226186 139758302918400 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6333205699920654, loss=6.448542594909668
I0501 23:56:02.241044 139985718949696 spec.py:298] Evaluating on the training split.
I0501 23:56:11.028467 139985718949696 spec.py:310] Evaluating on the validation split.
I0501 23:56:18.429614 139985718949696 spec.py:326] Evaluating on the test split.
I0501 23:56:20.176975 139985718949696 submission_runner.py:415] Time since start: 6659.83s, 	Step: 13901, 	{'train/accuracy': 0.014414061792194843, 'train/loss': 6.219668865203857, 'validation/accuracy': 0.013679999858140945, 'validation/loss': 6.23712682723999, 'validation/num_examples': 50000, 'test/accuracy': 0.010900000110268593, 'test/loss': 6.288051128387451, 'test/num_examples': 10000, 'score': 6374.585117816925, 'total_duration': 6659.829132080078, 'accumulated_submission_time': 6374.585117816925, 'accumulated_eval_time': 284.4993278980255, 'accumulated_logging_time': 0.481769323348999}
I0501 23:56:20.196176 139758386779904 logging_writer.py:48] [13901] accumulated_eval_time=284.499328, accumulated_logging_time=0.481769, accumulated_submission_time=6374.585118, global_step=13901, preemption_count=0, score=6374.585118, test/accuracy=0.010900, test/loss=6.288051, test/num_examples=10000, total_duration=6659.829132, train/accuracy=0.014414, train/loss=6.219669, validation/accuracy=0.013680, validation/loss=6.237127, validation/num_examples=50000
I0501 23:57:06.653388 139758302918400 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5914337038993835, loss=6.449104309082031
I0501 23:57:52.280400 139758386779904 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.621459424495697, loss=6.399740219116211
I0501 23:58:37.959825 139758302918400 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5890489816665649, loss=6.397287368774414
I0501 23:59:23.676417 139758386779904 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5842323303222656, loss=6.3849053382873535
I0502 00:00:09.884075 139758302918400 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5238525867462158, loss=6.646318435668945
I0502 00:00:55.598187 139758386779904 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5895183086395264, loss=6.353545188903809
I0502 00:01:41.325292 139758302918400 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.617483377456665, loss=6.329229831695557
I0502 00:02:26.985740 139758386779904 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6929138898849487, loss=6.288130760192871
I0502 00:03:12.615319 139758302918400 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6141412854194641, loss=6.263119220733643
I0502 00:03:20.457845 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:03:29.572617 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:03:37.075499 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:03:38.812257 139985718949696 submission_runner.py:415] Time since start: 7098.47s, 	Step: 14818, 	{'train/accuracy': 0.025136718526482582, 'train/loss': 6.009947299957275, 'validation/accuracy': 0.022779999300837517, 'validation/loss': 6.03596305847168, 'validation/num_examples': 50000, 'test/accuracy': 0.019500000402331352, 'test/loss': 6.1152262687683105, 'test/num_examples': 10000, 'score': 6794.808881282806, 'total_duration': 7098.471710920334, 'accumulated_submission_time': 6794.808881282806, 'accumulated_eval_time': 302.8536841869354, 'accumulated_logging_time': 0.5199065208435059}
I0502 00:03:38.826573 139758386779904 logging_writer.py:48] [14818] accumulated_eval_time=302.853684, accumulated_logging_time=0.519907, accumulated_submission_time=6794.808881, global_step=14818, preemption_count=0, score=6794.808881, test/accuracy=0.019500, test/loss=6.115226, test/num_examples=10000, total_duration=7098.471711, train/accuracy=0.025137, train/loss=6.009947, validation/accuracy=0.022780, validation/loss=6.035963, validation/num_examples=50000
I0502 00:04:17.581537 139758302918400 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.622523307800293, loss=6.309166431427002
I0502 00:05:03.512498 139758386779904 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5460312366485596, loss=6.663014888763428
I0502 00:05:49.250078 139758302918400 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7508686184883118, loss=6.2492594718933105
I0502 00:06:35.031236 139758386779904 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.55507493019104, loss=6.5136399269104
I0502 00:07:21.262413 139758302918400 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6168107986450195, loss=6.262975692749023
I0502 00:08:07.601639 139758386779904 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5519973039627075, loss=6.491495132446289
I0502 00:08:53.269940 139758302918400 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6500086188316345, loss=6.176371097564697
I0502 00:09:39.087835 139758386779904 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5950155258178711, loss=6.623286247253418
I0502 00:10:24.684420 139758302918400 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.625113308429718, loss=6.232093811035156
I0502 00:10:38.878810 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:10:48.078304 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:10:55.563982 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:10:57.302081 139985718949696 submission_runner.py:415] Time since start: 7536.96s, 	Step: 15732, 	{'train/accuracy': 0.03847656026482582, 'train/loss': 5.756903171539307, 'validation/accuracy': 0.03641999885439873, 'validation/loss': 5.786839962005615, 'validation/num_examples': 50000, 'test/accuracy': 0.029500002041459084, 'test/loss': 5.902904033660889, 'test/num_examples': 10000, 'score': 7214.822942972183, 'total_duration': 7536.961553573608, 'accumulated_submission_time': 7214.822942972183, 'accumulated_eval_time': 321.27693605422974, 'accumulated_logging_time': 0.5539584159851074}
I0502 00:10:57.317677 139758386779904 logging_writer.py:48] [15732] accumulated_eval_time=321.276936, accumulated_logging_time=0.553958, accumulated_submission_time=7214.822943, global_step=15732, preemption_count=0, score=7214.822943, test/accuracy=0.029500, test/loss=5.902904, test/num_examples=10000, total_duration=7536.961554, train/accuracy=0.038477, train/loss=5.756903, validation/accuracy=0.036420, validation/loss=5.786840, validation/num_examples=50000
I0502 00:11:29.496730 139758302918400 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6437731981277466, loss=6.12530517578125
I0502 00:12:15.316673 139758386779904 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6770129203796387, loss=6.147876739501953
I0502 00:13:01.028506 139758302918400 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7562774419784546, loss=6.139303684234619
I0502 00:13:46.843506 139758386779904 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5663551688194275, loss=6.4952168464660645
I0502 00:14:32.557732 139758302918400 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6447330117225647, loss=6.110673904418945
I0502 00:15:18.356076 139758386779904 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6740162968635559, loss=6.078279495239258
I0502 00:16:04.122163 139758302918400 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6084292531013489, loss=6.4215240478515625
I0502 00:16:50.708884 139758386779904 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6510406136512756, loss=6.259418487548828
I0502 00:17:36.545137 139758302918400 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7660781145095825, loss=6.097846508026123
I0502 00:17:57.617650 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:18:07.277073 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:18:15.235847 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:18:16.968626 139985718949696 submission_runner.py:415] Time since start: 7976.63s, 	Step: 16647, 	{'train/accuracy': 0.05453125014901161, 'train/loss': 5.520346641540527, 'validation/accuracy': 0.05297999829053879, 'validation/loss': 5.546545028686523, 'validation/num_examples': 50000, 'test/accuracy': 0.04200000315904617, 'test/loss': 5.695729732513428, 'test/num_examples': 10000, 'score': 7635.063677787781, 'total_duration': 7976.628078222275, 'accumulated_submission_time': 7635.063677787781, 'accumulated_eval_time': 340.6278612613678, 'accumulated_logging_time': 0.6102526187896729}
I0502 00:18:16.986509 139758386779904 logging_writer.py:48] [16647] accumulated_eval_time=340.627861, accumulated_logging_time=0.610253, accumulated_submission_time=7635.063678, global_step=16647, preemption_count=0, score=7635.063678, test/accuracy=0.042000, test/loss=5.695730, test/num_examples=10000, total_duration=7976.628078, train/accuracy=0.054531, train/loss=5.520347, validation/accuracy=0.052980, validation/loss=5.546545, validation/num_examples=50000
I0502 00:18:42.324463 139758302918400 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6173660159111023, loss=6.182088851928711
I0502 00:19:28.216968 139758386779904 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5653017163276672, loss=6.5678253173828125
I0502 00:20:13.741012 139758302918400 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6406274437904358, loss=6.46104097366333
I0502 00:20:59.136697 139758386779904 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7320423722267151, loss=5.9262003898620605
I0502 00:21:44.777705 139758302918400 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7273412346839905, loss=6.003969192504883
I0502 00:22:30.386487 139758386779904 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.7371804118156433, loss=5.902490615844727
I0502 00:23:16.295012 139758302918400 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7792418599128723, loss=6.052150726318359
I0502 00:24:02.202894 139758386779904 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6580854654312134, loss=6.222383499145508
I0502 00:24:48.674431 139758302918400 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8891423940658569, loss=5.847095966339111
I0502 00:25:17.067508 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:25:26.842047 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:25:35.265430 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:25:36.989968 139985718949696 submission_runner.py:415] Time since start: 8416.65s, 	Step: 17563, 	{'train/accuracy': 0.07349608838558197, 'train/loss': 5.310449600219727, 'validation/accuracy': 0.06675999611616135, 'validation/loss': 5.352139949798584, 'validation/num_examples': 50000, 'test/accuracy': 0.052400004118680954, 'test/loss': 5.5176520347595215, 'test/num_examples': 10000, 'score': 8055.113179206848, 'total_duration': 8416.649431705475, 'accumulated_submission_time': 8055.113179206848, 'accumulated_eval_time': 360.55031657218933, 'accumulated_logging_time': 0.6408324241638184}
I0502 00:25:37.008587 139758386779904 logging_writer.py:48] [17563] accumulated_eval_time=360.550317, accumulated_logging_time=0.640832, accumulated_submission_time=8055.113179, global_step=17563, preemption_count=0, score=8055.113179, test/accuracy=0.052400, test/loss=5.517652, test/num_examples=10000, total_duration=8416.649432, train/accuracy=0.073496, train/loss=5.310450, validation/accuracy=0.066760, validation/loss=5.352140, validation/num_examples=50000
I0502 00:25:55.295058 139758302918400 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8381548523902893, loss=5.890196800231934
I0502 00:26:41.441263 139758386779904 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7850024104118347, loss=5.840459823608398
I0502 00:27:27.421178 139758302918400 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6328697204589844, loss=6.594515323638916
I0502 00:28:13.134683 139758386779904 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7462328672409058, loss=5.902547836303711
I0502 00:28:58.881398 139758302918400 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6451008915901184, loss=6.578506946563721
I0502 00:29:44.752840 139758386779904 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9789767265319824, loss=5.841488838195801
I0502 00:30:30.486346 139758302918400 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8073418736457825, loss=5.8045334815979
I0502 00:31:16.333954 139758386779904 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6296516060829163, loss=6.435031414031982
I0502 00:32:02.014510 139758302918400 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6978539228439331, loss=6.463181495666504
I0502 00:32:37.223376 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:32:46.723596 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:32:55.276940 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:32:57.016488 139985718949696 submission_runner.py:415] Time since start: 8856.67s, 	Step: 18477, 	{'train/accuracy': 0.0935351550579071, 'train/loss': 5.113314628601074, 'validation/accuracy': 0.08619999885559082, 'validation/loss': 5.15183162689209, 'validation/num_examples': 50000, 'test/accuracy': 0.06620000302791595, 'test/loss': 5.359890937805176, 'test/num_examples': 10000, 'score': 8475.274995326996, 'total_duration': 8856.66515469551, 'accumulated_submission_time': 8475.274995326996, 'accumulated_eval_time': 380.33263206481934, 'accumulated_logging_time': 0.6940550804138184}
I0502 00:32:57.035410 139758386779904 logging_writer.py:48] [18477] accumulated_eval_time=380.332632, accumulated_logging_time=0.694055, accumulated_submission_time=8475.274995, global_step=18477, preemption_count=0, score=8475.274995, test/accuracy=0.066200, test/loss=5.359891, test/num_examples=10000, total_duration=8856.665155, train/accuracy=0.093535, train/loss=5.113315, validation/accuracy=0.086200, validation/loss=5.151832, validation/num_examples=50000
I0502 00:33:08.587806 139758302918400 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7241341471672058, loss=6.498069763183594
I0502 00:33:55.099002 139758386779904 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.797296941280365, loss=5.71583366394043
I0502 00:34:41.102676 139758302918400 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.009987235069275, loss=5.791073799133301
I0502 00:35:26.999141 139758386779904 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.8115032315254211, loss=5.737992763519287
I0502 00:36:13.006077 139758302918400 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8281052708625793, loss=6.244091987609863
I0502 00:36:59.167008 139758386779904 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7835882306098938, loss=5.767659664154053
I0502 00:37:44.899020 139758302918400 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8027037382125854, loss=5.639796257019043
I0502 00:38:30.873392 139758386779904 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8877454400062561, loss=5.697988510131836
I0502 00:39:16.895939 139758302918400 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7634671330451965, loss=6.314820766448975
I0502 00:39:57.123317 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:40:07.433606 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:40:15.939440 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:40:17.684608 139985718949696 submission_runner.py:415] Time since start: 9297.33s, 	Step: 19389, 	{'train/accuracy': 0.11347655951976776, 'train/loss': 4.906436443328857, 'validation/accuracy': 0.10415999591350555, 'validation/loss': 4.951474666595459, 'validation/num_examples': 50000, 'test/accuracy': 0.08130000531673431, 'test/loss': 5.182310581207275, 'test/num_examples': 10000, 'score': 8895.328304290771, 'total_duration': 9297.334980487823, 'accumulated_submission_time': 8895.328304290771, 'accumulated_eval_time': 400.88481545448303, 'accumulated_logging_time': 0.7288496494293213}
I0502 00:40:17.707514 139758386779904 logging_writer.py:48] [19389] accumulated_eval_time=400.884815, accumulated_logging_time=0.728850, accumulated_submission_time=8895.328304, global_step=19389, preemption_count=0, score=8895.328304, test/accuracy=0.081300, test/loss=5.182311, test/num_examples=10000, total_duration=9297.334980, train/accuracy=0.113477, train/loss=4.906436, validation/accuracy=0.104160, validation/loss=4.951475, validation/num_examples=50000
I0502 00:40:23.511242 139758302918400 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.8060910105705261, loss=5.666854381561279
I0502 00:41:10.203830 139758386779904 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7732776999473572, loss=5.625032424926758
I0502 00:41:56.415246 139758302918400 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8155380487442017, loss=5.727260589599609
I0502 00:42:42.306468 139758386779904 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.05254065990448, loss=5.537300109863281
I0502 00:43:28.050437 139758302918400 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8386049270629883, loss=5.480841636657715
I0502 00:44:13.973882 139758386779904 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9076865911483765, loss=5.597660064697266
I0502 00:44:59.834552 139758302918400 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9714471101760864, loss=5.602977752685547
I0502 00:45:45.577706 139758386779904 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0755889415740967, loss=5.494285583496094
I0502 00:46:31.369709 139758302918400 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6971429586410522, loss=6.521095275878906
I0502 00:47:17.533885 139758386779904 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7915282845497131, loss=5.641360282897949
I0502 00:47:17.985342 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:47:28.466731 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:47:37.326703 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:47:39.079095 139985718949696 submission_runner.py:415] Time since start: 9738.73s, 	Step: 20302, 	{'train/accuracy': 0.1340624988079071, 'train/loss': 4.724271774291992, 'validation/accuracy': 0.12409999966621399, 'validation/loss': 4.783267498016357, 'validation/num_examples': 50000, 'test/accuracy': 0.1007000058889389, 'test/loss': 5.04459285736084, 'test/num_examples': 10000, 'score': 9315.569348096848, 'total_duration': 9738.731276273727, 'accumulated_submission_time': 9315.569348096848, 'accumulated_eval_time': 421.971239566803, 'accumulated_logging_time': 0.7700347900390625}
I0502 00:47:39.097304 139758302918400 logging_writer.py:48] [20302] accumulated_eval_time=421.971240, accumulated_logging_time=0.770035, accumulated_submission_time=9315.569348, global_step=20302, preemption_count=0, score=9315.569348, test/accuracy=0.100700, test/loss=5.044593, test/num_examples=10000, total_duration=9738.731276, train/accuracy=0.134062, train/loss=4.724272, validation/accuracy=0.124100, validation/loss=4.783267, validation/num_examples=50000
I0502 00:48:25.570935 139758386779904 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8916990756988525, loss=5.491175651550293
I0502 00:49:11.857630 139758302918400 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8890467286109924, loss=5.460972785949707
I0502 00:49:57.743608 139758386779904 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7284297347068787, loss=6.476830005645752
I0502 00:50:43.671448 139758302918400 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7918002605438232, loss=6.144053936004639
I0502 00:51:29.542814 139758386779904 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.033349871635437, loss=5.407866954803467
I0502 00:52:15.461482 139758302918400 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1281036138534546, loss=5.529259204864502
I0502 00:53:01.207082 139758386779904 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0213569402694702, loss=5.356630325317383
I0502 00:53:47.007380 139758302918400 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.9538618326187134, loss=5.366150379180908
I0502 00:54:32.796603 139758386779904 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.8956856727600098, loss=5.768935680389404
I0502 00:54:39.269792 139985718949696 spec.py:298] Evaluating on the training split.
I0502 00:54:49.873357 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 00:54:58.226951 139985718949696 spec.py:326] Evaluating on the test split.
I0502 00:54:59.973765 139985718949696 submission_runner.py:415] Time since start: 10179.63s, 	Step: 21215, 	{'train/accuracy': 0.1669921875, 'train/loss': 4.492825031280518, 'validation/accuracy': 0.15060000121593475, 'validation/loss': 4.573497295379639, 'validation/num_examples': 50000, 'test/accuracy': 0.11550000309944153, 'test/loss': 4.872710704803467, 'test/num_examples': 10000, 'score': 9735.70060300827, 'total_duration': 10179.627565860748, 'accumulated_submission_time': 9735.70060300827, 'accumulated_eval_time': 442.6695284843445, 'accumulated_logging_time': 0.8109028339385986}
I0502 00:54:59.997313 139758302918400 logging_writer.py:48] [21215] accumulated_eval_time=442.669528, accumulated_logging_time=0.810903, accumulated_submission_time=9735.700603, global_step=21215, preemption_count=0, score=9735.700603, test/accuracy=0.115500, test/loss=4.872711, test/num_examples=10000, total_duration=10179.627566, train/accuracy=0.166992, train/loss=4.492825, validation/accuracy=0.150600, validation/loss=4.573497, validation/num_examples=50000
I0502 00:55:40.289044 139758386779904 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7914204597473145, loss=6.050711154937744
I0502 00:56:26.349394 139758302918400 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.0933600664138794, loss=5.302413463592529
I0502 00:57:12.315434 139758386779904 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.9399004578590393, loss=5.379623889923096
I0502 00:57:58.686357 139758302918400 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7143374681472778, loss=6.471144676208496
I0502 00:58:44.659244 139758386779904 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.9727265238761902, loss=5.46724796295166
I0502 00:59:30.776307 139758302918400 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.0783318281173706, loss=5.262448310852051
I0502 01:00:16.620415 139758386779904 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8763551115989685, loss=6.051217079162598
I0502 01:01:02.504799 139758302918400 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9562700986862183, loss=5.32853889465332
I0502 01:01:48.498930 139758386779904 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9577153325080872, loss=5.175839424133301
I0502 01:02:00.359025 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:02:09.906071 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:02:19.412489 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:02:21.142703 139985718949696 submission_runner.py:415] Time since start: 10620.80s, 	Step: 22127, 	{'train/accuracy': 0.18605467677116394, 'train/loss': 4.303596496582031, 'validation/accuracy': 0.17387999594211578, 'validation/loss': 4.3748369216918945, 'validation/num_examples': 50000, 'test/accuracy': 0.1340000033378601, 'test/loss': 4.688068866729736, 'test/num_examples': 10000, 'score': 10156.019765615463, 'total_duration': 10620.801187753677, 'accumulated_submission_time': 10156.019765615463, 'accumulated_eval_time': 463.4522132873535, 'accumulated_logging_time': 0.8583340644836426}
I0502 01:02:21.156270 139758302918400 logging_writer.py:48] [22127] accumulated_eval_time=463.452213, accumulated_logging_time=0.858334, accumulated_submission_time=10156.019766, global_step=22127, preemption_count=0, score=10156.019766, test/accuracy=0.134000, test/loss=4.688069, test/num_examples=10000, total_duration=10620.801188, train/accuracy=0.186055, train/loss=4.303596, validation/accuracy=0.173880, validation/loss=4.374837, validation/num_examples=50000
I0502 01:02:55.624904 139758386779904 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9564436078071594, loss=5.352514743804932
I0502 01:03:41.780546 139758302918400 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8116242289543152, loss=5.282059192657471
I0502 01:04:27.370416 139758386779904 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9521466493606567, loss=5.194684982299805
I0502 01:05:13.102691 139758302918400 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.1839901208877563, loss=5.1770830154418945
I0502 01:05:59.210195 139758386779904 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9531626105308533, loss=5.077528476715088
I0502 01:06:45.001842 139758302918400 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9935293793678284, loss=5.678896903991699
I0502 01:07:30.717828 139758386779904 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.171992301940918, loss=5.129392147064209
I0502 01:08:16.598442 139758302918400 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.880057692527771, loss=6.351154327392578
I0502 01:09:02.187519 139758386779904 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8230978846549988, loss=6.326313495635986
I0502 01:09:21.389045 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:09:29.419688 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:09:39.182732 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:09:40.927901 139985718949696 submission_runner.py:415] Time since start: 11060.59s, 	Step: 23043, 	{'train/accuracy': 0.2113085836172104, 'train/loss': 4.099295616149902, 'validation/accuracy': 0.20031999051570892, 'validation/loss': 4.17698335647583, 'validation/num_examples': 50000, 'test/accuracy': 0.15560001134872437, 'test/loss': 4.518993377685547, 'test/num_examples': 10000, 'score': 10576.22094798088, 'total_duration': 11060.586032390594, 'accumulated_submission_time': 10576.22094798088, 'accumulated_eval_time': 482.98971581459045, 'accumulated_logging_time': 0.8849108219146729}
I0502 01:09:40.947194 139758302918400 logging_writer.py:48] [23043] accumulated_eval_time=482.989716, accumulated_logging_time=0.884911, accumulated_submission_time=10576.220948, global_step=23043, preemption_count=0, score=10576.220948, test/accuracy=0.155600, test/loss=4.518993, test/num_examples=10000, total_duration=11060.586032, train/accuracy=0.211309, train/loss=4.099296, validation/accuracy=0.200320, validation/loss=4.176983, validation/num_examples=50000
I0502 01:10:08.111812 139758386779904 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0531965494155884, loss=5.151101112365723
I0502 01:10:54.287371 139758302918400 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.1763216257095337, loss=5.254220008850098
I0502 01:11:40.047147 139758386779904 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0720564126968384, loss=5.048816680908203
I0502 01:12:26.157056 139758302918400 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8672148585319519, loss=5.383194923400879
I0502 01:13:12.157198 139758386779904 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8001561760902405, loss=6.360879421234131
I0502 01:13:58.451502 139758302918400 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.8885576128959656, loss=5.767280101776123
I0502 01:14:44.388845 139758386779904 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.0590789318084717, loss=5.2029290199279785
I0502 01:15:30.052347 139758302918400 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.2118743658065796, loss=5.135852813720703
I0502 01:16:15.801659 139758386779904 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.0142618417739868, loss=5.337015151977539
I0502 01:16:41.098451 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:16:48.992453 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:16:58.767688 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:17:00.491602 139985718949696 submission_runner.py:415] Time since start: 11500.15s, 	Step: 23956, 	{'train/accuracy': 0.23869140446186066, 'train/loss': 3.954511880874634, 'validation/accuracy': 0.21701999008655548, 'validation/loss': 4.059271335601807, 'validation/num_examples': 50000, 'test/accuracy': 0.1615000069141388, 'test/loss': 4.447052001953125, 'test/num_examples': 10000, 'score': 10996.339548110962, 'total_duration': 11500.149591684341, 'accumulated_submission_time': 10996.339548110962, 'accumulated_eval_time': 502.38133811950684, 'accumulated_logging_time': 0.9184000492095947}
I0502 01:17:00.511961 139758302918400 logging_writer.py:48] [23956] accumulated_eval_time=502.381338, accumulated_logging_time=0.918400, accumulated_submission_time=10996.339548, global_step=23956, preemption_count=0, score=10996.339548, test/accuracy=0.161500, test/loss=4.447052, test/num_examples=10000, total_duration=11500.149592, train/accuracy=0.238691, train/loss=3.954512, validation/accuracy=0.217020, validation/loss=4.059271, validation/num_examples=50000
I0502 01:17:21.782576 139758386779904 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1100366115570068, loss=5.00653076171875
I0502 01:18:08.156387 139758302918400 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.8823668956756592, loss=5.733781337738037
I0502 01:18:54.107861 139758386779904 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7119519710540771, loss=6.338906764984131
I0502 01:19:39.936634 139758302918400 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9979604482650757, loss=5.552962303161621
I0502 01:20:25.618094 139758386779904 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8776686191558838, loss=6.300815105438232
I0502 01:21:11.405119 139758302918400 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0544146299362183, loss=4.999795436859131
I0502 01:21:57.521117 139758386779904 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.7561739087104797, loss=5.94065523147583
I0502 01:22:43.689502 139758302918400 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.1132738590240479, loss=4.962490081787109
I0502 01:23:29.668438 139758386779904 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9146670699119568, loss=6.345283031463623
I0502 01:24:00.746866 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:24:08.984314 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:24:18.857558 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:24:20.590652 139985718949696 submission_runner.py:415] Time since start: 11940.25s, 	Step: 24869, 	{'train/accuracy': 0.26126953959465027, 'train/loss': 3.7425291538238525, 'validation/accuracy': 0.24473999440670013, 'validation/loss': 3.8421661853790283, 'validation/num_examples': 50000, 'test/accuracy': 0.18380001187324524, 'test/loss': 4.258681297302246, 'test/num_examples': 10000, 'score': 11416.541731834412, 'total_duration': 11940.249184846878, 'accumulated_submission_time': 11416.541731834412, 'accumulated_eval_time': 522.2241768836975, 'accumulated_logging_time': 0.9525105953216553}
I0502 01:24:20.604464 139758302918400 logging_writer.py:48] [24869] accumulated_eval_time=522.224177, accumulated_logging_time=0.952511, accumulated_submission_time=11416.541732, global_step=24869, preemption_count=0, score=11416.541732, test/accuracy=0.183800, test/loss=4.258681, test/num_examples=10000, total_duration=11940.249185, train/accuracy=0.261270, train/loss=3.742529, validation/accuracy=0.244740, validation/loss=3.842166, validation/num_examples=50000
I0502 01:24:35.749294 139758386779904 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0544246435165405, loss=4.982049942016602
I0502 01:25:21.680062 139758302918400 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8965173959732056, loss=6.024891376495361
I0502 01:26:07.363614 139758386779904 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.107765555381775, loss=4.929478645324707
I0502 01:26:53.236230 139758302918400 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1041234731674194, loss=4.87536096572876
I0502 01:27:39.124246 139758386779904 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1695054769515991, loss=4.857604503631592
I0502 01:28:25.186648 139758302918400 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0645413398742676, loss=4.875051975250244
I0502 01:29:11.040857 139758386779904 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.2041456699371338, loss=4.811010837554932
I0502 01:29:57.302280 139758302918400 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0979191064834595, loss=5.012267112731934
I0502 01:30:43.033489 139758386779904 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1816048622131348, loss=4.9400315284729
I0502 01:31:20.670222 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:31:28.637419 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:31:38.453783 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:31:40.182153 139985718949696 submission_runner.py:415] Time since start: 12379.84s, 	Step: 25783, 	{'train/accuracy': 0.2789257764816284, 'train/loss': 3.6430723667144775, 'validation/accuracy': 0.2633799910545349, 'validation/loss': 3.73403000831604, 'validation/num_examples': 50000, 'test/accuracy': 0.2006000131368637, 'test/loss': 4.163596153259277, 'test/num_examples': 10000, 'score': 11836.578191518784, 'total_duration': 12379.840129852295, 'accumulated_submission_time': 11836.578191518784, 'accumulated_eval_time': 541.7346119880676, 'accumulated_logging_time': 0.9769608974456787}
I0502 01:31:40.202619 139758302918400 logging_writer.py:48] [25783] accumulated_eval_time=541.734612, accumulated_logging_time=0.976961, accumulated_submission_time=11836.578192, global_step=25783, preemption_count=0, score=11836.578192, test/accuracy=0.200600, test/loss=4.163596, test/num_examples=10000, total_duration=12379.840130, train/accuracy=0.278926, train/loss=3.643072, validation/accuracy=0.263380, validation/loss=3.734030, validation/num_examples=50000
I0502 01:31:48.813325 139758386779904 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1941906213760376, loss=4.635788440704346
I0502 01:32:35.325650 139758302918400 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0622564554214478, loss=5.287191390991211
I0502 01:33:21.273431 139758386779904 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.1309678554534912, loss=4.681864261627197
I0502 01:34:07.015820 139758302918400 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.968515157699585, loss=5.259915828704834
I0502 01:34:52.825835 139758386779904 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0776774883270264, loss=6.32039737701416
I0502 01:35:38.931840 139758302918400 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0858426094055176, loss=4.936084270477295
I0502 01:36:24.646336 139758386779904 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.2781391143798828, loss=4.809546947479248
I0502 01:37:10.840079 139758302918400 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.026536226272583, loss=5.457529067993164
I0502 01:37:57.348741 139758386779904 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.2400046586990356, loss=4.741820812225342
I0502 01:38:40.533560 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:38:48.621020 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:38:58.455052 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:39:00.189399 139985718949696 submission_runner.py:415] Time since start: 12819.85s, 	Step: 26695, 	{'train/accuracy': 0.3047265410423279, 'train/loss': 3.473586320877075, 'validation/accuracy': 0.28007999062538147, 'validation/loss': 3.6065399646759033, 'validation/num_examples': 50000, 'test/accuracy': 0.21490001678466797, 'test/loss': 4.067553997039795, 'test/num_examples': 10000, 'score': 12256.875454902649, 'total_duration': 12819.847035884857, 'accumulated_submission_time': 12256.875454902649, 'accumulated_eval_time': 561.3886034488678, 'accumulated_logging_time': 1.0120232105255127}
I0502 01:39:00.210062 139758302918400 logging_writer.py:48] [26695] accumulated_eval_time=561.388603, accumulated_logging_time=1.012023, accumulated_submission_time=12256.875455, global_step=26695, preemption_count=0, score=12256.875455, test/accuracy=0.214900, test/loss=4.067554, test/num_examples=10000, total_duration=12819.847036, train/accuracy=0.304727, train/loss=3.473586, validation/accuracy=0.280080, validation/loss=3.606540, validation/num_examples=50000
I0502 01:39:03.078563 139758386779904 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.2007315158843994, loss=4.664775371551514
I0502 01:39:49.425165 139758302918400 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.888131320476532, loss=5.957973480224609
I0502 01:40:35.623606 139758386779904 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1962592601776123, loss=4.5613789558410645
I0502 01:41:21.676862 139758302918400 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1890215873718262, loss=4.6749982833862305
I0502 01:42:07.895421 139758386779904 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9884095788002014, loss=5.639165878295898
I0502 01:42:53.760044 139758302918400 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1233912706375122, loss=5.313177108764648
I0502 01:43:39.786165 139758386779904 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1170017719268799, loss=4.8705058097839355
I0502 01:44:25.611285 139758302918400 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1071165800094604, loss=4.724869251251221
I0502 01:45:11.611511 139758386779904 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2264106273651123, loss=4.748507022857666
I0502 01:45:57.362044 139758302918400 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.2933266162872314, loss=4.637942314147949
I0502 01:46:00.588079 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:46:08.662399 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:46:18.402156 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:46:20.124510 139985718949696 submission_runner.py:415] Time since start: 13259.78s, 	Step: 27608, 	{'train/accuracy': 0.3232421875, 'train/loss': 3.352670431137085, 'validation/accuracy': 0.3027999997138977, 'validation/loss': 3.4646828174591064, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 3.9299354553222656, 'test/num_examples': 10000, 'score': 12677.220152139664, 'total_duration': 13259.782879114151, 'accumulated_submission_time': 12677.220152139664, 'accumulated_eval_time': 580.9238889217377, 'accumulated_logging_time': 1.0475108623504639}
I0502 01:46:20.138804 139758386779904 logging_writer.py:48] [27608] accumulated_eval_time=580.923889, accumulated_logging_time=1.047511, accumulated_submission_time=12677.220152, global_step=27608, preemption_count=0, score=12677.220152, test/accuracy=0.229700, test/loss=3.929935, test/num_examples=10000, total_duration=13259.782879, train/accuracy=0.323242, train/loss=3.352670, validation/accuracy=0.302800, validation/loss=3.464683, validation/num_examples=50000
I0502 01:47:03.940782 139758302918400 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.1991162300109863, loss=4.584865570068359
I0502 01:47:49.959618 139758386779904 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.0756767988204956, loss=4.566934585571289
I0502 01:48:35.775774 139758302918400 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.3742762804031372, loss=4.614507675170898
I0502 01:49:21.393624 139985718949696 spec.py:298] Evaluating on the training split.
I0502 01:49:29.483034 139985718949696 spec.py:310] Evaluating on the validation split.
I0502 01:49:39.334213 139985718949696 spec.py:326] Evaluating on the test split.
I0502 01:49:41.064147 139985718949696 submission_runner.py:415] Time since start: 13460.72s, 	Step: 28000, 	{'train/accuracy': 0.33927732706069946, 'train/loss': 3.3142194747924805, 'validation/accuracy': 0.3077400028705597, 'validation/loss': 3.472968339920044, 'validation/num_examples': 50000, 'test/accuracy': 0.233800008893013, 'test/loss': 3.940352201461792, 'test/num_examples': 10000, 'score': 12858.452586650848, 'total_duration': 13460.722616434097, 'accumulated_submission_time': 12858.452586650848, 'accumulated_eval_time': 600.5933916568756, 'accumulated_logging_time': 1.0756964683532715}
I0502 01:49:41.077297 139758386779904 logging_writer.py:48] [28000] accumulated_eval_time=600.593392, accumulated_logging_time=1.075696, accumulated_submission_time=12858.452587, global_step=28000, preemption_count=0, score=12858.452587, test/accuracy=0.233800, test/loss=3.940352, test/num_examples=10000, total_duration=13460.722616, train/accuracy=0.339277, train/loss=3.314219, validation/accuracy=0.307740, validation/loss=3.472968, validation/num_examples=50000
I0502 01:49:41.101512 139758302918400 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=12858.452587
I0502 01:49:41.381398 139985718949696 checkpoints.py:356] Saving checkpoint at step: 28000
I0502 01:49:42.829408 139985718949696 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1/checkpoint_28000
I0502 01:49:42.856230 139985718949696 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_lamb/imagenet_vit_jax/trial_1/checkpoint_28000.
I0502 01:49:43.266814 139985718949696 submission_runner.py:578] Tuning trial 1/1
I0502 01:49:43.267056 139985718949696 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 01:49:43.279986 139985718949696 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 70.77880144119263, 'total_duration': 122.28314208984375, 'accumulated_submission_time': 70.77880144119263, 'accumulated_eval_time': 51.5041766166687, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (870, {'train/accuracy': 0.002070312388241291, 'train/loss': 6.907751083374023, 'validation/accuracy': 0.00203999993391335, 'validation/loss': 6.907751560211182, 'validation/num_examples': 50000, 'test/accuracy': 0.002199999988079071, 'test/loss': 6.9077534675598145, 'test/num_examples': 10000, 'score': 491.0510485172272, 'total_duration': 557.8910610675812, 'accumulated_submission_time': 491.0510485172272, 'accumulated_eval_time': 66.80093789100647, 'accumulated_logging_time': 0.02406930923461914, 'global_step': 870, 'preemption_count': 0}), (1821, {'train/accuracy': 0.0032031249720603228, 'train/loss': 6.907746315002441, 'validation/accuracy': 0.002859999891370535, 'validation/loss': 6.907747268676758, 'validation/num_examples': 50000, 'test/accuracy': 0.0026000002399086952, 'test/loss': 6.907749652862549, 'test/num_examples': 10000, 'score': 911.1428608894348, 'total_duration': 993.0932540893555, 'accumulated_submission_time': 911.1428608894348, 'accumulated_eval_time': 81.86409640312195, 'accumulated_logging_time': 0.05510687828063965, 'global_step': 1821, 'preemption_count': 0}), (2771, {'train/accuracy': 0.0036328125279396772, 'train/loss': 6.907740116119385, 'validation/accuracy': 0.0031799999997019768, 'validation/loss': 6.907739639282227, 'validation/num_examples': 50000, 'test/accuracy': 0.002400000113993883, 'test/loss': 6.907741546630859, 'test/num_examples': 10000, 'score': 1331.477290391922, 'total_duration': 1428.7446813583374, 'accumulated_submission_time': 1331.477290391922, 'accumulated_eval_time': 97.14090657234192, 'accumulated_logging_time': 0.07891082763671875, 'global_step': 2771, 'preemption_count': 0}), (3720, {'train/accuracy': 0.0047851563431322575, 'train/loss': 6.907717704772949, 'validation/accuracy': 0.004299999680370092, 'validation/loss': 6.907718658447266, 'validation/num_examples': 50000, 'test/accuracy': 0.0034000002779066563, 'test/loss': 6.9077229499816895, 'test/num_examples': 10000, 'score': 1751.861798286438, 'total_duration': 1864.4822659492493, 'accumulated_submission_time': 1751.861798286438, 'accumulated_eval_time': 112.45414781570435, 'accumulated_logging_time': 0.10265111923217773, 'global_step': 3720, 'preemption_count': 0}), (4672, {'train/accuracy': 0.005156249739229679, 'train/loss': 6.907676696777344, 'validation/accuracy': 0.00443999981507659, 'validation/loss': 6.907678604125977, 'validation/num_examples': 50000, 'test/accuracy': 0.004399999976158142, 'test/loss': 6.907683849334717, 'test/num_examples': 10000, 'score': 2172.172273874283, 'total_duration': 2299.881747484207, 'accumulated_submission_time': 2172.172273874283, 'accumulated_eval_time': 127.49950432777405, 'accumulated_logging_time': 0.13039588928222656, 'global_step': 4672, 'preemption_count': 0}), (5611, {'train/accuracy': 0.005546874832361937, 'train/loss': 6.907561302185059, 'validation/accuracy': 0.005799999926239252, 'validation/loss': 6.907567501068115, 'validation/num_examples': 50000, 'test/accuracy': 0.00570000009611249, 'test/loss': 6.9075775146484375, 'test/num_examples': 10000, 'score': 2592.5282533168793, 'total_duration': 2735.6432070732117, 'accumulated_submission_time': 2592.5282533168793, 'accumulated_eval_time': 142.86485958099365, 'accumulated_logging_time': 0.1527261734008789, 'global_step': 5611, 'preemption_count': 0}), (6549, {'train/accuracy': 0.0044140624813735485, 'train/loss': 6.907231330871582, 'validation/accuracy': 0.0041600000113248825, 'validation/loss': 6.9072442054748535, 'validation/num_examples': 50000, 'test/accuracy': 0.005100000184029341, 'test/loss': 6.907261848449707, 'test/num_examples': 10000, 'score': 3012.8977262973785, 'total_duration': 3171.387901544571, 'accumulated_submission_time': 3012.8977262973785, 'accumulated_eval_time': 158.19697952270508, 'accumulated_logging_time': 0.17829203605651855, 'global_step': 6549, 'preemption_count': 0}), (7478, {'train/accuracy': 0.004062499850988388, 'train/loss': 6.906137466430664, 'validation/accuracy': 0.003399999812245369, 'validation/loss': 6.906174182891846, 'validation/num_examples': 50000, 'test/accuracy': 0.0034000002779066563, 'test/loss': 6.906256675720215, 'test/num_examples': 10000, 'score': 3433.2519192695618, 'total_duration': 3607.0225265026093, 'accumulated_submission_time': 3433.2519192695618, 'accumulated_eval_time': 173.43251752853394, 'accumulated_logging_time': 0.20505642890930176, 'global_step': 7478, 'preemption_count': 0}), (8402, {'train/accuracy': 0.0027343749534338713, 'train/loss': 6.902018070220947, 'validation/accuracy': 0.0025400000158697367, 'validation/loss': 6.902092933654785, 'validation/num_examples': 50000, 'test/accuracy': 0.0027000000700354576, 'test/loss': 6.90219259262085, 'test/num_examples': 10000, 'score': 3853.4424431324005, 'total_duration': 4042.4840536117554, 'accumulated_submission_time': 3853.4424431324005, 'accumulated_eval_time': 188.6567726135254, 'accumulated_logging_time': 0.23351550102233887, 'global_step': 8402, 'preemption_count': 0}), (9320, {'train/accuracy': 0.0023242186289280653, 'train/loss': 6.884580135345459, 'validation/accuracy': 0.002099999925121665, 'validation/loss': 6.884843349456787, 'validation/num_examples': 50000, 'test/accuracy': 0.002199999988079071, 'test/loss': 6.885444641113281, 'test/num_examples': 10000, 'score': 4273.608917713165, 'total_duration': 4477.834264278412, 'accumulated_submission_time': 4273.608917713165, 'accumulated_eval_time': 203.79075837135315, 'accumulated_logging_time': 0.2649216651916504, 'global_step': 9320, 'preemption_count': 0}), (10238, {'train/accuracy': 0.0022656249348074198, 'train/loss': 6.823245048522949, 'validation/accuracy': 0.00215999991632998, 'validation/loss': 6.824596881866455, 'validation/num_examples': 50000, 'test/accuracy': 0.0019000000320374966, 'test/loss': 6.825493335723877, 'test/num_examples': 10000, 'score': 4693.982902050018, 'total_duration': 4913.63467669487, 'accumulated_submission_time': 4693.982902050018, 'accumulated_eval_time': 219.17631793022156, 'accumulated_logging_time': 0.2870972156524658, 'global_step': 10238, 'preemption_count': 0}), (11152, {'train/accuracy': 0.00341796875, 'train/loss': 6.678938865661621, 'validation/accuracy': 0.0035800000187009573, 'validation/loss': 6.683973789215088, 'validation/num_examples': 50000, 'test/accuracy': 0.0035000001080334187, 'test/loss': 6.696055889129639, 'test/num_examples': 10000, 'score': 5114.257411003113, 'total_duration': 5349.4219622612, 'accumulated_submission_time': 5114.257411003113, 'accumulated_eval_time': 234.60003757476807, 'accumulated_logging_time': 0.35730838775634766, 'global_step': 11152, 'preemption_count': 0}), (12069, {'train/accuracy': 0.006269531324505806, 'train/loss': 6.516449928283691, 'validation/accuracy': 0.0050999997183680534, 'validation/loss': 6.528395652770996, 'validation/num_examples': 50000, 'test/accuracy': 0.0052000004798173904, 'test/loss': 6.551764965057373, 'test/num_examples': 10000, 'score': 5534.556992530823, 'total_duration': 5785.266585350037, 'accumulated_submission_time': 5534.556992530823, 'accumulated_eval_time': 250.06222891807556, 'accumulated_logging_time': 0.421358585357666, 'global_step': 12069, 'preemption_count': 0}), (12985, {'train/accuracy': 0.00830078125, 'train/loss': 6.399616718292236, 'validation/accuracy': 0.007400000002235174, 'validation/loss': 6.413402557373047, 'validation/num_examples': 50000, 'test/accuracy': 0.007300000172108412, 'test/loss': 6.45481014251709, 'test/num_examples': 10000, 'score': 5954.575271606445, 'total_duration': 6221.843695640564, 'accumulated_submission_time': 5954.575271606445, 'accumulated_eval_time': 266.5707266330719, 'accumulated_logging_time': 0.4531431198120117, 'global_step': 12985, 'preemption_count': 0}), (13901, {'train/accuracy': 0.014414061792194843, 'train/loss': 6.219668865203857, 'validation/accuracy': 0.013679999858140945, 'validation/loss': 6.23712682723999, 'validation/num_examples': 50000, 'test/accuracy': 0.010900000110268593, 'test/loss': 6.288051128387451, 'test/num_examples': 10000, 'score': 6374.585117816925, 'total_duration': 6659.829132080078, 'accumulated_submission_time': 6374.585117816925, 'accumulated_eval_time': 284.4993278980255, 'accumulated_logging_time': 0.481769323348999, 'global_step': 13901, 'preemption_count': 0}), (14818, {'train/accuracy': 0.025136718526482582, 'train/loss': 6.009947299957275, 'validation/accuracy': 0.022779999300837517, 'validation/loss': 6.03596305847168, 'validation/num_examples': 50000, 'test/accuracy': 0.019500000402331352, 'test/loss': 6.1152262687683105, 'test/num_examples': 10000, 'score': 6794.808881282806, 'total_duration': 7098.471710920334, 'accumulated_submission_time': 6794.808881282806, 'accumulated_eval_time': 302.8536841869354, 'accumulated_logging_time': 0.5199065208435059, 'global_step': 14818, 'preemption_count': 0}), (15732, {'train/accuracy': 0.03847656026482582, 'train/loss': 5.756903171539307, 'validation/accuracy': 0.03641999885439873, 'validation/loss': 5.786839962005615, 'validation/num_examples': 50000, 'test/accuracy': 0.029500002041459084, 'test/loss': 5.902904033660889, 'test/num_examples': 10000, 'score': 7214.822942972183, 'total_duration': 7536.961553573608, 'accumulated_submission_time': 7214.822942972183, 'accumulated_eval_time': 321.27693605422974, 'accumulated_logging_time': 0.5539584159851074, 'global_step': 15732, 'preemption_count': 0}), (16647, {'train/accuracy': 0.05453125014901161, 'train/loss': 5.520346641540527, 'validation/accuracy': 0.05297999829053879, 'validation/loss': 5.546545028686523, 'validation/num_examples': 50000, 'test/accuracy': 0.04200000315904617, 'test/loss': 5.695729732513428, 'test/num_examples': 10000, 'score': 7635.063677787781, 'total_duration': 7976.628078222275, 'accumulated_submission_time': 7635.063677787781, 'accumulated_eval_time': 340.6278612613678, 'accumulated_logging_time': 0.6102526187896729, 'global_step': 16647, 'preemption_count': 0}), (17563, {'train/accuracy': 0.07349608838558197, 'train/loss': 5.310449600219727, 'validation/accuracy': 0.06675999611616135, 'validation/loss': 5.352139949798584, 'validation/num_examples': 50000, 'test/accuracy': 0.052400004118680954, 'test/loss': 5.5176520347595215, 'test/num_examples': 10000, 'score': 8055.113179206848, 'total_duration': 8416.649431705475, 'accumulated_submission_time': 8055.113179206848, 'accumulated_eval_time': 360.55031657218933, 'accumulated_logging_time': 0.6408324241638184, 'global_step': 17563, 'preemption_count': 0}), (18477, {'train/accuracy': 0.0935351550579071, 'train/loss': 5.113314628601074, 'validation/accuracy': 0.08619999885559082, 'validation/loss': 5.15183162689209, 'validation/num_examples': 50000, 'test/accuracy': 0.06620000302791595, 'test/loss': 5.359890937805176, 'test/num_examples': 10000, 'score': 8475.274995326996, 'total_duration': 8856.66515469551, 'accumulated_submission_time': 8475.274995326996, 'accumulated_eval_time': 380.33263206481934, 'accumulated_logging_time': 0.6940550804138184, 'global_step': 18477, 'preemption_count': 0}), (19389, {'train/accuracy': 0.11347655951976776, 'train/loss': 4.906436443328857, 'validation/accuracy': 0.10415999591350555, 'validation/loss': 4.951474666595459, 'validation/num_examples': 50000, 'test/accuracy': 0.08130000531673431, 'test/loss': 5.182310581207275, 'test/num_examples': 10000, 'score': 8895.328304290771, 'total_duration': 9297.334980487823, 'accumulated_submission_time': 8895.328304290771, 'accumulated_eval_time': 400.88481545448303, 'accumulated_logging_time': 0.7288496494293213, 'global_step': 19389, 'preemption_count': 0}), (20302, {'train/accuracy': 0.1340624988079071, 'train/loss': 4.724271774291992, 'validation/accuracy': 0.12409999966621399, 'validation/loss': 4.783267498016357, 'validation/num_examples': 50000, 'test/accuracy': 0.1007000058889389, 'test/loss': 5.04459285736084, 'test/num_examples': 10000, 'score': 9315.569348096848, 'total_duration': 9738.731276273727, 'accumulated_submission_time': 9315.569348096848, 'accumulated_eval_time': 421.971239566803, 'accumulated_logging_time': 0.7700347900390625, 'global_step': 20302, 'preemption_count': 0}), (21215, {'train/accuracy': 0.1669921875, 'train/loss': 4.492825031280518, 'validation/accuracy': 0.15060000121593475, 'validation/loss': 4.573497295379639, 'validation/num_examples': 50000, 'test/accuracy': 0.11550000309944153, 'test/loss': 4.872710704803467, 'test/num_examples': 10000, 'score': 9735.70060300827, 'total_duration': 10179.627565860748, 'accumulated_submission_time': 9735.70060300827, 'accumulated_eval_time': 442.6695284843445, 'accumulated_logging_time': 0.8109028339385986, 'global_step': 21215, 'preemption_count': 0}), (22127, {'train/accuracy': 0.18605467677116394, 'train/loss': 4.303596496582031, 'validation/accuracy': 0.17387999594211578, 'validation/loss': 4.3748369216918945, 'validation/num_examples': 50000, 'test/accuracy': 0.1340000033378601, 'test/loss': 4.688068866729736, 'test/num_examples': 10000, 'score': 10156.019765615463, 'total_duration': 10620.801187753677, 'accumulated_submission_time': 10156.019765615463, 'accumulated_eval_time': 463.4522132873535, 'accumulated_logging_time': 0.8583340644836426, 'global_step': 22127, 'preemption_count': 0}), (23043, {'train/accuracy': 0.2113085836172104, 'train/loss': 4.099295616149902, 'validation/accuracy': 0.20031999051570892, 'validation/loss': 4.17698335647583, 'validation/num_examples': 50000, 'test/accuracy': 0.15560001134872437, 'test/loss': 4.518993377685547, 'test/num_examples': 10000, 'score': 10576.22094798088, 'total_duration': 11060.586032390594, 'accumulated_submission_time': 10576.22094798088, 'accumulated_eval_time': 482.98971581459045, 'accumulated_logging_time': 0.8849108219146729, 'global_step': 23043, 'preemption_count': 0}), (23956, {'train/accuracy': 0.23869140446186066, 'train/loss': 3.954511880874634, 'validation/accuracy': 0.21701999008655548, 'validation/loss': 4.059271335601807, 'validation/num_examples': 50000, 'test/accuracy': 0.1615000069141388, 'test/loss': 4.447052001953125, 'test/num_examples': 10000, 'score': 10996.339548110962, 'total_duration': 11500.149591684341, 'accumulated_submission_time': 10996.339548110962, 'accumulated_eval_time': 502.38133811950684, 'accumulated_logging_time': 0.9184000492095947, 'global_step': 23956, 'preemption_count': 0}), (24869, {'train/accuracy': 0.26126953959465027, 'train/loss': 3.7425291538238525, 'validation/accuracy': 0.24473999440670013, 'validation/loss': 3.8421661853790283, 'validation/num_examples': 50000, 'test/accuracy': 0.18380001187324524, 'test/loss': 4.258681297302246, 'test/num_examples': 10000, 'score': 11416.541731834412, 'total_duration': 11940.249184846878, 'accumulated_submission_time': 11416.541731834412, 'accumulated_eval_time': 522.2241768836975, 'accumulated_logging_time': 0.9525105953216553, 'global_step': 24869, 'preemption_count': 0}), (25783, {'train/accuracy': 0.2789257764816284, 'train/loss': 3.6430723667144775, 'validation/accuracy': 0.2633799910545349, 'validation/loss': 3.73403000831604, 'validation/num_examples': 50000, 'test/accuracy': 0.2006000131368637, 'test/loss': 4.163596153259277, 'test/num_examples': 10000, 'score': 11836.578191518784, 'total_duration': 12379.840129852295, 'accumulated_submission_time': 11836.578191518784, 'accumulated_eval_time': 541.7346119880676, 'accumulated_logging_time': 0.9769608974456787, 'global_step': 25783, 'preemption_count': 0}), (26695, {'train/accuracy': 0.3047265410423279, 'train/loss': 3.473586320877075, 'validation/accuracy': 0.28007999062538147, 'validation/loss': 3.6065399646759033, 'validation/num_examples': 50000, 'test/accuracy': 0.21490001678466797, 'test/loss': 4.067553997039795, 'test/num_examples': 10000, 'score': 12256.875454902649, 'total_duration': 12819.847035884857, 'accumulated_submission_time': 12256.875454902649, 'accumulated_eval_time': 561.3886034488678, 'accumulated_logging_time': 1.0120232105255127, 'global_step': 26695, 'preemption_count': 0}), (27608, {'train/accuracy': 0.3232421875, 'train/loss': 3.352670431137085, 'validation/accuracy': 0.3027999997138977, 'validation/loss': 3.4646828174591064, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 3.9299354553222656, 'test/num_examples': 10000, 'score': 12677.220152139664, 'total_duration': 13259.782879114151, 'accumulated_submission_time': 12677.220152139664, 'accumulated_eval_time': 580.9238889217377, 'accumulated_logging_time': 1.0475108623504639, 'global_step': 27608, 'preemption_count': 0}), (28000, {'train/accuracy': 0.33927732706069946, 'train/loss': 3.3142194747924805, 'validation/accuracy': 0.3077400028705597, 'validation/loss': 3.472968339920044, 'validation/num_examples': 50000, 'test/accuracy': 0.233800008893013, 'test/loss': 3.940352201461792, 'test/num_examples': 10000, 'score': 12858.452586650848, 'total_duration': 13460.722616434097, 'accumulated_submission_time': 12858.452586650848, 'accumulated_eval_time': 600.5933916568756, 'accumulated_logging_time': 1.0756964683532715, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0502 01:49:43.280173 139985718949696 submission_runner.py:581] Timing: 12858.452586650848
I0502 01:49:43.280221 139985718949696 submission_runner.py:582] ====================
I0502 01:49:43.280394 139985718949696 submission_runner.py:645] Final imagenet_vit score: 12858.452586650848
