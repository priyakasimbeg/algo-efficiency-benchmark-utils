python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_lamb --overwrite=True --save_checkpoints=False --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_05-02-2023-13-04-44.log
I0502 13:05:04.353455 140118975928128 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax.
I0502 13:05:04.431763 140118975928128 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0502 13:05:05.235090 140118975928128 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0502 13:05:05.235878 140118975928128 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0502 13:05:05.239487 140118975928128 submission_runner.py:538] Using RNG seed 3623384723
I0502 13:05:07.781167 140118975928128 submission_runner.py:547] --- Tuning run 1/1 ---
I0502 13:05:07.781361 140118975928128 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1.
I0502 13:05:07.781672 140118975928128 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1/hparams.json.
I0502 13:05:07.907048 140118975928128 submission_runner.py:241] Initializing dataset.
I0502 13:05:07.907245 140118975928128 submission_runner.py:248] Initializing model.
I0502 13:05:13.584208 140118975928128 submission_runner.py:258] Initializing optimizer.
I0502 13:05:14.377787 140118975928128 submission_runner.py:265] Initializing metrics bundle.
I0502 13:05:14.377977 140118975928128 submission_runner.py:282] Initializing checkpoint and logger.
I0502 13:05:14.379018 140118975928128 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0502 13:05:14.379261 140118975928128 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0502 13:05:14.379322 140118975928128 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0502 13:05:15.159399 140118975928128 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0502 13:05:15.160320 140118975928128 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1/flags_0.json.
I0502 13:05:15.166559 140118975928128 submission_runner.py:318] Starting training loop.
I0502 13:05:15.361630 140118975928128 input_pipeline.py:20] Loading split = train-clean-100
I0502 13:05:15.393697 140118975928128 input_pipeline.py:20] Loading split = train-clean-360
I0502 13:05:15.714395 140118975928128 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0502 13:06:35.434639 139942206351104 logging_writer.py:48] [0] global_step=0, grad_norm=39.97489547729492, loss=31.432048797607422
I0502 13:06:35.461411 140118975928128 spec.py:298] Evaluating on the training split.
I0502 13:06:35.563721 140118975928128 input_pipeline.py:20] Loading split = train-clean-100
I0502 13:06:35.591544 140118975928128 input_pipeline.py:20] Loading split = train-clean-360
I0502 13:06:35.870845 140118975928128 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0502 13:07:24.076858 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 13:07:24.136646 140118975928128 input_pipeline.py:20] Loading split = dev-clean
I0502 13:07:24.141788 140118975928128 input_pipeline.py:20] Loading split = dev-other
I0502 13:08:03.853790 140118975928128 spec.py:326] Evaluating on the test split.
I0502 13:08:03.916506 140118975928128 input_pipeline.py:20] Loading split = test-clean
I0502 13:08:32.524739 140118975928128 submission_runner.py:415] Time since start: 197.36s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(30.934254, dtype=float32), 'train/wer': 0.9820971201299378, 'validation/ctc_loss': DeviceArray(30.049446, dtype=float32), 'validation/wer': 0.9543845092572046, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.041254, dtype=float32), 'test/wer': 0.9716247232547275, 'test/num_examples': 2472, 'score': 80.29468321800232, 'total_duration': 197.35642838478088, 'accumulated_submission_time': 80.29468321800232, 'accumulated_eval_time': 117.06159496307373, 'accumulated_logging_time': 0}
I0502 13:08:32.548837 139939714946816 logging_writer.py:48] [1] accumulated_eval_time=117.061595, accumulated_logging_time=0, accumulated_submission_time=80.294683, global_step=1, preemption_count=0, score=80.294683, test/ctc_loss=30.0412540435791, test/num_examples=2472, test/wer=0.971625, total_duration=197.356428, train/ctc_loss=30.934253692626953, train/wer=0.982097, validation/ctc_loss=30.04944610595703, validation/num_examples=5348, validation/wer=0.954385
I0502 13:10:21.241792 139943950067456 logging_writer.py:48] [100] global_step=100, grad_norm=106.57505798339844, loss=21.518539428710938
I0502 13:11:37.480517 139943958460160 logging_writer.py:48] [200] global_step=200, grad_norm=7.612424373626709, loss=8.221182823181152
I0502 13:12:53.795607 139943950067456 logging_writer.py:48] [300] global_step=300, grad_norm=1.6281226873397827, loss=6.462258338928223
I0502 13:14:10.169100 139943958460160 logging_writer.py:48] [400] global_step=400, grad_norm=1.3607759475708008, loss=6.062412261962891
I0502 13:15:26.445648 139943950067456 logging_writer.py:48] [500] global_step=500, grad_norm=2.9136359691619873, loss=5.933642864227295
I0502 13:16:42.714732 139943958460160 logging_writer.py:48] [600] global_step=600, grad_norm=0.4967651963233948, loss=5.855495452880859
I0502 13:17:58.983972 139943950067456 logging_writer.py:48] [700] global_step=700, grad_norm=2.006962299346924, loss=5.829249382019043
I0502 13:19:15.253504 139943958460160 logging_writer.py:48] [800] global_step=800, grad_norm=2.9779720306396484, loss=5.8371758460998535
I0502 13:20:31.607468 139943950067456 logging_writer.py:48] [900] global_step=900, grad_norm=1.057458519935608, loss=5.794528484344482
I0502 13:21:47.945955 139943958460160 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6609025001525879, loss=5.811056613922119
I0502 13:23:07.307339 139946411316992 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.191663384437561, loss=5.814775466918945
I0502 13:24:23.816003 139946402924288 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.8909410238265991, loss=5.797972202301025
I0502 13:25:40.088519 139946411316992 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.2900991439819336, loss=5.779153347015381
I0502 13:26:56.403010 139946402924288 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.39922434091567993, loss=5.800075054168701
I0502 13:28:12.811311 139946411316992 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3187339305877686, loss=5.7802629470825195
I0502 13:29:29.177732 139946402924288 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9277799725532532, loss=5.780152797698975
I0502 13:30:45.408530 139946411316992 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.3424501121044159, loss=5.757804870605469
I0502 13:32:01.791267 139946402924288 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5262317061424255, loss=5.79166841506958
I0502 13:33:18.082040 139946411316992 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7073587775230408, loss=5.6458964347839355
I0502 13:34:34.392349 139946402924288 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.7017004489898682, loss=5.60136604309082
I0502 13:35:54.164652 139945755956992 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4531378149986267, loss=5.543604373931885
I0502 13:37:10.506355 139945747564288 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5764904618263245, loss=5.530842304229736
I0502 13:38:26.805818 139945755956992 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.46288827061653137, loss=5.527438163757324
I0502 13:39:43.127970 139945747564288 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6840689778327942, loss=5.506117820739746
I0502 13:40:59.492074 139945755956992 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8429804444313049, loss=5.499665260314941
I0502 13:42:15.842459 139945747564288 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6479839086532593, loss=5.515680313110352
I0502 13:43:32.177047 139945755956992 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9243520498275757, loss=5.4785003662109375
I0502 13:44:48.511115 139945747564288 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7776687741279602, loss=5.469419956207275
I0502 13:46:04.861749 139945755956992 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.47241437435150146, loss=5.474950313568115
I0502 13:47:21.513438 139945747564288 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9177840352058411, loss=5.462007999420166
I0502 13:48:33.534186 140118975928128 spec.py:298] Evaluating on the training split.
I0502 13:49:00.072325 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 13:49:34.652765 140118975928128 spec.py:326] Evaluating on the test split.
I0502 13:49:51.959517 140118975928128 submission_runner.py:415] Time since start: 2676.79s, 	Step: 3091, 	{'train/ctc_loss': DeviceArray(6.1198583, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.3290195, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.242953, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2481.2242801189423, 'total_duration': 2676.789923429489, 'accumulated_submission_time': 2481.2242801189423, 'accumulated_eval_time': 195.48413372039795, 'accumulated_logging_time': 0.03548765182495117}
I0502 13:49:51.980597 139944445236992 logging_writer.py:48] [3091] accumulated_eval_time=195.484134, accumulated_logging_time=0.035488, accumulated_submission_time=2481.224280, global_step=3091, preemption_count=0, score=2481.224280, test/ctc_loss=6.242952823638916, test/num_examples=2472, test/wer=0.899580, total_duration=2676.789923, train/ctc_loss=6.119858264923096, train/wer=0.944636, validation/ctc_loss=6.329019546508789, validation/num_examples=5348, validation/wer=0.895995
I0502 13:49:59.666115 139944436844288 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.739875316619873, loss=5.4399003982543945
I0502 13:51:15.939746 139944445236992 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9937573075294495, loss=5.430896759033203
I0502 13:52:32.278009 139944436844288 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.704119086265564, loss=5.44533109664917
I0502 13:53:48.597327 139944445236992 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0138275623321533, loss=5.420244216918945
I0502 13:55:04.900484 139944436844288 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.067017674446106, loss=5.410495281219482
I0502 13:56:21.225884 139944445236992 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.563491106033325, loss=5.41223669052124
I0502 13:57:37.519115 139944436844288 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.216597318649292, loss=5.395902633666992
I0502 13:58:53.828248 139944445236992 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.5923422574996948, loss=5.3896894454956055
I0502 14:00:10.288538 139944436844288 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.2091341018676758, loss=5.3894524574279785
I0502 14:01:26.469334 139944445236992 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.270153284072876, loss=5.374099254608154
I0502 14:02:42.705223 139944436844288 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.1978049278259277, loss=5.372581958770752
I0502 14:04:02.427114 139944445236992 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1004745960235596, loss=5.325542449951172
I0502 14:05:18.676199 139944436844288 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.5182929039001465, loss=5.325014114379883
I0502 14:06:34.913287 139944445236992 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.7288901805877686, loss=5.323951244354248
I0502 14:07:51.075803 139944436844288 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.2335129976272583, loss=5.308091640472412
I0502 14:09:07.187766 139944445236992 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.794417679309845, loss=5.291712760925293
I0502 14:10:23.270203 139944436844288 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.3914310932159424, loss=5.287473201751709
I0502 14:11:39.634590 139944445236992 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8296003341674805, loss=5.270596981048584
I0502 14:12:55.718838 139944436844288 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.135986328125, loss=5.2695231437683105
I0502 14:14:11.773916 139944445236992 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.468727469444275, loss=5.243247985839844
I0502 14:15:27.866358 139944436844288 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.160909652709961, loss=5.266373634338379
I0502 14:16:47.436647 139945755956992 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.005042314529419, loss=5.262497901916504
I0502 14:18:03.429826 139945747564288 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6776015758514404, loss=5.197762966156006
I0502 14:19:19.458462 139945755956992 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9871188402175903, loss=5.1351470947265625
I0502 14:20:35.512840 139945747564288 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9435837268829346, loss=5.1372551918029785
I0502 14:21:51.582319 139945755956992 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.8412280082702637, loss=5.023993015289307
I0502 14:23:07.867284 139945747564288 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.2235567569732666, loss=4.763529300689697
I0502 14:24:23.969147 139945755956992 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.3286043405532837, loss=4.595333099365234
I0502 14:25:40.086937 139945747564288 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.5338048934936523, loss=4.381363868713379
I0502 14:26:56.166007 139945755956992 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.6809544563293457, loss=4.259191036224365
I0502 14:28:12.335698 139945747564288 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.9850085973739624, loss=4.1186842918396
I0502 14:29:32.124452 139945100596992 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.2356626987457275, loss=4.018669605255127
I0502 14:29:52.493321 140118975928128 spec.py:298] Evaluating on the training split.
I0502 14:30:28.082744 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 14:31:05.742528 140118975928128 spec.py:326] Evaluating on the test split.
I0502 14:31:25.163763 140118975928128 submission_runner.py:415] Time since start: 5169.99s, 	Step: 6228, 	{'train/ctc_loss': DeviceArray(2.9235482, dtype=float32), 'train/wer': 0.6300029299736303, 'validation/ctc_loss': DeviceArray(3.1920831, dtype=float32), 'validation/wer': 0.6483998880838213, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.9425929, dtype=float32), 'test/wer': 0.6137956248857473, 'test/num_examples': 2472, 'score': 4881.678156852722, 'total_duration': 5169.994038581848, 'accumulated_submission_time': 4881.678156852722, 'accumulated_eval_time': 288.1514732837677, 'accumulated_logging_time': 0.07079219818115234}
I0502 14:31:25.184715 139945827636992 logging_writer.py:48] [6228] accumulated_eval_time=288.151473, accumulated_logging_time=0.070792, accumulated_submission_time=4881.678157, global_step=6228, preemption_count=0, score=4881.678157, test/ctc_loss=2.9425928592681885, test/num_examples=2472, test/wer=0.613796, total_duration=5169.994039, train/ctc_loss=2.9235482215881348, train/wer=0.630003, validation/ctc_loss=3.1920831203460693, validation/num_examples=5348, validation/wer=0.648400
I0502 14:32:20.721339 139945819244288 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.3960658311843872, loss=3.8146984577178955
I0502 14:33:36.871688 139945827636992 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.8353369235992432, loss=3.706732749938965
I0502 14:34:52.923293 139945819244288 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3571923971176147, loss=3.618917465209961
I0502 14:36:09.321272 139945827636992 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.4066824913024902, loss=3.4933109283447266
I0502 14:37:25.517546 139945819244288 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.6690454483032227, loss=3.4629061222076416
I0502 14:38:41.660904 139945827636992 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.378574252128601, loss=3.3933451175689697
I0502 14:39:57.736733 139945819244288 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.1617722511291504, loss=3.2591946125030518
I0502 14:41:13.867110 139945827636992 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.1106040477752686, loss=3.2627711296081543
I0502 14:42:30.011647 139945819244288 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.6259677410125732, loss=3.229501247406006
I0502 14:43:46.129495 139945827636992 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.5450893640518188, loss=3.1219983100891113
I0502 14:45:05.728477 139945172276992 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.264345407485962, loss=3.016937255859375
I0502 14:46:21.756153 139945163884288 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.4887369871139526, loss=3.050750732421875
I0502 14:47:38.059802 139945172276992 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.315709114074707, loss=2.9300029277801514
I0502 14:48:54.110642 139945163884288 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.2695733308792114, loss=2.9018876552581787
I0502 14:50:10.115096 139945172276992 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.3776224851608276, loss=2.900040864944458
I0502 14:51:26.104623 139945163884288 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.239729881286621, loss=2.8444156646728516
I0502 14:52:42.201418 139945172276992 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.20121169090271, loss=2.7920467853546143
I0502 14:53:58.272725 139945163884288 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.2539222240447998, loss=2.804652214050293
I0502 14:55:14.282764 139945172276992 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.404579997062683, loss=2.797443389892578
I0502 14:56:30.268607 139945163884288 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.121998906135559, loss=2.6707022190093994
I0502 14:57:49.859374 139945172276992 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.3412816524505615, loss=2.7127442359924316
I0502 14:59:06.196558 139945163884288 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.2570867538452148, loss=2.6571402549743652
I0502 15:00:22.418124 139945172276992 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.394963026046753, loss=2.6642820835113525
I0502 15:01:38.529626 139945163884288 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.118964433670044, loss=2.5653722286224365
I0502 15:02:54.581656 139945172276992 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.408892035484314, loss=2.584209442138672
I0502 15:04:10.661941 139945163884288 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.1491312980651855, loss=2.565408706665039
I0502 15:05:26.736713 139945172276992 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.2909597158432007, loss=2.490036725997925
I0502 15:06:42.889500 139945163884288 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0547871589660645, loss=2.4747750759124756
I0502 15:07:58.948418 139945172276992 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.0603079795837402, loss=2.514836072921753
I0502 15:09:14.989180 139945163884288 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.15572190284729, loss=2.508079767227173
I0502 15:10:34.691060 139945827636992 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.2083187103271484, loss=2.330002546310425
I0502 15:11:25.415861 140118975928128 spec.py:298] Evaluating on the training split.
I0502 15:12:02.542264 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 15:12:40.660526 140118975928128 spec.py:326] Evaluating on the test split.
I0502 15:13:00.163697 140118975928128 submission_runner.py:415] Time since start: 7664.99s, 	Step: 9368, 	{'train/ctc_loss': DeviceArray(1.0552036, dtype=float32), 'train/wer': 0.31992346252984805, 'validation/ctc_loss': DeviceArray(1.4594338, dtype=float32), 'validation/wer': 0.38334185568601725, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.1347238, dtype=float32), 'test/wer': 0.321532305567404, 'test/num_examples': 2472, 'score': 7281.847655534744, 'total_duration': 7664.994225263596, 'accumulated_submission_time': 7281.847655534744, 'accumulated_eval_time': 382.8964591026306, 'accumulated_logging_time': 0.10547590255737305}
I0502 15:13:00.185400 139945397556992 logging_writer.py:48] [9368] accumulated_eval_time=382.896459, accumulated_logging_time=0.105476, accumulated_submission_time=7281.847656, global_step=9368, preemption_count=0, score=7281.847656, test/ctc_loss=1.1347237825393677, test/num_examples=2472, test/wer=0.321532, total_duration=7664.994225, train/ctc_loss=1.0552035570144653, train/wer=0.319923, validation/ctc_loss=1.4594337940216064, validation/num_examples=5348, validation/wer=0.383342
I0502 15:13:25.297641 139945389164288 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.2794177532196045, loss=2.4502475261688232
I0502 15:14:41.310131 139945397556992 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3197695016860962, loss=2.351407051086426
I0502 15:15:57.328068 139945389164288 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.2031729221343994, loss=2.311457633972168
I0502 15:17:13.339996 139945397556992 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1153382062911987, loss=2.344405174255371
I0502 15:18:29.394170 139945389164288 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.6156672239303589, loss=2.4130284786224365
I0502 15:19:45.530645 139945397556992 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.3501609563827515, loss=2.3196029663085938
I0502 15:21:01.504552 139945389164288 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.3609893321990967, loss=2.3260021209716797
I0502 15:22:17.843462 139945397556992 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.2088414430618286, loss=2.2930681705474854
I0502 15:23:33.836299 139945389164288 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.4145586490631104, loss=2.3650057315826416
I0502 15:24:53.459259 139945397556992 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.2633388042449951, loss=2.265641689300537
I0502 15:26:09.524100 139945389164288 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.365799903869629, loss=2.2405221462249756
I0502 15:27:25.583566 139945397556992 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4147542715072632, loss=2.2638421058654785
I0502 15:28:41.635339 139945389164288 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.1703239679336548, loss=2.244490385055542
I0502 15:29:57.690371 139945397556992 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.302483320236206, loss=2.163804531097412
I0502 15:31:13.771369 139945389164288 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.3029670715332031, loss=2.251159191131592
I0502 15:32:29.881809 139945397556992 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5273305177688599, loss=2.1723971366882324
I0502 15:33:46.173410 139945389164288 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.3504853248596191, loss=2.1443331241607666
I0502 15:35:02.226062 139945397556992 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.116313099861145, loss=2.155536413192749
I0502 15:36:18.219461 139945389164288 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.4111921787261963, loss=2.1065361499786377
I0502 15:37:34.295822 139945397556992 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.1611666679382324, loss=2.0283827781677246
I0502 15:38:53.782862 139945397556992 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0802940130233765, loss=2.087158203125
I0502 15:40:09.868073 139945389164288 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.4571832418441772, loss=2.141237497329712
I0502 15:41:25.904234 139945397556992 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.1138274669647217, loss=2.096823215484619
I0502 15:42:41.915798 139945389164288 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.3307186365127563, loss=2.0806081295013428
I0502 15:43:57.882029 139945397556992 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.3235167264938354, loss=2.0661725997924805
I0502 15:45:14.194738 139945389164288 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.1793692111968994, loss=2.0900614261627197
I0502 15:46:30.162240 139945397556992 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.117627739906311, loss=2.0758373737335205
I0502 15:47:46.186622 139945389164288 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0991936922073364, loss=2.0275440216064453
I0502 15:49:02.178217 139945397556992 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0479707717895508, loss=2.00028133392334
I0502 15:50:18.152605 139945389164288 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.17622709274292, loss=1.9832978248596191
I0502 15:51:37.661489 139945397556992 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.2179888486862183, loss=2.0190799236297607
I0502 15:52:53.710446 139945389164288 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.0449607372283936, loss=2.0043342113494873
I0502 15:53:00.413408 140118975928128 spec.py:298] Evaluating on the training split.
I0502 15:53:37.407120 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 15:54:15.482558 140118975928128 spec.py:326] Evaluating on the test split.
I0502 15:54:35.204753 140118975928128 submission_runner.py:415] Time since start: 10160.03s, 	Step: 12510, 	{'train/ctc_loss': DeviceArray(0.6890884, dtype=float32), 'train/wer': 0.22219836839845428, 'validation/ctc_loss': DeviceArray(1.0650474, dtype=float32), 'validation/wer': 0.2929984852724098, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7560169, dtype=float32), 'test/wer': 0.23053642881806918, 'test/num_examples': 2472, 'score': 9682.014013767242, 'total_duration': 10160.03490281105, 'accumulated_submission_time': 9682.014013767242, 'accumulated_eval_time': 477.68458008766174, 'accumulated_logging_time': 0.14229106903076172}
I0502 15:54:35.226998 139945827636992 logging_writer.py:48] [12510] accumulated_eval_time=477.684580, accumulated_logging_time=0.142291, accumulated_submission_time=9682.014014, global_step=12510, preemption_count=0, score=9682.014014, test/ctc_loss=0.7560169100761414, test/num_examples=2472, test/wer=0.230536, total_duration=10160.034903, train/ctc_loss=0.6890884041786194, train/wer=0.222198, validation/ctc_loss=1.0650473833084106, validation/num_examples=5348, validation/wer=0.292998
I0502 15:55:44.272393 139945819244288 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0902830362319946, loss=1.9515830278396606
I0502 15:57:00.198508 139945827636992 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1305073499679565, loss=2.0522615909576416
I0502 15:58:16.380844 139945819244288 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.1535515785217285, loss=2.0186688899993896
I0502 15:59:32.411759 139945827636992 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.2016761302947998, loss=1.9582479000091553
I0502 16:00:48.432741 139945819244288 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.019056797027588, loss=1.9117412567138672
I0502 16:02:04.446471 139945827636992 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9968510270118713, loss=2.012925148010254
I0502 16:03:20.387432 139945819244288 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.247176170349121, loss=1.9954754114151
I0502 16:04:36.377407 139945827636992 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0949853658676147, loss=2.0175929069519043
I0502 16:05:58.103521 139945499956992 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.410959005355835, loss=1.9249986410140991
I0502 16:07:14.113500 139945491564288 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3785207271575928, loss=1.8875839710235596
I0502 16:08:30.075491 139945499956992 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.575548768043518, loss=1.9533120393753052
I0502 16:09:46.211351 139945491564288 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.4392073154449463, loss=1.9215339422225952
I0502 16:11:02.141888 139945499956992 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1341208219528198, loss=1.9044241905212402
I0502 16:12:18.162627 139945491564288 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0611094236373901, loss=1.8841426372528076
I0502 16:13:34.151648 139945499956992 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.2093061208724976, loss=1.9043502807617188
I0502 16:14:50.114990 139945491564288 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.005573034286499, loss=1.8611037731170654
I0502 16:16:12.347296 139945499956992 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.013892650604248, loss=1.8853634595870972
I0502 16:17:33.761056 139945491564288 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.050941824913025, loss=1.879528522491455
I0502 16:18:55.328231 139945499956992 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9689760208129883, loss=1.8928345441818237
I0502 16:20:15.693022 139945499956992 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.1593780517578125, loss=1.8205716609954834
I0502 16:21:31.936525 139945491564288 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.058280348777771, loss=1.8074195384979248
I0502 16:22:47.868079 139945499956992 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.164070725440979, loss=1.8378641605377197
I0502 16:24:03.815503 139945491564288 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.4683740139007568, loss=1.8792388439178467
I0502 16:25:19.844687 139945499956992 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9733650088310242, loss=1.8594294786453247
I0502 16:26:35.800981 139945491564288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9085890650749207, loss=1.8255894184112549
I0502 16:27:51.768888 139945499956992 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.360223650932312, loss=1.8133310079574585
I0502 16:29:13.958091 139945491564288 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1282250881195068, loss=1.8245294094085693
I0502 16:30:37.168608 139945499956992 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0687958002090454, loss=1.795092225074768
I0502 16:31:58.976441 139945491564288 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.1918978691101074, loss=1.791806697845459
I0502 16:33:22.360157 139945827636992 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.0750699043273926, loss=1.7925918102264404
I0502 16:34:35.698044 140118975928128 spec.py:298] Evaluating on the training split.
I0502 16:35:12.642596 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 16:35:51.605532 140118975928128 spec.py:326] Evaluating on the test split.
I0502 16:36:11.988524 140118975928128 submission_runner.py:415] Time since start: 12656.82s, 	Step: 15598, 	{'train/ctc_loss': DeviceArray(0.5310502, dtype=float32), 'train/wer': 0.17813498880166997, 'validation/ctc_loss': DeviceArray(0.9022625, dtype=float32), 'validation/wer': 0.2552750147131183, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6096567, dtype=float32), 'test/wer': 0.1887351979363435, 'test/num_examples': 2472, 'score': 12082.423231601715, 'total_duration': 12656.818460226059, 'accumulated_submission_time': 12082.423231601715, 'accumulated_eval_time': 573.9716050624847, 'accumulated_logging_time': 0.18034625053405762}
I0502 16:36:12.012096 139945827636992 logging_writer.py:48] [15598] accumulated_eval_time=573.971605, accumulated_logging_time=0.180346, accumulated_submission_time=12082.423232, global_step=15598, preemption_count=0, score=12082.423232, test/ctc_loss=0.6096566915512085, test/num_examples=2472, test/wer=0.188735, total_duration=12656.818460, train/ctc_loss=0.5310502052307129, train/wer=0.178135, validation/ctc_loss=0.9022625088691711, validation/num_examples=5348, validation/wer=0.255275
I0502 16:36:14.378514 139945819244288 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0459002256393433, loss=1.7552589178085327
I0502 16:37:30.266782 139945827636992 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.6668823957443237, loss=1.808325171470642
I0502 16:38:46.216980 139945819244288 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.7750766277313232, loss=1.7964651584625244
I0502 16:40:02.132174 139945827636992 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.4328409433364868, loss=1.8064382076263428
I0502 16:41:18.125326 139945819244288 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.01320219039917, loss=1.7526965141296387
I0502 16:42:34.095495 139945827636992 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.2292040586471558, loss=1.71810781955719
I0502 16:43:50.009418 139945819244288 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0498685836791992, loss=1.73141348361969
I0502 16:45:09.556514 139945827636992 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.3204957246780396, loss=1.8299124240875244
I0502 16:46:29.970321 139945819244288 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0626357793807983, loss=1.7491146326065063
I0502 16:47:53.432762 139945499956992 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.2521662712097168, loss=1.7994893789291382
I0502 16:49:09.409371 139945491564288 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.443782925605774, loss=1.7107667922973633
I0502 16:50:25.367690 139945499956992 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.1830413341522217, loss=1.7439734935760498
I0502 16:51:41.371558 139945491564288 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.2345224618911743, loss=1.6955147981643677
I0502 16:52:57.452975 139945499956992 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.1400542259216309, loss=1.8136682510375977
I0502 16:54:13.415984 139945491564288 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1979916095733643, loss=1.704111099243164
I0502 16:55:29.447783 139945499956992 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4120514392852783, loss=1.7232822179794312
I0502 16:56:45.532248 139945491564288 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9708306193351746, loss=1.6676416397094727
I0502 16:58:01.775626 139945499956992 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.3732753992080688, loss=1.733818531036377
I0502 16:59:17.775769 139945491564288 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.1462295055389404, loss=1.6968226432800293
I0502 17:00:33.770682 139945499956992 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.123435616493225, loss=1.7224643230438232
I0502 17:01:53.573934 139945499956992 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6042749881744385, loss=1.7659225463867188
I0502 17:03:09.526490 139945491564288 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.1776587963104248, loss=1.7064812183380127
I0502 17:04:25.587471 139945499956992 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.0708600282669067, loss=1.6142938137054443
I0502 17:05:41.516913 139945491564288 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.3360627889633179, loss=1.6087281703948975
I0502 17:06:57.476074 139945499956992 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1875765323638916, loss=1.6555542945861816
I0502 17:08:13.518752 139945491564288 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.226649522781372, loss=1.731955885887146
I0502 17:09:29.791333 139945499956992 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.1026164293289185, loss=1.666133999824524
I0502 17:10:45.751069 139945491564288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9144680500030518, loss=1.7002840042114258
I0502 17:12:01.734549 139945499956992 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.109798789024353, loss=1.6501871347427368
I0502 17:13:17.765035 139945491564288 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0595017671585083, loss=1.6570954322814941
I0502 17:14:37.223095 139945827636992 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.1043695211410522, loss=1.6554356813430786
I0502 17:15:53.232498 139945819244288 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9771482348442078, loss=1.6577743291854858
I0502 17:16:12.089564 140118975928128 spec.py:298] Evaluating on the training split.
I0502 17:16:49.411381 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 17:17:27.622788 140118975928128 spec.py:326] Evaluating on the test split.
I0502 17:17:47.293656 140118975928128 submission_runner.py:415] Time since start: 15152.12s, 	Step: 18726, 	{'train/ctc_loss': DeviceArray(0.4676235, dtype=float32), 'train/wer': 0.15931630132674673, 'validation/ctc_loss': DeviceArray(0.8077485, dtype=float32), 'validation/wer': 0.22807745371397697, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5339928, dtype=float32), 'test/wer': 0.1670830540491134, 'test/num_examples': 2472, 'score': 14482.437268972397, 'total_duration': 15152.123973846436, 'accumulated_submission_time': 14482.437268972397, 'accumulated_eval_time': 669.1726400852203, 'accumulated_logging_time': 0.21997356414794922}
I0502 17:17:47.318260 139945827636992 logging_writer.py:48] [18726] accumulated_eval_time=669.172640, accumulated_logging_time=0.219974, accumulated_submission_time=14482.437269, global_step=18726, preemption_count=0, score=14482.437269, test/ctc_loss=0.5339928269386292, test/num_examples=2472, test/wer=0.167083, total_duration=15152.123974, train/ctc_loss=0.4676235020160675, train/wer=0.159316, validation/ctc_loss=0.8077484965324402, validation/num_examples=5348, validation/wer=0.228077
I0502 17:18:44.222490 139945819244288 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.626585841178894, loss=1.7303171157836914
I0502 17:20:00.198474 139945827636992 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9474517107009888, loss=1.6302096843719482
I0502 17:21:16.147315 139945819244288 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.1163151264190674, loss=1.7397830486297607
I0502 17:22:32.348073 139945827636992 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0240522623062134, loss=1.6313722133636475
I0502 17:23:48.314711 139945819244288 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1004581451416016, loss=1.637064814567566
I0502 17:25:04.294052 139945827636992 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.075073480606079, loss=1.5507690906524658
I0502 17:26:20.195216 139945819244288 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0617262125015259, loss=1.6468724012374878
I0502 17:27:36.159350 139945827636992 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.519505262374878, loss=1.6188926696777344
I0502 17:28:55.793785 139945827636992 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0157235860824585, loss=1.629807710647583
I0502 17:30:11.802601 139945819244288 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9773093461990356, loss=1.5839673280715942
I0502 17:31:27.826312 139945827636992 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.171019434928894, loss=1.630173683166504
I0502 17:32:43.827580 139945819244288 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.4281388521194458, loss=1.704317569732666
I0502 17:33:58.988346 140118975928128 spec.py:298] Evaluating on the training split.
I0502 17:34:35.570641 140118975928128 spec.py:310] Evaluating on the validation split.
I0502 17:35:13.511054 140118975928128 spec.py:326] Evaluating on the test split.
I0502 17:35:33.318717 140118975928128 submission_runner.py:415] Time since start: 16218.15s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.41937006, dtype=float32), 'train/wer': 0.14531087146108523, 'validation/ctc_loss': DeviceArray(0.7742781, dtype=float32), 'validation/wer': 0.2185935223687638, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.50216633, dtype=float32), 'test/wer': 0.15751630004265432, 'test/num_examples': 2472, 'score': 15454.073668956757, 'total_duration': 16218.150212287903, 'accumulated_submission_time': 15454.073668956757, 'accumulated_eval_time': 763.5011303424835, 'accumulated_logging_time': 0.25862789154052734}
I0502 17:35:33.342589 139945981236992 logging_writer.py:48] [20000] accumulated_eval_time=763.501130, accumulated_logging_time=0.258628, accumulated_submission_time=15454.073669, global_step=20000, preemption_count=0, score=15454.073669, test/ctc_loss=0.5021663308143616, test/num_examples=2472, test/wer=0.157516, total_duration=16218.150212, train/ctc_loss=0.41937005519866943, train/wer=0.145311, validation/ctc_loss=0.7742781043052673, validation/num_examples=5348, validation/wer=0.218594
I0502 17:35:33.367125 139945972844288 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15454.073669
I0502 17:35:33.681473 140118975928128 checkpoints.py:356] Saving checkpoint at step: 20000
I0502 17:35:35.157936 140118975928128 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1/checkpoint_20000
I0502 17:35:35.191143 140118975928128 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_lamb/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0502 17:35:36.574764 140118975928128 submission_runner.py:578] Tuning trial 1/1
I0502 17:35:36.575021 140118975928128 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0502 17:35:36.581701 140118975928128 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(30.934254, dtype=float32), 'train/wer': 0.9820971201299378, 'validation/ctc_loss': DeviceArray(30.049446, dtype=float32), 'validation/wer': 0.9543845092572046, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.041254, dtype=float32), 'test/wer': 0.9716247232547275, 'test/num_examples': 2472, 'score': 80.29468321800232, 'total_duration': 197.35642838478088, 'accumulated_submission_time': 80.29468321800232, 'accumulated_eval_time': 117.06159496307373, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3091, {'train/ctc_loss': DeviceArray(6.1198583, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.3290195, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.242953, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2481.2242801189423, 'total_duration': 2676.789923429489, 'accumulated_submission_time': 2481.2242801189423, 'accumulated_eval_time': 195.48413372039795, 'accumulated_logging_time': 0.03548765182495117, 'global_step': 3091, 'preemption_count': 0}), (6228, {'train/ctc_loss': DeviceArray(2.9235482, dtype=float32), 'train/wer': 0.6300029299736303, 'validation/ctc_loss': DeviceArray(3.1920831, dtype=float32), 'validation/wer': 0.6483998880838213, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.9425929, dtype=float32), 'test/wer': 0.6137956248857473, 'test/num_examples': 2472, 'score': 4881.678156852722, 'total_duration': 5169.994038581848, 'accumulated_submission_time': 4881.678156852722, 'accumulated_eval_time': 288.1514732837677, 'accumulated_logging_time': 0.07079219818115234, 'global_step': 6228, 'preemption_count': 0}), (9368, {'train/ctc_loss': DeviceArray(1.0552036, dtype=float32), 'train/wer': 0.31992346252984805, 'validation/ctc_loss': DeviceArray(1.4594338, dtype=float32), 'validation/wer': 0.38334185568601725, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.1347238, dtype=float32), 'test/wer': 0.321532305567404, 'test/num_examples': 2472, 'score': 7281.847655534744, 'total_duration': 7664.994225263596, 'accumulated_submission_time': 7281.847655534744, 'accumulated_eval_time': 382.8964591026306, 'accumulated_logging_time': 0.10547590255737305, 'global_step': 9368, 'preemption_count': 0}), (12510, {'train/ctc_loss': DeviceArray(0.6890884, dtype=float32), 'train/wer': 0.22219836839845428, 'validation/ctc_loss': DeviceArray(1.0650474, dtype=float32), 'validation/wer': 0.2929984852724098, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7560169, dtype=float32), 'test/wer': 0.23053642881806918, 'test/num_examples': 2472, 'score': 9682.014013767242, 'total_duration': 10160.03490281105, 'accumulated_submission_time': 9682.014013767242, 'accumulated_eval_time': 477.68458008766174, 'accumulated_logging_time': 0.14229106903076172, 'global_step': 12510, 'preemption_count': 0}), (15598, {'train/ctc_loss': DeviceArray(0.5310502, dtype=float32), 'train/wer': 0.17813498880166997, 'validation/ctc_loss': DeviceArray(0.9022625, dtype=float32), 'validation/wer': 0.2552750147131183, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6096567, dtype=float32), 'test/wer': 0.1887351979363435, 'test/num_examples': 2472, 'score': 12082.423231601715, 'total_duration': 12656.818460226059, 'accumulated_submission_time': 12082.423231601715, 'accumulated_eval_time': 573.9716050624847, 'accumulated_logging_time': 0.18034625053405762, 'global_step': 15598, 'preemption_count': 0}), (18726, {'train/ctc_loss': DeviceArray(0.4676235, dtype=float32), 'train/wer': 0.15931630132674673, 'validation/ctc_loss': DeviceArray(0.8077485, dtype=float32), 'validation/wer': 0.22807745371397697, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5339928, dtype=float32), 'test/wer': 0.1670830540491134, 'test/num_examples': 2472, 'score': 14482.437268972397, 'total_duration': 15152.123973846436, 'accumulated_submission_time': 14482.437268972397, 'accumulated_eval_time': 669.1726400852203, 'accumulated_logging_time': 0.21997356414794922, 'global_step': 18726, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.41937006, dtype=float32), 'train/wer': 0.14531087146108523, 'validation/ctc_loss': DeviceArray(0.7742781, dtype=float32), 'validation/wer': 0.2185935223687638, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.50216633, dtype=float32), 'test/wer': 0.15751630004265432, 'test/num_examples': 2472, 'score': 15454.073668956757, 'total_duration': 16218.150212287903, 'accumulated_submission_time': 15454.073668956757, 'accumulated_eval_time': 763.5011303424835, 'accumulated_logging_time': 0.25862789154052734, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0502 17:35:36.581921 140118975928128 submission_runner.py:581] Timing: 15454.073668956757
I0502 17:35:36.581988 140118975928128 submission_runner.py:582] ====================
I0502 17:35:36.582626 140118975928128 submission_runner.py:645] Final librispeech_conformer score: 15454.073668956757
