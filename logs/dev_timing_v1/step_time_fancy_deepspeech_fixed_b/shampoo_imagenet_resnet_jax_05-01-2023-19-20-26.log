python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/shampoo/jax/submission.py --tuning_search_space=baselines/shampoo/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_shampoo --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_05-01-2023-19-20-26.log
I0501 19:20:48.869476 139795264595776 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax.
I0501 19:20:48.975681 139795264595776 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 19:20:49.848929 139795264595776 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0501 19:20:49.849493 139795264595776 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 19:20:49.853239 139795264595776 submission_runner.py:538] Using RNG seed 493750475
I0501 19:20:52.429991 139795264595776 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 19:20:52.430217 139795264595776 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1.
I0501 19:20:52.430518 139795264595776 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1/hparams.json.
I0501 19:20:52.553514 139795264595776 submission_runner.py:241] Initializing dataset.
I0501 19:20:52.575250 139795264595776 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:20:52.588676 139795264595776 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:20:52.588782 139795264595776 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:20:52.850085 139795264595776 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:20:53.922096 139795264595776 submission_runner.py:248] Initializing model.
I0501 19:21:05.455935 139795264595776 submission_runner.py:258] Initializing optimizer.
I0501 19:21:20.401264 139795264595776 submission_runner.py:265] Initializing metrics bundle.
I0501 19:21:20.401457 139795264595776 submission_runner.py:282] Initializing checkpoint and logger.
I0501 19:21:20.402434 139795264595776 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0501 19:21:21.119896 139795264595776 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0501 19:21:21.120779 139795264595776 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1/flags_0.json.
I0501 19:21:21.125413 139795264595776 submission_runner.py:318] Starting training loop.
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:812: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:813: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)
/algorithmic-efficiency/baselines/shampoo/jax/distributed_shampoo.py:814: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in eye is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)
I0501 19:24:25.370571 139617621759744 logging_writer.py:48] [0] global_step=0, grad_norm=0.6001937389373779, loss=6.932651519775391
I0501 19:24:25.436400 139795264595776 spec.py:298] Evaluating on the training split.
I0501 19:24:25.932067 139795264595776 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:24:25.938809 139795264595776 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:24:25.938927 139795264595776 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:24:25.998974 139795264595776 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:24:37.875521 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 19:24:38.602655 139795264595776 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:24:38.633316 139795264595776 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:24:38.633585 139795264595776 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:24:38.682675 139795264595776 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:24:57.078285 139795264595776 spec.py:326] Evaluating on the test split.
I0501 19:24:57.539444 139795264595776 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:24:57.544228 139795264595776 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0501 19:24:57.574360 139795264595776 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:25:07.312267 139795264595776 submission_runner.py:415] Time since start: 226.19s, 	Step: 1, 	{'train/accuracy': 0.0007573341717943549, 'train/loss': 6.912168979644775, 'validation/accuracy': 0.0010400000028312206, 'validation/loss': 6.912391662597656, 'validation/num_examples': 50000, 'test/accuracy': 0.0004000000189989805, 'test/loss': 6.911768913269043, 'test/num_examples': 10000, 'score': 184.3108127117157, 'total_duration': 226.18678855895996, 'accumulated_submission_time': 184.3108127117157, 'accumulated_eval_time': 41.87582302093506, 'accumulated_logging_time': 0}
I0501 19:25:07.328732 139588676855552 logging_writer.py:48] [1] accumulated_eval_time=41.875823, accumulated_logging_time=0, accumulated_submission_time=184.310813, global_step=1, preemption_count=0, score=184.310813, test/accuracy=0.000400, test/loss=6.911769, test/num_examples=10000, total_duration=226.186789, train/accuracy=0.000757, train/loss=6.912169, validation/accuracy=0.001040, validation/loss=6.912392, validation/num_examples=50000
I0501 19:26:16.416236 139588685248256 logging_writer.py:48] [100] global_step=100, grad_norm=0.6576266288757324, loss=6.7320027351379395
I0501 19:27:25.973606 139588676855552 logging_writer.py:48] [200] global_step=200, grad_norm=0.7368889451026917, loss=6.537350177764893
I0501 19:28:35.736994 139588685248256 logging_writer.py:48] [300] global_step=300, grad_norm=0.8015002608299255, loss=6.344843864440918
I0501 19:29:45.941324 139588676855552 logging_writer.py:48] [400] global_step=400, grad_norm=1.1271523237228394, loss=6.145756244659424
I0501 19:30:56.724991 139588685248256 logging_writer.py:48] [500] global_step=500, grad_norm=1.2200746536254883, loss=6.027814865112305
I0501 19:32:07.413043 139588676855552 logging_writer.py:48] [600] global_step=600, grad_norm=1.7171481847763062, loss=5.852299213409424
I0501 19:33:18.185760 139588685248256 logging_writer.py:48] [700] global_step=700, grad_norm=1.8988735675811768, loss=5.654749870300293
I0501 19:33:37.916833 139795264595776 spec.py:298] Evaluating on the training split.
I0501 19:33:44.831278 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 19:33:52.539215 139795264595776 spec.py:326] Evaluating on the test split.
I0501 19:33:54.856222 139795264595776 submission_runner.py:415] Time since start: 753.73s, 	Step: 731, 	{'train/accuracy': 0.08153299987316132, 'train/loss': 5.262387752532959, 'validation/accuracy': 0.07642000168561935, 'validation/loss': 5.31176233291626, 'validation/num_examples': 50000, 'test/accuracy': 0.053700003772974014, 'test/loss': 5.564613342285156, 'test/num_examples': 10000, 'score': 694.8820705413818, 'total_duration': 753.7307419776917, 'accumulated_submission_time': 694.8820705413818, 'accumulated_eval_time': 58.81516718864441, 'accumulated_logging_time': 0.02440667152404785}
I0501 19:33:54.865387 139589012399872 logging_writer.py:48] [731] accumulated_eval_time=58.815167, accumulated_logging_time=0.024407, accumulated_submission_time=694.882071, global_step=731, preemption_count=0, score=694.882071, test/accuracy=0.053700, test/loss=5.564613, test/num_examples=10000, total_duration=753.730742, train/accuracy=0.081533, train/loss=5.262388, validation/accuracy=0.076420, validation/loss=5.311762, validation/num_examples=50000
I0501 19:34:45.802516 139589020792576 logging_writer.py:48] [800] global_step=800, grad_norm=2.504708766937256, loss=5.646332740783691
I0501 19:35:56.741897 139589012399872 logging_writer.py:48] [900] global_step=900, grad_norm=1.9780621528625488, loss=5.519179821014404
I0501 19:37:07.712228 139589020792576 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.3026604652404785, loss=5.350064277648926
I0501 19:38:18.653326 139589012399872 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.3360915184020996, loss=5.412116050720215
I0501 19:39:30.200902 139589020792576 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.5198986530303955, loss=5.158904075622559
I0501 19:40:41.141278 139589012399872 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.078766345977783, loss=5.068478584289551
I0501 19:41:52.152139 139589020792576 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.093624114990234, loss=5.015265941619873
I0501 19:42:25.338827 139795264595776 spec.py:298] Evaluating on the training split.
I0501 19:42:32.237221 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 19:42:40.063091 139795264595776 spec.py:326] Evaluating on the test split.
I0501 19:42:42.118419 139795264595776 submission_runner.py:415] Time since start: 1280.99s, 	Step: 1450, 	{'train/accuracy': 0.19359852373600006, 'train/loss': 4.1333160400390625, 'validation/accuracy': 0.1640399992465973, 'validation/loss': 4.33566951751709, 'validation/num_examples': 50000, 'test/accuracy': 0.12090000510215759, 'test/loss': 4.774815082550049, 'test/num_examples': 10000, 'score': 1205.3396894931793, 'total_duration': 1280.9929327964783, 'accumulated_submission_time': 1205.3396894931793, 'accumulated_eval_time': 75.59471559524536, 'accumulated_logging_time': 0.0406954288482666}
I0501 19:42:42.127007 139589012399872 logging_writer.py:48] [1450] accumulated_eval_time=75.594716, accumulated_logging_time=0.040695, accumulated_submission_time=1205.339689, global_step=1450, preemption_count=0, score=1205.339689, test/accuracy=0.120900, test/loss=4.774815, test/num_examples=10000, total_duration=1280.992933, train/accuracy=0.193599, train/loss=4.133316, validation/accuracy=0.164040, validation/loss=4.335670, validation/num_examples=50000
I0501 19:43:19.838582 139589020792576 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.030552864074707, loss=4.945804119110107
I0501 19:44:30.268525 139589012399872 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.071347236633301, loss=4.919891357421875
I0501 19:45:41.242027 139589020792576 logging_writer.py:48] [1700] global_step=1700, grad_norm=5.898217678070068, loss=4.8971781730651855
I0501 19:46:52.243467 139589012399872 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.2196462154388428, loss=4.706598281860352
I0501 19:48:03.337570 139589020792576 logging_writer.py:48] [1900] global_step=1900, grad_norm=7.041833400726318, loss=4.6498260498046875
I0501 19:49:14.328616 139589012399872 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.3128156661987305, loss=4.626898288726807
I0501 19:50:26.022693 139589020792576 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.911755084991455, loss=4.5180792808532715
I0501 19:51:12.620152 139795264595776 spec.py:298] Evaluating on the training split.
I0501 19:51:19.418417 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 19:51:27.147309 139795264595776 spec.py:326] Evaluating on the test split.
I0501 19:51:29.461196 139795264595776 submission_runner.py:415] Time since start: 1808.34s, 	Step: 2168, 	{'train/accuracy': 0.2691127061843872, 'train/loss': 3.5797245502471924, 'validation/accuracy': 0.249439999461174, 'validation/loss': 3.703864812850952, 'validation/num_examples': 50000, 'test/accuracy': 0.17520001530647278, 'test/loss': 4.291595935821533, 'test/num_examples': 10000, 'score': 1715.8171577453613, 'total_duration': 1808.3357088565826, 'accumulated_submission_time': 1715.8171577453613, 'accumulated_eval_time': 92.43571162223816, 'accumulated_logging_time': 0.05651593208312988}
I0501 19:51:29.469591 139589012399872 logging_writer.py:48] [2168] accumulated_eval_time=92.435712, accumulated_logging_time=0.056516, accumulated_submission_time=1715.817158, global_step=2168, preemption_count=0, score=1715.817158, test/accuracy=0.175200, test/loss=4.291596, test/num_examples=10000, total_duration=1808.335709, train/accuracy=0.269113, train/loss=3.579725, validation/accuracy=0.249440, validation/loss=3.703865, validation/num_examples=50000
I0501 19:51:54.238039 139589020792576 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.413649082183838, loss=4.43796443939209
I0501 19:53:05.271984 139589012399872 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.577150821685791, loss=4.455178737640381
I0501 19:54:16.568079 139589020792576 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.81317663192749, loss=4.299037456512451
I0501 19:55:28.218429 139589012399872 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.375530242919922, loss=4.296254634857178
I0501 19:56:39.956340 139589020792576 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.372988700866699, loss=4.265995025634766
I0501 19:57:51.004821 139589012399872 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.90030837059021, loss=4.148520469665527
I0501 19:59:02.146900 139589020792576 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.258981227874756, loss=4.180501461029053
I0501 19:59:59.873427 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:00:06.688559 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:00:14.430685 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:00:16.693643 139795264595776 submission_runner.py:415] Time since start: 2335.57s, 	Step: 2881, 	{'train/accuracy': 0.35738199949264526, 'train/loss': 2.994899034500122, 'validation/accuracy': 0.321260005235672, 'validation/loss': 3.2152562141418457, 'validation/num_examples': 50000, 'test/accuracy': 0.2337000072002411, 'test/loss': 3.820979118347168, 'test/num_examples': 10000, 'score': 2226.204894065857, 'total_duration': 2335.5681705474854, 'accumulated_submission_time': 2226.204894065857, 'accumulated_eval_time': 109.25591373443604, 'accumulated_logging_time': 0.07246899604797363}
I0501 20:00:16.701980 139589012399872 logging_writer.py:48] [2881] accumulated_eval_time=109.255914, accumulated_logging_time=0.072469, accumulated_submission_time=2226.204894, global_step=2881, preemption_count=0, score=2226.204894, test/accuracy=0.233700, test/loss=3.820979, test/num_examples=10000, total_duration=2335.568171, train/accuracy=0.357382, train/loss=2.994899, validation/accuracy=0.321260, validation/loss=3.215256, validation/num_examples=50000
I0501 20:00:31.051490 139589020792576 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.97373366355896, loss=4.159362316131592
I0501 20:01:43.034465 139589012399872 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.225933074951172, loss=4.037346363067627
I0501 20:02:54.730216 139589020792576 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.604868173599243, loss=3.995783805847168
I0501 20:04:07.081121 139589012399872 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.084404706954956, loss=4.026461124420166
I0501 20:05:19.042177 139589020792576 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.741223096847534, loss=4.0284576416015625
I0501 20:06:30.811579 139589012399872 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.7054688930511475, loss=3.8141326904296875
I0501 20:07:42.628771 139589020792576 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.6684722900390625, loss=3.795149803161621
I0501 20:08:47.233358 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:08:54.080919 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:09:01.936311 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:09:04.132276 139795264595776 submission_runner.py:415] Time since start: 2863.01s, 	Step: 3593, 	{'train/accuracy': 0.4160754084587097, 'train/loss': 2.6838724613189697, 'validation/accuracy': 0.39023998379707336, 'validation/loss': 2.8285982608795166, 'validation/num_examples': 50000, 'test/accuracy': 0.29350000619888306, 'test/loss': 3.4642977714538574, 'test/num_examples': 10000, 'score': 2736.7196679115295, 'total_duration': 2863.0068032741547, 'accumulated_submission_time': 2736.7196679115295, 'accumulated_eval_time': 126.15480351448059, 'accumulated_logging_time': 0.08878445625305176}
I0501 20:09:04.140466 139589012399872 logging_writer.py:48] [3593] accumulated_eval_time=126.154804, accumulated_logging_time=0.088784, accumulated_submission_time=2736.719668, global_step=3593, preemption_count=0, score=2736.719668, test/accuracy=0.293500, test/loss=3.464298, test/num_examples=10000, total_duration=2863.006803, train/accuracy=0.416075, train/loss=2.683872, validation/accuracy=0.390240, validation/loss=2.828598, validation/num_examples=50000
I0501 20:09:11.689667 139589020792576 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.064855098724365, loss=3.8060739040374756
I0501 20:10:22.991600 139589012399872 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.4618234634399414, loss=3.809584617614746
I0501 20:11:34.245442 139589020792576 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.047550916671753, loss=3.8295347690582275
I0501 20:12:45.458136 139589012399872 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.8223233222961426, loss=3.7326955795288086
I0501 20:13:56.652441 139589020792576 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.4913182258605957, loss=3.689147710800171
I0501 20:15:08.478909 139589012399872 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.4539988040924072, loss=3.7213070392608643
I0501 20:16:19.737270 139589020792576 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.803069829940796, loss=3.6634249687194824
I0501 20:17:31.226068 139589012399872 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.4318954944610596, loss=3.6893279552459717
I0501 20:17:34.478269 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:17:41.438959 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:17:49.964405 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:17:51.863306 139795264595776 submission_runner.py:415] Time since start: 3390.74s, 	Step: 4307, 	{'train/accuracy': 0.48698580265045166, 'train/loss': 2.318234443664551, 'validation/accuracy': 0.43751999735832214, 'validation/loss': 2.569716691970825, 'validation/num_examples': 50000, 'test/accuracy': 0.33010002970695496, 'test/loss': 3.2452661991119385, 'test/num_examples': 10000, 'score': 3247.041827440262, 'total_duration': 3390.7378237247467, 'accumulated_submission_time': 3247.041827440262, 'accumulated_eval_time': 143.53979754447937, 'accumulated_logging_time': 0.10400032997131348}
I0501 20:17:51.879028 139589020792576 logging_writer.py:48] [4307] accumulated_eval_time=143.539798, accumulated_logging_time=0.104000, accumulated_submission_time=3247.041827, global_step=4307, preemption_count=0, score=3247.041827, test/accuracy=0.330100, test/loss=3.245266, test/num_examples=10000, total_duration=3390.737824, train/accuracy=0.486986, train/loss=2.318234, validation/accuracy=0.437520, validation/loss=2.569717, validation/num_examples=50000
I0501 20:19:02.638934 139589012399872 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.3293139934539795, loss=3.6664130687713623
I0501 20:20:14.326606 139589020792576 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.4691572189331055, loss=3.5980710983276367
I0501 20:21:25.795393 139589012399872 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.8866443634033203, loss=3.533731460571289
I0501 20:22:37.718715 139589020792576 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.0902137756347656, loss=3.4700284004211426
I0501 20:23:49.034428 139589012399872 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.5831141471862793, loss=3.4397637844085693
I0501 20:25:00.909729 139589020792576 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.556445598602295, loss=3.4634315967559814
I0501 20:26:12.525934 139589012399872 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.6084728240966797, loss=3.3722407817840576
I0501 20:26:27.185761 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:26:34.105913 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:26:42.690707 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:26:44.781977 139795264595776 submission_runner.py:415] Time since start: 3923.66s, 	Step: 5021, 	{'train/accuracy': 0.500996470451355, 'train/loss': 2.2706990242004395, 'validation/accuracy': 0.4649199843406677, 'validation/loss': 2.4524424076080322, 'validation/num_examples': 50000, 'test/accuracy': 0.3525000214576721, 'test/loss': 3.148200035095215, 'test/num_examples': 10000, 'score': 3759.9000754356384, 'total_duration': 3923.6564865112305, 'accumulated_submission_time': 3759.9000754356384, 'accumulated_eval_time': 161.13597583770752, 'accumulated_logging_time': 2.5595126152038574}
I0501 20:26:44.790803 139589020792576 logging_writer.py:48] [5021] accumulated_eval_time=161.135976, accumulated_logging_time=2.559513, accumulated_submission_time=3759.900075, global_step=5021, preemption_count=0, score=3759.900075, test/accuracy=0.352500, test/loss=3.148200, test/num_examples=10000, total_duration=3923.656487, train/accuracy=0.500996, train/loss=2.270699, validation/accuracy=0.464920, validation/loss=2.452442, validation/num_examples=50000
I0501 20:27:42.119423 139589012399872 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.6203773021698, loss=3.4066100120544434
I0501 20:28:54.565016 139589020792576 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.0121164321899414, loss=3.3635380268096924
I0501 20:30:06.403163 139589012399872 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.314784526824951, loss=3.519508123397827
I0501 20:31:18.514122 139589020792576 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9022682905197144, loss=3.2893121242523193
I0501 20:32:29.681834 139589012399872 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.9836424589157104, loss=3.2663705348968506
I0501 20:33:41.269519 139589020792576 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.901766300201416, loss=3.336620569229126
I0501 20:34:52.973728 139589012399872 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.834937334060669, loss=3.2506802082061768
I0501 20:35:14.885522 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:35:22.163830 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:35:31.294708 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:35:33.303957 139795264595776 submission_runner.py:415] Time since start: 4452.18s, 	Step: 5734, 	{'train/accuracy': 0.5507014989852905, 'train/loss': 1.9923250675201416, 'validation/accuracy': 0.5020999908447266, 'validation/loss': 2.2343759536743164, 'validation/num_examples': 50000, 'test/accuracy': 0.38600000739097595, 'test/loss': 2.9205596446990967, 'test/num_examples': 10000, 'score': 4269.978981494904, 'total_duration': 4452.178487062454, 'accumulated_submission_time': 4269.978981494904, 'accumulated_eval_time': 179.55438876152039, 'accumulated_logging_time': 2.5754308700561523}
I0501 20:35:33.312364 139589020792576 logging_writer.py:48] [5734] accumulated_eval_time=179.554389, accumulated_logging_time=2.575431, accumulated_submission_time=4269.978981, global_step=5734, preemption_count=0, score=4269.978981, test/accuracy=0.386000, test/loss=2.920560, test/num_examples=10000, total_duration=4452.178487, train/accuracy=0.550701, train/loss=1.992325, validation/accuracy=0.502100, validation/loss=2.234376, validation/num_examples=50000
I0501 20:36:23.401136 139589012399872 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.776349425315857, loss=3.15639591217041
I0501 20:37:35.183479 139589020792576 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.7956135272979736, loss=3.2756242752075195
I0501 20:38:46.601197 139589012399872 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.8040798902511597, loss=3.166276216506958
I0501 20:39:58.065199 139589020792576 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.1638197898864746, loss=3.233832597732544
I0501 20:41:09.284657 139589012399872 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.0184555053710938, loss=3.154188632965088
I0501 20:42:20.550402 139589020792576 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.14516019821167, loss=3.2057833671569824
I0501 20:43:31.837080 139589012399872 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.3439712524414062, loss=3.2493581771850586
I0501 20:44:03.560474 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:44:10.486537 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:44:19.718195 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:44:21.698205 139795264595776 submission_runner.py:415] Time since start: 4980.57s, 	Step: 6447, 	{'train/accuracy': 0.5715082883834839, 'train/loss': 1.9059982299804688, 'validation/accuracy': 0.5346999764442444, 'validation/loss': 2.0963432788848877, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.7739269733428955, 'test/num_examples': 10000, 'score': 4780.21021771431, 'total_duration': 4980.572717428207, 'accumulated_submission_time': 4780.21021771431, 'accumulated_eval_time': 197.69207096099854, 'accumulated_logging_time': 2.591991662979126}
I0501 20:44:21.707226 139589020792576 logging_writer.py:48] [6447] accumulated_eval_time=197.692071, accumulated_logging_time=2.591992, accumulated_submission_time=4780.210218, global_step=6447, preemption_count=0, score=4780.210218, test/accuracy=0.418800, test/loss=2.773927, test/num_examples=10000, total_duration=4980.572717, train/accuracy=0.571508, train/loss=1.905998, validation/accuracy=0.534700, validation/loss=2.096343, validation/num_examples=50000
I0501 20:45:01.153064 139589012399872 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.789528727531433, loss=3.383632183074951
I0501 20:46:12.493670 139589020792576 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.355818510055542, loss=3.230273485183716
I0501 20:47:23.772799 139589012399872 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.9317288398742676, loss=3.092804431915283
I0501 20:48:35.032897 139589020792576 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.2641081809997559, loss=3.052060127258301
I0501 20:49:46.664740 139589012399872 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.5390080213546753, loss=3.1545612812042236
I0501 20:50:58.926038 139589020792576 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.2568939924240112, loss=3.0456559658050537
I0501 20:52:10.309428 139589012399872 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.256549596786499, loss=3.1059961318969727
I0501 20:52:53.115814 139795264595776 spec.py:298] Evaluating on the training split.
I0501 20:53:00.183274 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 20:53:09.863432 139795264595776 spec.py:326] Evaluating on the test split.
I0501 20:53:11.941399 139795264595776 submission_runner.py:415] Time since start: 5510.82s, 	Step: 7161, 	{'train/accuracy': 0.6029974222183228, 'train/loss': 1.7656290531158447, 'validation/accuracy': 0.5460000038146973, 'validation/loss': 2.027294635772705, 'validation/num_examples': 50000, 'test/accuracy': 0.42500001192092896, 'test/loss': 2.7199454307556152, 'test/num_examples': 10000, 'score': 5291.603226661682, 'total_duration': 5510.81591963768, 'accumulated_submission_time': 5291.603226661682, 'accumulated_eval_time': 216.5176374912262, 'accumulated_logging_time': 2.607814311981201}
I0501 20:53:11.950398 139589020792576 logging_writer.py:48] [7161] accumulated_eval_time=216.517637, accumulated_logging_time=2.607814, accumulated_submission_time=5291.603227, global_step=7161, preemption_count=0, score=5291.603227, test/accuracy=0.425000, test/loss=2.719945, test/num_examples=10000, total_duration=5510.815920, train/accuracy=0.602997, train/loss=1.765629, validation/accuracy=0.546000, validation/loss=2.027295, validation/num_examples=50000
I0501 20:53:40.354106 139589012399872 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.8112692832946777, loss=3.1019983291625977
I0501 20:54:51.676658 139589020792576 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.791365623474121, loss=3.098602533340454
I0501 20:56:02.861205 139589012399872 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.6851810216903687, loss=3.107409715652466
I0501 20:57:13.978337 139589020792576 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.7435568571090698, loss=3.0844478607177734
I0501 20:58:25.181041 139589012399872 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.218239665031433, loss=2.951575994491577
I0501 20:59:36.135182 139589020792576 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.212990403175354, loss=2.9884417057037354
I0501 21:00:47.509200 139589012399872 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.0631619691848755, loss=3.0698747634887695
I0501 21:01:44.570823 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:01:51.678024 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:02:01.624442 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:02:03.846920 139795264595776 submission_runner.py:415] Time since start: 6042.72s, 	Step: 7881, 	{'train/accuracy': 0.6047911047935486, 'train/loss': 1.7481797933578491, 'validation/accuracy': 0.5658400058746338, 'validation/loss': 1.9366120100021362, 'validation/num_examples': 50000, 'test/accuracy': 0.43630000948905945, 'test/loss': 2.648446559906006, 'test/num_examples': 10000, 'score': 5804.2067885398865, 'total_duration': 6042.720299959183, 'accumulated_submission_time': 5804.2067885398865, 'accumulated_eval_time': 235.79255294799805, 'accumulated_logging_time': 2.625305652618408}
I0501 21:02:03.856151 139589020792576 logging_writer.py:48] [7881] accumulated_eval_time=235.792553, accumulated_logging_time=2.625306, accumulated_submission_time=5804.206789, global_step=7881, preemption_count=0, score=5804.206789, test/accuracy=0.436300, test/loss=2.648447, test/num_examples=10000, total_duration=6042.720300, train/accuracy=0.604791, train/loss=1.748180, validation/accuracy=0.565840, validation/loss=1.936612, validation/num_examples=50000
I0501 21:02:18.083044 139589012399872 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.2215044498443604, loss=3.1377878189086914
I0501 21:03:29.268841 139589020792576 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.0830695629119873, loss=3.010012149810791
I0501 21:04:41.223731 139589012399872 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0366696119308472, loss=3.019407272338867
I0501 21:05:52.308200 139589020792576 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0024625062942505, loss=2.8575637340545654
I0501 21:07:03.488209 139589012399872 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0837537050247192, loss=2.99804949760437
I0501 21:08:15.239516 139589020792576 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.7217600345611572, loss=3.049041271209717
I0501 21:09:26.315521 139589012399872 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.3450995683670044, loss=2.9513468742370605
I0501 21:10:37.301909 139589020792576 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.4397753477096558, loss=2.9044697284698486
I0501 21:10:37.335467 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:10:44.782491 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:10:54.830884 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:10:57.040485 139795264595776 submission_runner.py:415] Time since start: 6575.92s, 	Step: 8601, 	{'train/accuracy': 0.6332708597183228, 'train/loss': 1.6400469541549683, 'validation/accuracy': 0.5717999935150146, 'validation/loss': 1.9243814945220947, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.608710289001465, 'test/num_examples': 10000, 'score': 6317.669976472855, 'total_duration': 6575.915004253387, 'accumulated_submission_time': 6317.669976472855, 'accumulated_eval_time': 255.49750328063965, 'accumulated_logging_time': 2.642008066177368}
I0501 21:10:57.048738 139589012399872 logging_writer.py:48] [8601] accumulated_eval_time=255.497503, accumulated_logging_time=2.642008, accumulated_submission_time=6317.669976, global_step=8601, preemption_count=0, score=6317.669976, test/accuracy=0.444500, test/loss=2.608710, test/num_examples=10000, total_duration=6575.915004, train/accuracy=0.633271, train/loss=1.640047, validation/accuracy=0.571800, validation/loss=1.924381, validation/num_examples=50000
I0501 21:12:07.971414 139589020792576 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.3102208375930786, loss=2.9332387447357178
I0501 21:13:19.609344 139589012399872 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.3471741676330566, loss=2.919220209121704
I0501 21:14:31.001129 139589020792576 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.523522973060608, loss=3.016033411026001
I0501 21:15:42.077704 139589012399872 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4116195440292358, loss=2.863455057144165
I0501 21:16:53.769143 139589020792576 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1201863288879395, loss=2.9058265686035156
I0501 21:18:05.102956 139589012399872 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.046638011932373, loss=2.9468212127685547
I0501 21:19:16.335007 139589020792576 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.021770715713501, loss=2.8460559844970703
I0501 21:19:30.622271 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:19:37.883548 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:19:47.923170 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:19:50.143567 139795264595776 submission_runner.py:415] Time since start: 7109.02s, 	Step: 9321, 	{'train/accuracy': 0.6429169178009033, 'train/loss': 1.5778065919876099, 'validation/accuracy': 0.5904399752616882, 'validation/loss': 1.8167951107025146, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.4893362522125244, 'test/num_examples': 10000, 'score': 6831.228151082993, 'total_duration': 7109.0167760849, 'accumulated_submission_time': 6831.228151082993, 'accumulated_eval_time': 275.0174503326416, 'accumulated_logging_time': 2.657024621963501}
I0501 21:19:50.152074 139589012399872 logging_writer.py:48] [9321] accumulated_eval_time=275.017450, accumulated_logging_time=2.657025, accumulated_submission_time=6831.228151, global_step=9321, preemption_count=0, score=6831.228151, test/accuracy=0.469000, test/loss=2.489336, test/num_examples=10000, total_duration=7109.016776, train/accuracy=0.642917, train/loss=1.577807, validation/accuracy=0.590440, validation/loss=1.816795, validation/num_examples=50000
I0501 21:20:47.045603 139589020792576 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8667845726013184, loss=2.8531150817871094
I0501 21:21:58.306940 139589012399872 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.0556089878082275, loss=2.8546204566955566
I0501 21:23:09.919868 139589020792576 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.0307799577713013, loss=2.9526896476745605
I0501 21:24:21.533149 139589012399872 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.0739028453826904, loss=2.8864059448242188
I0501 21:25:32.740333 139589020792576 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9495299458503723, loss=2.8380794525146484
I0501 21:26:43.638048 139589012399872 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.2197822332382202, loss=2.8405063152313232
I0501 21:27:54.781077 139589020792576 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8924848437309265, loss=2.8449974060058594
I0501 21:28:23.356411 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:28:30.710096 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:28:40.574381 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:28:42.643179 139795264595776 submission_runner.py:415] Time since start: 7641.52s, 	Step: 10041, 	{'train/accuracy': 0.6679487824440002, 'train/loss': 1.4493895769119263, 'validation/accuracy': 0.6062600016593933, 'validation/loss': 1.7397924661636353, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.429551124572754, 'test/num_examples': 10000, 'score': 7344.415848016739, 'total_duration': 7641.517694711685, 'accumulated_submission_time': 7344.415848016739, 'accumulated_eval_time': 294.30417704582214, 'accumulated_logging_time': 2.673269271850586}
I0501 21:28:42.651928 139589012399872 logging_writer.py:48] [10041] accumulated_eval_time=294.304177, accumulated_logging_time=2.673269, accumulated_submission_time=7344.415848, global_step=10041, preemption_count=0, score=7344.415848, test/accuracy=0.481600, test/loss=2.429551, test/num_examples=10000, total_duration=7641.517695, train/accuracy=0.667949, train/loss=1.449390, validation/accuracy=0.606260, validation/loss=1.739792, validation/num_examples=50000
I0501 21:29:25.404417 139589020792576 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.1313605308532715, loss=2.7974798679351807
I0501 21:30:36.715277 139589012399872 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9737666249275208, loss=2.8497653007507324
I0501 21:31:47.648643 139589020792576 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.071226716041565, loss=2.799006938934326
I0501 21:32:59.289045 139589012399872 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.969479501247406, loss=2.8076677322387695
I0501 21:34:10.552206 139589020792576 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7962950468063354, loss=2.7871439456939697
I0501 21:35:22.108650 139589012399872 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7343506217002869, loss=2.7317378520965576
I0501 21:36:33.326011 139589020792576 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7343584895133972, loss=2.7662975788116455
I0501 21:37:16.076052 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:37:23.459501 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:37:33.751584 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:37:35.762260 139795264595776 submission_runner.py:415] Time since start: 8174.64s, 	Step: 10761, 	{'train/accuracy': 0.698640763759613, 'train/loss': 1.324961543083191, 'validation/accuracy': 0.6159999966621399, 'validation/loss': 1.691796064376831, 'validation/num_examples': 50000, 'test/accuracy': 0.4904000163078308, 'test/loss': 2.390176296234131, 'test/num_examples': 10000, 'score': 7857.824132204056, 'total_duration': 8174.63578915596, 'accumulated_submission_time': 7857.824132204056, 'accumulated_eval_time': 313.9893500804901, 'accumulated_logging_time': 2.689209461212158}
I0501 21:37:35.770809 139589012399872 logging_writer.py:48] [10761] accumulated_eval_time=313.989350, accumulated_logging_time=2.689209, accumulated_submission_time=7857.824132, global_step=10761, preemption_count=0, score=7857.824132, test/accuracy=0.490400, test/loss=2.390176, test/num_examples=10000, total_duration=8174.635789, train/accuracy=0.698641, train/loss=1.324962, validation/accuracy=0.616000, validation/loss=1.691796, validation/num_examples=50000
I0501 21:38:04.626807 139589020792576 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.1427502632141113, loss=2.7872395515441895
I0501 21:39:16.726895 139589012399872 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8808338046073914, loss=2.7628908157348633
I0501 21:40:29.312496 139589020792576 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7101786136627197, loss=2.8328678607940674
I0501 21:41:41.576920 139589012399872 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7354515194892883, loss=2.7166054248809814
I0501 21:42:53.759618 139589020792576 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7004207968711853, loss=2.570317029953003
I0501 21:44:05.996270 139589012399872 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.859247624874115, loss=2.7775025367736816
I0501 21:45:18.780300 139589020792576 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0876322984695435, loss=2.717963695526123
I0501 21:46:05.877254 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:46:13.307843 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:46:23.277318 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:46:25.533733 139795264595776 submission_runner.py:415] Time since start: 8704.41s, 	Step: 11468, 	{'train/accuracy': 0.69144606590271, 'train/loss': 1.3458995819091797, 'validation/accuracy': 0.6236400008201599, 'validation/loss': 1.6607420444488525, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3417680263519287, 'test/num_examples': 10000, 'score': 8367.91419506073, 'total_duration': 8704.408233642578, 'accumulated_submission_time': 8367.91419506073, 'accumulated_eval_time': 333.645770072937, 'accumulated_logging_time': 2.7056169509887695}
I0501 21:46:25.544502 139589012399872 logging_writer.py:48] [11468] accumulated_eval_time=333.645770, accumulated_logging_time=2.705617, accumulated_submission_time=8367.914195, global_step=11468, preemption_count=0, score=8367.914195, test/accuracy=0.491700, test/loss=2.341768, test/num_examples=10000, total_duration=8704.408234, train/accuracy=0.691446, train/loss=1.345900, validation/accuracy=0.623640, validation/loss=1.660742, validation/num_examples=50000
I0501 21:46:50.108736 139589020792576 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.954609751701355, loss=2.7807483673095703
I0501 21:48:01.560254 139589012399872 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8131862282752991, loss=2.8049588203430176
I0501 21:49:13.078178 139589020792576 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7923041582107544, loss=2.712789297103882
I0501 21:50:24.351773 139589012399872 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8925935626029968, loss=2.7022271156311035
I0501 21:51:35.427768 139589020792576 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9788234829902649, loss=2.7470576763153076
I0501 21:52:47.053332 139589012399872 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8383133411407471, loss=2.656646728515625
I0501 21:53:58.485987 139589020792576 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6902158856391907, loss=2.6738064289093018
I0501 21:54:55.857825 139795264595776 spec.py:298] Evaluating on the training split.
I0501 21:55:03.513871 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 21:55:13.501166 139795264595776 spec.py:326] Evaluating on the test split.
I0501 21:55:15.724884 139795264595776 submission_runner.py:415] Time since start: 9234.60s, 	Step: 12182, 	{'train/accuracy': 0.7506775856018066, 'train/loss': 1.0964970588684082, 'validation/accuracy': 0.6337400078773499, 'validation/loss': 1.611820101737976, 'validation/num_examples': 50000, 'test/accuracy': 0.5075000524520874, 'test/loss': 2.2831242084503174, 'test/num_examples': 10000, 'score': 8878.209555625916, 'total_duration': 9234.59941124916, 'accumulated_submission_time': 8878.209555625916, 'accumulated_eval_time': 353.5127999782562, 'accumulated_logging_time': 2.725825786590576}
I0501 21:55:15.733713 139589012399872 logging_writer.py:48] [12182] accumulated_eval_time=353.512800, accumulated_logging_time=2.725826, accumulated_submission_time=8878.209556, global_step=12182, preemption_count=0, score=8878.209556, test/accuracy=0.507500, test/loss=2.283124, test/num_examples=10000, total_duration=9234.599411, train/accuracy=0.750678, train/loss=1.096497, validation/accuracy=0.633740, validation/loss=1.611820, validation/num_examples=50000
I0501 21:55:29.403693 139589020792576 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8428474068641663, loss=2.7191052436828613
I0501 21:56:41.684381 139589012399872 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.804877758026123, loss=2.698485851287842
I0501 21:57:54.002283 139589020792576 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8844479918479919, loss=2.6987571716308594
I0501 21:59:06.800446 139589012399872 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7142149209976196, loss=2.6020796298980713
I0501 22:00:19.182102 139589020792576 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7665574550628662, loss=2.6680712699890137
I0501 22:01:31.550670 139589012399872 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5918959975242615, loss=2.5685219764709473
I0501 22:02:43.533354 139589020792576 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7318719625473022, loss=2.726318836212158
I0501 22:03:46.158257 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:03:53.956929 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:04:04.077494 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:04:06.022314 139795264595776 submission_runner.py:415] Time since start: 9764.90s, 	Step: 12889, 	{'train/accuracy': 0.7086256146430969, 'train/loss': 1.2864586114883423, 'validation/accuracy': 0.6391400098800659, 'validation/loss': 1.6044695377349854, 'validation/num_examples': 50000, 'test/accuracy': 0.5127000212669373, 'test/loss': 2.2629776000976562, 'test/num_examples': 10000, 'score': 9388.616441965103, 'total_duration': 9764.896841049194, 'accumulated_submission_time': 9388.616441965103, 'accumulated_eval_time': 373.37684392929077, 'accumulated_logging_time': 2.7437617778778076}
I0501 22:04:06.032092 139589012399872 logging_writer.py:48] [12889] accumulated_eval_time=373.376844, accumulated_logging_time=2.743762, accumulated_submission_time=9388.616442, global_step=12889, preemption_count=0, score=9388.616442, test/accuracy=0.512700, test/loss=2.262978, test/num_examples=10000, total_duration=9764.896841, train/accuracy=0.708626, train/loss=1.286459, validation/accuracy=0.639140, validation/loss=1.604470, validation/num_examples=50000
I0501 22:04:15.778966 139589020792576 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7729242444038391, loss=2.6327779293060303
I0501 22:05:27.743908 139589012399872 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.801246702671051, loss=2.5809383392333984
I0501 22:06:39.642503 139589020792576 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8979541659355164, loss=2.683500051498413
I0501 22:07:51.940587 139589012399872 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.8752545118331909, loss=2.744962453842163
I0501 22:09:03.609519 139589020792576 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7847931981086731, loss=2.645857095718384
I0501 22:10:15.214692 139589012399872 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7721700668334961, loss=2.6683883666992188
I0501 22:11:26.757419 139589020792576 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5285021662712097, loss=2.5848870277404785
I0501 22:12:38.778757 139589012399872 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8178401589393616, loss=2.5411555767059326
I0501 22:12:38.813075 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:12:46.320612 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:12:56.255996 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:12:58.434591 139795264595776 submission_runner.py:415] Time since start: 10297.31s, 	Step: 13601, 	{'train/accuracy': 0.7509765625, 'train/loss': 1.0933494567871094, 'validation/accuracy': 0.6419000029563904, 'validation/loss': 1.563372254371643, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.237349271774292, 'test/num_examples': 10000, 'score': 9901.380869626999, 'total_duration': 10297.307758808136, 'accumulated_submission_time': 9901.380869626999, 'accumulated_eval_time': 392.99694299697876, 'accumulated_logging_time': 2.7616302967071533}
I0501 22:12:58.443251 139589020792576 logging_writer.py:48] [13601] accumulated_eval_time=392.996943, accumulated_logging_time=2.761630, accumulated_submission_time=9901.380870, global_step=13601, preemption_count=0, score=9901.380870, test/accuracy=0.518800, test/loss=2.237349, test/num_examples=10000, total_duration=10297.307759, train/accuracy=0.750977, train/loss=1.093349, validation/accuracy=0.641900, validation/loss=1.563372, validation/num_examples=50000
I0501 22:14:10.248311 139589012399872 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6134834289550781, loss=2.672086238861084
I0501 22:15:22.005391 139589020792576 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.764660656452179, loss=2.5619590282440186
I0501 22:16:34.264010 139589012399872 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6240634918212891, loss=2.7012417316436768
I0501 22:17:46.017855 139589020792576 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.688912570476532, loss=2.5146632194519043
I0501 22:18:58.031636 139589012399872 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.7517322897911072, loss=2.5334160327911377
I0501 22:20:10.635849 139589020792576 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8549127578735352, loss=2.6148269176483154
I0501 22:21:23.822683 139589012399872 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7240831851959229, loss=2.5801632404327393
I0501 22:21:28.776534 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:21:36.447656 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:21:46.457719 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:21:48.445642 139795264595776 submission_runner.py:415] Time since start: 10827.32s, 	Step: 14310, 	{'train/accuracy': 0.7264628410339355, 'train/loss': 1.198747158050537, 'validation/accuracy': 0.6506400108337402, 'validation/loss': 1.5374244451522827, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.2349441051483154, 'test/num_examples': 10000, 'score': 10411.697776794434, 'total_duration': 10827.320155143738, 'accumulated_submission_time': 10411.697776794434, 'accumulated_eval_time': 412.66600608825684, 'accumulated_logging_time': 2.777998685836792}
I0501 22:21:48.456374 139589020792576 logging_writer.py:48] [14310] accumulated_eval_time=412.666006, accumulated_logging_time=2.777999, accumulated_submission_time=10411.697777, global_step=14310, preemption_count=0, score=10411.697777, test/accuracy=0.515500, test/loss=2.234944, test/num_examples=10000, total_duration=10827.320155, train/accuracy=0.726463, train/loss=1.198747, validation/accuracy=0.650640, validation/loss=1.537424, validation/num_examples=50000
I0501 22:22:55.914840 139589012399872 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6396402716636658, loss=2.515218496322632
I0501 22:24:08.528118 139589020792576 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6759408712387085, loss=2.483591318130493
I0501 22:25:21.469010 139589012399872 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8368334174156189, loss=2.5361781120300293
I0501 22:26:33.670732 139589020792576 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7622172236442566, loss=2.5688560009002686
I0501 22:27:46.141997 139589012399872 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.765480101108551, loss=2.5694704055786133
I0501 22:28:59.065590 139589020792576 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.8102364540100098, loss=2.531320333480835
I0501 22:30:11.797135 139589012399872 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7017508149147034, loss=2.585191488265991
I0501 22:30:18.623966 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:30:26.168706 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:30:36.311890 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:30:38.496349 139795264595776 submission_runner.py:415] Time since start: 11357.37s, 	Step: 15013, 	{'train/accuracy': 0.76273512840271, 'train/loss': 1.0433768033981323, 'validation/accuracy': 0.6574599742889404, 'validation/loss': 1.5106465816497803, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.173691749572754, 'test/num_examples': 10000, 'score': 10921.849606990814, 'total_duration': 11357.370880365372, 'accumulated_submission_time': 10921.849606990814, 'accumulated_eval_time': 432.53836035728455, 'accumulated_logging_time': 2.7959823608398438}
I0501 22:30:38.505632 139589020792576 logging_writer.py:48] [15013] accumulated_eval_time=432.538360, accumulated_logging_time=2.795982, accumulated_submission_time=10921.849607, global_step=15013, preemption_count=0, score=10921.849607, test/accuracy=0.529000, test/loss=2.173692, test/num_examples=10000, total_duration=11357.370880, train/accuracy=0.762735, train/loss=1.043377, validation/accuracy=0.657460, validation/loss=1.510647, validation/num_examples=50000
I0501 22:31:44.347367 139589012399872 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7626691460609436, loss=2.5823254585266113
I0501 22:32:57.366353 139589020792576 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8539569973945618, loss=2.589951992034912
I0501 22:34:10.965343 139589012399872 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6398552060127258, loss=2.4839537143707275
I0501 22:35:23.913148 139589020792576 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5973362326622009, loss=2.487537145614624
I0501 22:36:37.413067 139589012399872 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6160709261894226, loss=2.534635543823242
I0501 22:37:50.402855 139589020792576 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6615880727767944, loss=2.3918654918670654
I0501 22:39:03.326215 139589012399872 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6749421954154968, loss=2.544825792312622
I0501 22:39:08.551505 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:39:15.948348 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:39:26.082928 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:39:27.973672 139795264595776 submission_runner.py:415] Time since start: 11886.85s, 	Step: 15710, 	{'train/accuracy': 0.7366071343421936, 'train/loss': 1.1491557359695435, 'validation/accuracy': 0.65829998254776, 'validation/loss': 1.5001418590545654, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.1721935272216797, 'test/num_examples': 10000, 'score': 11431.880207777023, 'total_duration': 11886.84819483757, 'accumulated_submission_time': 11431.880207777023, 'accumulated_eval_time': 451.9604871273041, 'accumulated_logging_time': 2.8121540546417236}
I0501 22:39:27.983252 139589020792576 logging_writer.py:48] [15710] accumulated_eval_time=451.960487, accumulated_logging_time=2.812154, accumulated_submission_time=11431.880208, global_step=15710, preemption_count=0, score=11431.880208, test/accuracy=0.530500, test/loss=2.172194, test/num_examples=10000, total_duration=11886.848195, train/accuracy=0.736607, train/loss=1.149156, validation/accuracy=0.658300, validation/loss=1.500142, validation/num_examples=50000
I0501 22:40:35.797770 139589012399872 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6914432644844055, loss=2.4896504878997803
I0501 22:41:49.084657 139589020792576 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.9106183052062988, loss=2.4572699069976807
I0501 22:43:02.807280 139589012399872 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6272335648536682, loss=2.5552496910095215
I0501 22:44:15.563596 139589020792576 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8468396663665771, loss=2.5154080390930176
I0501 22:45:28.264084 139589012399872 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7027158737182617, loss=2.5208535194396973
I0501 22:46:41.093830 139589020792576 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6813270449638367, loss=2.4643168449401855
I0501 22:47:53.702286 139589012399872 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7135503888130188, loss=2.58953857421875
I0501 22:47:58.358684 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:48:05.833585 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:48:16.095667 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:48:18.272054 139795264595776 submission_runner.py:415] Time since start: 12417.15s, 	Step: 16409, 	{'train/accuracy': 0.7708266973495483, 'train/loss': 1.0031853914260864, 'validation/accuracy': 0.6664599776268005, 'validation/loss': 1.4600509405136108, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.1476893424987793, 'test/num_examples': 10000, 'score': 11942.240266084671, 'total_duration': 12417.146569013596, 'accumulated_submission_time': 11942.240266084671, 'accumulated_eval_time': 471.87380838394165, 'accumulated_logging_time': 2.8287084102630615}
I0501 22:48:18.282461 139589020792576 logging_writer.py:48] [16409] accumulated_eval_time=471.873808, accumulated_logging_time=2.828708, accumulated_submission_time=11942.240266, global_step=16409, preemption_count=0, score=11942.240266, test/accuracy=0.533700, test/loss=2.147689, test/num_examples=10000, total_duration=12417.146569, train/accuracy=0.770827, train/loss=1.003185, validation/accuracy=0.666460, validation/loss=1.460051, validation/num_examples=50000
I0501 22:49:26.912661 139589012399872 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6488686800003052, loss=2.5375823974609375
I0501 22:50:40.115316 139589020792576 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7791585922241211, loss=2.4191112518310547
I0501 22:51:52.872153 139589012399872 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5907859802246094, loss=2.5585994720458984
I0501 22:53:06.116052 139589020792576 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7475090026855469, loss=2.4978384971618652
I0501 22:54:19.778729 139589012399872 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7984331846237183, loss=2.4852371215820312
I0501 22:55:33.124222 139589020792576 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5280622839927673, loss=2.5027711391448975
I0501 22:56:46.404330 139589012399872 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6271778345108032, loss=2.5817642211914062
I0501 22:56:48.625854 139795264595776 spec.py:298] Evaluating on the training split.
I0501 22:56:56.091357 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 22:57:06.084949 139795264595776 spec.py:326] Evaluating on the test split.
I0501 22:57:08.025317 139795264595776 submission_runner.py:415] Time since start: 12946.90s, 	Step: 17105, 	{'train/accuracy': 0.75296950340271, 'train/loss': 1.091102123260498, 'validation/accuracy': 0.6696199774742126, 'validation/loss': 1.4519827365875244, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.1196722984313965, 'test/num_examples': 10000, 'score': 12452.567568778992, 'total_duration': 12946.899812698364, 'accumulated_submission_time': 12452.567568778992, 'accumulated_eval_time': 491.2732219696045, 'accumulated_logging_time': 2.8467726707458496}
I0501 22:57:08.035299 139589020792576 logging_writer.py:48] [17105] accumulated_eval_time=491.273222, accumulated_logging_time=2.846773, accumulated_submission_time=12452.567569, global_step=17105, preemption_count=0, score=12452.567569, test/accuracy=0.544400, test/loss=2.119672, test/num_examples=10000, total_duration=12946.899813, train/accuracy=0.752970, train/loss=1.091102, validation/accuracy=0.669620, validation/loss=1.451983, validation/num_examples=50000
I0501 22:58:19.543946 139589012399872 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.7588188648223877, loss=2.5264339447021484
I0501 22:59:32.612303 139589020792576 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6094077825546265, loss=2.4037251472473145
I0501 23:00:45.926944 139589012399872 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6030521988868713, loss=2.511120319366455
I0501 23:01:59.256614 139589020792576 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6201209425926208, loss=2.3991217613220215
I0501 23:03:12.554900 139589012399872 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.590840756893158, loss=2.379420518875122
I0501 23:04:26.346882 139589020792576 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6708800792694092, loss=2.4491751194000244
I0501 23:05:39.685162 139589012399872 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6353862285614014, loss=2.465782642364502
I0501 23:05:39.720614 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:05:46.817412 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:05:56.394715 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:05:58.577347 139795264595776 submission_runner.py:415] Time since start: 13477.45s, 	Step: 17801, 	{'train/accuracy': 0.7762675285339355, 'train/loss': 0.9717506766319275, 'validation/accuracy': 0.6721799969673157, 'validation/loss': 1.4405771493911743, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.1070916652679443, 'test/num_examples': 10000, 'score': 12964.237343072891, 'total_duration': 13477.450688362122, 'accumulated_submission_time': 12964.237343072891, 'accumulated_eval_time': 510.1287124156952, 'accumulated_logging_time': 2.863987684249878}
I0501 23:05:58.587219 139589020792576 logging_writer.py:48] [17801] accumulated_eval_time=510.128712, accumulated_logging_time=2.863988, accumulated_submission_time=12964.237343, global_step=17801, preemption_count=0, score=12964.237343, test/accuracy=0.545400, test/loss=2.107092, test/num_examples=10000, total_duration=13477.450688, train/accuracy=0.776268, train/loss=0.971751, validation/accuracy=0.672180, validation/loss=1.440577, validation/num_examples=50000
I0501 23:07:11.708974 139589012399872 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8086060285568237, loss=2.519329786300659
I0501 23:08:25.089563 139589020792576 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6273711919784546, loss=2.5070338249206543
I0501 23:09:38.441643 139589012399872 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5702265501022339, loss=2.333967447280884
I0501 23:10:51.810851 139589020792576 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.813697099685669, loss=2.373608350753784
I0501 23:12:05.361920 139589012399872 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5874865651130676, loss=2.4089548587799072
I0501 23:13:18.771862 139589020792576 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7195344567298889, loss=2.3818273544311523
I0501 23:14:32.130455 139589012399872 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6362115740776062, loss=2.4468300342559814
I0501 23:14:32.171676 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:14:39.015095 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:14:48.704429 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:14:50.684857 139795264595776 submission_runner.py:415] Time since start: 14009.56s, 	Step: 18501, 	{'train/accuracy': 0.7543646097183228, 'train/loss': 1.0677211284637451, 'validation/accuracy': 0.6736400127410889, 'validation/loss': 1.4409561157226562, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.107705593109131, 'test/num_examples': 10000, 'score': 13477.805985212326, 'total_duration': 14009.558349847794, 'accumulated_submission_time': 13477.805985212326, 'accumulated_eval_time': 528.6408140659332, 'accumulated_logging_time': 2.881488800048828}
I0501 23:14:50.694966 139589020792576 logging_writer.py:48] [18501] accumulated_eval_time=528.640814, accumulated_logging_time=2.881489, accumulated_submission_time=13477.805985, global_step=18501, preemption_count=0, score=13477.805985, test/accuracy=0.546700, test/loss=2.107706, test/num_examples=10000, total_duration=14009.558350, train/accuracy=0.754365, train/loss=1.067721, validation/accuracy=0.673640, validation/loss=1.440956, validation/num_examples=50000
I0501 23:16:04.710457 139589012399872 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6313329339027405, loss=2.4159340858459473
I0501 23:17:18.883787 139589020792576 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6285660862922668, loss=2.455049514770508
I0501 23:18:32.822433 139589012399872 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7514818906784058, loss=2.5174496173858643
I0501 23:19:46.429845 139589020792576 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6876996755599976, loss=2.4524664878845215
I0501 23:21:00.101311 139589012399872 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6037864089012146, loss=2.376835823059082
I0501 23:22:14.469597 139589020792576 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6243899464607239, loss=2.4243457317352295
I0501 23:23:20.786279 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:23:27.605448 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:23:37.167044 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:23:39.421475 139795264595776 submission_runner.py:415] Time since start: 14538.29s, 	Step: 19194, 	{'train/accuracy': 0.7835020422935486, 'train/loss': 0.9610673189163208, 'validation/accuracy': 0.6777799725532532, 'validation/loss': 1.4195795059204102, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.1053524017333984, 'test/num_examples': 10000, 'score': 13987.88150548935, 'total_duration': 14538.29470872879, 'accumulated_submission_time': 13987.88150548935, 'accumulated_eval_time': 547.2746822834015, 'accumulated_logging_time': 2.8990910053253174}
I0501 23:23:39.435534 139589012399872 logging_writer.py:48] [19194] accumulated_eval_time=547.274682, accumulated_logging_time=2.899091, accumulated_submission_time=13987.881505, global_step=19194, preemption_count=0, score=13987.881505, test/accuracy=0.543300, test/loss=2.105352, test/num_examples=10000, total_duration=14538.294709, train/accuracy=0.783502, train/loss=0.961067, validation/accuracy=0.677780, validation/loss=1.419580, validation/num_examples=50000
I0501 23:23:46.688309 139589020792576 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5550821423530579, loss=2.5045571327209473
I0501 23:25:00.886061 139589012399872 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6771376729011536, loss=2.4340944290161133
I0501 23:26:15.011236 139589020792576 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7626982927322388, loss=2.461059808731079
I0501 23:27:28.913461 139589012399872 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6471256017684937, loss=2.3804502487182617
I0501 23:28:43.785340 139589020792576 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.648432195186615, loss=2.4651618003845215
I0501 23:29:58.378931 139589012399872 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9212040305137634, loss=2.4251911640167236
I0501 23:31:12.514307 139589020792576 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5548830628395081, loss=2.38376522064209
I0501 23:32:11.987432 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:32:18.732694 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:32:28.299490 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:32:30.241429 139795264595776 submission_runner.py:415] Time since start: 15069.11s, 	Step: 19881, 	{'train/accuracy': 0.7664620280265808, 'train/loss': 1.0314183235168457, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.4280818700790405, 'validation/num_examples': 50000, 'test/accuracy': 0.5480000376701355, 'test/loss': 2.102640390396118, 'test/num_examples': 10000, 'score': 14500.414884328842, 'total_duration': 15069.114894390106, 'accumulated_submission_time': 14500.414884328842, 'accumulated_eval_time': 565.5276010036469, 'accumulated_logging_time': 2.9234137535095215}
I0501 23:32:30.253245 139589012399872 logging_writer.py:48] [19881] accumulated_eval_time=565.527601, accumulated_logging_time=2.923414, accumulated_submission_time=14500.414884, global_step=19881, preemption_count=0, score=14500.414884, test/accuracy=0.548000, test/loss=2.102640, test/num_examples=10000, total_duration=15069.114894, train/accuracy=0.766462, train/loss=1.031418, validation/accuracy=0.676560, validation/loss=1.428082, validation/num_examples=50000
I0501 23:32:45.123986 139589020792576 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6838052272796631, loss=2.3496477603912354
I0501 23:33:59.495732 139589012399872 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5886930823326111, loss=2.4262802600860596
I0501 23:35:14.101529 139589020792576 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.618409276008606, loss=2.3769068717956543
I0501 23:36:28.262008 139589012399872 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6795040369033813, loss=2.354478597640991
I0501 23:37:43.120543 139589020792576 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5985028743743896, loss=2.409313440322876
I0501 23:38:57.717474 139589012399872 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6314640045166016, loss=2.4968442916870117
I0501 23:40:11.881722 139589020792576 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6895673871040344, loss=2.402444839477539
I0501 23:41:00.811777 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:41:07.578450 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:41:17.189638 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:41:19.289200 139795264595776 submission_runner.py:415] Time since start: 15598.16s, 	Step: 20568, 	{'train/accuracy': 0.7885243892669678, 'train/loss': 0.9114050269126892, 'validation/accuracy': 0.6808800101280212, 'validation/loss': 1.3792877197265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.061779260635376, 'test/num_examples': 10000, 'score': 15010.95687699318, 'total_duration': 15598.162800073624, 'accumulated_submission_time': 15010.95687699318, 'accumulated_eval_time': 584.0040574073792, 'accumulated_logging_time': 2.9434139728546143}
I0501 23:41:19.298967 139589012399872 logging_writer.py:48] [20568] accumulated_eval_time=584.004057, accumulated_logging_time=2.943414, accumulated_submission_time=15010.956877, global_step=20568, preemption_count=0, score=15010.956877, test/accuracy=0.558200, test/loss=2.061779, test/num_examples=10000, total_duration=15598.162800, train/accuracy=0.788524, train/loss=0.911405, validation/accuracy=0.680880, validation/loss=1.379288, validation/num_examples=50000
I0501 23:41:44.645864 139589020792576 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7135152816772461, loss=2.3883674144744873
I0501 23:42:58.899984 139589012399872 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.671159565448761, loss=2.3212385177612305
I0501 23:44:13.211470 139589020792576 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5641816854476929, loss=2.441671371459961
I0501 23:45:27.456563 139589012399872 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6310644745826721, loss=2.349078893661499
I0501 23:46:41.816265 139589020792576 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6291260719299316, loss=2.36179256439209
I0501 23:47:56.390595 139589012399872 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5142337679862976, loss=2.243997573852539
I0501 23:49:10.383906 139589020792576 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6297768354415894, loss=2.438277244567871
I0501 23:49:49.313050 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:49:55.989027 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:50:05.771059 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:50:07.944455 139795264595776 submission_runner.py:415] Time since start: 16126.82s, 	Step: 21257, 	{'train/accuracy': 0.7743343114852905, 'train/loss': 0.9759835600852966, 'validation/accuracy': 0.6828399896621704, 'validation/loss': 1.38865327835083, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.055602550506592, 'test/num_examples': 10000, 'score': 15520.955194711685, 'total_duration': 16126.817510604858, 'accumulated_submission_time': 15520.955194711685, 'accumulated_eval_time': 602.6339566707611, 'accumulated_logging_time': 2.960822343826294}
I0501 23:50:07.954095 139589012399872 logging_writer.py:48] [21257] accumulated_eval_time=602.633957, accumulated_logging_time=2.960822, accumulated_submission_time=15520.955195, global_step=21257, preemption_count=0, score=15520.955195, test/accuracy=0.559100, test/loss=2.055603, test/num_examples=10000, total_duration=16126.817511, train/accuracy=0.774334, train/loss=0.975984, validation/accuracy=0.682840, validation/loss=1.388653, validation/num_examples=50000
I0501 23:50:43.751231 139589020792576 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.537787139415741, loss=2.3066635131835938
I0501 23:51:58.454656 139589012399872 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6557751893997192, loss=2.412165880203247
I0501 23:53:13.801826 139589020792576 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6724955439567566, loss=2.3617348670959473
I0501 23:54:28.062120 139589012399872 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5531219244003296, loss=2.3305137157440186
I0501 23:55:42.644092 139589020792576 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6247236728668213, loss=2.447420597076416
I0501 23:56:57.926604 139589012399872 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5597943067550659, loss=2.2781591415405273
I0501 23:58:12.469233 139589020792576 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5066418051719666, loss=2.3817050457000732
I0501 23:58:38.347009 139795264595776 spec.py:298] Evaluating on the training split.
I0501 23:58:45.023522 139795264595776 spec.py:310] Evaluating on the validation split.
I0501 23:58:54.735185 139795264595776 spec.py:326] Evaluating on the test split.
I0501 23:58:56.835329 139795264595776 submission_runner.py:415] Time since start: 16655.71s, 	Step: 21940, 	{'train/accuracy': 0.7967952489852905, 'train/loss': 0.8868597149848938, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.3736740350723267, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 2.0553371906280518, 'test/num_examples': 10000, 'score': 16031.331669569016, 'total_duration': 16655.708733797073, 'accumulated_submission_time': 16031.331669569016, 'accumulated_eval_time': 621.121120929718, 'accumulated_logging_time': 2.978412628173828}
I0501 23:58:56.845434 139589012399872 logging_writer.py:48] [21940] accumulated_eval_time=621.121121, accumulated_logging_time=2.978413, accumulated_submission_time=16031.331670, global_step=21940, preemption_count=0, score=16031.331670, test/accuracy=0.558000, test/loss=2.055337, test/num_examples=10000, total_duration=16655.708734, train/accuracy=0.796795, train/loss=0.886860, validation/accuracy=0.687380, validation/loss=1.373674, validation/num_examples=50000
I0501 23:59:45.403123 139589020792576 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7900146842002869, loss=2.374588966369629
I0502 00:01:00.074608 139589012399872 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5978411436080933, loss=2.3849310874938965
I0502 00:02:14.679650 139589020792576 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6422864198684692, loss=2.3443307876586914
I0502 00:03:28.984602 139589012399872 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.843143105506897, loss=2.281703472137451
I0502 00:04:43.760534 139589020792576 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6248201131820679, loss=2.3688642978668213
I0502 00:05:58.276392 139589012399872 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5220726132392883, loss=2.378420829772949
I0502 00:07:12.406189 139589020792576 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6363168954849243, loss=2.3898043632507324
I0502 00:07:27.216924 139795264595776 spec.py:298] Evaluating on the training split.
I0502 00:07:33.964852 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 00:07:43.690402 139795264595776 spec.py:326] Evaluating on the test split.
I0502 00:07:45.879567 139795264595776 submission_runner.py:415] Time since start: 17184.75s, 	Step: 22621, 	{'train/accuracy': 0.7855947017669678, 'train/loss': 0.9289692044258118, 'validation/accuracy': 0.6868199706077576, 'validation/loss': 1.372194528579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 2.035114288330078, 'test/num_examples': 10000, 'score': 16541.68720960617, 'total_duration': 17184.752910375595, 'accumulated_submission_time': 16541.68720960617, 'accumulated_eval_time': 639.7825446128845, 'accumulated_logging_time': 2.9963178634643555}
I0502 00:07:45.889587 139589012399872 logging_writer.py:48] [22621] accumulated_eval_time=639.782545, accumulated_logging_time=2.996318, accumulated_submission_time=16541.687210, global_step=22621, preemption_count=0, score=16541.687210, test/accuracy=0.562300, test/loss=2.035114, test/num_examples=10000, total_duration=17184.752910, train/accuracy=0.785595, train/loss=0.928969, validation/accuracy=0.686820, validation/loss=1.372195, validation/num_examples=50000
I0502 00:08:44.847619 139589020792576 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6469212174415588, loss=2.3553457260131836
I0502 00:09:59.567019 139589012399872 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6858350038528442, loss=2.195472002029419
I0502 00:11:14.050027 139589020792576 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6981042623519897, loss=2.306406259536743
I0502 00:12:28.245342 139589012399872 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6441377401351929, loss=2.3492231369018555
I0502 00:13:43.195037 139589020792576 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6865630149841309, loss=2.3268699645996094
I0502 00:14:57.572191 139589012399872 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.48715320229530334, loss=2.2575864791870117
I0502 00:16:12.183580 139589020792576 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5331882834434509, loss=2.325648784637451
I0502 00:16:16.185065 139795264595776 spec.py:298] Evaluating on the training split.
I0502 00:16:22.910118 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 00:16:32.606935 139795264595776 spec.py:326] Evaluating on the test split.
I0502 00:16:34.569075 139795264595776 submission_runner.py:415] Time since start: 17713.44s, 	Step: 23308, 	{'train/accuracy': 0.8003228306770325, 'train/loss': 0.883234441280365, 'validation/accuracy': 0.6845999956130981, 'validation/loss': 1.3807764053344727, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.0618605613708496, 'test/num_examples': 10000, 'score': 17051.966700077057, 'total_duration': 17713.442389965057, 'accumulated_submission_time': 17051.966700077057, 'accumulated_eval_time': 658.1653201580048, 'accumulated_logging_time': 3.014238119125366}
I0502 00:16:34.581385 139589012399872 logging_writer.py:48] [23308] accumulated_eval_time=658.165320, accumulated_logging_time=3.014238, accumulated_submission_time=17051.966700, global_step=23308, preemption_count=0, score=17051.966700, test/accuracy=0.557300, test/loss=2.061861, test/num_examples=10000, total_duration=17713.442390, train/accuracy=0.800323, train/loss=0.883234, validation/accuracy=0.684600, validation/loss=1.380776, validation/num_examples=50000
I0502 00:17:46.073680 139589020792576 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.47150948643684387, loss=2.3101067543029785
I0502 00:19:00.459300 139589012399872 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5989450812339783, loss=2.3092944622039795
I0502 00:20:15.401395 139589020792576 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7070804834365845, loss=2.2875664234161377
I0502 00:21:29.763158 139589012399872 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5649543404579163, loss=2.392367362976074
I0502 00:22:44.139939 139589020792576 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6756473183631897, loss=2.325051784515381
I0502 00:23:58.945563 139589012399872 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6403638124465942, loss=2.353621244430542
I0502 00:25:04.767609 139795264595776 spec.py:298] Evaluating on the training split.
I0502 00:25:11.429625 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 00:25:21.274783 139795264595776 spec.py:326] Evaluating on the test split.
I0502 00:25:23.234909 139795264595776 submission_runner.py:415] Time since start: 18242.11s, 	Step: 23992, 	{'train/accuracy': 0.7869499325752258, 'train/loss': 0.9319261908531189, 'validation/accuracy': 0.6903199553489685, 'validation/loss': 1.3706865310668945, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.054962158203125, 'test/num_examples': 10000, 'score': 17562.13594031334, 'total_duration': 18242.10812830925, 'accumulated_submission_time': 17562.13594031334, 'accumulated_eval_time': 676.631281375885, 'accumulated_logging_time': 3.035020112991333}
I0502 00:25:23.245715 139589020792576 logging_writer.py:48] [23992] accumulated_eval_time=676.631281, accumulated_logging_time=3.035020, accumulated_submission_time=17562.135940, global_step=23992, preemption_count=0, score=17562.135940, test/accuracy=0.558800, test/loss=2.054962, test/num_examples=10000, total_duration=18242.108128, train/accuracy=0.786950, train/loss=0.931926, validation/accuracy=0.690320, validation/loss=1.370687, validation/num_examples=50000
I0502 00:25:31.716850 139589012399872 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5882666707038879, loss=2.3358707427978516
I0502 00:26:46.290867 139589020792576 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7402462959289551, loss=2.3657169342041016
I0502 00:28:00.886756 139589012399872 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7342708706855774, loss=2.3596127033233643
I0502 00:29:15.541436 139589020792576 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7717385292053223, loss=2.3792362213134766
I0502 00:30:30.005693 139589012399872 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7164784669876099, loss=2.3169620037078857
I0502 00:31:44.523228 139589020792576 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6073299050331116, loss=2.315670967102051
I0502 00:32:58.898785 139589012399872 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.48773127794265747, loss=2.2572383880615234
I0502 00:33:53.290567 139795264595776 spec.py:298] Evaluating on the training split.
I0502 00:33:59.882183 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 00:34:09.694690 139795264595776 spec.py:326] Evaluating on the test split.
I0502 00:34:11.777383 139795264595776 submission_runner.py:415] Time since start: 18770.65s, 	Step: 24677, 	{'train/accuracy': 0.8069395422935486, 'train/loss': 0.8627704381942749, 'validation/accuracy': 0.6888599991798401, 'validation/loss': 1.3719855546951294, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.044919490814209, 'test/num_examples': 10000, 'score': 18072.16477894783, 'total_duration': 18770.650775671005, 'accumulated_submission_time': 18072.16477894783, 'accumulated_eval_time': 695.1169271469116, 'accumulated_logging_time': 3.0536508560180664}
I0502 00:34:11.787160 139589020792576 logging_writer.py:48] [24677] accumulated_eval_time=695.116927, accumulated_logging_time=3.053651, accumulated_submission_time=18072.164779, global_step=24677, preemption_count=0, score=18072.164779, test/accuracy=0.559000, test/loss=2.044919, test/num_examples=10000, total_duration=18770.650776, train/accuracy=0.806940, train/loss=0.862770, validation/accuracy=0.688860, validation/loss=1.371986, validation/num_examples=50000
I0502 00:34:32.226193 139589012399872 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8299655914306641, loss=2.365886688232422
I0502 00:35:46.786116 139589020792576 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.646874725818634, loss=2.298560857772827
I0502 00:37:01.201299 139589012399872 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.5877239108085632, loss=2.2514076232910156
I0502 00:38:16.528366 139589020792576 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6572192907333374, loss=2.386772871017456
I0502 00:39:31.301342 139589012399872 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6355104446411133, loss=2.336188316345215
I0502 00:40:45.761527 139589020792576 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6063659191131592, loss=2.2493157386779785
I0502 00:42:00.698780 139589012399872 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7908086180686951, loss=2.384884834289551
I0502 00:42:41.804112 139795264595776 spec.py:298] Evaluating on the training split.
I0502 00:42:48.372759 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 00:42:58.207224 139795264595776 spec.py:326] Evaluating on the test split.
I0502 00:43:00.442982 139795264595776 submission_runner.py:415] Time since start: 19299.31s, 	Step: 25360, 	{'train/accuracy': 0.7915138602256775, 'train/loss': 0.908660352230072, 'validation/accuracy': 0.6911599636077881, 'validation/loss': 1.3602062463760376, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.0536582469940186, 'test/num_examples': 10000, 'score': 18582.164834022522, 'total_duration': 19299.3141579628, 'accumulated_submission_time': 18582.164834022522, 'accumulated_eval_time': 713.752418756485, 'accumulated_logging_time': 3.072237491607666}
I0502 00:43:00.452950 139589020792576 logging_writer.py:48] [25360] accumulated_eval_time=713.752419, accumulated_logging_time=3.072237, accumulated_submission_time=18582.164834, global_step=25360, preemption_count=0, score=18582.164834, test/accuracy=0.553200, test/loss=2.053658, test/num_examples=10000, total_duration=19299.314158, train/accuracy=0.791514, train/loss=0.908660, validation/accuracy=0.691160, validation/loss=1.360206, validation/num_examples=50000
I0502 00:43:34.172994 139589012399872 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5860974192619324, loss=2.2666168212890625
I0502 00:44:49.434345 139589020792576 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6303038597106934, loss=2.386964797973633
I0502 00:46:04.192928 139589012399872 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6371311545372009, loss=2.2768797874450684
I0502 00:47:19.082809 139589020792576 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6222278475761414, loss=2.257459878921509
I0502 00:48:33.832204 139589012399872 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.5887276530265808, loss=2.2698185443878174
I0502 00:49:48.678436 139589020792576 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.642326295375824, loss=2.237764835357666
I0502 00:51:03.690165 139589012399872 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4830041825771332, loss=2.2138853073120117
I0502 00:51:33.570607 139795264595776 spec.py:298] Evaluating on the training split.
I0502 00:51:40.108888 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 00:51:49.991497 139795264595776 spec.py:326] Evaluating on the test split.
I0502 00:51:52.019455 139795264595776 submission_runner.py:415] Time since start: 19830.89s, 	Step: 26041, 	{'train/accuracy': 0.8130181431770325, 'train/loss': 0.8215488195419312, 'validation/accuracy': 0.6912599802017212, 'validation/loss': 1.3496968746185303, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 2.0181632041931152, 'test/num_examples': 10000, 'score': 19095.26652264595, 'total_duration': 19830.89278793335, 'accumulated_submission_time': 19095.26652264595, 'accumulated_eval_time': 732.2000391483307, 'accumulated_logging_time': 3.090096950531006}
I0502 00:51:52.029285 139589020792576 logging_writer.py:48] [26041] accumulated_eval_time=732.200039, accumulated_logging_time=3.090097, accumulated_submission_time=19095.266523, global_step=26041, preemption_count=0, score=19095.266523, test/accuracy=0.565000, test/loss=2.018163, test/num_examples=10000, total_duration=19830.892788, train/accuracy=0.813018, train/loss=0.821549, validation/accuracy=0.691260, validation/loss=1.349697, validation/num_examples=50000
I0502 00:52:37.340867 139589012399872 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.4860174357891083, loss=2.3259055614471436
I0502 00:53:52.155413 139589020792576 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6332867741584778, loss=2.3237061500549316
I0502 00:55:07.690773 139589012399872 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5674657225608826, loss=2.2510154247283936
I0502 00:56:22.459233 139589020792576 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5934543013572693, loss=2.315552234649658
I0502 00:57:37.195435 139589012399872 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6112374067306519, loss=2.2381505966186523
I0502 00:58:51.972849 139589020792576 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.637793779373169, loss=2.2556257247924805
I0502 01:00:06.628354 139589012399872 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6484014391899109, loss=2.306636095046997
I0502 01:00:22.254023 139795264595776 spec.py:298] Evaluating on the training split.
I0502 01:00:28.823452 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 01:00:38.697839 139795264595776 spec.py:326] Evaluating on the test split.
I0502 01:00:40.652473 139795264595776 submission_runner.py:415] Time since start: 20359.53s, 	Step: 26722, 	{'train/accuracy': 0.8001833558082581, 'train/loss': 0.8687778115272522, 'validation/accuracy': 0.6958199739456177, 'validation/loss': 1.3403400182724, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 2.004615068435669, 'test/num_examples': 10000, 'score': 19605.47409749031, 'total_duration': 20359.525676965714, 'accumulated_submission_time': 19605.47409749031, 'accumulated_eval_time': 750.5971372127533, 'accumulated_logging_time': 3.108966112136841}
I0502 01:00:40.662827 139589020792576 logging_writer.py:48] [26722] accumulated_eval_time=750.597137, accumulated_logging_time=3.108966, accumulated_submission_time=19605.474097, global_step=26722, preemption_count=0, score=19605.474097, test/accuracy=0.567100, test/loss=2.004615, test/num_examples=10000, total_duration=20359.525677, train/accuracy=0.800183, train/loss=0.868778, validation/accuracy=0.695820, validation/loss=1.340340, validation/num_examples=50000
I0502 01:01:40.200375 139589012399872 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5704837441444397, loss=2.286369562149048
I0502 01:02:54.976256 139589020792576 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6352983117103577, loss=2.3361098766326904
I0502 01:04:09.972607 139589012399872 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8006112575531006, loss=2.2390527725219727
I0502 01:05:24.691734 139589020792576 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5855246782302856, loss=2.284362554550171
I0502 01:06:39.621402 139589012399872 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7180997133255005, loss=2.412041425704956
I0502 01:07:55.096057 139589020792576 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5321991443634033, loss=2.2071566581726074
I0502 01:09:10.231967 139589012399872 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5439417362213135, loss=2.2810070514678955
I0502 01:09:10.800512 139795264595776 spec.py:298] Evaluating on the training split.
I0502 01:09:17.259554 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 01:09:27.163021 139795264595776 spec.py:326] Evaluating on the test split.
I0502 01:09:29.285681 139795264595776 submission_runner.py:415] Time since start: 20888.16s, 	Step: 27402, 	{'train/accuracy': 0.8245774507522583, 'train/loss': 0.7910658121109009, 'validation/accuracy': 0.693839967250824, 'validation/loss': 1.3471602201461792, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 2.019191026687622, 'test/num_examples': 10000, 'score': 20115.59223008156, 'total_duration': 20888.158397436142, 'accumulated_submission_time': 20115.59223008156, 'accumulated_eval_time': 769.0804598331451, 'accumulated_logging_time': 3.1306746006011963}
I0502 01:09:29.295895 139589020792576 logging_writer.py:48] [27402] accumulated_eval_time=769.080460, accumulated_logging_time=3.130675, accumulated_submission_time=20115.592230, global_step=27402, preemption_count=0, score=20115.592230, test/accuracy=0.568300, test/loss=2.019191, test/num_examples=10000, total_duration=20888.158397, train/accuracy=0.824577, train/loss=0.791066, validation/accuracy=0.693840, validation/loss=1.347160, validation/num_examples=50000
I0502 01:10:43.547149 139589012399872 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6421201825141907, loss=2.32033109664917
I0502 01:11:58.516581 139589020792576 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.566374659538269, loss=2.219388723373413
I0502 01:13:14.608315 139589012399872 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6096456050872803, loss=2.3635685443878174
I0502 01:14:30.209214 139589020792576 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5462623238563538, loss=2.1583144664764404
I0502 01:15:45.184770 139589012399872 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.577555239200592, loss=2.231135845184326
I0502 01:16:55.947695 139795264595776 spec.py:298] Evaluating on the training split.
I0502 01:17:02.345632 139795264595776 spec.py:310] Evaluating on the validation split.
I0502 01:17:12.249228 139795264595776 spec.py:326] Evaluating on the test split.
I0502 01:17:14.338129 139795264595776 submission_runner.py:415] Time since start: 21353.21s, 	Step: 28000, 	{'train/accuracy': 0.8056241869926453, 'train/loss': 0.8508058786392212, 'validation/accuracy': 0.6954999566078186, 'validation/loss': 1.328111171722412, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.9926079511642456, 'test/num_examples': 10000, 'score': 20562.22887468338, 'total_duration': 21353.211349010468, 'accumulated_submission_time': 20562.22887468338, 'accumulated_eval_time': 787.4695565700531, 'accumulated_logging_time': 3.148615837097168}
I0502 01:17:14.348366 139589020792576 logging_writer.py:48] [28000] accumulated_eval_time=787.469557, accumulated_logging_time=3.148616, accumulated_submission_time=20562.228875, global_step=28000, preemption_count=0, score=20562.228875, test/accuracy=0.571300, test/loss=1.992608, test/num_examples=10000, total_duration=21353.211349, train/accuracy=0.805624, train/loss=0.850806, validation/accuracy=0.695500, validation/loss=1.328111, validation/num_examples=50000
I0502 01:17:14.363595 139589012399872 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=20562.228875
I0502 01:17:15.723349 139795264595776 checkpoints.py:356] Saving checkpoint at step: 28000
I0502 01:17:19.765274 139795264595776 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1/checkpoint_28000
I0502 01:17:19.849010 139795264595776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_shampoo/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0502 01:17:20.510022 139795264595776 submission_runner.py:578] Tuning trial 1/1
I0502 01:17:20.511279 139795264595776 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.07758862577375368, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0502 01:17:20.525440 139795264595776 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007573341717943549, 'train/loss': 6.912168979644775, 'validation/accuracy': 0.0010400000028312206, 'validation/loss': 6.912391662597656, 'validation/num_examples': 50000, 'test/accuracy': 0.0004000000189989805, 'test/loss': 6.911768913269043, 'test/num_examples': 10000, 'score': 184.3108127117157, 'total_duration': 226.18678855895996, 'accumulated_submission_time': 184.3108127117157, 'accumulated_eval_time': 41.87582302093506, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (731, {'train/accuracy': 0.08153299987316132, 'train/loss': 5.262387752532959, 'validation/accuracy': 0.07642000168561935, 'validation/loss': 5.31176233291626, 'validation/num_examples': 50000, 'test/accuracy': 0.053700003772974014, 'test/loss': 5.564613342285156, 'test/num_examples': 10000, 'score': 694.8820705413818, 'total_duration': 753.7307419776917, 'accumulated_submission_time': 694.8820705413818, 'accumulated_eval_time': 58.81516718864441, 'accumulated_logging_time': 0.02440667152404785, 'global_step': 731, 'preemption_count': 0}), (1450, {'train/accuracy': 0.19359852373600006, 'train/loss': 4.1333160400390625, 'validation/accuracy': 0.1640399992465973, 'validation/loss': 4.33566951751709, 'validation/num_examples': 50000, 'test/accuracy': 0.12090000510215759, 'test/loss': 4.774815082550049, 'test/num_examples': 10000, 'score': 1205.3396894931793, 'total_duration': 1280.9929327964783, 'accumulated_submission_time': 1205.3396894931793, 'accumulated_eval_time': 75.59471559524536, 'accumulated_logging_time': 0.0406954288482666, 'global_step': 1450, 'preemption_count': 0}), (2168, {'train/accuracy': 0.2691127061843872, 'train/loss': 3.5797245502471924, 'validation/accuracy': 0.249439999461174, 'validation/loss': 3.703864812850952, 'validation/num_examples': 50000, 'test/accuracy': 0.17520001530647278, 'test/loss': 4.291595935821533, 'test/num_examples': 10000, 'score': 1715.8171577453613, 'total_duration': 1808.3357088565826, 'accumulated_submission_time': 1715.8171577453613, 'accumulated_eval_time': 92.43571162223816, 'accumulated_logging_time': 0.05651593208312988, 'global_step': 2168, 'preemption_count': 0}), (2881, {'train/accuracy': 0.35738199949264526, 'train/loss': 2.994899034500122, 'validation/accuracy': 0.321260005235672, 'validation/loss': 3.2152562141418457, 'validation/num_examples': 50000, 'test/accuracy': 0.2337000072002411, 'test/loss': 3.820979118347168, 'test/num_examples': 10000, 'score': 2226.204894065857, 'total_duration': 2335.5681705474854, 'accumulated_submission_time': 2226.204894065857, 'accumulated_eval_time': 109.25591373443604, 'accumulated_logging_time': 0.07246899604797363, 'global_step': 2881, 'preemption_count': 0}), (3593, {'train/accuracy': 0.4160754084587097, 'train/loss': 2.6838724613189697, 'validation/accuracy': 0.39023998379707336, 'validation/loss': 2.8285982608795166, 'validation/num_examples': 50000, 'test/accuracy': 0.29350000619888306, 'test/loss': 3.4642977714538574, 'test/num_examples': 10000, 'score': 2736.7196679115295, 'total_duration': 2863.0068032741547, 'accumulated_submission_time': 2736.7196679115295, 'accumulated_eval_time': 126.15480351448059, 'accumulated_logging_time': 0.08878445625305176, 'global_step': 3593, 'preemption_count': 0}), (4307, {'train/accuracy': 0.48698580265045166, 'train/loss': 2.318234443664551, 'validation/accuracy': 0.43751999735832214, 'validation/loss': 2.569716691970825, 'validation/num_examples': 50000, 'test/accuracy': 0.33010002970695496, 'test/loss': 3.2452661991119385, 'test/num_examples': 10000, 'score': 3247.041827440262, 'total_duration': 3390.7378237247467, 'accumulated_submission_time': 3247.041827440262, 'accumulated_eval_time': 143.53979754447937, 'accumulated_logging_time': 0.10400032997131348, 'global_step': 4307, 'preemption_count': 0}), (5021, {'train/accuracy': 0.500996470451355, 'train/loss': 2.2706990242004395, 'validation/accuracy': 0.4649199843406677, 'validation/loss': 2.4524424076080322, 'validation/num_examples': 50000, 'test/accuracy': 0.3525000214576721, 'test/loss': 3.148200035095215, 'test/num_examples': 10000, 'score': 3759.9000754356384, 'total_duration': 3923.6564865112305, 'accumulated_submission_time': 3759.9000754356384, 'accumulated_eval_time': 161.13597583770752, 'accumulated_logging_time': 2.5595126152038574, 'global_step': 5021, 'preemption_count': 0}), (5734, {'train/accuracy': 0.5507014989852905, 'train/loss': 1.9923250675201416, 'validation/accuracy': 0.5020999908447266, 'validation/loss': 2.2343759536743164, 'validation/num_examples': 50000, 'test/accuracy': 0.38600000739097595, 'test/loss': 2.9205596446990967, 'test/num_examples': 10000, 'score': 4269.978981494904, 'total_duration': 4452.178487062454, 'accumulated_submission_time': 4269.978981494904, 'accumulated_eval_time': 179.55438876152039, 'accumulated_logging_time': 2.5754308700561523, 'global_step': 5734, 'preemption_count': 0}), (6447, {'train/accuracy': 0.5715082883834839, 'train/loss': 1.9059982299804688, 'validation/accuracy': 0.5346999764442444, 'validation/loss': 2.0963432788848877, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.7739269733428955, 'test/num_examples': 10000, 'score': 4780.21021771431, 'total_duration': 4980.572717428207, 'accumulated_submission_time': 4780.21021771431, 'accumulated_eval_time': 197.69207096099854, 'accumulated_logging_time': 2.591991662979126, 'global_step': 6447, 'preemption_count': 0}), (7161, {'train/accuracy': 0.6029974222183228, 'train/loss': 1.7656290531158447, 'validation/accuracy': 0.5460000038146973, 'validation/loss': 2.027294635772705, 'validation/num_examples': 50000, 'test/accuracy': 0.42500001192092896, 'test/loss': 2.7199454307556152, 'test/num_examples': 10000, 'score': 5291.603226661682, 'total_duration': 5510.81591963768, 'accumulated_submission_time': 5291.603226661682, 'accumulated_eval_time': 216.5176374912262, 'accumulated_logging_time': 2.607814311981201, 'global_step': 7161, 'preemption_count': 0}), (7881, {'train/accuracy': 0.6047911047935486, 'train/loss': 1.7481797933578491, 'validation/accuracy': 0.5658400058746338, 'validation/loss': 1.9366120100021362, 'validation/num_examples': 50000, 'test/accuracy': 0.43630000948905945, 'test/loss': 2.648446559906006, 'test/num_examples': 10000, 'score': 5804.2067885398865, 'total_duration': 6042.720299959183, 'accumulated_submission_time': 5804.2067885398865, 'accumulated_eval_time': 235.79255294799805, 'accumulated_logging_time': 2.625305652618408, 'global_step': 7881, 'preemption_count': 0}), (8601, {'train/accuracy': 0.6332708597183228, 'train/loss': 1.6400469541549683, 'validation/accuracy': 0.5717999935150146, 'validation/loss': 1.9243814945220947, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.608710289001465, 'test/num_examples': 10000, 'score': 6317.669976472855, 'total_duration': 6575.915004253387, 'accumulated_submission_time': 6317.669976472855, 'accumulated_eval_time': 255.49750328063965, 'accumulated_logging_time': 2.642008066177368, 'global_step': 8601, 'preemption_count': 0}), (9321, {'train/accuracy': 0.6429169178009033, 'train/loss': 1.5778065919876099, 'validation/accuracy': 0.5904399752616882, 'validation/loss': 1.8167951107025146, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.4893362522125244, 'test/num_examples': 10000, 'score': 6831.228151082993, 'total_duration': 7109.0167760849, 'accumulated_submission_time': 6831.228151082993, 'accumulated_eval_time': 275.0174503326416, 'accumulated_logging_time': 2.657024621963501, 'global_step': 9321, 'preemption_count': 0}), (10041, {'train/accuracy': 0.6679487824440002, 'train/loss': 1.4493895769119263, 'validation/accuracy': 0.6062600016593933, 'validation/loss': 1.7397924661636353, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.429551124572754, 'test/num_examples': 10000, 'score': 7344.415848016739, 'total_duration': 7641.517694711685, 'accumulated_submission_time': 7344.415848016739, 'accumulated_eval_time': 294.30417704582214, 'accumulated_logging_time': 2.673269271850586, 'global_step': 10041, 'preemption_count': 0}), (10761, {'train/accuracy': 0.698640763759613, 'train/loss': 1.324961543083191, 'validation/accuracy': 0.6159999966621399, 'validation/loss': 1.691796064376831, 'validation/num_examples': 50000, 'test/accuracy': 0.4904000163078308, 'test/loss': 2.390176296234131, 'test/num_examples': 10000, 'score': 7857.824132204056, 'total_duration': 8174.63578915596, 'accumulated_submission_time': 7857.824132204056, 'accumulated_eval_time': 313.9893500804901, 'accumulated_logging_time': 2.689209461212158, 'global_step': 10761, 'preemption_count': 0}), (11468, {'train/accuracy': 0.69144606590271, 'train/loss': 1.3458995819091797, 'validation/accuracy': 0.6236400008201599, 'validation/loss': 1.6607420444488525, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3417680263519287, 'test/num_examples': 10000, 'score': 8367.91419506073, 'total_duration': 8704.408233642578, 'accumulated_submission_time': 8367.91419506073, 'accumulated_eval_time': 333.645770072937, 'accumulated_logging_time': 2.7056169509887695, 'global_step': 11468, 'preemption_count': 0}), (12182, {'train/accuracy': 0.7506775856018066, 'train/loss': 1.0964970588684082, 'validation/accuracy': 0.6337400078773499, 'validation/loss': 1.611820101737976, 'validation/num_examples': 50000, 'test/accuracy': 0.5075000524520874, 'test/loss': 2.2831242084503174, 'test/num_examples': 10000, 'score': 8878.209555625916, 'total_duration': 9234.59941124916, 'accumulated_submission_time': 8878.209555625916, 'accumulated_eval_time': 353.5127999782562, 'accumulated_logging_time': 2.725825786590576, 'global_step': 12182, 'preemption_count': 0}), (12889, {'train/accuracy': 0.7086256146430969, 'train/loss': 1.2864586114883423, 'validation/accuracy': 0.6391400098800659, 'validation/loss': 1.6044695377349854, 'validation/num_examples': 50000, 'test/accuracy': 0.5127000212669373, 'test/loss': 2.2629776000976562, 'test/num_examples': 10000, 'score': 9388.616441965103, 'total_duration': 9764.896841049194, 'accumulated_submission_time': 9388.616441965103, 'accumulated_eval_time': 373.37684392929077, 'accumulated_logging_time': 2.7437617778778076, 'global_step': 12889, 'preemption_count': 0}), (13601, {'train/accuracy': 0.7509765625, 'train/loss': 1.0933494567871094, 'validation/accuracy': 0.6419000029563904, 'validation/loss': 1.563372254371643, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.237349271774292, 'test/num_examples': 10000, 'score': 9901.380869626999, 'total_duration': 10297.307758808136, 'accumulated_submission_time': 9901.380869626999, 'accumulated_eval_time': 392.99694299697876, 'accumulated_logging_time': 2.7616302967071533, 'global_step': 13601, 'preemption_count': 0}), (14310, {'train/accuracy': 0.7264628410339355, 'train/loss': 1.198747158050537, 'validation/accuracy': 0.6506400108337402, 'validation/loss': 1.5374244451522827, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.2349441051483154, 'test/num_examples': 10000, 'score': 10411.697776794434, 'total_duration': 10827.320155143738, 'accumulated_submission_time': 10411.697776794434, 'accumulated_eval_time': 412.66600608825684, 'accumulated_logging_time': 2.777998685836792, 'global_step': 14310, 'preemption_count': 0}), (15013, {'train/accuracy': 0.76273512840271, 'train/loss': 1.0433768033981323, 'validation/accuracy': 0.6574599742889404, 'validation/loss': 1.5106465816497803, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.173691749572754, 'test/num_examples': 10000, 'score': 10921.849606990814, 'total_duration': 11357.370880365372, 'accumulated_submission_time': 10921.849606990814, 'accumulated_eval_time': 432.53836035728455, 'accumulated_logging_time': 2.7959823608398438, 'global_step': 15013, 'preemption_count': 0}), (15710, {'train/accuracy': 0.7366071343421936, 'train/loss': 1.1491557359695435, 'validation/accuracy': 0.65829998254776, 'validation/loss': 1.5001418590545654, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.1721935272216797, 'test/num_examples': 10000, 'score': 11431.880207777023, 'total_duration': 11886.84819483757, 'accumulated_submission_time': 11431.880207777023, 'accumulated_eval_time': 451.9604871273041, 'accumulated_logging_time': 2.8121540546417236, 'global_step': 15710, 'preemption_count': 0}), (16409, {'train/accuracy': 0.7708266973495483, 'train/loss': 1.0031853914260864, 'validation/accuracy': 0.6664599776268005, 'validation/loss': 1.4600509405136108, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.1476893424987793, 'test/num_examples': 10000, 'score': 11942.240266084671, 'total_duration': 12417.146569013596, 'accumulated_submission_time': 11942.240266084671, 'accumulated_eval_time': 471.87380838394165, 'accumulated_logging_time': 2.8287084102630615, 'global_step': 16409, 'preemption_count': 0}), (17105, {'train/accuracy': 0.75296950340271, 'train/loss': 1.091102123260498, 'validation/accuracy': 0.6696199774742126, 'validation/loss': 1.4519827365875244, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.1196722984313965, 'test/num_examples': 10000, 'score': 12452.567568778992, 'total_duration': 12946.899812698364, 'accumulated_submission_time': 12452.567568778992, 'accumulated_eval_time': 491.2732219696045, 'accumulated_logging_time': 2.8467726707458496, 'global_step': 17105, 'preemption_count': 0}), (17801, {'train/accuracy': 0.7762675285339355, 'train/loss': 0.9717506766319275, 'validation/accuracy': 0.6721799969673157, 'validation/loss': 1.4405771493911743, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.1070916652679443, 'test/num_examples': 10000, 'score': 12964.237343072891, 'total_duration': 13477.450688362122, 'accumulated_submission_time': 12964.237343072891, 'accumulated_eval_time': 510.1287124156952, 'accumulated_logging_time': 2.863987684249878, 'global_step': 17801, 'preemption_count': 0}), (18501, {'train/accuracy': 0.7543646097183228, 'train/loss': 1.0677211284637451, 'validation/accuracy': 0.6736400127410889, 'validation/loss': 1.4409561157226562, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.107705593109131, 'test/num_examples': 10000, 'score': 13477.805985212326, 'total_duration': 14009.558349847794, 'accumulated_submission_time': 13477.805985212326, 'accumulated_eval_time': 528.6408140659332, 'accumulated_logging_time': 2.881488800048828, 'global_step': 18501, 'preemption_count': 0}), (19194, {'train/accuracy': 0.7835020422935486, 'train/loss': 0.9610673189163208, 'validation/accuracy': 0.6777799725532532, 'validation/loss': 1.4195795059204102, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.1053524017333984, 'test/num_examples': 10000, 'score': 13987.88150548935, 'total_duration': 14538.29470872879, 'accumulated_submission_time': 13987.88150548935, 'accumulated_eval_time': 547.2746822834015, 'accumulated_logging_time': 2.8990910053253174, 'global_step': 19194, 'preemption_count': 0}), (19881, {'train/accuracy': 0.7664620280265808, 'train/loss': 1.0314183235168457, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.4280818700790405, 'validation/num_examples': 50000, 'test/accuracy': 0.5480000376701355, 'test/loss': 2.102640390396118, 'test/num_examples': 10000, 'score': 14500.414884328842, 'total_duration': 15069.114894390106, 'accumulated_submission_time': 14500.414884328842, 'accumulated_eval_time': 565.5276010036469, 'accumulated_logging_time': 2.9234137535095215, 'global_step': 19881, 'preemption_count': 0}), (20568, {'train/accuracy': 0.7885243892669678, 'train/loss': 0.9114050269126892, 'validation/accuracy': 0.6808800101280212, 'validation/loss': 1.3792877197265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.061779260635376, 'test/num_examples': 10000, 'score': 15010.95687699318, 'total_duration': 15598.162800073624, 'accumulated_submission_time': 15010.95687699318, 'accumulated_eval_time': 584.0040574073792, 'accumulated_logging_time': 2.9434139728546143, 'global_step': 20568, 'preemption_count': 0}), (21257, {'train/accuracy': 0.7743343114852905, 'train/loss': 0.9759835600852966, 'validation/accuracy': 0.6828399896621704, 'validation/loss': 1.38865327835083, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.055602550506592, 'test/num_examples': 10000, 'score': 15520.955194711685, 'total_duration': 16126.817510604858, 'accumulated_submission_time': 15520.955194711685, 'accumulated_eval_time': 602.6339566707611, 'accumulated_logging_time': 2.960822343826294, 'global_step': 21257, 'preemption_count': 0}), (21940, {'train/accuracy': 0.7967952489852905, 'train/loss': 0.8868597149848938, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.3736740350723267, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 2.0553371906280518, 'test/num_examples': 10000, 'score': 16031.331669569016, 'total_duration': 16655.708733797073, 'accumulated_submission_time': 16031.331669569016, 'accumulated_eval_time': 621.121120929718, 'accumulated_logging_time': 2.978412628173828, 'global_step': 21940, 'preemption_count': 0}), (22621, {'train/accuracy': 0.7855947017669678, 'train/loss': 0.9289692044258118, 'validation/accuracy': 0.6868199706077576, 'validation/loss': 1.372194528579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 2.035114288330078, 'test/num_examples': 10000, 'score': 16541.68720960617, 'total_duration': 17184.752910375595, 'accumulated_submission_time': 16541.68720960617, 'accumulated_eval_time': 639.7825446128845, 'accumulated_logging_time': 2.9963178634643555, 'global_step': 22621, 'preemption_count': 0}), (23308, {'train/accuracy': 0.8003228306770325, 'train/loss': 0.883234441280365, 'validation/accuracy': 0.6845999956130981, 'validation/loss': 1.3807764053344727, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.0618605613708496, 'test/num_examples': 10000, 'score': 17051.966700077057, 'total_duration': 17713.442389965057, 'accumulated_submission_time': 17051.966700077057, 'accumulated_eval_time': 658.1653201580048, 'accumulated_logging_time': 3.014238119125366, 'global_step': 23308, 'preemption_count': 0}), (23992, {'train/accuracy': 0.7869499325752258, 'train/loss': 0.9319261908531189, 'validation/accuracy': 0.6903199553489685, 'validation/loss': 1.3706865310668945, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.054962158203125, 'test/num_examples': 10000, 'score': 17562.13594031334, 'total_duration': 18242.10812830925, 'accumulated_submission_time': 17562.13594031334, 'accumulated_eval_time': 676.631281375885, 'accumulated_logging_time': 3.035020112991333, 'global_step': 23992, 'preemption_count': 0}), (24677, {'train/accuracy': 0.8069395422935486, 'train/loss': 0.8627704381942749, 'validation/accuracy': 0.6888599991798401, 'validation/loss': 1.3719855546951294, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.044919490814209, 'test/num_examples': 10000, 'score': 18072.16477894783, 'total_duration': 18770.650775671005, 'accumulated_submission_time': 18072.16477894783, 'accumulated_eval_time': 695.1169271469116, 'accumulated_logging_time': 3.0536508560180664, 'global_step': 24677, 'preemption_count': 0}), (25360, {'train/accuracy': 0.7915138602256775, 'train/loss': 0.908660352230072, 'validation/accuracy': 0.6911599636077881, 'validation/loss': 1.3602062463760376, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.0536582469940186, 'test/num_examples': 10000, 'score': 18582.164834022522, 'total_duration': 19299.3141579628, 'accumulated_submission_time': 18582.164834022522, 'accumulated_eval_time': 713.752418756485, 'accumulated_logging_time': 3.072237491607666, 'global_step': 25360, 'preemption_count': 0}), (26041, {'train/accuracy': 0.8130181431770325, 'train/loss': 0.8215488195419312, 'validation/accuracy': 0.6912599802017212, 'validation/loss': 1.3496968746185303, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 2.0181632041931152, 'test/num_examples': 10000, 'score': 19095.26652264595, 'total_duration': 19830.89278793335, 'accumulated_submission_time': 19095.26652264595, 'accumulated_eval_time': 732.2000391483307, 'accumulated_logging_time': 3.090096950531006, 'global_step': 26041, 'preemption_count': 0}), (26722, {'train/accuracy': 0.8001833558082581, 'train/loss': 0.8687778115272522, 'validation/accuracy': 0.6958199739456177, 'validation/loss': 1.3403400182724, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 2.004615068435669, 'test/num_examples': 10000, 'score': 19605.47409749031, 'total_duration': 20359.525676965714, 'accumulated_submission_time': 19605.47409749031, 'accumulated_eval_time': 750.5971372127533, 'accumulated_logging_time': 3.108966112136841, 'global_step': 26722, 'preemption_count': 0}), (27402, {'train/accuracy': 0.8245774507522583, 'train/loss': 0.7910658121109009, 'validation/accuracy': 0.693839967250824, 'validation/loss': 1.3471602201461792, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 2.019191026687622, 'test/num_examples': 10000, 'score': 20115.59223008156, 'total_duration': 20888.158397436142, 'accumulated_submission_time': 20115.59223008156, 'accumulated_eval_time': 769.0804598331451, 'accumulated_logging_time': 3.1306746006011963, 'global_step': 27402, 'preemption_count': 0}), (28000, {'train/accuracy': 0.8056241869926453, 'train/loss': 0.8508058786392212, 'validation/accuracy': 0.6954999566078186, 'validation/loss': 1.328111171722412, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.9926079511642456, 'test/num_examples': 10000, 'score': 20562.22887468338, 'total_duration': 21353.211349010468, 'accumulated_submission_time': 20562.22887468338, 'accumulated_eval_time': 787.4695565700531, 'accumulated_logging_time': 3.148615837097168, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0502 01:17:20.525570 139795264595776 submission_runner.py:581] Timing: 20562.22887468338
I0502 01:17:20.525612 139795264595776 submission_runner.py:582] ====================
I0502 01:17:20.525756 139795264595776 submission_runner.py:645] Final imagenet_resnet score: 20562.22887468338
