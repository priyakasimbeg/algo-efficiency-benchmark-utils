python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_2/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_05-01-2023-18-58-08.log
I0501 18:58:31.453637 140561424062272 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax.
I0501 18:58:31.524961 140561424062272 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0501 18:58:32.366227 140561424062272 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0501 18:58:32.367832 140561424062272 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0501 18:58:32.373589 140561424062272 submission_runner.py:538] Using RNG seed 1128157079
I0501 18:58:35.263488 140561424062272 submission_runner.py:547] --- Tuning run 1/1 ---
I0501 18:58:35.263752 140561424062272 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1.
I0501 18:58:35.266507 140561424062272 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1/hparams.json.
I0501 18:58:35.400851 140561424062272 submission_runner.py:241] Initializing dataset.
I0501 18:58:35.417547 140561424062272 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 18:58:35.428063 140561424062272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 18:58:35.428195 140561424062272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 18:58:35.718946 140561424062272 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 18:58:37.040709 140561424062272 submission_runner.py:248] Initializing model.
I0501 18:58:49.540057 140561424062272 submission_runner.py:258] Initializing optimizer.
I0501 18:58:53.012675 140561424062272 submission_runner.py:265] Initializing metrics bundle.
I0501 18:58:53.012908 140561424062272 submission_runner.py:282] Initializing checkpoint and logger.
I0501 18:58:53.013921 140561424062272 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0501 18:58:53.881218 140561424062272 submission_runner.py:303] Saving meta data to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0501 18:58:53.882124 140561424062272 submission_runner.py:306] Saving flags to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1/flags_0.json.
I0501 18:58:53.886968 140561424062272 submission_runner.py:318] Starting training loop.
I0501 19:00:35.296966 140383535224576 logging_writer.py:48] [0] global_step=0, grad_norm=0.6052982807159424, loss=6.93840217590332
I0501 19:00:35.323918 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:00:35.919038 140561424062272 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:00:35.927536 140561424062272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:00:35.927666 140561424062272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:00:36.004584 140561424062272 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:00:48.879619 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:00:49.778272 140561424062272 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:00:49.806237 140561424062272 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0501 19:00:49.806507 140561424062272 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0501 19:00:49.866470 140561424062272 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0501 19:01:08.314820 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:01:08.835649 140561424062272 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:01:08.840831 140561424062272 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0501 19:01:08.873392 140561424062272 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0501 19:01:18.509904 140561424062272 submission_runner.py:415] Time since start: 144.62s, 	Step: 1, 	{'train/accuracy': 0.00047831633128225803, 'train/loss': 6.913181781768799, 'validation/accuracy': 0.0005999999702908099, 'validation/loss': 6.9127984046936035, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.913119316101074, 'test/num_examples': 10000, 'score': 101.43676424026489, 'total_duration': 144.62286186218262, 'accumulated_submission_time': 101.43676424026489, 'accumulated_eval_time': 43.185946464538574, 'accumulated_logging_time': 0}
I0501 19:01:18.527724 140353046816512 logging_writer.py:48] [1] accumulated_eval_time=43.185946, accumulated_logging_time=0, accumulated_submission_time=101.436764, global_step=1, preemption_count=0, score=101.436764, test/accuracy=0.000900, test/loss=6.913119, test/num_examples=10000, total_duration=144.622862, train/accuracy=0.000478, train/loss=6.913182, validation/accuracy=0.000600, validation/loss=6.912798, validation/num_examples=50000
I0501 19:01:54.754537 140353055209216 logging_writer.py:48] [100] global_step=100, grad_norm=0.6081482172012329, loss=6.868610858917236
I0501 19:02:30.896984 140353046816512 logging_writer.py:48] [200] global_step=200, grad_norm=0.6885443925857544, loss=6.7361249923706055
I0501 19:03:07.111742 140353055209216 logging_writer.py:48] [300] global_step=300, grad_norm=0.7487833499908447, loss=6.503283977508545
I0501 19:03:43.217042 140353046816512 logging_writer.py:48] [400] global_step=400, grad_norm=0.8234730362892151, loss=6.32726526260376
I0501 19:04:19.367702 140353055209216 logging_writer.py:48] [500] global_step=500, grad_norm=1.2137641906738281, loss=6.234854698181152
I0501 19:04:55.496431 140353046816512 logging_writer.py:48] [600] global_step=600, grad_norm=3.3444604873657227, loss=6.139429092407227
I0501 19:05:31.712952 140353055209216 logging_writer.py:48] [700] global_step=700, grad_norm=1.65794038772583, loss=5.96069860458374
I0501 19:06:07.859839 140353046816512 logging_writer.py:48] [800] global_step=800, grad_norm=1.948034405708313, loss=5.963492393493652
I0501 19:06:44.030091 140353055209216 logging_writer.py:48] [900] global_step=900, grad_norm=4.248128890991211, loss=5.7558207511901855
I0501 19:07:20.157399 140353046816512 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.010741949081421, loss=5.699656963348389
I0501 19:07:56.359856 140353055209216 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.3160221576690674, loss=5.6275315284729
I0501 19:08:32.484448 140353046816512 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.5937821865081787, loss=5.553303241729736
I0501 19:09:08.794101 140353055209216 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.37705397605896, loss=5.5035905838012695
I0501 19:09:44.962940 140353046816512 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.8474152088165283, loss=5.329730033874512
I0501 19:09:48.798540 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:09:55.713855 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:10:03.513556 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:10:05.841637 140561424062272 submission_runner.py:415] Time since start: 671.95s, 	Step: 1412, 	{'train/accuracy': 0.11664938926696777, 'train/loss': 4.781286239624023, 'validation/accuracy': 0.09945999830961227, 'validation/loss': 4.945098400115967, 'validation/num_examples': 50000, 'test/accuracy': 0.06960000097751617, 'test/loss': 5.2959418296813965, 'test/num_examples': 10000, 'score': 611.6793704032898, 'total_duration': 671.954597234726, 'accumulated_submission_time': 611.6793704032898, 'accumulated_eval_time': 60.229008436203, 'accumulated_logging_time': 0.026158809661865234}
I0501 19:10:05.851077 140353579493120 logging_writer.py:48] [1412] accumulated_eval_time=60.229008, accumulated_logging_time=0.026159, accumulated_submission_time=611.679370, global_step=1412, preemption_count=0, score=611.679370, test/accuracy=0.069600, test/loss=5.295942, test/num_examples=10000, total_duration=671.954597, train/accuracy=0.116649, train/loss=4.781286, validation/accuracy=0.099460, validation/loss=4.945098, validation/num_examples=50000
I0501 19:10:38.153282 140353587885824 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.024074554443359, loss=5.379273414611816
I0501 19:11:14.322317 140353579493120 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.544422149658203, loss=5.212108135223389
I0501 19:11:50.526328 140353587885824 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.6524691581726074, loss=5.158964157104492
I0501 19:12:26.686014 140353579493120 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.7712175846099854, loss=5.178177356719971
I0501 19:13:02.934624 140353587885824 logging_writer.py:48] [1900] global_step=1900, grad_norm=6.098441123962402, loss=5.04642915725708
I0501 19:13:39.064359 140353579493120 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.9375863075256348, loss=4.973358154296875
I0501 19:14:15.288042 140353587885824 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.512019634246826, loss=4.935279846191406
I0501 19:14:51.439550 140353579493120 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.9732842445373535, loss=4.8784918785095215
I0501 19:15:27.668904 140353587885824 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.632762432098389, loss=4.689503192901611
I0501 19:16:03.799207 140353579493120 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.6781105995178223, loss=4.787459373474121
I0501 19:16:39.999987 140353587885824 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.2990498542785645, loss=4.6846394538879395
I0501 19:17:16.070561 140353579493120 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.0012967586517334, loss=4.498536109924316
I0501 19:17:52.349579 140353587885824 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.367214202880859, loss=4.443130016326904
I0501 19:18:28.508742 140353579493120 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.9975671768188477, loss=4.460844039916992
I0501 19:18:35.968575 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:18:43.028040 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:18:50.859442 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:18:53.092202 140561424062272 submission_runner.py:415] Time since start: 1199.21s, 	Step: 2822, 	{'train/accuracy': 0.2737962305545807, 'train/loss': 3.5180482864379883, 'validation/accuracy': 0.2448599934577942, 'validation/loss': 3.69538950920105, 'validation/num_examples': 50000, 'test/accuracy': 0.18710000813007355, 'test/loss': 4.203489303588867, 'test/num_examples': 10000, 'score': 1121.7690916061401, 'total_duration': 1199.205161333084, 'accumulated_submission_time': 1121.7690916061401, 'accumulated_eval_time': 77.35259890556335, 'accumulated_logging_time': 0.04346895217895508}
I0501 19:18:53.100990 140353587885824 logging_writer.py:48] [2822] accumulated_eval_time=77.352599, accumulated_logging_time=0.043469, accumulated_submission_time=1121.769092, global_step=2822, preemption_count=0, score=1121.769092, test/accuracy=0.187100, test/loss=4.203489, test/num_examples=10000, total_duration=1199.205161, train/accuracy=0.273796, train/loss=3.518048, validation/accuracy=0.244860, validation/loss=3.695390, validation/num_examples=50000
I0501 19:19:21.750813 140353579493120 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.0483880043029785, loss=4.404845237731934
I0501 19:19:57.912652 140353587885824 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.703434944152832, loss=4.223730087280273
I0501 19:20:34.150989 140353579493120 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.2203879356384277, loss=4.261455059051514
I0501 19:21:10.298274 140353587885824 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.350529670715332, loss=4.226844787597656
I0501 19:21:46.504984 140353579493120 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.7160487174987793, loss=4.1286091804504395
I0501 19:22:22.646763 140353587885824 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.019576072692871, loss=4.113731861114502
I0501 19:22:58.883155 140353579493120 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.766932487487793, loss=4.075690269470215
I0501 19:23:35.050937 140353587885824 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.972506046295166, loss=4.05970573425293
I0501 19:24:11.307260 140353579493120 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.288991689682007, loss=3.954998254776001
I0501 19:24:47.425662 140353587885824 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.5694339275360107, loss=3.9352710247039795
I0501 19:25:23.677561 140353579493120 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.2755441665649414, loss=3.8531906604766846
I0501 19:25:59.821211 140353587885824 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.274970054626465, loss=3.8182663917541504
I0501 19:26:36.085643 140353579493120 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7100698947906494, loss=3.9213502407073975
I0501 19:27:12.242745 140353587885824 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.620213270187378, loss=3.8345155715942383
I0501 19:27:23.319423 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:27:30.289006 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:27:38.129183 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:27:40.247040 140561424062272 submission_runner.py:415] Time since start: 1726.36s, 	Step: 4232, 	{'train/accuracy': 0.40963807702064514, 'train/loss': 2.7385308742523193, 'validation/accuracy': 0.37477999925613403, 'validation/loss': 2.9342598915100098, 'validation/num_examples': 50000, 'test/accuracy': 0.281900018453598, 'test/loss': 3.594959259033203, 'test/num_examples': 10000, 'score': 1631.9597959518433, 'total_duration': 1726.3599789142609, 'accumulated_submission_time': 1631.9597959518433, 'accumulated_eval_time': 94.2801604270935, 'accumulated_logging_time': 0.06043362617492676}
I0501 19:27:40.257913 140353579493120 logging_writer.py:48] [4232] accumulated_eval_time=94.280160, accumulated_logging_time=0.060434, accumulated_submission_time=1631.959796, global_step=4232, preemption_count=0, score=1631.959796, test/accuracy=0.281900, test/loss=3.594959, test/num_examples=10000, total_duration=1726.359979, train/accuracy=0.409638, train/loss=2.738531, validation/accuracy=0.374780, validation/loss=2.934260, validation/num_examples=50000
I0501 19:28:05.304289 140353587885824 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.0359432697296143, loss=3.757716655731201
I0501 19:28:41.440068 140353579493120 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.8758289813995361, loss=3.7885494232177734
I0501 19:29:17.575019 140353587885824 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.8996728658676147, loss=3.6714067459106445
I0501 19:29:53.817439 140353579493120 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.0805203914642334, loss=3.6778454780578613
I0501 19:30:30.075169 140353587885824 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.05716872215271, loss=3.7701237201690674
I0501 19:31:06.176520 140353579493120 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.8202857971191406, loss=3.787600517272949
I0501 19:31:42.427620 140353587885824 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.8306231498718262, loss=3.591583013534546
I0501 19:32:18.536091 140353579493120 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5122207403182983, loss=3.563600778579712
I0501 19:32:54.663188 140353587885824 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.6402000188827515, loss=3.553593397140503
I0501 19:33:30.879961 140353579493120 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.8145315647125244, loss=3.6175453662872314
I0501 19:34:07.056110 140353587885824 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6820696592330933, loss=3.447659969329834
I0501 19:34:43.191027 140353579493120 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.2630289793014526, loss=3.49752140045166
I0501 19:35:19.424393 140353587885824 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.228975534439087, loss=3.4892311096191406
I0501 19:35:55.568782 140353579493120 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.2952979803085327, loss=3.478902578353882
I0501 19:36:10.607251 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:36:17.553551 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:36:25.572129 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:36:27.813166 140561424062272 submission_runner.py:415] Time since start: 2253.93s, 	Step: 5643, 	{'train/accuracy': 0.4970104992389679, 'train/loss': 2.298945188522339, 'validation/accuracy': 0.4556399881839752, 'validation/loss': 2.5166385173797607, 'validation/num_examples': 50000, 'test/accuracy': 0.3483000099658966, 'test/loss': 3.1608636379241943, 'test/num_examples': 10000, 'score': 2142.2815775871277, 'total_duration': 2253.9261000156403, 'accumulated_submission_time': 2142.2815775871277, 'accumulated_eval_time': 111.48601818084717, 'accumulated_logging_time': 0.07985401153564453}
I0501 19:36:27.824689 140353587885824 logging_writer.py:48] [5643] accumulated_eval_time=111.486018, accumulated_logging_time=0.079854, accumulated_submission_time=2142.281578, global_step=5643, preemption_count=0, score=2142.281578, test/accuracy=0.348300, test/loss=3.160864, test/num_examples=10000, total_duration=2253.926100, train/accuracy=0.497010, train/loss=2.298945, validation/accuracy=0.455640, validation/loss=2.516639, validation/num_examples=50000
I0501 19:36:48.868620 140353579493120 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.6528147459030151, loss=3.473907232284546
I0501 19:37:24.967425 140353587885824 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.2697715759277344, loss=3.3002872467041016
I0501 19:38:01.087710 140353579493120 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.8531434535980225, loss=3.3207249641418457
I0501 19:38:37.282916 140353587885824 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.2655810117721558, loss=3.327099561691284
I0501 19:39:13.363263 140353579493120 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.0547770261764526, loss=3.3314568996429443
I0501 19:39:49.563318 140353587885824 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.6078391075134277, loss=3.275167465209961
I0501 19:40:25.793442 140353579493120 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.273005485534668, loss=3.241807699203491
I0501 19:41:01.922088 140353587885824 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.3240596055984497, loss=3.219411849975586
I0501 19:41:38.074798 140353579493120 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.640788197517395, loss=3.2963671684265137
I0501 19:42:14.267478 140353587885824 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.4349877834320068, loss=3.157010555267334
I0501 19:42:50.357248 140353579493120 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.324436068534851, loss=3.12681245803833
I0501 19:43:26.520268 140353587885824 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9124764800071716, loss=3.317782402038574
I0501 19:44:02.754921 140353579493120 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.2155696153640747, loss=3.177302837371826
I0501 19:44:38.910194 140353587885824 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.6056411266326904, loss=3.34700870513916
I0501 19:44:57.872388 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:45:04.738154 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:45:12.678195 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:45:15.013407 140561424062272 submission_runner.py:415] Time since start: 2781.13s, 	Step: 7054, 	{'train/accuracy': 0.5483697056770325, 'train/loss': 2.02362322807312, 'validation/accuracy': 0.5046799778938293, 'validation/loss': 2.237910032272339, 'validation/num_examples': 50000, 'test/accuracy': 0.38510000705718994, 'test/loss': 2.923192024230957, 'test/num_examples': 10000, 'score': 2652.3014986515045, 'total_duration': 2781.12637090683, 'accumulated_submission_time': 2652.3014986515045, 'accumulated_eval_time': 128.62700939178467, 'accumulated_logging_time': 0.10037040710449219}
I0501 19:45:15.023082 140353579493120 logging_writer.py:48] [7054] accumulated_eval_time=128.627009, accumulated_logging_time=0.100370, accumulated_submission_time=2652.301499, global_step=7054, preemption_count=0, score=2652.301499, test/accuracy=0.385100, test/loss=2.923192, test/num_examples=10000, total_duration=2781.126371, train/accuracy=0.548370, train/loss=2.023623, validation/accuracy=0.504680, validation/loss=2.237910, validation/num_examples=50000
I0501 19:45:31.975839 140353587885824 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8310227990150452, loss=3.227391481399536
I0501 19:46:08.172859 140353579493120 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.1591265201568604, loss=3.1938023567199707
I0501 19:46:44.326488 140353587885824 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8176552653312683, loss=3.186739683151245
I0501 19:47:20.480246 140353579493120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9404641389846802, loss=3.0712814331054688
I0501 19:47:56.572997 140353587885824 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8817226886749268, loss=3.1438183784484863
I0501 19:48:32.768738 140353579493120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7384527325630188, loss=3.1538963317871094
I0501 19:49:08.888628 140353587885824 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.152740478515625, loss=3.1319451332092285
I0501 19:49:45.084002 140353579493120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9416418075561523, loss=3.0474891662597656
I0501 19:50:21.192396 140353587885824 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7752628326416016, loss=3.098145008087158
I0501 19:50:57.383803 140353579493120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9091905951499939, loss=3.12304425239563
I0501 19:51:33.433134 140353587885824 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8230466842651367, loss=3.089052200317383
I0501 19:52:09.644013 140353579493120 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.1485092639923096, loss=3.1315882205963135
I0501 19:52:45.672261 140353587885824 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7340289950370789, loss=3.023729085922241
I0501 19:53:21.926121 140353579493120 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.744564414024353, loss=3.0491020679473877
I0501 19:53:45.233996 140561424062272 spec.py:298] Evaluating on the training split.
I0501 19:53:52.097822 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 19:54:00.838632 140561424062272 spec.py:326] Evaluating on the test split.
I0501 19:54:03.152873 140561424062272 submission_runner.py:415] Time since start: 3309.27s, 	Step: 8466, 	{'train/accuracy': 0.5847018361091614, 'train/loss': 1.8203755617141724, 'validation/accuracy': 0.5388999581336975, 'validation/loss': 2.0554256439208984, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.7409839630126953, 'test/num_examples': 10000, 'score': 3162.485634326935, 'total_duration': 3309.2658355236053, 'accumulated_submission_time': 3162.485634326935, 'accumulated_eval_time': 146.54587507247925, 'accumulated_logging_time': 0.1181333065032959}
I0501 19:54:03.166684 140353587885824 logging_writer.py:48] [8466] accumulated_eval_time=146.545875, accumulated_logging_time=0.118133, accumulated_submission_time=3162.485634, global_step=8466, preemption_count=0, score=3162.485634, test/accuracy=0.424200, test/loss=2.740984, test/num_examples=10000, total_duration=3309.265836, train/accuracy=0.584702, train/loss=1.820376, validation/accuracy=0.538900, validation/loss=2.055426, validation/num_examples=50000
I0501 19:54:15.781313 140353579493120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9220514893531799, loss=2.9876301288604736
I0501 19:54:51.978951 140353587885824 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6816471219062805, loss=2.9817862510681152
I0501 19:55:28.089740 140353579493120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9463745355606079, loss=3.171966791152954
I0501 19:56:04.310420 140353587885824 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6484649181365967, loss=3.179741859436035
I0501 19:56:40.418187 140353579493120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5971015095710754, loss=3.007650852203369
I0501 19:57:16.660949 140353587885824 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5616204738616943, loss=2.996852159500122
I0501 19:57:52.729596 140353579493120 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7664798498153687, loss=3.068964958190918
I0501 19:58:28.880792 140353587885824 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6238298416137695, loss=2.9937400817871094
I0501 19:59:05.028976 140353579493120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7220369577407837, loss=2.947108268737793
I0501 19:59:41.158348 140353587885824 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.46696600317955017, loss=2.9701828956604004
I0501 20:00:17.343750 140353579493120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5051429271697998, loss=2.9575448036193848
I0501 20:00:53.563098 140353587885824 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5344189405441284, loss=2.9446024894714355
I0501 20:01:29.646219 140353579493120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5800781846046448, loss=2.988131523132324
I0501 20:02:05.840985 140353587885824 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5507219433784485, loss=2.9573569297790527
I0501 20:02:33.517886 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:02:40.391571 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:02:49.798247 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:02:51.998107 140561424062272 submission_runner.py:415] Time since start: 3838.11s, 	Step: 9878, 	{'train/accuracy': 0.6240234375, 'train/loss': 1.6609609127044678, 'validation/accuracy': 0.5707799792289734, 'validation/loss': 1.9146699905395508, 'validation/num_examples': 50000, 'test/accuracy': 0.4416000247001648, 'test/loss': 2.623234510421753, 'test/num_examples': 10000, 'score': 3672.8094897270203, 'total_duration': 3838.1110508441925, 'accumulated_submission_time': 3672.8094897270203, 'accumulated_eval_time': 165.02604842185974, 'accumulated_logging_time': 0.14054155349731445}
I0501 20:02:52.009762 140353579493120 logging_writer.py:48] [9878] accumulated_eval_time=165.026048, accumulated_logging_time=0.140542, accumulated_submission_time=3672.809490, global_step=9878, preemption_count=0, score=3672.809490, test/accuracy=0.441600, test/loss=2.623235, test/num_examples=10000, total_duration=3838.111051, train/accuracy=0.624023, train/loss=1.660961, validation/accuracy=0.570780, validation/loss=1.914670, validation/num_examples=50000
I0501 20:03:00.327313 140353587885824 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.506174623966217, loss=2.8330352306365967
I0501 20:03:36.579766 140353579493120 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7180017232894897, loss=2.9135122299194336
I0501 20:04:12.644597 140353587885824 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.48370441794395447, loss=2.9022345542907715
I0501 20:04:48.742422 140353579493120 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4344013035297394, loss=2.912266492843628
I0501 20:05:24.936013 140353587885824 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6233492493629456, loss=2.9577414989471436
I0501 20:06:01.033376 140353579493120 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5184080004692078, loss=2.8378119468688965
I0501 20:06:37.267939 140353587885824 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7072452902793884, loss=2.9355413913726807
I0501 20:07:13.459883 140353579493120 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6163250803947449, loss=2.8471319675445557
I0501 20:07:49.586438 140353587885824 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.450880229473114, loss=2.8777403831481934
I0501 20:08:25.716323 140353579493120 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.501549482345581, loss=2.8965837955474854
I0501 20:09:01.973288 140353587885824 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.42875540256500244, loss=2.8202333450317383
I0501 20:09:38.115130 140353579493120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.4044601619243622, loss=2.8229217529296875
I0501 20:10:14.354580 140353587885824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.41652223467826843, loss=2.717280387878418
I0501 20:10:50.670856 140353579493120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5005518794059753, loss=2.8388314247131348
I0501 20:11:22.300078 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:11:29.245832 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:11:38.508546 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:11:40.857806 140561424062272 submission_runner.py:415] Time since start: 4366.97s, 	Step: 11289, 	{'train/accuracy': 0.648855984210968, 'train/loss': 1.540156602859497, 'validation/accuracy': 0.5960599780082703, 'validation/loss': 1.7866590023040771, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.4964287281036377, 'test/num_examples': 10000, 'score': 4183.07145357132, 'total_duration': 4366.970770597458, 'accumulated_submission_time': 4183.07145357132, 'accumulated_eval_time': 183.583753824234, 'accumulated_logging_time': 0.1615769863128662}
I0501 20:11:40.867915 140353587885824 logging_writer.py:48] [11289] accumulated_eval_time=183.583754, accumulated_logging_time=0.161577, accumulated_submission_time=4183.071454, global_step=11289, preemption_count=0, score=4183.071454, test/accuracy=0.469800, test/loss=2.496429, test/num_examples=10000, total_duration=4366.970771, train/accuracy=0.648856, train/loss=1.540157, validation/accuracy=0.596060, validation/loss=1.786659, validation/num_examples=50000
I0501 20:11:45.199912 140353579493120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5104659199714661, loss=2.7239418029785156
I0501 20:12:21.359104 140353587885824 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.4760178029537201, loss=2.7281675338745117
I0501 20:12:57.453233 140353579493120 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3987681269645691, loss=2.8123064041137695
I0501 20:13:33.657641 140353587885824 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4352039098739624, loss=2.775209426879883
I0501 20:14:09.754961 140353579493120 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.47886011004447937, loss=2.729794502258301
I0501 20:14:45.842400 140353587885824 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4923676550388336, loss=2.8464975357055664
I0501 20:15:22.101190 140353579493120 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5587942004203796, loss=2.6554861068725586
I0501 20:15:58.196774 140353587885824 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.430593341588974, loss=2.6548807621002197
I0501 20:16:34.462306 140353579493120 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.42447012662887573, loss=2.766294240951538
I0501 20:17:10.729185 140353587885824 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4711476266384125, loss=2.840531349182129
I0501 20:17:46.840711 140353579493120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3994482457637787, loss=2.804144859313965
I0501 20:18:22.939834 140353587885824 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4112224578857422, loss=2.8550312519073486
I0501 20:18:59.149858 140353579493120 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3447253406047821, loss=2.657522201538086
I0501 20:19:35.295634 140353587885824 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.38548436760902405, loss=2.7580103874206543
I0501 20:20:10.957811 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:20:18.546995 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:20:28.058673 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:20:30.384818 140561424062272 submission_runner.py:415] Time since start: 4896.50s, 	Step: 12700, 	{'train/accuracy': 0.6737882494926453, 'train/loss': 1.4531214237213135, 'validation/accuracy': 0.6132400035858154, 'validation/loss': 1.732688546180725, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.4086310863494873, 'test/num_examples': 10000, 'score': 4693.132774591446, 'total_duration': 4896.496756315231, 'accumulated_submission_time': 4693.132774591446, 'accumulated_eval_time': 203.00971841812134, 'accumulated_logging_time': 0.18139219284057617}
I0501 20:20:30.399571 140353579493120 logging_writer.py:48] [12700] accumulated_eval_time=203.009718, accumulated_logging_time=0.181392, accumulated_submission_time=4693.132775, global_step=12700, preemption_count=0, score=4693.132775, test/accuracy=0.489300, test/loss=2.408631, test/num_examples=10000, total_duration=4896.496756, train/accuracy=0.673788, train/loss=1.453121, validation/accuracy=0.613240, validation/loss=1.732689, validation/num_examples=50000
I0501 20:20:30.789264 140353587885824 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3633497655391693, loss=2.6052684783935547
I0501 20:21:06.854514 140353579493120 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.37416210770606995, loss=2.6536502838134766
I0501 20:21:43.025164 140353587885824 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3220313489437103, loss=2.7294371128082275
I0501 20:22:19.240567 140353579493120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3950374126434326, loss=2.6185967922210693
I0501 20:22:55.391324 140353587885824 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3652999997138977, loss=2.6925151348114014
I0501 20:23:31.447312 140353579493120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.46406543254852295, loss=2.681560516357422
I0501 20:24:07.717107 140353587885824 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4502509832382202, loss=2.7615513801574707
I0501 20:24:43.787601 140353579493120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3697442412376404, loss=2.748610734939575
I0501 20:25:20.028585 140353587885824 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.39232337474823, loss=2.6386048793792725
I0501 20:25:56.161998 140353579493120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.35288205742836, loss=2.760186195373535
I0501 20:26:32.424568 140353587885824 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3195578157901764, loss=2.6035518646240234
I0501 20:27:08.589863 140353579493120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.37431904673576355, loss=2.706707000732422
I0501 20:27:44.854985 140353587885824 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.40189528465270996, loss=2.681262254714966
I0501 20:28:20.984435 140353579493120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.35231831669807434, loss=2.6763856410980225
I0501 20:28:57.205541 140353587885824 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3400501310825348, loss=2.6594619750976562
I0501 20:29:00.677749 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:29:07.763151 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:29:17.459513 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:29:19.789693 140561424062272 submission_runner.py:415] Time since start: 5425.90s, 	Step: 14111, 	{'train/accuracy': 0.6912268400192261, 'train/loss': 1.3569697141647339, 'validation/accuracy': 0.6285399794578552, 'validation/loss': 1.6437764167785645, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.3190810680389404, 'test/num_examples': 10000, 'score': 5203.381317853928, 'total_duration': 5425.90149140358, 'accumulated_submission_time': 5203.381317853928, 'accumulated_eval_time': 222.12046575546265, 'accumulated_logging_time': 0.20693016052246094}
I0501 20:29:19.801368 140353579493120 logging_writer.py:48] [14111] accumulated_eval_time=222.120466, accumulated_logging_time=0.206930, accumulated_submission_time=5203.381318, global_step=14111, preemption_count=0, score=5203.381318, test/accuracy=0.505700, test/loss=2.319081, test/num_examples=10000, total_duration=5425.901491, train/accuracy=0.691227, train/loss=1.356970, validation/accuracy=0.628540, validation/loss=1.643776, validation/num_examples=50000
I0501 20:29:52.293270 140353587885824 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.37471356987953186, loss=2.6869192123413086
I0501 20:30:28.422952 140353579493120 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3818112909793854, loss=2.6260602474212646
I0501 20:31:04.544452 140353587885824 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.33251816034317017, loss=2.621673822402954
I0501 20:31:40.738615 140353579493120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.32309865951538086, loss=2.7584145069122314
I0501 20:32:16.849016 140353587885824 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.37603914737701416, loss=2.637957811355591
I0501 20:32:53.072958 140353579493120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3236059546470642, loss=2.729461908340454
I0501 20:33:29.180755 140353587885824 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.34068068861961365, loss=2.583109140396118
I0501 20:34:05.319139 140353579493120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3252124488353729, loss=2.5802221298217773
I0501 20:34:41.498827 140353587885824 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3715047538280487, loss=2.5942788124084473
I0501 20:35:17.622127 140353579493120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.39487317204475403, loss=2.641317367553711
I0501 20:35:53.847881 140353587885824 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.35785263776779175, loss=2.6471266746520996
I0501 20:36:30.067469 140353579493120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.44710981845855713, loss=2.560136318206787
I0501 20:37:06.167382 140353587885824 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3211977183818817, loss=2.5912857055664062
I0501 20:37:42.278521 140353579493120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.347709596157074, loss=2.5855796337127686
I0501 20:37:50.106347 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:37:57.852834 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:38:07.711066 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:38:10.034864 140561424062272 submission_runner.py:415] Time since start: 5956.15s, 	Step: 15523, 	{'train/accuracy': 0.7023875713348389, 'train/loss': 1.314147710800171, 'validation/accuracy': 0.6282399892807007, 'validation/loss': 1.6419637203216553, 'validation/num_examples': 50000, 'test/accuracy': 0.5023000240325928, 'test/loss': 2.336332082748413, 'test/num_examples': 10000, 'score': 5713.658360004425, 'total_duration': 5956.146651983261, 'accumulated_submission_time': 5713.658360004425, 'accumulated_eval_time': 242.04777693748474, 'accumulated_logging_time': 0.22761869430541992}
I0501 20:38:10.049734 140353587885824 logging_writer.py:48] [15523] accumulated_eval_time=242.047777, accumulated_logging_time=0.227619, accumulated_submission_time=5713.658360, global_step=15523, preemption_count=0, score=5713.658360, test/accuracy=0.502300, test/loss=2.336332, test/num_examples=10000, total_duration=5956.146652, train/accuracy=0.702388, train/loss=1.314148, validation/accuracy=0.628240, validation/loss=1.641964, validation/num_examples=50000
I0501 20:38:38.259675 140353579493120 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.365254670381546, loss=2.6110634803771973
I0501 20:39:14.391000 140353587885824 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3564148247241974, loss=2.561058521270752
I0501 20:39:50.586740 140353579493120 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3630516231060028, loss=2.59869647026062
I0501 20:40:26.719196 140353587885824 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3274695873260498, loss=2.715017795562744
I0501 20:41:02.898900 140353579493120 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.33165600895881653, loss=2.5448737144470215
I0501 20:41:38.981848 140353587885824 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3331453502178192, loss=2.564587116241455
I0501 20:42:15.185199 140353579493120 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.36988019943237305, loss=2.6400375366210938
I0501 20:42:51.261897 140353587885824 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3154270350933075, loss=2.5884041786193848
I0501 20:43:27.544736 140353579493120 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.32260438799858093, loss=2.595186233520508
I0501 20:44:03.679434 140353587885824 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.37978529930114746, loss=2.5111796855926514
I0501 20:44:39.746635 140353579493120 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.33372750878334045, loss=2.615096092224121
I0501 20:45:16.020646 140353587885824 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.37032559514045715, loss=2.6192991733551025
I0501 20:45:52.123949 140353579493120 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3035900294780731, loss=2.5608818531036377
I0501 20:46:28.362419 140353587885824 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3238259255886078, loss=2.498987913131714
I0501 20:46:40.146121 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:46:47.346966 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:46:57.289456 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:46:59.290820 140561424062272 submission_runner.py:415] Time since start: 6485.40s, 	Step: 16934, 	{'train/accuracy': 0.7584900856018066, 'train/loss': 1.0991981029510498, 'validation/accuracy': 0.6437399983406067, 'validation/loss': 1.5964267253875732, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.2868399620056152, 'test/num_examples': 10000, 'score': 6223.723320245743, 'total_duration': 6485.402654647827, 'accumulated_submission_time': 6223.723320245743, 'accumulated_eval_time': 261.19131207466125, 'accumulated_logging_time': 0.25510501861572266}
I0501 20:46:59.302210 140353579493120 logging_writer.py:48] [16934] accumulated_eval_time=261.191312, accumulated_logging_time=0.255105, accumulated_submission_time=6223.723320, global_step=16934, preemption_count=0, score=6223.723320, test/accuracy=0.516400, test/loss=2.286840, test/num_examples=10000, total_duration=6485.402655, train/accuracy=0.758490, train/loss=1.099198, validation/accuracy=0.643740, validation/loss=1.596427, validation/num_examples=50000
I0501 20:47:23.650535 140353587885824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.33882346749305725, loss=2.5756781101226807
I0501 20:47:59.760980 140353579493120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3174953758716583, loss=2.5275189876556396
I0501 20:48:35.986696 140353587885824 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2806774973869324, loss=2.5179741382598877
I0501 20:49:12.117436 140353579493120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.352124959230423, loss=2.5053980350494385
I0501 20:49:48.367137 140353587885824 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.32347625494003296, loss=2.5446527004241943
I0501 20:50:24.535425 140353579493120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3382345736026764, loss=2.4924089908599854
I0501 20:51:00.675435 140353587885824 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2991349697113037, loss=2.569545269012451
I0501 20:51:36.884308 140353579493120 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.30048248171806335, loss=2.5064313411712646
I0501 20:52:12.968229 140353587885824 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.31800180673599243, loss=2.4164552688598633
I0501 20:52:49.184878 140353579493120 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3140842616558075, loss=2.51552677154541
I0501 20:53:25.295544 140353587885824 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3060934841632843, loss=2.5773191452026367
I0501 20:54:01.562155 140353579493120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3195801079273224, loss=2.586127281188965
I0501 20:54:37.671396 140353587885824 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3201536536216736, loss=2.4963836669921875
I0501 20:55:13.898528 140353579493120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.2843831777572632, loss=2.6167187690734863
I0501 20:55:29.631457 140561424062272 spec.py:298] Evaluating on the training split.
I0501 20:55:37.411471 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 20:55:47.274742 140561424062272 spec.py:326] Evaluating on the test split.
I0501 20:55:49.417872 140561424062272 submission_runner.py:415] Time since start: 7015.53s, 	Step: 18345, 	{'train/accuracy': 0.7609016299247742, 'train/loss': 1.08008873462677, 'validation/accuracy': 0.6540799736976624, 'validation/loss': 1.5490473508834839, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.2369234561920166, 'test/num_examples': 10000, 'score': 6734.023561000824, 'total_duration': 7015.529815673828, 'accumulated_submission_time': 6734.023561000824, 'accumulated_eval_time': 280.97669291496277, 'accumulated_logging_time': 0.27663397789001465}
I0501 20:55:49.433726 140353587885824 logging_writer.py:48] [18345] accumulated_eval_time=280.976693, accumulated_logging_time=0.276634, accumulated_submission_time=6734.023561, global_step=18345, preemption_count=0, score=6734.023561, test/accuracy=0.526600, test/loss=2.236923, test/num_examples=10000, total_duration=7015.529816, train/accuracy=0.760902, train/loss=1.080089, validation/accuracy=0.654080, validation/loss=1.549047, validation/num_examples=50000
I0501 20:56:09.638051 140353579493120 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.32516154646873474, loss=2.4407992362976074
I0501 20:56:45.829328 140353587885824 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2973068654537201, loss=2.576444625854492
I0501 20:57:21.916716 140353579493120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3104982376098633, loss=2.5161564350128174
I0501 20:57:58.097283 140353587885824 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.29695382714271545, loss=2.536961078643799
I0501 20:58:34.198293 140353579493120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.31431257724761963, loss=2.514458179473877
I0501 20:59:10.454861 140353587885824 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.36731305718421936, loss=2.471561908721924
I0501 20:59:46.559666 140353579493120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3134535551071167, loss=2.5348401069641113
I0501 21:00:22.695855 140353587885824 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2792763113975525, loss=2.4664392471313477
I0501 21:00:58.884779 140353579493120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3220497965812683, loss=2.454106569290161
I0501 21:01:35.028483 140353587885824 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2727482318878174, loss=2.5595576763153076
I0501 21:02:11.281026 140353579493120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3070638179779053, loss=2.4195871353149414
I0501 21:02:47.418942 140353587885824 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.318181574344635, loss=2.44933819770813
I0501 21:03:23.659520 140353579493120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.32331275939941406, loss=2.506866693496704
I0501 21:03:59.850440 140353587885824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.31457653641700745, loss=2.4920902252197266
I0501 21:04:19.585417 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:04:26.838079 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:04:36.756979 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:04:39.114647 140561424062272 submission_runner.py:415] Time since start: 7545.23s, 	Step: 19756, 	{'train/accuracy': 0.7610211968421936, 'train/loss': 1.061102271080017, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.508509635925293, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.194878101348877, 'test/num_examples': 10000, 'score': 7244.145133733749, 'total_duration': 7545.226433515549, 'accumulated_submission_time': 7244.145133733749, 'accumulated_eval_time': 300.50472021102905, 'accumulated_logging_time': 0.30364990234375}
I0501 21:04:39.125948 140353579493120 logging_writer.py:48] [19756] accumulated_eval_time=300.504720, accumulated_logging_time=0.303650, accumulated_submission_time=7244.145134, global_step=19756, preemption_count=0, score=7244.145134, test/accuracy=0.536100, test/loss=2.194878, test/num_examples=10000, total_duration=7545.226434, train/accuracy=0.761021, train/loss=1.061102, validation/accuracy=0.660320, validation/loss=1.508510, validation/num_examples=50000
I0501 21:04:55.477398 140353587885824 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3131712079048157, loss=2.4980950355529785
I0501 21:05:31.630006 140353579493120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3195526599884033, loss=2.491806745529175
I0501 21:06:07.903121 140353587885824 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.279373437166214, loss=2.434911012649536
I0501 21:06:44.033069 140353579493120 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.30498844385147095, loss=2.4431276321411133
I0501 21:07:20.277169 140353587885824 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.2976815104484558, loss=2.485549211502075
I0501 21:07:56.406447 140353579493120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.303542822599411, loss=2.4723010063171387
I0501 21:08:32.546829 140353587885824 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.3441540598869324, loss=2.624798059463501
I0501 21:09:08.857797 140353579493120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.280999094247818, loss=2.41336727142334
I0501 21:09:44.968564 140353587885824 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.2674557566642761, loss=2.3618998527526855
I0501 21:10:21.222642 140353579493120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3324262499809265, loss=2.4352011680603027
I0501 21:10:57.451109 140353587885824 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3254495859146118, loss=2.461714267730713
I0501 21:11:33.611061 140353579493120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3113466203212738, loss=2.463407516479492
I0501 21:12:09.774527 140353587885824 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.28996145725250244, loss=2.5004780292510986
I0501 21:12:46.005853 140353579493120 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.28472384810447693, loss=2.434347629547119
I0501 21:13:09.363138 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:13:16.685021 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:13:26.609302 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:13:28.926216 140561424062272 submission_runner.py:415] Time since start: 8075.04s, 	Step: 21166, 	{'train/accuracy': 0.7658043503761292, 'train/loss': 1.0472811460494995, 'validation/accuracy': 0.6670199632644653, 'validation/loss': 1.4846476316452026, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.1873974800109863, 'test/num_examples': 10000, 'score': 7754.353482961655, 'total_duration': 8075.038236141205, 'accumulated_submission_time': 7754.353482961655, 'accumulated_eval_time': 320.06684279441833, 'accumulated_logging_time': 0.3241441249847412}
I0501 21:13:28.943970 140353587885824 logging_writer.py:48] [21166] accumulated_eval_time=320.066843, accumulated_logging_time=0.324144, accumulated_submission_time=7754.353483, global_step=21166, preemption_count=0, score=7754.353483, test/accuracy=0.542300, test/loss=2.187397, test/num_examples=10000, total_duration=8075.038236, train/accuracy=0.765804, train/loss=1.047281, validation/accuracy=0.667020, validation/loss=1.484648, validation/num_examples=50000
I0501 21:13:41.594214 140353579493120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.3352060914039612, loss=2.4463491439819336
I0501 21:14:17.812185 140353587885824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.29076477885246277, loss=2.4408576488494873
I0501 21:14:53.928737 140353579493120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.2616766691207886, loss=2.4190073013305664
I0501 21:15:30.140947 140353587885824 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2860317826271057, loss=2.5447998046875
I0501 21:16:06.276138 140353579493120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3153768479824066, loss=2.4250714778900146
I0501 21:16:42.526248 140353587885824 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3225865364074707, loss=2.50815749168396
I0501 21:17:18.668550 140353579493120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.27046507596969604, loss=2.419832706451416
I0501 21:17:54.959241 140353587885824 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.2746524214744568, loss=2.399456024169922
I0501 21:18:31.055101 140353579493120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.31144022941589355, loss=2.385164737701416
I0501 21:19:07.353407 140353587885824 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.25710517168045044, loss=2.2927048206329346
I0501 21:19:43.503219 140353579493120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3187291622161865, loss=2.4364447593688965
I0501 21:20:19.651425 140353587885824 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.2866886258125305, loss=2.4678597450256348
I0501 21:20:55.872472 140353579493120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.2866642475128174, loss=2.2928543090820312
I0501 21:21:32.018045 140353587885824 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3073597550392151, loss=2.5350944995880127
I0501 21:21:59.127466 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:22:06.871840 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:22:16.960832 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:22:19.266987 140561424062272 submission_runner.py:415] Time since start: 8605.38s, 	Step: 22576, 	{'train/accuracy': 0.7651665806770325, 'train/loss': 1.0214049816131592, 'validation/accuracy': 0.6650399565696716, 'validation/loss': 1.4704805612564087, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.1716396808624268, 'test/num_examples': 10000, 'score': 8264.506025075912, 'total_duration': 8605.378944635391, 'accumulated_submission_time': 8264.506025075912, 'accumulated_eval_time': 340.20533871650696, 'accumulated_logging_time': 0.35373950004577637}
I0501 21:22:19.278450 140353579493120 logging_writer.py:48] [22576] accumulated_eval_time=340.205339, accumulated_logging_time=0.353740, accumulated_submission_time=8264.506025, global_step=22576, preemption_count=0, score=8264.506025, test/accuracy=0.537000, test/loss=2.171640, test/num_examples=10000, total_duration=8605.378945, train/accuracy=0.765167, train/loss=1.021405, validation/accuracy=0.665040, validation/loss=1.470481, validation/num_examples=50000
I0501 21:22:28.336709 140353587885824 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.2665002644062042, loss=2.4127309322357178
I0501 21:23:04.577973 140353579493120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.30614081025123596, loss=2.4487881660461426
I0501 21:23:40.737209 140353587885824 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2872541546821594, loss=2.4757909774780273
I0501 21:24:16.996038 140353579493120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.2639290392398834, loss=2.318345069885254
I0501 21:24:53.146732 140353587885824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.335440993309021, loss=2.531094551086426
I0501 21:25:29.256980 140353579493120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2878948152065277, loss=2.434743881225586
I0501 21:26:05.491861 140353587885824 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2816913425922394, loss=2.426619052886963
I0501 21:26:41.617398 140353579493120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.2930845618247986, loss=2.4004406929016113
I0501 21:27:17.923676 140353587885824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.27927684783935547, loss=2.3069801330566406
I0501 21:27:54.203084 140353579493120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2800236642360687, loss=2.4411051273345947
I0501 21:28:30.354064 140353587885824 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.30572596192359924, loss=2.4482593536376953
I0501 21:29:06.518346 140353579493120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2970038950443268, loss=2.4414820671081543
I0501 21:29:42.774257 140353587885824 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.29680049419403076, loss=2.4500925540924072
I0501 21:30:18.987919 140353579493120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3103543519973755, loss=2.37684965133667
I0501 21:30:49.323094 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:30:56.779329 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:31:06.900413 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:31:09.018853 140561424062272 submission_runner.py:415] Time since start: 9135.13s, 	Step: 23985, 	{'train/accuracy': 0.7691923975944519, 'train/loss': 1.014432430267334, 'validation/accuracy': 0.6712200045585632, 'validation/loss': 1.4673763513565063, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.164595603942871, 'test/num_examples': 10000, 'score': 8774.521947860718, 'total_duration': 9135.130654096603, 'accumulated_submission_time': 8774.521947860718, 'accumulated_eval_time': 359.8999195098877, 'accumulated_logging_time': 0.3752017021179199}
I0501 21:31:09.045876 140353587885824 logging_writer.py:48] [23985] accumulated_eval_time=359.899920, accumulated_logging_time=0.375202, accumulated_submission_time=8774.521948, global_step=23985, preemption_count=0, score=8774.521948, test/accuracy=0.543800, test/loss=2.164596, test/num_examples=10000, total_duration=9135.130654, train/accuracy=0.769192, train/loss=1.014432, validation/accuracy=0.671220, validation/loss=1.467376, validation/num_examples=50000
I0501 21:31:14.847245 140353579493120 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2796609699726105, loss=2.340489149093628
I0501 21:31:50.964804 140353587885824 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.31153127551078796, loss=2.2989742755889893
I0501 21:32:27.185232 140353579493120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2394719421863556, loss=2.3660764694213867
I0501 21:33:03.317135 140353587885824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2793006896972656, loss=2.384949207305908
I0501 21:33:39.653959 140353579493120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2698294520378113, loss=2.429231882095337
I0501 21:34:15.779054 140353587885824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.2734195590019226, loss=2.3876755237579346
I0501 21:34:52.052665 140353579493120 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.28175780177116394, loss=2.3414576053619385
I0501 21:35:28.241502 140353587885824 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2851618230342865, loss=2.439945936203003
I0501 21:36:04.382547 140353579493120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.27076029777526855, loss=2.3618619441986084
I0501 21:36:40.658997 140353587885824 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3384995758533478, loss=2.3887479305267334
I0501 21:37:16.776843 140353579493120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2857327461242676, loss=2.2602956295013428
I0501 21:37:52.964953 140353587885824 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2914363145828247, loss=2.3190064430236816
I0501 21:38:29.274268 140353579493120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2662038207054138, loss=2.2726328372955322
I0501 21:39:05.416922 140353587885824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3005496561527252, loss=2.32723331451416
I0501 21:39:39.262909 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:39:46.668620 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:39:56.815304 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:39:59.025882 140561424062272 submission_runner.py:415] Time since start: 9665.14s, 	Step: 25395, 	{'train/accuracy': 0.7673588991165161, 'train/loss': 0.9958425760269165, 'validation/accuracy': 0.6686399579048157, 'validation/loss': 1.4527508020401, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.1739847660064697, 'test/num_examples': 10000, 'score': 9284.708994626999, 'total_duration': 9665.137627124786, 'accumulated_submission_time': 9284.708994626999, 'accumulated_eval_time': 379.6616575717926, 'accumulated_logging_time': 0.41355299949645996}
I0501 21:39:59.038578 140353579493120 logging_writer.py:48] [25395] accumulated_eval_time=379.661658, accumulated_logging_time=0.413553, accumulated_submission_time=9284.708995, global_step=25395, preemption_count=0, score=9284.708995, test/accuracy=0.541000, test/loss=2.173985, test/num_examples=10000, total_duration=9665.137627, train/accuracy=0.767359, train/loss=0.995843, validation/accuracy=0.668640, validation/loss=1.452751, validation/num_examples=50000
I0501 21:40:01.223333 140353587885824 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.27117472887039185, loss=2.360233783721924
I0501 21:40:37.489457 140353579493120 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2684725224971771, loss=2.3747916221618652
I0501 21:41:13.636658 140353587885824 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.271485298871994, loss=2.382471799850464
I0501 21:41:49.841177 140353579493120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.27916038036346436, loss=2.4078073501586914
I0501 21:42:25.987330 140353587885824 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2845584452152252, loss=2.3424854278564453
I0501 21:43:02.227559 140353579493120 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.2811603844165802, loss=2.326045274734497
I0501 21:43:38.379582 140353587885824 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3096371591091156, loss=2.3788535594940186
I0501 21:44:14.622500 140353579493120 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.31585878133773804, loss=2.3601601123809814
I0501 21:44:50.792313 140353587885824 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.28770554065704346, loss=2.317321538925171
I0501 21:45:27.075939 140353579493120 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.3128093183040619, loss=2.4192874431610107
I0501 21:46:03.239301 140353587885824 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2875378429889679, loss=2.42781662940979
I0501 21:46:39.565890 140353579493120 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.30009838938713074, loss=2.37984299659729
I0501 21:47:15.737923 140353587885824 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.27997130155563354, loss=2.3714025020599365
I0501 21:47:51.925185 140353579493120 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.26535114645957947, loss=2.371166944503784
I0501 21:48:28.185428 140353587885824 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.2671327590942383, loss=2.3546295166015625
I0501 21:48:29.150079 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:48:36.525086 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:48:46.706100 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:48:49.043448 140561424062272 submission_runner.py:415] Time since start: 10195.16s, 	Step: 26804, 	{'train/accuracy': 0.7837611436843872, 'train/loss': 0.9623169898986816, 'validation/accuracy': 0.6777200102806091, 'validation/loss': 1.4252703189849854, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.115262508392334, 'test/num_examples': 10000, 'score': 9794.792797088623, 'total_duration': 10195.15522480011, 'accumulated_submission_time': 9794.792797088623, 'accumulated_eval_time': 399.55380392074585, 'accumulated_logging_time': 0.4352681636810303}
I0501 21:48:49.055317 140353579493120 logging_writer.py:48] [26804] accumulated_eval_time=399.553804, accumulated_logging_time=0.435268, accumulated_submission_time=9794.792797, global_step=26804, preemption_count=0, score=9794.792797, test/accuracy=0.553500, test/loss=2.115263, test/num_examples=10000, total_duration=10195.155225, train/accuracy=0.783761, train/loss=0.962317, validation/accuracy=0.677720, validation/loss=1.425270, validation/num_examples=50000
I0501 21:49:24.101303 140353587885824 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2678932547569275, loss=2.313713788986206
I0501 21:50:00.338960 140353579493120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.27212169766426086, loss=2.408567428588867
I0501 21:50:36.493472 140353587885824 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.28504687547683716, loss=2.3401215076446533
I0501 21:51:12.818909 140353579493120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2878376841545105, loss=2.335275650024414
I0501 21:51:48.989038 140353587885824 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.2934310734272003, loss=2.3405070304870605
I0501 21:52:25.276732 140353579493120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2908155024051666, loss=2.2589666843414307
I0501 21:53:01.396935 140353587885824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2609272599220276, loss=2.3205742835998535
I0501 21:53:37.655825 140353579493120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2794317603111267, loss=2.451266050338745
I0501 21:54:13.795332 140353587885824 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.2874542474746704, loss=2.3827157020568848
I0501 21:54:50.059478 140353579493120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.28050297498703003, loss=2.416863441467285
I0501 21:55:26.216492 140353587885824 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.25647783279418945, loss=2.2728452682495117
I0501 21:56:01.995264 140561424062272 spec.py:298] Evaluating on the training split.
I0501 21:56:09.384822 140561424062272 spec.py:310] Evaluating on the validation split.
I0501 21:56:19.717230 140561424062272 spec.py:326] Evaluating on the test split.
I0501 21:56:22.031092 140561424062272 submission_runner.py:415] Time since start: 10648.14s, 	Step: 28000, 	{'train/accuracy': 0.78515625, 'train/loss': 0.9292498230934143, 'validation/accuracy': 0.677079975605011, 'validation/loss': 1.4123295545578003, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 2.1033759117126465, 'test/num_examples': 10000, 'score': 10227.708572864532, 'total_duration': 10648.142876386642, 'accumulated_submission_time': 10227.708572864532, 'accumulated_eval_time': 419.58842372894287, 'accumulated_logging_time': 0.4555954933166504}
I0501 21:56:22.043619 140353579493120 logging_writer.py:48] [28000] accumulated_eval_time=419.588424, accumulated_logging_time=0.455595, accumulated_submission_time=10227.708573, global_step=28000, preemption_count=0, score=10227.708573, test/accuracy=0.554400, test/loss=2.103376, test/num_examples=10000, total_duration=10648.142876, train/accuracy=0.785156, train/loss=0.929250, validation/accuracy=0.677080, validation/loss=1.412330, validation/num_examples=50000
I0501 21:56:22.062499 140353587885824 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10227.708573
I0501 21:56:22.262219 140561424062272 checkpoints.py:356] Saving checkpoint at step: 28000
I0501 21:56:22.930056 140561424062272 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1/checkpoint_28000
I0501 21:56:22.939557 140561424062272 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_2/timing_adafactor/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0501 21:56:23.265850 140561424062272 submission_runner.py:578] Tuning trial 1/1
I0501 21:56:23.266085 140561424062272 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0501 21:56:23.271832 140561424062272 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00047831633128225803, 'train/loss': 6.913181781768799, 'validation/accuracy': 0.0005999999702908099, 'validation/loss': 6.9127984046936035, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.913119316101074, 'test/num_examples': 10000, 'score': 101.43676424026489, 'total_duration': 144.62286186218262, 'accumulated_submission_time': 101.43676424026489, 'accumulated_eval_time': 43.185946464538574, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1412, {'train/accuracy': 0.11664938926696777, 'train/loss': 4.781286239624023, 'validation/accuracy': 0.09945999830961227, 'validation/loss': 4.945098400115967, 'validation/num_examples': 50000, 'test/accuracy': 0.06960000097751617, 'test/loss': 5.2959418296813965, 'test/num_examples': 10000, 'score': 611.6793704032898, 'total_duration': 671.954597234726, 'accumulated_submission_time': 611.6793704032898, 'accumulated_eval_time': 60.229008436203, 'accumulated_logging_time': 0.026158809661865234, 'global_step': 1412, 'preemption_count': 0}), (2822, {'train/accuracy': 0.2737962305545807, 'train/loss': 3.5180482864379883, 'validation/accuracy': 0.2448599934577942, 'validation/loss': 3.69538950920105, 'validation/num_examples': 50000, 'test/accuracy': 0.18710000813007355, 'test/loss': 4.203489303588867, 'test/num_examples': 10000, 'score': 1121.7690916061401, 'total_duration': 1199.205161333084, 'accumulated_submission_time': 1121.7690916061401, 'accumulated_eval_time': 77.35259890556335, 'accumulated_logging_time': 0.04346895217895508, 'global_step': 2822, 'preemption_count': 0}), (4232, {'train/accuracy': 0.40963807702064514, 'train/loss': 2.7385308742523193, 'validation/accuracy': 0.37477999925613403, 'validation/loss': 2.9342598915100098, 'validation/num_examples': 50000, 'test/accuracy': 0.281900018453598, 'test/loss': 3.594959259033203, 'test/num_examples': 10000, 'score': 1631.9597959518433, 'total_duration': 1726.3599789142609, 'accumulated_submission_time': 1631.9597959518433, 'accumulated_eval_time': 94.2801604270935, 'accumulated_logging_time': 0.06043362617492676, 'global_step': 4232, 'preemption_count': 0}), (5643, {'train/accuracy': 0.4970104992389679, 'train/loss': 2.298945188522339, 'validation/accuracy': 0.4556399881839752, 'validation/loss': 2.5166385173797607, 'validation/num_examples': 50000, 'test/accuracy': 0.3483000099658966, 'test/loss': 3.1608636379241943, 'test/num_examples': 10000, 'score': 2142.2815775871277, 'total_duration': 2253.9261000156403, 'accumulated_submission_time': 2142.2815775871277, 'accumulated_eval_time': 111.48601818084717, 'accumulated_logging_time': 0.07985401153564453, 'global_step': 5643, 'preemption_count': 0}), (7054, {'train/accuracy': 0.5483697056770325, 'train/loss': 2.02362322807312, 'validation/accuracy': 0.5046799778938293, 'validation/loss': 2.237910032272339, 'validation/num_examples': 50000, 'test/accuracy': 0.38510000705718994, 'test/loss': 2.923192024230957, 'test/num_examples': 10000, 'score': 2652.3014986515045, 'total_duration': 2781.12637090683, 'accumulated_submission_time': 2652.3014986515045, 'accumulated_eval_time': 128.62700939178467, 'accumulated_logging_time': 0.10037040710449219, 'global_step': 7054, 'preemption_count': 0}), (8466, {'train/accuracy': 0.5847018361091614, 'train/loss': 1.8203755617141724, 'validation/accuracy': 0.5388999581336975, 'validation/loss': 2.0554256439208984, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.7409839630126953, 'test/num_examples': 10000, 'score': 3162.485634326935, 'total_duration': 3309.2658355236053, 'accumulated_submission_time': 3162.485634326935, 'accumulated_eval_time': 146.54587507247925, 'accumulated_logging_time': 0.1181333065032959, 'global_step': 8466, 'preemption_count': 0}), (9878, {'train/accuracy': 0.6240234375, 'train/loss': 1.6609609127044678, 'validation/accuracy': 0.5707799792289734, 'validation/loss': 1.9146699905395508, 'validation/num_examples': 50000, 'test/accuracy': 0.4416000247001648, 'test/loss': 2.623234510421753, 'test/num_examples': 10000, 'score': 3672.8094897270203, 'total_duration': 3838.1110508441925, 'accumulated_submission_time': 3672.8094897270203, 'accumulated_eval_time': 165.02604842185974, 'accumulated_logging_time': 0.14054155349731445, 'global_step': 9878, 'preemption_count': 0}), (11289, {'train/accuracy': 0.648855984210968, 'train/loss': 1.540156602859497, 'validation/accuracy': 0.5960599780082703, 'validation/loss': 1.7866590023040771, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.4964287281036377, 'test/num_examples': 10000, 'score': 4183.07145357132, 'total_duration': 4366.970770597458, 'accumulated_submission_time': 4183.07145357132, 'accumulated_eval_time': 183.583753824234, 'accumulated_logging_time': 0.1615769863128662, 'global_step': 11289, 'preemption_count': 0}), (12700, {'train/accuracy': 0.6737882494926453, 'train/loss': 1.4531214237213135, 'validation/accuracy': 0.6132400035858154, 'validation/loss': 1.732688546180725, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.4086310863494873, 'test/num_examples': 10000, 'score': 4693.132774591446, 'total_duration': 4896.496756315231, 'accumulated_submission_time': 4693.132774591446, 'accumulated_eval_time': 203.00971841812134, 'accumulated_logging_time': 0.18139219284057617, 'global_step': 12700, 'preemption_count': 0}), (14111, {'train/accuracy': 0.6912268400192261, 'train/loss': 1.3569697141647339, 'validation/accuracy': 0.6285399794578552, 'validation/loss': 1.6437764167785645, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.3190810680389404, 'test/num_examples': 10000, 'score': 5203.381317853928, 'total_duration': 5425.90149140358, 'accumulated_submission_time': 5203.381317853928, 'accumulated_eval_time': 222.12046575546265, 'accumulated_logging_time': 0.20693016052246094, 'global_step': 14111, 'preemption_count': 0}), (15523, {'train/accuracy': 0.7023875713348389, 'train/loss': 1.314147710800171, 'validation/accuracy': 0.6282399892807007, 'validation/loss': 1.6419637203216553, 'validation/num_examples': 50000, 'test/accuracy': 0.5023000240325928, 'test/loss': 2.336332082748413, 'test/num_examples': 10000, 'score': 5713.658360004425, 'total_duration': 5956.146651983261, 'accumulated_submission_time': 5713.658360004425, 'accumulated_eval_time': 242.04777693748474, 'accumulated_logging_time': 0.22761869430541992, 'global_step': 15523, 'preemption_count': 0}), (16934, {'train/accuracy': 0.7584900856018066, 'train/loss': 1.0991981029510498, 'validation/accuracy': 0.6437399983406067, 'validation/loss': 1.5964267253875732, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.2868399620056152, 'test/num_examples': 10000, 'score': 6223.723320245743, 'total_duration': 6485.402654647827, 'accumulated_submission_time': 6223.723320245743, 'accumulated_eval_time': 261.19131207466125, 'accumulated_logging_time': 0.25510501861572266, 'global_step': 16934, 'preemption_count': 0}), (18345, {'train/accuracy': 0.7609016299247742, 'train/loss': 1.08008873462677, 'validation/accuracy': 0.6540799736976624, 'validation/loss': 1.5490473508834839, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.2369234561920166, 'test/num_examples': 10000, 'score': 6734.023561000824, 'total_duration': 7015.529815673828, 'accumulated_submission_time': 6734.023561000824, 'accumulated_eval_time': 280.97669291496277, 'accumulated_logging_time': 0.27663397789001465, 'global_step': 18345, 'preemption_count': 0}), (19756, {'train/accuracy': 0.7610211968421936, 'train/loss': 1.061102271080017, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.508509635925293, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.194878101348877, 'test/num_examples': 10000, 'score': 7244.145133733749, 'total_duration': 7545.226433515549, 'accumulated_submission_time': 7244.145133733749, 'accumulated_eval_time': 300.50472021102905, 'accumulated_logging_time': 0.30364990234375, 'global_step': 19756, 'preemption_count': 0}), (21166, {'train/accuracy': 0.7658043503761292, 'train/loss': 1.0472811460494995, 'validation/accuracy': 0.6670199632644653, 'validation/loss': 1.4846476316452026, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.1873974800109863, 'test/num_examples': 10000, 'score': 7754.353482961655, 'total_duration': 8075.038236141205, 'accumulated_submission_time': 7754.353482961655, 'accumulated_eval_time': 320.06684279441833, 'accumulated_logging_time': 0.3241441249847412, 'global_step': 21166, 'preemption_count': 0}), (22576, {'train/accuracy': 0.7651665806770325, 'train/loss': 1.0214049816131592, 'validation/accuracy': 0.6650399565696716, 'validation/loss': 1.4704805612564087, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.1716396808624268, 'test/num_examples': 10000, 'score': 8264.506025075912, 'total_duration': 8605.378944635391, 'accumulated_submission_time': 8264.506025075912, 'accumulated_eval_time': 340.20533871650696, 'accumulated_logging_time': 0.35373950004577637, 'global_step': 22576, 'preemption_count': 0}), (23985, {'train/accuracy': 0.7691923975944519, 'train/loss': 1.014432430267334, 'validation/accuracy': 0.6712200045585632, 'validation/loss': 1.4673763513565063, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.164595603942871, 'test/num_examples': 10000, 'score': 8774.521947860718, 'total_duration': 9135.130654096603, 'accumulated_submission_time': 8774.521947860718, 'accumulated_eval_time': 359.8999195098877, 'accumulated_logging_time': 0.3752017021179199, 'global_step': 23985, 'preemption_count': 0}), (25395, {'train/accuracy': 0.7673588991165161, 'train/loss': 0.9958425760269165, 'validation/accuracy': 0.6686399579048157, 'validation/loss': 1.4527508020401, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.1739847660064697, 'test/num_examples': 10000, 'score': 9284.708994626999, 'total_duration': 9665.137627124786, 'accumulated_submission_time': 9284.708994626999, 'accumulated_eval_time': 379.6616575717926, 'accumulated_logging_time': 0.41355299949645996, 'global_step': 25395, 'preemption_count': 0}), (26804, {'train/accuracy': 0.7837611436843872, 'train/loss': 0.9623169898986816, 'validation/accuracy': 0.6777200102806091, 'validation/loss': 1.4252703189849854, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.115262508392334, 'test/num_examples': 10000, 'score': 9794.792797088623, 'total_duration': 10195.15522480011, 'accumulated_submission_time': 9794.792797088623, 'accumulated_eval_time': 399.55380392074585, 'accumulated_logging_time': 0.4352681636810303, 'global_step': 26804, 'preemption_count': 0}), (28000, {'train/accuracy': 0.78515625, 'train/loss': 0.9292498230934143, 'validation/accuracy': 0.677079975605011, 'validation/loss': 1.4123295545578003, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 2.1033759117126465, 'test/num_examples': 10000, 'score': 10227.708572864532, 'total_duration': 10648.142876386642, 'accumulated_submission_time': 10227.708572864532, 'accumulated_eval_time': 419.58842372894287, 'accumulated_logging_time': 0.4555954933166504, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0501 21:56:23.271969 140561424062272 submission_runner.py:581] Timing: 10227.708572864532
I0501 21:56:23.272020 140561424062272 submission_runner.py:582] ====================
I0501 21:56:23.272162 140561424062272 submission_runner.py:645] Final imagenet_resnet score: 10227.708572864532
