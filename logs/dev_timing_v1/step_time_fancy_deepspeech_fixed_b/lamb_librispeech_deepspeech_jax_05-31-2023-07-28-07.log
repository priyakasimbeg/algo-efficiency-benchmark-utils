python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/lamb/jax/submission.py --tuning_search_space=baselines/lamb/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_jax_upgrade_b/lamb --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-31-2023-07-28-07.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0531 07:28:29.526857 140288245086016 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax.
I0531 07:28:30.443771 140288245086016 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0531 07:28:30.444483 140288245086016 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 07:28:30.444627 140288245086016 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0531 07:28:30.449852 140288245086016 submission_runner.py:549] Using RNG seed 447245817
I0531 07:28:35.923534 140288245086016 submission_runner.py:558] --- Tuning run 1/1 ---
I0531 07:28:35.923807 140288245086016 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1.
I0531 07:28:35.924199 140288245086016 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1/hparams.json.
I0531 07:28:36.141330 140288245086016 submission_runner.py:243] Initializing dataset.
I0531 07:28:36.141640 140288245086016 submission_runner.py:250] Initializing model.
I0531 07:28:38.737303 140288245086016 submission_runner.py:260] Initializing optimizer.
I0531 07:28:39.431888 140288245086016 submission_runner.py:267] Initializing metrics bundle.
I0531 07:28:39.432138 140288245086016 submission_runner.py:285] Initializing checkpoint and logger.
I0531 07:28:39.433344 140288245086016 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0531 07:28:39.433711 140288245086016 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0531 07:28:39.433817 140288245086016 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0531 07:28:40.263553 140288245086016 submission_runner.py:306] Saving meta data to /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0531 07:28:40.264710 140288245086016 submission_runner.py:309] Saving flags to /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0531 07:28:40.271493 140288245086016 submission_runner.py:321] Starting training loop.
I0531 07:28:40.590469 140288245086016 input_pipeline.py:20] Loading split = train-clean-100
I0531 07:28:40.630155 140288245086016 input_pipeline.py:20] Loading split = train-clean-360
I0531 07:28:41.047198 140288245086016 input_pipeline.py:20] Loading split = train-other-500
2023-05-31 07:29:39.369407: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-31 07:29:39.517291: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0531 07:29:44.302734 140125950437120 logging_writer.py:48] [0] global_step=0, grad_norm=18.66516876220703, loss=32.945106506347656
I0531 07:29:44.327765 140288245086016 spec.py:298] Evaluating on the training split.
I0531 07:29:44.587535 140288245086016 input_pipeline.py:20] Loading split = train-clean-100
I0531 07:29:44.621390 140288245086016 input_pipeline.py:20] Loading split = train-clean-360
I0531 07:29:44.929591 140288245086016 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0531 07:31:32.592532 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 07:31:32.795711 140288245086016 input_pipeline.py:20] Loading split = dev-clean
I0531 07:31:32.800955 140288245086016 input_pipeline.py:20] Loading split = dev-other
I0531 07:32:30.150336 140288245086016 spec.py:326] Evaluating on the test split.
I0531 07:32:30.355067 140288245086016 input_pipeline.py:20] Loading split = test-clean
I0531 07:33:09.402186 140288245086016 submission_runner.py:426] Time since start: 269.13s, 	Step: 1, 	{'train/ctc_loss': Array(29.451366, dtype=float32), 'train/wer': 3.2654301064073463, 'validation/ctc_loss': Array(28.358381, dtype=float32), 'validation/wer': 2.87474071143957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.283052, dtype=float32), 'test/wer': 3.152093108281031, 'test/num_examples': 2472, 'score': 64.05605888366699, 'total_duration': 269.128536939621, 'accumulated_submission_time': 64.05605888366699, 'accumulated_data_selection_time': 5.129445791244507, 'accumulated_eval_time': 205.07228326797485, 'accumulated_logging_time': 0}
I0531 07:33:09.427934 140121215018752 logging_writer.py:48] [1] accumulated_data_selection_time=5.129446, accumulated_eval_time=205.072283, accumulated_logging_time=0, accumulated_submission_time=64.056059, global_step=1, preemption_count=0, score=64.056059, test/ctc_loss=28.283052444458008, test/num_examples=2472, test/wer=3.152093, total_duration=269.128537, train/ctc_loss=29.451366424560547, train/wer=3.265430, validation/ctc_loss=28.358381271362305, validation/num_examples=5348, validation/wer=2.874741
I0531 07:34:36.484520 140128116954880 logging_writer.py:48] [100] global_step=100, grad_norm=38.872772216796875, loss=30.806121826171875
I0531 07:35:53.098434 140128125347584 logging_writer.py:48] [200] global_step=200, grad_norm=39.247310638427734, loss=21.5938777923584
I0531 07:37:09.250821 140128116954880 logging_writer.py:48] [300] global_step=300, grad_norm=20.252668380737305, loss=11.905957221984863
I0531 07:38:25.532135 140128125347584 logging_writer.py:48] [400] global_step=400, grad_norm=3.733592987060547, loss=7.624312877655029
I0531 07:39:42.832416 140128116954880 logging_writer.py:48] [500] global_step=500, grad_norm=1.6199183464050293, loss=6.510941982269287
I0531 07:41:05.853555 140128125347584 logging_writer.py:48] [600] global_step=600, grad_norm=0.8127068281173706, loss=6.095500469207764
I0531 07:42:29.014456 140128116954880 logging_writer.py:48] [700] global_step=700, grad_norm=0.8792941570281982, loss=5.931768417358398
I0531 07:43:51.542149 140128125347584 logging_writer.py:48] [800] global_step=800, grad_norm=1.3011374473571777, loss=5.837311267852783
I0531 07:45:10.326533 140128116954880 logging_writer.py:48] [900] global_step=900, grad_norm=0.6862348318099976, loss=5.788143157958984
I0531 07:46:32.204581 140128125347584 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8631417751312256, loss=5.729267597198486
I0531 07:47:53.564297 140129553565440 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1922780275344849, loss=5.660744667053223
I0531 07:49:09.575949 140129545172736 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.203665852546692, loss=5.5795063972473145
I0531 07:50:26.620097 140129553565440 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2543597221374512, loss=5.46699857711792
I0531 07:51:43.347295 140129545172736 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.3061060905456543, loss=5.3567914962768555
I0531 07:53:00.040899 140129553565440 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.5598255395889282, loss=5.208322048187256
I0531 07:54:20.257239 140129545172736 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.20152747631073, loss=5.020196437835693
I0531 07:55:43.601675 140129553565440 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9112874865531921, loss=4.921171188354492
I0531 07:57:07.046312 140129545172736 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0492411851882935, loss=4.766690254211426
I0531 07:58:33.044599 140129553565440 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.2220864295959473, loss=4.637407302856445
I0531 07:59:56.265466 140129545172736 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.8204281330108643, loss=4.543344497680664
I0531 08:01:21.377338 140130208925440 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.8203282356262207, loss=4.376534938812256
I0531 08:02:37.314466 140130200532736 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.656378149986267, loss=4.2811994552612305
I0531 08:03:53.205299 140130208925440 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.0433361530303955, loss=4.125016689300537
I0531 08:05:09.132856 140130200532736 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.9741923809051514, loss=4.031692981719971
I0531 08:06:25.747812 140130208925440 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.2485482692718506, loss=3.9859836101531982
I0531 08:07:54.514500 140130200532736 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.8242772817611694, loss=3.890084981918335
I0531 08:09:18.532449 140130208925440 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.9430263042449951, loss=3.833259344100952
I0531 08:10:44.779443 140130200532736 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4852937459945679, loss=3.6700515747070312
I0531 08:12:12.370528 140130208925440 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.6411646604537964, loss=3.640831708908081
I0531 08:13:09.757862 140288245086016 spec.py:298] Evaluating on the training split.
I0531 08:13:55.981232 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 08:14:38.800096 140288245086016 spec.py:326] Evaluating on the test split.
I0531 08:15:01.501124 140288245086016 submission_runner.py:426] Time since start: 2781.22s, 	Step: 2968, 	{'train/ctc_loss': Array(6.318609, dtype=float32), 'train/wer': 0.9463038866346728, 'validation/ctc_loss': Array(6.2772756, dtype=float32), 'validation/wer': 0.9177705525378923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.103213, dtype=float32), 'test/wer': 0.9201551804683851, 'test/num_examples': 2472, 'score': 2464.3369357585907, 'total_duration': 2781.2241513729095, 'accumulated_submission_time': 2464.3369357585907, 'accumulated_data_selection_time': 537.235538482666, 'accumulated_eval_time': 316.8101451396942, 'accumulated_logging_time': 0.038315534591674805}
I0531 08:15:01.526686 140130208925440 logging_writer.py:48] [2968] accumulated_data_selection_time=537.235538, accumulated_eval_time=316.810145, accumulated_logging_time=0.038316, accumulated_submission_time=2464.336936, global_step=2968, preemption_count=0, score=2464.336936, test/ctc_loss=6.103212833404541, test/num_examples=2472, test/wer=0.920155, total_duration=2781.224151, train/ctc_loss=6.318609237670898, train/wer=0.946304, validation/ctc_loss=6.277275562286377, validation/num_examples=5348, validation/wer=0.917771
I0531 08:15:26.560248 140130200532736 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.7672373056411743, loss=3.5057969093322754
I0531 08:16:46.000962 140130208925440 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.1410348415374756, loss=3.474935293197632
I0531 08:18:01.678603 140130200532736 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.0797340869903564, loss=3.4283883571624756
I0531 08:19:18.473519 140130208925440 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.133814811706543, loss=3.3699028491973877
I0531 08:20:34.997475 140130200532736 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.812086343765259, loss=3.3765978813171387
I0531 08:21:53.744471 140130208925440 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.8943185806274414, loss=3.201371669769287
I0531 08:23:20.510576 140130200532736 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.518641233444214, loss=3.2202539443969727
I0531 08:24:46.016292 140130208925440 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.675821304321289, loss=3.2382612228393555
I0531 08:26:13.603673 140130200532736 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.902069568634033, loss=3.174370288848877
I0531 08:27:37.606029 140130208925440 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.570383071899414, loss=3.0632941722869873
I0531 08:29:03.657678 140130200532736 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.504437208175659, loss=3.027369499206543
I0531 08:30:28.896086 140130208925440 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.601010799407959, loss=2.93786358833313
I0531 08:31:49.576365 140130208925440 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.4866764545440674, loss=2.877302646636963
I0531 08:33:05.520137 140130200532736 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.8084521293640137, loss=2.9403293132781982
I0531 08:34:21.433119 140130208925440 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.6545517444610596, loss=2.8031435012817383
I0531 08:35:42.048688 140130200532736 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.8600406646728516, loss=2.849431037902832
I0531 08:37:06.166352 140130208925440 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.2738616466522217, loss=2.72672963142395
I0531 08:38:29.150527 140130200532736 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.993560314178467, loss=2.77101731300354
I0531 08:39:53.931422 140130208925440 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.10406756401062, loss=2.706873655319214
I0531 08:41:18.700319 140130200532736 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.155777931213379, loss=2.74991512298584
I0531 08:42:42.159866 140130208925440 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.2683050632476807, loss=2.762056589126587
I0531 08:44:11.018086 140130200532736 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.2486181259155273, loss=2.6100637912750244
I0531 08:45:34.915924 140130208925440 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.048585891723633, loss=2.5561795234680176
I0531 08:46:50.928153 140130200532736 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.724217176437378, loss=2.5648751258850098
I0531 08:48:06.563348 140130208925440 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.289033889770508, loss=2.6279149055480957
I0531 08:49:24.757001 140130200532736 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.1717135906219482, loss=2.6141912937164307
I0531 08:50:51.052995 140130208925440 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.831139326095581, loss=2.5226500034332275
I0531 08:52:15.086689 140130200532736 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.7336745262145996, loss=2.572369337081909
I0531 08:53:42.257797 140130208925440 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.827462911605835, loss=2.5006330013275146
I0531 08:55:02.072511 140288245086016 spec.py:298] Evaluating on the training split.
I0531 08:55:49.350429 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 08:56:32.662305 140288245086016 spec.py:326] Evaluating on the test split.
I0531 08:56:56.317938 140288245086016 submission_runner.py:426] Time since start: 5296.04s, 	Step: 5895, 	{'train/ctc_loss': Array(1.3605129, dtype=float32), 'train/wer': 0.3887542284846709, 'validation/ctc_loss': Array(1.7552879, dtype=float32), 'validation/wer': 0.4390105066136673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.359532, dtype=float32), 'test/wer': 0.3807608717729978, 'test/num_examples': 2472, 'score': 4864.82848072052, 'total_duration': 5296.042528152466, 'accumulated_submission_time': 4864.82848072052, 'accumulated_data_selection_time': 1150.8172585964203, 'accumulated_eval_time': 431.0517303943634, 'accumulated_logging_time': 0.08106279373168945}
I0531 08:56:56.339662 140130208925440 logging_writer.py:48] [5895] accumulated_data_selection_time=1150.817259, accumulated_eval_time=431.051730, accumulated_logging_time=0.081063, accumulated_submission_time=4864.828481, global_step=5895, preemption_count=0, score=4864.828481, test/ctc_loss=1.3595319986343384, test/num_examples=2472, test/wer=0.380761, total_duration=5296.042528, train/ctc_loss=1.3605128526687622, train/wer=0.388754, validation/ctc_loss=1.7552878856658936, validation/num_examples=5348, validation/wer=0.439011
I0531 08:57:00.931450 140130200532736 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.3608615398406982, loss=2.4738574028015137
I0531 08:58:16.277779 140130208925440 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.670422077178955, loss=2.378466844558716
I0531 08:59:32.072827 140130200532736 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.400993585586548, loss=2.4307751655578613
I0531 09:00:52.647527 140130208925440 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.132941484451294, loss=2.4626471996307373
I0531 09:02:08.234746 140130200532736 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.773789405822754, loss=2.3280491828918457
I0531 09:03:23.802470 140130208925440 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.615812301635742, loss=2.3260409832000732
I0531 09:04:41.767181 140130200532736 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.712412118911743, loss=2.2983028888702393
I0531 09:06:05.475404 140130208925440 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.668161153793335, loss=2.358457088470459
I0531 09:07:31.969750 140130200532736 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.12988018989563, loss=2.2293872833251953
I0531 09:08:59.915289 140130208925440 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.316396236419678, loss=2.2760870456695557
I0531 09:10:27.788007 140130200532736 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.2919883728027344, loss=2.243398904800415
I0531 09:11:55.538173 140130208925440 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.247486591339111, loss=2.2184641361236572
I0531 09:13:20.034081 140130200532736 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.1269140243530273, loss=2.226890802383423
I0531 09:14:43.571031 140130208925440 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.965221881866455, loss=2.1510486602783203
I0531 09:16:03.093965 140130208925440 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.1116890907287598, loss=2.211894989013672
I0531 09:17:18.538758 140130200532736 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.246699333190918, loss=2.189502239227295
I0531 09:18:34.425238 140130208925440 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.089604377746582, loss=2.1299266815185547
I0531 09:19:50.202583 140130200532736 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.0957138538360596, loss=2.121232509613037
I0531 09:21:15.266028 140130208925440 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.8316447734832764, loss=2.135390520095825
I0531 09:22:44.282405 140130200532736 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.4642200469970703, loss=2.1310923099517822
I0531 09:24:13.192268 140130208925440 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.0468268394470215, loss=2.2069685459136963
I0531 09:25:44.379482 140130200532736 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.0297162532806396, loss=2.1223199367523193
I0531 09:27:12.682448 140130208925440 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.147145748138428, loss=2.1366477012634277
I0531 09:28:39.167064 140130200532736 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.219606637954712, loss=2.0516672134399414
I0531 09:30:00.068376 140130208925440 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.0172672271728516, loss=2.016798257827759
I0531 09:31:15.359378 140130200532736 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.8579094409942627, loss=2.0721542835235596
I0531 09:32:31.051366 140130208925440 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.330749034881592, loss=2.0365967750549316
I0531 09:33:53.290468 140130200532736 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.0518956184387207, loss=1.996679425239563
I0531 09:35:16.171448 140130208925440 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.0219309329986572, loss=2.0285091400146484
I0531 09:36:40.254840 140130200532736 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.6644201278686523, loss=1.9629642963409424
I0531 09:36:56.412539 140288245086016 spec.py:298] Evaluating on the training split.
I0531 09:37:45.261274 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 09:38:30.161284 140288245086016 spec.py:326] Evaluating on the test split.
I0531 09:38:53.602641 140288245086016 submission_runner.py:426] Time since start: 7813.33s, 	Step: 8821, 	{'train/ctc_loss': Array(0.66282105, dtype=float32), 'train/wer': 0.2271560426121838, 'validation/ctc_loss': Array(1.053084, dtype=float32), 'validation/wer': 0.3037945373327287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7258549, dtype=float32), 'test/wer': 0.233420673125749, 'test/num_examples': 2472, 'score': 7264.84740114212, 'total_duration': 7813.327271461487, 'accumulated_submission_time': 7264.84740114212, 'accumulated_data_selection_time': 1784.4445383548737, 'accumulated_eval_time': 548.2380123138428, 'accumulated_logging_time': 0.11923456192016602}
I0531 09:38:53.624695 140130208925440 logging_writer.py:48] [8821] accumulated_data_selection_time=1784.444538, accumulated_eval_time=548.238012, accumulated_logging_time=0.119235, accumulated_submission_time=7264.847401, global_step=8821, preemption_count=0, score=7264.847401, test/ctc_loss=0.7258548736572266, test/num_examples=2472, test/wer=0.233421, total_duration=7813.327271, train/ctc_loss=0.6628210544586182, train/wer=0.227156, validation/ctc_loss=1.0530840158462524, validation/num_examples=5348, validation/wer=0.303795
I0531 09:39:53.781773 140130200532736 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.991278648376465, loss=1.9512704610824585
I0531 09:41:09.108458 140130208925440 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.439028263092041, loss=2.036430835723877
I0531 09:42:29.758764 140130200532736 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.281205892562866, loss=1.9647462368011475
I0531 09:43:56.994938 140130208925440 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.027513027191162, loss=2.008779287338257
I0531 09:45:19.890136 140130208925440 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.912428617477417, loss=1.941555142402649
I0531 09:46:34.856808 140130200532736 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.961557149887085, loss=1.9723912477493286
I0531 09:47:50.144994 140130208925440 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.3815059661865234, loss=1.8349523544311523
I0531 09:49:08.846303 140130200532736 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.822432041168213, loss=1.9628899097442627
I0531 09:50:29.172533 140130208925440 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.525041341781616, loss=1.9546443223953247
I0531 09:51:55.915235 140130200532736 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.7998147010803223, loss=1.8549504280090332
I0531 09:53:20.109516 140130208925440 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.7165684700012207, loss=1.8720287084579468
I0531 09:54:47.689655 140130200532736 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.049992084503174, loss=1.8682270050048828
I0531 09:56:15.962373 140130208925440 logging_writer.py:48] [10100] global_step=10100, grad_norm=5.587096691131592, loss=1.9321620464324951
I0531 09:57:41.864776 140130200532736 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.471006393432617, loss=1.8912357091903687
I0531 09:59:07.891297 140130208925440 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.387059211730957, loss=1.881070852279663
I0531 10:00:22.951585 140130200532736 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.2902302742004395, loss=1.8419564962387085
I0531 10:01:38.098539 140130208925440 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.354412078857422, loss=1.801507830619812
I0531 10:02:58.908132 140130200532736 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.563544988632202, loss=1.7864660024642944
I0531 10:04:17.627891 140130208925440 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.863889455795288, loss=1.8795772790908813
I0531 10:05:44.144215 140130200532736 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.1894500255584717, loss=1.8571103811264038
I0531 10:07:08.584438 140130208925440 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.235790967941284, loss=1.8148796558380127
I0531 10:08:34.656699 140130200532736 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.724071502685547, loss=1.7868452072143555
I0531 10:09:59.112483 140130208925440 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.4025778770446777, loss=1.8331232070922852
I0531 10:11:23.785222 140130200532736 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.8473849296569824, loss=1.7812098264694214
I0531 10:12:48.879594 140130208925440 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.7524325847625732, loss=1.8084639310836792
I0531 10:14:11.221236 140130208925440 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.99762225151062, loss=1.7885010242462158
I0531 10:15:26.406813 140130200532736 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.929924964904785, loss=1.7696410417556763
I0531 10:16:44.344918 140130208925440 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.8246982097625732, loss=1.7932931184768677
I0531 10:18:07.214043 140130200532736 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.379642963409424, loss=1.8089849948883057
I0531 10:18:54.798235 140288245086016 spec.py:298] Evaluating on the training split.
I0531 10:19:44.381354 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 10:20:28.458012 140288245086016 spec.py:326] Evaluating on the test split.
I0531 10:20:50.679100 140288245086016 submission_runner.py:426] Time since start: 10330.40s, 	Step: 11757, 	{'train/ctc_loss': Array(0.51418155, dtype=float32), 'train/wer': 0.17557519714312644, 'validation/ctc_loss': Array(0.8727024, dtype=float32), 'validation/wer': 0.2552557188202491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55819875, dtype=float32), 'test/wer': 0.18333231775435174, 'test/num_examples': 2472, 'score': 9665.96957373619, 'total_duration': 10330.403761863708, 'accumulated_submission_time': 9665.96957373619, 'accumulated_data_selection_time': 2422.839151620865, 'accumulated_eval_time': 664.1150944232941, 'accumulated_logging_time': 0.15587663650512695}
I0531 10:20:50.700696 140130208925440 logging_writer.py:48] [11757] accumulated_data_selection_time=2422.839152, accumulated_eval_time=664.115094, accumulated_logging_time=0.155877, accumulated_submission_time=9665.969574, global_step=11757, preemption_count=0, score=9665.969574, test/ctc_loss=0.5581987500190735, test/num_examples=2472, test/wer=0.183332, total_duration=10330.403762, train/ctc_loss=0.5141815543174744, train/wer=0.175575, validation/ctc_loss=0.872702419757843, validation/num_examples=5348, validation/wer=0.255256
I0531 10:21:23.647288 140130200532736 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.5115621089935303, loss=1.9103426933288574
I0531 10:22:38.725697 140130208925440 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.4959194660186768, loss=1.7144354581832886
I0531 10:23:54.351973 140130200532736 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.3039443492889404, loss=1.8280187845230103
I0531 10:25:22.462579 140130208925440 logging_writer.py:48] [12100] global_step=12100, grad_norm=4.16762113571167, loss=1.7685755491256714
I0531 10:26:45.412914 140130200532736 logging_writer.py:48] [12200] global_step=12200, grad_norm=3.266127586364746, loss=1.7959023714065552
I0531 10:28:08.283195 140130208925440 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.611324310302734, loss=1.7335821390151978
I0531 10:29:31.833007 140130208925440 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.850486755371094, loss=1.7948551177978516
I0531 10:30:47.076516 140130200532736 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.157587051391602, loss=1.7714074850082397
I0531 10:32:03.024223 140130208925440 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.521222114562988, loss=1.7331676483154297
I0531 10:33:20.913399 140130200532736 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.43027400970459, loss=1.7544211149215698
I0531 10:34:44.379798 140130208925440 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.954761028289795, loss=1.7376148700714111
I0531 10:36:14.169065 140130200532736 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.9992873668670654, loss=1.7509095668792725
I0531 10:37:40.357164 140130208925440 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.100726366043091, loss=1.7628282308578491
I0531 10:39:04.258209 140130200532736 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.8133771419525146, loss=1.7501978874206543
I0531 10:40:30.151616 140130208925440 logging_writer.py:48] [13200] global_step=13200, grad_norm=6.370117664337158, loss=1.808315634727478
I0531 10:41:55.115826 140130200532736 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.636237382888794, loss=1.6739139556884766
I0531 10:43:21.331273 140130208925440 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.3014087677001953, loss=1.745105504989624
I0531 10:44:36.418313 140130200532736 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.2168588638305664, loss=1.730492353439331
I0531 10:45:51.329697 140130208925440 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.30545711517334, loss=1.7724863290786743
I0531 10:47:09.059505 140130200532736 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.418895959854126, loss=1.6651413440704346
I0531 10:48:31.192397 140130208925440 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.500523567199707, loss=1.6620341539382935
I0531 10:49:57.966860 140130200532736 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.767702341079712, loss=1.7255407571792603
I0531 10:51:23.394111 140130208925440 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.3601531982421875, loss=1.7540881633758545
I0531 10:52:49.594712 140130200532736 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.5458180904388428, loss=1.7008150815963745
I0531 10:54:15.673519 140130208925440 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.588688373565674, loss=1.6960052251815796
I0531 10:55:43.632632 140130200532736 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.9545843601226807, loss=1.6636368036270142
I0531 10:57:13.896616 140130208925440 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.976181983947754, loss=1.6922924518585205
I0531 10:58:34.730874 140130208925440 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.7894442081451416, loss=1.6415730714797974
I0531 10:59:49.630242 140130200532736 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.025880813598633, loss=1.6546034812927246
I0531 11:00:50.800847 140288245086016 spec.py:298] Evaluating on the training split.
I0531 11:01:39.578205 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 11:02:24.665972 140288245086016 spec.py:326] Evaluating on the test split.
I0531 11:02:48.554496 140288245086016 submission_runner.py:426] Time since start: 12848.28s, 	Step: 14683, 	{'train/ctc_loss': Array(0.42583644, dtype=float32), 'train/wer': 0.14989453999869534, 'validation/ctc_loss': Array(0.7825395, dtype=float32), 'validation/wer': 0.23059556773340795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47780722, dtype=float32), 'test/wer': 0.15987244327991387, 'test/num_examples': 2472, 'score': 12066.015416622162, 'total_duration': 12848.279166460037, 'accumulated_submission_time': 12066.015416622162, 'accumulated_data_selection_time': 3071.302515029907, 'accumulated_eval_time': 781.8649756908417, 'accumulated_logging_time': 0.1941695213317871}
I0531 11:02:48.576107 140130208925440 logging_writer.py:48] [14683] accumulated_data_selection_time=3071.302515, accumulated_eval_time=781.864976, accumulated_logging_time=0.194170, accumulated_submission_time=12066.015417, global_step=14683, preemption_count=0, score=12066.015417, test/ctc_loss=0.4778072237968445, test/num_examples=2472, test/wer=0.159872, total_duration=12848.279166, train/ctc_loss=0.425836443901062, train/wer=0.149895, validation/ctc_loss=0.7825394868850708, validation/num_examples=5348, validation/wer=0.230596
I0531 11:03:02.126784 140130200532736 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.639196872711182, loss=1.6343659162521362
I0531 11:04:18.165597 140130208925440 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.803915977478027, loss=1.7362632751464844
I0531 11:05:33.169574 140130200532736 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.269031524658203, loss=1.6543455123901367
I0531 11:06:56.481120 140130208925440 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.654282808303833, loss=1.6842286586761475
I0531 11:08:21.196592 140130200532736 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.5624797344207764, loss=1.6904964447021484
I0531 11:09:45.196267 140130208925440 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.0356152057647705, loss=1.666412115097046
I0531 11:11:13.634083 140130200532736 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.346754550933838, loss=1.6891714334487915
I0531 11:12:44.926239 140130208925440 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.799770832061768, loss=1.6901289224624634
I0531 11:14:10.249239 140130208925440 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.224196434020996, loss=1.674697995185852
I0531 11:15:25.582772 140130200532736 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.188746452331543, loss=1.6577692031860352
I0531 11:16:41.266351 140130208925440 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.6540563106536865, loss=1.6718177795410156
I0531 11:17:57.740305 140130200532736 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.913959980010986, loss=1.68900465965271
I0531 11:19:22.632163 140130208925440 logging_writer.py:48] [15900] global_step=15900, grad_norm=7.610279560089111, loss=1.623171329498291
I0531 11:20:48.711063 140288245086016 spec.py:298] Evaluating on the training split.
I0531 11:21:37.745626 140288245086016 spec.py:310] Evaluating on the validation split.
I0531 11:22:20.895121 140288245086016 spec.py:326] Evaluating on the test split.
I0531 11:22:44.186275 140288245086016 submission_runner.py:426] Time since start: 14043.91s, 	Step: 16000, 	{'train/ctc_loss': Array(0.42659414, dtype=float32), 'train/wer': 0.14822006817781713, 'validation/ctc_loss': Array(0.7563801, dtype=float32), 'validation/wer': 0.22172910496000925, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45284322, dtype=float32), 'test/wer': 0.15176812300692624, 'test/num_examples': 2472, 'score': 13146.117279052734, 'total_duration': 14043.912252426147, 'accumulated_submission_time': 13146.117279052734, 'accumulated_data_selection_time': 3361.150216817856, 'accumulated_eval_time': 897.3377604484558, 'accumulated_logging_time': 0.23148846626281738}
I0531 11:22:44.209160 140130208925440 logging_writer.py:48] [16000] accumulated_data_selection_time=3361.150217, accumulated_eval_time=897.337760, accumulated_logging_time=0.231488, accumulated_submission_time=13146.117279, global_step=16000, preemption_count=0, score=13146.117279, test/ctc_loss=0.45284321904182434, test/num_examples=2472, test/wer=0.151768, total_duration=14043.912252, train/ctc_loss=0.4265941381454468, train/wer=0.148220, validation/ctc_loss=0.7563800811767578, validation/num_examples=5348, validation/wer=0.221729
I0531 11:22:44.232779 140130200532736 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=13146.117279
I0531 11:22:44.377985 140288245086016 checkpoints.py:490] Saving checkpoint at step: 16000
I0531 11:22:45.240063 140288245086016 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0531 11:22:45.259919 140288245086016 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_jax_upgrade_b/lamb/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0531 11:22:46.286387 140288245086016 submission_runner.py:589] Tuning trial 1/1
I0531 11:22:46.286606 140288245086016 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.19395352613343847, beta2=0.999, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0531 11:22:46.292419 140288245086016 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(29.451366, dtype=float32), 'train/wer': 3.2654301064073463, 'validation/ctc_loss': Array(28.358381, dtype=float32), 'validation/wer': 2.87474071143957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.283052, dtype=float32), 'test/wer': 3.152093108281031, 'test/num_examples': 2472, 'score': 64.05605888366699, 'total_duration': 269.128536939621, 'accumulated_submission_time': 64.05605888366699, 'accumulated_data_selection_time': 5.129445791244507, 'accumulated_eval_time': 205.07228326797485, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2968, {'train/ctc_loss': Array(6.318609, dtype=float32), 'train/wer': 0.9463038866346728, 'validation/ctc_loss': Array(6.2772756, dtype=float32), 'validation/wer': 0.9177705525378923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.103213, dtype=float32), 'test/wer': 0.9201551804683851, 'test/num_examples': 2472, 'score': 2464.3369357585907, 'total_duration': 2781.2241513729095, 'accumulated_submission_time': 2464.3369357585907, 'accumulated_data_selection_time': 537.235538482666, 'accumulated_eval_time': 316.8101451396942, 'accumulated_logging_time': 0.038315534591674805, 'global_step': 2968, 'preemption_count': 0}), (5895, {'train/ctc_loss': Array(1.3605129, dtype=float32), 'train/wer': 0.3887542284846709, 'validation/ctc_loss': Array(1.7552879, dtype=float32), 'validation/wer': 0.4390105066136673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.359532, dtype=float32), 'test/wer': 0.3807608717729978, 'test/num_examples': 2472, 'score': 4864.82848072052, 'total_duration': 5296.042528152466, 'accumulated_submission_time': 4864.82848072052, 'accumulated_data_selection_time': 1150.8172585964203, 'accumulated_eval_time': 431.0517303943634, 'accumulated_logging_time': 0.08106279373168945, 'global_step': 5895, 'preemption_count': 0}), (8821, {'train/ctc_loss': Array(0.66282105, dtype=float32), 'train/wer': 0.2271560426121838, 'validation/ctc_loss': Array(1.053084, dtype=float32), 'validation/wer': 0.3037945373327287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7258549, dtype=float32), 'test/wer': 0.233420673125749, 'test/num_examples': 2472, 'score': 7264.84740114212, 'total_duration': 7813.327271461487, 'accumulated_submission_time': 7264.84740114212, 'accumulated_data_selection_time': 1784.4445383548737, 'accumulated_eval_time': 548.2380123138428, 'accumulated_logging_time': 0.11923456192016602, 'global_step': 8821, 'preemption_count': 0}), (11757, {'train/ctc_loss': Array(0.51418155, dtype=float32), 'train/wer': 0.17557519714312644, 'validation/ctc_loss': Array(0.8727024, dtype=float32), 'validation/wer': 0.2552557188202491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55819875, dtype=float32), 'test/wer': 0.18333231775435174, 'test/num_examples': 2472, 'score': 9665.96957373619, 'total_duration': 10330.403761863708, 'accumulated_submission_time': 9665.96957373619, 'accumulated_data_selection_time': 2422.839151620865, 'accumulated_eval_time': 664.1150944232941, 'accumulated_logging_time': 0.15587663650512695, 'global_step': 11757, 'preemption_count': 0}), (14683, {'train/ctc_loss': Array(0.42583644, dtype=float32), 'train/wer': 0.14989453999869534, 'validation/ctc_loss': Array(0.7825395, dtype=float32), 'validation/wer': 0.23059556773340795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47780722, dtype=float32), 'test/wer': 0.15987244327991387, 'test/num_examples': 2472, 'score': 12066.015416622162, 'total_duration': 12848.279166460037, 'accumulated_submission_time': 12066.015416622162, 'accumulated_data_selection_time': 3071.302515029907, 'accumulated_eval_time': 781.8649756908417, 'accumulated_logging_time': 0.1941695213317871, 'global_step': 14683, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.42659414, dtype=float32), 'train/wer': 0.14822006817781713, 'validation/ctc_loss': Array(0.7563801, dtype=float32), 'validation/wer': 0.22172910496000925, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45284322, dtype=float32), 'test/wer': 0.15176812300692624, 'test/num_examples': 2472, 'score': 13146.117279052734, 'total_duration': 14043.912252426147, 'accumulated_submission_time': 13146.117279052734, 'accumulated_data_selection_time': 3361.150216817856, 'accumulated_eval_time': 897.3377604484558, 'accumulated_logging_time': 0.23148846626281738, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0531 11:22:46.292585 140288245086016 submission_runner.py:592] Timing: 13146.117279052734
I0531 11:22:46.292649 140288245086016 submission_runner.py:593] ====================
I0531 11:22:46.293625 140288245086016 submission_runner.py:661] Final librispeech_deepspeech score: 13146.117279052734
