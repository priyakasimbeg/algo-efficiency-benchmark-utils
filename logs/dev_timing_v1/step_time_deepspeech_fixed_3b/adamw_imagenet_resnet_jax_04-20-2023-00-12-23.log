I0420 00:12:45.672589 140415007438656 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax.
I0420 00:12:45.735007 140415007438656 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 00:12:46.558856 140415007438656 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0420 00:12:46.559607 140415007438656 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 00:12:46.563817 140415007438656 submission_runner.py:528] Using RNG seed 3915598785
I0420 00:12:49.452522 140415007438656 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 00:12:49.452704 140415007438656 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1.
I0420 00:12:49.452919 140415007438656 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/hparams.json.
I0420 00:12:49.575236 140415007438656 submission_runner.py:232] Initializing dataset.
I0420 00:12:49.588025 140415007438656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:12:49.594958 140415007438656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:12:49.595072 140415007438656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:12:49.854073 140415007438656 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:12:50.941395 140415007438656 submission_runner.py:239] Initializing model.
I0420 00:13:02.211051 140415007438656 submission_runner.py:249] Initializing optimizer.
I0420 00:13:03.322759 140415007438656 submission_runner.py:256] Initializing metrics bundle.
I0420 00:13:03.322971 140415007438656 submission_runner.py:273] Initializing checkpoint and logger.
I0420 00:13:03.323856 140415007438656 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0420 00:13:04.039764 140415007438656 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0420 00:13:04.040703 140415007438656 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0420 00:13:04.045620 140415007438656 submission_runner.py:309] Starting training loop.
I0420 00:13:52.816770 140238903047936 logging_writer.py:48] [0] global_step=0, grad_norm=0.5977557897567749, loss=6.9275922775268555
I0420 00:13:52.830736 140415007438656 spec.py:298] Evaluating on the training split.
I0420 00:13:53.309267 140415007438656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:13:53.315191 140415007438656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:13:53.315304 140415007438656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:13:53.375744 140415007438656 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:14:04.856293 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 00:14:05.522530 140415007438656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:14:05.536642 140415007438656 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 00:14:05.536914 140415007438656 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 00:14:05.603900 140415007438656 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 00:14:23.129138 140415007438656 spec.py:326] Evaluating on the test split.
I0420 00:14:23.530797 140415007438656 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 00:14:23.536260 140415007438656 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0420 00:14:23.565288 140415007438656 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 00:14:32.810836 140415007438656 submission_runner.py:406] Time since start: 88.77s, 	Step: 1, 	{'train/accuracy': 0.0009367027669213712, 'train/loss': 6.9118804931640625, 'validation/accuracy': 0.0011599999852478504, 'validation/loss': 6.911995887756348, 'validation/num_examples': 50000, 'test/accuracy': 0.0017000001389533281, 'test/loss': 6.911821365356445, 'test/num_examples': 10000, 'score': 48.78495240211487, 'total_duration': 88.76513004302979, 'accumulated_submission_time': 48.78495240211487, 'accumulated_eval_time': 39.98003005981445, 'accumulated_logging_time': 0}
I0420 00:14:32.828004 140209467418368 logging_writer.py:48] [1] accumulated_eval_time=39.980030, accumulated_logging_time=0, accumulated_submission_time=48.784952, global_step=1, preemption_count=0, score=48.784952, test/accuracy=0.001700, test/loss=6.911821, test/num_examples=10000, total_duration=88.765130, train/accuracy=0.000937, train/loss=6.911880, validation/accuracy=0.001160, validation/loss=6.911996, validation/num_examples=50000
I0420 00:14:32.999028 140415007438656 checkpoints.py:356] Saving checkpoint at step: 1
I0420 00:14:33.629364 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1
I0420 00:14:33.630276 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1.
I0420 00:15:07.763566 140209475811072 logging_writer.py:48] [100] global_step=100, grad_norm=0.5902866721153259, loss=6.88146448135376
I0420 00:15:41.685636 140209698055936 logging_writer.py:48] [200] global_step=200, grad_norm=0.6535279154777527, loss=6.743161201477051
I0420 00:16:15.642443 140209475811072 logging_writer.py:48] [300] global_step=300, grad_norm=0.7212547659873962, loss=6.575701713562012
I0420 00:16:49.589406 140209698055936 logging_writer.py:48] [400] global_step=400, grad_norm=0.9170354604721069, loss=6.3902082443237305
I0420 00:17:23.534579 140209475811072 logging_writer.py:48] [500] global_step=500, grad_norm=1.8687411546707153, loss=6.233687400817871
I0420 00:17:57.429209 140209698055936 logging_writer.py:48] [600] global_step=600, grad_norm=1.8132851123809814, loss=6.158222198486328
I0420 00:18:31.362202 140209475811072 logging_writer.py:48] [700] global_step=700, grad_norm=3.291166305541992, loss=5.985260486602783
I0420 00:19:05.273004 140209698055936 logging_writer.py:48] [800] global_step=800, grad_norm=2.154438018798828, loss=5.843096733093262
I0420 00:19:39.244345 140209475811072 logging_writer.py:48] [900] global_step=900, grad_norm=2.851010799407959, loss=5.71498966217041
I0420 00:20:13.195822 140209698055936 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.072789430618286, loss=5.709977626800537
I0420 00:20:47.160738 140209475811072 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.841313123703003, loss=5.546139717102051
I0420 00:21:21.165293 140209698055936 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.36527156829834, loss=5.568972587585449
I0420 00:21:55.079678 140209475811072 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.428104400634766, loss=5.451845645904541
I0420 00:22:29.057816 140209698055936 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.9887304306030273, loss=5.408640384674072
I0420 00:23:03.063114 140209475811072 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.130387783050537, loss=5.330503463745117
I0420 00:23:03.822350 140415007438656 spec.py:298] Evaluating on the training split.
I0420 00:23:10.537681 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 00:23:18.120597 140415007438656 spec.py:326] Evaluating on the test split.
I0420 00:23:20.281712 140415007438656 submission_runner.py:406] Time since start: 616.24s, 	Step: 1504, 	{'train/accuracy': 0.12454161047935486, 'train/loss': 4.71127986907959, 'validation/accuracy': 0.1078999936580658, 'validation/loss': 4.840988636016846, 'validation/num_examples': 50000, 'test/accuracy': 0.08260000497102737, 'test/loss': 5.176387786865234, 'test/num_examples': 10000, 'score': 558.9548420906067, 'total_duration': 616.2360188961029, 'accumulated_submission_time': 558.9548420906067, 'accumulated_eval_time': 56.43934869766235, 'accumulated_logging_time': 0.8240387439727783}
I0420 00:23:20.289331 140209714841344 logging_writer.py:48] [1504] accumulated_eval_time=56.439349, accumulated_logging_time=0.824039, accumulated_submission_time=558.954842, global_step=1504, preemption_count=0, score=558.954842, test/accuracy=0.082600, test/loss=5.176388, test/num_examples=10000, total_duration=616.236019, train/accuracy=0.124542, train/loss=4.711280, validation/accuracy=0.107900, validation/loss=4.840989, validation/num_examples=50000
I0420 00:23:20.464222 140415007438656 checkpoints.py:356] Saving checkpoint at step: 1504
I0420 00:23:21.076913 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1504
I0420 00:23:21.077967 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_1504.
I0420 00:23:53.909929 140209723234048 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.914072513580322, loss=5.225078105926514
I0420 00:24:27.888236 140239330830080 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.2474846839904785, loss=5.197333812713623
I0420 00:25:02.051514 140209723234048 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.2399158477783203, loss=5.059271335601807
I0420 00:25:35.936289 140239330830080 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.229980945587158, loss=4.935428619384766
I0420 00:26:09.621741 140209723234048 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.894545793533325, loss=4.935839653015137
I0420 00:26:43.400648 140239330830080 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.81813907623291, loss=4.889249324798584
I0420 00:27:17.276868 140209723234048 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.63356876373291, loss=4.858696937561035
I0420 00:27:51.266821 140239330830080 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.9201691150665283, loss=4.651984214782715
I0420 00:28:25.260469 140209723234048 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.986683368682861, loss=4.704876899719238
I0420 00:28:59.068541 140239330830080 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.197488307952881, loss=4.7441277503967285
I0420 00:29:33.011049 140209723234048 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.9167256355285645, loss=4.666571617126465
I0420 00:30:07.012406 140239330830080 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.432863473892212, loss=4.493646621704102
I0420 00:30:40.969681 140209723234048 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.935882568359375, loss=4.442667484283447
I0420 00:31:14.731113 140239330830080 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.32761812210083, loss=4.3973283767700195
I0420 00:31:48.547214 140209723234048 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.254863977432251, loss=4.3957366943359375
I0420 00:31:51.347487 140415007438656 spec.py:298] Evaluating on the training split.
I0420 00:31:57.984159 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 00:32:05.711302 140415007438656 spec.py:326] Evaluating on the test split.
I0420 00:32:07.570800 140415007438656 submission_runner.py:406] Time since start: 1143.53s, 	Step: 3010, 	{'train/accuracy': 0.24770806729793549, 'train/loss': 3.6940927505493164, 'validation/accuracy': 0.2250799983739853, 'validation/loss': 3.859097480773926, 'validation/num_examples': 50000, 'test/accuracy': 0.17020000517368317, 'test/loss': 4.3999528884887695, 'test/num_examples': 10000, 'score': 1069.2006242275238, 'total_duration': 1143.5250790119171, 'accumulated_submission_time': 1069.2006242275238, 'accumulated_eval_time': 72.66259002685547, 'accumulated_logging_time': 1.625957727432251}
I0420 00:32:07.578638 140239330830080 logging_writer.py:48] [3010] accumulated_eval_time=72.662590, accumulated_logging_time=1.625958, accumulated_submission_time=1069.200624, global_step=3010, preemption_count=0, score=1069.200624, test/accuracy=0.170200, test/loss=4.399953, test/num_examples=10000, total_duration=1143.525079, train/accuracy=0.247708, train/loss=3.694093, validation/accuracy=0.225080, validation/loss=3.859097, validation/num_examples=50000
I0420 00:32:07.757552 140415007438656 checkpoints.py:356] Saving checkpoint at step: 3010
I0420 00:32:08.371879 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_3010
I0420 00:32:08.372957 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_3010.
I0420 00:32:39.278895 140209723234048 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.830447196960449, loss=4.347990036010742
I0420 00:33:13.294565 140239297259264 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.330850839614868, loss=4.279589653015137
I0420 00:33:47.391293 140209723234048 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.233760356903076, loss=4.271191596984863
I0420 00:34:21.250685 140239297259264 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.6335949897766113, loss=4.18375825881958
I0420 00:34:55.036912 140209723234048 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.2094075679779053, loss=4.142362594604492
I0420 00:35:28.974348 140239297259264 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.6238903999328613, loss=4.092596530914307
I0420 00:36:03.118021 140209723234048 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.2038841247558594, loss=4.024369239807129
I0420 00:36:37.182596 140239297259264 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.1708996295928955, loss=3.986870527267456
I0420 00:37:11.253128 140209723234048 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.065272569656372, loss=4.038177013397217
I0420 00:37:45.303225 140239297259264 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.173510789871216, loss=3.8651983737945557
I0420 00:38:19.265595 140209723234048 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.468655824661255, loss=3.8929123878479004
I0420 00:38:53.208060 140239297259264 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.6123414039611816, loss=4.02992057800293
I0420 00:39:27.146683 140209723234048 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.011749505996704, loss=3.954406976699829
I0420 00:40:01.190740 140239297259264 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.6695494651794434, loss=3.894575595855713
I0420 00:40:35.205114 140209723234048 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.8198145627975464, loss=3.902789831161499
I0420 00:40:38.711557 140415007438656 spec.py:298] Evaluating on the training split.
I0420 00:40:45.309765 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 00:40:53.012099 140415007438656 spec.py:326] Evaluating on the test split.
I0420 00:40:54.926127 140415007438656 submission_runner.py:406] Time since start: 1670.88s, 	Step: 4512, 	{'train/accuracy': 0.35509008169174194, 'train/loss': 3.0180928707122803, 'validation/accuracy': 0.32729998230934143, 'validation/loss': 3.1676151752471924, 'validation/num_examples': 50000, 'test/accuracy': 0.2345000058412552, 'test/loss': 3.84676194190979, 'test/num_examples': 10000, 'score': 1579.5160419940948, 'total_duration': 1670.880448102951, 'accumulated_submission_time': 1579.5160419940948, 'accumulated_eval_time': 88.87712860107422, 'accumulated_logging_time': 2.4337689876556396}
I0420 00:40:54.933682 140239297259264 logging_writer.py:48] [4512] accumulated_eval_time=88.877129, accumulated_logging_time=2.433769, accumulated_submission_time=1579.516042, global_step=4512, preemption_count=0, score=1579.516042, test/accuracy=0.234500, test/loss=3.846762, test/num_examples=10000, total_duration=1670.880448, train/accuracy=0.355090, train/loss=3.018093, validation/accuracy=0.327300, validation/loss=3.167615, validation/num_examples=50000
I0420 00:40:55.101684 140415007438656 checkpoints.py:356] Saving checkpoint at step: 4512
I0420 00:40:55.714710 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_4512
I0420 00:40:55.715648 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_4512.
I0420 00:41:25.945171 140209723234048 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.6670267581939697, loss=3.752784252166748
I0420 00:41:59.703401 140239288866560 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.1513724327087402, loss=3.843801259994507
I0420 00:42:33.714671 140209723234048 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.1111552715301514, loss=3.7558348178863525
I0420 00:43:07.756629 140239288866560 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.684285283088684, loss=3.752058744430542
I0420 00:43:41.839547 140209723234048 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.9132105112075806, loss=3.6143252849578857
I0420 00:44:15.866344 140239288866560 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.5613551139831543, loss=3.6353819370269775
I0420 00:44:49.707432 140209723234048 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.855258822441101, loss=3.64810848236084
I0420 00:45:23.782396 140239288866560 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.8107645511627197, loss=3.560526132583618
I0420 00:45:57.669404 140209723234048 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9743086099624634, loss=3.6273202896118164
I0420 00:46:31.646092 140239288866560 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.1342077255249023, loss=3.5772252082824707
I0420 00:47:05.434841 140209723234048 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.6633018255233765, loss=3.562027931213379
I0420 00:47:39.198962 140239288866560 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9723573923110962, loss=3.4779412746429443
I0420 00:48:12.965634 140209723234048 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.163048505783081, loss=3.506739377975464
I0420 00:48:46.970252 140239288866560 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.9181950092315674, loss=3.4957218170166016
I0420 00:49:20.900479 140209723234048 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.4751251935958862, loss=3.4609217643737793
I0420 00:49:25.781873 140415007438656 spec.py:298] Evaluating on the training split.
I0420 00:49:32.492966 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 00:49:39.970297 140415007438656 spec.py:326] Evaluating on the test split.
I0420 00:49:42.091830 140415007438656 submission_runner.py:406] Time since start: 2198.05s, 	Step: 6016, 	{'train/accuracy': 0.47757890820503235, 'train/loss': 2.3753726482391357, 'validation/accuracy': 0.4452599883079529, 'validation/loss': 2.528930187225342, 'validation/num_examples': 50000, 'test/accuracy': 0.3375000059604645, 'test/loss': 3.220114231109619, 'test/num_examples': 10000, 'score': 2089.560332775116, 'total_duration': 2198.046154499054, 'accumulated_submission_time': 2089.560332775116, 'accumulated_eval_time': 105.18708562850952, 'accumulated_logging_time': 3.228041887283325}
I0420 00:49:42.099808 140239288866560 logging_writer.py:48] [6016] accumulated_eval_time=105.187086, accumulated_logging_time=3.228042, accumulated_submission_time=2089.560333, global_step=6016, preemption_count=0, score=2089.560333, test/accuracy=0.337500, test/loss=3.220114, test/num_examples=10000, total_duration=2198.046154, train/accuracy=0.477579, train/loss=2.375373, validation/accuracy=0.445260, validation/loss=2.528930, validation/num_examples=50000
I0420 00:49:42.253942 140415007438656 checkpoints.py:356] Saving checkpoint at step: 6016
I0420 00:49:43.106144 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_6016
I0420 00:49:43.123866 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_6016.
I0420 00:50:11.944864 140209723234048 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.143858790397644, loss=3.4430761337280273
I0420 00:50:45.915081 140239079180032 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.499953269958496, loss=3.3750853538513184
I0420 00:51:19.802992 140209723234048 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.3704124689102173, loss=3.457446813583374
I0420 00:51:53.692304 140239079180032 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.3244903087615967, loss=3.4765284061431885
I0420 00:52:27.520168 140209723234048 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.6320855617523193, loss=3.391540288925171
I0420 00:53:01.513522 140239079180032 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.3827866315841675, loss=3.515693426132202
I0420 00:53:35.323536 140209723234048 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.4463319778442383, loss=3.41851544380188
I0420 00:54:09.291991 140239079180032 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.3017915487289429, loss=3.364572763442993
I0420 00:54:43.124875 140209723234048 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.6960965394973755, loss=3.426699161529541
I0420 00:55:16.783373 140239079180032 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.856882095336914, loss=3.287943124771118
I0420 00:55:50.576175 140209723234048 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.2634268999099731, loss=3.3229191303253174
I0420 00:56:24.597200 140239079180032 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.2975099086761475, loss=3.3207740783691406
I0420 00:56:58.399735 140209723234048 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.3757009506225586, loss=3.2418456077575684
I0420 00:57:32.092770 140239079180032 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0215024948120117, loss=3.2876946926116943
I0420 00:58:06.027081 140209723234048 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.1679713726043701, loss=3.2809414863586426
I0420 00:58:13.314191 140415007438656 spec.py:298] Evaluating on the training split.
I0420 00:58:19.980520 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 00:58:27.588572 140415007438656 spec.py:326] Evaluating on the test split.
I0420 00:58:29.705866 140415007438656 submission_runner.py:406] Time since start: 2725.66s, 	Step: 7523, 	{'train/accuracy': 0.5130141973495483, 'train/loss': 2.2139453887939453, 'validation/accuracy': 0.47567999362945557, 'validation/loss': 2.39107084274292, 'validation/num_examples': 50000, 'test/accuracy': 0.3710000216960907, 'test/loss': 3.06379771232605, 'test/num_examples': 10000, 'score': 2599.728401660919, 'total_duration': 2725.6601889133453, 'accumulated_submission_time': 2599.728401660919, 'accumulated_eval_time': 121.57873201370239, 'accumulated_logging_time': 4.264488220214844}
I0420 00:58:29.713423 140239079180032 logging_writer.py:48] [7523] accumulated_eval_time=121.578732, accumulated_logging_time=4.264488, accumulated_submission_time=2599.728402, global_step=7523, preemption_count=0, score=2599.728402, test/accuracy=0.371000, test/loss=3.063798, test/num_examples=10000, total_duration=2725.660189, train/accuracy=0.513014, train/loss=2.213945, validation/accuracy=0.475680, validation/loss=2.391071, validation/num_examples=50000
I0420 00:58:29.913959 140415007438656 checkpoints.py:356] Saving checkpoint at step: 7523
I0420 00:58:30.858964 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_7523
I0420 00:58:30.872386 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_7523.
I0420 00:58:57.380573 140209723234048 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9194391965866089, loss=3.2453978061676025
I0420 00:59:31.158413 140239070787328 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0174564123153687, loss=3.195765733718872
I0420 01:00:05.166288 140209723234048 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.1535621881484985, loss=3.2903618812561035
I0420 01:00:39.003473 140239070787328 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.3713266849517822, loss=3.2390520572662354
I0420 01:01:12.880958 140209723234048 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.850448727607727, loss=3.213533878326416
I0420 01:01:46.988059 140239070787328 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0160489082336426, loss=3.1807942390441895
I0420 01:02:20.875810 140209723234048 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.171827793121338, loss=3.091925859451294
I0420 01:02:54.723472 140239070787328 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.110334873199463, loss=3.117589235305786
I0420 01:03:28.590964 140209723234048 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.1443790197372437, loss=3.0647571086883545
I0420 01:04:02.379141 140239070787328 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.15162992477417, loss=3.144254207611084
I0420 01:04:36.136222 140209723234048 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8328163623809814, loss=3.145063638687134
I0420 01:05:09.958183 140239070787328 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8326624035835266, loss=3.1446073055267334
I0420 01:05:43.658530 140209723234048 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.1900759935379028, loss=3.08502197265625
I0420 01:06:17.445565 140239070787328 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8629036545753479, loss=2.9643023014068604
I0420 01:06:51.347221 140209723234048 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0770931243896484, loss=3.095759153366089
I0420 01:07:00.921296 140415007438656 spec.py:298] Evaluating on the training split.
I0420 01:07:07.735062 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 01:07:15.541860 140415007438656 spec.py:326] Evaluating on the test split.
I0420 01:07:19.183711 140415007438656 submission_runner.py:406] Time since start: 3255.14s, 	Step: 9030, 	{'train/accuracy': 0.5517578125, 'train/loss': 1.9773160219192505, 'validation/accuracy': 0.5109800100326538, 'validation/loss': 2.182656764984131, 'validation/num_examples': 50000, 'test/accuracy': 0.3954000174999237, 'test/loss': 2.8749191761016846, 'test/num_examples': 10000, 'score': 3109.7510163784027, 'total_duration': 3255.1380252838135, 'accumulated_submission_time': 3109.7510163784027, 'accumulated_eval_time': 139.84110975265503, 'accumulated_logging_time': 5.440221786499023}
I0420 01:07:19.191527 140239070787328 logging_writer.py:48] [9030] accumulated_eval_time=139.841110, accumulated_logging_time=5.440222, accumulated_submission_time=3109.751016, global_step=9030, preemption_count=0, score=3109.751016, test/accuracy=0.395400, test/loss=2.874919, test/num_examples=10000, total_duration=3255.138025, train/accuracy=0.551758, train/loss=1.977316, validation/accuracy=0.510980, validation/loss=2.182657, validation/num_examples=50000
I0420 01:07:19.372824 140415007438656 checkpoints.py:356] Saving checkpoint at step: 9030
I0420 01:07:20.030256 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_9030
I0420 01:07:20.031220 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_9030.
I0420 01:07:44.060819 140209723234048 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1808109283447266, loss=3.0330448150634766
I0420 01:08:17.896735 140239062394624 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9014605283737183, loss=3.111534833908081
I0420 01:08:51.871134 140209723234048 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9992343783378601, loss=3.046147108078003
I0420 01:09:25.680859 140239062394624 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.039415717124939, loss=3.0011370182037354
I0420 01:09:59.441896 140209723234048 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8275839686393738, loss=3.070281744003296
I0420 01:10:33.454007 140239062394624 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.9741216897964478, loss=3.135711193084717
I0420 01:11:07.219453 140209723234048 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8714412450790405, loss=3.0124473571777344
I0420 01:11:41.306507 140239062394624 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.841144323348999, loss=2.988144636154175
I0420 01:12:15.110577 140209723234048 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7256740927696228, loss=2.9502291679382324
I0420 01:12:48.809764 140239062394624 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8321709036827087, loss=2.9069972038269043
I0420 01:13:22.713470 140209723234048 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7215611934661865, loss=3.010570764541626
I0420 01:13:56.723319 140239062394624 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8306542634963989, loss=3.017162799835205
I0420 01:14:30.681520 140209723234048 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.76548171043396, loss=2.939530849456787
I0420 01:15:04.510869 140239062394624 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7909133434295654, loss=3.0272271633148193
I0420 01:15:38.324955 140209723234048 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8386317491531372, loss=2.9372215270996094
I0420 01:15:50.118813 140415007438656 spec.py:298] Evaluating on the training split.
I0420 01:15:56.676077 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 01:16:04.911020 140415007438656 spec.py:326] Evaluating on the test split.
I0420 01:16:07.021954 140415007438656 submission_runner.py:406] Time since start: 3782.98s, 	Step: 10537, 	{'train/accuracy': 0.6278300285339355, 'train/loss': 1.666878342628479, 'validation/accuracy': 0.5579400062561035, 'validation/loss': 2.0028305053710938, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.7190849781036377, 'test/num_examples': 10000, 'score': 3619.8183178901672, 'total_duration': 3782.9762799739838, 'accumulated_submission_time': 3619.8183178901672, 'accumulated_eval_time': 156.744238615036, 'accumulated_logging_time': 6.291198968887329}
I0420 01:16:07.030313 140239062394624 logging_writer.py:48] [10537] accumulated_eval_time=156.744239, accumulated_logging_time=6.291199, accumulated_submission_time=3619.818318, global_step=10537, preemption_count=0, score=3619.818318, test/accuracy=0.431200, test/loss=2.719085, test/num_examples=10000, total_duration=3782.976280, train/accuracy=0.627830, train/loss=1.666878, validation/accuracy=0.557940, validation/loss=2.002831, validation/num_examples=50000
I0420 01:16:07.279104 140415007438656 checkpoints.py:356] Saving checkpoint at step: 10537
I0420 01:16:08.201110 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_10537
I0420 01:16:08.214342 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_10537.
I0420 01:16:29.845113 140209723234048 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6295159459114075, loss=2.9458513259887695
I0420 01:17:03.522293 140239054001920 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8209843635559082, loss=2.8383002281188965
I0420 01:17:37.474401 140209723234048 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.863375723361969, loss=2.9863829612731934
I0420 01:18:11.359020 140239054001920 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7879823446273804, loss=2.923279285430908
I0420 01:18:45.212007 140209723234048 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6986532807350159, loss=2.9850361347198486
I0420 01:19:19.041637 140239054001920 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8293009996414185, loss=2.9960525035858154
I0420 01:19:53.013887 140209723234048 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.690143883228302, loss=2.9029600620269775
I0420 01:20:26.986088 140239054001920 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6685746312141418, loss=2.8283097743988037
I0420 01:21:00.786981 140209723234048 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.887639582157135, loss=2.8311767578125
I0420 01:21:34.549017 140239054001920 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6337109804153442, loss=2.7760112285614014
I0420 01:22:08.332121 140209723234048 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8673532009124756, loss=2.8689775466918945
I0420 01:22:42.288980 140239054001920 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5863776803016663, loss=2.8008108139038086
I0420 01:23:16.270231 140209723234048 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.608376145362854, loss=2.9385199546813965
I0420 01:23:50.158620 140239054001920 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.558377742767334, loss=2.749506950378418
I0420 01:24:23.851723 140209723234048 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6528962254524231, loss=2.8010683059692383
I0420 01:24:38.500604 140415007438656 spec.py:298] Evaluating on the training split.
I0420 01:24:45.078630 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 01:24:54.850090 140415007438656 spec.py:326] Evaluating on the test split.
I0420 01:24:56.728372 140415007438656 submission_runner.py:406] Time since start: 4312.68s, 	Step: 12045, 	{'train/accuracy': 0.652363657951355, 'train/loss': 1.5056946277618408, 'validation/accuracy': 0.5895400047302246, 'validation/loss': 1.8114515542984009, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.527686357498169, 'test/num_examples': 10000, 'score': 4130.079380512238, 'total_duration': 4312.682667970657, 'accumulated_submission_time': 4130.079380512238, 'accumulated_eval_time': 174.97196292877197, 'accumulated_logging_time': 7.491811990737915}
I0420 01:24:56.738776 140239054001920 logging_writer.py:48] [12045] accumulated_eval_time=174.971963, accumulated_logging_time=7.491812, accumulated_submission_time=4130.079381, global_step=12045, preemption_count=0, score=4130.079381, test/accuracy=0.460100, test/loss=2.527686, test/num_examples=10000, total_duration=4312.682668, train/accuracy=0.652364, train/loss=1.505695, validation/accuracy=0.589540, validation/loss=1.811452, validation/num_examples=50000
I0420 01:24:57.013773 140415007438656 checkpoints.py:356] Saving checkpoint at step: 12045
I0420 01:24:58.154190 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_12045
I0420 01:24:58.169306 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_12045.
I0420 01:25:17.053549 140209723234048 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6913354396820068, loss=2.829253911972046
I0420 01:25:50.927914 140239045609216 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7621504068374634, loss=2.874161720275879
I0420 01:26:24.738344 140209723234048 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.661607563495636, loss=2.730804443359375
I0420 01:26:58.419445 140239045609216 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7164084911346436, loss=2.8586087226867676
I0420 01:27:32.295431 140209723234048 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6274880170822144, loss=2.7845513820648193
I0420 01:28:05.870497 140239045609216 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7564057111740112, loss=2.840301990509033
I0420 01:28:39.767456 140209723234048 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5122635960578918, loss=2.820096254348755
I0420 01:29:13.809884 140239045609216 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5412835478782654, loss=2.850083827972412
I0420 01:29:47.798689 140209723234048 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6125062108039856, loss=2.804598808288574
I0420 01:30:21.562754 140239045609216 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5475675463676453, loss=2.849804639816284
I0420 01:30:55.509104 140209723234048 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.55653977394104, loss=2.6815576553344727
I0420 01:31:29.271405 140239045609216 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5927240252494812, loss=2.8425111770629883
I0420 01:32:02.926727 140209723234048 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6499801874160767, loss=2.6945998668670654
I0420 01:32:36.776824 140239045609216 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5868114829063416, loss=2.8112473487854004
I0420 01:33:10.662925 140209723234048 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5802991390228271, loss=2.852447986602783
I0420 01:33:28.264709 140415007438656 spec.py:298] Evaluating on the training split.
I0420 01:33:35.857656 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 01:33:45.359700 140415007438656 spec.py:326] Evaluating on the test split.
I0420 01:33:47.414757 140415007438656 submission_runner.py:406] Time since start: 4843.37s, 	Step: 13554, 	{'train/accuracy': 0.6536591053009033, 'train/loss': 1.5391321182250977, 'validation/accuracy': 0.5922200083732605, 'validation/loss': 1.8361238241195679, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.565978765487671, 'test/num_examples': 10000, 'score': 4640.149284601212, 'total_duration': 4843.368032217026, 'accumulated_submission_time': 4640.149284601212, 'accumulated_eval_time': 194.12093544006348, 'accumulated_logging_time': 8.941060304641724}
I0420 01:33:47.428857 140239045609216 logging_writer.py:48] [13554] accumulated_eval_time=194.120935, accumulated_logging_time=8.941060, accumulated_submission_time=4640.149285, global_step=13554, preemption_count=0, score=4640.149285, test/accuracy=0.455700, test/loss=2.565979, test/num_examples=10000, total_duration=4843.368032, train/accuracy=0.653659, train/loss=1.539132, validation/accuracy=0.592220, validation/loss=1.836124, validation/num_examples=50000
I0420 01:33:47.655130 140415007438656 checkpoints.py:356] Saving checkpoint at step: 13554
I0420 01:33:48.590655 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_13554
I0420 01:33:48.603728 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_13554.
I0420 01:34:04.505363 140209723234048 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5758070945739746, loss=2.6675260066986084
I0420 01:34:38.407151 140239037216512 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5419775247573853, loss=2.714991569519043
I0420 01:35:12.178910 140209723234048 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5487107038497925, loss=2.78230357170105
I0420 01:35:46.050775 140239037216512 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6279671788215637, loss=2.7630114555358887
I0420 01:36:19.797770 140209723234048 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5522789359092712, loss=2.703397750854492
I0420 01:36:53.424750 140239037216512 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.4836656451225281, loss=2.701857805252075
I0420 01:37:27.197713 140209723234048 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5719251036643982, loss=2.750335931777954
I0420 01:38:00.987498 140239037216512 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6068369150161743, loss=2.6301209926605225
I0420 01:38:34.886806 140209723234048 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5393412709236145, loss=2.763024091720581
I0420 01:39:08.679754 140239037216512 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5147897601127625, loss=2.682335138320923
I0420 01:39:42.379303 140209723234048 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5764994621276855, loss=2.7359724044799805
I0420 01:40:16.301767 140239037216512 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5924255847930908, loss=2.7172465324401855
I0420 01:40:49.974534 140209723234048 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5190241932868958, loss=2.731851100921631
I0420 01:41:23.614680 140239037216512 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.44685402512550354, loss=2.644505023956299
I0420 01:41:57.318695 140209723234048 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.49567118287086487, loss=2.583953380584717
I0420 01:42:18.716295 140415007438656 spec.py:298] Evaluating on the training split.
I0420 01:42:25.515091 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 01:42:35.074382 140415007438656 spec.py:326] Evaluating on the test split.
I0420 01:42:37.117635 140415007438656 submission_runner.py:406] Time since start: 5373.07s, 	Step: 15065, 	{'train/accuracy': 0.6876793503761292, 'train/loss': 1.3867483139038086, 'validation/accuracy': 0.6176999807357788, 'validation/loss': 1.7002989053726196, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.4263758659362793, 'test/num_examples': 10000, 'score': 5150.239653348923, 'total_duration': 5373.070691823959, 'accumulated_submission_time': 5150.239653348923, 'accumulated_eval_time': 212.52098274230957, 'accumulated_logging_time': 10.134826183319092}
I0420 01:42:37.127856 140239037216512 logging_writer.py:48] [15065] accumulated_eval_time=212.520983, accumulated_logging_time=10.134826, accumulated_submission_time=5150.239653, global_step=15065, preemption_count=0, score=5150.239653, test/accuracy=0.488100, test/loss=2.426376, test/num_examples=10000, total_duration=5373.070692, train/accuracy=0.687679, train/loss=1.386748, validation/accuracy=0.617700, validation/loss=1.700299, validation/num_examples=50000
I0420 01:42:37.482546 140415007438656 checkpoints.py:356] Saving checkpoint at step: 15065
I0420 01:42:38.666760 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_15065
I0420 01:42:38.682720 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_15065.
I0420 01:42:50.828706 140209723234048 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5029398202896118, loss=2.6850993633270264
I0420 01:43:24.676412 140239028823808 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.44975945353507996, loss=2.5464816093444824
I0420 01:43:58.447704 140209723234048 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6164457201957703, loss=2.7907679080963135
I0420 01:44:32.209603 140239028823808 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5447938442230225, loss=2.6262245178222656
I0420 01:45:06.044005 140209723234048 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.5239994525909424, loss=2.6898975372314453
I0420 01:45:39.835778 140239028823808 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.43536919355392456, loss=2.745396137237549
I0420 01:46:13.770989 140209723234048 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5433909296989441, loss=2.6186161041259766
I0420 01:46:47.671302 140239028823808 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.4884636700153351, loss=2.623293876647949
I0420 01:47:21.434070 140209723234048 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.46809303760528564, loss=2.735719680786133
I0420 01:47:55.142139 140239028823808 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.520412266254425, loss=2.6872198581695557
I0420 01:48:28.933484 140209723234048 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.4982808232307434, loss=2.639498233795166
I0420 01:49:02.688358 140239028823808 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.46698126196861267, loss=2.557680130004883
I0420 01:49:36.421888 140209723234048 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5014296770095825, loss=2.717205762863159
I0420 01:50:10.196774 140239028823808 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.48019498586654663, loss=2.6490373611450195
I0420 01:50:43.890034 140209723234048 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5165395736694336, loss=2.6084096431732178
I0420 01:51:08.984703 140415007438656 spec.py:298] Evaluating on the training split.
I0420 01:51:16.470504 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 01:51:26.168935 140415007438656 spec.py:326] Evaluating on the test split.
I0420 01:51:28.037971 140415007438656 submission_runner.py:406] Time since start: 5903.99s, 	Step: 16576, 	{'train/accuracy': 0.6853076815605164, 'train/loss': 1.3572250604629517, 'validation/accuracy': 0.6198199987411499, 'validation/loss': 1.6744951009750366, 'validation/num_examples': 50000, 'test/accuracy': 0.4815000295639038, 'test/loss': 2.395092248916626, 'test/num_examples': 10000, 'score': 5660.513456821442, 'total_duration': 5903.991253376007, 'accumulated_submission_time': 5660.513456821442, 'accumulated_eval_time': 231.5731828212738, 'accumulated_logging_time': 11.710589408874512}
I0420 01:51:28.052559 140239028823808 logging_writer.py:48] [16576] accumulated_eval_time=231.573183, accumulated_logging_time=11.710589, accumulated_submission_time=5660.513457, global_step=16576, preemption_count=0, score=5660.513457, test/accuracy=0.481500, test/loss=2.395092, test/num_examples=10000, total_duration=5903.991253, train/accuracy=0.685308, train/loss=1.357225, validation/accuracy=0.619820, validation/loss=1.674495, validation/num_examples=50000
I0420 01:51:28.325648 140415007438656 checkpoints.py:356] Saving checkpoint at step: 16576
I0420 01:51:29.263312 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_16576
I0420 01:51:29.279593 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_16576.
I0420 01:51:37.696859 140209723234048 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.49923351407051086, loss=2.581063747406006
I0420 01:52:11.632591 140239020431104 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5470564961433411, loss=2.651515007019043
I0420 01:52:45.334286 140209723234048 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.49440905451774597, loss=2.6720356941223145
I0420 01:53:19.118061 140239020431104 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5651081204414368, loss=2.630464553833008
I0420 01:53:53.137208 140209723234048 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5143837928771973, loss=2.673637866973877
I0420 01:54:27.056560 140239020431104 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.4932839572429657, loss=2.6882941722869873
I0420 01:55:00.755017 140209723234048 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.48436081409454346, loss=2.609957695007324
I0420 01:55:34.489440 140239020431104 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5087640285491943, loss=2.5683112144470215
I0420 01:56:08.128974 140209723234048 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.45380091667175293, loss=2.4718093872070312
I0420 01:56:41.784687 140239020431104 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5659621357917786, loss=2.5206258296966553
I0420 01:57:15.585000 140209723234048 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.49808767437934875, loss=2.623462200164795
I0420 01:57:49.243995 140239020431104 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.5092745423316956, loss=2.5163795948028564
I0420 01:58:23.275106 140209723234048 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5275221467018127, loss=2.561617612838745
I0420 01:58:57.177206 140239020431104 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.43434056639671326, loss=2.562912702560425
I0420 01:59:30.827170 140209723234048 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4484255611896515, loss=2.5565009117126465
I0420 01:59:59.571433 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:00:06.615155 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:00:16.866010 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:00:19.031566 140415007438656 submission_runner.py:406] Time since start: 6434.98s, 	Step: 18087, 	{'train/accuracy': 0.7104990482330322, 'train/loss': 1.269576907157898, 'validation/accuracy': 0.6380199790000916, 'validation/loss': 1.5999759435653687, 'validation/num_examples': 50000, 'test/accuracy': 0.5107000470161438, 'test/loss': 2.2953944206237793, 'test/num_examples': 10000, 'score': 6170.783210515976, 'total_duration': 6434.984543800354, 'accumulated_submission_time': 6170.783210515976, 'accumulated_eval_time': 251.0319426059723, 'accumulated_logging_time': 12.95676064491272}
I0420 02:00:19.041695 140239020431104 logging_writer.py:48] [18087] accumulated_eval_time=251.031943, accumulated_logging_time=12.956761, accumulated_submission_time=6170.783211, global_step=18087, preemption_count=0, score=6170.783211, test/accuracy=0.510700, test/loss=2.295394, test/num_examples=10000, total_duration=6434.984544, train/accuracy=0.710499, train/loss=1.269577, validation/accuracy=0.638020, validation/loss=1.599976, validation/num_examples=50000
I0420 02:00:19.388914 140415007438656 checkpoints.py:356] Saving checkpoint at step: 18087
I0420 02:00:20.618377 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_18087
I0420 02:00:20.635974 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_18087.
I0420 02:00:25.421667 140209723234048 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5319607853889465, loss=2.6109752655029297
I0420 02:00:59.484934 140238877869824 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5004667639732361, loss=2.635861396789551
I0420 02:01:33.380311 140209723234048 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.456909716129303, loss=2.5435738563537598
I0420 02:02:07.233621 140238877869824 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5591799020767212, loss=2.4922261238098145
I0420 02:02:41.212391 140209723234048 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4871121942996979, loss=2.562812089920044
I0420 02:03:15.181322 140238877869824 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5280864238739014, loss=2.480752468109131
I0420 02:03:48.957641 140209723234048 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.4944746494293213, loss=2.570713520050049
I0420 02:04:22.813246 140238877869824 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5843797326087952, loss=2.6330370903015137
I0420 02:04:56.585004 140209723234048 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.47523561120033264, loss=2.4791581630706787
I0420 02:05:30.597621 140238877869824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.449618935585022, loss=2.522472858428955
I0420 02:06:04.497920 140209723234048 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.46807312965393066, loss=2.521772861480713
I0420 02:06:38.357974 140238877869824 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.39107760787010193, loss=2.48599910736084
I0420 02:07:12.272004 140209723234048 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.45743653178215027, loss=2.603158950805664
I0420 02:07:46.236949 140238877869824 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.496436208486557, loss=2.611737012863159
I0420 02:08:19.887169 140209723234048 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4822132885456085, loss=2.540454864501953
I0420 02:08:50.947238 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:08:59.072432 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:09:08.979041 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:09:11.089118 140415007438656 submission_runner.py:406] Time since start: 6967.04s, 	Step: 19593, 	{'train/accuracy': 0.753348171710968, 'train/loss': 1.0889217853546143, 'validation/accuracy': 0.6452199816703796, 'validation/loss': 1.5629624128341675, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.239403486251831, 'test/num_examples': 10000, 'score': 6681.065675020218, 'total_duration': 6967.042475223541, 'accumulated_submission_time': 6681.065675020218, 'accumulated_eval_time': 271.1728296279907, 'accumulated_logging_time': 14.572669506072998}
I0420 02:09:11.104681 140238877869824 logging_writer.py:48] [19593] accumulated_eval_time=271.172830, accumulated_logging_time=14.572670, accumulated_submission_time=6681.065675, global_step=19593, preemption_count=0, score=6681.065675, test/accuracy=0.521100, test/loss=2.239403, test/num_examples=10000, total_duration=6967.042475, train/accuracy=0.753348, train/loss=1.088922, validation/accuracy=0.645220, validation/loss=1.562962, validation/num_examples=50000
I0420 02:09:11.366075 140415007438656 checkpoints.py:356] Saving checkpoint at step: 19593
I0420 02:09:12.317070 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_19593
I0420 02:09:12.334021 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_19593.
I0420 02:09:15.116018 140209723234048 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.4865833818912506, loss=2.488543748855591
I0420 02:09:48.730761 140238869477120 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.48132291436195374, loss=2.589174270629883
I0420 02:10:22.647109 140209723234048 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.470020055770874, loss=2.490063428878784
I0420 02:10:56.359640 140238869477120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5128507018089294, loss=2.5271522998809814
I0420 02:11:30.125468 140209723234048 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.45131656527519226, loss=2.525217056274414
I0420 02:12:03.851545 140238869477120 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5119251012802124, loss=2.5426323413848877
I0420 02:12:37.678480 140209723234048 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4808657765388489, loss=2.3681015968322754
I0420 02:13:11.461152 140238869477120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5067086815834045, loss=2.531707286834717
I0420 02:13:45.328994 140209723234048 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6091724038124084, loss=2.5167155265808105
I0420 02:14:19.246642 140238869477120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.4338854253292084, loss=2.399143695831299
I0420 02:14:52.898888 140209723234048 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.4116962254047394, loss=2.518038272857666
I0420 02:15:26.897187 140238869477120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5204970836639404, loss=2.519627571105957
I0420 02:16:00.619752 140209723234048 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5019455552101135, loss=2.4703710079193115
I0420 02:16:34.415783 140238869477120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.4492282271385193, loss=2.476052761077881
I0420 02:17:08.258266 140209723234048 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4290037751197815, loss=2.5425899028778076
I0420 02:17:42.095669 140238869477120 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4615541100502014, loss=2.475722312927246
I0420 02:17:42.505839 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:17:50.451933 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:18:00.327424 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:18:02.219564 140415007438656 submission_runner.py:406] Time since start: 7498.17s, 	Step: 21103, 	{'train/accuracy': 0.7302495241165161, 'train/loss': 1.1815403699874878, 'validation/accuracy': 0.644279956817627, 'validation/loss': 1.587799310684204, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.2992470264434814, 'test/num_examples': 10000, 'score': 7191.211231708527, 'total_duration': 7498.172576904297, 'accumulated_submission_time': 7191.211231708527, 'accumulated_eval_time': 290.8852138519287, 'accumulated_logging_time': 15.826356649398804}
I0420 02:18:02.237524 140209723234048 logging_writer.py:48] [21103] accumulated_eval_time=290.885214, accumulated_logging_time=15.826357, accumulated_submission_time=7191.211232, global_step=21103, preemption_count=0, score=7191.211232, test/accuracy=0.511100, test/loss=2.299247, test/num_examples=10000, total_duration=7498.172577, train/accuracy=0.730250, train/loss=1.181540, validation/accuracy=0.644280, validation/loss=1.587799, validation/num_examples=50000
I0420 02:18:02.567997 140415007438656 checkpoints.py:356] Saving checkpoint at step: 21103
I0420 02:18:03.788972 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_21103
I0420 02:18:03.811250 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_21103.
I0420 02:18:36.935388 140238869477120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.476586252450943, loss=2.444570302963257
I0420 02:19:10.593561 140238861084416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.46293866634368896, loss=2.5619451999664307
I0420 02:19:44.382097 140238869477120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.5487467050552368, loss=2.4577879905700684
I0420 02:20:18.233725 140238861084416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3967146575450897, loss=2.4918789863586426
I0420 02:20:51.790995 140238869477120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5241984724998474, loss=2.464975595474243
I0420 02:21:25.641746 140238861084416 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.426900178194046, loss=2.4623684883117676
I0420 02:21:59.427987 140238869477120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.4844464957714081, loss=2.473111391067505
I0420 02:22:33.247633 140238861084416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.4692607820034027, loss=2.4314823150634766
I0420 02:23:07.096146 140238869477120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4650029242038727, loss=2.4549384117126465
I0420 02:23:40.932906 140238861084416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5231603384017944, loss=2.5049619674682617
I0420 02:24:14.862520 140238869477120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.462319552898407, loss=2.4872827529907227
I0420 02:24:48.633538 140238861084416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.44957152009010315, loss=2.4944798946380615
I0420 02:25:22.596126 140238869477120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4148940443992615, loss=2.5096187591552734
I0420 02:25:56.364185 140238861084416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.4633932113647461, loss=2.4566311836242676
I0420 02:26:30.257213 140238869477120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.427888423204422, loss=2.4975414276123047
I0420 02:26:34.003173 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:26:41.401080 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:26:51.449884 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:26:53.508418 140415007438656 submission_runner.py:406] Time since start: 8029.46s, 	Step: 22613, 	{'train/accuracy': 0.7429448366165161, 'train/loss': 1.1033999919891357, 'validation/accuracy': 0.6552799940109253, 'validation/loss': 1.5104166269302368, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.222768783569336, 'test/num_examples': 10000, 'score': 7701.3723220825195, 'total_duration': 8029.461374044418, 'accumulated_submission_time': 7701.3723220825195, 'accumulated_eval_time': 310.38906264305115, 'accumulated_logging_time': 17.431657314300537}
I0420 02:26:53.518186 140238861084416 logging_writer.py:48] [22613] accumulated_eval_time=310.389063, accumulated_logging_time=17.431657, accumulated_submission_time=7701.372322, global_step=22613, preemption_count=0, score=7701.372322, test/accuracy=0.523200, test/loss=2.222769, test/num_examples=10000, total_duration=8029.461374, train/accuracy=0.742945, train/loss=1.103400, validation/accuracy=0.655280, validation/loss=1.510417, validation/num_examples=50000
I0420 02:26:53.790081 140415007438656 checkpoints.py:356] Saving checkpoint at step: 22613
I0420 02:26:54.746766 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_22613
I0420 02:26:54.763842 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_22613.
I0420 02:27:24.389616 140238869477120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.4157077968120575, loss=2.513213634490967
I0420 02:27:58.068831 140238844299008 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.45827916264533997, loss=2.527750253677368
I0420 02:28:31.803574 140238869477120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.43941134214401245, loss=2.4954581260681152
I0420 02:29:05.589065 140238844299008 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4429647922515869, loss=2.3763303756713867
I0420 02:29:39.362070 140238869477120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4075813293457031, loss=2.3845551013946533
I0420 02:30:13.139723 140238844299008 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.455317884683609, loss=2.4778037071228027
I0420 02:30:46.988814 140238869477120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.4020289182662964, loss=2.3720130920410156
I0420 02:31:20.836734 140238844299008 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4441987872123718, loss=2.388798475265503
I0420 02:31:54.669838 140238869477120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4032113254070282, loss=2.418816328048706
I0420 02:32:28.453987 140238844299008 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.42432549595832825, loss=2.4427430629730225
I0420 02:33:02.199903 140238869477120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.40489646792411804, loss=2.3937299251556396
I0420 02:33:35.915101 140238844299008 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.46369853615760803, loss=2.4755117893218994
I0420 02:34:09.710663 140238869477120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4117063879966736, loss=2.3833439350128174
I0420 02:34:43.485720 140238844299008 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.4142581522464752, loss=2.521991491317749
I0420 02:35:17.274923 140238869477120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.44958481192588806, loss=2.537393093109131
I0420 02:35:25.101812 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:35:33.243542 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:35:43.390493 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:35:45.516702 140415007438656 submission_runner.py:406] Time since start: 8561.47s, 	Step: 24125, 	{'train/accuracy': 0.7496213316917419, 'train/loss': 1.0824013948440552, 'validation/accuracy': 0.658519983291626, 'validation/loss': 1.4942312240600586, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.2027244567871094, 'test/num_examples': 10000, 'score': 8211.68372130394, 'total_duration': 8561.469876766205, 'accumulated_submission_time': 8211.68372130394, 'accumulated_eval_time': 330.80278491973877, 'accumulated_logging_time': 18.696420669555664}
I0420 02:35:45.531252 140238844299008 logging_writer.py:48] [24125] accumulated_eval_time=330.802785, accumulated_logging_time=18.696421, accumulated_submission_time=8211.683721, global_step=24125, preemption_count=0, score=8211.683721, test/accuracy=0.530800, test/loss=2.202724, test/num_examples=10000, total_duration=8561.469877, train/accuracy=0.749621, train/loss=1.082401, validation/accuracy=0.658520, validation/loss=1.494231, validation/num_examples=50000
I0420 02:35:45.810484 140415007438656 checkpoints.py:356] Saving checkpoint at step: 24125
I0420 02:35:46.785610 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_24125
I0420 02:35:46.804848 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_24125.
I0420 02:36:12.432491 140238869477120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.40943729877471924, loss=2.2988295555114746
I0420 02:36:46.087529 140238835906304 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3990863859653473, loss=2.5137746334075928
I0420 02:37:19.836191 140238869477120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4404146075248718, loss=2.4015085697174072
I0420 02:37:53.844063 140238835906304 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4245307147502899, loss=2.4117143154144287
I0420 02:38:27.640074 140238869477120 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4281821548938751, loss=2.3729586601257324
I0420 02:39:01.233785 140238835906304 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.42967870831489563, loss=2.356801986694336
I0420 02:39:35.180816 140238869477120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.43542519211769104, loss=2.3516321182250977
I0420 02:40:08.959861 140238835906304 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.501954197883606, loss=2.4646079540252686
I0420 02:40:42.604253 140238869477120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.45694538950920105, loss=2.4113481044769287
I0420 02:41:16.522402 140238835906304 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.4278188645839691, loss=2.46152663230896
I0420 02:41:50.426538 140238869477120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4286384582519531, loss=2.4643585681915283
I0420 02:42:24.161177 140238835906304 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3913419842720032, loss=2.387521266937256
I0420 02:42:57.991342 140238869477120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3854946494102478, loss=2.302941083908081
I0420 02:43:31.803968 140238835906304 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4327515959739685, loss=2.3994059562683105
I0420 02:44:05.508277 140238869477120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4332551062107086, loss=2.3916680812835693
I0420 02:44:17.058380 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:44:24.864157 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:44:34.798779 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:44:36.922871 140415007438656 submission_runner.py:406] Time since start: 9092.88s, 	Step: 25636, 	{'train/accuracy': 0.760164201259613, 'train/loss': 1.0526974201202393, 'validation/accuracy': 0.6687399744987488, 'validation/loss': 1.4602266550064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.1688432693481445, 'test/num_examples': 10000, 'score': 8721.910294532776, 'total_duration': 9092.876178503036, 'accumulated_submission_time': 8721.910294532776, 'accumulated_eval_time': 350.6662278175354, 'accumulated_logging_time': 19.994673013687134}
I0420 02:44:36.931842 140238835906304 logging_writer.py:48] [25636] accumulated_eval_time=350.666228, accumulated_logging_time=19.994673, accumulated_submission_time=8721.910295, global_step=25636, preemption_count=0, score=8721.910295, test/accuracy=0.537200, test/loss=2.168843, test/num_examples=10000, total_duration=9092.876179, train/accuracy=0.760164, train/loss=1.052697, validation/accuracy=0.668740, validation/loss=1.460227, validation/num_examples=50000
I0420 02:44:37.184377 140415007438656 checkpoints.py:356] Saving checkpoint at step: 25636
I0420 02:44:38.171637 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_25636
I0420 02:44:38.189790 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_25636.
I0420 02:45:00.294566 140238869477120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3961212635040283, loss=2.335620403289795
I0420 02:45:34.292102 140238827513600 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4265296757221222, loss=2.415879726409912
I0420 02:46:08.128767 140238869477120 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.4000788927078247, loss=2.4435853958129883
I0420 02:46:42.074308 140238827513600 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.43027204275131226, loss=2.4786839485168457
I0420 02:47:15.809551 140238869477120 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.4205261170864105, loss=2.4338324069976807
I0420 02:47:49.838377 140238827513600 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4108014404773712, loss=2.3984715938568115
I0420 02:48:23.813664 140238869477120 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.43830350041389465, loss=2.4408257007598877
I0420 02:48:57.846818 140238827513600 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4085022509098053, loss=2.3726966381073
I0420 02:49:31.496025 140238869477120 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.39253100752830505, loss=2.4056968688964844
I0420 02:50:05.354510 140238827513600 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.42458951473236084, loss=2.423800468444824
I0420 02:50:39.290155 140238869477120 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.38442888855934143, loss=2.401350975036621
I0420 02:51:13.202961 140238827513600 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4246622622013092, loss=2.354487895965576
I0420 02:51:47.174720 140238869477120 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4358009994029999, loss=2.383333921432495
I0420 02:52:20.926635 140238827513600 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.40542465448379517, loss=2.2879879474639893
I0420 02:52:54.953754 140238869477120 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4333554804325104, loss=2.364384889602661
I0420 02:53:08.217587 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:53:16.740903 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:53:26.755353 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:53:28.858835 140415007438656 submission_runner.py:406] Time since start: 9624.81s, 	Step: 27141, 	{'train/accuracy': 0.7572146058082581, 'train/loss': 1.0470035076141357, 'validation/accuracy': 0.6620599627494812, 'validation/loss': 1.470693588256836, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.1952178478240967, 'test/num_examples': 10000, 'score': 9231.911622285843, 'total_duration': 9624.812209367752, 'accumulated_submission_time': 9231.911622285843, 'accumulated_eval_time': 371.3064978122711, 'accumulated_logging_time': 21.27103304862976}
I0420 02:53:28.874618 140238827513600 logging_writer.py:48] [27141] accumulated_eval_time=371.306498, accumulated_logging_time=21.271033, accumulated_submission_time=9231.911622, global_step=27141, preemption_count=0, score=9231.911622, test/accuracy=0.528700, test/loss=2.195218, test/num_examples=10000, total_duration=9624.812209, train/accuracy=0.757215, train/loss=1.047004, validation/accuracy=0.662060, validation/loss=1.470694, validation/num_examples=50000
I0420 02:53:29.143024 140415007438656 checkpoints.py:356] Saving checkpoint at step: 27141
I0420 02:53:30.137056 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_27141
I0420 02:53:30.155977 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_27141.
I0420 02:53:50.427680 140238869477120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.44051602482795715, loss=2.470431089401245
I0420 02:54:24.211203 140238819120896 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.39330315589904785, loss=2.256148338317871
I0420 02:54:58.109503 140238869477120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.42872729897499084, loss=2.451258659362793
I0420 02:55:31.801001 140238819120896 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.42537975311279297, loss=2.29494571685791
I0420 02:56:05.589226 140238869477120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.41911616921424866, loss=2.3285839557647705
I0420 02:56:39.222308 140238819120896 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3966514468193054, loss=2.3815975189208984
I0420 02:57:12.986925 140238869477120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.42268049716949463, loss=2.411768913269043
I0420 02:57:46.960397 140238819120896 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.4180894196033478, loss=2.307793140411377
I0420 02:58:20.085908 140415007438656 spec.py:298] Evaluating on the training split.
I0420 02:58:27.911779 140415007438656 spec.py:310] Evaluating on the validation split.
I0420 02:58:37.934891 140415007438656 spec.py:326] Evaluating on the test split.
I0420 02:58:40.060200 140415007438656 submission_runner.py:406] Time since start: 9936.01s, 	Step: 28000, 	{'train/accuracy': 0.7688735723495483, 'train/loss': 1.0224099159240723, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.4724980592727661, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.1434929370880127, 'test/num_examples': 10000, 'score': 9521.82117509842, 'total_duration': 9936.013282299042, 'accumulated_submission_time': 9521.82117509842, 'accumulated_eval_time': 391.2795398235321, 'accumulated_logging_time': 22.57839846611023}
I0420 02:58:40.070347 140238869477120 logging_writer.py:48] [28000] accumulated_eval_time=391.279540, accumulated_logging_time=22.578398, accumulated_submission_time=9521.821175, global_step=28000, preemption_count=0, score=9521.821175, test/accuracy=0.539300, test/loss=2.143493, test/num_examples=10000, total_duration=9936.013282, train/accuracy=0.768874, train/loss=1.022410, validation/accuracy=0.666960, validation/loss=1.472498, validation/num_examples=50000
I0420 02:58:40.328504 140415007438656 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 02:58:41.302107 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0420 02:58:41.319804 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0420 02:58:41.338937 140238819120896 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=9521.821175
I0420 02:58:41.511516 140415007438656 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 02:58:42.778254 140415007438656 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000
I0420 02:58:42.794204 140415007438656 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0420 02:58:43.172845 140415007438656 submission_runner.py:567] Tuning trial 1/1
I0420 02:58:43.173059 140415007438656 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 02:58:43.188746 140415007438656 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009367027669213712, 'train/loss': 6.9118804931640625, 'validation/accuracy': 0.0011599999852478504, 'validation/loss': 6.911995887756348, 'validation/num_examples': 50000, 'test/accuracy': 0.0017000001389533281, 'test/loss': 6.911821365356445, 'test/num_examples': 10000, 'score': 48.78495240211487, 'total_duration': 88.76513004302979, 'accumulated_submission_time': 48.78495240211487, 'accumulated_eval_time': 39.98003005981445, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1504, {'train/accuracy': 0.12454161047935486, 'train/loss': 4.71127986907959, 'validation/accuracy': 0.1078999936580658, 'validation/loss': 4.840988636016846, 'validation/num_examples': 50000, 'test/accuracy': 0.08260000497102737, 'test/loss': 5.176387786865234, 'test/num_examples': 10000, 'score': 558.9548420906067, 'total_duration': 616.2360188961029, 'accumulated_submission_time': 558.9548420906067, 'accumulated_eval_time': 56.43934869766235, 'accumulated_logging_time': 0.8240387439727783, 'global_step': 1504, 'preemption_count': 0}), (3010, {'train/accuracy': 0.24770806729793549, 'train/loss': 3.6940927505493164, 'validation/accuracy': 0.2250799983739853, 'validation/loss': 3.859097480773926, 'validation/num_examples': 50000, 'test/accuracy': 0.17020000517368317, 'test/loss': 4.3999528884887695, 'test/num_examples': 10000, 'score': 1069.2006242275238, 'total_duration': 1143.5250790119171, 'accumulated_submission_time': 1069.2006242275238, 'accumulated_eval_time': 72.66259002685547, 'accumulated_logging_time': 1.625957727432251, 'global_step': 3010, 'preemption_count': 0}), (4512, {'train/accuracy': 0.35509008169174194, 'train/loss': 3.0180928707122803, 'validation/accuracy': 0.32729998230934143, 'validation/loss': 3.1676151752471924, 'validation/num_examples': 50000, 'test/accuracy': 0.2345000058412552, 'test/loss': 3.84676194190979, 'test/num_examples': 10000, 'score': 1579.5160419940948, 'total_duration': 1670.880448102951, 'accumulated_submission_time': 1579.5160419940948, 'accumulated_eval_time': 88.87712860107422, 'accumulated_logging_time': 2.4337689876556396, 'global_step': 4512, 'preemption_count': 0}), (6016, {'train/accuracy': 0.47757890820503235, 'train/loss': 2.3753726482391357, 'validation/accuracy': 0.4452599883079529, 'validation/loss': 2.528930187225342, 'validation/num_examples': 50000, 'test/accuracy': 0.3375000059604645, 'test/loss': 3.220114231109619, 'test/num_examples': 10000, 'score': 2089.560332775116, 'total_duration': 2198.046154499054, 'accumulated_submission_time': 2089.560332775116, 'accumulated_eval_time': 105.18708562850952, 'accumulated_logging_time': 3.228041887283325, 'global_step': 6016, 'preemption_count': 0}), (7523, {'train/accuracy': 0.5130141973495483, 'train/loss': 2.2139453887939453, 'validation/accuracy': 0.47567999362945557, 'validation/loss': 2.39107084274292, 'validation/num_examples': 50000, 'test/accuracy': 0.3710000216960907, 'test/loss': 3.06379771232605, 'test/num_examples': 10000, 'score': 2599.728401660919, 'total_duration': 2725.6601889133453, 'accumulated_submission_time': 2599.728401660919, 'accumulated_eval_time': 121.57873201370239, 'accumulated_logging_time': 4.264488220214844, 'global_step': 7523, 'preemption_count': 0}), (9030, {'train/accuracy': 0.5517578125, 'train/loss': 1.9773160219192505, 'validation/accuracy': 0.5109800100326538, 'validation/loss': 2.182656764984131, 'validation/num_examples': 50000, 'test/accuracy': 0.3954000174999237, 'test/loss': 2.8749191761016846, 'test/num_examples': 10000, 'score': 3109.7510163784027, 'total_duration': 3255.1380252838135, 'accumulated_submission_time': 3109.7510163784027, 'accumulated_eval_time': 139.84110975265503, 'accumulated_logging_time': 5.440221786499023, 'global_step': 9030, 'preemption_count': 0}), (10537, {'train/accuracy': 0.6278300285339355, 'train/loss': 1.666878342628479, 'validation/accuracy': 0.5579400062561035, 'validation/loss': 2.0028305053710938, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.7190849781036377, 'test/num_examples': 10000, 'score': 3619.8183178901672, 'total_duration': 3782.9762799739838, 'accumulated_submission_time': 3619.8183178901672, 'accumulated_eval_time': 156.744238615036, 'accumulated_logging_time': 6.291198968887329, 'global_step': 10537, 'preemption_count': 0}), (12045, {'train/accuracy': 0.652363657951355, 'train/loss': 1.5056946277618408, 'validation/accuracy': 0.5895400047302246, 'validation/loss': 1.8114515542984009, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.527686357498169, 'test/num_examples': 10000, 'score': 4130.079380512238, 'total_duration': 4312.682667970657, 'accumulated_submission_time': 4130.079380512238, 'accumulated_eval_time': 174.97196292877197, 'accumulated_logging_time': 7.491811990737915, 'global_step': 12045, 'preemption_count': 0}), (13554, {'train/accuracy': 0.6536591053009033, 'train/loss': 1.5391321182250977, 'validation/accuracy': 0.5922200083732605, 'validation/loss': 1.8361238241195679, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.565978765487671, 'test/num_examples': 10000, 'score': 4640.149284601212, 'total_duration': 4843.368032217026, 'accumulated_submission_time': 4640.149284601212, 'accumulated_eval_time': 194.12093544006348, 'accumulated_logging_time': 8.941060304641724, 'global_step': 13554, 'preemption_count': 0}), (15065, {'train/accuracy': 0.6876793503761292, 'train/loss': 1.3867483139038086, 'validation/accuracy': 0.6176999807357788, 'validation/loss': 1.7002989053726196, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.4263758659362793, 'test/num_examples': 10000, 'score': 5150.239653348923, 'total_duration': 5373.070691823959, 'accumulated_submission_time': 5150.239653348923, 'accumulated_eval_time': 212.52098274230957, 'accumulated_logging_time': 10.134826183319092, 'global_step': 15065, 'preemption_count': 0}), (16576, {'train/accuracy': 0.6853076815605164, 'train/loss': 1.3572250604629517, 'validation/accuracy': 0.6198199987411499, 'validation/loss': 1.6744951009750366, 'validation/num_examples': 50000, 'test/accuracy': 0.4815000295639038, 'test/loss': 2.395092248916626, 'test/num_examples': 10000, 'score': 5660.513456821442, 'total_duration': 5903.991253376007, 'accumulated_submission_time': 5660.513456821442, 'accumulated_eval_time': 231.5731828212738, 'accumulated_logging_time': 11.710589408874512, 'global_step': 16576, 'preemption_count': 0}), (18087, {'train/accuracy': 0.7104990482330322, 'train/loss': 1.269576907157898, 'validation/accuracy': 0.6380199790000916, 'validation/loss': 1.5999759435653687, 'validation/num_examples': 50000, 'test/accuracy': 0.5107000470161438, 'test/loss': 2.2953944206237793, 'test/num_examples': 10000, 'score': 6170.783210515976, 'total_duration': 6434.984543800354, 'accumulated_submission_time': 6170.783210515976, 'accumulated_eval_time': 251.0319426059723, 'accumulated_logging_time': 12.95676064491272, 'global_step': 18087, 'preemption_count': 0}), (19593, {'train/accuracy': 0.753348171710968, 'train/loss': 1.0889217853546143, 'validation/accuracy': 0.6452199816703796, 'validation/loss': 1.5629624128341675, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.239403486251831, 'test/num_examples': 10000, 'score': 6681.065675020218, 'total_duration': 6967.042475223541, 'accumulated_submission_time': 6681.065675020218, 'accumulated_eval_time': 271.1728296279907, 'accumulated_logging_time': 14.572669506072998, 'global_step': 19593, 'preemption_count': 0}), (21103, {'train/accuracy': 0.7302495241165161, 'train/loss': 1.1815403699874878, 'validation/accuracy': 0.644279956817627, 'validation/loss': 1.587799310684204, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.2992470264434814, 'test/num_examples': 10000, 'score': 7191.211231708527, 'total_duration': 7498.172576904297, 'accumulated_submission_time': 7191.211231708527, 'accumulated_eval_time': 290.8852138519287, 'accumulated_logging_time': 15.826356649398804, 'global_step': 21103, 'preemption_count': 0}), (22613, {'train/accuracy': 0.7429448366165161, 'train/loss': 1.1033999919891357, 'validation/accuracy': 0.6552799940109253, 'validation/loss': 1.5104166269302368, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.222768783569336, 'test/num_examples': 10000, 'score': 7701.3723220825195, 'total_duration': 8029.461374044418, 'accumulated_submission_time': 7701.3723220825195, 'accumulated_eval_time': 310.38906264305115, 'accumulated_logging_time': 17.431657314300537, 'global_step': 22613, 'preemption_count': 0}), (24125, {'train/accuracy': 0.7496213316917419, 'train/loss': 1.0824013948440552, 'validation/accuracy': 0.658519983291626, 'validation/loss': 1.4942312240600586, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.2027244567871094, 'test/num_examples': 10000, 'score': 8211.68372130394, 'total_duration': 8561.469876766205, 'accumulated_submission_time': 8211.68372130394, 'accumulated_eval_time': 330.80278491973877, 'accumulated_logging_time': 18.696420669555664, 'global_step': 24125, 'preemption_count': 0}), (25636, {'train/accuracy': 0.760164201259613, 'train/loss': 1.0526974201202393, 'validation/accuracy': 0.6687399744987488, 'validation/loss': 1.4602266550064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.1688432693481445, 'test/num_examples': 10000, 'score': 8721.910294532776, 'total_duration': 9092.876178503036, 'accumulated_submission_time': 8721.910294532776, 'accumulated_eval_time': 350.6662278175354, 'accumulated_logging_time': 19.994673013687134, 'global_step': 25636, 'preemption_count': 0}), (27141, {'train/accuracy': 0.7572146058082581, 'train/loss': 1.0470035076141357, 'validation/accuracy': 0.6620599627494812, 'validation/loss': 1.470693588256836, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.1952178478240967, 'test/num_examples': 10000, 'score': 9231.911622285843, 'total_duration': 9624.812209367752, 'accumulated_submission_time': 9231.911622285843, 'accumulated_eval_time': 371.3064978122711, 'accumulated_logging_time': 21.27103304862976, 'global_step': 27141, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7688735723495483, 'train/loss': 1.0224099159240723, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.4724980592727661, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.1434929370880127, 'test/num_examples': 10000, 'score': 9521.82117509842, 'total_duration': 9936.013282299042, 'accumulated_submission_time': 9521.82117509842, 'accumulated_eval_time': 391.2795398235321, 'accumulated_logging_time': 22.57839846611023, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0420 02:58:43.188869 140415007438656 submission_runner.py:570] Timing: 9521.82117509842
I0420 02:58:43.188911 140415007438656 submission_runner.py:571] ====================
I0420 02:58:43.189029 140415007438656 submission_runner.py:631] Final imagenet_resnet score: 9521.82117509842
