I0420 06:56:34.466427 140687085266752 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax.
I0420 06:56:34.535119 140687085266752 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 06:56:35.420552 140687085266752 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0420 06:56:35.421219 140687085266752 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 06:56:35.424643 140687085266752 submission_runner.py:528] Using RNG seed 772368111
I0420 06:56:38.036174 140687085266752 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 06:56:38.036391 140687085266752 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1.
I0420 06:56:38.036671 140687085266752 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/hparams.json.
I0420 06:56:38.157421 140687085266752 submission_runner.py:232] Initializing dataset.
I0420 06:56:38.388730 140687085266752 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 06:56:38.393930 140687085266752 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 06:56:38.616015 140687085266752 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 06:56:38.670543 140687085266752 submission_runner.py:239] Initializing model.
I0420 06:56:45.746186 140687085266752 submission_runner.py:249] Initializing optimizer.
I0420 06:56:46.130295 140687085266752 submission_runner.py:256] Initializing metrics bundle.
I0420 06:56:46.130480 140687085266752 submission_runner.py:273] Initializing checkpoint and logger.
I0420 06:56:46.131505 140687085266752 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1 with prefix checkpoint_
I0420 06:56:46.131797 140687085266752 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 06:56:46.131892 140687085266752 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 06:56:46.986543 140687085266752 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/meta_data_0.json.
I0420 06:56:46.987745 140687085266752 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/flags_0.json.
I0420 06:56:46.993968 140687085266752 submission_runner.py:309] Starting training loop.
I0420 06:57:07.654315 140510819772160 logging_writer.py:48] [0] global_step=0, grad_norm=2.803802251815796, loss=0.7441614866256714
I0420 06:57:07.667523 140687085266752 spec.py:298] Evaluating on the training split.
I0420 06:57:07.675408 140687085266752 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 06:57:07.679665 140687085266752 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 06:57:07.735420 140687085266752 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0420 06:57:23.569398 140687085266752 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0420 06:58:36.604851 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 06:58:36.607736 140687085266752 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 06:58:36.611621 140687085266752 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 06:58:36.664358 140687085266752 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 06:59:39.611352 140687085266752 spec.py:326] Evaluating on the test split.
I0420 06:59:39.614267 140687085266752 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 06:59:39.618089 140687085266752 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 06:59:39.670076 140687085266752 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:00:43.875069 140687085266752 submission_runner.py:406] Time since start: 236.88s, 	Step: 1, 	{'train/accuracy': 0.5110936164855957, 'train/loss': 0.7456163167953491, 'train/mean_average_precision': 0.02329463101405079, 'validation/accuracy': 0.5076742768287659, 'validation/loss': 0.7476675510406494, 'validation/mean_average_precision': 0.026392045786451514, 'validation/num_examples': 43793, 'test/accuracy': 0.5058948397636414, 'test/loss': 0.7486602067947388, 'test/mean_average_precision': 0.02873946711436334, 'test/num_examples': 43793, 'score': 20.673376083374023, 'total_duration': 236.88105463981628, 'accumulated_submission_time': 20.673376083374023, 'accumulated_eval_time': 216.20752501487732, 'accumulated_logging_time': 0}
I0420 07:00:43.892512 140500962703104 logging_writer.py:48] [1] accumulated_eval_time=216.207525, accumulated_logging_time=0, accumulated_submission_time=20.673376, global_step=1, preemption_count=0, score=20.673376, test/accuracy=0.505895, test/loss=0.748660, test/mean_average_precision=0.028739, test/num_examples=43793, total_duration=236.881055, train/accuracy=0.511094, train/loss=0.745616, train/mean_average_precision=0.023295, validation/accuracy=0.507674, validation/loss=0.747668, validation/mean_average_precision=0.026392, validation/num_examples=43793
I0420 07:00:43.925759 140687085266752 checkpoints.py:356] Saving checkpoint at step: 1
I0420 07:00:44.022207 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_1
I0420 07:00:44.022426 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_1.
I0420 07:01:07.916747 140500971095808 logging_writer.py:48] [100] global_step=100, grad_norm=0.3928331434726715, loss=0.3574771583080292
I0420 07:01:31.508620 140502095619840 logging_writer.py:48] [200] global_step=200, grad_norm=0.21608057618141174, loss=0.19875124096870422
I0420 07:01:54.910683 140500971095808 logging_writer.py:48] [300] global_step=300, grad_norm=0.09565934538841248, loss=0.10959288477897644
I0420 07:02:18.381571 140502095619840 logging_writer.py:48] [400] global_step=400, grad_norm=0.049782197922468185, loss=0.07637342810630798
I0420 07:02:42.198884 140500971095808 logging_writer.py:48] [500] global_step=500, grad_norm=0.13546310365200043, loss=0.05749893933534622
I0420 07:03:05.966471 140502095619840 logging_writer.py:48] [600] global_step=600, grad_norm=0.13222987949848175, loss=0.058674003928899765
I0420 07:03:29.336409 140500971095808 logging_writer.py:48] [700] global_step=700, grad_norm=0.05302644148468971, loss=0.05936258286237717
I0420 07:03:52.972047 140502095619840 logging_writer.py:48] [800] global_step=800, grad_norm=0.15177345275878906, loss=0.05482695996761322
I0420 07:04:16.320766 140500971095808 logging_writer.py:48] [900] global_step=900, grad_norm=0.03381529077887535, loss=0.05289338529109955
I0420 07:04:39.662270 140502095619840 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.13742493093013763, loss=0.051197025924921036
I0420 07:04:44.043281 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:05:57.349699 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:05:59.924001 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:06:02.516130 140687085266752 submission_runner.py:406] Time since start: 555.52s, 	Step: 1020, 	{'train/accuracy': 0.9869014620780945, 'train/loss': 0.0484786219894886, 'train/mean_average_precision': 0.08814932935958919, 'validation/accuracy': 0.9844004511833191, 'validation/loss': 0.057836636900901794, 'validation/mean_average_precision': 0.09118839840818964, 'validation/num_examples': 43793, 'test/accuracy': 0.9834137558937073, 'test/loss': 0.0612342432141304, 'test/mean_average_precision': 0.09118883971207395, 'test/num_examples': 43793, 'score': 260.6853218078613, 'total_duration': 555.5221028327942, 'accumulated_submission_time': 260.6853218078613, 'accumulated_eval_time': 294.6803369522095, 'accumulated_logging_time': 0.14761757850646973}
I0420 07:06:02.524744 140500971095808 logging_writer.py:48] [1020] accumulated_eval_time=294.680337, accumulated_logging_time=0.147618, accumulated_submission_time=260.685322, global_step=1020, preemption_count=0, score=260.685322, test/accuracy=0.983414, test/loss=0.061234, test/mean_average_precision=0.091189, test/num_examples=43793, total_duration=555.522103, train/accuracy=0.986901, train/loss=0.048479, train/mean_average_precision=0.088149, validation/accuracy=0.984400, validation/loss=0.057837, validation/mean_average_precision=0.091188, validation/num_examples=43793
I0420 07:06:02.556464 140687085266752 checkpoints.py:356] Saving checkpoint at step: 1020
I0420 07:06:02.657557 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_1020
I0420 07:06:02.657797 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_1020.
I0420 07:06:21.589296 140502095619840 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.09917629510164261, loss=0.045761995017528534
I0420 07:06:45.125250 140502070441728 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.05576569214463234, loss=0.04565230756998062
I0420 07:07:08.800244 140502095619840 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.07498854398727417, loss=0.04995759204030037
I0420 07:07:32.440045 140502070441728 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.056504178792238235, loss=0.048785094171762466
I0420 07:07:56.183020 140502095619840 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.10300660133361816, loss=0.04599134251475334
I0420 07:08:19.894036 140502070441728 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.044170185923576355, loss=0.04416702315211296
I0420 07:08:43.705060 140502095619840 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.040144577622413635, loss=0.04496154934167862
I0420 07:09:07.501286 140502070441728 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.052198030054569244, loss=0.04439195990562439
I0420 07:09:31.448847 140502095619840 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.029755761846899986, loss=0.03841862455010414
I0420 07:09:55.278592 140502070441728 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.065300852060318, loss=0.047360748052597046
I0420 07:10:02.885164 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:11:17.287175 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:11:19.820700 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:11:22.282688 140687085266752 submission_runner.py:406] Time since start: 875.29s, 	Step: 2033, 	{'train/accuracy': 0.9880001544952393, 'train/loss': 0.04317844286561012, 'train/mean_average_precision': 0.14953955315804734, 'validation/accuracy': 0.985159158706665, 'validation/loss': 0.05183469504117966, 'validation/mean_average_precision': 0.1512491336380951, 'validation/num_examples': 43793, 'test/accuracy': 0.9842498302459717, 'test/loss': 0.054563283920288086, 'test/mean_average_precision': 0.1508685760919853, 'test/num_examples': 43793, 'score': 500.903933763504, 'total_duration': 875.2886598110199, 'accumulated_submission_time': 500.903933763504, 'accumulated_eval_time': 374.07782793045044, 'accumulated_logging_time': 0.2896759510040283}
I0420 07:11:22.290632 140502095619840 logging_writer.py:48] [2033] accumulated_eval_time=374.077828, accumulated_logging_time=0.289676, accumulated_submission_time=500.903934, global_step=2033, preemption_count=0, score=500.903934, test/accuracy=0.984250, test/loss=0.054563, test/mean_average_precision=0.150869, test/num_examples=43793, total_duration=875.288660, train/accuracy=0.988000, train/loss=0.043178, train/mean_average_precision=0.149540, validation/accuracy=0.985159, validation/loss=0.051835, validation/mean_average_precision=0.151249, validation/num_examples=43793
I0420 07:11:22.324519 140687085266752 checkpoints.py:356] Saving checkpoint at step: 2033
I0420 07:11:22.424113 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_2033
I0420 07:11:22.424333 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_2033.
I0420 07:11:38.551310 140502070441728 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.026998667046427727, loss=0.04462118074297905
I0420 07:12:02.409569 140502045263616 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.04818716645240784, loss=0.04409756138920784
I0420 07:12:26.144726 140502070441728 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0463699996471405, loss=0.04492240399122238
I0420 07:12:49.972176 140502045263616 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0426814928650856, loss=0.0460033044219017
I0420 07:13:13.879189 140502070441728 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.04364226385951042, loss=0.04447613283991814
I0420 07:13:37.459288 140502045263616 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.06330553442239761, loss=0.04292646795511246
I0420 07:14:01.182398 140502070441728 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.04038415476679802, loss=0.046681616455316544
I0420 07:14:24.829755 140502045263616 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.05058647692203522, loss=0.04008853808045387
I0420 07:14:48.384489 140502070441728 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.03788551315665245, loss=0.044025857001543045
I0420 07:15:11.926020 140502045263616 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.095403291285038, loss=0.042730264365673065
I0420 07:15:22.596368 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:16:36.556014 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:16:39.055558 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:16:41.491750 140687085266752 submission_runner.py:406] Time since start: 1194.50s, 	Step: 3046, 	{'train/accuracy': 0.9885160326957703, 'train/loss': 0.039860352873802185, 'train/mean_average_precision': 0.20810971782476279, 'validation/accuracy': 0.9855350852012634, 'validation/loss': 0.049451544880867004, 'validation/mean_average_precision': 0.17858840640900395, 'validation/num_examples': 43793, 'test/accuracy': 0.9846373200416565, 'test/loss': 0.052019547671079636, 'test/mean_average_precision': 0.17776589655271421, 'test/num_examples': 43793, 'score': 741.0670647621155, 'total_duration': 1194.4977238178253, 'accumulated_submission_time': 741.0670647621155, 'accumulated_eval_time': 452.97317481040955, 'accumulated_logging_time': 0.431643009185791}
I0420 07:16:41.500387 140502070441728 logging_writer.py:48] [3046] accumulated_eval_time=452.973175, accumulated_logging_time=0.431643, accumulated_submission_time=741.067065, global_step=3046, preemption_count=0, score=741.067065, test/accuracy=0.984637, test/loss=0.052020, test/mean_average_precision=0.177766, test/num_examples=43793, total_duration=1194.497724, train/accuracy=0.988516, train/loss=0.039860, train/mean_average_precision=0.208110, validation/accuracy=0.985535, validation/loss=0.049452, validation/mean_average_precision=0.178588, validation/num_examples=43793
I0420 07:16:41.534058 140687085266752 checkpoints.py:356] Saving checkpoint at step: 3046
I0420 07:16:41.624136 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_3046
I0420 07:16:41.624720 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_3046.
I0420 07:16:54.552589 140502045263616 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.052475254982709885, loss=0.04340155050158501
I0420 07:17:18.008423 140502036870912 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.03146267682313919, loss=0.04201612249016762
I0420 07:17:41.344534 140502045263616 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.06366952508687973, loss=0.04544468969106674
I0420 07:18:04.807567 140502036870912 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.023286497220396996, loss=0.04038136079907417
I0420 07:18:28.412372 140502045263616 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.025028834119439125, loss=0.0445258654654026
I0420 07:18:52.236332 140502036870912 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.02157723158597946, loss=0.03848482668399811
I0420 07:19:15.881846 140502045263616 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.033839140087366104, loss=0.04094841331243515
I0420 07:19:39.241732 140502036870912 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.017580127343535423, loss=0.03777476027607918
I0420 07:20:02.734765 140502045263616 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.02352038025856018, loss=0.04087714105844498
I0420 07:20:26.212027 140502036870912 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.03763725608587265, loss=0.045788928866386414
I0420 07:20:41.642540 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:21:53.997222 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:21:56.715437 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:21:59.151973 140687085266752 submission_runner.py:406] Time since start: 1512.16s, 	Step: 4067, 	{'train/accuracy': 0.9887028336524963, 'train/loss': 0.03856830671429634, 'train/mean_average_precision': 0.2261006713681126, 'validation/accuracy': 0.9857088327407837, 'validation/loss': 0.04885789006948471, 'validation/mean_average_precision': 0.19490838290433732, 'validation/num_examples': 43793, 'test/accuracy': 0.984780490398407, 'test/loss': 0.051588308066129684, 'test/mean_average_precision': 0.19401721167353012, 'test/num_examples': 43793, 'score': 981.0759329795837, 'total_duration': 1512.1579344272614, 'accumulated_submission_time': 981.0759329795837, 'accumulated_eval_time': 530.4825625419617, 'accumulated_logging_time': 0.5650036334991455}
I0420 07:21:59.159572 140502045263616 logging_writer.py:48] [4067] accumulated_eval_time=530.482563, accumulated_logging_time=0.565004, accumulated_submission_time=981.075933, global_step=4067, preemption_count=0, score=981.075933, test/accuracy=0.984780, test/loss=0.051588, test/mean_average_precision=0.194017, test/num_examples=43793, total_duration=1512.157934, train/accuracy=0.988703, train/loss=0.038568, train/mean_average_precision=0.226101, validation/accuracy=0.985709, validation/loss=0.048858, validation/mean_average_precision=0.194908, validation/num_examples=43793
I0420 07:21:59.192297 140687085266752 checkpoints.py:356] Saving checkpoint at step: 4067
I0420 07:21:59.285574 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_4067
I0420 07:21:59.286227 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_4067.
I0420 07:22:07.458915 140502036870912 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.014443785883486271, loss=0.04051624611020088
I0420 07:22:30.853048 140502028478208 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.016721663996577263, loss=0.03970872238278389
I0420 07:22:54.301189 140502036870912 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.031760524958372116, loss=0.044786401093006134
I0420 07:23:17.794061 140502028478208 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.023547088727355003, loss=0.040841083973646164
I0420 07:23:41.285683 140502036870912 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.01806042715907097, loss=0.04011170193552971
I0420 07:24:04.971823 140502028478208 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01536477729678154, loss=0.04172607883810997
I0420 07:24:28.442801 140502036870912 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.023804564028978348, loss=0.044710297137498856
I0420 07:24:51.797085 140502028478208 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01504143513739109, loss=0.03832981735467911
I0420 07:25:15.214383 140502036870912 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.015916362404823303, loss=0.04109054058790207
I0420 07:25:38.691295 140502028478208 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.015934020280838013, loss=0.04129210487008095
I0420 07:25:59.380650 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:27:11.723304 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:27:14.221738 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:27:16.669352 140687085266752 submission_runner.py:406] Time since start: 1829.68s, 	Step: 5090, 	{'train/accuracy': 0.9889498949050903, 'train/loss': 0.03742704540491104, 'train/mean_average_precision': 0.26010676276318845, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.04755784943699837, 'validation/mean_average_precision': 0.20434966439567903, 'validation/num_examples': 43793, 'test/accuracy': 0.9851305484771729, 'test/loss': 0.05013447627425194, 'test/mean_average_precision': 0.20815465779794443, 'test/num_examples': 43793, 'score': 1221.1616275310516, 'total_duration': 1829.6753106117249, 'accumulated_submission_time': 1221.1616275310516, 'accumulated_eval_time': 607.7712187767029, 'accumulated_logging_time': 0.6997482776641846}
I0420 07:27:16.677335 140502036870912 logging_writer.py:48] [5090] accumulated_eval_time=607.771219, accumulated_logging_time=0.699748, accumulated_submission_time=1221.161628, global_step=5090, preemption_count=0, score=1221.161628, test/accuracy=0.985131, test/loss=0.050134, test/mean_average_precision=0.208155, test/num_examples=43793, total_duration=1829.675311, train/accuracy=0.988950, train/loss=0.037427, train/mean_average_precision=0.260107, validation/accuracy=0.986042, validation/loss=0.047558, validation/mean_average_precision=0.204350, validation/num_examples=43793
I0420 07:27:16.709550 140687085266752 checkpoints.py:356] Saving checkpoint at step: 5090
I0420 07:27:16.807469 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_5090
I0420 07:27:16.807680 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_5090.
I0420 07:27:19.395879 140502028478208 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.01298413798213005, loss=0.04153384268283844
I0420 07:27:42.610844 140502020085504 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.013915013521909714, loss=0.03837994113564491
I0420 07:28:06.014058 140502028478208 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.02666587010025978, loss=0.042718660086393356
I0420 07:28:29.192606 140502020085504 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.019587550312280655, loss=0.04369540885090828
I0420 07:28:52.516677 140502028478208 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013944373466074467, loss=0.04062202572822571
I0420 07:29:15.827133 140502020085504 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.01347433216869831, loss=0.03734458237886429
I0420 07:29:39.070506 140502028478208 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0241591464728117, loss=0.03777674213051796
I0420 07:30:02.197441 140502020085504 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.012184406630694866, loss=0.03744348883628845
I0420 07:30:25.299395 140502028478208 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.012717636302113533, loss=0.03580323979258537
I0420 07:30:48.496334 140502020085504 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.027897311374545097, loss=0.0425168015062809
I0420 07:31:11.945789 140502028478208 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.014351004734635353, loss=0.041773684322834015
I0420 07:31:16.861476 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:32:29.311399 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:32:31.788371 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:32:34.238063 140687085266752 submission_runner.py:406] Time since start: 2147.24s, 	Step: 6122, 	{'train/accuracy': 0.9890832901000977, 'train/loss': 0.03685196489095688, 'train/mean_average_precision': 0.26947347958626616, 'validation/accuracy': 0.9861037731170654, 'validation/loss': 0.04679630696773529, 'validation/mean_average_precision': 0.21783348200408964, 'validation/num_examples': 43793, 'test/accuracy': 0.9851772785186768, 'test/loss': 0.049345072358846664, 'test/mean_average_precision': 0.22344394047604246, 'test/num_examples': 43793, 'score': 1461.2066218852997, 'total_duration': 2147.244038581848, 'accumulated_submission_time': 1461.2066218852997, 'accumulated_eval_time': 685.1477692127228, 'accumulated_logging_time': 0.8384785652160645}
I0420 07:32:34.246815 140502020085504 logging_writer.py:48] [6122] accumulated_eval_time=685.147769, accumulated_logging_time=0.838479, accumulated_submission_time=1461.206622, global_step=6122, preemption_count=0, score=1461.206622, test/accuracy=0.985177, test/loss=0.049345, test/mean_average_precision=0.223444, test/num_examples=43793, total_duration=2147.244039, train/accuracy=0.989083, train/loss=0.036852, train/mean_average_precision=0.269473, validation/accuracy=0.986104, validation/loss=0.046796, validation/mean_average_precision=0.217833, validation/num_examples=43793
I0420 07:32:34.280271 140687085266752 checkpoints.py:356] Saving checkpoint at step: 6122
I0420 07:32:34.383030 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_6122
I0420 07:32:34.383249 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_6122.
I0420 07:32:52.787129 140502028478208 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.016618095338344574, loss=0.039015427231788635
I0420 07:33:16.309358 140502011692800 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.012930331751704216, loss=0.03902510553598404
I0420 07:33:39.805013 140502028478208 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.013141805306077003, loss=0.034430019557476044
I0420 07:34:03.252682 140502011692800 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.013677878305315971, loss=0.042055126279592514
I0420 07:34:26.450742 140502028478208 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.012658634223043919, loss=0.03763885796070099
I0420 07:34:49.690778 140502011692800 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.017771165817975998, loss=0.03666432946920395
I0420 07:35:13.053335 140502028478208 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.012029008008539677, loss=0.042382918298244476
I0420 07:35:36.432931 140502011692800 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.012035051360726357, loss=0.0376124307513237
I0420 07:35:59.999021 140502028478208 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.01738787814974785, loss=0.04216940328478813
I0420 07:36:23.415836 140502011692800 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.013772711157798767, loss=0.03727828711271286
I0420 07:36:34.511387 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:37:46.785813 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:37:49.270714 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:37:51.747244 140687085266752 submission_runner.py:406] Time since start: 2464.75s, 	Step: 7149, 	{'train/accuracy': 0.9897762537002563, 'train/loss': 0.03483475744724274, 'train/mean_average_precision': 0.30884145205816144, 'validation/accuracy': 0.9863566756248474, 'validation/loss': 0.04602137953042984, 'validation/mean_average_precision': 0.2298793378191102, 'validation/num_examples': 43793, 'test/accuracy': 0.9855256080627441, 'test/loss': 0.0483911894261837, 'test/mean_average_precision': 0.23905387998470412, 'test/num_examples': 43793, 'score': 1701.3257138729095, 'total_duration': 2464.7532160282135, 'accumulated_submission_time': 1701.3257138729095, 'accumulated_eval_time': 762.3835854530334, 'accumulated_logging_time': 0.9841015338897705}
I0420 07:37:51.755443 140502028478208 logging_writer.py:48] [7149] accumulated_eval_time=762.383585, accumulated_logging_time=0.984102, accumulated_submission_time=1701.325714, global_step=7149, preemption_count=0, score=1701.325714, test/accuracy=0.985526, test/loss=0.048391, test/mean_average_precision=0.239054, test/num_examples=43793, total_duration=2464.753216, train/accuracy=0.989776, train/loss=0.034835, train/mean_average_precision=0.308841, validation/accuracy=0.986357, validation/loss=0.046021, validation/mean_average_precision=0.229879, validation/num_examples=43793
I0420 07:37:51.787596 140687085266752 checkpoints.py:356] Saving checkpoint at step: 7149
I0420 07:37:51.882029 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_7149
I0420 07:37:51.882451 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_7149.
I0420 07:38:04.027969 140502011692800 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.013960321433842182, loss=0.03810293599963188
I0420 07:38:27.123076 140502003300096 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.010136185213923454, loss=0.038108911365270615
I0420 07:38:50.145289 140502011692800 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.011847354471683502, loss=0.03894280269742012
I0420 07:39:13.107373 140502003300096 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.010973386466503143, loss=0.036095425486564636
I0420 07:39:36.180516 140502011692800 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.013216506689786911, loss=0.03449012339115143
I0420 07:39:59.211374 140502003300096 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.018371328711509705, loss=0.0392545685172081
I0420 07:40:22.229152 140502011692800 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.01829805225133896, loss=0.03987657651305199
I0420 07:40:45.141087 140502003300096 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.01131858117878437, loss=0.037698205560445786
I0420 07:41:08.050495 140502011692800 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.011122564785182476, loss=0.038144417107105255
I0420 07:41:30.924548 140502003300096 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.01045418344438076, loss=0.035436179488897324
I0420 07:41:52.054913 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:43:03.666194 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:43:06.191786 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:43:08.618118 140687085266752 submission_runner.py:406] Time since start: 2781.62s, 	Step: 8192, 	{'train/accuracy': 0.990064799785614, 'train/loss': 0.0332697331905365, 'train/mean_average_precision': 0.342635779191877, 'validation/accuracy': 0.9864678978919983, 'validation/loss': 0.045229747891426086, 'validation/mean_average_precision': 0.2400200454297398, 'validation/num_examples': 43793, 'test/accuracy': 0.9856165647506714, 'test/loss': 0.04761410504579544, 'test/mean_average_precision': 0.23890747563342912, 'test/num_examples': 43793, 'score': 1941.4890439510345, 'total_duration': 2781.6240923404694, 'accumulated_submission_time': 1941.4890439510345, 'accumulated_eval_time': 838.9467575550079, 'accumulated_logging_time': 1.119746208190918}
I0420 07:43:08.626136 140502011692800 logging_writer.py:48] [8192] accumulated_eval_time=838.946758, accumulated_logging_time=1.119746, accumulated_submission_time=1941.489044, global_step=8192, preemption_count=0, score=1941.489044, test/accuracy=0.985617, test/loss=0.047614, test/mean_average_precision=0.238907, test/num_examples=43793, total_duration=2781.624092, train/accuracy=0.990065, train/loss=0.033270, train/mean_average_precision=0.342636, validation/accuracy=0.986468, validation/loss=0.045230, validation/mean_average_precision=0.240020, validation/num_examples=43793
I0420 07:43:08.659230 140687085266752 checkpoints.py:356] Saving checkpoint at step: 8192
I0420 07:43:08.748503 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_8192
I0420 07:43:08.748902 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_8192.
I0420 07:43:10.814610 140502003300096 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.012551008723676205, loss=0.03944254666566849
I0420 07:43:33.843818 140501994907392 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.011568327434360981, loss=0.036461953073740005
I0420 07:43:56.731028 140502003300096 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.012123997323215008, loss=0.03923940658569336
I0420 07:44:19.369677 140501994907392 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01153427455574274, loss=0.039245959371328354
I0420 07:44:41.953775 140502003300096 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.011038143187761307, loss=0.03551582247018814
I0420 07:45:04.679143 140501994907392 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.013524888083338737, loss=0.0405212827026844
I0420 07:45:27.007956 140502003300096 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01037009246647358, loss=0.03815823420882225
I0420 07:45:49.541050 140501994907392 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.015790877863764763, loss=0.035792287439107895
I0420 07:46:12.097944 140502003300096 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0111303785815835, loss=0.03719053044915199
I0420 07:46:34.374918 140501994907392 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.008750640787184238, loss=0.033913977444171906
I0420 07:46:56.780356 140502003300096 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.014183779247105122, loss=0.040298301726579666
I0420 07:47:08.768385 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:48:16.862983 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:48:19.356099 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:48:21.781805 140687085266752 submission_runner.py:406] Time since start: 3094.79s, 	Step: 9254, 	{'train/accuracy': 0.9901297688484192, 'train/loss': 0.03316609933972359, 'train/mean_average_precision': 0.3534454634768571, 'validation/accuracy': 0.9865812063217163, 'validation/loss': 0.04518292844295502, 'validation/mean_average_precision': 0.2474220904628969, 'validation/num_examples': 43793, 'test/accuracy': 0.9857408404350281, 'test/loss': 0.04780265688896179, 'test/mean_average_precision': 0.2487724701686865, 'test/num_examples': 43793, 'score': 2181.499445438385, 'total_duration': 3094.7877564430237, 'accumulated_submission_time': 2181.499445438385, 'accumulated_eval_time': 911.9601147174835, 'accumulated_logging_time': 1.250946044921875}
I0420 07:48:21.789486 140501994907392 logging_writer.py:48] [9254] accumulated_eval_time=911.960115, accumulated_logging_time=1.250946, accumulated_submission_time=2181.499445, global_step=9254, preemption_count=0, score=2181.499445, test/accuracy=0.985741, test/loss=0.047803, test/mean_average_precision=0.248772, test/num_examples=43793, total_duration=3094.787756, train/accuracy=0.990130, train/loss=0.033166, train/mean_average_precision=0.353445, validation/accuracy=0.986581, validation/loss=0.045183, validation/mean_average_precision=0.247422, validation/num_examples=43793
I0420 07:48:21.820374 140687085266752 checkpoints.py:356] Saving checkpoint at step: 9254
I0420 07:48:21.910974 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_9254
I0420 07:48:21.911408 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_9254.
I0420 07:48:32.396819 140502003300096 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.011769761331379414, loss=0.03584551438689232
I0420 07:48:54.718338 140501986514688 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.012685836292803288, loss=0.03927616775035858
I0420 07:49:17.188460 140502003300096 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.011811358854174614, loss=0.03604016825556755
I0420 07:49:39.473207 140501986514688 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.013200364075601101, loss=0.03391993045806885
I0420 07:50:02.237269 140502003300096 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.011881270445883274, loss=0.04172191768884659
I0420 07:50:24.488281 140501986514688 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.010871867649257183, loss=0.03315487131476402
I0420 07:50:46.721093 140502003300096 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.009567927569150925, loss=0.03447205573320389
I0420 07:51:09.117962 140501986514688 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.010798037052154541, loss=0.035044580698013306
I0420 07:51:31.561131 140502003300096 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.010519941337406635, loss=0.03804027661681175
I0420 07:51:53.830113 140501986514688 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.010020975954830647, loss=0.0322447307407856
I0420 07:52:16.247720 140502003300096 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.011160439811646938, loss=0.03631272166967392
I0420 07:52:22.119712 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:53:33.514707 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:53:36.018421 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:53:38.434107 140687085266752 submission_runner.py:406] Time since start: 3411.44s, 	Step: 10327, 	{'train/accuracy': 0.9907184839248657, 'train/loss': 0.031047070398926735, 'train/mean_average_precision': 0.399250652832955, 'validation/accuracy': 0.9866546392440796, 'validation/loss': 0.0448431670665741, 'validation/mean_average_precision': 0.2563345679990861, 'validation/num_examples': 43793, 'test/accuracy': 0.9858933091163635, 'test/loss': 0.047271933406591415, 'test/mean_average_precision': 0.257083315206387, 'test/num_examples': 43793, 'score': 2421.698768377304, 'total_duration': 3411.4400827884674, 'accumulated_submission_time': 2421.698768377304, 'accumulated_eval_time': 988.2744736671448, 'accumulated_logging_time': 1.3810019493103027}
I0420 07:53:38.442404 140501986514688 logging_writer.py:48] [10327] accumulated_eval_time=988.274474, accumulated_logging_time=1.381002, accumulated_submission_time=2421.698768, global_step=10327, preemption_count=0, score=2421.698768, test/accuracy=0.985893, test/loss=0.047272, test/mean_average_precision=0.257083, test/num_examples=43793, total_duration=3411.440083, train/accuracy=0.990718, train/loss=0.031047, train/mean_average_precision=0.399251, validation/accuracy=0.986655, validation/loss=0.044843, validation/mean_average_precision=0.256335, validation/num_examples=43793
I0420 07:53:38.473417 140687085266752 checkpoints.py:356] Saving checkpoint at step: 10327
I0420 07:53:38.576629 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_10327
I0420 07:53:38.576835 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_10327.
I0420 07:53:55.505690 140502003300096 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.011470411904156208, loss=0.03635105490684509
I0420 07:54:17.890177 140501902685952 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.016330711543560028, loss=0.035707686096429825
I0420 07:54:40.150286 140502003300096 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.012416834011673927, loss=0.037398770451545715
I0420 07:55:02.799639 140501902685952 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.012548795901238918, loss=0.03828302398324013
I0420 07:55:25.512228 140502003300096 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.011990932747721672, loss=0.03175508230924606
I0420 07:55:48.202661 140501902685952 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.013896265998482704, loss=0.038461390882730484
I0420 07:56:11.074953 140502003300096 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.01029328815639019, loss=0.03574496880173683
I0420 07:56:33.595376 140501902685952 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.010709045454859734, loss=0.03582918271422386
I0420 07:56:56.155545 140502003300096 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.009350947104394436, loss=0.03306230902671814
I0420 07:57:18.639420 140501902685952 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.008610717952251434, loss=0.03434628248214722
I0420 07:57:38.582679 140687085266752 spec.py:298] Evaluating on the training split.
I0420 07:58:48.182518 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 07:58:50.748970 140687085266752 spec.py:326] Evaluating on the test split.
I0420 07:58:53.221388 140687085266752 submission_runner.py:406] Time since start: 3726.23s, 	Step: 11390, 	{'train/accuracy': 0.9908456206321716, 'train/loss': 0.03021898865699768, 'train/mean_average_precision': 0.4206014194045308, 'validation/accuracy': 0.9867703318595886, 'validation/loss': 0.044726550579071045, 'validation/mean_average_precision': 0.2562293342095405, 'validation/num_examples': 43793, 'test/accuracy': 0.9859556555747986, 'test/loss': 0.04721144214272499, 'test/mean_average_precision': 0.25946815537044715, 'test/num_examples': 43793, 'score': 2661.6956963539124, 'total_duration': 3726.227314710617, 'accumulated_submission_time': 2661.6956963539124, 'accumulated_eval_time': 1062.913102388382, 'accumulated_logging_time': 1.5240674018859863}
I0420 07:58:53.229750 140502003300096 logging_writer.py:48] [11390] accumulated_eval_time=1062.913102, accumulated_logging_time=1.524067, accumulated_submission_time=2661.695696, global_step=11390, preemption_count=0, score=2661.695696, test/accuracy=0.985956, test/loss=0.047211, test/mean_average_precision=0.259468, test/num_examples=43793, total_duration=3726.227315, train/accuracy=0.990846, train/loss=0.030219, train/mean_average_precision=0.420601, validation/accuracy=0.986770, validation/loss=0.044727, validation/mean_average_precision=0.256229, validation/num_examples=43793
I0420 07:58:53.262589 140687085266752 checkpoints.py:356] Saving checkpoint at step: 11390
I0420 07:58:53.380601 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_11390
I0420 07:58:53.380998 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_11390.
I0420 07:58:55.849394 140501902685952 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.009759951382875443, loss=0.033019501715898514
I0420 07:59:18.326077 140501894293248 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.011055190116167068, loss=0.03457317873835564
I0420 07:59:40.642597 140501902685952 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.012000394985079765, loss=0.03565625101327896
I0420 08:00:03.108526 140501894293248 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.019453924149274826, loss=0.03907712548971176
I0420 08:00:25.879293 140501902685952 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.009526531212031841, loss=0.03471127152442932
I0420 08:00:48.768486 140501894293248 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.010595614090561867, loss=0.0350303091108799
I0420 08:01:11.225640 140687085266752 spec.py:298] Evaluating on the training split.
I0420 08:02:19.855295 140687085266752 spec.py:310] Evaluating on the validation split.
I0420 08:02:22.363821 140687085266752 spec.py:326] Evaluating on the test split.
I0420 08:02:24.791379 140687085266752 submission_runner.py:406] Time since start: 3937.80s, 	Step: 12000, 	{'train/accuracy': 0.990940272808075, 'train/loss': 0.029714278876781464, 'train/mean_average_precision': 0.4452323775570017, 'validation/accuracy': 0.9868174195289612, 'validation/loss': 0.04465600475668907, 'validation/mean_average_precision': 0.2653403381844489, 'validation/num_examples': 43793, 'test/accuracy': 0.9860007166862488, 'test/loss': 0.04728112369775772, 'test/mean_average_precision': 0.2583874795285673, 'test/num_examples': 43793, 'score': 2799.534986257553, 'total_duration': 3937.7973380088806, 'accumulated_submission_time': 2799.534986257553, 'accumulated_eval_time': 1136.4787895679474, 'accumulated_logging_time': 1.6841177940368652}
I0420 08:02:24.799724 140501902685952 logging_writer.py:48] [12000] accumulated_eval_time=1136.478790, accumulated_logging_time=1.684118, accumulated_submission_time=2799.534986, global_step=12000, preemption_count=0, score=2799.534986, test/accuracy=0.986001, test/loss=0.047281, test/mean_average_precision=0.258387, test/num_examples=43793, total_duration=3937.797338, train/accuracy=0.990940, train/loss=0.029714, train/mean_average_precision=0.445232, validation/accuracy=0.986817, validation/loss=0.044656, validation/mean_average_precision=0.265340, validation/num_examples=43793
I0420 08:02:24.832453 140687085266752 checkpoints.py:356] Saving checkpoint at step: 12000
I0420 08:02:24.937980 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000
I0420 08:02:24.938221 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000.
I0420 08:02:24.945520 140501894293248 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2799.534986
I0420 08:02:24.971187 140687085266752 checkpoints.py:356] Saving checkpoint at step: 12000
I0420 08:02:25.135230 140687085266752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000
I0420 08:02:25.135453 140687085266752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/ogbg_jax/trial_1/checkpoint_12000.
I0420 08:02:25.284954 140687085266752 submission_runner.py:567] Tuning trial 1/1
I0420 08:02:25.285184 140687085266752 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 08:02:25.286482 140687085266752 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5110936164855957, 'train/loss': 0.7456163167953491, 'train/mean_average_precision': 0.02329463101405079, 'validation/accuracy': 0.5076742768287659, 'validation/loss': 0.7476675510406494, 'validation/mean_average_precision': 0.026392045786451514, 'validation/num_examples': 43793, 'test/accuracy': 0.5058948397636414, 'test/loss': 0.7486602067947388, 'test/mean_average_precision': 0.02873946711436334, 'test/num_examples': 43793, 'score': 20.673376083374023, 'total_duration': 236.88105463981628, 'accumulated_submission_time': 20.673376083374023, 'accumulated_eval_time': 216.20752501487732, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1020, {'train/accuracy': 0.9869014620780945, 'train/loss': 0.0484786219894886, 'train/mean_average_precision': 0.08814932935958919, 'validation/accuracy': 0.9844004511833191, 'validation/loss': 0.057836636900901794, 'validation/mean_average_precision': 0.09118839840818964, 'validation/num_examples': 43793, 'test/accuracy': 0.9834137558937073, 'test/loss': 0.0612342432141304, 'test/mean_average_precision': 0.09118883971207395, 'test/num_examples': 43793, 'score': 260.6853218078613, 'total_duration': 555.5221028327942, 'accumulated_submission_time': 260.6853218078613, 'accumulated_eval_time': 294.6803369522095, 'accumulated_logging_time': 0.14761757850646973, 'global_step': 1020, 'preemption_count': 0}), (2033, {'train/accuracy': 0.9880001544952393, 'train/loss': 0.04317844286561012, 'train/mean_average_precision': 0.14953955315804734, 'validation/accuracy': 0.985159158706665, 'validation/loss': 0.05183469504117966, 'validation/mean_average_precision': 0.1512491336380951, 'validation/num_examples': 43793, 'test/accuracy': 0.9842498302459717, 'test/loss': 0.054563283920288086, 'test/mean_average_precision': 0.1508685760919853, 'test/num_examples': 43793, 'score': 500.903933763504, 'total_duration': 875.2886598110199, 'accumulated_submission_time': 500.903933763504, 'accumulated_eval_time': 374.07782793045044, 'accumulated_logging_time': 0.2896759510040283, 'global_step': 2033, 'preemption_count': 0}), (3046, {'train/accuracy': 0.9885160326957703, 'train/loss': 0.039860352873802185, 'train/mean_average_precision': 0.20810971782476279, 'validation/accuracy': 0.9855350852012634, 'validation/loss': 0.049451544880867004, 'validation/mean_average_precision': 0.17858840640900395, 'validation/num_examples': 43793, 'test/accuracy': 0.9846373200416565, 'test/loss': 0.052019547671079636, 'test/mean_average_precision': 0.17776589655271421, 'test/num_examples': 43793, 'score': 741.0670647621155, 'total_duration': 1194.4977238178253, 'accumulated_submission_time': 741.0670647621155, 'accumulated_eval_time': 452.97317481040955, 'accumulated_logging_time': 0.431643009185791, 'global_step': 3046, 'preemption_count': 0}), (4067, {'train/accuracy': 0.9887028336524963, 'train/loss': 0.03856830671429634, 'train/mean_average_precision': 0.2261006713681126, 'validation/accuracy': 0.9857088327407837, 'validation/loss': 0.04885789006948471, 'validation/mean_average_precision': 0.19490838290433732, 'validation/num_examples': 43793, 'test/accuracy': 0.984780490398407, 'test/loss': 0.051588308066129684, 'test/mean_average_precision': 0.19401721167353012, 'test/num_examples': 43793, 'score': 981.0759329795837, 'total_duration': 1512.1579344272614, 'accumulated_submission_time': 981.0759329795837, 'accumulated_eval_time': 530.4825625419617, 'accumulated_logging_time': 0.5650036334991455, 'global_step': 4067, 'preemption_count': 0}), (5090, {'train/accuracy': 0.9889498949050903, 'train/loss': 0.03742704540491104, 'train/mean_average_precision': 0.26010676276318845, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.04755784943699837, 'validation/mean_average_precision': 0.20434966439567903, 'validation/num_examples': 43793, 'test/accuracy': 0.9851305484771729, 'test/loss': 0.05013447627425194, 'test/mean_average_precision': 0.20815465779794443, 'test/num_examples': 43793, 'score': 1221.1616275310516, 'total_duration': 1829.6753106117249, 'accumulated_submission_time': 1221.1616275310516, 'accumulated_eval_time': 607.7712187767029, 'accumulated_logging_time': 0.6997482776641846, 'global_step': 5090, 'preemption_count': 0}), (6122, {'train/accuracy': 0.9890832901000977, 'train/loss': 0.03685196489095688, 'train/mean_average_precision': 0.26947347958626616, 'validation/accuracy': 0.9861037731170654, 'validation/loss': 0.04679630696773529, 'validation/mean_average_precision': 0.21783348200408964, 'validation/num_examples': 43793, 'test/accuracy': 0.9851772785186768, 'test/loss': 0.049345072358846664, 'test/mean_average_precision': 0.22344394047604246, 'test/num_examples': 43793, 'score': 1461.2066218852997, 'total_duration': 2147.244038581848, 'accumulated_submission_time': 1461.2066218852997, 'accumulated_eval_time': 685.1477692127228, 'accumulated_logging_time': 0.8384785652160645, 'global_step': 6122, 'preemption_count': 0}), (7149, {'train/accuracy': 0.9897762537002563, 'train/loss': 0.03483475744724274, 'train/mean_average_precision': 0.30884145205816144, 'validation/accuracy': 0.9863566756248474, 'validation/loss': 0.04602137953042984, 'validation/mean_average_precision': 0.2298793378191102, 'validation/num_examples': 43793, 'test/accuracy': 0.9855256080627441, 'test/loss': 0.0483911894261837, 'test/mean_average_precision': 0.23905387998470412, 'test/num_examples': 43793, 'score': 1701.3257138729095, 'total_duration': 2464.7532160282135, 'accumulated_submission_time': 1701.3257138729095, 'accumulated_eval_time': 762.3835854530334, 'accumulated_logging_time': 0.9841015338897705, 'global_step': 7149, 'preemption_count': 0}), (8192, {'train/accuracy': 0.990064799785614, 'train/loss': 0.0332697331905365, 'train/mean_average_precision': 0.342635779191877, 'validation/accuracy': 0.9864678978919983, 'validation/loss': 0.045229747891426086, 'validation/mean_average_precision': 0.2400200454297398, 'validation/num_examples': 43793, 'test/accuracy': 0.9856165647506714, 'test/loss': 0.04761410504579544, 'test/mean_average_precision': 0.23890747563342912, 'test/num_examples': 43793, 'score': 1941.4890439510345, 'total_duration': 2781.6240923404694, 'accumulated_submission_time': 1941.4890439510345, 'accumulated_eval_time': 838.9467575550079, 'accumulated_logging_time': 1.119746208190918, 'global_step': 8192, 'preemption_count': 0}), (9254, {'train/accuracy': 0.9901297688484192, 'train/loss': 0.03316609933972359, 'train/mean_average_precision': 0.3534454634768571, 'validation/accuracy': 0.9865812063217163, 'validation/loss': 0.04518292844295502, 'validation/mean_average_precision': 0.2474220904628969, 'validation/num_examples': 43793, 'test/accuracy': 0.9857408404350281, 'test/loss': 0.04780265688896179, 'test/mean_average_precision': 0.2487724701686865, 'test/num_examples': 43793, 'score': 2181.499445438385, 'total_duration': 3094.7877564430237, 'accumulated_submission_time': 2181.499445438385, 'accumulated_eval_time': 911.9601147174835, 'accumulated_logging_time': 1.250946044921875, 'global_step': 9254, 'preemption_count': 0}), (10327, {'train/accuracy': 0.9907184839248657, 'train/loss': 0.031047070398926735, 'train/mean_average_precision': 0.399250652832955, 'validation/accuracy': 0.9866546392440796, 'validation/loss': 0.0448431670665741, 'validation/mean_average_precision': 0.2563345679990861, 'validation/num_examples': 43793, 'test/accuracy': 0.9858933091163635, 'test/loss': 0.047271933406591415, 'test/mean_average_precision': 0.257083315206387, 'test/num_examples': 43793, 'score': 2421.698768377304, 'total_duration': 3411.4400827884674, 'accumulated_submission_time': 2421.698768377304, 'accumulated_eval_time': 988.2744736671448, 'accumulated_logging_time': 1.3810019493103027, 'global_step': 10327, 'preemption_count': 0}), (11390, {'train/accuracy': 0.9908456206321716, 'train/loss': 0.03021898865699768, 'train/mean_average_precision': 0.4206014194045308, 'validation/accuracy': 0.9867703318595886, 'validation/loss': 0.044726550579071045, 'validation/mean_average_precision': 0.2562293342095405, 'validation/num_examples': 43793, 'test/accuracy': 0.9859556555747986, 'test/loss': 0.04721144214272499, 'test/mean_average_precision': 0.25946815537044715, 'test/num_examples': 43793, 'score': 2661.6956963539124, 'total_duration': 3726.227314710617, 'accumulated_submission_time': 2661.6956963539124, 'accumulated_eval_time': 1062.913102388382, 'accumulated_logging_time': 1.5240674018859863, 'global_step': 11390, 'preemption_count': 0}), (12000, {'train/accuracy': 0.990940272808075, 'train/loss': 0.029714278876781464, 'train/mean_average_precision': 0.4452323775570017, 'validation/accuracy': 0.9868174195289612, 'validation/loss': 0.04465600475668907, 'validation/mean_average_precision': 0.2653403381844489, 'validation/num_examples': 43793, 'test/accuracy': 0.9860007166862488, 'test/loss': 0.04728112369775772, 'test/mean_average_precision': 0.2583874795285673, 'test/num_examples': 43793, 'score': 2799.534986257553, 'total_duration': 3937.7973380088806, 'accumulated_submission_time': 2799.534986257553, 'accumulated_eval_time': 1136.4787895679474, 'accumulated_logging_time': 1.6841177940368652, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0420 08:02:25.286601 140687085266752 submission_runner.py:570] Timing: 2799.534986257553
I0420 08:02:25.286643 140687085266752 submission_runner.py:571] ====================
I0420 08:02:25.286752 140687085266752 submission_runner.py:631] Final ogbg score: 2799.534986257553
