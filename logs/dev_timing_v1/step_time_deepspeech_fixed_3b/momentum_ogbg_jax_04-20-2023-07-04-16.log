I0420 07:04:37.913792 140715718510400 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax.
I0420 07:04:37.978299 140715718510400 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 07:04:38.792102 140715718510400 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0420 07:04:38.792784 140715718510400 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 07:04:38.796969 140715718510400 submission_runner.py:528] Using RNG seed 2202513782
I0420 07:04:41.526484 140715718510400 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 07:04:41.526661 140715718510400 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1.
I0420 07:04:41.526839 140715718510400 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/hparams.json.
I0420 07:04:41.645917 140715718510400 submission_runner.py:232] Initializing dataset.
I0420 07:04:41.875990 140715718510400 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:04:41.880900 140715718510400 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:04:42.116495 140715718510400 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:04:42.172457 140715718510400 submission_runner.py:239] Initializing model.
I0420 07:04:49.966271 140715718510400 submission_runner.py:249] Initializing optimizer.
I0420 07:04:50.308047 140715718510400 submission_runner.py:256] Initializing metrics bundle.
I0420 07:04:50.308293 140715718510400 submission_runner.py:273] Initializing checkpoint and logger.
I0420 07:04:50.309426 140715718510400 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1 with prefix checkpoint_
I0420 07:04:50.309715 140715718510400 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 07:04:50.309816 140715718510400 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 07:04:51.314335 140715718510400 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/meta_data_0.json.
I0420 07:04:51.315485 140715718510400 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/flags_0.json.
I0420 07:04:51.321738 140715718510400 submission_runner.py:309] Starting training loop.
I0420 07:05:10.200019 140539483649792 logging_writer.py:48] [0] global_step=0, grad_norm=2.725400924682617, loss=0.6966128945350647
I0420 07:05:10.212852 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:05:10.220473 140715718510400 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:05:10.224107 140715718510400 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:05:10.280528 140715718510400 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
W0420 07:05:25.795041 140715718510400 metrics.py:232] Ignoring mask for model output 'loss' because of shape mismatch: output.shape=() vs. mask.shape=(4097, 128)
I0420 07:06:40.877983 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:06:40.880636 140715718510400 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:06:40.884268 140715718510400 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:06:40.935560 140715718510400 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:07:43.169691 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:07:43.172418 140715718510400 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:07:43.176064 140715718510400 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0420 07:07:43.228325 140715718510400 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0420 07:08:46.953948 140715718510400 submission_runner.py:406] Time since start: 235.63s, 	Step: 1, 	{'train/accuracy': 0.5763778686523438, 'train/loss': 0.6968065500259399, 'train/mean_average_precision': 0.023237913545750145, 'validation/accuracy': 0.5727439522743225, 'validation/loss': 0.6998342275619507, 'validation/mean_average_precision': 0.026464395493344436, 'validation/num_examples': 43793, 'test/accuracy': 0.5716279149055481, 'test/loss': 0.7015213370323181, 'test/mean_average_precision': 0.027864573736120092, 'test/num_examples': 43793, 'score': 18.890955448150635, 'total_duration': 235.63217091560364, 'accumulated_submission_time': 18.890955448150635, 'accumulated_eval_time': 216.74106001853943, 'accumulated_logging_time': 0}
I0420 07:08:46.969759 140529995679488 logging_writer.py:48] [1] accumulated_eval_time=216.741060, accumulated_logging_time=0, accumulated_submission_time=18.890955, global_step=1, preemption_count=0, score=18.890955, test/accuracy=0.571628, test/loss=0.701521, test/mean_average_precision=0.027865, test/num_examples=43793, total_duration=235.632171, train/accuracy=0.576378, train/loss=0.696807, train/mean_average_precision=0.023238, validation/accuracy=0.572744, validation/loss=0.699834, validation/mean_average_precision=0.026464, validation/num_examples=43793
I0420 07:08:46.995699 140715718510400 checkpoints.py:356] Saving checkpoint at step: 1
I0420 07:08:47.064395 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_1
I0420 07:08:47.064617 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_1.
I0420 07:09:09.814034 140530004072192 logging_writer.py:48] [100] global_step=100, grad_norm=0.06588296592235565, loss=0.09108531475067139
I0420 07:09:32.523241 140531128596224 logging_writer.py:48] [200] global_step=200, grad_norm=0.01668412610888481, loss=0.06200290471315384
I0420 07:09:55.460997 140530004072192 logging_writer.py:48] [300] global_step=300, grad_norm=0.01140191126614809, loss=0.050230324268341064
I0420 07:10:18.490844 140531128596224 logging_writer.py:48] [400] global_step=400, grad_norm=0.011724582873284817, loss=0.05982445925474167
I0420 07:10:41.467034 140530004072192 logging_writer.py:48] [500] global_step=500, grad_norm=0.007395070977509022, loss=0.053174298256635666
I0420 07:11:04.342285 140531128596224 logging_writer.py:48] [600] global_step=600, grad_norm=0.011833020485937595, loss=0.05054067447781563
I0420 07:11:27.364664 140530004072192 logging_writer.py:48] [700] global_step=700, grad_norm=0.020541392266750336, loss=0.06221422925591469
I0420 07:11:50.491487 140531128596224 logging_writer.py:48] [800] global_step=800, grad_norm=0.02563510090112686, loss=0.0517895482480526
I0420 07:12:13.655326 140530004072192 logging_writer.py:48] [900] global_step=900, grad_norm=0.01333470270037651, loss=0.052924398332834244
I0420 07:12:36.866121 140531128596224 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.07590099424123764, loss=0.05729417875409126
I0420 07:12:47.073116 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:14:02.519573 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:14:05.055671 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:14:07.506352 140715718510400 submission_runner.py:406] Time since start: 556.18s, 	Step: 1045, 	{'train/accuracy': 0.986743688583374, 'train/loss': 0.054528165608644485, 'train/mean_average_precision': 0.03390355656744816, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06456995010375977, 'validation/mean_average_precision': 0.03696588391907003, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06783725321292877, 'test/mean_average_precision': 0.03810085222859725, 'test/num_examples': 43793, 'score': 258.8908050060272, 'total_duration': 556.1845576763153, 'accumulated_submission_time': 258.8908050060272, 'accumulated_eval_time': 297.1742582321167, 'accumulated_logging_time': 0.11083030700683594}
I0420 07:14:07.514854 140530004072192 logging_writer.py:48] [1045] accumulated_eval_time=297.174258, accumulated_logging_time=0.110830, accumulated_submission_time=258.890805, global_step=1045, preemption_count=0, score=258.890805, test/accuracy=0.983142, test/loss=0.067837, test/mean_average_precision=0.038101, test/num_examples=43793, total_duration=556.184558, train/accuracy=0.986744, train/loss=0.054528, train/mean_average_precision=0.033904, validation/accuracy=0.984118, validation/loss=0.064570, validation/mean_average_precision=0.036966, validation/num_examples=43793
I0420 07:14:07.540194 140715718510400 checkpoints.py:356] Saving checkpoint at step: 1045
I0420 07:14:07.596612 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_1045
I0420 07:14:07.596797 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_1045.
I0420 07:14:20.562851 140531128596224 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.04608214274048805, loss=0.05659880116581917
I0420 07:14:43.749357 140531103418112 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.06253302097320557, loss=0.055397119373083115
I0420 07:15:07.007606 140531128596224 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.05989046394824982, loss=0.05420069023966789
I0420 07:15:30.468132 140531103418112 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.05030279979109764, loss=0.055163267999887466
I0420 07:15:53.663609 140531128596224 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.044058866798877716, loss=0.057090964168310165
I0420 07:16:16.996393 140531103418112 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.11924026906490326, loss=0.0536334291100502
I0420 07:16:39.848950 140531128596224 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.04778575897216797, loss=0.053529515862464905
I0420 07:17:02.596158 140531103418112 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0469270758330822, loss=0.054943326860666275
I0420 07:17:25.251018 140531128596224 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.04928893968462944, loss=0.051799118518829346
I0420 07:17:48.057945 140531103418112 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.017736872658133507, loss=0.05133805423974991
I0420 07:18:07.744209 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:19:20.212950 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:19:22.706295 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:19:25.151047 140715718510400 submission_runner.py:406] Time since start: 873.83s, 	Step: 2086, 	{'train/accuracy': 0.986762523651123, 'train/loss': 0.05191529169678688, 'train/mean_average_precision': 0.051916519807026794, 'validation/accuracy': 0.9841325283050537, 'validation/loss': 0.06155897676944733, 'validation/mean_average_precision': 0.053209505013883024, 'validation/num_examples': 43793, 'test/accuracy': 0.9831568002700806, 'test/loss': 0.06486614048480988, 'test/mean_average_precision': 0.05455546427448808, 'test/num_examples': 43793, 'score': 499.02963852882385, 'total_duration': 873.829252243042, 'accumulated_submission_time': 499.02963852882385, 'accumulated_eval_time': 374.5810604095459, 'accumulated_logging_time': 0.20139431953430176}
I0420 07:19:25.158466 140531128596224 logging_writer.py:48] [2086] accumulated_eval_time=374.581060, accumulated_logging_time=0.201394, accumulated_submission_time=499.029639, global_step=2086, preemption_count=0, score=499.029639, test/accuracy=0.983157, test/loss=0.064866, test/mean_average_precision=0.054555, test/num_examples=43793, total_duration=873.829252, train/accuracy=0.986763, train/loss=0.051915, train/mean_average_precision=0.051917, validation/accuracy=0.984133, validation/loss=0.061559, validation/mean_average_precision=0.053210, validation/num_examples=43793
I0420 07:19:25.185131 140715718510400 checkpoints.py:356] Saving checkpoint at step: 2086
I0420 07:19:25.252687 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_2086
I0420 07:19:25.252903 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_2086.
I0420 07:19:28.728299 140531103418112 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.06100422888994217, loss=0.049622971564531326
I0420 07:19:51.705658 140531078240000 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.03103157877922058, loss=0.048372168093919754
I0420 07:20:14.716474 140531103418112 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04420042410492897, loss=0.05685928836464882
I0420 07:20:37.871609 140531078240000 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.07211413979530334, loss=0.050793543457984924
I0420 07:21:01.097431 140531103418112 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.10124377906322479, loss=0.05246986821293831
I0420 07:21:24.150427 140531078240000 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.09132511168718338, loss=0.057341691106557846
I0420 07:21:47.100972 140531103418112 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.04288751259446144, loss=0.052608463913202286
I0420 07:22:10.459164 140531078240000 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.043191514909267426, loss=0.049866933375597
I0420 07:22:33.896362 140531103418112 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.03960241377353668, loss=0.049016810953617096
I0420 07:22:57.649425 140531078240000 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.04546178877353668, loss=0.04818860441446304
I0420 07:23:21.037693 140531103418112 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0477164201438427, loss=0.05020610988140106
I0420 07:23:25.421388 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:24:38.386160 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:24:40.879467 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:24:43.331403 140715718510400 submission_runner.py:406] Time since start: 1192.01s, 	Step: 3120, 	{'train/accuracy': 0.9868236780166626, 'train/loss': 0.0498686321079731, 'train/mean_average_precision': 0.08384551563775214, 'validation/accuracy': 0.9841800332069397, 'validation/loss': 0.058765605092048645, 'validation/mean_average_precision': 0.07825618086213128, 'validation/num_examples': 43793, 'test/accuracy': 0.9832035899162292, 'test/loss': 0.06177526339888573, 'test/mean_average_precision': 0.08132213076054275, 'test/num_examples': 43793, 'score': 739.189546585083, 'total_duration': 1192.0096113681793, 'accumulated_submission_time': 739.189546585083, 'accumulated_eval_time': 452.4910366535187, 'accumulated_logging_time': 0.3034024238586426}
I0420 07:24:43.339046 140531078240000 logging_writer.py:48] [3120] accumulated_eval_time=452.491037, accumulated_logging_time=0.303402, accumulated_submission_time=739.189547, global_step=3120, preemption_count=0, score=739.189547, test/accuracy=0.983204, test/loss=0.061775, test/mean_average_precision=0.081322, test/num_examples=43793, total_duration=1192.009611, train/accuracy=0.986824, train/loss=0.049869, train/mean_average_precision=0.083846, validation/accuracy=0.984180, validation/loss=0.058766, validation/mean_average_precision=0.078256, validation/num_examples=43793
I0420 07:24:43.368989 140715718510400 checkpoints.py:356] Saving checkpoint at step: 3120
I0420 07:24:43.424941 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_3120
I0420 07:24:43.425136 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_3120.
I0420 07:25:02.004477 140531103418112 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05167784541845322, loss=0.054003458470106125
I0420 07:25:24.945821 140531069847296 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.04208527132868767, loss=0.046266719698905945
I0420 07:25:47.840877 140531103418112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.065383180975914, loss=0.053309015929698944
I0420 07:26:11.163778 140531069847296 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05013536661863327, loss=0.05010633170604706
I0420 07:26:34.612260 140531103418112 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.05303981900215149, loss=0.045947134494781494
I0420 07:26:57.946180 140531069847296 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.05293390899896622, loss=0.05033343657851219
I0420 07:27:21.250191 140531103418112 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.05716778337955475, loss=0.050687480717897415
I0420 07:27:44.607126 140531069847296 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.07385678589344025, loss=0.05277227237820625
I0420 07:28:08.008776 140531103418112 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.09181457757949829, loss=0.05562754347920418
I0420 07:28:30.974029 140531069847296 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.12317513674497604, loss=0.05257285386323929
I0420 07:28:43.558135 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:29:54.942871 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:29:57.453123 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:29:59.907734 140715718510400 submission_runner.py:406] Time since start: 1508.59s, 	Step: 4157, 	{'train/accuracy': 0.9869639873504639, 'train/loss': 0.04759415239095688, 'train/mean_average_precision': 0.10388764169260686, 'validation/accuracy': 0.9842908382415771, 'validation/loss': 0.05688828229904175, 'validation/mean_average_precision': 0.10535924104926349, 'validation/num_examples': 43793, 'test/accuracy': 0.9832941293716431, 'test/loss': 0.06015659123659134, 'test/mean_average_precision': 0.10334512727657637, 'test/num_examples': 43793, 'score': 979.3141267299652, 'total_duration': 1508.5859277248383, 'accumulated_submission_time': 979.3141267299652, 'accumulated_eval_time': 528.8405935764313, 'accumulated_logging_time': 0.397291898727417}
I0420 07:29:59.916110 140531103418112 logging_writer.py:48] [4157] accumulated_eval_time=528.840594, accumulated_logging_time=0.397292, accumulated_submission_time=979.314127, global_step=4157, preemption_count=0, score=979.314127, test/accuracy=0.983294, test/loss=0.060157, test/mean_average_precision=0.103345, test/num_examples=43793, total_duration=1508.585928, train/accuracy=0.986964, train/loss=0.047594, train/mean_average_precision=0.103888, validation/accuracy=0.984291, validation/loss=0.056888, validation/mean_average_precision=0.105359, validation/num_examples=43793
I0420 07:29:59.941313 140715718510400 checkpoints.py:356] Saving checkpoint at step: 4157
I0420 07:29:59.994282 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_4157
I0420 07:29:59.994500 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_4157.
I0420 07:30:09.995476 140531069847296 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.09998466074466705, loss=0.04758191108703613
I0420 07:30:32.568482 140531061454592 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04919462278485298, loss=0.04899947717785835
I0420 07:30:55.082881 140531069847296 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.033322058618068695, loss=0.04485303908586502
I0420 07:31:17.665416 140531061454592 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.041400179266929626, loss=0.04713577777147293
I0420 07:31:40.375506 140531069847296 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.06988202780485153, loss=0.04887484386563301
I0420 07:32:02.867767 140531061454592 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.04503752291202545, loss=0.048357486724853516
I0420 07:32:25.283553 140531069847296 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0868770107626915, loss=0.05055728927254677
I0420 07:32:47.836276 140531061454592 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.03547046706080437, loss=0.04843270778656006
I0420 07:33:10.448068 140531069847296 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.06466981023550034, loss=0.048351217061281204
I0420 07:33:33.000247 140531061454592 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.04179645702242851, loss=0.04750135540962219
I0420 07:33:55.803112 140531069847296 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03766743466258049, loss=0.04417416453361511
I0420 07:34:00.123797 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:35:11.716165 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:35:14.199584 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:35:16.623000 140715718510400 submission_runner.py:406] Time since start: 1825.30s, 	Step: 5220, 	{'train/accuracy': 0.9875674247741699, 'train/loss': 0.044705040752887726, 'train/mean_average_precision': 0.13513406435917022, 'validation/accuracy': 0.9847674369812012, 'validation/loss': 0.054231345653533936, 'validation/mean_average_precision': 0.12262685458778906, 'validation/num_examples': 43793, 'test/accuracy': 0.9837843775749207, 'test/loss': 0.057103000581264496, 'test/mean_average_precision': 0.12704215520242798, 'test/num_examples': 43793, 'score': 1219.4351851940155, 'total_duration': 1825.3011920452118, 'accumulated_submission_time': 1219.4351851940155, 'accumulated_eval_time': 605.3397443294525, 'accumulated_logging_time': 0.48420143127441406}
I0420 07:35:16.630261 140531061454592 logging_writer.py:48] [5220] accumulated_eval_time=605.339744, accumulated_logging_time=0.484201, accumulated_submission_time=1219.435185, global_step=5220, preemption_count=0, score=1219.435185, test/accuracy=0.983784, test/loss=0.057103, test/mean_average_precision=0.127042, test/num_examples=43793, total_duration=1825.301192, train/accuracy=0.987567, train/loss=0.044705, train/mean_average_precision=0.135134, validation/accuracy=0.984767, validation/loss=0.054231, validation/mean_average_precision=0.122627, validation/num_examples=43793
I0420 07:35:16.654385 140715718510400 checkpoints.py:356] Saving checkpoint at step: 5220
I0420 07:35:16.708536 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_5220
I0420 07:35:16.708708 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_5220.
I0420 07:35:35.163230 140531069847296 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.05656915158033371, loss=0.04776528850197792
I0420 07:35:58.036884 140531053061888 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04970533028244972, loss=0.04554944485425949
I0420 07:36:20.378764 140531069847296 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.03307264670729637, loss=0.043831717222929
I0420 07:36:42.746933 140531053061888 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.04533911868929863, loss=0.043791331350803375
I0420 07:37:05.513359 140531069847296 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.04482731223106384, loss=0.0477617122232914
I0420 07:37:27.916042 140531053061888 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03313323110342026, loss=0.048046477138996124
I0420 07:37:50.143008 140531069847296 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.07697097957134247, loss=0.046883806586265564
I0420 07:38:12.111568 140531053061888 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.04837009310722351, loss=0.04287809506058693
I0420 07:38:34.160299 140531069847296 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0444808192551136, loss=0.04590413346886635
I0420 07:38:56.266257 140531053061888 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.03997362405061722, loss=0.042547304183244705
I0420 07:39:16.841875 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:40:26.766350 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:40:29.249355 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:40:31.669483 140715718510400 submission_runner.py:406] Time since start: 2140.35s, 	Step: 6294, 	{'train/accuracy': 0.9875965714454651, 'train/loss': 0.04402443766593933, 'train/mean_average_precision': 0.15279966748618615, 'validation/accuracy': 0.9850187301635742, 'validation/loss': 0.05335491895675659, 'validation/mean_average_precision': 0.14226458631350392, 'validation/num_examples': 43793, 'test/accuracy': 0.9840270280838013, 'test/loss': 0.056335967034101486, 'test/mean_average_precision': 0.14457183059407552, 'test/num_examples': 43793, 'score': 1459.5598266124725, 'total_duration': 2140.3476927280426, 'accumulated_submission_time': 1459.5598266124725, 'accumulated_eval_time': 680.1673183441162, 'accumulated_logging_time': 0.5700342655181885}
I0420 07:40:31.677038 140531069847296 logging_writer.py:48] [6294] accumulated_eval_time=680.167318, accumulated_logging_time=0.570034, accumulated_submission_time=1459.559827, global_step=6294, preemption_count=0, score=1459.559827, test/accuracy=0.984027, test/loss=0.056336, test/mean_average_precision=0.144572, test/num_examples=43793, total_duration=2140.347693, train/accuracy=0.987597, train/loss=0.044024, train/mean_average_precision=0.152800, validation/accuracy=0.985019, validation/loss=0.053355, validation/mean_average_precision=0.142265, validation/num_examples=43793
I0420 07:40:31.701131 140715718510400 checkpoints.py:356] Saving checkpoint at step: 6294
I0420 07:40:31.754068 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_6294
I0420 07:40:31.754269 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_6294.
I0420 07:40:33.313176 140531053061888 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03931991010904312, loss=0.048682380467653275
I0420 07:40:55.309251 140531044669184 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.036651819944381714, loss=0.04597406089305878
I0420 07:41:17.424055 140531053061888 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.03584137558937073, loss=0.04499994218349457
I0420 07:41:39.649172 140531044669184 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.058804575353860855, loss=0.045082345604896545
I0420 07:42:01.858584 140531053061888 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.04169740900397301, loss=0.04466290399432182
I0420 07:42:24.385341 140531044669184 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.053975801914930344, loss=0.044134777039289474
I0420 07:42:46.693877 140531053061888 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.053638968616724014, loss=0.04677555337548256
I0420 07:43:09.328272 140531044669184 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.06789019703865051, loss=0.048422250896692276
I0420 07:43:31.704384 140531053061888 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.05725094676017761, loss=0.04631531611084938
I0420 07:43:54.213444 140531044669184 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0513109490275383, loss=0.04817337542772293
I0420 07:44:16.334658 140531053061888 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.06985068321228027, loss=0.04304305464029312
I0420 07:44:31.921239 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:45:42.919469 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:45:45.401085 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:45:47.788763 140715718510400 submission_runner.py:406] Time since start: 2456.47s, 	Step: 7372, 	{'train/accuracy': 0.9878565073013306, 'train/loss': 0.042140401899814606, 'train/mean_average_precision': 0.1756673911590159, 'validation/accuracy': 0.9850901365280151, 'validation/loss': 0.051629625260829926, 'validation/mean_average_precision': 0.15182327645475896, 'validation/num_examples': 43793, 'test/accuracy': 0.984144926071167, 'test/loss': 0.054758038371801376, 'test/mean_average_precision': 0.1567634018232528, 'test/num_examples': 43793, 'score': 1699.7182536125183, 'total_duration': 2456.4669349193573, 'accumulated_submission_time': 1699.7182536125183, 'accumulated_eval_time': 756.0347700119019, 'accumulated_logging_time': 0.6549685001373291}
I0420 07:45:47.796805 140531044669184 logging_writer.py:48] [7372] accumulated_eval_time=756.034770, accumulated_logging_time=0.654969, accumulated_submission_time=1699.718254, global_step=7372, preemption_count=0, score=1699.718254, test/accuracy=0.984145, test/loss=0.054758, test/mean_average_precision=0.156763, test/num_examples=43793, total_duration=2456.466935, train/accuracy=0.987857, train/loss=0.042140, train/mean_average_precision=0.175667, validation/accuracy=0.985090, validation/loss=0.051630, validation/mean_average_precision=0.151823, validation/num_examples=43793
I0420 07:45:47.821021 140715718510400 checkpoints.py:356] Saving checkpoint at step: 7372
I0420 07:45:47.875222 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_7372
I0420 07:45:47.875391 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_7372.
I0420 07:45:54.365191 140531053061888 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0714072585105896, loss=0.04665786772966385
I0420 07:46:16.813744 140531036276480 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0389644131064415, loss=0.03942755237221718
I0420 07:46:39.752072 140531053061888 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.04668887332081795, loss=0.04328657314181328
I0420 07:47:02.687911 140531036276480 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.10506197810173035, loss=0.050345733761787415
I0420 07:47:25.236380 140531053061888 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.042626798152923584, loss=0.04436478763818741
I0420 07:47:47.383091 140531036276480 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.04317806288599968, loss=0.04403890669345856
I0420 07:48:09.487905 140531053061888 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.04184844717383385, loss=0.04702058061957359
I0420 07:48:31.355681 140531036276480 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.05113084986805916, loss=0.04474613443017006
I0420 07:48:53.442450 140531053061888 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03506975620985031, loss=0.043911587446928024
I0420 07:49:15.621364 140531036276480 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.08942761272192001, loss=0.04979173094034195
I0420 07:49:37.709223 140531053061888 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.04911874234676361, loss=0.04307510703802109
I0420 07:49:47.999207 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:51:00.623344 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:51:03.130813 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:51:05.559744 140715718510400 submission_runner.py:406] Time since start: 2774.24s, 	Step: 8448, 	{'train/accuracy': 0.9881747961044312, 'train/loss': 0.04172274470329285, 'train/mean_average_precision': 0.1882117710003608, 'validation/accuracy': 0.9852391481399536, 'validation/loss': 0.05171152949333191, 'validation/mean_average_precision': 0.16648324518653723, 'validation/num_examples': 43793, 'test/accuracy': 0.9842401146888733, 'test/loss': 0.05464910343289375, 'test/mean_average_precision': 0.16777060997363255, 'test/num_examples': 43793, 'score': 1939.8336091041565, 'total_duration': 2774.237932443619, 'accumulated_submission_time': 1939.8336091041565, 'accumulated_eval_time': 833.5952479839325, 'accumulated_logging_time': 0.7417163848876953}
I0420 07:51:05.567546 140531036276480 logging_writer.py:48] [8448] accumulated_eval_time=833.595248, accumulated_logging_time=0.741716, accumulated_submission_time=1939.833609, global_step=8448, preemption_count=0, score=1939.833609, test/accuracy=0.984240, test/loss=0.054649, test/mean_average_precision=0.167771, test/num_examples=43793, total_duration=2774.237932, train/accuracy=0.988175, train/loss=0.041723, train/mean_average_precision=0.188212, validation/accuracy=0.985239, validation/loss=0.051712, validation/mean_average_precision=0.166483, validation/num_examples=43793
I0420 07:51:05.591441 140715718510400 checkpoints.py:356] Saving checkpoint at step: 8448
I0420 07:51:05.641304 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_8448
I0420 07:51:05.641501 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_8448.
I0420 07:51:17.305855 140531053061888 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.05432553216814995, loss=0.04274392127990723
I0420 07:51:39.482316 140531027883776 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.07140189409255981, loss=0.04419519379734993
I0420 07:52:01.729722 140531053061888 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0521991141140461, loss=0.04333328455686569
I0420 07:52:24.211973 140531027883776 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.05553653836250305, loss=0.04299435764551163
I0420 07:52:46.430987 140531053061888 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.06764960289001465, loss=0.03938945755362511
I0420 07:53:09.117272 140531027883776 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03859160467982292, loss=0.0412590466439724
I0420 07:53:31.606233 140531053061888 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.047734055668115616, loss=0.04511483013629913
I0420 07:53:54.291128 140531027883776 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.03267936781048775, loss=0.04225459322333336
I0420 07:54:16.835096 140531053061888 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.03271738812327385, loss=0.041715141385793686
I0420 07:54:39.431888 140531027883776 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.05905184522271156, loss=0.042859580367803574
I0420 07:55:01.821653 140531053061888 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.047155652195215225, loss=0.04413213953375816
I0420 07:55:05.840561 140715718510400 spec.py:298] Evaluating on the training split.
I0420 07:56:16.225102 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 07:56:18.681645 140715718510400 spec.py:326] Evaluating on the test split.
I0420 07:56:21.064200 140715718510400 submission_runner.py:406] Time since start: 3089.74s, 	Step: 9519, 	{'train/accuracy': 0.9882425665855408, 'train/loss': 0.04041565954685211, 'train/mean_average_precision': 0.2074558147739693, 'validation/accuracy': 0.9852866530418396, 'validation/loss': 0.05120415240526199, 'validation/mean_average_precision': 0.1761443908103017, 'validation/num_examples': 43793, 'test/accuracy': 0.9843248128890991, 'test/loss': 0.05431773513555527, 'test/mean_average_precision': 0.17950107227627746, 'test/num_examples': 43793, 'score': 2180.0237913131714, 'total_duration': 3089.742404937744, 'accumulated_submission_time': 2180.0237913131714, 'accumulated_eval_time': 908.8188443183899, 'accumulated_logging_time': 0.8236253261566162}
I0420 07:56:21.072240 140531027883776 logging_writer.py:48] [9519] accumulated_eval_time=908.818844, accumulated_logging_time=0.823625, accumulated_submission_time=2180.023791, global_step=9519, preemption_count=0, score=2180.023791, test/accuracy=0.984325, test/loss=0.054318, test/mean_average_precision=0.179501, test/num_examples=43793, total_duration=3089.742405, train/accuracy=0.988243, train/loss=0.040416, train/mean_average_precision=0.207456, validation/accuracy=0.985287, validation/loss=0.051204, validation/mean_average_precision=0.176144, validation/num_examples=43793
I0420 07:56:21.097403 140715718510400 checkpoints.py:356] Saving checkpoint at step: 9519
I0420 07:56:21.148315 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_9519
I0420 07:56:21.148541 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_9519.
I0420 07:56:39.542698 140531053061888 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.05702296271920204, loss=0.048924222588539124
I0420 07:57:01.725981 140531019491072 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03390108421444893, loss=0.04244837537407875
I0420 07:57:23.985028 140531053061888 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0427987314760685, loss=0.045643799006938934
I0420 07:57:46.243587 140531019491072 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.05637640133500099, loss=0.04361667111515999
I0420 07:58:08.224688 140531053061888 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.05030243843793869, loss=0.04098459705710411
I0420 07:58:30.178719 140531019491072 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.04412617161870003, loss=0.04406098648905754
I0420 07:58:52.357223 140531053061888 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.059856027364730835, loss=0.042001042515039444
I0420 07:59:14.827713 140531019491072 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.05433128401637077, loss=0.04065629467368126
I0420 07:59:37.143542 140531053061888 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.04416023567318916, loss=0.043615762144327164
I0420 07:59:59.085844 140531019491072 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.035759683698415756, loss=0.04358936473727226
I0420 08:00:20.937890 140531053061888 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.05351170897483826, loss=0.0428495928645134
I0420 08:00:21.158580 140715718510400 spec.py:298] Evaluating on the training split.
I0420 08:01:30.275157 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 08:01:32.737261 140715718510400 spec.py:326] Evaluating on the test split.
I0420 08:01:35.144176 140715718510400 submission_runner.py:406] Time since start: 3403.82s, 	Step: 10602, 	{'train/accuracy': 0.9885146021842957, 'train/loss': 0.039012107998132706, 'train/mean_average_precision': 0.22613658766251438, 'validation/accuracy': 0.985609769821167, 'validation/loss': 0.04913591220974922, 'validation/mean_average_precision': 0.19235041937261566, 'validation/num_examples': 43793, 'test/accuracy': 0.9846512079238892, 'test/loss': 0.05177324265241623, 'test/mean_average_precision': 0.19038417377150685, 'test/num_examples': 43793, 'score': 2420.0250220298767, 'total_duration': 3403.822384119034, 'accumulated_submission_time': 2420.0250220298767, 'accumulated_eval_time': 982.8043982982635, 'accumulated_logging_time': 0.9081747531890869}
I0420 08:01:35.152098 140531019491072 logging_writer.py:48] [10602] accumulated_eval_time=982.804398, accumulated_logging_time=0.908175, accumulated_submission_time=2420.025022, global_step=10602, preemption_count=0, score=2420.025022, test/accuracy=0.984651, test/loss=0.051773, test/mean_average_precision=0.190384, test/num_examples=43793, total_duration=3403.822384, train/accuracy=0.988515, train/loss=0.039012, train/mean_average_precision=0.226137, validation/accuracy=0.985610, validation/loss=0.049136, validation/mean_average_precision=0.192350, validation/num_examples=43793
I0420 08:01:35.176127 140715718510400 checkpoints.py:356] Saving checkpoint at step: 10602
I0420 08:01:35.234192 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_10602
I0420 08:01:35.234382 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_10602.
I0420 08:01:57.062150 140531053061888 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.09239738434553146, loss=0.04287935420870781
I0420 08:02:19.199266 140531011098368 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05137905105948448, loss=0.04568197578191757
I0420 08:02:41.656539 140531053061888 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.06509979814291, loss=0.04312867671251297
I0420 08:03:04.069464 140531011098368 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.04286768659949303, loss=0.04296015575528145
I0420 08:03:26.111374 140531053061888 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03589250147342682, loss=0.04381365329027176
I0420 08:03:47.934421 140531011098368 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0724840760231018, loss=0.04600554332137108
I0420 08:04:09.861108 140531053061888 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.04022470861673355, loss=0.045745063573122025
I0420 08:04:31.863312 140531011098368 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04027562960982323, loss=0.04285233095288277
I0420 08:04:53.702855 140531053061888 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.038394033908843994, loss=0.04234332591295242
I0420 08:05:15.579860 140531011098368 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.045565180480480194, loss=0.044161807745695114
I0420 08:05:35.276758 140715718510400 spec.py:298] Evaluating on the training split.
I0420 08:06:43.785257 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 08:06:46.301198 140715718510400 spec.py:326] Evaluating on the test split.
I0420 08:06:48.691398 140715718510400 submission_runner.py:406] Time since start: 3717.37s, 	Step: 11690, 	{'train/accuracy': 0.9889081716537476, 'train/loss': 0.038084082305431366, 'train/mean_average_precision': 0.24480703500129747, 'validation/accuracy': 0.9859690070152283, 'validation/loss': 0.047997262328863144, 'validation/mean_average_precision': 0.2024867691331001, 'validation/num_examples': 43793, 'test/accuracy': 0.9850277304649353, 'test/loss': 0.050633564591407776, 'test/mean_average_precision': 0.20018639922018205, 'test/num_examples': 43793, 'score': 2660.0587224960327, 'total_duration': 3717.369601726532, 'accumulated_submission_time': 2660.0587224960327, 'accumulated_eval_time': 1056.2189991474152, 'accumulated_logging_time': 0.9985501766204834}
I0420 08:06:48.699637 140531053061888 logging_writer.py:48] [11690] accumulated_eval_time=1056.218999, accumulated_logging_time=0.998550, accumulated_submission_time=2660.058722, global_step=11690, preemption_count=0, score=2660.058722, test/accuracy=0.985028, test/loss=0.050634, test/mean_average_precision=0.200186, test/num_examples=43793, total_duration=3717.369602, train/accuracy=0.988908, train/loss=0.038084, train/mean_average_precision=0.244807, validation/accuracy=0.985969, validation/loss=0.047997, validation/mean_average_precision=0.202487, validation/num_examples=43793
I0420 08:06:48.723683 140715718510400 checkpoints.py:356] Saving checkpoint at step: 11690
I0420 08:06:48.786188 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_11690
I0420 08:06:48.786359 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_11690.
I0420 08:06:51.229433 140531011098368 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.051518432796001434, loss=0.045356228947639465
I0420 08:07:13.661071 140530793051904 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.045435287058353424, loss=0.0409364327788353
I0420 08:07:35.975827 140531011098368 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0502583310008049, loss=0.03978496044874191
I0420 08:07:58.039265 140715718510400 spec.py:298] Evaluating on the training split.
I0420 08:09:06.396750 140715718510400 spec.py:310] Evaluating on the validation split.
I0420 08:09:08.881850 140715718510400 spec.py:326] Evaluating on the test split.
I0420 08:09:11.285486 140715718510400 submission_runner.py:406] Time since start: 3859.96s, 	Step: 12000, 	{'train/accuracy': 0.9884368777275085, 'train/loss': 0.03870129957795143, 'train/mean_average_precision': 0.24925553042322776, 'validation/accuracy': 0.9854972958564758, 'validation/loss': 0.049520306289196014, 'validation/mean_average_precision': 0.20110033525902865, 'validation/num_examples': 43793, 'test/accuracy': 0.9845038056373596, 'test/loss': 0.052345987409353256, 'test/mean_average_precision': 0.19862680431608368, 'test/num_examples': 43793, 'score': 2729.308831214905, 'total_duration': 3859.9636952877045, 'accumulated_submission_time': 2729.308831214905, 'accumulated_eval_time': 1129.4651820659637, 'accumulated_logging_time': 1.0936229228973389}
I0420 08:09:11.293535 140530793051904 logging_writer.py:48] [12000] accumulated_eval_time=1129.465182, accumulated_logging_time=1.093623, accumulated_submission_time=2729.308831, global_step=12000, preemption_count=0, score=2729.308831, test/accuracy=0.984504, test/loss=0.052346, test/mean_average_precision=0.198627, test/num_examples=43793, total_duration=3859.963695, train/accuracy=0.988437, train/loss=0.038701, train/mean_average_precision=0.249256, validation/accuracy=0.985497, validation/loss=0.049520, validation/mean_average_precision=0.201100, validation/num_examples=43793
I0420 08:09:11.316995 140715718510400 checkpoints.py:356] Saving checkpoint at step: 12000
I0420 08:09:11.385201 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_12000
I0420 08:09:11.385396 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_12000.
I0420 08:09:11.392249 140531011098368 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=2729.308831
I0420 08:09:11.410610 140715718510400 checkpoints.py:356] Saving checkpoint at step: 12000
I0420 08:09:11.515986 140715718510400 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_12000
I0420 08:09:11.516163 140715718510400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/ogbg_jax/trial_1/checkpoint_12000.
I0420 08:09:11.642183 140715718510400 submission_runner.py:567] Tuning trial 1/1
I0420 08:09:11.642396 140715718510400 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0420 08:09:11.643501 140715718510400 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5763778686523438, 'train/loss': 0.6968065500259399, 'train/mean_average_precision': 0.023237913545750145, 'validation/accuracy': 0.5727439522743225, 'validation/loss': 0.6998342275619507, 'validation/mean_average_precision': 0.026464395493344436, 'validation/num_examples': 43793, 'test/accuracy': 0.5716279149055481, 'test/loss': 0.7015213370323181, 'test/mean_average_precision': 0.027864573736120092, 'test/num_examples': 43793, 'score': 18.890955448150635, 'total_duration': 235.63217091560364, 'accumulated_submission_time': 18.890955448150635, 'accumulated_eval_time': 216.74106001853943, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1045, {'train/accuracy': 0.986743688583374, 'train/loss': 0.054528165608644485, 'train/mean_average_precision': 0.03390355656744816, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06456995010375977, 'validation/mean_average_precision': 0.03696588391907003, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.06783725321292877, 'test/mean_average_precision': 0.03810085222859725, 'test/num_examples': 43793, 'score': 258.8908050060272, 'total_duration': 556.1845576763153, 'accumulated_submission_time': 258.8908050060272, 'accumulated_eval_time': 297.1742582321167, 'accumulated_logging_time': 0.11083030700683594, 'global_step': 1045, 'preemption_count': 0}), (2086, {'train/accuracy': 0.986762523651123, 'train/loss': 0.05191529169678688, 'train/mean_average_precision': 0.051916519807026794, 'validation/accuracy': 0.9841325283050537, 'validation/loss': 0.06155897676944733, 'validation/mean_average_precision': 0.053209505013883024, 'validation/num_examples': 43793, 'test/accuracy': 0.9831568002700806, 'test/loss': 0.06486614048480988, 'test/mean_average_precision': 0.05455546427448808, 'test/num_examples': 43793, 'score': 499.02963852882385, 'total_duration': 873.829252243042, 'accumulated_submission_time': 499.02963852882385, 'accumulated_eval_time': 374.5810604095459, 'accumulated_logging_time': 0.20139431953430176, 'global_step': 2086, 'preemption_count': 0}), (3120, {'train/accuracy': 0.9868236780166626, 'train/loss': 0.0498686321079731, 'train/mean_average_precision': 0.08384551563775214, 'validation/accuracy': 0.9841800332069397, 'validation/loss': 0.058765605092048645, 'validation/mean_average_precision': 0.07825618086213128, 'validation/num_examples': 43793, 'test/accuracy': 0.9832035899162292, 'test/loss': 0.06177526339888573, 'test/mean_average_precision': 0.08132213076054275, 'test/num_examples': 43793, 'score': 739.189546585083, 'total_duration': 1192.0096113681793, 'accumulated_submission_time': 739.189546585083, 'accumulated_eval_time': 452.4910366535187, 'accumulated_logging_time': 0.3034024238586426, 'global_step': 3120, 'preemption_count': 0}), (4157, {'train/accuracy': 0.9869639873504639, 'train/loss': 0.04759415239095688, 'train/mean_average_precision': 0.10388764169260686, 'validation/accuracy': 0.9842908382415771, 'validation/loss': 0.05688828229904175, 'validation/mean_average_precision': 0.10535924104926349, 'validation/num_examples': 43793, 'test/accuracy': 0.9832941293716431, 'test/loss': 0.06015659123659134, 'test/mean_average_precision': 0.10334512727657637, 'test/num_examples': 43793, 'score': 979.3141267299652, 'total_duration': 1508.5859277248383, 'accumulated_submission_time': 979.3141267299652, 'accumulated_eval_time': 528.8405935764313, 'accumulated_logging_time': 0.397291898727417, 'global_step': 4157, 'preemption_count': 0}), (5220, {'train/accuracy': 0.9875674247741699, 'train/loss': 0.044705040752887726, 'train/mean_average_precision': 0.13513406435917022, 'validation/accuracy': 0.9847674369812012, 'validation/loss': 0.054231345653533936, 'validation/mean_average_precision': 0.12262685458778906, 'validation/num_examples': 43793, 'test/accuracy': 0.9837843775749207, 'test/loss': 0.057103000581264496, 'test/mean_average_precision': 0.12704215520242798, 'test/num_examples': 43793, 'score': 1219.4351851940155, 'total_duration': 1825.3011920452118, 'accumulated_submission_time': 1219.4351851940155, 'accumulated_eval_time': 605.3397443294525, 'accumulated_logging_time': 0.48420143127441406, 'global_step': 5220, 'preemption_count': 0}), (6294, {'train/accuracy': 0.9875965714454651, 'train/loss': 0.04402443766593933, 'train/mean_average_precision': 0.15279966748618615, 'validation/accuracy': 0.9850187301635742, 'validation/loss': 0.05335491895675659, 'validation/mean_average_precision': 0.14226458631350392, 'validation/num_examples': 43793, 'test/accuracy': 0.9840270280838013, 'test/loss': 0.056335967034101486, 'test/mean_average_precision': 0.14457183059407552, 'test/num_examples': 43793, 'score': 1459.5598266124725, 'total_duration': 2140.3476927280426, 'accumulated_submission_time': 1459.5598266124725, 'accumulated_eval_time': 680.1673183441162, 'accumulated_logging_time': 0.5700342655181885, 'global_step': 6294, 'preemption_count': 0}), (7372, {'train/accuracy': 0.9878565073013306, 'train/loss': 0.042140401899814606, 'train/mean_average_precision': 0.1756673911590159, 'validation/accuracy': 0.9850901365280151, 'validation/loss': 0.051629625260829926, 'validation/mean_average_precision': 0.15182327645475896, 'validation/num_examples': 43793, 'test/accuracy': 0.984144926071167, 'test/loss': 0.054758038371801376, 'test/mean_average_precision': 0.1567634018232528, 'test/num_examples': 43793, 'score': 1699.7182536125183, 'total_duration': 2456.4669349193573, 'accumulated_submission_time': 1699.7182536125183, 'accumulated_eval_time': 756.0347700119019, 'accumulated_logging_time': 0.6549685001373291, 'global_step': 7372, 'preemption_count': 0}), (8448, {'train/accuracy': 0.9881747961044312, 'train/loss': 0.04172274470329285, 'train/mean_average_precision': 0.1882117710003608, 'validation/accuracy': 0.9852391481399536, 'validation/loss': 0.05171152949333191, 'validation/mean_average_precision': 0.16648324518653723, 'validation/num_examples': 43793, 'test/accuracy': 0.9842401146888733, 'test/loss': 0.05464910343289375, 'test/mean_average_precision': 0.16777060997363255, 'test/num_examples': 43793, 'score': 1939.8336091041565, 'total_duration': 2774.237932443619, 'accumulated_submission_time': 1939.8336091041565, 'accumulated_eval_time': 833.5952479839325, 'accumulated_logging_time': 0.7417163848876953, 'global_step': 8448, 'preemption_count': 0}), (9519, {'train/accuracy': 0.9882425665855408, 'train/loss': 0.04041565954685211, 'train/mean_average_precision': 0.2074558147739693, 'validation/accuracy': 0.9852866530418396, 'validation/loss': 0.05120415240526199, 'validation/mean_average_precision': 0.1761443908103017, 'validation/num_examples': 43793, 'test/accuracy': 0.9843248128890991, 'test/loss': 0.05431773513555527, 'test/mean_average_precision': 0.17950107227627746, 'test/num_examples': 43793, 'score': 2180.0237913131714, 'total_duration': 3089.742404937744, 'accumulated_submission_time': 2180.0237913131714, 'accumulated_eval_time': 908.8188443183899, 'accumulated_logging_time': 0.8236253261566162, 'global_step': 9519, 'preemption_count': 0}), (10602, {'train/accuracy': 0.9885146021842957, 'train/loss': 0.039012107998132706, 'train/mean_average_precision': 0.22613658766251438, 'validation/accuracy': 0.985609769821167, 'validation/loss': 0.04913591220974922, 'validation/mean_average_precision': 0.19235041937261566, 'validation/num_examples': 43793, 'test/accuracy': 0.9846512079238892, 'test/loss': 0.05177324265241623, 'test/mean_average_precision': 0.19038417377150685, 'test/num_examples': 43793, 'score': 2420.0250220298767, 'total_duration': 3403.822384119034, 'accumulated_submission_time': 2420.0250220298767, 'accumulated_eval_time': 982.8043982982635, 'accumulated_logging_time': 0.9081747531890869, 'global_step': 10602, 'preemption_count': 0}), (11690, {'train/accuracy': 0.9889081716537476, 'train/loss': 0.038084082305431366, 'train/mean_average_precision': 0.24480703500129747, 'validation/accuracy': 0.9859690070152283, 'validation/loss': 0.047997262328863144, 'validation/mean_average_precision': 0.2024867691331001, 'validation/num_examples': 43793, 'test/accuracy': 0.9850277304649353, 'test/loss': 0.050633564591407776, 'test/mean_average_precision': 0.20018639922018205, 'test/num_examples': 43793, 'score': 2660.0587224960327, 'total_duration': 3717.369601726532, 'accumulated_submission_time': 2660.0587224960327, 'accumulated_eval_time': 1056.2189991474152, 'accumulated_logging_time': 0.9985501766204834, 'global_step': 11690, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9884368777275085, 'train/loss': 0.03870129957795143, 'train/mean_average_precision': 0.24925553042322776, 'validation/accuracy': 0.9854972958564758, 'validation/loss': 0.049520306289196014, 'validation/mean_average_precision': 0.20110033525902865, 'validation/num_examples': 43793, 'test/accuracy': 0.9845038056373596, 'test/loss': 0.052345987409353256, 'test/mean_average_precision': 0.19862680431608368, 'test/num_examples': 43793, 'score': 2729.308831214905, 'total_duration': 3859.9636952877045, 'accumulated_submission_time': 2729.308831214905, 'accumulated_eval_time': 1129.4651820659637, 'accumulated_logging_time': 1.0936229228973389, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0420 08:09:11.643654 140715718510400 submission_runner.py:570] Timing: 2729.308831214905
I0420 08:09:11.643707 140715718510400 submission_runner.py:571] ====================
I0420 08:09:11.643821 140715718510400 submission_runner.py:631] Final ogbg score: 2729.308831214905
