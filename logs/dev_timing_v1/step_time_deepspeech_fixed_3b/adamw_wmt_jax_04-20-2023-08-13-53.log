I0420 08:14:15.739382 139765933279040 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax.
I0420 08:14:15.802345 139765933279040 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 08:14:16.597920 139765933279040 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0420 08:14:16.598506 139765933279040 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 08:14:16.602577 139765933279040 submission_runner.py:528] Using RNG seed 74809669
I0420 08:14:19.457516 139765933279040 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 08:14:19.457714 139765933279040 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1.
I0420 08:14:19.457935 139765933279040 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/hparams.json.
I0420 08:14:19.579593 139765933279040 submission_runner.py:232] Initializing dataset.
I0420 08:14:19.589324 139765933279040 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:14:19.592667 139765933279040 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:14:19.592795 139765933279040 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:14:19.707627 139765933279040 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:14:21.637490 139765933279040 submission_runner.py:239] Initializing model.
I0420 08:14:33.395473 139765933279040 submission_runner.py:249] Initializing optimizer.
I0420 08:14:34.308571 139765933279040 submission_runner.py:256] Initializing metrics bundle.
I0420 08:14:34.308760 139765933279040 submission_runner.py:273] Initializing checkpoint and logger.
I0420 08:14:34.309856 139765933279040 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1 with prefix checkpoint_
I0420 08:14:34.310093 139765933279040 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 08:14:34.310154 139765933279040 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 08:14:35.120307 139765933279040 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/meta_data_0.json.
I0420 08:14:35.121218 139765933279040 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/flags_0.json.
I0420 08:14:35.125486 139765933279040 submission_runner.py:309] Starting training loop.
I0420 08:15:08.317416 139589951940352 logging_writer.py:48] [0] global_step=0, grad_norm=5.592487335205078, loss=11.045881271362305
I0420 08:15:08.330746 139765933279040 spec.py:298] Evaluating on the training split.
I0420 08:15:08.333471 139765933279040 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:15:08.336385 139765933279040 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:15:08.336495 139765933279040 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:15:08.365864 139765933279040 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:15:16.474207 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 08:20:23.219172 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 08:20:23.223029 139765933279040 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:20:23.226148 139765933279040 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:20:23.226253 139765933279040 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:20:23.255469 139765933279040 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:20:30.526748 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 08:25:29.674523 139765933279040 spec.py:326] Evaluating on the test split.
I0420 08:25:29.676891 139765933279040 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:25:29.679503 139765933279040 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:25:29.679608 139765933279040 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:25:29.708032 139765933279040 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:25:36.673534 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 08:30:30.318379 139765933279040 submission_runner.py:406] Time since start: 955.19s, 	Step: 1, 	{'train/accuracy': 0.000615897006355226, 'train/loss': 11.054659843444824, 'train/bleu': 1.9168886195025588e-10, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.054000854492188, 'validation/bleu': 1.5614406590780203e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.057171821594238, 'test/bleu': 4.3370890740967724e-10, 'test/num_examples': 3003, 'score': 33.205100536346436, 'total_duration': 955.1927862167358, 'accumulated_submission_time': 33.205100536346436, 'accumulated_eval_time': 921.9875423908234, 'accumulated_logging_time': 0}
I0420 08:30:30.335331 139578685536000 logging_writer.py:48] [1] accumulated_eval_time=921.987542, accumulated_logging_time=0, accumulated_submission_time=33.205101, global_step=1, preemption_count=0, score=33.205101, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.057172, test/num_examples=3003, total_duration=955.192786, train/accuracy=0.000616, train/bleu=0.000000, train/loss=11.054660, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.054001, validation/num_examples=3000
I0420 08:30:31.269013 139765933279040 checkpoints.py:356] Saving checkpoint at step: 1
I0420 08:30:34.642214 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_1
I0420 08:30:34.645673 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_1.
I0420 08:31:11.027394 139578693928704 logging_writer.py:48] [100] global_step=100, grad_norm=0.23944884538650513, loss=8.580645561218262
I0420 08:31:47.343114 139578794641152 logging_writer.py:48] [200] global_step=200, grad_norm=0.7663446664810181, loss=7.999236583709717
I0420 08:32:23.751681 139578693928704 logging_writer.py:48] [300] global_step=300, grad_norm=0.7414801716804504, loss=7.4604949951171875
I0420 08:33:00.122189 139578794641152 logging_writer.py:48] [400] global_step=400, grad_norm=0.7040666341781616, loss=7.201354503631592
I0420 08:33:36.542824 139578693928704 logging_writer.py:48] [500] global_step=500, grad_norm=0.7153853178024292, loss=6.819643497467041
I0420 08:34:12.913370 139578794641152 logging_writer.py:48] [600] global_step=600, grad_norm=0.7003448009490967, loss=6.635085582733154
I0420 08:34:49.236471 139578693928704 logging_writer.py:48] [700] global_step=700, grad_norm=0.5769108533859253, loss=6.368951320648193
I0420 08:35:25.650313 139578794641152 logging_writer.py:48] [800] global_step=800, grad_norm=0.7279983162879944, loss=6.212602615356445
I0420 08:36:02.047640 139578693928704 logging_writer.py:48] [900] global_step=900, grad_norm=0.5654245018959045, loss=5.999192714691162
I0420 08:36:38.467725 139578794641152 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5052706003189087, loss=5.871190547943115
I0420 08:37:14.886353 139578693928704 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7683737874031067, loss=5.704143524169922
I0420 08:37:51.299999 139578794641152 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.630002498626709, loss=5.509289741516113
I0420 08:38:27.663214 139578693928704 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5884917378425598, loss=5.493621826171875
I0420 08:39:04.101929 139578794641152 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.655880868434906, loss=5.3896989822387695
I0420 08:39:40.492419 139578693928704 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5194910764694214, loss=5.071414947509766
I0420 08:40:16.911193 139578794641152 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8493571877479553, loss=5.062045574188232
I0420 08:40:53.300791 139578693928704 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6080668568611145, loss=4.818214416503906
I0420 08:41:29.700239 139578794641152 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.1982474327087402, loss=5.372492790222168
I0420 08:42:06.045831 139578693928704 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.12193256616592407, loss=7.3416748046875
I0420 08:42:42.310372 139578794641152 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0902518555521965, loss=7.095283508300781
I0420 08:43:18.690608 139578693928704 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.19561873376369476, loss=6.63743257522583
I0420 08:43:55.029423 139578794641152 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5452076196670532, loss=6.499165058135986
I0420 08:44:31.399814 139578693928704 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.2299608439207077, loss=6.232344150543213
I0420 08:44:34.744389 139765933279040 spec.py:298] Evaluating on the training split.
I0420 08:44:37.751356 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 08:47:20.986738 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 08:47:23.659043 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 08:49:53.271456 139765933279040 spec.py:326] Evaluating on the test split.
I0420 08:49:55.981397 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 08:52:25.968873 139765933279040 submission_runner.py:406] Time since start: 2270.84s, 	Step: 2311, 	{'train/accuracy': 0.21024225652217865, 'train/loss': 5.50177001953125, 'train/bleu': 0.25685936715641683, 'validation/accuracy': 0.18808197975158691, 'validation/loss': 5.735093593597412, 'validation/bleu': 0.09924330320980963, 'validation/num_examples': 3000, 'test/accuracy': 0.18034978210926056, 'test/loss': 5.978095054626465, 'test/bleu': 0.09587588221693083, 'test/num_examples': 3003, 'score': 873.2738935947418, 'total_duration': 2270.843296289444, 'accumulated_submission_time': 873.2738935947418, 'accumulated_eval_time': 1393.2119617462158, 'accumulated_logging_time': 4.329891681671143}
I0420 08:52:25.976434 139578794641152 logging_writer.py:48] [2311] accumulated_eval_time=1393.211962, accumulated_logging_time=4.329892, accumulated_submission_time=873.273894, global_step=2311, preemption_count=0, score=873.273894, test/accuracy=0.180350, test/bleu=0.095876, test/loss=5.978095, test/num_examples=3003, total_duration=2270.843296, train/accuracy=0.210242, train/bleu=0.256859, train/loss=5.501770, validation/accuracy=0.188082, validation/bleu=0.099243, validation/loss=5.735094, validation/num_examples=3000
I0420 08:52:26.897620 139765933279040 checkpoints.py:356] Saving checkpoint at step: 2311
I0420 08:52:30.238112 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_2311
I0420 08:52:30.241516 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_2311.
I0420 08:53:02.935795 139578693928704 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.1556878238916397, loss=6.061520099639893
I0420 08:53:39.287154 139578769463040 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.17212645709514618, loss=5.986751556396484
I0420 08:54:15.648579 139578693928704 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2941381633281708, loss=5.745772838592529
I0420 08:54:51.988690 139578769463040 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.6996796131134033, loss=8.353893280029297
I0420 08:55:28.355724 139578693928704 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.32486242055892944, loss=5.802282333374023
I0420 08:56:04.690638 139578769463040 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.275063157081604, loss=5.7634477615356445
I0420 08:56:41.084926 139578693928704 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.17108096182346344, loss=5.806958198547363
I0420 08:57:17.432548 139578769463040 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2621334195137024, loss=5.648245811462402
I0420 08:57:53.789707 139578693928704 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.2565331757068634, loss=5.665074348449707
I0420 08:58:30.118381 139578769463040 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.24688388407230377, loss=5.589369773864746
I0420 08:59:06.473992 139578693928704 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2213180810213089, loss=5.4799089431762695
I0420 08:59:42.831677 139578769463040 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.24023640155792236, loss=5.430930137634277
I0420 09:00:19.275502 139578693928704 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.21177934110164642, loss=5.412787914276123
I0420 09:00:55.619166 139578769463040 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2709309160709381, loss=5.3591532707214355
I0420 09:01:31.963250 139578693928704 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.27141568064689636, loss=5.279420852661133
I0420 09:02:08.286238 139578769463040 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.24215970933437347, loss=5.201033115386963
I0420 09:02:44.626978 139578693928704 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3036966323852539, loss=5.011817932128906
I0420 09:03:20.954360 139578769463040 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.16748763620853424, loss=5.557652950286865
I0420 09:03:57.362400 139578693928704 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3333304524421692, loss=5.092364311218262
I0420 09:04:33.760473 139578769463040 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.30190858244895935, loss=4.713075160980225
I0420 09:05:10.101554 139578693928704 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3595079481601715, loss=4.616866111755371
I0420 09:05:46.444925 139578769463040 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.36717620491981506, loss=4.522586345672607
I0420 09:06:22.814930 139578693928704 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5206405520439148, loss=4.626321315765381
I0420 09:06:30.524072 139765933279040 spec.py:298] Evaluating on the training split.
I0420 09:06:33.526766 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:10:04.686415 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 09:10:07.350618 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:13:32.750622 139765933279040 spec.py:326] Evaluating on the test split.
I0420 09:13:35.461339 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:17:02.149664 139765933279040 submission_runner.py:406] Time since start: 3747.02s, 	Step: 4623, 	{'train/accuracy': 0.4133537709712982, 'train/loss': 3.619962215423584, 'train/bleu': 11.800149874083603, 'validation/accuracy': 0.393187940120697, 'validation/loss': 3.8119516372680664, 'validation/bleu': 7.760326751227607, 'validation/num_examples': 3000, 'test/accuracy': 0.3797455132007599, 'test/loss': 4.003962516784668, 'test/bleu': 6.226705464222686, 'test/num_examples': 3003, 'score': 1713.5280511379242, 'total_duration': 3747.0240755081177, 'accumulated_submission_time': 1713.5280511379242, 'accumulated_eval_time': 2024.8374953269958, 'accumulated_logging_time': 8.604756116867065}
I0420 09:17:02.158009 139578769463040 logging_writer.py:48] [4623] accumulated_eval_time=2024.837495, accumulated_logging_time=8.604756, accumulated_submission_time=1713.528051, global_step=4623, preemption_count=0, score=1713.528051, test/accuracy=0.379746, test/bleu=6.226705, test/loss=4.003963, test/num_examples=3003, total_duration=3747.024076, train/accuracy=0.413354, train/bleu=11.800150, train/loss=3.619962, validation/accuracy=0.393188, validation/bleu=7.760327, validation/loss=3.811952, validation/num_examples=3000
I0420 09:17:03.070626 139765933279040 checkpoints.py:356] Saving checkpoint at step: 4623
I0420 09:17:06.331713 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_4623
I0420 09:17:06.335187 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_4623.
I0420 09:17:34.667801 139578693928704 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.632190465927124, loss=4.61541748046875
I0420 09:18:11.044178 139578752677632 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7154611945152283, loss=4.757421493530273
I0420 09:18:47.444658 139578693928704 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7522814273834229, loss=4.7382636070251465
I0420 09:19:23.796742 139578752677632 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9887843728065491, loss=4.7140631675720215
I0420 09:20:00.102174 139578693928704 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.5401833057403564, loss=4.886900424957275
I0420 09:20:36.474942 139578752677632 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.72544527053833, loss=4.836578369140625
I0420 09:21:12.780088 139578693928704 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.7774218320846558, loss=4.739493370056152
I0420 09:21:49.139516 139578752677632 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.284365177154541, loss=4.843482494354248
I0420 09:22:25.493179 139578693928704 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.418704867362976, loss=4.786419868469238
I0420 09:23:01.817639 139578752677632 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.5756056308746338, loss=4.770605087280273
I0420 09:23:38.162560 139578693928704 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9917452335357666, loss=4.773374080657959
I0420 09:24:14.481830 139578752677632 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.7520828247070312, loss=4.841894149780273
I0420 09:24:50.853454 139578693928704 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.8340641260147095, loss=4.771942138671875
I0420 09:25:27.179837 139578752677632 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.714934825897217, loss=4.832841396331787
I0420 09:26:03.544469 139578693928704 logging_writer.py:48] [6100] global_step=6100, grad_norm=13.545248031616211, loss=4.804695129394531
I0420 09:26:39.861514 139578752677632 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.057237148284912, loss=4.815570831298828
I0420 09:27:16.232009 139578693928704 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.3071095943450928, loss=4.815147876739502
I0420 09:27:52.608655 139578752677632 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.0585808753967285, loss=4.826054573059082
I0420 09:28:28.951529 139578693928704 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.4851975440979, loss=4.850265026092529
I0420 09:29:05.276992 139578752677632 logging_writer.py:48] [6600] global_step=6600, grad_norm=5.677457332611084, loss=4.849627494812012
I0420 09:29:41.556891 139578693928704 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.8192641735076904, loss=4.927304744720459
I0420 09:30:17.935102 139578752677632 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.718393802642822, loss=4.857279300689697
I0420 09:30:54.230128 139578693928704 logging_writer.py:48] [6900] global_step=6900, grad_norm=10.776947021484375, loss=4.697281837463379
I0420 09:31:06.645371 139765933279040 spec.py:298] Evaluating on the training split.
I0420 09:31:09.658284 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:34:55.478867 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 09:34:58.128149 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:38:11.949973 139765933279040 spec.py:326] Evaluating on the test split.
I0420 09:38:14.656705 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:41:49.876017 139765933279040 submission_runner.py:406] Time since start: 5234.75s, 	Step: 6936, 	{'train/accuracy': 0.3788565397262573, 'train/loss': 3.768223524093628, 'train/bleu': 7.961753784835494, 'validation/accuracy': 0.3484395742416382, 'validation/loss': 4.062211513519287, 'validation/bleu': 4.626484092501946, 'validation/num_examples': 3000, 'test/accuracy': 0.3286386728286743, 'test/loss': 4.304903030395508, 'test/bleu': 3.614604692457374, 'test/num_examples': 3003, 'score': 2553.8112738132477, 'total_duration': 5234.750430822372, 'accumulated_submission_time': 2553.8112738132477, 'accumulated_eval_time': 2668.068067073822, 'accumulated_logging_time': 12.792430877685547}
I0420 09:41:49.884115 139578752677632 logging_writer.py:48] [6936] accumulated_eval_time=2668.068067, accumulated_logging_time=12.792431, accumulated_submission_time=2553.811274, global_step=6936, preemption_count=0, score=2553.811274, test/accuracy=0.328639, test/bleu=3.614605, test/loss=4.304903, test/num_examples=3003, total_duration=5234.750431, train/accuracy=0.378857, train/bleu=7.961754, train/loss=3.768224, validation/accuracy=0.348440, validation/bleu=4.626484, validation/loss=4.062212, validation/num_examples=3000
I0420 09:41:50.788781 139765933279040 checkpoints.py:356] Saving checkpoint at step: 6936
I0420 09:41:54.029983 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_6936
I0420 09:41:54.033598 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_6936.
I0420 09:42:17.628999 139578693928704 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.26145339012146, loss=4.7979254722595215
I0420 09:42:53.967650 139578744284928 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.578428268432617, loss=4.781601428985596
I0420 09:43:30.295006 139578693928704 logging_writer.py:48] [7200] global_step=7200, grad_norm=5.820786952972412, loss=4.791754722595215
I0420 09:44:06.629496 139578744284928 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.756075143814087, loss=4.768319129943848
I0420 09:44:42.976111 139578693928704 logging_writer.py:48] [7400] global_step=7400, grad_norm=6.167449951171875, loss=4.662453651428223
I0420 09:45:19.298888 139578744284928 logging_writer.py:48] [7500] global_step=7500, grad_norm=19.005170822143555, loss=4.696568489074707
I0420 09:45:55.606614 139578693928704 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.820643901824951, loss=4.744966983795166
I0420 09:46:31.909638 139578744284928 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.3222241401672363, loss=4.755997180938721
I0420 09:47:08.248213 139578693928704 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.7778470516204834, loss=4.754580020904541
I0420 09:47:44.527352 139578744284928 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.0547492504119873, loss=4.7439422607421875
I0420 09:48:20.874932 139578693928704 logging_writer.py:48] [8000] global_step=8000, grad_norm=24.597341537475586, loss=4.7310099601745605
I0420 09:48:57.173388 139578744284928 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.0189805030822754, loss=4.6934428215026855
I0420 09:49:33.467204 139578693928704 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.6925361156463623, loss=4.656855583190918
I0420 09:50:09.748678 139578744284928 logging_writer.py:48] [8300] global_step=8300, grad_norm=13.551323890686035, loss=4.672481536865234
I0420 09:50:46.081123 139578693928704 logging_writer.py:48] [8400] global_step=8400, grad_norm=6.507411003112793, loss=4.647577285766602
I0420 09:51:22.384908 139578744284928 logging_writer.py:48] [8500] global_step=8500, grad_norm=8.87037467956543, loss=4.658117294311523
I0420 09:51:58.720826 139578693928704 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.1866202354431152, loss=4.725697994232178
I0420 09:52:35.071539 139578744284928 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.672760248184204, loss=4.6625590324401855
I0420 09:53:11.391683 139578693928704 logging_writer.py:48] [8800] global_step=8800, grad_norm=10.962725639343262, loss=4.626927852630615
I0420 09:53:47.721228 139578744284928 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.5773887634277344, loss=4.6502556800842285
I0420 09:54:24.016493 139578693928704 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.6241910457611084, loss=4.58695650100708
I0420 09:55:00.349719 139578744284928 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.4491748809814453, loss=4.614639759063721
I0420 09:55:36.627879 139578693928704 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.860379219055176, loss=4.700982570648193
I0420 09:55:54.127392 139765933279040 spec.py:298] Evaluating on the training split.
I0420 09:55:57.136688 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 09:59:54.841111 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 09:59:57.515742 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:03:53.803353 139765933279040 spec.py:326] Evaluating on the test split.
I0420 10:03:56.509207 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:08:17.479340 139765933279040 submission_runner.py:406] Time since start: 6822.35s, 	Step: 9250, 	{'train/accuracy': 0.3840128779411316, 'train/loss': 3.7115955352783203, 'train/bleu': 7.043833681360767, 'validation/accuracy': 0.35013824701309204, 'validation/loss': 4.027622699737549, 'validation/bleu': 3.723412807142296, 'validation/num_examples': 3000, 'test/accuracy': 0.3317762017250061, 'test/loss': 4.250222206115723, 'test/bleu': 2.6174909442258842, 'test/num_examples': 3003, 'score': 3393.876946210861, 'total_duration': 6822.353744268417, 'accumulated_submission_time': 3393.876946210861, 'accumulated_eval_time': 3411.4199526309967, 'accumulated_logging_time': 16.952259063720703}
I0420 10:08:17.488157 139578744284928 logging_writer.py:48] [9250] accumulated_eval_time=3411.419953, accumulated_logging_time=16.952259, accumulated_submission_time=3393.876946, global_step=9250, preemption_count=0, score=3393.876946, test/accuracy=0.331776, test/bleu=2.617491, test/loss=4.250222, test/num_examples=3003, total_duration=6822.353744, train/accuracy=0.384013, train/bleu=7.043834, train/loss=3.711596, validation/accuracy=0.350138, validation/bleu=3.723413, validation/loss=4.027623, validation/num_examples=3000
I0420 10:08:18.404377 139765933279040 checkpoints.py:356] Saving checkpoint at step: 9250
I0420 10:08:21.677845 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_9250
I0420 10:08:21.681203 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_9250.
I0420 10:08:40.159236 139578693928704 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.6626906394958496, loss=4.555324554443359
I0420 10:09:16.410891 139578735892224 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.9399867057800293, loss=4.601311683654785
I0420 10:09:52.688812 139578693928704 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.0344133377075195, loss=4.567310333251953
I0420 10:10:28.997793 139578735892224 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.729982852935791, loss=4.568082332611084
I0420 10:11:05.306341 139578693928704 logging_writer.py:48] [9700] global_step=9700, grad_norm=5.3867058753967285, loss=4.570436000823975
I0420 10:11:41.627544 139578735892224 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.3207926750183105, loss=4.563695907592773
I0420 10:12:17.886557 139578693928704 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.9548401832580566, loss=4.63165283203125
I0420 10:12:54.197876 139578735892224 logging_writer.py:48] [10000] global_step=10000, grad_norm=6.022009372711182, loss=4.674738883972168
I0420 10:13:30.527825 139578693928704 logging_writer.py:48] [10100] global_step=10100, grad_norm=10.894556045532227, loss=4.534884452819824
I0420 10:14:06.860031 139578735892224 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.4138343334198, loss=4.59151029586792
I0420 10:14:43.148217 139578693928704 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.4033756256103516, loss=4.544277191162109
I0420 10:15:19.486510 139578735892224 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.452487468719482, loss=4.547751426696777
I0420 10:15:55.783614 139578693928704 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.6881659030914307, loss=4.526342391967773
I0420 10:16:32.087121 139578735892224 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9180982112884521, loss=4.492892742156982
I0420 10:17:08.392283 139578693928704 logging_writer.py:48] [10700] global_step=10700, grad_norm=13.517539024353027, loss=4.4985671043396
I0420 10:17:44.695774 139578735892224 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.259438991546631, loss=4.4251179695129395
I0420 10:18:21.051389 139578693928704 logging_writer.py:48] [10900] global_step=10900, grad_norm=5.81256103515625, loss=4.527078628540039
I0420 10:18:57.382646 139578735892224 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.0701041221618652, loss=4.518108367919922
I0420 10:19:33.725296 139578693928704 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.3778514862060547, loss=4.45189094543457
I0420 10:20:10.023597 139578735892224 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.2231926918029785, loss=4.439874649047852
I0420 10:20:46.355109 139578693928704 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.5335192680358887, loss=4.4895195960998535
I0420 10:21:22.628207 139578735892224 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.6625990867614746, loss=4.474283218383789
I0420 10:21:58.927037 139578693928704 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.0984723567962646, loss=4.445733547210693
I0420 10:22:21.848205 139765933279040 spec.py:298] Evaluating on the training split.
I0420 10:22:24.858723 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:25:53.090840 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 10:25:55.766868 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:29:17.832043 139765933279040 spec.py:326] Evaluating on the test split.
I0420 10:29:20.539151 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:32:43.080619 139765933279040 submission_runner.py:406] Time since start: 8287.96s, 	Step: 11565, 	{'train/accuracy': 0.42044195532798767, 'train/loss': 3.413877010345459, 'train/bleu': 9.803393211492079, 'validation/accuracy': 0.39145204424858093, 'validation/loss': 3.6946914196014404, 'validation/bleu': 5.675816986702896, 'validation/num_examples': 3000, 'test/accuracy': 0.3727964758872986, 'test/loss': 3.9127721786499023, 'test/bleu': 4.368229661811422, 'test/num_examples': 3003, 'score': 4234.016389131546, 'total_duration': 8287.955007314682, 'accumulated_submission_time': 4234.016389131546, 'accumulated_eval_time': 4032.652271270752, 'accumulated_logging_time': 21.15631651878357}
I0420 10:32:43.089186 139578735892224 logging_writer.py:48] [11565] accumulated_eval_time=4032.652271, accumulated_logging_time=21.156317, accumulated_submission_time=4234.016389, global_step=11565, preemption_count=0, score=4234.016389, test/accuracy=0.372796, test/bleu=4.368230, test/loss=3.912772, test/num_examples=3003, total_duration=8287.955007, train/accuracy=0.420442, train/bleu=9.803393, train/loss=3.413877, validation/accuracy=0.391452, validation/bleu=5.675817, validation/loss=3.694691, validation/num_examples=3000
I0420 10:32:44.009547 139765933279040 checkpoints.py:356] Saving checkpoint at step: 11565
I0420 10:32:47.240990 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_11565
I0420 10:32:47.244477 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_11565.
I0420 10:33:00.314377 139578693928704 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.646496295928955, loss=4.358732223510742
I0420 10:33:36.586551 139578727499520 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.055227279663086, loss=4.3860273361206055
I0420 10:34:12.871367 139578693928704 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.511866569519043, loss=4.377303600311279
I0420 10:34:49.124631 139578727499520 logging_writer.py:48] [11900] global_step=11900, grad_norm=5.827258586883545, loss=4.417721748352051
I0420 10:35:25.390097 139578693928704 logging_writer.py:48] [12000] global_step=12000, grad_norm=7.3468780517578125, loss=4.417940139770508
I0420 10:36:01.669724 139578727499520 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.7936991453170776, loss=4.478675365447998
I0420 10:36:37.970255 139578693928704 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.185739517211914, loss=4.320671081542969
I0420 10:37:14.298911 139578727499520 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.0475969314575195, loss=4.364874839782715
I0420 10:37:50.574380 139578693928704 logging_writer.py:48] [12400] global_step=12400, grad_norm=5.480888843536377, loss=4.284669399261475
I0420 10:38:26.867080 139578727499520 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.3374584913253784, loss=4.354767322540283
I0420 10:39:03.203209 139578693928704 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.3874382972717285, loss=4.339271545410156
I0420 10:39:39.540354 139578727499520 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.3894292116165161, loss=4.266810417175293
I0420 10:40:15.833443 139578693928704 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.2233211994171143, loss=4.327792167663574
I0420 10:40:52.133804 139578727499520 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.3328133821487427, loss=4.275516986846924
I0420 10:41:28.426589 139578693928704 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.306686878204346, loss=4.292299270629883
I0420 10:42:04.725298 139578727499520 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6421245336532593, loss=4.232675075531006
I0420 10:42:41.010449 139578693928704 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.7773783206939697, loss=4.245269775390625
I0420 10:43:17.331555 139578727499520 logging_writer.py:48] [13300] global_step=13300, grad_norm=7.196547508239746, loss=4.255683898925781
I0420 10:43:53.585320 139578693928704 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5833311080932617, loss=4.250405311584473
I0420 10:44:29.854231 139578727499520 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.1057395935058594, loss=4.2044806480407715
I0420 10:45:06.131035 139578693928704 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.3152967691421509, loss=4.203697204589844
I0420 10:45:42.441581 139578727499520 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.9608412981033325, loss=4.247079372406006
I0420 10:46:18.758148 139578693928704 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.649479389190674, loss=4.252749443054199
I0420 10:46:47.506144 139765933279040 spec.py:298] Evaluating on the training split.
I0420 10:46:50.507669 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:50:38.307081 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 10:50:40.966924 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:54:05.961732 139765933279040 spec.py:326] Evaluating on the test split.
I0420 10:54:08.664029 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 10:57:42.145316 139765933279040 submission_runner.py:406] Time since start: 9787.02s, 	Step: 13881, 	{'train/accuracy': 0.45285695791244507, 'train/loss': 3.17085599899292, 'train/bleu': 12.850105501676524, 'validation/accuracy': 0.427235871553421, 'validation/loss': 3.433032512664795, 'validation/bleu': 7.169095466262899, 'validation/num_examples': 3000, 'test/accuracy': 0.4134216606616974, 'test/loss': 3.600409746170044, 'test/bleu': 6.0505022560885475, 'test/num_examples': 3003, 'score': 5074.251702547073, 'total_duration': 9787.019736766815, 'accumulated_submission_time': 5074.251702547073, 'accumulated_eval_time': 4687.291373252869, 'accumulated_logging_time': 25.32247543334961}
I0420 10:57:42.154151 139578727499520 logging_writer.py:48] [13881] accumulated_eval_time=4687.291373, accumulated_logging_time=25.322475, accumulated_submission_time=5074.251703, global_step=13881, preemption_count=0, score=5074.251703, test/accuracy=0.413422, test/bleu=6.050502, test/loss=3.600410, test/num_examples=3003, total_duration=9787.019737, train/accuracy=0.452857, train/bleu=12.850106, train/loss=3.170856, validation/accuracy=0.427236, validation/bleu=7.169095, validation/loss=3.433033, validation/num_examples=3000
I0420 10:57:43.073354 139765933279040 checkpoints.py:356] Saving checkpoint at step: 13881
I0420 10:57:46.316819 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_13881
I0420 10:57:46.320322 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_13881.
I0420 10:57:53.585102 139578693928704 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.4325921535491943, loss=4.235084533691406
I0420 10:58:29.868605 139578719106816 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.978527545928955, loss=4.167141914367676
I0420 10:59:06.168232 139578693928704 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.9283593893051147, loss=4.215173244476318
I0420 10:59:42.460685 139578719106816 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.3493859767913818, loss=4.097046375274658
I0420 11:00:18.754708 139578693928704 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.9323976039886475, loss=4.128570556640625
I0420 11:00:55.075996 139578719106816 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.3655153512954712, loss=4.134160041809082
I0420 11:01:31.379307 139578693928704 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.807462215423584, loss=4.160726547241211
I0420 11:02:07.711215 139578719106816 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.7545523643493652, loss=4.093389987945557
I0420 11:02:43.987208 139578693928704 logging_writer.py:48] [14700] global_step=14700, grad_norm=10.229060173034668, loss=4.070930480957031
I0420 11:03:20.272895 139578719106816 logging_writer.py:48] [14800] global_step=14800, grad_norm=9.125639915466309, loss=4.109584331512451
I0420 11:03:56.562152 139578693928704 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.6292240619659424, loss=4.089250564575195
I0420 11:04:32.836498 139578719106816 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.0912909507751465, loss=4.157584190368652
I0420 11:05:09.119661 139578693928704 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.3880717754364014, loss=4.167080879211426
I0420 11:05:45.401201 139578719106816 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.7128009796142578, loss=4.1135945320129395
I0420 11:06:21.683667 139578693928704 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.9882001876831055, loss=4.16885232925415
I0420 11:06:57.937852 139578719106816 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.187251329421997, loss=4.116224765777588
I0420 11:07:34.197287 139578693928704 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.0924181938171387, loss=4.165619373321533
I0420 11:08:10.469342 139578719106816 logging_writer.py:48] [15600] global_step=15600, grad_norm=10.818314552307129, loss=4.0627241134643555
I0420 11:08:46.769302 139578693928704 logging_writer.py:48] [15700] global_step=15700, grad_norm=20.67926025390625, loss=4.134795188903809
I0420 11:09:23.048865 139578719106816 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.1021077632904053, loss=4.103628635406494
I0420 11:09:59.334666 139578693928704 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.448975086212158, loss=4.023989200592041
I0420 11:10:35.612335 139578719106816 logging_writer.py:48] [16000] global_step=16000, grad_norm=4.200580596923828, loss=4.133448123931885
I0420 11:11:11.907636 139578693928704 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.838286876678467, loss=4.076193332672119
I0420 11:11:46.424221 139765933279040 spec.py:298] Evaluating on the training split.
I0420 11:11:49.410411 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 11:16:02.991740 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 11:16:05.648082 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 11:19:54.227051 139765933279040 spec.py:326] Evaluating on the test split.
I0420 11:19:56.928621 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 11:23:40.177481 139765933279040 submission_runner.py:406] Time since start: 11345.05s, 	Step: 16197, 	{'train/accuracy': 0.4783369302749634, 'train/loss': 2.9965920448303223, 'train/bleu': 14.153141526237018, 'validation/accuracy': 0.45458829402923584, 'validation/loss': 3.2221438884735107, 'validation/bleu': 9.772364926452557, 'validation/num_examples': 3000, 'test/accuracy': 0.4450525939464569, 'test/loss': 3.349766969680786, 'test/bleu': 8.192233493496891, 'test/num_examples': 3003, 'score': 5914.329973697662, 'total_duration': 11345.051842451096, 'accumulated_submission_time': 5914.329973697662, 'accumulated_eval_time': 5401.0445125103, 'accumulated_logging_time': 29.499809741973877}
I0420 11:23:40.186559 139578719106816 logging_writer.py:48] [16197] accumulated_eval_time=5401.044513, accumulated_logging_time=29.499810, accumulated_submission_time=5914.329974, global_step=16197, preemption_count=0, score=5914.329974, test/accuracy=0.445053, test/bleu=8.192233, test/loss=3.349767, test/num_examples=3003, total_duration=11345.051842, train/accuracy=0.478337, train/bleu=14.153142, train/loss=2.996592, validation/accuracy=0.454588, validation/bleu=9.772365, validation/loss=3.222144, validation/num_examples=3000
I0420 11:23:41.108807 139765933279040 checkpoints.py:356] Saving checkpoint at step: 16197
I0420 11:23:44.401000 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_16197
I0420 11:23:44.404340 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_16197.
I0420 11:23:45.862506 139578693928704 logging_writer.py:48] [16200] global_step=16200, grad_norm=10.870626449584961, loss=3.9913618564605713
I0420 11:24:22.104529 139578710714112 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.9073420763015747, loss=4.012767791748047
I0420 11:24:58.402508 139578693928704 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.026276111602783, loss=4.062340259552002
I0420 11:25:34.685909 139578710714112 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.2102279663085938, loss=4.023472309112549
I0420 11:26:10.975612 139578693928704 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.8894745111465454, loss=3.988630771636963
I0420 11:26:47.287850 139578710714112 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.119510173797607, loss=4.077315807342529
I0420 11:27:23.594329 139578693928704 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.730624794960022, loss=4.077692031860352
I0420 11:27:59.904462 139578710714112 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.6400399208068848, loss=4.037790298461914
I0420 11:28:36.170137 139578693928704 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.051967144012451, loss=3.9866249561309814
I0420 11:29:12.533144 139578710714112 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.0095553398132324, loss=4.026037693023682
I0420 11:29:48.826983 139578693928704 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.664076030254364, loss=4.0341033935546875
I0420 11:30:25.149011 139578710714112 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.342961311340332, loss=3.9740564823150635
I0420 11:31:01.428465 139578693928704 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.469024896621704, loss=3.8943233489990234
I0420 11:31:37.775104 139578710714112 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.233882188796997, loss=3.948700189590454
I0420 11:32:14.080099 139578693928704 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7117099165916443, loss=3.9069430828094482
I0420 11:32:50.348501 139578710714112 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9920687079429626, loss=3.879603624343872
I0420 11:33:26.656747 139578693928704 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.53830885887146, loss=3.8333277702331543
I0420 11:34:02.945793 139578710714112 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7907618880271912, loss=3.8651068210601807
I0420 11:34:39.233646 139578693928704 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0579190254211426, loss=3.803764820098877
I0420 11:35:15.499293 139578710714112 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.1794015169143677, loss=3.8499221801757812
I0420 11:35:51.794256 139578693928704 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.171175479888916, loss=3.891472816467285
I0420 11:36:28.058115 139578710714112 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.630612373352051, loss=3.8196091651916504
I0420 11:37:04.364938 139578693928704 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.067106008529663, loss=3.8588130474090576
I0420 11:37:40.655333 139578710714112 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.0475845336914062, loss=3.80930233001709
I0420 11:37:44.719728 139765933279040 spec.py:298] Evaluating on the training split.
I0420 11:37:47.723406 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 11:41:48.965062 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 11:41:51.623847 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 11:45:23.060258 139765933279040 spec.py:326] Evaluating on the test split.
I0420 11:45:25.766031 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 11:48:40.305160 139765933279040 submission_runner.py:406] Time since start: 12845.18s, 	Step: 18513, 	{'train/accuracy': 0.5122460722923279, 'train/loss': 2.757366180419922, 'train/bleu': 17.72537894749262, 'validation/accuracy': 0.494005024433136, 'validation/loss': 2.9250235557556152, 'validation/bleu': 12.987370039311083, 'validation/num_examples': 3000, 'test/accuracy': 0.4851316213607788, 'test/loss': 3.0279324054718018, 'test/bleu': 10.932451598466892, 'test/num_examples': 3003, 'score': 6754.612389802933, 'total_duration': 12845.179557561874, 'accumulated_submission_time': 6754.612389802933, 'accumulated_eval_time': 6056.629852294922, 'accumulated_logging_time': 33.72912907600403}
I0420 11:48:40.314371 139578693928704 logging_writer.py:48] [18513] accumulated_eval_time=6056.629852, accumulated_logging_time=33.729129, accumulated_submission_time=6754.612390, global_step=18513, preemption_count=0, score=6754.612390, test/accuracy=0.485132, test/bleu=10.932452, test/loss=3.027932, test/num_examples=3003, total_duration=12845.179558, train/accuracy=0.512246, train/bleu=17.725379, train/loss=2.757366, validation/accuracy=0.494005, validation/bleu=12.987370, validation/loss=2.925024, validation/num_examples=3000
I0420 11:48:41.235279 139765933279040 checkpoints.py:356] Saving checkpoint at step: 18513
I0420 11:48:44.548429 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_18513
I0420 11:48:44.551900 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_18513.
I0420 11:49:16.537996 139578710714112 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5780841112136841, loss=3.840867280960083
I0420 11:49:52.806271 139578702321408 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6650097966194153, loss=3.8554482460021973
I0420 11:50:29.111023 139578710714112 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6333907246589661, loss=3.815284490585327
I0420 11:51:05.383112 139578702321408 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6666561365127563, loss=3.8637197017669678
I0420 11:51:41.699284 139578710714112 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7807720899581909, loss=3.8296031951904297
I0420 11:52:17.989731 139578702321408 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8900982141494751, loss=3.7862486839294434
I0420 11:52:54.339270 139578710714112 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9358053803443909, loss=3.7791242599487305
I0420 11:53:30.637427 139578702321408 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6872543692588806, loss=3.7574782371520996
I0420 11:54:06.911183 139578710714112 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.1487380266189575, loss=3.7807204723358154
I0420 11:54:43.215044 139578702321408 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.558190107345581, loss=3.7721357345581055
I0420 11:55:19.497425 139578710714112 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7703400254249573, loss=3.7748196125030518
I0420 11:55:55.820013 139578702321408 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6350959539413452, loss=3.7764906883239746
I0420 11:56:32.124767 139578710714112 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0158438682556152, loss=3.7079505920410156
I0420 11:57:08.435784 139578702321408 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5164307355880737, loss=3.748800039291382
I0420 11:57:44.065935 139765933279040 spec.py:298] Evaluating on the training split.
I0420 11:57:47.063064 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 12:01:04.876277 139765933279040 spec.py:310] Evaluating on the validation split.
I0420 12:01:07.536300 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 12:04:35.446220 139765933279040 spec.py:326] Evaluating on the test split.
I0420 12:04:38.170556 139765933279040 workload.py:179] Translating evaluation dataset.
I0420 12:07:58.916188 139765933279040 submission_runner.py:406] Time since start: 14003.79s, 	Step: 20000, 	{'train/accuracy': 0.5405910611152649, 'train/loss': 2.548186779022217, 'train/bleu': 20.816145784248818, 'validation/accuracy': 0.5211094617843628, 'validation/loss': 2.722407102584839, 'validation/bleu': 15.41688330044205, 'validation/num_examples': 3000, 'test/accuracy': 0.5137411952018738, 'test/loss': 2.8023083209991455, 'test/bleu': 13.301272124220372, 'test/num_examples': 3003, 'score': 7294.107146024704, 'total_duration': 14003.790630340576, 'accumulated_submission_time': 7294.107146024704, 'accumulated_eval_time': 6671.480080366135, 'accumulated_logging_time': 37.97818922996521}
I0420 12:07:58.926455 139578710714112 logging_writer.py:48] [20000] accumulated_eval_time=6671.480080, accumulated_logging_time=37.978189, accumulated_submission_time=7294.107146, global_step=20000, preemption_count=0, score=7294.107146, test/accuracy=0.513741, test/bleu=13.301272, test/loss=2.802308, test/num_examples=3003, total_duration=14003.790630, train/accuracy=0.540591, train/bleu=20.816146, train/loss=2.548187, validation/accuracy=0.521109, validation/bleu=15.416883, validation/loss=2.722407, validation/num_examples=3000
I0420 12:07:59.853547 139765933279040 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 12:08:03.133475 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_20000
I0420 12:08:03.137428 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_20000.
I0420 12:08:03.147660 139578702321408 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7294.107146
I0420 12:08:03.646301 139765933279040 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 12:08:09.130670 139765933279040 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_20000
I0420 12:08:09.134232 139765933279040 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/wmt_jax/trial_1/checkpoint_20000.
I0420 12:08:09.195765 139765933279040 submission_runner.py:567] Tuning trial 1/1
I0420 12:08:09.195953 139765933279040 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 12:08:09.197075 139765933279040 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000615897006355226, 'train/loss': 11.054659843444824, 'train/bleu': 1.9168886195025588e-10, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.054000854492188, 'validation/bleu': 1.5614406590780203e-09, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.057171821594238, 'test/bleu': 4.3370890740967724e-10, 'test/num_examples': 3003, 'score': 33.205100536346436, 'total_duration': 955.1927862167358, 'accumulated_submission_time': 33.205100536346436, 'accumulated_eval_time': 921.9875423908234, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2311, {'train/accuracy': 0.21024225652217865, 'train/loss': 5.50177001953125, 'train/bleu': 0.25685936715641683, 'validation/accuracy': 0.18808197975158691, 'validation/loss': 5.735093593597412, 'validation/bleu': 0.09924330320980963, 'validation/num_examples': 3000, 'test/accuracy': 0.18034978210926056, 'test/loss': 5.978095054626465, 'test/bleu': 0.09587588221693083, 'test/num_examples': 3003, 'score': 873.2738935947418, 'total_duration': 2270.843296289444, 'accumulated_submission_time': 873.2738935947418, 'accumulated_eval_time': 1393.2119617462158, 'accumulated_logging_time': 4.329891681671143, 'global_step': 2311, 'preemption_count': 0}), (4623, {'train/accuracy': 0.4133537709712982, 'train/loss': 3.619962215423584, 'train/bleu': 11.800149874083603, 'validation/accuracy': 0.393187940120697, 'validation/loss': 3.8119516372680664, 'validation/bleu': 7.760326751227607, 'validation/num_examples': 3000, 'test/accuracy': 0.3797455132007599, 'test/loss': 4.003962516784668, 'test/bleu': 6.226705464222686, 'test/num_examples': 3003, 'score': 1713.5280511379242, 'total_duration': 3747.0240755081177, 'accumulated_submission_time': 1713.5280511379242, 'accumulated_eval_time': 2024.8374953269958, 'accumulated_logging_time': 8.604756116867065, 'global_step': 4623, 'preemption_count': 0}), (6936, {'train/accuracy': 0.3788565397262573, 'train/loss': 3.768223524093628, 'train/bleu': 7.961753784835494, 'validation/accuracy': 0.3484395742416382, 'validation/loss': 4.062211513519287, 'validation/bleu': 4.626484092501946, 'validation/num_examples': 3000, 'test/accuracy': 0.3286386728286743, 'test/loss': 4.304903030395508, 'test/bleu': 3.614604692457374, 'test/num_examples': 3003, 'score': 2553.8112738132477, 'total_duration': 5234.750430822372, 'accumulated_submission_time': 2553.8112738132477, 'accumulated_eval_time': 2668.068067073822, 'accumulated_logging_time': 12.792430877685547, 'global_step': 6936, 'preemption_count': 0}), (9250, {'train/accuracy': 0.3840128779411316, 'train/loss': 3.7115955352783203, 'train/bleu': 7.043833681360767, 'validation/accuracy': 0.35013824701309204, 'validation/loss': 4.027622699737549, 'validation/bleu': 3.723412807142296, 'validation/num_examples': 3000, 'test/accuracy': 0.3317762017250061, 'test/loss': 4.250222206115723, 'test/bleu': 2.6174909442258842, 'test/num_examples': 3003, 'score': 3393.876946210861, 'total_duration': 6822.353744268417, 'accumulated_submission_time': 3393.876946210861, 'accumulated_eval_time': 3411.4199526309967, 'accumulated_logging_time': 16.952259063720703, 'global_step': 9250, 'preemption_count': 0}), (11565, {'train/accuracy': 0.42044195532798767, 'train/loss': 3.413877010345459, 'train/bleu': 9.803393211492079, 'validation/accuracy': 0.39145204424858093, 'validation/loss': 3.6946914196014404, 'validation/bleu': 5.675816986702896, 'validation/num_examples': 3000, 'test/accuracy': 0.3727964758872986, 'test/loss': 3.9127721786499023, 'test/bleu': 4.368229661811422, 'test/num_examples': 3003, 'score': 4234.016389131546, 'total_duration': 8287.955007314682, 'accumulated_submission_time': 4234.016389131546, 'accumulated_eval_time': 4032.652271270752, 'accumulated_logging_time': 21.15631651878357, 'global_step': 11565, 'preemption_count': 0}), (13881, {'train/accuracy': 0.45285695791244507, 'train/loss': 3.17085599899292, 'train/bleu': 12.850105501676524, 'validation/accuracy': 0.427235871553421, 'validation/loss': 3.433032512664795, 'validation/bleu': 7.169095466262899, 'validation/num_examples': 3000, 'test/accuracy': 0.4134216606616974, 'test/loss': 3.600409746170044, 'test/bleu': 6.0505022560885475, 'test/num_examples': 3003, 'score': 5074.251702547073, 'total_duration': 9787.019736766815, 'accumulated_submission_time': 5074.251702547073, 'accumulated_eval_time': 4687.291373252869, 'accumulated_logging_time': 25.32247543334961, 'global_step': 13881, 'preemption_count': 0}), (16197, {'train/accuracy': 0.4783369302749634, 'train/loss': 2.9965920448303223, 'train/bleu': 14.153141526237018, 'validation/accuracy': 0.45458829402923584, 'validation/loss': 3.2221438884735107, 'validation/bleu': 9.772364926452557, 'validation/num_examples': 3000, 'test/accuracy': 0.4450525939464569, 'test/loss': 3.349766969680786, 'test/bleu': 8.192233493496891, 'test/num_examples': 3003, 'score': 5914.329973697662, 'total_duration': 11345.051842451096, 'accumulated_submission_time': 5914.329973697662, 'accumulated_eval_time': 5401.0445125103, 'accumulated_logging_time': 29.499809741973877, 'global_step': 16197, 'preemption_count': 0}), (18513, {'train/accuracy': 0.5122460722923279, 'train/loss': 2.757366180419922, 'train/bleu': 17.72537894749262, 'validation/accuracy': 0.494005024433136, 'validation/loss': 2.9250235557556152, 'validation/bleu': 12.987370039311083, 'validation/num_examples': 3000, 'test/accuracy': 0.4851316213607788, 'test/loss': 3.0279324054718018, 'test/bleu': 10.932451598466892, 'test/num_examples': 3003, 'score': 6754.612389802933, 'total_duration': 12845.179557561874, 'accumulated_submission_time': 6754.612389802933, 'accumulated_eval_time': 6056.629852294922, 'accumulated_logging_time': 33.72912907600403, 'global_step': 18513, 'preemption_count': 0}), (20000, {'train/accuracy': 0.5405910611152649, 'train/loss': 2.548186779022217, 'train/bleu': 20.816145784248818, 'validation/accuracy': 0.5211094617843628, 'validation/loss': 2.722407102584839, 'validation/bleu': 15.41688330044205, 'validation/num_examples': 3000, 'test/accuracy': 0.5137411952018738, 'test/loss': 2.8023083209991455, 'test/bleu': 13.301272124220372, 'test/num_examples': 3003, 'score': 7294.107146024704, 'total_duration': 14003.790630340576, 'accumulated_submission_time': 7294.107146024704, 'accumulated_eval_time': 6671.480080366135, 'accumulated_logging_time': 37.97818922996521, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0420 12:08:09.197201 139765933279040 submission_runner.py:570] Timing: 7294.107146024704
I0420 12:08:09.197245 139765933279040 submission_runner.py:571] ====================
I0420 12:08:09.197340 139765933279040 submission_runner.py:631] Final wmt score: 7294.107146024704
