python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_jax_upgrade_b/adamw --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-30-2023-19-49-48.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0530 19:50:10.490075 140457411839808 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax.
I0530 19:50:11.472402 140457411839808 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0530 19:50:11.473075 140457411839808 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0530 19:50:11.473193 140457411839808 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0530 19:50:11.478479 140457411839808 submission_runner.py:549] Using RNG seed 3145706297
I0530 19:50:16.710849 140457411839808 submission_runner.py:558] --- Tuning run 1/1 ---
I0530 19:50:16.711072 140457411839808 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1.
I0530 19:50:16.711301 140457411839808 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0530 19:50:16.891073 140457411839808 submission_runner.py:243] Initializing dataset.
I0530 19:50:16.891354 140457411839808 submission_runner.py:250] Initializing model.
I0530 19:50:18.877775 140457411839808 submission_runner.py:260] Initializing optimizer.
I0530 19:50:19.549749 140457411839808 submission_runner.py:267] Initializing metrics bundle.
I0530 19:50:19.549921 140457411839808 submission_runner.py:285] Initializing checkpoint and logger.
I0530 19:50:19.550912 140457411839808 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0530 19:50:19.551197 140457411839808 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0530 19:50:19.551269 140457411839808 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0530 19:50:20.140650 140457411839808 submission_runner.py:306] Saving meta data to /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0530 19:50:20.142437 140457411839808 submission_runner.py:309] Saving flags to /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0530 19:50:20.148702 140457411839808 submission_runner.py:321] Starting training loop.
I0530 19:50:20.429098 140457411839808 input_pipeline.py:20] Loading split = train-clean-100
I0530 19:50:20.469568 140457411839808 input_pipeline.py:20] Loading split = train-clean-360
I0530 19:50:20.806626 140457411839808 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0530 19:51:19.873389 140295945565952 logging_writer.py:48] [0] global_step=0, grad_norm=29.506837844848633, loss=33.27382278442383
I0530 19:51:19.894704 140457411839808 spec.py:298] Evaluating on the training split.
I0530 19:51:20.144133 140457411839808 input_pipeline.py:20] Loading split = train-clean-100
I0530 19:51:20.178086 140457411839808 input_pipeline.py:20] Loading split = train-clean-360
I0530 19:51:20.477977 140457411839808 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0530 19:53:06.103898 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 19:53:06.293764 140457411839808 input_pipeline.py:20] Loading split = dev-clean
I0530 19:53:06.299371 140457411839808 input_pipeline.py:20] Loading split = dev-other
I0530 19:54:02.348132 140457411839808 spec.py:326] Evaluating on the test split.
I0530 19:54:02.538863 140457411839808 input_pipeline.py:20] Loading split = test-clean
I0530 19:54:39.881198 140457411839808 submission_runner.py:426] Time since start: 259.73s, 	Step: 1, 	{'train/ctc_loss': Array(31.336977, dtype=float32), 'train/wer': 3.226266580596797, 'validation/ctc_loss': Array(30.298693, dtype=float32), 'validation/wer': 2.9246688342386324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.36078, dtype=float32), 'test/wer': 3.118579001888977, 'test/num_examples': 2472, 'score': 59.74584412574768, 'total_duration': 259.73064494132996, 'accumulated_submission_time': 59.74584412574768, 'accumulated_data_selection_time': 4.983752965927124, 'accumulated_eval_time': 199.98466610908508, 'accumulated_logging_time': 0}
I0530 19:54:39.902865 140288664270592 logging_writer.py:48] [1] accumulated_data_selection_time=4.983753, accumulated_eval_time=199.984666, accumulated_logging_time=0, accumulated_submission_time=59.745844, global_step=1, preemption_count=0, score=59.745844, test/ctc_loss=30.360780715942383, test/num_examples=2472, test/wer=3.118579, total_duration=259.730645, train/ctc_loss=31.336977005004883, train/wer=3.226267, validation/ctc_loss=30.29869270324707, validation/num_examples=5348, validation/wer=2.924669
I0530 19:56:02.143650 140297508120320 logging_writer.py:48] [100] global_step=100, grad_norm=3.1280789375305176, loss=7.525699138641357
I0530 19:57:19.744847 140297516513024 logging_writer.py:48] [200] global_step=200, grad_norm=1.5378847122192383, loss=6.058457851409912
I0530 19:58:37.582930 140297508120320 logging_writer.py:48] [300] global_step=300, grad_norm=1.009007215499878, loss=5.868052005767822
I0530 19:59:53.715635 140297516513024 logging_writer.py:48] [400] global_step=400, grad_norm=0.6663628816604614, loss=5.821331977844238
I0530 20:01:09.017022 140297508120320 logging_writer.py:48] [500] global_step=500, grad_norm=1.4957126379013062, loss=5.790074348449707
I0530 20:02:24.467312 140297516513024 logging_writer.py:48] [600] global_step=600, grad_norm=0.6266953349113464, loss=5.679758548736572
I0530 20:03:41.223670 140297508120320 logging_writer.py:48] [700] global_step=700, grad_norm=1.0953164100646973, loss=5.543786525726318
I0530 20:04:57.162559 140297516513024 logging_writer.py:48] [800] global_step=800, grad_norm=1.5652382373809814, loss=5.362685203552246
I0530 20:06:12.789001 140297508120320 logging_writer.py:48] [900] global_step=900, grad_norm=1.6119223833084106, loss=5.107924461364746
I0530 20:07:35.097134 140297516513024 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1180115938186646, loss=4.705753803253174
I0530 20:08:54.679178 140297558476544 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.5570449829101562, loss=4.359363555908203
I0530 20:10:10.191601 140297550083840 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.153272867202759, loss=4.007415771484375
I0530 20:11:27.377766 140297558476544 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.208137273788452, loss=3.8531558513641357
I0530 20:12:43.025547 140297550083840 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.0992746353149414, loss=3.6406373977661133
I0530 20:13:57.986699 140297558476544 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.0139408111572266, loss=3.510624408721924
I0530 20:15:13.247540 140297550083840 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.2353310585021973, loss=3.3089656829833984
I0530 20:16:28.744507 140297558476544 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.2957634925842285, loss=3.1870810985565186
I0530 20:17:45.427805 140297550083840 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.656680107116699, loss=3.1774587631225586
I0530 20:19:05.229836 140297558476544 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.0749263763427734, loss=3.016245126724243
I0530 20:20:29.624681 140297550083840 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.4704039096832275, loss=2.9568631649017334
I0530 20:21:52.842495 140299524556544 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.710204601287842, loss=2.905377149581909
I0530 20:23:07.613812 140299516163840 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.533050060272217, loss=2.8215014934539795
I0530 20:24:22.320793 140299524556544 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.9833271503448486, loss=2.736346960067749
I0530 20:25:36.676094 140299516163840 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.5305354595184326, loss=2.7117345333099365
I0530 20:26:50.829816 140299524556544 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.784248113632202, loss=2.723703145980835
I0530 20:28:07.925822 140299516163840 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.710059881210327, loss=2.602592706680298
I0530 20:29:31.272684 140299524556544 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.822553873062134, loss=2.5861361026763916
I0530 20:30:54.966460 140299516163840 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.3255324363708496, loss=2.5983264446258545
I0530 20:32:17.575802 140299524556544 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.9467532634735107, loss=2.4966275691986084
I0530 20:33:37.090511 140299516163840 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.363062381744385, loss=2.39756441116333
I0530 20:34:40.146486 140457411839808 spec.py:298] Evaluating on the training split.
I0530 20:35:19.188645 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 20:35:56.937788 140457411839808 spec.py:326] Evaluating on the test split.
I0530 20:36:16.580563 140457411839808 submission_runner.py:426] Time since start: 2756.43s, 	Step: 3077, 	{'train/ctc_loss': Array(3.4224555, dtype=float32), 'train/wer': 0.7070348705484953, 'validation/ctc_loss': Array(3.6791599, dtype=float32), 'validation/wer': 0.7297610203668149, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2677572, dtype=float32), 'test/wer': 0.6746288058822335, 'test/num_examples': 2472, 'score': 2459.941887140274, 'total_duration': 2756.427994251251, 'accumulated_submission_time': 2459.941887140274, 'accumulated_data_selection_time': 462.80777955055237, 'accumulated_eval_time': 296.41497445106506, 'accumulated_logging_time': 0.03282427787780762}
I0530 20:36:16.600801 140299524556544 logging_writer.py:48] [3077] accumulated_data_selection_time=462.807780, accumulated_eval_time=296.414974, accumulated_logging_time=0.032824, accumulated_submission_time=2459.941887, global_step=3077, preemption_count=0, score=2459.941887, test/ctc_loss=3.2677571773529053, test/num_examples=2472, test/wer=0.674629, total_duration=2756.427994, train/ctc_loss=3.4224555492401123, train/wer=0.707035, validation/ctc_loss=3.6791598796844482, validation/num_examples=5348, validation/wer=0.729761
I0530 20:36:37.335532 140298869196544 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.784968614578247, loss=2.438669443130493
I0530 20:37:51.284829 140298860803840 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.9293843507766724, loss=2.3101556301116943
I0530 20:39:06.603505 140298869196544 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.4536757469177246, loss=2.3523824214935303
I0530 20:40:21.020703 140298860803840 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.5111305713653564, loss=2.3025834560394287
I0530 20:41:35.080050 140298869196544 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.4764177799224854, loss=2.218435764312744
I0530 20:42:53.357189 140298860803840 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.161878824234009, loss=2.286304235458374
I0530 20:44:14.385885 140298869196544 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.60338020324707, loss=2.3337557315826416
I0530 20:45:32.515614 140298860803840 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.167808771133423, loss=2.2601187229156494
I0530 20:46:52.521131 140298869196544 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.686436176300049, loss=2.2035372257232666
I0530 20:48:14.523344 140298860803840 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.0949795246124268, loss=2.2629406452178955
I0530 20:49:34.723860 140298869196544 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.9822936058044434, loss=2.2246031761169434
I0530 20:50:53.189842 140298869196544 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.8307416439056396, loss=2.1920087337493896
I0530 20:52:08.016091 140298860803840 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.0432567596435547, loss=2.213867425918579
I0530 20:53:21.893331 140298869196544 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.7669336795806885, loss=2.139059066772461
I0530 20:54:36.163578 140298860803840 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.131197452545166, loss=1.9916739463806152
I0530 20:55:55.118369 140298869196544 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.682511806488037, loss=2.0513553619384766
I0530 20:57:16.064936 140298860803840 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.9119778871536255, loss=2.0682690143585205
I0530 20:58:39.774609 140298869196544 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.510422706604004, loss=2.146798849105835
I0530 21:00:03.531118 140298860803840 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.415600061416626, loss=2.044938325881958
I0530 21:01:26.093865 140298869196544 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.4687750339508057, loss=1.967900037765503
I0530 21:02:47.546399 140298860803840 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.54127836227417, loss=1.993783712387085
I0530 21:04:08.881825 140297558476544 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.182417869567871, loss=2.0002894401550293
I0530 21:05:25.064554 140297550083840 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.184316635131836, loss=1.9997485876083374
I0530 21:06:38.930732 140297558476544 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9377856254577637, loss=2.0660033226013184
I0530 21:07:52.626836 140297550083840 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.820780038833618, loss=1.960662841796875
I0530 21:09:06.465804 140297558476544 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.9166947603225708, loss=1.9103689193725586
I0530 21:10:29.625514 140297550083840 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.4221081733703613, loss=1.9706642627716064
I0530 21:11:55.997502 140297558476544 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.016289710998535, loss=2.00254487991333
I0530 21:13:18.467616 140297550083840 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.1723148822784424, loss=1.9585301876068115
I0530 21:14:39.512284 140297558476544 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.8017157316207886, loss=1.9392004013061523
I0530 21:16:00.544675 140297550083840 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.3301050662994385, loss=1.9710116386413574
I0530 21:16:17.189066 140457411839808 spec.py:298] Evaluating on the training split.
I0530 21:17:03.555132 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 21:17:46.420791 140457411839808 spec.py:326] Evaluating on the test split.
I0530 21:18:07.547454 140457411839808 submission_runner.py:426] Time since start: 5267.39s, 	Step: 6121, 	{'train/ctc_loss': Array(0.6322971, dtype=float32), 'train/wer': 0.21323282635910823, 'validation/ctc_loss': Array(1.0210454, dtype=float32), 'validation/wer': 0.28528977607116324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6665019, dtype=float32), 'test/wer': 0.21451059248877785, 'test/num_examples': 2472, 'score': 4860.476452589035, 'total_duration': 5267.394803762436, 'accumulated_submission_time': 4860.476452589035, 'accumulated_data_selection_time': 1027.9804849624634, 'accumulated_eval_time': 406.7694847583771, 'accumulated_logging_time': 0.06768274307250977}
I0530 21:18:07.567442 140297558476544 logging_writer.py:48] [6121] accumulated_data_selection_time=1027.980485, accumulated_eval_time=406.769485, accumulated_logging_time=0.067683, accumulated_submission_time=4860.476453, global_step=6121, preemption_count=0, score=4860.476453, test/ctc_loss=0.6665018796920776, test/num_examples=2472, test/wer=0.214511, total_duration=5267.394804, train/ctc_loss=0.6322970986366272, train/wer=0.213233, validation/ctc_loss=1.021045446395874, validation/num_examples=5348, validation/wer=0.285290
I0530 21:19:10.085478 140298869196544 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.0063273906707764, loss=1.9295597076416016
I0530 21:20:23.818526 140298860803840 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.4920382499694824, loss=1.786424160003662
I0530 21:21:37.526840 140298869196544 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.9509167671203613, loss=1.940611481666565
I0530 21:22:51.495298 140298860803840 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.561666250228882, loss=1.9058423042297363
I0530 21:24:07.426000 140298869196544 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.6570993661880493, loss=1.818055510520935
I0530 21:25:30.043235 140298860803840 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.0309691429138184, loss=1.9508006572723389
I0530 21:26:49.036754 140298869196544 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.8836686611175537, loss=1.9123265743255615
I0530 21:28:06.579176 140298860803840 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.0576541423797607, loss=1.8188936710357666
I0530 21:29:25.950952 140298869196544 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.3467071056365967, loss=1.9290851354599
I0530 21:30:48.690976 140298860803840 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.3654088973999023, loss=1.8574635982513428
I0530 21:32:11.650008 140298869196544 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.1010799407958984, loss=1.8270373344421387
I0530 21:33:29.319070 140299524556544 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.710275411605835, loss=1.8630940914154053
I0530 21:34:43.257038 140299516163840 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.8581900596618652, loss=1.80238676071167
I0530 21:35:57.556594 140299524556544 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.6731715202331543, loss=1.8530439138412476
I0530 21:37:12.974551 140299516163840 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.041975975036621, loss=1.782441258430481
I0530 21:38:28.356536 140299524556544 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.924244999885559, loss=1.8430951833724976
I0530 21:39:47.995189 140299516163840 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.0945584774017334, loss=1.8054635524749756
I0530 21:41:06.164403 140299524556544 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.6110761165618896, loss=1.7785953283309937
I0530 21:42:31.121366 140299516163840 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.8513391017913818, loss=1.9088932275772095
I0530 21:43:56.138702 140299524556544 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.124497413635254, loss=1.7752692699432373
I0530 21:45:18.292098 140299516163840 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.054260015487671, loss=1.7875007390975952
I0530 21:46:38.672022 140299524556544 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.233607530593872, loss=1.797972321510315
I0530 21:47:53.116933 140299516163840 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.875319480895996, loss=1.739292025566101
I0530 21:49:09.484940 140299524556544 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.50080943107605, loss=1.7575631141662598
I0530 21:50:24.148361 140299516163840 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.4221458435058594, loss=1.7707750797271729
I0530 21:51:38.207350 140299524556544 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.9102433919906616, loss=1.8072782754898071
I0530 21:52:55.306121 140299516163840 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.569406509399414, loss=1.7318776845932007
I0530 21:54:14.323194 140299524556544 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.778023600578308, loss=1.709040641784668
I0530 21:55:36.060415 140299516163840 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.6157751083374023, loss=1.7043436765670776
I0530 21:56:54.559010 140299524556544 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.8754050731658936, loss=1.7212369441986084
I0530 21:58:08.107920 140457411839808 spec.py:298] Evaluating on the training split.
I0530 21:58:55.553490 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 21:59:37.563220 140457411839808 spec.py:326] Evaluating on the test split.
I0530 21:59:58.315808 140457411839808 submission_runner.py:426] Time since start: 7778.16s, 	Step: 9194, 	{'train/ctc_loss': Array(0.44016284, dtype=float32), 'train/wer': 0.155826493982953, 'validation/ctc_loss': Array(0.83872765, dtype=float32), 'validation/wer': 0.24049436077530897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5177572, dtype=float32), 'test/wer': 0.16874860357890034, 'test/num_examples': 2472, 'score': 7260.962044239044, 'total_duration': 7778.163358449936, 'accumulated_submission_time': 7260.962044239044, 'accumulated_data_selection_time': 1577.2230832576752, 'accumulated_eval_time': 516.973696231842, 'accumulated_logging_time': 0.10215210914611816}
I0530 21:59:58.336201 140299524556544 logging_writer.py:48] [9194] accumulated_data_selection_time=1577.223083, accumulated_eval_time=516.973696, accumulated_logging_time=0.102152, accumulated_submission_time=7260.962044, global_step=9194, preemption_count=0, score=7260.962044, test/ctc_loss=0.5177571773529053, test/num_examples=2472, test/wer=0.168749, total_duration=7778.163358, train/ctc_loss=0.4401628375053406, train/wer=0.155826, validation/ctc_loss=0.8387276530265808, validation/num_examples=5348, validation/wer=0.240494
I0530 22:00:03.760342 140299516163840 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.2612338066101074, loss=1.7974549531936646
I0530 22:01:22.051511 140298869196544 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.2060868740081787, loss=1.7729700803756714
I0530 22:02:36.483184 140298860803840 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.7530601024627686, loss=1.738546371459961
I0530 22:03:52.294092 140298869196544 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.003617286682129, loss=1.8082371950149536
I0530 22:05:07.339478 140298860803840 logging_writer.py:48] [9600] global_step=9600, grad_norm=5.520440578460693, loss=1.843227744102478
I0530 22:06:21.261638 140298869196544 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.056920289993286, loss=1.667710781097412
I0530 22:07:37.868754 140298860803840 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.7290616035461426, loss=1.7473784685134888
I0530 22:09:01.284595 140298869196544 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.099229335784912, loss=1.7638297080993652
I0530 22:10:21.751883 140298860803840 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.5085458755493164, loss=1.795992136001587
I0530 22:11:42.845922 140298869196544 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.6706228256225586, loss=1.719895601272583
I0530 22:13:05.594904 140298860803840 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7759941816329956, loss=1.6929714679718018
I0530 22:14:33.005144 140298869196544 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.076200008392334, loss=1.6860607862472534
I0530 22:15:46.938428 140298860803840 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.8973119258880615, loss=1.6824322938919067
I0530 22:17:01.384580 140298869196544 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.1505346298217773, loss=1.6618804931640625
I0530 22:18:16.686219 140298860803840 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.859424114227295, loss=1.6720776557922363
I0530 22:19:31.911709 140298869196544 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.07578444480896, loss=1.6458723545074463
I0530 22:20:54.709635 140298860803840 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.415741443634033, loss=1.6647374629974365
I0530 22:22:15.454488 140298869196544 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.9794626235961914, loss=1.657364845275879
I0530 22:23:40.302470 140298860803840 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.910444974899292, loss=1.7283297777175903
I0530 22:25:03.333632 140298869196544 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.5706114768981934, loss=1.7090916633605957
I0530 22:26:23.219349 140298860803840 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.2051074504852295, loss=1.6798843145370483
I0530 22:27:43.972743 140298869196544 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.6458334922790527, loss=1.7749838829040527
I0530 22:29:05.923391 140298869196544 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.180698871612549, loss=1.6549955606460571
I0530 22:30:20.244156 140298860803840 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.6005048751831055, loss=1.6504249572753906
I0530 22:31:34.089836 140298869196544 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.361396551132202, loss=1.7323167324066162
I0530 22:32:49.139233 140298860803840 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.5258090496063232, loss=1.6850284337997437
I0530 22:34:08.112871 140298869196544 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.745418071746826, loss=1.6513421535491943
I0530 22:35:29.443323 140298860803840 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.246030807495117, loss=1.688531756401062
I0530 22:36:52.466241 140298869196544 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.797774314880371, loss=1.6696484088897705
I0530 22:38:12.981561 140298860803840 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.6381001472473145, loss=1.6773523092269897
I0530 22:39:35.330967 140298869196544 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.5380256175994873, loss=1.6721631288528442
I0530 22:39:59.039731 140457411839808 spec.py:298] Evaluating on the training split.
I0530 22:40:47.437252 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 22:41:28.784722 140457411839808 spec.py:326] Evaluating on the test split.
I0530 22:41:50.833468 140457411839808 submission_runner.py:426] Time since start: 10290.68s, 	Step: 12230, 	{'train/ctc_loss': Array(0.4012222, dtype=float32), 'train/wer': 0.137633654138173, 'validation/ctc_loss': Array(0.7523031, dtype=float32), 'validation/wer': 0.21415546700884716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44854486, dtype=float32), 'test/wer': 0.14622306176751365, 'test/num_examples': 2472, 'score': 9661.61095070839, 'total_duration': 10290.680930376053, 'accumulated_submission_time': 9661.61095070839, 'accumulated_data_selection_time': 2155.4977428913116, 'accumulated_eval_time': 628.7636811733246, 'accumulated_logging_time': 0.13759922981262207}
I0530 22:41:50.854121 140298285516544 logging_writer.py:48] [12230] accumulated_data_selection_time=2155.497743, accumulated_eval_time=628.763681, accumulated_logging_time=0.137599, accumulated_submission_time=9661.610951, global_step=12230, preemption_count=0, score=9661.610951, test/ctc_loss=0.44854485988616943, test/num_examples=2472, test/wer=0.146223, total_duration=10290.680930, train/ctc_loss=0.40122219920158386, train/wer=0.137634, validation/ctc_loss=0.7523031234741211, validation/num_examples=5348, validation/wer=0.214155
I0530 22:42:43.912030 140298277123840 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.772531270980835, loss=1.6260128021240234
I0530 22:44:00.898566 140297630156544 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.6624393463134766, loss=1.589951515197754
I0530 22:45:15.537292 140297621763840 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.8566646575927734, loss=1.6384778022766113
I0530 22:46:29.748722 140297630156544 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.375408411026001, loss=1.6501187086105347
I0530 22:47:45.641610 140297621763840 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.9062016010284424, loss=1.605684757232666
I0530 22:49:06.821257 140297630156544 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.9137510061264038, loss=1.6479090452194214
I0530 22:50:32.733795 140297621763840 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.244210720062256, loss=1.6125067472457886
I0530 22:51:50.539814 140297630156544 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.5765655040740967, loss=1.6763569116592407
I0530 22:53:11.686041 140297621763840 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.4647157192230225, loss=1.655818223953247
I0530 22:54:33.042222 140297630156544 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.96712064743042, loss=1.6450825929641724
I0530 22:55:52.458212 140297621763840 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.7785236835479736, loss=1.7283720970153809
I0530 22:57:18.845292 140298285516544 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.7481822967529297, loss=1.5801639556884766
I0530 22:58:32.932521 140298277123840 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.865054130554199, loss=1.7130751609802246
I0530 22:59:47.154484 140298285516544 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.383752346038818, loss=1.6470842361450195
I0530 23:01:01.735052 140298277123840 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.193028450012207, loss=1.6850411891937256
I0530 23:02:23.269362 140298285516544 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.746673583984375, loss=1.57648766040802
I0530 23:03:45.356788 140298277123840 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.301950931549072, loss=1.6592050790786743
I0530 23:05:09.428529 140298285516544 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.9291508197784424, loss=1.7381857633590698
I0530 23:06:35.168757 140298277123840 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.7263387441635132, loss=1.6081300973892212
I0530 23:08:01.249578 140298285516544 logging_writer.py:48] [14200] global_step=14200, grad_norm=7.55782413482666, loss=1.7026129961013794
I0530 23:09:21.749199 140298277123840 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.8471007347106934, loss=1.6382133960723877
I0530 23:10:44.148329 140298285516544 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.424886703491211, loss=1.5995780229568481
I0530 23:12:03.802273 140299524556544 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.539092540740967, loss=1.6346988677978516
I0530 23:13:18.872852 140299516163840 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.022199630737305, loss=1.5524730682373047
I0530 23:14:33.582288 140299524556544 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.3873796463012695, loss=1.6629996299743652
I0530 23:15:48.323291 140299516163840 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.896136999130249, loss=1.6202290058135986
I0530 23:17:02.324898 140299524556544 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.4043726921081543, loss=1.5730518102645874
I0530 23:18:22.102543 140299516163840 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.456207036972046, loss=1.6479332447052002
I0530 23:19:44.594614 140299524556544 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.78776478767395, loss=1.5975691080093384
I0530 23:21:09.653407 140299516163840 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.3211190700531006, loss=1.5935049057006836
I0530 23:21:51.070863 140457411839808 spec.py:298] Evaluating on the training split.
I0530 23:22:38.255723 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 23:23:20.030357 140457411839808 spec.py:326] Evaluating on the test split.
I0530 23:23:41.019883 140457411839808 submission_runner.py:426] Time since start: 12800.87s, 	Step: 15253, 	{'train/ctc_loss': Array(0.36143264, dtype=float32), 'train/wer': 0.12553001804779404, 'validation/ctc_loss': Array(0.70607024, dtype=float32), 'validation/wer': 0.20306997655549017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41645497, dtype=float32), 'test/wer': 0.13490951191274145, 'test/num_examples': 2472, 'score': 12061.772027492523, 'total_duration': 12800.867428064346, 'accumulated_submission_time': 12061.772027492523, 'accumulated_data_selection_time': 2749.5442090034485, 'accumulated_eval_time': 738.7090077400208, 'accumulated_logging_time': 0.1737959384918213}
I0530 23:23:41.040415 140299524556544 logging_writer.py:48] [15253] accumulated_data_selection_time=2749.544209, accumulated_eval_time=738.709008, accumulated_logging_time=0.173796, accumulated_submission_time=12061.772027, global_step=15253, preemption_count=0, score=12061.772027, test/ctc_loss=0.4164549708366394, test/num_examples=2472, test/wer=0.134910, total_duration=12800.867428, train/ctc_loss=0.36143264174461365, train/wer=0.125530, validation/ctc_loss=0.7060702443122864, validation/num_examples=5348, validation/wer=0.203070
I0530 23:24:16.419705 140299516163840 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.542051315307617, loss=1.6103565692901611
I0530 23:25:30.789142 140299524556544 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.8508951663970947, loss=1.5719263553619385
I0530 23:26:48.724351 140299524556544 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.2315008640289307, loss=1.5650982856750488
I0530 23:28:02.703434 140299516163840 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.654301404953003, loss=1.5750038623809814
I0530 23:29:16.973109 140299524556544 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.8907930850982666, loss=1.603872299194336
I0530 23:30:36.896556 140299516163840 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.1833419799804688, loss=1.5943080186843872
I0530 23:31:56.146198 140299524556544 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.116145372390747, loss=1.5915930271148682
I0530 23:33:13.749287 140457411839808 spec.py:298] Evaluating on the training split.
I0530 23:34:00.753400 140457411839808 spec.py:310] Evaluating on the validation split.
I0530 23:34:42.788323 140457411839808 spec.py:326] Evaluating on the test split.
I0530 23:35:05.127189 140457411839808 submission_runner.py:426] Time since start: 13484.98s, 	Step: 16000, 	{'train/ctc_loss': Array(0.36736304, dtype=float32), 'train/wer': 0.12326421304767646, 'validation/ctc_loss': Array(0.68200964, dtype=float32), 'validation/wer': 0.19351850958523478, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39159974, dtype=float32), 'test/wer': 0.12688643795828, 'test/num_examples': 2472, 'score': 12634.455384016037, 'total_duration': 13484.976140499115, 'accumulated_submission_time': 12634.455384016037, 'accumulated_data_selection_time': 2870.917601108551, 'accumulated_eval_time': 850.0846302509308, 'accumulated_logging_time': 0.209381103515625}
I0530 23:35:05.146399 140299524556544 logging_writer.py:48] [16000] accumulated_data_selection_time=2870.917601, accumulated_eval_time=850.084630, accumulated_logging_time=0.209381, accumulated_submission_time=12634.455384, global_step=16000, preemption_count=0, score=12634.455384, test/ctc_loss=0.39159974455833435, test/num_examples=2472, test/wer=0.126886, total_duration=13484.976140, train/ctc_loss=0.3673630356788635, train/wer=0.123264, validation/ctc_loss=0.6820096373558044, validation/num_examples=5348, validation/wer=0.193519
I0530 23:35:05.163204 140299516163840 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12634.455384
I0530 23:35:05.313434 140457411839808 checkpoints.py:490] Saving checkpoint at step: 16000
I0530 23:35:06.165593 140457411839808 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0530 23:35:06.184828 140457411839808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_upgrade_b/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0530 23:35:07.364193 140457411839808 submission_runner.py:589] Tuning trial 1/1
I0530 23:35:07.364432 140457411839808 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0530 23:35:07.369577 140457411839808 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.336977, dtype=float32), 'train/wer': 3.226266580596797, 'validation/ctc_loss': Array(30.298693, dtype=float32), 'validation/wer': 2.9246688342386324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.36078, dtype=float32), 'test/wer': 3.118579001888977, 'test/num_examples': 2472, 'score': 59.74584412574768, 'total_duration': 259.73064494132996, 'accumulated_submission_time': 59.74584412574768, 'accumulated_data_selection_time': 4.983752965927124, 'accumulated_eval_time': 199.98466610908508, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3077, {'train/ctc_loss': Array(3.4224555, dtype=float32), 'train/wer': 0.7070348705484953, 'validation/ctc_loss': Array(3.6791599, dtype=float32), 'validation/wer': 0.7297610203668149, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2677572, dtype=float32), 'test/wer': 0.6746288058822335, 'test/num_examples': 2472, 'score': 2459.941887140274, 'total_duration': 2756.427994251251, 'accumulated_submission_time': 2459.941887140274, 'accumulated_data_selection_time': 462.80777955055237, 'accumulated_eval_time': 296.41497445106506, 'accumulated_logging_time': 0.03282427787780762, 'global_step': 3077, 'preemption_count': 0}), (6121, {'train/ctc_loss': Array(0.6322971, dtype=float32), 'train/wer': 0.21323282635910823, 'validation/ctc_loss': Array(1.0210454, dtype=float32), 'validation/wer': 0.28528977607116324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6665019, dtype=float32), 'test/wer': 0.21451059248877785, 'test/num_examples': 2472, 'score': 4860.476452589035, 'total_duration': 5267.394803762436, 'accumulated_submission_time': 4860.476452589035, 'accumulated_data_selection_time': 1027.9804849624634, 'accumulated_eval_time': 406.7694847583771, 'accumulated_logging_time': 0.06768274307250977, 'global_step': 6121, 'preemption_count': 0}), (9194, {'train/ctc_loss': Array(0.44016284, dtype=float32), 'train/wer': 0.155826493982953, 'validation/ctc_loss': Array(0.83872765, dtype=float32), 'validation/wer': 0.24049436077530897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5177572, dtype=float32), 'test/wer': 0.16874860357890034, 'test/num_examples': 2472, 'score': 7260.962044239044, 'total_duration': 7778.163358449936, 'accumulated_submission_time': 7260.962044239044, 'accumulated_data_selection_time': 1577.2230832576752, 'accumulated_eval_time': 516.973696231842, 'accumulated_logging_time': 0.10215210914611816, 'global_step': 9194, 'preemption_count': 0}), (12230, {'train/ctc_loss': Array(0.4012222, dtype=float32), 'train/wer': 0.137633654138173, 'validation/ctc_loss': Array(0.7523031, dtype=float32), 'validation/wer': 0.21415546700884716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44854486, dtype=float32), 'test/wer': 0.14622306176751365, 'test/num_examples': 2472, 'score': 9661.61095070839, 'total_duration': 10290.680930376053, 'accumulated_submission_time': 9661.61095070839, 'accumulated_data_selection_time': 2155.4977428913116, 'accumulated_eval_time': 628.7636811733246, 'accumulated_logging_time': 0.13759922981262207, 'global_step': 12230, 'preemption_count': 0}), (15253, {'train/ctc_loss': Array(0.36143264, dtype=float32), 'train/wer': 0.12553001804779404, 'validation/ctc_loss': Array(0.70607024, dtype=float32), 'validation/wer': 0.20306997655549017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41645497, dtype=float32), 'test/wer': 0.13490951191274145, 'test/num_examples': 2472, 'score': 12061.772027492523, 'total_duration': 12800.867428064346, 'accumulated_submission_time': 12061.772027492523, 'accumulated_data_selection_time': 2749.5442090034485, 'accumulated_eval_time': 738.7090077400208, 'accumulated_logging_time': 0.1737959384918213, 'global_step': 15253, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.36736304, dtype=float32), 'train/wer': 0.12326421304767646, 'validation/ctc_loss': Array(0.68200964, dtype=float32), 'validation/wer': 0.19351850958523478, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39159974, dtype=float32), 'test/wer': 0.12688643795828, 'test/num_examples': 2472, 'score': 12634.455384016037, 'total_duration': 13484.976140499115, 'accumulated_submission_time': 12634.455384016037, 'accumulated_data_selection_time': 2870.917601108551, 'accumulated_eval_time': 850.0846302509308, 'accumulated_logging_time': 0.209381103515625, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0530 23:35:07.369800 140457411839808 submission_runner.py:592] Timing: 12634.455384016037
I0530 23:35:07.369861 140457411839808 submission_runner.py:593] ====================
I0530 23:35:07.370642 140457411839808 submission_runner.py:661] Final librispeech_deepspeech score: 12634.455384016037
