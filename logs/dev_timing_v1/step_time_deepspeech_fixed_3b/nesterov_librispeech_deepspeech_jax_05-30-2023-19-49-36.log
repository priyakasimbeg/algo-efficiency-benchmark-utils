python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/nesterov/jax/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_jax_upgrade_b/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-30-2023-19-49-36.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0530 19:49:58.782664 140343430178624 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax.
I0530 19:49:59.720455 140343430178624 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0530 19:49:59.721014 140343430178624 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0530 19:49:59.721157 140343430178624 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0530 19:49:59.726834 140343430178624 submission_runner.py:549] Using RNG seed 2579491148
I0530 19:50:05.117158 140343430178624 submission_runner.py:558] --- Tuning run 1/1 ---
I0530 19:50:05.117334 140343430178624 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1.
I0530 19:50:05.117618 140343430178624 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1/hparams.json.
I0530 19:50:05.297189 140343430178624 submission_runner.py:243] Initializing dataset.
I0530 19:50:05.297375 140343430178624 submission_runner.py:250] Initializing model.
I0530 19:50:07.285409 140343430178624 submission_runner.py:260] Initializing optimizer.
I0530 19:50:07.889595 140343430178624 submission_runner.py:267] Initializing metrics bundle.
I0530 19:50:07.889759 140343430178624 submission_runner.py:285] Initializing checkpoint and logger.
I0530 19:50:07.890775 140343430178624 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0530 19:50:07.891077 140343430178624 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0530 19:50:07.891174 140343430178624 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0530 19:50:08.481242 140343430178624 submission_runner.py:306] Saving meta data to /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0530 19:50:08.482150 140343430178624 submission_runner.py:309] Saving flags to /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0530 19:50:08.488142 140343430178624 submission_runner.py:321] Starting training loop.
I0530 19:50:08.773309 140343430178624 input_pipeline.py:20] Loading split = train-clean-100
I0530 19:50:08.814662 140343430178624 input_pipeline.py:20] Loading split = train-clean-360
I0530 19:50:09.154774 140343430178624 input_pipeline.py:20] Loading split = train-other-500
2023-05-30 19:51:03.242256: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-05-30 19:51:03.359511: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0530 19:51:08.306347 140177104783104 logging_writer.py:48] [0] global_step=0, grad_norm=27.515302658081055, loss=32.68458938598633
I0530 19:51:08.332243 140343430178624 spec.py:298] Evaluating on the training split.
I0530 19:51:08.593354 140343430178624 input_pipeline.py:20] Loading split = train-clean-100
I0530 19:51:08.630268 140343430178624 input_pipeline.py:20] Loading split = train-clean-360
I0530 19:51:09.047964 140343430178624 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0530 19:53:09.604985 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 19:53:09.797928 140343430178624 input_pipeline.py:20] Loading split = dev-clean
I0530 19:53:09.803267 140343430178624 input_pipeline.py:20] Loading split = dev-other
I0530 19:54:13.063991 140343430178624 spec.py:326] Evaluating on the test split.
I0530 19:54:13.263556 140343430178624 input_pipeline.py:20] Loading split = test-clean
I0530 19:54:53.277529 140343430178624 submission_runner.py:426] Time since start: 284.79s, 	Step: 1, 	{'train/ctc_loss': Array(32.629192, dtype=float32), 'train/wer': 3.9676300939133333, 'validation/ctc_loss': Array(31.704493, dtype=float32), 'validation/wer': 3.6335516985209697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.745237, dtype=float32), 'test/wer': 3.8643389596408912, 'test/num_examples': 2472, 'score': 59.84392189979553, 'total_duration': 284.7875916957855, 'accumulated_submission_time': 59.84392189979553, 'accumulated_data_selection_time': 5.048012733459473, 'accumulated_eval_time': 224.94352436065674, 'accumulated_logging_time': 0}
I0530 19:54:53.299432 140174998624000 logging_writer.py:48] [1] accumulated_data_selection_time=5.048013, accumulated_eval_time=224.943524, accumulated_logging_time=0, accumulated_submission_time=59.843922, global_step=1, preemption_count=0, score=59.843922, test/ctc_loss=31.745237350463867, test/num_examples=2472, test/wer=3.864339, total_duration=284.787592, train/ctc_loss=32.62919235229492, train/wer=3.967630, validation/ctc_loss=31.704492568969727, validation/num_examples=5348, validation/wer=3.633552
I0530 19:56:14.985060 140183515338496 logging_writer.py:48] [100] global_step=100, grad_norm=6.630863189697266, loss=6.185614109039307
I0530 19:57:31.278022 140183523731200 logging_writer.py:48] [200] global_step=200, grad_norm=5.191328525543213, loss=6.137206554412842
I0530 19:58:46.916328 140183515338496 logging_writer.py:48] [300] global_step=300, grad_norm=1.5950915813446045, loss=5.948419570922852
I0530 20:00:02.057728 140183523731200 logging_writer.py:48] [400] global_step=400, grad_norm=2.1610231399536133, loss=6.268798828125
I0530 20:01:18.549395 140183515338496 logging_writer.py:48] [500] global_step=500, grad_norm=0.8585325479507446, loss=5.838226795196533
I0530 20:02:33.514150 140183523731200 logging_writer.py:48] [600] global_step=600, grad_norm=0.41822686791419983, loss=5.699166297912598
I0530 20:03:52.537825 140183515338496 logging_writer.py:48] [700] global_step=700, grad_norm=2.4575841426849365, loss=5.472506046295166
I0530 20:05:13.580875 140183523731200 logging_writer.py:48] [800] global_step=800, grad_norm=1.0107128620147705, loss=4.923712253570557
I0530 20:06:35.832559 140183515338496 logging_writer.py:48] [900] global_step=900, grad_norm=1.7974669933319092, loss=4.6067609786987305
I0530 20:07:56.112519 140183523731200 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2012263536453247, loss=4.212714672088623
I0530 20:09:15.931676 140184876414720 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.8517587184906006, loss=4.1497297286987305
I0530 20:10:30.695702 140184868022016 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3999509811401367, loss=3.780057430267334
I0530 20:11:45.962976 140184876414720 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.6325409412384033, loss=3.64717960357666
I0530 20:13:00.518053 140184868022016 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.894858717918396, loss=3.477050542831421
I0530 20:14:15.477305 140184876414720 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.2591006755828857, loss=3.370435953140259
I0530 20:15:29.901493 140184868022016 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.8673360347747803, loss=3.384601354598999
I0530 20:16:50.675202 140184876414720 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.2409560680389404, loss=3.1694300174713135
I0530 20:18:13.382787 140184868022016 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0088553428649902, loss=3.071080446243286
I0530 20:19:32.878826 140184876414720 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.742362380027771, loss=3.116379737854004
I0530 20:20:53.476543 140184868022016 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0314676761627197, loss=2.9722800254821777
I0530 20:22:16.504540 140185531774720 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3730237483978271, loss=2.9975638389587402
I0530 20:23:31.788091 140185523382016 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1194690465927124, loss=2.884587049484253
I0530 20:24:46.443819 140185531774720 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.18671452999115, loss=2.8448235988616943
I0530 20:26:00.296499 140185523382016 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3442834615707397, loss=2.806248903274536
I0530 20:27:14.404015 140185531774720 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.116171956062317, loss=2.7707035541534424
I0530 20:28:31.469765 140185523382016 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.072508454322815, loss=2.7526423931121826
I0530 20:29:56.382062 140185531774720 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2163112163543701, loss=2.7408647537231445
I0530 20:31:18.799791 140185523382016 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9251285195350647, loss=2.660048007965088
I0530 20:32:39.498051 140185531774720 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.225911021232605, loss=2.746962785720825
I0530 20:34:03.127466 140185523382016 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1897.540283203125
I0530 20:34:53.797349 140343430178624 spec.py:298] Evaluating on the training split.
I0530 20:35:23.616347 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 20:36:01.613965 140343430178624 spec.py:326] Evaluating on the test split.
I0530 20:36:19.369119 140343430178624 submission_runner.py:426] Time since start: 2770.88s, 	Step: 3061, 	{'train/ctc_loss': Array(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2460.2899267673492, 'total_duration': 2770.8774259090424, 'accumulated_submission_time': 2460.2899267673492, 'accumulated_data_selection_time': 513.4844875335693, 'accumulated_eval_time': 310.51180839538574, 'accumulated_logging_time': 0.032887935638427734}
I0530 20:36:19.387729 140185531774720 logging_writer.py:48] [3061] accumulated_data_selection_time=513.484488, accumulated_eval_time=310.511808, accumulated_logging_time=0.032888, accumulated_submission_time=2460.289927, global_step=3061, preemption_count=0, score=2460.289927, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2770.877426, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0530 20:36:52.464977 140184876414720 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1862.203857421875
I0530 20:38:06.348639 140184868022016 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1838.3997802734375
I0530 20:39:20.029930 140184876414720 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1835.1051025390625
I0530 20:40:35.635350 140184868022016 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1849.156005859375
I0530 20:41:55.408631 140184876414720 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1799.501953125
I0530 20:43:16.006244 140184868022016 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1822.1734619140625
I0530 20:44:40.027161 140184876414720 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1778.4990234375
I0530 20:45:59.380213 140184868022016 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1823.9912109375
I0530 20:47:18.173025 140184876414720 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1845.556640625
I0530 20:48:41.384481 140184868022016 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1854.111328125
I0530 20:50:04.119211 140184876414720 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1857.8790283203125
I0530 20:51:22.753104 140184221054720 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1772.09716796875
I0530 20:52:36.805560 140184212662016 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1819.712646484375
I0530 20:53:50.608354 140184221054720 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1766.838134765625
I0530 20:55:04.771882 140184212662016 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1840.6468505859375
I0530 20:56:25.783490 140184221054720 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1906.01708984375
I0530 20:57:50.421895 140184212662016 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1793.45068359375
I0530 20:59:13.076358 140184221054720 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1774.0621337890625
I0530 21:00:35.087793 140184212662016 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1846.0889892578125
I0530 21:01:53.802813 140184221054720 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1812.75439453125
I0530 21:03:16.741391 140184212662016 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1780.1068115234375
I0530 21:04:37.152276 140183565694720 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1818.032470703125
I0530 21:05:50.931400 140183557302016 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1795.085693359375
I0530 21:07:04.615888 140183565694720 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1810.0625
I0530 21:08:19.390176 140183557302016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1869.2747802734375
I0530 21:09:38.369681 140183565694720 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1801.14794921875
I0530 21:11:00.363170 140183557302016 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1787.315185546875
I0530 21:12:23.863531 140183565694720 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1862.068359375
I0530 21:13:47.377918 140183557302016 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1828.8125
I0530 21:15:09.956100 140183565694720 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1803.304931640625
I0530 21:16:20.601898 140343430178624 spec.py:298] Evaluating on the training split.
I0530 21:16:50.252694 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 21:17:28.476735 140343430178624 spec.py:326] Evaluating on the test split.
I0530 21:17:48.795058 140343430178624 submission_runner.py:426] Time since start: 5260.30s, 	Step: 6084, 	{'train/ctc_loss': Array(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4861.447588682175, 'total_duration': 5260.30313372612, 'accumulated_submission_time': 4861.447588682175, 'accumulated_data_selection_time': 1126.5808508396149, 'accumulated_eval_time': 398.7012503147125, 'accumulated_logging_time': 0.06521248817443848}
I0530 21:17:48.814683 140183565694720 logging_writer.py:48] [6084] accumulated_data_selection_time=1126.580851, accumulated_eval_time=398.701250, accumulated_logging_time=0.065212, accumulated_submission_time=4861.447589, global_step=6084, preemption_count=0, score=4861.447589, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5260.303134, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0530 21:18:01.423873 140183557302016 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1782.8341064453125
I0530 21:19:19.127347 140184876414720 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1780.849853515625
I0530 21:20:32.796832 140184868022016 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1839.9853515625
I0530 21:21:46.711040 140184876414720 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1771.6065673828125
I0530 21:23:01.012190 140184868022016 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1773.570556640625
I0530 21:24:16.129180 140184876414720 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1853.0377197265625
I0530 21:25:35.425232 140184868022016 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1764.4024658203125
I0530 21:26:55.326284 140184876414720 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1833.9219970703125
I0530 21:28:16.685123 140184868022016 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1845.1575927734375
I0530 21:29:39.619851 140184876414720 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1844.625732421875
I0530 21:31:00.754139 140184868022016 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1789.31396484375
I0530 21:32:21.661963 140184876414720 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1731.9339599609375
I0530 21:33:39.626702 140184876414720 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1775.41552734375
I0530 21:34:54.484893 140184868022016 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1815.7115478515625
I0530 21:36:08.488040 140184876414720 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1794.204833984375
I0530 21:37:22.688611 140184868022016 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1822.822265625
I0530 21:38:43.853375 140184876414720 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1799.62841796875
I0530 21:40:07.258003 140184868022016 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1867.91064453125
I0530 21:41:29.913980 140184876414720 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1817.3873291015625
I0530 21:42:50.633311 140184868022016 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1821.9141845703125
I0530 21:44:11.810048 140184876414720 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1836.0263671875
I0530 21:45:29.920244 140184868022016 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1854.7830810546875
I0530 21:46:52.690175 140184876414720 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1777.3878173828125
I0530 21:48:06.407635 140184868022016 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1765.7412109375
I0530 21:49:20.147167 140184876414720 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1873.3782958984375
I0530 21:50:35.067891 140184868022016 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1819.1953125
I0530 21:51:56.101175 140184876414720 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1903.46630859375
I0530 21:53:18.328364 140184868022016 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1862.7459716796875
I0530 21:54:43.018048 140184876414720 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1834.1845703125
I0530 21:56:03.918602 140184868022016 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1864.102294921875
I0530 21:57:24.746401 140184876414720 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1810.57470703125
I0530 21:57:49.013374 140343430178624 spec.py:298] Evaluating on the training split.
I0530 21:58:20.277906 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 21:58:58.425815 140343430178624 spec.py:326] Evaluating on the test split.
I0530 21:59:18.043998 140343430178624 submission_runner.py:426] Time since start: 7749.55s, 	Step: 9133, 	{'train/ctc_loss': Array(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7261.587747097015, 'total_duration': 7749.552248477936, 'accumulated_submission_time': 7261.587747097015, 'accumulated_data_selection_time': 1713.2121760845184, 'accumulated_eval_time': 487.7283399105072, 'accumulated_logging_time': 0.09965872764587402}
I0530 21:59:18.064383 140184876414720 logging_writer.py:48] [9133] accumulated_data_selection_time=1713.212176, accumulated_eval_time=487.728340, accumulated_logging_time=0.099659, accumulated_submission_time=7261.587747, global_step=9133, preemption_count=0, score=7261.587747, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7749.552248, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0530 22:00:08.315662 140184868022016 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1748.017578125
I0530 22:01:25.387763 140183565694720 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1832.4779052734375
I0530 22:02:39.514151 140183557302016 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1823.8612060546875
I0530 22:03:53.492411 140183565694720 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1801.7818603515625
I0530 22:05:08.736360 140183557302016 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1813.6533203125
I0530 22:06:31.825531 140183565694720 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1774.30810546875
I0530 22:07:51.477001 140183557302016 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1856.397216796875
I0530 22:09:13.295691 140183565694720 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1752.56494140625
I0530 22:10:34.603327 140183557302016 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1847.421142578125
I0530 22:11:52.279159 140183565694720 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1809.1671142578125
I0530 22:13:14.707843 140183557302016 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1844.0941162109375
I0530 22:14:43.018870 140185531774720 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1819.8419189453125
I0530 22:15:58.415010 140185523382016 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1784.6986083984375
I0530 22:17:13.319367 140185531774720 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1843.0321044921875
I0530 22:18:27.282326 140185523382016 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1809.5509033203125
I0530 22:19:41.207480 140185531774720 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1820.3594970703125
I0530 22:20:59.394103 140185523382016 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1819.3245849609375
I0530 22:22:21.328233 140185531774720 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1916.86376953125
I0530 22:23:47.413666 140185523382016 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1763.308837890625
I0530 22:25:13.849167 140185531774720 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1801.5281982421875
I0530 22:26:38.526401 140185523382016 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1947.4869384765625
I0530 22:28:04.453726 140185531774720 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1836.289794921875
I0530 22:29:24.919122 140184221054720 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1919.1632080078125
I0530 22:30:39.242999 140184212662016 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1794.7081298828125
I0530 22:31:54.341248 140184221054720 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1820.1007080078125
I0530 22:33:10.855184 140184212662016 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1928.560791015625
I0530 22:34:31.161489 140184221054720 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1809.6788330078125
I0530 22:35:55.927655 140184212662016 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1824.12109375
I0530 22:37:20.074946 140184221054720 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1820.1007080078125
I0530 22:38:48.317328 140184212662016 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1811.5997314453125
I0530 22:39:18.225842 140343430178624 spec.py:298] Evaluating on the training split.
I0530 22:39:49.348604 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 22:40:25.488418 140343430178624 spec.py:326] Evaluating on the test split.
I0530 22:40:44.677835 140343430178624 submission_runner.py:426] Time since start: 10236.19s, 	Step: 12139, 	{'train/ctc_loss': Array(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9661.68955540657, 'total_duration': 10236.18584895134, 'accumulated_submission_time': 9661.68955540657, 'accumulated_data_selection_time': 2336.750143289566, 'accumulated_eval_time': 574.1765534877777, 'accumulated_logging_time': 0.13521599769592285}
I0530 22:40:44.698410 140185531774720 logging_writer.py:48] [12139] accumulated_data_selection_time=2336.750143, accumulated_eval_time=574.176553, accumulated_logging_time=0.135216, accumulated_submission_time=9661.689555, global_step=12139, preemption_count=0, score=9661.689555, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=10236.185849, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0530 22:41:30.618965 140185523382016 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1808.0172119140625
I0530 22:42:44.265643 140185531774720 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.021082747727632523, loss=1822.953125
I0530 22:44:02.018104 140185531774720 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1801.274658203125
I0530 22:45:15.946261 140185523382016 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1817.645263671875
I0530 22:46:29.730882 140185531774720 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1826.4638671875
I0530 22:47:44.275679 140185523382016 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1879.5679931640625
I0530 22:49:03.586352 140185531774720 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1847.021240234375
I0530 22:50:24.348529 140185523382016 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1881.501953125
I0530 22:51:47.806217 140185531774720 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1923.9228515625
I0530 22:53:15.659243 140185523382016 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1833.52783203125
I0530 22:54:39.110625 140185531774720 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1890.8128662109375
I0530 22:56:05.675858 140185523382016 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1872.55615234375
I0530 22:57:34.674776 140185531774720 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1820.1007080078125
I0530 22:58:49.245370 140185523382016 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1804.32177734375
I0530 23:00:02.909501 140185531774720 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1831.953369140625
I0530 23:01:17.298380 140185523382016 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1868.4559326171875
I0530 23:02:37.033059 140185531774720 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1861.7974853515625
I0530 23:04:00.284645 140185523382016 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1875.02490234375
I0530 23:05:24.648850 140185531774720 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1776.27783203125
I0530 23:06:48.570949 140185523382016 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1834.316162109375
I0530 23:08:13.044835 140185531774720 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1823.6014404296875
I0530 23:09:35.126577 140185523382016 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1925.0802001953125
I0530 23:11:01.202270 140185531774720 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1808.78369140625
I0530 23:12:20.202899 140185531774720 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1842.8995361328125
I0530 23:13:34.925939 140185523382016 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1749.809814453125
I0530 23:14:48.885331 140185531774720 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1826.4638671875
I0530 23:16:02.859650 140185523382016 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1772.7108154296875
I0530 23:17:27.614833 140185531774720 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1851.295654296875
I0530 23:18:49.970210 140185523382016 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1881.9169921875
I0530 23:20:14.128942 140185531774720 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1951.495361328125
I0530 23:20:45.082134 140343430178624 spec.py:298] Evaluating on the training split.
I0530 23:21:16.658621 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 23:21:53.274053 140343430178624 spec.py:326] Evaluating on the test split.
I0530 23:22:12.767951 140343430178624 submission_runner.py:426] Time since start: 12724.28s, 	Step: 15137, 	{'train/ctc_loss': Array(1832.9214, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12062.012372732162, 'total_duration': 12724.275902032852, 'accumulated_submission_time': 12062.012372732162, 'accumulated_data_selection_time': 2972.1731798648834, 'accumulated_eval_time': 661.8585214614868, 'accumulated_logging_time': 0.17133045196533203}
I0530 23:22:12.789341 140185531774720 logging_writer.py:48] [15137] accumulated_data_selection_time=2972.173180, accumulated_eval_time=661.858521, accumulated_logging_time=0.171330, accumulated_submission_time=12062.012373, global_step=15137, preemption_count=0, score=12062.012373, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=12724.275902, train/ctc_loss=1832.92138671875, train/wer=0.941551, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0530 23:23:00.583047 140185523382016 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1802.4161376953125
I0530 23:24:14.579149 140185531774720 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1829.4658203125
I0530 23:25:28.447029 140185523382016 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1794.0791015625
I0530 23:26:50.955010 140185531774720 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1773.570556640625
I0530 23:28:05.173776 140185523382016 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1827.1156005859375
I0530 23:29:20.357454 140185531774720 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1855.7242431640625
I0530 23:30:35.772471 140185523382016 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1822.822265625
I0530 23:31:53.354503 140185531774720 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1814.9390869140625
I0530 23:33:14.913220 140343430178624 spec.py:298] Evaluating on the training split.
I0530 23:33:46.382743 140343430178624 spec.py:310] Evaluating on the validation split.
I0530 23:34:22.663295 140343430178624 spec.py:326] Evaluating on the test split.
I0530 23:34:42.151645 140343430178624 submission_runner.py:426] Time since start: 13473.66s, 	Step: 16000, 	{'train/ctc_loss': Array(1752.7933, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12724.107449054718, 'total_duration': 13473.661215543747, 'accumulated_submission_time': 12724.107449054718, 'accumulated_data_selection_time': 3113.138128757477, 'accumulated_eval_time': 749.0947515964508, 'accumulated_logging_time': 0.20829558372497559}
I0530 23:34:42.170489 140184948094720 logging_writer.py:48] [16000] accumulated_data_selection_time=3113.138129, accumulated_eval_time=749.094752, accumulated_logging_time=0.208296, accumulated_submission_time=12724.107449, global_step=16000, preemption_count=0, score=12724.107449, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=13473.661216, train/ctc_loss=1752.7933349609375, train/wer=0.942641, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0530 23:34:42.189435 140184939702016 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12724.107449
I0530 23:34:42.276557 140343430178624 checkpoints.py:490] Saving checkpoint at step: 16000
I0530 23:34:42.841817 140343430178624 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0530 23:34:42.856137 140343430178624 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_upgrade_b/nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0530 23:34:44.001359 140343430178624 submission_runner.py:589] Tuning trial 1/1
I0530 23:34:44.001580 140343430178624 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0530 23:34:44.006199 140343430178624 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.629192, dtype=float32), 'train/wer': 3.9676300939133333, 'validation/ctc_loss': Array(31.704493, dtype=float32), 'validation/wer': 3.6335516985209697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.745237, dtype=float32), 'test/wer': 3.8643389596408912, 'test/num_examples': 2472, 'score': 59.84392189979553, 'total_duration': 284.7875916957855, 'accumulated_submission_time': 59.84392189979553, 'accumulated_data_selection_time': 5.048012733459473, 'accumulated_eval_time': 224.94352436065674, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3061, {'train/ctc_loss': Array(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2460.2899267673492, 'total_duration': 2770.8774259090424, 'accumulated_submission_time': 2460.2899267673492, 'accumulated_data_selection_time': 513.4844875335693, 'accumulated_eval_time': 310.51180839538574, 'accumulated_logging_time': 0.032887935638427734, 'global_step': 3061, 'preemption_count': 0}), (6084, {'train/ctc_loss': Array(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4861.447588682175, 'total_duration': 5260.30313372612, 'accumulated_submission_time': 4861.447588682175, 'accumulated_data_selection_time': 1126.5808508396149, 'accumulated_eval_time': 398.7012503147125, 'accumulated_logging_time': 0.06521248817443848, 'global_step': 6084, 'preemption_count': 0}), (9133, {'train/ctc_loss': Array(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7261.587747097015, 'total_duration': 7749.552248477936, 'accumulated_submission_time': 7261.587747097015, 'accumulated_data_selection_time': 1713.2121760845184, 'accumulated_eval_time': 487.7283399105072, 'accumulated_logging_time': 0.09965872764587402, 'global_step': 9133, 'preemption_count': 0}), (12139, {'train/ctc_loss': Array(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9661.68955540657, 'total_duration': 10236.18584895134, 'accumulated_submission_time': 9661.68955540657, 'accumulated_data_selection_time': 2336.750143289566, 'accumulated_eval_time': 574.1765534877777, 'accumulated_logging_time': 0.13521599769592285, 'global_step': 12139, 'preemption_count': 0}), (15137, {'train/ctc_loss': Array(1832.9214, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12062.012372732162, 'total_duration': 12724.275902032852, 'accumulated_submission_time': 12062.012372732162, 'accumulated_data_selection_time': 2972.1731798648834, 'accumulated_eval_time': 661.8585214614868, 'accumulated_logging_time': 0.17133045196533203, 'global_step': 15137, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(1752.7933, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12724.107449054718, 'total_duration': 13473.661215543747, 'accumulated_submission_time': 12724.107449054718, 'accumulated_data_selection_time': 3113.138128757477, 'accumulated_eval_time': 749.0947515964508, 'accumulated_logging_time': 0.20829558372497559, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0530 23:34:44.006342 140343430178624 submission_runner.py:592] Timing: 12724.107449054718
I0530 23:34:44.006410 140343430178624 submission_runner.py:593] ====================
I0530 23:34:44.007384 140343430178624 submission_runner.py:661] Final librispeech_deepspeech score: 12724.107449054718
