2023-08-08 22:32:23.056747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File "submission_runner.py", line 620, in <module>
    app.run(main)
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "submission_runner.py", line 576, in main
    workload = workloads.import_workload(
  File "/algorithmic-efficiency/algorithmic_efficiency/workloads/workloads.py", line 87, in import_workload
    workload_module = importlib.import_module(workload_path)
  File "/usr/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 848, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/algorithmic-efficiency/algorithmic_efficiency/workloads/ogbg/ogbg_jax/workload.py", line 9, in <module>
    import optax
  File "/usr/local/lib/python3.8/dist-packages/optax/__init__.py", line 18, in <module>
    from optax._src.alias import adabelief
  File "/usr/local/lib/python3.8/dist-packages/optax/_src/alias.py", line 23, in <module>
    from optax._src import clipping
  File "/usr/local/lib/python3.8/dist-packages/optax/_src/clipping.py", line 130, in <module>
    ) -> Tuple[List[chex.Array], jax.Array]:
AttributeError: module 'jax' has no attribute 'Array'
2023-08-08 22:53:13.020727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0808 22:53:25.114324 140178263553856 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax.
Fatal Python error: Segmentation fault

Current thread 0x00007f7dcb9ab740 (most recent call first):
  File "/usr/local/lib/python3.8/dist-packages/jaxlib/xla_client.py", line 63 in make_cpu_client
  File "/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py", line 377 in _init_backend
  File "/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py", line 326 in backends
  File "/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py", line 401 in _get_backend_uncached
  File "/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py", line 417 in get_backend
  File "/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py", line 449 in local_device_count
  File "submission_runner.py", line 458 in score_submission_on_workload
  File "submission_runner.py", line 591 in main
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 254 in _run_main
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 308 in run
  File "submission_runner.py", line 620 in <module>
2023-08-08 22:59:09.492378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0808 22:59:21.161365 140716693473088 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax because --overwrite was set.
I0808 22:59:21.161753 140716693473088 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax.
I0808 22:59:21.221378 140716693473088 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0808 22:59:22.080161 140716693473088 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0808 22:59:22.080815 140716693473088 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0808 22:59:22.358708 140716693473088 submission_runner.py:490] Using RNG seed 1373049251
I0808 22:59:25.009732 140716693473088 submission_runner.py:499] --- Tuning run 1/1 ---
I0808 22:59:25.009926 140716693473088 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1.
I0808 22:59:25.010259 140716693473088 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1/hparams.json.
I0808 22:59:25.155172 140716693473088 submission_runner.py:176] Initializing dataset.
I0808 22:59:25.430353 140716693473088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 22:59:25.435828 140716693473088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
W0808 22:59:25.650552 140716693473088 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0808 22:59:25.710820 140716693473088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 22:59:25.785370 140716693473088 submission_runner.py:183] Initializing model.
I0808 22:59:33.600862 140716693473088 submission_runner.py:217] Initializing optimizer.
I0808 22:59:34.048640 140716693473088 submission_runner.py:224] Initializing metrics bundle.
I0808 22:59:34.048834 140716693473088 submission_runner.py:242] Initializing checkpoint and logger.
I0808 22:59:34.049800 140716693473088 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1 with prefix checkpoint_
I0808 22:59:34.050060 140716693473088 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0808 22:59:34.050343 140716693473088 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0808 22:59:34.813884 140716693473088 submission_runner.py:263] Saving meta data to /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1/meta_data_0.json.
I0808 22:59:34.815043 140716693473088 submission_runner.py:266] Saving flags to /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1/flags_0.json.
I0808 22:59:34.824532 140716693473088 submission_runner.py:276] Starting training loop.
I0808 22:59:58.080376 140534307874560 logging_writer.py:48] [0] global_step=0, grad_norm=3.135578155517578, loss=0.7091574668884277
I0808 22:59:58.097465 140716693473088 spec.py:320] Evaluating on the training split.
I0808 22:59:58.105859 140716693473088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 22:59:58.110226 140716693473088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0808 22:59:58.178815 140716693473088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 23:01:46.567157 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:01:46.570499 140716693473088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 23:01:46.574715 140716693473088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0808 23:01:46.643692 140716693473088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 23:03:02.828706 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:03:02.832125 140716693473088 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 23:03:02.836652 140716693473088 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0808 23:03:02.907661 140716693473088 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 23:04:21.397737 140716693473088 submission_runner.py:364] Time since start: 286.57s, 	Step: 1, 	{'train/accuracy': 0.5509048104286194, 'train/loss': 0.7097761631011963, 'train/mean_average_precision': 0.02417895735785936, 'validation/accuracy': 0.5491881966590881, 'validation/loss': 0.7092559337615967, 'validation/mean_average_precision': 0.026155667842552938, 'validation/num_examples': 43793, 'test/accuracy': 0.5480591654777527, 'test/loss': 0.7092627882957458, 'test/mean_average_precision': 0.027720831926397668, 'test/num_examples': 43793, 'score': 23.272895574569702, 'total_duration': 286.5731453895569, 'accumulated_submission_time': 23.272895574569702, 'accumulated_eval_time': 263.3002288341522, 'accumulated_logging_time': 0}
I0808 23:04:21.419065 140524081706752 logging_writer.py:48] [1] accumulated_eval_time=263.300229, accumulated_logging_time=0, accumulated_submission_time=23.272896, global_step=1, preemption_count=0, score=23.272896, test/accuracy=0.548059, test/loss=0.709263, test/mean_average_precision=0.027721, test/num_examples=43793, total_duration=286.573145, train/accuracy=0.550905, train/loss=0.709776, train/mean_average_precision=0.024179, validation/accuracy=0.549188, validation/loss=0.709256, validation/mean_average_precision=0.026156, validation/num_examples=43793
I0808 23:04:51.806707 140524090099456 logging_writer.py:48] [100] global_step=100, grad_norm=0.38697272539138794, loss=0.3373229205608368
I0808 23:05:22.471575 140524081706752 logging_writer.py:48] [200] global_step=200, grad_norm=0.2437126636505127, loss=0.1994144469499588
I0808 23:05:53.455872 140524090099456 logging_writer.py:48] [300] global_step=300, grad_norm=0.12323956191539764, loss=0.11488669365644455
I0808 23:06:24.093222 140524081706752 logging_writer.py:48] [400] global_step=400, grad_norm=0.06660518795251846, loss=0.07860312610864639
I0808 23:06:54.820742 140524090099456 logging_writer.py:48] [500] global_step=500, grad_norm=0.05084037408232689, loss=0.06374270468950272
I0808 23:07:25.531834 140524081706752 logging_writer.py:48] [600] global_step=600, grad_norm=0.020413566380739212, loss=0.056910011917352676
I0808 23:07:56.565600 140524090099456 logging_writer.py:48] [700] global_step=700, grad_norm=0.014364080503582954, loss=0.058514442294836044
I0808 23:08:21.497224 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:09:49.976998 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:09:52.730677 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:09:55.360135 140716693473088 submission_runner.py:364] Time since start: 620.54s, 	Step: 782, 	{'train/accuracy': 0.9867521524429321, 'train/loss': 0.05552644655108452, 'train/mean_average_precision': 0.03981013318739947, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06525621563196182, 'validation/mean_average_precision': 0.040737630084511145, 'validation/num_examples': 43793, 'test/accuracy': 0.9831416606903076, 'test/loss': 0.06844700127840042, 'test/mean_average_precision': 0.042179951585058724, 'test/num_examples': 43793, 'score': 263.32942152023315, 'total_duration': 620.5355370044708, 'accumulated_submission_time': 263.32942152023315, 'accumulated_eval_time': 357.1631269454956, 'accumulated_logging_time': 0.03316473960876465}
I0808 23:09:55.374911 140524081706752 logging_writer.py:48] [782] accumulated_eval_time=357.163127, accumulated_logging_time=0.033165, accumulated_submission_time=263.329422, global_step=782, preemption_count=0, score=263.329422, test/accuracy=0.983142, test/loss=0.068447, test/mean_average_precision=0.042180, test/num_examples=43793, total_duration=620.535537, train/accuracy=0.986752, train/loss=0.055526, train/mean_average_precision=0.039810, validation/accuracy=0.984118, validation/loss=0.065256, validation/mean_average_precision=0.040738, validation/num_examples=43793
I0808 23:10:01.218593 140524090099456 logging_writer.py:48] [800] global_step=800, grad_norm=0.019794495776295662, loss=0.049643684178590775
I0808 23:10:31.699982 140524081706752 logging_writer.py:48] [900] global_step=900, grad_norm=0.0439738929271698, loss=0.06198534741997719
I0808 23:11:02.602652 140524090099456 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03249967098236084, loss=0.05198115110397339
I0808 23:11:33.135169 140524081706752 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.023519938811659813, loss=0.05940000340342522
I0808 23:12:03.978000 140524090099456 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.023159053176641464, loss=0.05565064400434494
I0808 23:12:34.691691 140524081706752 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.021149197593331337, loss=0.051121510565280914
I0808 23:13:05.472011 140524090099456 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.017364848405122757, loss=0.0448492169380188
I0808 23:13:36.182405 140524081706752 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.01656222529709339, loss=0.052963439375162125
I0808 23:13:55.525861 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:15:24.107949 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:15:26.800878 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:15:29.400982 140716693473088 submission_runner.py:364] Time since start: 954.58s, 	Step: 1564, 	{'train/accuracy': 0.9873191118240356, 'train/loss': 0.04756956174969673, 'train/mean_average_precision': 0.08729697997935545, 'validation/accuracy': 0.9843878746032715, 'validation/loss': 0.05753958597779274, 'validation/mean_average_precision': 0.08749819625195836, 'validation/num_examples': 43793, 'test/accuracy': 0.983444094657898, 'test/loss': 0.06091226637363434, 'test/mean_average_precision': 0.08504896795049782, 'test/num_examples': 43793, 'score': 503.4597280025482, 'total_duration': 954.5763819217682, 'accumulated_submission_time': 503.4597280025482, 'accumulated_eval_time': 451.0382101535797, 'accumulated_logging_time': 0.05925488471984863}
I0808 23:15:29.415868 140524090099456 logging_writer.py:48] [1564] accumulated_eval_time=451.038210, accumulated_logging_time=0.059255, accumulated_submission_time=503.459728, global_step=1564, preemption_count=0, score=503.459728, test/accuracy=0.983444, test/loss=0.060912, test/mean_average_precision=0.085049, test/num_examples=43793, total_duration=954.576382, train/accuracy=0.987319, train/loss=0.047570, train/mean_average_precision=0.087297, validation/accuracy=0.984388, validation/loss=0.057540, validation/mean_average_precision=0.087498, validation/num_examples=43793
I0808 23:15:40.991988 140524081706752 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.022345533594489098, loss=0.05210893973708153
I0808 23:16:12.211070 140524090099456 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.02112426422536373, loss=0.04491080343723297
I0808 23:16:42.542896 140524081706752 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03608526289463043, loss=0.04803667962551117
I0808 23:17:13.113557 140524090099456 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.017867574468255043, loss=0.04778806120157242
I0808 23:17:43.639000 140524081706752 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.03599921241402626, loss=0.04579788073897362
I0808 23:18:14.396665 140524090099456 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.02761571668088436, loss=0.0537542887032032
I0808 23:18:45.107642 140524081706752 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.03393859788775444, loss=0.04827936366200447
I0808 23:19:16.049044 140524090099456 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.043547872453927994, loss=0.048244085162878036
I0808 23:19:29.693666 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:21:02.223835 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:21:04.865379 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:21:07.448189 140716693473088 submission_runner.py:364] Time since start: 1292.62s, 	Step: 2346, 	{'train/accuracy': 0.9874812960624695, 'train/loss': 0.0461803637444973, 'train/mean_average_precision': 0.1212880646554185, 'validation/accuracy': 0.9847479462623596, 'validation/loss': 0.054831549525260925, 'validation/mean_average_precision': 0.11518333135725743, 'validation/num_examples': 43793, 'test/accuracy': 0.9837309122085571, 'test/loss': 0.05799630656838417, 'test/mean_average_precision': 0.11341232453402282, 'test/num_examples': 43793, 'score': 743.7169682979584, 'total_duration': 1292.623574256897, 'accumulated_submission_time': 743.7169682979584, 'accumulated_eval_time': 548.7926774024963, 'accumulated_logging_time': 0.08535122871398926}
I0808 23:21:07.463112 140524081706752 logging_writer.py:48] [2346] accumulated_eval_time=548.792677, accumulated_logging_time=0.085351, accumulated_submission_time=743.716968, global_step=2346, preemption_count=0, score=743.716968, test/accuracy=0.983731, test/loss=0.057996, test/mean_average_precision=0.113412, test/num_examples=43793, total_duration=1292.623574, train/accuracy=0.987481, train/loss=0.046180, train/mean_average_precision=0.121288, validation/accuracy=0.984748, validation/loss=0.054832, validation/mean_average_precision=0.115183, validation/num_examples=43793
I0808 23:21:24.050714 140524090099456 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.015178773552179337, loss=0.04744425788521767
I0808 23:21:54.421292 140524081706752 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.023188497871160507, loss=0.04882588982582092
I0808 23:22:25.088757 140524090099456 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.04855254292488098, loss=0.04995224252343178
I0808 23:22:55.854484 140524081706752 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.01911759376525879, loss=0.0513659305870533
I0808 23:23:26.671696 140524090099456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.01825222559273243, loss=0.047468725591897964
I0808 23:23:57.438830 140524081706752 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.019636735320091248, loss=0.044237468391656876
I0808 23:24:28.027070 140524090099456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.023925984278321266, loss=0.05456234887242317
I0808 23:24:58.289320 140524081706752 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.024807274341583252, loss=0.049537915736436844
I0808 23:25:07.522137 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:26:38.822712 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:26:41.476410 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:26:44.052870 140716693473088 submission_runner.py:364] Time since start: 1629.23s, 	Step: 3131, 	{'train/accuracy': 0.9875819087028503, 'train/loss': 0.04418851062655449, 'train/mean_average_precision': 0.14521321833596165, 'validation/accuracy': 0.9848875999450684, 'validation/loss': 0.05314984545111656, 'validation/mean_average_precision': 0.13927737548499036, 'validation/num_examples': 43793, 'test/accuracy': 0.9839520454406738, 'test/loss': 0.056110680103302, 'test/mean_average_precision': 0.13789227830344322, 'test/num_examples': 43793, 'score': 983.754711151123, 'total_duration': 1629.2282497882843, 'accumulated_submission_time': 983.754711151123, 'accumulated_eval_time': 645.3233511447906, 'accumulated_logging_time': 0.11208057403564453}
I0808 23:26:44.067851 140524090099456 logging_writer.py:48] [3131] accumulated_eval_time=645.323351, accumulated_logging_time=0.112081, accumulated_submission_time=983.754711, global_step=3131, preemption_count=0, score=983.754711, test/accuracy=0.983952, test/loss=0.056111, test/mean_average_precision=0.137892, test/num_examples=43793, total_duration=1629.228250, train/accuracy=0.987582, train/loss=0.044189, train/mean_average_precision=0.145213, validation/accuracy=0.984888, validation/loss=0.053150, validation/mean_average_precision=0.139277, validation/num_examples=43793
I0808 23:27:05.737159 140524081706752 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.01618061028420925, loss=0.044650182127952576
I0808 23:27:36.776260 140524090099456 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.011528696864843369, loss=0.05080787092447281
I0808 23:28:07.396821 140524081706752 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.013049636036157608, loss=0.046423107385635376
I0808 23:28:38.221030 140524090099456 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.013939857482910156, loss=0.043693769723176956
I0808 23:29:08.814224 140524081706752 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.011313887313008308, loss=0.04383080452680588
I0808 23:29:39.877938 140524090099456 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.015957823023200035, loss=0.04675212875008583
I0808 23:30:11.084836 140524081706752 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.014918255619704723, loss=0.04005837440490723
I0808 23:30:42.187408 140524090099456 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.009550010785460472, loss=0.046054985374212265
I0808 23:30:44.079233 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:32:13.564863 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:32:16.206276 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:32:18.784220 140716693473088 submission_runner.py:364] Time since start: 1963.96s, 	Step: 3907, 	{'train/accuracy': 0.987963855266571, 'train/loss': 0.04250131547451019, 'train/mean_average_precision': 0.15958180685949958, 'validation/accuracy': 0.9851506352424622, 'validation/loss': 0.051961708813905716, 'validation/mean_average_precision': 0.15176133594847055, 'validation/num_examples': 43793, 'test/accuracy': 0.9842531681060791, 'test/loss': 0.05488543584942818, 'test/mean_average_precision': 0.1501420091286211, 'test/num_examples': 43793, 'score': 1223.7456929683685, 'total_duration': 1963.9596226215363, 'accumulated_submission_time': 1223.7456929683685, 'accumulated_eval_time': 740.0282995700836, 'accumulated_logging_time': 0.13820314407348633}
I0808 23:32:18.800126 140524081706752 logging_writer.py:48] [3907] accumulated_eval_time=740.028300, accumulated_logging_time=0.138203, accumulated_submission_time=1223.745693, global_step=3907, preemption_count=0, score=1223.745693, test/accuracy=0.984253, test/loss=0.054885, test/mean_average_precision=0.150142, test/num_examples=43793, total_duration=1963.959623, train/accuracy=0.987964, train/loss=0.042501, train/mean_average_precision=0.159582, validation/accuracy=0.985151, validation/loss=0.051962, validation/mean_average_precision=0.151761, validation/num_examples=43793
I0808 23:32:47.558540 140524090099456 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.025966351851820946, loss=0.04437028616666794
I0808 23:33:17.912624 140524081706752 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01892119273543358, loss=0.04148856922984123
I0808 23:33:47.819305 140524090099456 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.02059982158243656, loss=0.05115126818418503
I0808 23:34:17.832625 140524081706752 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.020740605890750885, loss=0.04719036445021629
I0808 23:34:47.864086 140524090099456 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.010430642403662205, loss=0.039407387375831604
I0808 23:35:18.162916 140524081706752 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011917014606297016, loss=0.04205408692359924
I0808 23:35:48.748317 140524090099456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.02615899033844471, loss=0.04402773082256317
I0808 23:36:18.795138 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:37:48.303360 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:37:50.952358 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:37:53.560664 140716693473088 submission_runner.py:364] Time since start: 2298.74s, 	Step: 4700, 	{'train/accuracy': 0.9883637428283691, 'train/loss': 0.04054323583841324, 'train/mean_average_precision': 0.204209460108698, 'validation/accuracy': 0.9854559302330017, 'validation/loss': 0.04954395070672035, 'validation/mean_average_precision': 0.17679262829503528, 'validation/num_examples': 43793, 'test/accuracy': 0.9845012426376343, 'test/loss': 0.052348311990499496, 'test/mean_average_precision': 0.17126458653492962, 'test/num_examples': 43793, 'score': 1463.7202608585358, 'total_duration': 2298.736065387726, 'accumulated_submission_time': 1463.7202608585358, 'accumulated_eval_time': 834.7937891483307, 'accumulated_logging_time': 0.16519951820373535}
I0808 23:37:53.575905 140524081706752 logging_writer.py:48] [4700] accumulated_eval_time=834.793789, accumulated_logging_time=0.165200, accumulated_submission_time=1463.720261, global_step=4700, preemption_count=0, score=1463.720261, test/accuracy=0.984501, test/loss=0.052348, test/mean_average_precision=0.171265, test/num_examples=43793, total_duration=2298.736065, train/accuracy=0.988364, train/loss=0.040543, train/mean_average_precision=0.204209, validation/accuracy=0.985456, validation/loss=0.049544, validation/mean_average_precision=0.176793, validation/num_examples=43793
I0808 23:37:53.904789 140524090099456 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.014722323976457119, loss=0.04275516793131828
I0808 23:38:24.376283 140524081706752 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.010716395452618599, loss=0.04465066269040108
I0808 23:38:55.071917 140524090099456 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0075498903170228004, loss=0.039441969245672226
I0808 23:39:25.920575 140524081706752 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.011934055015444756, loss=0.04215443879365921
I0808 23:39:56.585813 140524090099456 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.013741987757384777, loss=0.04366485774517059
I0808 23:40:27.004863 140524081706752 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.01151897944509983, loss=0.041159335523843765
I0808 23:40:57.781742 140524090099456 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.014578860253095627, loss=0.044725656509399414
I0808 23:41:28.561158 140524081706752 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.02248738519847393, loss=0.04335736855864525
I0808 23:41:53.658425 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:43:22.520513 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:43:25.155348 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:43:27.768696 140716693473088 submission_runner.py:364] Time since start: 2632.94s, 	Step: 5483, 	{'train/accuracy': 0.9887433648109436, 'train/loss': 0.039443179965019226, 'train/mean_average_precision': 0.22515806639541963, 'validation/accuracy': 0.9856081604957581, 'validation/loss': 0.0487796850502491, 'validation/mean_average_precision': 0.18688946662364592, 'validation/num_examples': 43793, 'test/accuracy': 0.9847573637962341, 'test/loss': 0.051322996616363525, 'test/mean_average_precision': 0.1842455114166707, 'test/num_examples': 43793, 'score': 1703.7825529575348, 'total_duration': 2632.944098472595, 'accumulated_submission_time': 1703.7825529575348, 'accumulated_eval_time': 928.904022693634, 'accumulated_logging_time': 0.19126296043395996}
I0808 23:43:27.784075 140524090099456 logging_writer.py:48] [5483] accumulated_eval_time=928.904023, accumulated_logging_time=0.191263, accumulated_submission_time=1703.782553, global_step=5483, preemption_count=0, score=1703.782553, test/accuracy=0.984757, test/loss=0.051323, test/mean_average_precision=0.184246, test/num_examples=43793, total_duration=2632.944098, train/accuracy=0.988743, train/loss=0.039443, train/mean_average_precision=0.225158, validation/accuracy=0.985608, validation/loss=0.048780, validation/mean_average_precision=0.186889, validation/num_examples=43793
I0808 23:43:33.283034 140524081706752 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.010340217500925064, loss=0.04475774988532066
I0808 23:44:03.428124 140524090099456 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.016856370493769646, loss=0.04164263606071472
I0808 23:44:35.301887 140524081706752 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.012012571096420288, loss=0.036835819482803345
I0808 23:45:06.503926 140524090099456 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.007669524755328894, loss=0.03628897666931152
I0808 23:45:36.488939 140524081706752 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.008030549623072147, loss=0.040660470724105835
I0808 23:46:06.332588 140716693473088 spec.py:320] Evaluating on the training split.
I0808 23:47:35.060284 140716693473088 spec.py:332] Evaluating on the validation split.
I0808 23:47:37.716787 140716693473088 spec.py:348] Evaluating on the test split.
I0808 23:47:40.274199 140716693473088 submission_runner.py:364] Time since start: 2885.45s, 	Step: 6000, 	{'train/accuracy': 0.9890667200088501, 'train/loss': 0.03762779384851456, 'train/mean_average_precision': 0.26664871763933085, 'validation/accuracy': 0.9858062267303467, 'validation/loss': 0.04839012771844864, 'validation/mean_average_precision': 0.19711352614109282, 'validation/num_examples': 43793, 'test/accuracy': 0.9848874807357788, 'test/loss': 0.05111534148454666, 'test/mean_average_precision': 0.1940564979699422, 'test/num_examples': 43793, 'score': 1862.3139309883118, 'total_duration': 2885.449563264847, 'accumulated_submission_time': 1862.3139309883118, 'accumulated_eval_time': 1022.8455605506897, 'accumulated_logging_time': 0.2175612449645996}
I0808 23:47:40.290077 140524090099456 logging_writer.py:48] [6000] accumulated_eval_time=1022.845561, accumulated_logging_time=0.217561, accumulated_submission_time=1862.313931, global_step=6000, preemption_count=0, score=1862.313931, test/accuracy=0.984887, test/loss=0.051115, test/mean_average_precision=0.194056, test/num_examples=43793, total_duration=2885.449563, train/accuracy=0.989067, train/loss=0.037628, train/mean_average_precision=0.266649, validation/accuracy=0.985806, validation/loss=0.048390, validation/mean_average_precision=0.197114, validation/num_examples=43793
I0808 23:47:40.307016 140524081706752 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1862.313931
I0808 23:47:40.338697 140716693473088 checkpoints.py:356] Saving checkpoint at step: 6000
I0808 23:47:40.431475 140716693473088 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1/checkpoint_6000
I0808 23:47:40.432135 140716693473088 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_comparison_ogbg/adamw/ogbg_jax/trial_1/checkpoint_6000.
I0808 23:47:40.598442 140716693473088 submission_runner.py:530] Tuning trial 1/1
I0808 23:47:40.598720 140716693473088 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0808 23:47:40.599884 140716693473088 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5509048104286194, 'train/loss': 0.7097761631011963, 'train/mean_average_precision': 0.02417895735785936, 'validation/accuracy': 0.5491881966590881, 'validation/loss': 0.7092559337615967, 'validation/mean_average_precision': 0.026155667842552938, 'validation/num_examples': 43793, 'test/accuracy': 0.5480591654777527, 'test/loss': 0.7092627882957458, 'test/mean_average_precision': 0.027720831926397668, 'test/num_examples': 43793, 'score': 23.272895574569702, 'total_duration': 286.5731453895569, 'accumulated_submission_time': 23.272895574569702, 'accumulated_eval_time': 263.3002288341522, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (782, {'train/accuracy': 0.9867521524429321, 'train/loss': 0.05552644655108452, 'train/mean_average_precision': 0.03981013318739947, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.06525621563196182, 'validation/mean_average_precision': 0.040737630084511145, 'validation/num_examples': 43793, 'test/accuracy': 0.9831416606903076, 'test/loss': 0.06844700127840042, 'test/mean_average_precision': 0.042179951585058724, 'test/num_examples': 43793, 'score': 263.32942152023315, 'total_duration': 620.5355370044708, 'accumulated_submission_time': 263.32942152023315, 'accumulated_eval_time': 357.1631269454956, 'accumulated_logging_time': 0.03316473960876465, 'global_step': 782, 'preemption_count': 0}), (1564, {'train/accuracy': 0.9873191118240356, 'train/loss': 0.04756956174969673, 'train/mean_average_precision': 0.08729697997935545, 'validation/accuracy': 0.9843878746032715, 'validation/loss': 0.05753958597779274, 'validation/mean_average_precision': 0.08749819625195836, 'validation/num_examples': 43793, 'test/accuracy': 0.983444094657898, 'test/loss': 0.06091226637363434, 'test/mean_average_precision': 0.08504896795049782, 'test/num_examples': 43793, 'score': 503.4597280025482, 'total_duration': 954.5763819217682, 'accumulated_submission_time': 503.4597280025482, 'accumulated_eval_time': 451.0382101535797, 'accumulated_logging_time': 0.05925488471984863, 'global_step': 1564, 'preemption_count': 0}), (2346, {'train/accuracy': 0.9874812960624695, 'train/loss': 0.0461803637444973, 'train/mean_average_precision': 0.1212880646554185, 'validation/accuracy': 0.9847479462623596, 'validation/loss': 0.054831549525260925, 'validation/mean_average_precision': 0.11518333135725743, 'validation/num_examples': 43793, 'test/accuracy': 0.9837309122085571, 'test/loss': 0.05799630656838417, 'test/mean_average_precision': 0.11341232453402282, 'test/num_examples': 43793, 'score': 743.7169682979584, 'total_duration': 1292.623574256897, 'accumulated_submission_time': 743.7169682979584, 'accumulated_eval_time': 548.7926774024963, 'accumulated_logging_time': 0.08535122871398926, 'global_step': 2346, 'preemption_count': 0}), (3131, {'train/accuracy': 0.9875819087028503, 'train/loss': 0.04418851062655449, 'train/mean_average_precision': 0.14521321833596165, 'validation/accuracy': 0.9848875999450684, 'validation/loss': 0.05314984545111656, 'validation/mean_average_precision': 0.13927737548499036, 'validation/num_examples': 43793, 'test/accuracy': 0.9839520454406738, 'test/loss': 0.056110680103302, 'test/mean_average_precision': 0.13789227830344322, 'test/num_examples': 43793, 'score': 983.754711151123, 'total_duration': 1629.2282497882843, 'accumulated_submission_time': 983.754711151123, 'accumulated_eval_time': 645.3233511447906, 'accumulated_logging_time': 0.11208057403564453, 'global_step': 3131, 'preemption_count': 0}), (3907, {'train/accuracy': 0.987963855266571, 'train/loss': 0.04250131547451019, 'train/mean_average_precision': 0.15958180685949958, 'validation/accuracy': 0.9851506352424622, 'validation/loss': 0.051961708813905716, 'validation/mean_average_precision': 0.15176133594847055, 'validation/num_examples': 43793, 'test/accuracy': 0.9842531681060791, 'test/loss': 0.05488543584942818, 'test/mean_average_precision': 0.1501420091286211, 'test/num_examples': 43793, 'score': 1223.7456929683685, 'total_duration': 1963.9596226215363, 'accumulated_submission_time': 1223.7456929683685, 'accumulated_eval_time': 740.0282995700836, 'accumulated_logging_time': 0.13820314407348633, 'global_step': 3907, 'preemption_count': 0}), (4700, {'train/accuracy': 0.9883637428283691, 'train/loss': 0.04054323583841324, 'train/mean_average_precision': 0.204209460108698, 'validation/accuracy': 0.9854559302330017, 'validation/loss': 0.04954395070672035, 'validation/mean_average_precision': 0.17679262829503528, 'validation/num_examples': 43793, 'test/accuracy': 0.9845012426376343, 'test/loss': 0.052348311990499496, 'test/mean_average_precision': 0.17126458653492962, 'test/num_examples': 43793, 'score': 1463.7202608585358, 'total_duration': 2298.736065387726, 'accumulated_submission_time': 1463.7202608585358, 'accumulated_eval_time': 834.7937891483307, 'accumulated_logging_time': 0.16519951820373535, 'global_step': 4700, 'preemption_count': 0}), (5483, {'train/accuracy': 0.9887433648109436, 'train/loss': 0.039443179965019226, 'train/mean_average_precision': 0.22515806639541963, 'validation/accuracy': 0.9856081604957581, 'validation/loss': 0.0487796850502491, 'validation/mean_average_precision': 0.18688946662364592, 'validation/num_examples': 43793, 'test/accuracy': 0.9847573637962341, 'test/loss': 0.051322996616363525, 'test/mean_average_precision': 0.1842455114166707, 'test/num_examples': 43793, 'score': 1703.7825529575348, 'total_duration': 2632.944098472595, 'accumulated_submission_time': 1703.7825529575348, 'accumulated_eval_time': 928.904022693634, 'accumulated_logging_time': 0.19126296043395996, 'global_step': 5483, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9890667200088501, 'train/loss': 0.03762779384851456, 'train/mean_average_precision': 0.26664871763933085, 'validation/accuracy': 0.9858062267303467, 'validation/loss': 0.04839012771844864, 'validation/mean_average_precision': 0.19711352614109282, 'validation/num_examples': 43793, 'test/accuracy': 0.9848874807357788, 'test/loss': 0.05111534148454666, 'test/mean_average_precision': 0.1940564979699422, 'test/num_examples': 43793, 'score': 1862.3139309883118, 'total_duration': 2885.449563264847, 'accumulated_submission_time': 1862.3139309883118, 'accumulated_eval_time': 1022.8455605506897, 'accumulated_logging_time': 0.2175612449645996, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0808 23:47:40.600006 140716693473088 submission_runner.py:533] Timing: 1862.3139309883118
I0808 23:47:40.600056 140716693473088 submission_runner.py:535] Total number of evals: 9
I0808 23:47:40.600099 140716693473088 submission_runner.py:536] ====================
I0808 23:47:40.600204 140716693473088 submission_runner.py:604] Final ogbg score: 1862.3139309883118
