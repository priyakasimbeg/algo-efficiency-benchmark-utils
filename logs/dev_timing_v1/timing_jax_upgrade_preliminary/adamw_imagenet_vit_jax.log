python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_jax_upgrade_preliminary/adamw --overwrite=True --save_checkpoints=False --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_08-07-2023-21-59-17.log
2023-08-07 21:59:22.500308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0807 21:59:40.866476 140339756345152 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax.
I0807 21:59:41.816707 140339756345152 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0807 21:59:41.817426 140339756345152 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0807 21:59:41.817580 140339756345152 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0807 21:59:41.823057 140339756345152 submission_runner.py:490] Using RNG seed 1133935099
I0807 21:59:47.036669 140339756345152 submission_runner.py:499] --- Tuning run 1/1 ---
I0807 21:59:47.036891 140339756345152 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1.
I0807 21:59:47.038968 140339756345152 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1/hparams.json.
I0807 21:59:47.217622 140339756345152 submission_runner.py:176] Initializing dataset.
I0807 21:59:47.236661 140339756345152 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0807 21:59:47.248040 140339756345152 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0807 21:59:47.609473 140339756345152 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0807 21:59:55.537747 140339756345152 submission_runner.py:183] Initializing model.
I0807 22:00:04.621002 140339756345152 submission_runner.py:217] Initializing optimizer.
I0807 22:00:05.582515 140339756345152 submission_runner.py:224] Initializing metrics bundle.
I0807 22:00:05.582723 140339756345152 submission_runner.py:242] Initializing checkpoint and logger.
I0807 22:00:05.583798 140339756345152 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0807 22:00:06.365523 140339756345152 submission_runner.py:263] Saving meta data to /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0807 22:00:06.367927 140339756345152 submission_runner.py:266] Saving flags to /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1/flags_0.json.
I0807 22:00:06.377030 140339756345152 submission_runner.py:276] Starting training loop.
2023-08-07 22:00:55.310756: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-08-07 22:00:58.607127: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
I0807 22:01:00.179873 140175409669888 logging_writer.py:48] [0] global_step=0, grad_norm=0.3195238709449768, loss=6.9077534675598145
I0807 22:01:00.197294 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:01:00.204927 140339756345152 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0807 22:01:00.213523 140339756345152 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0807 22:01:00.294769 140339756345152 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0807 22:01:17.358501 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:01:17.371373 140339756345152 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0807 22:01:17.390673 140339756345152 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0807 22:01:17.467116 140339756345152 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0807 22:01:34.552216 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:01:34.563024 140339756345152 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0807 22:01:34.569695 140339756345152 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0807 22:01:34.613989 140339756345152 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0807 22:01:43.405478 140339756345152 submission_runner.py:364] Time since start: 97.03s, 	Step: 1, 	{'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 53.820223569869995, 'total_duration': 97.02837252616882, 'accumulated_submission_time': 53.820223569869995, 'accumulated_eval_time': 43.20811939239502, 'accumulated_logging_time': 0}
I0807 22:01:43.426199 140134003484416 logging_writer.py:48] [1] accumulated_eval_time=43.208119, accumulated_logging_time=0, accumulated_submission_time=53.820224, global_step=1, preemption_count=0, score=53.820224, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=97.028373, train/accuracy=0.000898, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0807 22:02:47.022196 140173329278720 logging_writer.py:48] [100] global_step=100, grad_norm=0.47964587807655334, loss=6.894460678100586
I0807 22:03:29.274965 140134028662528 logging_writer.py:48] [200] global_step=200, grad_norm=0.5739091634750366, loss=6.843555927276611
I0807 22:04:13.877846 140173329278720 logging_writer.py:48] [300] global_step=300, grad_norm=0.5361749529838562, loss=6.79133415222168
I0807 22:04:58.294501 140134028662528 logging_writer.py:48] [400] global_step=400, grad_norm=0.603816568851471, loss=6.7298994064331055
I0807 22:05:42.865909 140173329278720 logging_writer.py:48] [500] global_step=500, grad_norm=0.701366126537323, loss=6.639904499053955
I0807 22:06:27.244919 140134028662528 logging_writer.py:48] [600] global_step=600, grad_norm=0.6269396543502808, loss=6.595648288726807
I0807 22:07:11.809780 140173329278720 logging_writer.py:48] [700] global_step=700, grad_norm=0.8750995993614197, loss=6.751428127288818
I0807 22:07:56.160039 140134028662528 logging_writer.py:48] [800] global_step=800, grad_norm=1.2069072723388672, loss=6.497793197631836
I0807 22:08:40.576696 140173329278720 logging_writer.py:48] [900] global_step=900, grad_norm=0.8896476626396179, loss=6.456050872802734
I0807 22:08:43.776053 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:08:55.582762 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:09:02.662256 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:09:04.440424 140339756345152 submission_runner.py:364] Time since start: 538.06s, 	Step: 909, 	{'train/accuracy': 0.021406250074505806, 'train/loss': 6.172215461730957, 'validation/accuracy': 0.019619999453425407, 'validation/loss': 6.184725284576416, 'validation/num_examples': 50000, 'test/accuracy': 0.015400000847876072, 'test/loss': 6.264852523803711, 'test/num_examples': 10000, 'score': 474.13825583457947, 'total_duration': 538.063310623169, 'accumulated_submission_time': 474.13825583457947, 'accumulated_eval_time': 63.872461795806885, 'accumulated_logging_time': 0.03165864944458008}
I0807 22:09:04.456470 140134037055232 logging_writer.py:48] [909] accumulated_eval_time=63.872462, accumulated_logging_time=0.031659, accumulated_submission_time=474.138256, global_step=909, preemption_count=0, score=474.138256, test/accuracy=0.015400, test/loss=6.264853, test/num_examples=10000, total_duration=538.063311, train/accuracy=0.021406, train/loss=6.172215, validation/accuracy=0.019620, validation/loss=6.184725, validation/num_examples=50000
I0807 22:09:41.093856 140134045447936 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.892869234085083, loss=6.412946701049805
I0807 22:10:25.287419 140134037055232 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4569250345230103, loss=6.746592998504639
I0807 22:11:10.057638 140134045447936 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9772717952728271, loss=6.311254501342773
I0807 22:11:54.373305 140134037055232 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8000428080558777, loss=6.741701126098633
I0807 22:12:38.589488 140134045447936 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.3686283826828003, loss=6.178603172302246
I0807 22:13:22.954776 140134037055232 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0508018732070923, loss=6.3635478019714355
I0807 22:14:07.329427 140134045447936 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0844407081604004, loss=6.147237777709961
I0807 22:14:51.682547 140134037055232 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9342024922370911, loss=6.393120765686035
I0807 22:15:36.208895 140134045447936 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7545584440231323, loss=6.296911239624023
I0807 22:16:04.729985 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:16:16.592781 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:16:23.324089 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:16:24.933464 140339756345152 submission_runner.py:364] Time since start: 978.56s, 	Step: 1866, 	{'train/accuracy': 0.057988278567790985, 'train/loss': 5.563216209411621, 'validation/accuracy': 0.05883999913930893, 'validation/loss': 5.598766326904297, 'validation/num_examples': 50000, 'test/accuracy': 0.04410000145435333, 'test/loss': 5.762573719024658, 'test/num_examples': 10000, 'score': 894.3801882266998, 'total_duration': 978.5563359260559, 'accumulated_submission_time': 894.3801882266998, 'accumulated_eval_time': 84.07589817047119, 'accumulated_logging_time': 0.05753636360168457}
I0807 22:16:24.955062 140134037055232 logging_writer.py:48] [1866] accumulated_eval_time=84.075898, accumulated_logging_time=0.057536, accumulated_submission_time=894.380188, global_step=1866, preemption_count=0, score=894.380188, test/accuracy=0.044100, test/loss=5.762574, test/num_examples=10000, total_duration=978.556336, train/accuracy=0.057988, train/loss=5.563216, validation/accuracy=0.058840, validation/loss=5.598766, validation/num_examples=50000
I0807 22:16:39.058684 140134045447936 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8775309920310974, loss=5.98964262008667
I0807 22:17:20.259603 140134037055232 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9061055779457092, loss=6.667903900146484
I0807 22:18:04.646030 140134045447936 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9152220487594604, loss=6.569580078125
I0807 22:18:49.020761 140134037055232 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7962138056755066, loss=5.97288703918457
I0807 22:19:33.185500 140134045447936 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8003325462341309, loss=5.876519203186035
I0807 22:20:17.453608 140134037055232 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9510725736618042, loss=6.178767204284668
I0807 22:21:02.033727 140134045447936 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7977566123008728, loss=5.85300350189209
I0807 22:21:46.236625 140134037055232 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9091762900352478, loss=5.837073802947998
I0807 22:22:30.589967 140134045447936 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9072509407997131, loss=6.391500473022461
I0807 22:23:15.109734 140134037055232 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8350853323936462, loss=5.936997890472412
I0807 22:23:25.321741 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:23:37.060405 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:23:43.903699 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:23:45.497241 140339756345152 submission_runner.py:364] Time since start: 1419.12s, 	Step: 2825, 	{'train/accuracy': 0.1086718738079071, 'train/loss': 5.0168609619140625, 'validation/accuracy': 0.09953999519348145, 'validation/loss': 5.083603382110596, 'validation/num_examples': 50000, 'test/accuracy': 0.0747000053524971, 'test/loss': 5.309108257293701, 'test/num_examples': 10000, 'score': 1314.7102065086365, 'total_duration': 1419.1200242042542, 'accumulated_submission_time': 1314.7102065086365, 'accumulated_eval_time': 104.25128531455994, 'accumulated_logging_time': 0.09458541870117188}
I0807 22:23:45.518366 140134045447936 logging_writer.py:48] [2825] accumulated_eval_time=104.251285, accumulated_logging_time=0.094585, accumulated_submission_time=1314.710207, global_step=2825, preemption_count=0, score=1314.710207, test/accuracy=0.074700, test/loss=5.309108, test/num_examples=10000, total_duration=1419.120024, train/accuracy=0.108672, train/loss=5.016861, validation/accuracy=0.099540, validation/loss=5.083603, validation/num_examples=50000
I0807 22:24:16.024659 140134037055232 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0538184642791748, loss=5.705947399139404
I0807 22:24:59.386015 140134045447936 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0366469621658325, loss=5.672399044036865
I0807 22:25:43.635210 140134037055232 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8766663670539856, loss=5.801393985748291
I0807 22:26:27.987215 140134045447936 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8410413861274719, loss=5.9757256507873535
I0807 22:27:12.453148 140134037055232 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0130409002304077, loss=5.801840782165527
I0807 22:27:56.747442 140134045447936 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9812204241752625, loss=5.619640350341797
I0807 22:28:40.922282 140134037055232 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8272987604141235, loss=5.679852485656738
I0807 22:29:25.145363 140134045447936 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8928725719451904, loss=5.478865623474121
I0807 22:30:09.742427 140134037055232 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9208285808563232, loss=5.455246925354004
I0807 22:30:45.689125 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:30:57.665709 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:31:04.682299 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:31:06.288021 140339756345152 submission_runner.py:364] Time since start: 1859.91s, 	Step: 3783, 	{'train/accuracy': 0.14656250178813934, 'train/loss': 4.617877960205078, 'validation/accuracy': 0.14075998961925507, 'validation/loss': 4.670630931854248, 'validation/num_examples': 50000, 'test/accuracy': 0.10870000720024109, 'test/loss': 4.964067459106445, 'test/num_examples': 10000, 'score': 1734.8450605869293, 'total_duration': 1859.9109137058258, 'accumulated_submission_time': 1734.8450605869293, 'accumulated_eval_time': 124.85016179084778, 'accumulated_logging_time': 0.1302797794342041}
I0807 22:31:06.305493 140134045447936 logging_writer.py:48] [3783] accumulated_eval_time=124.850162, accumulated_logging_time=0.130280, accumulated_submission_time=1734.845061, global_step=3783, preemption_count=0, score=1734.845061, test/accuracy=0.108700, test/loss=4.964067, test/num_examples=10000, total_duration=1859.910914, train/accuracy=0.146563, train/loss=4.617878, validation/accuracy=0.140760, validation/loss=4.670631, validation/num_examples=50000
I0807 22:31:13.502381 140134037055232 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0295817852020264, loss=5.572867393493652
I0807 22:31:54.093084 140134045447936 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7835257053375244, loss=6.138866901397705
I0807 22:32:38.449888 140134037055232 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.08719003200531, loss=5.463734149932861
I0807 22:33:22.860830 140134045447936 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7446275353431702, loss=6.171816349029541
I0807 22:34:07.296293 140134037055232 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9017767906188965, loss=5.350264549255371
I0807 22:34:51.791179 140134045447936 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.5549570322036743, loss=5.485525131225586
I0807 22:35:36.510963 140134037055232 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.604548454284668, loss=6.502004146575928
I0807 22:36:21.031781 140134045447936 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6323207020759583, loss=6.414597988128662
I0807 22:37:05.504928 140134037055232 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.055936574935913, loss=5.348187446594238
I0807 22:37:50.042258 140134045447936 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8577470779418945, loss=5.489287376403809
I0807 22:38:06.790151 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:38:18.770696 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:38:25.656830 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:38:27.244804 140339756345152 submission_runner.py:364] Time since start: 2300.87s, 	Step: 4739, 	{'train/accuracy': 0.189453125, 'train/loss': 4.259493827819824, 'validation/accuracy': 0.17885999381542206, 'validation/loss': 4.325858116149902, 'validation/num_examples': 50000, 'test/accuracy': 0.1339000016450882, 'test/loss': 4.692418098449707, 'test/num_examples': 10000, 'score': 2155.2976756095886, 'total_duration': 2300.867700815201, 'accumulated_submission_time': 2155.2976756095886, 'accumulated_eval_time': 145.30480670928955, 'accumulated_logging_time': 0.15850090980529785}
I0807 22:38:27.261089 140134037055232 logging_writer.py:48] [4739] accumulated_eval_time=145.304807, accumulated_logging_time=0.158501, accumulated_submission_time=2155.297676, global_step=4739, preemption_count=0, score=2155.297676, test/accuracy=0.133900, test/loss=4.692418, test/num_examples=10000, total_duration=2300.867701, train/accuracy=0.189453, train/loss=4.259494, validation/accuracy=0.178860, validation/loss=4.325858, validation/num_examples=50000
I0807 22:38:52.078082 140134045447936 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.778218686580658, loss=5.164516925811768
I0807 22:39:34.944296 140134037055232 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8777956366539001, loss=5.253222465515137
I0807 22:40:19.580745 140134045447936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9187912940979004, loss=5.319799423217773
I0807 22:41:04.137847 140134037055232 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7285366058349609, loss=5.9814863204956055
I0807 22:41:48.866421 140134045447936 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6822845339775085, loss=6.5111894607543945
I0807 22:42:33.813113 140134037055232 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7631041407585144, loss=5.308871269226074
I0807 22:43:18.914074 140134045447936 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7249743938446045, loss=5.565286636352539
I0807 22:44:03.576257 140134037055232 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8086760640144348, loss=5.100508213043213
I0807 22:44:48.245456 140134045447936 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6830929517745972, loss=5.711859703063965
I0807 22:45:27.422947 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:45:39.231662 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:45:45.952899 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:45:47.555728 140339756345152 submission_runner.py:364] Time since start: 2741.18s, 	Step: 5690, 	{'train/accuracy': 0.23291015625, 'train/loss': 3.9422318935394287, 'validation/accuracy': 0.21375998854637146, 'validation/loss': 4.053785800933838, 'validation/num_examples': 50000, 'test/accuracy': 0.16130000352859497, 'test/loss': 4.477299213409424, 'test/num_examples': 10000, 'score': 2575.428438425064, 'total_duration': 2741.1785893440247, 'accumulated_submission_time': 2575.428438425064, 'accumulated_eval_time': 165.43752789497375, 'accumulated_logging_time': 0.18448996543884277}
I0807 22:45:47.581718 140134037055232 logging_writer.py:48] [5690] accumulated_eval_time=165.437528, accumulated_logging_time=0.184490, accumulated_submission_time=2575.428438, global_step=5690, preemption_count=0, score=2575.428438, test/accuracy=0.161300, test/loss=4.477299, test/num_examples=10000, total_duration=2741.178589, train/accuracy=0.232910, train/loss=3.942232, validation/accuracy=0.213760, validation/loss=4.053786, validation/num_examples=50000
I0807 22:45:51.997569 140134045447936 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9265419244766235, loss=5.0316338539123535
I0807 22:46:32.830693 140134037055232 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6102995276451111, loss=6.256307601928711
I0807 22:47:17.305421 140134045447936 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8431035876274109, loss=5.493492126464844
I0807 22:48:01.815556 140134037055232 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8504341244697571, loss=5.074067115783691
I0807 22:48:46.403978 140134045447936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8146496415138245, loss=5.1256537437438965
I0807 22:49:30.979338 140134037055232 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6304201483726501, loss=5.940267562866211
I0807 22:50:16.001283 140134045447936 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8604384660720825, loss=4.946619987487793
I0807 22:51:00.643478 140134037055232 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7218132019042969, loss=5.456299304962158
I0807 22:51:45.220723 140134045447936 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.615873396396637, loss=6.380010604858398
I0807 22:52:30.222168 140134037055232 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6152987480163574, loss=6.278698444366455
I0807 22:52:47.749902 140339756345152 spec.py:320] Evaluating on the training split.
I0807 22:52:59.167993 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 22:53:06.066282 140339756345152 spec.py:348] Evaluating on the test split.
I0807 22:53:07.673013 140339756345152 submission_runner.py:364] Time since start: 3181.30s, 	Step: 6641, 	{'train/accuracy': 0.2694140672683716, 'train/loss': 3.7433431148529053, 'validation/accuracy': 0.2343599945306778, 'validation/loss': 3.935878038406372, 'validation/num_examples': 50000, 'test/accuracy': 0.18410000205039978, 'test/loss': 4.35874605178833, 'test/num_examples': 10000, 'score': 2995.562174320221, 'total_duration': 3181.2959048748016, 'accumulated_submission_time': 2995.562174320221, 'accumulated_eval_time': 185.3606059551239, 'accumulated_logging_time': 0.22372841835021973}
I0807 22:53:07.697802 140134045447936 logging_writer.py:48] [6641] accumulated_eval_time=185.360606, accumulated_logging_time=0.223728, accumulated_submission_time=2995.562174, global_step=6641, preemption_count=0, score=2995.562174, test/accuracy=0.184100, test/loss=4.358746, test/num_examples=10000, total_duration=3181.295905, train/accuracy=0.269414, train/loss=3.743343, validation/accuracy=0.234360, validation/loss=3.935878, validation/num_examples=50000
I0807 22:53:31.717432 140134037055232 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6861510872840881, loss=5.87977409362793
I0807 22:54:15.543926 140134045447936 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7823017239570618, loss=4.86967658996582
I0807 22:55:00.064905 140134037055232 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7698251008987427, loss=4.853054046630859
I0807 22:55:44.546107 140134045447936 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6844427585601807, loss=5.534620761871338
I0807 22:56:29.464625 140134037055232 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6662117838859558, loss=4.827486991882324
I0807 22:57:13.874134 140134045447936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.723632276058197, loss=4.7912092208862305
I0807 22:57:58.155290 140134037055232 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6755267977714539, loss=4.882510185241699
I0807 22:58:42.781562 140134045447936 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7820976376533508, loss=5.759670257568359
I0807 22:59:27.754531 140134037055232 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5643357634544373, loss=6.0445380210876465
I0807 23:00:07.850283 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:00:19.350707 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:00:26.395513 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:00:27.987222 140339756345152 submission_runner.py:364] Time since start: 3621.61s, 	Step: 7591, 	{'train/accuracy': 0.30726560950279236, 'train/loss': 3.4286210536956787, 'validation/accuracy': 0.28283998370170593, 'validation/loss': 3.5561726093292236, 'validation/num_examples': 50000, 'test/accuracy': 0.217600017786026, 'test/loss': 4.042065143585205, 'test/num_examples': 10000, 'score': 3415.675166130066, 'total_duration': 3621.610086917877, 'accumulated_submission_time': 3415.675166130066, 'accumulated_eval_time': 205.49753308296204, 'accumulated_logging_time': 0.26677966117858887}
I0807 23:00:28.014775 140134045447936 logging_writer.py:48] [7591] accumulated_eval_time=205.497533, accumulated_logging_time=0.266780, accumulated_submission_time=3415.675166, global_step=7591, preemption_count=0, score=3415.675166, test/accuracy=0.217600, test/loss=4.042065, test/num_examples=10000, total_duration=3621.610087, train/accuracy=0.307266, train/loss=3.428621, validation/accuracy=0.282840, validation/loss=3.556173, validation/num_examples=50000
I0807 23:00:32.045654 140134037055232 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7728166580200195, loss=4.628627777099609
I0807 23:01:13.974642 140134045447936 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6035997867584229, loss=6.165794372558594
I0807 23:01:58.694028 140134037055232 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5682876706123352, loss=5.541252136230469
I0807 23:02:43.562740 140134045447936 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6282299757003784, loss=4.626011371612549
I0807 23:03:28.043138 140134037055232 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.554667055606842, loss=5.408230781555176
I0807 23:04:12.558962 140134045447936 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6844953298568726, loss=4.720308780670166
I0807 23:04:57.045663 140134037055232 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6929948925971985, loss=4.545903205871582
I0807 23:05:41.797211 140134045447936 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7320665121078491, loss=4.629117488861084
I0807 23:06:26.272054 140134037055232 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5095023512840271, loss=6.015127658843994
I0807 23:07:10.810104 140134045447936 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.646159827709198, loss=4.616652011871338
I0807 23:07:28.139543 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:07:39.642944 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:07:47.071966 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:07:48.662935 140339756345152 submission_runner.py:364] Time since start: 4062.29s, 	Step: 8540, 	{'train/accuracy': 0.35066404938697815, 'train/loss': 3.1138417720794678, 'validation/accuracy': 0.3219199776649475, 'validation/loss': 3.2698118686676025, 'validation/num_examples': 50000, 'test/accuracy': 0.24700000882148743, 'test/loss': 3.798135757446289, 'test/num_examples': 10000, 'score': 3835.7655651569366, 'total_duration': 4062.2857666015625, 'accumulated_submission_time': 3835.7655651569366, 'accumulated_eval_time': 226.02083897590637, 'accumulated_logging_time': 0.3077211380004883}
I0807 23:07:48.686503 140134037055232 logging_writer.py:48] [8540] accumulated_eval_time=226.020839, accumulated_logging_time=0.307721, accumulated_submission_time=3835.765565, global_step=8540, preemption_count=0, score=3835.765565, test/accuracy=0.247000, test/loss=3.798136, test/num_examples=10000, total_duration=4062.285767, train/accuracy=0.350664, train/loss=3.113842, validation/accuracy=0.321920, validation/loss=3.269812, validation/num_examples=50000
I0807 23:08:13.058381 140134045447936 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8219459056854248, loss=4.548067092895508
I0807 23:08:56.875777 140134037055232 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6008236408233643, loss=4.796370983123779
I0807 23:09:42.322882 140134045447936 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5452806353569031, loss=5.8454508781433105
I0807 23:10:27.721346 140134037055232 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6435603499412537, loss=4.556248188018799
I0807 23:11:13.052114 140134045447936 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.616605818271637, loss=4.664065361022949
I0807 23:11:57.656183 140134037055232 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5594081878662109, loss=4.890672206878662
I0807 23:12:42.682942 140134045447936 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.625374972820282, loss=4.423789978027344
I0807 23:13:27.606293 140134037055232 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4905951917171478, loss=6.167832851409912
I0807 23:14:12.518426 140134045447936 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6478614807128906, loss=4.471419334411621
I0807 23:14:48.773753 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:15:00.637460 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:15:08.360074 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:15:09.956578 140339756345152 submission_runner.py:364] Time since start: 4503.58s, 	Step: 9483, 	{'train/accuracy': 0.36921873688697815, 'train/loss': 3.0441274642944336, 'validation/accuracy': 0.3342599868774414, 'validation/loss': 3.237107992172241, 'validation/num_examples': 50000, 'test/accuracy': 0.25530001521110535, 'test/loss': 3.7978737354278564, 'test/num_examples': 10000, 'score': 4255.818341493607, 'total_duration': 4503.579435825348, 'accumulated_submission_time': 4255.818341493607, 'accumulated_eval_time': 247.20360660552979, 'accumulated_logging_time': 0.344663143157959}
I0807 23:15:09.985800 140134037055232 logging_writer.py:48] [9483] accumulated_eval_time=247.203607, accumulated_logging_time=0.344663, accumulated_submission_time=4255.818341, global_step=9483, preemption_count=0, score=4255.818341, test/accuracy=0.255300, test/loss=3.797874, test/num_examples=10000, total_duration=4503.579436, train/accuracy=0.369219, train/loss=3.044127, validation/accuracy=0.334260, validation/loss=3.237108, validation/num_examples=50000
I0807 23:15:17.216742 140134045447936 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6330977082252502, loss=4.42955207824707
I0807 23:15:59.069623 140134037055232 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6926589608192444, loss=4.286098957061768
I0807 23:16:43.951712 140134045447936 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6299299001693726, loss=4.393321990966797
I0807 23:17:29.290389 140134037055232 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5921270251274109, loss=4.638525009155273
I0807 23:18:14.375933 140134045447936 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4633190929889679, loss=5.836793899536133
I0807 23:18:59.466186 140134037055232 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5470438003540039, loss=4.795949935913086
I0807 23:19:44.460397 140134045447936 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6334459781646729, loss=4.342432022094727
I0807 23:20:29.628957 140134037055232 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6241222620010376, loss=4.402999401092529
I0807 23:21:15.388457 140134045447936 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6574993133544922, loss=4.314560413360596
I0807 23:22:00.530871 140134037055232 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5430602431297302, loss=5.147160053253174
I0807 23:22:09.991395 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:22:22.073142 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:22:29.916365 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:22:31.513545 140339756345152 submission_runner.py:364] Time since start: 4945.14s, 	Step: 10422, 	{'train/accuracy': 0.41191405057907104, 'train/loss': 2.772068738937378, 'validation/accuracy': 0.38492000102996826, 'validation/loss': 2.929823398590088, 'validation/num_examples': 50000, 'test/accuracy': 0.2987000048160553, 'test/loss': 3.499066114425659, 'test/num_examples': 10000, 'score': 4675.793411016464, 'total_duration': 4945.136442899704, 'accumulated_submission_time': 4675.793411016464, 'accumulated_eval_time': 268.72573590278625, 'accumulated_logging_time': 0.3831052780151367}
I0807 23:22:31.539670 140134045447936 logging_writer.py:48] [10422] accumulated_eval_time=268.725736, accumulated_logging_time=0.383105, accumulated_submission_time=4675.793411, global_step=10422, preemption_count=0, score=4675.793411, test/accuracy=0.298700, test/loss=3.499066, test/num_examples=10000, total_duration=4945.136443, train/accuracy=0.411914, train/loss=2.772069, validation/accuracy=0.384920, validation/loss=2.929823, validation/num_examples=50000
I0807 23:23:03.659921 140134037055232 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5914258360862732, loss=4.509692668914795
I0807 23:23:48.102348 140134045447936 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6021149158477783, loss=5.389277458190918
I0807 23:24:32.765983 140134037055232 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6536787152290344, loss=4.252870559692383
I0807 23:25:17.789797 140134045447936 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.606189489364624, loss=4.1149797439575195
I0807 23:26:02.758890 140134037055232 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5726773738861084, loss=4.422823905944824
I0807 23:26:47.789836 140134045447936 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6467543840408325, loss=4.240484714508057
I0807 23:27:33.091235 140134037055232 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.671455442905426, loss=4.245779991149902
I0807 23:28:17.911833 140134045447936 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5168377161026001, loss=5.083651542663574
I0807 23:29:03.214830 140134037055232 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5480518341064453, loss=5.932984828948975
I0807 23:29:31.554184 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:29:43.892127 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:29:52.098573 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:29:53.695839 140339756345152 submission_runner.py:364] Time since start: 5387.32s, 	Step: 11365, 	{'train/accuracy': 0.43812498450279236, 'train/loss': 2.6499311923980713, 'validation/accuracy': 0.4040199816226959, 'validation/loss': 2.818134069442749, 'validation/num_examples': 50000, 'test/accuracy': 0.3060000240802765, 'test/loss': 3.403308391571045, 'test/num_examples': 10000, 'score': 5095.773666143417, 'total_duration': 5387.318740129471, 'accumulated_submission_time': 5095.773666143417, 'accumulated_eval_time': 290.8673732280731, 'accumulated_logging_time': 0.42226624488830566}
I0807 23:29:53.715599 140134045447936 logging_writer.py:48] [11365] accumulated_eval_time=290.867373, accumulated_logging_time=0.422266, accumulated_submission_time=5095.773666, global_step=11365, preemption_count=0, score=5095.773666, test/accuracy=0.306000, test/loss=3.403308, test/num_examples=10000, total_duration=5387.318740, train/accuracy=0.438125, train/loss=2.649931, validation/accuracy=0.404020, validation/loss=2.818134, validation/num_examples=50000
I0807 23:30:08.132336 140134037055232 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6674742698669434, loss=4.212919235229492
I0807 23:30:51.404446 140134045447936 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7236469984054565, loss=4.154669761657715
I0807 23:31:36.697384 140134037055232 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5731678009033203, loss=5.26497745513916
I0807 23:32:21.833233 140134045447936 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5600927472114563, loss=5.2793121337890625
I0807 23:33:06.942633 140134037055232 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5711802840232849, loss=6.034753799438477
I0807 23:33:51.499916 140134045447936 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.621806263923645, loss=4.141616344451904
I0807 23:34:36.722635 140134037055232 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6299252510070801, loss=4.062613010406494
I0807 23:35:21.744455 140134045447936 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.53850919008255, loss=6.010188102722168
I0807 23:36:07.149818 140134037055232 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6401736736297607, loss=4.291287899017334
I0807 23:36:52.425992 140134045447936 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5448288321495056, loss=6.031906604766846
I0807 23:36:54.022749 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:37:06.877036 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:37:15.013450 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:37:16.618112 140339756345152 submission_runner.py:364] Time since start: 5830.24s, 	Step: 12305, 	{'train/accuracy': 0.4661913812160492, 'train/loss': 2.4755072593688965, 'validation/accuracy': 0.4280399978160858, 'validation/loss': 2.6841909885406494, 'validation/num_examples': 50000, 'test/accuracy': 0.3247000277042389, 'test/loss': 3.288618803024292, 'test/num_examples': 10000, 'score': 5516.050617456436, 'total_duration': 5830.241009950638, 'accumulated_submission_time': 5516.050617456436, 'accumulated_eval_time': 313.4627215862274, 'accumulated_logging_time': 0.4514930248260498}
I0807 23:37:16.635892 140134037055232 logging_writer.py:48] [12305] accumulated_eval_time=313.462722, accumulated_logging_time=0.451493, accumulated_submission_time=5516.050617, global_step=12305, preemption_count=0, score=5516.050617, test/accuracy=0.324700, test/loss=3.288619, test/num_examples=10000, total_duration=5830.241010, train/accuracy=0.466191, train/loss=2.475507, validation/accuracy=0.428040, validation/loss=2.684191, validation/num_examples=50000
I0807 23:37:56.348919 140134045447936 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6639040112495422, loss=3.9594063758850098
I0807 23:38:41.626258 140134037055232 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8545859456062317, loss=3.929532051086426
I0807 23:39:27.771628 140134045447936 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6154213547706604, loss=3.964250326156616
I0807 23:40:13.644548 140134037055232 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.607158899307251, loss=3.7759478092193604
I0807 23:40:59.485965 140134045447936 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6377190947532654, loss=3.9817256927490234
I0807 23:41:45.174884 140134037055232 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.64140784740448, loss=4.044622421264648
I0807 23:42:31.376694 140134045447936 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6817499995231628, loss=4.143333911895752
I0807 23:43:16.771342 140134037055232 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.577838122844696, loss=4.498310089111328
I0807 23:44:02.320221 140134045447936 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6204736828804016, loss=4.00962495803833
I0807 23:44:16.871588 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:44:29.739101 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:44:38.108452 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:44:39.718278 140339756345152 submission_runner.py:364] Time since start: 6273.34s, 	Step: 13234, 	{'train/accuracy': 0.5001367330551147, 'train/loss': 2.2797000408172607, 'validation/accuracy': 0.4499399960041046, 'validation/loss': 2.52043080329895, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.1521353721618652, 'test/num_examples': 10000, 'score': 5936.255434036255, 'total_duration': 6273.341181516647, 'accumulated_submission_time': 5936.255434036255, 'accumulated_eval_time': 336.30939626693726, 'accumulated_logging_time': 0.47919249534606934}
I0807 23:44:39.738674 140134037055232 logging_writer.py:48] [13234] accumulated_eval_time=336.309396, accumulated_logging_time=0.479192, accumulated_submission_time=5936.255434, global_step=13234, preemption_count=0, score=5936.255434, test/accuracy=0.346200, test/loss=3.152135, test/num_examples=10000, total_duration=6273.341182, train/accuracy=0.500137, train/loss=2.279700, validation/accuracy=0.449940, validation/loss=2.520431, validation/num_examples=50000
I0807 23:45:07.138571 140134045447936 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5808917880058289, loss=4.631709098815918
I0807 23:45:51.665889 140134037055232 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6133503317832947, loss=5.081981658935547
I0807 23:46:37.016547 140134045447936 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6817352771759033, loss=3.887860059738159
I0807 23:47:22.428891 140134037055232 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7108069062232971, loss=3.9322478771209717
I0807 23:48:07.394979 140134045447936 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5966809988021851, loss=4.693869590759277
I0807 23:48:53.319869 140134037055232 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7282849550247192, loss=3.8311767578125
I0807 23:49:38.606456 140134045447936 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6448882818222046, loss=4.105342864990234
I0807 23:50:23.046787 140339756345152 spec.py:320] Evaluating on the training split.
I0807 23:50:36.511436 140339756345152 spec.py:332] Evaluating on the validation split.
I0807 23:50:45.194019 140339756345152 spec.py:348] Evaluating on the test split.
I0807 23:50:46.796358 140339756345152 submission_runner.py:364] Time since start: 6640.42s, 	Step: 14000, 	{'train/accuracy': 0.5000781416893005, 'train/loss': 2.2665956020355225, 'validation/accuracy': 0.4649999737739563, 'validation/loss': 2.4579949378967285, 'validation/num_examples': 50000, 'test/accuracy': 0.3594000041484833, 'test/loss': 3.096022844314575, 'test/num_examples': 10000, 'score': 6279.535490036011, 'total_duration': 6640.419254541397, 'accumulated_submission_time': 6279.535490036011, 'accumulated_eval_time': 360.0589427947998, 'accumulated_logging_time': 0.5106737613677979}
I0807 23:50:46.817031 140134037055232 logging_writer.py:48] [14000] accumulated_eval_time=360.058943, accumulated_logging_time=0.510674, accumulated_submission_time=6279.535490, global_step=14000, preemption_count=0, score=6279.535490, test/accuracy=0.359400, test/loss=3.096023, test/num_examples=10000, total_duration=6640.419255, train/accuracy=0.500078, train/loss=2.266596, validation/accuracy=0.465000, validation/loss=2.457995, validation/num_examples=50000
I0807 23:50:46.838170 140134045447936 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=6279.535490
I0807 23:50:47.098053 140339756345152 checkpoints.py:490] Saving checkpoint at step: 14000
I0807 23:50:48.092146 140339756345152 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1/checkpoint_14000
I0807 23:50:48.116915 140339756345152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_upgrade_preliminary/adamw/imagenet_vit_jax/trial_1/checkpoint_14000.
I0807 23:50:48.589770 140339756345152 submission_runner.py:530] Tuning trial 1/1
I0807 23:50:48.590013 140339756345152 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0807 23:50:48.590524 140339756345152 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 53.820223569869995, 'total_duration': 97.02837252616882, 'accumulated_submission_time': 53.820223569869995, 'accumulated_eval_time': 43.20811939239502, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (909, {'train/accuracy': 0.021406250074505806, 'train/loss': 6.172215461730957, 'validation/accuracy': 0.019619999453425407, 'validation/loss': 6.184725284576416, 'validation/num_examples': 50000, 'test/accuracy': 0.015400000847876072, 'test/loss': 6.264852523803711, 'test/num_examples': 10000, 'score': 474.13825583457947, 'total_duration': 538.063310623169, 'accumulated_submission_time': 474.13825583457947, 'accumulated_eval_time': 63.872461795806885, 'accumulated_logging_time': 0.03165864944458008, 'global_step': 909, 'preemption_count': 0}), (1866, {'train/accuracy': 0.057988278567790985, 'train/loss': 5.563216209411621, 'validation/accuracy': 0.05883999913930893, 'validation/loss': 5.598766326904297, 'validation/num_examples': 50000, 'test/accuracy': 0.04410000145435333, 'test/loss': 5.762573719024658, 'test/num_examples': 10000, 'score': 894.3801882266998, 'total_duration': 978.5563359260559, 'accumulated_submission_time': 894.3801882266998, 'accumulated_eval_time': 84.07589817047119, 'accumulated_logging_time': 0.05753636360168457, 'global_step': 1866, 'preemption_count': 0}), (2825, {'train/accuracy': 0.1086718738079071, 'train/loss': 5.0168609619140625, 'validation/accuracy': 0.09953999519348145, 'validation/loss': 5.083603382110596, 'validation/num_examples': 50000, 'test/accuracy': 0.0747000053524971, 'test/loss': 5.309108257293701, 'test/num_examples': 10000, 'score': 1314.7102065086365, 'total_duration': 1419.1200242042542, 'accumulated_submission_time': 1314.7102065086365, 'accumulated_eval_time': 104.25128531455994, 'accumulated_logging_time': 0.09458541870117188, 'global_step': 2825, 'preemption_count': 0}), (3783, {'train/accuracy': 0.14656250178813934, 'train/loss': 4.617877960205078, 'validation/accuracy': 0.14075998961925507, 'validation/loss': 4.670630931854248, 'validation/num_examples': 50000, 'test/accuracy': 0.10870000720024109, 'test/loss': 4.964067459106445, 'test/num_examples': 10000, 'score': 1734.8450605869293, 'total_duration': 1859.9109137058258, 'accumulated_submission_time': 1734.8450605869293, 'accumulated_eval_time': 124.85016179084778, 'accumulated_logging_time': 0.1302797794342041, 'global_step': 3783, 'preemption_count': 0}), (4739, {'train/accuracy': 0.189453125, 'train/loss': 4.259493827819824, 'validation/accuracy': 0.17885999381542206, 'validation/loss': 4.325858116149902, 'validation/num_examples': 50000, 'test/accuracy': 0.1339000016450882, 'test/loss': 4.692418098449707, 'test/num_examples': 10000, 'score': 2155.2976756095886, 'total_duration': 2300.867700815201, 'accumulated_submission_time': 2155.2976756095886, 'accumulated_eval_time': 145.30480670928955, 'accumulated_logging_time': 0.15850090980529785, 'global_step': 4739, 'preemption_count': 0}), (5690, {'train/accuracy': 0.23291015625, 'train/loss': 3.9422318935394287, 'validation/accuracy': 0.21375998854637146, 'validation/loss': 4.053785800933838, 'validation/num_examples': 50000, 'test/accuracy': 0.16130000352859497, 'test/loss': 4.477299213409424, 'test/num_examples': 10000, 'score': 2575.428438425064, 'total_duration': 2741.1785893440247, 'accumulated_submission_time': 2575.428438425064, 'accumulated_eval_time': 165.43752789497375, 'accumulated_logging_time': 0.18448996543884277, 'global_step': 5690, 'preemption_count': 0}), (6641, {'train/accuracy': 0.2694140672683716, 'train/loss': 3.7433431148529053, 'validation/accuracy': 0.2343599945306778, 'validation/loss': 3.935878038406372, 'validation/num_examples': 50000, 'test/accuracy': 0.18410000205039978, 'test/loss': 4.35874605178833, 'test/num_examples': 10000, 'score': 2995.562174320221, 'total_duration': 3181.2959048748016, 'accumulated_submission_time': 2995.562174320221, 'accumulated_eval_time': 185.3606059551239, 'accumulated_logging_time': 0.22372841835021973, 'global_step': 6641, 'preemption_count': 0}), (7591, {'train/accuracy': 0.30726560950279236, 'train/loss': 3.4286210536956787, 'validation/accuracy': 0.28283998370170593, 'validation/loss': 3.5561726093292236, 'validation/num_examples': 50000, 'test/accuracy': 0.217600017786026, 'test/loss': 4.042065143585205, 'test/num_examples': 10000, 'score': 3415.675166130066, 'total_duration': 3621.610086917877, 'accumulated_submission_time': 3415.675166130066, 'accumulated_eval_time': 205.49753308296204, 'accumulated_logging_time': 0.26677966117858887, 'global_step': 7591, 'preemption_count': 0}), (8540, {'train/accuracy': 0.35066404938697815, 'train/loss': 3.1138417720794678, 'validation/accuracy': 0.3219199776649475, 'validation/loss': 3.2698118686676025, 'validation/num_examples': 50000, 'test/accuracy': 0.24700000882148743, 'test/loss': 3.798135757446289, 'test/num_examples': 10000, 'score': 3835.7655651569366, 'total_duration': 4062.2857666015625, 'accumulated_submission_time': 3835.7655651569366, 'accumulated_eval_time': 226.02083897590637, 'accumulated_logging_time': 0.3077211380004883, 'global_step': 8540, 'preemption_count': 0}), (9483, {'train/accuracy': 0.36921873688697815, 'train/loss': 3.0441274642944336, 'validation/accuracy': 0.3342599868774414, 'validation/loss': 3.237107992172241, 'validation/num_examples': 50000, 'test/accuracy': 0.25530001521110535, 'test/loss': 3.7978737354278564, 'test/num_examples': 10000, 'score': 4255.818341493607, 'total_duration': 4503.579435825348, 'accumulated_submission_time': 4255.818341493607, 'accumulated_eval_time': 247.20360660552979, 'accumulated_logging_time': 0.344663143157959, 'global_step': 9483, 'preemption_count': 0}), (10422, {'train/accuracy': 0.41191405057907104, 'train/loss': 2.772068738937378, 'validation/accuracy': 0.38492000102996826, 'validation/loss': 2.929823398590088, 'validation/num_examples': 50000, 'test/accuracy': 0.2987000048160553, 'test/loss': 3.499066114425659, 'test/num_examples': 10000, 'score': 4675.793411016464, 'total_duration': 4945.136442899704, 'accumulated_submission_time': 4675.793411016464, 'accumulated_eval_time': 268.72573590278625, 'accumulated_logging_time': 0.3831052780151367, 'global_step': 10422, 'preemption_count': 0}), (11365, {'train/accuracy': 0.43812498450279236, 'train/loss': 2.6499311923980713, 'validation/accuracy': 0.4040199816226959, 'validation/loss': 2.818134069442749, 'validation/num_examples': 50000, 'test/accuracy': 0.3060000240802765, 'test/loss': 3.403308391571045, 'test/num_examples': 10000, 'score': 5095.773666143417, 'total_duration': 5387.318740129471, 'accumulated_submission_time': 5095.773666143417, 'accumulated_eval_time': 290.8673732280731, 'accumulated_logging_time': 0.42226624488830566, 'global_step': 11365, 'preemption_count': 0}), (12305, {'train/accuracy': 0.4661913812160492, 'train/loss': 2.4755072593688965, 'validation/accuracy': 0.4280399978160858, 'validation/loss': 2.6841909885406494, 'validation/num_examples': 50000, 'test/accuracy': 0.3247000277042389, 'test/loss': 3.288618803024292, 'test/num_examples': 10000, 'score': 5516.050617456436, 'total_duration': 5830.241009950638, 'accumulated_submission_time': 5516.050617456436, 'accumulated_eval_time': 313.4627215862274, 'accumulated_logging_time': 0.4514930248260498, 'global_step': 12305, 'preemption_count': 0}), (13234, {'train/accuracy': 0.5001367330551147, 'train/loss': 2.2797000408172607, 'validation/accuracy': 0.4499399960041046, 'validation/loss': 2.52043080329895, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.1521353721618652, 'test/num_examples': 10000, 'score': 5936.255434036255, 'total_duration': 6273.341181516647, 'accumulated_submission_time': 5936.255434036255, 'accumulated_eval_time': 336.30939626693726, 'accumulated_logging_time': 0.47919249534606934, 'global_step': 13234, 'preemption_count': 0}), (14000, {'train/accuracy': 0.5000781416893005, 'train/loss': 2.2665956020355225, 'validation/accuracy': 0.4649999737739563, 'validation/loss': 2.4579949378967285, 'validation/num_examples': 50000, 'test/accuracy': 0.3594000041484833, 'test/loss': 3.096022844314575, 'test/num_examples': 10000, 'score': 6279.535490036011, 'total_duration': 6640.419254541397, 'accumulated_submission_time': 6279.535490036011, 'accumulated_eval_time': 360.0589427947998, 'accumulated_logging_time': 0.5106737613677979, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0807 23:50:48.590628 140339756345152 submission_runner.py:533] Timing: 6279.535490036011
I0807 23:50:48.590683 140339756345152 submission_runner.py:535] Total number of evals: 16
I0807 23:50:48.590734 140339756345152 submission_runner.py:536] ====================
I0807 23:50:48.590893 140339756345152 submission_runner.py:604] Final imagenet_vit score: 6279.535490036011
