I0419 00:11:19.339161 139660649875264 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax.
I0419 00:11:19.407652 139660649875264 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0419 00:11:20.245839 139660649875264 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0419 00:11:20.246567 139660649875264 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0419 00:11:20.250430 139660649875264 submission_runner.py:528] Using RNG seed 1923341384
I0419 00:11:22.874973 139660649875264 submission_runner.py:537] --- Tuning run 1/1 ---
I0419 00:11:22.875168 139660649875264 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1.
I0419 00:11:22.875447 139660649875264 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/hparams.json.
I0419 00:11:23.002154 139660649875264 submission_runner.py:232] Initializing dataset.
I0419 00:11:23.002348 139660649875264 submission_runner.py:239] Initializing model.
I0419 00:11:28.632069 139660649875264 submission_runner.py:249] Initializing optimizer.
I0419 00:11:29.442602 139660649875264 submission_runner.py:256] Initializing metrics bundle.
I0419 00:11:29.442789 139660649875264 submission_runner.py:273] Initializing checkpoint and logger.
I0419 00:11:29.444068 139660649875264 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0419 00:11:29.444384 139660649875264 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0419 00:11:29.444453 139660649875264 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0419 00:11:30.249264 139660649875264 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0419 00:11:30.250089 139660649875264 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0419 00:11:30.257651 139660649875264 submission_runner.py:309] Starting training loop.
I0419 00:11:30.455307 139660649875264 input_pipeline.py:20] Loading split = train-clean-100
I0419 00:11:30.487243 139660649875264 input_pipeline.py:20] Loading split = train-clean-360
I0419 00:11:30.805792 139660649875264 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0419 00:12:33.529742 139482988803840 logging_writer.py:48] [0] global_step=0, grad_norm=36.00730514526367, loss=32.12656784057617
I0419 00:12:33.556212 139660649875264 spec.py:298] Evaluating on the training split.
I0419 00:12:33.655908 139660649875264 input_pipeline.py:20] Loading split = train-clean-100
I0419 00:12:33.682579 139660649875264 input_pipeline.py:20] Loading split = train-clean-360
I0419 00:12:33.961464 139660649875264 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0419 00:13:13.441401 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 00:13:13.507518 139660649875264 input_pipeline.py:20] Loading split = dev-clean
I0419 00:13:13.512153 139660649875264 input_pipeline.py:20] Loading split = dev-other
I0419 00:13:51.196960 139660649875264 spec.py:326] Evaluating on the test split.
I0419 00:13:51.258839 139660649875264 input_pipeline.py:20] Loading split = test-clean
I0419 00:14:18.492420 139660649875264 submission_runner.py:406] Time since start: 168.23s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.578602, dtype=float32), 'train/wer': 0.9440268204818525, 'validation/ctc_loss': DeviceArray(30.926134, dtype=float32), 'validation/wer': 0.8960530251136045, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.875137, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 63.29837656021118, 'total_duration': 168.23331713676453, 'accumulated_submission_time': 63.29837656021118, 'accumulated_eval_time': 104.93476891517639, 'accumulated_logging_time': 0}
I0419 00:14:18.515481 139480774211328 logging_writer.py:48] [1] accumulated_eval_time=104.934769, accumulated_logging_time=0, accumulated_submission_time=63.298377, global_step=1, preemption_count=0, score=63.298377, test/ctc_loss=30.875137329101562, test/num_examples=2472, test/wer=0.899580, total_duration=168.233317, train/ctc_loss=31.578601837158203, train/wer=0.944027, validation/ctc_loss=30.92613410949707, validation/num_examples=5348, validation/wer=0.896053
I0419 00:14:18.798371 139660649875264 checkpoints.py:356] Saving checkpoint at step: 1
I0419 00:14:19.662375 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_1
I0419 00:14:19.663814 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_1.
I0419 00:15:51.520031 139486914746112 logging_writer.py:48] [100] global_step=100, grad_norm=1.6663109064102173, loss=6.260849475860596
I0419 00:17:07.478627 139486923138816 logging_writer.py:48] [200] global_step=200, grad_norm=4.011375427246094, loss=5.85914421081543
I0419 00:18:23.396720 139486914746112 logging_writer.py:48] [300] global_step=300, grad_norm=2.304978609085083, loss=5.816645622253418
I0419 00:19:39.374116 139486923138816 logging_writer.py:48] [400] global_step=400, grad_norm=4.785191535949707, loss=5.803657531738281
I0419 00:20:55.283200 139486914746112 logging_writer.py:48] [500] global_step=500, grad_norm=5.211289882659912, loss=5.804999351501465
I0419 00:22:11.282912 139486923138816 logging_writer.py:48] [600] global_step=600, grad_norm=5.6090826988220215, loss=5.818396091461182
I0419 00:23:27.312108 139486914746112 logging_writer.py:48] [700] global_step=700, grad_norm=0.43724915385246277, loss=5.773750305175781
I0419 00:24:43.317347 139486923138816 logging_writer.py:48] [800] global_step=800, grad_norm=2.5652852058410645, loss=5.7869062423706055
I0419 00:25:59.317381 139486914746112 logging_writer.py:48] [900] global_step=900, grad_norm=4.201748371124268, loss=5.786323547363281
I0419 00:27:15.305423 139486923138816 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.73516845703125, loss=5.651697158813477
I0419 00:28:34.038098 139487721174784 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.5733540058135986, loss=5.562824249267578
I0419 00:29:49.874438 139487712782080 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8683822751045227, loss=5.4445624351501465
I0419 00:31:05.653833 139487721174784 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.651104211807251, loss=5.202493667602539
I0419 00:32:21.375367 139487712782080 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.1814337968826294, loss=4.474017143249512
I0419 00:33:37.088299 139487721174784 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0362656116485596, loss=3.9606430530548096
I0419 00:34:52.870824 139487712782080 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.021744966506958, loss=3.7187416553497314
I0419 00:36:08.596864 139487721174784 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9481926560401917, loss=3.4851324558258057
I0419 00:37:24.372318 139487712782080 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0093307495117188, loss=3.290271520614624
I0419 00:38:40.152812 139487721174784 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.072791576385498, loss=3.1969399452209473
I0419 00:39:55.906847 139487712782080 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2225699424743652, loss=3.037402629852295
I0419 00:41:14.612787 139488376534784 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9462634325027466, loss=2.955793857574463
I0419 00:42:30.266147 139488368142080 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1250038146972656, loss=2.9525768756866455
I0419 00:43:45.920076 139488376534784 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9774678349494934, loss=2.8109285831451416
I0419 00:45:01.526215 139488368142080 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0476475954055786, loss=2.7743866443634033
I0419 00:46:17.286249 139488376534784 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.92946457862854, loss=2.6728057861328125
I0419 00:47:32.902141 139488368142080 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0611534118652344, loss=2.708209991455078
I0419 00:48:48.559718 139488376534784 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9134731888771057, loss=2.598285675048828
I0419 00:50:04.128887 139488368142080 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.24131441116333, loss=2.5154056549072266
I0419 00:51:19.654374 139488376534784 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9009351134300232, loss=2.5568833351135254
I0419 00:52:35.271891 139488368142080 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8021073937416077, loss=2.5057013034820557
I0419 00:53:53.894775 139487721174784 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8831998109817505, loss=2.4261746406555176
I0419 00:54:20.009145 139660649875264 spec.py:298] Evaluating on the training split.
I0419 00:54:56.165827 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 00:55:33.023595 139660649875264 spec.py:326] Evaluating on the test split.
I0419 00:55:52.012884 139660649875264 submission_runner.py:406] Time since start: 2661.75s, 	Step: 3136, 	{'train/ctc_loss': DeviceArray(2.1686404, dtype=float32), 'train/wer': 0.5057310868983131, 'validation/ctc_loss': DeviceArray(2.6264699, dtype=float32), 'validation/wer': 0.5568601723123233, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.2879589, dtype=float32), 'test/wer': 0.5082769686998558, 'test/num_examples': 2472, 'score': 2463.6030707359314, 'total_duration': 2661.7520101070404, 'accumulated_submission_time': 2463.6030707359314, 'accumulated_eval_time': 196.9353334903717, 'accumulated_logging_time': 1.173588514328003}
I0419 00:55:52.037787 139487721174784 logging_writer.py:48] [3136] accumulated_eval_time=196.935333, accumulated_logging_time=1.173589, accumulated_submission_time=2463.603071, global_step=3136, preemption_count=0, score=2463.603071, test/ctc_loss=2.287958860397339, test/num_examples=2472, test/wer=0.508277, total_duration=2661.752010, train/ctc_loss=2.168640375137329, train/wer=0.505731, validation/ctc_loss=2.626469850540161, validation/num_examples=5348, validation/wer=0.556860
I0419 00:55:52.334593 139660649875264 checkpoints.py:356] Saving checkpoint at step: 3136
I0419 00:55:53.770101 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_3136
I0419 00:55:53.802478 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_3136.
I0419 00:56:42.776143 139487712782080 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8856680989265442, loss=2.4257094860076904
I0419 00:57:58.190768 139482250606336 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.958387553691864, loss=2.342087984085083
I0419 00:59:13.530685 139487712782080 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8782223463058472, loss=2.287621259689331
I0419 01:00:29.003299 139482250606336 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7530720829963684, loss=2.2432491779327393
I0419 01:01:44.442128 139487712782080 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1767082214355469, loss=2.2343311309814453
I0419 01:02:59.979330 139482250606336 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1895716190338135, loss=2.2619524002075195
I0419 01:04:15.499463 139487712782080 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9742577075958252, loss=2.11657977104187
I0419 01:05:30.964051 139482250606336 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9105780720710754, loss=2.1845996379852295
I0419 01:06:46.425404 139487712782080 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7783701419830322, loss=2.091353416442871
I0419 01:08:01.780939 139482250606336 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8581536412239075, loss=2.0738439559936523
I0419 01:09:20.438327 139487721174784 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1943504810333252, loss=2.0996193885803223
I0419 01:10:35.846743 139487712782080 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7335890531539917, loss=2.054973840713501
I0419 01:11:51.208719 139487721174784 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7465389966964722, loss=2.0009024143218994
I0419 01:13:06.430080 139487712782080 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0695117712020874, loss=1.9813358783721924
I0419 01:14:21.861869 139487721174784 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6950036883354187, loss=2.030587673187256
I0419 01:15:37.142810 139487712782080 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7890498042106628, loss=1.9831676483154297
I0419 01:16:52.528251 139487721174784 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7043769359588623, loss=1.950202465057373
I0419 01:18:07.741007 139487712782080 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8070796728134155, loss=1.9787002801895142
I0419 01:19:22.956624 139487721174784 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.614445149898529, loss=1.9382238388061523
I0419 01:20:38.146302 139487712782080 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6043457388877869, loss=1.9717260599136353
I0419 01:21:56.511934 139487065814784 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6786127090454102, loss=1.8874305486679077
I0419 01:23:11.751999 139487057422080 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6163482666015625, loss=1.895287275314331
I0419 01:24:27.015383 139487065814784 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8540452122688293, loss=1.8881120681762695
I0419 01:25:42.209339 139487057422080 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6534417867660522, loss=1.86690354347229
I0419 01:26:57.330986 139487065814784 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7340115904808044, loss=1.8636029958724976
I0419 01:28:12.394520 139487057422080 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6267592906951904, loss=1.8795217275619507
I0419 01:29:27.435786 139487065814784 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5751564502716064, loss=1.8447209596633911
I0419 01:30:42.546500 139487057422080 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6005101203918457, loss=1.8785898685455322
I0419 01:31:57.638783 139487065814784 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5987080335617065, loss=1.8864917755126953
I0419 01:33:12.672222 139487057422080 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6300503015518188, loss=1.8832221031188965
I0419 01:34:31.111425 139487065814784 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6535049676895142, loss=1.863269567489624
I0419 01:35:46.144673 139487057422080 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5814123153686523, loss=1.8384956121444702
I0419 01:35:54.094803 139660649875264 spec.py:298] Evaluating on the training split.
I0419 01:36:30.587501 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 01:37:08.189714 139660649875264 spec.py:326] Evaluating on the test split.
I0419 01:37:27.414005 139660649875264 submission_runner.py:406] Time since start: 5157.15s, 	Step: 6312, 	{'train/ctc_loss': DeviceArray(0.56508136, dtype=float32), 'train/wer': 0.19521082492075753, 'validation/ctc_loss': DeviceArray(0.89559597, dtype=float32), 'validation/wer': 0.26368802400409075, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6094822, dtype=float32), 'test/wer': 0.20183616679869193, 'test/num_examples': 2472, 'score': 4863.841623544693, 'total_duration': 5157.152729034424, 'accumulated_submission_time': 4863.841623544693, 'accumulated_eval_time': 290.25095772743225, 'accumulated_logging_time': 2.9777743816375732}
I0419 01:37:27.436282 139487291094784 logging_writer.py:48] [6312] accumulated_eval_time=290.250958, accumulated_logging_time=2.977774, accumulated_submission_time=4863.841624, global_step=6312, preemption_count=0, score=4863.841624, test/ctc_loss=0.6094822287559509, test/num_examples=2472, test/wer=0.201836, total_duration=5157.152729, train/ctc_loss=0.5650813579559326, train/wer=0.195211, validation/ctc_loss=0.8955959677696228, validation/num_examples=5348, validation/wer=0.263688
I0419 01:37:27.694356 139660649875264 checkpoints.py:356] Saving checkpoint at step: 6312
I0419 01:37:28.962571 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_6312
I0419 01:37:28.990758 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_6312.
I0419 01:38:35.666886 139487282702080 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7139318585395813, loss=1.8195213079452515
I0419 01:39:50.696236 139487232345856 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.905783474445343, loss=1.836177110671997
I0419 01:41:05.816077 139487282702080 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.700556218624115, loss=1.7518566846847534
I0419 01:42:20.906237 139487232345856 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5212218165397644, loss=1.7373555898666382
I0419 01:43:35.905735 139487282702080 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5007733702659607, loss=1.7900506258010864
I0419 01:44:50.959216 139487232345856 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5553600788116455, loss=1.7583683729171753
I0419 01:46:05.950483 139487282702080 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5278931856155396, loss=1.7562675476074219
I0419 01:47:20.882463 139487232345856 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5622637271881104, loss=1.6999058723449707
I0419 01:48:35.827014 139487282702080 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.42768535017967224, loss=1.7663265466690063
I0419 01:49:54.135518 139487291094784 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4424521327018738, loss=1.7562717199325562
I0419 01:51:09.173736 139487282702080 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4347999095916748, loss=1.660999059677124
I0419 01:52:24.189940 139487291094784 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7562248110771179, loss=1.7366390228271484
I0419 01:53:39.278037 139487282702080 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5715188980102539, loss=1.682131290435791
I0419 01:54:54.375504 139487291094784 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5896403193473816, loss=1.6361777782440186
I0419 01:56:09.460333 139487282702080 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.44291457533836365, loss=1.686511516571045
I0419 01:57:24.456170 139487291094784 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.39389684796333313, loss=1.651764154434204
I0419 01:58:39.488885 139487282702080 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6050019860267639, loss=1.6868171691894531
I0419 01:59:54.486385 139487291094784 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.512499988079071, loss=1.6562882661819458
I0419 02:01:09.445292 139487282702080 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.44137245416641235, loss=1.6906965970993042
I0419 02:02:27.571409 139487291094784 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5299493074417114, loss=1.5762827396392822
I0419 02:03:42.546242 139487282702080 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.42276039719581604, loss=1.6524556875228882
I0419 02:04:57.486480 139487291094784 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5373309850692749, loss=1.6597485542297363
I0419 02:06:12.454384 139487282702080 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4780718684196472, loss=1.6434919834136963
I0419 02:07:27.403862 139487291094784 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5635334253311157, loss=1.633446455001831
I0419 02:08:42.399378 139487282702080 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5119885802268982, loss=1.6437177658081055
I0419 02:09:57.413496 139487291094784 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6124148964881897, loss=1.6274123191833496
I0419 02:11:12.424188 139487282702080 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4041631817817688, loss=1.6499098539352417
I0419 02:12:27.310284 139487291094784 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5784359574317932, loss=1.5775686502456665
I0419 02:13:42.267022 139487282702080 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.41434305906295776, loss=1.6175117492675781
I0419 02:15:00.437962 139487291094784 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4409439265727997, loss=1.579464316368103
I0419 02:16:15.489737 139487282702080 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3922242224216461, loss=1.5143135786056519
I0419 02:17:29.243543 139660649875264 spec.py:298] Evaluating on the training split.
I0419 02:18:06.494076 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 02:18:44.293375 139660649875264 spec.py:326] Evaluating on the test split.
I0419 02:19:04.102833 139660649875264 submission_runner.py:406] Time since start: 7653.84s, 	Step: 9500, 	{'train/ctc_loss': DeviceArray(0.36682707, dtype=float32), 'train/wer': 0.13511604493150106, 'validation/ctc_loss': DeviceArray(0.6963921, dtype=float32), 'validation/wer': 0.2088683923626856, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44902402, dtype=float32), 'test/wer': 0.15028537769382325, 'test/num_examples': 2472, 'score': 7264.047858953476, 'total_duration': 7653.8419444561005, 'accumulated_submission_time': 7264.047858953476, 'accumulated_eval_time': 385.10705399513245, 'accumulated_logging_time': 4.56264066696167}
I0419 02:19:04.124035 139487721174784 logging_writer.py:48] [9500] accumulated_eval_time=385.107054, accumulated_logging_time=4.562641, accumulated_submission_time=7264.047859, global_step=9500, preemption_count=0, score=7264.047859, test/ctc_loss=0.4490240216255188, test/num_examples=2472, test/wer=0.150285, total_duration=7653.841944, train/ctc_loss=0.3668270707130432, train/wer=0.135116, validation/ctc_loss=0.6963921189308167, validation/num_examples=5348, validation/wer=0.208868
I0419 02:19:04.387763 139660649875264 checkpoints.py:356] Saving checkpoint at step: 9500
I0419 02:19:05.795506 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_9500
I0419 02:19:05.827826 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_9500.
I0419 02:19:06.662282 139487712782080 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5451087951660156, loss=1.6013468503952026
I0419 02:20:21.615946 139487654033152 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.38421377539634705, loss=1.6278852224349976
I0419 02:21:36.624002 139487712782080 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3682765066623688, loss=1.6028804779052734
I0419 02:22:51.602075 139487654033152 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.42186450958251953, loss=1.612008810043335
I0419 02:24:06.633447 139487712782080 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5002257823944092, loss=1.5883679389953613
I0419 02:25:21.661295 139487654033152 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.42876866459846497, loss=1.5450325012207031
I0419 02:26:36.779207 139487712782080 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5516071915626526, loss=1.598494529724121
I0419 02:27:51.816800 139487654033152 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3905541002750397, loss=1.578688144683838
I0419 02:29:09.938639 139487721174784 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5112825036048889, loss=1.5637661218643188
I0419 02:30:24.878750 139487712782080 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4356577396392822, loss=1.5609853267669678
I0419 02:31:39.867643 139487721174784 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4276180565357208, loss=1.5285991430282593
I0419 02:32:54.811723 139487712782080 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.42843756079673767, loss=1.5832723379135132
I0419 02:34:09.800328 139487721174784 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4356271028518677, loss=1.5561444759368896
I0419 02:35:24.809350 139487712782080 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4576771855354309, loss=1.5375531911849976
I0419 02:36:39.716394 139487721174784 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4186629056930542, loss=1.492703914642334
I0419 02:37:54.702525 139487712782080 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5843779444694519, loss=1.4964038133621216
I0419 02:39:09.786791 139487721174784 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3766329884529114, loss=1.5405555963516235
I0419 02:40:24.707381 139487712782080 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3544172942638397, loss=1.5207942724227905
I0419 02:41:39.702683 139487721174784 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.37017038464546204, loss=1.489562749862671
I0419 02:42:57.735165 139487393494784 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.4414409101009369, loss=1.418960690498352
I0419 02:44:12.654927 139487385102080 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.36408957839012146, loss=1.5046298503875732
I0419 02:45:27.573570 139487393494784 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4727669060230255, loss=1.4770066738128662
I0419 02:46:42.492082 139487385102080 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.4294823706150055, loss=1.5145806074142456
I0419 02:47:57.406657 139487393494784 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.37237548828125, loss=1.5152430534362793
I0419 02:49:12.481603 139487385102080 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4896087348461151, loss=1.5211135149002075
I0419 02:50:27.403766 139487393494784 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3869597315788269, loss=1.4787575006484985
I0419 02:51:42.328312 139487385102080 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5569462776184082, loss=1.4890283346176147
I0419 02:52:57.259625 139487393494784 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.4252474308013916, loss=1.4581079483032227
I0419 02:54:12.213464 139487385102080 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.40428784489631653, loss=1.4379855394363403
I0419 02:55:30.313340 139487721174784 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.42442888021469116, loss=1.447555422782898
I0419 02:56:45.298318 139487712782080 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3265724182128906, loss=1.4490125179290771
I0419 02:58:00.146250 139487721174784 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4325766861438751, loss=1.5073556900024414
I0419 02:59:06.353862 139660649875264 spec.py:298] Evaluating on the training split.
I0419 02:59:43.535042 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 03:00:21.224818 139660649875264 spec.py:326] Evaluating on the test split.
I0419 03:00:40.504562 139660649875264 submission_runner.py:406] Time since start: 10150.24s, 	Step: 12690, 	{'train/ctc_loss': DeviceArray(0.30394873, dtype=float32), 'train/wer': 0.1127616793557373, 'validation/ctc_loss': DeviceArray(0.6129082, dtype=float32), 'validation/wer': 0.18712192109909406, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.37197873, dtype=float32), 'test/wer': 0.12688643795828, 'test/num_examples': 2472, 'score': 9664.524187803268, 'total_duration': 10150.243290662766, 'accumulated_submission_time': 9664.524187803268, 'accumulated_eval_time': 479.2541880607605, 'accumulated_logging_time': 6.299361944198608}
I0419 03:00:40.526737 139487792854784 logging_writer.py:48] [12690] accumulated_eval_time=479.254188, accumulated_logging_time=6.299362, accumulated_submission_time=9664.524188, global_step=12690, preemption_count=0, score=9664.524188, test/ctc_loss=0.3719787299633026, test/num_examples=2472, test/wer=0.126886, total_duration=10150.243291, train/ctc_loss=0.3039487302303314, train/wer=0.112762, validation/ctc_loss=0.6129081845283508, validation/num_examples=5348, validation/wer=0.187122
I0419 03:00:40.782958 139660649875264 checkpoints.py:356] Saving checkpoint at step: 12690
I0419 03:00:42.188862 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_12690
I0419 03:00:42.221258 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_12690.
I0419 03:00:50.491443 139487784462080 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.4281959533691406, loss=1.4229941368103027
I0419 03:02:05.538223 139487717320448 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3434045910835266, loss=1.4213294982910156
I0419 03:03:20.507113 139487784462080 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.391501784324646, loss=1.473752737045288
I0419 03:04:35.424527 139487717320448 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.38761186599731445, loss=1.4154856204986572
I0419 03:05:50.394496 139487784462080 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4058375656604767, loss=1.4779032468795776
I0419 03:07:05.391145 139487717320448 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.32737886905670166, loss=1.4611153602600098
I0419 03:08:20.317867 139487784462080 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.33867233991622925, loss=1.4411261081695557
I0419 03:09:38.521496 139487792854784 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3919839560985565, loss=1.4636061191558838
I0419 03:10:53.550593 139487784462080 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.47526970505714417, loss=1.511161208152771
I0419 03:12:08.527438 139487792854784 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.35169580578804016, loss=1.4212419986724854
I0419 03:13:23.440635 139487784462080 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.39820513129234314, loss=1.4140653610229492
I0419 03:14:38.369021 139487792854784 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.32579994201660156, loss=1.4617091417312622
I0419 03:15:53.322030 139487784462080 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3665120005607605, loss=1.516234278678894
I0419 03:17:08.259022 139487792854784 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.38924574851989746, loss=1.455625295639038
I0419 03:18:23.247765 139487784462080 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.36717528104782104, loss=1.406821370124817
I0419 03:19:38.176139 139487792854784 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2817862033843994, loss=1.461101770401001
I0419 03:20:53.124466 139487784462080 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3948812484741211, loss=1.4180989265441895
I0419 03:22:08.048775 139487792854784 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.30755606293678284, loss=1.4596527814865112
I0419 03:23:26.251106 139487792854784 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.36572322249412537, loss=1.420377492904663
I0419 03:24:41.266758 139487784462080 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.44039130210876465, loss=1.3621865510940552
I0419 03:25:56.240245 139487792854784 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.38567638397216797, loss=1.4175324440002441
I0419 03:27:11.054444 139487784462080 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3173045217990875, loss=1.399496078491211
I0419 03:28:26.017472 139487792854784 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.36276137828826904, loss=1.3786733150482178
I0419 03:29:41.070327 139487784462080 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4243305027484894, loss=1.3754664659500122
I0419 03:30:56.074280 139487792854784 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2842763066291809, loss=1.387816071510315
I0419 03:32:10.936359 139487784462080 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3718884587287903, loss=1.4183777570724487
I0419 03:33:25.866823 139487792854784 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.30100077390670776, loss=1.3403346538543701
I0419 03:34:40.851493 139487784462080 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.32950758934020996, loss=1.3421270847320557
I0419 03:35:59.336378 139487792854784 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.379771888256073, loss=1.43252694606781
I0419 03:37:14.316953 139487784462080 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.37005341053009033, loss=1.351393461227417
I0419 03:38:29.205603 139487792854784 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3061746656894684, loss=1.4098968505859375
I0419 03:39:44.089432 139487784462080 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3736060857772827, loss=1.3464304208755493
I0419 03:40:42.893196 139660649875264 spec.py:298] Evaluating on the training split.
I0419 03:41:19.381664 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 03:41:56.651113 139660649875264 spec.py:326] Evaluating on the test split.
I0419 03:42:15.829300 139660649875264 submission_runner.py:406] Time since start: 12645.57s, 	Step: 15880, 	{'train/ctc_loss': DeviceArray(0.26483682, dtype=float32), 'train/wer': 0.10023048989975863, 'validation/ctc_loss': DeviceArray(0.5590781, dtype=float32), 'validation/wer': 0.17134752867852077, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.340109, dtype=float32), 'test/wer': 0.11563382284240245, 'test/num_examples': 2472, 'score': 12065.14809846878, 'total_duration': 12645.568236589432, 'accumulated_submission_time': 12065.14809846878, 'accumulated_eval_time': 572.186933517456, 'accumulated_logging_time': 8.026122093200684}
I0419 03:42:15.851536 139488084694784 logging_writer.py:48] [15880] accumulated_eval_time=572.186934, accumulated_logging_time=8.026122, accumulated_submission_time=12065.148098, global_step=15880, preemption_count=0, score=12065.148098, test/ctc_loss=0.3401089906692505, test/num_examples=2472, test/wer=0.115634, total_duration=12645.568237, train/ctc_loss=0.2648368179798126, train/wer=0.100230, validation/ctc_loss=0.5590780973434448, validation/num_examples=5348, validation/wer=0.171348
I0419 03:42:16.119862 139660649875264 checkpoints.py:356] Saving checkpoint at step: 15880
I0419 03:42:17.531622 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_15880
I0419 03:42:17.564002 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_15880.
I0419 03:42:33.312065 139488076302080 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.4304702877998352, loss=1.395748496055603
I0419 03:43:48.209473 139488000767744 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.32086414098739624, loss=1.3590689897537231
I0419 03:45:03.198272 139488076302080 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.35338813066482544, loss=1.3277634382247925
I0419 03:46:18.124686 139488000767744 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.40465813875198364, loss=1.3624379634857178
I0419 03:47:33.017587 139488076302080 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3495948314666748, loss=1.3644510507583618
I0419 03:48:48.159559 139488000767744 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3877241313457489, loss=1.390372395515442
I0419 03:50:06.438262 139487757014784 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.38277316093444824, loss=1.3772581815719604
I0419 03:51:21.441702 139487748622080 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.42308369278907776, loss=1.3426361083984375
I0419 03:52:36.356420 139487757014784 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3956574499607086, loss=1.332257628440857
I0419 03:53:51.249932 139487748622080 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3430696725845337, loss=1.3177694082260132
I0419 03:55:06.154155 139487757014784 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.34386518597602844, loss=1.3469115495681763
I0419 03:56:21.162861 139487748622080 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3549608886241913, loss=1.3651779890060425
I0419 03:57:36.259019 139487757014784 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2826417088508606, loss=1.3709819316864014
I0419 03:58:51.250817 139487748622080 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.37837833166122437, loss=1.3639706373214722
I0419 04:00:06.220067 139487757014784 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2798866927623749, loss=1.4054529666900635
I0419 04:01:21.171298 139487748622080 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3988072872161865, loss=1.436519980430603
I0419 04:02:36.092806 139487757014784 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.31106388568878174, loss=1.3908815383911133
I0419 04:03:54.260548 139487757014784 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.31864699721336365, loss=1.3173116445541382
I0419 04:05:09.087668 139487748622080 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.46421319246292114, loss=1.3435577154159546
I0419 04:06:23.899375 139487757014784 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3935622572898865, loss=1.3430556058883667
I0419 04:07:38.826390 139487748622080 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3264209032058716, loss=1.370624303817749
I0419 04:08:53.672957 139487757014784 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4005514681339264, loss=1.3464471101760864
I0419 04:10:08.519228 139487748622080 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.39989569783210754, loss=1.3336029052734375
I0419 04:11:23.624976 139487757014784 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.29914620518684387, loss=1.3108866214752197
I0419 04:12:38.485162 139487748622080 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.32928651571273804, loss=1.3850518465042114
I0419 04:13:53.430531 139487757014784 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2755112051963806, loss=1.3457462787628174
I0419 04:15:08.362664 139487748622080 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.39926356077194214, loss=1.3220118284225464
I0419 04:16:26.645246 139487757014784 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3595041334629059, loss=1.3292126655578613
I0419 04:17:41.528318 139487748622080 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3435826897621155, loss=1.2805852890014648
I0419 04:18:56.451034 139487757014784 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.27207598090171814, loss=1.3321926593780518
I0419 04:20:11.354482 139487748622080 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.336380273103714, loss=1.3428069353103638
I0419 04:21:26.341210 139487757014784 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.27726513147354126, loss=1.3002355098724365
I0419 04:22:17.693914 139660649875264 spec.py:298] Evaluating on the training split.
I0419 04:22:54.582772 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 04:23:32.485605 139660649875264 spec.py:326] Evaluating on the test split.
I0419 04:23:51.713222 139660649875264 submission_runner.py:406] Time since start: 15141.45s, 	Step: 19070, 	{'train/ctc_loss': DeviceArray(0.25172147, dtype=float32), 'train/wer': 0.09357111991933892, 'validation/ctc_loss': DeviceArray(0.5241464, dtype=float32), 'validation/wer': 0.1582938571525051, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30777663, dtype=float32), 'test/wer': 0.1033656287449475, 'test/num_examples': 2472, 'score': 14465.229469299316, 'total_duration': 15141.452164411545, 'accumulated_submission_time': 14465.229469299316, 'accumulated_eval_time': 666.2028665542603, 'accumulated_logging_time': 9.772164106369019}
I0419 04:23:51.735403 139487465174784 logging_writer.py:48] [19070] accumulated_eval_time=666.202867, accumulated_logging_time=9.772164, accumulated_submission_time=14465.229469, global_step=19070, preemption_count=0, score=14465.229469, test/ctc_loss=0.30777662992477417, test/num_examples=2472, test/wer=0.103366, total_duration=15141.452164, train/ctc_loss=0.25172147154808044, train/wer=0.093571, validation/ctc_loss=0.5241463780403137, validation/num_examples=5348, validation/wer=0.158294
I0419 04:23:52.013319 139660649875264 checkpoints.py:356] Saving checkpoint at step: 19070
I0419 04:23:53.443139 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_19070
I0419 04:23:53.475409 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_19070.
I0419 04:24:16.679485 139487456782080 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2885027229785919, loss=1.2881640195846558
I0419 04:25:31.537918 139486715221760 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3804270923137665, loss=1.2974134683609009
I0419 04:26:46.416505 139487456782080 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.37414658069610596, loss=1.3030112981796265
I0419 04:28:01.270039 139486715221760 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3120758533477783, loss=1.2895147800445557
I0419 04:29:16.189825 139487456782080 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.33437225222587585, loss=1.2959269285202026
I0419 04:30:34.253763 139487465174784 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.28574350476264954, loss=1.3007032871246338
I0419 04:31:49.120062 139487456782080 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3648752272129059, loss=1.3441768884658813
I0419 04:33:04.008713 139487465174784 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3560725748538971, loss=1.3402644395828247
I0419 04:34:18.966883 139487456782080 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3536069393157959, loss=1.3250077962875366
I0419 04:35:32.819183 139660649875264 spec.py:298] Evaluating on the training split.
I0419 04:36:09.461040 139660649875264 spec.py:310] Evaluating on the validation split.
I0419 04:36:46.717344 139660649875264 spec.py:326] Evaluating on the test split.
I0419 04:37:05.855644 139660649875264 submission_runner.py:406] Time since start: 15935.60s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.2260082, dtype=float32), 'train/wer': 0.08691213068725671, 'validation/ctc_loss': DeviceArray(0.516604, dtype=float32), 'validation/wer': 0.1562195486690658, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3055162, dtype=float32), 'test/wer': 0.10356874454126297, 'test/num_examples': 2472, 'score': 15164.549176454544, 'total_duration': 15935.596105575562, 'accumulated_submission_time': 15164.549176454544, 'accumulated_eval_time': 759.2374794483185, 'accumulated_logging_time': 11.547072410583496}
I0419 04:37:05.873740 139487792854784 logging_writer.py:48] [20000] accumulated_eval_time=759.237479, accumulated_logging_time=11.547072, accumulated_submission_time=15164.549176, global_step=20000, preemption_count=0, score=15164.549176, test/ctc_loss=0.30551621317863464, test/num_examples=2472, test/wer=0.103569, total_duration=15935.596106, train/ctc_loss=0.22600820660591125, train/wer=0.086912, validation/ctc_loss=0.5166040062904358, validation/num_examples=5348, validation/wer=0.156220
I0419 04:37:06.126891 139660649875264 checkpoints.py:356] Saving checkpoint at step: 20000
I0419 04:37:07.532424 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0419 04:37:07.565216 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0419 04:37:07.583967 139487784462080 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15164.549176
I0419 04:37:07.766521 139660649875264 checkpoints.py:356] Saving checkpoint at step: 20000
I0419 04:37:09.726111 139660649875264 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0419 04:37:09.758851 139660649875264 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0419 04:37:11.192462 139660649875264 submission_runner.py:567] Tuning trial 1/1
I0419 04:37:11.192737 139660649875264 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0419 04:37:11.198882 139660649875264 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.578602, dtype=float32), 'train/wer': 0.9440268204818525, 'validation/ctc_loss': DeviceArray(30.926134, dtype=float32), 'validation/wer': 0.8960530251136045, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.875137, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 63.29837656021118, 'total_duration': 168.23331713676453, 'accumulated_submission_time': 63.29837656021118, 'accumulated_eval_time': 104.93476891517639, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3136, {'train/ctc_loss': DeviceArray(2.1686404, dtype=float32), 'train/wer': 0.5057310868983131, 'validation/ctc_loss': DeviceArray(2.6264699, dtype=float32), 'validation/wer': 0.5568601723123233, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.2879589, dtype=float32), 'test/wer': 0.5082769686998558, 'test/num_examples': 2472, 'score': 2463.6030707359314, 'total_duration': 2661.7520101070404, 'accumulated_submission_time': 2463.6030707359314, 'accumulated_eval_time': 196.9353334903717, 'accumulated_logging_time': 1.173588514328003, 'global_step': 3136, 'preemption_count': 0}), (6312, {'train/ctc_loss': DeviceArray(0.56508136, dtype=float32), 'train/wer': 0.19521082492075753, 'validation/ctc_loss': DeviceArray(0.89559597, dtype=float32), 'validation/wer': 0.26368802400409075, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.6094822, dtype=float32), 'test/wer': 0.20183616679869193, 'test/num_examples': 2472, 'score': 4863.841623544693, 'total_duration': 5157.152729034424, 'accumulated_submission_time': 4863.841623544693, 'accumulated_eval_time': 290.25095772743225, 'accumulated_logging_time': 2.9777743816375732, 'global_step': 6312, 'preemption_count': 0}), (9500, {'train/ctc_loss': DeviceArray(0.36682707, dtype=float32), 'train/wer': 0.13511604493150106, 'validation/ctc_loss': DeviceArray(0.6963921, dtype=float32), 'validation/wer': 0.2088683923626856, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44902402, dtype=float32), 'test/wer': 0.15028537769382325, 'test/num_examples': 2472, 'score': 7264.047858953476, 'total_duration': 7653.8419444561005, 'accumulated_submission_time': 7264.047858953476, 'accumulated_eval_time': 385.10705399513245, 'accumulated_logging_time': 4.56264066696167, 'global_step': 9500, 'preemption_count': 0}), (12690, {'train/ctc_loss': DeviceArray(0.30394873, dtype=float32), 'train/wer': 0.1127616793557373, 'validation/ctc_loss': DeviceArray(0.6129082, dtype=float32), 'validation/wer': 0.18712192109909406, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.37197873, dtype=float32), 'test/wer': 0.12688643795828, 'test/num_examples': 2472, 'score': 9664.524187803268, 'total_duration': 10150.243290662766, 'accumulated_submission_time': 9664.524187803268, 'accumulated_eval_time': 479.2541880607605, 'accumulated_logging_time': 6.299361944198608, 'global_step': 12690, 'preemption_count': 0}), (15880, {'train/ctc_loss': DeviceArray(0.26483682, dtype=float32), 'train/wer': 0.10023048989975863, 'validation/ctc_loss': DeviceArray(0.5590781, dtype=float32), 'validation/wer': 0.17134752867852077, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.340109, dtype=float32), 'test/wer': 0.11563382284240245, 'test/num_examples': 2472, 'score': 12065.14809846878, 'total_duration': 12645.568236589432, 'accumulated_submission_time': 12065.14809846878, 'accumulated_eval_time': 572.186933517456, 'accumulated_logging_time': 8.026122093200684, 'global_step': 15880, 'preemption_count': 0}), (19070, {'train/ctc_loss': DeviceArray(0.25172147, dtype=float32), 'train/wer': 0.09357111991933892, 'validation/ctc_loss': DeviceArray(0.5241464, dtype=float32), 'validation/wer': 0.1582938571525051, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.30777663, dtype=float32), 'test/wer': 0.1033656287449475, 'test/num_examples': 2472, 'score': 14465.229469299316, 'total_duration': 15141.452164411545, 'accumulated_submission_time': 14465.229469299316, 'accumulated_eval_time': 666.2028665542603, 'accumulated_logging_time': 9.772164106369019, 'global_step': 19070, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.2260082, dtype=float32), 'train/wer': 0.08691213068725671, 'validation/ctc_loss': DeviceArray(0.516604, dtype=float32), 'validation/wer': 0.1562195486690658, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3055162, dtype=float32), 'test/wer': 0.10356874454126297, 'test/num_examples': 2472, 'score': 15164.549176454544, 'total_duration': 15935.596105575562, 'accumulated_submission_time': 15164.549176454544, 'accumulated_eval_time': 759.2374794483185, 'accumulated_logging_time': 11.547072410583496, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0419 04:37:11.199065 139660649875264 submission_runner.py:570] Timing: 15164.549176454544
I0419 04:37:11.199117 139660649875264 submission_runner.py:571] ====================
I0419 04:37:11.199623 139660649875264 submission_runner.py:631] Final librispeech_conformer score: 15164.549176454544
