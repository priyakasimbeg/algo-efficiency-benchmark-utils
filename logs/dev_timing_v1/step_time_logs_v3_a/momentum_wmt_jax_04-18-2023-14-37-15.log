I0418 14:37:37.113451 139862779823936 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_momentum/wmt_jax.
I0418 14:37:37.181939 139862779823936 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 14:37:38.004317 139862779823936 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0418 14:37:38.005713 139862779823936 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 14:37:38.010195 139862779823936 submission_runner.py:528] Using RNG seed 3881285355
I0418 14:37:40.644472 139862779823936 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 14:37:40.644730 139862779823936 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1.
I0418 14:37:40.645028 139862779823936 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/hparams.json.
I0418 14:37:40.770533 139862779823936 submission_runner.py:232] Initializing dataset.
I0418 14:37:40.779711 139862779823936 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:37:40.783324 139862779823936 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:37:40.783457 139862779823936 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:37:40.907236 139862779823936 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:37:42.910368 139862779823936 submission_runner.py:239] Initializing model.
I0418 14:37:55.324712 139862779823936 submission_runner.py:249] Initializing optimizer.
I0418 14:37:55.875882 139862779823936 submission_runner.py:256] Initializing metrics bundle.
I0418 14:37:55.876151 139862779823936 submission_runner.py:273] Initializing checkpoint and logger.
I0418 14:37:55.877223 139862779823936 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1 with prefix checkpoint_
I0418 14:37:55.877554 139862779823936 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 14:37:55.877634 139862779823936 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 14:37:56.781720 139862779823936 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/meta_data_0.json.
I0418 14:37:56.782749 139862779823936 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/flags_0.json.
I0418 14:37:56.786917 139862779823936 submission_runner.py:309] Starting training loop.
I0418 14:38:26.812320 139686588704512 logging_writer.py:48] [0] global_step=0, grad_norm=5.122814178466797, loss=11.181848526000977
I0418 14:38:26.824990 139862779823936 spec.py:298] Evaluating on the training split.
I0418 14:38:26.827659 139862779823936 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:38:26.830234 139862779823936 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:38:26.830345 139862779823936 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:38:26.861282 139862779823936 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0418 14:38:35.223829 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 14:43:39.724634 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 14:43:39.728643 139862779823936 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 14:43:39.732162 139862779823936 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:43:39.732272 139862779823936 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:43:39.762947 139862779823936 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 14:43:47.325744 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 14:48:44.528565 139862779823936 spec.py:326] Evaluating on the test split.
I0418 14:48:44.531112 139862779823936 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 14:48:44.533680 139862779823936 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 14:48:44.533788 139862779823936 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 14:48:44.562088 139862779823936 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0418 14:48:51.374248 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 14:53:42.514577 139862779823936 submission_runner.py:406] Time since start: 945.73s, 	Step: 1, 	{'train/accuracy': 0.0007201152038760483, 'train/loss': 11.241480827331543, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.269661903381348, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.259050369262695, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.037915229797363, 'total_duration': 945.7275602817535, 'accumulated_submission_time': 30.037915229797363, 'accumulated_eval_time': 915.6895022392273, 'accumulated_logging_time': 0}
I0418 14:53:42.531723 139675532019456 logging_writer.py:48] [1] accumulated_eval_time=915.689502, accumulated_logging_time=0, accumulated_submission_time=30.037915, global_step=1, preemption_count=0, score=30.037915, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.259050, test/num_examples=3003, total_duration=945.727560, train/accuracy=0.000720, train/bleu=0.000000, train/loss=11.241481, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.269662, validation/num_examples=3000
I0418 14:53:43.232697 139862779823936 checkpoints.py:356] Saving checkpoint at step: 1
I0418 14:53:45.719542 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_1
I0418 14:53:45.722759 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_1.
I0418 14:54:21.542466 139675540412160 logging_writer.py:48] [100] global_step=100, grad_norm=0.17849817872047424, loss=9.109102249145508
I0418 14:54:57.322498 139675649517312 logging_writer.py:48] [200] global_step=200, grad_norm=0.14156845211982727, loss=8.928301811218262
I0418 14:55:33.118910 139675540412160 logging_writer.py:48] [300] global_step=300, grad_norm=0.1504989117383957, loss=8.822799682617188
I0418 14:56:08.980561 139675649517312 logging_writer.py:48] [400] global_step=400, grad_norm=0.23339606821537018, loss=8.632670402526855
I0418 14:56:44.781545 139675540412160 logging_writer.py:48] [500] global_step=500, grad_norm=0.7069346308708191, loss=8.469006538391113
I0418 14:57:20.620022 139675649517312 logging_writer.py:48] [600] global_step=600, grad_norm=0.5566343665122986, loss=8.308839797973633
I0418 14:57:56.459795 139675540412160 logging_writer.py:48] [700] global_step=700, grad_norm=0.8157141208648682, loss=8.121219635009766
I0418 14:58:32.308368 139675649517312 logging_writer.py:48] [800] global_step=800, grad_norm=0.8037617206573486, loss=7.988441467285156
I0418 14:59:08.192277 139675540412160 logging_writer.py:48] [900] global_step=900, grad_norm=0.9550120234489441, loss=7.8937153816223145
I0418 14:59:44.072414 139675649517312 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6987642049789429, loss=7.841810703277588
I0418 15:00:19.950179 139675540412160 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.007017970085144, loss=7.728523254394531
I0418 15:00:55.803885 139675649517312 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6995158195495605, loss=7.63943338394165
I0418 15:01:31.680139 139675540412160 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5909442901611328, loss=7.616933345794678
I0418 15:02:07.528311 139675649517312 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8703820705413818, loss=7.5036187171936035
I0418 15:02:43.376700 139675540412160 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6528266668319702, loss=7.449004173278809
I0418 15:03:19.199552 139675649517312 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5726486444473267, loss=7.302881717681885
I0418 15:03:55.100849 139675540412160 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7638797163963318, loss=7.292956829071045
I0418 15:04:30.981126 139675649517312 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5690112709999084, loss=7.185295581817627
I0418 15:05:06.834214 139675540412160 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6014465093612671, loss=7.130837917327881
I0418 15:05:42.700320 139675649517312 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7077611088752747, loss=7.033564567565918
I0418 15:06:18.546019 139675540412160 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6262175440788269, loss=6.991170883178711
I0418 15:06:54.430365 139675649517312 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7738065123558044, loss=6.950401306152344
I0418 15:07:30.350453 139675540412160 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6110226511955261, loss=6.872346878051758
I0418 15:07:45.844799 139862779823936 spec.py:298] Evaluating on the training split.
I0418 15:07:48.847992 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 15:12:38.809209 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 15:12:41.481357 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 15:17:25.867354 139862779823936 spec.py:326] Evaluating on the test split.
I0418 15:17:28.579263 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 15:22:19.499777 139862779823936 submission_runner.py:406] Time since start: 2662.71s, 	Step: 2345, 	{'train/accuracy': 0.30981510877609253, 'train/loss': 5.539774417877197, 'train/bleu': 7.015453071578684, 'validation/accuracy': 0.28588610887527466, 'validation/loss': 5.838521480560303, 'validation/bleu': 3.67790258800453, 'validation/num_examples': 3000, 'test/accuracy': 0.26416826248168945, 'test/loss': 6.121819019317627, 'test/bleu': 2.6933993730025336, 'test/num_examples': 3003, 'score': 870.1244316101074, 'total_duration': 2662.712661743164, 'accumulated_submission_time': 870.1244316101074, 'accumulated_eval_time': 1789.3443326950073, 'accumulated_logging_time': 3.2119481563568115}
I0418 15:22:19.509933 139675649517312 logging_writer.py:48] [2345] accumulated_eval_time=1789.344333, accumulated_logging_time=3.211948, accumulated_submission_time=870.124432, global_step=2345, preemption_count=0, score=870.124432, test/accuracy=0.264168, test/bleu=2.693399, test/loss=6.121819, test/num_examples=3003, total_duration=2662.712662, train/accuracy=0.309815, train/bleu=7.015453, train/loss=5.539774, validation/accuracy=0.285886, validation/bleu=3.677903, validation/loss=5.838521, validation/num_examples=3000
I0418 15:22:20.205512 139862779823936 checkpoints.py:356] Saving checkpoint at step: 2345
I0418 15:22:22.663407 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_2345
I0418 15:22:22.666471 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_2345.
I0418 15:22:42.748858 139675540412160 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6889337301254272, loss=6.786154747009277
I0418 15:23:18.643023 139675624339200 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6674778461456299, loss=6.763309001922607
I0418 15:23:54.546205 139675540412160 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6383131742477417, loss=6.649075508117676
I0418 15:24:30.385633 139675624339200 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6909714937210083, loss=6.5819525718688965
I0418 15:25:06.292187 139675540412160 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7414078116416931, loss=6.527573585510254
I0418 15:25:42.190620 139675624339200 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6655232310295105, loss=6.5398783683776855
I0418 15:26:18.031822 139675540412160 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7234337329864502, loss=6.450274467468262
I0418 15:26:53.937458 139675624339200 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6919040679931641, loss=6.388573169708252
I0418 15:27:29.758257 139675540412160 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.79130619764328, loss=6.385708332061768
I0418 15:28:05.605925 139675624339200 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8137192726135254, loss=6.3283820152282715
I0418 15:28:41.460354 139675540412160 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6273910999298096, loss=6.230469703674316
I0418 15:29:17.322723 139675624339200 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6153343915939331, loss=6.062190532684326
I0418 15:29:53.180294 139675540412160 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6759099960327148, loss=6.133182048797607
I0418 15:30:28.980115 139675624339200 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5723146796226501, loss=6.098400592803955
I0418 15:31:04.803476 139675540412160 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7267144322395325, loss=6.047972679138184
I0418 15:31:40.660747 139675624339200 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6905656456947327, loss=5.909402370452881
I0418 15:32:16.503777 139675540412160 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6349602341651917, loss=5.965003490447998
I0418 15:32:52.376371 139675624339200 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8631194829940796, loss=5.857666015625
I0418 15:33:28.227353 139675540412160 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8512771129608154, loss=5.829253196716309
I0418 15:34:04.100966 139675624339200 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6280720829963684, loss=5.709616184234619
I0418 15:34:39.942883 139675540412160 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5759268999099731, loss=5.733891010284424
I0418 15:35:15.803984 139675624339200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6136236190795898, loss=5.684218883514404
I0418 15:35:51.638560 139675540412160 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6324273943901062, loss=5.681771278381348
I0418 15:36:22.875972 139862779823936 spec.py:298] Evaluating on the training split.
I0418 15:36:25.865900 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 15:40:00.286860 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 15:40:02.934534 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 15:43:19.467624 139862779823936 spec.py:326] Evaluating on the test split.
I0418 15:43:22.170337 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 15:46:23.822809 139862779823936 submission_runner.py:406] Time since start: 4107.04s, 	Step: 4689, 	{'train/accuracy': 0.4555214047431946, 'train/loss': 3.9396183490753174, 'train/bleu': 18.001376417349896, 'validation/accuracy': 0.4477315843105316, 'validation/loss': 3.9965288639068604, 'validation/bleu': 13.157272537284788, 'validation/num_examples': 3000, 'test/accuracy': 0.438010573387146, 'test/loss': 4.140241622924805, 'test/bleu': 11.223804380624161, 'test/num_examples': 3003, 'score': 1710.3009190559387, 'total_duration': 4107.035729885101, 'accumulated_submission_time': 1710.3009190559387, 'accumulated_eval_time': 2390.2910418510437, 'accumulated_logging_time': 6.382060527801514}
I0418 15:46:23.831659 139675624339200 logging_writer.py:48] [4689] accumulated_eval_time=2390.291042, accumulated_logging_time=6.382061, accumulated_submission_time=1710.300919, global_step=4689, preemption_count=0, score=1710.300919, test/accuracy=0.438011, test/bleu=11.223804, test/loss=4.140242, test/num_examples=3003, total_duration=4107.035730, train/accuracy=0.455521, train/bleu=18.001376, train/loss=3.939618, validation/accuracy=0.447732, validation/bleu=13.157273, validation/loss=3.996529, validation/num_examples=3000
I0418 15:46:24.516948 139862779823936 checkpoints.py:356] Saving checkpoint at step: 4689
I0418 15:46:26.993288 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_4689
I0418 15:46:26.996372 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_4689.
I0418 15:46:31.307204 139675540412160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5416529178619385, loss=5.517236232757568
I0418 15:47:07.164780 139675607553792 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5763460397720337, loss=5.480498313903809
I0418 15:47:43.011668 139675540412160 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5651148557662964, loss=5.464179992675781
I0418 15:48:18.847810 139675607553792 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6117208003997803, loss=5.4639668464660645
I0418 15:48:54.754091 139675540412160 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5956612825393677, loss=5.427133083343506
I0418 15:49:30.579088 139675607553792 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5520474910736084, loss=5.496251583099365
I0418 15:50:06.487495 139675540412160 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5496403574943542, loss=5.314464092254639
I0418 15:50:42.359554 139675607553792 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5097170472145081, loss=5.380462646484375
I0418 15:51:18.214450 139675540412160 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.543294370174408, loss=5.368992328643799
I0418 15:51:54.101055 139675607553792 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5184571146965027, loss=5.340741157531738
I0418 15:52:29.945269 139675540412160 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5222971439361572, loss=5.3185648918151855
I0418 15:53:05.793891 139675607553792 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.523996114730835, loss=5.337741374969482
I0418 15:53:41.630669 139675540412160 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5377756953239441, loss=5.224660873413086
I0418 15:54:17.471693 139675607553792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.46504849195480347, loss=5.141697406768799
I0418 15:54:53.303293 139675540412160 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.46326932311058044, loss=5.132162094116211
I0418 15:55:29.171315 139675607553792 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.48083847761154175, loss=5.161411762237549
I0418 15:56:05.012185 139675540412160 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4478828012943268, loss=5.141559600830078
I0418 15:56:40.916095 139675607553792 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.45206061005592346, loss=5.250323295593262
I0418 15:57:16.758744 139675540412160 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.45467790961265564, loss=5.032938480377197
I0418 15:57:52.620999 139675607553792 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.43763861060142517, loss=5.039524555206299
I0418 15:58:28.457190 139675540412160 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.42031508684158325, loss=5.007450580596924
I0418 15:59:04.299300 139675607553792 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4435868561267853, loss=5.005100727081299
I0418 15:59:40.136578 139675540412160 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.43402621150016785, loss=5.055401802062988
I0418 16:00:15.942364 139675607553792 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4679812788963318, loss=5.093190670013428
I0418 16:00:27.137295 139862779823936 spec.py:298] Evaluating on the training split.
I0418 16:00:30.122371 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:03:05.939709 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 16:03:08.582727 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:05:37.161967 139862779823936 spec.py:326] Evaluating on the test split.
I0418 16:05:39.859860 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:08:01.570599 139862779823936 submission_runner.py:406] Time since start: 5404.78s, 	Step: 7033, 	{'train/accuracy': 0.5372858047485352, 'train/loss': 3.15136456489563, 'train/bleu': 23.863529826302738, 'validation/accuracy': 0.5344260931015015, 'validation/loss': 3.147289752960205, 'validation/bleu': 19.825665092035823, 'validation/num_examples': 3000, 'test/accuracy': 0.5324153304100037, 'test/loss': 3.2082905769348145, 'test/bleu': 17.914285718478205, 'test/num_examples': 3003, 'score': 2550.4111490249634, 'total_duration': 5404.783592939377, 'accumulated_submission_time': 2550.4111490249634, 'accumulated_eval_time': 2844.7242982387543, 'accumulated_logging_time': 9.558482646942139}
I0418 16:08:01.579350 139675540412160 logging_writer.py:48] [7033] accumulated_eval_time=2844.724298, accumulated_logging_time=9.558483, accumulated_submission_time=2550.411149, global_step=7033, preemption_count=0, score=2550.411149, test/accuracy=0.532415, test/bleu=17.914286, test/loss=3.208291, test/num_examples=3003, total_duration=5404.783593, train/accuracy=0.537286, train/bleu=23.863530, train/loss=3.151365, validation/accuracy=0.534426, validation/bleu=19.825665, validation/loss=3.147290, validation/num_examples=3000
I0418 16:08:02.273935 139862779823936 checkpoints.py:356] Saving checkpoint at step: 7033
I0418 16:08:04.775474 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_7033
I0418 16:08:04.778603 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_7033.
I0418 16:08:29.144729 139675607553792 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4020099937915802, loss=4.959292411804199
I0418 16:09:05.002810 139675599161088 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.39177605509757996, loss=4.992103576660156
I0418 16:09:40.800162 139675607553792 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.41183510422706604, loss=5.003185272216797
I0418 16:10:16.661514 139675599161088 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.39496883749961853, loss=4.96435022354126
I0418 16:10:52.462038 139675607553792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.40211910009384155, loss=4.929648399353027
I0418 16:11:28.265609 139675599161088 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4063720405101776, loss=4.984796047210693
I0418 16:12:04.132121 139675607553792 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4029163718223572, loss=4.95308256149292
I0418 16:12:39.954604 139675599161088 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3955371677875519, loss=4.83001708984375
I0418 16:13:15.808572 139675607553792 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4164738059043884, loss=5.044545650482178
I0418 16:13:51.646426 139675599161088 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3847501873970032, loss=5.0013508796691895
I0418 16:14:27.464267 139675607553792 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3988167643547058, loss=4.832185745239258
I0418 16:15:03.326323 139675599161088 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.36987531185150146, loss=4.910106658935547
I0418 16:15:39.187719 139675607553792 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.395346075296402, loss=4.8816680908203125
I0418 16:16:15.063948 139675599161088 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3797828257083893, loss=4.806544303894043
I0418 16:16:50.964097 139675607553792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.37114056944847107, loss=4.8727030754089355
I0418 16:17:26.789854 139675599161088 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.38250765204429626, loss=4.815807819366455
I0418 16:18:02.635890 139675607553792 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3638598620891571, loss=4.836421489715576
I0418 16:18:38.455460 139675599161088 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3779808580875397, loss=4.878203392028809
I0418 16:19:14.350557 139675607553792 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.38884058594703674, loss=4.81949520111084
I0418 16:19:50.250233 139675599161088 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.362027645111084, loss=4.835243225097656
I0418 16:20:26.131498 139675607553792 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3655044436454773, loss=4.858399868011475
I0418 16:21:02.004589 139675599161088 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3823550045490265, loss=4.909569263458252
I0418 16:21:37.846184 139675607553792 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.37843751907348633, loss=4.82514762878418
I0418 16:22:04.786910 139862779823936 spec.py:298] Evaluating on the training split.
I0418 16:22:07.763294 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:24:44.006145 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 16:24:46.649754 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:27:15.347891 139862779823936 spec.py:326] Evaluating on the test split.
I0418 16:27:18.041581 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:29:42.350753 139862779823936 submission_runner.py:406] Time since start: 6705.56s, 	Step: 9377, 	{'train/accuracy': 0.5632085800170898, 'train/loss': 2.87933087348938, 'train/bleu': 25.96617144555462, 'validation/accuracy': 0.5697139501571655, 'validation/loss': 2.786851167678833, 'validation/bleu': 22.252867548403493, 'validation/num_examples': 3000, 'test/accuracy': 0.5696938037872314, 'test/loss': 2.811368227005005, 'test/bleu': 20.67292630609854, 'test/num_examples': 3003, 'score': 3390.3858489990234, 'total_duration': 6705.56374835968, 'accumulated_submission_time': 3390.3858489990234, 'accumulated_eval_time': 3302.288085460663, 'accumulated_logging_time': 12.769812107086182}
I0418 16:29:42.359109 139675599161088 logging_writer.py:48] [9377] accumulated_eval_time=3302.288085, accumulated_logging_time=12.769812, accumulated_submission_time=3390.385849, global_step=9377, preemption_count=0, score=3390.385849, test/accuracy=0.569694, test/bleu=20.672926, test/loss=2.811368, test/num_examples=3003, total_duration=6705.563748, train/accuracy=0.563209, train/bleu=25.966171, train/loss=2.879331, validation/accuracy=0.569714, validation/bleu=22.252868, validation/loss=2.786851, validation/num_examples=3000
I0418 16:29:43.036526 139862779823936 checkpoints.py:356] Saving checkpoint at step: 9377
I0418 16:29:45.518091 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_9377
I0418 16:29:45.521137 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_9377.
I0418 16:29:54.119572 139675607553792 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.36044690012931824, loss=4.78703498840332
I0418 16:30:29.910891 139675320223488 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.35810765624046326, loss=4.73980188369751
I0418 16:31:05.687465 139675607553792 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.36926355957984924, loss=4.770954608917236
I0418 16:31:41.507238 139675320223488 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3587343096733093, loss=4.8139142990112305
I0418 16:32:17.334084 139675607553792 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.36218395829200745, loss=4.825098514556885
I0418 16:32:53.174269 139675320223488 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3727218508720398, loss=4.733964920043945
I0418 16:33:29.037650 139675607553792 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3708966076374054, loss=4.738527774810791
I0418 16:34:04.848613 139675320223488 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.36070770025253296, loss=4.7145256996154785
I0418 16:34:40.696432 139675607553792 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3538581132888794, loss=4.6411519050598145
I0418 16:35:16.490736 139675320223488 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3570113778114319, loss=4.663499355316162
I0418 16:35:52.274532 139675607553792 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.368879109621048, loss=4.680709362030029
I0418 16:36:28.064523 139675320223488 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3453039824962616, loss=4.644203186035156
I0418 16:37:03.861718 139675607553792 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3548130691051483, loss=4.707396507263184
I0418 16:37:39.660452 139675320223488 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3659324645996094, loss=4.750667095184326
I0418 16:38:15.533667 139675607553792 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3411494791507721, loss=4.631534099578857
I0418 16:38:51.353980 139675320223488 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.35669049620628357, loss=4.739163875579834
I0418 16:39:27.155951 139675607553792 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3719215989112854, loss=4.722293853759766
I0418 16:40:02.971816 139675320223488 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.34409046173095703, loss=4.551300048828125
I0418 16:40:38.820008 139675607553792 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3394719362258911, loss=4.608952522277832
I0418 16:41:14.638191 139675320223488 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.36094608902931213, loss=4.714178085327148
I0418 16:41:50.404378 139675607553792 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.33985111117362976, loss=4.648107051849365
I0418 16:42:26.193127 139675320223488 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.34360072016716003, loss=4.601015090942383
I0418 16:43:01.998749 139675607553792 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3317663073539734, loss=4.643880367279053
I0418 16:43:37.804712 139675320223488 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3482626676559448, loss=4.575094223022461
I0418 16:43:45.757083 139862779823936 spec.py:298] Evaluating on the training split.
I0418 16:43:48.742876 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:46:14.623261 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 16:46:17.257188 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:48:41.693219 139862779823936 spec.py:326] Evaluating on the test split.
I0418 16:48:44.387250 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 16:51:07.101101 139862779823936 submission_runner.py:406] Time since start: 7990.31s, 	Step: 11724, 	{'train/accuracy': 0.5796862840652466, 'train/loss': 2.709535598754883, 'train/bleu': 27.380516911805685, 'validation/accuracy': 0.5885481834411621, 'validation/loss': 2.5991148948669434, 'validation/bleu': 23.48727301341078, 'validation/num_examples': 3000, 'test/accuracy': 0.5913195013999939, 'test/loss': 2.6036434173583984, 'test/bleu': 22.193775457638726, 'test/num_examples': 3003, 'score': 4230.589122772217, 'total_duration': 7990.3139617443085, 'accumulated_submission_time': 4230.589122772217, 'accumulated_eval_time': 3743.631924390793, 'accumulated_logging_time': 15.943118572235107}
I0418 16:51:07.112976 139675607553792 logging_writer.py:48] [11724] accumulated_eval_time=3743.631924, accumulated_logging_time=15.943119, accumulated_submission_time=4230.589123, global_step=11724, preemption_count=0, score=4230.589123, test/accuracy=0.591320, test/bleu=22.193775, test/loss=2.603643, test/num_examples=3003, total_duration=7990.313962, train/accuracy=0.579686, train/bleu=27.380517, train/loss=2.709536, validation/accuracy=0.588548, validation/bleu=23.487273, validation/loss=2.599115, validation/num_examples=3000
I0418 16:51:07.961061 139862779823936 checkpoints.py:356] Saving checkpoint at step: 11724
I0418 16:51:11.041227 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_11724
I0418 16:51:11.044158 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_11724.
I0418 16:51:38.677747 139675320223488 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.3437099754810333, loss=4.5754313468933105
I0418 16:52:14.470919 139675311830784 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.32512056827545166, loss=4.591646671295166
I0418 16:52:50.283828 139675320223488 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3377077877521515, loss=4.706396102905273
I0418 16:53:26.078707 139675311830784 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3432103991508484, loss=4.611766338348389
I0418 16:54:01.937921 139675320223488 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.34074872732162476, loss=4.55196475982666
I0418 16:54:37.807002 139675311830784 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3499529957771301, loss=4.605965614318848
I0418 16:55:13.638763 139675320223488 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3447653651237488, loss=4.528020858764648
I0418 16:55:49.437657 139675311830784 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.34038791060447693, loss=4.594666481018066
I0418 16:56:25.303188 139675320223488 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3396620452404022, loss=4.588204383850098
I0418 16:57:01.169104 139675311830784 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.35530880093574524, loss=4.5621490478515625
I0418 16:57:37.012748 139675320223488 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.33781638741493225, loss=4.655629634857178
I0418 16:58:12.816596 139675311830784 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.34567710757255554, loss=4.510654926300049
I0418 16:58:48.669585 139675320223488 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3290700316429138, loss=4.615930080413818
I0418 16:59:24.516458 139675311830784 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3378801941871643, loss=4.5829362869262695
I0418 17:00:00.331662 139675320223488 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3255186676979065, loss=4.6170735359191895
I0418 17:00:36.184567 139675311830784 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3270573318004608, loss=4.594350337982178
I0418 17:01:11.982620 139675320223488 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3259541094303131, loss=4.533365249633789
I0418 17:01:47.817815 139675311830784 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3325001299381256, loss=4.658368110656738
I0418 17:02:23.639022 139675320223488 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.31739285588264465, loss=4.5037102699279785
I0418 17:02:59.471813 139675311830784 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3468714952468872, loss=4.584817409515381
I0418 17:03:35.258619 139675320223488 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3303492069244385, loss=4.611903667449951
I0418 17:04:11.077737 139675311830784 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3442884087562561, loss=4.584719657897949
I0418 17:04:46.877985 139675320223488 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.32579296827316284, loss=4.5809197425842285
I0418 17:05:11.302670 139862779823936 spec.py:298] Evaluating on the training split.
I0418 17:05:14.302686 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:07:48.230973 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 17:07:50.873414 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:10:15.660933 139862779823936 spec.py:326] Evaluating on the test split.
I0418 17:10:18.373284 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:12:39.367476 139862779823936 submission_runner.py:406] Time since start: 9282.58s, 	Step: 14070, 	{'train/accuracy': 0.6002973318099976, 'train/loss': 2.5383806228637695, 'train/bleu': 28.46851996404356, 'validation/accuracy': 0.6051877737045288, 'validation/loss': 2.4724843502044678, 'validation/bleu': 24.486117111397082, 'validation/num_examples': 3000, 'test/accuracy': 0.6079135537147522, 'test/loss': 2.4588003158569336, 'test/bleu': 23.18389332257941, 'test/num_examples': 3003, 'score': 5070.815846204758, 'total_duration': 9282.580471515656, 'accumulated_submission_time': 5070.815846204758, 'accumulated_eval_time': 4191.696689128876, 'accumulated_logging_time': 19.88908290863037}
I0418 17:12:39.376266 139675311830784 logging_writer.py:48] [14070] accumulated_eval_time=4191.696689, accumulated_logging_time=19.889083, accumulated_submission_time=5070.815846, global_step=14070, preemption_count=0, score=5070.815846, test/accuracy=0.607914, test/bleu=23.183893, test/loss=2.458800, test/num_examples=3003, total_duration=9282.580472, train/accuracy=0.600297, train/bleu=28.468520, train/loss=2.538381, validation/accuracy=0.605188, validation/bleu=24.486117, validation/loss=2.472484, validation/num_examples=3000
I0418 17:12:40.070868 139862779823936 checkpoints.py:356] Saving checkpoint at step: 14070
I0418 17:12:42.534562 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_14070
I0418 17:12:42.537545 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_14070.
I0418 17:12:53.650015 139675320223488 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3302578032016754, loss=4.569602966308594
I0418 17:13:29.466550 139675303438080 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.322348028421402, loss=4.531321048736572
I0418 17:14:05.300013 139675320223488 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.33543267846107483, loss=4.5482964515686035
I0418 17:14:41.100177 139675303438080 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.31392183899879456, loss=4.5201873779296875
I0418 17:15:16.896380 139675320223488 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.32943958044052124, loss=4.575113296508789
I0418 17:15:52.723401 139675303438080 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.32287290692329407, loss=4.58281135559082
I0418 17:16:28.561778 139675320223488 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.31957995891571045, loss=4.495868682861328
I0418 17:17:04.420315 139675303438080 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.33506450057029724, loss=4.454161167144775
I0418 17:17:40.269624 139675320223488 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3325548768043518, loss=4.5176801681518555
I0418 17:18:16.099530 139675303438080 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3202000558376312, loss=4.464330196380615
I0418 17:18:51.936755 139675320223488 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.32847923040390015, loss=4.502169609069824
I0418 17:19:27.843602 139675303438080 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3265598714351654, loss=4.456417560577393
I0418 17:20:03.668870 139675320223488 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.33167046308517456, loss=4.611435413360596
I0418 17:20:39.529390 139675303438080 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.32685568928718567, loss=4.41110897064209
I0418 17:21:15.329805 139675320223488 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.33095380663871765, loss=4.542555332183838
I0418 17:21:51.159500 139675303438080 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3169562816619873, loss=4.563755035400391
I0418 17:22:26.975904 139675320223488 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.32662060856819153, loss=4.4872283935546875
I0418 17:23:02.825358 139675303438080 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.334166944026947, loss=4.547099590301514
I0418 17:23:38.674305 139675320223488 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.31342607736587524, loss=4.415925979614258
I0418 17:24:14.487476 139675303438080 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3309319019317627, loss=4.4707536697387695
I0418 17:24:50.336698 139675320223488 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.31754744052886963, loss=4.45734167098999
I0418 17:25:26.191339 139675303438080 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.31732597947120667, loss=4.487986087799072
I0418 17:26:02.060543 139675320223488 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.31979432702064514, loss=4.462229251861572
I0418 17:26:37.899875 139675303438080 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3158046305179596, loss=4.400216579437256
I0418 17:26:42.645886 139862779823936 spec.py:298] Evaluating on the training split.
I0418 17:26:45.639074 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:29:23.981464 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 17:29:26.622340 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:31:53.062532 139862779823936 spec.py:326] Evaluating on the test split.
I0418 17:31:55.748115 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:34:18.199862 139862779823936 submission_runner.py:406] Time since start: 10581.41s, 	Step: 16415, 	{'train/accuracy': 0.6034440994262695, 'train/loss': 2.5005345344543457, 'train/bleu': 29.19832296690706, 'validation/accuracy': 0.6124412417411804, 'validation/loss': 2.3839523792266846, 'validation/bleu': 25.370344677057027, 'validation/num_examples': 3000, 'test/accuracy': 0.6168032288551331, 'test/loss': 2.3622796535491943, 'test/bleu': 24.073121185655136, 'test/num_examples': 3003, 'score': 5910.891007900238, 'total_duration': 10581.412861585617, 'accumulated_submission_time': 5910.891007900238, 'accumulated_eval_time': 4647.250611066818, 'accumulated_logging_time': 23.06192922592163}
I0418 17:34:18.208593 139675320223488 logging_writer.py:48] [16415] accumulated_eval_time=4647.250611, accumulated_logging_time=23.061929, accumulated_submission_time=5910.891008, global_step=16415, preemption_count=0, score=5910.891008, test/accuracy=0.616803, test/bleu=24.073121, test/loss=2.362280, test/num_examples=3003, total_duration=10581.412862, train/accuracy=0.603444, train/bleu=29.198323, train/loss=2.500535, validation/accuracy=0.612441, validation/bleu=25.370345, validation/loss=2.383952, validation/num_examples=3000
I0418 17:34:18.886260 139862779823936 checkpoints.py:356] Saving checkpoint at step: 16415
I0418 17:34:21.362082 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_16415
I0418 17:34:21.365084 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_16415.
I0418 17:34:52.167878 139675303438080 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.31577762961387634, loss=4.562793731689453
I0418 17:35:27.961331 139675295045376 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3190884292125702, loss=4.401050567626953
I0418 17:36:03.777581 139675303438080 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.31646427512168884, loss=4.511658668518066
I0418 17:36:39.599206 139675295045376 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.33148273825645447, loss=4.464888095855713
I0418 17:37:15.443799 139675303438080 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3276612162590027, loss=4.549045085906982
I0418 17:37:51.297026 139675295045376 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3193126916885376, loss=4.524461269378662
I0418 17:38:27.126837 139675303438080 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.30131372809410095, loss=4.473286151885986
I0418 17:39:02.968988 139675295045376 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3419448435306549, loss=4.434821605682373
I0418 17:39:38.797381 139675303438080 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3193308711051941, loss=4.410451412200928
I0418 17:40:14.616626 139675295045376 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3238627016544342, loss=4.4932780265808105
I0418 17:40:50.438885 139675303438080 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3168177604675293, loss=4.4930949211120605
I0418 17:41:26.301741 139675295045376 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.31718385219573975, loss=4.441586017608643
I0418 17:42:02.135943 139675303438080 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.31926852464675903, loss=4.459695339202881
I0418 17:42:37.997615 139675295045376 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.31867584586143494, loss=4.51029634475708
I0418 17:43:13.838319 139675303438080 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3176098167896271, loss=4.390559673309326
I0418 17:43:49.669583 139675295045376 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3106398284435272, loss=4.384991645812988
I0418 17:44:25.500306 139675303438080 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3169202506542206, loss=4.458680629730225
I0418 17:45:01.344175 139675295045376 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3213079571723938, loss=4.486233711242676
I0418 17:45:37.167049 139675303438080 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.31290456652641296, loss=4.44492244720459
I0418 17:46:12.986052 139675295045376 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3277285695075989, loss=4.493058204650879
I0418 17:46:48.769563 139675303438080 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.31624892354011536, loss=4.3289947509765625
I0418 17:47:24.620422 139675295045376 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3354641795158386, loss=4.369296073913574
I0418 17:48:00.465268 139675303438080 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3169821798801422, loss=4.329080581665039
I0418 17:48:21.688851 139862779823936 spec.py:298] Evaluating on the training split.
I0418 17:48:24.690557 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:50:59.896026 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 17:51:02.558548 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:53:30.759679 139862779823936 spec.py:326] Evaluating on the test split.
I0418 17:53:33.467604 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 17:55:59.635565 139862779823936 submission_runner.py:406] Time since start: 11882.85s, 	Step: 18761, 	{'train/accuracy': 0.6096113920211792, 'train/loss': 2.443347930908203, 'train/bleu': 29.036680194652057, 'validation/accuracy': 0.6183432340621948, 'validation/loss': 2.325253486633301, 'validation/bleu': 25.69074265464107, 'validation/num_examples': 3000, 'test/accuracy': 0.6249956488609314, 'test/loss': 2.297701120376587, 'test/bleu': 24.270298299597954, 'test/num_examples': 3003, 'score': 6751.18402838707, 'total_duration': 11882.848571777344, 'accumulated_submission_time': 6751.18402838707, 'accumulated_eval_time': 5105.197293996811, 'accumulated_logging_time': 26.23004651069641}
I0418 17:55:59.645261 139675295045376 logging_writer.py:48] [18761] accumulated_eval_time=5105.197294, accumulated_logging_time=26.230047, accumulated_submission_time=6751.184028, global_step=18761, preemption_count=0, score=6751.184028, test/accuracy=0.624996, test/bleu=24.270298, test/loss=2.297701, test/num_examples=3003, total_duration=11882.848572, train/accuracy=0.609611, train/bleu=29.036680, train/loss=2.443348, validation/accuracy=0.618343, validation/bleu=25.690743, validation/loss=2.325253, validation/num_examples=3000
I0418 17:56:00.321872 139862779823936 checkpoints.py:356] Saving checkpoint at step: 18761
I0418 17:56:02.774510 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_18761
I0418 17:56:02.777632 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_18761.
I0418 17:56:17.138955 139675303438080 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.30803829431533813, loss=4.4068427085876465
I0418 17:56:52.992744 139675286652672 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.30530351400375366, loss=4.429180145263672
I0418 17:57:28.826757 139675303438080 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3159748315811157, loss=4.385653018951416
I0418 17:58:04.643985 139675286652672 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3027328848838806, loss=4.412965774536133
I0418 17:58:40.447007 139675303438080 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.31630587577819824, loss=4.356774806976318
I0418 17:59:16.255133 139675286652672 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3140484690666199, loss=4.339984893798828
I0418 17:59:52.102503 139675303438080 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3223567306995392, loss=4.418863296508789
I0418 18:00:27.942673 139675286652672 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3138696551322937, loss=4.35927152633667
I0418 18:01:03.713537 139675303438080 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.313701331615448, loss=4.399657726287842
I0418 18:01:39.513430 139675286652672 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.30606433749198914, loss=4.434855937957764
I0418 18:02:15.309562 139675303438080 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.31044700741767883, loss=4.449600696563721
I0418 18:02:51.143837 139675286652672 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.31996387243270874, loss=4.471440315246582
I0418 18:03:26.276485 139862779823936 spec.py:298] Evaluating on the training split.
I0418 18:03:29.265571 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 18:05:57.254784 139862779823936 spec.py:310] Evaluating on the validation split.
I0418 18:05:59.904726 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 18:08:22.499721 139862779823936 spec.py:326] Evaluating on the test split.
I0418 18:08:25.202624 139862779823936 workload.py:179] Translating evaluation dataset.
I0418 18:10:46.396608 139862779823936 submission_runner.py:406] Time since start: 12769.61s, 	Step: 20000, 	{'train/accuracy': 0.6238821148872375, 'train/loss': 2.3226475715637207, 'train/bleu': 29.737829787276144, 'validation/accuracy': 0.6220877766609192, 'validation/loss': 2.2752809524536133, 'validation/bleu': 25.821456503301537, 'validation/num_examples': 3000, 'test/accuracy': 0.6294347047805786, 'test/loss': 2.2511579990386963, 'test/bleu': 24.701663530431695, 'test/num_examples': 3003, 'score': 7194.664098024368, 'total_duration': 12769.609462499619, 'accumulated_submission_time': 7194.664098024368, 'accumulated_eval_time': 5545.317233800888, 'accumulated_logging_time': 29.375240087509155}
I0418 18:10:46.406981 139675303438080 logging_writer.py:48] [20000] accumulated_eval_time=5545.317234, accumulated_logging_time=29.375240, accumulated_submission_time=7194.664098, global_step=20000, preemption_count=0, score=7194.664098, test/accuracy=0.629435, test/bleu=24.701664, test/loss=2.251158, test/num_examples=3003, total_duration=12769.609462, train/accuracy=0.623882, train/bleu=29.737830, train/loss=2.322648, validation/accuracy=0.622088, validation/bleu=25.821457, validation/loss=2.275281, validation/num_examples=3000
I0418 18:10:47.095140 139862779823936 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:10:49.518696 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_20000
I0418 18:10:49.521863 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_20000.
I0418 18:10:49.532508 139675286652672 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7194.664098
I0418 18:10:49.892492 139862779823936 checkpoints.py:356] Saving checkpoint at step: 20000
I0418 18:10:53.561080 139862779823936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_20000
I0418 18:10:53.564076 139862779823936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/wmt_jax/trial_1/checkpoint_20000.
I0418 18:10:53.613445 139862779823936 submission_runner.py:567] Tuning trial 1/1
I0418 18:10:53.613626 139862779823936 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0418 18:10:53.614636 139862779823936 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007201152038760483, 'train/loss': 11.241480827331543, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.269661903381348, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.259050369262695, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.037915229797363, 'total_duration': 945.7275602817535, 'accumulated_submission_time': 30.037915229797363, 'accumulated_eval_time': 915.6895022392273, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2345, {'train/accuracy': 0.30981510877609253, 'train/loss': 5.539774417877197, 'train/bleu': 7.015453071578684, 'validation/accuracy': 0.28588610887527466, 'validation/loss': 5.838521480560303, 'validation/bleu': 3.67790258800453, 'validation/num_examples': 3000, 'test/accuracy': 0.26416826248168945, 'test/loss': 6.121819019317627, 'test/bleu': 2.6933993730025336, 'test/num_examples': 3003, 'score': 870.1244316101074, 'total_duration': 2662.712661743164, 'accumulated_submission_time': 870.1244316101074, 'accumulated_eval_time': 1789.3443326950073, 'accumulated_logging_time': 3.2119481563568115, 'global_step': 2345, 'preemption_count': 0}), (4689, {'train/accuracy': 0.4555214047431946, 'train/loss': 3.9396183490753174, 'train/bleu': 18.001376417349896, 'validation/accuracy': 0.4477315843105316, 'validation/loss': 3.9965288639068604, 'validation/bleu': 13.157272537284788, 'validation/num_examples': 3000, 'test/accuracy': 0.438010573387146, 'test/loss': 4.140241622924805, 'test/bleu': 11.223804380624161, 'test/num_examples': 3003, 'score': 1710.3009190559387, 'total_duration': 4107.035729885101, 'accumulated_submission_time': 1710.3009190559387, 'accumulated_eval_time': 2390.2910418510437, 'accumulated_logging_time': 6.382060527801514, 'global_step': 4689, 'preemption_count': 0}), (7033, {'train/accuracy': 0.5372858047485352, 'train/loss': 3.15136456489563, 'train/bleu': 23.863529826302738, 'validation/accuracy': 0.5344260931015015, 'validation/loss': 3.147289752960205, 'validation/bleu': 19.825665092035823, 'validation/num_examples': 3000, 'test/accuracy': 0.5324153304100037, 'test/loss': 3.2082905769348145, 'test/bleu': 17.914285718478205, 'test/num_examples': 3003, 'score': 2550.4111490249634, 'total_duration': 5404.783592939377, 'accumulated_submission_time': 2550.4111490249634, 'accumulated_eval_time': 2844.7242982387543, 'accumulated_logging_time': 9.558482646942139, 'global_step': 7033, 'preemption_count': 0}), (9377, {'train/accuracy': 0.5632085800170898, 'train/loss': 2.87933087348938, 'train/bleu': 25.96617144555462, 'validation/accuracy': 0.5697139501571655, 'validation/loss': 2.786851167678833, 'validation/bleu': 22.252867548403493, 'validation/num_examples': 3000, 'test/accuracy': 0.5696938037872314, 'test/loss': 2.811368227005005, 'test/bleu': 20.67292630609854, 'test/num_examples': 3003, 'score': 3390.3858489990234, 'total_duration': 6705.56374835968, 'accumulated_submission_time': 3390.3858489990234, 'accumulated_eval_time': 3302.288085460663, 'accumulated_logging_time': 12.769812107086182, 'global_step': 9377, 'preemption_count': 0}), (11724, {'train/accuracy': 0.5796862840652466, 'train/loss': 2.709535598754883, 'train/bleu': 27.380516911805685, 'validation/accuracy': 0.5885481834411621, 'validation/loss': 2.5991148948669434, 'validation/bleu': 23.48727301341078, 'validation/num_examples': 3000, 'test/accuracy': 0.5913195013999939, 'test/loss': 2.6036434173583984, 'test/bleu': 22.193775457638726, 'test/num_examples': 3003, 'score': 4230.589122772217, 'total_duration': 7990.3139617443085, 'accumulated_submission_time': 4230.589122772217, 'accumulated_eval_time': 3743.631924390793, 'accumulated_logging_time': 15.943118572235107, 'global_step': 11724, 'preemption_count': 0}), (14070, {'train/accuracy': 0.6002973318099976, 'train/loss': 2.5383806228637695, 'train/bleu': 28.46851996404356, 'validation/accuracy': 0.6051877737045288, 'validation/loss': 2.4724843502044678, 'validation/bleu': 24.486117111397082, 'validation/num_examples': 3000, 'test/accuracy': 0.6079135537147522, 'test/loss': 2.4588003158569336, 'test/bleu': 23.18389332257941, 'test/num_examples': 3003, 'score': 5070.815846204758, 'total_duration': 9282.580471515656, 'accumulated_submission_time': 5070.815846204758, 'accumulated_eval_time': 4191.696689128876, 'accumulated_logging_time': 19.88908290863037, 'global_step': 14070, 'preemption_count': 0}), (16415, {'train/accuracy': 0.6034440994262695, 'train/loss': 2.5005345344543457, 'train/bleu': 29.19832296690706, 'validation/accuracy': 0.6124412417411804, 'validation/loss': 2.3839523792266846, 'validation/bleu': 25.370344677057027, 'validation/num_examples': 3000, 'test/accuracy': 0.6168032288551331, 'test/loss': 2.3622796535491943, 'test/bleu': 24.073121185655136, 'test/num_examples': 3003, 'score': 5910.891007900238, 'total_duration': 10581.412861585617, 'accumulated_submission_time': 5910.891007900238, 'accumulated_eval_time': 4647.250611066818, 'accumulated_logging_time': 23.06192922592163, 'global_step': 16415, 'preemption_count': 0}), (18761, {'train/accuracy': 0.6096113920211792, 'train/loss': 2.443347930908203, 'train/bleu': 29.036680194652057, 'validation/accuracy': 0.6183432340621948, 'validation/loss': 2.325253486633301, 'validation/bleu': 25.69074265464107, 'validation/num_examples': 3000, 'test/accuracy': 0.6249956488609314, 'test/loss': 2.297701120376587, 'test/bleu': 24.270298299597954, 'test/num_examples': 3003, 'score': 6751.18402838707, 'total_duration': 11882.848571777344, 'accumulated_submission_time': 6751.18402838707, 'accumulated_eval_time': 5105.197293996811, 'accumulated_logging_time': 26.23004651069641, 'global_step': 18761, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6238821148872375, 'train/loss': 2.3226475715637207, 'train/bleu': 29.737829787276144, 'validation/accuracy': 0.6220877766609192, 'validation/loss': 2.2752809524536133, 'validation/bleu': 25.821456503301537, 'validation/num_examples': 3000, 'test/accuracy': 0.6294347047805786, 'test/loss': 2.2511579990386963, 'test/bleu': 24.701663530431695, 'test/num_examples': 3003, 'score': 7194.664098024368, 'total_duration': 12769.609462499619, 'accumulated_submission_time': 7194.664098024368, 'accumulated_eval_time': 5545.317233800888, 'accumulated_logging_time': 29.375240087509155, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0418 18:10:53.614760 139862779823936 submission_runner.py:570] Timing: 7194.664098024368
I0418 18:10:53.614813 139862779823936 submission_runner.py:571] ====================
I0418 18:10:53.614931 139862779823936 submission_runner.py:631] Final wmt score: 7194.664098024368
