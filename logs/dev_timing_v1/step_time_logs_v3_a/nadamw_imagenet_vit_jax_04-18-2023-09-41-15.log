I0418 09:41:38.163805 139939736233792 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax.
I0418 09:41:38.226238 139939736233792 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 09:41:39.010282 139939736233792 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0418 09:41:39.011282 139939736233792 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 09:41:39.015106 139939736233792 submission_runner.py:528] Using RNG seed 300819920
I0418 09:41:41.734540 139939736233792 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 09:41:41.734735 139939736233792 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1.
I0418 09:41:41.734934 139939736233792 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/hparams.json.
I0418 09:41:41.854926 139939736233792 submission_runner.py:232] Initializing dataset.
I0418 09:41:41.867414 139939736233792 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:41:41.874575 139939736233792 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 09:41:41.874687 139939736233792 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 09:41:42.133436 139939736233792 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:41:48.712033 139939736233792 submission_runner.py:239] Initializing model.
I0418 09:41:59.408758 139939736233792 submission_runner.py:249] Initializing optimizer.
I0418 09:42:00.037770 139939736233792 submission_runner.py:256] Initializing metrics bundle.
I0418 09:42:00.037953 139939736233792 submission_runner.py:273] Initializing checkpoint and logger.
I0418 09:42:00.038851 139939736233792 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0418 09:42:00.740536 139939736233792 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0418 09:42:00.741579 139939736233792 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/flags_0.json.
I0418 09:42:00.746419 139939736233792 submission_runner.py:309] Starting training loop.
I0418 09:42:50.901850 139761779992320 logging_writer.py:48] [0] global_step=0, grad_norm=0.34194815158843994, loss=6.907756805419922
I0418 09:42:50.916488 139939736233792 spec.py:298] Evaluating on the training split.
I0418 09:42:50.922485 139939736233792 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:42:50.928550 139939736233792 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 09:42:50.928693 139939736233792 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 09:42:50.987889 139939736233792 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:43:10.591877 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 09:43:10.598950 139939736233792 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:43:10.611200 139939736233792 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0418 09:43:10.611544 139939736233792 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0418 09:43:10.677585 139939736233792 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0418 09:43:28.630609 139939736233792 spec.py:326] Evaluating on the test split.
I0418 09:43:28.637779 139939736233792 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 09:43:28.642959 139939736233792 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0418 09:43:28.673455 139939736233792 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0418 09:43:39.605833 139939736233792 submission_runner.py:406] Time since start: 98.86s, 	Step: 1, 	{'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 50.1699116230011, 'total_duration': 98.85930919647217, 'accumulated_submission_time': 50.1699116230011, 'accumulated_eval_time': 48.689247846603394, 'accumulated_logging_time': 0}
I0418 09:43:39.623898 139699238725376 logging_writer.py:48] [1] accumulated_eval_time=48.689248, accumulated_logging_time=0, accumulated_submission_time=50.169912, global_step=1, preemption_count=0, score=50.169912, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=98.859309, train/accuracy=0.000918, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0418 09:43:39.819300 139939736233792 checkpoints.py:356] Saving checkpoint at step: 1
I0418 09:43:40.334659 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_1
I0418 09:43:40.335541 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_1.
I0418 09:44:37.911000 139757698873088 logging_writer.py:48] [100] global_step=100, grad_norm=0.40348708629608154, loss=6.902105331420898
I0418 09:45:17.859457 139757707265792 logging_writer.py:48] [200] global_step=200, grad_norm=0.420651376247406, loss=6.872698783874512
I0418 09:45:57.656016 139757698873088 logging_writer.py:48] [300] global_step=300, grad_norm=0.572531521320343, loss=6.836602687835693
I0418 09:46:38.061452 139757707265792 logging_writer.py:48] [400] global_step=400, grad_norm=0.7024019360542297, loss=6.714064121246338
I0418 09:47:18.973376 139757698873088 logging_writer.py:48] [500] global_step=500, grad_norm=0.8019906282424927, loss=6.600722312927246
I0418 09:48:00.002113 139757707265792 logging_writer.py:48] [600] global_step=600, grad_norm=1.1613211631774902, loss=6.542773723602295
I0418 09:48:40.872792 139757698873088 logging_writer.py:48] [700] global_step=700, grad_norm=1.045143961906433, loss=6.502346038818359
I0418 09:49:22.191739 139757707265792 logging_writer.py:48] [800] global_step=800, grad_norm=1.135338306427002, loss=6.423098564147949
I0418 09:50:03.222984 139757698873088 logging_writer.py:48] [900] global_step=900, grad_norm=1.1176977157592773, loss=6.690334320068359
I0418 09:50:40.602441 139939736233792 spec.py:298] Evaluating on the training split.
I0418 09:50:51.564520 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 09:50:58.060004 139939736233792 spec.py:326] Evaluating on the test split.
I0418 09:51:00.021806 139939736233792 submission_runner.py:406] Time since start: 539.28s, 	Step: 992, 	{'train/accuracy': 0.03228515386581421, 'train/loss': 5.959888935089111, 'validation/accuracy': 0.031099999323487282, 'validation/loss': 5.980340957641602, 'validation/num_examples': 50000, 'test/accuracy': 0.024000000208616257, 'test/loss': 6.07896614074707, 'test/num_examples': 10000, 'score': 470.41340684890747, 'total_duration': 539.2752826213837, 'accumulated_submission_time': 470.41340684890747, 'accumulated_eval_time': 68.10858678817749, 'accumulated_logging_time': 0.7311644554138184}
I0418 09:51:00.038409 139700723513088 logging_writer.py:48] [992] accumulated_eval_time=68.108587, accumulated_logging_time=0.731164, accumulated_submission_time=470.413407, global_step=992, preemption_count=0, score=470.413407, test/accuracy=0.024000, test/loss=6.078966, test/num_examples=10000, total_duration=539.275283, train/accuracy=0.032285, train/loss=5.959889, validation/accuracy=0.031100, validation/loss=5.980341, validation/num_examples=50000
I0418 09:51:01.735706 139939736233792 checkpoints.py:356] Saving checkpoint at step: 992
I0418 09:51:03.016545 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_992
I0418 09:51:03.017451 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_992.
I0418 09:51:06.671926 139700807374592 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.5213764905929565, loss=6.42365837097168
I0418 09:51:46.486161 139762111338240 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0714573860168457, loss=6.188401699066162
I0418 09:52:26.360174 139700807374592 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2295550107955933, loss=6.2952165603637695
I0418 09:53:06.711939 139762111338240 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.431471347808838, loss=6.191961288452148
I0418 09:53:47.795892 139700807374592 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.417299747467041, loss=6.1962199211120605
I0418 09:54:28.871516 139762111338240 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.252758502960205, loss=6.112125873565674
I0418 09:55:09.970148 139700807374592 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1601641178131104, loss=6.021641731262207
I0418 09:55:50.876497 139762111338240 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0029776096343994, loss=6.692498683929443
I0418 09:56:32.157185 139700807374592 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.3036258220672607, loss=5.933523654937744
I0418 09:57:12.806457 139762111338240 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1958645582199097, loss=6.598999977111816
I0418 09:57:53.984068 139700807374592 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.233702301979065, loss=5.967318534851074
I0418 09:58:03.199946 139939736233792 spec.py:298] Evaluating on the training split.
I0418 09:58:14.111844 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 09:58:20.627634 139939736233792 spec.py:326] Evaluating on the test split.
I0418 09:58:22.339426 139939736233792 submission_runner.py:406] Time since start: 981.59s, 	Step: 2024, 	{'train/accuracy': 0.07947265356779099, 'train/loss': 5.2394328117370605, 'validation/accuracy': 0.07485999912023544, 'validation/loss': 5.27151346206665, 'validation/num_examples': 50000, 'test/accuracy': 0.05640000104904175, 'test/loss': 5.4720001220703125, 'test/num_examples': 10000, 'score': 890.5732078552246, 'total_duration': 981.5929033756256, 'accumulated_submission_time': 890.5732078552246, 'accumulated_eval_time': 87.24804615974426, 'accumulated_logging_time': 3.728252649307251}
I0418 09:58:22.355472 139762111338240 logging_writer.py:48] [2024] accumulated_eval_time=87.248046, accumulated_logging_time=3.728253, accumulated_submission_time=890.573208, global_step=2024, preemption_count=0, score=890.573208, test/accuracy=0.056400, test/loss=5.472000, test/num_examples=10000, total_duration=981.592903, train/accuracy=0.079473, train/loss=5.239433, validation/accuracy=0.074860, validation/loss=5.271513, validation/num_examples=50000
I0418 09:58:22.523653 139939736233792 checkpoints.py:356] Saving checkpoint at step: 2024
I0418 09:58:25.697057 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_2024
I0418 09:58:25.708541 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_2024.
I0418 09:58:56.409214 139700807374592 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3034312725067139, loss=5.909101486206055
I0418 09:59:36.387742 139762077767424 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.269347906112671, loss=6.169252872467041
I0418 10:00:16.421334 139700807374592 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0407863855361938, loss=5.963090896606445
I0418 10:00:57.523659 139762077767424 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8014179468154907, loss=6.621716499328613
I0418 10:01:38.634186 139700807374592 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.017586588859558, loss=6.442791938781738
I0418 10:02:19.628835 139762077767424 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8297394514083862, loss=6.205404281616211
I0418 10:03:00.602358 139700807374592 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2554349899291992, loss=5.780580997467041
I0418 10:03:41.818574 139762077767424 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9454641342163086, loss=6.520309925079346
I0418 10:04:22.623355 139700807374592 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9101558923721313, loss=6.5299391746521
I0418 10:05:03.711509 139762077767424 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2117432355880737, loss=5.57460880279541
I0418 10:05:25.799738 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:05:37.065967 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:05:43.861006 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:05:45.589060 139939736233792 submission_runner.py:406] Time since start: 1424.84s, 	Step: 3055, 	{'train/accuracy': 0.14162109792232513, 'train/loss': 4.651334285736084, 'validation/accuracy': 0.13210000097751617, 'validation/loss': 4.712193012237549, 'validation/num_examples': 50000, 'test/accuracy': 0.10280000418424606, 'test/loss': 5.005489349365234, 'test/num_examples': 10000, 'score': 1310.6416590213776, 'total_duration': 1424.8425040245056, 'accumulated_submission_time': 1310.6416590213776, 'accumulated_eval_time': 107.03730773925781, 'accumulated_logging_time': 7.098662853240967}
I0418 10:05:45.601290 139700807374592 logging_writer.py:48] [3055] accumulated_eval_time=107.037308, accumulated_logging_time=7.098663, accumulated_submission_time=1310.641659, global_step=3055, preemption_count=0, score=1310.641659, test/accuracy=0.102800, test/loss=5.005489, test/num_examples=10000, total_duration=1424.842504, train/accuracy=0.141621, train/loss=4.651334, validation/accuracy=0.132100, validation/loss=4.712193, validation/num_examples=50000
I0418 10:05:45.766378 139939736233792 checkpoints.py:356] Saving checkpoint at step: 3055
I0418 10:05:48.934098 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_3055
I0418 10:05:48.947606 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_3055.
I0418 10:06:07.379144 139762077767424 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0193705558776855, loss=5.635794639587402
I0418 10:06:47.394761 139761792579328 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1548701524734497, loss=5.559860706329346
I0418 10:07:27.615493 139762077767424 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0464892387390137, loss=5.555476665496826
I0418 10:08:07.956594 139761792579328 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0561864376068115, loss=5.607497215270996
I0418 10:08:48.717333 139762077767424 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2076210975646973, loss=5.495143890380859
I0418 10:09:29.894057 139761792579328 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9335439801216125, loss=5.9990553855896
I0418 10:10:11.091893 139762077767424 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.4200414419174194, loss=5.3108649253845215
I0418 10:10:51.986961 139761792579328 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9745455980300903, loss=5.858805179595947
I0418 10:11:33.143440 139762077767424 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.2541152238845825, loss=5.1783270835876465
I0418 10:12:14.172565 139761792579328 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0341664552688599, loss=5.199894905090332
I0418 10:12:49.203799 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:13:00.489413 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:13:07.816462 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:13:09.491119 139939736233792 submission_runner.py:406] Time since start: 1868.74s, 	Step: 4087, 	{'train/accuracy': 0.20749999582767487, 'train/loss': 4.1564435958862305, 'validation/accuracy': 0.1842999905347824, 'validation/loss': 4.2814459800720215, 'validation/num_examples': 50000, 'test/accuracy': 0.14420001208782196, 'test/loss': 4.637369155883789, 'test/num_examples': 10000, 'score': 1730.875303030014, 'total_duration': 1868.7446064949036, 'accumulated_submission_time': 1730.875303030014, 'accumulated_eval_time': 127.3246021270752, 'accumulated_logging_time': 10.458579540252686}
I0418 10:13:09.502920 139762077767424 logging_writer.py:48] [4087] accumulated_eval_time=127.324602, accumulated_logging_time=10.458580, accumulated_submission_time=1730.875303, global_step=4087, preemption_count=0, score=1730.875303, test/accuracy=0.144200, test/loss=4.637369, test/num_examples=10000, total_duration=1868.744606, train/accuracy=0.207500, train/loss=4.156444, validation/accuracy=0.184300, validation/loss=4.281446, validation/num_examples=50000
I0418 10:13:09.612254 139939736233792 checkpoints.py:356] Saving checkpoint at step: 4087
I0418 10:13:12.069128 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_4087
I0418 10:13:12.086138 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_4087.
I0418 10:13:17.798992 139761792579328 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9317908883094788, loss=6.339341640472412
I0418 10:13:57.968109 139761721243392 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8918027877807617, loss=6.2408037185668945
I0418 10:14:37.930252 139761792579328 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1801255941390991, loss=5.215424537658691
I0418 10:15:18.466316 139761721243392 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.147659182548523, loss=5.154090881347656
I0418 10:15:59.703890 139761792579328 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.336077094078064, loss=5.1393022537231445
I0418 10:16:41.152654 139761721243392 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.195634365081787, loss=5.086480140686035
I0418 10:17:22.134072 139761792579328 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.108378291130066, loss=5.099367618560791
I0418 10:18:02.879549 139761721243392 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.074122428894043, loss=5.067266464233398
I0418 10:18:43.904040 139761792579328 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9959460496902466, loss=5.037815093994141
I0418 10:19:25.254576 139761721243392 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9525856375694275, loss=5.201032638549805
I0418 10:20:06.367733 139761792579328 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7996659278869629, loss=6.235978126525879
I0418 10:20:12.112749 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:20:23.452038 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:20:31.854771 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:20:33.534132 139939736233792 submission_runner.py:406] Time since start: 2312.79s, 	Step: 5116, 	{'train/accuracy': 0.25458982586860657, 'train/loss': 3.759672164916992, 'validation/accuracy': 0.23841999471187592, 'validation/loss': 3.8421905040740967, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.268174648284912, 'test/num_examples': 10000, 'score': 2150.878241300583, 'total_duration': 2312.7876489162445, 'accumulated_submission_time': 2150.878241300583, 'accumulated_eval_time': 148.74596571922302, 'accumulated_logging_time': 13.05631709098816}
I0418 10:20:33.544933 139761721243392 logging_writer.py:48] [5116] accumulated_eval_time=148.745966, accumulated_logging_time=13.056317, accumulated_submission_time=2150.878241, global_step=5116, preemption_count=0, score=2150.878241, test/accuracy=0.179100, test/loss=4.268175, test/num_examples=10000, total_duration=2312.787649, train/accuracy=0.254590, train/loss=3.759672, validation/accuracy=0.238420, validation/loss=3.842191, validation/num_examples=50000
I0418 10:20:33.748681 139939736233792 checkpoints.py:356] Saving checkpoint at step: 5116
I0418 10:20:35.320317 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_5116
I0418 10:20:35.337668 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_5116.
I0418 10:21:09.409120 139761792579328 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0687683820724487, loss=4.8747239112854
I0418 10:21:49.442718 139761712850688 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.104086995124817, loss=4.933435440063477
I0418 10:22:29.894255 139761792579328 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.1719166040420532, loss=4.781060695648193
I0418 10:23:10.937608 139761712850688 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0465245246887207, loss=5.647247314453125
I0418 10:23:52.225898 139761792579328 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.0459314584732056, loss=4.809384822845459
I0418 10:24:33.415295 139761712850688 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.234316110610962, loss=4.823205471038818
I0418 10:25:14.564502 139761792579328 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.84858238697052, loss=5.734404563903809
I0418 10:25:55.824875 139761712850688 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8892634510993958, loss=4.8762664794921875
I0418 10:26:37.160789 139761792579328 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9276447296142578, loss=4.763225555419922
I0418 10:27:18.502302 139761712850688 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9300627112388611, loss=4.779986381530762
I0418 10:27:35.390640 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:27:47.819293 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:27:56.868615 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:27:58.552464 139939736233792 submission_runner.py:406] Time since start: 2757.81s, 	Step: 6143, 	{'train/accuracy': 0.3039843738079071, 'train/loss': 3.473700761795044, 'validation/accuracy': 0.28139999508857727, 'validation/loss': 3.598806142807007, 'validation/num_examples': 50000, 'test/accuracy': 0.21300001442432404, 'test/loss': 4.075122833251953, 'test/num_examples': 10000, 'score': 2570.9080505371094, 'total_duration': 2757.805949449539, 'accumulated_submission_time': 2570.9080505371094, 'accumulated_eval_time': 171.90778875350952, 'accumulated_logging_time': 14.861746549606323}
I0418 10:27:58.565509 139761792579328 logging_writer.py:48] [6143] accumulated_eval_time=171.907789, accumulated_logging_time=14.861747, accumulated_submission_time=2570.908051, global_step=6143, preemption_count=0, score=2570.908051, test/accuracy=0.213000, test/loss=4.075123, test/num_examples=10000, total_duration=2757.805949, train/accuracy=0.303984, train/loss=3.473701, validation/accuracy=0.281400, validation/loss=3.598806, validation/num_examples=50000
I0418 10:27:58.719292 139939736233792 checkpoints.py:356] Saving checkpoint at step: 6143
I0418 10:27:59.773231 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_6143
I0418 10:27:59.786761 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_6143.
I0418 10:28:23.294728 139761712850688 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.1074813604354858, loss=4.766345500946045
I0418 10:29:03.412722 139761704457984 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9700332880020142, loss=4.735682487487793
I0418 10:29:44.419364 139761712850688 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.1064273118972778, loss=4.707819938659668
I0418 10:30:25.697850 139761704457984 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8410677313804626, loss=5.803167343139648
I0418 10:31:06.892789 139761712850688 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.1414241790771484, loss=4.6100993156433105
I0418 10:31:48.178649 139761704457984 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7726535201072693, loss=6.224875450134277
I0418 10:32:29.568903 139761712850688 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9754453301429749, loss=4.764825344085693
I0418 10:33:10.729095 139761704457984 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.1645599603652954, loss=4.6566572189331055
I0418 10:33:52.577422 139761712850688 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0352792739868164, loss=4.505605220794678
I0418 10:34:33.736162 139761704457984 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8991338610649109, loss=4.482723712921143
I0418 10:35:00.033110 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:35:13.870658 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:35:23.356583 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:35:25.021011 139939736233792 submission_runner.py:406] Time since start: 3204.27s, 	Step: 7165, 	{'train/accuracy': 0.35169920325279236, 'train/loss': 3.118074893951416, 'validation/accuracy': 0.3258799910545349, 'validation/loss': 3.255978584289551, 'validation/num_examples': 50000, 'test/accuracy': 0.2515999972820282, 'test/loss': 3.7957961559295654, 'test/num_examples': 10000, 'score': 2991.131687402725, 'total_duration': 3204.274514436722, 'accumulated_submission_time': 2991.131687402725, 'accumulated_eval_time': 196.8956606388092, 'accumulated_logging_time': 16.09781575202942}
I0418 10:35:25.031221 139761712850688 logging_writer.py:48] [7165] accumulated_eval_time=196.895661, accumulated_logging_time=16.097816, accumulated_submission_time=2991.131687, global_step=7165, preemption_count=0, score=2991.131687, test/accuracy=0.251600, test/loss=3.795796, test/num_examples=10000, total_duration=3204.274514, train/accuracy=0.351699, train/loss=3.118075, validation/accuracy=0.325880, validation/loss=3.255979, validation/num_examples=50000
I0418 10:35:25.166267 139939736233792 checkpoints.py:356] Saving checkpoint at step: 7165
I0418 10:35:26.210006 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_7165
I0418 10:35:26.226314 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_7165.
I0418 10:35:40.670698 139761704457984 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7998853325843811, loss=5.185803413391113
I0418 10:36:20.793321 139761306015488 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9603450298309326, loss=4.694335460662842
I0418 10:37:01.889484 139761704457984 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9793463349342346, loss=4.4209489822387695
I0418 10:37:43.082242 139761306015488 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8964536786079407, loss=4.518150329589844
I0418 10:38:24.406262 139761704457984 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8786640763282776, loss=4.451322555541992
I0418 10:39:05.602979 139761306015488 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9043814539909363, loss=4.378113269805908
I0418 10:39:46.907580 139761704457984 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9312047362327576, loss=4.363430500030518
I0418 10:40:28.207272 139761306015488 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9457928538322449, loss=4.342697620391846
I0418 10:41:09.535497 139761704457984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7743666768074036, loss=5.147769927978516
I0418 10:41:50.897392 139761306015488 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9982644319534302, loss=4.378013610839844
I0418 10:42:26.583790 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:42:40.823119 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:42:50.808599 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:42:52.469142 139939736233792 submission_runner.py:406] Time since start: 3651.72s, 	Step: 8188, 	{'train/accuracy': 0.394843727350235, 'train/loss': 2.910557270050049, 'validation/accuracy': 0.35933998227119446, 'validation/loss': 3.087632179260254, 'validation/num_examples': 50000, 'test/accuracy': 0.27390000224113464, 'test/loss': 3.6293017864227295, 'test/num_examples': 10000, 'score': 3411.466136932373, 'total_duration': 3651.7226235866547, 'accumulated_submission_time': 3411.466136932373, 'accumulated_eval_time': 222.7809739112854, 'accumulated_logging_time': 17.305179119110107}
I0418 10:42:52.482999 139761704457984 logging_writer.py:48] [8188] accumulated_eval_time=222.780974, accumulated_logging_time=17.305179, accumulated_submission_time=3411.466137, global_step=8188, preemption_count=0, score=3411.466137, test/accuracy=0.273900, test/loss=3.629302, test/num_examples=10000, total_duration=3651.722624, train/accuracy=0.394844, train/loss=2.910557, validation/accuracy=0.359340, validation/loss=3.087632, validation/num_examples=50000
I0418 10:42:52.635656 139939736233792 checkpoints.py:356] Saving checkpoint at step: 8188
I0418 10:42:53.862268 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_8188
I0418 10:42:53.877753 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_8188.
I0418 10:42:59.139011 139761306015488 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9061927795410156, loss=4.411223411560059
I0418 10:43:39.130024 139761297622784 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7567408084869385, loss=5.7124223709106445
I0418 10:44:20.347995 139761306015488 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0419059991836548, loss=4.388040065765381
I0418 10:45:01.643783 139761297622784 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7901560068130493, loss=4.684500217437744
I0418 10:45:42.582407 139761306015488 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6638122200965881, loss=6.007311820983887
I0418 10:46:23.871754 139761297622784 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7658754587173462, loss=4.872997760772705
I0418 10:47:05.229176 139761306015488 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7835400700569153, loss=4.934076309204102
I0418 10:47:46.493229 139761297622784 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.789350152015686, loss=4.474079132080078
I0418 10:48:27.711034 139761306015488 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7798032760620117, loss=4.136978626251221
I0418 10:49:09.123649 139761297622784 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.715356707572937, loss=6.031008720397949
I0418 10:49:50.466732 139761306015488 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7196120619773865, loss=4.939817905426025
I0418 10:49:54.220552 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:50:08.683123 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:50:19.292976 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:50:20.963878 139939736233792 submission_runner.py:406] Time since start: 4100.22s, 	Step: 9211, 	{'train/accuracy': 0.4273437261581421, 'train/loss': 2.6984026432037354, 'validation/accuracy': 0.3906799852848053, 'validation/loss': 2.868025779724121, 'validation/num_examples': 50000, 'test/accuracy': 0.3052000105381012, 'test/loss': 3.430504083633423, 'test/num_examples': 10000, 'score': 3831.785843372345, 'total_duration': 4100.217361688614, 'accumulated_submission_time': 3831.785843372345, 'accumulated_eval_time': 249.52425932884216, 'accumulated_logging_time': 18.71648383140564}
I0418 10:50:20.978279 139761297622784 logging_writer.py:48] [9211] accumulated_eval_time=249.524259, accumulated_logging_time=18.716484, accumulated_submission_time=3831.785843, global_step=9211, preemption_count=0, score=3831.785843, test/accuracy=0.305200, test/loss=3.430504, test/num_examples=10000, total_duration=4100.217362, train/accuracy=0.427344, train/loss=2.698403, validation/accuracy=0.390680, validation/loss=2.868026, validation/num_examples=50000
I0418 10:50:21.133579 139939736233792 checkpoints.py:356] Saving checkpoint at step: 9211
I0418 10:50:22.376380 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_9211
I0418 10:50:22.392105 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_9211.
I0418 10:50:58.465031 139761306015488 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5913994312286377, loss=6.006312847137451
I0418 10:51:39.764442 139761289230080 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8666815161705017, loss=4.213438510894775
I0418 10:52:20.961403 139761306015488 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6353830695152283, loss=5.385428428649902
I0418 10:53:02.012125 139761289230080 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8685216903686523, loss=4.147194862365723
I0418 10:53:43.300580 139761306015488 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6101782917976379, loss=5.635451316833496
I0418 10:54:24.494748 139761289230080 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7367953062057495, loss=4.121912002563477
I0418 10:55:05.687036 139761306015488 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8167957067489624, loss=4.103977680206299
I0418 10:55:46.809047 139761289230080 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6922627091407776, loss=5.231612205505371
I0418 10:56:28.292998 139761306015488 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8971726894378662, loss=4.006392478942871
I0418 10:57:09.778296 139761289230080 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.1024152040481567, loss=4.100130081176758
I0418 10:57:22.457210 139939736233792 spec.py:298] Evaluating on the training split.
I0418 10:57:35.981014 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 10:57:48.206979 139939736233792 spec.py:326] Evaluating on the test split.
I0418 10:57:49.878261 139939736233792 submission_runner.py:406] Time since start: 4549.13s, 	Step: 10232, 	{'train/accuracy': 0.44794920086860657, 'train/loss': 2.5984833240509033, 'validation/accuracy': 0.4193199872970581, 'validation/loss': 2.744295597076416, 'validation/num_examples': 50000, 'test/accuracy': 0.3257000148296356, 'test/loss': 3.327915668487549, 'test/num_examples': 10000, 'score': 4251.828115701675, 'total_duration': 4549.131719589233, 'accumulated_submission_time': 4251.828115701675, 'accumulated_eval_time': 276.9452495574951, 'accumulated_logging_time': 20.14720606803894}
I0418 10:57:49.892394 139761306015488 logging_writer.py:48] [10232] accumulated_eval_time=276.945250, accumulated_logging_time=20.147206, accumulated_submission_time=4251.828116, global_step=10232, preemption_count=0, score=4251.828116, test/accuracy=0.325700, test/loss=3.327916, test/num_examples=10000, total_duration=4549.131720, train/accuracy=0.447949, train/loss=2.598483, validation/accuracy=0.419320, validation/loss=2.744296, validation/num_examples=50000
I0418 10:57:50.083702 139939736233792 checkpoints.py:356] Saving checkpoint at step: 10232
I0418 10:57:51.338355 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_10232
I0418 10:57:51.353836 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_10232.
I0418 10:58:18.850327 139761289230080 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7958202362060547, loss=4.589853286743164
I0418 10:58:59.465761 139761280837376 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6655927300453186, loss=5.210404396057129
I0418 10:59:40.968815 139761289230080 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9362601041793823, loss=3.993980884552002
I0418 11:00:22.414048 139761280837376 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8678368926048279, loss=4.081681251525879
I0418 11:01:03.553674 139761289230080 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7869507670402527, loss=4.1770453453063965
I0418 11:01:44.651161 139761280837376 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.764473557472229, loss=3.943326234817505
I0418 11:02:25.633826 139761289230080 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8755505084991455, loss=3.873202323913574
I0418 11:03:06.733645 139761280837376 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.0974581241607666, loss=4.3372416496276855
I0418 11:03:48.428795 139761289230080 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7090870141983032, loss=4.16316556930542
I0418 11:04:30.586468 139761280837376 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6515079140663147, loss=5.907753944396973
I0418 11:04:51.395297 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:05:05.618930 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:05:17.107403 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:05:18.791801 139939736233792 submission_runner.py:406] Time since start: 4998.05s, 	Step: 11252, 	{'train/accuracy': 0.48451170325279236, 'train/loss': 2.374006986618042, 'validation/accuracy': 0.4479599893093109, 'validation/loss': 2.544445514678955, 'validation/num_examples': 50000, 'test/accuracy': 0.3549000024795532, 'test/loss': 3.1430864334106445, 'test/num_examples': 10000, 'score': 4671.847253084183, 'total_duration': 4998.045312643051, 'accumulated_submission_time': 4671.847253084183, 'accumulated_eval_time': 304.3417296409607, 'accumulated_logging_time': 21.624865293502808}
I0418 11:05:18.802066 139761289230080 logging_writer.py:48] [11252] accumulated_eval_time=304.341730, accumulated_logging_time=21.624865, accumulated_submission_time=4671.847253, global_step=11252, preemption_count=0, score=4671.847253, test/accuracy=0.354900, test/loss=3.143086, test/num_examples=10000, total_duration=4998.045313, train/accuracy=0.484512, train/loss=2.374007, validation/accuracy=0.447960, validation/loss=2.544446, validation/num_examples=50000
I0418 11:05:18.966932 139939736233792 checkpoints.py:356] Saving checkpoint at step: 11252
I0418 11:05:20.042023 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_11252
I0418 11:05:20.056982 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_11252.
I0418 11:05:39.658142 139761280837376 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.733816921710968, loss=4.086369514465332
I0418 11:06:20.877115 139761272444672 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7810791730880737, loss=3.87034273147583
I0418 11:07:03.603254 139761280837376 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7486172914505005, loss=3.9316012859344482
I0418 11:07:46.086306 139761272444672 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7333616018295288, loss=3.858680009841919
I0418 11:08:27.819748 139761280837376 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8830009698867798, loss=5.510706424713135
I0418 11:09:09.006074 139761272444672 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8078416585922241, loss=3.9789950847625732
I0418 11:09:50.549535 139761280837376 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6699433922767639, loss=5.216264724731445
I0418 11:10:32.072048 139761272444672 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9078353643417358, loss=3.937847137451172
I0418 11:11:13.658220 139761280837376 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7650839686393738, loss=5.119787216186523
I0418 11:11:55.252488 139761272444672 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8231514692306519, loss=3.878559112548828
I0418 11:12:20.456537 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:12:34.805655 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:12:45.751039 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:12:47.405299 139939736233792 submission_runner.py:406] Time since start: 5446.66s, 	Step: 12262, 	{'train/accuracy': 0.5203320384025574, 'train/loss': 2.2236344814300537, 'validation/accuracy': 0.472460001707077, 'validation/loss': 2.440772294998169, 'validation/num_examples': 50000, 'test/accuracy': 0.3751000165939331, 'test/loss': 3.0486156940460205, 'test/num_examples': 10000, 'score': 5092.225755214691, 'total_duration': 5446.6587908267975, 'accumulated_submission_time': 5092.225755214691, 'accumulated_eval_time': 331.29045939445496, 'accumulated_logging_time': 22.891602277755737}
I0418 11:12:47.419476 139761280837376 logging_writer.py:48] [12262] accumulated_eval_time=331.290459, accumulated_logging_time=22.891602, accumulated_submission_time=5092.225755, global_step=12262, preemption_count=0, score=5092.225755, test/accuracy=0.375100, test/loss=3.048616, test/num_examples=10000, total_duration=5446.658791, train/accuracy=0.520332, train/loss=2.223634, validation/accuracy=0.472460, validation/loss=2.440772, validation/num_examples=50000
I0418 11:12:47.621518 139939736233792 checkpoints.py:356] Saving checkpoint at step: 12262
I0418 11:12:48.931097 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_12262
I0418 11:12:48.948025 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_12262.
I0418 11:13:04.595826 139761272444672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.86993008852005, loss=3.842108726501465
I0418 11:13:45.623028 139761171797760 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7383366823196411, loss=5.48502254486084
I0418 11:14:27.553779 139761272444672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8734225034713745, loss=3.7918591499328613
I0418 11:15:09.305466 139761171797760 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6448615789413452, loss=5.847612380981445
I0418 11:15:51.061155 139761272444672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.752702534198761, loss=5.794495582580566
I0418 11:16:32.360579 139761171797760 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.890900194644928, loss=3.902052879333496
I0418 11:17:13.778211 139761272444672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7308939695358276, loss=4.112673759460449
I0418 11:17:55.138942 139761171797760 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.783119797706604, loss=4.210006237030029
I0418 11:18:36.395869 139761272444672 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7938215136528015, loss=3.8971707820892334
I0418 11:19:18.055937 139761171797760 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.894909143447876, loss=3.8744797706604004
I0418 11:19:49.135027 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:20:03.031871 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:20:14.381408 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:20:16.061157 139939736233792 submission_runner.py:406] Time since start: 5895.31s, 	Step: 13275, 	{'train/accuracy': 0.5576171875, 'train/loss': 1.9679107666015625, 'validation/accuracy': 0.4920399785041809, 'validation/loss': 2.2809665203094482, 'validation/num_examples': 50000, 'test/accuracy': 0.3841000199317932, 'test/loss': 2.9072213172912598, 'test/num_examples': 10000, 'score': 5512.3914222717285, 'total_duration': 5895.314643859863, 'accumulated_submission_time': 5512.3914222717285, 'accumulated_eval_time': 358.2165570259094, 'accumulated_logging_time': 24.436192274093628}
I0418 11:20:16.075524 139761272444672 logging_writer.py:48] [13275] accumulated_eval_time=358.216557, accumulated_logging_time=24.436192, accumulated_submission_time=5512.391422, global_step=13275, preemption_count=0, score=5512.391422, test/accuracy=0.384100, test/loss=2.907221, test/num_examples=10000, total_duration=5895.314644, train/accuracy=0.557617, train/loss=1.967911, validation/accuracy=0.492040, validation/loss=2.280967, validation/num_examples=50000
I0418 11:20:16.335867 139939736233792 checkpoints.py:356] Saving checkpoint at step: 13275
I0418 11:20:18.694244 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_13275
I0418 11:20:18.712095 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_13275.
I0418 11:20:29.111711 139761171797760 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7568890452384949, loss=3.840818405151367
I0418 11:21:09.380988 139761163405056 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8131795525550842, loss=3.6986260414123535
I0418 11:21:50.626233 139761171797760 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.628603994846344, loss=4.930230140686035
I0418 11:22:31.858527 139761163405056 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8329058289527893, loss=3.680054187774658
I0418 11:23:13.040236 139761171797760 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7455096244812012, loss=3.6284728050231934
I0418 11:23:54.485224 139761163405056 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7132967114448547, loss=3.6863961219787598
I0418 11:24:35.637389 139761171797760 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7348840832710266, loss=3.8109543323516846
I0418 11:25:16.826188 139761163405056 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8437886834144592, loss=3.624128818511963
I0418 11:25:58.531910 139761171797760 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9174870848655701, loss=3.702784299850464
I0418 11:26:40.099415 139761163405056 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7227010130882263, loss=3.6407670974731445
I0418 11:27:18.977843 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:27:32.918884 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:27:45.034674 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:27:46.700883 139939736233792 submission_runner.py:406] Time since start: 6345.95s, 	Step: 14293, 	{'train/accuracy': 0.5441210865974426, 'train/loss': 2.074860095977783, 'validation/accuracy': 0.508080005645752, 'validation/loss': 2.2444303035736084, 'validation/num_examples': 50000, 'test/accuracy': 0.39430001378059387, 'test/loss': 2.8699193000793457, 'test/num_examples': 10000, 'score': 5932.636206865311, 'total_duration': 6345.95436835289, 'accumulated_submission_time': 5932.636206865311, 'accumulated_eval_time': 385.9395899772644, 'accumulated_logging_time': 27.088396072387695}
I0418 11:27:46.716087 139761171797760 logging_writer.py:48] [14293] accumulated_eval_time=385.939590, accumulated_logging_time=27.088396, accumulated_submission_time=5932.636207, global_step=14293, preemption_count=0, score=5932.636207, test/accuracy=0.394300, test/loss=2.869919, test/num_examples=10000, total_duration=6345.954368, train/accuracy=0.544121, train/loss=2.074860, validation/accuracy=0.508080, validation/loss=2.244430, validation/num_examples=50000
I0418 11:27:46.991032 139939736233792 checkpoints.py:356] Saving checkpoint at step: 14293
I0418 11:27:48.283015 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_14293
I0418 11:27:48.299430 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_14293.
I0418 11:27:51.546430 139761163405056 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6993311047554016, loss=5.586566925048828
I0418 11:28:32.053440 139761155012352 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.8450883626937866, loss=3.706723928451538
I0418 11:29:13.542044 139761163405056 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8200289607048035, loss=3.6088316440582275
I0418 11:29:54.595009 139761155012352 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7782822251319885, loss=3.831437587738037
I0418 11:30:36.467033 139761163405056 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7425752282142639, loss=5.147786617279053
I0418 11:31:17.980111 139761155012352 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8190667033195496, loss=3.5789222717285156
I0418 11:31:59.276116 139761163405056 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6593498587608337, loss=4.925310134887695
I0418 11:32:41.343895 139761155012352 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9252262115478516, loss=3.703545570373535
I0418 11:33:23.605615 139761155012352 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8126091361045837, loss=3.9614648818969727
I0418 11:34:05.860365 139761163405056 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8353127241134644, loss=5.616647720336914
I0418 11:34:47.250407 139761155012352 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6804972290992737, loss=4.454814910888672
I0418 11:34:48.727169 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:35:03.127566 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:35:14.535734 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:35:16.202473 139939736233792 submission_runner.py:406] Time since start: 6795.46s, 	Step: 15305, 	{'train/accuracy': 0.5626562237739563, 'train/loss': 2.003962278366089, 'validation/accuracy': 0.5169999599456787, 'validation/loss': 2.209202289581299, 'validation/num_examples': 50000, 'test/accuracy': 0.406000018119812, 'test/loss': 2.8362109661102295, 'test/num_examples': 10000, 'score': 6353.042263746262, 'total_duration': 6795.4559025764465, 'accumulated_submission_time': 6353.042263746262, 'accumulated_eval_time': 413.41479110717773, 'accumulated_logging_time': 28.689209699630737}
I0418 11:35:16.217524 139761163405056 logging_writer.py:48] [15305] accumulated_eval_time=413.414791, accumulated_logging_time=28.689210, accumulated_submission_time=6353.042264, global_step=15305, preemption_count=0, score=6353.042264, test/accuracy=0.406000, test/loss=2.836211, test/num_examples=10000, total_duration=6795.455903, train/accuracy=0.562656, train/loss=2.003962, validation/accuracy=0.517000, validation/loss=2.209202, validation/num_examples=50000
I0418 11:35:16.466696 139939736233792 checkpoints.py:356] Saving checkpoint at step: 15305
I0418 11:35:17.745238 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_15305
I0418 11:35:17.767941 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_15305.
I0418 11:35:57.364921 139761155012352 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.598120927810669, loss=4.573535919189453
I0418 11:36:41.171746 139761146619648 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7310177087783813, loss=4.691821098327637
I0418 11:37:25.511658 139761155012352 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6401336193084717, loss=4.958783149719238
I0418 11:38:09.176054 139761146619648 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.781365692615509, loss=3.724715232849121
I0418 11:38:52.127966 139761155012352 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9297654032707214, loss=3.6037111282348633
I0418 11:39:35.377459 139761146619648 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8107181191444397, loss=3.5010204315185547
I0418 11:40:18.514517 139761155012352 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6125083565711975, loss=5.425868511199951
I0418 11:41:00.662192 139761146619648 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7878431081771851, loss=3.5349786281585693
I0418 11:41:43.416747 139761155012352 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8230640292167664, loss=3.4685845375061035
I0418 11:42:17.817605 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:42:31.940530 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:42:43.640637 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:42:45.316177 139939736233792 submission_runner.py:406] Time since start: 7244.57s, 	Step: 16283, 	{'train/accuracy': 0.5830468535423279, 'train/loss': 1.8617783784866333, 'validation/accuracy': 0.5351600050926208, 'validation/loss': 2.094477653503418, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.7396042346954346, 'test/num_examples': 10000, 'score': 6773.071553230286, 'total_duration': 7244.5696222782135, 'accumulated_submission_time': 6773.071553230286, 'accumulated_eval_time': 440.913277387619, 'accumulated_logging_time': 30.25677728652954}
I0418 11:42:45.326950 139761146619648 logging_writer.py:48] [16283] accumulated_eval_time=440.913277, accumulated_logging_time=30.256777, accumulated_submission_time=6773.071553, global_step=16283, preemption_count=0, score=6773.071553, test/accuracy=0.415800, test/loss=2.739604, test/num_examples=10000, total_duration=7244.569622, train/accuracy=0.583047, train/loss=1.861778, validation/accuracy=0.535160, validation/loss=2.094478, validation/num_examples=50000
I0418 11:42:45.590255 139939736233792 checkpoints.py:356] Saving checkpoint at step: 16283
I0418 11:42:47.125902 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_16283
I0418 11:42:47.149693 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_16283.
I0418 11:42:54.383727 139761155012352 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.7639588713645935, loss=3.5207321643829346
I0418 11:43:34.830217 139761138226944 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.745731770992279, loss=4.166392803192139
I0418 11:44:17.146129 139761155012352 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7998102903366089, loss=3.863138198852539
I0418 11:44:59.261062 139761138226944 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.678361713886261, loss=5.358531475067139
I0418 11:45:40.552077 139761155012352 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.8485138416290283, loss=3.4967737197875977
I0418 11:46:22.884285 139761138226944 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7765108346939087, loss=3.509082794189453
I0418 11:47:05.044704 139761155012352 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8087990283966064, loss=3.4781711101531982
I0418 11:47:47.718167 139761138226944 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7716208100318909, loss=3.4282655715942383
I0418 11:48:30.014821 139761155012352 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6914288997650146, loss=5.466123104095459
I0418 11:49:12.642940 139761138226944 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.7619255185127258, loss=3.806586265563965
I0418 11:49:47.519830 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:50:01.570756 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:50:13.496716 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:50:15.178055 139939736233792 submission_runner.py:406] Time since start: 7694.43s, 	Step: 17285, 	{'train/accuracy': 0.6036523580551147, 'train/loss': 1.7766649723052979, 'validation/accuracy': 0.5485000014305115, 'validation/loss': 2.0465567111968994, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.661457061767578, 'test/num_examples': 10000, 'score': 7193.4205141067505, 'total_duration': 7694.431536436081, 'accumulated_submission_time': 7193.4205141067505, 'accumulated_eval_time': 468.57150506973267, 'accumulated_logging_time': 32.09254240989685}
I0418 11:50:15.193930 139761155012352 logging_writer.py:48] [17285] accumulated_eval_time=468.571505, accumulated_logging_time=32.092542, accumulated_submission_time=7193.420514, global_step=17285, preemption_count=0, score=7193.420514, test/accuracy=0.429500, test/loss=2.661457, test/num_examples=10000, total_duration=7694.431536, train/accuracy=0.603652, train/loss=1.776665, validation/accuracy=0.548500, validation/loss=2.046557, validation/num_examples=50000
I0418 11:50:15.497808 139939736233792 checkpoints.py:356] Saving checkpoint at step: 17285
I0418 11:50:16.606713 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_17285
I0418 11:50:16.625496 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_17285.
I0418 11:50:23.007886 139761138226944 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.8099720478057861, loss=4.530252933502197
I0418 11:51:03.778634 139760584619776 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7684228420257568, loss=3.4497482776641846
I0418 11:51:45.536319 139761138226944 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7624390721321106, loss=5.678830146789551
I0418 11:52:28.311161 139760584619776 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8260082006454468, loss=3.486589193344116
I0418 11:53:10.549620 139761138226944 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7448384761810303, loss=4.119171142578125
I0418 11:53:53.247973 139760584619776 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7952879071235657, loss=5.581585884094238
I0418 11:54:35.241520 139761138226944 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7617729902267456, loss=3.4126193523406982
I0418 11:55:17.043327 139760584619776 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7741613388061523, loss=3.5163533687591553
I0418 11:55:58.935474 139761138226944 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.8750457763671875, loss=3.553483724594116
I0418 11:56:40.999151 139760584619776 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7547769546508789, loss=3.82574200630188
I0418 11:57:17.007485 139939736233792 spec.py:298] Evaluating on the training split.
I0418 11:57:31.422096 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 11:57:43.132168 139939736233792 spec.py:326] Evaluating on the test split.
I0418 11:57:44.803796 139939736233792 submission_runner.py:406] Time since start: 8144.06s, 	Step: 18287, 	{'train/accuracy': 0.6024023294448853, 'train/loss': 1.788543701171875, 'validation/accuracy': 0.558459997177124, 'validation/loss': 1.9851806163787842, 'validation/num_examples': 50000, 'test/accuracy': 0.4443000257015228, 'test/loss': 2.622553586959839, 'test/num_examples': 10000, 'score': 7613.782323598862, 'total_duration': 8144.057251691818, 'accumulated_submission_time': 7613.782323598862, 'accumulated_eval_time': 496.3677477836609, 'accumulated_logging_time': 33.541542291641235}
I0418 11:57:44.820075 139761138226944 logging_writer.py:48] [18287] accumulated_eval_time=496.367748, accumulated_logging_time=33.541542, accumulated_submission_time=7613.782324, global_step=18287, preemption_count=0, score=7613.782324, test/accuracy=0.444300, test/loss=2.622554, test/num_examples=10000, total_duration=8144.057252, train/accuracy=0.602402, train/loss=1.788544, validation/accuracy=0.558460, validation/loss=1.985181, validation/num_examples=50000
I0418 11:57:45.087466 139939736233792 checkpoints.py:356] Saving checkpoint at step: 18287
I0418 11:57:46.241668 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_18287
I0418 11:57:46.260610 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_18287.
I0418 11:57:51.998409 139760584619776 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8723897933959961, loss=3.3306241035461426
I0418 11:58:33.583289 139760576227072 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8976244926452637, loss=3.4242711067199707
I0418 11:59:16.859620 139760584619776 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.811347484588623, loss=3.2278642654418945
I0418 11:59:59.579661 139760576227072 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8909255266189575, loss=5.546659469604492
I0418 12:00:42.308549 139760584619776 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7854166626930237, loss=3.279421091079712
I0418 12:01:25.981286 139760576227072 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7384276986122131, loss=3.4645793437957764
I0418 12:02:08.657964 139760584619776 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9143631458282471, loss=3.3331785202026367
I0418 12:02:52.080035 139760576227072 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8131177425384521, loss=3.5161008834838867
I0418 12:03:35.301991 139760584619776 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7301047444343567, loss=3.3571460247039795
I0418 12:04:18.607823 139760576227072 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9747143387794495, loss=5.5632123947143555
I0418 12:04:46.443119 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:05:00.603478 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:05:12.694237 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:05:14.367100 139939736233792 submission_runner.py:406] Time since start: 8593.62s, 	Step: 19266, 	{'train/accuracy': 0.6140429377555847, 'train/loss': 1.740160584449768, 'validation/accuracy': 0.5695799589157104, 'validation/loss': 1.9441649913787842, 'validation/num_examples': 50000, 'test/accuracy': 0.4483000338077545, 'test/loss': 2.5798985958099365, 'test/num_examples': 10000, 'score': 8033.945492267609, 'total_duration': 8593.620598554611, 'accumulated_submission_time': 8033.945492267609, 'accumulated_eval_time': 524.2917003631592, 'accumulated_logging_time': 34.99981355667114}
I0418 12:05:14.381536 139760584619776 logging_writer.py:48] [19266] accumulated_eval_time=524.291700, accumulated_logging_time=34.999814, accumulated_submission_time=8033.945492, global_step=19266, preemption_count=0, score=8033.945492, test/accuracy=0.448300, test/loss=2.579899, test/num_examples=10000, total_duration=8593.620599, train/accuracy=0.614043, train/loss=1.740161, validation/accuracy=0.569580, validation/loss=1.944165, validation/num_examples=50000
I0418 12:05:14.689111 139939736233792 checkpoints.py:356] Saving checkpoint at step: 19266
I0418 12:05:16.077904 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_19266
I0418 12:05:16.099790 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_19266.
I0418 12:05:30.068051 139760576227072 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8048025369644165, loss=3.35064959526062
I0418 12:06:11.505012 139760223880960 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.8660451173782349, loss=5.307845115661621
I0418 12:06:53.474766 139760576227072 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7809029817581177, loss=4.177374839782715
I0418 12:07:35.019879 139760223880960 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7812094688415527, loss=5.444720268249512
I0418 12:08:17.225073 139760576227072 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7863314151763916, loss=3.319488048553467
I0418 12:08:59.666953 139760223880960 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7882909774780273, loss=3.763638734817505
I0418 12:09:41.743708 139760576227072 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8442457318305969, loss=3.3668131828308105
I0418 12:10:23.641731 139760223880960 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8311794400215149, loss=3.6795804500579834
I0418 12:11:05.803843 139760576227072 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.7847453951835632, loss=3.383596420288086
I0418 12:11:48.248099 139760223880960 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7313625812530518, loss=3.9576478004455566
I0418 12:12:16.464126 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:12:30.594698 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:12:42.800202 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:12:44.499384 139939736233792 submission_runner.py:406] Time since start: 9043.75s, 	Step: 20269, 	{'train/accuracy': 0.6289452910423279, 'train/loss': 1.6494953632354736, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.8796662092208862, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.516413688659668, 'test/num_examples': 10000, 'score': 8454.288666248322, 'total_duration': 9043.752842664719, 'accumulated_submission_time': 8454.288666248322, 'accumulated_eval_time': 552.3268949985504, 'accumulated_logging_time': 36.73433494567871}
I0418 12:12:44.512321 139760576227072 logging_writer.py:48] [20269] accumulated_eval_time=552.326895, accumulated_logging_time=36.734335, accumulated_submission_time=8454.288666, global_step=20269, preemption_count=0, score=8454.288666, test/accuracy=0.461700, test/loss=2.516414, test/num_examples=10000, total_duration=9043.752843, train/accuracy=0.628945, train/loss=1.649495, validation/accuracy=0.580680, validation/loss=1.879666, validation/num_examples=50000
I0418 12:12:44.786275 139939736233792 checkpoints.py:356] Saving checkpoint at step: 20269
I0418 12:12:46.060640 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_20269
I0418 12:12:46.082314 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_20269.
I0418 12:12:58.858267 139760223880960 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7573755383491516, loss=3.1982102394104004
I0418 12:13:40.071523 139760215488256 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.789730966091156, loss=3.5078861713409424
I0418 12:14:22.251716 139760223880960 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8087102770805359, loss=3.3871960639953613
I0418 12:15:05.243309 139760215488256 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7700287699699402, loss=4.4190239906311035
I0418 12:15:47.311252 139760223880960 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7400434017181396, loss=5.4519524574279785
I0418 12:16:29.432890 139760215488256 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7784456610679626, loss=4.237170219421387
I0418 12:17:11.223955 139760223880960 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7719853520393372, loss=5.046926975250244
I0418 12:17:53.691415 139760215488256 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8186051845550537, loss=3.2287421226501465
I0418 12:18:36.310314 139760223880960 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8133592009544373, loss=3.589731216430664
I0418 12:19:18.970520 139760215488256 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.828995406627655, loss=3.7043044567108154
I0418 12:19:46.364976 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:20:01.375288 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:20:13.429273 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:20:15.096187 139939736233792 submission_runner.py:406] Time since start: 9494.35s, 	Step: 21266, 	{'train/accuracy': 0.6465234160423279, 'train/loss': 1.5763294696807861, 'validation/accuracy': 0.5854200124740601, 'validation/loss': 1.866695761680603, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.5001611709594727, 'test/num_examples': 10000, 'score': 8874.550342559814, 'total_duration': 9494.349689006805, 'accumulated_submission_time': 8874.550342559814, 'accumulated_eval_time': 581.058084487915, 'accumulated_logging_time': 38.319214820861816}
I0418 12:20:15.107459 139760223880960 logging_writer.py:48] [21266] accumulated_eval_time=581.058084, accumulated_logging_time=38.319215, accumulated_submission_time=8874.550343, global_step=21266, preemption_count=0, score=8874.550343, test/accuracy=0.462900, test/loss=2.500161, test/num_examples=10000, total_duration=9494.349689, train/accuracy=0.646523, train/loss=1.576329, validation/accuracy=0.585420, validation/loss=1.866696, validation/num_examples=50000
I0418 12:20:15.428213 139939736233792 checkpoints.py:356] Saving checkpoint at step: 21266
I0418 12:20:16.865817 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_21266
I0418 12:20:16.892592 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_21266.
I0418 12:20:30.808810 139760215488256 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8710311055183411, loss=3.6530964374542236
I0418 12:21:11.134711 139760207095552 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8143882155418396, loss=3.6136248111724854
I0418 12:21:52.784078 139760215488256 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8460517525672913, loss=3.3301165103912354
I0418 12:22:34.961037 139760207095552 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8428391814231873, loss=4.049044609069824
I0418 12:23:16.754383 139760215488256 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8336238265037537, loss=3.581268310546875
I0418 12:23:59.273029 139760207095552 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.942585289478302, loss=3.242361545562744
I0418 12:24:40.817873 139760215488256 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.846311628818512, loss=3.2894885540008545
I0418 12:25:22.616201 139760207095552 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7910085916519165, loss=5.005746841430664
I0418 12:26:04.022479 139760215488256 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.921306848526001, loss=3.2309179306030273
I0418 12:26:46.023211 139760207095552 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.833881139755249, loss=4.2110819816589355
I0418 12:27:17.076871 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:27:31.779256 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:27:43.896826 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:27:45.577619 139939736233792 submission_runner.py:406] Time since start: 9944.83s, 	Step: 22277, 	{'train/accuracy': 0.6388476490974426, 'train/loss': 1.597163200378418, 'validation/accuracy': 0.5904399752616882, 'validation/loss': 1.8202325105667114, 'validation/num_examples': 50000, 'test/accuracy': 0.4734000265598297, 'test/loss': 2.453092336654663, 'test/num_examples': 10000, 'score': 9294.713446855545, 'total_duration': 9944.831125497818, 'accumulated_submission_time': 9294.713446855545, 'accumulated_eval_time': 609.5588173866272, 'accumulated_logging_time': 40.11748290061951}
I0418 12:27:45.592895 139760215488256 logging_writer.py:48] [22277] accumulated_eval_time=609.558817, accumulated_logging_time=40.117483, accumulated_submission_time=9294.713447, global_step=22277, preemption_count=0, score=9294.713447, test/accuracy=0.473400, test/loss=2.453092, test/num_examples=10000, total_duration=9944.831125, train/accuracy=0.638848, train/loss=1.597163, validation/accuracy=0.590440, validation/loss=1.820233, validation/num_examples=50000
I0418 12:27:45.963641 139939736233792 checkpoints.py:356] Saving checkpoint at step: 22277
I0418 12:27:47.307025 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_22277
I0418 12:27:47.328521 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_22277.
I0418 12:27:56.942049 139760207095552 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8319227695465088, loss=3.481731414794922
I0418 12:28:37.818390 139760198702848 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7524304986000061, loss=3.8330812454223633
I0418 12:29:19.467145 139760207095552 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7509788870811462, loss=3.706389904022217
I0418 12:30:01.047411 139760198702848 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.8807157874107361, loss=3.2482290267944336
I0418 12:30:43.619468 139760207095552 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9042173027992249, loss=3.242377996444702
I0418 12:31:25.552732 139760198702848 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8874035477638245, loss=3.6653664112091064
I0418 12:32:08.024003 139760207095552 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8570641875267029, loss=3.2393248081207275
I0418 12:32:50.238444 139760198702848 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8256906867027283, loss=3.257904052734375
I0418 12:33:32.919420 139760207095552 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.9669715166091919, loss=3.139263391494751
I0418 12:34:15.439253 139760198702848 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8286013603210449, loss=4.679633617401123
I0418 12:34:47.699261 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:35:02.185148 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:35:13.795304 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:35:15.489572 139939736233792 submission_runner.py:406] Time since start: 10394.74s, 	Step: 23278, 	{'train/accuracy': 0.6475781202316284, 'train/loss': 1.611580729484558, 'validation/accuracy': 0.5999599695205688, 'validation/loss': 1.8268135786056519, 'validation/num_examples': 50000, 'test/accuracy': 0.483100026845932, 'test/loss': 2.456960678100586, 'test/num_examples': 10000, 'score': 9715.06336259842, 'total_duration': 10394.742853879929, 'accumulated_submission_time': 9715.06336259842, 'accumulated_eval_time': 637.3488862514496, 'accumulated_logging_time': 41.87013506889343}
I0418 12:35:15.510593 139760207095552 logging_writer.py:48] [23278] accumulated_eval_time=637.348886, accumulated_logging_time=41.870135, accumulated_submission_time=9715.063363, global_step=23278, preemption_count=0, score=9715.063363, test/accuracy=0.483100, test/loss=2.456961, test/num_examples=10000, total_duration=10394.742854, train/accuracy=0.647578, train/loss=1.611581, validation/accuracy=0.599960, validation/loss=1.826814, validation/num_examples=50000
I0418 12:35:16.323024 139939736233792 checkpoints.py:356] Saving checkpoint at step: 23278
I0418 12:35:17.705317 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_23278
I0418 12:35:17.732978 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_23278.
I0418 12:35:27.022052 139760198702848 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8958193063735962, loss=3.3392300605773926
I0418 12:36:08.766683 139760123234048 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8648491501808167, loss=4.804543495178223
I0418 12:36:52.016199 139760198702848 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8355498909950256, loss=3.5271501541137695
I0418 12:37:34.642225 139760123234048 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9165964722633362, loss=3.1963181495666504
I0418 12:38:17.267086 139760198702848 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9176251292228699, loss=4.736945629119873
I0418 12:39:00.043249 139760123234048 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.9177352786064148, loss=5.071116924285889
I0418 12:39:43.073423 139760198702848 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8431814312934875, loss=5.208054542541504
I0418 12:40:25.997940 139760123234048 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8853915333747864, loss=3.8180410861968994
I0418 12:41:08.778392 139760198702848 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.8726516962051392, loss=3.2205452919006348
I0418 12:41:51.056853 139760123234048 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8966596126556396, loss=3.044220447540283
I0418 12:42:17.843118 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:42:28.770063 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:42:40.823816 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:42:42.519111 139939736233792 submission_runner.py:406] Time since start: 10841.77s, 	Step: 24263, 	{'train/accuracy': 0.6537694931030273, 'train/loss': 1.5519863367080688, 'validation/accuracy': 0.601419985294342, 'validation/loss': 1.7925915718078613, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.4177072048187256, 'test/num_examples': 10000, 'score': 10135.152450561523, 'total_duration': 10841.772545814514, 'accumulated_submission_time': 10135.152450561523, 'accumulated_eval_time': 662.0248160362244, 'accumulated_logging_time': 44.11581039428711}
I0418 12:42:42.535928 139760198702848 logging_writer.py:48] [24263] accumulated_eval_time=662.024816, accumulated_logging_time=44.115810, accumulated_submission_time=10135.152451, global_step=24263, preemption_count=0, score=10135.152451, test/accuracy=0.486800, test/loss=2.417707, test/num_examples=10000, total_duration=10841.772546, train/accuracy=0.653769, train/loss=1.551986, validation/accuracy=0.601420, validation/loss=1.792592, validation/num_examples=50000
I0418 12:42:42.945159 139939736233792 checkpoints.py:356] Saving checkpoint at step: 24263
I0418 12:42:44.041891 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_24263
I0418 12:42:44.062262 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_24263.
I0418 12:42:59.164338 139760123234048 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7302207350730896, loss=4.023916721343994
I0418 12:43:40.539103 139760114841344 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8503257632255554, loss=3.53873610496521
I0418 12:44:22.699238 139760123234048 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8885725736618042, loss=3.486342191696167
I0418 12:45:04.900964 139760114841344 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.8404449224472046, loss=3.7487404346466064
I0418 12:45:47.192216 139760123234048 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0291352272033691, loss=5.3590474128723145
I0418 12:46:29.200975 139760114841344 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7624767422676086, loss=4.4830732345581055
I0418 12:47:11.125507 139760123234048 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8927966356277466, loss=4.345181465148926
I0418 12:47:53.613035 139760114841344 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8037749528884888, loss=4.065502643585205
I0418 12:48:35.794223 139760123234048 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.864126980304718, loss=3.1134352684020996
I0418 12:49:18.185482 139760114841344 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.7622838020324707, loss=3.6673240661621094
I0418 12:49:44.074922 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:49:54.151608 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:50:06.803497 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:50:08.489164 139939736233792 submission_runner.py:406] Time since start: 11287.74s, 	Step: 25262, 	{'train/accuracy': 0.6682031154632568, 'train/loss': 1.4845765829086304, 'validation/accuracy': 0.6108399629592896, 'validation/loss': 1.7569043636322021, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.395411968231201, 'test/num_examples': 10000, 'score': 10555.144511938095, 'total_duration': 11287.74263548851, 'accumulated_submission_time': 10555.144511938095, 'accumulated_eval_time': 686.4390137195587, 'accumulated_logging_time': 45.66046953201294}
I0418 12:50:08.508011 139760123234048 logging_writer.py:48] [25262] accumulated_eval_time=686.439014, accumulated_logging_time=45.660470, accumulated_submission_time=10555.144512, global_step=25262, preemption_count=0, score=10555.144512, test/accuracy=0.488600, test/loss=2.395412, test/num_examples=10000, total_duration=11287.742635, train/accuracy=0.668203, train/loss=1.484577, validation/accuracy=0.610840, validation/loss=1.756904, validation/num_examples=50000
I0418 12:50:08.856881 139939736233792 checkpoints.py:356] Saving checkpoint at step: 25262
I0418 12:50:10.320838 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_25262
I0418 12:50:10.346671 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_25262.
I0418 12:50:25.858435 139760114841344 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9151820540428162, loss=5.24698543548584
I0418 12:51:07.047841 139760106448640 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8017695546150208, loss=3.505692958831787
I0418 12:51:48.877388 139760114841344 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8911375403404236, loss=3.1382997035980225
I0418 12:52:30.231396 139760106448640 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8418539762496948, loss=3.390775203704834
I0418 12:53:11.908164 139760114841344 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8489135503768921, loss=3.6695940494537354
I0418 12:53:54.163254 139760106448640 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9110076427459717, loss=3.1092450618743896
I0418 12:54:36.036083 139760114841344 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.9407515525817871, loss=4.8397216796875
I0418 12:55:18.001208 139760106448640 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8433735966682434, loss=4.655699253082275
I0418 12:55:59.854628 139760114841344 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9266448616981506, loss=3.1755669116973877
I0418 12:56:41.670467 139760106448640 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8401196599006653, loss=3.060030221939087
I0418 12:57:10.483435 139939736233792 spec.py:298] Evaluating on the training split.
I0418 12:57:20.467013 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 12:57:32.920687 139939736233792 spec.py:326] Evaluating on the test split.
I0418 12:57:34.577422 139939736233792 submission_runner.py:406] Time since start: 11733.83s, 	Step: 26269, 	{'train/accuracy': 0.666210949420929, 'train/loss': 1.4780532121658325, 'validation/accuracy': 0.6159799695014954, 'validation/loss': 1.7089484930038452, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.3577775955200195, 'test/num_examples': 10000, 'score': 10975.259412527084, 'total_duration': 11733.830866098404, 'accumulated_submission_time': 10975.259412527084, 'accumulated_eval_time': 710.5329375267029, 'accumulated_logging_time': 47.519981145858765}
I0418 12:57:34.594855 139760114841344 logging_writer.py:48] [26269] accumulated_eval_time=710.532938, accumulated_logging_time=47.519981, accumulated_submission_time=10975.259413, global_step=26269, preemption_count=0, score=10975.259413, test/accuracy=0.491000, test/loss=2.357778, test/num_examples=10000, total_duration=11733.830866, train/accuracy=0.666211, train/loss=1.478053, validation/accuracy=0.615980, validation/loss=1.708948, validation/num_examples=50000
I0418 12:57:34.949975 139939736233792 checkpoints.py:356] Saving checkpoint at step: 26269
I0418 12:57:36.118317 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_26269
I0418 12:57:36.140395 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_26269.
I0418 12:57:48.893738 139760106448640 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8143228888511658, loss=3.3611183166503906
I0418 12:58:30.047814 139760098055936 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8685708045959473, loss=3.1690309047698975
I0418 12:59:11.742912 139760106448640 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.8866472840309143, loss=3.093371868133545
I0418 12:59:53.120161 139760098055936 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8224397301673889, loss=3.109247922897339
I0418 13:00:34.934561 139760106448640 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8671470284461975, loss=3.880896806716919
I0418 13:01:16.889553 139760098055936 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.814613401889801, loss=3.366870164871216
I0418 13:01:58.031049 139760106448640 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.8708595037460327, loss=3.020249366760254
I0418 13:02:40.449917 139760098055936 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8467466235160828, loss=3.1337411403656006
I0418 13:03:22.228468 139760106448640 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1454333066940308, loss=5.154891490936279
I0418 13:04:04.332272 139760098055936 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8810250759124756, loss=3.2525432109832764
I0418 13:04:36.412421 139939736233792 spec.py:298] Evaluating on the training split.
I0418 13:04:46.222743 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 13:04:59.097700 139939736233792 spec.py:326] Evaluating on the test split.
I0418 13:05:00.757481 139939736233792 submission_runner.py:406] Time since start: 12180.01s, 	Step: 27280, 	{'train/accuracy': 0.6732812523841858, 'train/loss': 1.4883195161819458, 'validation/accuracy': 0.6201599836349487, 'validation/loss': 1.719746708869934, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.358444929122925, 'test/num_examples': 10000, 'score': 11395.510884284973, 'total_duration': 12180.010907173157, 'accumulated_submission_time': 11395.510884284973, 'accumulated_eval_time': 734.8779029846191, 'accumulated_logging_time': 49.084484577178955}
I0418 13:05:00.775812 139760106448640 logging_writer.py:48] [27280] accumulated_eval_time=734.877903, accumulated_logging_time=49.084485, accumulated_submission_time=11395.510884, global_step=27280, preemption_count=0, score=11395.510884, test/accuracy=0.495100, test/loss=2.358445, test/num_examples=10000, total_duration=12180.010907, train/accuracy=0.673281, train/loss=1.488320, validation/accuracy=0.620160, validation/loss=1.719747, validation/num_examples=50000
I0418 13:05:01.073669 139939736233792 checkpoints.py:356] Saving checkpoint at step: 27280
I0418 13:05:02.419239 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_27280
I0418 13:05:02.442031 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_27280.
I0418 13:05:10.851389 139760098055936 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8009335994720459, loss=4.241722583770752
I0418 13:05:51.562091 139760089663232 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9263121485710144, loss=5.06575870513916
I0418 13:06:33.633071 139760098055936 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.9179725050926208, loss=3.1040260791778564
I0418 13:07:16.257000 139760089663232 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8348729014396667, loss=4.210877418518066
I0418 13:07:58.557794 139760098055936 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8309767842292786, loss=3.0460364818573
I0418 13:08:40.157838 139760089663232 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8742193579673767, loss=3.142801523208618
I0418 13:09:22.742555 139760098055936 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8782138824462891, loss=3.1201112270355225
I0418 13:10:04.332513 139939736233792 spec.py:298] Evaluating on the training split.
I0418 13:10:14.081124 139939736233792 spec.py:310] Evaluating on the validation split.
I0418 13:10:26.604549 139939736233792 spec.py:326] Evaluating on the test split.
I0418 13:10:28.291952 139939736233792 submission_runner.py:406] Time since start: 12507.55s, 	Step: 28000, 	{'train/accuracy': 0.6813085675239563, 'train/loss': 1.4235388040542603, 'validation/accuracy': 0.6242200136184692, 'validation/loss': 1.6835814714431763, 'validation/num_examples': 50000, 'test/accuracy': 0.5039000511169434, 'test/loss': 2.322793483734131, 'test/num_examples': 10000, 'score': 11697.385766983032, 'total_duration': 12507.545429468155, 'accumulated_submission_time': 11697.385766983032, 'accumulated_eval_time': 758.8373136520386, 'accumulated_logging_time': 50.77108645439148}
I0418 13:10:28.310201 139760089663232 logging_writer.py:48] [28000] accumulated_eval_time=758.837314, accumulated_logging_time=50.771086, accumulated_submission_time=11697.385767, global_step=28000, preemption_count=0, score=11697.385767, test/accuracy=0.503900, test/loss=2.322793, test/num_examples=10000, total_duration=12507.545429, train/accuracy=0.681309, train/loss=1.423539, validation/accuracy=0.624220, validation/loss=1.683581, validation/num_examples=50000
I0418 13:10:28.605674 139939736233792 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 13:10:29.766400 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_28000
I0418 13:10:29.787555 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_28000.
I0418 13:10:29.799283 139760098055936 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11697.385767
I0418 13:10:29.921208 139939736233792 checkpoints.py:356] Saving checkpoint at step: 28000
I0418 13:10:31.152016 139939736233792 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_28000
I0418 13:10:31.168712 139939736233792 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_28000.
I0418 13:10:32.055994 139939736233792 submission_runner.py:567] Tuning trial 1/1
I0418 13:10:32.056965 139939736233792 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0418 13:10:32.061410 139939736233792 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 50.1699116230011, 'total_duration': 98.85930919647217, 'accumulated_submission_time': 50.1699116230011, 'accumulated_eval_time': 48.689247846603394, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (992, {'train/accuracy': 0.03228515386581421, 'train/loss': 5.959888935089111, 'validation/accuracy': 0.031099999323487282, 'validation/loss': 5.980340957641602, 'validation/num_examples': 50000, 'test/accuracy': 0.024000000208616257, 'test/loss': 6.07896614074707, 'test/num_examples': 10000, 'score': 470.41340684890747, 'total_duration': 539.2752826213837, 'accumulated_submission_time': 470.41340684890747, 'accumulated_eval_time': 68.10858678817749, 'accumulated_logging_time': 0.7311644554138184, 'global_step': 992, 'preemption_count': 0}), (2024, {'train/accuracy': 0.07947265356779099, 'train/loss': 5.2394328117370605, 'validation/accuracy': 0.07485999912023544, 'validation/loss': 5.27151346206665, 'validation/num_examples': 50000, 'test/accuracy': 0.05640000104904175, 'test/loss': 5.4720001220703125, 'test/num_examples': 10000, 'score': 890.5732078552246, 'total_duration': 981.5929033756256, 'accumulated_submission_time': 890.5732078552246, 'accumulated_eval_time': 87.24804615974426, 'accumulated_logging_time': 3.728252649307251, 'global_step': 2024, 'preemption_count': 0}), (3055, {'train/accuracy': 0.14162109792232513, 'train/loss': 4.651334285736084, 'validation/accuracy': 0.13210000097751617, 'validation/loss': 4.712193012237549, 'validation/num_examples': 50000, 'test/accuracy': 0.10280000418424606, 'test/loss': 5.005489349365234, 'test/num_examples': 10000, 'score': 1310.6416590213776, 'total_duration': 1424.8425040245056, 'accumulated_submission_time': 1310.6416590213776, 'accumulated_eval_time': 107.03730773925781, 'accumulated_logging_time': 7.098662853240967, 'global_step': 3055, 'preemption_count': 0}), (4087, {'train/accuracy': 0.20749999582767487, 'train/loss': 4.1564435958862305, 'validation/accuracy': 0.1842999905347824, 'validation/loss': 4.2814459800720215, 'validation/num_examples': 50000, 'test/accuracy': 0.14420001208782196, 'test/loss': 4.637369155883789, 'test/num_examples': 10000, 'score': 1730.875303030014, 'total_duration': 1868.7446064949036, 'accumulated_submission_time': 1730.875303030014, 'accumulated_eval_time': 127.3246021270752, 'accumulated_logging_time': 10.458579540252686, 'global_step': 4087, 'preemption_count': 0}), (5116, {'train/accuracy': 0.25458982586860657, 'train/loss': 3.759672164916992, 'validation/accuracy': 0.23841999471187592, 'validation/loss': 3.8421905040740967, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.268174648284912, 'test/num_examples': 10000, 'score': 2150.878241300583, 'total_duration': 2312.7876489162445, 'accumulated_submission_time': 2150.878241300583, 'accumulated_eval_time': 148.74596571922302, 'accumulated_logging_time': 13.05631709098816, 'global_step': 5116, 'preemption_count': 0}), (6143, {'train/accuracy': 0.3039843738079071, 'train/loss': 3.473700761795044, 'validation/accuracy': 0.28139999508857727, 'validation/loss': 3.598806142807007, 'validation/num_examples': 50000, 'test/accuracy': 0.21300001442432404, 'test/loss': 4.075122833251953, 'test/num_examples': 10000, 'score': 2570.9080505371094, 'total_duration': 2757.805949449539, 'accumulated_submission_time': 2570.9080505371094, 'accumulated_eval_time': 171.90778875350952, 'accumulated_logging_time': 14.861746549606323, 'global_step': 6143, 'preemption_count': 0}), (7165, {'train/accuracy': 0.35169920325279236, 'train/loss': 3.118074893951416, 'validation/accuracy': 0.3258799910545349, 'validation/loss': 3.255978584289551, 'validation/num_examples': 50000, 'test/accuracy': 0.2515999972820282, 'test/loss': 3.7957961559295654, 'test/num_examples': 10000, 'score': 2991.131687402725, 'total_duration': 3204.274514436722, 'accumulated_submission_time': 2991.131687402725, 'accumulated_eval_time': 196.8956606388092, 'accumulated_logging_time': 16.09781575202942, 'global_step': 7165, 'preemption_count': 0}), (8188, {'train/accuracy': 0.394843727350235, 'train/loss': 2.910557270050049, 'validation/accuracy': 0.35933998227119446, 'validation/loss': 3.087632179260254, 'validation/num_examples': 50000, 'test/accuracy': 0.27390000224113464, 'test/loss': 3.6293017864227295, 'test/num_examples': 10000, 'score': 3411.466136932373, 'total_duration': 3651.7226235866547, 'accumulated_submission_time': 3411.466136932373, 'accumulated_eval_time': 222.7809739112854, 'accumulated_logging_time': 17.305179119110107, 'global_step': 8188, 'preemption_count': 0}), (9211, {'train/accuracy': 0.4273437261581421, 'train/loss': 2.6984026432037354, 'validation/accuracy': 0.3906799852848053, 'validation/loss': 2.868025779724121, 'validation/num_examples': 50000, 'test/accuracy': 0.3052000105381012, 'test/loss': 3.430504083633423, 'test/num_examples': 10000, 'score': 3831.785843372345, 'total_duration': 4100.217361688614, 'accumulated_submission_time': 3831.785843372345, 'accumulated_eval_time': 249.52425932884216, 'accumulated_logging_time': 18.71648383140564, 'global_step': 9211, 'preemption_count': 0}), (10232, {'train/accuracy': 0.44794920086860657, 'train/loss': 2.5984833240509033, 'validation/accuracy': 0.4193199872970581, 'validation/loss': 2.744295597076416, 'validation/num_examples': 50000, 'test/accuracy': 0.3257000148296356, 'test/loss': 3.327915668487549, 'test/num_examples': 10000, 'score': 4251.828115701675, 'total_duration': 4549.131719589233, 'accumulated_submission_time': 4251.828115701675, 'accumulated_eval_time': 276.9452495574951, 'accumulated_logging_time': 20.14720606803894, 'global_step': 10232, 'preemption_count': 0}), (11252, {'train/accuracy': 0.48451170325279236, 'train/loss': 2.374006986618042, 'validation/accuracy': 0.4479599893093109, 'validation/loss': 2.544445514678955, 'validation/num_examples': 50000, 'test/accuracy': 0.3549000024795532, 'test/loss': 3.1430864334106445, 'test/num_examples': 10000, 'score': 4671.847253084183, 'total_duration': 4998.045312643051, 'accumulated_submission_time': 4671.847253084183, 'accumulated_eval_time': 304.3417296409607, 'accumulated_logging_time': 21.624865293502808, 'global_step': 11252, 'preemption_count': 0}), (12262, {'train/accuracy': 0.5203320384025574, 'train/loss': 2.2236344814300537, 'validation/accuracy': 0.472460001707077, 'validation/loss': 2.440772294998169, 'validation/num_examples': 50000, 'test/accuracy': 0.3751000165939331, 'test/loss': 3.0486156940460205, 'test/num_examples': 10000, 'score': 5092.225755214691, 'total_duration': 5446.6587908267975, 'accumulated_submission_time': 5092.225755214691, 'accumulated_eval_time': 331.29045939445496, 'accumulated_logging_time': 22.891602277755737, 'global_step': 12262, 'preemption_count': 0}), (13275, {'train/accuracy': 0.5576171875, 'train/loss': 1.9679107666015625, 'validation/accuracy': 0.4920399785041809, 'validation/loss': 2.2809665203094482, 'validation/num_examples': 50000, 'test/accuracy': 0.3841000199317932, 'test/loss': 2.9072213172912598, 'test/num_examples': 10000, 'score': 5512.3914222717285, 'total_duration': 5895.314643859863, 'accumulated_submission_time': 5512.3914222717285, 'accumulated_eval_time': 358.2165570259094, 'accumulated_logging_time': 24.436192274093628, 'global_step': 13275, 'preemption_count': 0}), (14293, {'train/accuracy': 0.5441210865974426, 'train/loss': 2.074860095977783, 'validation/accuracy': 0.508080005645752, 'validation/loss': 2.2444303035736084, 'validation/num_examples': 50000, 'test/accuracy': 0.39430001378059387, 'test/loss': 2.8699193000793457, 'test/num_examples': 10000, 'score': 5932.636206865311, 'total_duration': 6345.95436835289, 'accumulated_submission_time': 5932.636206865311, 'accumulated_eval_time': 385.9395899772644, 'accumulated_logging_time': 27.088396072387695, 'global_step': 14293, 'preemption_count': 0}), (15305, {'train/accuracy': 0.5626562237739563, 'train/loss': 2.003962278366089, 'validation/accuracy': 0.5169999599456787, 'validation/loss': 2.209202289581299, 'validation/num_examples': 50000, 'test/accuracy': 0.406000018119812, 'test/loss': 2.8362109661102295, 'test/num_examples': 10000, 'score': 6353.042263746262, 'total_duration': 6795.4559025764465, 'accumulated_submission_time': 6353.042263746262, 'accumulated_eval_time': 413.41479110717773, 'accumulated_logging_time': 28.689209699630737, 'global_step': 15305, 'preemption_count': 0}), (16283, {'train/accuracy': 0.5830468535423279, 'train/loss': 1.8617783784866333, 'validation/accuracy': 0.5351600050926208, 'validation/loss': 2.094477653503418, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.7396042346954346, 'test/num_examples': 10000, 'score': 6773.071553230286, 'total_duration': 7244.5696222782135, 'accumulated_submission_time': 6773.071553230286, 'accumulated_eval_time': 440.913277387619, 'accumulated_logging_time': 30.25677728652954, 'global_step': 16283, 'preemption_count': 0}), (17285, {'train/accuracy': 0.6036523580551147, 'train/loss': 1.7766649723052979, 'validation/accuracy': 0.5485000014305115, 'validation/loss': 2.0465567111968994, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.661457061767578, 'test/num_examples': 10000, 'score': 7193.4205141067505, 'total_duration': 7694.431536436081, 'accumulated_submission_time': 7193.4205141067505, 'accumulated_eval_time': 468.57150506973267, 'accumulated_logging_time': 32.09254240989685, 'global_step': 17285, 'preemption_count': 0}), (18287, {'train/accuracy': 0.6024023294448853, 'train/loss': 1.788543701171875, 'validation/accuracy': 0.558459997177124, 'validation/loss': 1.9851806163787842, 'validation/num_examples': 50000, 'test/accuracy': 0.4443000257015228, 'test/loss': 2.622553586959839, 'test/num_examples': 10000, 'score': 7613.782323598862, 'total_duration': 8144.057251691818, 'accumulated_submission_time': 7613.782323598862, 'accumulated_eval_time': 496.3677477836609, 'accumulated_logging_time': 33.541542291641235, 'global_step': 18287, 'preemption_count': 0}), (19266, {'train/accuracy': 0.6140429377555847, 'train/loss': 1.740160584449768, 'validation/accuracy': 0.5695799589157104, 'validation/loss': 1.9441649913787842, 'validation/num_examples': 50000, 'test/accuracy': 0.4483000338077545, 'test/loss': 2.5798985958099365, 'test/num_examples': 10000, 'score': 8033.945492267609, 'total_duration': 8593.620598554611, 'accumulated_submission_time': 8033.945492267609, 'accumulated_eval_time': 524.2917003631592, 'accumulated_logging_time': 34.99981355667114, 'global_step': 19266, 'preemption_count': 0}), (20269, {'train/accuracy': 0.6289452910423279, 'train/loss': 1.6494953632354736, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.8796662092208862, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.516413688659668, 'test/num_examples': 10000, 'score': 8454.288666248322, 'total_duration': 9043.752842664719, 'accumulated_submission_time': 8454.288666248322, 'accumulated_eval_time': 552.3268949985504, 'accumulated_logging_time': 36.73433494567871, 'global_step': 20269, 'preemption_count': 0}), (21266, {'train/accuracy': 0.6465234160423279, 'train/loss': 1.5763294696807861, 'validation/accuracy': 0.5854200124740601, 'validation/loss': 1.866695761680603, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.5001611709594727, 'test/num_examples': 10000, 'score': 8874.550342559814, 'total_duration': 9494.349689006805, 'accumulated_submission_time': 8874.550342559814, 'accumulated_eval_time': 581.058084487915, 'accumulated_logging_time': 38.319214820861816, 'global_step': 21266, 'preemption_count': 0}), (22277, {'train/accuracy': 0.6388476490974426, 'train/loss': 1.597163200378418, 'validation/accuracy': 0.5904399752616882, 'validation/loss': 1.8202325105667114, 'validation/num_examples': 50000, 'test/accuracy': 0.4734000265598297, 'test/loss': 2.453092336654663, 'test/num_examples': 10000, 'score': 9294.713446855545, 'total_duration': 9944.831125497818, 'accumulated_submission_time': 9294.713446855545, 'accumulated_eval_time': 609.5588173866272, 'accumulated_logging_time': 40.11748290061951, 'global_step': 22277, 'preemption_count': 0}), (23278, {'train/accuracy': 0.6475781202316284, 'train/loss': 1.611580729484558, 'validation/accuracy': 0.5999599695205688, 'validation/loss': 1.8268135786056519, 'validation/num_examples': 50000, 'test/accuracy': 0.483100026845932, 'test/loss': 2.456960678100586, 'test/num_examples': 10000, 'score': 9715.06336259842, 'total_duration': 10394.742853879929, 'accumulated_submission_time': 9715.06336259842, 'accumulated_eval_time': 637.3488862514496, 'accumulated_logging_time': 41.87013506889343, 'global_step': 23278, 'preemption_count': 0}), (24263, {'train/accuracy': 0.6537694931030273, 'train/loss': 1.5519863367080688, 'validation/accuracy': 0.601419985294342, 'validation/loss': 1.7925915718078613, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.4177072048187256, 'test/num_examples': 10000, 'score': 10135.152450561523, 'total_duration': 10841.772545814514, 'accumulated_submission_time': 10135.152450561523, 'accumulated_eval_time': 662.0248160362244, 'accumulated_logging_time': 44.11581039428711, 'global_step': 24263, 'preemption_count': 0}), (25262, {'train/accuracy': 0.6682031154632568, 'train/loss': 1.4845765829086304, 'validation/accuracy': 0.6108399629592896, 'validation/loss': 1.7569043636322021, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.395411968231201, 'test/num_examples': 10000, 'score': 10555.144511938095, 'total_duration': 11287.74263548851, 'accumulated_submission_time': 10555.144511938095, 'accumulated_eval_time': 686.4390137195587, 'accumulated_logging_time': 45.66046953201294, 'global_step': 25262, 'preemption_count': 0}), (26269, {'train/accuracy': 0.666210949420929, 'train/loss': 1.4780532121658325, 'validation/accuracy': 0.6159799695014954, 'validation/loss': 1.7089484930038452, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.3577775955200195, 'test/num_examples': 10000, 'score': 10975.259412527084, 'total_duration': 11733.830866098404, 'accumulated_submission_time': 10975.259412527084, 'accumulated_eval_time': 710.5329375267029, 'accumulated_logging_time': 47.519981145858765, 'global_step': 26269, 'preemption_count': 0}), (27280, {'train/accuracy': 0.6732812523841858, 'train/loss': 1.4883195161819458, 'validation/accuracy': 0.6201599836349487, 'validation/loss': 1.719746708869934, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.358444929122925, 'test/num_examples': 10000, 'score': 11395.510884284973, 'total_duration': 12180.010907173157, 'accumulated_submission_time': 11395.510884284973, 'accumulated_eval_time': 734.8779029846191, 'accumulated_logging_time': 49.084484577178955, 'global_step': 27280, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6813085675239563, 'train/loss': 1.4235388040542603, 'validation/accuracy': 0.6242200136184692, 'validation/loss': 1.6835814714431763, 'validation/num_examples': 50000, 'test/accuracy': 0.5039000511169434, 'test/loss': 2.322793483734131, 'test/num_examples': 10000, 'score': 11697.385766983032, 'total_duration': 12507.545429468155, 'accumulated_submission_time': 11697.385766983032, 'accumulated_eval_time': 758.8373136520386, 'accumulated_logging_time': 50.77108645439148, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0418 13:10:32.061524 139939736233792 submission_runner.py:570] Timing: 11697.385766983032
I0418 13:10:32.061567 139939736233792 submission_runner.py:571] ====================
I0418 13:10:32.061694 139939736233792 submission_runner.py:631] Final imagenet_vit score: 11697.385766983032
