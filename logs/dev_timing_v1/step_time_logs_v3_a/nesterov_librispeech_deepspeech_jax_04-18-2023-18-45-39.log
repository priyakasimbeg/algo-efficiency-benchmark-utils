I0418 18:46:00.651557 140114918360896 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax.
I0418 18:46:00.715160 140114918360896 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0418 18:46:01.623111 140114918360896 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0418 18:46:01.623712 140114918360896 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0418 18:46:01.627650 140114918360896 submission_runner.py:528] Using RNG seed 1967887891
I0418 18:46:04.246246 140114918360896 submission_runner.py:537] --- Tuning run 1/1 ---
I0418 18:46:04.246423 140114918360896 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1.
I0418 18:46:04.246579 140114918360896 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/hparams.json.
I0418 18:46:04.369477 140114918360896 submission_runner.py:232] Initializing dataset.
I0418 18:46:04.369659 140114918360896 submission_runner.py:239] Initializing model.
I0418 18:46:20.802397 140114918360896 submission_runner.py:249] Initializing optimizer.
I0418 18:46:21.353029 140114918360896 submission_runner.py:256] Initializing metrics bundle.
I0418 18:46:21.353244 140114918360896 submission_runner.py:273] Initializing checkpoint and logger.
I0418 18:46:21.354119 140114918360896 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0418 18:46:21.354368 140114918360896 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0418 18:46:21.354429 140114918360896 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0418 18:46:22.098979 140114918360896 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0418 18:46:22.099851 140114918360896 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0418 18:46:22.106564 140114918360896 submission_runner.py:309] Starting training loop.
I0418 18:46:22.298760 140114918360896 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:46:22.330902 140114918360896 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:46:22.621538 140114918360896 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0418 18:47:17.266680 139938087548672 logging_writer.py:48] [0] global_step=0, grad_norm=17.158388137817383, loss=33.169979095458984
I0418 18:47:17.290717 140114918360896 spec.py:298] Evaluating on the training split.
I0418 18:47:17.414940 140114918360896 input_pipeline.py:20] Loading split = train-clean-100
I0418 18:47:17.446164 140114918360896 input_pipeline.py:20] Loading split = train-clean-360
I0418 18:47:17.819638 140114918360896 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0418 18:48:54.883207 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 18:48:54.976413 140114918360896 input_pipeline.py:20] Loading split = dev-clean
I0418 18:48:54.981756 140114918360896 input_pipeline.py:20] Loading split = dev-other
I0418 18:49:49.978282 140114918360896 spec.py:326] Evaluating on the test split.
I0418 18:49:50.075732 140114918360896 input_pipeline.py:20] Loading split = test-clean
I0418 18:50:27.024538 140114918360896 submission_runner.py:406] Time since start: 244.92s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.908821, dtype=float32), 'train/wer': 4.985543385461133, 'validation/ctc_loss': DeviceArray(31.015093, dtype=float32), 'validation/wer': 4.396520950515683, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.091825, dtype=float32), 'test/wer': 4.771190055450613, 'test/num_examples': 2472, 'score': 55.18397498130798, 'total_duration': 244.916410446167, 'accumulated_submission_time': 55.18397498130798, 'accumulated_eval_time': 189.73228526115417, 'accumulated_logging_time': 0}
I0418 18:50:27.046342 139935168329472 logging_writer.py:48] [1] accumulated_eval_time=189.732285, accumulated_logging_time=0, accumulated_submission_time=55.183975, global_step=1, preemption_count=0, score=55.183975, test/ctc_loss=31.091825485229492, test/num_examples=2472, test/wer=4.771190, total_duration=244.916410, train/ctc_loss=31.90882110595703, train/wer=4.985543, validation/ctc_loss=31.015092849731445, validation/num_examples=5348, validation/wer=4.396521
I0418 18:50:27.138929 140114918360896 checkpoints.py:356] Saving checkpoint at step: 1
I0418 18:50:27.393843 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0418 18:50:27.394500 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0418 18:52:35.202963 139938700011264 logging_writer.py:48] [100] global_step=100, grad_norm=1.1787075996398926, loss=5.983527183532715
I0418 18:54:30.436913 139938708403968 logging_writer.py:48] [200] global_step=200, grad_norm=1.9052798748016357, loss=5.969418048858643
I0418 18:56:26.658678 139938700011264 logging_writer.py:48] [300] global_step=300, grad_norm=0.6538785099983215, loss=5.837895393371582
I0418 18:58:19.473752 139938708403968 logging_writer.py:48] [400] global_step=400, grad_norm=0.3145647943019867, loss=5.792625904083252
I0418 19:00:14.486493 139938700011264 logging_writer.py:48] [500] global_step=500, grad_norm=1.48067045211792, loss=7.043849468231201
I0418 19:02:07.516500 139938708403968 logging_writer.py:48] [600] global_step=600, grad_norm=3.424013376235962, loss=6.259764194488525
I0418 19:03:59.552826 139938700011264 logging_writer.py:48] [700] global_step=700, grad_norm=1.33863365650177, loss=5.96222448348999
I0418 19:05:51.564424 139938708403968 logging_writer.py:48] [800] global_step=800, grad_norm=2.437692880630493, loss=6.025749683380127
I0418 19:07:43.627670 139938700011264 logging_writer.py:48] [900] global_step=900, grad_norm=0.9953867197036743, loss=5.897496223449707
I0418 19:09:37.934703 139938708403968 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.4740115404129028, loss=5.925058364868164
I0418 19:11:36.176794 139937135564544 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.26921790838241577, loss=5.84367036819458
I0418 19:13:28.722164 139937127171840 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1402231454849243, loss=5.86821174621582
I0418 19:15:26.182789 139937135564544 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5120975971221924, loss=5.830018997192383
I0418 19:17:21.106586 139937127171840 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6355459690093994, loss=5.800186634063721
I0418 19:19:16.908406 139937135564544 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6195839047431946, loss=5.784398078918457
I0418 19:21:09.685969 139937127171840 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8138518929481506, loss=5.826014041900635
I0418 19:23:01.865775 139937135564544 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6928508877754211, loss=5.790803909301758
I0418 19:24:57.790738 139937127171840 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6950888633728027, loss=5.804388523101807
I0418 19:26:54.898163 139937135564544 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.20159736275672913, loss=5.764206886291504
I0418 19:28:50.283819 139937127171840 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.19763748347759247, loss=5.748324871063232
I0418 19:30:28.086774 140114918360896 spec.py:298] Evaluating on the training split.
I0418 19:30:56.888538 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 19:31:30.943846 140114918360896 spec.py:326] Evaluating on the test split.
I0418 19:31:48.396102 140114918360896 submission_runner.py:406] Time since start: 2726.29s, 	Step: 2085, 	{'train/ctc_loss': DeviceArray(5.726761, dtype=float32), 'train/wer': 0.9446252394389405, 'validation/ctc_loss': DeviceArray(5.6790266, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.661396, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2455.846287727356, 'total_duration': 2726.2858827114105, 'accumulated_submission_time': 2455.846287727356, 'accumulated_eval_time': 270.03833055496216, 'accumulated_logging_time': 0.3723735809326172}
I0418 19:31:48.416549 139937135564544 logging_writer.py:48] [2085] accumulated_eval_time=270.038331, accumulated_logging_time=0.372374, accumulated_submission_time=2455.846288, global_step=2085, preemption_count=0, score=2455.846288, test/ctc_loss=5.661396026611328, test/num_examples=2472, test/wer=0.899580, total_duration=2726.285883, train/ctc_loss=5.7267608642578125, train/wer=0.944625, validation/ctc_loss=5.6790266036987305, validation/num_examples=5348, validation/wer=0.895995
I0418 19:31:48.530894 140114918360896 checkpoints.py:356] Saving checkpoint at step: 2085
I0418 19:31:48.863277 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_2085
I0418 19:31:48.868942 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_2085.
I0418 19:32:06.811691 139937127171840 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.16800926625728607, loss=5.733513355255127
I0418 19:33:58.868105 139935965243136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.14995265007019043, loss=5.712815284729004
I0418 19:35:50.833722 139937127171840 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4379059374332428, loss=5.686269760131836
I0418 19:37:44.460382 139935965243136 logging_writer.py:48] [2400] global_step=2400, grad_norm=220.4036102294922, loss=212.95632934570312
I0418 19:39:41.269800 139937127171840 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1860.9932861328125
I0418 19:41:38.746184 139935965243136 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1807.6417236328125
I0418 19:43:34.326822 139937127171840 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1783.711181640625
I0418 19:45:29.538333 139935965243136 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1782.4691162109375
I0418 19:47:26.494857 139937127171840 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1822.829833984375
I0418 19:49:23.460877 139935965243136 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1874.3463134765625
I0418 19:51:18.855786 139940170192640 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1846.895751953125
I0418 19:53:11.262778 139940161799936 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1822.9595947265625
I0418 19:55:03.589320 139940170192640 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1849.5645751953125
I0418 19:56:55.218824 139940161799936 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1863.024658203125
I0418 19:58:50.362589 139940170192640 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1822.829833984375
I0418 20:00:45.771119 139940161799936 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1802.0428466796875
I0418 20:02:37.913437 139940170192640 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1833.666748046875
I0418 20:04:29.706963 139940161799936 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1812.24853515625
I0418 20:06:23.368830 139940170192640 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1818.8150634765625
I0418 20:08:15.409762 139940161799936 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1844.2347412109375
I0418 20:10:08.648921 139940170192640 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1758.2296142578125
I0418 20:11:49.318965 140114918360896 spec.py:298] Evaluating on the training split.
I0418 20:12:17.406045 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 20:12:53.446568 140114918360896 spec.py:326] Evaluating on the test split.
I0418 20:13:11.990057 140114918360896 submission_runner.py:406] Time since start: 5209.88s, 	Step: 4186, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4856.2680830955505, 'total_duration': 5209.880450487137, 'accumulated_submission_time': 4856.2680830955505, 'accumulated_eval_time': 352.7066271305084, 'accumulated_logging_time': 0.8473877906799316}
I0418 20:13:12.009306 139939586512640 logging_writer.py:48] [4186] accumulated_eval_time=352.706627, accumulated_logging_time=0.847388, accumulated_submission_time=4856.268083, global_step=4186, preemption_count=0, score=4856.268083, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5209.880450, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 20:13:12.110459 140114918360896 checkpoints.py:356] Saving checkpoint at step: 4186
I0418 20:13:12.512043 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_4186
I0418 20:13:12.520808 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_4186.
I0418 20:13:30.113056 139939578119936 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1938.05908203125
I0418 20:15:21.874585 139939527763712 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1863.431396484375
I0418 20:17:13.662151 139939578119936 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1779.2481689453125
I0418 20:19:05.439366 139939527763712 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1844.6334228515625
I0418 20:20:57.179829 139939578119936 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1843.0396728515625
I0418 20:22:50.631040 139939527763712 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1866.8287353515625
I0418 20:24:42.416110 139939578119936 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1853.3135986328125
I0418 20:26:37.671404 139939527763712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1806.111328125
I0418 20:28:33.034790 139939578119936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1833.666748046875
I0418 20:30:27.178244 139939527763712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1801.408935546875
I0418 20:32:22.085922 139939586512640 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1856.6741943359375
I0418 20:34:14.372956 139939578119936 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1830.9127197265625
I0418 20:36:10.082479 139939586512640 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1846.2296142578125
I0418 20:38:02.324756 139939578119936 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1843.836181640625
I0418 20:39:54.076457 139939586512640 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1833.5355224609375
I0418 20:41:45.809795 139939578119936 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1826.9927978515625
I0418 20:43:37.525950 139939586512640 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1819.7200927734375
I0418 20:45:30.924920 139939578119936 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1806.23876953125
I0418 20:47:22.852662 139939586512640 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1818.29833984375
I0418 20:49:19.805632 139939578119936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1835.90234375
I0418 20:51:16.844505 139939586512640 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1860.85791015625
I0418 20:53:12.046854 139939578119936 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1825.820068359375
I0418 20:53:13.061270 140114918360896 spec.py:298] Evaluating on the training split.
I0418 20:53:41.687997 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 20:54:17.630346 140114918360896 spec.py:326] Evaluating on the test split.
I0418 20:54:34.960812 140114918360896 submission_runner.py:406] Time since start: 7692.85s, 	Step: 6302, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7256.779740333557, 'total_duration': 7692.851051568985, 'accumulated_submission_time': 7256.779740333557, 'accumulated_eval_time': 434.6030251979828, 'accumulated_logging_time': 1.3800806999206543}
I0418 20:54:34.979176 139939586512640 logging_writer.py:48] [6302] accumulated_eval_time=434.603025, accumulated_logging_time=1.380081, accumulated_submission_time=7256.779740, global_step=6302, preemption_count=0, score=7256.779740, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7692.851052, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 20:54:35.083297 140114918360896 checkpoints.py:356] Saving checkpoint at step: 6302
I0418 20:54:35.501737 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_6302
I0418 20:54:35.510485 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_6302.
I0418 20:56:26.918810 139939578119936 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1842.2440185546875
I0418 20:58:19.106505 139937135564544 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1815.718994140625
I0418 21:00:11.281266 139939578119936 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1778.5064697265625
I0418 21:02:03.487467 139937135564544 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1838.4073486328125
I0418 21:03:55.616603 139939578119936 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1834.1922607421875
I0418 21:05:47.805030 139937135564544 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1880.818603515625
I0418 21:07:41.719099 139939578119936 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1837.219970703125
I0418 21:09:36.059513 139937135564544 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1771.6138916015625
I0418 21:11:28.151743 139939578119936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1801.155517578125
I0418 21:13:26.520150 139939586512640 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1779.4954833984375
I0418 21:15:19.666041 139939578119936 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1848.7630615234375
I0418 21:17:12.735685 139939586512640 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1810.582275390625
I0418 21:19:04.659919 139939578119936 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1853.9847412109375
I0418 21:21:00.315982 139939586512640 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1906.1669921875
I0418 21:22:52.427350 139939578119936 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1811.3509521484375
I0418 21:24:44.389316 139939586512640 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1834.5865478515625
I0418 21:26:36.418853 139939578119936 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1937.912353515625
I0418 21:28:28.255505 139939586512640 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1855.73193359375
I0418 21:30:20.066688 139939578119936 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1885.112060546875
I0418 21:32:17.342200 139939586512640 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1813.275390625
I0418 21:34:11.908724 139939578119936 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1803.6934814453125
I0418 21:34:36.319053 140114918360896 spec.py:298] Evaluating on the training split.
I0418 21:35:05.074440 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 21:35:40.519733 140114918360896 spec.py:326] Evaluating on the test split.
I0418 21:35:58.673644 140114918360896 submission_runner.py:406] Time since start: 10176.56s, 	Step: 8423, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9657.55945467949, 'total_duration': 10176.56350159645, 'accumulated_submission_time': 9657.55945467949, 'accumulated_eval_time': 516.9540822505951, 'accumulated_logging_time': 1.9318363666534424}
I0418 21:35:58.692781 139939156432640 logging_writer.py:48] [8423] accumulated_eval_time=516.954082, accumulated_logging_time=1.931836, accumulated_submission_time=9657.559455, global_step=8423, preemption_count=0, score=9657.559455, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10176.563502, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 21:35:58.797523 140114918360896 checkpoints.py:356] Saving checkpoint at step: 8423
I0418 21:35:59.171377 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8423
I0418 21:35:59.180156 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8423.
I0418 21:37:29.093713 139939148039936 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1847.562255859375
I0418 21:39:20.817343 139939080898304 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1782.0970458984375
I0418 21:41:12.557984 139939148039936 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1795.7227783203125
I0418 21:43:04.340399 139939080898304 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1862.482421875
I0418 21:44:56.122020 139939148039936 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1787.3226318359375
I0418 21:46:47.838022 139939080898304 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1836.4290771484375
I0418 21:48:39.917531 139939148039936 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1861.263671875
I0418 21:50:31.958403 139939080898304 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1833.5355224609375
I0418 21:52:27.055431 139939156432640 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1878.6102294921875
I0418 21:54:22.272946 139939148039936 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1864.7889404296875
I0418 21:56:19.648783 139939156432640 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1911.0048828125
I0418 21:58:17.471950 139939148039936 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1813.5322265625
I0418 22:00:12.628208 139939156432640 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1803.439453125
I0418 22:02:09.994048 139939148039936 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1817.394775390625
I0418 22:04:06.535274 139939156432640 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1816.2342529296875
I0418 22:06:04.058314 139939148039936 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1829.86572265625
I0418 22:08:01.604670 139939156432640 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1894.0384521484375
I0418 22:09:55.847655 139939148039936 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1853.8505859375
I0418 22:11:51.106376 139939156432640 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1849.4307861328125
I0418 22:13:44.040238 139939148039936 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1738.40966796875
I0418 22:15:36.262451 139939156432640 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1866.0123291015625
I0418 22:15:59.537948 140114918360896 spec.py:298] Evaluating on the training split.
I0418 22:16:28.476467 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 22:17:02.671395 140114918360896 spec.py:326] Evaluating on the test split.
I0418 22:17:19.982225 140114918360896 submission_runner.py:406] Time since start: 12657.87s, 	Step: 10522, 	{'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12057.889045000076, 'total_duration': 12657.872564315796, 'accumulated_submission_time': 12057.889045000076, 'accumulated_eval_time': 597.3953156471252, 'accumulated_logging_time': 2.4400947093963623}
I0418 22:17:20.002152 139940170192640 logging_writer.py:48] [10522] accumulated_eval_time=597.395316, accumulated_logging_time=2.440095, accumulated_submission_time=12057.889045, global_step=10522, preemption_count=0, score=12057.889045, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=12657.872564, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 22:17:20.104111 140114918360896 checkpoints.py:356] Saving checkpoint at step: 10522
I0418 22:17:20.477591 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_10522
I0418 22:17:20.486404 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_10522.
I0418 22:18:49.633791 139940161799936 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1808.2802734375
I0418 22:20:41.728866 139940086265600 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1888.5888671875
I0418 22:22:36.528126 139940161799936 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1826.7320556640625
I0418 22:24:29.160075 139940086265600 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1879.7138671875
I0418 22:26:21.139994 139940161799936 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1767.577392578125
I0418 22:28:15.969726 139940086265600 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1806.23876953125
I0418 22:30:11.609915 139940161799936 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1832.0921630859375
I0418 22:32:03.355868 139940086265600 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1808.4078369140625
I0418 22:33:58.314035 139939084752640 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1875.7196044921875
I0418 22:35:55.820635 139939076359936 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1843.1724853515625
I0418 22:37:51.385515 139939084752640 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1824.6488037109375
I0418 22:39:46.072156 139939076359936 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1833.1414794921875
I0418 22:41:41.209381 139939084752640 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1818.29833984375
I0418 22:43:36.440434 139939076359936 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1836.033935546875
I0418 22:45:31.863124 139939084752640 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1785.2037353515625
I0418 22:47:26.454497 139939076359936 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1795.2188720703125
I0418 22:49:18.591552 139939084752640 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1858.8311767578125
I0418 22:51:11.144375 139939076359936 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0, loss=1880.680419921875
I0418 22:53:11.857862 139939084752640 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1869.9652099609375
I0418 22:55:09.138516 139939076359936 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1823.21923828125
I0418 22:57:07.332468 139939084752640 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1888.5888671875
I0418 22:57:21.249984 140114918360896 spec.py:298] Evaluating on the training split.
I0418 22:57:49.948117 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 22:58:25.224710 140114918360896 spec.py:326] Evaluating on the test split.
I0418 22:58:42.752834 140114918360896 submission_runner.py:406] Time since start: 15140.64s, 	Step: 12613, 	{'train/ctc_loss': DeviceArray(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14458.623795032501, 'total_duration': 15140.643317222595, 'accumulated_submission_time': 14458.623795032501, 'accumulated_eval_time': 678.8952658176422, 'accumulated_logging_time': 2.9458231925964355}
I0418 22:58:42.772277 139940170192640 logging_writer.py:48] [12613] accumulated_eval_time=678.895266, accumulated_logging_time=2.945823, accumulated_submission_time=14458.623795, global_step=12613, preemption_count=0, score=14458.623795, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=15140.643317, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 22:58:42.873190 140114918360896 checkpoints.py:356] Saving checkpoint at step: 12613
I0418 22:58:43.261301 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_12613
I0418 22:58:43.270168 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_12613.
I0418 23:00:22.607805 139940161799936 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1767.577392578125
I0418 23:02:20.275155 139940077872896 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1840.125244140625
I0418 23:04:14.249359 139940161799936 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1838.6715087890625
I0418 23:06:12.041882 139940077872896 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1811.22265625
I0418 23:08:06.524143 139940161799936 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1754.9752197265625
I0418 23:09:58.348177 139940077872896 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1844.766357421875
I0418 23:11:50.241256 139940161799936 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1806.4937744140625
I0418 23:13:45.559835 139939299792640 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1799.1300048828125
I0418 23:15:37.489889 139939291399936 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1775.1766357421875
I0418 23:17:29.334544 139939299792640 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1789.571533203125
I0418 23:19:21.100805 139939291399936 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1875.5821533203125
I0418 23:21:12.886188 139939299792640 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1754.13330078125
I0418 23:23:04.672376 139939291399936 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1753.5325927734375
I0418 23:24:56.688882 139939299792640 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1816.3631591796875
I0418 23:26:49.047118 139939291399936 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1776.038818359375
I0418 23:28:46.357053 139939299792640 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1815.59033203125
I0418 23:30:39.396086 139939291399936 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1794.5897216796875
I0418 23:32:31.520700 139939299792640 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1819.202880859375
I0418 23:34:27.927703 139939299792640 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1849.965576171875
I0418 23:36:19.788516 139939291399936 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1822.3109130859375
I0418 23:38:11.693152 139939299792640 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1841.4488525390625
I0418 23:38:43.917838 140114918360896 spec.py:298] Evaluating on the training split.
I0418 23:39:12.763578 140114918360896 spec.py:310] Evaluating on the validation split.
I0418 23:39:47.255616 140114918360896 spec.py:326] Evaluating on the test split.
I0418 23:40:04.441148 140114918360896 submission_runner.py:406] Time since start: 17622.33s, 	Step: 14730, 	{'train/ctc_loss': DeviceArray(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16859.24216389656, 'total_duration': 17622.331510305405, 'accumulated_submission_time': 16859.24216389656, 'accumulated_eval_time': 759.4155583381653, 'accumulated_logging_time': 3.464796781539917}
I0418 23:40:04.461856 139938501072640 logging_writer.py:48] [14730] accumulated_eval_time=759.415558, accumulated_logging_time=3.464797, accumulated_submission_time=16859.242164, global_step=14730, preemption_count=0, score=16859.242164, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=17622.331510, train/ctc_loss=1746.1109619140625, train/wer=0.942824, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0418 23:40:04.563577 140114918360896 checkpoints.py:356] Saving checkpoint at step: 14730
I0418 23:40:04.895699 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_14730
I0418 23:40:04.900599 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_14730.
I0418 23:41:24.168133 139938492679936 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1846.629150390625
I0418 23:43:16.077963 139938400360192 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1854.3875732421875
I0418 23:45:08.210218 139938492679936 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1898.3924560546875
I0418 23:47:01.513241 139938400360192 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1784.0841064453125
I0418 23:48:59.358543 139938492679936 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1788.8212890625
I0418 23:50:56.815778 139938400360192 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1823.8687744140625
I0418 23:52:53.015136 139938492679936 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1749.458251953125
I0418 23:54:52.683931 139940170192640 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1807.0037841796875
I0418 23:56:44.475326 139940161799936 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1847.8289794921875
I0418 23:58:40.347876 139940170192640 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1784.332763671875
I0419 00:00:36.203151 139940161799936 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1824.128662109375
I0419 00:02:28.056710 139940170192640 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1748.621826171875
I0419 00:04:19.321779 140114918360896 spec.py:298] Evaluating on the training split.
I0419 00:04:48.113869 140114918360896 spec.py:310] Evaluating on the validation split.
I0419 00:05:24.002679 140114918360896 spec.py:326] Evaluating on the test split.
I0419 00:05:41.678248 140114918360896 submission_runner.py:406] Time since start: 19159.57s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18313.644594669342, 'total_duration': 19159.56995153427, 'accumulated_submission_time': 18313.644594669342, 'accumulated_eval_time': 841.7703411579132, 'accumulated_logging_time': 3.926475763320923}
I0419 00:05:41.699971 139939663312640 logging_writer.py:48] [16000] accumulated_eval_time=841.770341, accumulated_logging_time=3.926476, accumulated_submission_time=18313.644595, global_step=16000, preemption_count=0, score=18313.644595, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=19159.569952, train/ctc_loss=1733.7392578125, train/wer=0.944086, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0419 00:05:41.801136 140114918360896 checkpoints.py:356] Saving checkpoint at step: 16000
I0419 00:05:42.213319 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0419 00:05:42.222198 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0419 00:05:42.232564 139939654919936 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18313.644595
I0419 00:05:42.293413 140114918360896 checkpoints.py:356] Saving checkpoint at step: 16000
I0419 00:05:42.835957 140114918360896 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0419 00:05:42.844828 140114918360896 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0419 00:05:44.022869 140114918360896 submission_runner.py:567] Tuning trial 1/1
I0419 00:05:44.023124 140114918360896 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0419 00:05:44.027943 140114918360896 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.908821, dtype=float32), 'train/wer': 4.985543385461133, 'validation/ctc_loss': DeviceArray(31.015093, dtype=float32), 'validation/wer': 4.396520950515683, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.091825, dtype=float32), 'test/wer': 4.771190055450613, 'test/num_examples': 2472, 'score': 55.18397498130798, 'total_duration': 244.916410446167, 'accumulated_submission_time': 55.18397498130798, 'accumulated_eval_time': 189.73228526115417, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2085, {'train/ctc_loss': DeviceArray(5.726761, dtype=float32), 'train/wer': 0.9446252394389405, 'validation/ctc_loss': DeviceArray(5.6790266, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.661396, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2455.846287727356, 'total_duration': 2726.2858827114105, 'accumulated_submission_time': 2455.846287727356, 'accumulated_eval_time': 270.03833055496216, 'accumulated_logging_time': 0.3723735809326172, 'global_step': 2085, 'preemption_count': 0}), (4186, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4856.2680830955505, 'total_duration': 5209.880450487137, 'accumulated_submission_time': 4856.2680830955505, 'accumulated_eval_time': 352.7066271305084, 'accumulated_logging_time': 0.8473877906799316, 'global_step': 4186, 'preemption_count': 0}), (6302, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7256.779740333557, 'total_duration': 7692.851051568985, 'accumulated_submission_time': 7256.779740333557, 'accumulated_eval_time': 434.6030251979828, 'accumulated_logging_time': 1.3800806999206543, 'global_step': 6302, 'preemption_count': 0}), (8423, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9657.55945467949, 'total_duration': 10176.56350159645, 'accumulated_submission_time': 9657.55945467949, 'accumulated_eval_time': 516.9540822505951, 'accumulated_logging_time': 1.9318363666534424, 'global_step': 8423, 'preemption_count': 0}), (10522, {'train/ctc_loss': DeviceArray(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12057.889045000076, 'total_duration': 12657.872564315796, 'accumulated_submission_time': 12057.889045000076, 'accumulated_eval_time': 597.3953156471252, 'accumulated_logging_time': 2.4400947093963623, 'global_step': 10522, 'preemption_count': 0}), (12613, {'train/ctc_loss': DeviceArray(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14458.623795032501, 'total_duration': 15140.643317222595, 'accumulated_submission_time': 14458.623795032501, 'accumulated_eval_time': 678.8952658176422, 'accumulated_logging_time': 2.9458231925964355, 'global_step': 12613, 'preemption_count': 0}), (14730, {'train/ctc_loss': DeviceArray(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16859.24216389656, 'total_duration': 17622.331510305405, 'accumulated_submission_time': 16859.24216389656, 'accumulated_eval_time': 759.4155583381653, 'accumulated_logging_time': 3.464796781539917, 'global_step': 14730, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18313.644594669342, 'total_duration': 19159.56995153427, 'accumulated_submission_time': 18313.644594669342, 'accumulated_eval_time': 841.7703411579132, 'accumulated_logging_time': 3.926475763320923, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0419 00:05:44.028112 140114918360896 submission_runner.py:570] Timing: 18313.644594669342
I0419 00:05:44.028161 140114918360896 submission_runner.py:571] ====================
I0419 00:05:44.028669 140114918360896 submission_runner.py:631] Final librispeech_deepspeech score: 18313.644594669342
