torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-07-2023-02-56-38.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 02:57:04.169290 139932693083968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 02:57:04.169322 139641609430848 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 02:57:04.169362 140686703052608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 02:57:04.169426 140029200299840 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 02:57:04.170236 140177573685056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 02:57:04.170382 140004808804160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 02:57:04.170366 140153176590144 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 02:57:04.171211 140382832805696 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 02:57:04.171565 140382832805696 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.180089 139932693083968 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.180113 139641609430848 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.180133 140686703052608 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.180161 140029200299840 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.180987 140177573685056 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.181017 140153176590144 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:04.181046 140004808804160 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:06.640654 140004808804160 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch because --overwrite was set.
I0607 02:57:06.664715 140004808804160 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch.
W0607 02:57:06.682159 139641609430848 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:06.682539 139932693083968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:06.682937 140686703052608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:06.696635 140004808804160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 02:57:06.701457 140004808804160 submission_runner.py:541] Using RNG seed 2934286202
I0607 02:57:06.702812 140004808804160 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 02:57:06.702938 140004808804160 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch/trial_1.
I0607 02:57:06.703174 140004808804160 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0607 02:57:06.704123 140004808804160 submission_runner.py:255] Initializing dataset.
W0607 02:57:06.730002 140382832805696 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:06.732420 140177573685056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:06.733736 140029200299840 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:06.734486 140153176590144 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 02:57:13.050103 140004808804160 submission_runner.py:262] Initializing model.
I0607 02:57:17.590525 140004808804160 submission_runner.py:272] Initializing optimizer.
I0607 02:57:17.591786 140004808804160 submission_runner.py:279] Initializing metrics bundle.
I0607 02:57:17.591893 140004808804160 submission_runner.py:297] Initializing checkpoint and logger.
I0607 02:57:18.050848 140004808804160 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0607 02:57:18.052511 140004808804160 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0607 02:57:18.107010 140004808804160 submission_runner.py:332] Starting training loop.
I0607 02:57:26.284331 139975752406784 logging_writer.py:48] [0] global_step=0, grad_norm=0.604935, loss=6.924083
I0607 02:57:26.307039 140004808804160 submission.py:296] 0) loss = 6.924, grad_norm = 0.605
I0607 02:57:26.308217 140004808804160 spec.py:298] Evaluating on the training split.
I0607 02:58:27.799336 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 02:59:25.789906 140004808804160 spec.py:326] Evaluating on the test split.
I0607 02:59:25.809542 140004808804160 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 02:59:25.815940 140004808804160 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0607 02:59:25.895598 140004808804160 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 02:59:37.730624 140004808804160 submission_runner.py:419] Time since start: 139.62s, 	Step: 1, 	{'train/accuracy': 0.0011559311224489796, 'train/loss': 6.92365685287787, 'validation/accuracy': 0.00086, 'validation/loss': 6.923295, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.92518125, 'test/num_examples': 10000, 'score': 8.20128583908081, 'total_duration': 139.6240816116333, 'accumulated_submission_time': 8.20128583908081, 'accumulated_eval_time': 131.42235279083252, 'accumulated_logging_time': 0}
I0607 02:59:37.749360 139952138462976 logging_writer.py:48] [1] accumulated_eval_time=131.422353, accumulated_logging_time=0, accumulated_submission_time=8.201286, global_step=1, preemption_count=0, score=8.201286, test/accuracy=0.001100, test/loss=6.925181, test/num_examples=10000, total_duration=139.624082, train/accuracy=0.001156, train/loss=6.923657, validation/accuracy=0.000860, validation/loss=6.923295, validation/num_examples=50000
I0607 02:59:37.768393 140004808804160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.768414 140153176590144 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.769171 140029200299840 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.769323 139932693083968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.769366 140686703052608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.769358 139641609430848 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.769354 140177573685056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:37.769443 140382832805696 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:38.152208 139952130070272 logging_writer.py:48] [1] global_step=1, grad_norm=0.631445, loss=6.924283
I0607 02:59:38.155446 140004808804160 submission.py:296] 1) loss = 6.924, grad_norm = 0.631
I0607 02:59:38.546764 139952138462976 logging_writer.py:48] [2] global_step=2, grad_norm=0.621373, loss=6.926770
I0607 02:59:38.550535 140004808804160 submission.py:296] 2) loss = 6.927, grad_norm = 0.621
I0607 02:59:38.937704 139952130070272 logging_writer.py:48] [3] global_step=3, grad_norm=0.624624, loss=6.939653
I0607 02:59:38.941196 140004808804160 submission.py:296] 3) loss = 6.940, grad_norm = 0.625
I0607 02:59:39.329590 139952138462976 logging_writer.py:48] [4] global_step=4, grad_norm=0.622247, loss=6.925655
I0607 02:59:39.333905 140004808804160 submission.py:296] 4) loss = 6.926, grad_norm = 0.622
I0607 02:59:39.727521 139952130070272 logging_writer.py:48] [5] global_step=5, grad_norm=0.610728, loss=6.925886
I0607 02:59:39.731088 140004808804160 submission.py:296] 5) loss = 6.926, grad_norm = 0.611
I0607 02:59:40.126111 139952138462976 logging_writer.py:48] [6] global_step=6, grad_norm=0.629520, loss=6.931849
I0607 02:59:40.129817 140004808804160 submission.py:296] 6) loss = 6.932, grad_norm = 0.630
I0607 02:59:40.525651 139952130070272 logging_writer.py:48] [7] global_step=7, grad_norm=0.628204, loss=6.931591
I0607 02:59:40.530497 140004808804160 submission.py:296] 7) loss = 6.932, grad_norm = 0.628
I0607 02:59:40.923743 139952138462976 logging_writer.py:48] [8] global_step=8, grad_norm=0.622572, loss=6.934175
I0607 02:59:40.928295 140004808804160 submission.py:296] 8) loss = 6.934, grad_norm = 0.623
I0607 02:59:41.329081 139952130070272 logging_writer.py:48] [9] global_step=9, grad_norm=0.643876, loss=6.930553
I0607 02:59:41.332858 140004808804160 submission.py:296] 9) loss = 6.931, grad_norm = 0.644
I0607 02:59:41.721741 139952138462976 logging_writer.py:48] [10] global_step=10, grad_norm=0.618991, loss=6.921263
I0607 02:59:41.725642 140004808804160 submission.py:296] 10) loss = 6.921, grad_norm = 0.619
I0607 02:59:42.128851 139952130070272 logging_writer.py:48] [11] global_step=11, grad_norm=0.610817, loss=6.924209
I0607 02:59:42.132554 140004808804160 submission.py:296] 11) loss = 6.924, grad_norm = 0.611
I0607 02:59:42.523377 139952138462976 logging_writer.py:48] [12] global_step=12, grad_norm=0.609853, loss=6.927448
I0607 02:59:42.527067 140004808804160 submission.py:296] 12) loss = 6.927, grad_norm = 0.610
I0607 02:59:42.932901 139952130070272 logging_writer.py:48] [13] global_step=13, grad_norm=0.621649, loss=6.923563
I0607 02:59:42.936526 140004808804160 submission.py:296] 13) loss = 6.924, grad_norm = 0.622
I0607 02:59:43.349325 139952138462976 logging_writer.py:48] [14] global_step=14, grad_norm=0.634733, loss=6.928388
I0607 02:59:43.352910 140004808804160 submission.py:296] 14) loss = 6.928, grad_norm = 0.635
I0607 02:59:43.742132 139952130070272 logging_writer.py:48] [15] global_step=15, grad_norm=0.603998, loss=6.932039
I0607 02:59:43.745767 140004808804160 submission.py:296] 15) loss = 6.932, grad_norm = 0.604
I0607 02:59:44.134259 139952138462976 logging_writer.py:48] [16] global_step=16, grad_norm=0.607975, loss=6.926984
I0607 02:59:44.137843 140004808804160 submission.py:296] 16) loss = 6.927, grad_norm = 0.608
I0607 02:59:44.545414 139952130070272 logging_writer.py:48] [17] global_step=17, grad_norm=0.613603, loss=6.922576
I0607 02:59:44.549067 140004808804160 submission.py:296] 17) loss = 6.923, grad_norm = 0.614
I0607 02:59:44.941231 139952138462976 logging_writer.py:48] [18] global_step=18, grad_norm=0.626741, loss=6.931501
I0607 02:59:44.944866 140004808804160 submission.py:296] 18) loss = 6.932, grad_norm = 0.627
I0607 02:59:45.354163 139952130070272 logging_writer.py:48] [19] global_step=19, grad_norm=0.619939, loss=6.932153
I0607 02:59:45.357910 140004808804160 submission.py:296] 19) loss = 6.932, grad_norm = 0.620
I0607 02:59:45.764025 139952138462976 logging_writer.py:48] [20] global_step=20, grad_norm=0.628365, loss=6.935077
I0607 02:59:45.772855 140004808804160 submission.py:296] 20) loss = 6.935, grad_norm = 0.628
I0607 02:59:46.170963 139952130070272 logging_writer.py:48] [21] global_step=21, grad_norm=0.616091, loss=6.923310
I0607 02:59:46.174731 140004808804160 submission.py:296] 21) loss = 6.923, grad_norm = 0.616
I0607 02:59:46.565091 139952138462976 logging_writer.py:48] [22] global_step=22, grad_norm=0.608491, loss=6.917945
I0607 02:59:46.569395 140004808804160 submission.py:296] 22) loss = 6.918, grad_norm = 0.608
I0607 02:59:46.962162 139952130070272 logging_writer.py:48] [23] global_step=23, grad_norm=0.616200, loss=6.931363
I0607 02:59:46.966274 140004808804160 submission.py:296] 23) loss = 6.931, grad_norm = 0.616
I0607 02:59:47.361865 139952138462976 logging_writer.py:48] [24] global_step=24, grad_norm=0.614920, loss=6.925152
I0607 02:59:47.365586 140004808804160 submission.py:296] 24) loss = 6.925, grad_norm = 0.615
I0607 02:59:47.761934 139952130070272 logging_writer.py:48] [25] global_step=25, grad_norm=0.615553, loss=6.928347
I0607 02:59:47.769214 140004808804160 submission.py:296] 25) loss = 6.928, grad_norm = 0.616
I0607 02:59:48.160700 139952138462976 logging_writer.py:48] [26] global_step=26, grad_norm=0.628490, loss=6.932172
I0607 02:59:48.164999 140004808804160 submission.py:296] 26) loss = 6.932, grad_norm = 0.628
I0607 02:59:48.566710 139952130070272 logging_writer.py:48] [27] global_step=27, grad_norm=0.608064, loss=6.919652
I0607 02:59:48.571711 140004808804160 submission.py:296] 27) loss = 6.920, grad_norm = 0.608
I0607 02:59:48.963588 139952138462976 logging_writer.py:48] [28] global_step=28, grad_norm=0.618730, loss=6.904977
I0607 02:59:48.967326 140004808804160 submission.py:296] 28) loss = 6.905, grad_norm = 0.619
I0607 02:59:49.356544 139952130070272 logging_writer.py:48] [29] global_step=29, grad_norm=0.617558, loss=6.919699
I0607 02:59:49.361691 140004808804160 submission.py:296] 29) loss = 6.920, grad_norm = 0.618
I0607 02:59:49.753205 139952138462976 logging_writer.py:48] [30] global_step=30, grad_norm=0.620131, loss=6.913163
I0607 02:59:49.757858 140004808804160 submission.py:296] 30) loss = 6.913, grad_norm = 0.620
I0607 02:59:50.150756 139952130070272 logging_writer.py:48] [31] global_step=31, grad_norm=0.602045, loss=6.914147
I0607 02:59:50.155103 140004808804160 submission.py:296] 31) loss = 6.914, grad_norm = 0.602
I0607 02:59:50.550498 139952138462976 logging_writer.py:48] [32] global_step=32, grad_norm=0.592290, loss=6.913101
I0607 02:59:50.557880 140004808804160 submission.py:296] 32) loss = 6.913, grad_norm = 0.592
I0607 02:59:50.951532 139952130070272 logging_writer.py:48] [33] global_step=33, grad_norm=0.636275, loss=6.919682
I0607 02:59:50.958644 140004808804160 submission.py:296] 33) loss = 6.920, grad_norm = 0.636
I0607 02:59:51.351071 139952138462976 logging_writer.py:48] [34] global_step=34, grad_norm=0.608485, loss=6.914845
I0607 02:59:51.354694 140004808804160 submission.py:296] 34) loss = 6.915, grad_norm = 0.608
I0607 02:59:51.745878 139952130070272 logging_writer.py:48] [35] global_step=35, grad_norm=0.611957, loss=6.917799
I0607 02:59:51.749721 140004808804160 submission.py:296] 35) loss = 6.918, grad_norm = 0.612
I0607 02:59:52.153704 139952138462976 logging_writer.py:48] [36] global_step=36, grad_norm=0.626865, loss=6.922996
I0607 02:59:52.157575 140004808804160 submission.py:296] 36) loss = 6.923, grad_norm = 0.627
I0607 02:59:52.548865 139952130070272 logging_writer.py:48] [37] global_step=37, grad_norm=0.624578, loss=6.917164
I0607 02:59:52.553514 140004808804160 submission.py:296] 37) loss = 6.917, grad_norm = 0.625
I0607 02:59:52.952278 139952138462976 logging_writer.py:48] [38] global_step=38, grad_norm=0.607491, loss=6.924062
I0607 02:59:52.956076 140004808804160 submission.py:296] 38) loss = 6.924, grad_norm = 0.607
I0607 02:59:53.355834 139952130070272 logging_writer.py:48] [39] global_step=39, grad_norm=0.612618, loss=6.925611
I0607 02:59:53.359931 140004808804160 submission.py:296] 39) loss = 6.926, grad_norm = 0.613
I0607 02:59:53.755719 139952138462976 logging_writer.py:48] [40] global_step=40, grad_norm=0.625211, loss=6.914657
I0607 02:59:53.760062 140004808804160 submission.py:296] 40) loss = 6.915, grad_norm = 0.625
I0607 02:59:54.162097 139952130070272 logging_writer.py:48] [41] global_step=41, grad_norm=0.597627, loss=6.923324
I0607 02:59:54.171816 140004808804160 submission.py:296] 41) loss = 6.923, grad_norm = 0.598
I0607 02:59:54.574110 139952138462976 logging_writer.py:48] [42] global_step=42, grad_norm=0.608554, loss=6.914084
I0607 02:59:54.577988 140004808804160 submission.py:296] 42) loss = 6.914, grad_norm = 0.609
I0607 02:59:54.971331 139952130070272 logging_writer.py:48] [43] global_step=43, grad_norm=0.622420, loss=6.915459
I0607 02:59:54.978009 140004808804160 submission.py:296] 43) loss = 6.915, grad_norm = 0.622
I0607 02:59:55.371696 139952138462976 logging_writer.py:48] [44] global_step=44, grad_norm=0.612788, loss=6.920727
I0607 02:59:55.375765 140004808804160 submission.py:296] 44) loss = 6.921, grad_norm = 0.613
I0607 02:59:55.770755 139952130070272 logging_writer.py:48] [45] global_step=45, grad_norm=0.595262, loss=6.910251
I0607 02:59:55.783172 140004808804160 submission.py:296] 45) loss = 6.910, grad_norm = 0.595
I0607 02:59:56.179710 139952138462976 logging_writer.py:48] [46] global_step=46, grad_norm=0.623475, loss=6.908603
I0607 02:59:56.184041 140004808804160 submission.py:296] 46) loss = 6.909, grad_norm = 0.623
I0607 02:59:56.586899 139952130070272 logging_writer.py:48] [47] global_step=47, grad_norm=0.614298, loss=6.913758
I0607 02:59:56.591506 140004808804160 submission.py:296] 47) loss = 6.914, grad_norm = 0.614
I0607 02:59:56.989569 139952138462976 logging_writer.py:48] [48] global_step=48, grad_norm=0.606607, loss=6.913375
I0607 02:59:56.993714 140004808804160 submission.py:296] 48) loss = 6.913, grad_norm = 0.607
I0607 02:59:57.392245 139952130070272 logging_writer.py:48] [49] global_step=49, grad_norm=0.627417, loss=6.914004
I0607 02:59:57.397910 140004808804160 submission.py:296] 49) loss = 6.914, grad_norm = 0.627
I0607 02:59:57.797148 139952138462976 logging_writer.py:48] [50] global_step=50, grad_norm=0.610757, loss=6.912260
I0607 02:59:57.801106 140004808804160 submission.py:296] 50) loss = 6.912, grad_norm = 0.611
I0607 02:59:58.194131 139952130070272 logging_writer.py:48] [51] global_step=51, grad_norm=0.615609, loss=6.915809
I0607 02:59:58.199367 140004808804160 submission.py:296] 51) loss = 6.916, grad_norm = 0.616
I0607 02:59:58.596354 139952138462976 logging_writer.py:48] [52] global_step=52, grad_norm=0.606174, loss=6.902386
I0607 02:59:58.600662 140004808804160 submission.py:296] 52) loss = 6.902, grad_norm = 0.606
I0607 02:59:58.995849 139952130070272 logging_writer.py:48] [53] global_step=53, grad_norm=0.605938, loss=6.905046
I0607 02:59:59.003378 140004808804160 submission.py:296] 53) loss = 6.905, grad_norm = 0.606
I0607 02:59:59.405073 139952138462976 logging_writer.py:48] [54] global_step=54, grad_norm=0.621105, loss=6.910212
I0607 02:59:59.409809 140004808804160 submission.py:296] 54) loss = 6.910, grad_norm = 0.621
I0607 02:59:59.807821 139952130070272 logging_writer.py:48] [55] global_step=55, grad_norm=0.607232, loss=6.912894
I0607 02:59:59.811586 140004808804160 submission.py:296] 55) loss = 6.913, grad_norm = 0.607
I0607 03:00:00.207888 139952138462976 logging_writer.py:48] [56] global_step=56, grad_norm=0.621449, loss=6.907598
I0607 03:00:00.212326 140004808804160 submission.py:296] 56) loss = 6.908, grad_norm = 0.621
I0607 03:00:00.615620 139952130070272 logging_writer.py:48] [57] global_step=57, grad_norm=0.606338, loss=6.899194
I0607 03:00:00.619664 140004808804160 submission.py:296] 57) loss = 6.899, grad_norm = 0.606
I0607 03:00:01.012165 139952138462976 logging_writer.py:48] [58] global_step=58, grad_norm=0.627199, loss=6.907049
I0607 03:00:01.016589 140004808804160 submission.py:296] 58) loss = 6.907, grad_norm = 0.627
I0607 03:00:01.413916 139952130070272 logging_writer.py:48] [59] global_step=59, grad_norm=0.625077, loss=6.909831
I0607 03:00:01.419421 140004808804160 submission.py:296] 59) loss = 6.910, grad_norm = 0.625
I0607 03:00:01.816603 139952138462976 logging_writer.py:48] [60] global_step=60, grad_norm=0.612630, loss=6.908347
I0607 03:00:01.821881 140004808804160 submission.py:296] 60) loss = 6.908, grad_norm = 0.613
I0607 03:00:02.220850 139952130070272 logging_writer.py:48] [61] global_step=61, grad_norm=0.592501, loss=6.898545
I0607 03:00:02.224955 140004808804160 submission.py:296] 61) loss = 6.899, grad_norm = 0.593
I0607 03:00:02.622073 139952138462976 logging_writer.py:48] [62] global_step=62, grad_norm=0.600255, loss=6.897662
I0607 03:00:02.626203 140004808804160 submission.py:296] 62) loss = 6.898, grad_norm = 0.600
I0607 03:00:03.022467 139952130070272 logging_writer.py:48] [63] global_step=63, grad_norm=0.621226, loss=6.902244
I0607 03:00:03.026164 140004808804160 submission.py:296] 63) loss = 6.902, grad_norm = 0.621
I0607 03:00:03.431466 139952138462976 logging_writer.py:48] [64] global_step=64, grad_norm=0.637306, loss=6.896361
I0607 03:00:03.435536 140004808804160 submission.py:296] 64) loss = 6.896, grad_norm = 0.637
I0607 03:00:03.828457 139952130070272 logging_writer.py:48] [65] global_step=65, grad_norm=0.603719, loss=6.898803
I0607 03:00:03.835450 140004808804160 submission.py:296] 65) loss = 6.899, grad_norm = 0.604
I0607 03:00:04.224659 139952138462976 logging_writer.py:48] [66] global_step=66, grad_norm=0.611123, loss=6.897710
I0607 03:00:04.229001 140004808804160 submission.py:296] 66) loss = 6.898, grad_norm = 0.611
I0607 03:00:04.639717 139952130070272 logging_writer.py:48] [67] global_step=67, grad_norm=0.618376, loss=6.899088
I0607 03:00:04.643527 140004808804160 submission.py:296] 67) loss = 6.899, grad_norm = 0.618
I0607 03:00:05.037015 139952138462976 logging_writer.py:48] [68] global_step=68, grad_norm=0.615690, loss=6.892621
I0607 03:00:05.040827 140004808804160 submission.py:296] 68) loss = 6.893, grad_norm = 0.616
I0607 03:00:05.434652 139952130070272 logging_writer.py:48] [69] global_step=69, grad_norm=0.590188, loss=6.899822
I0607 03:00:05.442260 140004808804160 submission.py:296] 69) loss = 6.900, grad_norm = 0.590
I0607 03:00:05.834069 139952138462976 logging_writer.py:48] [70] global_step=70, grad_norm=0.612743, loss=6.898568
I0607 03:00:05.838320 140004808804160 submission.py:296] 70) loss = 6.899, grad_norm = 0.613
I0607 03:00:06.232588 139952130070272 logging_writer.py:48] [71] global_step=71, grad_norm=0.614169, loss=6.910055
I0607 03:00:06.236699 140004808804160 submission.py:296] 71) loss = 6.910, grad_norm = 0.614
I0607 03:00:06.630611 139952138462976 logging_writer.py:48] [72] global_step=72, grad_norm=0.610009, loss=6.889149
I0607 03:00:06.634626 140004808804160 submission.py:296] 72) loss = 6.889, grad_norm = 0.610
I0607 03:00:07.030795 139952130070272 logging_writer.py:48] [73] global_step=73, grad_norm=0.626730, loss=6.895523
I0607 03:00:07.034659 140004808804160 submission.py:296] 73) loss = 6.896, grad_norm = 0.627
I0607 03:00:07.428032 139952138462976 logging_writer.py:48] [74] global_step=74, grad_norm=0.592896, loss=6.890924
I0607 03:00:07.432351 140004808804160 submission.py:296] 74) loss = 6.891, grad_norm = 0.593
I0607 03:00:07.838389 139952130070272 logging_writer.py:48] [75] global_step=75, grad_norm=0.603002, loss=6.893095
I0607 03:00:07.842984 140004808804160 submission.py:296] 75) loss = 6.893, grad_norm = 0.603
I0607 03:00:08.267066 139952138462976 logging_writer.py:48] [76] global_step=76, grad_norm=0.603394, loss=6.893128
I0607 03:00:08.271092 140004808804160 submission.py:296] 76) loss = 6.893, grad_norm = 0.603
I0607 03:00:08.664325 139952130070272 logging_writer.py:48] [77] global_step=77, grad_norm=0.612853, loss=6.894648
I0607 03:00:08.667941 140004808804160 submission.py:296] 77) loss = 6.895, grad_norm = 0.613
I0607 03:00:09.061857 139952138462976 logging_writer.py:48] [78] global_step=78, grad_norm=0.616290, loss=6.893893
I0607 03:00:09.070940 140004808804160 submission.py:296] 78) loss = 6.894, grad_norm = 0.616
I0607 03:00:09.471600 139952130070272 logging_writer.py:48] [79] global_step=79, grad_norm=0.622479, loss=6.899036
I0607 03:00:09.475463 140004808804160 submission.py:296] 79) loss = 6.899, grad_norm = 0.622
I0607 03:00:09.871844 139952138462976 logging_writer.py:48] [80] global_step=80, grad_norm=0.584846, loss=6.890018
I0607 03:00:09.875956 140004808804160 submission.py:296] 80) loss = 6.890, grad_norm = 0.585
I0607 03:00:10.280383 139952130070272 logging_writer.py:48] [81] global_step=81, grad_norm=0.595373, loss=6.890118
I0607 03:00:10.284155 140004808804160 submission.py:296] 81) loss = 6.890, grad_norm = 0.595
I0607 03:00:10.682473 139952138462976 logging_writer.py:48] [82] global_step=82, grad_norm=0.598116, loss=6.882646
I0607 03:00:10.687693 140004808804160 submission.py:296] 82) loss = 6.883, grad_norm = 0.598
I0607 03:00:11.084028 139952130070272 logging_writer.py:48] [83] global_step=83, grad_norm=0.614476, loss=6.883064
I0607 03:00:11.088953 140004808804160 submission.py:296] 83) loss = 6.883, grad_norm = 0.614
I0607 03:00:11.482460 139952138462976 logging_writer.py:48] [84] global_step=84, grad_norm=0.608355, loss=6.890797
I0607 03:00:11.487403 140004808804160 submission.py:296] 84) loss = 6.891, grad_norm = 0.608
I0607 03:00:11.892894 139952130070272 logging_writer.py:48] [85] global_step=85, grad_norm=0.597255, loss=6.882769
I0607 03:00:11.900681 140004808804160 submission.py:296] 85) loss = 6.883, grad_norm = 0.597
I0607 03:00:12.297724 139952138462976 logging_writer.py:48] [86] global_step=86, grad_norm=0.614873, loss=6.875319
I0607 03:00:12.301749 140004808804160 submission.py:296] 86) loss = 6.875, grad_norm = 0.615
I0607 03:00:12.691679 139952130070272 logging_writer.py:48] [87] global_step=87, grad_norm=0.613671, loss=6.874964
I0607 03:00:12.696444 140004808804160 submission.py:296] 87) loss = 6.875, grad_norm = 0.614
I0607 03:00:13.088232 139952138462976 logging_writer.py:48] [88] global_step=88, grad_norm=0.606206, loss=6.879315
I0607 03:00:13.092220 140004808804160 submission.py:296] 88) loss = 6.879, grad_norm = 0.606
I0607 03:00:13.487151 139952130070272 logging_writer.py:48] [89] global_step=89, grad_norm=0.608203, loss=6.878263
I0607 03:00:13.491653 140004808804160 submission.py:296] 89) loss = 6.878, grad_norm = 0.608
I0607 03:00:13.887974 139952138462976 logging_writer.py:48] [90] global_step=90, grad_norm=0.618586, loss=6.881174
I0607 03:00:13.892585 140004808804160 submission.py:296] 90) loss = 6.881, grad_norm = 0.619
I0607 03:00:14.285312 139952130070272 logging_writer.py:48] [91] global_step=91, grad_norm=0.593224, loss=6.885307
I0607 03:00:14.289453 140004808804160 submission.py:296] 91) loss = 6.885, grad_norm = 0.593
I0607 03:00:14.683210 139952138462976 logging_writer.py:48] [92] global_step=92, grad_norm=0.614896, loss=6.876805
I0607 03:00:14.687676 140004808804160 submission.py:296] 92) loss = 6.877, grad_norm = 0.615
I0607 03:00:15.079660 139952130070272 logging_writer.py:48] [93] global_step=93, grad_norm=0.604201, loss=6.875358
I0607 03:00:15.083554 140004808804160 submission.py:296] 93) loss = 6.875, grad_norm = 0.604
I0607 03:00:15.476910 139952138462976 logging_writer.py:48] [94] global_step=94, grad_norm=0.595988, loss=6.884381
I0607 03:00:15.481374 140004808804160 submission.py:296] 94) loss = 6.884, grad_norm = 0.596
I0607 03:00:15.874950 139952130070272 logging_writer.py:48] [95] global_step=95, grad_norm=0.602870, loss=6.876834
I0607 03:00:15.878838 140004808804160 submission.py:296] 95) loss = 6.877, grad_norm = 0.603
I0607 03:00:16.271282 139952138462976 logging_writer.py:48] [96] global_step=96, grad_norm=0.601688, loss=6.864194
I0607 03:00:16.275330 140004808804160 submission.py:296] 96) loss = 6.864, grad_norm = 0.602
I0607 03:00:16.667165 139952130070272 logging_writer.py:48] [97] global_step=97, grad_norm=0.615193, loss=6.861957
I0607 03:00:16.671232 140004808804160 submission.py:296] 97) loss = 6.862, grad_norm = 0.615
I0607 03:00:17.068033 139952138462976 logging_writer.py:48] [98] global_step=98, grad_norm=0.621383, loss=6.864330
I0607 03:00:17.074032 140004808804160 submission.py:296] 98) loss = 6.864, grad_norm = 0.621
I0607 03:00:17.476490 139952130070272 logging_writer.py:48] [99] global_step=99, grad_norm=0.618358, loss=6.876758
I0607 03:00:17.481496 140004808804160 submission.py:296] 99) loss = 6.877, grad_norm = 0.618
I0607 03:00:17.874169 139952138462976 logging_writer.py:48] [100] global_step=100, grad_norm=0.603783, loss=6.871182
I0607 03:00:17.878704 140004808804160 submission.py:296] 100) loss = 6.871, grad_norm = 0.604
I0607 03:02:52.397043 139952130070272 logging_writer.py:48] [500] global_step=500, grad_norm=1.183994, loss=6.325796
I0607 03:02:52.401433 140004808804160 submission.py:296] 500) loss = 6.326, grad_norm = 1.184
I0607 03:06:06.183919 139952138462976 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.571573, loss=5.677222
I0607 03:06:06.189671 140004808804160 submission.py:296] 1000) loss = 5.677, grad_norm = 2.572
I0607 03:08:07.840065 140004808804160 spec.py:298] Evaluating on the training split.
I0607 03:08:49.525190 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 03:09:47.370578 140004808804160 spec.py:326] Evaluating on the test split.
I0607 03:09:48.811516 140004808804160 submission_runner.py:419] Time since start: 750.70s, 	Step: 1311, 	{'train/accuracy': 0.11838329081632654, 'train/loss': 4.771433616171078, 'validation/accuracy': 0.10602, 'validation/loss': 4.8671125, 'validation/num_examples': 50000, 'test/accuracy': 0.0744, 'test/loss': 5.27236796875, 'test/num_examples': 10000, 'score': 517.6582624912262, 'total_duration': 750.7049851417542, 'accumulated_submission_time': 517.6582624912262, 'accumulated_eval_time': 232.39377307891846, 'accumulated_logging_time': 0.026567697525024414}
I0607 03:09:48.821488 139952146855680 logging_writer.py:48] [1311] accumulated_eval_time=232.393773, accumulated_logging_time=0.026568, accumulated_submission_time=517.658262, global_step=1311, preemption_count=0, score=517.658262, test/accuracy=0.074400, test/loss=5.272368, test/num_examples=10000, total_duration=750.704985, train/accuracy=0.118383, train/loss=4.771434, validation/accuracy=0.106020, validation/loss=4.867113, validation/num_examples=50000
I0607 03:11:01.618515 139952155248384 logging_writer.py:48] [1500] global_step=1500, grad_norm=8.579477, loss=5.251945
I0607 03:11:01.622484 140004808804160 submission.py:296] 1500) loss = 5.252, grad_norm = 8.579
I0607 03:14:13.283702 139952146855680 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.899269, loss=4.881648
I0607 03:14:13.288154 140004808804160 submission.py:296] 2000) loss = 4.882, grad_norm = 3.899
I0607 03:17:26.352278 139952155248384 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.524419, loss=4.555481
I0607 03:17:26.356107 140004808804160 submission.py:296] 2500) loss = 4.555, grad_norm = 4.524
I0607 03:18:18.850203 140004808804160 spec.py:298] Evaluating on the training split.
I0607 03:19:05.262419 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 03:20:05.332504 140004808804160 spec.py:326] Evaluating on the test split.
I0607 03:20:06.743340 140004808804160 submission_runner.py:419] Time since start: 1368.64s, 	Step: 2634, 	{'train/accuracy': 0.2629544005102041, 'train/loss': 3.6012663549306443, 'validation/accuracy': 0.23842, 'validation/loss': 3.7587809375, 'validation/num_examples': 50000, 'test/accuracy': 0.1714, 'test/loss': 4.342373046875, 'test/num_examples': 10000, 'score': 1027.1198320388794, 'total_duration': 1368.6354155540466, 'accumulated_submission_time': 1027.1198320388794, 'accumulated_eval_time': 340.2854278087616, 'accumulated_logging_time': 0.04519534111022949}
I0607 03:20:06.753031 139952146855680 logging_writer.py:48] [2634] accumulated_eval_time=340.285428, accumulated_logging_time=0.045195, accumulated_submission_time=1027.119832, global_step=2634, preemption_count=0, score=1027.119832, test/accuracy=0.171400, test/loss=4.342373, test/num_examples=10000, total_duration=1368.635416, train/accuracy=0.262954, train/loss=3.601266, validation/accuracy=0.238420, validation/loss=3.758781, validation/num_examples=50000
I0607 03:22:27.399772 139952155248384 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.067573, loss=4.275301
I0607 03:22:27.404183 140004808804160 submission.py:296] 3000) loss = 4.275, grad_norm = 4.068
I0607 03:25:39.393607 139952146855680 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.502234, loss=3.939518
I0607 03:25:39.397810 140004808804160 submission.py:296] 3500) loss = 3.940, grad_norm = 5.502
I0607 03:28:36.944013 140004808804160 spec.py:298] Evaluating on the training split.
I0607 03:29:18.482687 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 03:30:02.172727 140004808804160 spec.py:326] Evaluating on the test split.
I0607 03:30:03.581939 140004808804160 submission_runner.py:419] Time since start: 1965.48s, 	Step: 3959, 	{'train/accuracy': 0.4006297831632653, 'train/loss': 2.782810133330676, 'validation/accuracy': 0.36214, 'validation/loss': 2.9776453125, 'validation/num_examples': 50000, 'test/accuracy': 0.2636, 'test/loss': 3.690039453125, 'test/num_examples': 10000, 'score': 1536.755285024643, 'total_duration': 1965.4753835201263, 'accumulated_submission_time': 1536.755285024643, 'accumulated_eval_time': 426.9232590198517, 'accumulated_logging_time': 0.06279301643371582}
I0607 03:30:03.593075 139952155248384 logging_writer.py:48] [3959] accumulated_eval_time=426.923259, accumulated_logging_time=0.062793, accumulated_submission_time=1536.755285, global_step=3959, preemption_count=0, score=1536.755285, test/accuracy=0.263600, test/loss=3.690039, test/num_examples=10000, total_duration=1965.475384, train/accuracy=0.400630, train/loss=2.782810, validation/accuracy=0.362140, validation/loss=2.977645, validation/num_examples=50000
I0607 03:30:19.603364 139952146855680 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.255476, loss=3.910718
I0607 03:30:19.607523 140004808804160 submission.py:296] 4000) loss = 3.911, grad_norm = 3.255
I0607 03:33:31.179952 139952155248384 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.361075, loss=3.659094
I0607 03:33:31.185190 140004808804160 submission.py:296] 4500) loss = 3.659, grad_norm = 3.361
I0607 03:36:44.442245 139952146855680 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.878998, loss=3.531237
I0607 03:36:44.446995 140004808804160 submission.py:296] 5000) loss = 3.531, grad_norm = 3.879
I0607 03:38:33.670389 140004808804160 spec.py:298] Evaluating on the training split.
I0607 03:39:19.425393 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 03:40:08.365370 140004808804160 spec.py:326] Evaluating on the test split.
I0607 03:40:09.773245 140004808804160 submission_runner.py:419] Time since start: 2571.66s, 	Step: 5282, 	{'train/accuracy': 0.47415098852040816, 'train/loss': 2.389582108478157, 'validation/accuracy': 0.4335, 'validation/loss': 2.60697359375, 'validation/num_examples': 50000, 'test/accuracy': 0.3211, 'test/loss': 3.34855390625, 'test/num_examples': 10000, 'score': 2046.2643127441406, 'total_duration': 2571.6646733283997, 'accumulated_submission_time': 2046.2643127441406, 'accumulated_eval_time': 523.0240201950073, 'accumulated_logging_time': 0.08153700828552246}
I0607 03:40:09.782939 139952155248384 logging_writer.py:48] [5282] accumulated_eval_time=523.024020, accumulated_logging_time=0.081537, accumulated_submission_time=2046.264313, global_step=5282, preemption_count=0, score=2046.264313, test/accuracy=0.321100, test/loss=3.348554, test/num_examples=10000, total_duration=2571.664673, train/accuracy=0.474151, train/loss=2.389582, validation/accuracy=0.433500, validation/loss=2.606974, validation/num_examples=50000
I0607 03:41:33.621290 139952146855680 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.887746, loss=3.523073
I0607 03:41:33.625725 140004808804160 submission.py:296] 5500) loss = 3.523, grad_norm = 5.888
I0607 03:44:45.787531 139952155248384 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.654444, loss=3.366701
I0607 03:44:45.791716 140004808804160 submission.py:296] 6000) loss = 3.367, grad_norm = 4.654
I0607 03:48:00.091661 139952146855680 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.715939, loss=3.232514
I0607 03:48:00.097439 140004808804160 submission.py:296] 6500) loss = 3.233, grad_norm = 2.716
I0607 03:48:39.978980 140004808804160 spec.py:298] Evaluating on the training split.
I0607 03:49:22.408503 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 03:50:17.298095 140004808804160 spec.py:326] Evaluating on the test split.
I0607 03:50:18.705388 140004808804160 submission_runner.py:419] Time since start: 3180.60s, 	Step: 6605, 	{'train/accuracy': 0.5466757015306123, 'train/loss': 2.049748946209343, 'validation/accuracy': 0.4983, 'validation/loss': 2.28106578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3701, 'test/loss': 3.013934765625, 'test/num_examples': 10000, 'score': 2555.89058303833, 'total_duration': 3180.5988535881042, 'accumulated_submission_time': 2555.89058303833, 'accumulated_eval_time': 621.7503380775452, 'accumulated_logging_time': 0.09926700592041016}
I0607 03:50:18.716245 139952155248384 logging_writer.py:48] [6605] accumulated_eval_time=621.750338, accumulated_logging_time=0.099267, accumulated_submission_time=2555.890583, global_step=6605, preemption_count=0, score=2555.890583, test/accuracy=0.370100, test/loss=3.013935, test/num_examples=10000, total_duration=3180.598854, train/accuracy=0.546676, train/loss=2.049749, validation/accuracy=0.498300, validation/loss=2.281066, validation/num_examples=50000
I0607 03:52:50.459980 139952146855680 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.631413, loss=3.220990
I0607 03:52:50.464085 140004808804160 submission.py:296] 7000) loss = 3.221, grad_norm = 4.631
I0607 03:56:03.433630 139952155248384 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.540141, loss=3.145513
I0607 03:56:03.438199 140004808804160 submission.py:296] 7500) loss = 3.146, grad_norm = 2.540
I0607 03:58:48.960715 140004808804160 spec.py:298] Evaluating on the training split.
I0607 03:59:35.496174 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 04:00:24.471605 140004808804160 spec.py:326] Evaluating on the test split.
I0607 04:00:25.878681 140004808804160 submission_runner.py:419] Time since start: 3787.77s, 	Step: 7929, 	{'train/accuracy': 0.5787228954081632, 'train/loss': 1.844898457429847, 'validation/accuracy': 0.52854, 'validation/loss': 2.09928328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4002, 'test/loss': 2.860564453125, 'test/num_examples': 10000, 'score': 3065.562953710556, 'total_duration': 3787.7710270881653, 'accumulated_submission_time': 3065.562953710556, 'accumulated_eval_time': 718.6671142578125, 'accumulated_logging_time': 0.11838626861572266}
I0607 04:00:25.890038 139952146855680 logging_writer.py:48] [7929] accumulated_eval_time=718.667114, accumulated_logging_time=0.118386, accumulated_submission_time=3065.562954, global_step=7929, preemption_count=0, score=3065.562954, test/accuracy=0.400200, test/loss=2.860564, test/num_examples=10000, total_duration=3787.771027, train/accuracy=0.578723, train/loss=1.844898, validation/accuracy=0.528540, validation/loss=2.099283, validation/num_examples=50000
I0607 04:00:53.379374 139952155248384 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.459275, loss=3.104307
I0607 04:00:53.383400 140004808804160 submission.py:296] 8000) loss = 3.104, grad_norm = 2.459
I0607 04:04:05.536695 139952146855680 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.276603, loss=3.085809
I0607 04:04:05.540613 140004808804160 submission.py:296] 8500) loss = 3.086, grad_norm = 2.277
I0607 04:07:19.396714 139952155248384 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.283401, loss=3.052318
I0607 04:07:19.401762 140004808804160 submission.py:296] 9000) loss = 3.052, grad_norm = 3.283
I0607 04:08:56.030741 140004808804160 spec.py:298] Evaluating on the training split.
I0607 04:09:39.982283 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 04:10:34.117354 140004808804160 spec.py:326] Evaluating on the test split.
I0607 04:10:35.522746 140004808804160 submission_runner.py:419] Time since start: 4397.42s, 	Step: 9253, 	{'train/accuracy': 0.6120655293367347, 'train/loss': 1.7327216012137276, 'validation/accuracy': 0.55562, 'validation/loss': 1.99900765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4224, 'test/loss': 2.7563958984375, 'test/num_examples': 10000, 'score': 3575.133541584015, 'total_duration': 4397.416157960892, 'accumulated_submission_time': 3575.133541584015, 'accumulated_eval_time': 818.1590366363525, 'accumulated_logging_time': 0.13785719871520996}
I0607 04:10:35.535284 139952146855680 logging_writer.py:48] [9253] accumulated_eval_time=818.159037, accumulated_logging_time=0.137857, accumulated_submission_time=3575.133542, global_step=9253, preemption_count=0, score=3575.133542, test/accuracy=0.422400, test/loss=2.756396, test/num_examples=10000, total_duration=4397.416158, train/accuracy=0.612066, train/loss=1.732722, validation/accuracy=0.555620, validation/loss=1.999008, validation/num_examples=50000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0607 04:12:10.487197 139952155248384 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.072570, loss=3.028270
I0607 04:12:10.491169 140004808804160 submission.py:296] 9500) loss = 3.028, grad_norm = 2.073
I0607 04:15:23.447622 139952146855680 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.674242, loss=2.887837
I0607 04:15:23.452789 140004808804160 submission.py:296] 10000) loss = 2.888, grad_norm = 1.674
I0607 04:18:36.326470 139952155248384 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.540522, loss=2.959070
I0607 04:18:36.331269 140004808804160 submission.py:296] 10500) loss = 2.959, grad_norm = 2.541
I0607 04:19:05.887184 140004808804160 spec.py:298] Evaluating on the training split.
I0607 04:19:50.359474 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 04:20:41.120456 140004808804160 spec.py:326] Evaluating on the test split.
I0607 04:20:42.525451 140004808804160 submission_runner.py:419] Time since start: 5004.42s, 	Step: 10578, 	{'train/accuracy': 0.6469626913265306, 'train/loss': 1.5442599860989317, 'validation/accuracy': 0.58622, 'validation/loss': 1.8415790625, 'validation/num_examples': 50000, 'test/accuracy': 0.4501, 'test/loss': 2.57535, 'test/num_examples': 10000, 'score': 4084.9152443408966, 'total_duration': 5004.417831897736, 'accumulated_submission_time': 4084.9152443408966, 'accumulated_eval_time': 914.796186208725, 'accumulated_logging_time': 0.15862464904785156}
I0607 04:20:42.535015 139952146855680 logging_writer.py:48] [10578] accumulated_eval_time=914.796186, accumulated_logging_time=0.158625, accumulated_submission_time=4084.915244, global_step=10578, preemption_count=0, score=4084.915244, test/accuracy=0.450100, test/loss=2.575350, test/num_examples=10000, total_duration=5004.417832, train/accuracy=0.646963, train/loss=1.544260, validation/accuracy=0.586220, validation/loss=1.841579, validation/num_examples=50000
I0607 04:23:24.898709 139952155248384 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.864411, loss=2.772257
I0607 04:23:24.903532 140004808804160 submission.py:296] 11000) loss = 2.772, grad_norm = 1.864
I0607 04:26:39.048168 139952146855680 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.956545, loss=2.653007
I0607 04:26:39.053638 140004808804160 submission.py:296] 11500) loss = 2.653, grad_norm = 1.957
I0607 04:29:12.667794 140004808804160 spec.py:298] Evaluating on the training split.
I0607 04:29:56.063749 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 04:30:41.625780 140004808804160 spec.py:326] Evaluating on the test split.
I0607 04:30:43.026615 140004808804160 submission_runner.py:419] Time since start: 5604.92s, 	Step: 11902, 	{'train/accuracy': 0.6732900191326531, 'train/loss': 1.4505661944953763, 'validation/accuracy': 0.6027, 'validation/loss': 1.77018703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4689, 'test/loss': 2.5045111328125, 'test/num_examples': 10000, 'score': 4594.4837164878845, 'total_duration': 5604.920085906982, 'accumulated_submission_time': 4594.4837164878845, 'accumulated_eval_time': 1005.1549050807953, 'accumulated_logging_time': 0.1758861541748047}
I0607 04:30:43.037604 139952155248384 logging_writer.py:48] [11902] accumulated_eval_time=1005.154905, accumulated_logging_time=0.175886, accumulated_submission_time=4594.483716, global_step=11902, preemption_count=0, score=4594.483716, test/accuracy=0.468900, test/loss=2.504511, test/num_examples=10000, total_duration=5604.920086, train/accuracy=0.673290, train/loss=1.450566, validation/accuracy=0.602700, validation/loss=1.770187, validation/num_examples=50000
I0607 04:31:20.849744 139952146855680 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.987163, loss=2.708109
I0607 04:31:20.853621 140004808804160 submission.py:296] 12000) loss = 2.708, grad_norm = 1.987
I0607 04:34:33.624819 139952155248384 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.919551, loss=2.785392
I0607 04:34:33.630671 140004808804160 submission.py:296] 12500) loss = 2.785, grad_norm = 1.920
I0607 04:37:46.739911 139952146855680 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.068957, loss=2.706532
I0607 04:37:46.744006 140004808804160 submission.py:296] 13000) loss = 2.707, grad_norm = 2.069
I0607 04:39:13.394062 140004808804160 spec.py:298] Evaluating on the training split.
I0607 04:39:57.090749 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 04:40:41.779633 140004808804160 spec.py:326] Evaluating on the test split.
I0607 04:40:43.185626 140004808804160 submission_runner.py:419] Time since start: 6205.08s, 	Step: 13227, 	{'train/accuracy': 0.6868223852040817, 'train/loss': 1.3832058030731824, 'validation/accuracy': 0.61468, 'validation/loss': 1.72589328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4778, 'test/loss': 2.481524609375, 'test/num_examples': 10000, 'score': 5104.2706599235535, 'total_duration': 6205.079120397568, 'accumulated_submission_time': 5104.2706599235535, 'accumulated_eval_time': 1094.946460723877, 'accumulated_logging_time': 0.19472670555114746}
I0607 04:40:43.196036 139952155248384 logging_writer.py:48] [13227] accumulated_eval_time=1094.946461, accumulated_logging_time=0.194727, accumulated_submission_time=5104.270660, global_step=13227, preemption_count=0, score=5104.270660, test/accuracy=0.477800, test/loss=2.481525, test/num_examples=10000, total_duration=6205.079120, train/accuracy=0.686822, train/loss=1.383206, validation/accuracy=0.614680, validation/loss=1.725893, validation/num_examples=50000
I0607 04:42:28.360199 139952146855680 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.230365, loss=2.630401
I0607 04:42:28.364259 140004808804160 submission.py:296] 13500) loss = 2.630, grad_norm = 1.230
I0607 04:45:42.520642 139952155248384 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.822974, loss=2.639102
I0607 04:45:42.526188 140004808804160 submission.py:296] 14000) loss = 2.639, grad_norm = 1.823
I0607 04:48:54.162993 139952146855680 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.464520, loss=2.628562
I0607 04:48:54.167036 140004808804160 submission.py:296] 14500) loss = 2.629, grad_norm = 1.465
I0607 04:49:13.400537 140004808804160 spec.py:298] Evaluating on the training split.
I0607 04:49:56.389933 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 04:50:41.017594 140004808804160 spec.py:326] Evaluating on the test split.
I0607 04:50:42.421425 140004808804160 submission_runner.py:419] Time since start: 6804.31s, 	Step: 14551, 	{'train/accuracy': 0.7127909757653061, 'train/loss': 1.287602950115593, 'validation/accuracy': 0.6338, 'validation/loss': 1.64525609375, 'validation/num_examples': 50000, 'test/accuracy': 0.4959, 'test/loss': 2.3677927734375, 'test/num_examples': 10000, 'score': 5613.90745973587, 'total_duration': 6804.314833641052, 'accumulated_submission_time': 5613.90745973587, 'accumulated_eval_time': 1183.9671936035156, 'accumulated_logging_time': 0.21288537979125977}
I0607 04:50:42.432686 139952155248384 logging_writer.py:48] [14551] accumulated_eval_time=1183.967194, accumulated_logging_time=0.212885, accumulated_submission_time=5613.907460, global_step=14551, preemption_count=0, score=5613.907460, test/accuracy=0.495900, test/loss=2.367793, test/num_examples=10000, total_duration=6804.314834, train/accuracy=0.712791, train/loss=1.287603, validation/accuracy=0.633800, validation/loss=1.645256, validation/num_examples=50000
I0607 04:53:36.137608 139952146855680 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.984255, loss=2.613052
I0607 04:53:36.143831 140004808804160 submission.py:296] 15000) loss = 2.613, grad_norm = 0.984
I0607 04:56:49.616860 139952155248384 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.924315, loss=2.512868
I0607 04:56:49.621407 140004808804160 submission.py:296] 15500) loss = 2.513, grad_norm = 0.924
I0607 04:59:12.628391 140004808804160 spec.py:298] Evaluating on the training split.
I0607 04:59:55.438815 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 05:00:40.445884 140004808804160 spec.py:326] Evaluating on the test split.
I0607 05:00:41.847165 140004808804160 submission_runner.py:419] Time since start: 7403.74s, 	Step: 15874, 	{'train/accuracy': 0.7256656568877551, 'train/loss': 1.1978219479930645, 'validation/accuracy': 0.6423, 'validation/loss': 1.5761996875, 'validation/num_examples': 50000, 'test/accuracy': 0.5033, 'test/loss': 2.2986087890625, 'test/num_examples': 10000, 'score': 6123.534600019455, 'total_duration': 7403.740636348724, 'accumulated_submission_time': 6123.534600019455, 'accumulated_eval_time': 1273.1859765052795, 'accumulated_logging_time': 0.2324082851409912}
I0607 05:00:41.857035 139952146855680 logging_writer.py:48] [15874] accumulated_eval_time=1273.185977, accumulated_logging_time=0.232408, accumulated_submission_time=6123.534600, global_step=15874, preemption_count=0, score=6123.534600, test/accuracy=0.503300, test/loss=2.298609, test/num_examples=10000, total_duration=7403.740636, train/accuracy=0.725666, train/loss=1.197822, validation/accuracy=0.642300, validation/loss=1.576200, validation/num_examples=50000
I0607 05:01:30.759732 139952155248384 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.112875, loss=2.572052
I0607 05:01:30.764191 140004808804160 submission.py:296] 16000) loss = 2.572, grad_norm = 1.113
I0607 05:04:44.846979 139952146855680 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.013023, loss=2.544930
I0607 05:04:44.851146 140004808804160 submission.py:296] 16500) loss = 2.545, grad_norm = 1.013
I0607 05:07:56.540602 139952155248384 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.094878, loss=2.558614
I0607 05:07:56.545378 140004808804160 submission.py:296] 17000) loss = 2.559, grad_norm = 1.095
I0607 05:09:12.007395 140004808804160 spec.py:298] Evaluating on the training split.
I0607 05:09:55.515884 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 05:10:46.222447 140004808804160 spec.py:326] Evaluating on the test split.
I0607 05:10:47.625179 140004808804160 submission_runner.py:419] Time since start: 8009.52s, 	Step: 17197, 	{'train/accuracy': 0.7350326849489796, 'train/loss': 1.1646810259137834, 'validation/accuracy': 0.64952, 'validation/loss': 1.553265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5028, 'test/loss': 2.2891421875, 'test/num_examples': 10000, 'score': 6633.109671592712, 'total_duration': 8009.518573284149, 'accumulated_submission_time': 6633.109671592712, 'accumulated_eval_time': 1368.8037421703339, 'accumulated_logging_time': 0.2500033378601074}
I0607 05:10:47.635534 139952146855680 logging_writer.py:48] [17197] accumulated_eval_time=1368.803742, accumulated_logging_time=0.250003, accumulated_submission_time=6633.109672, global_step=17197, preemption_count=0, score=6633.109672, test/accuracy=0.502800, test/loss=2.289142, test/num_examples=10000, total_duration=8009.518573, train/accuracy=0.735033, train/loss=1.164681, validation/accuracy=0.649520, validation/loss=1.553266, validation/num_examples=50000
I0607 05:12:45.187813 139952155248384 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.183812, loss=2.477701
I0607 05:12:45.192664 140004808804160 submission.py:296] 17500) loss = 2.478, grad_norm = 1.184
I0607 05:15:58.270298 139952146855680 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.941503, loss=2.485893
I0607 05:15:58.275146 140004808804160 submission.py:296] 18000) loss = 2.486, grad_norm = 0.942
I0607 05:19:10.282859 139952155248384 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.876966, loss=2.592293
I0607 05:19:10.290958 140004808804160 submission.py:296] 18500) loss = 2.592, grad_norm = 0.877
I0607 05:19:17.659590 140004808804160 spec.py:298] Evaluating on the training split.
I0607 05:20:02.903866 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 05:20:52.196955 140004808804160 spec.py:326] Evaluating on the test split.
I0607 05:20:53.603717 140004808804160 submission_runner.py:419] Time since start: 8615.50s, 	Step: 18520, 	{'train/accuracy': 0.7452168367346939, 'train/loss': 1.1101100688077965, 'validation/accuracy': 0.65614, 'validation/loss': 1.50616921875, 'validation/num_examples': 50000, 'test/accuracy': 0.5138, 'test/loss': 2.2492470703125, 'test/num_examples': 10000, 'score': 7142.562040805817, 'total_duration': 8615.495393037796, 'accumulated_submission_time': 7142.562040805817, 'accumulated_eval_time': 1464.7460627555847, 'accumulated_logging_time': 0.2690119743347168}
I0607 05:20:53.616079 139952146855680 logging_writer.py:48] [18520] accumulated_eval_time=1464.746063, accumulated_logging_time=0.269012, accumulated_submission_time=7142.562041, global_step=18520, preemption_count=0, score=7142.562041, test/accuracy=0.513800, test/loss=2.249247, test/num_examples=10000, total_duration=8615.495393, train/accuracy=0.745217, train/loss=1.110110, validation/accuracy=0.656140, validation/loss=1.506169, validation/num_examples=50000
I0607 05:24:00.644428 139952155248384 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.024531, loss=2.395062
I0607 05:24:00.649405 140004808804160 submission.py:296] 19000) loss = 2.395, grad_norm = 1.025
I0607 05:27:12.287664 139952146855680 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.890713, loss=2.443282
I0607 05:27:12.291928 140004808804160 submission.py:296] 19500) loss = 2.443, grad_norm = 0.891
I0607 05:29:23.679282 140004808804160 spec.py:298] Evaluating on the training split.
I0607 05:30:07.035692 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 05:31:04.278510 140004808804160 spec.py:326] Evaluating on the test split.
I0607 05:31:05.682228 140004808804160 submission_runner.py:419] Time since start: 9227.58s, 	Step: 19842, 	{'train/accuracy': 0.7525908801020408, 'train/loss': 1.0787965424206791, 'validation/accuracy': 0.65958, 'validation/loss': 1.496446875, 'validation/num_examples': 50000, 'test/accuracy': 0.513, 'test/loss': 2.2582080078125, 'test/num_examples': 10000, 'score': 7652.047701120377, 'total_duration': 9227.575708389282, 'accumulated_submission_time': 7652.047701120377, 'accumulated_eval_time': 1566.7490181922913, 'accumulated_logging_time': 0.2898693084716797}
I0607 05:31:05.692532 139952155248384 logging_writer.py:48] [19842] accumulated_eval_time=1566.749018, accumulated_logging_time=0.289869, accumulated_submission_time=7652.047701, global_step=19842, preemption_count=0, score=7652.047701, test/accuracy=0.513000, test/loss=2.258208, test/num_examples=10000, total_duration=9227.575708, train/accuracy=0.752591, train/loss=1.078797, validation/accuracy=0.659580, validation/loss=1.496447, validation/num_examples=50000
I0607 05:32:07.002404 139952146855680 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.023282, loss=2.467907
I0607 05:32:07.007315 140004808804160 submission.py:296] 20000) loss = 2.468, grad_norm = 1.023
I0607 05:35:19.970035 139952155248384 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.663502, loss=2.488290
I0607 05:35:19.975727 140004808804160 submission.py:296] 20500) loss = 2.488, grad_norm = 0.664
I0607 05:38:32.012958 139952146855680 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.913765, loss=2.437235
I0607 05:38:32.021805 140004808804160 submission.py:296] 21000) loss = 2.437, grad_norm = 0.914
I0607 05:39:35.794084 140004808804160 spec.py:298] Evaluating on the training split.
I0607 05:40:19.407819 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 05:41:09.585628 140004808804160 spec.py:326] Evaluating on the test split.
I0607 05:41:10.991941 140004808804160 submission_runner.py:419] Time since start: 9832.88s, 	Step: 21166, 	{'train/accuracy': 0.7594666772959183, 'train/loss': 1.0697390497947226, 'validation/accuracy': 0.66086, 'validation/loss': 1.50130328125, 'validation/num_examples': 50000, 'test/accuracy': 0.5243, 'test/loss': 2.232816015625, 'test/num_examples': 10000, 'score': 8161.5731201171875, 'total_duration': 9832.883700847626, 'accumulated_submission_time': 8161.5731201171875, 'accumulated_eval_time': 1661.9452939033508, 'accumulated_logging_time': 0.30889272689819336}
I0607 05:41:11.005262 139952155248384 logging_writer.py:48] [21166] accumulated_eval_time=1661.945294, accumulated_logging_time=0.308893, accumulated_submission_time=8161.573120, global_step=21166, preemption_count=0, score=8161.573120, test/accuracy=0.524300, test/loss=2.232816, test/num_examples=10000, total_duration=9832.883701, train/accuracy=0.759467, train/loss=1.069739, validation/accuracy=0.660860, validation/loss=1.501303, validation/num_examples=50000
I0607 05:43:21.300098 139952146855680 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.803025, loss=2.387877
I0607 05:43:21.304136 140004808804160 submission.py:296] 21500) loss = 2.388, grad_norm = 0.803
I0607 05:46:32.819375 139952155248384 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.091252, loss=2.513168
I0607 05:46:32.824540 140004808804160 submission.py:296] 22000) loss = 2.513, grad_norm = 1.091
I0607 05:49:41.011199 140004808804160 spec.py:298] Evaluating on the training split.
I0607 05:50:24.489055 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 05:51:20.814686 140004808804160 spec.py:326] Evaluating on the test split.
I0607 05:51:22.220654 140004808804160 submission_runner.py:419] Time since start: 10444.11s, 	Step: 22489, 	{'train/accuracy': 0.7742745535714286, 'train/loss': 0.995418237180126, 'validation/accuracy': 0.67734, 'validation/loss': 1.4263675, 'validation/num_examples': 50000, 'test/accuracy': 0.5431, 'test/loss': 2.147154296875, 'test/num_examples': 10000, 'score': 8671.006883382797, 'total_duration': 10444.114087343216, 'accumulated_submission_time': 8671.006883382797, 'accumulated_eval_time': 1763.1547322273254, 'accumulated_logging_time': 0.32997822761535645}
I0607 05:51:22.238846 139952146855680 logging_writer.py:48] [22489] accumulated_eval_time=1763.154732, accumulated_logging_time=0.329978, accumulated_submission_time=8671.006883, global_step=22489, preemption_count=0, score=8671.006883, test/accuracy=0.543100, test/loss=2.147154, test/num_examples=10000, total_duration=10444.114087, train/accuracy=0.774275, train/loss=0.995418, validation/accuracy=0.677340, validation/loss=1.426368, validation/num_examples=50000
I0607 05:51:26.851483 139952155248384 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.828193, loss=2.418215
I0607 05:51:26.856317 140004808804160 submission.py:296] 22500) loss = 2.418, grad_norm = 0.828
I0607 05:54:39.982496 139952146855680 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.711054, loss=2.385941
I0607 05:54:39.986406 140004808804160 submission.py:296] 23000) loss = 2.386, grad_norm = 0.711
I0607 05:57:51.812661 139952155248384 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.779679, loss=2.312000
I0607 05:57:51.817867 140004808804160 submission.py:296] 23500) loss = 2.312, grad_norm = 0.780
I0607 05:59:52.567136 140004808804160 spec.py:298] Evaluating on the training split.
I0607 06:00:36.186912 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 06:01:36.234299 140004808804160 spec.py:326] Evaluating on the test split.
I0607 06:01:37.661138 140004808804160 submission_runner.py:419] Time since start: 11059.55s, 	Step: 23810, 	{'train/accuracy': 0.7763273278061225, 'train/loss': 1.0029028289172115, 'validation/accuracy': 0.66942, 'validation/loss': 1.46424078125, 'validation/num_examples': 50000, 'test/accuracy': 0.5335, 'test/loss': 2.1788943359375, 'test/num_examples': 10000, 'score': 9180.76886677742, 'total_duration': 11059.552989959717, 'accumulated_submission_time': 9180.76886677742, 'accumulated_eval_time': 1868.2471170425415, 'accumulated_logging_time': 0.35728907585144043}
I0607 06:01:37.674727 139952146855680 logging_writer.py:48] [23810] accumulated_eval_time=1868.247117, accumulated_logging_time=0.357289, accumulated_submission_time=9180.768867, global_step=23810, preemption_count=0, score=9180.768867, test/accuracy=0.533500, test/loss=2.178894, test/num_examples=10000, total_duration=11059.552990, train/accuracy=0.776327, train/loss=1.002903, validation/accuracy=0.669420, validation/loss=1.464241, validation/num_examples=50000
I0607 06:02:50.756538 139952155248384 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.775270, loss=2.338330
I0607 06:02:50.760560 140004808804160 submission.py:296] 24000) loss = 2.338, grad_norm = 0.775
I0607 06:06:02.187334 139952146855680 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.625239, loss=2.315624
I0607 06:06:02.192158 140004808804160 submission.py:296] 24500) loss = 2.316, grad_norm = 0.625
I0607 06:09:15.226410 139952155248384 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.853011, loss=2.312590
I0607 06:09:15.232872 140004808804160 submission.py:296] 25000) loss = 2.313, grad_norm = 0.853
I0607 06:10:07.728300 140004808804160 spec.py:298] Evaluating on the training split.
I0607 06:10:51.277461 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 06:11:50.742281 140004808804160 spec.py:326] Evaluating on the test split.
I0607 06:11:52.145996 140004808804160 submission_runner.py:419] Time since start: 11674.04s, 	Step: 25134, 	{'train/accuracy': 0.7883250956632653, 'train/loss': 0.9375830669792331, 'validation/accuracy': 0.6781, 'validation/loss': 1.4102946875, 'validation/num_examples': 50000, 'test/accuracy': 0.5439, 'test/loss': 2.1467986328125, 'test/num_examples': 10000, 'score': 9690.249232769012, 'total_duration': 11674.039400100708, 'accumulated_submission_time': 9690.249232769012, 'accumulated_eval_time': 1972.6647806167603, 'accumulated_logging_time': 0.3797757625579834}
I0607 06:11:52.157173 139952146855680 logging_writer.py:48] [25134] accumulated_eval_time=1972.664781, accumulated_logging_time=0.379776, accumulated_submission_time=9690.249233, global_step=25134, preemption_count=0, score=9690.249233, test/accuracy=0.543900, test/loss=2.146799, test/num_examples=10000, total_duration=11674.039400, train/accuracy=0.788325, train/loss=0.937583, validation/accuracy=0.678100, validation/loss=1.410295, validation/num_examples=50000
I0607 06:14:12.467225 139952155248384 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.749695, loss=2.446820
I0607 06:14:12.471136 140004808804160 submission.py:296] 25500) loss = 2.447, grad_norm = 0.750
I0607 06:17:24.524691 139952146855680 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.761551, loss=2.279017
I0607 06:17:24.530782 140004808804160 submission.py:296] 26000) loss = 2.279, grad_norm = 0.762
I0607 06:20:22.183959 140004808804160 spec.py:298] Evaluating on the training split.
I0607 06:21:06.549398 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 06:21:57.076578 140004808804160 spec.py:326] Evaluating on the test split.
I0607 06:21:58.481267 140004808804160 submission_runner.py:419] Time since start: 12280.37s, 	Step: 26459, 	{'train/accuracy': 0.7884048150510204, 'train/loss': 0.9385730976961097, 'validation/accuracy': 0.67868, 'validation/loss': 1.42383296875, 'validation/num_examples': 50000, 'test/accuracy': 0.5399, 'test/loss': 2.1403703125, 'test/num_examples': 10000, 'score': 10199.70736861229, 'total_duration': 12280.37341594696, 'accumulated_submission_time': 10199.70736861229, 'accumulated_eval_time': 2068.960671901703, 'accumulated_logging_time': 0.39928317070007324}
I0607 06:21:58.491817 139952155248384 logging_writer.py:48] [26459] accumulated_eval_time=2068.960672, accumulated_logging_time=0.399283, accumulated_submission_time=10199.707369, global_step=26459, preemption_count=0, score=10199.707369, test/accuracy=0.539900, test/loss=2.140370, test/num_examples=10000, total_duration=12280.373416, train/accuracy=0.788405, train/loss=0.938573, validation/accuracy=0.678680, validation/loss=1.423833, validation/num_examples=50000
I0607 06:22:14.519447 139952146855680 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.750960, loss=2.400069
I0607 06:22:14.523030 140004808804160 submission.py:296] 26500) loss = 2.400, grad_norm = 0.751
I0607 06:25:26.034829 139952155248384 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.760709, loss=2.324776
I0607 06:25:26.038960 140004808804160 submission.py:296] 27000) loss = 2.325, grad_norm = 0.761
I0607 06:28:39.339047 139952146855680 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.680018, loss=2.270963
I0607 06:28:39.345195 140004808804160 submission.py:296] 27500) loss = 2.271, grad_norm = 0.680
I0607 06:30:28.812585 140004808804160 spec.py:298] Evaluating on the training split.
I0607 06:31:14.194568 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 06:32:08.975450 140004808804160 spec.py:326] Evaluating on the test split.
I0607 06:32:10.382777 140004808804160 submission_runner.py:419] Time since start: 12892.28s, 	Step: 27782, 	{'train/accuracy': 0.7997249681122449, 'train/loss': 0.8736061563297194, 'validation/accuracy': 0.68378, 'validation/loss': 1.375431875, 'validation/num_examples': 50000, 'test/accuracy': 0.545, 'test/loss': 2.1234048828125, 'test/num_examples': 10000, 'score': 10709.447305440903, 'total_duration': 12892.276243925095, 'accumulated_submission_time': 10709.447305440903, 'accumulated_eval_time': 2170.530772924423, 'accumulated_logging_time': 0.4194660186767578}
I0607 06:32:10.394334 139952155248384 logging_writer.py:48] [27782] accumulated_eval_time=2170.530773, accumulated_logging_time=0.419466, accumulated_submission_time=10709.447305, global_step=27782, preemption_count=0, score=10709.447305, test/accuracy=0.545000, test/loss=2.123405, test/num_examples=10000, total_duration=12892.276244, train/accuracy=0.799725, train/loss=0.873606, validation/accuracy=0.683780, validation/loss=1.375432, validation/num_examples=50000
I0607 06:33:33.946829 140004808804160 spec.py:298] Evaluating on the training split.
I0607 06:34:16.957156 140004808804160 spec.py:310] Evaluating on the validation split.
I0607 06:35:01.514427 140004808804160 spec.py:326] Evaluating on the test split.
I0607 06:35:02.935428 140004808804160 submission_runner.py:419] Time since start: 13064.83s, 	Step: 28000, 	{'train/accuracy': 0.7911152742346939, 'train/loss': 0.9229784595723055, 'validation/accuracy': 0.6785, 'validation/loss': 1.41717703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5351, 'test/loss': 2.15633828125, 'test/num_examples': 10000, 'score': 10792.901026964188, 'total_duration': 13064.828911542892, 'accumulated_submission_time': 10792.901026964188, 'accumulated_eval_time': 2259.5195004940033, 'accumulated_logging_time': 0.43929290771484375}
I0607 06:35:02.947735 139952146855680 logging_writer.py:48] [28000] accumulated_eval_time=2259.519500, accumulated_logging_time=0.439293, accumulated_submission_time=10792.901027, global_step=28000, preemption_count=0, score=10792.901027, test/accuracy=0.535100, test/loss=2.156338, test/num_examples=10000, total_duration=13064.828912, train/accuracy=0.791115, train/loss=0.922978, validation/accuracy=0.678500, validation/loss=1.417177, validation/num_examples=50000
I0607 06:35:02.964815 139952155248384 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10792.901027
I0607 06:35:03.671192 140004808804160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0607 06:35:03.955679 140004808804160 submission_runner.py:581] Tuning trial 1/1
I0607 06:35:03.955908 140004808804160 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 06:35:03.956998 140004808804160 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011559311224489796, 'train/loss': 6.92365685287787, 'validation/accuracy': 0.00086, 'validation/loss': 6.923295, 'validation/num_examples': 50000, 'test/accuracy': 0.0011, 'test/loss': 6.92518125, 'test/num_examples': 10000, 'score': 8.20128583908081, 'total_duration': 139.6240816116333, 'accumulated_submission_time': 8.20128583908081, 'accumulated_eval_time': 131.42235279083252, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1311, {'train/accuracy': 0.11838329081632654, 'train/loss': 4.771433616171078, 'validation/accuracy': 0.10602, 'validation/loss': 4.8671125, 'validation/num_examples': 50000, 'test/accuracy': 0.0744, 'test/loss': 5.27236796875, 'test/num_examples': 10000, 'score': 517.6582624912262, 'total_duration': 750.7049851417542, 'accumulated_submission_time': 517.6582624912262, 'accumulated_eval_time': 232.39377307891846, 'accumulated_logging_time': 0.026567697525024414, 'global_step': 1311, 'preemption_count': 0}), (2634, {'train/accuracy': 0.2629544005102041, 'train/loss': 3.6012663549306443, 'validation/accuracy': 0.23842, 'validation/loss': 3.7587809375, 'validation/num_examples': 50000, 'test/accuracy': 0.1714, 'test/loss': 4.342373046875, 'test/num_examples': 10000, 'score': 1027.1198320388794, 'total_duration': 1368.6354155540466, 'accumulated_submission_time': 1027.1198320388794, 'accumulated_eval_time': 340.2854278087616, 'accumulated_logging_time': 0.04519534111022949, 'global_step': 2634, 'preemption_count': 0}), (3959, {'train/accuracy': 0.4006297831632653, 'train/loss': 2.782810133330676, 'validation/accuracy': 0.36214, 'validation/loss': 2.9776453125, 'validation/num_examples': 50000, 'test/accuracy': 0.2636, 'test/loss': 3.690039453125, 'test/num_examples': 10000, 'score': 1536.755285024643, 'total_duration': 1965.4753835201263, 'accumulated_submission_time': 1536.755285024643, 'accumulated_eval_time': 426.9232590198517, 'accumulated_logging_time': 0.06279301643371582, 'global_step': 3959, 'preemption_count': 0}), (5282, {'train/accuracy': 0.47415098852040816, 'train/loss': 2.389582108478157, 'validation/accuracy': 0.4335, 'validation/loss': 2.60697359375, 'validation/num_examples': 50000, 'test/accuracy': 0.3211, 'test/loss': 3.34855390625, 'test/num_examples': 10000, 'score': 2046.2643127441406, 'total_duration': 2571.6646733283997, 'accumulated_submission_time': 2046.2643127441406, 'accumulated_eval_time': 523.0240201950073, 'accumulated_logging_time': 0.08153700828552246, 'global_step': 5282, 'preemption_count': 0}), (6605, {'train/accuracy': 0.5466757015306123, 'train/loss': 2.049748946209343, 'validation/accuracy': 0.4983, 'validation/loss': 2.28106578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3701, 'test/loss': 3.013934765625, 'test/num_examples': 10000, 'score': 2555.89058303833, 'total_duration': 3180.5988535881042, 'accumulated_submission_time': 2555.89058303833, 'accumulated_eval_time': 621.7503380775452, 'accumulated_logging_time': 0.09926700592041016, 'global_step': 6605, 'preemption_count': 0}), (7929, {'train/accuracy': 0.5787228954081632, 'train/loss': 1.844898457429847, 'validation/accuracy': 0.52854, 'validation/loss': 2.09928328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4002, 'test/loss': 2.860564453125, 'test/num_examples': 10000, 'score': 3065.562953710556, 'total_duration': 3787.7710270881653, 'accumulated_submission_time': 3065.562953710556, 'accumulated_eval_time': 718.6671142578125, 'accumulated_logging_time': 0.11838626861572266, 'global_step': 7929, 'preemption_count': 0}), (9253, {'train/accuracy': 0.6120655293367347, 'train/loss': 1.7327216012137276, 'validation/accuracy': 0.55562, 'validation/loss': 1.99900765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4224, 'test/loss': 2.7563958984375, 'test/num_examples': 10000, 'score': 3575.133541584015, 'total_duration': 4397.416157960892, 'accumulated_submission_time': 3575.133541584015, 'accumulated_eval_time': 818.1590366363525, 'accumulated_logging_time': 0.13785719871520996, 'global_step': 9253, 'preemption_count': 0}), (10578, {'train/accuracy': 0.6469626913265306, 'train/loss': 1.5442599860989317, 'validation/accuracy': 0.58622, 'validation/loss': 1.8415790625, 'validation/num_examples': 50000, 'test/accuracy': 0.4501, 'test/loss': 2.57535, 'test/num_examples': 10000, 'score': 4084.9152443408966, 'total_duration': 5004.417831897736, 'accumulated_submission_time': 4084.9152443408966, 'accumulated_eval_time': 914.796186208725, 'accumulated_logging_time': 0.15862464904785156, 'global_step': 10578, 'preemption_count': 0}), (11902, {'train/accuracy': 0.6732900191326531, 'train/loss': 1.4505661944953763, 'validation/accuracy': 0.6027, 'validation/loss': 1.77018703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4689, 'test/loss': 2.5045111328125, 'test/num_examples': 10000, 'score': 4594.4837164878845, 'total_duration': 5604.920085906982, 'accumulated_submission_time': 4594.4837164878845, 'accumulated_eval_time': 1005.1549050807953, 'accumulated_logging_time': 0.1758861541748047, 'global_step': 11902, 'preemption_count': 0}), (13227, {'train/accuracy': 0.6868223852040817, 'train/loss': 1.3832058030731824, 'validation/accuracy': 0.61468, 'validation/loss': 1.72589328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4778, 'test/loss': 2.481524609375, 'test/num_examples': 10000, 'score': 5104.2706599235535, 'total_duration': 6205.079120397568, 'accumulated_submission_time': 5104.2706599235535, 'accumulated_eval_time': 1094.946460723877, 'accumulated_logging_time': 0.19472670555114746, 'global_step': 13227, 'preemption_count': 0}), (14551, {'train/accuracy': 0.7127909757653061, 'train/loss': 1.287602950115593, 'validation/accuracy': 0.6338, 'validation/loss': 1.64525609375, 'validation/num_examples': 50000, 'test/accuracy': 0.4959, 'test/loss': 2.3677927734375, 'test/num_examples': 10000, 'score': 5613.90745973587, 'total_duration': 6804.314833641052, 'accumulated_submission_time': 5613.90745973587, 'accumulated_eval_time': 1183.9671936035156, 'accumulated_logging_time': 0.21288537979125977, 'global_step': 14551, 'preemption_count': 0}), (15874, {'train/accuracy': 0.7256656568877551, 'train/loss': 1.1978219479930645, 'validation/accuracy': 0.6423, 'validation/loss': 1.5761996875, 'validation/num_examples': 50000, 'test/accuracy': 0.5033, 'test/loss': 2.2986087890625, 'test/num_examples': 10000, 'score': 6123.534600019455, 'total_duration': 7403.740636348724, 'accumulated_submission_time': 6123.534600019455, 'accumulated_eval_time': 1273.1859765052795, 'accumulated_logging_time': 0.2324082851409912, 'global_step': 15874, 'preemption_count': 0}), (17197, {'train/accuracy': 0.7350326849489796, 'train/loss': 1.1646810259137834, 'validation/accuracy': 0.64952, 'validation/loss': 1.553265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5028, 'test/loss': 2.2891421875, 'test/num_examples': 10000, 'score': 6633.109671592712, 'total_duration': 8009.518573284149, 'accumulated_submission_time': 6633.109671592712, 'accumulated_eval_time': 1368.8037421703339, 'accumulated_logging_time': 0.2500033378601074, 'global_step': 17197, 'preemption_count': 0}), (18520, {'train/accuracy': 0.7452168367346939, 'train/loss': 1.1101100688077965, 'validation/accuracy': 0.65614, 'validation/loss': 1.50616921875, 'validation/num_examples': 50000, 'test/accuracy': 0.5138, 'test/loss': 2.2492470703125, 'test/num_examples': 10000, 'score': 7142.562040805817, 'total_duration': 8615.495393037796, 'accumulated_submission_time': 7142.562040805817, 'accumulated_eval_time': 1464.7460627555847, 'accumulated_logging_time': 0.2690119743347168, 'global_step': 18520, 'preemption_count': 0}), (19842, {'train/accuracy': 0.7525908801020408, 'train/loss': 1.0787965424206791, 'validation/accuracy': 0.65958, 'validation/loss': 1.496446875, 'validation/num_examples': 50000, 'test/accuracy': 0.513, 'test/loss': 2.2582080078125, 'test/num_examples': 10000, 'score': 7652.047701120377, 'total_duration': 9227.575708389282, 'accumulated_submission_time': 7652.047701120377, 'accumulated_eval_time': 1566.7490181922913, 'accumulated_logging_time': 0.2898693084716797, 'global_step': 19842, 'preemption_count': 0}), (21166, {'train/accuracy': 0.7594666772959183, 'train/loss': 1.0697390497947226, 'validation/accuracy': 0.66086, 'validation/loss': 1.50130328125, 'validation/num_examples': 50000, 'test/accuracy': 0.5243, 'test/loss': 2.232816015625, 'test/num_examples': 10000, 'score': 8161.5731201171875, 'total_duration': 9832.883700847626, 'accumulated_submission_time': 8161.5731201171875, 'accumulated_eval_time': 1661.9452939033508, 'accumulated_logging_time': 0.30889272689819336, 'global_step': 21166, 'preemption_count': 0}), (22489, {'train/accuracy': 0.7742745535714286, 'train/loss': 0.995418237180126, 'validation/accuracy': 0.67734, 'validation/loss': 1.4263675, 'validation/num_examples': 50000, 'test/accuracy': 0.5431, 'test/loss': 2.147154296875, 'test/num_examples': 10000, 'score': 8671.006883382797, 'total_duration': 10444.114087343216, 'accumulated_submission_time': 8671.006883382797, 'accumulated_eval_time': 1763.1547322273254, 'accumulated_logging_time': 0.32997822761535645, 'global_step': 22489, 'preemption_count': 0}), (23810, {'train/accuracy': 0.7763273278061225, 'train/loss': 1.0029028289172115, 'validation/accuracy': 0.66942, 'validation/loss': 1.46424078125, 'validation/num_examples': 50000, 'test/accuracy': 0.5335, 'test/loss': 2.1788943359375, 'test/num_examples': 10000, 'score': 9180.76886677742, 'total_duration': 11059.552989959717, 'accumulated_submission_time': 9180.76886677742, 'accumulated_eval_time': 1868.2471170425415, 'accumulated_logging_time': 0.35728907585144043, 'global_step': 23810, 'preemption_count': 0}), (25134, {'train/accuracy': 0.7883250956632653, 'train/loss': 0.9375830669792331, 'validation/accuracy': 0.6781, 'validation/loss': 1.4102946875, 'validation/num_examples': 50000, 'test/accuracy': 0.5439, 'test/loss': 2.1467986328125, 'test/num_examples': 10000, 'score': 9690.249232769012, 'total_duration': 11674.039400100708, 'accumulated_submission_time': 9690.249232769012, 'accumulated_eval_time': 1972.6647806167603, 'accumulated_logging_time': 0.3797757625579834, 'global_step': 25134, 'preemption_count': 0}), (26459, {'train/accuracy': 0.7884048150510204, 'train/loss': 0.9385730976961097, 'validation/accuracy': 0.67868, 'validation/loss': 1.42383296875, 'validation/num_examples': 50000, 'test/accuracy': 0.5399, 'test/loss': 2.1403703125, 'test/num_examples': 10000, 'score': 10199.70736861229, 'total_duration': 12280.37341594696, 'accumulated_submission_time': 10199.70736861229, 'accumulated_eval_time': 2068.960671901703, 'accumulated_logging_time': 0.39928317070007324, 'global_step': 26459, 'preemption_count': 0}), (27782, {'train/accuracy': 0.7997249681122449, 'train/loss': 0.8736061563297194, 'validation/accuracy': 0.68378, 'validation/loss': 1.375431875, 'validation/num_examples': 50000, 'test/accuracy': 0.545, 'test/loss': 2.1234048828125, 'test/num_examples': 10000, 'score': 10709.447305440903, 'total_duration': 12892.276243925095, 'accumulated_submission_time': 10709.447305440903, 'accumulated_eval_time': 2170.530772924423, 'accumulated_logging_time': 0.4194660186767578, 'global_step': 27782, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7911152742346939, 'train/loss': 0.9229784595723055, 'validation/accuracy': 0.6785, 'validation/loss': 1.41717703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5351, 'test/loss': 2.15633828125, 'test/num_examples': 10000, 'score': 10792.901026964188, 'total_duration': 13064.828911542892, 'accumulated_submission_time': 10792.901026964188, 'accumulated_eval_time': 2259.5195004940033, 'accumulated_logging_time': 0.43929290771484375, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0607 06:35:03.957144 140004808804160 submission_runner.py:584] Timing: 10792.901026964188
I0607 06:35:03.957197 140004808804160 submission_runner.py:586] Total number of evals: 23
I0607 06:35:03.957242 140004808804160 submission_runner.py:587] ====================
I0607 06:35:03.957366 140004808804160 submission_runner.py:655] Final imagenet_resnet score: 10792.901026964188
