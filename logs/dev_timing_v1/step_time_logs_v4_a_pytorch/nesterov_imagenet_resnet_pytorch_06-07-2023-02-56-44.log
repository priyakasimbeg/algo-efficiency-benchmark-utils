torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-07-2023-02-56-44.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 02:57:11.975934 140353388304192 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 02:57:11.976005 139872851277632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 02:57:11.976030 139897472137024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 02:57:11.976757 139630544459584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 02:57:11.976975 140127899395904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 02:57:11.977108 139763850061632 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 02:57:11.977168 140292456515392 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 02:57:11.977660 140125077804864 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 02:57:11.978013 140125077804864 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.986589 140353388304192 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.986641 139897472137024 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.986637 139872851277632 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.987290 139630544459584 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.987503 140127899395904 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.987765 139763850061632 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:11.987816 140292456515392 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:57:14.447493 140127899395904 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch because --overwrite was set.
I0607 02:57:14.462518 140127899395904 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch.
W0607 02:57:14.578979 139872851277632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.579197 139897472137024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.579897 140353388304192 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.580609 139763850061632 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.580794 139630544459584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.581176 140125077804864 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.583285 140127899395904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:57:14.584561 140292456515392 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 02:57:14.588910 140127899395904 submission_runner.py:541] Using RNG seed 3599768039
I0607 02:57:14.590287 140127899395904 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 02:57:14.590413 140127899395904 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch/trial_1.
I0607 02:57:14.590640 140127899395904 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/hparams.json.
I0607 02:57:14.591746 140127899395904 submission_runner.py:255] Initializing dataset.
I0607 02:57:20.899510 140127899395904 submission_runner.py:262] Initializing model.
I0607 02:57:25.416951 140127899395904 submission_runner.py:272] Initializing optimizer.
I0607 02:57:25.980387 140127899395904 submission_runner.py:279] Initializing metrics bundle.
I0607 02:57:25.980582 140127899395904 submission_runner.py:297] Initializing checkpoint and logger.
I0607 02:57:26.486494 140127899395904 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0607 02:57:26.487495 140127899395904 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0607 02:57:26.547758 140127899395904 submission_runner.py:332] Starting training loop.
I0607 02:57:34.721967 140100046399232 logging_writer.py:48] [0] global_step=0, grad_norm=0.531494, loss=6.930475
I0607 02:57:34.744259 140127899395904 submission.py:139] 0) loss = 6.930, grad_norm = 0.531
I0607 02:57:34.745504 140127899395904 spec.py:298] Evaluating on the training split.
I0607 02:58:38.237048 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 02:59:37.029388 140127899395904 spec.py:326] Evaluating on the test split.
I0607 02:59:37.049938 140127899395904 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 02:59:37.057056 140127899395904 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0607 02:59:37.141304 140127899395904 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 02:59:48.722591 140127899395904 submission_runner.py:419] Time since start: 142.18s, 	Step: 1, 	{'train/accuracy': 0.0010762117346938774, 'train/loss': 6.922629842952806, 'validation/accuracy': 0.00114, 'validation/loss': 6.922711875, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.924103125, 'test/num_examples': 10000, 'score': 8.197705745697021, 'total_duration': 142.1752429008484, 'accumulated_submission_time': 8.197705745697021, 'accumulated_eval_time': 133.9770278930664, 'accumulated_logging_time': 0}
I0607 02:59:48.741413 140075207726848 logging_writer.py:48] [1] accumulated_eval_time=133.977028, accumulated_logging_time=0, accumulated_submission_time=8.197706, global_step=1, preemption_count=0, score=8.197706, test/accuracy=0.000600, test/loss=6.924103, test/num_examples=10000, total_duration=142.175243, train/accuracy=0.001076, train/loss=6.922630, validation/accuracy=0.001140, validation/loss=6.922712, validation/num_examples=50000
I0607 02:59:48.784243 140127899395904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.784272 139872851277632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.784279 140353388304192 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.784274 139897472137024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.785355 140292456515392 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.785367 139630544459584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.785411 139763850061632 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:48.785679 140125077804864 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:49.147182 140069105002240 logging_writer.py:48] [1] global_step=1, grad_norm=0.553564, loss=6.930620
I0607 02:59:49.150831 140127899395904 submission.py:139] 1) loss = 6.931, grad_norm = 0.554
I0607 02:59:49.523255 140075207726848 logging_writer.py:48] [2] global_step=2, grad_norm=0.548283, loss=6.932792
I0607 02:59:49.526958 140127899395904 submission.py:139] 2) loss = 6.933, grad_norm = 0.548
I0607 02:59:49.902328 140069105002240 logging_writer.py:48] [3] global_step=3, grad_norm=0.548531, loss=6.934147
I0607 02:59:49.905978 140127899395904 submission.py:139] 3) loss = 6.934, grad_norm = 0.549
I0607 02:59:50.286893 140075207726848 logging_writer.py:48] [4] global_step=4, grad_norm=0.538828, loss=6.924066
I0607 02:59:50.291522 140127899395904 submission.py:139] 4) loss = 6.924, grad_norm = 0.539
I0607 02:59:50.666577 140069105002240 logging_writer.py:48] [5] global_step=5, grad_norm=0.535447, loss=6.919126
I0607 02:59:50.670687 140127899395904 submission.py:139] 5) loss = 6.919, grad_norm = 0.535
I0607 02:59:51.055079 140075207726848 logging_writer.py:48] [6] global_step=6, grad_norm=0.552530, loss=6.924849
I0607 02:59:51.061747 140127899395904 submission.py:139] 6) loss = 6.925, grad_norm = 0.553
I0607 02:59:51.436275 140069105002240 logging_writer.py:48] [7] global_step=7, grad_norm=0.553742, loss=6.926517
I0607 02:59:51.441692 140127899395904 submission.py:139] 7) loss = 6.927, grad_norm = 0.554
I0607 02:59:51.821617 140075207726848 logging_writer.py:48] [8] global_step=8, grad_norm=0.547770, loss=6.924940
I0607 02:59:51.826909 140127899395904 submission.py:139] 8) loss = 6.925, grad_norm = 0.548
I0607 02:59:52.201747 140069105002240 logging_writer.py:48] [9] global_step=9, grad_norm=0.555194, loss=6.933551
I0607 02:59:52.206977 140127899395904 submission.py:139] 9) loss = 6.934, grad_norm = 0.555
I0607 02:59:52.583252 140075207726848 logging_writer.py:48] [10] global_step=10, grad_norm=0.548791, loss=6.927881
I0607 02:59:52.587538 140127899395904 submission.py:139] 10) loss = 6.928, grad_norm = 0.549
I0607 02:59:52.968150 140069105002240 logging_writer.py:48] [11] global_step=11, grad_norm=0.542373, loss=6.928277
I0607 02:59:52.973073 140127899395904 submission.py:139] 11) loss = 6.928, grad_norm = 0.542
I0607 02:59:53.346386 140075207726848 logging_writer.py:48] [12] global_step=12, grad_norm=0.537036, loss=6.932505
I0607 02:59:53.350584 140127899395904 submission.py:139] 12) loss = 6.933, grad_norm = 0.537
I0607 02:59:53.729038 140069105002240 logging_writer.py:48] [13] global_step=13, grad_norm=0.542497, loss=6.924853
I0607 02:59:53.734118 140127899395904 submission.py:139] 13) loss = 6.925, grad_norm = 0.542
I0607 02:59:54.110888 140075207726848 logging_writer.py:48] [14] global_step=14, grad_norm=0.551017, loss=6.923735
I0607 02:59:54.114832 140127899395904 submission.py:139] 14) loss = 6.924, grad_norm = 0.551
I0607 02:59:54.491375 140069105002240 logging_writer.py:48] [15] global_step=15, grad_norm=0.533508, loss=6.924821
I0607 02:59:54.495355 140127899395904 submission.py:139] 15) loss = 6.925, grad_norm = 0.534
I0607 02:59:54.868390 140075207726848 logging_writer.py:48] [16] global_step=16, grad_norm=0.527721, loss=6.924659
I0607 02:59:54.873052 140127899395904 submission.py:139] 16) loss = 6.925, grad_norm = 0.528
I0607 02:59:55.249289 140069105002240 logging_writer.py:48] [17] global_step=17, grad_norm=0.541049, loss=6.924067
I0607 02:59:55.253829 140127899395904 submission.py:139] 17) loss = 6.924, grad_norm = 0.541
I0607 02:59:55.628201 140075207726848 logging_writer.py:48] [18] global_step=18, grad_norm=0.555380, loss=6.938066
I0607 02:59:55.633799 140127899395904 submission.py:139] 18) loss = 6.938, grad_norm = 0.555
I0607 02:59:56.011383 140069105002240 logging_writer.py:48] [19] global_step=19, grad_norm=0.546873, loss=6.927137
I0607 02:59:56.016447 140127899395904 submission.py:139] 19) loss = 6.927, grad_norm = 0.547
I0607 02:59:56.392258 140075207726848 logging_writer.py:48] [20] global_step=20, grad_norm=0.550015, loss=6.931751
I0607 02:59:56.396799 140127899395904 submission.py:139] 20) loss = 6.932, grad_norm = 0.550
I0607 02:59:56.772204 140069105002240 logging_writer.py:48] [21] global_step=21, grad_norm=0.542592, loss=6.924626
I0607 02:59:56.776894 140127899395904 submission.py:139] 21) loss = 6.925, grad_norm = 0.543
I0607 02:59:57.153405 140075207726848 logging_writer.py:48] [22] global_step=22, grad_norm=0.544931, loss=6.923197
I0607 02:59:57.158237 140127899395904 submission.py:139] 22) loss = 6.923, grad_norm = 0.545
I0607 02:59:57.537968 140069105002240 logging_writer.py:48] [23] global_step=23, grad_norm=0.538951, loss=6.931882
I0607 02:59:57.541883 140127899395904 submission.py:139] 23) loss = 6.932, grad_norm = 0.539
I0607 02:59:57.914454 140075207726848 logging_writer.py:48] [24] global_step=24, grad_norm=0.546111, loss=6.924419
I0607 02:59:57.918219 140127899395904 submission.py:139] 24) loss = 6.924, grad_norm = 0.546
I0607 02:59:58.294297 140069105002240 logging_writer.py:48] [25] global_step=25, grad_norm=0.538776, loss=6.924056
I0607 02:59:58.298395 140127899395904 submission.py:139] 25) loss = 6.924, grad_norm = 0.539
I0607 02:59:58.681178 140075207726848 logging_writer.py:48] [26] global_step=26, grad_norm=0.549212, loss=6.921178
I0607 02:59:58.685213 140127899395904 submission.py:139] 26) loss = 6.921, grad_norm = 0.549
I0607 02:59:59.061081 140069105002240 logging_writer.py:48] [27] global_step=27, grad_norm=0.530094, loss=6.919713
I0607 02:59:59.066008 140127899395904 submission.py:139] 27) loss = 6.920, grad_norm = 0.530
I0607 02:59:59.441719 140075207726848 logging_writer.py:48] [28] global_step=28, grad_norm=0.545651, loss=6.923532
I0607 02:59:59.446183 140127899395904 submission.py:139] 28) loss = 6.924, grad_norm = 0.546
I0607 02:59:59.821372 140069105002240 logging_writer.py:48] [29] global_step=29, grad_norm=0.547160, loss=6.918674
I0607 02:59:59.826356 140127899395904 submission.py:139] 29) loss = 6.919, grad_norm = 0.547
I0607 03:00:00.210757 140075207726848 logging_writer.py:48] [30] global_step=30, grad_norm=0.546228, loss=6.925778
I0607 03:00:00.215040 140127899395904 submission.py:139] 30) loss = 6.926, grad_norm = 0.546
I0607 03:00:00.592178 140069105002240 logging_writer.py:48] [31] global_step=31, grad_norm=0.532531, loss=6.925627
I0607 03:00:00.595982 140127899395904 submission.py:139] 31) loss = 6.926, grad_norm = 0.533
I0607 03:00:00.973489 140075207726848 logging_writer.py:48] [32] global_step=32, grad_norm=0.519072, loss=6.912942
I0607 03:00:00.977880 140127899395904 submission.py:139] 32) loss = 6.913, grad_norm = 0.519
I0607 03:00:01.355552 140069105002240 logging_writer.py:48] [33] global_step=33, grad_norm=0.562104, loss=6.930169
I0607 03:00:01.359489 140127899395904 submission.py:139] 33) loss = 6.930, grad_norm = 0.562
I0607 03:00:01.735861 140075207726848 logging_writer.py:48] [34] global_step=34, grad_norm=0.541166, loss=6.927985
I0607 03:00:01.741533 140127899395904 submission.py:139] 34) loss = 6.928, grad_norm = 0.541
I0607 03:00:02.121970 140069105002240 logging_writer.py:48] [35] global_step=35, grad_norm=0.535731, loss=6.921538
I0607 03:00:02.128187 140127899395904 submission.py:139] 35) loss = 6.922, grad_norm = 0.536
I0607 03:00:02.513076 140075207726848 logging_writer.py:48] [36] global_step=36, grad_norm=0.545408, loss=6.918045
I0607 03:00:02.517691 140127899395904 submission.py:139] 36) loss = 6.918, grad_norm = 0.545
I0607 03:00:02.894232 140069105002240 logging_writer.py:48] [37] global_step=37, grad_norm=0.544329, loss=6.913642
I0607 03:00:02.901215 140127899395904 submission.py:139] 37) loss = 6.914, grad_norm = 0.544
I0607 03:00:03.280857 140075207726848 logging_writer.py:48] [38] global_step=38, grad_norm=0.528753, loss=6.913408
I0607 03:00:03.285484 140127899395904 submission.py:139] 38) loss = 6.913, grad_norm = 0.529
I0607 03:00:03.663394 140069105002240 logging_writer.py:48] [39] global_step=39, grad_norm=0.533859, loss=6.922588
I0607 03:00:03.668279 140127899395904 submission.py:139] 39) loss = 6.923, grad_norm = 0.534
I0607 03:00:04.046117 140075207726848 logging_writer.py:48] [40] global_step=40, grad_norm=0.547207, loss=6.919949
I0607 03:00:04.050530 140127899395904 submission.py:139] 40) loss = 6.920, grad_norm = 0.547
I0607 03:00:04.431311 140069105002240 logging_writer.py:48] [41] global_step=41, grad_norm=0.524743, loss=6.915996
I0607 03:00:04.436311 140127899395904 submission.py:139] 41) loss = 6.916, grad_norm = 0.525
I0607 03:00:04.810422 140075207726848 logging_writer.py:48] [42] global_step=42, grad_norm=0.537049, loss=6.918912
I0607 03:00:04.815025 140127899395904 submission.py:139] 42) loss = 6.919, grad_norm = 0.537
I0607 03:00:05.191800 140069105002240 logging_writer.py:48] [43] global_step=43, grad_norm=0.542433, loss=6.913182
I0607 03:00:05.199372 140127899395904 submission.py:139] 43) loss = 6.913, grad_norm = 0.542
I0607 03:00:05.582293 140075207726848 logging_writer.py:48] [44] global_step=44, grad_norm=0.536341, loss=6.924273
I0607 03:00:05.587564 140127899395904 submission.py:139] 44) loss = 6.924, grad_norm = 0.536
I0607 03:00:05.971011 140069105002240 logging_writer.py:48] [45] global_step=45, grad_norm=0.521119, loss=6.915419
I0607 03:00:05.975596 140127899395904 submission.py:139] 45) loss = 6.915, grad_norm = 0.521
I0607 03:00:06.353197 140075207726848 logging_writer.py:48] [46] global_step=46, grad_norm=0.553226, loss=6.915959
I0607 03:00:06.357167 140127899395904 submission.py:139] 46) loss = 6.916, grad_norm = 0.553
I0607 03:00:06.732827 140069105002240 logging_writer.py:48] [47] global_step=47, grad_norm=0.538434, loss=6.918218
I0607 03:00:06.737161 140127899395904 submission.py:139] 47) loss = 6.918, grad_norm = 0.538
I0607 03:00:07.113321 140075207726848 logging_writer.py:48] [48] global_step=48, grad_norm=0.531893, loss=6.917719
I0607 03:00:07.118631 140127899395904 submission.py:139] 48) loss = 6.918, grad_norm = 0.532
I0607 03:00:07.494175 140069105002240 logging_writer.py:48] [49] global_step=49, grad_norm=0.545542, loss=6.910051
I0607 03:00:07.498569 140127899395904 submission.py:139] 49) loss = 6.910, grad_norm = 0.546
I0607 03:00:07.874922 140075207726848 logging_writer.py:48] [50] global_step=50, grad_norm=0.530265, loss=6.916154
I0607 03:00:07.878853 140127899395904 submission.py:139] 50) loss = 6.916, grad_norm = 0.530
I0607 03:00:08.255728 140069105002240 logging_writer.py:48] [51] global_step=51, grad_norm=0.533283, loss=6.914417
I0607 03:00:08.259927 140127899395904 submission.py:139] 51) loss = 6.914, grad_norm = 0.533
I0607 03:00:08.637303 140075207726848 logging_writer.py:48] [52] global_step=52, grad_norm=0.528289, loss=6.912985
I0607 03:00:08.641490 140127899395904 submission.py:139] 52) loss = 6.913, grad_norm = 0.528
I0607 03:00:09.017500 140069105002240 logging_writer.py:48] [53] global_step=53, grad_norm=0.524538, loss=6.904103
I0607 03:00:09.022313 140127899395904 submission.py:139] 53) loss = 6.904, grad_norm = 0.525
I0607 03:00:09.398122 140075207726848 logging_writer.py:48] [54] global_step=54, grad_norm=0.548009, loss=6.913125
I0607 03:00:09.402931 140127899395904 submission.py:139] 54) loss = 6.913, grad_norm = 0.548
I0607 03:00:09.778886 140069105002240 logging_writer.py:48] [55] global_step=55, grad_norm=0.534559, loss=6.920800
I0607 03:00:09.784047 140127899395904 submission.py:139] 55) loss = 6.921, grad_norm = 0.535
I0607 03:00:10.160074 140075207726848 logging_writer.py:48] [56] global_step=56, grad_norm=0.529167, loss=6.909243
I0607 03:00:10.164862 140127899395904 submission.py:139] 56) loss = 6.909, grad_norm = 0.529
I0607 03:00:10.539447 140069105002240 logging_writer.py:48] [57] global_step=57, grad_norm=0.536574, loss=6.905848
I0607 03:00:10.543742 140127899395904 submission.py:139] 57) loss = 6.906, grad_norm = 0.537
I0607 03:00:10.919580 140075207726848 logging_writer.py:48] [58] global_step=58, grad_norm=0.553102, loss=6.910615
I0607 03:00:10.925084 140127899395904 submission.py:139] 58) loss = 6.911, grad_norm = 0.553
I0607 03:00:11.300692 140069105002240 logging_writer.py:48] [59] global_step=59, grad_norm=0.547005, loss=6.910704
I0607 03:00:11.306054 140127899395904 submission.py:139] 59) loss = 6.911, grad_norm = 0.547
I0607 03:00:11.682049 140075207726848 logging_writer.py:48] [60] global_step=60, grad_norm=0.536698, loss=6.904693
I0607 03:00:11.686012 140127899395904 submission.py:139] 60) loss = 6.905, grad_norm = 0.537
I0607 03:00:12.060453 140069105002240 logging_writer.py:48] [61] global_step=61, grad_norm=0.519439, loss=6.912838
I0607 03:00:12.066109 140127899395904 submission.py:139] 61) loss = 6.913, grad_norm = 0.519
I0607 03:00:12.442187 140075207726848 logging_writer.py:48] [62] global_step=62, grad_norm=0.521514, loss=6.910037
I0607 03:00:12.446142 140127899395904 submission.py:139] 62) loss = 6.910, grad_norm = 0.522
I0607 03:00:12.822976 140069105002240 logging_writer.py:48] [63] global_step=63, grad_norm=0.542150, loss=6.903429
I0607 03:00:12.827702 140127899395904 submission.py:139] 63) loss = 6.903, grad_norm = 0.542
I0607 03:00:13.204809 140075207726848 logging_writer.py:48] [64] global_step=64, grad_norm=0.557343, loss=6.910284
I0607 03:00:13.208505 140127899395904 submission.py:139] 64) loss = 6.910, grad_norm = 0.557
I0607 03:00:13.592426 140069105002240 logging_writer.py:48] [65] global_step=65, grad_norm=0.531381, loss=6.897521
I0607 03:00:13.596626 140127899395904 submission.py:139] 65) loss = 6.898, grad_norm = 0.531
I0607 03:00:13.972496 140075207726848 logging_writer.py:48] [66] global_step=66, grad_norm=0.532518, loss=6.899727
I0607 03:00:13.977676 140127899395904 submission.py:139] 66) loss = 6.900, grad_norm = 0.533
I0607 03:00:14.354660 140069105002240 logging_writer.py:48] [67] global_step=67, grad_norm=0.540940, loss=6.902672
I0607 03:00:14.359702 140127899395904 submission.py:139] 67) loss = 6.903, grad_norm = 0.541
I0607 03:00:14.736444 140075207726848 logging_writer.py:48] [68] global_step=68, grad_norm=0.533940, loss=6.898284
I0607 03:00:14.740206 140127899395904 submission.py:139] 68) loss = 6.898, grad_norm = 0.534
I0607 03:00:15.118799 140069105002240 logging_writer.py:48] [69] global_step=69, grad_norm=0.519900, loss=6.912746
I0607 03:00:15.123165 140127899395904 submission.py:139] 69) loss = 6.913, grad_norm = 0.520
I0607 03:00:15.500360 140075207726848 logging_writer.py:48] [70] global_step=70, grad_norm=0.536008, loss=6.914229
I0607 03:00:15.506078 140127899395904 submission.py:139] 70) loss = 6.914, grad_norm = 0.536
I0607 03:00:15.886673 140069105002240 logging_writer.py:48] [71] global_step=71, grad_norm=0.537077, loss=6.907759
I0607 03:00:15.893178 140127899395904 submission.py:139] 71) loss = 6.908, grad_norm = 0.537
I0607 03:00:16.282249 140075207726848 logging_writer.py:48] [72] global_step=72, grad_norm=0.538331, loss=6.901029
I0607 03:00:16.287236 140127899395904 submission.py:139] 72) loss = 6.901, grad_norm = 0.538
I0607 03:00:16.670878 140069105002240 logging_writer.py:48] [73] global_step=73, grad_norm=0.545619, loss=6.898847
I0607 03:00:16.677546 140127899395904 submission.py:139] 73) loss = 6.899, grad_norm = 0.546
I0607 03:00:17.063302 140075207726848 logging_writer.py:48] [74] global_step=74, grad_norm=0.515646, loss=6.895193
I0607 03:00:17.068157 140127899395904 submission.py:139] 74) loss = 6.895, grad_norm = 0.516
I0607 03:00:17.445746 140069105002240 logging_writer.py:48] [75] global_step=75, grad_norm=0.525057, loss=6.896744
I0607 03:00:17.450365 140127899395904 submission.py:139] 75) loss = 6.897, grad_norm = 0.525
I0607 03:00:17.827396 140075207726848 logging_writer.py:48] [76] global_step=76, grad_norm=0.525940, loss=6.901444
I0607 03:00:17.831180 140127899395904 submission.py:139] 76) loss = 6.901, grad_norm = 0.526
I0607 03:00:18.210098 140069105002240 logging_writer.py:48] [77] global_step=77, grad_norm=0.532287, loss=6.897277
I0607 03:00:18.214718 140127899395904 submission.py:139] 77) loss = 6.897, grad_norm = 0.532
I0607 03:00:18.596616 140075207726848 logging_writer.py:48] [78] global_step=78, grad_norm=0.547832, loss=6.904582
I0607 03:00:18.600889 140127899395904 submission.py:139] 78) loss = 6.905, grad_norm = 0.548
I0607 03:00:18.977751 140069105002240 logging_writer.py:48] [79] global_step=79, grad_norm=0.551580, loss=6.911039
I0607 03:00:18.982874 140127899395904 submission.py:139] 79) loss = 6.911, grad_norm = 0.552
I0607 03:00:19.359409 140075207726848 logging_writer.py:48] [80] global_step=80, grad_norm=0.516279, loss=6.892743
I0607 03:00:19.366483 140127899395904 submission.py:139] 80) loss = 6.893, grad_norm = 0.516
I0607 03:00:19.742762 140069105002240 logging_writer.py:48] [81] global_step=81, grad_norm=0.520542, loss=6.893557
I0607 03:00:19.746572 140127899395904 submission.py:139] 81) loss = 6.894, grad_norm = 0.521
I0607 03:00:20.131949 140075207726848 logging_writer.py:48] [82] global_step=82, grad_norm=0.527016, loss=6.896352
I0607 03:00:20.137085 140127899395904 submission.py:139] 82) loss = 6.896, grad_norm = 0.527
I0607 03:00:20.533535 140069105002240 logging_writer.py:48] [83] global_step=83, grad_norm=0.534814, loss=6.891330
I0607 03:00:20.538125 140127899395904 submission.py:139] 83) loss = 6.891, grad_norm = 0.535
I0607 03:00:20.920520 140075207726848 logging_writer.py:48] [84] global_step=84, grad_norm=0.533229, loss=6.902887
I0607 03:00:20.930679 140127899395904 submission.py:139] 84) loss = 6.903, grad_norm = 0.533
I0607 03:00:21.308058 140069105002240 logging_writer.py:48] [85] global_step=85, grad_norm=0.520023, loss=6.891853
I0607 03:00:21.313046 140127899395904 submission.py:139] 85) loss = 6.892, grad_norm = 0.520
I0607 03:00:21.690579 140075207726848 logging_writer.py:48] [86] global_step=86, grad_norm=0.543598, loss=6.891189
I0607 03:00:21.695967 140127899395904 submission.py:139] 86) loss = 6.891, grad_norm = 0.544
I0607 03:00:22.072891 140069105002240 logging_writer.py:48] [87] global_step=87, grad_norm=0.534597, loss=6.894953
I0607 03:00:22.076946 140127899395904 submission.py:139] 87) loss = 6.895, grad_norm = 0.535
I0607 03:00:22.457408 140075207726848 logging_writer.py:48] [88] global_step=88, grad_norm=0.530715, loss=6.895802
I0607 03:00:22.461376 140127899395904 submission.py:139] 88) loss = 6.896, grad_norm = 0.531
I0607 03:00:22.836827 140069105002240 logging_writer.py:48] [89] global_step=89, grad_norm=0.526753, loss=6.881563
I0607 03:00:22.841753 140127899395904 submission.py:139] 89) loss = 6.882, grad_norm = 0.527
I0607 03:00:23.216670 140075207726848 logging_writer.py:48] [90] global_step=90, grad_norm=0.540046, loss=6.900339
I0607 03:00:23.220565 140127899395904 submission.py:139] 90) loss = 6.900, grad_norm = 0.540
I0607 03:00:23.597108 140069105002240 logging_writer.py:48] [91] global_step=91, grad_norm=0.530341, loss=6.892342
I0607 03:00:23.601863 140127899395904 submission.py:139] 91) loss = 6.892, grad_norm = 0.530
I0607 03:00:23.982204 140075207726848 logging_writer.py:48] [92] global_step=92, grad_norm=0.545276, loss=6.883292
I0607 03:00:23.989567 140127899395904 submission.py:139] 92) loss = 6.883, grad_norm = 0.545
I0607 03:00:24.366598 140069105002240 logging_writer.py:48] [93] global_step=93, grad_norm=0.530817, loss=6.884626
I0607 03:00:24.371505 140127899395904 submission.py:139] 93) loss = 6.885, grad_norm = 0.531
I0607 03:00:24.749886 140075207726848 logging_writer.py:48] [94] global_step=94, grad_norm=0.520060, loss=6.894785
I0607 03:00:24.755037 140127899395904 submission.py:139] 94) loss = 6.895, grad_norm = 0.520
I0607 03:00:25.134628 140069105002240 logging_writer.py:48] [95] global_step=95, grad_norm=0.529761, loss=6.891274
I0607 03:00:25.138116 140127899395904 submission.py:139] 95) loss = 6.891, grad_norm = 0.530
I0607 03:00:25.516129 140075207726848 logging_writer.py:48] [96] global_step=96, grad_norm=0.530130, loss=6.890429
I0607 03:00:25.520287 140127899395904 submission.py:139] 96) loss = 6.890, grad_norm = 0.530
I0607 03:00:25.899229 140069105002240 logging_writer.py:48] [97] global_step=97, grad_norm=0.531763, loss=6.879325
I0607 03:00:25.904076 140127899395904 submission.py:139] 97) loss = 6.879, grad_norm = 0.532
I0607 03:00:26.286292 140075207726848 logging_writer.py:48] [98] global_step=98, grad_norm=0.540060, loss=6.881451
I0607 03:00:26.292350 140127899395904 submission.py:139] 98) loss = 6.881, grad_norm = 0.540
I0607 03:00:26.673078 140069105002240 logging_writer.py:48] [99] global_step=99, grad_norm=0.535097, loss=6.881592
I0607 03:00:26.676762 140127899395904 submission.py:139] 99) loss = 6.882, grad_norm = 0.535
I0607 03:00:27.058211 140075207726848 logging_writer.py:48] [100] global_step=100, grad_norm=0.529043, loss=6.885976
I0607 03:00:27.062559 140127899395904 submission.py:139] 100) loss = 6.886, grad_norm = 0.529
I0607 03:02:55.212272 140069105002240 logging_writer.py:48] [500] global_step=500, grad_norm=0.645286, loss=6.558570
I0607 03:02:55.217036 140127899395904 submission.py:139] 500) loss = 6.559, grad_norm = 0.645
I0607 03:06:01.303042 140075207726848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.725210, loss=6.277894
I0607 03:06:01.308253 140127899395904 submission.py:139] 1000) loss = 6.278, grad_norm = 0.725
I0607 03:08:18.954635 140127899395904 spec.py:298] Evaluating on the training split.
I0607 03:09:01.099282 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 03:09:59.492077 140127899395904 spec.py:326] Evaluating on the test split.
I0607 03:10:00.888152 140127899395904 submission_runner.py:419] Time since start: 754.34s, 	Step: 1366, 	{'train/accuracy': 0.0670639349489796, 'train/loss': 5.555107895208865, 'validation/accuracy': 0.06168, 'validation/loss': 5.613920625, 'validation/num_examples': 50000, 'test/accuracy': 0.0391, 'test/loss': 5.8523609375, 'test/num_examples': 10000, 'score': 517.5517296791077, 'total_duration': 754.3408401012421, 'accumulated_submission_time': 517.5517296791077, 'accumulated_eval_time': 235.91050672531128, 'accumulated_logging_time': 0.026723384857177734}
I0607 03:10:00.898023 140075216119552 logging_writer.py:48] [1366] accumulated_eval_time=235.910507, accumulated_logging_time=0.026723, accumulated_submission_time=517.551730, global_step=1366, preemption_count=0, score=517.551730, test/accuracy=0.039100, test/loss=5.852361, test/num_examples=10000, total_duration=754.340840, train/accuracy=0.067064, train/loss=5.555108, validation/accuracy=0.061680, validation/loss=5.613921, validation/num_examples=50000
I0607 03:10:50.625103 140075224512256 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.910541, loss=6.009349
I0607 03:10:50.628981 140127899395904 submission.py:139] 1500) loss = 6.009, grad_norm = 0.911
I0607 03:13:55.537424 140075216119552 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.010578, loss=5.760375
I0607 03:13:55.541751 140127899395904 submission.py:139] 2000) loss = 5.760, grad_norm = 1.011
I0607 03:17:00.830248 140075224512256 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.015134, loss=5.487201
I0607 03:17:00.835479 140127899395904 submission.py:139] 2500) loss = 5.487, grad_norm = 1.015
I0607 03:18:31.059616 140127899395904 spec.py:298] Evaluating on the training split.
I0607 03:19:17.049721 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 03:20:15.108589 140127899395904 spec.py:326] Evaluating on the test split.
I0607 03:20:16.463760 140127899395904 submission_runner.py:419] Time since start: 1369.92s, 	Step: 2741, 	{'train/accuracy': 0.1756218112244898, 'train/loss': 4.357638300681601, 'validation/accuracy': 0.16042, 'validation/loss': 4.462910625, 'validation/num_examples': 50000, 'test/accuracy': 0.11, 'test/loss': 4.92325703125, 'test/num_examples': 10000, 'score': 1026.9730308055878, 'total_duration': 1369.9150912761688, 'accumulated_submission_time': 1026.9730308055878, 'accumulated_eval_time': 341.3132915496826, 'accumulated_logging_time': 0.0449986457824707}
I0607 03:20:16.474596 140075216119552 logging_writer.py:48] [2741] accumulated_eval_time=341.313292, accumulated_logging_time=0.044999, accumulated_submission_time=1026.973031, global_step=2741, preemption_count=0, score=1026.973031, test/accuracy=0.110000, test/loss=4.923257, test/num_examples=10000, total_duration=1369.915091, train/accuracy=0.175622, train/loss=4.357638, validation/accuracy=0.160420, validation/loss=4.462911, validation/num_examples=50000
I0607 03:21:52.330166 140075224512256 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.952666, loss=5.165827
I0607 03:21:52.334310 140127899395904 submission.py:139] 3000) loss = 5.166, grad_norm = 0.953
I0607 03:24:57.176970 140075216119552 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.941619, loss=4.861854
I0607 03:24:57.181190 140127899395904 submission.py:139] 3500) loss = 4.862, grad_norm = 0.942
I0607 03:28:03.628618 140075224512256 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.870321, loss=4.769272
I0607 03:28:03.633205 140127899395904 submission.py:139] 4000) loss = 4.769, grad_norm = 0.870
I0607 03:28:46.487864 140127899395904 spec.py:298] Evaluating on the training split.
I0607 03:29:27.706872 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 03:30:11.397914 140127899395904 spec.py:326] Evaluating on the test split.
I0607 03:30:12.760138 140127899395904 submission_runner.py:419] Time since start: 1966.21s, 	Step: 4117, 	{'train/accuracy': 0.31405452806122447, 'train/loss': 3.4779339225924746, 'validation/accuracy': 0.28992, 'validation/loss': 3.603274375, 'validation/num_examples': 50000, 'test/accuracy': 0.2063, 'test/loss': 4.20418984375, 'test/num_examples': 10000, 'score': 1536.2213823795319, 'total_duration': 1966.2128376960754, 'accumulated_submission_time': 1536.2213823795319, 'accumulated_eval_time': 427.58559489250183, 'accumulated_logging_time': 0.06344199180603027}
I0607 03:30:12.771493 140075216119552 logging_writer.py:48] [4117] accumulated_eval_time=427.585595, accumulated_logging_time=0.063442, accumulated_submission_time=1536.221382, global_step=4117, preemption_count=0, score=1536.221382, test/accuracy=0.206300, test/loss=4.204190, test/num_examples=10000, total_duration=1966.212838, train/accuracy=0.314055, train/loss=3.477934, validation/accuracy=0.289920, validation/loss=3.603274, validation/num_examples=50000
I0607 03:32:34.515445 140075224512256 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.760032, loss=4.437017
I0607 03:32:34.520235 140127899395904 submission.py:139] 4500) loss = 4.437, grad_norm = 0.760
I0607 03:35:39.572173 140075216119552 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.740426, loss=4.451600
I0607 03:35:39.576785 140127899395904 submission.py:139] 5000) loss = 4.452, grad_norm = 0.740
I0607 03:38:42.989514 140127899395904 spec.py:298] Evaluating on the training split.
I0607 03:39:29.826551 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 03:40:27.011373 140127899395904 spec.py:326] Evaluating on the test split.
I0607 03:40:28.371541 140127899395904 submission_runner.py:419] Time since start: 2581.82s, 	Step: 5493, 	{'train/accuracy': 0.40646922831632654, 'train/loss': 2.8474133549904335, 'validation/accuracy': 0.37642, 'validation/loss': 3.0007084375, 'validation/num_examples': 50000, 'test/accuracy': 0.2787, 'test/loss': 3.65896640625, 'test/num_examples': 10000, 'score': 2045.70898103714, 'total_duration': 2581.8241901397705, 'accumulated_submission_time': 2045.70898103714, 'accumulated_eval_time': 532.967541217804, 'accumulated_logging_time': 0.08276534080505371}
I0607 03:40:28.383644 140075224512256 logging_writer.py:48] [5493] accumulated_eval_time=532.967541, accumulated_logging_time=0.082765, accumulated_submission_time=2045.708981, global_step=5493, preemption_count=0, score=2045.708981, test/accuracy=0.278700, test/loss=3.658966, test/num_examples=10000, total_duration=2581.824190, train/accuracy=0.406469, train/loss=2.847413, validation/accuracy=0.376420, validation/loss=3.000708, validation/num_examples=50000
I0607 03:40:31.390189 140075216119552 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.723858, loss=4.412925
I0607 03:40:31.394301 140127899395904 submission.py:139] 5500) loss = 4.413, grad_norm = 0.724
I0607 03:43:36.120151 140075224512256 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.698723, loss=4.211888
I0607 03:43:36.124061 140127899395904 submission.py:139] 6000) loss = 4.212, grad_norm = 0.699
I0607 03:46:42.194105 140075216119552 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.663363, loss=4.071717
I0607 03:46:42.198654 140127899395904 submission.py:139] 6500) loss = 4.072, grad_norm = 0.663
I0607 03:48:58.408884 140127899395904 spec.py:298] Evaluating on the training split.
I0607 03:49:41.933792 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 03:50:27.458757 140127899395904 spec.py:326] Evaluating on the test split.
I0607 03:50:28.822225 140127899395904 submission_runner.py:419] Time since start: 3182.27s, 	Step: 6870, 	{'train/accuracy': 0.47947225765306123, 'train/loss': 2.4807220770388234, 'validation/accuracy': 0.4447, 'validation/loss': 2.652315, 'validation/num_examples': 50000, 'test/accuracy': 0.3244, 'test/loss': 3.353175390625, 'test/num_examples': 10000, 'score': 2554.9891760349274, 'total_duration': 3182.274914741516, 'accumulated_submission_time': 2554.9891760349274, 'accumulated_eval_time': 623.3808205127716, 'accumulated_logging_time': 0.10378694534301758}
I0607 03:50:28.832660 140075224512256 logging_writer.py:48] [6870] accumulated_eval_time=623.380821, accumulated_logging_time=0.103787, accumulated_submission_time=2554.989176, global_step=6870, preemption_count=0, score=2554.989176, test/accuracy=0.324400, test/loss=3.353175, test/num_examples=10000, total_duration=3182.274915, train/accuracy=0.479472, train/loss=2.480722, validation/accuracy=0.444700, validation/loss=2.652315, validation/num_examples=50000
I0607 03:51:17.150109 140075216119552 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.618041, loss=3.998180
I0607 03:51:17.154271 140127899395904 submission.py:139] 7000) loss = 3.998, grad_norm = 0.618
I0607 03:54:22.114644 140075224512256 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.617057, loss=3.985464
I0607 03:54:22.122109 140127899395904 submission.py:139] 7500) loss = 3.985, grad_norm = 0.617
I0607 03:57:28.123365 140075216119552 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.654531, loss=3.868458
I0607 03:57:28.128125 140127899395904 submission.py:139] 8000) loss = 3.868, grad_norm = 0.655
I0607 03:58:58.987585 140127899395904 spec.py:298] Evaluating on the training split.
I0607 03:59:42.437511 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 04:00:33.667233 140127899395904 spec.py:326] Evaluating on the test split.
I0607 04:00:35.022599 140127899395904 submission_runner.py:419] Time since start: 3788.48s, 	Step: 8247, 	{'train/accuracy': 0.5415537308673469, 'train/loss': 2.2385376910774077, 'validation/accuracy': 0.49714, 'validation/loss': 2.43774, 'validation/num_examples': 50000, 'test/accuracy': 0.3726, 'test/loss': 3.114408203125, 'test/num_examples': 10000, 'score': 3064.4037692546844, 'total_duration': 3788.475273370743, 'accumulated_submission_time': 3064.4037692546844, 'accumulated_eval_time': 719.416038274765, 'accumulated_logging_time': 0.12192320823669434}
I0607 04:00:35.032859 140075224512256 logging_writer.py:48] [8247] accumulated_eval_time=719.416038, accumulated_logging_time=0.121923, accumulated_submission_time=3064.403769, global_step=8247, preemption_count=0, score=3064.403769, test/accuracy=0.372600, test/loss=3.114408, test/num_examples=10000, total_duration=3788.475273, train/accuracy=0.541554, train/loss=2.238538, validation/accuracy=0.497140, validation/loss=2.437740, validation/num_examples=50000
I0607 04:02:08.857340 140075216119552 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.601163, loss=3.822263
I0607 04:02:08.861292 140127899395904 submission.py:139] 8500) loss = 3.822, grad_norm = 0.601
I0607 04:05:15.032727 140075224512256 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.574603, loss=3.845123
I0607 04:05:15.037824 140127899395904 submission.py:139] 9000) loss = 3.845, grad_norm = 0.575
I0607 04:08:19.754572 140075216119552 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.563296, loss=3.777113
I0607 04:08:19.759214 140127899395904 submission.py:139] 9500) loss = 3.777, grad_norm = 0.563
I0607 04:09:05.306804 140127899395904 spec.py:298] Evaluating on the training split.
I0607 04:09:48.531671 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 04:10:33.189142 140127899395904 spec.py:326] Evaluating on the test split.
I0607 04:10:34.543834 140127899395904 submission_runner.py:419] Time since start: 4388.00s, 	Step: 9624, 	{'train/accuracy': 0.5809151785714286, 'train/loss': 1.960569109235491, 'validation/accuracy': 0.53676, 'validation/loss': 2.17140453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3979, 'test/loss': 2.91213046875, 'test/num_examples': 10000, 'score': 3573.9347767829895, 'total_duration': 4387.996520042419, 'accumulated_submission_time': 3573.9347767829895, 'accumulated_eval_time': 808.6529870033264, 'accumulated_logging_time': 0.13988447189331055}
I0607 04:10:34.554056 140075224512256 logging_writer.py:48] [9624] accumulated_eval_time=808.652987, accumulated_logging_time=0.139884, accumulated_submission_time=3573.934777, global_step=9624, preemption_count=0, score=3573.934777, test/accuracy=0.397900, test/loss=2.912130, test/num_examples=10000, total_duration=4387.996520, train/accuracy=0.580915, train/loss=1.960569, validation/accuracy=0.536760, validation/loss=2.171405, validation/num_examples=50000
I0607 04:12:53.884371 140075216119552 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.543366, loss=3.621827
I0607 04:12:53.888939 140127899395904 submission.py:139] 10000) loss = 3.622, grad_norm = 0.543
I0607 04:15:59.988574 140075224512256 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.530566, loss=3.666926
I0607 04:15:59.992754 140127899395904 submission.py:139] 10500) loss = 3.667, grad_norm = 0.531
I0607 04:19:04.632362 140127899395904 spec.py:298] Evaluating on the training split.
I0607 04:19:46.620616 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 04:20:45.308971 140127899395904 spec.py:326] Evaluating on the test split.
I0607 04:20:46.662232 140127899395904 submission_runner.py:419] Time since start: 5000.11s, 	Step: 11000, 	{'train/accuracy': 0.6076809630102041, 'train/loss': 1.8471587823361766, 'validation/accuracy': 0.55684, 'validation/loss': 2.074729375, 'validation/num_examples': 50000, 'test/accuracy': 0.4295, 'test/loss': 2.7654509765625, 'test/num_examples': 10000, 'score': 4083.284147977829, 'total_duration': 5000.113629817963, 'accumulated_submission_time': 4083.284147977829, 'accumulated_eval_time': 910.681648015976, 'accumulated_logging_time': 0.1579761505126953}
I0607 04:20:46.673283 140075216119552 logging_writer.py:48] [11000] accumulated_eval_time=910.681648, accumulated_logging_time=0.157976, accumulated_submission_time=4083.284148, global_step=11000, preemption_count=0, score=4083.284148, test/accuracy=0.429500, test/loss=2.765451, test/num_examples=10000, total_duration=5000.113630, train/accuracy=0.607681, train/loss=1.847159, validation/accuracy=0.556840, validation/loss=2.074729, validation/num_examples=50000
I0607 04:20:47.058613 140075224512256 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.543936, loss=3.559245
I0607 04:20:47.062858 140127899395904 submission.py:139] 11000) loss = 3.559, grad_norm = 0.544
I0607 04:23:53.215935 140075216119552 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.513076, loss=3.523922
I0607 04:23:53.220327 140127899395904 submission.py:139] 11500) loss = 3.524, grad_norm = 0.513
I0607 04:26:57.938751 140075224512256 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.511861, loss=3.528579
I0607 04:26:57.942803 140127899395904 submission.py:139] 12000) loss = 3.529, grad_norm = 0.512
I0607 04:29:16.732074 140127899395904 spec.py:298] Evaluating on the training split.
I0607 04:29:58.985091 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 04:30:42.887372 140127899395904 spec.py:326] Evaluating on the test split.
I0607 04:30:44.249528 140127899395904 submission_runner.py:419] Time since start: 5597.70s, 	Step: 12376, 	{'train/accuracy': 0.6506098533163265, 'train/loss': 1.6867383061623087, 'validation/accuracy': 0.59454, 'validation/loss': 1.94410515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4588, 'test/loss': 2.6372748046875, 'test/num_examples': 10000, 'score': 4592.600123882294, 'total_duration': 5597.702228546143, 'accumulated_submission_time': 4592.600123882294, 'accumulated_eval_time': 998.1991100311279, 'accumulated_logging_time': 0.17851996421813965}
I0607 04:30:44.260618 140075216119552 logging_writer.py:48] [12376] accumulated_eval_time=998.199110, accumulated_logging_time=0.178520, accumulated_submission_time=4592.600124, global_step=12376, preemption_count=0, score=4592.600124, test/accuracy=0.458800, test/loss=2.637275, test/num_examples=10000, total_duration=5597.702229, train/accuracy=0.650610, train/loss=1.686738, validation/accuracy=0.594540, validation/loss=1.944105, validation/num_examples=50000
I0607 04:31:30.302934 140075224512256 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.497265, loss=3.603155
I0607 04:31:30.308595 140127899395904 submission.py:139] 12500) loss = 3.603, grad_norm = 0.497
I0607 04:34:36.436997 140075216119552 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.544152, loss=3.478177
I0607 04:34:36.441457 140127899395904 submission.py:139] 13000) loss = 3.478, grad_norm = 0.544
I0607 04:37:41.196398 140075224512256 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.487632, loss=3.423024
I0607 04:37:41.205289 140127899395904 submission.py:139] 13500) loss = 3.423, grad_norm = 0.488
I0607 04:39:14.450011 140127899395904 spec.py:298] Evaluating on the training split.
I0607 04:39:56.997880 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 04:40:57.006132 140127899395904 spec.py:326] Evaluating on the test split.
I0607 04:40:58.365902 140127899395904 submission_runner.py:419] Time since start: 6211.82s, 	Step: 13753, 	{'train/accuracy': 0.6641820790816326, 'train/loss': 1.5923748405612246, 'validation/accuracy': 0.6058, 'validation/loss': 1.8625078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4607, 'test/loss': 2.620711328125, 'test/num_examples': 10000, 'score': 5102.053514719009, 'total_duration': 6211.8172442913055, 'accumulated_submission_time': 5102.053514719009, 'accumulated_eval_time': 1102.1136133670807, 'accumulated_logging_time': 0.19801926612854004}
I0607 04:40:58.379021 140075216119552 logging_writer.py:48] [13753] accumulated_eval_time=1102.113613, accumulated_logging_time=0.198019, accumulated_submission_time=5102.053515, global_step=13753, preemption_count=0, score=5102.053515, test/accuracy=0.460700, test/loss=2.620711, test/num_examples=10000, total_duration=6211.817244, train/accuracy=0.664182, train/loss=1.592375, validation/accuracy=0.605800, validation/loss=1.862508, validation/num_examples=50000
I0607 04:42:31.374697 140075224512256 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.484543, loss=3.419844
I0607 04:42:31.378848 140127899395904 submission.py:139] 14000) loss = 3.420, grad_norm = 0.485
I0607 04:45:35.964037 140075216119552 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.486507, loss=3.457991
I0607 04:45:35.968301 140127899395904 submission.py:139] 14500) loss = 3.458, grad_norm = 0.487
I0607 04:48:40.898811 140075224512256 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.478643, loss=3.416526
I0607 04:48:40.905779 140127899395904 submission.py:139] 15000) loss = 3.417, grad_norm = 0.479
I0607 04:49:28.706581 140127899395904 spec.py:298] Evaluating on the training split.
I0607 04:50:11.057971 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 04:51:07.012321 140127899395904 spec.py:326] Evaluating on the test split.
I0607 04:51:08.368475 140127899395904 submission_runner.py:419] Time since start: 6821.82s, 	Step: 15126, 	{'train/accuracy': 0.6772759885204082, 'train/loss': 1.5326472691127233, 'validation/accuracy': 0.61254, 'validation/loss': 1.82324859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4696, 'test/loss': 2.569881640625, 'test/num_examples': 10000, 'score': 5611.649382829666, 'total_duration': 6821.821150779724, 'accumulated_submission_time': 5611.649382829666, 'accumulated_eval_time': 1201.7755510807037, 'accumulated_logging_time': 0.21904468536376953}
I0607 04:51:08.378662 140075216119552 logging_writer.py:48] [15126] accumulated_eval_time=1201.775551, accumulated_logging_time=0.219045, accumulated_submission_time=5611.649383, global_step=15126, preemption_count=0, score=5611.649383, test/accuracy=0.469600, test/loss=2.569882, test/num_examples=10000, total_duration=6821.821151, train/accuracy=0.677276, train/loss=1.532647, validation/accuracy=0.612540, validation/loss=1.823249, validation/num_examples=50000
I0607 04:53:26.704827 140075224512256 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.470612, loss=3.346376
I0607 04:53:26.708849 140127899395904 submission.py:139] 15500) loss = 3.346, grad_norm = 0.471
I0607 04:56:31.451195 140075216119552 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.476159, loss=3.296286
I0607 04:56:31.455965 140127899395904 submission.py:139] 16000) loss = 3.296, grad_norm = 0.476
I0607 04:59:37.543786 140075224512256 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.493220, loss=3.340463
I0607 04:59:37.548872 140127899395904 submission.py:139] 16500) loss = 3.340, grad_norm = 0.493
I0607 04:59:38.653314 140127899395904 spec.py:298] Evaluating on the training split.
I0607 05:00:21.431950 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 05:01:19.012042 140127899395904 spec.py:326] Evaluating on the test split.
I0607 05:01:20.367837 140127899395904 submission_runner.py:419] Time since start: 7433.82s, 	Step: 16504, 	{'train/accuracy': 0.6994379783163265, 'train/loss': 1.4283205927634726, 'validation/accuracy': 0.63096, 'validation/loss': 1.7329678125, 'validation/num_examples': 50000, 'test/accuracy': 0.4836, 'test/loss': 2.481253125, 'test/num_examples': 10000, 'score': 6121.189129114151, 'total_duration': 7433.819253206253, 'accumulated_submission_time': 6121.189129114151, 'accumulated_eval_time': 1303.488763809204, 'accumulated_logging_time': 0.23794269561767578}
I0607 05:01:20.377695 140075216119552 logging_writer.py:48] [16504] accumulated_eval_time=1303.488764, accumulated_logging_time=0.237943, accumulated_submission_time=6121.189129, global_step=16504, preemption_count=0, score=6121.189129, test/accuracy=0.483600, test/loss=2.481253, test/num_examples=10000, total_duration=7433.819253, train/accuracy=0.699438, train/loss=1.428321, validation/accuracy=0.630960, validation/loss=1.732968, validation/num_examples=50000
I0607 05:04:23.770833 140075224512256 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.476527, loss=3.323905
I0607 05:04:23.775155 140127899395904 submission.py:139] 17000) loss = 3.324, grad_norm = 0.477
I0607 05:07:28.635405 140075216119552 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.477023, loss=3.319299
I0607 05:07:28.642626 140127899395904 submission.py:139] 17500) loss = 3.319, grad_norm = 0.477
I0607 05:09:50.645858 140127899395904 spec.py:298] Evaluating on the training split.
I0607 05:10:36.463659 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 05:11:25.945583 140127899395904 spec.py:326] Evaluating on the test split.
I0607 05:11:27.301315 140127899395904 submission_runner.py:419] Time since start: 8040.75s, 	Step: 17882, 	{'train/accuracy': 0.7134685905612245, 'train/loss': 1.4105059565330038, 'validation/accuracy': 0.64164, 'validation/loss': 1.73096, 'validation/num_examples': 50000, 'test/accuracy': 0.5076, 'test/loss': 2.4100671875, 'test/num_examples': 10000, 'score': 6630.717525959015, 'total_duration': 8040.754019498825, 'accumulated_submission_time': 6630.717525959015, 'accumulated_eval_time': 1400.1442284584045, 'accumulated_logging_time': 0.2552154064178467}
I0607 05:11:27.312587 140075224512256 logging_writer.py:48] [17882] accumulated_eval_time=1400.144228, accumulated_logging_time=0.255215, accumulated_submission_time=6630.717526, global_step=17882, preemption_count=0, score=6630.717526, test/accuracy=0.507600, test/loss=2.410067, test/num_examples=10000, total_duration=8040.754019, train/accuracy=0.713469, train/loss=1.410506, validation/accuracy=0.641640, validation/loss=1.730960, validation/num_examples=50000
I0607 05:12:11.102381 140075216119552 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.483293, loss=3.294471
I0607 05:12:11.107302 140127899395904 submission.py:139] 18000) loss = 3.294, grad_norm = 0.483
I0607 05:15:16.064247 140075224512256 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.472926, loss=3.344383
I0607 05:15:16.068289 140127899395904 submission.py:139] 18500) loss = 3.344, grad_norm = 0.473
I0607 05:18:22.356925 140075216119552 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.467769, loss=3.247052
I0607 05:18:22.361444 140127899395904 submission.py:139] 19000) loss = 3.247, grad_norm = 0.468
I0607 05:19:57.612988 140127899395904 spec.py:298] Evaluating on the training split.
I0607 05:20:43.001950 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 05:21:40.376294 140127899395904 spec.py:326] Evaluating on the test split.
I0607 05:21:41.744058 140127899395904 submission_runner.py:419] Time since start: 8655.20s, 	Step: 19259, 	{'train/accuracy': 0.7205038265306123, 'train/loss': 1.3986006756218112, 'validation/accuracy': 0.6435, 'validation/loss': 1.73336203125, 'validation/num_examples': 50000, 'test/accuracy': 0.5075, 'test/loss': 2.421093359375, 'test/num_examples': 10000, 'score': 7140.2758095264435, 'total_duration': 8655.195516347885, 'accumulated_submission_time': 7140.2758095264435, 'accumulated_eval_time': 1504.2741305828094, 'accumulated_logging_time': 0.27437448501586914}
I0607 05:21:41.754454 140075224512256 logging_writer.py:48] [19259] accumulated_eval_time=1504.274131, accumulated_logging_time=0.274374, accumulated_submission_time=7140.275810, global_step=19259, preemption_count=0, score=7140.275810, test/accuracy=0.507500, test/loss=2.421093, test/num_examples=10000, total_duration=8655.195516, train/accuracy=0.720504, train/loss=1.398601, validation/accuracy=0.643500, validation/loss=1.733362, validation/num_examples=50000
I0607 05:23:11.076201 140075216119552 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.450540, loss=3.216197
I0607 05:23:11.080816 140127899395904 submission.py:139] 19500) loss = 3.216, grad_norm = 0.451
I0607 05:26:15.911356 140075224512256 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.488504, loss=3.185753
I0607 05:26:15.918269 140127899395904 submission.py:139] 20000) loss = 3.186, grad_norm = 0.489
I0607 05:29:22.074535 140075216119552 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.462991, loss=3.215582
I0607 05:29:22.078825 140127899395904 submission.py:139] 20500) loss = 3.216, grad_norm = 0.463
I0607 05:30:11.934006 140127899395904 spec.py:298] Evaluating on the training split.
I0607 05:30:55.575291 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 05:31:45.129182 140127899395904 spec.py:326] Evaluating on the test split.
I0607 05:31:46.483907 140127899395904 submission_runner.py:419] Time since start: 9259.94s, 	Step: 20636, 	{'train/accuracy': 0.7292729591836735, 'train/loss': 1.3391636439732142, 'validation/accuracy': 0.64898, 'validation/loss': 1.69703234375, 'validation/num_examples': 50000, 'test/accuracy': 0.5132, 'test/loss': 2.406480859375, 'test/num_examples': 10000, 'score': 7649.7112810611725, 'total_duration': 9259.93651843071, 'accumulated_submission_time': 7649.7112810611725, 'accumulated_eval_time': 1598.8239550590515, 'accumulated_logging_time': 0.29239487648010254}
I0607 05:31:46.494260 140075224512256 logging_writer.py:48] [20636] accumulated_eval_time=1598.823955, accumulated_logging_time=0.292395, accumulated_submission_time=7649.711281, global_step=20636, preemption_count=0, score=7649.711281, test/accuracy=0.513200, test/loss=2.406481, test/num_examples=10000, total_duration=9259.936518, train/accuracy=0.729273, train/loss=1.339164, validation/accuracy=0.648980, validation/loss=1.697032, validation/num_examples=50000
I0607 05:34:01.237885 140075216119552 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.449631, loss=3.154665
I0607 05:34:01.241989 140127899395904 submission.py:139] 21000) loss = 3.155, grad_norm = 0.450
I0607 05:37:07.352686 140075224512256 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.463406, loss=3.216820
I0607 05:37:07.358688 140127899395904 submission.py:139] 21500) loss = 3.217, grad_norm = 0.463
I0607 05:40:11.849977 140075216119552 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.460324, loss=3.223220
I0607 05:40:11.853909 140127899395904 submission.py:139] 22000) loss = 3.223, grad_norm = 0.460
I0607 05:40:16.659446 140127899395904 spec.py:298] Evaluating on the training split.
I0607 05:41:00.361584 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 05:41:52.362059 140127899395904 spec.py:326] Evaluating on the test split.
I0607 05:41:53.716686 140127899395904 submission_runner.py:419] Time since start: 9867.17s, 	Step: 22014, 	{'train/accuracy': 0.7479671556122449, 'train/loss': 1.2938701863191566, 'validation/accuracy': 0.66462, 'validation/loss': 1.6569175, 'validation/num_examples': 50000, 'test/accuracy': 0.522, 'test/loss': 2.361599609375, 'test/num_examples': 10000, 'score': 8159.133804559708, 'total_duration': 9867.169343471527, 'accumulated_submission_time': 8159.133804559708, 'accumulated_eval_time': 1695.8810880184174, 'accumulated_logging_time': 0.31127429008483887}
I0607 05:41:53.726536 140075224512256 logging_writer.py:48] [22014] accumulated_eval_time=1695.881088, accumulated_logging_time=0.311274, accumulated_submission_time=8159.133805, global_step=22014, preemption_count=0, score=8159.133805, test/accuracy=0.522000, test/loss=2.361600, test/num_examples=10000, total_duration=9867.169343, train/accuracy=0.747967, train/loss=1.293870, validation/accuracy=0.664620, validation/loss=1.656918, validation/num_examples=50000
I0607 05:44:53.535717 140075216119552 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.445291, loss=3.168653
I0607 05:44:53.540995 140127899395904 submission.py:139] 22500) loss = 3.169, grad_norm = 0.445
I0607 05:47:59.431390 140075224512256 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.460123, loss=3.182912
I0607 05:47:59.435823 140127899395904 submission.py:139] 23000) loss = 3.183, grad_norm = 0.460
I0607 05:50:23.871169 140127899395904 spec.py:298] Evaluating on the training split.
I0607 05:51:09.773364 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 05:51:57.731924 140127899395904 spec.py:326] Evaluating on the test split.
I0607 05:51:59.084247 140127899395904 submission_runner.py:419] Time since start: 10472.54s, 	Step: 23392, 	{'train/accuracy': 0.7571548150510204, 'train/loss': 1.1821642505879304, 'validation/accuracy': 0.6628, 'validation/loss': 1.5687196875, 'validation/num_examples': 50000, 'test/accuracy': 0.5294, 'test/loss': 2.2718841796875, 'test/num_examples': 10000, 'score': 8668.540434837341, 'total_duration': 10472.536887645721, 'accumulated_submission_time': 8668.540434837341, 'accumulated_eval_time': 1791.094134092331, 'accumulated_logging_time': 0.32869887351989746}
I0607 05:51:59.095880 140075216119552 logging_writer.py:48] [23392] accumulated_eval_time=1791.094134, accumulated_logging_time=0.328699, accumulated_submission_time=8668.540435, global_step=23392, preemption_count=0, score=8668.540435, test/accuracy=0.529400, test/loss=2.271884, test/num_examples=10000, total_duration=10472.536888, train/accuracy=0.757155, train/loss=1.182164, validation/accuracy=0.662800, validation/loss=1.568720, validation/num_examples=50000
I0607 05:52:39.266882 140075224512256 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.453451, loss=3.142980
I0607 05:52:39.270769 140127899395904 submission.py:139] 23500) loss = 3.143, grad_norm = 0.453
I0607 05:55:45.353891 140075216119552 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.455170, loss=3.080966
I0607 05:55:45.358162 140127899395904 submission.py:139] 24000) loss = 3.081, grad_norm = 0.455
I0607 05:58:49.960191 140075224512256 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.450120, loss=3.103561
I0607 05:58:49.967125 140127899395904 submission.py:139] 24500) loss = 3.104, grad_norm = 0.450
I0607 06:00:29.428187 140127899395904 spec.py:298] Evaluating on the training split.
I0607 06:01:12.928071 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 06:02:10.531095 140127899395904 spec.py:326] Evaluating on the test split.
I0607 06:02:11.896709 140127899395904 submission_runner.py:419] Time since start: 11085.35s, 	Step: 24770, 	{'train/accuracy': 0.7612603635204082, 'train/loss': 1.1663130935357542, 'validation/accuracy': 0.67054, 'validation/loss': 1.56396625, 'validation/num_examples': 50000, 'test/accuracy': 0.5359, 'test/loss': 2.2589095703125, 'test/num_examples': 10000, 'score': 9178.138320922852, 'total_duration': 11085.348155021667, 'accumulated_submission_time': 9178.138320922852, 'accumulated_eval_time': 1893.5616009235382, 'accumulated_logging_time': 0.347944974899292}
I0607 06:02:11.908037 140075216119552 logging_writer.py:48] [24770] accumulated_eval_time=1893.561601, accumulated_logging_time=0.347945, accumulated_submission_time=9178.138321, global_step=24770, preemption_count=0, score=9178.138321, test/accuracy=0.535900, test/loss=2.258910, test/num_examples=10000, total_duration=11085.348155, train/accuracy=0.761260, train/loss=1.166313, validation/accuracy=0.670540, validation/loss=1.563966, validation/num_examples=50000
I0607 06:03:37.110212 140075224512256 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.457693, loss=3.126563
I0607 06:03:37.117911 140127899395904 submission.py:139] 25000) loss = 3.127, grad_norm = 0.458
I0607 06:06:43.169328 140075216119552 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.461332, loss=3.121164
I0607 06:06:43.173210 140127899395904 submission.py:139] 25500) loss = 3.121, grad_norm = 0.461
I0607 06:09:48.026683 140075224512256 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.464543, loss=3.116245
I0607 06:09:48.032839 140127899395904 submission.py:139] 26000) loss = 3.116, grad_norm = 0.465
I0607 06:10:42.073317 140127899395904 spec.py:298] Evaluating on the training split.
I0607 06:11:26.641591 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 06:12:18.720998 140127899395904 spec.py:326] Evaluating on the test split.
I0607 06:12:20.077031 140127899395904 submission_runner.py:419] Time since start: 11693.53s, 	Step: 26147, 	{'train/accuracy': 0.7668407206632653, 'train/loss': 1.1508838108607702, 'validation/accuracy': 0.67274, 'validation/loss': 1.55172984375, 'validation/num_examples': 50000, 'test/accuracy': 0.5406, 'test/loss': 2.233976171875, 'test/num_examples': 10000, 'score': 9687.566795825958, 'total_duration': 11693.529639720917, 'accumulated_submission_time': 9687.566795825958, 'accumulated_eval_time': 1991.5651762485504, 'accumulated_logging_time': 0.36700868606567383}
I0607 06:12:20.090373 140075216119552 logging_writer.py:48] [26147] accumulated_eval_time=1991.565176, accumulated_logging_time=0.367009, accumulated_submission_time=9687.566796, global_step=26147, preemption_count=0, score=9687.566796, test/accuracy=0.540600, test/loss=2.233976, test/num_examples=10000, total_duration=11693.529640, train/accuracy=0.766841, train/loss=1.150884, validation/accuracy=0.672740, validation/loss=1.551730, validation/num_examples=50000
I0607 06:14:31.987353 140075224512256 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.474961, loss=3.200255
I0607 06:14:31.991792 140127899395904 submission.py:139] 26500) loss = 3.200, grad_norm = 0.475
I0607 06:17:36.626744 140075216119552 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.461108, loss=3.161042
I0607 06:17:36.630844 140127899395904 submission.py:139] 27000) loss = 3.161, grad_norm = 0.461
I0607 06:20:41.488636 140075224512256 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.439511, loss=3.006498
I0607 06:20:41.496974 140127899395904 submission.py:139] 27500) loss = 3.006, grad_norm = 0.440
I0607 06:20:50.998867 140127899395904 spec.py:298] Evaluating on the training split.
I0607 06:21:36.808795 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 06:22:38.412246 140127899395904 spec.py:326] Evaluating on the test split.
I0607 06:22:39.765380 140127899395904 submission_runner.py:419] Time since start: 12313.22s, 	Step: 27523, 	{'train/accuracy': 0.7766262755102041, 'train/loss': 1.1170588123555085, 'validation/accuracy': 0.67594, 'validation/loss': 1.54817109375, 'validation/num_examples': 50000, 'test/accuracy': 0.5442, 'test/loss': 2.2355720703125, 'test/num_examples': 10000, 'score': 10197.735749959946, 'total_duration': 12313.216807603836, 'accumulated_submission_time': 10197.735749959946, 'accumulated_eval_time': 2100.3304052352905, 'accumulated_logging_time': 0.3894689083099365}
I0607 06:22:39.775829 140075216119552 logging_writer.py:48] [27523] accumulated_eval_time=2100.330405, accumulated_logging_time=0.389469, accumulated_submission_time=10197.735750, global_step=27523, preemption_count=0, score=10197.735750, test/accuracy=0.544200, test/loss=2.235572, test/num_examples=10000, total_duration=12313.216808, train/accuracy=0.776626, train/loss=1.117059, validation/accuracy=0.675940, validation/loss=1.548171, validation/num_examples=50000
I0607 06:25:35.582749 140127899395904 spec.py:298] Evaluating on the training split.
I0607 06:26:18.565640 140127899395904 spec.py:310] Evaluating on the validation split.
I0607 06:27:03.434993 140127899395904 spec.py:326] Evaluating on the test split.
I0607 06:27:04.798268 140127899395904 submission_runner.py:419] Time since start: 12578.25s, 	Step: 28000, 	{'train/accuracy': 0.7729990433673469, 'train/loss': 1.1291446296536192, 'validation/accuracy': 0.67526, 'validation/loss': 1.559620625, 'validation/num_examples': 50000, 'test/accuracy': 0.5274, 'test/loss': 2.3010892578125, 'test/num_examples': 10000, 'score': 10373.288924455643, 'total_duration': 12578.250985145569, 'accumulated_submission_time': 10373.288924455643, 'accumulated_eval_time': 2189.545974254608, 'accumulated_logging_time': 0.4074442386627197}
I0607 06:27:04.808846 140075224512256 logging_writer.py:48] [28000] accumulated_eval_time=2189.545974, accumulated_logging_time=0.407444, accumulated_submission_time=10373.288924, global_step=28000, preemption_count=0, score=10373.288924, test/accuracy=0.527400, test/loss=2.301089, test/num_examples=10000, total_duration=12578.250985, train/accuracy=0.772999, train/loss=1.129145, validation/accuracy=0.675260, validation/loss=1.559621, validation/num_examples=50000
I0607 06:27:04.826487 140075216119552 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10373.288924
I0607 06:27:05.409265 140127899395904 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nesterov/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0607 06:27:05.678848 140127899395904 submission_runner.py:581] Tuning trial 1/1
I0607 06:27:05.679148 140127899395904 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 06:27:05.680292 140127899395904 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010762117346938774, 'train/loss': 6.922629842952806, 'validation/accuracy': 0.00114, 'validation/loss': 6.922711875, 'validation/num_examples': 50000, 'test/accuracy': 0.0006, 'test/loss': 6.924103125, 'test/num_examples': 10000, 'score': 8.197705745697021, 'total_duration': 142.1752429008484, 'accumulated_submission_time': 8.197705745697021, 'accumulated_eval_time': 133.9770278930664, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1366, {'train/accuracy': 0.0670639349489796, 'train/loss': 5.555107895208865, 'validation/accuracy': 0.06168, 'validation/loss': 5.613920625, 'validation/num_examples': 50000, 'test/accuracy': 0.0391, 'test/loss': 5.8523609375, 'test/num_examples': 10000, 'score': 517.5517296791077, 'total_duration': 754.3408401012421, 'accumulated_submission_time': 517.5517296791077, 'accumulated_eval_time': 235.91050672531128, 'accumulated_logging_time': 0.026723384857177734, 'global_step': 1366, 'preemption_count': 0}), (2741, {'train/accuracy': 0.1756218112244898, 'train/loss': 4.357638300681601, 'validation/accuracy': 0.16042, 'validation/loss': 4.462910625, 'validation/num_examples': 50000, 'test/accuracy': 0.11, 'test/loss': 4.92325703125, 'test/num_examples': 10000, 'score': 1026.9730308055878, 'total_duration': 1369.9150912761688, 'accumulated_submission_time': 1026.9730308055878, 'accumulated_eval_time': 341.3132915496826, 'accumulated_logging_time': 0.0449986457824707, 'global_step': 2741, 'preemption_count': 0}), (4117, {'train/accuracy': 0.31405452806122447, 'train/loss': 3.4779339225924746, 'validation/accuracy': 0.28992, 'validation/loss': 3.603274375, 'validation/num_examples': 50000, 'test/accuracy': 0.2063, 'test/loss': 4.20418984375, 'test/num_examples': 10000, 'score': 1536.2213823795319, 'total_duration': 1966.2128376960754, 'accumulated_submission_time': 1536.2213823795319, 'accumulated_eval_time': 427.58559489250183, 'accumulated_logging_time': 0.06344199180603027, 'global_step': 4117, 'preemption_count': 0}), (5493, {'train/accuracy': 0.40646922831632654, 'train/loss': 2.8474133549904335, 'validation/accuracy': 0.37642, 'validation/loss': 3.0007084375, 'validation/num_examples': 50000, 'test/accuracy': 0.2787, 'test/loss': 3.65896640625, 'test/num_examples': 10000, 'score': 2045.70898103714, 'total_duration': 2581.8241901397705, 'accumulated_submission_time': 2045.70898103714, 'accumulated_eval_time': 532.967541217804, 'accumulated_logging_time': 0.08276534080505371, 'global_step': 5493, 'preemption_count': 0}), (6870, {'train/accuracy': 0.47947225765306123, 'train/loss': 2.4807220770388234, 'validation/accuracy': 0.4447, 'validation/loss': 2.652315, 'validation/num_examples': 50000, 'test/accuracy': 0.3244, 'test/loss': 3.353175390625, 'test/num_examples': 10000, 'score': 2554.9891760349274, 'total_duration': 3182.274914741516, 'accumulated_submission_time': 2554.9891760349274, 'accumulated_eval_time': 623.3808205127716, 'accumulated_logging_time': 0.10378694534301758, 'global_step': 6870, 'preemption_count': 0}), (8247, {'train/accuracy': 0.5415537308673469, 'train/loss': 2.2385376910774077, 'validation/accuracy': 0.49714, 'validation/loss': 2.43774, 'validation/num_examples': 50000, 'test/accuracy': 0.3726, 'test/loss': 3.114408203125, 'test/num_examples': 10000, 'score': 3064.4037692546844, 'total_duration': 3788.475273370743, 'accumulated_submission_time': 3064.4037692546844, 'accumulated_eval_time': 719.416038274765, 'accumulated_logging_time': 0.12192320823669434, 'global_step': 8247, 'preemption_count': 0}), (9624, {'train/accuracy': 0.5809151785714286, 'train/loss': 1.960569109235491, 'validation/accuracy': 0.53676, 'validation/loss': 2.17140453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3979, 'test/loss': 2.91213046875, 'test/num_examples': 10000, 'score': 3573.9347767829895, 'total_duration': 4387.996520042419, 'accumulated_submission_time': 3573.9347767829895, 'accumulated_eval_time': 808.6529870033264, 'accumulated_logging_time': 0.13988447189331055, 'global_step': 9624, 'preemption_count': 0}), (11000, {'train/accuracy': 0.6076809630102041, 'train/loss': 1.8471587823361766, 'validation/accuracy': 0.55684, 'validation/loss': 2.074729375, 'validation/num_examples': 50000, 'test/accuracy': 0.4295, 'test/loss': 2.7654509765625, 'test/num_examples': 10000, 'score': 4083.284147977829, 'total_duration': 5000.113629817963, 'accumulated_submission_time': 4083.284147977829, 'accumulated_eval_time': 910.681648015976, 'accumulated_logging_time': 0.1579761505126953, 'global_step': 11000, 'preemption_count': 0}), (12376, {'train/accuracy': 0.6506098533163265, 'train/loss': 1.6867383061623087, 'validation/accuracy': 0.59454, 'validation/loss': 1.94410515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4588, 'test/loss': 2.6372748046875, 'test/num_examples': 10000, 'score': 4592.600123882294, 'total_duration': 5597.702228546143, 'accumulated_submission_time': 4592.600123882294, 'accumulated_eval_time': 998.1991100311279, 'accumulated_logging_time': 0.17851996421813965, 'global_step': 12376, 'preemption_count': 0}), (13753, {'train/accuracy': 0.6641820790816326, 'train/loss': 1.5923748405612246, 'validation/accuracy': 0.6058, 'validation/loss': 1.8625078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4607, 'test/loss': 2.620711328125, 'test/num_examples': 10000, 'score': 5102.053514719009, 'total_duration': 6211.8172442913055, 'accumulated_submission_time': 5102.053514719009, 'accumulated_eval_time': 1102.1136133670807, 'accumulated_logging_time': 0.19801926612854004, 'global_step': 13753, 'preemption_count': 0}), (15126, {'train/accuracy': 0.6772759885204082, 'train/loss': 1.5326472691127233, 'validation/accuracy': 0.61254, 'validation/loss': 1.82324859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4696, 'test/loss': 2.569881640625, 'test/num_examples': 10000, 'score': 5611.649382829666, 'total_duration': 6821.821150779724, 'accumulated_submission_time': 5611.649382829666, 'accumulated_eval_time': 1201.7755510807037, 'accumulated_logging_time': 0.21904468536376953, 'global_step': 15126, 'preemption_count': 0}), (16504, {'train/accuracy': 0.6994379783163265, 'train/loss': 1.4283205927634726, 'validation/accuracy': 0.63096, 'validation/loss': 1.7329678125, 'validation/num_examples': 50000, 'test/accuracy': 0.4836, 'test/loss': 2.481253125, 'test/num_examples': 10000, 'score': 6121.189129114151, 'total_duration': 7433.819253206253, 'accumulated_submission_time': 6121.189129114151, 'accumulated_eval_time': 1303.488763809204, 'accumulated_logging_time': 0.23794269561767578, 'global_step': 16504, 'preemption_count': 0}), (17882, {'train/accuracy': 0.7134685905612245, 'train/loss': 1.4105059565330038, 'validation/accuracy': 0.64164, 'validation/loss': 1.73096, 'validation/num_examples': 50000, 'test/accuracy': 0.5076, 'test/loss': 2.4100671875, 'test/num_examples': 10000, 'score': 6630.717525959015, 'total_duration': 8040.754019498825, 'accumulated_submission_time': 6630.717525959015, 'accumulated_eval_time': 1400.1442284584045, 'accumulated_logging_time': 0.2552154064178467, 'global_step': 17882, 'preemption_count': 0}), (19259, {'train/accuracy': 0.7205038265306123, 'train/loss': 1.3986006756218112, 'validation/accuracy': 0.6435, 'validation/loss': 1.73336203125, 'validation/num_examples': 50000, 'test/accuracy': 0.5075, 'test/loss': 2.421093359375, 'test/num_examples': 10000, 'score': 7140.2758095264435, 'total_duration': 8655.195516347885, 'accumulated_submission_time': 7140.2758095264435, 'accumulated_eval_time': 1504.2741305828094, 'accumulated_logging_time': 0.27437448501586914, 'global_step': 19259, 'preemption_count': 0}), (20636, {'train/accuracy': 0.7292729591836735, 'train/loss': 1.3391636439732142, 'validation/accuracy': 0.64898, 'validation/loss': 1.69703234375, 'validation/num_examples': 50000, 'test/accuracy': 0.5132, 'test/loss': 2.406480859375, 'test/num_examples': 10000, 'score': 7649.7112810611725, 'total_duration': 9259.93651843071, 'accumulated_submission_time': 7649.7112810611725, 'accumulated_eval_time': 1598.8239550590515, 'accumulated_logging_time': 0.29239487648010254, 'global_step': 20636, 'preemption_count': 0}), (22014, {'train/accuracy': 0.7479671556122449, 'train/loss': 1.2938701863191566, 'validation/accuracy': 0.66462, 'validation/loss': 1.6569175, 'validation/num_examples': 50000, 'test/accuracy': 0.522, 'test/loss': 2.361599609375, 'test/num_examples': 10000, 'score': 8159.133804559708, 'total_duration': 9867.169343471527, 'accumulated_submission_time': 8159.133804559708, 'accumulated_eval_time': 1695.8810880184174, 'accumulated_logging_time': 0.31127429008483887, 'global_step': 22014, 'preemption_count': 0}), (23392, {'train/accuracy': 0.7571548150510204, 'train/loss': 1.1821642505879304, 'validation/accuracy': 0.6628, 'validation/loss': 1.5687196875, 'validation/num_examples': 50000, 'test/accuracy': 0.5294, 'test/loss': 2.2718841796875, 'test/num_examples': 10000, 'score': 8668.540434837341, 'total_duration': 10472.536887645721, 'accumulated_submission_time': 8668.540434837341, 'accumulated_eval_time': 1791.094134092331, 'accumulated_logging_time': 0.32869887351989746, 'global_step': 23392, 'preemption_count': 0}), (24770, {'train/accuracy': 0.7612603635204082, 'train/loss': 1.1663130935357542, 'validation/accuracy': 0.67054, 'validation/loss': 1.56396625, 'validation/num_examples': 50000, 'test/accuracy': 0.5359, 'test/loss': 2.2589095703125, 'test/num_examples': 10000, 'score': 9178.138320922852, 'total_duration': 11085.348155021667, 'accumulated_submission_time': 9178.138320922852, 'accumulated_eval_time': 1893.5616009235382, 'accumulated_logging_time': 0.347944974899292, 'global_step': 24770, 'preemption_count': 0}), (26147, {'train/accuracy': 0.7668407206632653, 'train/loss': 1.1508838108607702, 'validation/accuracy': 0.67274, 'validation/loss': 1.55172984375, 'validation/num_examples': 50000, 'test/accuracy': 0.5406, 'test/loss': 2.233976171875, 'test/num_examples': 10000, 'score': 9687.566795825958, 'total_duration': 11693.529639720917, 'accumulated_submission_time': 9687.566795825958, 'accumulated_eval_time': 1991.5651762485504, 'accumulated_logging_time': 0.36700868606567383, 'global_step': 26147, 'preemption_count': 0}), (27523, {'train/accuracy': 0.7766262755102041, 'train/loss': 1.1170588123555085, 'validation/accuracy': 0.67594, 'validation/loss': 1.54817109375, 'validation/num_examples': 50000, 'test/accuracy': 0.5442, 'test/loss': 2.2355720703125, 'test/num_examples': 10000, 'score': 10197.735749959946, 'total_duration': 12313.216807603836, 'accumulated_submission_time': 10197.735749959946, 'accumulated_eval_time': 2100.3304052352905, 'accumulated_logging_time': 0.3894689083099365, 'global_step': 27523, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7729990433673469, 'train/loss': 1.1291446296536192, 'validation/accuracy': 0.67526, 'validation/loss': 1.559620625, 'validation/num_examples': 50000, 'test/accuracy': 0.5274, 'test/loss': 2.3010892578125, 'test/num_examples': 10000, 'score': 10373.288924455643, 'total_duration': 12578.250985145569, 'accumulated_submission_time': 10373.288924455643, 'accumulated_eval_time': 2189.545974254608, 'accumulated_logging_time': 0.4074442386627197, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0607 06:27:05.680437 140127899395904 submission_runner.py:584] Timing: 10373.288924455643
I0607 06:27:05.680493 140127899395904 submission_runner.py:586] Total number of evals: 22
I0607 06:27:05.680541 140127899395904 submission_runner.py:587] ====================
I0607 06:27:05.680701 140127899395904 submission_runner.py:655] Final imagenet_resnet score: 10373.288924455643
