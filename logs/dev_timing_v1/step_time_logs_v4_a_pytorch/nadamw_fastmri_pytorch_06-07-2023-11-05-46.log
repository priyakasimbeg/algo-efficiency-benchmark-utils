torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=5428 2>&1 | tee -a /logs/fastmri_pytorch_06-07-2023-11-05-46.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 11:06:11.569366 139727297414976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 11:06:11.569386 140602513590080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 11:06:11.569411 139982508291904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 11:06:11.569467 140646059710272 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 11:06:11.570010 140640132794176 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 11:06:12.556120 139714054383424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 11:06:12.556289 140207774009152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 11:06:12.563258 139691936032576 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 11:06:12.563711 139691936032576 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.566844 140207774009152 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.566868 139714054383424 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.572223 139727297414976 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.572260 139982508291904 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.572342 140602513590080 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.572319 140646059710272 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:12.572348 140640132794176 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:06:13.164480 139691936032576 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch because --overwrite was set.
I0607 11:06:13.175556 139691936032576 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch.
W0607 11:06:13.201541 139727297414976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.203222 140646059710272 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.204339 140207774009152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.204513 139982508291904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.204849 140602513590080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.205533 139714054383424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.206933 140640132794176 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:06:13.210442 139691936032576 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 11:06:13.215201 139691936032576 submission_runner.py:541] Using RNG seed 1770506926
I0607 11:06:13.216655 139691936032576 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 11:06:13.216790 139691936032576 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch/trial_1.
I0607 11:06:13.217005 139691936032576 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch/trial_1/hparams.json.
I0607 11:06:13.217902 139691936032576 submission_runner.py:255] Initializing dataset.
I0607 11:06:13.218017 139691936032576 submission_runner.py:262] Initializing model.
I0607 11:06:17.409142 139691936032576 submission_runner.py:272] Initializing optimizer.
I0607 11:06:17.410083 139691936032576 submission_runner.py:279] Initializing metrics bundle.
I0607 11:06:17.410186 139691936032576 submission_runner.py:297] Initializing checkpoint and logger.
I0607 11:06:17.413670 139691936032576 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 11:06:17.413782 139691936032576 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 11:06:17.873707 139691936032576 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0607 11:06:17.874536 139691936032576 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch/trial_1/flags_0.json.
I0607 11:06:17.926881 139691936032576 submission_runner.py:332] Starting training loop.
I0607 11:07:07.884554 139649553004288 logging_writer.py:48] [0] global_step=0, grad_norm=3.918568, loss=0.790489
I0607 11:07:07.893525 139691936032576 submission.py:296] 0) loss = 0.790, grad_norm = 3.919
I0607 11:07:07.894820 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:08:50.392305 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:09:59.177677 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:11:04.751191 139691936032576 submission_runner.py:419] Time since start: 286.82s, 	Step: 1, 	{'train/ssim': 0.2116518361227853, 'train/loss': 0.8418434006827218, 'validation/ssim': 0.20459178437258194, 'validation/loss': 0.8573658202025887, 'validation/num_examples': 3554, 'test/ssim': 0.22654049598248568, 'test/loss': 0.8559334421251047, 'test/num_examples': 3581, 'score': 49.96812844276428, 'total_duration': 286.82483673095703, 'accumulated_submission_time': 49.96812844276428, 'accumulated_eval_time': 236.85631203651428, 'accumulated_logging_time': 0}
I0607 11:11:04.770086 139626366891776 logging_writer.py:48] [1] accumulated_eval_time=236.856312, accumulated_logging_time=0, accumulated_submission_time=49.968128, global_step=1, preemption_count=0, score=49.968128, test/loss=0.855933, test/num_examples=3581, test/ssim=0.226540, total_duration=286.824837, train/loss=0.841843, train/ssim=0.211652, validation/loss=0.857366, validation/num_examples=3554, validation/ssim=0.204592
I0607 11:11:04.793022 139982508291904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793018 139727297414976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793009 140602513590080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793094 139714054383424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793101 140640132794176 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793208 140207774009152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793216 140646059710272 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.793833 139691936032576 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:11:04.856298 139626358499072 logging_writer.py:48] [1] global_step=1, grad_norm=3.598364, loss=0.800783
I0607 11:11:04.861362 139691936032576 submission.py:296] 1) loss = 0.801, grad_norm = 3.598
I0607 11:11:04.937404 139626366891776 logging_writer.py:48] [2] global_step=2, grad_norm=3.582522, loss=0.887182
I0607 11:11:04.944118 139691936032576 submission.py:296] 2) loss = 0.887, grad_norm = 3.583
I0607 11:11:05.021558 139626358499072 logging_writer.py:48] [3] global_step=3, grad_norm=3.466476, loss=0.800206
I0607 11:11:05.024908 139691936032576 submission.py:296] 3) loss = 0.800, grad_norm = 3.466
I0607 11:11:05.092718 139626366891776 logging_writer.py:48] [4] global_step=4, grad_norm=3.593694, loss=0.855759
I0607 11:11:05.096049 139691936032576 submission.py:296] 4) loss = 0.856, grad_norm = 3.594
I0607 11:11:05.169291 139626358499072 logging_writer.py:48] [5] global_step=5, grad_norm=3.345643, loss=0.872927
I0607 11:11:05.173629 139691936032576 submission.py:296] 5) loss = 0.873, grad_norm = 3.346
I0607 11:11:05.247746 139626366891776 logging_writer.py:48] [6] global_step=6, grad_norm=3.271672, loss=0.864297
I0607 11:11:05.253081 139691936032576 submission.py:296] 6) loss = 0.864, grad_norm = 3.272
I0607 11:11:05.325738 139626358499072 logging_writer.py:48] [7] global_step=7, grad_norm=3.349888, loss=0.784014
I0607 11:11:05.329356 139691936032576 submission.py:296] 7) loss = 0.784, grad_norm = 3.350
I0607 11:11:05.397662 139626366891776 logging_writer.py:48] [8] global_step=8, grad_norm=3.481273, loss=0.777691
I0607 11:11:05.401409 139691936032576 submission.py:296] 8) loss = 0.778, grad_norm = 3.481
I0607 11:11:05.477024 139626358499072 logging_writer.py:48] [9] global_step=9, grad_norm=3.309881, loss=0.776214
I0607 11:11:05.482931 139691936032576 submission.py:296] 9) loss = 0.776, grad_norm = 3.310
I0607 11:11:05.560045 139626366891776 logging_writer.py:48] [10] global_step=10, grad_norm=2.911766, loss=0.807031
I0607 11:11:05.565175 139691936032576 submission.py:296] 10) loss = 0.807, grad_norm = 2.912
I0607 11:11:05.636399 139626358499072 logging_writer.py:48] [11] global_step=11, grad_norm=2.864729, loss=0.758857
I0607 11:11:05.639701 139691936032576 submission.py:296] 11) loss = 0.759, grad_norm = 2.865
I0607 11:11:05.709807 139626366891776 logging_writer.py:48] [12] global_step=12, grad_norm=2.912620, loss=0.756954
I0607 11:11:05.715326 139691936032576 submission.py:296] 12) loss = 0.757, grad_norm = 2.913
I0607 11:11:05.800971 139626358499072 logging_writer.py:48] [13] global_step=13, grad_norm=2.924327, loss=0.786717
I0607 11:11:05.806543 139691936032576 submission.py:296] 13) loss = 0.787, grad_norm = 2.924
I0607 11:11:05.922603 139626366891776 logging_writer.py:48] [14] global_step=14, grad_norm=2.621434, loss=0.788636
I0607 11:11:05.925902 139691936032576 submission.py:296] 14) loss = 0.789, grad_norm = 2.621
I0607 11:11:06.158710 139626358499072 logging_writer.py:48] [15] global_step=15, grad_norm=2.815815, loss=0.685395
I0607 11:11:06.164531 139691936032576 submission.py:296] 15) loss = 0.685, grad_norm = 2.816
I0607 11:11:06.419640 139626366891776 logging_writer.py:48] [16] global_step=16, grad_norm=2.391530, loss=0.711887
I0607 11:11:06.423259 139691936032576 submission.py:296] 16) loss = 0.712, grad_norm = 2.392
I0607 11:11:06.683912 139626358499072 logging_writer.py:48] [17] global_step=17, grad_norm=2.485705, loss=0.711654
I0607 11:11:06.688860 139691936032576 submission.py:296] 17) loss = 0.712, grad_norm = 2.486
I0607 11:11:06.953826 139626366891776 logging_writer.py:48] [18] global_step=18, grad_norm=2.394273, loss=0.639876
I0607 11:11:06.960359 139691936032576 submission.py:296] 18) loss = 0.640, grad_norm = 2.394
I0607 11:11:07.267039 139626358499072 logging_writer.py:48] [19] global_step=19, grad_norm=1.884769, loss=0.735924
I0607 11:11:07.272140 139691936032576 submission.py:296] 19) loss = 0.736, grad_norm = 1.885
I0607 11:11:07.577502 139626366891776 logging_writer.py:48] [20] global_step=20, grad_norm=1.944597, loss=0.684857
I0607 11:11:07.580803 139691936032576 submission.py:296] 20) loss = 0.685, grad_norm = 1.945
I0607 11:11:07.795914 139626358499072 logging_writer.py:48] [21] global_step=21, grad_norm=2.013494, loss=0.617121
I0607 11:11:07.800366 139691936032576 submission.py:296] 21) loss = 0.617, grad_norm = 2.013
I0607 11:11:08.073100 139626366891776 logging_writer.py:48] [22] global_step=22, grad_norm=1.939914, loss=0.577322
I0607 11:11:08.079239 139691936032576 submission.py:296] 22) loss = 0.577, grad_norm = 1.940
I0607 11:11:08.337769 139626358499072 logging_writer.py:48] [23] global_step=23, grad_norm=1.740371, loss=0.573203
I0607 11:11:08.342287 139691936032576 submission.py:296] 23) loss = 0.573, grad_norm = 1.740
I0607 11:11:08.568275 139626366891776 logging_writer.py:48] [24] global_step=24, grad_norm=1.561981, loss=0.584530
I0607 11:11:08.573411 139691936032576 submission.py:296] 24) loss = 0.585, grad_norm = 1.562
I0607 11:11:08.835384 139626358499072 logging_writer.py:48] [25] global_step=25, grad_norm=1.435982, loss=0.596296
I0607 11:11:08.840611 139691936032576 submission.py:296] 25) loss = 0.596, grad_norm = 1.436
I0607 11:11:09.082842 139626366891776 logging_writer.py:48] [26] global_step=26, grad_norm=1.404588, loss=0.618618
I0607 11:11:09.087032 139691936032576 submission.py:296] 26) loss = 0.619, grad_norm = 1.405
I0607 11:11:09.379341 139626358499072 logging_writer.py:48] [27] global_step=27, grad_norm=1.435094, loss=0.587096
I0607 11:11:09.385680 139691936032576 submission.py:296] 27) loss = 0.587, grad_norm = 1.435
I0607 11:11:09.632868 139626366891776 logging_writer.py:48] [28] global_step=28, grad_norm=1.301617, loss=0.596776
I0607 11:11:09.636863 139691936032576 submission.py:296] 28) loss = 0.597, grad_norm = 1.302
I0607 11:11:09.917429 139626358499072 logging_writer.py:48] [29] global_step=29, grad_norm=1.186222, loss=0.560032
I0607 11:11:09.921218 139691936032576 submission.py:296] 29) loss = 0.560, grad_norm = 1.186
I0607 11:11:10.181917 139626366891776 logging_writer.py:48] [30] global_step=30, grad_norm=1.160087, loss=0.563510
I0607 11:11:10.185455 139691936032576 submission.py:296] 30) loss = 0.564, grad_norm = 1.160
I0607 11:11:10.439375 139626358499072 logging_writer.py:48] [31] global_step=31, grad_norm=1.066261, loss=0.572578
I0607 11:11:10.443302 139691936032576 submission.py:296] 31) loss = 0.573, grad_norm = 1.066
I0607 11:11:10.698007 139626366891776 logging_writer.py:48] [32] global_step=32, grad_norm=1.003740, loss=0.596654
I0607 11:11:10.701715 139691936032576 submission.py:296] 32) loss = 0.597, grad_norm = 1.004
I0607 11:11:11.002294 139626358499072 logging_writer.py:48] [33] global_step=33, grad_norm=1.113621, loss=0.499235
I0607 11:11:11.005908 139691936032576 submission.py:296] 33) loss = 0.499, grad_norm = 1.114
I0607 11:11:11.275926 139626366891776 logging_writer.py:48] [34] global_step=34, grad_norm=1.119886, loss=0.578834
I0607 11:11:11.279678 139691936032576 submission.py:296] 34) loss = 0.579, grad_norm = 1.120
I0607 11:11:11.538108 139626358499072 logging_writer.py:48] [35] global_step=35, grad_norm=0.973609, loss=0.508994
I0607 11:11:11.544516 139691936032576 submission.py:296] 35) loss = 0.509, grad_norm = 0.974
I0607 11:11:11.790487 139626366891776 logging_writer.py:48] [36] global_step=36, grad_norm=1.013084, loss=0.493948
I0607 11:11:11.794496 139691936032576 submission.py:296] 36) loss = 0.494, grad_norm = 1.013
I0607 11:11:12.016505 139626358499072 logging_writer.py:48] [37] global_step=37, grad_norm=1.022882, loss=0.538201
I0607 11:11:12.022999 139691936032576 submission.py:296] 37) loss = 0.538, grad_norm = 1.023
I0607 11:11:12.299578 139626366891776 logging_writer.py:48] [38] global_step=38, grad_norm=0.996314, loss=0.567812
I0607 11:11:12.305248 139691936032576 submission.py:296] 38) loss = 0.568, grad_norm = 0.996
I0607 11:11:12.552173 139626358499072 logging_writer.py:48] [39] global_step=39, grad_norm=0.957269, loss=0.500448
I0607 11:11:12.556977 139691936032576 submission.py:296] 39) loss = 0.500, grad_norm = 0.957
I0607 11:11:12.810241 139626366891776 logging_writer.py:48] [40] global_step=40, grad_norm=0.961961, loss=0.486611
I0607 11:11:12.815486 139691936032576 submission.py:296] 40) loss = 0.487, grad_norm = 0.962
I0607 11:11:13.087044 139626358499072 logging_writer.py:48] [41] global_step=41, grad_norm=0.823400, loss=0.502858
I0607 11:11:13.092636 139691936032576 submission.py:296] 41) loss = 0.503, grad_norm = 0.823
I0607 11:11:13.378844 139626366891776 logging_writer.py:48] [42] global_step=42, grad_norm=0.847940, loss=0.449283
I0607 11:11:13.384105 139691936032576 submission.py:296] 42) loss = 0.449, grad_norm = 0.848
I0607 11:11:13.659355 139626358499072 logging_writer.py:48] [43] global_step=43, grad_norm=0.884776, loss=0.472237
I0607 11:11:13.665555 139691936032576 submission.py:296] 43) loss = 0.472, grad_norm = 0.885
I0607 11:11:13.924314 139626366891776 logging_writer.py:48] [44] global_step=44, grad_norm=0.860769, loss=0.428155
I0607 11:11:13.930489 139691936032576 submission.py:296] 44) loss = 0.428, grad_norm = 0.861
I0607 11:11:14.146937 139626358499072 logging_writer.py:48] [45] global_step=45, grad_norm=0.803620, loss=0.500414
I0607 11:11:14.150642 139691936032576 submission.py:296] 45) loss = 0.500, grad_norm = 0.804
I0607 11:11:14.450960 139626366891776 logging_writer.py:48] [46] global_step=46, grad_norm=0.834148, loss=0.495215
I0607 11:11:14.454744 139691936032576 submission.py:296] 46) loss = 0.495, grad_norm = 0.834
I0607 11:11:14.681864 139626358499072 logging_writer.py:48] [47] global_step=47, grad_norm=0.773581, loss=0.508408
I0607 11:11:14.685093 139691936032576 submission.py:296] 47) loss = 0.508, grad_norm = 0.774
I0607 11:11:14.938685 139626366891776 logging_writer.py:48] [48] global_step=48, grad_norm=0.789421, loss=0.430906
I0607 11:11:14.942122 139691936032576 submission.py:296] 48) loss = 0.431, grad_norm = 0.789
I0607 11:11:15.228811 139626358499072 logging_writer.py:48] [49] global_step=49, grad_norm=0.748190, loss=0.454572
I0607 11:11:15.232380 139691936032576 submission.py:296] 49) loss = 0.455, grad_norm = 0.748
I0607 11:11:15.457498 139626366891776 logging_writer.py:48] [50] global_step=50, grad_norm=0.738642, loss=0.514422
I0607 11:11:15.461746 139691936032576 submission.py:296] 50) loss = 0.514, grad_norm = 0.739
I0607 11:11:15.737439 139626358499072 logging_writer.py:48] [51] global_step=51, grad_norm=0.820660, loss=0.498779
I0607 11:11:15.742231 139691936032576 submission.py:296] 51) loss = 0.499, grad_norm = 0.821
I0607 11:11:16.048671 139626366891776 logging_writer.py:48] [52] global_step=52, grad_norm=0.688272, loss=0.592459
I0607 11:11:16.054551 139691936032576 submission.py:296] 52) loss = 0.592, grad_norm = 0.688
I0607 11:11:16.293747 139626358499072 logging_writer.py:48] [53] global_step=53, grad_norm=0.736418, loss=0.481343
I0607 11:11:16.296931 139691936032576 submission.py:296] 53) loss = 0.481, grad_norm = 0.736
I0607 11:11:16.536209 139626366891776 logging_writer.py:48] [54] global_step=54, grad_norm=0.735162, loss=0.449565
I0607 11:11:16.542062 139691936032576 submission.py:296] 54) loss = 0.450, grad_norm = 0.735
I0607 11:11:16.820897 139626358499072 logging_writer.py:48] [55] global_step=55, grad_norm=0.727779, loss=0.455469
I0607 11:11:16.825544 139691936032576 submission.py:296] 55) loss = 0.455, grad_norm = 0.728
I0607 11:11:17.034850 139626366891776 logging_writer.py:48] [56] global_step=56, grad_norm=0.680894, loss=0.440728
I0607 11:11:17.039957 139691936032576 submission.py:296] 56) loss = 0.441, grad_norm = 0.681
I0607 11:11:17.293019 139626358499072 logging_writer.py:48] [57] global_step=57, grad_norm=0.662305, loss=0.455953
I0607 11:11:17.299207 139691936032576 submission.py:296] 57) loss = 0.456, grad_norm = 0.662
I0607 11:11:17.553550 139626366891776 logging_writer.py:48] [58] global_step=58, grad_norm=0.652777, loss=0.414314
I0607 11:11:17.558211 139691936032576 submission.py:296] 58) loss = 0.414, grad_norm = 0.653
I0607 11:11:17.869719 139626358499072 logging_writer.py:48] [59] global_step=59, grad_norm=0.672686, loss=0.593236
I0607 11:11:17.874619 139691936032576 submission.py:296] 59) loss = 0.593, grad_norm = 0.673
I0607 11:11:18.144718 139626366891776 logging_writer.py:48] [60] global_step=60, grad_norm=0.687236, loss=0.407952
I0607 11:11:18.151342 139691936032576 submission.py:296] 60) loss = 0.408, grad_norm = 0.687
I0607 11:11:18.406589 139626358499072 logging_writer.py:48] [61] global_step=61, grad_norm=0.683249, loss=0.464512
I0607 11:11:18.412658 139691936032576 submission.py:296] 61) loss = 0.465, grad_norm = 0.683
I0607 11:11:18.718916 139626366891776 logging_writer.py:48] [62] global_step=62, grad_norm=0.658408, loss=0.388391
I0607 11:11:18.725469 139691936032576 submission.py:296] 62) loss = 0.388, grad_norm = 0.658
I0607 11:11:18.959508 139626358499072 logging_writer.py:48] [63] global_step=63, grad_norm=0.641939, loss=0.524976
I0607 11:11:18.963760 139691936032576 submission.py:296] 63) loss = 0.525, grad_norm = 0.642
I0607 11:11:19.188177 139626366891776 logging_writer.py:48] [64] global_step=64, grad_norm=0.650811, loss=0.377509
I0607 11:11:19.191528 139691936032576 submission.py:296] 64) loss = 0.378, grad_norm = 0.651
I0607 11:11:19.444139 139626358499072 logging_writer.py:48] [65] global_step=65, grad_norm=0.603473, loss=0.366678
I0607 11:11:19.449886 139691936032576 submission.py:296] 65) loss = 0.367, grad_norm = 0.603
I0607 11:11:19.750190 139626366891776 logging_writer.py:48] [66] global_step=66, grad_norm=0.577713, loss=0.372040
I0607 11:11:19.756185 139691936032576 submission.py:296] 66) loss = 0.372, grad_norm = 0.578
I0607 11:11:20.028074 139626358499072 logging_writer.py:48] [67] global_step=67, grad_norm=0.565979, loss=0.414986
I0607 11:11:20.033691 139691936032576 submission.py:296] 67) loss = 0.415, grad_norm = 0.566
I0607 11:11:20.263904 139626366891776 logging_writer.py:48] [68] global_step=68, grad_norm=0.649371, loss=0.431956
I0607 11:11:20.271740 139691936032576 submission.py:296] 68) loss = 0.432, grad_norm = 0.649
I0607 11:11:20.477904 139626358499072 logging_writer.py:48] [69] global_step=69, grad_norm=0.519998, loss=0.401851
I0607 11:11:20.481295 139691936032576 submission.py:296] 69) loss = 0.402, grad_norm = 0.520
I0607 11:11:20.806641 139626366891776 logging_writer.py:48] [70] global_step=70, grad_norm=0.503548, loss=0.465123
I0607 11:11:20.809942 139691936032576 submission.py:296] 70) loss = 0.465, grad_norm = 0.504
I0607 11:11:21.069567 139626358499072 logging_writer.py:48] [71] global_step=71, grad_norm=0.473870, loss=0.391271
I0607 11:11:21.072921 139691936032576 submission.py:296] 71) loss = 0.391, grad_norm = 0.474
I0607 11:11:21.393780 139626366891776 logging_writer.py:48] [72] global_step=72, grad_norm=0.465193, loss=0.390450
I0607 11:11:21.397092 139691936032576 submission.py:296] 72) loss = 0.390, grad_norm = 0.465
I0607 11:11:21.612272 139626358499072 logging_writer.py:48] [73] global_step=73, grad_norm=0.450166, loss=0.490602
I0607 11:11:21.619017 139691936032576 submission.py:296] 73) loss = 0.491, grad_norm = 0.450
I0607 11:11:21.885366 139626366891776 logging_writer.py:48] [74] global_step=74, grad_norm=0.431622, loss=0.376131
I0607 11:11:21.891072 139691936032576 submission.py:296] 74) loss = 0.376, grad_norm = 0.432
I0607 11:11:22.199845 139626358499072 logging_writer.py:48] [75] global_step=75, grad_norm=0.408193, loss=0.344870
I0607 11:11:22.205162 139691936032576 submission.py:296] 75) loss = 0.345, grad_norm = 0.408
I0607 11:11:22.442085 139626366891776 logging_writer.py:48] [76] global_step=76, grad_norm=0.387558, loss=0.348438
I0607 11:11:22.446951 139691936032576 submission.py:296] 76) loss = 0.348, grad_norm = 0.388
I0607 11:11:22.703483 139626358499072 logging_writer.py:48] [77] global_step=77, grad_norm=0.406624, loss=0.387783
I0607 11:11:22.707946 139691936032576 submission.py:296] 77) loss = 0.388, grad_norm = 0.407
I0607 11:11:22.957229 139626366891776 logging_writer.py:48] [78] global_step=78, grad_norm=0.384232, loss=0.279723
I0607 11:11:22.963132 139691936032576 submission.py:296] 78) loss = 0.280, grad_norm = 0.384
I0607 11:11:23.217369 139626358499072 logging_writer.py:48] [79] global_step=79, grad_norm=0.458017, loss=0.287885
I0607 11:11:23.220907 139691936032576 submission.py:296] 79) loss = 0.288, grad_norm = 0.458
I0607 11:11:23.590671 139626366891776 logging_writer.py:48] [80] global_step=80, grad_norm=0.371182, loss=0.331533
I0607 11:11:23.595360 139691936032576 submission.py:296] 80) loss = 0.332, grad_norm = 0.371
I0607 11:11:23.788619 139626358499072 logging_writer.py:48] [81] global_step=81, grad_norm=0.379616, loss=0.306108
I0607 11:11:23.792022 139691936032576 submission.py:296] 81) loss = 0.306, grad_norm = 0.380
I0607 11:11:24.091431 139626366891776 logging_writer.py:48] [82] global_step=82, grad_norm=0.350054, loss=0.367788
I0607 11:11:24.094898 139691936032576 submission.py:296] 82) loss = 0.368, grad_norm = 0.350
I0607 11:11:24.389803 139626358499072 logging_writer.py:48] [83] global_step=83, grad_norm=0.327349, loss=0.314500
I0607 11:11:24.393245 139691936032576 submission.py:296] 83) loss = 0.315, grad_norm = 0.327
I0607 11:11:24.602779 139626366891776 logging_writer.py:48] [84] global_step=84, grad_norm=0.312005, loss=0.306259
I0607 11:11:24.608289 139691936032576 submission.py:296] 84) loss = 0.306, grad_norm = 0.312
I0607 11:11:24.873236 139626358499072 logging_writer.py:48] [85] global_step=85, grad_norm=0.296718, loss=0.325717
I0607 11:11:24.879099 139691936032576 submission.py:296] 85) loss = 0.326, grad_norm = 0.297
I0607 11:11:25.114690 139626366891776 logging_writer.py:48] [86] global_step=86, grad_norm=0.328192, loss=0.304808
I0607 11:11:25.117999 139691936032576 submission.py:296] 86) loss = 0.305, grad_norm = 0.328
I0607 11:11:25.399300 139626358499072 logging_writer.py:48] [87] global_step=87, grad_norm=0.267108, loss=0.314317
I0607 11:11:25.402678 139691936032576 submission.py:296] 87) loss = 0.314, grad_norm = 0.267
I0607 11:11:25.705391 139626366891776 logging_writer.py:48] [88] global_step=88, grad_norm=0.306846, loss=0.268500
I0607 11:11:25.708856 139691936032576 submission.py:296] 88) loss = 0.269, grad_norm = 0.307
I0607 11:11:25.928087 139626358499072 logging_writer.py:48] [89] global_step=89, grad_norm=0.246635, loss=0.295695
I0607 11:11:25.931919 139691936032576 submission.py:296] 89) loss = 0.296, grad_norm = 0.247
I0607 11:11:26.191796 139626366891776 logging_writer.py:48] [90] global_step=90, grad_norm=0.282888, loss=0.321260
I0607 11:11:26.195356 139691936032576 submission.py:296] 90) loss = 0.321, grad_norm = 0.283
I0607 11:11:26.473096 139626358499072 logging_writer.py:48] [91] global_step=91, grad_norm=0.250166, loss=0.363676
I0607 11:11:26.479355 139691936032576 submission.py:296] 91) loss = 0.364, grad_norm = 0.250
I0607 11:11:26.758468 139626366891776 logging_writer.py:48] [92] global_step=92, grad_norm=0.286977, loss=0.310526
I0607 11:11:26.763586 139691936032576 submission.py:296] 92) loss = 0.311, grad_norm = 0.287
I0607 11:11:26.991886 139626358499072 logging_writer.py:48] [93] global_step=93, grad_norm=0.286599, loss=0.385259
I0607 11:11:26.995927 139691936032576 submission.py:296] 93) loss = 0.385, grad_norm = 0.287
I0607 11:11:27.261535 139626366891776 logging_writer.py:48] [94] global_step=94, grad_norm=0.183322, loss=0.320780
I0607 11:11:27.265115 139691936032576 submission.py:296] 94) loss = 0.321, grad_norm = 0.183
I0607 11:11:27.494341 139626358499072 logging_writer.py:48] [95] global_step=95, grad_norm=0.204221, loss=0.280119
I0607 11:11:27.497869 139691936032576 submission.py:296] 95) loss = 0.280, grad_norm = 0.204
I0607 11:11:27.756702 139626366891776 logging_writer.py:48] [96] global_step=96, grad_norm=0.246948, loss=0.319606
I0607 11:11:27.760045 139691936032576 submission.py:296] 96) loss = 0.320, grad_norm = 0.247
I0607 11:11:28.087774 139626358499072 logging_writer.py:48] [97] global_step=97, grad_norm=0.323300, loss=0.316715
I0607 11:11:28.093518 139691936032576 submission.py:296] 97) loss = 0.317, grad_norm = 0.323
I0607 11:11:28.288256 139626366891776 logging_writer.py:48] [98] global_step=98, grad_norm=0.213579, loss=0.290505
I0607 11:11:28.293478 139691936032576 submission.py:296] 98) loss = 0.291, grad_norm = 0.214
I0607 11:11:28.620627 139626358499072 logging_writer.py:48] [99] global_step=99, grad_norm=0.270810, loss=0.266201
I0607 11:11:28.624073 139691936032576 submission.py:296] 99) loss = 0.266, grad_norm = 0.271
I0607 11:11:28.808656 139626366891776 logging_writer.py:48] [100] global_step=100, grad_norm=0.257329, loss=0.408609
I0607 11:11:28.813717 139691936032576 submission.py:296] 100) loss = 0.409, grad_norm = 0.257
I0607 11:12:25.083858 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:12:27.284339 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:12:29.530420 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:12:31.764022 139691936032576 submission_runner.py:419] Time since start: 373.84s, 	Step: 308, 	{'train/ssim': 0.700502736227853, 'train/loss': 0.3075435161590576, 'validation/ssim': 0.6790234979512522, 'validation/loss': 0.32938022216384705, 'validation/num_examples': 3554, 'test/ssim': 0.6975149470512776, 'test/loss': 0.3309831020053756, 'test/num_examples': 3581, 'score': 130.0499768257141, 'total_duration': 373.8372871875763, 'accumulated_submission_time': 130.0499768257141, 'accumulated_eval_time': 243.53676652908325, 'accumulated_logging_time': 0.026914596557617188}
I0607 11:12:31.774664 139626358499072 logging_writer.py:48] [308] accumulated_eval_time=243.536767, accumulated_logging_time=0.026915, accumulated_submission_time=130.049977, global_step=308, preemption_count=0, score=130.049977, test/loss=0.330983, test/num_examples=3581, test/ssim=0.697515, total_duration=373.837287, train/loss=0.307544, train/ssim=0.700503, validation/loss=0.329380, validation/num_examples=3554, validation/ssim=0.679023
I0607 11:13:47.276075 139626366891776 logging_writer.py:48] [500] global_step=500, grad_norm=0.346357, loss=0.230595
I0607 11:13:47.282477 139691936032576 submission.py:296] 500) loss = 0.231, grad_norm = 0.346
I0607 11:13:51.964286 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:13:54.080435 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:13:56.267980 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:13:58.484607 139691936032576 submission_runner.py:419] Time since start: 460.56s, 	Step: 512, 	{'train/ssim': 0.7178097452436175, 'train/loss': 0.2908285345349993, 'validation/ssim': 0.6970486187438449, 'validation/loss': 0.3113144685125914, 'validation/num_examples': 3554, 'test/ssim': 0.7143157217563181, 'test/loss': 0.3136605713095155, 'test/num_examples': 3581, 'score': 210.0313036441803, 'total_duration': 460.5582756996155, 'accumulated_submission_time': 210.0313036441803, 'accumulated_eval_time': 250.0572874546051, 'accumulated_logging_time': 0.05283665657043457}
I0607 11:13:58.495038 139626358499072 logging_writer.py:48] [512] accumulated_eval_time=250.057287, accumulated_logging_time=0.052837, accumulated_submission_time=210.031304, global_step=512, preemption_count=0, score=210.031304, test/loss=0.313661, test/num_examples=3581, test/ssim=0.714316, total_duration=460.558276, train/loss=0.290829, train/ssim=0.717810, validation/loss=0.311314, validation/num_examples=3554, validation/ssim=0.697049
I0607 11:15:18.542618 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:15:20.771649 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:15:23.084248 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:15:25.312163 139691936032576 submission_runner.py:419] Time since start: 547.39s, 	Step: 723, 	{'train/ssim': 0.7251120294843402, 'train/loss': 0.2833503314426967, 'validation/ssim': 0.704357587291608, 'validation/loss': 0.3034392159912247, 'validation/num_examples': 3554, 'test/ssim': 0.7214979966969771, 'test/loss': 0.30559288606534485, 'test/num_examples': 3581, 'score': 289.87903881073, 'total_duration': 547.3858644962311, 'accumulated_submission_time': 289.87903881073, 'accumulated_eval_time': 256.82699275016785, 'accumulated_logging_time': 0.07946276664733887}
I0607 11:15:25.322775 139626366891776 logging_writer.py:48] [723] accumulated_eval_time=256.826993, accumulated_logging_time=0.079463, accumulated_submission_time=289.879039, global_step=723, preemption_count=0, score=289.879039, test/loss=0.305593, test/num_examples=3581, test/ssim=0.721498, total_duration=547.385864, train/loss=0.283350, train/ssim=0.725112, validation/loss=0.303439, validation/num_examples=3554, validation/ssim=0.704358
I0607 11:16:45.509021 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:16:47.672955 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:16:49.899040 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:16:52.125823 139691936032576 submission_runner.py:419] Time since start: 634.20s, 	Step: 928, 	{'train/ssim': 0.7281434195382255, 'train/loss': 0.2802517754690988, 'validation/ssim': 0.7077301489079206, 'validation/loss': 0.3002457068619865, 'validation/num_examples': 3554, 'test/ssim': 0.7247348200572465, 'test/loss': 0.30232119034269406, 'test/num_examples': 3581, 'score': 369.87013936042786, 'total_duration': 634.1994915008545, 'accumulated_submission_time': 369.87013936042786, 'accumulated_eval_time': 263.44409561157227, 'accumulated_logging_time': 0.10257959365844727}
I0607 11:16:52.138972 139626358499072 logging_writer.py:48] [928] accumulated_eval_time=263.444096, accumulated_logging_time=0.102580, accumulated_submission_time=369.870139, global_step=928, preemption_count=0, score=369.870139, test/loss=0.302321, test/num_examples=3581, test/ssim=0.724735, total_duration=634.199492, train/loss=0.280252, train/ssim=0.728143, validation/loss=0.300246, validation/num_examples=3554, validation/ssim=0.707730
I0607 11:17:09.520652 139626366891776 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.122318, loss=0.222322
I0607 11:17:09.525342 139691936032576 submission.py:296] 1000) loss = 0.222, grad_norm = 0.122
I0607 11:18:12.226422 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:18:14.335690 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:18:16.447515 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:18:18.582100 139691936032576 submission_runner.py:419] Time since start: 720.66s, 	Step: 1237, 	{'train/ssim': 0.7326398577008929, 'train/loss': 0.27629356724875315, 'validation/ssim': 0.7107573827465532, 'validation/loss': 0.2969548231306275, 'validation/num_examples': 3554, 'test/ssim': 0.7277808169418458, 'test/loss': 0.2988600998542656, 'test/num_examples': 3581, 'score': 449.7743012905121, 'total_duration': 720.6558058261871, 'accumulated_submission_time': 449.7743012905121, 'accumulated_eval_time': 269.7998745441437, 'accumulated_logging_time': 0.12771964073181152}
I0607 11:18:18.593035 139626358499072 logging_writer.py:48] [1237] accumulated_eval_time=269.799875, accumulated_logging_time=0.127720, accumulated_submission_time=449.774301, global_step=1237, preemption_count=0, score=449.774301, test/loss=0.298860, test/num_examples=3581, test/ssim=0.727781, total_duration=720.655806, train/loss=0.276294, train/ssim=0.732640, validation/loss=0.296955, validation/num_examples=3554, validation/ssim=0.710757
I0607 11:19:26.158072 139626366891776 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.507734, loss=0.335188
I0607 11:19:26.162157 139691936032576 submission.py:296] 1500) loss = 0.335, grad_norm = 0.508
I0607 11:19:38.738264 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:19:40.841270 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:19:42.984534 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:19:45.133733 139691936032576 submission_runner.py:419] Time since start: 807.21s, 	Step: 1548, 	{'train/ssim': 0.735579354422433, 'train/loss': 0.27395243304116385, 'validation/ssim': 0.7144444969576533, 'validation/loss': 0.29405714731640403, 'validation/num_examples': 3554, 'test/ssim': 0.7315647580110305, 'test/loss': 0.295750630497766, 'test/num_examples': 3581, 'score': 529.7901875972748, 'total_duration': 807.2074253559113, 'accumulated_submission_time': 529.7901875972748, 'accumulated_eval_time': 276.19536542892456, 'accumulated_logging_time': 0.14760398864746094}
I0607 11:19:45.144223 139626358499072 logging_writer.py:48] [1548] accumulated_eval_time=276.195365, accumulated_logging_time=0.147604, accumulated_submission_time=529.790188, global_step=1548, preemption_count=0, score=529.790188, test/loss=0.295751, test/num_examples=3581, test/ssim=0.731565, total_duration=807.207425, train/loss=0.273952, train/ssim=0.735579, validation/loss=0.294057, validation/num_examples=3554, validation/ssim=0.714444
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0607 11:21:05.368103 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:21:07.403171 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:21:09.491872 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:21:11.623045 139691936032576 submission_runner.py:419] Time since start: 893.70s, 	Step: 1861, 	{'train/ssim': 0.7362689290727887, 'train/loss': 0.2728473458971296, 'validation/ssim': 0.7156367605163196, 'validation/loss': 0.29284846574722145, 'validation/num_examples': 3554, 'test/ssim': 0.7327148982913292, 'test/loss': 0.2944854761894373, 'test/num_examples': 3581, 'score': 609.8886177539825, 'total_duration': 893.6966917514801, 'accumulated_submission_time': 609.8886177539825, 'accumulated_eval_time': 282.4502410888672, 'accumulated_logging_time': 0.1657571792602539}
I0607 11:21:11.635380 139626366891776 logging_writer.py:48] [1861] accumulated_eval_time=282.450241, accumulated_logging_time=0.165757, accumulated_submission_time=609.888618, global_step=1861, preemption_count=0, score=609.888618, test/loss=0.294485, test/num_examples=3581, test/ssim=0.732715, total_duration=893.696692, train/loss=0.272847, train/ssim=0.736269, validation/loss=0.292848, validation/num_examples=3554, validation/ssim=0.715637
I0607 11:21:46.005814 139626358499072 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.112669, loss=0.303480
I0607 11:21:46.009478 139691936032576 submission.py:296] 2000) loss = 0.303, grad_norm = 0.113
I0607 11:22:31.785776 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:22:33.966541 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:22:36.163084 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:22:38.386512 139691936032576 submission_runner.py:419] Time since start: 980.46s, 	Step: 2171, 	{'train/ssim': 0.7381311144147601, 'train/loss': 0.27134922572544645, 'validation/ssim': 0.7164206346502181, 'validation/loss': 0.29174031873197454, 'validation/num_examples': 3554, 'test/ssim': 0.7335698336271292, 'test/loss': 0.29344629342580636, 'test/num_examples': 3581, 'score': 689.9098920822144, 'total_duration': 980.4602289199829, 'accumulated_submission_time': 689.9098920822144, 'accumulated_eval_time': 289.0510241985321, 'accumulated_logging_time': 0.1875321865081787}
I0607 11:22:38.396615 139626366891776 logging_writer.py:48] [2171] accumulated_eval_time=289.051024, accumulated_logging_time=0.187532, accumulated_submission_time=689.909892, global_step=2171, preemption_count=0, score=689.909892, test/loss=0.293446, test/num_examples=3581, test/ssim=0.733570, total_duration=980.460229, train/loss=0.271349, train/ssim=0.738131, validation/loss=0.291740, validation/num_examples=3554, validation/ssim=0.716421
I0607 11:23:58.497609 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:24:00.637805 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:24:02.784245 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:24:04.929664 139691936032576 submission_runner.py:419] Time since start: 1067.00s, 	Step: 2485, 	{'train/ssim': 0.7351555143083844, 'train/loss': 0.27318150656563894, 'validation/ssim': 0.715792147712085, 'validation/loss': 0.2927564493242649, 'validation/num_examples': 3554, 'test/ssim': 0.7323325635733734, 'test/loss': 0.2944297076802918, 'test/num_examples': 3581, 'score': 769.8816759586334, 'total_duration': 1067.0033288002014, 'accumulated_submission_time': 769.8816759586334, 'accumulated_eval_time': 295.4830334186554, 'accumulated_logging_time': 0.20613670349121094}
I0607 11:24:04.942935 139626358499072 logging_writer.py:48] [2485] accumulated_eval_time=295.483033, accumulated_logging_time=0.206137, accumulated_submission_time=769.881676, global_step=2485, preemption_count=0, score=769.881676, test/loss=0.294430, test/num_examples=3581, test/ssim=0.732333, total_duration=1067.003329, train/loss=0.273182, train/ssim=0.735156, validation/loss=0.292756, validation/num_examples=3554, validation/ssim=0.715792
I0607 11:24:06.563938 139626366891776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.159980, loss=0.355666
I0607 11:24:06.567473 139691936032576 submission.py:296] 2500) loss = 0.356, grad_norm = 0.160
I0607 11:25:25.109090 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:25:27.213326 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:25:29.350277 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:25:31.492524 139691936032576 submission_runner.py:419] Time since start: 1153.57s, 	Step: 2800, 	{'train/ssim': 0.7403618948800224, 'train/loss': 0.26933411189488005, 'validation/ssim': 0.7186984790464969, 'validation/loss': 0.28985698745032007, 'validation/num_examples': 3554, 'test/ssim': 0.7358770682072047, 'test/loss': 0.29139465314987084, 'test/num_examples': 3581, 'score': 849.919011592865, 'total_duration': 1153.5662472248077, 'accumulated_submission_time': 849.919011592865, 'accumulated_eval_time': 301.866548538208, 'accumulated_logging_time': 0.227555513381958}
I0607 11:25:31.503246 139626358499072 logging_writer.py:48] [2800] accumulated_eval_time=301.866549, accumulated_logging_time=0.227556, accumulated_submission_time=849.919012, global_step=2800, preemption_count=0, score=849.919012, test/loss=0.291395, test/num_examples=3581, test/ssim=0.735877, total_duration=1153.566247, train/loss=0.269334, train/ssim=0.740362, validation/loss=0.289857, validation/num_examples=3554, validation/ssim=0.718698
I0607 11:26:21.790063 139626366891776 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.088630, loss=0.282520
I0607 11:26:21.793621 139691936032576 submission.py:296] 3000) loss = 0.283, grad_norm = 0.089
I0607 11:26:51.518173 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:26:53.600532 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:26:55.734542 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:26:57.870618 139691936032576 submission_runner.py:419] Time since start: 1239.94s, 	Step: 3111, 	{'train/ssim': 0.737274169921875, 'train/loss': 0.27305684770856586, 'validation/ssim': 0.7167676110984103, 'validation/loss': 0.29264121412492966, 'validation/num_examples': 3554, 'test/ssim': 0.7337262990653798, 'test/loss': 0.2944356731381772, 'test/num_examples': 3581, 'score': 929.8059141635895, 'total_duration': 1239.9443428516388, 'accumulated_submission_time': 929.8059141635895, 'accumulated_eval_time': 308.2190535068512, 'accumulated_logging_time': 0.24633216857910156}
I0607 11:26:57.880646 139626358499072 logging_writer.py:48] [3111] accumulated_eval_time=308.219054, accumulated_logging_time=0.246332, accumulated_submission_time=929.805914, global_step=3111, preemption_count=0, score=929.805914, test/loss=0.294436, test/num_examples=3581, test/ssim=0.733726, total_duration=1239.944343, train/loss=0.273057, train/ssim=0.737274, validation/loss=0.292641, validation/num_examples=3554, validation/ssim=0.716768
I0607 11:28:18.001445 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:28:20.131973 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:28:22.249462 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:28:24.393628 139691936032576 submission_runner.py:419] Time since start: 1326.47s, 	Step: 3423, 	{'train/ssim': 0.7423457418169294, 'train/loss': 0.2687666927065168, 'validation/ssim': 0.7199502321328081, 'validation/loss': 0.28986615818004363, 'validation/num_examples': 3554, 'test/ssim': 0.7371198605705459, 'test/loss': 0.29141268587685004, 'test/num_examples': 3581, 'score': 1009.7982206344604, 'total_duration': 1326.467344045639, 'accumulated_submission_time': 1009.7982206344604, 'accumulated_eval_time': 314.6112551689148, 'accumulated_logging_time': 0.264193058013916}
I0607 11:28:24.404155 139626366891776 logging_writer.py:48] [3423] accumulated_eval_time=314.611255, accumulated_logging_time=0.264193, accumulated_submission_time=1009.798221, global_step=3423, preemption_count=0, score=1009.798221, test/loss=0.291413, test/num_examples=3581, test/ssim=0.737120, total_duration=1326.467344, train/loss=0.268767, train/ssim=0.742346, validation/loss=0.289866, validation/num_examples=3554, validation/ssim=0.719950
I0607 11:28:42.198653 139626358499072 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.176924, loss=0.240956
I0607 11:28:42.202612 139691936032576 submission.py:296] 3500) loss = 0.241, grad_norm = 0.177
I0607 11:29:44.578815 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:29:46.676868 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:29:48.794800 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:29:50.918201 139691936032576 submission_runner.py:419] Time since start: 1412.99s, 	Step: 3738, 	{'train/ssim': 0.7397374425615583, 'train/loss': 0.2699845177786691, 'validation/ssim': 0.7177869016425155, 'validation/loss': 0.29063986551245075, 'validation/num_examples': 3554, 'test/ssim': 0.7348409193050126, 'test/loss': 0.29234377454359817, 'test/num_examples': 3581, 'score': 1089.843805551529, 'total_duration': 1412.9919121265411, 'accumulated_submission_time': 1089.843805551529, 'accumulated_eval_time': 320.95065808296204, 'accumulated_logging_time': 0.28264546394348145}
I0607 11:29:50.929601 139626366891776 logging_writer.py:48] [3738] accumulated_eval_time=320.950658, accumulated_logging_time=0.282645, accumulated_submission_time=1089.843806, global_step=3738, preemption_count=0, score=1089.843806, test/loss=0.292344, test/num_examples=3581, test/ssim=0.734841, total_duration=1412.991912, train/loss=0.269985, train/ssim=0.739737, validation/loss=0.290640, validation/num_examples=3554, validation/ssim=0.717787
I0607 11:30:57.947196 139626358499072 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.041294, loss=0.217031
I0607 11:30:57.950943 139691936032576 submission.py:296] 4000) loss = 0.217, grad_norm = 0.041
I0607 11:31:11.152150 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:31:13.263994 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:31:15.379354 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:31:17.511527 139691936032576 submission_runner.py:419] Time since start: 1499.59s, 	Step: 4051, 	{'train/ssim': 0.7421721049717495, 'train/loss': 0.2693119559969221, 'validation/ssim': 0.7202960407727209, 'validation/loss': 0.29001275246641106, 'validation/num_examples': 3554, 'test/ssim': 0.7374222240645071, 'test/loss': 0.2915896724902262, 'test/num_examples': 3581, 'score': 1169.9402651786804, 'total_duration': 1499.585250377655, 'accumulated_submission_time': 1169.9402651786804, 'accumulated_eval_time': 327.31008648872375, 'accumulated_logging_time': 0.3021507263183594}
I0607 11:31:17.521388 139626366891776 logging_writer.py:48] [4051] accumulated_eval_time=327.310086, accumulated_logging_time=0.302151, accumulated_submission_time=1169.940265, global_step=4051, preemption_count=0, score=1169.940265, test/loss=0.291590, test/num_examples=3581, test/ssim=0.737422, total_duration=1499.585250, train/loss=0.269312, train/ssim=0.742172, validation/loss=0.290013, validation/num_examples=3554, validation/ssim=0.720296
I0607 11:32:37.707501 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:32:39.807342 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:32:41.928564 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:32:44.068179 139691936032576 submission_runner.py:419] Time since start: 1586.14s, 	Step: 4362, 	{'train/ssim': 0.7414654323032924, 'train/loss': 0.2693918602807181, 'validation/ssim': 0.7201447065586312, 'validation/loss': 0.28968573180087576, 'validation/num_examples': 3554, 'test/ssim': 0.7371745382531066, 'test/loss': 0.29116984060841944, 'test/num_examples': 3581, 'score': 1250.001128435135, 'total_duration': 1586.1418936252594, 'accumulated_submission_time': 1250.001128435135, 'accumulated_eval_time': 333.670814037323, 'accumulated_logging_time': 0.31999802589416504}
I0607 11:32:44.078861 139626358499072 logging_writer.py:48] [4362] accumulated_eval_time=333.670814, accumulated_logging_time=0.319998, accumulated_submission_time=1250.001128, global_step=4362, preemption_count=0, score=1250.001128, test/loss=0.291170, test/num_examples=3581, test/ssim=0.737175, total_duration=1586.141894, train/loss=0.269392, train/ssim=0.741465, validation/loss=0.289686, validation/num_examples=3554, validation/ssim=0.720145
I0607 11:33:18.382881 139626366891776 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.126373, loss=0.270010
I0607 11:33:18.386331 139691936032576 submission.py:296] 4500) loss = 0.270, grad_norm = 0.126
I0607 11:34:04.233985 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:34:06.341854 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:34:08.460592 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:34:10.586164 139691936032576 submission_runner.py:419] Time since start: 1672.66s, 	Step: 4676, 	{'train/ssim': 0.7419629096984863, 'train/loss': 0.2686117036002023, 'validation/ssim': 0.719787700698157, 'validation/loss': 0.2894202615011255, 'validation/num_examples': 3554, 'test/ssim': 0.7370122777986247, 'test/loss': 0.29097383270647165, 'test/num_examples': 3581, 'score': 1330.0278244018555, 'total_duration': 1672.6598455905914, 'accumulated_submission_time': 1330.0278244018555, 'accumulated_eval_time': 340.0230231285095, 'accumulated_logging_time': 0.3388943672180176}
I0607 11:34:10.596356 139626358499072 logging_writer.py:48] [4676] accumulated_eval_time=340.023023, accumulated_logging_time=0.338894, accumulated_submission_time=1330.027824, global_step=4676, preemption_count=0, score=1330.027824, test/loss=0.290974, test/num_examples=3581, test/ssim=0.737012, total_duration=1672.659846, train/loss=0.268612, train/ssim=0.741963, validation/loss=0.289420, validation/num_examples=3554, validation/ssim=0.719788
I0607 11:35:30.612934 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:35:32.724915 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:35:34.850558 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:35:36.996269 139691936032576 submission_runner.py:419] Time since start: 1759.07s, 	Step: 4992, 	{'train/ssim': 0.7425285066877093, 'train/loss': 0.2682758058820452, 'validation/ssim': 0.7204051278049382, 'validation/loss': 0.2891994083471089, 'validation/num_examples': 3554, 'test/ssim': 0.7376630922097529, 'test/loss': 0.29061638246998045, 'test/num_examples': 3581, 'score': 1409.9172642230988, 'total_duration': 1759.0699815750122, 'accumulated_submission_time': 1409.9172642230988, 'accumulated_eval_time': 346.4063649177551, 'accumulated_logging_time': 0.3570706844329834}
I0607 11:35:37.006734 139626366891776 logging_writer.py:48] [4992] accumulated_eval_time=346.406365, accumulated_logging_time=0.357071, accumulated_submission_time=1409.917264, global_step=4992, preemption_count=0, score=1409.917264, test/loss=0.290616, test/num_examples=3581, test/ssim=0.737663, total_duration=1759.069982, train/loss=0.268276, train/ssim=0.742529, validation/loss=0.289199, validation/num_examples=3554, validation/ssim=0.720405
I0607 11:35:37.561613 139626358499072 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.154071, loss=0.288508
I0607 11:35:37.565213 139691936032576 submission.py:296] 5000) loss = 0.289, grad_norm = 0.154
I0607 11:36:57.104229 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:36:59.203685 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:37:01.324590 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:37:03.473903 139691936032576 submission_runner.py:419] Time since start: 1845.55s, 	Step: 5305, 	{'train/ssim': 0.7416144098554339, 'train/loss': 0.2692042759486607, 'validation/ssim': 0.7206635569077097, 'validation/loss': 0.28966783685635905, 'validation/num_examples': 3554, 'test/ssim': 0.7377188607188984, 'test/loss': 0.2911965999480767, 'test/num_examples': 3581, 'score': 1489.8875041007996, 'total_duration': 1845.5476071834564, 'accumulated_submission_time': 1489.8875041007996, 'accumulated_eval_time': 352.7760200500488, 'accumulated_logging_time': 0.3762383460998535}
I0607 11:37:03.485024 139626366891776 logging_writer.py:48] [5305] accumulated_eval_time=352.776020, accumulated_logging_time=0.376238, accumulated_submission_time=1489.887504, global_step=5305, preemption_count=0, score=1489.887504, test/loss=0.291197, test/num_examples=3581, test/ssim=0.737719, total_duration=1845.547607, train/loss=0.269204, train/ssim=0.741614, validation/loss=0.289668, validation/num_examples=3554, validation/ssim=0.720664
I0607 11:37:33.401773 139691936032576 spec.py:298] Evaluating on the training split.
I0607 11:37:35.459800 139691936032576 spec.py:310] Evaluating on the validation split.
I0607 11:37:37.548274 139691936032576 spec.py:326] Evaluating on the test split.
I0607 11:37:39.619073 139691936032576 submission_runner.py:419] Time since start: 1881.69s, 	Step: 5428, 	{'train/ssim': 0.7420917238507952, 'train/loss': 0.26857028688703266, 'validation/ssim': 0.7204473749868107, 'validation/loss': 0.28913057635322875, 'validation/num_examples': 3554, 'test/ssim': 0.7376864768046635, 'test/loss': 0.2906013836044401, 'test/num_examples': 3581, 'score': 1519.7491235733032, 'total_duration': 1881.6927647590637, 'accumulated_submission_time': 1519.7491235733032, 'accumulated_eval_time': 358.9933834075928, 'accumulated_logging_time': 0.39499664306640625}
I0607 11:37:39.629822 139626358499072 logging_writer.py:48] [5428] accumulated_eval_time=358.993383, accumulated_logging_time=0.394997, accumulated_submission_time=1519.749124, global_step=5428, preemption_count=0, score=1519.749124, test/loss=0.290601, test/num_examples=3581, test/ssim=0.737686, total_duration=1881.692765, train/loss=0.268570, train/ssim=0.742092, validation/loss=0.289131, validation/num_examples=3554, validation/ssim=0.720447
I0607 11:37:39.647023 139626366891776 logging_writer.py:48] [5428] global_step=5428, preemption_count=0, score=1519.749124
I0607 11:37:39.790203 139691936032576 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nadamw/fastmri_pytorch/trial_1/checkpoint_5428.
I0607 11:37:40.572791 139691936032576 submission_runner.py:581] Tuning trial 1/1
I0607 11:37:40.573029 139691936032576 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 11:37:40.581338 139691936032576 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ssim': 0.2116518361227853, 'train/loss': 0.8418434006827218, 'validation/ssim': 0.20459178437258194, 'validation/loss': 0.8573658202025887, 'validation/num_examples': 3554, 'test/ssim': 0.22654049598248568, 'test/loss': 0.8559334421251047, 'test/num_examples': 3581, 'score': 49.96812844276428, 'total_duration': 286.82483673095703, 'accumulated_submission_time': 49.96812844276428, 'accumulated_eval_time': 236.85631203651428, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (308, {'train/ssim': 0.700502736227853, 'train/loss': 0.3075435161590576, 'validation/ssim': 0.6790234979512522, 'validation/loss': 0.32938022216384705, 'validation/num_examples': 3554, 'test/ssim': 0.6975149470512776, 'test/loss': 0.3309831020053756, 'test/num_examples': 3581, 'score': 130.0499768257141, 'total_duration': 373.8372871875763, 'accumulated_submission_time': 130.0499768257141, 'accumulated_eval_time': 243.53676652908325, 'accumulated_logging_time': 0.026914596557617188, 'global_step': 308, 'preemption_count': 0}), (512, {'train/ssim': 0.7178097452436175, 'train/loss': 0.2908285345349993, 'validation/ssim': 0.6970486187438449, 'validation/loss': 0.3113144685125914, 'validation/num_examples': 3554, 'test/ssim': 0.7143157217563181, 'test/loss': 0.3136605713095155, 'test/num_examples': 3581, 'score': 210.0313036441803, 'total_duration': 460.5582756996155, 'accumulated_submission_time': 210.0313036441803, 'accumulated_eval_time': 250.0572874546051, 'accumulated_logging_time': 0.05283665657043457, 'global_step': 512, 'preemption_count': 0}), (723, {'train/ssim': 0.7251120294843402, 'train/loss': 0.2833503314426967, 'validation/ssim': 0.704357587291608, 'validation/loss': 0.3034392159912247, 'validation/num_examples': 3554, 'test/ssim': 0.7214979966969771, 'test/loss': 0.30559288606534485, 'test/num_examples': 3581, 'score': 289.87903881073, 'total_duration': 547.3858644962311, 'accumulated_submission_time': 289.87903881073, 'accumulated_eval_time': 256.82699275016785, 'accumulated_logging_time': 0.07946276664733887, 'global_step': 723, 'preemption_count': 0}), (928, {'train/ssim': 0.7281434195382255, 'train/loss': 0.2802517754690988, 'validation/ssim': 0.7077301489079206, 'validation/loss': 0.3002457068619865, 'validation/num_examples': 3554, 'test/ssim': 0.7247348200572465, 'test/loss': 0.30232119034269406, 'test/num_examples': 3581, 'score': 369.87013936042786, 'total_duration': 634.1994915008545, 'accumulated_submission_time': 369.87013936042786, 'accumulated_eval_time': 263.44409561157227, 'accumulated_logging_time': 0.10257959365844727, 'global_step': 928, 'preemption_count': 0}), (1237, {'train/ssim': 0.7326398577008929, 'train/loss': 0.27629356724875315, 'validation/ssim': 0.7107573827465532, 'validation/loss': 0.2969548231306275, 'validation/num_examples': 3554, 'test/ssim': 0.7277808169418458, 'test/loss': 0.2988600998542656, 'test/num_examples': 3581, 'score': 449.7743012905121, 'total_duration': 720.6558058261871, 'accumulated_submission_time': 449.7743012905121, 'accumulated_eval_time': 269.7998745441437, 'accumulated_logging_time': 0.12771964073181152, 'global_step': 1237, 'preemption_count': 0}), (1548, {'train/ssim': 0.735579354422433, 'train/loss': 0.27395243304116385, 'validation/ssim': 0.7144444969576533, 'validation/loss': 0.29405714731640403, 'validation/num_examples': 3554, 'test/ssim': 0.7315647580110305, 'test/loss': 0.295750630497766, 'test/num_examples': 3581, 'score': 529.7901875972748, 'total_duration': 807.2074253559113, 'accumulated_submission_time': 529.7901875972748, 'accumulated_eval_time': 276.19536542892456, 'accumulated_logging_time': 0.14760398864746094, 'global_step': 1548, 'preemption_count': 0}), (1861, {'train/ssim': 0.7362689290727887, 'train/loss': 0.2728473458971296, 'validation/ssim': 0.7156367605163196, 'validation/loss': 0.29284846574722145, 'validation/num_examples': 3554, 'test/ssim': 0.7327148982913292, 'test/loss': 0.2944854761894373, 'test/num_examples': 3581, 'score': 609.8886177539825, 'total_duration': 893.6966917514801, 'accumulated_submission_time': 609.8886177539825, 'accumulated_eval_time': 282.4502410888672, 'accumulated_logging_time': 0.1657571792602539, 'global_step': 1861, 'preemption_count': 0}), (2171, {'train/ssim': 0.7381311144147601, 'train/loss': 0.27134922572544645, 'validation/ssim': 0.7164206346502181, 'validation/loss': 0.29174031873197454, 'validation/num_examples': 3554, 'test/ssim': 0.7335698336271292, 'test/loss': 0.29344629342580636, 'test/num_examples': 3581, 'score': 689.9098920822144, 'total_duration': 980.4602289199829, 'accumulated_submission_time': 689.9098920822144, 'accumulated_eval_time': 289.0510241985321, 'accumulated_logging_time': 0.1875321865081787, 'global_step': 2171, 'preemption_count': 0}), (2485, {'train/ssim': 0.7351555143083844, 'train/loss': 0.27318150656563894, 'validation/ssim': 0.715792147712085, 'validation/loss': 0.2927564493242649, 'validation/num_examples': 3554, 'test/ssim': 0.7323325635733734, 'test/loss': 0.2944297076802918, 'test/num_examples': 3581, 'score': 769.8816759586334, 'total_duration': 1067.0033288002014, 'accumulated_submission_time': 769.8816759586334, 'accumulated_eval_time': 295.4830334186554, 'accumulated_logging_time': 0.20613670349121094, 'global_step': 2485, 'preemption_count': 0}), (2800, {'train/ssim': 0.7403618948800224, 'train/loss': 0.26933411189488005, 'validation/ssim': 0.7186984790464969, 'validation/loss': 0.28985698745032007, 'validation/num_examples': 3554, 'test/ssim': 0.7358770682072047, 'test/loss': 0.29139465314987084, 'test/num_examples': 3581, 'score': 849.919011592865, 'total_duration': 1153.5662472248077, 'accumulated_submission_time': 849.919011592865, 'accumulated_eval_time': 301.866548538208, 'accumulated_logging_time': 0.227555513381958, 'global_step': 2800, 'preemption_count': 0}), (3111, {'train/ssim': 0.737274169921875, 'train/loss': 0.27305684770856586, 'validation/ssim': 0.7167676110984103, 'validation/loss': 0.29264121412492966, 'validation/num_examples': 3554, 'test/ssim': 0.7337262990653798, 'test/loss': 0.2944356731381772, 'test/num_examples': 3581, 'score': 929.8059141635895, 'total_duration': 1239.9443428516388, 'accumulated_submission_time': 929.8059141635895, 'accumulated_eval_time': 308.2190535068512, 'accumulated_logging_time': 0.24633216857910156, 'global_step': 3111, 'preemption_count': 0}), (3423, {'train/ssim': 0.7423457418169294, 'train/loss': 0.2687666927065168, 'validation/ssim': 0.7199502321328081, 'validation/loss': 0.28986615818004363, 'validation/num_examples': 3554, 'test/ssim': 0.7371198605705459, 'test/loss': 0.29141268587685004, 'test/num_examples': 3581, 'score': 1009.7982206344604, 'total_duration': 1326.467344045639, 'accumulated_submission_time': 1009.7982206344604, 'accumulated_eval_time': 314.6112551689148, 'accumulated_logging_time': 0.264193058013916, 'global_step': 3423, 'preemption_count': 0}), (3738, {'train/ssim': 0.7397374425615583, 'train/loss': 0.2699845177786691, 'validation/ssim': 0.7177869016425155, 'validation/loss': 0.29063986551245075, 'validation/num_examples': 3554, 'test/ssim': 0.7348409193050126, 'test/loss': 0.29234377454359817, 'test/num_examples': 3581, 'score': 1089.843805551529, 'total_duration': 1412.9919121265411, 'accumulated_submission_time': 1089.843805551529, 'accumulated_eval_time': 320.95065808296204, 'accumulated_logging_time': 0.28264546394348145, 'global_step': 3738, 'preemption_count': 0}), (4051, {'train/ssim': 0.7421721049717495, 'train/loss': 0.2693119559969221, 'validation/ssim': 0.7202960407727209, 'validation/loss': 0.29001275246641106, 'validation/num_examples': 3554, 'test/ssim': 0.7374222240645071, 'test/loss': 0.2915896724902262, 'test/num_examples': 3581, 'score': 1169.9402651786804, 'total_duration': 1499.585250377655, 'accumulated_submission_time': 1169.9402651786804, 'accumulated_eval_time': 327.31008648872375, 'accumulated_logging_time': 0.3021507263183594, 'global_step': 4051, 'preemption_count': 0}), (4362, {'train/ssim': 0.7414654323032924, 'train/loss': 0.2693918602807181, 'validation/ssim': 0.7201447065586312, 'validation/loss': 0.28968573180087576, 'validation/num_examples': 3554, 'test/ssim': 0.7371745382531066, 'test/loss': 0.29116984060841944, 'test/num_examples': 3581, 'score': 1250.001128435135, 'total_duration': 1586.1418936252594, 'accumulated_submission_time': 1250.001128435135, 'accumulated_eval_time': 333.670814037323, 'accumulated_logging_time': 0.31999802589416504, 'global_step': 4362, 'preemption_count': 0}), (4676, {'train/ssim': 0.7419629096984863, 'train/loss': 0.2686117036002023, 'validation/ssim': 0.719787700698157, 'validation/loss': 0.2894202615011255, 'validation/num_examples': 3554, 'test/ssim': 0.7370122777986247, 'test/loss': 0.29097383270647165, 'test/num_examples': 3581, 'score': 1330.0278244018555, 'total_duration': 1672.6598455905914, 'accumulated_submission_time': 1330.0278244018555, 'accumulated_eval_time': 340.0230231285095, 'accumulated_logging_time': 0.3388943672180176, 'global_step': 4676, 'preemption_count': 0}), (4992, {'train/ssim': 0.7425285066877093, 'train/loss': 0.2682758058820452, 'validation/ssim': 0.7204051278049382, 'validation/loss': 0.2891994083471089, 'validation/num_examples': 3554, 'test/ssim': 0.7376630922097529, 'test/loss': 0.29061638246998045, 'test/num_examples': 3581, 'score': 1409.9172642230988, 'total_duration': 1759.0699815750122, 'accumulated_submission_time': 1409.9172642230988, 'accumulated_eval_time': 346.4063649177551, 'accumulated_logging_time': 0.3570706844329834, 'global_step': 4992, 'preemption_count': 0}), (5305, {'train/ssim': 0.7416144098554339, 'train/loss': 0.2692042759486607, 'validation/ssim': 0.7206635569077097, 'validation/loss': 0.28966783685635905, 'validation/num_examples': 3554, 'test/ssim': 0.7377188607188984, 'test/loss': 0.2911965999480767, 'test/num_examples': 3581, 'score': 1489.8875041007996, 'total_duration': 1845.5476071834564, 'accumulated_submission_time': 1489.8875041007996, 'accumulated_eval_time': 352.7760200500488, 'accumulated_logging_time': 0.3762383460998535, 'global_step': 5305, 'preemption_count': 0}), (5428, {'train/ssim': 0.7420917238507952, 'train/loss': 0.26857028688703266, 'validation/ssim': 0.7204473749868107, 'validation/loss': 0.28913057635322875, 'validation/num_examples': 3554, 'test/ssim': 0.7376864768046635, 'test/loss': 0.2906013836044401, 'test/num_examples': 3581, 'score': 1519.7491235733032, 'total_duration': 1881.6927647590637, 'accumulated_submission_time': 1519.7491235733032, 'accumulated_eval_time': 358.9933834075928, 'accumulated_logging_time': 0.39499664306640625, 'global_step': 5428, 'preemption_count': 0})], 'global_step': 5428}
I0607 11:37:40.581542 139691936032576 submission_runner.py:584] Timing: 1519.7491235733032
I0607 11:37:40.581607 139691936032576 submission_runner.py:586] Total number of evals: 20
I0607 11:37:40.581678 139691936032576 submission_runner.py:587] ====================
I0607 11:37:40.581804 139691936032576 submission_runner.py:655] Final fastmri score: 1519.7491235733032
