torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_pytorch_06-07-2023-20-32-20.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 20:32:44.240336 140536894494528 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 20:32:44.240372 140303358338880 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 20:32:44.240366 140669036386112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 20:32:44.241177 139890678531904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 20:32:44.241190 140329642977088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 20:32:44.241373 140020466906944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 20:32:44.241549 139890678531904 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.241394 140010306078528 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 20:32:44.241582 140329642977088 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.241722 140020466906944 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.241592 139654476027712 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 20:32:44.241809 140010306078528 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.242012 139654476027712 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.250991 140303358338880 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.251017 140536894494528 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.251046 140669036386112 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:32:44.263803 140020466906944 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch because --overwrite was set.
W0607 20:32:44.392760 139890678531904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:32:44.392984 140536894494528 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 20:32:44.393297 140020466906944 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch.
W0607 20:32:44.393940 140669036386112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:32:44.394580 140303358338880 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:32:44.395083 139654476027712 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:32:44.395393 140010306078528 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:32:44.396910 140329642977088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:32:44.478035 140020466906944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 20:32:44.483633 140020466906944 submission_runner.py:541] Using RNG seed 673955925
I0607 20:32:44.485443 140020466906944 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 20:32:44.485568 140020466906944 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch/trial_1.
I0607 20:32:44.487342 140020466906944 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch/trial_1/hparams.json.
I0607 20:32:44.488490 140020466906944 submission_runner.py:255] Initializing dataset.
I0607 20:32:44.488620 140020466906944 submission_runner.py:262] Initializing model.
I0607 20:32:57.596134 140020466906944 submission_runner.py:272] Initializing optimizer.
I0607 20:32:58.071122 140020466906944 submission_runner.py:279] Initializing metrics bundle.
I0607 20:32:58.071369 140020466906944 submission_runner.py:297] Initializing checkpoint and logger.
I0607 20:32:58.075233 140020466906944 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 20:32:58.075383 140020466906944 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 20:32:58.572207 140020466906944 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0607 20:32:58.574023 140020466906944 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch/trial_1/flags_0.json.
I0607 20:32:58.626111 140020466906944 submission_runner.py:332] Starting training loop.
I0607 20:33:04.259579 139980315809536 logging_writer.py:48] [0] global_step=0, grad_norm=5.283187, loss=0.907001
I0607 20:33:04.267546 140020466906944 spec.py:298] Evaluating on the training split.
I0607 20:37:47.005033 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 20:42:43.912400 140020466906944 spec.py:326] Evaluating on the test split.
I0607 20:47:36.936443 140020466906944 submission_runner.py:419] Time since start: 878.31s, 	Step: 1, 	{'train/loss': 0.9069301445226269, 'validation/loss': 0.9097762696629214, 'validation/num_examples': 89000000, 'test/loss': 0.9076563145252554, 'test/num_examples': 89274637, 'score': 5.6414008140563965, 'total_duration': 878.3107686042786, 'accumulated_submission_time': 5.6414008140563965, 'accumulated_eval_time': 872.6689121723175, 'accumulated_logging_time': 0}
I0607 20:47:36.953534 139955988715264 logging_writer.py:48] [1] accumulated_eval_time=872.668912, accumulated_logging_time=0, accumulated_submission_time=5.641401, global_step=1, preemption_count=0, score=5.641401, test/loss=0.907656, test/num_examples=89274637, total_duration=878.310769, train/loss=0.906930, validation/loss=0.909776, validation/num_examples=89000000
I0607 20:47:36.978693 140020466906944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978705 140010306078528 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978708 140303358338880 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978721 139654476027712 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978726 140329642977088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978740 139890678531904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978726 140536894494528 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:36.978752 140669036386112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:47:38.112278 139955980322560 logging_writer.py:48] [1] global_step=1, grad_norm=5.292428, loss=0.906804
I0607 20:47:39.233962 139955988715264 logging_writer.py:48] [2] global_step=2, grad_norm=5.163904, loss=0.862675
I0607 20:47:40.354364 139955980322560 logging_writer.py:48] [3] global_step=3, grad_norm=4.837976, loss=0.743882
I0607 20:47:41.474856 139955988715264 logging_writer.py:48] [4] global_step=4, grad_norm=4.318445, loss=0.550388
I0607 20:47:42.594190 139955980322560 logging_writer.py:48] [5] global_step=5, grad_norm=2.877382, loss=0.339307
I0607 20:47:43.705312 139955988715264 logging_writer.py:48] [6] global_step=6, grad_norm=1.007808, loss=0.210776
I0607 20:47:44.872194 139955980322560 logging_writer.py:48] [7] global_step=7, grad_norm=0.631208, loss=0.199694
I0607 20:47:46.045157 139955988715264 logging_writer.py:48] [8] global_step=8, grad_norm=1.625229, loss=0.277248
I0607 20:47:47.221579 139955980322560 logging_writer.py:48] [9] global_step=9, grad_norm=2.426786, loss=0.392512
I0607 20:47:48.382215 139955988715264 logging_writer.py:48] [10] global_step=10, grad_norm=3.066921, loss=0.502925
I0607 20:47:49.567162 139955980322560 logging_writer.py:48] [11] global_step=11, grad_norm=3.335775, loss=0.551915
I0607 20:47:50.751425 139955988715264 logging_writer.py:48] [12] global_step=12, grad_norm=2.947595, loss=0.484866
I0607 20:47:51.930839 139955980322560 logging_writer.py:48] [13] global_step=13, grad_norm=2.317657, loss=0.376299
I0607 20:47:53.097303 139955988715264 logging_writer.py:48] [14] global_step=14, grad_norm=1.437427, loss=0.247475
I0607 20:47:54.276515 139955980322560 logging_writer.py:48] [15] global_step=15, grad_norm=0.283558, loss=0.171821
I0607 20:47:55.449345 139955988715264 logging_writer.py:48] [16] global_step=16, grad_norm=1.173829, loss=0.205116
I0607 20:47:56.669273 139955980322560 logging_writer.py:48] [17] global_step=17, grad_norm=1.847567, loss=0.280023
I0607 20:47:57.825683 139955988715264 logging_writer.py:48] [18] global_step=18, grad_norm=1.939555, loss=0.301759
I0607 20:47:58.987244 139955980322560 logging_writer.py:48] [19] global_step=19, grad_norm=1.718710, loss=0.249421
I0607 20:48:00.154374 139955988715264 logging_writer.py:48] [20] global_step=20, grad_norm=0.948983, loss=0.176904
I0607 20:48:01.327378 139955980322560 logging_writer.py:48] [21] global_step=21, grad_norm=0.144481, loss=0.149278
I0607 20:48:02.577093 139955988715264 logging_writer.py:48] [22] global_step=22, grad_norm=0.810946, loss=0.175768
I0607 20:48:03.742659 139955980322560 logging_writer.py:48] [23] global_step=23, grad_norm=1.187760, loss=0.216455
I0607 20:48:04.914788 139955988715264 logging_writer.py:48] [24] global_step=24, grad_norm=1.403550, loss=0.248338
I0607 20:48:06.083008 139955980322560 logging_writer.py:48] [25] global_step=25, grad_norm=1.280943, loss=0.230774
I0607 20:48:07.248793 139955988715264 logging_writer.py:48] [26] global_step=26, grad_norm=0.965261, loss=0.190967
I0607 20:48:08.406605 139955980322560 logging_writer.py:48] [27] global_step=27, grad_norm=0.458630, loss=0.150983
I0607 20:48:09.587427 139955988715264 logging_writer.py:48] [28] global_step=28, grad_norm=0.195618, loss=0.140265
I0607 20:48:10.765448 139955980322560 logging_writer.py:48] [29] global_step=29, grad_norm=0.682342, loss=0.163792
I0607 20:48:11.926966 139955988715264 logging_writer.py:48] [30] global_step=30, grad_norm=0.868547, loss=0.182603
I0607 20:48:13.083052 139955980322560 logging_writer.py:48] [31] global_step=31, grad_norm=0.829113, loss=0.183452
I0607 20:48:14.243653 139955988715264 logging_writer.py:48] [32] global_step=32, grad_norm=0.655142, loss=0.165221
I0607 20:48:15.399586 139955980322560 logging_writer.py:48] [33] global_step=33, grad_norm=0.336236, loss=0.144151
I0607 20:48:16.568893 139955988715264 logging_writer.py:48] [34] global_step=34, grad_norm=0.091660, loss=0.139243
I0607 20:48:17.727088 139955980322560 logging_writer.py:48] [35] global_step=35, grad_norm=0.431547, loss=0.150871
I0607 20:48:18.903445 139955988715264 logging_writer.py:48] [36] global_step=36, grad_norm=0.622128, loss=0.164180
I0607 20:48:20.078824 139955980322560 logging_writer.py:48] [37] global_step=37, grad_norm=0.684911, loss=0.173572
I0607 20:48:21.241804 139955988715264 logging_writer.py:48] [38] global_step=38, grad_norm=0.558268, loss=0.161971
I0607 20:48:22.408812 139955980322560 logging_writer.py:48] [39] global_step=39, grad_norm=0.363933, loss=0.150346
I0607 20:48:23.574402 139955988715264 logging_writer.py:48] [40] global_step=40, grad_norm=0.123587, loss=0.140513
I0607 20:48:24.725052 139955980322560 logging_writer.py:48] [41] global_step=41, grad_norm=0.083789, loss=0.139619
I0607 20:48:25.871106 139955988715264 logging_writer.py:48] [42] global_step=42, grad_norm=0.203920, loss=0.144992
I0607 20:48:27.022167 139955980322560 logging_writer.py:48] [43] global_step=43, grad_norm=0.259870, loss=0.149131
I0607 20:48:28.188543 139955988715264 logging_writer.py:48] [44] global_step=44, grad_norm=0.270483, loss=0.150373
I0607 20:48:29.310936 139955980322560 logging_writer.py:48] [45] global_step=45, grad_norm=0.242799, loss=0.150049
I0607 20:48:30.438827 139955988715264 logging_writer.py:48] [46] global_step=46, grad_norm=0.193284, loss=0.146027
I0607 20:48:31.566339 139955980322560 logging_writer.py:48] [47] global_step=47, grad_norm=0.116751, loss=0.142607
I0607 20:48:32.707037 139955988715264 logging_writer.py:48] [48] global_step=48, grad_norm=0.033754, loss=0.140415
I0607 20:48:33.854430 139955980322560 logging_writer.py:48] [49] global_step=49, grad_norm=0.072464, loss=0.139721
I0607 20:48:34.996204 139955988715264 logging_writer.py:48] [50] global_step=50, grad_norm=0.155244, loss=0.141985
I0607 20:48:36.121264 139955980322560 logging_writer.py:48] [51] global_step=51, grad_norm=0.209002, loss=0.144481
I0607 20:48:37.255671 139955988715264 logging_writer.py:48] [52] global_step=52, grad_norm=0.232857, loss=0.147912
I0607 20:48:38.390354 139955980322560 logging_writer.py:48] [53] global_step=53, grad_norm=0.201479, loss=0.145082
I0607 20:48:39.531720 139955988715264 logging_writer.py:48] [54] global_step=54, grad_norm=0.151096, loss=0.143975
I0607 20:48:40.671751 139955980322560 logging_writer.py:48] [55] global_step=55, grad_norm=0.077826, loss=0.140798
I0607 20:48:41.813483 139955988715264 logging_writer.py:48] [56] global_step=56, grad_norm=0.028110, loss=0.144002
I0607 20:48:42.965456 139955980322560 logging_writer.py:48] [57] global_step=57, grad_norm=0.057900, loss=0.139913
I0607 20:48:44.111906 139955988715264 logging_writer.py:48] [58] global_step=58, grad_norm=0.098399, loss=0.141109
I0607 20:48:45.251764 139955980322560 logging_writer.py:48] [59] global_step=59, grad_norm=0.122533, loss=0.141789
I0607 20:48:46.397514 139955988715264 logging_writer.py:48] [60] global_step=60, grad_norm=0.131389, loss=0.141681
I0607 20:48:47.537229 139955980322560 logging_writer.py:48] [61] global_step=61, grad_norm=0.121664, loss=0.142315
I0607 20:48:48.684142 139955988715264 logging_writer.py:48] [62] global_step=62, grad_norm=0.099768, loss=0.141669
I0607 20:48:49.864654 139955980322560 logging_writer.py:48] [63] global_step=63, grad_norm=0.076349, loss=0.137994
I0607 20:48:51.011757 139955988715264 logging_writer.py:48] [64] global_step=64, grad_norm=0.035644, loss=0.139154
I0607 20:48:52.147450 139955980322560 logging_writer.py:48] [65] global_step=65, grad_norm=0.018755, loss=0.137377
I0607 20:48:53.288794 139955988715264 logging_writer.py:48] [66] global_step=66, grad_norm=0.056635, loss=0.139810
I0607 20:48:54.438288 139955980322560 logging_writer.py:48] [67] global_step=67, grad_norm=0.085376, loss=0.139764
I0607 20:48:55.602156 139955988715264 logging_writer.py:48] [68] global_step=68, grad_norm=0.110847, loss=0.142588
I0607 20:48:56.742077 139955980322560 logging_writer.py:48] [69] global_step=69, grad_norm=0.106983, loss=0.141242
I0607 20:48:57.883449 139955988715264 logging_writer.py:48] [70] global_step=70, grad_norm=0.088906, loss=0.138977
I0607 20:48:59.027634 139955980322560 logging_writer.py:48] [71] global_step=71, grad_norm=0.057456, loss=0.137342
I0607 20:49:00.166476 139955988715264 logging_writer.py:48] [72] global_step=72, grad_norm=0.027093, loss=0.136560
I0607 20:49:01.308490 139955980322560 logging_writer.py:48] [73] global_step=73, grad_norm=0.015888, loss=0.137937
I0607 20:49:02.469373 139955988715264 logging_writer.py:48] [74] global_step=74, grad_norm=0.044941, loss=0.136702
I0607 20:49:03.608433 139955980322560 logging_writer.py:48] [75] global_step=75, grad_norm=0.061555, loss=0.138225
I0607 20:49:04.753285 139955988715264 logging_writer.py:48] [76] global_step=76, grad_norm=0.062900, loss=0.143148
I0607 20:49:05.887238 139955980322560 logging_writer.py:48] [77] global_step=77, grad_norm=0.058630, loss=0.146040
I0607 20:49:07.023175 139955988715264 logging_writer.py:48] [78] global_step=78, grad_norm=0.053864, loss=0.143944
I0607 20:49:08.160850 139955980322560 logging_writer.py:48] [79] global_step=79, grad_norm=0.040737, loss=0.143102
I0607 20:49:09.302607 139955988715264 logging_writer.py:48] [80] global_step=80, grad_norm=0.020391, loss=0.142448
I0607 20:49:10.436969 139955980322560 logging_writer.py:48] [81] global_step=81, grad_norm=0.017842, loss=0.145103
I0607 20:49:11.578049 139955988715264 logging_writer.py:48] [82] global_step=82, grad_norm=0.042318, loss=0.145912
I0607 20:49:12.716137 139955980322560 logging_writer.py:48] [83] global_step=83, grad_norm=0.052422, loss=0.144505
I0607 20:49:13.863377 139955988715264 logging_writer.py:48] [84] global_step=84, grad_norm=0.057039, loss=0.145483
I0607 20:49:15.009278 139955980322560 logging_writer.py:48] [85] global_step=85, grad_norm=0.050916, loss=0.145414
I0607 20:49:16.163263 139955988715264 logging_writer.py:48] [86] global_step=86, grad_norm=0.033844, loss=0.146407
I0607 20:49:17.314196 139955980322560 logging_writer.py:48] [87] global_step=87, grad_norm=0.010546, loss=0.145306
I0607 20:49:18.459353 139955988715264 logging_writer.py:48] [88] global_step=88, grad_norm=0.021563, loss=0.145133
I0607 20:49:19.618626 139955980322560 logging_writer.py:48] [89] global_step=89, grad_norm=0.038518, loss=0.144383
I0607 20:49:20.764948 139955988715264 logging_writer.py:48] [90] global_step=90, grad_norm=0.046201, loss=0.146406
I0607 20:49:21.903455 139955980322560 logging_writer.py:48] [91] global_step=91, grad_norm=0.051018, loss=0.143734
I0607 20:49:23.050548 139955988715264 logging_writer.py:48] [92] global_step=92, grad_norm=0.031934, loss=0.144871
I0607 20:49:24.195245 139955980322560 logging_writer.py:48] [93] global_step=93, grad_norm=0.014664, loss=0.144074
I0607 20:49:25.336912 139955988715264 logging_writer.py:48] [94] global_step=94, grad_norm=0.018703, loss=0.146516
I0607 20:49:26.485598 139955980322560 logging_writer.py:48] [95] global_step=95, grad_norm=0.010905, loss=0.139924
I0607 20:49:27.629694 139955988715264 logging_writer.py:48] [96] global_step=96, grad_norm=0.022011, loss=0.139720
I0607 20:49:28.765736 139955980322560 logging_writer.py:48] [97] global_step=97, grad_norm=0.026061, loss=0.137742
I0607 20:49:29.907954 139955988715264 logging_writer.py:48] [98] global_step=98, grad_norm=0.033908, loss=0.139795
I0607 20:49:31.048425 139955980322560 logging_writer.py:48] [99] global_step=99, grad_norm=0.018575, loss=0.138452
I0607 20:49:32.195914 139955988715264 logging_writer.py:48] [100] global_step=100, grad_norm=0.012823, loss=0.139617
I0607 20:49:37.842157 140020466906944 spec.py:298] Evaluating on the training split.
I0607 20:54:06.209775 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 20:58:33.236584 140020466906944 spec.py:326] Evaluating on the test split.
I0607 21:02:46.390969 140020466906944 submission_runner.py:419] Time since start: 1787.77s, 	Step: 106, 	{'train/loss': 0.13957515264627599, 'validation/loss': 0.13864555056179775, 'validation/num_examples': 89000000, 'test/loss': 0.14220785910336436, 'test/num_examples': 89274637, 'score': 126.4824526309967, 'total_duration': 1787.765310049057, 'accumulated_submission_time': 126.4824526309967, 'accumulated_eval_time': 1661.217607498169, 'accumulated_logging_time': 0.024285554885864258}
I0607 21:02:46.400468 139955980322560 logging_writer.py:48] [106] accumulated_eval_time=1661.217607, accumulated_logging_time=0.024286, accumulated_submission_time=126.482453, global_step=106, preemption_count=0, score=126.482453, test/loss=0.142208, test/num_examples=89274637, total_duration=1787.765310, train/loss=0.139575, validation/loss=0.138646, validation/num_examples=89000000
I0607 21:04:47.391498 140020466906944 spec.py:298] Evaluating on the training split.
I0607 21:09:36.492095 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 21:14:02.253760 140020466906944 spec.py:326] Evaluating on the test split.
I0607 21:18:38.003552 140020466906944 submission_runner.py:419] Time since start: 2739.38s, 	Step: 216, 	{'train/loss': 0.13706939899368792, 'validation/loss': 0.13751578651685392, 'validation/num_examples': 89000000, 'test/loss': 0.14127039239599484, 'test/num_examples': 89274637, 'score': 247.4194314479828, 'total_duration': 2739.377865076065, 'accumulated_submission_time': 247.4194314479828, 'accumulated_eval_time': 2491.8295392990112, 'accumulated_logging_time': 0.0406341552734375}
I0607 21:18:38.013916 139955988715264 logging_writer.py:48] [216] accumulated_eval_time=2491.829539, accumulated_logging_time=0.040634, accumulated_submission_time=247.419431, global_step=216, preemption_count=0, score=247.419431, test/loss=0.141270, test/num_examples=89274637, total_duration=2739.377865, train/loss=0.137069, validation/loss=0.137516, validation/num_examples=89000000
I0607 21:20:38.811135 140020466906944 spec.py:298] Evaluating on the training split.
I0607 21:25:18.273077 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 21:29:45.092666 140020466906944 spec.py:326] Evaluating on the test split.
I0607 21:34:05.057650 140020466906944 submission_runner.py:419] Time since start: 3666.43s, 	Step: 325, 	{'train/loss': 0.13633565222015268, 'validation/loss': 0.137001797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1407173797861536, 'test/num_examples': 89274637, 'score': 368.16141057014465, 'total_duration': 3666.4319722652435, 'accumulated_submission_time': 368.16141057014465, 'accumulated_eval_time': 3298.075918197632, 'accumulated_logging_time': 0.05793356895446777}
I0607 21:34:05.068354 139955980322560 logging_writer.py:48] [325] accumulated_eval_time=3298.075918, accumulated_logging_time=0.057934, accumulated_submission_time=368.161411, global_step=325, preemption_count=0, score=368.161411, test/loss=0.140717, test/num_examples=89274637, total_duration=3666.431972, train/loss=0.136336, validation/loss=0.137002, validation/num_examples=89000000
I0607 21:36:05.709315 140020466906944 spec.py:298] Evaluating on the training split.
I0607 21:40:46.104373 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 21:45:17.375272 140020466906944 spec.py:326] Evaluating on the test split.
I0607 21:49:59.796863 140020466906944 submission_runner.py:419] Time since start: 4621.17s, 	Step: 434, 	{'train/loss': 0.134353547871683, 'validation/loss': 0.1365566966292135, 'validation/num_examples': 89000000, 'test/loss': 0.14039620233908093, 'test/num_examples': 89274637, 'score': 488.749906539917, 'total_duration': 4621.171203136444, 'accumulated_submission_time': 488.749906539917, 'accumulated_eval_time': 4132.163373231888, 'accumulated_logging_time': 0.07540583610534668}
I0607 21:49:59.810226 139955988715264 logging_writer.py:48] [434] accumulated_eval_time=4132.163373, accumulated_logging_time=0.075406, accumulated_submission_time=488.749907, global_step=434, preemption_count=0, score=488.749907, test/loss=0.140396, test/num_examples=89274637, total_duration=4621.171203, train/loss=0.134354, validation/loss=0.136557, validation/num_examples=89000000
I0607 21:51:14.807778 139955980322560 logging_writer.py:48] [500] global_step=500, grad_norm=0.013093, loss=0.129659
I0607 21:52:00.703556 140020466906944 spec.py:298] Evaluating on the training split.
I0607 21:56:23.912042 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 22:00:51.844432 140020466906944 spec.py:326] Evaluating on the test split.
I0607 22:05:25.426722 140020466906944 submission_runner.py:419] Time since start: 5546.80s, 	Step: 542, 	{'train/loss': 0.13706609797530353, 'validation/loss': 0.13610888764044943, 'validation/num_examples': 89000000, 'test/loss': 0.13980424249722798, 'test/num_examples': 89274637, 'score': 609.5961384773254, 'total_duration': 5546.801068067551, 'accumulated_submission_time': 609.5961384773254, 'accumulated_eval_time': 4936.886412143707, 'accumulated_logging_time': 0.0953531265258789}
I0607 22:05:25.437336 139955988715264 logging_writer.py:48] [542] accumulated_eval_time=4936.886412, accumulated_logging_time=0.095353, accumulated_submission_time=609.596138, global_step=542, preemption_count=0, score=609.596138, test/loss=0.139804, test/num_examples=89274637, total_duration=5546.801068, train/loss=0.137066, validation/loss=0.136109, validation/num_examples=89000000
I0607 22:07:26.156733 140020466906944 spec.py:298] Evaluating on the training split.
I0607 22:12:11.874046 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 22:16:48.210835 140020466906944 spec.py:326] Evaluating on the test split.
I0607 22:21:28.625946 140020466906944 submission_runner.py:419] Time since start: 6510.00s, 	Step: 646, 	{'train/loss': 0.13543554085681683, 'validation/loss': 0.13523298876404494, 'validation/num_examples': 89000000, 'test/loss': 0.1391183702040704, 'test/num_examples': 89274637, 'score': 730.2654688358307, 'total_duration': 6510.000258684158, 'accumulated_submission_time': 730.2654688358307, 'accumulated_eval_time': 5779.355483531952, 'accumulated_logging_time': 0.11256170272827148}
I0607 22:21:28.635904 139955980322560 logging_writer.py:48] [646] accumulated_eval_time=5779.355484, accumulated_logging_time=0.112562, accumulated_submission_time=730.265469, global_step=646, preemption_count=0, score=730.265469, test/loss=0.139118, test/num_examples=89274637, total_duration=6510.000259, train/loss=0.135436, validation/loss=0.135233, validation/num_examples=89000000
I0607 22:23:29.321933 140020466906944 spec.py:298] Evaluating on the training split.
I0607 22:28:09.154914 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 22:32:47.995858 140020466906944 spec.py:326] Evaluating on the test split.
I0607 22:37:09.570108 140020466906944 submission_runner.py:419] Time since start: 7450.94s, 	Step: 745, 	{'train/loss': 0.1340446107259474, 'validation/loss': 0.134334202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13772243061598782, 'test/num_examples': 89274637, 'score': 850.9051194190979, 'total_duration': 7450.94439291954, 'accumulated_submission_time': 850.9051194190979, 'accumulated_eval_time': 6599.60350728035, 'accumulated_logging_time': 0.12960267066955566}
I0607 22:37:09.581043 139955988715264 logging_writer.py:48] [745] accumulated_eval_time=6599.603507, accumulated_logging_time=0.129603, accumulated_submission_time=850.905119, global_step=745, preemption_count=0, score=850.905119, test/loss=0.137722, test/num_examples=89274637, total_duration=7450.944393, train/loss=0.134045, validation/loss=0.134334, validation/num_examples=89000000
I0607 22:39:10.669156 140020466906944 spec.py:298] Evaluating on the training split.
I0607 22:44:01.007567 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 22:48:31.847522 140020466906944 spec.py:326] Evaluating on the test split.
I0607 22:53:10.076580 140020466906944 submission_runner.py:419] Time since start: 8411.45s, 	Step: 849, 	{'train/loss': 0.1327035551653617, 'validation/loss': 0.13383766292134833, 'validation/num_examples': 89000000, 'test/loss': 0.13719699582760556, 'test/num_examples': 89274637, 'score': 971.9434213638306, 'total_duration': 8411.450900554657, 'accumulated_submission_time': 971.9434213638306, 'accumulated_eval_time': 7439.010788202286, 'accumulated_logging_time': 0.14737987518310547}
I0607 22:53:10.087298 139955980322560 logging_writer.py:48] [849] accumulated_eval_time=7439.010788, accumulated_logging_time=0.147380, accumulated_submission_time=971.943421, global_step=849, preemption_count=0, score=971.943421, test/loss=0.137197, test/num_examples=89274637, total_duration=8411.450901, train/loss=0.132704, validation/loss=0.133838, validation/num_examples=89000000
I0607 22:55:10.645869 140020466906944 spec.py:298] Evaluating on the training split.
I0607 22:59:56.787945 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 23:04:22.913563 140020466906944 spec.py:326] Evaluating on the test split.
I0607 23:08:48.942955 140020466906944 submission_runner.py:419] Time since start: 9350.32s, 	Step: 951, 	{'train/loss': 0.13044106635738595, 'validation/loss': 0.13178793258426966, 'validation/num_examples': 89000000, 'test/loss': 0.1350026211811984, 'test/num_examples': 89274637, 'score': 1092.4554204940796, 'total_duration': 9350.317252635956, 'accumulated_submission_time': 1092.4554204940796, 'accumulated_eval_time': 8257.30771112442, 'accumulated_logging_time': 0.1648252010345459}
I0607 23:08:48.953973 139955988715264 logging_writer.py:48] [951] accumulated_eval_time=8257.307711, accumulated_logging_time=0.164825, accumulated_submission_time=1092.455420, global_step=951, preemption_count=0, score=1092.455420, test/loss=0.135003, test/num_examples=89274637, total_duration=9350.317253, train/loss=0.130441, validation/loss=0.131788, validation/num_examples=89000000
I0607 23:09:48.184420 139955980322560 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.071580, loss=0.134633
I0607 23:10:49.539709 140020466906944 spec.py:298] Evaluating on the training split.
I0607 23:15:27.976487 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 23:19:54.186408 140020466906944 spec.py:326] Evaluating on the test split.
I0607 23:24:30.798774 140020466906944 submission_runner.py:419] Time since start: 10292.17s, 	Step: 1055, 	{'train/loss': 0.13288971463751495, 'validation/loss': 0.1325921011235955, 'validation/num_examples': 89000000, 'test/loss': 0.13578130818946932, 'test/num_examples': 89274637, 'score': 1212.9902443885803, 'total_duration': 10292.17312335968, 'accumulated_submission_time': 1212.9902443885803, 'accumulated_eval_time': 9078.566670179367, 'accumulated_logging_time': 0.18282723426818848}
I0607 23:24:30.808900 139955988715264 logging_writer.py:48] [1055] accumulated_eval_time=9078.566670, accumulated_logging_time=0.182827, accumulated_submission_time=1212.990244, global_step=1055, preemption_count=0, score=1212.990244, test/loss=0.135781, test/num_examples=89274637, total_duration=10292.173123, train/loss=0.132890, validation/loss=0.132592, validation/num_examples=89000000
I0607 23:26:31.148236 140020466906944 spec.py:298] Evaluating on the training split.
I0607 23:31:06.496534 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 23:35:37.562961 140020466906944 spec.py:326] Evaluating on the test split.
I0607 23:40:17.201187 140020466906944 submission_runner.py:419] Time since start: 11238.58s, 	Step: 1159, 	{'train/loss': 0.13090025597937938, 'validation/loss': 0.13055812359550562, 'validation/num_examples': 89000000, 'test/loss': 0.13346033543659214, 'test/num_examples': 89274637, 'score': 1333.2804412841797, 'total_duration': 11238.575444221497, 'accumulated_submission_time': 1333.2804412841797, 'accumulated_eval_time': 9904.619437217712, 'accumulated_logging_time': 0.19938874244689941}
I0607 23:40:17.210606 139955980322560 logging_writer.py:48] [1159] accumulated_eval_time=9904.619437, accumulated_logging_time=0.199389, accumulated_submission_time=1333.280441, global_step=1159, preemption_count=0, score=1333.280441, test/loss=0.133460, test/num_examples=89274637, total_duration=11238.575444, train/loss=0.130900, validation/loss=0.130558, validation/num_examples=89000000
I0607 23:42:17.457453 140020466906944 spec.py:298] Evaluating on the training split.
I0607 23:47:03.352228 140020466906944 spec.py:310] Evaluating on the validation split.
I0607 23:51:34.156684 140020466906944 spec.py:326] Evaluating on the test split.
I0607 23:56:10.751898 140020466906944 submission_runner.py:419] Time since start: 12192.13s, 	Step: 1266, 	{'train/loss': 0.1301843391963013, 'validation/loss': 0.13103703370786518, 'validation/num_examples': 89000000, 'test/loss': 0.1336007784607402, 'test/num_examples': 89274637, 'score': 1453.4758687019348, 'total_duration': 12192.126219272614, 'accumulated_submission_time': 1453.4758687019348, 'accumulated_eval_time': 10737.913741827011, 'accumulated_logging_time': 0.21547651290893555}
I0607 23:56:10.762172 139955988715264 logging_writer.py:48] [1266] accumulated_eval_time=10737.913742, accumulated_logging_time=0.215477, accumulated_submission_time=1453.475869, global_step=1266, preemption_count=0, score=1453.475869, test/loss=0.133601, test/num_examples=89274637, total_duration=12192.126219, train/loss=0.130184, validation/loss=0.131037, validation/num_examples=89000000
I0607 23:58:11.882027 140020466906944 spec.py:298] Evaluating on the training split.
I0608 00:02:59.045450 140020466906944 spec.py:310] Evaluating on the validation split.
I0608 00:07:26.180076 140020466906944 spec.py:326] Evaluating on the test split.
I0608 00:12:02.669290 140020466906944 submission_runner.py:419] Time since start: 13144.04s, 	Step: 1364, 	{'train/loss': 0.12896096381832345, 'validation/loss': 0.1303546179775281, 'validation/num_examples': 89000000, 'test/loss': 0.13289637906900703, 'test/num_examples': 89274637, 'score': 1574.5505969524384, 'total_duration': 13144.043658018112, 'accumulated_submission_time': 1574.5505969524384, 'accumulated_eval_time': 11568.700909376144, 'accumulated_logging_time': 0.23242473602294922}
I0608 00:12:02.679980 139955980322560 logging_writer.py:48] [1364] accumulated_eval_time=11568.700909, accumulated_logging_time=0.232425, accumulated_submission_time=1574.550597, global_step=1364, preemption_count=0, score=1574.550597, test/loss=0.132896, test/num_examples=89274637, total_duration=13144.043658, train/loss=0.128961, validation/loss=0.130355, validation/num_examples=89000000
I0608 00:14:03.118388 140020466906944 spec.py:298] Evaluating on the training split.
I0608 00:18:35.718573 140020466906944 spec.py:310] Evaluating on the validation split.
I0608 00:23:02.654324 140020466906944 spec.py:326] Evaluating on the test split.
I0608 00:27:21.238612 140020466906944 submission_runner.py:419] Time since start: 14062.61s, 	Step: 1462, 	{'train/loss': 0.128039867409544, 'validation/loss': 0.1295739213483146, 'validation/num_examples': 89000000, 'test/loss': 0.13214655804201142, 'test/num_examples': 89274637, 'score': 1694.9464766979218, 'total_duration': 14062.612953186035, 'accumulated_submission_time': 1694.9464766979218, 'accumulated_eval_time': 12366.82100892067, 'accumulated_logging_time': 0.25009775161743164}
I0608 00:27:21.248515 139955988715264 logging_writer.py:48] [1462] accumulated_eval_time=12366.821009, accumulated_logging_time=0.250098, accumulated_submission_time=1694.946477, global_step=1462, preemption_count=0, score=1694.946477, test/loss=0.132147, test/num_examples=89274637, total_duration=14062.612953, train/loss=0.128040, validation/loss=0.129574, validation/num_examples=89000000
I0608 00:28:14.649202 139955980322560 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.040465, loss=0.130394
I0608 00:29:21.633863 140020466906944 spec.py:298] Evaluating on the training split.
I0608 00:34:17.478943 140020466906944 spec.py:310] Evaluating on the validation split.
I0608 00:38:51.882684 140020466906944 spec.py:326] Evaluating on the test split.
I0608 00:43:23.923423 140020466906944 submission_runner.py:419] Time since start: 15025.30s, 	Step: 1553, 	{'train/loss': 0.12929566911777388, 'validation/loss': 0.12966775280898876, 'validation/num_examples': 89000000, 'test/loss': 0.1325190490553325, 'test/num_examples': 89274637, 'score': 1815.2881062030792, 'total_duration': 15025.297790050507, 'accumulated_submission_time': 1815.2881062030792, 'accumulated_eval_time': 13209.110521316528, 'accumulated_logging_time': 0.26731038093566895}
I0608 00:43:23.934397 139955988715264 logging_writer.py:48] [1553] accumulated_eval_time=13209.110521, accumulated_logging_time=0.267310, accumulated_submission_time=1815.288106, global_step=1553, preemption_count=0, score=1815.288106, test/loss=0.132519, test/num_examples=89274637, total_duration=15025.297790, train/loss=0.129296, validation/loss=0.129668, validation/num_examples=89000000
I0608 00:44:16.335361 140020466906944 spec.py:298] Evaluating on the training split.
I0608 00:48:53.733735 140020466906944 spec.py:310] Evaluating on the validation split.
I0608 00:53:19.940871 140020466906944 spec.py:326] Evaluating on the test split.
I0608 00:57:37.743442 140020466906944 submission_runner.py:419] Time since start: 15879.12s, 	Step: 1600, 	{'train/loss': 0.12746269445019315, 'validation/loss': 0.12946123595505618, 'validation/num_examples': 89000000, 'test/loss': 0.1323303840484952, 'test/num_examples': 89274637, 'score': 1867.6614327430725, 'total_duration': 15879.117801904678, 'accumulated_submission_time': 1867.6614327430725, 'accumulated_eval_time': 14010.518503427505, 'accumulated_logging_time': 0.28510546684265137}
I0608 00:57:37.753456 139955980322560 logging_writer.py:48] [1600] accumulated_eval_time=14010.518503, accumulated_logging_time=0.285105, accumulated_submission_time=1867.661433, global_step=1600, preemption_count=0, score=1867.661433, test/loss=0.132330, test/num_examples=89274637, total_duration=15879.117802, train/loss=0.127463, validation/loss=0.129461, validation/num_examples=89000000
I0608 00:57:37.768556 139955988715264 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1867.661433
I0608 00:57:45.288746 140020466906944 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/momentum/criteo1tb_pytorch/trial_1/checkpoint_1600.
I0608 00:57:45.363765 140020466906944 submission_runner.py:581] Tuning trial 1/1
I0608 00:57:45.364005 140020466906944 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0608 00:57:45.365215 140020466906944 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/loss': 0.9069301445226269, 'validation/loss': 0.9097762696629214, 'validation/num_examples': 89000000, 'test/loss': 0.9076563145252554, 'test/num_examples': 89274637, 'score': 5.6414008140563965, 'total_duration': 878.3107686042786, 'accumulated_submission_time': 5.6414008140563965, 'accumulated_eval_time': 872.6689121723175, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (106, {'train/loss': 0.13957515264627599, 'validation/loss': 0.13864555056179775, 'validation/num_examples': 89000000, 'test/loss': 0.14220785910336436, 'test/num_examples': 89274637, 'score': 126.4824526309967, 'total_duration': 1787.765310049057, 'accumulated_submission_time': 126.4824526309967, 'accumulated_eval_time': 1661.217607498169, 'accumulated_logging_time': 0.024285554885864258, 'global_step': 106, 'preemption_count': 0}), (216, {'train/loss': 0.13706939899368792, 'validation/loss': 0.13751578651685392, 'validation/num_examples': 89000000, 'test/loss': 0.14127039239599484, 'test/num_examples': 89274637, 'score': 247.4194314479828, 'total_duration': 2739.377865076065, 'accumulated_submission_time': 247.4194314479828, 'accumulated_eval_time': 2491.8295392990112, 'accumulated_logging_time': 0.0406341552734375, 'global_step': 216, 'preemption_count': 0}), (325, {'train/loss': 0.13633565222015268, 'validation/loss': 0.137001797752809, 'validation/num_examples': 89000000, 'test/loss': 0.1407173797861536, 'test/num_examples': 89274637, 'score': 368.16141057014465, 'total_duration': 3666.4319722652435, 'accumulated_submission_time': 368.16141057014465, 'accumulated_eval_time': 3298.075918197632, 'accumulated_logging_time': 0.05793356895446777, 'global_step': 325, 'preemption_count': 0}), (434, {'train/loss': 0.134353547871683, 'validation/loss': 0.1365566966292135, 'validation/num_examples': 89000000, 'test/loss': 0.14039620233908093, 'test/num_examples': 89274637, 'score': 488.749906539917, 'total_duration': 4621.171203136444, 'accumulated_submission_time': 488.749906539917, 'accumulated_eval_time': 4132.163373231888, 'accumulated_logging_time': 0.07540583610534668, 'global_step': 434, 'preemption_count': 0}), (542, {'train/loss': 0.13706609797530353, 'validation/loss': 0.13610888764044943, 'validation/num_examples': 89000000, 'test/loss': 0.13980424249722798, 'test/num_examples': 89274637, 'score': 609.5961384773254, 'total_duration': 5546.801068067551, 'accumulated_submission_time': 609.5961384773254, 'accumulated_eval_time': 4936.886412143707, 'accumulated_logging_time': 0.0953531265258789, 'global_step': 542, 'preemption_count': 0}), (646, {'train/loss': 0.13543554085681683, 'validation/loss': 0.13523298876404494, 'validation/num_examples': 89000000, 'test/loss': 0.1391183702040704, 'test/num_examples': 89274637, 'score': 730.2654688358307, 'total_duration': 6510.000258684158, 'accumulated_submission_time': 730.2654688358307, 'accumulated_eval_time': 5779.355483531952, 'accumulated_logging_time': 0.11256170272827148, 'global_step': 646, 'preemption_count': 0}), (745, {'train/loss': 0.1340446107259474, 'validation/loss': 0.134334202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13772243061598782, 'test/num_examples': 89274637, 'score': 850.9051194190979, 'total_duration': 7450.94439291954, 'accumulated_submission_time': 850.9051194190979, 'accumulated_eval_time': 6599.60350728035, 'accumulated_logging_time': 0.12960267066955566, 'global_step': 745, 'preemption_count': 0}), (849, {'train/loss': 0.1327035551653617, 'validation/loss': 0.13383766292134833, 'validation/num_examples': 89000000, 'test/loss': 0.13719699582760556, 'test/num_examples': 89274637, 'score': 971.9434213638306, 'total_duration': 8411.450900554657, 'accumulated_submission_time': 971.9434213638306, 'accumulated_eval_time': 7439.010788202286, 'accumulated_logging_time': 0.14737987518310547, 'global_step': 849, 'preemption_count': 0}), (951, {'train/loss': 0.13044106635738595, 'validation/loss': 0.13178793258426966, 'validation/num_examples': 89000000, 'test/loss': 0.1350026211811984, 'test/num_examples': 89274637, 'score': 1092.4554204940796, 'total_duration': 9350.317252635956, 'accumulated_submission_time': 1092.4554204940796, 'accumulated_eval_time': 8257.30771112442, 'accumulated_logging_time': 0.1648252010345459, 'global_step': 951, 'preemption_count': 0}), (1055, {'train/loss': 0.13288971463751495, 'validation/loss': 0.1325921011235955, 'validation/num_examples': 89000000, 'test/loss': 0.13578130818946932, 'test/num_examples': 89274637, 'score': 1212.9902443885803, 'total_duration': 10292.17312335968, 'accumulated_submission_time': 1212.9902443885803, 'accumulated_eval_time': 9078.566670179367, 'accumulated_logging_time': 0.18282723426818848, 'global_step': 1055, 'preemption_count': 0}), (1159, {'train/loss': 0.13090025597937938, 'validation/loss': 0.13055812359550562, 'validation/num_examples': 89000000, 'test/loss': 0.13346033543659214, 'test/num_examples': 89274637, 'score': 1333.2804412841797, 'total_duration': 11238.575444221497, 'accumulated_submission_time': 1333.2804412841797, 'accumulated_eval_time': 9904.619437217712, 'accumulated_logging_time': 0.19938874244689941, 'global_step': 1159, 'preemption_count': 0}), (1266, {'train/loss': 0.1301843391963013, 'validation/loss': 0.13103703370786518, 'validation/num_examples': 89000000, 'test/loss': 0.1336007784607402, 'test/num_examples': 89274637, 'score': 1453.4758687019348, 'total_duration': 12192.126219272614, 'accumulated_submission_time': 1453.4758687019348, 'accumulated_eval_time': 10737.913741827011, 'accumulated_logging_time': 0.21547651290893555, 'global_step': 1266, 'preemption_count': 0}), (1364, {'train/loss': 0.12896096381832345, 'validation/loss': 0.1303546179775281, 'validation/num_examples': 89000000, 'test/loss': 0.13289637906900703, 'test/num_examples': 89274637, 'score': 1574.5505969524384, 'total_duration': 13144.043658018112, 'accumulated_submission_time': 1574.5505969524384, 'accumulated_eval_time': 11568.700909376144, 'accumulated_logging_time': 0.23242473602294922, 'global_step': 1364, 'preemption_count': 0}), (1462, {'train/loss': 0.128039867409544, 'validation/loss': 0.1295739213483146, 'validation/num_examples': 89000000, 'test/loss': 0.13214655804201142, 'test/num_examples': 89274637, 'score': 1694.9464766979218, 'total_duration': 14062.612953186035, 'accumulated_submission_time': 1694.9464766979218, 'accumulated_eval_time': 12366.82100892067, 'accumulated_logging_time': 0.25009775161743164, 'global_step': 1462, 'preemption_count': 0}), (1553, {'train/loss': 0.12929566911777388, 'validation/loss': 0.12966775280898876, 'validation/num_examples': 89000000, 'test/loss': 0.1325190490553325, 'test/num_examples': 89274637, 'score': 1815.2881062030792, 'total_duration': 15025.297790050507, 'accumulated_submission_time': 1815.2881062030792, 'accumulated_eval_time': 13209.110521316528, 'accumulated_logging_time': 0.26731038093566895, 'global_step': 1553, 'preemption_count': 0}), (1600, {'train/loss': 0.12746269445019315, 'validation/loss': 0.12946123595505618, 'validation/num_examples': 89000000, 'test/loss': 0.1323303840484952, 'test/num_examples': 89274637, 'score': 1867.6614327430725, 'total_duration': 15879.117801904678, 'accumulated_submission_time': 1867.6614327430725, 'accumulated_eval_time': 14010.518503427505, 'accumulated_logging_time': 0.28510546684265137, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0608 00:57:45.365344 140020466906944 submission_runner.py:584] Timing: 1867.6614327430725
I0608 00:57:45.365401 140020466906944 submission_runner.py:586] Total number of evals: 17
I0608 00:57:45.365458 140020466906944 submission_runner.py:587] ====================
I0608 00:57:45.365598 140020466906944 submission_runner.py:655] Final criteo1tb score: 1867.6614327430725
