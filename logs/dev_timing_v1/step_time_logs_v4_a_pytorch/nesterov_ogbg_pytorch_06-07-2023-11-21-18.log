torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-07-2023-11-21-18.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 11:21:42.971926 140594135021376 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 11:21:42.971950 140514159986496 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 11:21:42.971975 139994304153408 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 11:21:42.972032 139713876412224 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 11:21:43.961783 140357968090944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 11:21:43.961829 140082086459200 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 11:21:43.962834 139797908186944 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 11:21:43.972297 140357968090944 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.972327 140082086459200 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.972173 140035747448640 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 11:21:43.972584 140035747448640 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.973372 139797908186944 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.980321 140594135021376 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.980381 140514159986496 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.980322 139713876412224 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:43.980455 139994304153408 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:45.283396 140035747448640 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch because --overwrite was set.
I0607 11:21:45.291943 140035747448640 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch.
W0607 11:21:45.419441 140594135021376 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.420431 139797908186944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.421534 140514159986496 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.421522 139713876412224 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.421592 139994304153408 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.421696 140357968090944 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.422495 140082086459200 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:45.423535 140035747448640 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 11:21:45.429304 140035747448640 submission_runner.py:541] Using RNG seed 4272831010
I0607 11:21:45.430762 140035747448640 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 11:21:45.430892 140035747448640 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch/trial_1.
I0607 11:21:45.431106 140035747448640 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch/trial_1/hparams.json.
I0607 11:21:45.432048 140035747448640 submission_runner.py:255] Initializing dataset.
I0607 11:21:45.432157 140035747448640 submission_runner.py:262] Initializing model.
I0607 11:21:49.524764 140035747448640 submission_runner.py:272] Initializing optimizer.
I0607 11:21:50.094874 140035747448640 submission_runner.py:279] Initializing metrics bundle.
I0607 11:21:50.095107 140035747448640 submission_runner.py:297] Initializing checkpoint and logger.
I0607 11:21:50.100129 140035747448640 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 11:21:50.100286 140035747448640 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 11:21:50.597337 140035747448640 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch/trial_1/meta_data_0.json.
I0607 11:21:50.598380 140035747448640 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch/trial_1/flags_0.json.
I0607 11:21:50.661209 140035747448640 submission_runner.py:332] Starting training loop.
I0607 11:21:50.906639 140035747448640 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:21:50.913088 140035747448640 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:21:51.065207 140035747448640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:21:55.650248 139997445355264 logging_writer.py:48] [0] global_step=0, grad_norm=2.890888, loss=0.759741
I0607 11:21:55.660616 140035747448640 submission.py:139] 0) loss = 0.760, grad_norm = 2.891
I0607 11:21:55.661861 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:21:55.668299 140035747448640 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:21:55.672881 140035747448640 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:21:55.729249 140035747448640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:22:52.046216 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:22:52.049544 140035747448640 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:22:52.054383 140035747448640 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:22:52.108089 140035747448640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:23:37.429768 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:23:37.433129 140035747448640 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:23:37.437591 140035747448640 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:23:37.496998 140035747448640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:24:22.902397 140035747448640 submission_runner.py:419] Time since start: 152.24s, 	Step: 1, 	{'train/accuracy': 0.5014427297227166, 'train/loss': 0.758456196383179, 'train/mean_average_precision': 0.024607142514149877, 'validation/accuracy': 0.4930971813913536, 'validation/loss': 0.7691068675423619, 'validation/mean_average_precision': 0.02761421364392214, 'validation/num_examples': 43793, 'test/accuracy': 0.4930633447322655, 'test/loss': 0.7702336661754132, 'test/mean_average_precision': 0.029199770391899527, 'test/num_examples': 43793, 'score': 5.000657796859741, 'total_duration': 152.24140238761902, 'accumulated_submission_time': 5.000657796859741, 'accumulated_eval_time': 147.24025297164917, 'accumulated_logging_time': 0}
I0607 11:24:22.922119 139983503038208 logging_writer.py:48] [1] accumulated_eval_time=147.240253, accumulated_logging_time=0, accumulated_submission_time=5.000658, global_step=1, preemption_count=0, score=5.000658, test/accuracy=0.493063, test/loss=0.770234, test/mean_average_precision=0.029200, test/num_examples=43793, total_duration=152.241402, train/accuracy=0.501443, train/loss=0.758456, train/mean_average_precision=0.024607, validation/accuracy=0.493097, validation/loss=0.769107, validation/mean_average_precision=0.027614, validation/num_examples=43793
I0607 11:24:23.210832 140035747448640 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217025 140082086459200 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217447 139994304153408 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217452 140594135021376 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217460 139713876412224 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217466 140514159986496 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217458 139797908186944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.217474 140357968090944 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:24:23.248661 139983511430912 logging_writer.py:48] [1] global_step=1, grad_norm=2.931649, loss=0.757413
I0607 11:24:23.253017 140035747448640 submission.py:139] 1) loss = 0.757, grad_norm = 2.932
I0607 11:24:23.561655 139983503038208 logging_writer.py:48] [2] global_step=2, grad_norm=2.935272, loss=0.756440
I0607 11:24:23.565658 140035747448640 submission.py:139] 2) loss = 0.756, grad_norm = 2.935
I0607 11:24:23.876450 139983511430912 logging_writer.py:48] [3] global_step=3, grad_norm=2.896565, loss=0.750392
I0607 11:24:23.880356 140035747448640 submission.py:139] 3) loss = 0.750, grad_norm = 2.897
I0607 11:24:24.188181 139983503038208 logging_writer.py:48] [4] global_step=4, grad_norm=2.768856, loss=0.738670
I0607 11:24:24.192046 140035747448640 submission.py:139] 4) loss = 0.739, grad_norm = 2.769
I0607 11:24:24.496715 139983511430912 logging_writer.py:48] [5] global_step=5, grad_norm=2.559591, loss=0.721347
I0607 11:24:24.500582 140035747448640 submission.py:139] 5) loss = 0.721, grad_norm = 2.560
I0607 11:24:24.819911 139983503038208 logging_writer.py:48] [6] global_step=6, grad_norm=2.368832, loss=0.702097
I0607 11:24:24.823868 140035747448640 submission.py:139] 6) loss = 0.702, grad_norm = 2.369
I0607 11:24:25.145416 139983511430912 logging_writer.py:48] [7] global_step=7, grad_norm=2.029512, loss=0.678529
I0607 11:24:25.149422 140035747448640 submission.py:139] 7) loss = 0.679, grad_norm = 2.030
I0607 11:24:25.461923 139983503038208 logging_writer.py:48] [8] global_step=8, grad_norm=1.812502, loss=0.657437
I0607 11:24:25.465964 140035747448640 submission.py:139] 8) loss = 0.657, grad_norm = 1.813
I0607 11:24:25.777853 139983511430912 logging_writer.py:48] [9] global_step=9, grad_norm=1.631206, loss=0.638661
I0607 11:24:25.781833 140035747448640 submission.py:139] 9) loss = 0.639, grad_norm = 1.631
I0607 11:24:26.097592 139983503038208 logging_writer.py:48] [10] global_step=10, grad_norm=1.535577, loss=0.625879
I0607 11:24:26.101529 140035747448640 submission.py:139] 10) loss = 0.626, grad_norm = 1.536
I0607 11:24:26.419468 139983511430912 logging_writer.py:48] [11] global_step=11, grad_norm=1.465867, loss=0.609934
I0607 11:24:26.423378 140035747448640 submission.py:139] 11) loss = 0.610, grad_norm = 1.466
I0607 11:24:26.744580 139983503038208 logging_writer.py:48] [12] global_step=12, grad_norm=1.346441, loss=0.596052
I0607 11:24:26.748567 140035747448640 submission.py:139] 12) loss = 0.596, grad_norm = 1.346
I0607 11:24:27.073302 139983511430912 logging_writer.py:48] [13] global_step=13, grad_norm=1.224414, loss=0.584588
I0607 11:24:27.076908 140035747448640 submission.py:139] 13) loss = 0.585, grad_norm = 1.224
I0607 11:24:27.402149 139983503038208 logging_writer.py:48] [14] global_step=14, grad_norm=1.167282, loss=0.570630
I0607 11:24:27.406178 140035747448640 submission.py:139] 14) loss = 0.571, grad_norm = 1.167
I0607 11:24:27.704160 139983511430912 logging_writer.py:48] [15] global_step=15, grad_norm=1.083555, loss=0.560279
I0607 11:24:27.708010 140035747448640 submission.py:139] 15) loss = 0.560, grad_norm = 1.084
I0607 11:24:27.995242 139983503038208 logging_writer.py:48] [16] global_step=16, grad_norm=1.029169, loss=0.547731
I0607 11:24:27.999237 140035747448640 submission.py:139] 16) loss = 0.548, grad_norm = 1.029
I0607 11:24:28.288199 139983511430912 logging_writer.py:48] [17] global_step=17, grad_norm=0.976787, loss=0.537636
I0607 11:24:28.292324 140035747448640 submission.py:139] 17) loss = 0.538, grad_norm = 0.977
I0607 11:24:28.585667 139983503038208 logging_writer.py:48] [18] global_step=18, grad_norm=0.915114, loss=0.526832
I0607 11:24:28.589621 140035747448640 submission.py:139] 18) loss = 0.527, grad_norm = 0.915
I0607 11:24:28.875156 139983511430912 logging_writer.py:48] [19] global_step=19, grad_norm=0.867322, loss=0.510773
I0607 11:24:28.879064 140035747448640 submission.py:139] 19) loss = 0.511, grad_norm = 0.867
I0607 11:24:29.158536 139983503038208 logging_writer.py:48] [20] global_step=20, grad_norm=0.799492, loss=0.501007
I0607 11:24:29.162505 140035747448640 submission.py:139] 20) loss = 0.501, grad_norm = 0.799
I0607 11:24:29.442488 139983511430912 logging_writer.py:48] [21] global_step=21, grad_norm=0.737520, loss=0.486592
I0607 11:24:29.446359 140035747448640 submission.py:139] 21) loss = 0.487, grad_norm = 0.738
I0607 11:24:29.746679 139983503038208 logging_writer.py:48] [22] global_step=22, grad_norm=0.681005, loss=0.481915
I0607 11:24:29.750802 140035747448640 submission.py:139] 22) loss = 0.482, grad_norm = 0.681
I0607 11:24:30.053513 139983511430912 logging_writer.py:48] [23] global_step=23, grad_norm=0.649055, loss=0.467175
I0607 11:24:30.057606 140035747448640 submission.py:139] 23) loss = 0.467, grad_norm = 0.649
I0607 11:24:30.347426 139983503038208 logging_writer.py:48] [24] global_step=24, grad_norm=0.622140, loss=0.460513
I0607 11:24:30.351504 140035747448640 submission.py:139] 24) loss = 0.461, grad_norm = 0.622
I0607 11:24:30.638928 139983511430912 logging_writer.py:48] [25] global_step=25, grad_norm=0.585018, loss=0.449215
I0607 11:24:30.642902 140035747448640 submission.py:139] 25) loss = 0.449, grad_norm = 0.585
I0607 11:24:30.925353 139983503038208 logging_writer.py:48] [26] global_step=26, grad_norm=0.556127, loss=0.436799
I0607 11:24:30.929334 140035747448640 submission.py:139] 26) loss = 0.437, grad_norm = 0.556
I0607 11:24:31.210209 139983511430912 logging_writer.py:48] [27] global_step=27, grad_norm=0.533999, loss=0.427969
I0607 11:24:31.213894 140035747448640 submission.py:139] 27) loss = 0.428, grad_norm = 0.534
I0607 11:24:31.494171 139983503038208 logging_writer.py:48] [28] global_step=28, grad_norm=0.543668, loss=0.423816
I0607 11:24:31.497887 140035747448640 submission.py:139] 28) loss = 0.424, grad_norm = 0.544
I0607 11:24:31.780245 139983511430912 logging_writer.py:48] [29] global_step=29, grad_norm=0.542920, loss=0.411773
I0607 11:24:31.784081 140035747448640 submission.py:139] 29) loss = 0.412, grad_norm = 0.543
I0607 11:24:32.066864 139983503038208 logging_writer.py:48] [30] global_step=30, grad_norm=0.527315, loss=0.403353
I0607 11:24:32.071031 140035747448640 submission.py:139] 30) loss = 0.403, grad_norm = 0.527
I0607 11:24:32.355696 139983511430912 logging_writer.py:48] [31] global_step=31, grad_norm=0.501312, loss=0.397536
I0607 11:24:32.359711 140035747448640 submission.py:139] 31) loss = 0.398, grad_norm = 0.501
I0607 11:24:32.652364 139983503038208 logging_writer.py:48] [32] global_step=32, grad_norm=0.483871, loss=0.384744
I0607 11:24:32.656282 140035747448640 submission.py:139] 32) loss = 0.385, grad_norm = 0.484
I0607 11:24:32.967749 139983511430912 logging_writer.py:48] [33] global_step=33, grad_norm=0.469971, loss=0.371357
I0607 11:24:32.971782 140035747448640 submission.py:139] 33) loss = 0.371, grad_norm = 0.470
I0607 11:24:33.279821 139983503038208 logging_writer.py:48] [34] global_step=34, grad_norm=0.451634, loss=0.366288
I0607 11:24:33.283593 140035747448640 submission.py:139] 34) loss = 0.366, grad_norm = 0.452
I0607 11:24:33.590697 139983511430912 logging_writer.py:48] [35] global_step=35, grad_norm=0.436986, loss=0.356105
I0607 11:24:33.594518 140035747448640 submission.py:139] 35) loss = 0.356, grad_norm = 0.437
I0607 11:24:33.880527 139983503038208 logging_writer.py:48] [36] global_step=36, grad_norm=0.420957, loss=0.348042
I0607 11:24:33.884341 140035747448640 submission.py:139] 36) loss = 0.348, grad_norm = 0.421
I0607 11:24:34.170484 139983511430912 logging_writer.py:48] [37] global_step=37, grad_norm=0.401554, loss=0.340017
I0607 11:24:34.174503 140035747448640 submission.py:139] 37) loss = 0.340, grad_norm = 0.402
I0607 11:24:34.458177 139983503038208 logging_writer.py:48] [38] global_step=38, grad_norm=0.388239, loss=0.335304
I0607 11:24:34.461961 140035747448640 submission.py:139] 38) loss = 0.335, grad_norm = 0.388
I0607 11:24:34.748955 139983511430912 logging_writer.py:48] [39] global_step=39, grad_norm=0.376590, loss=0.324484
I0607 11:24:34.752785 140035747448640 submission.py:139] 39) loss = 0.324, grad_norm = 0.377
I0607 11:24:35.033044 139983503038208 logging_writer.py:48] [40] global_step=40, grad_norm=0.365934, loss=0.316344
I0607 11:24:35.036931 140035747448640 submission.py:139] 40) loss = 0.316, grad_norm = 0.366
I0607 11:24:35.319518 139983511430912 logging_writer.py:48] [41] global_step=41, grad_norm=0.355441, loss=0.305510
I0607 11:24:35.323401 140035747448640 submission.py:139] 41) loss = 0.306, grad_norm = 0.355
I0607 11:24:35.607795 139983503038208 logging_writer.py:48] [42] global_step=42, grad_norm=0.347110, loss=0.297114
I0607 11:24:35.611982 140035747448640 submission.py:139] 42) loss = 0.297, grad_norm = 0.347
I0607 11:24:35.898294 139983511430912 logging_writer.py:48] [43] global_step=43, grad_norm=0.337248, loss=0.290902
I0607 11:24:35.902405 140035747448640 submission.py:139] 43) loss = 0.291, grad_norm = 0.337
I0607 11:24:36.185669 139983503038208 logging_writer.py:48] [44] global_step=44, grad_norm=0.327647, loss=0.284234
I0607 11:24:36.189863 140035747448640 submission.py:139] 44) loss = 0.284, grad_norm = 0.328
I0607 11:24:36.479161 139983511430912 logging_writer.py:48] [45] global_step=45, grad_norm=0.319872, loss=0.277454
I0607 11:24:36.483110 140035747448640 submission.py:139] 45) loss = 0.277, grad_norm = 0.320
I0607 11:24:36.781061 139983503038208 logging_writer.py:48] [46] global_step=46, grad_norm=0.311657, loss=0.270783
I0607 11:24:36.784952 140035747448640 submission.py:139] 46) loss = 0.271, grad_norm = 0.312
I0607 11:24:37.077229 139983511430912 logging_writer.py:48] [47] global_step=47, grad_norm=0.299915, loss=0.266336
I0607 11:24:37.081009 140035747448640 submission.py:139] 47) loss = 0.266, grad_norm = 0.300
I0607 11:24:37.370815 139983503038208 logging_writer.py:48] [48] global_step=48, grad_norm=0.296620, loss=0.255665
I0607 11:24:37.374635 140035747448640 submission.py:139] 48) loss = 0.256, grad_norm = 0.297
I0607 11:24:37.661661 139983511430912 logging_writer.py:48] [49] global_step=49, grad_norm=0.289896, loss=0.250247
I0607 11:24:37.665672 140035747448640 submission.py:139] 49) loss = 0.250, grad_norm = 0.290
I0607 11:24:37.952594 139983503038208 logging_writer.py:48] [50] global_step=50, grad_norm=0.279909, loss=0.242864
I0607 11:24:37.956302 140035747448640 submission.py:139] 50) loss = 0.243, grad_norm = 0.280
I0607 11:24:38.242130 139983511430912 logging_writer.py:48] [51] global_step=51, grad_norm=0.271144, loss=0.233807
I0607 11:24:38.245777 140035747448640 submission.py:139] 51) loss = 0.234, grad_norm = 0.271
I0607 11:24:38.528476 139983503038208 logging_writer.py:48] [52] global_step=52, grad_norm=0.263771, loss=0.229341
I0607 11:24:38.532571 140035747448640 submission.py:139] 52) loss = 0.229, grad_norm = 0.264
I0607 11:24:38.815014 139983511430912 logging_writer.py:48] [53] global_step=53, grad_norm=0.255396, loss=0.222852
I0607 11:24:38.819183 140035747448640 submission.py:139] 53) loss = 0.223, grad_norm = 0.255
I0607 11:24:39.107510 139983503038208 logging_writer.py:48] [54] global_step=54, grad_norm=0.248639, loss=0.217814
I0607 11:24:39.111521 140035747448640 submission.py:139] 54) loss = 0.218, grad_norm = 0.249
I0607 11:24:39.396602 139983511430912 logging_writer.py:48] [55] global_step=55, grad_norm=0.239025, loss=0.215114
I0607 11:24:39.400691 140035747448640 submission.py:139] 55) loss = 0.215, grad_norm = 0.239
I0607 11:24:39.684687 139983503038208 logging_writer.py:48] [56] global_step=56, grad_norm=0.231339, loss=0.206349
I0607 11:24:39.688601 140035747448640 submission.py:139] 56) loss = 0.206, grad_norm = 0.231
I0607 11:24:39.975128 139983511430912 logging_writer.py:48] [57] global_step=57, grad_norm=0.224518, loss=0.197710
I0607 11:24:39.978854 140035747448640 submission.py:139] 57) loss = 0.198, grad_norm = 0.225
I0607 11:24:40.264595 139983503038208 logging_writer.py:48] [58] global_step=58, grad_norm=0.216507, loss=0.198993
I0607 11:24:40.268491 140035747448640 submission.py:139] 58) loss = 0.199, grad_norm = 0.217
I0607 11:24:40.554058 139983511430912 logging_writer.py:48] [59] global_step=59, grad_norm=0.209178, loss=0.194558
I0607 11:24:40.557785 140035747448640 submission.py:139] 59) loss = 0.195, grad_norm = 0.209
I0607 11:24:40.844859 139983503038208 logging_writer.py:48] [60] global_step=60, grad_norm=0.203292, loss=0.185129
I0607 11:24:40.848780 140035747448640 submission.py:139] 60) loss = 0.185, grad_norm = 0.203
I0607 11:24:41.132013 139983511430912 logging_writer.py:48] [61] global_step=61, grad_norm=0.195407, loss=0.184047
I0607 11:24:41.135919 140035747448640 submission.py:139] 61) loss = 0.184, grad_norm = 0.195
I0607 11:24:41.419791 139983503038208 logging_writer.py:48] [62] global_step=62, grad_norm=0.191811, loss=0.177111
I0607 11:24:41.423786 140035747448640 submission.py:139] 62) loss = 0.177, grad_norm = 0.192
I0607 11:24:41.710584 139983511430912 logging_writer.py:48] [63] global_step=63, grad_norm=0.184598, loss=0.173268
I0607 11:24:41.714405 140035747448640 submission.py:139] 63) loss = 0.173, grad_norm = 0.185
I0607 11:24:42.012684 139983503038208 logging_writer.py:48] [64] global_step=64, grad_norm=0.179418, loss=0.166309
I0607 11:24:42.016589 140035747448640 submission.py:139] 64) loss = 0.166, grad_norm = 0.179
I0607 11:24:42.304425 139983511430912 logging_writer.py:48] [65] global_step=65, grad_norm=0.173143, loss=0.160529
I0607 11:24:42.308313 140035747448640 submission.py:139] 65) loss = 0.161, grad_norm = 0.173
I0607 11:24:42.593218 139983503038208 logging_writer.py:48] [66] global_step=66, grad_norm=0.168234, loss=0.160527
I0607 11:24:42.597058 140035747448640 submission.py:139] 66) loss = 0.161, grad_norm = 0.168
I0607 11:24:42.881493 139983511430912 logging_writer.py:48] [67] global_step=67, grad_norm=0.162854, loss=0.156205
I0607 11:24:42.885491 140035747448640 submission.py:139] 67) loss = 0.156, grad_norm = 0.163
I0607 11:24:43.195882 139983503038208 logging_writer.py:48] [68] global_step=68, grad_norm=0.160143, loss=0.153323
I0607 11:24:43.199969 140035747448640 submission.py:139] 68) loss = 0.153, grad_norm = 0.160
I0607 11:24:43.504974 139983511430912 logging_writer.py:48] [69] global_step=69, grad_norm=0.154217, loss=0.151727
I0607 11:24:43.509032 140035747448640 submission.py:139] 69) loss = 0.152, grad_norm = 0.154
I0607 11:24:43.802877 139983503038208 logging_writer.py:48] [70] global_step=70, grad_norm=0.147314, loss=0.147093
I0607 11:24:43.806761 140035747448640 submission.py:139] 70) loss = 0.147, grad_norm = 0.147
I0607 11:24:44.097876 139983511430912 logging_writer.py:48] [71] global_step=71, grad_norm=0.142159, loss=0.142515
I0607 11:24:44.101947 140035747448640 submission.py:139] 71) loss = 0.143, grad_norm = 0.142
I0607 11:24:44.385884 139983503038208 logging_writer.py:48] [72] global_step=72, grad_norm=0.141035, loss=0.140072
I0607 11:24:44.389715 140035747448640 submission.py:139] 72) loss = 0.140, grad_norm = 0.141
I0607 11:24:44.672478 139983511430912 logging_writer.py:48] [73] global_step=73, grad_norm=0.136769, loss=0.134346
I0607 11:24:44.676279 140035747448640 submission.py:139] 73) loss = 0.134, grad_norm = 0.137
I0607 11:24:44.959284 139983503038208 logging_writer.py:48] [74] global_step=74, grad_norm=0.129035, loss=0.136048
I0607 11:24:44.963100 140035747448640 submission.py:139] 74) loss = 0.136, grad_norm = 0.129
I0607 11:24:45.242217 139983511430912 logging_writer.py:48] [75] global_step=75, grad_norm=0.129011, loss=0.129733
I0607 11:24:45.246242 140035747448640 submission.py:139] 75) loss = 0.130, grad_norm = 0.129
I0607 11:24:45.526780 139983503038208 logging_writer.py:48] [76] global_step=76, grad_norm=0.125830, loss=0.129879
I0607 11:24:45.530790 140035747448640 submission.py:139] 76) loss = 0.130, grad_norm = 0.126
I0607 11:24:45.822229 139983511430912 logging_writer.py:48] [77] global_step=77, grad_norm=0.119976, loss=0.127198
I0607 11:24:45.826265 140035747448640 submission.py:139] 77) loss = 0.127, grad_norm = 0.120
I0607 11:24:46.112726 139983503038208 logging_writer.py:48] [78] global_step=78, grad_norm=0.114541, loss=0.126677
I0607 11:24:46.116581 140035747448640 submission.py:139] 78) loss = 0.127, grad_norm = 0.115
I0607 11:24:46.404612 139983511430912 logging_writer.py:48] [79] global_step=79, grad_norm=0.115052, loss=0.121406
I0607 11:24:46.408533 140035747448640 submission.py:139] 79) loss = 0.121, grad_norm = 0.115
I0607 11:24:46.695433 139983503038208 logging_writer.py:48] [80] global_step=80, grad_norm=0.112076, loss=0.120120
I0607 11:24:46.699581 140035747448640 submission.py:139] 80) loss = 0.120, grad_norm = 0.112
I0607 11:24:46.979146 139983511430912 logging_writer.py:48] [81] global_step=81, grad_norm=0.105892, loss=0.117365
I0607 11:24:46.983148 140035747448640 submission.py:139] 81) loss = 0.117, grad_norm = 0.106
I0607 11:24:47.264523 139983503038208 logging_writer.py:48] [82] global_step=82, grad_norm=0.103993, loss=0.118243
I0607 11:24:47.268412 140035747448640 submission.py:139] 82) loss = 0.118, grad_norm = 0.104
I0607 11:24:47.547710 139983511430912 logging_writer.py:48] [83] global_step=83, grad_norm=0.105588, loss=0.114944
I0607 11:24:47.551783 140035747448640 submission.py:139] 83) loss = 0.115, grad_norm = 0.106
I0607 11:24:47.833791 139983503038208 logging_writer.py:48] [84] global_step=84, grad_norm=0.102089, loss=0.109832
I0607 11:24:47.837725 140035747448640 submission.py:139] 84) loss = 0.110, grad_norm = 0.102
I0607 11:24:48.117738 139983511430912 logging_writer.py:48] [85] global_step=85, grad_norm=0.095958, loss=0.107742
I0607 11:24:48.121705 140035747448640 submission.py:139] 85) loss = 0.108, grad_norm = 0.096
I0607 11:24:48.405507 139983503038208 logging_writer.py:48] [86] global_step=86, grad_norm=0.093101, loss=0.109653
I0607 11:24:48.409670 140035747448640 submission.py:139] 86) loss = 0.110, grad_norm = 0.093
I0607 11:24:48.692372 139983511430912 logging_writer.py:48] [87] global_step=87, grad_norm=0.094866, loss=0.105921
I0607 11:24:48.696928 140035747448640 submission.py:139] 87) loss = 0.106, grad_norm = 0.095
I0607 11:24:48.977980 139983503038208 logging_writer.py:48] [88] global_step=88, grad_norm=0.090955, loss=0.106200
I0607 11:24:48.981894 140035747448640 submission.py:139] 88) loss = 0.106, grad_norm = 0.091
I0607 11:24:49.262528 139983511430912 logging_writer.py:48] [89] global_step=89, grad_norm=0.090438, loss=0.101166
I0607 11:24:49.266295 140035747448640 submission.py:139] 89) loss = 0.101, grad_norm = 0.090
I0607 11:24:49.549145 139983503038208 logging_writer.py:48] [90] global_step=90, grad_norm=0.084928, loss=0.099193
I0607 11:24:49.553000 140035747448640 submission.py:139] 90) loss = 0.099, grad_norm = 0.085
I0607 11:24:49.837765 139983511430912 logging_writer.py:48] [91] global_step=91, grad_norm=0.082522, loss=0.103386
I0607 11:24:49.841851 140035747448640 submission.py:139] 91) loss = 0.103, grad_norm = 0.083
I0607 11:24:50.149889 139983503038208 logging_writer.py:48] [92] global_step=92, grad_norm=0.079581, loss=0.101741
I0607 11:24:50.153804 140035747448640 submission.py:139] 92) loss = 0.102, grad_norm = 0.080
I0607 11:24:50.433805 139983511430912 logging_writer.py:48] [93] global_step=93, grad_norm=0.082444, loss=0.096401
I0607 11:24:50.437551 140035747448640 submission.py:139] 93) loss = 0.096, grad_norm = 0.082
I0607 11:24:50.720816 139983503038208 logging_writer.py:48] [94] global_step=94, grad_norm=0.076402, loss=0.096976
I0607 11:24:50.725314 140035747448640 submission.py:139] 94) loss = 0.097, grad_norm = 0.076
I0607 11:24:51.009191 139983511430912 logging_writer.py:48] [95] global_step=95, grad_norm=0.073746, loss=0.099038
I0607 11:24:51.013266 140035747448640 submission.py:139] 95) loss = 0.099, grad_norm = 0.074
I0607 11:24:51.300041 139983503038208 logging_writer.py:48] [96] global_step=96, grad_norm=0.073556, loss=0.091043
I0607 11:24:51.303848 140035747448640 submission.py:139] 96) loss = 0.091, grad_norm = 0.074
I0607 11:24:51.591094 139983511430912 logging_writer.py:48] [97] global_step=97, grad_norm=0.070822, loss=0.095050
I0607 11:24:51.595004 140035747448640 submission.py:139] 97) loss = 0.095, grad_norm = 0.071
I0607 11:24:51.867571 139983503038208 logging_writer.py:48] [98] global_step=98, grad_norm=0.071690, loss=0.091358
I0607 11:24:51.871982 140035747448640 submission.py:139] 98) loss = 0.091, grad_norm = 0.072
I0607 11:24:52.155300 139983511430912 logging_writer.py:48] [99] global_step=99, grad_norm=0.070128, loss=0.088653
I0607 11:24:52.159213 140035747448640 submission.py:139] 99) loss = 0.089, grad_norm = 0.070
I0607 11:24:52.441372 139983503038208 logging_writer.py:48] [100] global_step=100, grad_norm=0.067649, loss=0.091480
I0607 11:24:52.445180 140035747448640 submission.py:139] 100) loss = 0.091, grad_norm = 0.068
I0607 11:26:45.375862 139983511430912 logging_writer.py:48] [500] global_step=500, grad_norm=0.019231, loss=0.057134
I0607 11:26:45.380477 140035747448640 submission.py:139] 500) loss = 0.057, grad_norm = 0.019
I0607 11:28:23.030445 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:29:17.970879 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:29:21.180695 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:29:24.339164 140035747448640 submission_runner.py:419] Time since start: 453.68s, 	Step: 849, 	{'train/accuracy': 0.9866670992849017, 'train/loss': 0.054866039517709325, 'train/mean_average_precision': 0.033796691086459094, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06454612506261635, 'validation/mean_average_precision': 0.03635894699696041, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06783273274481204, 'test/mean_average_precision': 0.037723102593672404, 'test/num_examples': 43793, 'score': 244.87062168121338, 'total_duration': 453.67824244499207, 'accumulated_submission_time': 244.87062168121338, 'accumulated_eval_time': 208.54874396324158, 'accumulated_logging_time': 0.029047012329101562}
I0607 11:29:24.349040 139983503038208 logging_writer.py:48] [849] accumulated_eval_time=208.548744, accumulated_logging_time=0.029047, accumulated_submission_time=244.870622, global_step=849, preemption_count=0, score=244.870622, test/accuracy=0.983142, test/loss=0.067833, test/mean_average_precision=0.037723, test/num_examples=43793, total_duration=453.678242, train/accuracy=0.986667, train/loss=0.054866, train/mean_average_precision=0.033797, validation/accuracy=0.984118, validation/loss=0.064546, validation/mean_average_precision=0.036359, validation/num_examples=43793
I0607 11:30:07.656798 139983511430912 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.077336, loss=0.056425
I0607 11:30:07.661513 140035747448640 submission.py:139] 1000) loss = 0.056, grad_norm = 0.077
I0607 11:32:31.058574 139983503038208 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.035562, loss=0.052596
I0607 11:32:31.063560 140035747448640 submission.py:139] 1500) loss = 0.053, grad_norm = 0.036
I0607 11:33:24.464231 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:34:22.637485 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:34:25.855946 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:34:29.042749 140035747448640 submission_runner.py:419] Time since start: 758.38s, 	Step: 1691, 	{'train/accuracy': 0.9869285331049386, 'train/loss': 0.051212996598689685, 'train/mean_average_precision': 0.05641059462003932, 'validation/accuracy': 0.98414558007708, 'validation/loss': 0.06101955964364902, 'validation/mean_average_precision': 0.05496837138691235, 'validation/num_examples': 43793, 'test/accuracy': 0.983172851138426, 'test/loss': 0.06417461741833771, 'test/mean_average_precision': 0.055121686053630055, 'test/num_examples': 43793, 'score': 484.7478406429291, 'total_duration': 758.3818759918213, 'accumulated_submission_time': 484.7478406429291, 'accumulated_eval_time': 273.12707710266113, 'accumulated_logging_time': 0.0494379997253418}
I0607 11:34:29.052711 139983511430912 logging_writer.py:48] [1691] accumulated_eval_time=273.127077, accumulated_logging_time=0.049438, accumulated_submission_time=484.747841, global_step=1691, preemption_count=0, score=484.747841, test/accuracy=0.983173, test/loss=0.064175, test/mean_average_precision=0.055122, test/num_examples=43793, total_duration=758.381876, train/accuracy=0.986929, train/loss=0.051213, train/mean_average_precision=0.056411, validation/accuracy=0.984146, validation/loss=0.061020, validation/mean_average_precision=0.054968, validation/num_examples=43793
I0607 11:35:58.322178 139983503038208 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.030884, loss=0.050113
I0607 11:35:58.327384 140035747448640 submission.py:139] 2000) loss = 0.050, grad_norm = 0.031
I0607 11:38:20.554624 139983511430912 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.099217, loss=0.054417
I0607 11:38:20.560185 140035747448640 submission.py:139] 2500) loss = 0.054, grad_norm = 0.099
I0607 11:38:29.157460 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:39:26.555999 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:39:29.776775 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:39:32.918622 140035747448640 submission_runner.py:419] Time since start: 1062.26s, 	Step: 2531, 	{'train/accuracy': 0.9869769865734527, 'train/loss': 0.04944711053917592, 'train/mean_average_precision': 0.0781643545683866, 'validation/accuracy': 0.984317698933269, 'validation/loss': 0.058849889066555604, 'validation/mean_average_precision': 0.07673652885622932, 'validation/num_examples': 43793, 'test/accuracy': 0.9833004731279897, 'test/loss': 0.06204019360845489, 'test/mean_average_precision': 0.07609901467994432, 'test/num_examples': 43793, 'score': 724.6174650192261, 'total_duration': 1062.257677078247, 'accumulated_submission_time': 724.6174650192261, 'accumulated_eval_time': 336.88799691200256, 'accumulated_logging_time': 0.06980299949645996}
I0607 11:39:32.928217 139983503038208 logging_writer.py:48] [2531] accumulated_eval_time=336.887997, accumulated_logging_time=0.069803, accumulated_submission_time=724.617465, global_step=2531, preemption_count=0, score=724.617465, test/accuracy=0.983300, test/loss=0.062040, test/mean_average_precision=0.076099, test/num_examples=43793, total_duration=1062.257677, train/accuracy=0.986977, train/loss=0.049447, train/mean_average_precision=0.078164, validation/accuracy=0.984318, validation/loss=0.058850, validation/mean_average_precision=0.076737, validation/num_examples=43793
I0607 11:41:46.902984 139983511430912 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.047001, loss=0.051922
I0607 11:41:46.908421 140035747448640 submission.py:139] 3000) loss = 0.052, grad_norm = 0.047
I0607 11:43:33.011729 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:44:30.994227 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:44:34.187401 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:44:37.432720 140035747448640 submission_runner.py:419] Time since start: 1366.77s, 	Step: 3372, 	{'train/accuracy': 0.9870375753458277, 'train/loss': 0.046976977762168644, 'train/mean_average_precision': 0.10341544166482744, 'validation/accuracy': 0.984348144485661, 'validation/loss': 0.05653080694109882, 'validation/mean_average_precision': 0.09947116589364079, 'validation/num_examples': 43793, 'test/accuracy': 0.9833754457819248, 'test/loss': 0.05966058184254985, 'test/mean_average_precision': 0.0988687503670478, 'test/num_examples': 43793, 'score': 964.4668383598328, 'total_duration': 1366.7717914581299, 'accumulated_submission_time': 964.4668383598328, 'accumulated_eval_time': 401.3087651729584, 'accumulated_logging_time': 0.08973050117492676}
I0607 11:44:37.442688 139983503038208 logging_writer.py:48] [3372] accumulated_eval_time=401.308765, accumulated_logging_time=0.089731, accumulated_submission_time=964.466838, global_step=3372, preemption_count=0, score=964.466838, test/accuracy=0.983375, test/loss=0.059661, test/mean_average_precision=0.098869, test/num_examples=43793, total_duration=1366.771791, train/accuracy=0.987038, train/loss=0.046977, train/mean_average_precision=0.103415, validation/accuracy=0.984348, validation/loss=0.056531, validation/mean_average_precision=0.099471, validation/num_examples=43793
I0607 11:45:14.134096 139983511430912 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.074974, loss=0.046435
I0607 11:45:14.138792 140035747448640 submission.py:139] 3500) loss = 0.046, grad_norm = 0.075
I0607 11:47:35.285656 139983503038208 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.047660, loss=0.043402
I0607 11:47:35.290713 140035747448640 submission.py:139] 4000) loss = 0.043, grad_norm = 0.048
I0607 11:48:37.701240 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:49:34.357742 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:49:37.533957 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:49:40.853119 140035747448640 submission_runner.py:419] Time since start: 1670.19s, 	Step: 4222, 	{'train/accuracy': 0.9873532980805853, 'train/loss': 0.045398074581902606, 'train/mean_average_precision': 0.12571221651061762, 'validation/accuracy': 0.9846436693142119, 'validation/loss': 0.05416887904347381, 'validation/mean_average_precision': 0.11450609148917902, 'validation/num_examples': 43793, 'test/accuracy': 0.9836782847604603, 'test/loss': 0.05694694294791633, 'test/mean_average_precision': 0.11684672323922457, 'test/num_examples': 43793, 'score': 1204.4843015670776, 'total_duration': 1670.192153453827, 'accumulated_submission_time': 1204.4843015670776, 'accumulated_eval_time': 464.46035981178284, 'accumulated_logging_time': 0.11015605926513672}
I0607 11:49:40.863224 139983511430912 logging_writer.py:48] [4222] accumulated_eval_time=464.460360, accumulated_logging_time=0.110156, accumulated_submission_time=1204.484302, global_step=4222, preemption_count=0, score=1204.484302, test/accuracy=0.983678, test/loss=0.056947, test/mean_average_precision=0.116847, test/num_examples=43793, total_duration=1670.192153, train/accuracy=0.987353, train/loss=0.045398, train/mean_average_precision=0.125712, validation/accuracy=0.984644, validation/loss=0.054169, validation/mean_average_precision=0.114506, validation/num_examples=43793
I0607 11:50:59.905170 139983503038208 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.054834, loss=0.044820
I0607 11:50:59.911026 140035747448640 submission.py:139] 4500) loss = 0.045, grad_norm = 0.055
I0607 11:53:21.940836 139983511430912 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.033769, loss=0.046396
I0607 11:53:21.950929 140035747448640 submission.py:139] 5000) loss = 0.046, grad_norm = 0.034
I0607 11:53:41.066651 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:54:40.251360 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:54:43.497914 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:54:46.737429 140035747448640 submission_runner.py:419] Time since start: 1976.08s, 	Step: 5067, 	{'train/accuracy': 0.9877052174116788, 'train/loss': 0.043317804466824095, 'train/mean_average_precision': 0.14292677582047467, 'validation/accuracy': 0.984981411975413, 'validation/loss': 0.052665845550118655, 'validation/mean_average_precision': 0.12850682357266152, 'validation/num_examples': 43793, 'test/accuracy': 0.9839952337609442, 'test/loss': 0.055457624866323336, 'test/mean_average_precision': 0.1348806392431518, 'test/num_examples': 43793, 'score': 1444.4454276561737, 'total_duration': 1976.0765132904053, 'accumulated_submission_time': 1444.4454276561737, 'accumulated_eval_time': 530.1308808326721, 'accumulated_logging_time': 0.1307971477508545}
I0607 11:54:46.747493 139983503038208 logging_writer.py:48] [5067] accumulated_eval_time=530.130881, accumulated_logging_time=0.130797, accumulated_submission_time=1444.445428, global_step=5067, preemption_count=0, score=1444.445428, test/accuracy=0.983995, test/loss=0.055458, test/mean_average_precision=0.134881, test/num_examples=43793, total_duration=1976.076513, train/accuracy=0.987705, train/loss=0.043318, train/mean_average_precision=0.142927, validation/accuracy=0.984981, validation/loss=0.052666, validation/mean_average_precision=0.128507, validation/num_examples=43793
I0607 11:56:52.421165 139983511430912 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.092500, loss=0.046964
I0607 11:56:52.432622 140035747448640 submission.py:139] 5500) loss = 0.047, grad_norm = 0.092
I0607 11:58:46.804947 140035747448640 spec.py:298] Evaluating on the training split.
I0607 11:59:46.267911 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 11:59:49.493810 140035747448640 spec.py:326] Evaluating on the test split.
I0607 11:59:52.749389 140035747448640 submission_runner.py:419] Time since start: 2282.09s, 	Step: 5898, 	{'train/accuracy': 0.9879950742562175, 'train/loss': 0.04185346241955117, 'train/mean_average_precision': 0.17634887941009497, 'validation/accuracy': 0.9851653031118602, 'validation/loss': 0.05200587477378955, 'validation/mean_average_precision': 0.1473855490705816, 'validation/num_examples': 43793, 'test/accuracy': 0.9841934058602502, 'test/loss': 0.055014080538320506, 'test/mean_average_precision': 0.1521352317315992, 'test/num_examples': 43793, 'score': 1684.2681124210358, 'total_duration': 2282.0883870124817, 'accumulated_submission_time': 1684.2681124210358, 'accumulated_eval_time': 596.0750288963318, 'accumulated_logging_time': 0.1520678997039795}
I0607 11:59:52.759871 139983503038208 logging_writer.py:48] [5898] accumulated_eval_time=596.075029, accumulated_logging_time=0.152068, accumulated_submission_time=1684.268112, global_step=5898, preemption_count=0, score=1684.268112, test/accuracy=0.984193, test/loss=0.055014, test/mean_average_precision=0.152135, test/num_examples=43793, total_duration=2282.088387, train/accuracy=0.987995, train/loss=0.041853, train/mean_average_precision=0.176349, validation/accuracy=0.985165, validation/loss=0.052006, validation/mean_average_precision=0.147386, validation/num_examples=43793
I0607 12:00:22.178350 139983511430912 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.057676, loss=0.046758
I0607 12:00:22.183335 140035747448640 submission.py:139] 6000) loss = 0.047, grad_norm = 0.058
I0607 12:02:44.948813 139983503038208 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.043508, loss=0.046141
I0607 12:02:44.954924 140035747448640 submission.py:139] 6500) loss = 0.046, grad_norm = 0.044
I0607 12:03:52.810852 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:04:52.104455 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:04:55.330609 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:04:58.514349 140035747448640 submission_runner.py:419] Time since start: 2587.85s, 	Step: 6737, 	{'train/accuracy': 0.9881525812513438, 'train/loss': 0.040990684773250026, 'train/mean_average_precision': 0.19780221096955947, 'validation/accuracy': 0.985335798205255, 'validation/loss': 0.050600815702313943, 'validation/mean_average_precision': 0.1614643534481779, 'validation/num_examples': 43793, 'test/accuracy': 0.9843370332478448, 'test/loss': 0.05331594861045767, 'test/mean_average_precision': 0.16900614352245985, 'test/num_examples': 43793, 'score': 1924.082769393921, 'total_duration': 2587.853394508362, 'accumulated_submission_time': 1924.082769393921, 'accumulated_eval_time': 661.7782618999481, 'accumulated_logging_time': 0.1736757755279541}
I0607 12:04:58.524473 139983511430912 logging_writer.py:48] [6737] accumulated_eval_time=661.778262, accumulated_logging_time=0.173676, accumulated_submission_time=1924.082769, global_step=6737, preemption_count=0, score=1924.082769, test/accuracy=0.984337, test/loss=0.053316, test/mean_average_precision=0.169006, test/num_examples=43793, total_duration=2587.853395, train/accuracy=0.988153, train/loss=0.040991, train/mean_average_precision=0.197802, validation/accuracy=0.985336, validation/loss=0.050601, validation/mean_average_precision=0.161464, validation/num_examples=43793
I0607 12:06:14.353124 139983503038208 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.048924, loss=0.042491
I0607 12:06:14.358270 140035747448640 submission.py:139] 7000) loss = 0.042, grad_norm = 0.049
I0607 12:08:37.302618 139983511430912 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.030126, loss=0.041560
I0607 12:08:37.308204 140035747448640 submission.py:139] 7500) loss = 0.042, grad_norm = 0.030
I0607 12:08:58.567594 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:09:58.162478 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:10:01.386119 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:10:04.639243 140035747448640 submission_runner.py:419] Time since start: 2893.98s, 	Step: 7575, 	{'train/accuracy': 0.9887259968850818, 'train/loss': 0.03897515415576127, 'train/mean_average_precision': 0.205133959224007, 'validation/accuracy': 0.9856394418477771, 'validation/loss': 0.0492928874572037, 'validation/mean_average_precision': 0.17512369957228016, 'validation/num_examples': 43793, 'test/accuracy': 0.984693785146064, 'test/loss': 0.05182997047109362, 'test/mean_average_precision': 0.17716824041304002, 'test/num_examples': 43793, 'score': 2163.8809053897858, 'total_duration': 2893.978265762329, 'accumulated_submission_time': 2163.8809053897858, 'accumulated_eval_time': 727.8495955467224, 'accumulated_logging_time': 0.19905996322631836}
I0607 12:10:04.649405 139983503038208 logging_writer.py:48] [7575] accumulated_eval_time=727.849596, accumulated_logging_time=0.199060, accumulated_submission_time=2163.880905, global_step=7575, preemption_count=0, score=2163.880905, test/accuracy=0.984694, test/loss=0.051830, test/mean_average_precision=0.177168, test/num_examples=43793, total_duration=2893.978266, train/accuracy=0.988726, train/loss=0.038975, train/mean_average_precision=0.205134, validation/accuracy=0.985639, validation/loss=0.049293, validation/mean_average_precision=0.175124, validation/num_examples=43793
I0607 12:12:06.558803 139983511430912 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.047492, loss=0.042987
I0607 12:12:06.564602 140035747448640 submission.py:139] 8000) loss = 0.043, grad_norm = 0.047
I0607 12:14:04.813509 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:15:05.474659 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:15:08.759706 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:15:11.993688 140035747448640 submission_runner.py:419] Time since start: 3201.33s, 	Step: 8414, 	{'train/accuracy': 0.9886685418693208, 'train/loss': 0.03896808204018168, 'train/mean_average_precision': 0.22558552911016885, 'validation/accuracy': 0.9856568972978151, 'validation/loss': 0.0494062463972763, 'validation/mean_average_precision': 0.18121408958693905, 'validation/num_examples': 43793, 'test/accuracy': 0.9846234456336642, 'test/loss': 0.05240037337329348, 'test/mean_average_precision': 0.18108975357301407, 'test/num_examples': 43793, 'score': 2403.8023483753204, 'total_duration': 3201.332774877548, 'accumulated_submission_time': 2403.8023483753204, 'accumulated_eval_time': 795.0295650959015, 'accumulated_logging_time': 0.22578215599060059}
I0607 12:15:12.004192 139983503038208 logging_writer.py:48] [8414] accumulated_eval_time=795.029565, accumulated_logging_time=0.225782, accumulated_submission_time=2403.802348, global_step=8414, preemption_count=0, score=2403.802348, test/accuracy=0.984623, test/loss=0.052400, test/mean_average_precision=0.181090, test/num_examples=43793, total_duration=3201.332775, train/accuracy=0.988669, train/loss=0.038968, train/mean_average_precision=0.225586, validation/accuracy=0.985657, validation/loss=0.049406, validation/mean_average_precision=0.181214, validation/num_examples=43793
I0607 12:15:39.154179 139983511430912 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.045707, loss=0.043429
I0607 12:15:39.160046 140035747448640 submission.py:139] 8500) loss = 0.043, grad_norm = 0.046
I0607 12:18:03.962794 139983503038208 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.068990, loss=0.043592
I0607 12:18:03.968287 140035747448640 submission.py:139] 9000) loss = 0.044, grad_norm = 0.069
I0607 12:19:12.053292 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:20:12.655365 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:20:15.930960 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:20:19.151429 140035747448640 submission_runner.py:419] Time since start: 3508.49s, 	Step: 9240, 	{'train/accuracy': 0.9888060414290691, 'train/loss': 0.038501741990874284, 'train/mean_average_precision': 0.2286527459117706, 'validation/accuracy': 0.9856871398798578, 'validation/loss': 0.04894208809400288, 'validation/mean_average_precision': 0.1870747796519883, 'validation/num_examples': 43793, 'test/accuracy': 0.9848150892153522, 'test/loss': 0.05162589835561383, 'test/mean_average_precision': 0.1853289291267217, 'test/num_examples': 43793, 'score': 2643.618306159973, 'total_duration': 3508.4905247688293, 'accumulated_submission_time': 2643.618306159973, 'accumulated_eval_time': 862.127466917038, 'accumulated_logging_time': 0.24776625633239746}
I0607 12:20:19.162227 139983511430912 logging_writer.py:48] [9240] accumulated_eval_time=862.127467, accumulated_logging_time=0.247766, accumulated_submission_time=2643.618306, global_step=9240, preemption_count=0, score=2643.618306, test/accuracy=0.984815, test/loss=0.051626, test/mean_average_precision=0.185329, test/num_examples=43793, total_duration=3508.490525, train/accuracy=0.988806, train/loss=0.038502, train/mean_average_precision=0.228653, validation/accuracy=0.985687, validation/loss=0.048942, validation/mean_average_precision=0.187075, validation/num_examples=43793
I0607 12:21:32.969628 139983503038208 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.052412, loss=0.044738
I0607 12:21:32.975022 140035747448640 submission.py:139] 9500) loss = 0.045, grad_norm = 0.052
I0607 12:23:55.290462 139983511430912 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.064818, loss=0.042366
I0607 12:23:55.296290 140035747448640 submission.py:139] 10000) loss = 0.042, grad_norm = 0.065
I0607 12:24:19.266344 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:25:20.342306 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:25:23.605247 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:25:26.799163 140035747448640 submission_runner.py:419] Time since start: 3816.14s, 	Step: 10085, 	{'train/accuracy': 0.988930710541422, 'train/loss': 0.03764263823650703, 'train/mean_average_precision': 0.25380363637335157, 'validation/accuracy': 0.9859211646925771, 'validation/loss': 0.04802374557727609, 'validation/mean_average_precision': 0.19925225885552264, 'validation/num_examples': 43793, 'test/accuracy': 0.9850092599651503, 'test/loss': 0.05084720810050042, 'test/mean_average_precision': 0.19936666002234385, 'test/num_examples': 43793, 'score': 2883.480799674988, 'total_duration': 3816.138242006302, 'accumulated_submission_time': 2883.480799674988, 'accumulated_eval_time': 929.6600739955902, 'accumulated_logging_time': 0.2696855068206787}
I0607 12:25:26.809289 139983503038208 logging_writer.py:48] [10085] accumulated_eval_time=929.660074, accumulated_logging_time=0.269686, accumulated_submission_time=2883.480800, global_step=10085, preemption_count=0, score=2883.480800, test/accuracy=0.985009, test/loss=0.050847, test/mean_average_precision=0.199367, test/num_examples=43793, total_duration=3816.138242, train/accuracy=0.988931, train/loss=0.037643, train/mean_average_precision=0.253804, validation/accuracy=0.985921, validation/loss=0.048024, validation/mean_average_precision=0.199252, validation/num_examples=43793
I0607 12:27:25.627056 139983511430912 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.071521, loss=0.040606
I0607 12:27:25.633161 140035747448640 submission.py:139] 10500) loss = 0.041, grad_norm = 0.072
I0607 12:29:27.024436 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:30:28.115043 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:30:31.417503 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:30:34.704628 140035747448640 submission_runner.py:419] Time since start: 4124.04s, 	Step: 10925, 	{'train/accuracy': 0.9888484521440761, 'train/loss': 0.03753084541814273, 'train/mean_average_precision': 0.25130441773554646, 'validation/accuracy': 0.9856646101710878, 'validation/loss': 0.04927705894137161, 'validation/mean_average_precision': 0.20155290762017086, 'validation/num_examples': 43793, 'test/accuracy': 0.98470936934941, 'test/loss': 0.05226573915034081, 'test/mean_average_precision': 0.20388198059448048, 'test/num_examples': 43793, 'score': 3123.453566789627, 'total_duration': 4124.043718338013, 'accumulated_submission_time': 3123.453566789627, 'accumulated_eval_time': 997.3400802612305, 'accumulated_logging_time': 0.2956252098083496}
I0607 12:30:34.716699 139983503038208 logging_writer.py:48] [10925] accumulated_eval_time=997.340080, accumulated_logging_time=0.295625, accumulated_submission_time=3123.453567, global_step=10925, preemption_count=0, score=3123.453567, test/accuracy=0.984709, test/loss=0.052266, test/mean_average_precision=0.203882, test/num_examples=43793, total_duration=4124.043718, train/accuracy=0.988848, train/loss=0.037531, train/mean_average_precision=0.251304, validation/accuracy=0.985665, validation/loss=0.049277, validation/mean_average_precision=0.201553, validation/num_examples=43793
I0607 12:30:56.583296 139983511430912 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.058320, loss=0.041149
I0607 12:30:56.588677 140035747448640 submission.py:139] 11000) loss = 0.041, grad_norm = 0.058
I0607 12:33:19.322682 139983503038208 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.055932, loss=0.042069
I0607 12:33:19.328439 140035747448640 submission.py:139] 11500) loss = 0.042, grad_norm = 0.056
I0607 12:34:34.884478 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:35:36.090239 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:35:39.364357 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:35:42.616727 140035747448640 submission_runner.py:419] Time since start: 4431.96s, 	Step: 11764, 	{'train/accuracy': 0.9893829191634367, 'train/loss': 0.03640523457612483, 'train/mean_average_precision': 0.27239834414362196, 'validation/accuracy': 0.9861111449395027, 'validation/loss': 0.04706185323498202, 'validation/mean_average_precision': 0.2078147016426748, 'validation/num_examples': 43793, 'test/accuracy': 0.9851448846537295, 'test/loss': 0.049645858850711336, 'test/mean_average_precision': 0.20443071683559555, 'test/num_examples': 43793, 'score': 3363.385430574417, 'total_duration': 4431.9558272361755, 'accumulated_submission_time': 3363.385430574417, 'accumulated_eval_time': 1065.0721080303192, 'accumulated_logging_time': 0.3187439441680908}
I0607 12:35:42.627990 139983511430912 logging_writer.py:48] [11764] accumulated_eval_time=1065.072108, accumulated_logging_time=0.318744, accumulated_submission_time=3363.385431, global_step=11764, preemption_count=0, score=3363.385431, test/accuracy=0.985145, test/loss=0.049646, test/mean_average_precision=0.204431, test/num_examples=43793, total_duration=4431.955827, train/accuracy=0.989383, train/loss=0.036405, train/mean_average_precision=0.272398, validation/accuracy=0.986111, validation/loss=0.047062, validation/mean_average_precision=0.207815, validation/num_examples=43793
I0607 12:36:50.121502 140035747448640 spec.py:298] Evaluating on the training split.
I0607 12:37:50.830438 140035747448640 spec.py:310] Evaluating on the validation split.
I0607 12:37:54.179326 140035747448640 spec.py:326] Evaluating on the test split.
I0607 12:37:57.455857 140035747448640 submission_runner.py:419] Time since start: 4566.79s, 	Step: 12000, 	{'train/accuracy': 0.989271104441314, 'train/loss': 0.03622299415488244, 'train/mean_average_precision': 0.2643689427991996, 'validation/accuracy': 0.9860652736405655, 'validation/loss': 0.04728952253863947, 'validation/mean_average_precision': 0.21321794572478323, 'validation/num_examples': 43793, 'test/accuracy': 0.9850720179732196, 'test/loss': 0.04998861918798719, 'test/mean_average_precision': 0.20723943312343146, 'test/num_examples': 43793, 'score': 3430.804360151291, 'total_duration': 4566.794937610626, 'accumulated_submission_time': 3430.804360151291, 'accumulated_eval_time': 1132.4062132835388, 'accumulated_logging_time': 0.34114527702331543}
I0607 12:37:57.466347 139983503038208 logging_writer.py:48] [12000] accumulated_eval_time=1132.406213, accumulated_logging_time=0.341145, accumulated_submission_time=3430.804360, global_step=12000, preemption_count=0, score=3430.804360, test/accuracy=0.985072, test/loss=0.049989, test/mean_average_precision=0.207239, test/num_examples=43793, total_duration=4566.794938, train/accuracy=0.989271, train/loss=0.036223, train/mean_average_precision=0.264369, validation/accuracy=0.986065, validation/loss=0.047290, validation/mean_average_precision=0.213218, validation/num_examples=43793
I0607 12:37:57.486409 139983511430912 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3430.804360
I0607 12:37:57.546200 140035747448640 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nesterov/ogbg_pytorch/trial_1/checkpoint_12000.
I0607 12:37:57.710786 140035747448640 submission_runner.py:581] Tuning trial 1/1
I0607 12:37:57.711042 140035747448640 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 12:37:57.712381 140035747448640 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5014427297227166, 'train/loss': 0.758456196383179, 'train/mean_average_precision': 0.024607142514149877, 'validation/accuracy': 0.4930971813913536, 'validation/loss': 0.7691068675423619, 'validation/mean_average_precision': 0.02761421364392214, 'validation/num_examples': 43793, 'test/accuracy': 0.4930633447322655, 'test/loss': 0.7702336661754132, 'test/mean_average_precision': 0.029199770391899527, 'test/num_examples': 43793, 'score': 5.000657796859741, 'total_duration': 152.24140238761902, 'accumulated_submission_time': 5.000657796859741, 'accumulated_eval_time': 147.24025297164917, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (849, {'train/accuracy': 0.9866670992849017, 'train/loss': 0.054866039517709325, 'train/mean_average_precision': 0.033796691086459094, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06454612506261635, 'validation/mean_average_precision': 0.03635894699696041, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06783273274481204, 'test/mean_average_precision': 0.037723102593672404, 'test/num_examples': 43793, 'score': 244.87062168121338, 'total_duration': 453.67824244499207, 'accumulated_submission_time': 244.87062168121338, 'accumulated_eval_time': 208.54874396324158, 'accumulated_logging_time': 0.029047012329101562, 'global_step': 849, 'preemption_count': 0}), (1691, {'train/accuracy': 0.9869285331049386, 'train/loss': 0.051212996598689685, 'train/mean_average_precision': 0.05641059462003932, 'validation/accuracy': 0.98414558007708, 'validation/loss': 0.06101955964364902, 'validation/mean_average_precision': 0.05496837138691235, 'validation/num_examples': 43793, 'test/accuracy': 0.983172851138426, 'test/loss': 0.06417461741833771, 'test/mean_average_precision': 0.055121686053630055, 'test/num_examples': 43793, 'score': 484.7478406429291, 'total_duration': 758.3818759918213, 'accumulated_submission_time': 484.7478406429291, 'accumulated_eval_time': 273.12707710266113, 'accumulated_logging_time': 0.0494379997253418, 'global_step': 1691, 'preemption_count': 0}), (2531, {'train/accuracy': 0.9869769865734527, 'train/loss': 0.04944711053917592, 'train/mean_average_precision': 0.0781643545683866, 'validation/accuracy': 0.984317698933269, 'validation/loss': 0.058849889066555604, 'validation/mean_average_precision': 0.07673652885622932, 'validation/num_examples': 43793, 'test/accuracy': 0.9833004731279897, 'test/loss': 0.06204019360845489, 'test/mean_average_precision': 0.07609901467994432, 'test/num_examples': 43793, 'score': 724.6174650192261, 'total_duration': 1062.257677078247, 'accumulated_submission_time': 724.6174650192261, 'accumulated_eval_time': 336.88799691200256, 'accumulated_logging_time': 0.06980299949645996, 'global_step': 2531, 'preemption_count': 0}), (3372, {'train/accuracy': 0.9870375753458277, 'train/loss': 0.046976977762168644, 'train/mean_average_precision': 0.10341544166482744, 'validation/accuracy': 0.984348144485661, 'validation/loss': 0.05653080694109882, 'validation/mean_average_precision': 0.09947116589364079, 'validation/num_examples': 43793, 'test/accuracy': 0.9833754457819248, 'test/loss': 0.05966058184254985, 'test/mean_average_precision': 0.0988687503670478, 'test/num_examples': 43793, 'score': 964.4668383598328, 'total_duration': 1366.7717914581299, 'accumulated_submission_time': 964.4668383598328, 'accumulated_eval_time': 401.3087651729584, 'accumulated_logging_time': 0.08973050117492676, 'global_step': 3372, 'preemption_count': 0}), (4222, {'train/accuracy': 0.9873532980805853, 'train/loss': 0.045398074581902606, 'train/mean_average_precision': 0.12571221651061762, 'validation/accuracy': 0.9846436693142119, 'validation/loss': 0.05416887904347381, 'validation/mean_average_precision': 0.11450609148917902, 'validation/num_examples': 43793, 'test/accuracy': 0.9836782847604603, 'test/loss': 0.05694694294791633, 'test/mean_average_precision': 0.11684672323922457, 'test/num_examples': 43793, 'score': 1204.4843015670776, 'total_duration': 1670.192153453827, 'accumulated_submission_time': 1204.4843015670776, 'accumulated_eval_time': 464.46035981178284, 'accumulated_logging_time': 0.11015605926513672, 'global_step': 4222, 'preemption_count': 0}), (5067, {'train/accuracy': 0.9877052174116788, 'train/loss': 0.043317804466824095, 'train/mean_average_precision': 0.14292677582047467, 'validation/accuracy': 0.984981411975413, 'validation/loss': 0.052665845550118655, 'validation/mean_average_precision': 0.12850682357266152, 'validation/num_examples': 43793, 'test/accuracy': 0.9839952337609442, 'test/loss': 0.055457624866323336, 'test/mean_average_precision': 0.1348806392431518, 'test/num_examples': 43793, 'score': 1444.4454276561737, 'total_duration': 1976.0765132904053, 'accumulated_submission_time': 1444.4454276561737, 'accumulated_eval_time': 530.1308808326721, 'accumulated_logging_time': 0.1307971477508545, 'global_step': 5067, 'preemption_count': 0}), (5898, {'train/accuracy': 0.9879950742562175, 'train/loss': 0.04185346241955117, 'train/mean_average_precision': 0.17634887941009497, 'validation/accuracy': 0.9851653031118602, 'validation/loss': 0.05200587477378955, 'validation/mean_average_precision': 0.1473855490705816, 'validation/num_examples': 43793, 'test/accuracy': 0.9841934058602502, 'test/loss': 0.055014080538320506, 'test/mean_average_precision': 0.1521352317315992, 'test/num_examples': 43793, 'score': 1684.2681124210358, 'total_duration': 2282.0883870124817, 'accumulated_submission_time': 1684.2681124210358, 'accumulated_eval_time': 596.0750288963318, 'accumulated_logging_time': 0.1520678997039795, 'global_step': 5898, 'preemption_count': 0}), (6737, {'train/accuracy': 0.9881525812513438, 'train/loss': 0.040990684773250026, 'train/mean_average_precision': 0.19780221096955947, 'validation/accuracy': 0.985335798205255, 'validation/loss': 0.050600815702313943, 'validation/mean_average_precision': 0.1614643534481779, 'validation/num_examples': 43793, 'test/accuracy': 0.9843370332478448, 'test/loss': 0.05331594861045767, 'test/mean_average_precision': 0.16900614352245985, 'test/num_examples': 43793, 'score': 1924.082769393921, 'total_duration': 2587.853394508362, 'accumulated_submission_time': 1924.082769393921, 'accumulated_eval_time': 661.7782618999481, 'accumulated_logging_time': 0.1736757755279541, 'global_step': 6737, 'preemption_count': 0}), (7575, {'train/accuracy': 0.9887259968850818, 'train/loss': 0.03897515415576127, 'train/mean_average_precision': 0.205133959224007, 'validation/accuracy': 0.9856394418477771, 'validation/loss': 0.0492928874572037, 'validation/mean_average_precision': 0.17512369957228016, 'validation/num_examples': 43793, 'test/accuracy': 0.984693785146064, 'test/loss': 0.05182997047109362, 'test/mean_average_precision': 0.17716824041304002, 'test/num_examples': 43793, 'score': 2163.8809053897858, 'total_duration': 2893.978265762329, 'accumulated_submission_time': 2163.8809053897858, 'accumulated_eval_time': 727.8495955467224, 'accumulated_logging_time': 0.19905996322631836, 'global_step': 7575, 'preemption_count': 0}), (8414, {'train/accuracy': 0.9886685418693208, 'train/loss': 0.03896808204018168, 'train/mean_average_precision': 0.22558552911016885, 'validation/accuracy': 0.9856568972978151, 'validation/loss': 0.0494062463972763, 'validation/mean_average_precision': 0.18121408958693905, 'validation/num_examples': 43793, 'test/accuracy': 0.9846234456336642, 'test/loss': 0.05240037337329348, 'test/mean_average_precision': 0.18108975357301407, 'test/num_examples': 43793, 'score': 2403.8023483753204, 'total_duration': 3201.332774877548, 'accumulated_submission_time': 2403.8023483753204, 'accumulated_eval_time': 795.0295650959015, 'accumulated_logging_time': 0.22578215599060059, 'global_step': 8414, 'preemption_count': 0}), (9240, {'train/accuracy': 0.9888060414290691, 'train/loss': 0.038501741990874284, 'train/mean_average_precision': 0.2286527459117706, 'validation/accuracy': 0.9856871398798578, 'validation/loss': 0.04894208809400288, 'validation/mean_average_precision': 0.1870747796519883, 'validation/num_examples': 43793, 'test/accuracy': 0.9848150892153522, 'test/loss': 0.05162589835561383, 'test/mean_average_precision': 0.1853289291267217, 'test/num_examples': 43793, 'score': 2643.618306159973, 'total_duration': 3508.4905247688293, 'accumulated_submission_time': 2643.618306159973, 'accumulated_eval_time': 862.127466917038, 'accumulated_logging_time': 0.24776625633239746, 'global_step': 9240, 'preemption_count': 0}), (10085, {'train/accuracy': 0.988930710541422, 'train/loss': 0.03764263823650703, 'train/mean_average_precision': 0.25380363637335157, 'validation/accuracy': 0.9859211646925771, 'validation/loss': 0.04802374557727609, 'validation/mean_average_precision': 0.19925225885552264, 'validation/num_examples': 43793, 'test/accuracy': 0.9850092599651503, 'test/loss': 0.05084720810050042, 'test/mean_average_precision': 0.19936666002234385, 'test/num_examples': 43793, 'score': 2883.480799674988, 'total_duration': 3816.138242006302, 'accumulated_submission_time': 2883.480799674988, 'accumulated_eval_time': 929.6600739955902, 'accumulated_logging_time': 0.2696855068206787, 'global_step': 10085, 'preemption_count': 0}), (10925, {'train/accuracy': 0.9888484521440761, 'train/loss': 0.03753084541814273, 'train/mean_average_precision': 0.25130441773554646, 'validation/accuracy': 0.9856646101710878, 'validation/loss': 0.04927705894137161, 'validation/mean_average_precision': 0.20155290762017086, 'validation/num_examples': 43793, 'test/accuracy': 0.98470936934941, 'test/loss': 0.05226573915034081, 'test/mean_average_precision': 0.20388198059448048, 'test/num_examples': 43793, 'score': 3123.453566789627, 'total_duration': 4124.043718338013, 'accumulated_submission_time': 3123.453566789627, 'accumulated_eval_time': 997.3400802612305, 'accumulated_logging_time': 0.2956252098083496, 'global_step': 10925, 'preemption_count': 0}), (11764, {'train/accuracy': 0.9893829191634367, 'train/loss': 0.03640523457612483, 'train/mean_average_precision': 0.27239834414362196, 'validation/accuracy': 0.9861111449395027, 'validation/loss': 0.04706185323498202, 'validation/mean_average_precision': 0.2078147016426748, 'validation/num_examples': 43793, 'test/accuracy': 0.9851448846537295, 'test/loss': 0.049645858850711336, 'test/mean_average_precision': 0.20443071683559555, 'test/num_examples': 43793, 'score': 3363.385430574417, 'total_duration': 4431.9558272361755, 'accumulated_submission_time': 3363.385430574417, 'accumulated_eval_time': 1065.0721080303192, 'accumulated_logging_time': 0.3187439441680908, 'global_step': 11764, 'preemption_count': 0}), (12000, {'train/accuracy': 0.989271104441314, 'train/loss': 0.03622299415488244, 'train/mean_average_precision': 0.2643689427991996, 'validation/accuracy': 0.9860652736405655, 'validation/loss': 0.04728952253863947, 'validation/mean_average_precision': 0.21321794572478323, 'validation/num_examples': 43793, 'test/accuracy': 0.9850720179732196, 'test/loss': 0.04998861918798719, 'test/mean_average_precision': 0.20723943312343146, 'test/num_examples': 43793, 'score': 3430.804360151291, 'total_duration': 4566.794937610626, 'accumulated_submission_time': 3430.804360151291, 'accumulated_eval_time': 1132.4062132835388, 'accumulated_logging_time': 0.34114527702331543, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0607 12:37:57.712526 140035747448640 submission_runner.py:584] Timing: 3430.804360151291
I0607 12:37:57.712586 140035747448640 submission_runner.py:586] Total number of evals: 16
I0607 12:37:57.712637 140035747448640 submission_runner.py:587] ====================
I0607 12:37:57.712772 140035747448640 submission_runner.py:655] Final ogbg score: 3430.804360151291
