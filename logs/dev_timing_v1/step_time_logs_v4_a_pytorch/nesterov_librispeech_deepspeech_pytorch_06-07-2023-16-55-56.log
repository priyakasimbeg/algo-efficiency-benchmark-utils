torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_06-07-2023-16-55-56.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 16:56:21.887932 140071203460928 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 16:56:21.887979 140553136105280 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 16:56:21.888002 140412741994304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 16:56:21.889131 140016008795968 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 16:56:21.889231 139909237708608 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 16:56:22.867193 140515185391424 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 16:56:22.867261 140536219617088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 16:56:22.872160 140706249529152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 16:56:22.872714 140706249529152 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.877860 140515185391424 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.877892 140536219617088 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.880208 140071203460928 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.880238 140553136105280 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.880264 140412741994304 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.880379 139909237708608 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:22.880402 140016008795968 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:56:23.307710 140706249529152 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch because --overwrite was set.
I0607 16:56:23.335568 140706249529152 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch.
W0607 16:56:23.648862 140553136105280 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:56:23.649965 140071203460928 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:56:23.650659 140412741994304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:56:23.650955 140536219617088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:56:23.652610 139909237708608 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:56:23.653052 140515185391424 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:56:23.677656 140706249529152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 16:56:23.683197 140706249529152 submission_runner.py:541] Using RNG seed 316073060
I0607 16:56:23.684629 140706249529152 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 16:56:23.684751 140706249529152 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch/trial_1.
I0607 16:56:23.684985 140706249529152 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0607 16:56:23.686050 140706249529152 submission_runner.py:255] Initializing dataset.
I0607 16:56:23.686187 140706249529152 input_pipeline.py:20] Loading split = train-clean-100
W0607 16:56:23.688731 140016008795968 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 16:56:23.721261 140706249529152 input_pipeline.py:20] Loading split = train-clean-360
I0607 16:56:24.057755 140706249529152 input_pipeline.py:20] Loading split = train-other-500
I0607 16:56:24.499891 140706249529152 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0607 16:56:32.248148 140706249529152 submission_runner.py:272] Initializing optimizer.
I0607 16:56:32.809184 140706249529152 submission_runner.py:279] Initializing metrics bundle.
I0607 16:56:32.809388 140706249529152 submission_runner.py:297] Initializing checkpoint and logger.
I0607 16:56:32.810723 140706249529152 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 16:56:32.810849 140706249529152 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 16:56:33.397282 140706249529152 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0607 16:56:33.398239 140706249529152 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0607 16:56:33.406765 140706249529152 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0607 16:56:42.684652 140679640512256 logging_writer.py:48] [0] global_step=0, grad_norm=25.828520, loss=33.296642
I0607 16:56:42.714459 140706249529152 submission.py:139] 0) loss = 33.297, grad_norm = 25.829
I0607 16:56:42.716073 140706249529152 spec.py:298] Evaluating on the training split.
I0607 16:56:42.717206 140706249529152 input_pipeline.py:20] Loading split = train-clean-100
I0607 16:56:42.749531 140706249529152 input_pipeline.py:20] Loading split = train-clean-360
I0607 16:56:43.172109 140706249529152 input_pipeline.py:20] Loading split = train-other-500
I0607 16:57:04.834119 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 16:57:04.835429 140706249529152 input_pipeline.py:20] Loading split = dev-clean
I0607 16:57:04.839970 140706249529152 input_pipeline.py:20] Loading split = dev-other
I0607 16:57:18.007595 140706249529152 spec.py:326] Evaluating on the test split.
I0607 16:57:18.008935 140706249529152 input_pipeline.py:20] Loading split = test-clean
I0607 16:57:25.860369 140706249529152 submission_runner.py:419] Time since start: 52.45s, 	Step: 1, 	{'train/ctc_loss': 31.59767087298022, 'train/wer': 4.490518296830364, 'validation/ctc_loss': 30.129445449065702, 'validation/wer': 4.025645729734949, 'validation/num_examples': 5348, 'test/ctc_loss': 30.193847990192914, 'test/wer': 4.5177421648081575, 'test/num_examples': 2472, 'score': 9.30928111076355, 'total_duration': 52.45373058319092, 'accumulated_submission_time': 9.30928111076355, 'accumulated_eval_time': 43.14396333694458, 'accumulated_logging_time': 0}
I0607 16:57:25.888624 140677048428288 logging_writer.py:48] [1] accumulated_eval_time=43.143963, accumulated_logging_time=0, accumulated_submission_time=9.309281, global_step=1, preemption_count=0, score=9.309281, test/ctc_loss=30.193848, test/num_examples=2472, test/wer=4.517742, total_duration=52.453731, train/ctc_loss=31.597671, train/wer=4.490518, validation/ctc_loss=30.129445, validation/num_examples=5348, validation/wer=4.025646
I0607 16:57:25.930895 140706249529152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.930905 140071203460928 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.930953 140016008795968 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.930943 139909237708608 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.930945 140412741994304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.930960 140553136105280 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.930967 140515185391424 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:25.932077 140536219617088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:57:27.038425 140677040035584 logging_writer.py:48] [1] global_step=1, grad_norm=25.036234, loss=32.646183
I0607 16:57:27.042029 140706249529152 submission.py:139] 1) loss = 32.646, grad_norm = 25.036
I0607 16:57:27.986279 140677048428288 logging_writer.py:48] [2] global_step=2, grad_norm=28.676676, loss=33.093781
I0607 16:57:27.989855 140706249529152 submission.py:139] 2) loss = 33.094, grad_norm = 28.677
I0607 16:57:28.843939 140677040035584 logging_writer.py:48] [3] global_step=3, grad_norm=33.943512, loss=32.792984
I0607 16:57:28.847072 140706249529152 submission.py:139] 3) loss = 32.793, grad_norm = 33.944
I0607 16:57:29.647192 140677048428288 logging_writer.py:48] [4] global_step=4, grad_norm=32.229439, loss=31.784569
I0607 16:57:29.650509 140706249529152 submission.py:139] 4) loss = 31.785, grad_norm = 32.229
I0607 16:57:30.454360 140677040035584 logging_writer.py:48] [5] global_step=5, grad_norm=36.039162, loss=31.025925
I0607 16:57:30.457875 140706249529152 submission.py:139] 5) loss = 31.026, grad_norm = 36.039
I0607 16:57:31.256850 140677048428288 logging_writer.py:48] [6] global_step=6, grad_norm=38.750648, loss=29.508900
I0607 16:57:31.260155 140706249529152 submission.py:139] 6) loss = 29.509, grad_norm = 38.751
I0607 16:57:32.089942 140677040035584 logging_writer.py:48] [7] global_step=7, grad_norm=39.018726, loss=26.787783
I0607 16:57:32.093347 140706249529152 submission.py:139] 7) loss = 26.788, grad_norm = 39.019
I0607 16:57:32.895761 140677048428288 logging_writer.py:48] [8] global_step=8, grad_norm=38.744183, loss=25.107059
I0607 16:57:32.899122 140706249529152 submission.py:139] 8) loss = 25.107, grad_norm = 38.744
I0607 16:57:33.696022 140677040035584 logging_writer.py:48] [9] global_step=9, grad_norm=34.445358, loss=22.285763
I0607 16:57:33.699511 140706249529152 submission.py:139] 9) loss = 22.286, grad_norm = 34.445
I0607 16:57:34.497403 140677048428288 logging_writer.py:48] [10] global_step=10, grad_norm=27.519148, loss=19.882385
I0607 16:57:34.500896 140706249529152 submission.py:139] 10) loss = 19.882, grad_norm = 27.519
I0607 16:57:35.300182 140677040035584 logging_writer.py:48] [11] global_step=11, grad_norm=22.289671, loss=18.034792
I0607 16:57:35.303395 140706249529152 submission.py:139] 11) loss = 18.035, grad_norm = 22.290
I0607 16:57:36.103673 140677048428288 logging_writer.py:48] [12] global_step=12, grad_norm=17.561092, loss=16.614111
I0607 16:57:36.106855 140706249529152 submission.py:139] 12) loss = 16.614, grad_norm = 17.561
I0607 16:57:36.904911 140677040035584 logging_writer.py:48] [13] global_step=13, grad_norm=14.825580, loss=15.441474
I0607 16:57:36.908082 140706249529152 submission.py:139] 13) loss = 15.441, grad_norm = 14.826
I0607 16:57:37.706367 140677048428288 logging_writer.py:48] [14] global_step=14, grad_norm=13.402634, loss=14.468312
I0607 16:57:37.709421 140706249529152 submission.py:139] 14) loss = 14.468, grad_norm = 13.403
I0607 16:57:38.503179 140677040035584 logging_writer.py:48] [15] global_step=15, grad_norm=11.075665, loss=13.456074
I0607 16:57:38.506402 140706249529152 submission.py:139] 15) loss = 13.456, grad_norm = 11.076
I0607 16:57:39.298485 140677048428288 logging_writer.py:48] [16] global_step=16, grad_norm=10.321748, loss=13.336462
I0607 16:57:39.301697 140706249529152 submission.py:139] 16) loss = 13.336, grad_norm = 10.322
I0607 16:57:40.103380 140677040035584 logging_writer.py:48] [17] global_step=17, grad_norm=11.726259, loss=13.136115
I0607 16:57:40.106920 140706249529152 submission.py:139] 17) loss = 13.136, grad_norm = 11.726
I0607 16:57:40.900280 140677048428288 logging_writer.py:48] [18] global_step=18, grad_norm=11.195211, loss=12.531820
I0607 16:57:40.903542 140706249529152 submission.py:139] 18) loss = 12.532, grad_norm = 11.195
I0607 16:57:41.698538 140677040035584 logging_writer.py:48] [19] global_step=19, grad_norm=9.858444, loss=12.236019
I0607 16:57:41.701722 140706249529152 submission.py:139] 19) loss = 12.236, grad_norm = 9.858
I0607 16:57:42.499698 140677048428288 logging_writer.py:48] [20] global_step=20, grad_norm=7.904498, loss=11.582260
I0607 16:57:42.502780 140706249529152 submission.py:139] 20) loss = 11.582, grad_norm = 7.904
I0607 16:57:43.300865 140677040035584 logging_writer.py:48] [21] global_step=21, grad_norm=7.370935, loss=11.387138
I0607 16:57:43.304041 140706249529152 submission.py:139] 21) loss = 11.387, grad_norm = 7.371
I0607 16:57:44.094835 140677048428288 logging_writer.py:48] [22] global_step=22, grad_norm=7.635731, loss=11.166316
I0607 16:57:44.098211 140706249529152 submission.py:139] 22) loss = 11.166, grad_norm = 7.636
I0607 16:57:44.895777 140677040035584 logging_writer.py:48] [23] global_step=23, grad_norm=7.147027, loss=10.630416
I0607 16:57:44.898949 140706249529152 submission.py:139] 23) loss = 10.630, grad_norm = 7.147
I0607 16:57:45.698920 140677048428288 logging_writer.py:48] [24] global_step=24, grad_norm=8.979458, loss=10.396949
I0607 16:57:45.702157 140706249529152 submission.py:139] 24) loss = 10.397, grad_norm = 8.979
I0607 16:57:46.499526 140677040035584 logging_writer.py:48] [25] global_step=25, grad_norm=10.722939, loss=9.968455
I0607 16:57:46.502793 140706249529152 submission.py:139] 25) loss = 9.968, grad_norm = 10.723
I0607 16:57:47.303200 140677048428288 logging_writer.py:48] [26] global_step=26, grad_norm=8.321276, loss=9.126644
I0607 16:57:47.306490 140706249529152 submission.py:139] 26) loss = 9.127, grad_norm = 8.321
I0607 16:57:48.105516 140677040035584 logging_writer.py:48] [27] global_step=27, grad_norm=9.242017, loss=8.871759
I0607 16:57:48.108677 140706249529152 submission.py:139] 27) loss = 8.872, grad_norm = 9.242
I0607 16:57:48.907929 140677048428288 logging_writer.py:48] [28] global_step=28, grad_norm=5.684618, loss=8.369640
I0607 16:57:48.911273 140706249529152 submission.py:139] 28) loss = 8.370, grad_norm = 5.685
I0607 16:57:49.701664 140677040035584 logging_writer.py:48] [29] global_step=29, grad_norm=6.792722, loss=8.334395
I0607 16:57:49.705303 140706249529152 submission.py:139] 29) loss = 8.334, grad_norm = 6.793
I0607 16:57:50.506247 140677048428288 logging_writer.py:48] [30] global_step=30, grad_norm=7.270330, loss=8.161771
I0607 16:57:50.509798 140706249529152 submission.py:139] 30) loss = 8.162, grad_norm = 7.270
I0607 16:57:51.306892 140677040035584 logging_writer.py:48] [31] global_step=31, grad_norm=5.990843, loss=8.135336
I0607 16:57:51.310927 140706249529152 submission.py:139] 31) loss = 8.135, grad_norm = 5.991
I0607 16:57:52.107177 140677048428288 logging_writer.py:48] [32] global_step=32, grad_norm=5.382029, loss=7.963840
I0607 16:57:52.111244 140706249529152 submission.py:139] 32) loss = 7.964, grad_norm = 5.382
I0607 16:57:52.922602 140677040035584 logging_writer.py:48] [33] global_step=33, grad_norm=4.189204, loss=7.804519
I0607 16:57:52.926722 140706249529152 submission.py:139] 33) loss = 7.805, grad_norm = 4.189
I0607 16:57:53.720839 140677048428288 logging_writer.py:48] [34] global_step=34, grad_norm=3.423866, loss=7.691371
I0607 16:57:53.724911 140706249529152 submission.py:139] 34) loss = 7.691, grad_norm = 3.424
I0607 16:57:54.521925 140677040035584 logging_writer.py:48] [35] global_step=35, grad_norm=4.054864, loss=7.587493
I0607 16:57:54.525886 140706249529152 submission.py:139] 35) loss = 7.587, grad_norm = 4.055
I0607 16:57:55.317106 140677048428288 logging_writer.py:48] [36] global_step=36, grad_norm=2.820277, loss=7.364064
I0607 16:57:55.321106 140706249529152 submission.py:139] 36) loss = 7.364, grad_norm = 2.820
I0607 16:57:56.115172 140677040035584 logging_writer.py:48] [37] global_step=37, grad_norm=3.815525, loss=7.324297
I0607 16:57:56.119155 140706249529152 submission.py:139] 37) loss = 7.324, grad_norm = 3.816
I0607 16:57:56.926281 140677048428288 logging_writer.py:48] [38] global_step=38, grad_norm=3.656034, loss=7.371318
I0607 16:57:56.929880 140706249529152 submission.py:139] 38) loss = 7.371, grad_norm = 3.656
I0607 16:57:57.738801 140677040035584 logging_writer.py:48] [39] global_step=39, grad_norm=3.382245, loss=7.318699
I0607 16:57:57.742316 140706249529152 submission.py:139] 39) loss = 7.319, grad_norm = 3.382
I0607 16:57:58.541746 140677048428288 logging_writer.py:48] [40] global_step=40, grad_norm=2.943083, loss=7.222917
I0607 16:57:58.545637 140706249529152 submission.py:139] 40) loss = 7.223, grad_norm = 2.943
I0607 16:57:59.350486 140677040035584 logging_writer.py:48] [41] global_step=41, grad_norm=3.467847, loss=7.160515
I0607 16:57:59.354111 140706249529152 submission.py:139] 41) loss = 7.161, grad_norm = 3.468
I0607 16:58:00.155977 140677048428288 logging_writer.py:48] [42] global_step=42, grad_norm=3.277537, loss=7.058197
I0607 16:58:00.160032 140706249529152 submission.py:139] 42) loss = 7.058, grad_norm = 3.278
I0607 16:58:00.962382 140677040035584 logging_writer.py:48] [43] global_step=43, grad_norm=4.104823, loss=7.050187
I0607 16:58:00.966164 140706249529152 submission.py:139] 43) loss = 7.050, grad_norm = 4.105
I0607 16:58:01.778496 140677048428288 logging_writer.py:48] [44] global_step=44, grad_norm=3.515155, loss=6.987019
I0607 16:58:01.782613 140706249529152 submission.py:139] 44) loss = 6.987, grad_norm = 3.515
I0607 16:58:02.585808 140677040035584 logging_writer.py:48] [45] global_step=45, grad_norm=4.277127, loss=6.965942
I0607 16:58:02.589905 140706249529152 submission.py:139] 45) loss = 6.966, grad_norm = 4.277
I0607 16:58:03.397290 140677048428288 logging_writer.py:48] [46] global_step=46, grad_norm=4.960193, loss=6.888049
I0607 16:58:03.400815 140706249529152 submission.py:139] 46) loss = 6.888, grad_norm = 4.960
I0607 16:58:04.205473 140677040035584 logging_writer.py:48] [47] global_step=47, grad_norm=3.547656, loss=6.812781
I0607 16:58:04.209184 140706249529152 submission.py:139] 47) loss = 6.813, grad_norm = 3.548
I0607 16:58:05.006925 140677048428288 logging_writer.py:48] [48] global_step=48, grad_norm=5.226143, loss=6.810166
I0607 16:58:05.010413 140706249529152 submission.py:139] 48) loss = 6.810, grad_norm = 5.226
I0607 16:58:05.824263 140677040035584 logging_writer.py:48] [49] global_step=49, grad_norm=28.198622, loss=6.919293
I0607 16:58:05.827835 140706249529152 submission.py:139] 49) loss = 6.919, grad_norm = 28.199
I0607 16:58:06.626435 140677048428288 logging_writer.py:48] [50] global_step=50, grad_norm=5.641890, loss=7.027987
I0607 16:58:06.630252 140706249529152 submission.py:139] 50) loss = 7.028, grad_norm = 5.642
I0607 16:58:07.443115 140677040035584 logging_writer.py:48] [51] global_step=51, grad_norm=3.476874, loss=6.921556
I0607 16:58:07.447285 140706249529152 submission.py:139] 51) loss = 6.922, grad_norm = 3.477
I0607 16:58:08.240316 140677048428288 logging_writer.py:48] [52] global_step=52, grad_norm=2.459383, loss=6.914657
I0607 16:58:08.243423 140706249529152 submission.py:139] 52) loss = 6.915, grad_norm = 2.459
I0607 16:58:09.039122 140677040035584 logging_writer.py:48] [53] global_step=53, grad_norm=3.027420, loss=6.918867
I0607 16:58:09.042215 140706249529152 submission.py:139] 53) loss = 6.919, grad_norm = 3.027
I0607 16:58:09.838190 140677048428288 logging_writer.py:48] [54] global_step=54, grad_norm=2.942735, loss=7.032218
I0607 16:58:09.841451 140706249529152 submission.py:139] 54) loss = 7.032, grad_norm = 2.943
I0607 16:58:10.636996 140677040035584 logging_writer.py:48] [55] global_step=55, grad_norm=2.714600, loss=6.984365
I0607 16:58:10.640059 140706249529152 submission.py:139] 55) loss = 6.984, grad_norm = 2.715
I0607 16:58:11.438382 140677048428288 logging_writer.py:48] [56] global_step=56, grad_norm=2.145188, loss=6.946408
I0607 16:58:11.442064 140706249529152 submission.py:139] 56) loss = 6.946, grad_norm = 2.145
I0607 16:58:12.240348 140677040035584 logging_writer.py:48] [57] global_step=57, grad_norm=2.080904, loss=6.976979
I0607 16:58:12.243579 140706249529152 submission.py:139] 57) loss = 6.977, grad_norm = 2.081
I0607 16:58:13.040007 140677048428288 logging_writer.py:48] [58] global_step=58, grad_norm=2.095386, loss=6.935326
I0607 16:58:13.043174 140706249529152 submission.py:139] 58) loss = 6.935, grad_norm = 2.095
I0607 16:58:13.838653 140677040035584 logging_writer.py:48] [59] global_step=59, grad_norm=1.665246, loss=6.937086
I0607 16:58:13.841754 140706249529152 submission.py:139] 59) loss = 6.937, grad_norm = 1.665
I0607 16:58:14.636659 140677048428288 logging_writer.py:48] [60] global_step=60, grad_norm=1.432861, loss=6.881199
I0607 16:58:14.639845 140706249529152 submission.py:139] 60) loss = 6.881, grad_norm = 1.433
I0607 16:58:15.441079 140677040035584 logging_writer.py:48] [61] global_step=61, grad_norm=1.606779, loss=6.756958
I0607 16:58:15.444576 140706249529152 submission.py:139] 61) loss = 6.757, grad_norm = 1.607
I0607 16:58:16.247001 140677048428288 logging_writer.py:48] [62] global_step=62, grad_norm=1.616718, loss=6.853895
I0607 16:58:16.250226 140706249529152 submission.py:139] 62) loss = 6.854, grad_norm = 1.617
I0607 16:58:17.047003 140677040035584 logging_writer.py:48] [63] global_step=63, grad_norm=1.467649, loss=6.815498
I0607 16:58:17.050262 140706249529152 submission.py:139] 63) loss = 6.815, grad_norm = 1.468
I0607 16:58:17.846701 140677048428288 logging_writer.py:48] [64] global_step=64, grad_norm=1.266437, loss=6.742595
I0607 16:58:17.850121 140706249529152 submission.py:139] 64) loss = 6.743, grad_norm = 1.266
I0607 16:58:18.649650 140677040035584 logging_writer.py:48] [65] global_step=65, grad_norm=0.986866, loss=6.700668
I0607 16:58:18.652844 140706249529152 submission.py:139] 65) loss = 6.701, grad_norm = 0.987
I0607 16:58:19.476515 140677048428288 logging_writer.py:48] [66] global_step=66, grad_norm=1.356585, loss=6.647934
I0607 16:58:19.479769 140706249529152 submission.py:139] 66) loss = 6.648, grad_norm = 1.357
I0607 16:58:20.302450 140677040035584 logging_writer.py:48] [67] global_step=67, grad_norm=1.080428, loss=6.556597
I0607 16:58:20.305524 140706249529152 submission.py:139] 67) loss = 6.557, grad_norm = 1.080
I0607 16:58:21.126329 140677048428288 logging_writer.py:48] [68] global_step=68, grad_norm=0.980775, loss=6.561243
I0607 16:58:21.129703 140706249529152 submission.py:139] 68) loss = 6.561, grad_norm = 0.981
I0607 16:58:21.940067 140677040035584 logging_writer.py:48] [69] global_step=69, grad_norm=1.875183, loss=6.708556
I0607 16:58:21.943600 140706249529152 submission.py:139] 69) loss = 6.709, grad_norm = 1.875
I0607 16:58:22.773015 140677048428288 logging_writer.py:48] [70] global_step=70, grad_norm=2.151855, loss=6.587561
I0607 16:58:22.776272 140706249529152 submission.py:139] 70) loss = 6.588, grad_norm = 2.152
I0607 16:58:23.588987 140677040035584 logging_writer.py:48] [71] global_step=71, grad_norm=2.290243, loss=6.591000
I0607 16:58:23.592258 140706249529152 submission.py:139] 71) loss = 6.591, grad_norm = 2.290
I0607 16:58:24.396232 140677048428288 logging_writer.py:48] [72] global_step=72, grad_norm=2.591053, loss=6.527205
I0607 16:58:24.399399 140706249529152 submission.py:139] 72) loss = 6.527, grad_norm = 2.591
I0607 16:58:25.204600 140677040035584 logging_writer.py:48] [73] global_step=73, grad_norm=2.322241, loss=6.503485
I0607 16:58:25.207772 140706249529152 submission.py:139] 73) loss = 6.503, grad_norm = 2.322
I0607 16:58:26.011379 140677048428288 logging_writer.py:48] [74] global_step=74, grad_norm=2.293922, loss=6.519454
I0607 16:58:26.014620 140706249529152 submission.py:139] 74) loss = 6.519, grad_norm = 2.294
I0607 16:58:26.839058 140677040035584 logging_writer.py:48] [75] global_step=75, grad_norm=5.336369, loss=6.551370
I0607 16:58:26.842447 140706249529152 submission.py:139] 75) loss = 6.551, grad_norm = 5.336
I0607 16:58:27.647026 140677048428288 logging_writer.py:48] [76] global_step=76, grad_norm=3.288858, loss=6.367527
I0607 16:58:27.650429 140706249529152 submission.py:139] 76) loss = 6.368, grad_norm = 3.289
I0607 16:58:28.464019 140677040035584 logging_writer.py:48] [77] global_step=77, grad_norm=3.671878, loss=6.439919
I0607 16:58:28.467463 140706249529152 submission.py:139] 77) loss = 6.440, grad_norm = 3.672
I0607 16:58:29.283218 140677048428288 logging_writer.py:48] [78] global_step=78, grad_norm=2.349689, loss=6.355705
I0607 16:58:29.286334 140706249529152 submission.py:139] 78) loss = 6.356, grad_norm = 2.350
I0607 16:58:30.101387 140677040035584 logging_writer.py:48] [79] global_step=79, grad_norm=1.827442, loss=6.352589
I0607 16:58:30.104602 140706249529152 submission.py:139] 79) loss = 6.353, grad_norm = 1.827
I0607 16:58:30.906656 140677048428288 logging_writer.py:48] [80] global_step=80, grad_norm=1.575633, loss=6.333239
I0607 16:58:30.909690 140706249529152 submission.py:139] 80) loss = 6.333, grad_norm = 1.576
I0607 16:58:31.716756 140677040035584 logging_writer.py:48] [81] global_step=81, grad_norm=1.751765, loss=6.297006
I0607 16:58:31.719881 140706249529152 submission.py:139] 81) loss = 6.297, grad_norm = 1.752
I0607 16:58:32.524863 140677048428288 logging_writer.py:48] [82] global_step=82, grad_norm=1.908702, loss=6.277194
I0607 16:58:32.528304 140706249529152 submission.py:139] 82) loss = 6.277, grad_norm = 1.909
I0607 16:58:33.342018 140677040035584 logging_writer.py:48] [83] global_step=83, grad_norm=1.855285, loss=6.246570
I0607 16:58:33.345378 140706249529152 submission.py:139] 83) loss = 6.247, grad_norm = 1.855
I0607 16:58:34.151246 140677048428288 logging_writer.py:48] [84] global_step=84, grad_norm=3.477463, loss=6.246757
I0607 16:58:34.154464 140706249529152 submission.py:139] 84) loss = 6.247, grad_norm = 3.477
I0607 16:58:34.952838 140677040035584 logging_writer.py:48] [85] global_step=85, grad_norm=11.300115, loss=6.339188
I0607 16:58:34.955969 140706249529152 submission.py:139] 85) loss = 6.339, grad_norm = 11.300
I0607 16:58:35.759332 140677048428288 logging_writer.py:48] [86] global_step=86, grad_norm=3.434106, loss=6.535645
I0607 16:58:35.762464 140706249529152 submission.py:139] 86) loss = 6.536, grad_norm = 3.434
I0607 16:58:36.576945 140677040035584 logging_writer.py:48] [87] global_step=87, grad_norm=1.300650, loss=6.460599
I0607 16:58:36.580111 140706249529152 submission.py:139] 87) loss = 6.461, grad_norm = 1.301
I0607 16:58:37.377204 140677048428288 logging_writer.py:48] [88] global_step=88, grad_norm=1.509148, loss=6.633074
I0607 16:58:37.380540 140706249529152 submission.py:139] 88) loss = 6.633, grad_norm = 1.509
I0607 16:58:38.178904 140677040035584 logging_writer.py:48] [89] global_step=89, grad_norm=1.843836, loss=6.613281
I0607 16:58:38.182090 140706249529152 submission.py:139] 89) loss = 6.613, grad_norm = 1.844
I0607 16:58:38.979450 140677048428288 logging_writer.py:48] [90] global_step=90, grad_norm=1.531396, loss=6.508994
I0607 16:58:38.982797 140706249529152 submission.py:139] 90) loss = 6.509, grad_norm = 1.531
I0607 16:58:39.798067 140677040035584 logging_writer.py:48] [91] global_step=91, grad_norm=1.308402, loss=6.608066
I0607 16:58:39.801425 140706249529152 submission.py:139] 91) loss = 6.608, grad_norm = 1.308
I0607 16:58:40.598781 140677048428288 logging_writer.py:48] [92] global_step=92, grad_norm=1.208637, loss=6.544314
I0607 16:58:40.602064 140706249529152 submission.py:139] 92) loss = 6.544, grad_norm = 1.209
I0607 16:58:41.401880 140677040035584 logging_writer.py:48] [93] global_step=93, grad_norm=1.257242, loss=6.589321
I0607 16:58:41.405439 140706249529152 submission.py:139] 93) loss = 6.589, grad_norm = 1.257
I0607 16:58:42.205492 140677048428288 logging_writer.py:48] [94] global_step=94, grad_norm=1.096191, loss=6.521654
I0607 16:58:42.209177 140706249529152 submission.py:139] 94) loss = 6.522, grad_norm = 1.096
I0607 16:58:43.017618 140677040035584 logging_writer.py:48] [95] global_step=95, grad_norm=1.263258, loss=6.428003
I0607 16:58:43.020962 140706249529152 submission.py:139] 95) loss = 6.428, grad_norm = 1.263
I0607 16:58:43.825465 140677048428288 logging_writer.py:48] [96] global_step=96, grad_norm=1.251859, loss=6.533131
I0607 16:58:43.828784 140706249529152 submission.py:139] 96) loss = 6.533, grad_norm = 1.252
I0607 16:58:44.626369 140677040035584 logging_writer.py:48] [97] global_step=97, grad_norm=0.985084, loss=6.464654
I0607 16:58:44.629778 140706249529152 submission.py:139] 97) loss = 6.465, grad_norm = 0.985
I0607 16:58:45.430280 140677048428288 logging_writer.py:48] [98] global_step=98, grad_norm=1.196532, loss=6.448204
I0607 16:58:45.433526 140706249529152 submission.py:139] 98) loss = 6.448, grad_norm = 1.197
I0607 16:58:46.248786 140677040035584 logging_writer.py:48] [99] global_step=99, grad_norm=1.983791, loss=6.478382
I0607 16:58:46.252175 140706249529152 submission.py:139] 99) loss = 6.478, grad_norm = 1.984
I0607 16:58:47.050076 140677048428288 logging_writer.py:48] [100] global_step=100, grad_norm=4.191020, loss=6.292946
I0607 16:58:47.053265 140706249529152 submission.py:139] 100) loss = 6.293, grad_norm = 4.191
I0607 17:04:10.187101 140677040035584 logging_writer.py:48] [500] global_step=500, grad_norm=2.681033, loss=5.966749
I0607 17:04:10.191310 140706249529152 submission.py:139] 500) loss = 5.967, grad_norm = 2.681
I0607 17:10:53.174922 140677048428288 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.421473, loss=5.853637
I0607 17:10:53.180164 140706249529152 submission.py:139] 1000) loss = 5.854, grad_norm = 0.421
I0607 17:17:35.302881 140677048428288 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.591263, loss=5.363621
I0607 17:17:35.310110 140706249529152 submission.py:139] 1500) loss = 5.364, grad_norm = 0.591
I0607 17:24:07.586680 140677040035584 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0607 17:24:07.591339 140706249529152 submission.py:139] 2000) loss = nan, grad_norm = nan
I0607 17:30:37.457224 140677040035584 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0607 17:30:37.465978 140706249529152 submission.py:139] 2500) loss = nan, grad_norm = nan
I0607 17:37:04.630285 140676764382976 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0607 17:37:04.635235 140706249529152 submission.py:139] 3000) loss = nan, grad_norm = nan
I0607 17:37:26.338317 140706249529152 spec.py:298] Evaluating on the training split.
I0607 17:37:35.859009 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 17:37:44.837778 140706249529152 spec.py:326] Evaluating on the test split.
I0607 17:37:49.730907 140706249529152 submission_runner.py:419] Time since start: 2476.32s, 	Step: 3029, 	{'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2408.1151778697968, 'total_duration': 2476.3232753276825, 'accumulated_submission_time': 2408.1151778697968, 'accumulated_eval_time': 66.53537082672119, 'accumulated_logging_time': 0.036933183670043945}
I0607 17:37:49.750484 140677040035584 logging_writer.py:48] [3029] accumulated_eval_time=66.535371, accumulated_logging_time=0.036933, accumulated_submission_time=2408.115178, global_step=3029, preemption_count=0, score=2408.115178, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2476.323275, train/ctc_loss=nan, train/wer=0.941494, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 17:43:58.069387 140676764382976 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0607 17:43:58.077492 140706249529152 submission.py:139] 3500) loss = nan, grad_norm = nan
I0607 17:50:27.419932 140676746405632 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0607 17:50:27.425623 140706249529152 submission.py:139] 4000) loss = nan, grad_norm = nan
I0607 17:56:57.386626 140676764382976 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0607 17:56:57.394096 140706249529152 submission.py:139] 4500) loss = nan, grad_norm = nan
I0607 18:03:25.989640 140676746405632 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0607 18:03:25.994243 140706249529152 submission.py:139] 5000) loss = nan, grad_norm = nan
I0607 18:09:54.880733 140676746405632 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0607 18:09:54.887052 140706249529152 submission.py:139] 5500) loss = nan, grad_norm = nan
I0607 18:16:22.346338 140676738012928 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0607 18:16:22.353688 140706249529152 submission.py:139] 6000) loss = nan, grad_norm = nan
I0607 18:17:49.785724 140706249529152 spec.py:298] Evaluating on the training split.
I0607 18:17:59.788108 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 18:18:08.746835 140706249529152 spec.py:326] Evaluating on the test split.
I0607 18:18:13.707044 140706249529152 submission_runner.py:419] Time since start: 4900.30s, 	Step: 6114, 	{'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4806.272583723068, 'total_duration': 4900.300364494324, 'accumulated_submission_time': 4806.272583723068, 'accumulated_eval_time': 90.45632815361023, 'accumulated_logging_time': 0.0661017894744873}
I0607 18:18:13.728765 140676746405632 logging_writer.py:48] [6114] accumulated_eval_time=90.456328, accumulated_logging_time=0.066102, accumulated_submission_time=4806.272584, global_step=6114, preemption_count=0, score=4806.272584, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=4900.300364, train/ctc_loss=nan, train/wer=0.941494, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 18:23:16.440152 140676747597568 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0607 18:23:16.447501 140706249529152 submission.py:139] 6500) loss = nan, grad_norm = nan
I0607 18:29:43.774795 140676739204864 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0607 18:29:43.782430 140706249529152 submission.py:139] 7000) loss = nan, grad_norm = nan
I0607 18:36:12.775059 140676747597568 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0607 18:36:12.782181 140706249529152 submission.py:139] 7500) loss = nan, grad_norm = nan
I0607 18:42:39.430002 140676739204864 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0607 18:42:39.435045 140706249529152 submission.py:139] 8000) loss = nan, grad_norm = nan
I0607 18:49:06.737357 140676747597568 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0607 18:49:06.744522 140706249529152 submission.py:139] 8500) loss = nan, grad_norm = nan
I0607 18:55:34.647154 140676739204864 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0607 18:55:34.654720 140706249529152 submission.py:139] 9000) loss = nan, grad_norm = nan
I0607 18:58:13.793962 140706249529152 spec.py:298] Evaluating on the training split.
I0607 18:58:23.672177 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 18:58:32.981637 140706249529152 spec.py:326] Evaluating on the test split.
I0607 18:58:38.173299 140706249529152 submission_runner.py:419] Time since start: 7324.77s, 	Step: 9204, 	{'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7204.414165735245, 'total_duration': 7324.766700744629, 'accumulated_submission_time': 7204.414165735245, 'accumulated_eval_time': 114.8354172706604, 'accumulated_logging_time': 0.09656572341918945}
I0607 18:58:38.193898 140676747597568 logging_writer.py:48] [9204] accumulated_eval_time=114.835417, accumulated_logging_time=0.096566, accumulated_submission_time=7204.414166, global_step=9204, preemption_count=0, score=7204.414166, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7324.766701, train/ctc_loss=nan, train/wer=0.941494, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 19:02:30.481162 140676747597568 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0607 19:02:30.491897 140706249529152 submission.py:139] 9500) loss = nan, grad_norm = nan
I0607 19:08:58.183438 140676739204864 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0607 19:08:58.189064 140706249529152 submission.py:139] 10000) loss = nan, grad_norm = nan
I0607 19:15:27.316358 140676747597568 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0607 19:15:27.322645 140706249529152 submission.py:139] 10500) loss = nan, grad_norm = nan
I0607 19:21:56.036918 140676739204864 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0607 19:21:56.042079 140706249529152 submission.py:139] 11000) loss = nan, grad_norm = nan
I0607 19:28:25.968973 140676747597568 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0607 19:28:25.975279 140706249529152 submission.py:139] 11500) loss = nan, grad_norm = nan
I0607 19:34:56.113684 140676739204864 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0607 19:34:56.119299 140706249529152 submission.py:139] 12000) loss = nan, grad_norm = nan
I0607 19:38:38.592839 140706249529152 spec.py:298] Evaluating on the training split.
I0607 19:38:48.334340 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 19:38:57.394132 140706249529152 spec.py:326] Evaluating on the test split.
I0607 19:39:02.333740 140706249529152 submission_runner.py:419] Time since start: 9748.93s, 	Step: 12288, 	{'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9602.854659557343, 'total_duration': 9748.927174568176, 'accumulated_submission_time': 9602.854659557343, 'accumulated_eval_time': 138.57618188858032, 'accumulated_logging_time': 0.12596845626831055}
I0607 19:39:02.353108 140676747597568 logging_writer.py:48] [12288] accumulated_eval_time=138.576182, accumulated_logging_time=0.125968, accumulated_submission_time=9602.854660, global_step=12288, preemption_count=0, score=9602.854660, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9748.927175, train/ctc_loss=nan, train/wer=0.941494, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 19:41:49.218497 140676747597568 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0607 19:41:49.226325 140706249529152 submission.py:139] 12500) loss = nan, grad_norm = nan
I0607 19:48:19.738668 140676739204864 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0607 19:48:19.744216 140706249529152 submission.py:139] 13000) loss = nan, grad_norm = nan
I0607 19:54:48.838151 140676747597568 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0607 19:54:48.844897 140706249529152 submission.py:139] 13500) loss = nan, grad_norm = nan
I0607 20:01:17.463256 140676739204864 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0607 20:01:17.467918 140706249529152 submission.py:139] 14000) loss = nan, grad_norm = nan
I0607 20:07:45.785090 140676747597568 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0607 20:07:45.792122 140706249529152 submission.py:139] 14500) loss = nan, grad_norm = nan
I0607 20:14:14.243321 140676739204864 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0607 20:14:14.287985 140706249529152 submission.py:139] 15000) loss = nan, grad_norm = nan
I0607 20:19:03.049418 140706249529152 spec.py:298] Evaluating on the training split.
I0607 20:19:12.854431 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 20:19:21.830118 140706249529152 spec.py:326] Evaluating on the test split.
I0607 20:19:26.771407 140706249529152 submission_runner.py:419] Time since start: 12173.36s, 	Step: 15374, 	{'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12001.58849477768, 'total_duration': 12173.364710092545, 'accumulated_submission_time': 12001.58849477768, 'accumulated_eval_time': 162.29802536964417, 'accumulated_logging_time': 0.1539902687072754}
I0607 20:19:26.792030 140676747597568 logging_writer.py:48] [15374] accumulated_eval_time=162.298025, accumulated_logging_time=0.153990, accumulated_submission_time=12001.588495, global_step=15374, preemption_count=0, score=12001.588495, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12173.364710, train/ctc_loss=nan, train/wer=0.941494, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 20:21:06.920552 140676747597568 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0607 20:21:06.927669 140706249529152 submission.py:139] 15500) loss = nan, grad_norm = nan
I0607 20:27:35.171115 140706249529152 spec.py:298] Evaluating on the training split.
I0607 20:27:44.777212 140706249529152 spec.py:310] Evaluating on the validation split.
I0607 20:27:53.746062 140706249529152 spec.py:326] Evaluating on the test split.
I0607 20:27:58.688022 140706249529152 submission_runner.py:419] Time since start: 12685.28s, 	Step: 16000, 	{'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12489.524162769318, 'total_duration': 12685.281433820724, 'accumulated_submission_time': 12489.524162769318, 'accumulated_eval_time': 185.81491255760193, 'accumulated_logging_time': 0.18459439277648926}
I0607 20:27:58.706844 140676747597568 logging_writer.py:48] [16000] accumulated_eval_time=185.814913, accumulated_logging_time=0.184594, accumulated_submission_time=12489.524163, global_step=16000, preemption_count=0, score=12489.524163, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12685.281434, train/ctc_loss=nan, train/wer=0.941494, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 20:27:58.726524 140676739204864 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12489.524163
I0607 20:27:58.984934 140706249529152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nesterov/librispeech_deepspeech_pytorch/trial_1/checkpoint_16000.
I0607 20:27:59.084321 140706249529152 submission_runner.py:581] Tuning trial 1/1
I0607 20:27:59.084557 140706249529152 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 20:27:59.085063 140706249529152 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.59767087298022, 'train/wer': 4.490518296830364, 'validation/ctc_loss': 30.129445449065702, 'validation/wer': 4.025645729734949, 'validation/num_examples': 5348, 'test/ctc_loss': 30.193847990192914, 'test/wer': 4.5177421648081575, 'test/num_examples': 2472, 'score': 9.30928111076355, 'total_duration': 52.45373058319092, 'accumulated_submission_time': 9.30928111076355, 'accumulated_eval_time': 43.14396333694458, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3029, {'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2408.1151778697968, 'total_duration': 2476.3232753276825, 'accumulated_submission_time': 2408.1151778697968, 'accumulated_eval_time': 66.53537082672119, 'accumulated_logging_time': 0.036933183670043945, 'global_step': 3029, 'preemption_count': 0}), (6114, {'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4806.272583723068, 'total_duration': 4900.300364494324, 'accumulated_submission_time': 4806.272583723068, 'accumulated_eval_time': 90.45632815361023, 'accumulated_logging_time': 0.0661017894744873, 'global_step': 6114, 'preemption_count': 0}), (9204, {'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7204.414165735245, 'total_duration': 7324.766700744629, 'accumulated_submission_time': 7204.414165735245, 'accumulated_eval_time': 114.8354172706604, 'accumulated_logging_time': 0.09656572341918945, 'global_step': 9204, 'preemption_count': 0}), (12288, {'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9602.854659557343, 'total_duration': 9748.927174568176, 'accumulated_submission_time': 9602.854659557343, 'accumulated_eval_time': 138.57618188858032, 'accumulated_logging_time': 0.12596845626831055, 'global_step': 12288, 'preemption_count': 0}), (15374, {'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12001.58849477768, 'total_duration': 12173.364710092545, 'accumulated_submission_time': 12001.58849477768, 'accumulated_eval_time': 162.29802536964417, 'accumulated_logging_time': 0.1539902687072754, 'global_step': 15374, 'preemption_count': 0}), (16000, {'train/ctc_loss': nan, 'train/wer': 0.9414936743300232, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12489.524162769318, 'total_duration': 12685.281433820724, 'accumulated_submission_time': 12489.524162769318, 'accumulated_eval_time': 185.81491255760193, 'accumulated_logging_time': 0.18459439277648926, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0607 20:27:59.085174 140706249529152 submission_runner.py:584] Timing: 12489.524162769318
I0607 20:27:59.085223 140706249529152 submission_runner.py:586] Total number of evals: 7
I0607 20:27:59.085286 140706249529152 submission_runner.py:587] ====================
I0607 20:27:59.085463 140706249529152 submission_runner.py:655] Final librispeech_deepspeech score: 12489.524162769318
