torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_06-07-2023-16-16-17.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 16:16:40.302510 140194048685888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 16:16:40.302544 139911936141120 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 16:16:40.302596 139854316205888 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 16:16:40.302630 140587134388032 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 16:16:40.303242 140437922645824 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 16:16:40.303280 139725686835008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 16:16:41.291084 139916016957248 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 16:16:41.299192 140035348019008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 16:16:41.299574 140035348019008 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301701 140194048685888 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301833 139916016957248 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301780 140437922645824 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301805 140587134388032 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301756 139911936141120 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301777 139854316205888 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.301831 139725686835008 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:16:41.677466 140035348019008 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch because --overwrite was set.
I0607 16:16:41.697272 140035348019008 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch.
W0607 16:16:41.704745 139854316205888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.706421 139916016957248 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.707031 140587134388032 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.707726 140437922645824 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.707785 139725686835008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.708323 139911936141120 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.708373 140194048685888 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:16:41.726636 140035348019008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 16:16:41.731778 140035348019008 submission_runner.py:541] Using RNG seed 98439114
I0607 16:16:41.733339 140035348019008 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 16:16:41.733440 140035348019008 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch/trial_1.
I0607 16:16:41.733750 140035348019008 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0607 16:16:41.734754 140035348019008 submission_runner.py:255] Initializing dataset.
I0607 16:16:41.734899 140035348019008 input_pipeline.py:20] Loading split = train-clean-100
I0607 16:16:41.981738 140035348019008 input_pipeline.py:20] Loading split = train-clean-360
I0607 16:16:42.323442 140035348019008 input_pipeline.py:20] Loading split = train-other-500
I0607 16:16:42.775317 140035348019008 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0607 16:16:50.358529 140035348019008 submission_runner.py:272] Initializing optimizer.
I0607 16:16:50.359523 140035348019008 submission_runner.py:279] Initializing metrics bundle.
I0607 16:16:50.359661 140035348019008 submission_runner.py:297] Initializing checkpoint and logger.
I0607 16:16:50.360990 140035348019008 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 16:16:50.361105 140035348019008 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 16:16:51.081723 140035348019008 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0607 16:16:51.082623 140035348019008 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0607 16:16:51.090590 140035348019008 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0607 16:17:00.418417 140008820307712 logging_writer.py:48] [0] global_step=0, grad_norm=26.502810, loss=33.473606
I0607 16:17:00.437372 140035348019008 submission.py:120] 0) loss = 33.474, grad_norm = 26.503
I0607 16:17:00.438666 140035348019008 spec.py:298] Evaluating on the training split.
I0607 16:17:00.439772 140035348019008 input_pipeline.py:20] Loading split = train-clean-100
I0607 16:17:00.474522 140035348019008 input_pipeline.py:20] Loading split = train-clean-360
I0607 16:17:00.911896 140035348019008 input_pipeline.py:20] Loading split = train-other-500
I0607 16:17:17.344804 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 16:17:17.346135 140035348019008 input_pipeline.py:20] Loading split = dev-clean
I0607 16:17:17.350157 140035348019008 input_pipeline.py:20] Loading split = dev-other
I0607 16:17:28.841774 140035348019008 spec.py:326] Evaluating on the test split.
I0607 16:17:28.843358 140035348019008 input_pipeline.py:20] Loading split = test-clean
I0607 16:17:35.585943 140035348019008 submission_runner.py:419] Time since start: 44.50s, 	Step: 1, 	{'train/ctc_loss': 30.986918144866408, 'train/wer': 2.094861831444659, 'validation/ctc_loss': 29.773225587703436, 'validation/wer': 2.051494230676387, 'validation/num_examples': 5348, 'test/ctc_loss': 30.069439963868636, 'test/wer': 1.9103446875063474, 'test/num_examples': 2472, 'score': 9.34815502166748, 'total_duration': 44.495596170425415, 'accumulated_submission_time': 9.34815502166748, 'accumulated_eval_time': 35.14701533317566, 'accumulated_logging_time': 0}
I0607 16:17:35.608389 140006773487360 logging_writer.py:48] [1] accumulated_eval_time=35.147015, accumulated_logging_time=0, accumulated_submission_time=9.348155, global_step=1, preemption_count=0, score=9.348155, test/ctc_loss=30.069440, test/num_examples=2472, test/wer=1.910345, total_duration=44.495596, train/ctc_loss=30.986918, train/wer=2.094862, validation/ctc_loss=29.773226, validation/num_examples=5348, validation/wer=2.051494
I0607 16:17:35.650180 140035348019008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.650156 139911936141120 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.650149 140437922645824 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.650179 140587134388032 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.650175 139725686835008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.650353 139916016957248 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.650908 140194048685888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:35.651877 139854316205888 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:17:36.766312 140006765094656 logging_writer.py:48] [1] global_step=1, grad_norm=27.738310, loss=32.862267
I0607 16:17:36.769914 140035348019008 submission.py:120] 1) loss = 32.862, grad_norm = 27.738
I0607 16:17:37.702211 140006773487360 logging_writer.py:48] [2] global_step=2, grad_norm=26.468378, loss=33.406364
I0607 16:17:37.705443 140035348019008 submission.py:120] 2) loss = 33.406, grad_norm = 26.468
I0607 16:17:38.514030 140006765094656 logging_writer.py:48] [3] global_step=3, grad_norm=27.776697, loss=33.435425
I0607 16:17:38.517218 140035348019008 submission.py:120] 3) loss = 33.435, grad_norm = 27.777
I0607 16:17:39.327034 140006773487360 logging_writer.py:48] [4] global_step=4, grad_norm=30.803122, loss=32.950233
I0607 16:17:39.330435 140035348019008 submission.py:120] 4) loss = 32.950, grad_norm = 30.803
I0607 16:17:40.136349 140006765094656 logging_writer.py:48] [5] global_step=5, grad_norm=33.773342, loss=33.228188
I0607 16:17:40.139724 140035348019008 submission.py:120] 5) loss = 33.228, grad_norm = 33.773
I0607 16:17:40.949358 140006773487360 logging_writer.py:48] [6] global_step=6, grad_norm=46.235989, loss=33.258636
I0607 16:17:40.952969 140035348019008 submission.py:120] 6) loss = 33.259, grad_norm = 46.236
I0607 16:17:41.758150 140006765094656 logging_writer.py:48] [7] global_step=7, grad_norm=52.702721, loss=32.199966
I0607 16:17:41.761356 140035348019008 submission.py:120] 7) loss = 32.200, grad_norm = 52.703
I0607 16:17:42.576540 140006773487360 logging_writer.py:48] [8] global_step=8, grad_norm=63.179493, loss=32.505440
I0607 16:17:42.579931 140035348019008 submission.py:120] 8) loss = 32.505, grad_norm = 63.179
I0607 16:17:43.389458 140006765094656 logging_writer.py:48] [9] global_step=9, grad_norm=67.706581, loss=31.994982
I0607 16:17:43.392730 140035348019008 submission.py:120] 9) loss = 31.995, grad_norm = 67.707
I0607 16:17:44.196938 140006773487360 logging_writer.py:48] [10] global_step=10, grad_norm=68.057121, loss=31.568420
I0607 16:17:44.200560 140035348019008 submission.py:120] 10) loss = 31.568, grad_norm = 68.057
I0607 16:17:45.004256 140006765094656 logging_writer.py:48] [11] global_step=11, grad_norm=66.232132, loss=31.510521
I0607 16:17:45.007786 140035348019008 submission.py:120] 11) loss = 31.511, grad_norm = 66.232
I0607 16:17:45.822482 140006773487360 logging_writer.py:48] [12] global_step=12, grad_norm=63.164738, loss=31.377407
I0607 16:17:45.826014 140035348019008 submission.py:120] 12) loss = 31.377, grad_norm = 63.165
I0607 16:17:46.632327 140006765094656 logging_writer.py:48] [13] global_step=13, grad_norm=60.519394, loss=30.684093
I0607 16:17:46.635999 140035348019008 submission.py:120] 13) loss = 30.684, grad_norm = 60.519
I0607 16:17:47.450805 140006773487360 logging_writer.py:48] [14] global_step=14, grad_norm=55.573116, loss=30.310497
I0607 16:17:47.454126 140035348019008 submission.py:120] 14) loss = 30.310, grad_norm = 55.573
I0607 16:17:48.260888 140006765094656 logging_writer.py:48] [15] global_step=15, grad_norm=50.712536, loss=29.214384
I0607 16:17:48.264206 140035348019008 submission.py:120] 15) loss = 29.214, grad_norm = 50.713
I0607 16:17:49.075443 140006773487360 logging_writer.py:48] [16] global_step=16, grad_norm=45.217354, loss=29.545828
I0607 16:17:49.078640 140035348019008 submission.py:120] 16) loss = 29.546, grad_norm = 45.217
I0607 16:17:49.881486 140006765094656 logging_writer.py:48] [17] global_step=17, grad_norm=41.956631, loss=29.037619
I0607 16:17:49.884974 140035348019008 submission.py:120] 17) loss = 29.038, grad_norm = 41.957
I0607 16:17:50.693668 140006773487360 logging_writer.py:48] [18] global_step=18, grad_norm=36.669315, loss=28.729675
I0607 16:17:50.697144 140035348019008 submission.py:120] 18) loss = 28.730, grad_norm = 36.669
I0607 16:17:51.507897 140006765094656 logging_writer.py:48] [19] global_step=19, grad_norm=33.549107, loss=28.039261
I0607 16:17:51.511456 140035348019008 submission.py:120] 19) loss = 28.039, grad_norm = 33.549
I0607 16:17:52.320524 140006773487360 logging_writer.py:48] [20] global_step=20, grad_norm=30.435484, loss=27.552992
I0607 16:17:52.323811 140035348019008 submission.py:120] 20) loss = 27.553, grad_norm = 30.435
I0607 16:17:53.135069 140006765094656 logging_writer.py:48] [21] global_step=21, grad_norm=28.618387, loss=27.593428
I0607 16:17:53.138267 140035348019008 submission.py:120] 21) loss = 27.593, grad_norm = 28.618
I0607 16:17:53.937598 140006773487360 logging_writer.py:48] [22] global_step=22, grad_norm=28.305689, loss=28.005659
I0607 16:17:53.941262 140035348019008 submission.py:120] 22) loss = 28.006, grad_norm = 28.306
I0607 16:17:54.751549 140006765094656 logging_writer.py:48] [23] global_step=23, grad_norm=26.122402, loss=27.243872
I0607 16:17:54.754925 140035348019008 submission.py:120] 23) loss = 27.244, grad_norm = 26.122
I0607 16:17:55.597198 140006773487360 logging_writer.py:48] [24] global_step=24, grad_norm=25.550293, loss=27.326052
I0607 16:17:55.601338 140035348019008 submission.py:120] 24) loss = 27.326, grad_norm = 25.550
I0607 16:17:56.407743 140006765094656 logging_writer.py:48] [25] global_step=25, grad_norm=25.152937, loss=27.168697
I0607 16:17:56.411220 140035348019008 submission.py:120] 25) loss = 27.169, grad_norm = 25.153
I0607 16:17:57.217666 140006773487360 logging_writer.py:48] [26] global_step=26, grad_norm=25.554094, loss=26.874193
I0607 16:17:57.221040 140035348019008 submission.py:120] 26) loss = 26.874, grad_norm = 25.554
I0607 16:17:58.036512 140006765094656 logging_writer.py:48] [27] global_step=27, grad_norm=24.800200, loss=26.696911
I0607 16:17:58.039956 140035348019008 submission.py:120] 27) loss = 26.697, grad_norm = 24.800
I0607 16:17:58.863411 140006773487360 logging_writer.py:48] [28] global_step=28, grad_norm=24.691833, loss=26.262169
I0607 16:17:58.866853 140035348019008 submission.py:120] 28) loss = 26.262, grad_norm = 24.692
I0607 16:17:59.675975 140006765094656 logging_writer.py:48] [29] global_step=29, grad_norm=25.198977, loss=25.819033
I0607 16:17:59.680013 140035348019008 submission.py:120] 29) loss = 25.819, grad_norm = 25.199
I0607 16:18:00.492103 140006773487360 logging_writer.py:48] [30] global_step=30, grad_norm=23.068359, loss=25.657995
I0607 16:18:00.495512 140035348019008 submission.py:120] 30) loss = 25.658, grad_norm = 23.068
I0607 16:18:01.307258 140006765094656 logging_writer.py:48] [31] global_step=31, grad_norm=25.163895, loss=25.709675
I0607 16:18:01.310464 140035348019008 submission.py:120] 31) loss = 25.710, grad_norm = 25.164
I0607 16:18:02.131481 140006773487360 logging_writer.py:48] [32] global_step=32, grad_norm=25.497341, loss=25.142605
I0607 16:18:02.135057 140035348019008 submission.py:120] 32) loss = 25.143, grad_norm = 25.497
I0607 16:18:02.964126 140006765094656 logging_writer.py:48] [33] global_step=33, grad_norm=24.896772, loss=25.475115
I0607 16:18:02.967553 140035348019008 submission.py:120] 33) loss = 25.475, grad_norm = 24.897
I0607 16:18:03.798077 140006773487360 logging_writer.py:48] [34] global_step=34, grad_norm=25.676041, loss=24.757620
I0607 16:18:03.801481 140035348019008 submission.py:120] 34) loss = 24.758, grad_norm = 25.676
I0607 16:18:04.627466 140006765094656 logging_writer.py:48] [35] global_step=35, grad_norm=25.893078, loss=24.599977
I0607 16:18:04.631231 140035348019008 submission.py:120] 35) loss = 24.600, grad_norm = 25.893
I0607 16:18:05.476573 140006773487360 logging_writer.py:48] [36] global_step=36, grad_norm=26.015020, loss=24.068411
I0607 16:18:05.480986 140035348019008 submission.py:120] 36) loss = 24.068, grad_norm = 26.015
I0607 16:18:06.292131 140006765094656 logging_writer.py:48] [37] global_step=37, grad_norm=26.318386, loss=23.420841
I0607 16:18:06.295752 140035348019008 submission.py:120] 37) loss = 23.421, grad_norm = 26.318
I0607 16:18:07.116867 140006773487360 logging_writer.py:48] [38] global_step=38, grad_norm=25.889830, loss=23.431545
I0607 16:18:07.121067 140035348019008 submission.py:120] 38) loss = 23.432, grad_norm = 25.890
I0607 16:18:07.943646 140006765094656 logging_writer.py:48] [39] global_step=39, grad_norm=26.683023, loss=23.141855
I0607 16:18:07.947905 140035348019008 submission.py:120] 39) loss = 23.142, grad_norm = 26.683
I0607 16:18:08.771070 140006773487360 logging_writer.py:48] [40] global_step=40, grad_norm=26.251616, loss=22.508118
I0607 16:18:08.775153 140035348019008 submission.py:120] 40) loss = 22.508, grad_norm = 26.252
I0607 16:18:09.589384 140006765094656 logging_writer.py:48] [41] global_step=41, grad_norm=26.256056, loss=21.724167
I0607 16:18:09.593649 140035348019008 submission.py:120] 41) loss = 21.724, grad_norm = 26.256
I0607 16:18:10.426538 140006773487360 logging_writer.py:48] [42] global_step=42, grad_norm=25.427908, loss=21.553656
I0607 16:18:10.430541 140035348019008 submission.py:120] 42) loss = 21.554, grad_norm = 25.428
I0607 16:18:11.241780 140006765094656 logging_writer.py:48] [43] global_step=43, grad_norm=25.229219, loss=20.905476
I0607 16:18:11.245792 140035348019008 submission.py:120] 43) loss = 20.905, grad_norm = 25.229
I0607 16:18:12.068257 140006773487360 logging_writer.py:48] [44] global_step=44, grad_norm=24.853258, loss=20.772873
I0607 16:18:12.072147 140035348019008 submission.py:120] 44) loss = 20.773, grad_norm = 24.853
I0607 16:18:12.879768 140006765094656 logging_writer.py:48] [45] global_step=45, grad_norm=24.582575, loss=20.131124
I0607 16:18:12.883298 140035348019008 submission.py:120] 45) loss = 20.131, grad_norm = 24.583
I0607 16:18:13.695995 140006773487360 logging_writer.py:48] [46] global_step=46, grad_norm=23.630531, loss=19.354010
I0607 16:18:13.700058 140035348019008 submission.py:120] 46) loss = 19.354, grad_norm = 23.631
I0607 16:18:14.508183 140006765094656 logging_writer.py:48] [47] global_step=47, grad_norm=23.670086, loss=19.075062
I0607 16:18:14.512346 140035348019008 submission.py:120] 47) loss = 19.075, grad_norm = 23.670
I0607 16:18:15.332017 140006773487360 logging_writer.py:48] [48] global_step=48, grad_norm=23.964256, loss=19.199350
I0607 16:18:15.335961 140035348019008 submission.py:120] 48) loss = 19.199, grad_norm = 23.964
I0607 16:18:16.155692 140006765094656 logging_writer.py:48] [49] global_step=49, grad_norm=22.813442, loss=18.718594
I0607 16:18:16.159232 140035348019008 submission.py:120] 49) loss = 18.719, grad_norm = 22.813
I0607 16:18:16.997757 140006773487360 logging_writer.py:48] [50] global_step=50, grad_norm=22.544102, loss=17.860020
I0607 16:18:17.001193 140035348019008 submission.py:120] 50) loss = 17.860, grad_norm = 22.544
I0607 16:18:17.820597 140006765094656 logging_writer.py:48] [51] global_step=51, grad_norm=21.001394, loss=17.495779
I0607 16:18:17.824811 140035348019008 submission.py:120] 51) loss = 17.496, grad_norm = 21.001
I0607 16:18:18.646797 140006773487360 logging_writer.py:48] [52] global_step=52, grad_norm=21.251001, loss=16.768776
I0607 16:18:18.650712 140035348019008 submission.py:120] 52) loss = 16.769, grad_norm = 21.251
I0607 16:18:19.456365 140006765094656 logging_writer.py:48] [53] global_step=53, grad_norm=21.373377, loss=16.605804
I0607 16:18:19.459805 140035348019008 submission.py:120] 53) loss = 16.606, grad_norm = 21.373
I0607 16:18:20.279179 140006773487360 logging_writer.py:48] [54] global_step=54, grad_norm=20.782866, loss=16.544529
I0607 16:18:20.282560 140035348019008 submission.py:120] 54) loss = 16.545, grad_norm = 20.783
I0607 16:18:21.092478 140006765094656 logging_writer.py:48] [55] global_step=55, grad_norm=20.733868, loss=16.388672
I0607 16:18:21.095819 140035348019008 submission.py:120] 55) loss = 16.389, grad_norm = 20.734
I0607 16:18:21.915509 140006773487360 logging_writer.py:48] [56] global_step=56, grad_norm=21.605497, loss=15.786599
I0607 16:18:21.919596 140035348019008 submission.py:120] 56) loss = 15.787, grad_norm = 21.605
I0607 16:18:22.729577 140006765094656 logging_writer.py:48] [57] global_step=57, grad_norm=19.828560, loss=15.376466
I0607 16:18:22.733181 140035348019008 submission.py:120] 57) loss = 15.376, grad_norm = 19.829
I0607 16:18:23.557859 140006773487360 logging_writer.py:48] [58] global_step=58, grad_norm=19.023045, loss=14.602015
I0607 16:18:23.561177 140035348019008 submission.py:120] 58) loss = 14.602, grad_norm = 19.023
I0607 16:18:24.376938 140006765094656 logging_writer.py:48] [59] global_step=59, grad_norm=18.968914, loss=14.349801
I0607 16:18:24.380373 140035348019008 submission.py:120] 59) loss = 14.350, grad_norm = 18.969
I0607 16:18:25.199895 140006773487360 logging_writer.py:48] [60] global_step=60, grad_norm=19.235168, loss=14.376344
I0607 16:18:25.203562 140035348019008 submission.py:120] 60) loss = 14.376, grad_norm = 19.235
I0607 16:18:26.014418 140006765094656 logging_writer.py:48] [61] global_step=61, grad_norm=17.977526, loss=13.556554
I0607 16:18:26.018404 140035348019008 submission.py:120] 61) loss = 13.557, grad_norm = 17.978
I0607 16:18:26.836688 140006773487360 logging_writer.py:48] [62] global_step=62, grad_norm=19.210070, loss=13.524410
I0607 16:18:26.840284 140035348019008 submission.py:120] 62) loss = 13.524, grad_norm = 19.210
I0607 16:18:27.662403 140006765094656 logging_writer.py:48] [63] global_step=63, grad_norm=17.697250, loss=13.021090
I0607 16:18:27.665785 140035348019008 submission.py:120] 63) loss = 13.021, grad_norm = 17.697
I0607 16:18:28.472400 140006773487360 logging_writer.py:48] [64] global_step=64, grad_norm=17.705332, loss=12.583904
I0607 16:18:28.476482 140035348019008 submission.py:120] 64) loss = 12.584, grad_norm = 17.705
I0607 16:18:29.299683 140006765094656 logging_writer.py:48] [65] global_step=65, grad_norm=17.399633, loss=12.184895
I0607 16:18:29.303761 140035348019008 submission.py:120] 65) loss = 12.185, grad_norm = 17.400
I0607 16:18:30.114187 140006773487360 logging_writer.py:48] [66] global_step=66, grad_norm=14.759905, loss=11.775094
I0607 16:18:30.118312 140035348019008 submission.py:120] 66) loss = 11.775, grad_norm = 14.760
I0607 16:18:30.926475 140006765094656 logging_writer.py:48] [67] global_step=67, grad_norm=15.569613, loss=11.341537
I0607 16:18:30.929986 140035348019008 submission.py:120] 67) loss = 11.342, grad_norm = 15.570
I0607 16:18:31.742747 140006773487360 logging_writer.py:48] [68] global_step=68, grad_norm=14.161740, loss=11.145867
I0607 16:18:31.745890 140035348019008 submission.py:120] 68) loss = 11.146, grad_norm = 14.162
I0607 16:18:32.554617 140006765094656 logging_writer.py:48] [69] global_step=69, grad_norm=13.326521, loss=11.102167
I0607 16:18:32.558632 140035348019008 submission.py:120] 69) loss = 11.102, grad_norm = 13.327
I0607 16:18:33.362594 140006773487360 logging_writer.py:48] [70] global_step=70, grad_norm=12.333323, loss=10.837607
I0607 16:18:33.366383 140035348019008 submission.py:120] 70) loss = 10.838, grad_norm = 12.333
I0607 16:18:34.171151 140006765094656 logging_writer.py:48] [71] global_step=71, grad_norm=11.576100, loss=10.641525
I0607 16:18:34.175154 140035348019008 submission.py:120] 71) loss = 10.642, grad_norm = 11.576
I0607 16:18:34.994349 140006773487360 logging_writer.py:48] [72] global_step=72, grad_norm=10.974230, loss=10.160037
I0607 16:18:34.998115 140035348019008 submission.py:120] 72) loss = 10.160, grad_norm = 10.974
I0607 16:18:35.804304 140006765094656 logging_writer.py:48] [73] global_step=73, grad_norm=10.368952, loss=10.114623
I0607 16:18:35.807706 140035348019008 submission.py:120] 73) loss = 10.115, grad_norm = 10.369
I0607 16:18:36.629018 140006773487360 logging_writer.py:48] [74] global_step=74, grad_norm=10.165368, loss=10.002422
I0607 16:18:36.632601 140035348019008 submission.py:120] 74) loss = 10.002, grad_norm = 10.165
I0607 16:18:37.449971 140006765094656 logging_writer.py:48] [75] global_step=75, grad_norm=9.997055, loss=9.982956
I0607 16:18:37.453635 140035348019008 submission.py:120] 75) loss = 9.983, grad_norm = 9.997
I0607 16:18:38.282577 140006773487360 logging_writer.py:48] [76] global_step=76, grad_norm=8.996002, loss=9.616843
I0607 16:18:38.286442 140035348019008 submission.py:120] 76) loss = 9.617, grad_norm = 8.996
I0607 16:18:39.098141 140006765094656 logging_writer.py:48] [77] global_step=77, grad_norm=8.950130, loss=9.620376
I0607 16:18:39.102073 140035348019008 submission.py:120] 77) loss = 9.620, grad_norm = 8.950
I0607 16:18:39.919734 140006773487360 logging_writer.py:48] [78] global_step=78, grad_norm=8.576955, loss=9.501099
I0607 16:18:39.923180 140035348019008 submission.py:120] 78) loss = 9.501, grad_norm = 8.577
I0607 16:18:40.744468 140006765094656 logging_writer.py:48] [79] global_step=79, grad_norm=8.155952, loss=9.342106
I0607 16:18:40.747832 140035348019008 submission.py:120] 79) loss = 9.342, grad_norm = 8.156
I0607 16:18:41.564813 140006773487360 logging_writer.py:48] [80] global_step=80, grad_norm=7.570462, loss=9.295510
I0607 16:18:41.568288 140035348019008 submission.py:120] 80) loss = 9.296, grad_norm = 7.570
I0607 16:18:42.387271 140006765094656 logging_writer.py:48] [81] global_step=81, grad_norm=7.738063, loss=9.192410
I0607 16:18:42.391398 140035348019008 submission.py:120] 81) loss = 9.192, grad_norm = 7.738
I0607 16:18:43.224968 140006773487360 logging_writer.py:48] [82] global_step=82, grad_norm=7.419885, loss=8.985373
I0607 16:18:43.228395 140035348019008 submission.py:120] 82) loss = 8.985, grad_norm = 7.420
I0607 16:18:44.048997 140006765094656 logging_writer.py:48] [83] global_step=83, grad_norm=7.323163, loss=8.912241
I0607 16:18:44.053063 140035348019008 submission.py:120] 83) loss = 8.912, grad_norm = 7.323
I0607 16:18:44.890726 140006773487360 logging_writer.py:48] [84] global_step=84, grad_norm=7.269549, loss=8.849472
I0607 16:18:44.894304 140035348019008 submission.py:120] 84) loss = 8.849, grad_norm = 7.270
I0607 16:18:45.732935 140006765094656 logging_writer.py:48] [85] global_step=85, grad_norm=6.542848, loss=8.694425
I0607 16:18:45.736192 140035348019008 submission.py:120] 85) loss = 8.694, grad_norm = 6.543
I0607 16:18:46.561309 140006773487360 logging_writer.py:48] [86] global_step=86, grad_norm=6.229923, loss=8.458970
I0607 16:18:46.564941 140035348019008 submission.py:120] 86) loss = 8.459, grad_norm = 6.230
I0607 16:18:47.393265 140006765094656 logging_writer.py:48] [87] global_step=87, grad_norm=6.243512, loss=8.452007
I0607 16:18:47.396748 140035348019008 submission.py:120] 87) loss = 8.452, grad_norm = 6.244
I0607 16:18:48.212941 140006773487360 logging_writer.py:48] [88] global_step=88, grad_norm=6.726617, loss=8.399040
I0607 16:18:48.217074 140035348019008 submission.py:120] 88) loss = 8.399, grad_norm = 6.727
I0607 16:18:49.024372 140006765094656 logging_writer.py:48] [89] global_step=89, grad_norm=5.731627, loss=8.387241
I0607 16:18:49.027971 140035348019008 submission.py:120] 89) loss = 8.387, grad_norm = 5.732
I0607 16:18:49.846385 140006773487360 logging_writer.py:48] [90] global_step=90, grad_norm=5.433175, loss=8.135258
I0607 16:18:49.850219 140035348019008 submission.py:120] 90) loss = 8.135, grad_norm = 5.433
I0607 16:18:50.667417 140006765094656 logging_writer.py:48] [91] global_step=91, grad_norm=5.646148, loss=8.205595
I0607 16:18:50.671090 140035348019008 submission.py:120] 91) loss = 8.206, grad_norm = 5.646
I0607 16:18:51.495227 140006773487360 logging_writer.py:48] [92] global_step=92, grad_norm=5.126118, loss=8.107582
I0607 16:18:51.498722 140035348019008 submission.py:120] 92) loss = 8.108, grad_norm = 5.126
I0607 16:18:52.312835 140006765094656 logging_writer.py:48] [93] global_step=93, grad_norm=5.063369, loss=8.090881
I0607 16:18:52.316074 140035348019008 submission.py:120] 93) loss = 8.091, grad_norm = 5.063
I0607 16:18:53.143179 140006773487360 logging_writer.py:48] [94] global_step=94, grad_norm=4.924113, loss=7.951832
I0607 16:18:53.146726 140035348019008 submission.py:120] 94) loss = 7.952, grad_norm = 4.924
I0607 16:18:53.961886 140006765094656 logging_writer.py:48] [95] global_step=95, grad_norm=4.466388, loss=7.915801
I0607 16:18:53.965476 140035348019008 submission.py:120] 95) loss = 7.916, grad_norm = 4.466
I0607 16:18:54.791076 140006773487360 logging_writer.py:48] [96] global_step=96, grad_norm=4.638913, loss=7.984764
I0607 16:18:54.794595 140035348019008 submission.py:120] 96) loss = 7.985, grad_norm = 4.639
I0607 16:18:55.605628 140006765094656 logging_writer.py:48] [97] global_step=97, grad_norm=4.309285, loss=7.771338
I0607 16:18:55.609213 140035348019008 submission.py:120] 97) loss = 7.771, grad_norm = 4.309
I0607 16:18:56.433568 140006773487360 logging_writer.py:48] [98] global_step=98, grad_norm=4.320735, loss=7.866477
I0607 16:18:56.437410 140035348019008 submission.py:120] 98) loss = 7.866, grad_norm = 4.321
I0607 16:18:57.254372 140006765094656 logging_writer.py:48] [99] global_step=99, grad_norm=4.447116, loss=7.875882
I0607 16:18:57.258313 140035348019008 submission.py:120] 99) loss = 7.876, grad_norm = 4.447
I0607 16:18:58.073436 140006773487360 logging_writer.py:48] [100] global_step=100, grad_norm=4.074971, loss=7.786322
I0607 16:18:58.077429 140035348019008 submission.py:120] 100) loss = 7.786, grad_norm = 4.075
I0607 16:24:20.595797 140006765094656 logging_writer.py:48] [500] global_step=500, grad_norm=1.290417, loss=5.773453
I0607 16:24:20.600519 140035348019008 submission.py:120] 500) loss = 5.773, grad_norm = 1.290
I0607 16:31:04.086821 140006773487360 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.014100, loss=4.966144
I0607 16:31:04.091661 140035348019008 submission.py:120] 1000) loss = 4.966, grad_norm = 1.014
I0607 16:37:48.777955 140006773487360 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.307081, loss=3.694944
I0607 16:37:48.787039 140035348019008 submission.py:120] 1500) loss = 3.695, grad_norm = 2.307
I0607 16:44:31.541380 140006765094656 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.657896, loss=3.057842
I0607 16:44:31.546396 140035348019008 submission.py:120] 2000) loss = 3.058, grad_norm = 2.658
I0607 16:51:15.305561 140006765094656 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.492519, loss=2.793623
I0607 16:51:15.312704 140035348019008 submission.py:120] 2500) loss = 2.794, grad_norm = 2.493
I0607 16:57:36.052608 140035348019008 spec.py:298] Evaluating on the training split.
I0607 16:57:46.437591 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 16:57:55.577440 140035348019008 spec.py:326] Evaluating on the test split.
I0607 16:58:00.552266 140035348019008 submission_runner.py:419] Time since start: 2469.46s, 	Step: 2972, 	{'train/ctc_loss': 4.830707154883586, 'train/wer': 0.8832544805198325, 'validation/ctc_loss': 4.755869578184649, 'validation/wer': 0.8519963308067398, 'validation/num_examples': 5348, 'test/ctc_loss': 4.474196722369185, 'test/wer': 0.8352527776085146, 'test/num_examples': 2472, 'score': 2408.407900571823, 'total_duration': 2469.4618306159973, 'accumulated_submission_time': 2408.407900571823, 'accumulated_eval_time': 59.646360874176025, 'accumulated_logging_time': 0.03127002716064453}
I0607 16:58:00.574063 140006765094656 logging_writer.py:48] [2972] accumulated_eval_time=59.646361, accumulated_logging_time=0.031270, accumulated_submission_time=2408.407901, global_step=2972, preemption_count=0, score=2408.407901, test/ctc_loss=4.474197, test/num_examples=2472, test/wer=0.835253, total_duration=2469.461831, train/ctc_loss=4.830707, train/wer=0.883254, validation/ctc_loss=4.755870, validation/num_examples=5348, validation/wer=0.851996
I0607 16:58:24.224473 140006756701952 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.227673, loss=2.547467
I0607 16:58:24.228204 140035348019008 submission.py:120] 3000) loss = 2.547, grad_norm = 2.228
I0607 17:05:07.198693 140006756701952 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.920127, loss=2.413562
I0607 17:05:07.206302 140035348019008 submission.py:120] 3500) loss = 2.414, grad_norm = 2.920
I0607 17:11:49.503338 140006035289856 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.245595, loss=2.287727
I0607 17:11:49.508044 140035348019008 submission.py:120] 4000) loss = 2.288, grad_norm = 3.246
I0607 17:18:31.870969 140006035289856 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.931210, loss=2.209102
I0607 17:18:31.878222 140035348019008 submission.py:120] 4500) loss = 2.209, grad_norm = 1.931
I0607 17:25:14.091009 140006026897152 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.725286, loss=2.105866
I0607 17:25:14.096682 140035348019008 submission.py:120] 5000) loss = 2.106, grad_norm = 1.725
I0607 17:31:58.942869 140006035289856 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.675106, loss=2.082398
I0607 17:31:58.949533 140035348019008 submission.py:120] 5500) loss = 2.082, grad_norm = 1.675
I0607 17:38:00.601989 140035348019008 spec.py:298] Evaluating on the training split.
I0607 17:38:12.506437 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 17:38:22.104119 140035348019008 spec.py:326] Evaluating on the test split.
I0607 17:38:27.423779 140035348019008 submission_runner.py:419] Time since start: 4896.33s, 	Step: 5950, 	{'train/ctc_loss': 0.8792706933791391, 'train/wer': 0.2803203971363693, 'validation/ctc_loss': 1.1584548689597147, 'validation/wer': 0.32386423984936996, 'validation/num_examples': 5348, 'test/ctc_loss': 0.777894239144461, 'test/wer': 0.25111205898482725, 'test/num_examples': 2472, 'score': 4806.97278881073, 'total_duration': 4896.333368778229, 'accumulated_submission_time': 4806.97278881073, 'accumulated_eval_time': 86.4750280380249, 'accumulated_logging_time': 0.06419992446899414}
I0607 17:38:27.446486 140006035289856 logging_writer.py:48] [5950] accumulated_eval_time=86.475028, accumulated_logging_time=0.064200, accumulated_submission_time=4806.972789, global_step=5950, preemption_count=0, score=4806.972789, test/ctc_loss=0.777894, test/num_examples=2472, test/wer=0.251112, total_duration=4896.333369, train/ctc_loss=0.879271, train/wer=0.280320, validation/ctc_loss=1.158455, validation/num_examples=5348, validation/wer=0.323864
I0607 17:39:08.499450 140006026897152 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.473736, loss=1.971638
I0607 17:39:08.503191 140035348019008 submission.py:120] 6000) loss = 1.972, grad_norm = 2.474
I0607 17:45:51.442115 140006035289856 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.732719, loss=1.952677
I0607 17:45:51.450587 140035348019008 submission.py:120] 6500) loss = 1.953, grad_norm = 1.733
I0607 17:52:33.636682 140006026897152 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.836929, loss=1.945272
I0607 17:52:33.644577 140035348019008 submission.py:120] 7000) loss = 1.945, grad_norm = 2.837
I0607 17:59:16.127976 140006026897152 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.636379, loss=1.895101
I0607 17:59:16.135458 140035348019008 submission.py:120] 7500) loss = 1.895, grad_norm = 2.636
I0607 18:05:57.660986 140006018504448 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.785225, loss=1.779038
I0607 18:05:57.666699 140035348019008 submission.py:120] 8000) loss = 1.779, grad_norm = 2.785
I0607 18:12:40.476927 140006026897152 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.987220, loss=1.766359
I0607 18:12:40.483373 140035348019008 submission.py:120] 8500) loss = 1.766, grad_norm = 1.987
I0607 18:18:27.852435 140035348019008 spec.py:298] Evaluating on the training split.
I0607 18:18:40.035772 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 18:18:50.094961 140035348019008 spec.py:326] Evaluating on the test split.
I0607 18:18:55.353074 140035348019008 submission_runner.py:419] Time since start: 7324.26s, 	Step: 8932, 	{'train/ctc_loss': 0.6582456227997074, 'train/wer': 0.21407319492090332, 'validation/ctc_loss': 0.9247248279586096, 'validation/wer': 0.25941196350118284, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5833895029518034, 'test/wer': 0.19005545061239412, 'test/num_examples': 2472, 'score': 7205.8086569309235, 'total_duration': 7324.262695074081, 'accumulated_submission_time': 7205.8086569309235, 'accumulated_eval_time': 113.97536396980286, 'accumulated_logging_time': 0.09653568267822266}
I0607 18:18:55.373330 140006026897152 logging_writer.py:48] [8932] accumulated_eval_time=113.975364, accumulated_logging_time=0.096536, accumulated_submission_time=7205.808657, global_step=8932, preemption_count=0, score=7205.808657, test/ctc_loss=0.583390, test/num_examples=2472, test/wer=0.190055, total_duration=7324.262695, train/ctc_loss=0.658246, train/wer=0.214073, validation/ctc_loss=0.924725, validation/num_examples=5348, validation/wer=0.259412
I0607 18:19:50.955231 140006018504448 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.592329, loss=1.833315
I0607 18:19:50.959454 140035348019008 submission.py:120] 9000) loss = 1.833, grad_norm = 2.592
I0607 18:26:33.871533 140006026897152 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.797091, loss=1.805544
I0607 18:26:33.879129 140035348019008 submission.py:120] 9500) loss = 1.806, grad_norm = 2.797
I0607 18:33:15.736604 140006018504448 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.786331, loss=1.798585
I0607 18:33:15.742033 140035348019008 submission.py:120] 10000) loss = 1.799, grad_norm = 1.786
I0607 18:39:59.212235 140006026897152 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.893104, loss=1.719093
I0607 18:39:59.219813 140035348019008 submission.py:120] 10500) loss = 1.719, grad_norm = 2.893
I0607 18:46:39.923980 140006018504448 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.563543, loss=1.712134
I0607 18:46:39.931817 140035348019008 submission.py:120] 11000) loss = 1.712, grad_norm = 2.564
I0607 18:53:22.326739 140006026897152 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.493178, loss=1.686064
I0607 18:53:22.333738 140035348019008 submission.py:120] 11500) loss = 1.686, grad_norm = 2.493
I0607 18:58:55.630945 140035348019008 spec.py:298] Evaluating on the training split.
I0607 18:59:07.564209 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 18:59:17.240618 140035348019008 spec.py:326] Evaluating on the test split.
I0607 18:59:22.683331 140035348019008 submission_runner.py:419] Time since start: 9751.59s, 	Step: 11916, 	{'train/ctc_loss': 0.5218284962329292, 'train/wer': 0.17214842755025173, 'validation/ctc_loss': 0.7933235068816556, 'validation/wer': 0.22662096268044224, 'validation/num_examples': 5348, 'test/ctc_loss': 0.47852981442351117, 'test/wer': 0.15524140312392096, 'test/num_examples': 2472, 'score': 9604.472831010818, 'total_duration': 9751.592906713486, 'accumulated_submission_time': 9604.472831010818, 'accumulated_eval_time': 141.02752304077148, 'accumulated_logging_time': 0.12660861015319824}
I0607 18:59:22.706956 140006026897152 logging_writer.py:48] [11916] accumulated_eval_time=141.027523, accumulated_logging_time=0.126609, accumulated_submission_time=9604.472831, global_step=11916, preemption_count=0, score=9604.472831, test/ctc_loss=0.478530, test/num_examples=2472, test/wer=0.155241, total_duration=9751.592907, train/ctc_loss=0.521828, train/wer=0.172148, validation/ctc_loss=0.793324, validation/num_examples=5348, validation/wer=0.226621
I0607 19:00:31.049270 140006018504448 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.500182, loss=1.747148
I0607 19:00:31.053063 140035348019008 submission.py:120] 12000) loss = 1.747, grad_norm = 2.500
I0607 19:07:15.495116 140006026897152 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.581101, loss=1.681556
I0607 19:07:15.502526 140035348019008 submission.py:120] 12500) loss = 1.682, grad_norm = 2.581
I0607 19:13:56.314795 140006018504448 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.776241, loss=1.676077
I0607 19:13:56.319816 140035348019008 submission.py:120] 13000) loss = 1.676, grad_norm = 2.776
I0607 19:20:39.975406 140006026897152 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.212710, loss=1.607238
I0607 19:20:39.982218 140035348019008 submission.py:120] 13500) loss = 1.607, grad_norm = 2.213
I0607 19:27:21.796651 140006018504448 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.070178, loss=1.617293
I0607 19:27:21.802739 140035348019008 submission.py:120] 14000) loss = 1.617, grad_norm = 4.070
I0607 19:34:05.788006 140006026897152 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.116697, loss=1.621414
I0607 19:34:05.797405 140035348019008 submission.py:120] 14500) loss = 1.621, grad_norm = 2.117
I0607 19:39:23.148063 140035348019008 spec.py:298] Evaluating on the training split.
I0607 19:39:35.104340 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 19:39:44.894158 140035348019008 spec.py:326] Evaluating on the test split.
I0607 19:39:50.219087 140035348019008 submission_runner.py:419] Time since start: 12179.13s, 	Step: 14895, 	{'train/ctc_loss': 0.4894037313559754, 'train/wer': 0.16440405157136587, 'validation/ctc_loss': 0.7586115859516275, 'validation/wer': 0.21768937382320283, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4522213630718111, 'test/wer': 0.14849795868624702, 'test/num_examples': 2472, 'score': 12003.38901090622, 'total_duration': 12179.128692150116, 'accumulated_submission_time': 12003.38901090622, 'accumulated_eval_time': 168.1020541191101, 'accumulated_logging_time': 0.16231274604797363}
I0607 19:39:50.238744 140006026897152 logging_writer.py:48] [14895] accumulated_eval_time=168.102054, accumulated_logging_time=0.162313, accumulated_submission_time=12003.389011, global_step=14895, preemption_count=0, score=12003.389011, test/ctc_loss=0.452221, test/num_examples=2472, test/wer=0.148498, total_duration=12179.128692, train/ctc_loss=0.489404, train/wer=0.164404, validation/ctc_loss=0.758612, validation/num_examples=5348, validation/wer=0.217689
I0607 19:41:15.292243 140006018504448 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.821023, loss=1.682062
I0607 19:41:15.297155 140035348019008 submission.py:120] 15000) loss = 1.682, grad_norm = 2.821
I0607 19:47:59.108948 140006026897152 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.029204, loss=1.599584
I0607 19:47:59.116560 140035348019008 submission.py:120] 15500) loss = 1.600, grad_norm = 2.029
I0607 19:54:38.189590 140035348019008 spec.py:298] Evaluating on the training split.
I0607 19:54:50.653263 140035348019008 spec.py:310] Evaluating on the validation split.
I0607 19:55:00.467297 140035348019008 spec.py:326] Evaluating on the test split.
I0607 19:55:06.077598 140035348019008 submission_runner.py:419] Time since start: 13094.99s, 	Step: 16000, 	{'train/ctc_loss': 0.45955049193013486, 'train/wer': 0.15184179578476162, 'validation/ctc_loss': 0.7152012538929073, 'validation/wer': 0.20443199922753827, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4230932741628492, 'test/wer': 0.13683911197773851, 'test/num_examples': 2472, 'score': 12890.618104457855, 'total_duration': 13094.986348867416, 'accumulated_submission_time': 12890.618104457855, 'accumulated_eval_time': 195.9890205860138, 'accumulated_logging_time': 0.19178366661071777}
I0607 19:55:06.102207 140006026897152 logging_writer.py:48] [16000] accumulated_eval_time=195.989021, accumulated_logging_time=0.191784, accumulated_submission_time=12890.618104, global_step=16000, preemption_count=0, score=12890.618104, test/ctc_loss=0.423093, test/num_examples=2472, test/wer=0.136839, total_duration=13094.986349, train/ctc_loss=0.459550, train/wer=0.151842, validation/ctc_loss=0.715201, validation/num_examples=5348, validation/wer=0.204432
I0607 19:55:06.129540 140006018504448 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12890.618104
I0607 19:55:06.482563 140035348019008 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_16000.
I0607 19:55:06.596772 140035348019008 submission_runner.py:581] Tuning trial 1/1
I0607 19:55:06.597015 140035348019008 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 19:55:06.597520 140035348019008 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 30.986918144866408, 'train/wer': 2.094861831444659, 'validation/ctc_loss': 29.773225587703436, 'validation/wer': 2.051494230676387, 'validation/num_examples': 5348, 'test/ctc_loss': 30.069439963868636, 'test/wer': 1.9103446875063474, 'test/num_examples': 2472, 'score': 9.34815502166748, 'total_duration': 44.495596170425415, 'accumulated_submission_time': 9.34815502166748, 'accumulated_eval_time': 35.14701533317566, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2972, {'train/ctc_loss': 4.830707154883586, 'train/wer': 0.8832544805198325, 'validation/ctc_loss': 4.755869578184649, 'validation/wer': 0.8519963308067398, 'validation/num_examples': 5348, 'test/ctc_loss': 4.474196722369185, 'test/wer': 0.8352527776085146, 'test/num_examples': 2472, 'score': 2408.407900571823, 'total_duration': 2469.4618306159973, 'accumulated_submission_time': 2408.407900571823, 'accumulated_eval_time': 59.646360874176025, 'accumulated_logging_time': 0.03127002716064453, 'global_step': 2972, 'preemption_count': 0}), (5950, {'train/ctc_loss': 0.8792706933791391, 'train/wer': 0.2803203971363693, 'validation/ctc_loss': 1.1584548689597147, 'validation/wer': 0.32386423984936996, 'validation/num_examples': 5348, 'test/ctc_loss': 0.777894239144461, 'test/wer': 0.25111205898482725, 'test/num_examples': 2472, 'score': 4806.97278881073, 'total_duration': 4896.333368778229, 'accumulated_submission_time': 4806.97278881073, 'accumulated_eval_time': 86.4750280380249, 'accumulated_logging_time': 0.06419992446899414, 'global_step': 5950, 'preemption_count': 0}), (8932, {'train/ctc_loss': 0.6582456227997074, 'train/wer': 0.21407319492090332, 'validation/ctc_loss': 0.9247248279586096, 'validation/wer': 0.25941196350118284, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5833895029518034, 'test/wer': 0.19005545061239412, 'test/num_examples': 2472, 'score': 7205.8086569309235, 'total_duration': 7324.262695074081, 'accumulated_submission_time': 7205.8086569309235, 'accumulated_eval_time': 113.97536396980286, 'accumulated_logging_time': 0.09653568267822266, 'global_step': 8932, 'preemption_count': 0}), (11916, {'train/ctc_loss': 0.5218284962329292, 'train/wer': 0.17214842755025173, 'validation/ctc_loss': 0.7933235068816556, 'validation/wer': 0.22662096268044224, 'validation/num_examples': 5348, 'test/ctc_loss': 0.47852981442351117, 'test/wer': 0.15524140312392096, 'test/num_examples': 2472, 'score': 9604.472831010818, 'total_duration': 9751.592906713486, 'accumulated_submission_time': 9604.472831010818, 'accumulated_eval_time': 141.02752304077148, 'accumulated_logging_time': 0.12660861015319824, 'global_step': 11916, 'preemption_count': 0}), (14895, {'train/ctc_loss': 0.4894037313559754, 'train/wer': 0.16440405157136587, 'validation/ctc_loss': 0.7586115859516275, 'validation/wer': 0.21768937382320283, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4522213630718111, 'test/wer': 0.14849795868624702, 'test/num_examples': 2472, 'score': 12003.38901090622, 'total_duration': 12179.128692150116, 'accumulated_submission_time': 12003.38901090622, 'accumulated_eval_time': 168.1020541191101, 'accumulated_logging_time': 0.16231274604797363, 'global_step': 14895, 'preemption_count': 0}), (16000, {'train/ctc_loss': 0.45955049193013486, 'train/wer': 0.15184179578476162, 'validation/ctc_loss': 0.7152012538929073, 'validation/wer': 0.20443199922753827, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4230932741628492, 'test/wer': 0.13683911197773851, 'test/num_examples': 2472, 'score': 12890.618104457855, 'total_duration': 13094.986348867416, 'accumulated_submission_time': 12890.618104457855, 'accumulated_eval_time': 195.9890205860138, 'accumulated_logging_time': 0.19178366661071777, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0607 19:55:06.597622 140035348019008 submission_runner.py:584] Timing: 12890.618104457855
I0607 19:55:06.597676 140035348019008 submission_runner.py:586] Total number of evals: 7
I0607 19:55:06.597743 140035348019008 submission_runner.py:587] ====================
I0607 19:55:06.597902 140035348019008 submission_runner.py:655] Final librispeech_deepspeech score: 12890.618104457855
