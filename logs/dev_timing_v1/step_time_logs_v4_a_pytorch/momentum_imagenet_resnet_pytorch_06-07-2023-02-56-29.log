torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-07-2023-02-56-29.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 02:56:52.724277 139875879094080 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 02:56:52.724308 140037203711808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 02:56:52.724421 140147066124096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 02:56:52.725408 139985326753600 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 02:56:52.725398 139969935103808 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 02:56:52.725669 140430963656512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 02:56:52.725805 139685515061056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 02:56:52.736070 139985326753600 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.736054 139969935103808 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.736052 139983768639296 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 02:56:52.736299 140430963656512 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.736329 139685515061056 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.736473 139983768639296 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.745263 139875879094080 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.745304 140037203711808 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:52.745383 140147066124096 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 02:56:55.033256 139983768639296 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch because --overwrite was set.
I0607 02:56:55.047155 139983768639296 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch.
W0607 02:56:55.121932 140037203711808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.122117 140147066124096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.122920 139969935103808 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.123466 139685515061056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.123622 139875879094080 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.124077 139983768639296 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.124232 140430963656512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 02:56:55.124243 139985326753600 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 02:56:55.129452 139983768639296 submission_runner.py:541] Using RNG seed 3940498847
I0607 02:56:55.130853 139983768639296 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 02:56:55.130960 139983768639296 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch/trial_1.
I0607 02:56:55.131271 139983768639296 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch/trial_1/hparams.json.
I0607 02:56:55.132343 139983768639296 submission_runner.py:255] Initializing dataset.
I0607 02:57:01.419432 139983768639296 submission_runner.py:262] Initializing model.
I0607 02:57:06.125584 139983768639296 submission_runner.py:272] Initializing optimizer.
I0607 02:57:06.599992 139983768639296 submission_runner.py:279] Initializing metrics bundle.
I0607 02:57:06.600194 139983768639296 submission_runner.py:297] Initializing checkpoint and logger.
I0607 02:57:07.069234 139983768639296 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0607 02:57:07.070251 139983768639296 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0607 02:57:07.122376 139983768639296 submission_runner.py:332] Starting training loop.
I0607 02:57:14.973087 139953438709504 logging_writer.py:48] [0] global_step=0, grad_norm=0.543757, loss=6.938514
I0607 02:57:14.994413 139983768639296 spec.py:298] Evaluating on the training split.
I0607 02:58:14.187439 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 02:59:09.328793 139983768639296 spec.py:326] Evaluating on the test split.
I0607 02:59:09.348208 139983768639296 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 02:59:09.354559 139983768639296 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0607 02:59:09.430214 139983768639296 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 02:59:21.744384 139983768639296 submission_runner.py:419] Time since start: 134.62s, 	Step: 1, 	{'train/accuracy': 0.0012157206632653062, 'train/loss': 6.9238181600765305, 'validation/accuracy': 0.00092, 'validation/loss': 6.923669375, 'validation/num_examples': 50000, 'test/accuracy': 0.0014, 'test/loss': 6.9268125, 'test/num_examples': 10000, 'score': 7.872222661972046, 'total_duration': 134.62248921394348, 'accumulated_submission_time': 7.872222661972046, 'accumulated_eval_time': 126.74984335899353, 'accumulated_logging_time': 0}
I0607 02:59:21.763095 139931133388544 logging_writer.py:48] [1] accumulated_eval_time=126.749843, accumulated_logging_time=0, accumulated_submission_time=7.872223, global_step=1, preemption_count=0, score=7.872223, test/accuracy=0.001400, test/loss=6.926812, test/num_examples=10000, total_duration=134.622489, train/accuracy=0.001216, train/loss=6.923818, validation/accuracy=0.000920, validation/loss=6.923669, validation/num_examples=50000
I0607 02:59:21.809494 139983768639296 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.809387 139685515061056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.810605 140147066124096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.810624 139985326753600 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.810625 139875879094080 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.810630 139969935103808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.810921 140037203711808 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:21.810914 140430963656512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 02:59:22.179892 139931124995840 logging_writer.py:48] [1] global_step=1, grad_norm=0.562855, loss=6.928993
I0607 02:59:22.557343 139931133388544 logging_writer.py:48] [2] global_step=2, grad_norm=0.544392, loss=6.925323
I0607 02:59:22.935963 139931124995840 logging_writer.py:48] [3] global_step=3, grad_norm=0.543488, loss=6.922613
I0607 02:59:23.314830 139931133388544 logging_writer.py:48] [4] global_step=4, grad_norm=0.543677, loss=6.939765
I0607 02:59:23.696489 139931124995840 logging_writer.py:48] [5] global_step=5, grad_norm=0.541065, loss=6.932242
I0607 02:59:24.079829 139931133388544 logging_writer.py:48] [6] global_step=6, grad_norm=0.552888, loss=6.923242
I0607 02:59:24.463548 139931124995840 logging_writer.py:48] [7] global_step=7, grad_norm=0.560350, loss=6.933318
I0607 02:59:24.847343 139931133388544 logging_writer.py:48] [8] global_step=8, grad_norm=0.548936, loss=6.935421
I0607 02:59:25.233375 139931124995840 logging_writer.py:48] [9] global_step=9, grad_norm=0.561001, loss=6.932458
I0607 02:59:25.614520 139931133388544 logging_writer.py:48] [10] global_step=10, grad_norm=0.548496, loss=6.934575
I0607 02:59:25.997217 139931124995840 logging_writer.py:48] [11] global_step=11, grad_norm=0.539789, loss=6.923081
I0607 02:59:26.377831 139931133388544 logging_writer.py:48] [12] global_step=12, grad_norm=0.541784, loss=6.927769
I0607 02:59:26.761285 139931124995840 logging_writer.py:48] [13] global_step=13, grad_norm=0.551036, loss=6.933028
I0607 02:59:27.146077 139931133388544 logging_writer.py:48] [14] global_step=14, grad_norm=0.557352, loss=6.935054
I0607 02:59:27.528077 139931124995840 logging_writer.py:48] [15] global_step=15, grad_norm=0.539992, loss=6.924008
I0607 02:59:27.910294 139931133388544 logging_writer.py:48] [16] global_step=16, grad_norm=0.530987, loss=6.931748
I0607 02:59:28.294809 139931124995840 logging_writer.py:48] [17] global_step=17, grad_norm=0.548116, loss=6.930386
I0607 02:59:28.680854 139931133388544 logging_writer.py:48] [18] global_step=18, grad_norm=0.555074, loss=6.936581
I0607 02:59:29.070537 139931124995840 logging_writer.py:48] [19] global_step=19, grad_norm=0.550350, loss=6.929360
I0607 02:59:29.455955 139931133388544 logging_writer.py:48] [20] global_step=20, grad_norm=0.552927, loss=6.927859
I0607 02:59:29.845237 139931124995840 logging_writer.py:48] [21] global_step=21, grad_norm=0.545848, loss=6.923719
I0607 02:59:30.226401 139931133388544 logging_writer.py:48] [22] global_step=22, grad_norm=0.548546, loss=6.929625
I0607 02:59:30.611635 139931124995840 logging_writer.py:48] [23] global_step=23, grad_norm=0.531068, loss=6.919044
I0607 02:59:30.996231 139931133388544 logging_writer.py:48] [24] global_step=24, grad_norm=0.548586, loss=6.928201
I0607 02:59:31.377157 139931124995840 logging_writer.py:48] [25] global_step=25, grad_norm=0.536879, loss=6.923404
I0607 02:59:31.762556 139931133388544 logging_writer.py:48] [26] global_step=26, grad_norm=0.548071, loss=6.923042
I0607 02:59:32.155973 139931124995840 logging_writer.py:48] [27] global_step=27, grad_norm=0.537660, loss=6.930781
I0607 02:59:32.542881 139931133388544 logging_writer.py:48] [28] global_step=28, grad_norm=0.549504, loss=6.925273
I0607 02:59:32.926344 139931124995840 logging_writer.py:48] [29] global_step=29, grad_norm=0.545174, loss=6.928703
I0607 02:59:33.310714 139931133388544 logging_writer.py:48] [30] global_step=30, grad_norm=0.546569, loss=6.922586
I0607 02:59:33.698256 139931124995840 logging_writer.py:48] [31] global_step=31, grad_norm=0.533399, loss=6.926570
I0607 02:59:34.078985 139931133388544 logging_writer.py:48] [32] global_step=32, grad_norm=0.526036, loss=6.921171
I0607 02:59:34.465065 139931124995840 logging_writer.py:48] [33] global_step=33, grad_norm=0.560141, loss=6.926135
I0607 02:59:34.852996 139931133388544 logging_writer.py:48] [34] global_step=34, grad_norm=0.539447, loss=6.926719
I0607 02:59:35.239566 139931124995840 logging_writer.py:48] [35] global_step=35, grad_norm=0.528685, loss=6.922465
I0607 02:59:35.622686 139931133388544 logging_writer.py:48] [36] global_step=36, grad_norm=0.547493, loss=6.922088
I0607 02:59:36.006218 139931124995840 logging_writer.py:48] [37] global_step=37, grad_norm=0.554376, loss=6.928133
I0607 02:59:36.390568 139931133388544 logging_writer.py:48] [38] global_step=38, grad_norm=0.528855, loss=6.914783
I0607 02:59:36.774749 139931124995840 logging_writer.py:48] [39] global_step=39, grad_norm=0.539620, loss=6.918739
I0607 02:59:37.158368 139931133388544 logging_writer.py:48] [40] global_step=40, grad_norm=0.544715, loss=6.911678
I0607 02:59:37.550931 139931124995840 logging_writer.py:48] [41] global_step=41, grad_norm=0.519021, loss=6.908589
I0607 02:59:37.933063 139931133388544 logging_writer.py:48] [42] global_step=42, grad_norm=0.535754, loss=6.914112
I0607 02:59:38.321457 139931124995840 logging_writer.py:48] [43] global_step=43, grad_norm=0.546616, loss=6.916617
I0607 02:59:38.706114 139931133388544 logging_writer.py:48] [44] global_step=44, grad_norm=0.537150, loss=6.921031
I0607 02:59:39.090273 139931124995840 logging_writer.py:48] [45] global_step=45, grad_norm=0.528649, loss=6.918829
I0607 02:59:39.474558 139931133388544 logging_writer.py:48] [46] global_step=46, grad_norm=0.551213, loss=6.909927
I0607 02:59:39.856745 139931124995840 logging_writer.py:48] [47] global_step=47, grad_norm=0.535445, loss=6.914155
I0607 02:59:40.240492 139931133388544 logging_writer.py:48] [48] global_step=48, grad_norm=0.534042, loss=6.915555
I0607 02:59:40.628169 139931124995840 logging_writer.py:48] [49] global_step=49, grad_norm=0.549578, loss=6.920521
I0607 02:59:41.015346 139931133388544 logging_writer.py:48] [50] global_step=50, grad_norm=0.532178, loss=6.914074
I0607 02:59:41.405905 139931124995840 logging_writer.py:48] [51] global_step=51, grad_norm=0.534267, loss=6.917960
I0607 02:59:41.789698 139931133388544 logging_writer.py:48] [52] global_step=52, grad_norm=0.527501, loss=6.913738
I0607 02:59:42.173043 139931124995840 logging_writer.py:48] [53] global_step=53, grad_norm=0.529557, loss=6.915100
I0607 02:59:42.557233 139931133388544 logging_writer.py:48] [54] global_step=54, grad_norm=0.542718, loss=6.905947
I0607 02:59:42.943584 139931124995840 logging_writer.py:48] [55] global_step=55, grad_norm=0.526978, loss=6.911240
I0607 02:59:43.324969 139931133388544 logging_writer.py:48] [56] global_step=56, grad_norm=0.536031, loss=6.906484
I0607 02:59:43.705727 139931124995840 logging_writer.py:48] [57] global_step=57, grad_norm=0.542567, loss=6.912934
I0607 02:59:44.091188 139931133388544 logging_writer.py:48] [58] global_step=58, grad_norm=0.546114, loss=6.906644
I0607 02:59:44.474751 139931124995840 logging_writer.py:48] [59] global_step=59, grad_norm=0.541150, loss=6.901850
I0607 02:59:44.862953 139931133388544 logging_writer.py:48] [60] global_step=60, grad_norm=0.537886, loss=6.909612
I0607 02:59:45.248990 139931124995840 logging_writer.py:48] [61] global_step=61, grad_norm=0.520652, loss=6.906479
I0607 02:59:45.642560 139931133388544 logging_writer.py:48] [62] global_step=62, grad_norm=0.528219, loss=6.907277
I0607 02:59:46.025139 139931124995840 logging_writer.py:48] [63] global_step=63, grad_norm=0.545473, loss=6.907993
I0607 02:59:46.418512 139931133388544 logging_writer.py:48] [64] global_step=64, grad_norm=0.561866, loss=6.906940
I0607 02:59:46.802984 139931124995840 logging_writer.py:48] [65] global_step=65, grad_norm=0.533476, loss=6.908554
I0607 02:59:47.185996 139931133388544 logging_writer.py:48] [66] global_step=66, grad_norm=0.533816, loss=6.904535
I0607 02:59:47.569384 139931124995840 logging_writer.py:48] [67] global_step=67, grad_norm=0.541140, loss=6.907994
I0607 02:59:47.961552 139931133388544 logging_writer.py:48] [68] global_step=68, grad_norm=0.547158, loss=6.907339
I0607 02:59:48.348443 139931124995840 logging_writer.py:48] [69] global_step=69, grad_norm=0.520754, loss=6.911928
I0607 02:59:48.733221 139931133388544 logging_writer.py:48] [70] global_step=70, grad_norm=0.532923, loss=6.896657
I0607 02:59:49.127351 139931124995840 logging_writer.py:48] [71] global_step=71, grad_norm=0.536941, loss=6.908751
I0607 02:59:49.515781 139931133388544 logging_writer.py:48] [72] global_step=72, grad_norm=0.538910, loss=6.906020
I0607 02:59:49.906817 139931124995840 logging_writer.py:48] [73] global_step=73, grad_norm=0.551228, loss=6.898505
I0607 02:59:50.291869 139931133388544 logging_writer.py:48] [74] global_step=74, grad_norm=0.520586, loss=6.902323
I0607 02:59:50.683069 139931124995840 logging_writer.py:48] [75] global_step=75, grad_norm=0.524257, loss=6.898410
I0607 02:59:51.068097 139931133388544 logging_writer.py:48] [76] global_step=76, grad_norm=0.526530, loss=6.899248
I0607 02:59:51.455787 139931124995840 logging_writer.py:48] [77] global_step=77, grad_norm=0.532445, loss=6.904226
I0607 02:59:51.839388 139931133388544 logging_writer.py:48] [78] global_step=78, grad_norm=0.543947, loss=6.896207
I0607 02:59:52.223574 139931124995840 logging_writer.py:48] [79] global_step=79, grad_norm=0.553379, loss=6.902683
I0607 02:59:52.609570 139931133388544 logging_writer.py:48] [80] global_step=80, grad_norm=0.519967, loss=6.894798
I0607 02:59:52.994545 139931124995840 logging_writer.py:48] [81] global_step=81, grad_norm=0.523599, loss=6.902284
I0607 02:59:53.379682 139931133388544 logging_writer.py:48] [82] global_step=82, grad_norm=0.527072, loss=6.896334
I0607 02:59:53.765097 139931124995840 logging_writer.py:48] [83] global_step=83, grad_norm=0.540549, loss=6.894909
I0607 02:59:54.149836 139931133388544 logging_writer.py:48] [84] global_step=84, grad_norm=0.530353, loss=6.902101
I0607 02:59:54.533455 139931124995840 logging_writer.py:48] [85] global_step=85, grad_norm=0.523726, loss=6.896322
I0607 02:59:54.918439 139931133388544 logging_writer.py:48] [86] global_step=86, grad_norm=0.541686, loss=6.895506
I0607 02:59:55.301350 139931124995840 logging_writer.py:48] [87] global_step=87, grad_norm=0.534720, loss=6.892053
I0607 02:59:55.688045 139931133388544 logging_writer.py:48] [88] global_step=88, grad_norm=0.528409, loss=6.896352
I0607 02:59:56.071735 139931124995840 logging_writer.py:48] [89] global_step=89, grad_norm=0.526982, loss=6.885647
I0607 02:59:56.454192 139931133388544 logging_writer.py:48] [90] global_step=90, grad_norm=0.543476, loss=6.895802
I0607 02:59:56.839130 139931124995840 logging_writer.py:48] [91] global_step=91, grad_norm=0.519626, loss=6.889425
I0607 02:59:57.220893 139931133388544 logging_writer.py:48] [92] global_step=92, grad_norm=0.538993, loss=6.892065
I0607 02:59:57.604091 139931124995840 logging_writer.py:48] [93] global_step=93, grad_norm=0.529338, loss=6.886539
I0607 02:59:57.988259 139931133388544 logging_writer.py:48] [94] global_step=94, grad_norm=0.520483, loss=6.890242
I0607 02:59:58.370205 139931124995840 logging_writer.py:48] [95] global_step=95, grad_norm=0.538754, loss=6.886953
I0607 02:59:58.752580 139931133388544 logging_writer.py:48] [96] global_step=96, grad_norm=0.532982, loss=6.891144
I0607 02:59:59.136409 139931124995840 logging_writer.py:48] [97] global_step=97, grad_norm=0.535399, loss=6.882596
I0607 02:59:59.527441 139931133388544 logging_writer.py:48] [98] global_step=98, grad_norm=0.534217, loss=6.880594
I0607 02:59:59.914063 139931124995840 logging_writer.py:48] [99] global_step=99, grad_norm=0.537673, loss=6.886414
I0607 03:00:00.301324 139931133388544 logging_writer.py:48] [100] global_step=100, grad_norm=0.535227, loss=6.895647
I0607 03:02:30.385114 139931124995840 logging_writer.py:48] [500] global_step=500, grad_norm=0.637673, loss=6.564664
I0607 03:05:38.110347 139931133388544 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.815482, loss=6.296459
I0607 03:07:51.918315 139983768639296 spec.py:298] Evaluating on the training split.
I0607 03:08:32.928184 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 03:09:28.053780 139983768639296 spec.py:326] Evaluating on the test split.
I0607 03:09:29.467635 139983768639296 submission_runner.py:419] Time since start: 742.35s, 	Step: 1353, 	{'train/accuracy': 0.06154336734693878, 'train/loss': 5.607895831672513, 'validation/accuracy': 0.05562, 'validation/loss': 5.66878125, 'validation/num_examples': 50000, 'test/accuracy': 0.0364, 'test/loss': 5.89911640625, 'test/num_examples': 10000, 'score': 517.1224446296692, 'total_duration': 742.3458242416382, 'accumulated_submission_time': 517.1224446296692, 'accumulated_eval_time': 224.29913711547852, 'accumulated_logging_time': 0.026717185974121094}
I0607 03:09:29.477536 139931141781248 logging_writer.py:48] [1353] accumulated_eval_time=224.299137, accumulated_logging_time=0.026717, accumulated_submission_time=517.122445, global_step=1353, preemption_count=0, score=517.122445, test/accuracy=0.036400, test/loss=5.899116, test/num_examples=10000, total_duration=742.345824, train/accuracy=0.061543, train/loss=5.607896, validation/accuracy=0.055620, validation/loss=5.668781, validation/num_examples=50000
I0607 03:10:24.933427 139931150173952 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.830096, loss=6.038314
I0607 03:13:33.046333 139931141781248 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.828352, loss=5.783109
I0607 03:16:41.107130 139931150173952 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.766070, loss=5.525723
I0607 03:17:59.833065 139983768639296 spec.py:298] Evaluating on the training split.
I0607 03:18:45.359357 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 03:19:41.069071 139983768639296 spec.py:326] Evaluating on the test split.
I0607 03:19:42.452080 139983768639296 submission_runner.py:419] Time since start: 1355.33s, 	Step: 2707, 	{'train/accuracy': 0.15100845025510204, 'train/loss': 4.530307691924426, 'validation/accuracy': 0.13986, 'validation/loss': 4.61481125, 'validation/num_examples': 50000, 'test/accuracy': 0.0997, 'test/loss': 5.026015625, 'test/num_examples': 10000, 'score': 1026.5917909145355, 'total_duration': 1355.3283433914185, 'accumulated_submission_time': 1026.5917909145355, 'accumulated_eval_time': 326.91613602638245, 'accumulated_logging_time': 0.045171260833740234}
I0607 03:19:42.462008 139931141781248 logging_writer.py:48] [2707] accumulated_eval_time=326.916136, accumulated_logging_time=0.045171, accumulated_submission_time=1026.591791, global_step=2707, preemption_count=0, score=1026.591791, test/accuracy=0.099700, test/loss=5.026016, test/num_examples=10000, total_duration=1355.328343, train/accuracy=0.151008, train/loss=4.530308, validation/accuracy=0.139860, validation/loss=4.614811, validation/num_examples=50000
I0607 03:21:32.625131 139931150173952 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.845833, loss=5.235380
I0607 03:24:40.586099 139931141781248 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.770357, loss=4.959686
I0607 03:27:49.870174 139931150173952 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.803857, loss=4.899556
I0607 03:28:12.770097 139983768639296 spec.py:298] Evaluating on the training split.
I0607 03:28:53.895353 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 03:29:37.819727 139983768639296 spec.py:326] Evaluating on the test split.
I0607 03:29:39.187382 139983768639296 submission_runner.py:419] Time since start: 1952.07s, 	Step: 4062, 	{'train/accuracy': 0.28413982780612246, 'train/loss': 3.6960044393734055, 'validation/accuracy': 0.2597, 'validation/loss': 3.8145640625, 'validation/num_examples': 50000, 'test/accuracy': 0.1877, 'test/loss': 4.3584203125, 'test/num_examples': 10000, 'score': 1536.0045495033264, 'total_duration': 1952.0655615329742, 'accumulated_submission_time': 1536.0045495033264, 'accumulated_eval_time': 413.3336319923401, 'accumulated_logging_time': 0.06297969818115234}
I0607 03:29:39.198056 139931141781248 logging_writer.py:48] [4062] accumulated_eval_time=413.333632, accumulated_logging_time=0.062980, accumulated_submission_time=1536.004550, global_step=4062, preemption_count=0, score=1536.004550, test/accuracy=0.187700, test/loss=4.358420, test/num_examples=10000, total_duration=1952.065562, train/accuracy=0.284140, train/loss=3.696004, validation/accuracy=0.259700, validation/loss=3.814564, validation/num_examples=50000
I0607 03:32:24.000804 139931150173952 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.716614, loss=4.533895
I0607 03:35:31.825721 139931141781248 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.696465, loss=4.550197
I0607 03:38:09.349188 139983768639296 spec.py:298] Evaluating on the training split.
I0607 03:38:55.761183 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 03:39:51.196432 139983768639296 spec.py:326] Evaluating on the test split.
I0607 03:39:52.567662 139983768639296 submission_runner.py:419] Time since start: 2565.44s, 	Step: 5417, 	{'train/accuracy': 0.3733458227040816, 'train/loss': 3.1562039122289542, 'validation/accuracy': 0.34164, 'validation/loss': 3.3021540625, 'validation/num_examples': 50000, 'test/accuracy': 0.2541, 'test/loss': 3.86814765625, 'test/num_examples': 10000, 'score': 2045.2509562969208, 'total_duration': 2565.4446227550507, 'accumulated_submission_time': 2045.2509562969208, 'accumulated_eval_time': 516.5510604381561, 'accumulated_logging_time': 0.08271360397338867}
I0607 03:39:52.577769 139931150173952 logging_writer.py:48] [5417] accumulated_eval_time=516.551060, accumulated_logging_time=0.082714, accumulated_submission_time=2045.250956, global_step=5417, preemption_count=0, score=2045.250956, test/accuracy=0.254100, test/loss=3.868148, test/num_examples=10000, total_duration=2565.444623, train/accuracy=0.373346, train/loss=3.156204, validation/accuracy=0.341640, validation/loss=3.302154, validation/num_examples=50000
I0607 03:40:24.026978 139931141781248 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.696394, loss=4.525064
I0607 03:43:31.991206 139931150173952 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.647066, loss=4.282982
I0607 03:46:40.940341 139931141781248 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.627650, loss=4.214934
I0607 03:48:22.593377 139983768639296 spec.py:298] Evaluating on the training split.
I0607 03:49:03.755181 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 03:49:48.749652 139983768639296 spec.py:326] Evaluating on the test split.
I0607 03:49:50.115909 139983768639296 submission_runner.py:419] Time since start: 3162.99s, 	Step: 6772, 	{'train/accuracy': 0.42305086096938777, 'train/loss': 2.7844674246651784, 'validation/accuracy': 0.39184, 'validation/loss': 2.9518771875, 'validation/num_examples': 50000, 'test/accuracy': 0.3007, 'test/loss': 3.561369140625, 'test/num_examples': 10000, 'score': 2554.38538146019, 'total_duration': 3162.9940977096558, 'accumulated_submission_time': 2554.38538146019, 'accumulated_eval_time': 604.0737164020538, 'accumulated_logging_time': 0.10042500495910645}
I0607 03:49:50.126680 139931150173952 logging_writer.py:48] [6772] accumulated_eval_time=604.073716, accumulated_logging_time=0.100425, accumulated_submission_time=2554.385381, global_step=6772, preemption_count=0, score=2554.385381, test/accuracy=0.300700, test/loss=3.561369, test/num_examples=10000, total_duration=3162.994098, train/accuracy=0.423051, train/loss=2.784467, validation/accuracy=0.391840, validation/loss=2.951877, validation/num_examples=50000
I0607 03:51:15.885559 139931141781248 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.609848, loss=4.095473
I0607 03:54:23.632395 139931150173952 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.570976, loss=4.049727
I0607 03:57:32.555919 139931141781248 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.589017, loss=3.943946
I0607 03:58:20.263766 139983768639296 spec.py:298] Evaluating on the training split.
I0607 03:59:02.577387 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 03:59:53.309099 139983768639296 spec.py:326] Evaluating on the test split.
I0607 03:59:54.678532 139983768639296 submission_runner.py:419] Time since start: 3767.56s, 	Step: 8128, 	{'train/accuracy': 0.5085897640306123, 'train/loss': 2.3133222618881537, 'validation/accuracy': 0.46826, 'validation/loss': 2.49880484375, 'validation/num_examples': 50000, 'test/accuracy': 0.3519, 'test/loss': 3.1757255859375, 'test/num_examples': 10000, 'score': 3063.6410009860992, 'total_duration': 3767.556667804718, 'accumulated_submission_time': 3063.6410009860992, 'accumulated_eval_time': 698.488621711731, 'accumulated_logging_time': 0.11915874481201172}
I0607 03:59:54.688805 139931150173952 logging_writer.py:48] [8128] accumulated_eval_time=698.488622, accumulated_logging_time=0.119159, accumulated_submission_time=3063.641001, global_step=8128, preemption_count=0, score=3063.641001, test/accuracy=0.351900, test/loss=3.175726, test/num_examples=10000, total_duration=3767.556668, train/accuracy=0.508590, train/loss=2.313322, validation/accuracy=0.468260, validation/loss=2.498805, validation/num_examples=50000
I0607 04:02:14.690608 139931141781248 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.547026, loss=3.838526
I0607 04:05:23.654271 139931150173952 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.534161, loss=3.841095
I0607 04:08:24.814091 139983768639296 spec.py:298] Evaluating on the training split.
I0607 04:09:06.868101 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 04:09:50.582239 139983768639296 spec.py:326] Evaluating on the test split.
I0607 04:09:51.950598 139983768639296 submission_runner.py:419] Time since start: 4364.83s, 	Step: 9484, 	{'train/accuracy': 0.5639748086734694, 'train/loss': 2.1987413678850447, 'validation/accuracy': 0.52306, 'validation/loss': 2.392125, 'validation/num_examples': 50000, 'test/accuracy': 0.3956, 'test/loss': 3.071123046875, 'test/num_examples': 10000, 'score': 3572.8882081508636, 'total_duration': 4364.828609704971, 'accumulated_submission_time': 3572.8882081508636, 'accumulated_eval_time': 785.624933719635, 'accumulated_logging_time': 0.13807916641235352}
I0607 04:09:51.960256 139931141781248 logging_writer.py:48] [9484] accumulated_eval_time=785.624934, accumulated_logging_time=0.138079, accumulated_submission_time=3572.888208, global_step=9484, preemption_count=0, score=3572.888208, test/accuracy=0.395600, test/loss=3.071123, test/num_examples=10000, total_duration=4364.828610, train/accuracy=0.563975, train/loss=2.198741, validation/accuracy=0.523060, validation/loss=2.392125, validation/num_examples=50000
I0607 04:09:58.302628 139931150173952 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.518062, loss=3.809309
I0607 04:13:05.819343 139931141781248 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.502508, loss=3.749372
I0607 04:16:14.793041 139931150173952 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.505950, loss=3.731514
I0607 04:18:22.178916 139983768639296 spec.py:298] Evaluating on the training split.
I0607 04:19:04.989462 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 04:19:52.668936 139983768639296 spec.py:326] Evaluating on the test split.
I0607 04:19:54.045211 139983768639296 submission_runner.py:419] Time since start: 4966.92s, 	Step: 10840, 	{'train/accuracy': 0.5916772959183674, 'train/loss': 2.052193310795998, 'validation/accuracy': 0.54068, 'validation/loss': 2.27782515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4095, 'test/loss': 2.9621052734375, 'test/num_examples': 10000, 'score': 4082.230962753296, 'total_duration': 4966.923355340958, 'accumulated_submission_time': 4082.230962753296, 'accumulated_eval_time': 877.49152302742, 'accumulated_logging_time': 0.15554094314575195}
I0607 04:19:54.055065 139931141781248 logging_writer.py:48] [10840] accumulated_eval_time=877.491523, accumulated_logging_time=0.155541, accumulated_submission_time=4082.230963, global_step=10840, preemption_count=0, score=4082.230963, test/accuracy=0.409500, test/loss=2.962105, test/num_examples=10000, total_duration=4966.923355, train/accuracy=0.591677, train/loss=2.052193, validation/accuracy=0.540680, validation/loss=2.277825, validation/num_examples=50000
I0607 04:20:54.381656 139931150173952 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.480777, loss=3.573783
I0607 04:24:03.424100 139931141781248 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.469002, loss=3.543056
I0607 04:27:11.011616 139931150173952 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.473429, loss=3.494030
I0607 04:28:24.260044 139983768639296 spec.py:298] Evaluating on the training split.
I0607 04:29:07.123788 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 04:29:51.666028 139983768639296 spec.py:326] Evaluating on the test split.
I0607 04:29:53.040898 139983768639296 submission_runner.py:419] Time since start: 5565.92s, 	Step: 12196, 	{'train/accuracy': 0.6266741071428571, 'train/loss': 1.8797012640505422, 'validation/accuracy': 0.5711, 'validation/loss': 2.120411875, 'validation/num_examples': 50000, 'test/accuracy': 0.4381, 'test/loss': 2.7999578125, 'test/num_examples': 10000, 'score': 4591.551668643951, 'total_duration': 5565.9190356731415, 'accumulated_submission_time': 4591.551668643951, 'accumulated_eval_time': 966.272655248642, 'accumulated_logging_time': 0.17348241806030273}
I0607 04:29:53.050925 139931141781248 logging_writer.py:48] [12196] accumulated_eval_time=966.272655, accumulated_logging_time=0.173482, accumulated_submission_time=4591.551669, global_step=12196, preemption_count=0, score=4591.551669, test/accuracy=0.438100, test/loss=2.799958, test/num_examples=10000, total_duration=5565.919036, train/accuracy=0.626674, train/loss=1.879701, validation/accuracy=0.571100, validation/loss=2.120412, validation/num_examples=50000
I0607 04:31:47.237695 139931150173952 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.473918, loss=3.552075
I0607 04:34:56.108525 139931141781248 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.473079, loss=3.424741
I0607 04:38:03.869821 139931150173952 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.453559, loss=3.472649
I0607 04:38:23.048451 139983768639296 spec.py:298] Evaluating on the training split.
I0607 04:39:05.356668 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 04:40:01.534074 139983768639296 spec.py:326] Evaluating on the test split.
I0607 04:40:02.910142 139983768639296 submission_runner.py:419] Time since start: 6175.79s, 	Step: 13552, 	{'train/accuracy': 0.6526825573979592, 'train/loss': 1.6607212923011, 'validation/accuracy': 0.5914, 'validation/loss': 1.93071171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4586, 'test/loss': 2.62545390625, 'test/num_examples': 10000, 'score': 5100.670483112335, 'total_duration': 6175.786456346512, 'accumulated_submission_time': 5100.670483112335, 'accumulated_eval_time': 1066.1328530311584, 'accumulated_logging_time': 0.19145917892456055}
I0607 04:40:02.921921 139931141781248 logging_writer.py:48] [13552] accumulated_eval_time=1066.132853, accumulated_logging_time=0.191459, accumulated_submission_time=5100.670483, global_step=13552, preemption_count=0, score=5100.670483, test/accuracy=0.458600, test/loss=2.625454, test/num_examples=10000, total_duration=6175.786456, train/accuracy=0.652683, train/loss=1.660721, validation/accuracy=0.591400, validation/loss=1.930712, validation/num_examples=50000
I0607 04:42:52.373180 139931150173952 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.460199, loss=3.459930
I0607 04:45:59.926677 139931141781248 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.451589, loss=3.459591
I0607 04:48:33.101604 139983768639296 spec.py:298] Evaluating on the training split.
I0607 04:49:17.102149 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 04:50:03.211758 139983768639296 spec.py:326] Evaluating on the test split.
I0607 04:50:04.589913 139983768639296 submission_runner.py:419] Time since start: 6777.47s, 	Step: 14909, 	{'train/accuracy': 0.6601961096938775, 'train/loss': 1.5668942198461415, 'validation/accuracy': 0.59622, 'validation/loss': 1.85217703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4777, 'test/loss': 2.4988982421875, 'test/num_examples': 10000, 'score': 5609.9683401584625, 'total_duration': 6777.468122959137, 'accumulated_submission_time': 5609.9683401584625, 'accumulated_eval_time': 1157.6213338375092, 'accumulated_logging_time': 0.2114417552947998}
I0607 04:50:04.601246 139931150173952 logging_writer.py:48] [14909] accumulated_eval_time=1157.621334, accumulated_logging_time=0.211442, accumulated_submission_time=5609.968340, global_step=14909, preemption_count=0, score=5609.968340, test/accuracy=0.477700, test/loss=2.498898, test/num_examples=10000, total_duration=6777.468123, train/accuracy=0.660196, train/loss=1.566894, validation/accuracy=0.596220, validation/loss=1.852177, validation/num_examples=50000
I0607 04:50:38.944079 139931141781248 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.456489, loss=3.342090
I0607 04:53:47.831501 139931150173952 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.442534, loss=3.353041
I0607 04:56:55.491943 139931141781248 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.444896, loss=3.331144
I0607 04:58:35.460093 139983768639296 spec.py:298] Evaluating on the training split.
I0607 04:59:18.116392 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 05:00:13.081016 139983768639296 spec.py:326] Evaluating on the test split.
I0607 05:00:14.453648 139983768639296 submission_runner.py:419] Time since start: 7387.33s, 	Step: 16264, 	{'train/accuracy': 0.6931999362244898, 'train/loss': 1.5175946294044962, 'validation/accuracy': 0.62648, 'validation/loss': 1.807746875, 'validation/num_examples': 50000, 'test/accuracy': 0.487, 'test/loss': 2.52806328125, 'test/num_examples': 10000, 'score': 6119.959393262863, 'total_duration': 7387.330399274826, 'accumulated_submission_time': 6119.959393262863, 'accumulated_eval_time': 1256.6136527061462, 'accumulated_logging_time': 0.23137164115905762}
I0607 05:00:14.463780 139931150173952 logging_writer.py:48] [16264] accumulated_eval_time=1256.613653, accumulated_logging_time=0.231372, accumulated_submission_time=6119.959393, global_step=16264, preemption_count=0, score=6119.959393, test/accuracy=0.487000, test/loss=2.528063, test/num_examples=10000, total_duration=7387.330399, train/accuracy=0.693200, train/loss=1.517595, validation/accuracy=0.626480, validation/loss=1.807747, validation/num_examples=50000
I0607 05:01:43.125496 139931141781248 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.474894, loss=3.337003
I0607 05:04:50.715919 139931150173952 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.458377, loss=3.331383
I0607 05:07:58.291405 139931141781248 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.438313, loss=3.228986
I0607 05:08:44.717461 139983768639296 spec.py:298] Evaluating on the training split.
I0607 05:09:27.161963 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 05:10:18.546835 139983768639296 spec.py:326] Evaluating on the test split.
I0607 05:10:19.911860 139983768639296 submission_runner.py:419] Time since start: 7992.79s, 	Step: 17621, 	{'train/accuracy': 0.7013313137755102, 'train/loss': 1.4414884606186225, 'validation/accuracy': 0.62664, 'validation/loss': 1.761160625, 'validation/num_examples': 50000, 'test/accuracy': 0.4973, 'test/loss': 2.45721328125, 'test/num_examples': 10000, 'score': 6629.343208312988, 'total_duration': 7992.789987325668, 'accumulated_submission_time': 6629.343208312988, 'accumulated_eval_time': 1351.8082029819489, 'accumulated_logging_time': 0.24940180778503418}
I0607 05:10:19.923616 139931150173952 logging_writer.py:48] [17621] accumulated_eval_time=1351.808203, accumulated_logging_time=0.249402, accumulated_submission_time=6629.343208, global_step=17621, preemption_count=0, score=6629.343208, test/accuracy=0.497300, test/loss=2.457213, test/num_examples=10000, total_duration=7992.789987, train/accuracy=0.701331, train/loss=1.441488, validation/accuracy=0.626640, validation/loss=1.761161, validation/num_examples=50000
I0607 05:12:42.257972 139931141781248 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.450970, loss=3.293206
I0607 05:15:50.013141 139931150173952 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.442494, loss=3.277705
I0607 05:18:50.190348 139983768639296 spec.py:298] Evaluating on the training split.
I0607 05:19:33.404452 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 05:20:21.890418 139983768639296 spec.py:326] Evaluating on the test split.
I0607 05:20:23.256289 139983768639296 submission_runner.py:419] Time since start: 8596.13s, 	Step: 18978, 	{'train/accuracy': 0.7163783482142857, 'train/loss': 1.4009748186383928, 'validation/accuracy': 0.64106, 'validation/loss': 1.734455625, 'validation/num_examples': 50000, 'test/accuracy': 0.5058, 'test/loss': 2.4160830078125, 'test/num_examples': 10000, 'score': 7138.725232601166, 'total_duration': 8596.134463787079, 'accumulated_submission_time': 7138.725232601166, 'accumulated_eval_time': 1444.8742496967316, 'accumulated_logging_time': 0.269991397857666}
I0607 05:20:23.267336 139931141781248 logging_writer.py:48] [18978] accumulated_eval_time=1444.874250, accumulated_logging_time=0.269991, accumulated_submission_time=7138.725233, global_step=18978, preemption_count=0, score=7138.725233, test/accuracy=0.505800, test/loss=2.416083, test/num_examples=10000, total_duration=8596.134464, train/accuracy=0.716378, train/loss=1.400975, validation/accuracy=0.641060, validation/loss=1.734456, validation/num_examples=50000
I0607 05:20:31.904742 139931150173952 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.439371, loss=3.221130
I0607 05:23:39.326275 139931141781248 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.457141, loss=3.220463
I0607 05:26:46.817512 139931150173952 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.432485, loss=3.190365
I0607 05:28:53.486960 139983768639296 spec.py:298] Evaluating on the training split.
I0607 05:29:39.509710 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 05:30:33.705214 139983768639296 spec.py:326] Evaluating on the test split.
I0607 05:30:35.075054 139983768639296 submission_runner.py:419] Time since start: 9207.95s, 	Step: 20335, 	{'train/accuracy': 0.7352718431122449, 'train/loss': 1.3410509070571588, 'validation/accuracy': 0.6572, 'validation/loss': 1.68540546875, 'validation/num_examples': 50000, 'test/accuracy': 0.5224, 'test/loss': 2.3626666015625, 'test/num_examples': 10000, 'score': 7648.061871767044, 'total_duration': 9207.953222751617, 'accumulated_submission_time': 7648.061871767044, 'accumulated_eval_time': 1546.4624552726746, 'accumulated_logging_time': 0.2904641628265381}
I0607 05:30:35.087601 139931141781248 logging_writer.py:48] [20335] accumulated_eval_time=1546.462455, accumulated_logging_time=0.290464, accumulated_submission_time=7648.061872, global_step=20335, preemption_count=0, score=7648.061872, test/accuracy=0.522400, test/loss=2.362667, test/num_examples=10000, total_duration=9207.953223, train/accuracy=0.735272, train/loss=1.341051, validation/accuracy=0.657200, validation/loss=1.685405, validation/num_examples=50000
I0607 05:31:37.179160 139931150173952 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.448212, loss=3.229076
I0607 05:34:44.838951 139931141781248 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.437307, loss=3.178472
I0607 05:37:53.619620 139931150173952 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.436475, loss=3.213639
I0607 05:39:05.228846 139983768639296 spec.py:298] Evaluating on the training split.
I0607 05:39:51.119302 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 05:40:42.659586 139983768639296 spec.py:326] Evaluating on the test split.
I0607 05:40:44.032584 139983768639296 submission_runner.py:419] Time since start: 9816.91s, 	Step: 21692, 	{'train/accuracy': 0.7335180165816326, 'train/loss': 1.2859507969447546, 'validation/accuracy': 0.64984, 'validation/loss': 1.64597609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5107, 'test/loss': 2.359495703125, 'test/num_examples': 10000, 'score': 8157.318119525909, 'total_duration': 9816.910727262497, 'accumulated_submission_time': 8157.318119525909, 'accumulated_eval_time': 1645.266283750534, 'accumulated_logging_time': 0.31096696853637695}
I0607 05:40:44.045923 139931141781248 logging_writer.py:48] [21692] accumulated_eval_time=1645.266284, accumulated_logging_time=0.310967, accumulated_submission_time=8157.318120, global_step=21692, preemption_count=0, score=8157.318120, test/accuracy=0.510700, test/loss=2.359496, test/num_examples=10000, total_duration=9816.910727, train/accuracy=0.733518, train/loss=1.285951, validation/accuracy=0.649840, validation/loss=1.645976, validation/num_examples=50000
I0607 05:42:39.767019 139931150173952 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.440208, loss=3.272510
I0607 05:45:47.292468 139931141781248 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.440276, loss=3.235517
I0607 05:48:56.095814 139931150173952 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.437167, loss=3.198083
I0607 05:49:14.114523 139983768639296 spec.py:298] Evaluating on the training split.
I0607 05:49:57.412471 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 05:50:50.527374 139983768639296 spec.py:326] Evaluating on the test split.
I0607 05:50:51.900393 139983768639296 submission_runner.py:419] Time since start: 10424.78s, 	Step: 23049, 	{'train/accuracy': 0.7372249681122449, 'train/loss': 1.2676460110411352, 'validation/accuracy': 0.6529, 'validation/loss': 1.6320271875, 'validation/num_examples': 50000, 'test/accuracy': 0.5182, 'test/loss': 2.338583984375, 'test/num_examples': 10000, 'score': 8666.509688854218, 'total_duration': 10424.778520345688, 'accumulated_submission_time': 8666.509688854218, 'accumulated_eval_time': 1743.0523545742035, 'accumulated_logging_time': 0.3319838047027588}
I0607 05:50:51.912008 139931141781248 logging_writer.py:48] [23049] accumulated_eval_time=1743.052355, accumulated_logging_time=0.331984, accumulated_submission_time=8666.509689, global_step=23049, preemption_count=0, score=8666.509689, test/accuracy=0.518200, test/loss=2.338584, test/num_examples=10000, total_duration=10424.778520, train/accuracy=0.737225, train/loss=1.267646, validation/accuracy=0.652900, validation/loss=1.632027, validation/num_examples=50000
I0607 05:53:41.460409 139931150173952 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.444509, loss=3.168830
I0607 05:56:50.225056 139931141781248 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.438329, loss=3.189052
I0607 05:59:22.110657 139983768639296 spec.py:298] Evaluating on the training split.
I0607 06:00:06.596711 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 06:00:56.216170 139983768639296 spec.py:326] Evaluating on the test split.
I0607 06:00:57.584092 139983768639296 submission_runner.py:419] Time since start: 11030.46s, 	Step: 24406, 	{'train/accuracy': 0.7430245535714286, 'train/loss': 1.2559861942213408, 'validation/accuracy': 0.6535, 'validation/loss': 1.63998234375, 'validation/num_examples': 50000, 'test/accuracy': 0.521, 'test/loss': 2.3131849609375, 'test/num_examples': 10000, 'score': 9175.820007562637, 'total_duration': 11030.462231397629, 'accumulated_submission_time': 9175.820007562637, 'accumulated_eval_time': 1838.5257749557495, 'accumulated_logging_time': 0.3519580364227295}
I0607 06:00:57.595283 139931150173952 logging_writer.py:48] [24406] accumulated_eval_time=1838.525775, accumulated_logging_time=0.351958, accumulated_submission_time=9175.820008, global_step=24406, preemption_count=0, score=9175.820008, test/accuracy=0.521000, test/loss=2.313185, test/num_examples=10000, total_duration=11030.462231, train/accuracy=0.743025, train/loss=1.255986, validation/accuracy=0.653500, validation/loss=1.639982, validation/num_examples=50000
I0607 06:01:33.161452 139931141781248 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.438717, loss=3.121362
I0607 06:04:40.641767 139931150173952 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.450375, loss=3.207026
I0607 06:07:49.561992 139931141781248 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.431294, loss=3.110741
I0607 06:09:27.875860 139983768639296 spec.py:298] Evaluating on the training split.
I0607 06:10:11.386723 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 06:11:00.006537 139983768639296 spec.py:326] Evaluating on the test split.
I0607 06:11:01.369491 139983768639296 submission_runner.py:419] Time since start: 11634.25s, 	Step: 25763, 	{'train/accuracy': 0.7580516581632653, 'train/loss': 1.1998385993801817, 'validation/accuracy': 0.66516, 'validation/loss': 1.6052321875, 'validation/num_examples': 50000, 'test/accuracy': 0.5325, 'test/loss': 2.2804501953125, 'test/num_examples': 10000, 'score': 9685.218310117722, 'total_duration': 11634.24762558937, 'accumulated_submission_time': 9685.218310117722, 'accumulated_eval_time': 1932.019739151001, 'accumulated_logging_time': 0.3718223571777344}
I0607 06:11:01.380641 139931150173952 logging_writer.py:48] [25763] accumulated_eval_time=1932.019739, accumulated_logging_time=0.371822, accumulated_submission_time=9685.218310, global_step=25763, preemption_count=0, score=9685.218310, test/accuracy=0.532500, test/loss=2.280450, test/num_examples=10000, total_duration=11634.247626, train/accuracy=0.758052, train/loss=1.199839, validation/accuracy=0.665160, validation/loss=1.605232, validation/num_examples=50000
I0607 06:12:30.551953 139931141781248 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.424502, loss=3.096416
I0607 06:15:39.140015 139931150173952 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.455031, loss=3.128570
I0607 06:18:46.532602 139931141781248 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.443475, loss=3.131972
I0607 06:19:31.585852 139983768639296 spec.py:298] Evaluating on the training split.
I0607 06:20:15.633746 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 06:21:05.974482 139983768639296 spec.py:326] Evaluating on the test split.
I0607 06:21:07.349680 139983768639296 submission_runner.py:419] Time since start: 12240.23s, 	Step: 27121, 	{'train/accuracy': 0.7628746811224489, 'train/loss': 1.1844555601781728, 'validation/accuracy': 0.6689, 'validation/loss': 1.5885790625, 'validation/num_examples': 50000, 'test/accuracy': 0.5392, 'test/loss': 2.281711328125, 'test/num_examples': 10000, 'score': 10194.534824609756, 'total_duration': 12240.22786974907, 'accumulated_submission_time': 10194.534824609756, 'accumulated_eval_time': 2027.783620595932, 'accumulated_logging_time': 0.3907926082611084}
I0607 06:21:07.361479 139931150173952 logging_writer.py:48] [27121] accumulated_eval_time=2027.783621, accumulated_logging_time=0.390793, accumulated_submission_time=10194.534825, global_step=27121, preemption_count=0, score=10194.534825, test/accuracy=0.539200, test/loss=2.281711, test/num_examples=10000, total_duration=12240.227870, train/accuracy=0.762875, train/loss=1.184456, validation/accuracy=0.668900, validation/loss=1.588579, validation/num_examples=50000
I0607 06:23:29.514921 139931141781248 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.441701, loss=3.079334
I0607 06:26:37.998432 139983768639296 spec.py:298] Evaluating on the training split.
I0607 06:27:20.730730 139983768639296 spec.py:310] Evaluating on the validation split.
I0607 06:28:06.385947 139983768639296 spec.py:326] Evaluating on the test split.
I0607 06:28:07.762357 139983768639296 submission_runner.py:419] Time since start: 12660.64s, 	Step: 28000, 	{'train/accuracy': 0.7689333545918368, 'train/loss': 1.141831144994619, 'validation/accuracy': 0.67332, 'validation/loss': 1.5667384375, 'validation/num_examples': 50000, 'test/accuracy': 0.5268, 'test/loss': 2.2907685546875, 'test/num_examples': 10000, 'score': 10524.603609085083, 'total_duration': 12660.640531301498, 'accumulated_submission_time': 10524.603609085083, 'accumulated_eval_time': 2117.547532081604, 'accumulated_logging_time': 0.41034531593322754}
I0607 06:28:07.772792 139931150173952 logging_writer.py:48] [28000] accumulated_eval_time=2117.547532, accumulated_logging_time=0.410345, accumulated_submission_time=10524.603609, global_step=28000, preemption_count=0, score=10524.603609, test/accuracy=0.526800, test/loss=2.290769, test/num_examples=10000, total_duration=12660.640531, train/accuracy=0.768933, train/loss=1.141831, validation/accuracy=0.673320, validation/loss=1.566738, validation/num_examples=50000
I0607 06:28:07.790258 139931141781248 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10524.603609
I0607 06:28:08.371710 139983768639296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/momentum/imagenet_resnet_pytorch/trial_1/checkpoint_28000.
I0607 06:28:08.631188 139983768639296 submission_runner.py:581] Tuning trial 1/1
I0607 06:28:08.631401 139983768639296 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 06:28:08.632293 139983768639296 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012157206632653062, 'train/loss': 6.9238181600765305, 'validation/accuracy': 0.00092, 'validation/loss': 6.923669375, 'validation/num_examples': 50000, 'test/accuracy': 0.0014, 'test/loss': 6.9268125, 'test/num_examples': 10000, 'score': 7.872222661972046, 'total_duration': 134.62248921394348, 'accumulated_submission_time': 7.872222661972046, 'accumulated_eval_time': 126.74984335899353, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1353, {'train/accuracy': 0.06154336734693878, 'train/loss': 5.607895831672513, 'validation/accuracy': 0.05562, 'validation/loss': 5.66878125, 'validation/num_examples': 50000, 'test/accuracy': 0.0364, 'test/loss': 5.89911640625, 'test/num_examples': 10000, 'score': 517.1224446296692, 'total_duration': 742.3458242416382, 'accumulated_submission_time': 517.1224446296692, 'accumulated_eval_time': 224.29913711547852, 'accumulated_logging_time': 0.026717185974121094, 'global_step': 1353, 'preemption_count': 0}), (2707, {'train/accuracy': 0.15100845025510204, 'train/loss': 4.530307691924426, 'validation/accuracy': 0.13986, 'validation/loss': 4.61481125, 'validation/num_examples': 50000, 'test/accuracy': 0.0997, 'test/loss': 5.026015625, 'test/num_examples': 10000, 'score': 1026.5917909145355, 'total_duration': 1355.3283433914185, 'accumulated_submission_time': 1026.5917909145355, 'accumulated_eval_time': 326.91613602638245, 'accumulated_logging_time': 0.045171260833740234, 'global_step': 2707, 'preemption_count': 0}), (4062, {'train/accuracy': 0.28413982780612246, 'train/loss': 3.6960044393734055, 'validation/accuracy': 0.2597, 'validation/loss': 3.8145640625, 'validation/num_examples': 50000, 'test/accuracy': 0.1877, 'test/loss': 4.3584203125, 'test/num_examples': 10000, 'score': 1536.0045495033264, 'total_duration': 1952.0655615329742, 'accumulated_submission_time': 1536.0045495033264, 'accumulated_eval_time': 413.3336319923401, 'accumulated_logging_time': 0.06297969818115234, 'global_step': 4062, 'preemption_count': 0}), (5417, {'train/accuracy': 0.3733458227040816, 'train/loss': 3.1562039122289542, 'validation/accuracy': 0.34164, 'validation/loss': 3.3021540625, 'validation/num_examples': 50000, 'test/accuracy': 0.2541, 'test/loss': 3.86814765625, 'test/num_examples': 10000, 'score': 2045.2509562969208, 'total_duration': 2565.4446227550507, 'accumulated_submission_time': 2045.2509562969208, 'accumulated_eval_time': 516.5510604381561, 'accumulated_logging_time': 0.08271360397338867, 'global_step': 5417, 'preemption_count': 0}), (6772, {'train/accuracy': 0.42305086096938777, 'train/loss': 2.7844674246651784, 'validation/accuracy': 0.39184, 'validation/loss': 2.9518771875, 'validation/num_examples': 50000, 'test/accuracy': 0.3007, 'test/loss': 3.561369140625, 'test/num_examples': 10000, 'score': 2554.38538146019, 'total_duration': 3162.9940977096558, 'accumulated_submission_time': 2554.38538146019, 'accumulated_eval_time': 604.0737164020538, 'accumulated_logging_time': 0.10042500495910645, 'global_step': 6772, 'preemption_count': 0}), (8128, {'train/accuracy': 0.5085897640306123, 'train/loss': 2.3133222618881537, 'validation/accuracy': 0.46826, 'validation/loss': 2.49880484375, 'validation/num_examples': 50000, 'test/accuracy': 0.3519, 'test/loss': 3.1757255859375, 'test/num_examples': 10000, 'score': 3063.6410009860992, 'total_duration': 3767.556667804718, 'accumulated_submission_time': 3063.6410009860992, 'accumulated_eval_time': 698.488621711731, 'accumulated_logging_time': 0.11915874481201172, 'global_step': 8128, 'preemption_count': 0}), (9484, {'train/accuracy': 0.5639748086734694, 'train/loss': 2.1987413678850447, 'validation/accuracy': 0.52306, 'validation/loss': 2.392125, 'validation/num_examples': 50000, 'test/accuracy': 0.3956, 'test/loss': 3.071123046875, 'test/num_examples': 10000, 'score': 3572.8882081508636, 'total_duration': 4364.828609704971, 'accumulated_submission_time': 3572.8882081508636, 'accumulated_eval_time': 785.624933719635, 'accumulated_logging_time': 0.13807916641235352, 'global_step': 9484, 'preemption_count': 0}), (10840, {'train/accuracy': 0.5916772959183674, 'train/loss': 2.052193310795998, 'validation/accuracy': 0.54068, 'validation/loss': 2.27782515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4095, 'test/loss': 2.9621052734375, 'test/num_examples': 10000, 'score': 4082.230962753296, 'total_duration': 4966.923355340958, 'accumulated_submission_time': 4082.230962753296, 'accumulated_eval_time': 877.49152302742, 'accumulated_logging_time': 0.15554094314575195, 'global_step': 10840, 'preemption_count': 0}), (12196, {'train/accuracy': 0.6266741071428571, 'train/loss': 1.8797012640505422, 'validation/accuracy': 0.5711, 'validation/loss': 2.120411875, 'validation/num_examples': 50000, 'test/accuracy': 0.4381, 'test/loss': 2.7999578125, 'test/num_examples': 10000, 'score': 4591.551668643951, 'total_duration': 5565.9190356731415, 'accumulated_submission_time': 4591.551668643951, 'accumulated_eval_time': 966.272655248642, 'accumulated_logging_time': 0.17348241806030273, 'global_step': 12196, 'preemption_count': 0}), (13552, {'train/accuracy': 0.6526825573979592, 'train/loss': 1.6607212923011, 'validation/accuracy': 0.5914, 'validation/loss': 1.93071171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4586, 'test/loss': 2.62545390625, 'test/num_examples': 10000, 'score': 5100.670483112335, 'total_duration': 6175.786456346512, 'accumulated_submission_time': 5100.670483112335, 'accumulated_eval_time': 1066.1328530311584, 'accumulated_logging_time': 0.19145917892456055, 'global_step': 13552, 'preemption_count': 0}), (14909, {'train/accuracy': 0.6601961096938775, 'train/loss': 1.5668942198461415, 'validation/accuracy': 0.59622, 'validation/loss': 1.85217703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4777, 'test/loss': 2.4988982421875, 'test/num_examples': 10000, 'score': 5609.9683401584625, 'total_duration': 6777.468122959137, 'accumulated_submission_time': 5609.9683401584625, 'accumulated_eval_time': 1157.6213338375092, 'accumulated_logging_time': 0.2114417552947998, 'global_step': 14909, 'preemption_count': 0}), (16264, {'train/accuracy': 0.6931999362244898, 'train/loss': 1.5175946294044962, 'validation/accuracy': 0.62648, 'validation/loss': 1.807746875, 'validation/num_examples': 50000, 'test/accuracy': 0.487, 'test/loss': 2.52806328125, 'test/num_examples': 10000, 'score': 6119.959393262863, 'total_duration': 7387.330399274826, 'accumulated_submission_time': 6119.959393262863, 'accumulated_eval_time': 1256.6136527061462, 'accumulated_logging_time': 0.23137164115905762, 'global_step': 16264, 'preemption_count': 0}), (17621, {'train/accuracy': 0.7013313137755102, 'train/loss': 1.4414884606186225, 'validation/accuracy': 0.62664, 'validation/loss': 1.761160625, 'validation/num_examples': 50000, 'test/accuracy': 0.4973, 'test/loss': 2.45721328125, 'test/num_examples': 10000, 'score': 6629.343208312988, 'total_duration': 7992.789987325668, 'accumulated_submission_time': 6629.343208312988, 'accumulated_eval_time': 1351.8082029819489, 'accumulated_logging_time': 0.24940180778503418, 'global_step': 17621, 'preemption_count': 0}), (18978, {'train/accuracy': 0.7163783482142857, 'train/loss': 1.4009748186383928, 'validation/accuracy': 0.64106, 'validation/loss': 1.734455625, 'validation/num_examples': 50000, 'test/accuracy': 0.5058, 'test/loss': 2.4160830078125, 'test/num_examples': 10000, 'score': 7138.725232601166, 'total_duration': 8596.134463787079, 'accumulated_submission_time': 7138.725232601166, 'accumulated_eval_time': 1444.8742496967316, 'accumulated_logging_time': 0.269991397857666, 'global_step': 18978, 'preemption_count': 0}), (20335, {'train/accuracy': 0.7352718431122449, 'train/loss': 1.3410509070571588, 'validation/accuracy': 0.6572, 'validation/loss': 1.68540546875, 'validation/num_examples': 50000, 'test/accuracy': 0.5224, 'test/loss': 2.3626666015625, 'test/num_examples': 10000, 'score': 7648.061871767044, 'total_duration': 9207.953222751617, 'accumulated_submission_time': 7648.061871767044, 'accumulated_eval_time': 1546.4624552726746, 'accumulated_logging_time': 0.2904641628265381, 'global_step': 20335, 'preemption_count': 0}), (21692, {'train/accuracy': 0.7335180165816326, 'train/loss': 1.2859507969447546, 'validation/accuracy': 0.64984, 'validation/loss': 1.64597609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5107, 'test/loss': 2.359495703125, 'test/num_examples': 10000, 'score': 8157.318119525909, 'total_duration': 9816.910727262497, 'accumulated_submission_time': 8157.318119525909, 'accumulated_eval_time': 1645.266283750534, 'accumulated_logging_time': 0.31096696853637695, 'global_step': 21692, 'preemption_count': 0}), (23049, {'train/accuracy': 0.7372249681122449, 'train/loss': 1.2676460110411352, 'validation/accuracy': 0.6529, 'validation/loss': 1.6320271875, 'validation/num_examples': 50000, 'test/accuracy': 0.5182, 'test/loss': 2.338583984375, 'test/num_examples': 10000, 'score': 8666.509688854218, 'total_duration': 10424.778520345688, 'accumulated_submission_time': 8666.509688854218, 'accumulated_eval_time': 1743.0523545742035, 'accumulated_logging_time': 0.3319838047027588, 'global_step': 23049, 'preemption_count': 0}), (24406, {'train/accuracy': 0.7430245535714286, 'train/loss': 1.2559861942213408, 'validation/accuracy': 0.6535, 'validation/loss': 1.63998234375, 'validation/num_examples': 50000, 'test/accuracy': 0.521, 'test/loss': 2.3131849609375, 'test/num_examples': 10000, 'score': 9175.820007562637, 'total_duration': 11030.462231397629, 'accumulated_submission_time': 9175.820007562637, 'accumulated_eval_time': 1838.5257749557495, 'accumulated_logging_time': 0.3519580364227295, 'global_step': 24406, 'preemption_count': 0}), (25763, {'train/accuracy': 0.7580516581632653, 'train/loss': 1.1998385993801817, 'validation/accuracy': 0.66516, 'validation/loss': 1.6052321875, 'validation/num_examples': 50000, 'test/accuracy': 0.5325, 'test/loss': 2.2804501953125, 'test/num_examples': 10000, 'score': 9685.218310117722, 'total_duration': 11634.24762558937, 'accumulated_submission_time': 9685.218310117722, 'accumulated_eval_time': 1932.019739151001, 'accumulated_logging_time': 0.3718223571777344, 'global_step': 25763, 'preemption_count': 0}), (27121, {'train/accuracy': 0.7628746811224489, 'train/loss': 1.1844555601781728, 'validation/accuracy': 0.6689, 'validation/loss': 1.5885790625, 'validation/num_examples': 50000, 'test/accuracy': 0.5392, 'test/loss': 2.281711328125, 'test/num_examples': 10000, 'score': 10194.534824609756, 'total_duration': 12240.22786974907, 'accumulated_submission_time': 10194.534824609756, 'accumulated_eval_time': 2027.783620595932, 'accumulated_logging_time': 0.3907926082611084, 'global_step': 27121, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7689333545918368, 'train/loss': 1.141831144994619, 'validation/accuracy': 0.67332, 'validation/loss': 1.5667384375, 'validation/num_examples': 50000, 'test/accuracy': 0.5268, 'test/loss': 2.2907685546875, 'test/num_examples': 10000, 'score': 10524.603609085083, 'total_duration': 12660.640531301498, 'accumulated_submission_time': 10524.603609085083, 'accumulated_eval_time': 2117.547532081604, 'accumulated_logging_time': 0.41034531593322754, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0607 06:28:08.632418 139983768639296 submission_runner.py:584] Timing: 10524.603609085083
I0607 06:28:08.632467 139983768639296 submission_runner.py:586] Total number of evals: 22
I0607 06:28:08.632528 139983768639296 submission_runner.py:587] ====================
I0607 06:28:08.632645 139983768639296 submission_runner.py:655] Final imagenet_resnet score: 10524.603609085083
