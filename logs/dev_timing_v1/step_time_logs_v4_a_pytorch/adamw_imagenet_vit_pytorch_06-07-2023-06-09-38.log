torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/adamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-07-2023-06-09-38.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 06:10:01.594621 139685970736960 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 06:10:01.594677 139876281538368 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 06:10:01.595821 140582058805056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 06:10:01.595998 139696767235904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 06:10:01.595927 139753860376384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 06:10:01.595995 139647831844672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 06:10:01.596078 140677860706112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 06:10:01.605975 140355837732672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 06:10:01.606395 140355837732672 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.606491 140582058805056 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.606663 139647831844672 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.606619 139696767235904 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.606654 139753860376384 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.606728 140677860706112 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.615675 139685970736960 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:01.615724 139876281538368 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 06:10:04.011461 140355837732672 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch because --overwrite was set.
I0607 06:10:04.013541 140355837732672 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch.
W0607 06:10:04.053735 139685970736960 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.054898 140582058805056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.055896 139753860376384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.055938 139876281538368 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.055938 140355837732672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.056125 140677860706112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.056417 139696767235904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 06:10:04.056915 139647831844672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 06:10:04.060917 140355837732672 submission_runner.py:541] Using RNG seed 1399184787
I0607 06:10:04.062465 140355837732672 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 06:10:04.062611 140355837732672 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch/trial_1.
I0607 06:10:04.062933 140355837732672 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0607 06:10:04.064506 140355837732672 submission_runner.py:255] Initializing dataset.
I0607 06:10:15.665622 140355837732672 submission_runner.py:262] Initializing model.
I0607 06:10:20.022615 140355837732672 submission_runner.py:272] Initializing optimizer.
I0607 06:10:20.024135 140355837732672 submission_runner.py:279] Initializing metrics bundle.
I0607 06:10:20.024269 140355837732672 submission_runner.py:297] Initializing checkpoint and logger.
I0607 06:10:20.628951 140355837732672 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0607 06:10:20.629761 140355837732672 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0607 06:10:20.683033 140355837732672 submission_runner.py:332] Starting training loop.
I0607 06:10:27.320126 140326756927232 logging_writer.py:48] [0] global_step=0, grad_norm=0.329553, loss=6.907756
I0607 06:10:27.339536 140355837732672 submission.py:120] 0) loss = 6.908, grad_norm = 0.330
I0607 06:10:27.340885 140355837732672 spec.py:298] Evaluating on the training split.
I0607 06:11:26.951858 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 06:12:21.388894 140355837732672 spec.py:326] Evaluating on the test split.
I0607 06:12:21.408451 140355837732672 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 06:12:21.415149 140355837732672 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0607 06:12:21.492913 140355837732672 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 06:12:33.372854 140355837732672 submission_runner.py:419] Time since start: 132.69s, 	Step: 1, 	{'train/accuracy': 0.0019921875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00224, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0027, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.658056020736694, 'total_duration': 132.6903772354126, 'accumulated_submission_time': 6.658056020736694, 'accumulated_eval_time': 126.03185486793518, 'accumulated_logging_time': 0}
I0607 06:12:33.393288 140321715386112 logging_writer.py:48] [1] accumulated_eval_time=126.031855, accumulated_logging_time=0, accumulated_submission_time=6.658056, global_step=1, preemption_count=0, score=6.658056, test/accuracy=0.002700, test/loss=6.907755, test/num_examples=10000, total_duration=132.690377, train/accuracy=0.001992, train/loss=6.907756, validation/accuracy=0.002240, validation/loss=6.907756, validation/num_examples=50000
I0607 06:12:33.412663 140355837732672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.412708 139876281538368 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.412741 140582058805056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.412764 139753860376384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.412775 139685970736960 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.412789 139647831844672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.413233 140677860706112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.413487 139696767235904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 06:12:33.996634 140321706993408 logging_writer.py:48] [1] global_step=1, grad_norm=0.335503, loss=6.907756
I0607 06:12:34.000565 140355837732672 submission.py:120] 1) loss = 6.908, grad_norm = 0.336
I0607 06:12:34.404292 140321715386112 logging_writer.py:48] [2] global_step=2, grad_norm=0.347732, loss=6.907754
I0607 06:12:34.408647 140355837732672 submission.py:120] 2) loss = 6.908, grad_norm = 0.348
I0607 06:12:34.815204 140321706993408 logging_writer.py:48] [3] global_step=3, grad_norm=0.332272, loss=6.907754
I0607 06:12:34.819022 140355837732672 submission.py:120] 3) loss = 6.908, grad_norm = 0.332
I0607 06:12:35.223316 140321715386112 logging_writer.py:48] [4] global_step=4, grad_norm=0.341264, loss=6.907754
I0607 06:12:35.227676 140355837732672 submission.py:120] 4) loss = 6.908, grad_norm = 0.341
I0607 06:12:35.634828 140321706993408 logging_writer.py:48] [5] global_step=5, grad_norm=0.332943, loss=6.907756
I0607 06:12:35.639012 140355837732672 submission.py:120] 5) loss = 6.908, grad_norm = 0.333
I0607 06:12:36.060185 140321715386112 logging_writer.py:48] [6] global_step=6, grad_norm=0.345011, loss=6.907738
I0607 06:12:36.065324 140355837732672 submission.py:120] 6) loss = 6.908, grad_norm = 0.345
I0607 06:12:36.478407 140321706993408 logging_writer.py:48] [7] global_step=7, grad_norm=0.347254, loss=6.907739
I0607 06:12:36.483453 140355837732672 submission.py:120] 7) loss = 6.908, grad_norm = 0.347
I0607 06:12:36.902270 140321715386112 logging_writer.py:48] [8] global_step=8, grad_norm=0.344322, loss=6.907744
I0607 06:12:36.906799 140355837732672 submission.py:120] 8) loss = 6.908, grad_norm = 0.344
I0607 06:12:37.329611 140321706993408 logging_writer.py:48] [9] global_step=9, grad_norm=0.343818, loss=6.907738
I0607 06:12:37.334032 140355837732672 submission.py:120] 9) loss = 6.908, grad_norm = 0.344
I0607 06:12:37.757058 140321715386112 logging_writer.py:48] [10] global_step=10, grad_norm=0.340024, loss=6.907726
I0607 06:12:37.762583 140355837732672 submission.py:120] 10) loss = 6.908, grad_norm = 0.340
I0607 06:12:38.172775 140321706993408 logging_writer.py:48] [11] global_step=11, grad_norm=0.335900, loss=6.907702
I0607 06:12:38.180890 140355837732672 submission.py:120] 11) loss = 6.908, grad_norm = 0.336
I0607 06:12:38.599161 140321715386112 logging_writer.py:48] [12] global_step=12, grad_norm=0.338183, loss=6.907743
I0607 06:12:38.603332 140355837732672 submission.py:120] 12) loss = 6.908, grad_norm = 0.338
I0607 06:12:39.008551 140321706993408 logging_writer.py:48] [13] global_step=13, grad_norm=0.326822, loss=6.907713
I0607 06:12:39.013807 140355837732672 submission.py:120] 13) loss = 6.908, grad_norm = 0.327
I0607 06:12:39.436687 140321715386112 logging_writer.py:48] [14] global_step=14, grad_norm=0.340583, loss=6.907727
I0607 06:12:39.441313 140355837732672 submission.py:120] 14) loss = 6.908, grad_norm = 0.341
I0607 06:12:39.848680 140321706993408 logging_writer.py:48] [15] global_step=15, grad_norm=0.333551, loss=6.907712
I0607 06:12:39.852745 140355837732672 submission.py:120] 15) loss = 6.908, grad_norm = 0.334
I0607 06:12:40.256222 140321715386112 logging_writer.py:48] [16] global_step=16, grad_norm=0.331945, loss=6.907690
I0607 06:12:40.260422 140355837732672 submission.py:120] 16) loss = 6.908, grad_norm = 0.332
I0607 06:12:40.664587 140321706993408 logging_writer.py:48] [17] global_step=17, grad_norm=0.339908, loss=6.907657
I0607 06:12:40.668353 140355837732672 submission.py:120] 17) loss = 6.908, grad_norm = 0.340
I0607 06:12:41.076899 140321715386112 logging_writer.py:48] [18] global_step=18, grad_norm=0.339951, loss=6.907661
I0607 06:12:41.082346 140355837732672 submission.py:120] 18) loss = 6.908, grad_norm = 0.340
I0607 06:12:41.504408 140321706993408 logging_writer.py:48] [19] global_step=19, grad_norm=0.339820, loss=6.907664
I0607 06:12:41.508381 140355837732672 submission.py:120] 19) loss = 6.908, grad_norm = 0.340
I0607 06:12:41.926808 140321715386112 logging_writer.py:48] [20] global_step=20, grad_norm=0.336676, loss=6.907610
I0607 06:12:41.932329 140355837732672 submission.py:120] 20) loss = 6.908, grad_norm = 0.337
I0607 06:12:42.338583 140321706993408 logging_writer.py:48] [21] global_step=21, grad_norm=0.335506, loss=6.907599
I0607 06:12:42.342560 140355837732672 submission.py:120] 21) loss = 6.908, grad_norm = 0.336
I0607 06:12:42.751444 140321715386112 logging_writer.py:48] [22] global_step=22, grad_norm=0.335295, loss=6.907568
I0607 06:12:42.756045 140355837732672 submission.py:120] 22) loss = 6.908, grad_norm = 0.335
I0607 06:12:43.162042 140321706993408 logging_writer.py:48] [23] global_step=23, grad_norm=0.337633, loss=6.907473
I0607 06:12:43.165911 140355837732672 submission.py:120] 23) loss = 6.907, grad_norm = 0.338
I0607 06:12:43.570662 140321715386112 logging_writer.py:48] [24] global_step=24, grad_norm=0.343357, loss=6.907483
I0607 06:12:43.574577 140355837732672 submission.py:120] 24) loss = 6.907, grad_norm = 0.343
I0607 06:12:43.977899 140321706993408 logging_writer.py:48] [25] global_step=25, grad_norm=0.333830, loss=6.907394
I0607 06:12:43.982368 140355837732672 submission.py:120] 25) loss = 6.907, grad_norm = 0.334
I0607 06:12:44.400401 140321715386112 logging_writer.py:48] [26] global_step=26, grad_norm=0.343570, loss=6.907422
I0607 06:12:44.405140 140355837732672 submission.py:120] 26) loss = 6.907, grad_norm = 0.344
I0607 06:12:44.823569 140321706993408 logging_writer.py:48] [27] global_step=27, grad_norm=0.333427, loss=6.907361
I0607 06:12:44.828770 140355837732672 submission.py:120] 27) loss = 6.907, grad_norm = 0.333
I0607 06:12:45.245227 140321715386112 logging_writer.py:48] [28] global_step=28, grad_norm=0.355634, loss=6.907391
I0607 06:12:45.254938 140355837732672 submission.py:120] 28) loss = 6.907, grad_norm = 0.356
I0607 06:12:45.661006 140321706993408 logging_writer.py:48] [29] global_step=29, grad_norm=0.346507, loss=6.907465
I0607 06:12:45.664933 140355837732672 submission.py:120] 29) loss = 6.907, grad_norm = 0.347
I0607 06:12:46.077884 140321715386112 logging_writer.py:48] [30] global_step=30, grad_norm=0.355650, loss=6.907189
I0607 06:12:46.083032 140355837732672 submission.py:120] 30) loss = 6.907, grad_norm = 0.356
I0607 06:12:46.487728 140321706993408 logging_writer.py:48] [31] global_step=31, grad_norm=0.349524, loss=6.907208
I0607 06:12:46.492566 140355837732672 submission.py:120] 31) loss = 6.907, grad_norm = 0.350
I0607 06:12:46.900561 140321715386112 logging_writer.py:48] [32] global_step=32, grad_norm=0.351184, loss=6.907263
I0607 06:12:46.909273 140355837732672 submission.py:120] 32) loss = 6.907, grad_norm = 0.351
I0607 06:12:47.317229 140321706993408 logging_writer.py:48] [33] global_step=33, grad_norm=0.354015, loss=6.907011
I0607 06:12:47.321156 140355837732672 submission.py:120] 33) loss = 6.907, grad_norm = 0.354
I0607 06:12:47.735557 140321715386112 logging_writer.py:48] [34] global_step=34, grad_norm=0.341273, loss=6.907043
I0607 06:12:47.740314 140355837732672 submission.py:120] 34) loss = 6.907, grad_norm = 0.341
I0607 06:12:48.156285 140321706993408 logging_writer.py:48] [35] global_step=35, grad_norm=0.364621, loss=6.906880
I0607 06:12:48.160640 140355837732672 submission.py:120] 35) loss = 6.907, grad_norm = 0.365
I0607 06:12:48.567945 140321715386112 logging_writer.py:48] [36] global_step=36, grad_norm=0.371373, loss=6.906928
I0607 06:12:48.573528 140355837732672 submission.py:120] 36) loss = 6.907, grad_norm = 0.371
I0607 06:12:48.987228 140321706993408 logging_writer.py:48] [37] global_step=37, grad_norm=0.361430, loss=6.907016
I0607 06:12:48.993525 140355837732672 submission.py:120] 37) loss = 6.907, grad_norm = 0.361
I0607 06:12:49.405107 140321715386112 logging_writer.py:48] [38] global_step=38, grad_norm=0.345148, loss=6.906909
I0607 06:12:49.411160 140355837732672 submission.py:120] 38) loss = 6.907, grad_norm = 0.345
I0607 06:12:49.825599 140321706993408 logging_writer.py:48] [39] global_step=39, grad_norm=0.376928, loss=6.906516
I0607 06:12:49.830724 140355837732672 submission.py:120] 39) loss = 6.907, grad_norm = 0.377
I0607 06:12:50.237735 140321715386112 logging_writer.py:48] [40] global_step=40, grad_norm=0.356480, loss=6.906430
I0607 06:12:50.242615 140355837732672 submission.py:120] 40) loss = 6.906, grad_norm = 0.356
I0607 06:12:50.647343 140321706993408 logging_writer.py:48] [41] global_step=41, grad_norm=0.367641, loss=6.906299
I0607 06:12:50.652034 140355837732672 submission.py:120] 41) loss = 6.906, grad_norm = 0.368
I0607 06:12:51.068826 140321715386112 logging_writer.py:48] [42] global_step=42, grad_norm=0.370246, loss=6.906264
I0607 06:12:51.073822 140355837732672 submission.py:120] 42) loss = 6.906, grad_norm = 0.370
I0607 06:12:51.498506 140321706993408 logging_writer.py:48] [43] global_step=43, grad_norm=0.379271, loss=6.906220
I0607 06:12:51.503621 140355837732672 submission.py:120] 43) loss = 6.906, grad_norm = 0.379
I0607 06:12:51.910701 140321715386112 logging_writer.py:48] [44] global_step=44, grad_norm=0.384074, loss=6.906044
I0607 06:12:51.915977 140355837732672 submission.py:120] 44) loss = 6.906, grad_norm = 0.384
I0607 06:12:52.329464 140321706993408 logging_writer.py:48] [45] global_step=45, grad_norm=0.365316, loss=6.905935
I0607 06:12:52.334787 140355837732672 submission.py:120] 45) loss = 6.906, grad_norm = 0.365
I0607 06:12:52.744507 140321715386112 logging_writer.py:48] [46] global_step=46, grad_norm=0.401240, loss=6.905519
I0607 06:12:52.749320 140355837732672 submission.py:120] 46) loss = 6.906, grad_norm = 0.401
I0607 06:12:53.169647 140321706993408 logging_writer.py:48] [47] global_step=47, grad_norm=0.404957, loss=6.905433
I0607 06:12:53.174679 140355837732672 submission.py:120] 47) loss = 6.905, grad_norm = 0.405
I0607 06:12:53.590726 140321715386112 logging_writer.py:48] [48] global_step=48, grad_norm=0.397153, loss=6.905033
I0607 06:12:53.595110 140355837732672 submission.py:120] 48) loss = 6.905, grad_norm = 0.397
I0607 06:12:54.000533 140321706993408 logging_writer.py:48] [49] global_step=49, grad_norm=0.390942, loss=6.905731
I0607 06:12:54.004771 140355837732672 submission.py:120] 49) loss = 6.906, grad_norm = 0.391
I0607 06:12:54.415758 140321715386112 logging_writer.py:48] [50] global_step=50, grad_norm=0.393264, loss=6.905229
I0607 06:12:54.421724 140355837732672 submission.py:120] 50) loss = 6.905, grad_norm = 0.393
I0607 06:12:54.832024 140321706993408 logging_writer.py:48] [51] global_step=51, grad_norm=0.404545, loss=6.905045
I0607 06:12:54.836988 140355837732672 submission.py:120] 51) loss = 6.905, grad_norm = 0.405
I0607 06:12:55.243041 140321715386112 logging_writer.py:48] [52] global_step=52, grad_norm=0.400084, loss=6.904966
I0607 06:12:55.248217 140355837732672 submission.py:120] 52) loss = 6.905, grad_norm = 0.400
I0607 06:12:55.678095 140321706993408 logging_writer.py:48] [53] global_step=53, grad_norm=0.389518, loss=6.904306
I0607 06:12:55.682995 140355837732672 submission.py:120] 53) loss = 6.904, grad_norm = 0.390
I0607 06:12:56.090178 140321715386112 logging_writer.py:48] [54] global_step=54, grad_norm=0.440840, loss=6.904291
I0607 06:12:56.095328 140355837732672 submission.py:120] 54) loss = 6.904, grad_norm = 0.441
I0607 06:12:56.509284 140321706993408 logging_writer.py:48] [55] global_step=55, grad_norm=0.391375, loss=6.904376
I0607 06:12:56.513466 140355837732672 submission.py:120] 55) loss = 6.904, grad_norm = 0.391
I0607 06:12:56.926801 140321715386112 logging_writer.py:48] [56] global_step=56, grad_norm=0.408924, loss=6.903707
I0607 06:12:56.931266 140355837732672 submission.py:120] 56) loss = 6.904, grad_norm = 0.409
I0607 06:12:57.364000 140321706993408 logging_writer.py:48] [57] global_step=57, grad_norm=0.409750, loss=6.903121
I0607 06:12:57.369194 140355837732672 submission.py:120] 57) loss = 6.903, grad_norm = 0.410
I0607 06:12:57.778142 140321715386112 logging_writer.py:48] [58] global_step=58, grad_norm=0.412627, loss=6.904109
I0607 06:12:57.781875 140355837732672 submission.py:120] 58) loss = 6.904, grad_norm = 0.413
I0607 06:12:58.191124 140321706993408 logging_writer.py:48] [59] global_step=59, grad_norm=0.431028, loss=6.902625
I0607 06:12:58.196743 140355837732672 submission.py:120] 59) loss = 6.903, grad_norm = 0.431
I0607 06:12:58.603918 140321715386112 logging_writer.py:48] [60] global_step=60, grad_norm=0.418519, loss=6.902757
I0607 06:12:58.608299 140355837732672 submission.py:120] 60) loss = 6.903, grad_norm = 0.419
I0607 06:12:59.015377 140321706993408 logging_writer.py:48] [61] global_step=61, grad_norm=0.418870, loss=6.902276
I0607 06:12:59.022631 140355837732672 submission.py:120] 61) loss = 6.902, grad_norm = 0.419
I0607 06:12:59.447252 140321715386112 logging_writer.py:48] [62] global_step=62, grad_norm=0.426178, loss=6.902054
I0607 06:12:59.454410 140355837732672 submission.py:120] 62) loss = 6.902, grad_norm = 0.426
I0607 06:12:59.879037 140321706993408 logging_writer.py:48] [63] global_step=63, grad_norm=0.427543, loss=6.902256
I0607 06:12:59.889349 140355837732672 submission.py:120] 63) loss = 6.902, grad_norm = 0.428
I0607 06:13:00.307597 140321715386112 logging_writer.py:48] [64] global_step=64, grad_norm=0.418908, loss=6.901866
I0607 06:13:00.311651 140355837732672 submission.py:120] 64) loss = 6.902, grad_norm = 0.419
I0607 06:13:00.735462 140321706993408 logging_writer.py:48] [65] global_step=65, grad_norm=0.442922, loss=6.901814
I0607 06:13:00.740202 140355837732672 submission.py:120] 65) loss = 6.902, grad_norm = 0.443
I0607 06:13:01.171727 140321715386112 logging_writer.py:48] [66] global_step=66, grad_norm=0.419306, loss=6.901704
I0607 06:13:01.176635 140355837732672 submission.py:120] 66) loss = 6.902, grad_norm = 0.419
I0607 06:13:01.603013 140321706993408 logging_writer.py:48] [67] global_step=67, grad_norm=0.428618, loss=6.901789
I0607 06:13:01.608832 140355837732672 submission.py:120] 67) loss = 6.902, grad_norm = 0.429
I0607 06:13:02.027761 140321715386112 logging_writer.py:48] [68] global_step=68, grad_norm=0.451411, loss=6.900050
I0607 06:13:02.035670 140355837732672 submission.py:120] 68) loss = 6.900, grad_norm = 0.451
I0607 06:13:02.464257 140321706993408 logging_writer.py:48] [69] global_step=69, grad_norm=0.437679, loss=6.901093
I0607 06:13:02.469612 140355837732672 submission.py:120] 69) loss = 6.901, grad_norm = 0.438
I0607 06:13:02.899751 140321715386112 logging_writer.py:48] [70] global_step=70, grad_norm=0.427034, loss=6.900934
I0607 06:13:02.904788 140355837732672 submission.py:120] 70) loss = 6.901, grad_norm = 0.427
I0607 06:13:03.312892 140321706993408 logging_writer.py:48] [71] global_step=71, grad_norm=0.446439, loss=6.900192
I0607 06:13:03.316672 140355837732672 submission.py:120] 71) loss = 6.900, grad_norm = 0.446
I0607 06:13:03.725390 140321715386112 logging_writer.py:48] [72] global_step=72, grad_norm=0.453492, loss=6.900352
I0607 06:13:03.729138 140355837732672 submission.py:120] 72) loss = 6.900, grad_norm = 0.453
I0607 06:13:04.143239 140321706993408 logging_writer.py:48] [73] global_step=73, grad_norm=0.418914, loss=6.900808
I0607 06:13:04.148548 140355837732672 submission.py:120] 73) loss = 6.901, grad_norm = 0.419
I0607 06:13:04.558464 140321715386112 logging_writer.py:48] [74] global_step=74, grad_norm=0.423181, loss=6.900406
I0607 06:13:04.566622 140355837732672 submission.py:120] 74) loss = 6.900, grad_norm = 0.423
I0607 06:13:04.974550 140321706993408 logging_writer.py:48] [75] global_step=75, grad_norm=0.436681, loss=6.898398
I0607 06:13:04.980503 140355837732672 submission.py:120] 75) loss = 6.898, grad_norm = 0.437
I0607 06:13:05.387410 140321715386112 logging_writer.py:48] [76] global_step=76, grad_norm=0.439118, loss=6.898402
I0607 06:13:05.391587 140355837732672 submission.py:120] 76) loss = 6.898, grad_norm = 0.439
I0607 06:13:05.798381 140321706993408 logging_writer.py:48] [77] global_step=77, grad_norm=0.437807, loss=6.899323
I0607 06:13:05.803187 140355837732672 submission.py:120] 77) loss = 6.899, grad_norm = 0.438
I0607 06:13:06.210887 140321715386112 logging_writer.py:48] [78] global_step=78, grad_norm=0.429891, loss=6.897375
I0607 06:13:06.215764 140355837732672 submission.py:120] 78) loss = 6.897, grad_norm = 0.430
I0607 06:13:06.622256 140321706993408 logging_writer.py:48] [79] global_step=79, grad_norm=0.457030, loss=6.895950
I0607 06:13:06.626570 140355837732672 submission.py:120] 79) loss = 6.896, grad_norm = 0.457
I0607 06:13:07.035291 140321715386112 logging_writer.py:48] [80] global_step=80, grad_norm=0.445746, loss=6.896762
I0607 06:13:07.040084 140355837732672 submission.py:120] 80) loss = 6.897, grad_norm = 0.446
I0607 06:13:07.476556 140321706993408 logging_writer.py:48] [81] global_step=81, grad_norm=0.452871, loss=6.895851
I0607 06:13:07.484364 140355837732672 submission.py:120] 81) loss = 6.896, grad_norm = 0.453
I0607 06:13:07.899879 140321715386112 logging_writer.py:48] [82] global_step=82, grad_norm=0.461752, loss=6.895464
I0607 06:13:07.904742 140355837732672 submission.py:120] 82) loss = 6.895, grad_norm = 0.462
I0607 06:13:08.314631 140321706993408 logging_writer.py:48] [83] global_step=83, grad_norm=0.454140, loss=6.894649
I0607 06:13:08.318265 140355837732672 submission.py:120] 83) loss = 6.895, grad_norm = 0.454
I0607 06:13:08.741621 140321715386112 logging_writer.py:48] [84] global_step=84, grad_norm=0.428898, loss=6.898888
I0607 06:13:08.746756 140355837732672 submission.py:120] 84) loss = 6.899, grad_norm = 0.429
I0607 06:13:09.153563 140321706993408 logging_writer.py:48] [85] global_step=85, grad_norm=0.450579, loss=6.893779
I0607 06:13:09.158456 140355837732672 submission.py:120] 85) loss = 6.894, grad_norm = 0.451
I0607 06:13:09.568650 140321715386112 logging_writer.py:48] [86] global_step=86, grad_norm=0.452106, loss=6.895625
I0607 06:13:09.575372 140355837732672 submission.py:120] 86) loss = 6.896, grad_norm = 0.452
I0607 06:13:09.986744 140321706993408 logging_writer.py:48] [87] global_step=87, grad_norm=0.474983, loss=6.894173
I0607 06:13:09.990444 140355837732672 submission.py:120] 87) loss = 6.894, grad_norm = 0.475
I0607 06:13:10.401310 140321715386112 logging_writer.py:48] [88] global_step=88, grad_norm=0.437896, loss=6.897134
I0607 06:13:10.405344 140355837732672 submission.py:120] 88) loss = 6.897, grad_norm = 0.438
I0607 06:13:10.818866 140321706993408 logging_writer.py:48] [89] global_step=89, grad_norm=0.473286, loss=6.892110
I0607 06:13:10.823757 140355837732672 submission.py:120] 89) loss = 6.892, grad_norm = 0.473
I0607 06:13:11.235896 140321715386112 logging_writer.py:48] [90] global_step=90, grad_norm=0.455923, loss=6.896338
I0607 06:13:11.240933 140355837732672 submission.py:120] 90) loss = 6.896, grad_norm = 0.456
I0607 06:13:11.650924 140321706993408 logging_writer.py:48] [91] global_step=91, grad_norm=0.456133, loss=6.892832
I0607 06:13:11.655300 140355837732672 submission.py:120] 91) loss = 6.893, grad_norm = 0.456
I0607 06:13:12.089148 140321715386112 logging_writer.py:48] [92] global_step=92, grad_norm=0.474824, loss=6.892635
I0607 06:13:12.093812 140355837732672 submission.py:120] 92) loss = 6.893, grad_norm = 0.475
I0607 06:13:12.511032 140321706993408 logging_writer.py:48] [93] global_step=93, grad_norm=0.445661, loss=6.893440
I0607 06:13:12.514877 140355837732672 submission.py:120] 93) loss = 6.893, grad_norm = 0.446
I0607 06:13:12.922771 140321715386112 logging_writer.py:48] [94] global_step=94, grad_norm=0.439691, loss=6.894443
I0607 06:13:12.928214 140355837732672 submission.py:120] 94) loss = 6.894, grad_norm = 0.440
I0607 06:13:13.335648 140321706993408 logging_writer.py:48] [95] global_step=95, grad_norm=0.467758, loss=6.893490
I0607 06:13:13.339544 140355837732672 submission.py:120] 95) loss = 6.893, grad_norm = 0.468
I0607 06:13:13.753283 140321715386112 logging_writer.py:48] [96] global_step=96, grad_norm=0.459502, loss=6.890122
I0607 06:13:13.757891 140355837732672 submission.py:120] 96) loss = 6.890, grad_norm = 0.460
I0607 06:13:14.168036 140321706993408 logging_writer.py:48] [97] global_step=97, grad_norm=0.480125, loss=6.889779
I0607 06:13:14.172668 140355837732672 submission.py:120] 97) loss = 6.890, grad_norm = 0.480
I0607 06:13:14.582164 140321715386112 logging_writer.py:48] [98] global_step=98, grad_norm=0.445423, loss=6.890035
I0607 06:13:14.586777 140355837732672 submission.py:120] 98) loss = 6.890, grad_norm = 0.445
I0607 06:13:14.994401 140321706993408 logging_writer.py:48] [99] global_step=99, grad_norm=0.453904, loss=6.890706
I0607 06:13:15.001007 140355837732672 submission.py:120] 99) loss = 6.891, grad_norm = 0.454
I0607 06:13:15.425765 140321715386112 logging_writer.py:48] [100] global_step=100, grad_norm=0.429868, loss=6.890460
I0607 06:13:15.431403 140355837732672 submission.py:120] 100) loss = 6.890, grad_norm = 0.430
I0607 06:15:56.039970 140321706993408 logging_writer.py:48] [500] global_step=500, grad_norm=0.872383, loss=6.712470
I0607 06:15:56.044366 140355837732672 submission.py:120] 500) loss = 6.712, grad_norm = 0.872
I0607 06:19:17.121654 140321715386112 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.842779, loss=6.498039
I0607 06:19:17.128780 140355837732672 submission.py:120] 1000) loss = 6.498, grad_norm = 0.843
I0607 06:19:33.533764 140355837732672 spec.py:298] Evaluating on the training split.
I0607 06:20:16.815124 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 06:21:00.556287 140355837732672 spec.py:326] Evaluating on the test split.
I0607 06:21:02.020874 140355837732672 submission_runner.py:419] Time since start: 641.34s, 	Step: 1042, 	{'train/accuracy': 0.03646484375, 'train/loss': 5.91804443359375, 'validation/accuracy': 0.03506, 'validation/loss': 5.94540875, 'validation/num_examples': 50000, 'test/accuracy': 0.0283, 'test/loss': 6.056621875, 'test/num_examples': 10000, 'score': 426.1962969303131, 'total_duration': 641.3383491039276, 'accumulated_submission_time': 426.1962969303131, 'accumulated_eval_time': 214.51882553100586, 'accumulated_logging_time': 0.029606103897094727}
I0607 06:21:02.030955 140310810195712 logging_writer.py:48] [1042] accumulated_eval_time=214.518826, accumulated_logging_time=0.029606, accumulated_submission_time=426.196297, global_step=1042, preemption_count=0, score=426.196297, test/accuracy=0.028300, test/loss=6.056622, test/num_examples=10000, total_duration=641.338349, train/accuracy=0.036465, train/loss=5.918044, validation/accuracy=0.035060, validation/loss=5.945409, validation/num_examples=50000
I0607 06:24:08.600847 140311963715328 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.537808, loss=6.313456
I0607 06:24:08.608801 140355837732672 submission.py:120] 1500) loss = 6.313, grad_norm = 1.538
I0607 06:27:30.790694 140310810195712 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.129931, loss=5.998851
I0607 06:27:30.795828 140355837732672 submission.py:120] 2000) loss = 5.999, grad_norm = 1.130
I0607 06:28:02.314622 140355837732672 spec.py:298] Evaluating on the training split.
I0607 06:28:47.087455 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 06:29:32.217355 140355837732672 spec.py:326] Evaluating on the test split.
I0607 06:29:33.645663 140355837732672 submission_runner.py:419] Time since start: 1152.96s, 	Step: 2079, 	{'train/accuracy': 0.08556640625, 'train/loss': 5.25283935546875, 'validation/accuracy': 0.07988, 'validation/loss': 5.296845625, 'validation/num_examples': 50000, 'test/accuracy': 0.063, 'test/loss': 5.488337890625, 'test/num_examples': 10000, 'score': 845.8562941551208, 'total_duration': 1152.9632256031036, 'accumulated_submission_time': 845.8562941551208, 'accumulated_eval_time': 305.8502666950226, 'accumulated_logging_time': 0.04791831970214844}
I0607 06:29:33.655735 140311963715328 logging_writer.py:48] [2079] accumulated_eval_time=305.850267, accumulated_logging_time=0.047918, accumulated_submission_time=845.856294, global_step=2079, preemption_count=0, score=845.856294, test/accuracy=0.063000, test/loss=5.488338, test/num_examples=10000, total_duration=1152.963226, train/accuracy=0.085566, train/loss=5.252839, validation/accuracy=0.079880, validation/loss=5.296846, validation/num_examples=50000
I0607 06:32:23.297180 140310810195712 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.885043, loss=5.796848
I0607 06:32:23.302090 140355837732672 submission.py:120] 2500) loss = 5.797, grad_norm = 0.885
I0607 06:35:47.493169 140311963715328 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.910743, loss=5.842062
I0607 06:35:47.499388 140355837732672 submission.py:120] 3000) loss = 5.842, grad_norm = 0.911
I0607 06:36:33.711244 140355837732672 spec.py:298] Evaluating on the training split.
I0607 06:37:18.661751 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 06:38:12.840998 140355837732672 spec.py:326] Evaluating on the test split.
I0607 06:38:14.265473 140355837732672 submission_runner.py:419] Time since start: 1673.58s, 	Step: 3115, 	{'train/accuracy': 0.1409765625, 'train/loss': 4.688375854492188, 'validation/accuracy': 0.13172, 'validation/loss': 4.75139625, 'validation/num_examples': 50000, 'test/accuracy': 0.0986, 'test/loss': 5.046523828125, 'test/num_examples': 10000, 'score': 1265.2948577404022, 'total_duration': 1673.583110332489, 'accumulated_submission_time': 1265.2948577404022, 'accumulated_eval_time': 406.4046471118927, 'accumulated_logging_time': 0.06601071357727051}
I0607 06:38:14.275503 140310810195712 logging_writer.py:48] [3115] accumulated_eval_time=406.404647, accumulated_logging_time=0.066011, accumulated_submission_time=1265.294858, global_step=3115, preemption_count=0, score=1265.294858, test/accuracy=0.098600, test/loss=5.046524, test/num_examples=10000, total_duration=1673.583110, train/accuracy=0.140977, train/loss=4.688376, validation/accuracy=0.131720, validation/loss=4.751396, validation/num_examples=50000
I0607 06:40:50.033539 140311963715328 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.842552, loss=5.759596
I0607 06:40:50.038166 140355837732672 submission.py:120] 3500) loss = 5.760, grad_norm = 0.843
I0607 06:44:14.023815 140310810195712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.865964, loss=5.695241
I0607 06:44:14.029233 140355837732672 submission.py:120] 4000) loss = 5.695, grad_norm = 0.866
I0607 06:45:14.368577 140355837732672 spec.py:298] Evaluating on the training split.
I0607 06:45:58.751928 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 06:46:44.491793 140355837732672 spec.py:326] Evaluating on the test split.
I0607 06:46:45.911560 140355837732672 submission_runner.py:419] Time since start: 2185.23s, 	Step: 4150, 	{'train/accuracy': 0.18654296875, 'train/loss': 4.316898193359375, 'validation/accuracy': 0.1697, 'validation/loss': 4.4082259375, 'validation/num_examples': 50000, 'test/accuracy': 0.133, 'test/loss': 4.74591484375, 'test/num_examples': 10000, 'score': 1684.7695672512054, 'total_duration': 2185.229195356369, 'accumulated_submission_time': 1684.7695672512054, 'accumulated_eval_time': 497.94770073890686, 'accumulated_logging_time': 0.08571600914001465}
I0607 06:46:45.921472 140311963715328 logging_writer.py:48] [4150] accumulated_eval_time=497.947701, accumulated_logging_time=0.085716, accumulated_submission_time=1684.769567, global_step=4150, preemption_count=0, score=1684.769567, test/accuracy=0.133000, test/loss=4.745915, test/num_examples=10000, total_duration=2185.229195, train/accuracy=0.186543, train/loss=4.316898, validation/accuracy=0.169700, validation/loss=4.408226, validation/num_examples=50000
I0607 06:49:08.122489 140310810195712 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.038311, loss=5.295961
I0607 06:49:08.128299 140355837732672 submission.py:120] 4500) loss = 5.296, grad_norm = 1.038
I0607 06:52:29.936650 140311963715328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.882079, loss=5.451625
I0607 06:52:29.942114 140355837732672 submission.py:120] 5000) loss = 5.452, grad_norm = 0.882
I0607 06:53:46.187813 140355837732672 spec.py:298] Evaluating on the training split.
I0607 06:54:31.461133 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 06:55:17.856032 140355837732672 spec.py:326] Evaluating on the test split.
I0607 06:55:19.286330 140355837732672 submission_runner.py:419] Time since start: 2698.60s, 	Step: 5184, 	{'train/accuracy': 0.23791015625, 'train/loss': 3.9363858032226564, 'validation/accuracy': 0.21782, 'validation/loss': 4.045349375, 'validation/num_examples': 50000, 'test/accuracy': 0.1662, 'test/loss': 4.4538421875, 'test/num_examples': 10000, 'score': 2104.415591478348, 'total_duration': 2698.602539539337, 'accumulated_submission_time': 2104.415591478348, 'accumulated_eval_time': 591.0448362827301, 'accumulated_logging_time': 0.10485219955444336}
I0607 06:55:19.296209 140310810195712 logging_writer.py:48] [5184] accumulated_eval_time=591.044836, accumulated_logging_time=0.104852, accumulated_submission_time=2104.415591, global_step=5184, preemption_count=0, score=2104.415591, test/accuracy=0.166200, test/loss=4.453842, test/num_examples=10000, total_duration=2698.602540, train/accuracy=0.237910, train/loss=3.936386, validation/accuracy=0.217820, validation/loss=4.045349, validation/num_examples=50000
I0607 06:57:27.524887 140311963715328 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.866158, loss=5.737805
I0607 06:57:27.529693 140355837732672 submission.py:120] 5500) loss = 5.738, grad_norm = 0.866
I0607 07:00:49.666017 140310810195712 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.686765, loss=5.384504
I0607 07:00:49.672854 140355837732672 submission.py:120] 6000) loss = 5.385, grad_norm = 0.687
I0607 07:02:19.482154 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:03:05.345355 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:03:51.461839 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:03:52.887403 140355837732672 submission_runner.py:419] Time since start: 3212.21s, 	Step: 6224, 	{'train/accuracy': 0.28390625, 'train/loss': 3.586393127441406, 'validation/accuracy': 0.25834, 'validation/loss': 3.7182171875, 'validation/num_examples': 50000, 'test/accuracy': 0.1995, 'test/loss': 4.16532109375, 'test/num_examples': 10000, 'score': 2523.9830849170685, 'total_duration': 3212.20502781868, 'accumulated_submission_time': 2523.9830849170685, 'accumulated_eval_time': 684.4501328468323, 'accumulated_logging_time': 0.12288975715637207}
I0607 07:03:52.900959 140311963715328 logging_writer.py:48] [6224] accumulated_eval_time=684.450133, accumulated_logging_time=0.122890, accumulated_submission_time=2523.983085, global_step=6224, preemption_count=0, score=2523.983085, test/accuracy=0.199500, test/loss=4.165321, test/num_examples=10000, total_duration=3212.205028, train/accuracy=0.283906, train/loss=3.586393, validation/accuracy=0.258340, validation/loss=3.718217, validation/num_examples=50000
I0607 07:05:46.994237 140310810195712 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.820299, loss=5.184826
I0607 07:05:47.002474 140355837732672 submission.py:120] 6500) loss = 5.185, grad_norm = 0.820
I0607 07:09:09.399531 140311963715328 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.757091, loss=5.133996
I0607 07:09:09.405346 140355837732672 submission.py:120] 7000) loss = 5.134, grad_norm = 0.757
I0607 07:10:53.236002 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:11:38.196472 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:12:23.513795 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:12:24.942567 140355837732672 submission_runner.py:419] Time since start: 3724.26s, 	Step: 7258, 	{'train/accuracy': 0.3312890625, 'train/loss': 3.2819879150390623, 'validation/accuracy': 0.30388, 'validation/loss': 3.4094725, 'validation/num_examples': 50000, 'test/accuracy': 0.2331, 'test/loss': 3.93082421875, 'test/num_examples': 10000, 'score': 2943.6967306137085, 'total_duration': 3724.260173559189, 'accumulated_submission_time': 2943.6967306137085, 'accumulated_eval_time': 776.156866312027, 'accumulated_logging_time': 0.14589166641235352}
I0607 07:12:24.952805 140310810195712 logging_writer.py:48] [7258] accumulated_eval_time=776.156866, accumulated_logging_time=0.145892, accumulated_submission_time=2943.696731, global_step=7258, preemption_count=0, score=2943.696731, test/accuracy=0.233100, test/loss=3.930824, test/num_examples=10000, total_duration=3724.260174, train/accuracy=0.331289, train/loss=3.281988, validation/accuracy=0.303880, validation/loss=3.409473, validation/num_examples=50000
I0607 07:14:02.714705 140311963715328 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.616778, loss=4.959123
I0607 07:14:02.718872 140355837732672 submission.py:120] 7500) loss = 4.959, grad_norm = 0.617
I0607 07:17:27.173178 140310810195712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.698087, loss=4.757053
I0607 07:17:27.178333 140355837732672 submission.py:120] 8000) loss = 4.757, grad_norm = 0.698
I0607 07:19:25.316953 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:20:10.183407 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:20:55.932431 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:20:57.354330 140355837732672 submission_runner.py:419] Time since start: 4236.67s, 	Step: 8293, 	{'train/accuracy': 0.3742578125, 'train/loss': 3.002818603515625, 'validation/accuracy': 0.34506, 'validation/loss': 3.1576003125, 'validation/num_examples': 50000, 'test/accuracy': 0.2647, 'test/loss': 3.706491796875, 'test/num_examples': 10000, 'score': 3363.442712545395, 'total_duration': 4236.671936511993, 'accumulated_submission_time': 3363.442712545395, 'accumulated_eval_time': 868.1944284439087, 'accumulated_logging_time': 0.16512298583984375}
I0607 07:20:57.364364 140311963715328 logging_writer.py:48] [8293] accumulated_eval_time=868.194428, accumulated_logging_time=0.165123, accumulated_submission_time=3363.442713, global_step=8293, preemption_count=0, score=3363.442713, test/accuracy=0.264700, test/loss=3.706492, test/num_examples=10000, total_duration=4236.671937, train/accuracy=0.374258, train/loss=3.002819, validation/accuracy=0.345060, validation/loss=3.157600, validation/num_examples=50000
I0607 07:22:21.129499 140310810195712 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.722112, loss=5.000925
I0607 07:22:21.134531 140355837732672 submission.py:120] 8500) loss = 5.001, grad_norm = 0.722
I0607 07:25:45.211112 140311963715328 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.984159, loss=4.602852
I0607 07:25:45.217785 140355837732672 submission.py:120] 9000) loss = 4.603, grad_norm = 0.984
I0607 07:27:57.750128 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:28:43.100049 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:29:29.532809 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:29:30.959681 140355837732672 submission_runner.py:419] Time since start: 4750.28s, 	Step: 9328, 	{'train/accuracy': 0.4045703125, 'train/loss': 2.8286138916015626, 'validation/accuracy': 0.37222, 'validation/loss': 2.995309375, 'validation/num_examples': 50000, 'test/accuracy': 0.2872, 'test/loss': 3.547658984375, 'test/num_examples': 10000, 'score': 3783.217795610428, 'total_duration': 4750.277279615402, 'accumulated_submission_time': 3783.217795610428, 'accumulated_eval_time': 961.4041628837585, 'accumulated_logging_time': 0.18491721153259277}
I0607 07:29:30.969611 140310810195712 logging_writer.py:48] [9328] accumulated_eval_time=961.404163, accumulated_logging_time=0.184917, accumulated_submission_time=3783.217796, global_step=9328, preemption_count=0, score=3783.217796, test/accuracy=0.287200, test/loss=3.547659, test/num_examples=10000, total_duration=4750.277280, train/accuracy=0.404570, train/loss=2.828614, validation/accuracy=0.372220, validation/loss=2.995309, validation/num_examples=50000
I0607 07:30:40.789032 140311963715328 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.579127, loss=5.085867
I0607 07:30:40.793322 140355837732672 submission.py:120] 9500) loss = 5.086, grad_norm = 0.579
I0607 07:34:02.293792 140310810195712 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.676860, loss=4.412602
I0607 07:34:02.301975 140355837732672 submission.py:120] 10000) loss = 4.413, grad_norm = 0.677
I0607 07:36:30.968258 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:37:16.385196 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:38:02.143678 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:38:03.567857 140355837732672 submission_runner.py:419] Time since start: 5262.89s, 	Step: 10363, 	{'train/accuracy': 0.4430859375, 'train/loss': 2.6064218139648436, 'validation/accuracy': 0.4105, 'validation/loss': 2.7815040625, 'validation/num_examples': 50000, 'test/accuracy': 0.319, 'test/loss': 3.35294375, 'test/num_examples': 10000, 'score': 4202.602222204208, 'total_duration': 5262.885450124741, 'accumulated_submission_time': 4202.602222204208, 'accumulated_eval_time': 1054.0037815570831, 'accumulated_logging_time': 0.2032320499420166}
I0607 07:38:03.578100 140311963715328 logging_writer.py:48] [10363] accumulated_eval_time=1054.003782, accumulated_logging_time=0.203232, accumulated_submission_time=4202.602222, global_step=10363, preemption_count=0, score=4202.602222, test/accuracy=0.319000, test/loss=3.352944, test/num_examples=10000, total_duration=5262.885450, train/accuracy=0.443086, train/loss=2.606422, validation/accuracy=0.410500, validation/loss=2.781504, validation/num_examples=50000
I0607 07:38:59.263614 140310810195712 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.590760, loss=4.273459
I0607 07:38:59.269285 140355837732672 submission.py:120] 10500) loss = 4.273, grad_norm = 0.591
I0607 07:42:21.245707 140311963715328 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.691918, loss=4.337399
I0607 07:42:21.251974 140355837732672 submission.py:120] 11000) loss = 4.337, grad_norm = 0.692
I0607 07:45:03.596927 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:45:49.299271 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:46:35.793663 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:46:37.219527 140355837732672 submission_runner.py:419] Time since start: 5776.54s, 	Step: 11399, 	{'train/accuracy': 0.47533203125, 'train/loss': 2.4305169677734373, 'validation/accuracy': 0.44, 'validation/loss': 2.60488375, 'validation/num_examples': 50000, 'test/accuracy': 0.3465, 'test/loss': 3.18068046875, 'test/num_examples': 10000, 'score': 4622.00354552269, 'total_duration': 5776.5371425151825, 'accumulated_submission_time': 4622.00354552269, 'accumulated_eval_time': 1147.626480102539, 'accumulated_logging_time': 0.22154998779296875}
I0607 07:46:37.230543 140310810195712 logging_writer.py:48] [11399] accumulated_eval_time=1147.626480, accumulated_logging_time=0.221550, accumulated_submission_time=4622.003546, global_step=11399, preemption_count=0, score=4622.003546, test/accuracy=0.346500, test/loss=3.180680, test/num_examples=10000, total_duration=5776.537143, train/accuracy=0.475332, train/loss=2.430517, validation/accuracy=0.440000, validation/loss=2.604884, validation/num_examples=50000
I0607 07:47:18.381772 140311963715328 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.629031, loss=4.305228
I0607 07:47:18.386108 140355837732672 submission.py:120] 11500) loss = 4.305, grad_norm = 0.629
I0607 07:50:40.870519 140310810195712 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.600355, loss=4.205353
I0607 07:50:40.875564 140355837732672 submission.py:120] 12000) loss = 4.205, grad_norm = 0.600
I0607 07:53:37.404403 140355837732672 spec.py:298] Evaluating on the training split.
I0607 07:54:23.055958 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 07:55:09.230421 140355837732672 spec.py:326] Evaluating on the test split.
I0607 07:55:10.652441 140355837732672 submission_runner.py:419] Time since start: 6289.97s, 	Step: 12439, 	{'train/accuracy': 0.50361328125, 'train/loss': 2.2897073364257814, 'validation/accuracy': 0.46502, 'validation/loss': 2.4779909375, 'validation/num_examples': 50000, 'test/accuracy': 0.3619, 'test/loss': 3.07566015625, 'test/num_examples': 10000, 'score': 5041.55931186676, 'total_duration': 6289.970022678375, 'accumulated_submission_time': 5041.55931186676, 'accumulated_eval_time': 1240.8746020793915, 'accumulated_logging_time': 0.24290847778320312}
I0607 07:55:10.662774 140311963715328 logging_writer.py:48] [12439] accumulated_eval_time=1240.874602, accumulated_logging_time=0.242908, accumulated_submission_time=5041.559312, global_step=12439, preemption_count=0, score=5041.559312, test/accuracy=0.361900, test/loss=3.075660, test/num_examples=10000, total_duration=6289.970023, train/accuracy=0.503613, train/loss=2.289707, validation/accuracy=0.465020, validation/loss=2.477991, validation/num_examples=50000
I0607 07:55:35.729526 140310810195712 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.674904, loss=4.077012
I0607 07:55:35.733724 140355837732672 submission.py:120] 12500) loss = 4.077, grad_norm = 0.675
I0607 07:58:59.822872 140311963715328 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.590795, loss=4.327772
I0607 07:58:59.827432 140355837732672 submission.py:120] 13000) loss = 4.328, grad_norm = 0.591
I0607 08:02:10.848665 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:02:56.064218 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:03:42.205080 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:03:43.628694 140355837732672 submission_runner.py:419] Time since start: 6802.95s, 	Step: 13473, 	{'train/accuracy': 0.5268359375, 'train/loss': 2.177492828369141, 'validation/accuracy': 0.48584, 'validation/loss': 2.36809, 'validation/num_examples': 50000, 'test/accuracy': 0.3756, 'test/loss': 2.9857076171875, 'test/num_examples': 10000, 'score': 5461.1261212825775, 'total_duration': 6802.946316480637, 'accumulated_submission_time': 5461.1261212825775, 'accumulated_eval_time': 1333.65469622612, 'accumulated_logging_time': 0.2621879577636719}
I0607 08:03:43.639644 140310810195712 logging_writer.py:48] [13473] accumulated_eval_time=1333.654696, accumulated_logging_time=0.262188, accumulated_submission_time=5461.126121, global_step=13473, preemption_count=0, score=5461.126121, test/accuracy=0.375600, test/loss=2.985708, test/num_examples=10000, total_duration=6802.946316, train/accuracy=0.526836, train/loss=2.177493, validation/accuracy=0.485840, validation/loss=2.368090, validation/num_examples=50000
I0607 08:03:54.901324 140311963715328 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.575603, loss=4.470695
I0607 08:03:54.907244 140355837732672 submission.py:120] 13500) loss = 4.471, grad_norm = 0.576
I0607 08:07:19.178504 140310810195712 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.805810, loss=4.189737
I0607 08:07:19.183348 140355837732672 submission.py:120] 14000) loss = 4.190, grad_norm = 0.806
I0607 08:10:41.935299 140311963715328 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.780026, loss=4.211451
I0607 08:10:41.945145 140355837732672 submission.py:120] 14500) loss = 4.211, grad_norm = 0.780
I0607 08:10:43.968751 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:11:28.990488 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:12:14.890686 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:12:16.315969 140355837732672 submission_runner.py:419] Time since start: 7315.63s, 	Step: 14506, 	{'train/accuracy': 0.544765625, 'train/loss': 2.0697732543945313, 'validation/accuracy': 0.50492, 'validation/loss': 2.26881453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3984, 'test/loss': 2.8586306640625, 'test/num_examples': 10000, 'score': 5880.832962274551, 'total_duration': 7315.633598327637, 'accumulated_submission_time': 5880.832962274551, 'accumulated_eval_time': 1426.0019567012787, 'accumulated_logging_time': 0.2828559875488281}
I0607 08:12:16.325851 140310810195712 logging_writer.py:48] [14506] accumulated_eval_time=1426.001957, accumulated_logging_time=0.282856, accumulated_submission_time=5880.832962, global_step=14506, preemption_count=0, score=5880.832962, test/accuracy=0.398400, test/loss=2.858631, test/num_examples=10000, total_duration=7315.633598, train/accuracy=0.544766, train/loss=2.069773, validation/accuracy=0.504920, validation/loss=2.268815, validation/num_examples=50000
I0607 08:15:35.718857 140311963715328 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.683850, loss=3.924836
I0607 08:15:35.724695 140355837732672 submission.py:120] 15000) loss = 3.925, grad_norm = 0.684
I0607 08:19:00.348521 140310810195712 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.637463, loss=3.951604
I0607 08:19:00.354237 140355837732672 submission.py:120] 15500) loss = 3.952, grad_norm = 0.637
I0607 08:19:16.566684 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:20:02.072188 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:20:47.903152 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:20:49.334115 140355837732672 submission_runner.py:419] Time since start: 7828.65s, 	Step: 15541, 	{'train/accuracy': 0.5683203125, 'train/loss': 1.9964154052734375, 'validation/accuracy': 0.52092, 'validation/loss': 2.20930859375, 'validation/num_examples': 50000, 'test/accuracy': 0.41, 'test/loss': 2.812844140625, 'test/num_examples': 10000, 'score': 6300.462772130966, 'total_duration': 7828.651714801788, 'accumulated_submission_time': 6300.462772130966, 'accumulated_eval_time': 1518.7693707942963, 'accumulated_logging_time': 0.3010823726654053}
I0607 08:20:49.344710 140311963715328 logging_writer.py:48] [15541] accumulated_eval_time=1518.769371, accumulated_logging_time=0.301082, accumulated_submission_time=6300.462772, global_step=15541, preemption_count=0, score=6300.462772, test/accuracy=0.410000, test/loss=2.812844, test/num_examples=10000, total_duration=7828.651715, train/accuracy=0.568320, train/loss=1.996415, validation/accuracy=0.520920, validation/loss=2.209309, validation/num_examples=50000
I0607 08:23:55.297694 140310810195712 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.666602, loss=4.231530
I0607 08:23:55.303739 140355837732672 submission.py:120] 16000) loss = 4.232, grad_norm = 0.667
I0607 08:27:19.343047 140311963715328 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.723650, loss=4.136952
I0607 08:27:19.353593 140355837732672 submission.py:120] 16500) loss = 4.137, grad_norm = 0.724
I0607 08:27:49.731676 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:28:35.682630 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:29:22.260463 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:29:23.680240 140355837732672 submission_runner.py:419] Time since start: 8343.00s, 	Step: 16576, 	{'train/accuracy': 0.5780859375, 'train/loss': 1.9075039672851561, 'validation/accuracy': 0.53016, 'validation/loss': 2.12244375, 'validation/num_examples': 50000, 'test/accuracy': 0.4224, 'test/loss': 2.71347421875, 'test/num_examples': 10000, 'score': 6720.2385430336, 'total_duration': 8342.997863531113, 'accumulated_submission_time': 6720.2385430336, 'accumulated_eval_time': 1612.718243598938, 'accumulated_logging_time': 0.31998181343078613}
I0607 08:29:23.690502 140310810195712 logging_writer.py:48] [16576] accumulated_eval_time=1612.718244, accumulated_logging_time=0.319982, accumulated_submission_time=6720.238543, global_step=16576, preemption_count=0, score=6720.238543, test/accuracy=0.422400, test/loss=2.713474, test/num_examples=10000, total_duration=8342.997864, train/accuracy=0.578086, train/loss=1.907504, validation/accuracy=0.530160, validation/loss=2.122444, validation/num_examples=50000
I0607 08:32:15.768585 140311963715328 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.795780, loss=4.044544
I0607 08:32:15.773493 140355837732672 submission.py:120] 17000) loss = 4.045, grad_norm = 0.796
I0607 08:35:37.340831 140310810195712 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.662052, loss=4.330868
I0607 08:35:37.347658 140355837732672 submission.py:120] 17500) loss = 4.331, grad_norm = 0.662
I0607 08:36:24.026990 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:37:09.494050 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:37:55.648803 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:37:57.078453 140355837732672 submission_runner.py:419] Time since start: 8856.40s, 	Step: 17612, 	{'train/accuracy': 0.59037109375, 'train/loss': 1.8758578491210938, 'validation/accuracy': 0.54222, 'validation/loss': 2.0978703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4341, 'test/loss': 2.6922171875, 'test/num_examples': 10000, 'score': 7139.963495731354, 'total_duration': 8856.396039009094, 'accumulated_submission_time': 7139.963495731354, 'accumulated_eval_time': 1705.7697086334229, 'accumulated_logging_time': 0.3397994041442871}
I0607 08:37:57.089127 140311963715328 logging_writer.py:48] [17612] accumulated_eval_time=1705.769709, accumulated_logging_time=0.339799, accumulated_submission_time=7139.963496, global_step=17612, preemption_count=0, score=7139.963496, test/accuracy=0.434100, test/loss=2.692217, test/num_examples=10000, total_duration=8856.396039, train/accuracy=0.590371, train/loss=1.875858, validation/accuracy=0.542220, validation/loss=2.097870, validation/num_examples=50000
I0607 08:40:34.622795 140310810195712 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.838912, loss=4.445633
I0607 08:40:34.629283 140355837732672 submission.py:120] 18000) loss = 4.446, grad_norm = 0.839
I0607 08:43:56.751695 140311963715328 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.861339, loss=3.937489
I0607 08:43:56.756415 140355837732672 submission.py:120] 18500) loss = 3.937, grad_norm = 0.861
I0607 08:44:57.112071 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:45:42.849743 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:46:28.820893 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:46:30.254203 140355837732672 submission_runner.py:419] Time since start: 9369.57s, 	Step: 18651, 	{'train/accuracy': 0.604921875, 'train/loss': 1.7993421936035157, 'validation/accuracy': 0.55302, 'validation/loss': 2.03806375, 'validation/num_examples': 50000, 'test/accuracy': 0.4389, 'test/loss': 2.6385896484375, 'test/num_examples': 10000, 'score': 7559.367151260376, 'total_duration': 9369.571798086166, 'accumulated_submission_time': 7559.367151260376, 'accumulated_eval_time': 1798.9118208885193, 'accumulated_logging_time': 0.3600587844848633}
I0607 08:46:30.270386 140310810195712 logging_writer.py:48] [18651] accumulated_eval_time=1798.911821, accumulated_logging_time=0.360059, accumulated_submission_time=7559.367151, global_step=18651, preemption_count=0, score=7559.367151, test/accuracy=0.438900, test/loss=2.638590, test/num_examples=10000, total_duration=9369.571798, train/accuracy=0.604922, train/loss=1.799342, validation/accuracy=0.553020, validation/loss=2.038064, validation/num_examples=50000
I0607 08:48:54.022381 140311963715328 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.764773, loss=3.860207
I0607 08:48:54.027759 140355837732672 submission.py:120] 19000) loss = 3.860, grad_norm = 0.765
I0607 08:52:16.532309 140310810195712 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.715968, loss=3.585694
I0607 08:52:16.538466 140355837732672 submission.py:120] 19500) loss = 3.586, grad_norm = 0.716
I0607 08:53:30.498622 140355837732672 spec.py:298] Evaluating on the training split.
I0607 08:54:15.761266 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 08:55:01.526807 140355837732672 spec.py:326] Evaluating on the test split.
I0607 08:55:02.954689 140355837732672 submission_runner.py:419] Time since start: 9882.27s, 	Step: 19684, 	{'train/accuracy': 0.61451171875, 'train/loss': 1.7478329467773437, 'validation/accuracy': 0.57004, 'validation/loss': 1.96893453125, 'validation/num_examples': 50000, 'test/accuracy': 0.4488, 'test/loss': 2.5810041015625, 'test/num_examples': 10000, 'score': 7978.980367183685, 'total_duration': 9882.272240400314, 'accumulated_submission_time': 7978.980367183685, 'accumulated_eval_time': 1891.3679041862488, 'accumulated_logging_time': 0.3856799602508545}
I0607 08:55:02.966288 140311963715328 logging_writer.py:48] [19684] accumulated_eval_time=1891.367904, accumulated_logging_time=0.385680, accumulated_submission_time=7978.980367, global_step=19684, preemption_count=0, score=7978.980367, test/accuracy=0.448800, test/loss=2.581004, test/num_examples=10000, total_duration=9882.272240, train/accuracy=0.614512, train/loss=1.747833, validation/accuracy=0.570040, validation/loss=1.968935, validation/num_examples=50000
I0607 08:57:10.478684 140310810195712 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.756183, loss=4.013019
I0607 08:57:10.485253 140355837732672 submission.py:120] 20000) loss = 4.013, grad_norm = 0.756
I0607 09:00:35.094323 140311963715328 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.725693, loss=4.141835
I0607 09:00:35.098653 140355837732672 submission.py:120] 20500) loss = 4.142, grad_norm = 0.726
I0607 09:02:03.276157 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:02:48.058121 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:03:34.185629 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:03:35.611536 140355837732672 submission_runner.py:419] Time since start: 10394.93s, 	Step: 20719, 	{'train/accuracy': 0.627265625, 'train/loss': 1.683457489013672, 'validation/accuracy': 0.5756, 'validation/loss': 1.91597328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4663, 'test/loss': 2.519436328125, 'test/num_examples': 10000, 'score': 8398.676521778107, 'total_duration': 10394.929157495499, 'accumulated_submission_time': 8398.676521778107, 'accumulated_eval_time': 1983.7033877372742, 'accumulated_logging_time': 0.40576720237731934}
I0607 09:03:35.622113 140310810195712 logging_writer.py:48] [20719] accumulated_eval_time=1983.703388, accumulated_logging_time=0.405767, accumulated_submission_time=8398.676522, global_step=20719, preemption_count=0, score=8398.676522, test/accuracy=0.466300, test/loss=2.519436, test/num_examples=10000, total_duration=10394.929157, train/accuracy=0.627266, train/loss=1.683457, validation/accuracy=0.575600, validation/loss=1.915973, validation/num_examples=50000
I0607 09:05:29.364904 140311963715328 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.763073, loss=3.993418
I0607 09:05:29.370811 140355837732672 submission.py:120] 21000) loss = 3.993, grad_norm = 0.763
I0607 09:08:53.017151 140310810195712 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.809089, loss=4.197257
I0607 09:08:53.025148 140355837732672 submission.py:120] 21500) loss = 4.197, grad_norm = 0.809
I0607 09:10:35.986670 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:11:21.829972 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:12:07.971806 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:12:09.398908 140355837732672 submission_runner.py:419] Time since start: 10908.72s, 	Step: 21755, 	{'train/accuracy': 0.63703125, 'train/loss': 1.626312255859375, 'validation/accuracy': 0.58874, 'validation/loss': 1.8553515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4672, 'test/loss': 2.486564453125, 'test/num_examples': 10000, 'score': 8818.421761751175, 'total_duration': 10908.71654510498, 'accumulated_submission_time': 8818.421761751175, 'accumulated_eval_time': 2077.1158678531647, 'accumulated_logging_time': 0.4248688220977783}
I0607 09:12:09.409450 140311963715328 logging_writer.py:48] [21755] accumulated_eval_time=2077.115868, accumulated_logging_time=0.424869, accumulated_submission_time=8818.421762, global_step=21755, preemption_count=0, score=8818.421762, test/accuracy=0.467200, test/loss=2.486564, test/num_examples=10000, total_duration=10908.716545, train/accuracy=0.637031, train/loss=1.626312, validation/accuracy=0.588740, validation/loss=1.855352, validation/num_examples=50000
I0607 09:13:49.062582 140310810195712 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.835986, loss=3.508342
I0607 09:13:49.067575 140355837732672 submission.py:120] 22000) loss = 3.508, grad_norm = 0.836
I0607 09:17:10.831858 140311963715328 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.808587, loss=4.260213
I0607 09:17:10.838069 140355837732672 submission.py:120] 22500) loss = 4.260, grad_norm = 0.809
I0607 09:19:09.678469 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:19:55.329815 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:20:41.582220 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:20:43.008373 140355837732672 submission_runner.py:419] Time since start: 11422.33s, 	Step: 22790, 	{'train/accuracy': 0.64626953125, 'train/loss': 1.5958937072753907, 'validation/accuracy': 0.59254, 'validation/loss': 1.8361365625, 'validation/num_examples': 50000, 'test/accuracy': 0.4699, 'test/loss': 2.449455078125, 'test/num_examples': 10000, 'score': 9238.074353933334, 'total_duration': 11422.325976610184, 'accumulated_submission_time': 9238.074353933334, 'accumulated_eval_time': 2170.445998430252, 'accumulated_logging_time': 0.443798303604126}
I0607 09:20:43.019694 140310810195712 logging_writer.py:48] [22790] accumulated_eval_time=2170.445998, accumulated_logging_time=0.443798, accumulated_submission_time=9238.074354, global_step=22790, preemption_count=0, score=9238.074354, test/accuracy=0.469900, test/loss=2.449455, test/num_examples=10000, total_duration=11422.325977, train/accuracy=0.646270, train/loss=1.595894, validation/accuracy=0.592540, validation/loss=1.836137, validation/num_examples=50000
I0607 09:22:08.280221 140311963715328 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.740056, loss=3.723509
I0607 09:22:08.284953 140355837732672 submission.py:120] 23000) loss = 3.724, grad_norm = 0.740
I0607 09:25:30.512971 140310810195712 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.752747, loss=3.722547
I0607 09:25:30.518653 140355837732672 submission.py:120] 23500) loss = 3.723, grad_norm = 0.753
I0607 09:27:43.136496 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:28:28.833813 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:29:14.895389 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:29:16.319235 140355837732672 submission_runner.py:419] Time since start: 11935.64s, 	Step: 23825, 	{'train/accuracy': 0.6496875, 'train/loss': 1.5202059936523438, 'validation/accuracy': 0.59796, 'validation/loss': 1.77451609375, 'validation/num_examples': 50000, 'test/accuracy': 0.4822, 'test/loss': 2.4167228515625, 'test/num_examples': 10000, 'score': 9657.57751750946, 'total_duration': 11935.636873722076, 'accumulated_submission_time': 9657.57751750946, 'accumulated_eval_time': 2263.628809452057, 'accumulated_logging_time': 0.4642508029937744}
I0607 09:29:16.330906 140311963715328 logging_writer.py:48] [23825] accumulated_eval_time=2263.628809, accumulated_logging_time=0.464251, accumulated_submission_time=9657.577518, global_step=23825, preemption_count=0, score=9657.577518, test/accuracy=0.482200, test/loss=2.416723, test/num_examples=10000, total_duration=11935.636874, train/accuracy=0.649687, train/loss=1.520206, validation/accuracy=0.597960, validation/loss=1.774516, validation/num_examples=50000
I0607 09:30:27.585541 140310810195712 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.814504, loss=3.696333
I0607 09:30:27.590613 140355837732672 submission.py:120] 24000) loss = 3.696, grad_norm = 0.815
I0607 09:33:50.234299 140311963715328 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.872579, loss=3.666374
I0607 09:33:50.240061 140355837732672 submission.py:120] 24500) loss = 3.666, grad_norm = 0.873
I0607 09:36:16.468948 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:37:01.753505 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:37:47.733913 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:37:49.162392 140355837732672 submission_runner.py:419] Time since start: 12448.48s, 	Step: 24863, 	{'train/accuracy': 0.6626171875, 'train/loss': 1.4778257751464843, 'validation/accuracy': 0.60812, 'validation/loss': 1.7232765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4917, 'test/loss': 2.350916796875, 'test/num_examples': 10000, 'score': 10077.08936715126, 'total_duration': 12448.479972362518, 'accumulated_submission_time': 10077.08936715126, 'accumulated_eval_time': 2356.322265148163, 'accumulated_logging_time': 0.48494982719421387}
I0607 09:37:49.173441 140310810195712 logging_writer.py:48] [24863] accumulated_eval_time=2356.322265, accumulated_logging_time=0.484950, accumulated_submission_time=10077.089367, global_step=24863, preemption_count=0, score=10077.089367, test/accuracy=0.491700, test/loss=2.350917, test/num_examples=10000, total_duration=12448.479972, train/accuracy=0.662617, train/loss=1.477826, validation/accuracy=0.608120, validation/loss=1.723277, validation/num_examples=50000
I0607 09:38:44.634401 140311963715328 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.790039, loss=3.328738
I0607 09:38:44.639719 140355837732672 submission.py:120] 25000) loss = 3.329, grad_norm = 0.790
I0607 09:42:09.308548 140310810195712 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.771239, loss=3.714863
I0607 09:42:09.312978 140355837732672 submission.py:120] 25500) loss = 3.715, grad_norm = 0.771
I0607 09:44:49.213746 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:45:34.405052 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:46:21.409000 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:46:22.835061 140355837732672 submission_runner.py:419] Time since start: 12962.15s, 	Step: 25896, 	{'train/accuracy': 0.6673046875, 'train/loss': 1.4727459716796876, 'validation/accuracy': 0.6144, 'validation/loss': 1.72434078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4922, 'test/loss': 2.347044140625, 'test/num_examples': 10000, 'score': 10496.50517654419, 'total_duration': 12962.152678966522, 'accumulated_submission_time': 10496.50517654419, 'accumulated_eval_time': 2449.9436647892, 'accumulated_logging_time': 0.5050101280212402}
I0607 09:46:22.846414 140311963715328 logging_writer.py:48] [25896] accumulated_eval_time=2449.943665, accumulated_logging_time=0.505010, accumulated_submission_time=10496.505177, global_step=25896, preemption_count=0, score=10496.505177, test/accuracy=0.492200, test/loss=2.347044, test/num_examples=10000, total_duration=12962.152679, train/accuracy=0.667305, train/loss=1.472746, validation/accuracy=0.614400, validation/loss=1.724341, validation/num_examples=50000
I0607 09:47:05.360988 140310810195712 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.072611, loss=4.086856
I0607 09:47:05.372822 140355837732672 submission.py:120] 26000) loss = 4.087, grad_norm = 1.073
I0607 09:50:29.005071 140311963715328 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.851045, loss=3.374047
I0607 09:50:29.011313 140355837732672 submission.py:120] 26500) loss = 3.374, grad_norm = 0.851
I0607 09:53:22.882462 140355837732672 spec.py:298] Evaluating on the training split.
I0607 09:54:08.561214 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 09:54:54.227950 140355837732672 spec.py:326] Evaluating on the test split.
I0607 09:54:55.653952 140355837732672 submission_runner.py:419] Time since start: 13474.97s, 	Step: 26930, 	{'train/accuracy': 0.672421875, 'train/loss': 1.4458030700683593, 'validation/accuracy': 0.61726, 'validation/loss': 1.7029534375, 'validation/num_examples': 50000, 'test/accuracy': 0.4973, 'test/loss': 2.3147396484375, 'test/num_examples': 10000, 'score': 10915.922728776932, 'total_duration': 13474.971536874771, 'accumulated_submission_time': 10915.922728776932, 'accumulated_eval_time': 2542.7151749134064, 'accumulated_logging_time': 0.524369478225708}
I0607 09:54:55.665219 140310810195712 logging_writer.py:48] [26930] accumulated_eval_time=2542.715175, accumulated_logging_time=0.524369, accumulated_submission_time=10915.922729, global_step=26930, preemption_count=0, score=10915.922729, test/accuracy=0.497300, test/loss=2.314740, test/num_examples=10000, total_duration=13474.971537, train/accuracy=0.672422, train/loss=1.445803, validation/accuracy=0.617260, validation/loss=1.702953, validation/num_examples=50000
I0607 09:55:24.237758 140311963715328 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.800240, loss=3.250072
I0607 09:55:24.242895 140355837732672 submission.py:120] 27000) loss = 3.250, grad_norm = 0.800
I0607 09:58:45.913868 140310810195712 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.885909, loss=3.312393
I0607 09:58:45.923718 140355837732672 submission.py:120] 27500) loss = 3.312, grad_norm = 0.886
I0607 10:01:55.677763 140355837732672 spec.py:298] Evaluating on the training split.
I0607 10:02:41.392942 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 10:03:27.801592 140355837732672 spec.py:326] Evaluating on the test split.
I0607 10:03:29.232458 140355837732672 submission_runner.py:419] Time since start: 13988.55s, 	Step: 27965, 	{'train/accuracy': 0.6773828125, 'train/loss': 1.420528564453125, 'validation/accuracy': 0.61972, 'validation/loss': 1.68040828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4988, 'test/loss': 2.329244921875, 'test/num_examples': 10000, 'score': 11335.317940950394, 'total_duration': 13988.550023078918, 'accumulated_submission_time': 11335.317940950394, 'accumulated_eval_time': 2636.2699539661407, 'accumulated_logging_time': 0.5442852973937988}
I0607 10:03:29.243521 140311963715328 logging_writer.py:48] [27965] accumulated_eval_time=2636.269954, accumulated_logging_time=0.544285, accumulated_submission_time=11335.317941, global_step=27965, preemption_count=0, score=11335.317941, test/accuracy=0.498800, test/loss=2.329245, test/num_examples=10000, total_duration=13988.550023, train/accuracy=0.677383, train/loss=1.420529, validation/accuracy=0.619720, validation/loss=1.680408, validation/num_examples=50000
I0607 10:03:43.324315 140355837732672 spec.py:298] Evaluating on the training split.
I0607 10:04:28.377813 140355837732672 spec.py:310] Evaluating on the validation split.
I0607 10:05:14.400169 140355837732672 spec.py:326] Evaluating on the test split.
I0607 10:05:15.824549 140355837732672 submission_runner.py:419] Time since start: 14095.14s, 	Step: 28000, 	{'train/accuracy': 0.6756640625, 'train/loss': 1.4457917785644532, 'validation/accuracy': 0.61652, 'validation/loss': 1.71086109375, 'validation/num_examples': 50000, 'test/accuracy': 0.4968, 'test/loss': 2.341841015625, 'test/num_examples': 10000, 'score': 11349.36898970604, 'total_duration': 14095.14215874672, 'accumulated_submission_time': 11349.36898970604, 'accumulated_eval_time': 2728.7703478336334, 'accumulated_logging_time': 0.5636539459228516}
I0607 10:05:15.835573 140310810195712 logging_writer.py:48] [28000] accumulated_eval_time=2728.770348, accumulated_logging_time=0.563654, accumulated_submission_time=11349.368990, global_step=28000, preemption_count=0, score=11349.368990, test/accuracy=0.496800, test/loss=2.341841, test/num_examples=10000, total_duration=14095.142159, train/accuracy=0.675664, train/loss=1.445792, validation/accuracy=0.616520, validation/loss=1.710861, validation/num_examples=50000
I0607 10:05:15.853427 140311963715328 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11349.368990
I0607 10:05:16.514992 140355837732672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/adamw/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0607 10:05:16.778851 140355837732672 submission_runner.py:581] Tuning trial 1/1
I0607 10:05:16.779090 140355837732672 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 10:05:16.780271 140355837732672 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0019921875, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00224, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.0027, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.658056020736694, 'total_duration': 132.6903772354126, 'accumulated_submission_time': 6.658056020736694, 'accumulated_eval_time': 126.03185486793518, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1042, {'train/accuracy': 0.03646484375, 'train/loss': 5.91804443359375, 'validation/accuracy': 0.03506, 'validation/loss': 5.94540875, 'validation/num_examples': 50000, 'test/accuracy': 0.0283, 'test/loss': 6.056621875, 'test/num_examples': 10000, 'score': 426.1962969303131, 'total_duration': 641.3383491039276, 'accumulated_submission_time': 426.1962969303131, 'accumulated_eval_time': 214.51882553100586, 'accumulated_logging_time': 0.029606103897094727, 'global_step': 1042, 'preemption_count': 0}), (2079, {'train/accuracy': 0.08556640625, 'train/loss': 5.25283935546875, 'validation/accuracy': 0.07988, 'validation/loss': 5.296845625, 'validation/num_examples': 50000, 'test/accuracy': 0.063, 'test/loss': 5.488337890625, 'test/num_examples': 10000, 'score': 845.8562941551208, 'total_duration': 1152.9632256031036, 'accumulated_submission_time': 845.8562941551208, 'accumulated_eval_time': 305.8502666950226, 'accumulated_logging_time': 0.04791831970214844, 'global_step': 2079, 'preemption_count': 0}), (3115, {'train/accuracy': 0.1409765625, 'train/loss': 4.688375854492188, 'validation/accuracy': 0.13172, 'validation/loss': 4.75139625, 'validation/num_examples': 50000, 'test/accuracy': 0.0986, 'test/loss': 5.046523828125, 'test/num_examples': 10000, 'score': 1265.2948577404022, 'total_duration': 1673.583110332489, 'accumulated_submission_time': 1265.2948577404022, 'accumulated_eval_time': 406.4046471118927, 'accumulated_logging_time': 0.06601071357727051, 'global_step': 3115, 'preemption_count': 0}), (4150, {'train/accuracy': 0.18654296875, 'train/loss': 4.316898193359375, 'validation/accuracy': 0.1697, 'validation/loss': 4.4082259375, 'validation/num_examples': 50000, 'test/accuracy': 0.133, 'test/loss': 4.74591484375, 'test/num_examples': 10000, 'score': 1684.7695672512054, 'total_duration': 2185.229195356369, 'accumulated_submission_time': 1684.7695672512054, 'accumulated_eval_time': 497.94770073890686, 'accumulated_logging_time': 0.08571600914001465, 'global_step': 4150, 'preemption_count': 0}), (5184, {'train/accuracy': 0.23791015625, 'train/loss': 3.9363858032226564, 'validation/accuracy': 0.21782, 'validation/loss': 4.045349375, 'validation/num_examples': 50000, 'test/accuracy': 0.1662, 'test/loss': 4.4538421875, 'test/num_examples': 10000, 'score': 2104.415591478348, 'total_duration': 2698.602539539337, 'accumulated_submission_time': 2104.415591478348, 'accumulated_eval_time': 591.0448362827301, 'accumulated_logging_time': 0.10485219955444336, 'global_step': 5184, 'preemption_count': 0}), (6224, {'train/accuracy': 0.28390625, 'train/loss': 3.586393127441406, 'validation/accuracy': 0.25834, 'validation/loss': 3.7182171875, 'validation/num_examples': 50000, 'test/accuracy': 0.1995, 'test/loss': 4.16532109375, 'test/num_examples': 10000, 'score': 2523.9830849170685, 'total_duration': 3212.20502781868, 'accumulated_submission_time': 2523.9830849170685, 'accumulated_eval_time': 684.4501328468323, 'accumulated_logging_time': 0.12288975715637207, 'global_step': 6224, 'preemption_count': 0}), (7258, {'train/accuracy': 0.3312890625, 'train/loss': 3.2819879150390623, 'validation/accuracy': 0.30388, 'validation/loss': 3.4094725, 'validation/num_examples': 50000, 'test/accuracy': 0.2331, 'test/loss': 3.93082421875, 'test/num_examples': 10000, 'score': 2943.6967306137085, 'total_duration': 3724.260173559189, 'accumulated_submission_time': 2943.6967306137085, 'accumulated_eval_time': 776.156866312027, 'accumulated_logging_time': 0.14589166641235352, 'global_step': 7258, 'preemption_count': 0}), (8293, {'train/accuracy': 0.3742578125, 'train/loss': 3.002818603515625, 'validation/accuracy': 0.34506, 'validation/loss': 3.1576003125, 'validation/num_examples': 50000, 'test/accuracy': 0.2647, 'test/loss': 3.706491796875, 'test/num_examples': 10000, 'score': 3363.442712545395, 'total_duration': 4236.671936511993, 'accumulated_submission_time': 3363.442712545395, 'accumulated_eval_time': 868.1944284439087, 'accumulated_logging_time': 0.16512298583984375, 'global_step': 8293, 'preemption_count': 0}), (9328, {'train/accuracy': 0.4045703125, 'train/loss': 2.8286138916015626, 'validation/accuracy': 0.37222, 'validation/loss': 2.995309375, 'validation/num_examples': 50000, 'test/accuracy': 0.2872, 'test/loss': 3.547658984375, 'test/num_examples': 10000, 'score': 3783.217795610428, 'total_duration': 4750.277279615402, 'accumulated_submission_time': 3783.217795610428, 'accumulated_eval_time': 961.4041628837585, 'accumulated_logging_time': 0.18491721153259277, 'global_step': 9328, 'preemption_count': 0}), (10363, {'train/accuracy': 0.4430859375, 'train/loss': 2.6064218139648436, 'validation/accuracy': 0.4105, 'validation/loss': 2.7815040625, 'validation/num_examples': 50000, 'test/accuracy': 0.319, 'test/loss': 3.35294375, 'test/num_examples': 10000, 'score': 4202.602222204208, 'total_duration': 5262.885450124741, 'accumulated_submission_time': 4202.602222204208, 'accumulated_eval_time': 1054.0037815570831, 'accumulated_logging_time': 0.2032320499420166, 'global_step': 10363, 'preemption_count': 0}), (11399, {'train/accuracy': 0.47533203125, 'train/loss': 2.4305169677734373, 'validation/accuracy': 0.44, 'validation/loss': 2.60488375, 'validation/num_examples': 50000, 'test/accuracy': 0.3465, 'test/loss': 3.18068046875, 'test/num_examples': 10000, 'score': 4622.00354552269, 'total_duration': 5776.5371425151825, 'accumulated_submission_time': 4622.00354552269, 'accumulated_eval_time': 1147.626480102539, 'accumulated_logging_time': 0.22154998779296875, 'global_step': 11399, 'preemption_count': 0}), (12439, {'train/accuracy': 0.50361328125, 'train/loss': 2.2897073364257814, 'validation/accuracy': 0.46502, 'validation/loss': 2.4779909375, 'validation/num_examples': 50000, 'test/accuracy': 0.3619, 'test/loss': 3.07566015625, 'test/num_examples': 10000, 'score': 5041.55931186676, 'total_duration': 6289.970022678375, 'accumulated_submission_time': 5041.55931186676, 'accumulated_eval_time': 1240.8746020793915, 'accumulated_logging_time': 0.24290847778320312, 'global_step': 12439, 'preemption_count': 0}), (13473, {'train/accuracy': 0.5268359375, 'train/loss': 2.177492828369141, 'validation/accuracy': 0.48584, 'validation/loss': 2.36809, 'validation/num_examples': 50000, 'test/accuracy': 0.3756, 'test/loss': 2.9857076171875, 'test/num_examples': 10000, 'score': 5461.1261212825775, 'total_duration': 6802.946316480637, 'accumulated_submission_time': 5461.1261212825775, 'accumulated_eval_time': 1333.65469622612, 'accumulated_logging_time': 0.2621879577636719, 'global_step': 13473, 'preemption_count': 0}), (14506, {'train/accuracy': 0.544765625, 'train/loss': 2.0697732543945313, 'validation/accuracy': 0.50492, 'validation/loss': 2.26881453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3984, 'test/loss': 2.8586306640625, 'test/num_examples': 10000, 'score': 5880.832962274551, 'total_duration': 7315.633598327637, 'accumulated_submission_time': 5880.832962274551, 'accumulated_eval_time': 1426.0019567012787, 'accumulated_logging_time': 0.2828559875488281, 'global_step': 14506, 'preemption_count': 0}), (15541, {'train/accuracy': 0.5683203125, 'train/loss': 1.9964154052734375, 'validation/accuracy': 0.52092, 'validation/loss': 2.20930859375, 'validation/num_examples': 50000, 'test/accuracy': 0.41, 'test/loss': 2.812844140625, 'test/num_examples': 10000, 'score': 6300.462772130966, 'total_duration': 7828.651714801788, 'accumulated_submission_time': 6300.462772130966, 'accumulated_eval_time': 1518.7693707942963, 'accumulated_logging_time': 0.3010823726654053, 'global_step': 15541, 'preemption_count': 0}), (16576, {'train/accuracy': 0.5780859375, 'train/loss': 1.9075039672851561, 'validation/accuracy': 0.53016, 'validation/loss': 2.12244375, 'validation/num_examples': 50000, 'test/accuracy': 0.4224, 'test/loss': 2.71347421875, 'test/num_examples': 10000, 'score': 6720.2385430336, 'total_duration': 8342.997863531113, 'accumulated_submission_time': 6720.2385430336, 'accumulated_eval_time': 1612.718243598938, 'accumulated_logging_time': 0.31998181343078613, 'global_step': 16576, 'preemption_count': 0}), (17612, {'train/accuracy': 0.59037109375, 'train/loss': 1.8758578491210938, 'validation/accuracy': 0.54222, 'validation/loss': 2.0978703125, 'validation/num_examples': 50000, 'test/accuracy': 0.4341, 'test/loss': 2.6922171875, 'test/num_examples': 10000, 'score': 7139.963495731354, 'total_duration': 8856.396039009094, 'accumulated_submission_time': 7139.963495731354, 'accumulated_eval_time': 1705.7697086334229, 'accumulated_logging_time': 0.3397994041442871, 'global_step': 17612, 'preemption_count': 0}), (18651, {'train/accuracy': 0.604921875, 'train/loss': 1.7993421936035157, 'validation/accuracy': 0.55302, 'validation/loss': 2.03806375, 'validation/num_examples': 50000, 'test/accuracy': 0.4389, 'test/loss': 2.6385896484375, 'test/num_examples': 10000, 'score': 7559.367151260376, 'total_duration': 9369.571798086166, 'accumulated_submission_time': 7559.367151260376, 'accumulated_eval_time': 1798.9118208885193, 'accumulated_logging_time': 0.3600587844848633, 'global_step': 18651, 'preemption_count': 0}), (19684, {'train/accuracy': 0.61451171875, 'train/loss': 1.7478329467773437, 'validation/accuracy': 0.57004, 'validation/loss': 1.96893453125, 'validation/num_examples': 50000, 'test/accuracy': 0.4488, 'test/loss': 2.5810041015625, 'test/num_examples': 10000, 'score': 7978.980367183685, 'total_duration': 9882.272240400314, 'accumulated_submission_time': 7978.980367183685, 'accumulated_eval_time': 1891.3679041862488, 'accumulated_logging_time': 0.3856799602508545, 'global_step': 19684, 'preemption_count': 0}), (20719, {'train/accuracy': 0.627265625, 'train/loss': 1.683457489013672, 'validation/accuracy': 0.5756, 'validation/loss': 1.91597328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4663, 'test/loss': 2.519436328125, 'test/num_examples': 10000, 'score': 8398.676521778107, 'total_duration': 10394.929157495499, 'accumulated_submission_time': 8398.676521778107, 'accumulated_eval_time': 1983.7033877372742, 'accumulated_logging_time': 0.40576720237731934, 'global_step': 20719, 'preemption_count': 0}), (21755, {'train/accuracy': 0.63703125, 'train/loss': 1.626312255859375, 'validation/accuracy': 0.58874, 'validation/loss': 1.8553515625, 'validation/num_examples': 50000, 'test/accuracy': 0.4672, 'test/loss': 2.486564453125, 'test/num_examples': 10000, 'score': 8818.421761751175, 'total_duration': 10908.71654510498, 'accumulated_submission_time': 8818.421761751175, 'accumulated_eval_time': 2077.1158678531647, 'accumulated_logging_time': 0.4248688220977783, 'global_step': 21755, 'preemption_count': 0}), (22790, {'train/accuracy': 0.64626953125, 'train/loss': 1.5958937072753907, 'validation/accuracy': 0.59254, 'validation/loss': 1.8361365625, 'validation/num_examples': 50000, 'test/accuracy': 0.4699, 'test/loss': 2.449455078125, 'test/num_examples': 10000, 'score': 9238.074353933334, 'total_duration': 11422.325976610184, 'accumulated_submission_time': 9238.074353933334, 'accumulated_eval_time': 2170.445998430252, 'accumulated_logging_time': 0.443798303604126, 'global_step': 22790, 'preemption_count': 0}), (23825, {'train/accuracy': 0.6496875, 'train/loss': 1.5202059936523438, 'validation/accuracy': 0.59796, 'validation/loss': 1.77451609375, 'validation/num_examples': 50000, 'test/accuracy': 0.4822, 'test/loss': 2.4167228515625, 'test/num_examples': 10000, 'score': 9657.57751750946, 'total_duration': 11935.636873722076, 'accumulated_submission_time': 9657.57751750946, 'accumulated_eval_time': 2263.628809452057, 'accumulated_logging_time': 0.4642508029937744, 'global_step': 23825, 'preemption_count': 0}), (24863, {'train/accuracy': 0.6626171875, 'train/loss': 1.4778257751464843, 'validation/accuracy': 0.60812, 'validation/loss': 1.7232765625, 'validation/num_examples': 50000, 'test/accuracy': 0.4917, 'test/loss': 2.350916796875, 'test/num_examples': 10000, 'score': 10077.08936715126, 'total_duration': 12448.479972362518, 'accumulated_submission_time': 10077.08936715126, 'accumulated_eval_time': 2356.322265148163, 'accumulated_logging_time': 0.48494982719421387, 'global_step': 24863, 'preemption_count': 0}), (25896, {'train/accuracy': 0.6673046875, 'train/loss': 1.4727459716796876, 'validation/accuracy': 0.6144, 'validation/loss': 1.72434078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4922, 'test/loss': 2.347044140625, 'test/num_examples': 10000, 'score': 10496.50517654419, 'total_duration': 12962.152678966522, 'accumulated_submission_time': 10496.50517654419, 'accumulated_eval_time': 2449.9436647892, 'accumulated_logging_time': 0.5050101280212402, 'global_step': 25896, 'preemption_count': 0}), (26930, {'train/accuracy': 0.672421875, 'train/loss': 1.4458030700683593, 'validation/accuracy': 0.61726, 'validation/loss': 1.7029534375, 'validation/num_examples': 50000, 'test/accuracy': 0.4973, 'test/loss': 2.3147396484375, 'test/num_examples': 10000, 'score': 10915.922728776932, 'total_duration': 13474.971536874771, 'accumulated_submission_time': 10915.922728776932, 'accumulated_eval_time': 2542.7151749134064, 'accumulated_logging_time': 0.524369478225708, 'global_step': 26930, 'preemption_count': 0}), (27965, {'train/accuracy': 0.6773828125, 'train/loss': 1.420528564453125, 'validation/accuracy': 0.61972, 'validation/loss': 1.68040828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4988, 'test/loss': 2.329244921875, 'test/num_examples': 10000, 'score': 11335.317940950394, 'total_duration': 13988.550023078918, 'accumulated_submission_time': 11335.317940950394, 'accumulated_eval_time': 2636.2699539661407, 'accumulated_logging_time': 0.5442852973937988, 'global_step': 27965, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6756640625, 'train/loss': 1.4457917785644532, 'validation/accuracy': 0.61652, 'validation/loss': 1.71086109375, 'validation/num_examples': 50000, 'test/accuracy': 0.4968, 'test/loss': 2.341841015625, 'test/num_examples': 10000, 'score': 11349.36898970604, 'total_duration': 14095.14215874672, 'accumulated_submission_time': 11349.36898970604, 'accumulated_eval_time': 2728.7703478336334, 'accumulated_logging_time': 0.5636539459228516, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0607 10:05:16.780394 140355837732672 submission_runner.py:584] Timing: 11349.36898970604
I0607 10:05:16.780446 140355837732672 submission_runner.py:586] Total number of evals: 29
I0607 10:05:16.780491 140355837732672 submission_runner.py:587] ====================
I0607 10:05:16.780605 140355837732672 submission_runner.py:655] Final imagenet_vit score: 11349.36898970604
