torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 2>&1 | tee -a /logs/ogbg_pytorch_06-27-2023-19-37-57.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-27 19:38:02.635387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.637572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.651060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.651074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.665679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.675014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.717654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 19:38:02.738559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 19:38:15.389407 140498361239360 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0627 19:38:15.389447 139935475853120 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0627 19:38:15.390565 139858429015872 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0627 19:38:15.390552 140189507086144 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0627 19:38:15.390633 140551743788864 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0627 19:38:15.391014 140526140589888 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0627 19:38:15.391549 140316451252032 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0627 19:38:15.400782 140297692706624 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0627 19:38:15.401071 140297692706624 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.401138 139858429015872 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.401190 140189507086144 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.401215 140551743788864 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.401597 140526140589888 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.402144 140316451252032 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.410320 140498361239360 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:15.410339 139935475853120 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 19:38:17.001789 140297692706624 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/ogbg_pytorch.
W0627 19:38:17.031854 139935475853120 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 19:38:17.033501 140526140589888 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 19:38:17.033917 140189507086144 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 19:38:17.033919 139858429015872 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 19:38:17.035636 140297692706624 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 19:38:17.035695 140551743788864 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 19:38:17.035903 140498361239360 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 19:38:17.042535 140297692706624 submission_runner.py:547] Using RNG seed 2403451176
I0627 19:38:17.044410 140297692706624 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 19:38:17.044584 140297692706624 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1.
I0627 19:38:17.045047 140297692706624 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/hparams.json.
I0627 19:38:17.046181 140297692706624 submission_runner.py:249] Initializing dataset.
I0627 19:38:17.046355 140297692706624 submission_runner.py:256] Initializing model.
W0627 19:38:17.069323 140316451252032 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 19:38:21.123052 140297692706624 submission_runner.py:268] Initializing optimizer.
I0627 19:38:21.124007 140297692706624 submission_runner.py:275] Initializing metrics bundle.
I0627 19:38:21.124161 140297692706624 submission_runner.py:292] Initializing checkpoint and logger.
I0627 19:38:21.125229 140297692706624 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 19:38:21.125359 140297692706624 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 19:38:21.729786 140297692706624 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0627 19:38:21.730700 140297692706624 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0627 19:38:21.781386 140297692706624 submission_runner.py:328] Starting training loop.
I0627 19:38:22.389716 140297692706624 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:38:22.395460 140297692706624 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0627 19:38:22.483571 140297692706624 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0627 19:38:22.548152 140297692706624 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:38:28.068917 140259127977728 logging_writer.py:48] [0] global_step=0, grad_norm=2.733153, loss=0.741293
I0627 19:38:28.082545 140297692706624 submission.py:119] 0) loss = 0.741, grad_norm = 2.733
I0627 19:38:28.118123 140297692706624 spec.py:298] Evaluating on the training split.
I0627 19:38:28.123214 140297692706624 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:38:28.127500 140297692706624 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0627 19:38:28.193178 140297692706624 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:39:27.156657 140297692706624 spec.py:310] Evaluating on the validation split.
I0627 19:39:27.160310 140297692706624 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:39:27.164975 140297692706624 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0627 19:39:27.229799 140297692706624 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:40:13.605640 140297692706624 spec.py:326] Evaluating on the test split.
I0627 19:40:13.609162 140297692706624 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:40:13.613655 140297692706624 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0627 19:40:13.680601 140297692706624 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 19:41:02.350849 140297692706624 submission_runner.py:424] Time since start: 160.57s, 	Step: 1, 	{'train/accuracy': 0.5665393040326331, 'train/loss': 0.7420014084082207, 'train/mean_average_precision': 0.022223391266503773, 'validation/accuracy': 0.5714995530592909, 'validation/loss': 0.7394521891164051, 'validation/mean_average_precision': 0.026141740283469845, 'validation/num_examples': 43793, 'test/accuracy': 0.5718741352346624, 'test/loss': 0.7383072669982592, 'test/mean_average_precision': 0.02750729346413685, 'test/num_examples': 43793, 'score': 6.335967302322388, 'total_duration': 160.56988406181335, 'accumulated_submission_time': 6.335967302322388, 'accumulated_eval_time': 154.23235249519348, 'accumulated_logging_time': 0}
I0627 19:41:02.358334 140245487650560 logging_writer.py:48] [1] accumulated_eval_time=154.232352, accumulated_logging_time=0, accumulated_submission_time=6.335967, global_step=1, preemption_count=0, score=6.335967, test/accuracy=0.571874, test/loss=0.738307, test/mean_average_precision=0.027507, test/num_examples=43793, total_duration=160.569884, train/accuracy=0.566539, train/loss=0.742001, train/mean_average_precision=0.022223, validation/accuracy=0.571500, validation/loss=0.739452, validation/mean_average_precision=0.026142, validation/num_examples=43793
I0627 19:41:02.677685 140297692706624 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687572 140189507086144 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687583 140498361239360 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687580 139858429015872 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687576 140551743788864 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687582 140316451252032 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687581 140526140589888 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.687648 139935475853120 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 19:41:02.719421 140245496043264 logging_writer.py:48] [1] global_step=1, grad_norm=2.767461, loss=0.741391
I0627 19:41:02.724148 140297692706624 submission.py:119] 1) loss = 0.741, grad_norm = 2.767
I0627 19:41:03.060809 140245487650560 logging_writer.py:48] [2] global_step=2, grad_norm=2.762705, loss=0.741697
I0627 19:41:03.065820 140297692706624 submission.py:119] 2) loss = 0.742, grad_norm = 2.763
I0627 19:41:03.397935 140245496043264 logging_writer.py:48] [3] global_step=3, grad_norm=2.757699, loss=0.740168
I0627 19:41:03.403225 140297692706624 submission.py:119] 3) loss = 0.740, grad_norm = 2.758
I0627 19:41:03.744036 140245487650560 logging_writer.py:48] [4] global_step=4, grad_norm=2.736135, loss=0.736304
I0627 19:41:03.749171 140297692706624 submission.py:119] 4) loss = 0.736, grad_norm = 2.736
I0627 19:41:04.085891 140245496043264 logging_writer.py:48] [5] global_step=5, grad_norm=2.752316, loss=0.732685
I0627 19:41:04.090801 140297692706624 submission.py:119] 5) loss = 0.733, grad_norm = 2.752
I0627 19:41:04.423991 140245487650560 logging_writer.py:48] [6] global_step=6, grad_norm=2.754802, loss=0.729502
I0627 19:41:04.428853 140297692706624 submission.py:119] 6) loss = 0.730, grad_norm = 2.755
I0627 19:41:04.770690 140245496043264 logging_writer.py:48] [7] global_step=7, grad_norm=2.832248, loss=0.726917
I0627 19:41:04.775486 140297692706624 submission.py:119] 7) loss = 0.727, grad_norm = 2.832
I0627 19:41:05.117172 140245487650560 logging_writer.py:48] [8] global_step=8, grad_norm=2.734405, loss=0.718219
I0627 19:41:05.121812 140297692706624 submission.py:119] 8) loss = 0.718, grad_norm = 2.734
I0627 19:41:05.472055 140245496043264 logging_writer.py:48] [9] global_step=9, grad_norm=2.711747, loss=0.714031
I0627 19:41:05.476891 140297692706624 submission.py:119] 9) loss = 0.714, grad_norm = 2.712
I0627 19:41:05.477817 140297692706624 spec.py:298] Evaluating on the training split.
I0627 19:42:09.206393 140297692706624 spec.py:310] Evaluating on the validation split.
I0627 19:42:12.971082 140297692706624 spec.py:326] Evaluating on the test split.
I0627 19:42:16.744244 140297692706624 submission_runner.py:424] Time since start: 234.96s, 	Step: 10, 	{'train/accuracy': 0.6132403186992906, 'train/loss': 0.7059319852626471, 'train/mean_average_precision': 0.022487211983036243, 'validation/accuracy': 0.6132359400409351, 'validation/loss': 0.7058088388715823, 'validation/mean_average_precision': 0.026397722946919992, 'validation/num_examples': 43793, 'test/accuracy': 0.6114681204060822, 'test/loss': 0.7053804777948268, 'test/mean_average_precision': 0.027367470987903122, 'test/num_examples': 43793, 'score': 9.438956022262573, 'total_duration': 234.96336436271667, 'accumulated_submission_time': 9.438956022262573, 'accumulated_eval_time': 225.4984438419342, 'accumulated_logging_time': 0.020377159118652344}
I0627 19:42:16.752941 140245487650560 logging_writer.py:48] [10] accumulated_eval_time=225.498444, accumulated_logging_time=0.020377, accumulated_submission_time=9.438956, global_step=10, preemption_count=0, score=9.438956, test/accuracy=0.611468, test/loss=0.705380, test/mean_average_precision=0.027367, test/num_examples=43793, total_duration=234.963364, train/accuracy=0.613240, train/loss=0.705932, train/mean_average_precision=0.022487, validation/accuracy=0.613236, validation/loss=0.705809, validation/mean_average_precision=0.026398, validation/num_examples=43793
I0627 19:42:16.772135 140245496043264 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=9.438956
I0627 19:42:16.871353 140297692706624 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/checkpoint_10.
I0627 19:42:16.980078 140297692706624 submission_runner.py:587] Tuning trial 1/1
I0627 19:42:16.980294 140297692706624 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 19:42:16.981288 140297692706624 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5665393040326331, 'train/loss': 0.7420014084082207, 'train/mean_average_precision': 0.022223391266503773, 'validation/accuracy': 0.5714995530592909, 'validation/loss': 0.7394521891164051, 'validation/mean_average_precision': 0.026141740283469845, 'validation/num_examples': 43793, 'test/accuracy': 0.5718741352346624, 'test/loss': 0.7383072669982592, 'test/mean_average_precision': 0.02750729346413685, 'test/num_examples': 43793, 'score': 6.335967302322388, 'total_duration': 160.56988406181335, 'accumulated_submission_time': 6.335967302322388, 'accumulated_eval_time': 154.23235249519348, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.6132403186992906, 'train/loss': 0.7059319852626471, 'train/mean_average_precision': 0.022487211983036243, 'validation/accuracy': 0.6132359400409351, 'validation/loss': 0.7058088388715823, 'validation/mean_average_precision': 0.026397722946919992, 'validation/num_examples': 43793, 'test/accuracy': 0.6114681204060822, 'test/loss': 0.7053804777948268, 'test/mean_average_precision': 0.027367470987903122, 'test/num_examples': 43793, 'score': 9.438956022262573, 'total_duration': 234.96336436271667, 'accumulated_submission_time': 9.438956022262573, 'accumulated_eval_time': 225.4984438419342, 'accumulated_logging_time': 0.020377159118652344, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0627 19:42:16.981420 140297692706624 submission_runner.py:590] Timing: 9.438956022262573
I0627 19:42:16.981487 140297692706624 submission_runner.py:591] ====================
I0627 19:42:16.981627 140297692706624 submission_runner.py:659] Final ogbg score: 9.438956022262573
