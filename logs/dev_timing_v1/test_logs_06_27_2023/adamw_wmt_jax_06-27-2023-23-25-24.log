python3 submission_runner.py --framework=jax --workload=wmt --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 2>&1 | tee -a /logs/wmt_jax_06-27-2023-23-25-24.log
2023-06-27 23:25:26.196557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 23:25:41.553458 140131018442560 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/wmt_jax because --overwrite was set.
I0627 23:25:41.561924 140131018442560 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/wmt_jax.
I0627 23:25:42.546760 140131018442560 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0627 23:25:42.547605 140131018442560 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0627 23:25:42.547747 140131018442560 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0627 23:25:42.552588 140131018442560 submission_runner.py:547] Using RNG seed 2373863351
I0627 23:25:44.914772 140131018442560 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 23:25:44.914977 140131018442560 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/wmt_jax/trial_1.
I0627 23:25:44.915215 140131018442560 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/wmt_jax/trial_1/hparams.json.
I0627 23:25:45.103626 140131018442560 submission_runner.py:249] Initializing dataset.
I0627 23:25:45.115200 140131018442560 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:25:45.119412 140131018442560 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0627 23:25:45.214154 140131018442560 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:25:47.106932 140131018442560 submission_runner.py:256] Initializing model.
I0627 23:25:56.264528 140131018442560 submission_runner.py:268] Initializing optimizer.
I0627 23:25:57.450463 140131018442560 submission_runner.py:275] Initializing metrics bundle.
I0627 23:25:57.450667 140131018442560 submission_runner.py:292] Initializing checkpoint and logger.
I0627 23:25:57.451817 140131018442560 checkpoints.py:915] Found no checkpoint files in /experiment_runs/test_today/adamw/wmt_jax/trial_1 with prefix checkpoint_
I0627 23:25:57.452089 140131018442560 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 23:25:57.452158 140131018442560 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 23:25:58.584235 140131018442560 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/wmt_jax/trial_1/meta_data_0.json.
I0627 23:25:58.585328 140131018442560 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/wmt_jax/trial_1/flags_0.json.
I0627 23:25:58.593291 140131018442560 submission_runner.py:328] Starting training loop.
I0627 23:26:42.007341 139966575277824 logging_writer.py:48] [0] global_step=0, grad_norm=5.460920333862305, loss=11.031963348388672
I0627 23:26:42.022024 140131018442560 spec.py:298] Evaluating on the training split.
I0627 23:26:42.025692 140131018442560 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:26:42.028797 140131018442560 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0627 23:26:42.065936 140131018442560 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0627 23:26:49.607466 140131018442560 workload.py:179] Translating evaluation dataset.
I0627 23:31:44.087650 140131018442560 spec.py:310] Evaluating on the validation split.
I0627 23:31:44.091769 140131018442560 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0627 23:31:44.095759 140131018442560 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0627 23:31:44.133630 140131018442560 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0627 23:31:51.305721 140131018442560 workload.py:179] Translating evaluation dataset.
I0627 23:36:39.070644 140131018442560 spec.py:326] Evaluating on the test split.
I0627 23:36:39.073485 140131018442560 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0627 23:36:39.076548 140131018442560 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0627 23:36:39.115777 140131018442560 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0627 23:36:45.854713 140131018442560 workload.py:179] Translating evaluation dataset.
I0627 23:41:30.010144 140131018442560 submission_runner.py:424] Time since start: 931.42s, 	Step: 1, 	{'train/accuracy': 0.0005839783698320389, 'train/loss': 11.028894424438477, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.024791717529297, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.032015800476074, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 43.428571462631226, 'total_duration': 931.416755437851, 'accumulated_submission_time': 43.428571462631226, 'accumulated_eval_time': 887.9880349636078, 'accumulated_logging_time': 0}
I0627 23:41:30.017992 139954351662848 logging_writer.py:48] [1] accumulated_eval_time=887.988035, accumulated_logging_time=0, accumulated_submission_time=43.428571, global_step=1, preemption_count=0, score=43.428571, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.032016, test/num_examples=3003, total_duration=931.416755, train/accuracy=0.000584, train/bleu=0.000000, train/loss=11.028894, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.024792, validation/num_examples=3000
I0627 23:41:32.850218 140131018442560 spec.py:298] Evaluating on the training split.
I0627 23:41:35.839745 140131018442560 workload.py:179] Translating evaluation dataset.
I0627 23:46:21.621757 140131018442560 spec.py:310] Evaluating on the validation split.
I0627 23:46:24.295413 140131018442560 workload.py:179] Translating evaluation dataset.
I0627 23:51:02.482635 140131018442560 spec.py:326] Evaluating on the test split.
I0627 23:51:05.185391 140131018442560 workload.py:179] Translating evaluation dataset.
I0627 23:55:48.862251 140131018442560 submission_runner.py:424] Time since start: 1790.27s, 	Step: 10, 	{'train/accuracy': 0.0006387007306329906, 'train/loss': 10.672099113464355, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.677258491516113, 'validation/bleu': 5.081142435153546e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.69917106628418, 'test/bleu': 7.718977587432967e-11, 'test/num_examples': 3003, 'score': 46.250096559524536, 'total_duration': 1790.268862247467, 'accumulated_submission_time': 46.250096559524536, 'accumulated_eval_time': 1743.999997138977, 'accumulated_logging_time': 0.01810765266418457}
I0627 23:55:48.870322 139954360055552 logging_writer.py:48] [10] accumulated_eval_time=1743.999997, accumulated_logging_time=0.018108, accumulated_submission_time=46.250097, global_step=10, preemption_count=0, score=46.250097, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.699171, test/num_examples=3003, total_duration=1790.268862, train/accuracy=0.000639, train/bleu=0.000000, train/loss=10.672099, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.677258, validation/num_examples=3000
I0627 23:55:48.885231 139954351662848 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=46.250097
I0627 23:55:50.142015 140131018442560 checkpoints.py:490] Saving checkpoint at step: 10
I0627 23:55:55.261560 140131018442560 checkpoints.py:422] Saved checkpoint at /experiment_runs/test_today/adamw/wmt_jax/trial_1/checkpoint_10
I0627 23:55:55.388458 140131018442560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/wmt_jax/trial_1/checkpoint_10.
I0627 23:55:55.487859 140131018442560 submission_runner.py:587] Tuning trial 1/1
I0627 23:55:55.488085 140131018442560 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 23:55:55.490256 140131018442560 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005839783698320389, 'train/loss': 11.028894424438477, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.024791717529297, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.032015800476074, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 43.428571462631226, 'total_duration': 931.416755437851, 'accumulated_submission_time': 43.428571462631226, 'accumulated_eval_time': 887.9880349636078, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.0006387007306329906, 'train/loss': 10.672099113464355, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.677258491516113, 'validation/bleu': 5.081142435153546e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.69917106628418, 'test/bleu': 7.718977587432967e-11, 'test/num_examples': 3003, 'score': 46.250096559524536, 'total_duration': 1790.268862247467, 'accumulated_submission_time': 46.250096559524536, 'accumulated_eval_time': 1743.999997138977, 'accumulated_logging_time': 0.01810765266418457, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0627 23:55:55.490433 140131018442560 submission_runner.py:590] Timing: 46.250096559524536
I0627 23:55:55.490480 140131018442560 submission_runner.py:591] ====================
I0627 23:55:55.490613 140131018442560 submission_runner.py:659] Final wmt score: 46.250096559524536
