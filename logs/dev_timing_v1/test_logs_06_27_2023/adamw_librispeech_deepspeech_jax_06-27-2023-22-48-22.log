python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_06-27-2023-22-48-22.log
2023-06-27 22:48:24.198422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 22:48:37.248928 140608388360000 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/librispeech_deepspeech_jax because --overwrite was set.
I0627 22:48:37.249734 140608388360000 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/librispeech_deepspeech_jax.
I0627 22:48:38.154721 140608388360000 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0627 22:48:38.155828 140608388360000 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0627 22:48:38.156216 140608388360000 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0627 22:48:38.160928 140608388360000 submission_runner.py:547] Using RNG seed 3322897948
I0627 22:48:40.521010 140608388360000 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 22:48:40.521229 140608388360000 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1.
I0627 22:48:40.521472 140608388360000 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0627 22:48:40.707058 140608388360000 submission_runner.py:249] Initializing dataset.
I0627 22:48:40.707283 140608388360000 submission_runner.py:256] Initializing model.
I0627 22:48:43.206191 140608388360000 submission_runner.py:268] Initializing optimizer.
I0627 22:48:43.927174 140608388360000 submission_runner.py:275] Initializing metrics bundle.
I0627 22:48:43.927366 140608388360000 submission_runner.py:292] Initializing checkpoint and logger.
I0627 22:48:43.928502 140608388360000 checkpoints.py:915] Found no checkpoint files in /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0627 22:48:43.928767 140608388360000 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 22:48:43.928894 140608388360000 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 22:48:44.883594 140608388360000 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0627 22:48:44.884674 140608388360000 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0627 22:48:44.891926 140608388360000 submission_runner.py:328] Starting training loop.
I0627 22:48:45.192564 140608388360000 input_pipeline.py:20] Loading split = train-clean-100
I0627 22:48:45.228162 140608388360000 input_pipeline.py:20] Loading split = train-clean-360
I0627 22:48:45.356204 140608388360000 input_pipeline.py:20] Loading split = train-other-500
2023-06-27 22:49:38.214310: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-06-27 22:49:40.583321: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0627 22:49:45.624871 140441782077184 logging_writer.py:48] [0] global_step=0, grad_norm=27.695953369140625, loss=32.851383209228516
I0627 22:49:45.646414 140608388360000 spec.py:298] Evaluating on the training split.
I0627 22:49:45.911478 140608388360000 input_pipeline.py:20] Loading split = train-clean-100
I0627 22:49:45.947240 140608388360000 input_pipeline.py:20] Loading split = train-clean-360
I0627 22:49:46.358004 140608388360000 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0627 22:51:14.978252 140608388360000 spec.py:310] Evaluating on the validation split.
I0627 22:51:15.184959 140608388360000 input_pipeline.py:20] Loading split = dev-clean
I0627 22:51:15.189552 140608388360000 input_pipeline.py:20] Loading split = dev-other
I0627 22:52:09.612018 140608388360000 spec.py:326] Evaluating on the test split.
I0627 22:52:09.840578 140608388360000 input_pipeline.py:20] Loading split = test-clean
I0627 22:52:45.850674 140608388360000 submission_runner.py:424] Time since start: 240.96s, 	Step: 1, 	{'train/ctc_loss': Array(31.981445, dtype=float32), 'train/wer': 2.0289809049830287, 'validation/ctc_loss': Array(30.803259, dtype=float32), 'validation/wer': 2.0813804281758626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.917955, dtype=float32), 'test/wer': 2.092559868380964, 'test/num_examples': 2472, 'score': 60.75427842140198, 'total_duration': 240.95636701583862, 'accumulated_submission_time': 60.75427842140198, 'accumulated_eval_time': 180.20189690589905, 'accumulated_logging_time': 0}
I0627 22:52:45.865166 140439550158592 logging_writer.py:48] [1] accumulated_eval_time=180.201897, accumulated_logging_time=0, accumulated_submission_time=60.754278, global_step=1, preemption_count=0, score=60.754278, test/ctc_loss=30.91795539855957, test/num_examples=2472, test/wer=2.092560, total_duration=240.956367, train/ctc_loss=31.9814453125, train/wer=2.028981, validation/ctc_loss=30.803258895874023, validation/num_examples=5348, validation/wer=2.081380
I0627 22:53:00.294347 140608388360000 spec.py:298] Evaluating on the training split.
I0627 22:54:17.130255 140608388360000 spec.py:310] Evaluating on the validation split.
I0627 22:55:08.721745 140608388360000 spec.py:326] Evaluating on the test split.
I0627 22:55:35.133230 140608388360000 submission_runner.py:424] Time since start: 410.24s, 	Step: 10, 	{'train/ctc_loss': Array(32.822548, dtype=float32), 'train/wer': 2.3374647278119913, 'validation/ctc_loss': Array(30.836378, dtype=float32), 'validation/wer': 2.2250383505870777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.954546, dtype=float32), 'test/wer': 2.2534478906424553, 'test/num_examples': 2472, 'score': 75.16856932640076, 'total_duration': 410.23855900764465, 'accumulated_submission_time': 75.16856932640076, 'accumulated_eval_time': 335.0380890369415, 'accumulated_logging_time': 0.028641700744628906}
I0627 22:55:35.148957 140448201156352 logging_writer.py:48] [10] accumulated_eval_time=335.038089, accumulated_logging_time=0.028642, accumulated_submission_time=75.168569, global_step=10, preemption_count=0, score=75.168569, test/ctc_loss=30.954545974731445, test/num_examples=2472, test/wer=2.253448, total_duration=410.238559, train/ctc_loss=32.822547912597656, train/wer=2.337465, validation/ctc_loss=30.83637809753418, validation/num_examples=5348, validation/wer=2.225038
I0627 22:55:35.165274 140448192763648 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=75.168569
I0627 22:55:35.324543 140608388360000 checkpoints.py:490] Saving checkpoint at step: 10
I0627 22:55:35.989958 140608388360000 checkpoints.py:422] Saved checkpoint at /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_10
I0627 22:55:36.007951 140608388360000 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_10.
I0627 22:55:37.121386 140608388360000 submission_runner.py:587] Tuning trial 1/1
I0627 22:55:37.121627 140608388360000 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 22:55:37.125336 140608388360000 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.981445, dtype=float32), 'train/wer': 2.0289809049830287, 'validation/ctc_loss': Array(30.803259, dtype=float32), 'validation/wer': 2.0813804281758626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.917955, dtype=float32), 'test/wer': 2.092559868380964, 'test/num_examples': 2472, 'score': 60.75427842140198, 'total_duration': 240.95636701583862, 'accumulated_submission_time': 60.75427842140198, 'accumulated_eval_time': 180.20189690589905, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/ctc_loss': Array(32.822548, dtype=float32), 'train/wer': 2.3374647278119913, 'validation/ctc_loss': Array(30.836378, dtype=float32), 'validation/wer': 2.2250383505870777, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.954546, dtype=float32), 'test/wer': 2.2534478906424553, 'test/num_examples': 2472, 'score': 75.16856932640076, 'total_duration': 410.23855900764465, 'accumulated_submission_time': 75.16856932640076, 'accumulated_eval_time': 335.0380890369415, 'accumulated_logging_time': 0.028641700744628906, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0627 22:55:37.125488 140608388360000 submission_runner.py:590] Timing: 75.16856932640076
I0627 22:55:37.125549 140608388360000 submission_runner.py:591] ====================
I0627 22:55:37.125973 140608388360000 submission_runner.py:659] Final librispeech_deepspeech score: 75.16856932640076
