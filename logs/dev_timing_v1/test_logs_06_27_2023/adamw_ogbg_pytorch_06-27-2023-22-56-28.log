torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 2>&1 | tee -a /logs/ogbg_pytorch_06-27-2023-22-56-28.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-27 22:56:33.412238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0627 22:56:46.238660 140206553261888 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0627 22:56:47.173208 140001803814720 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0627 22:56:47.175292 139854742710080 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0627 22:56:47.194674 140583656421184 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0627 22:56:47.228297 139830218024768 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0627 22:56:47.230646 140229532350272 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0627 22:56:47.235919 139869594580800 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0627 22:56:47.237295 140050230060864 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0627 22:56:47.237593 140050230060864 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.237608 139854742710080 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.238843 139830218024768 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.241356 140229532350272 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.246263 140206553261888 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.246294 140001803814720 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.246316 140583656421184 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:47.246611 139869594580800 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0627 22:56:48.766061 140050230060864 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/ogbg_pytorch because --overwrite was set.
I0627 22:56:48.773852 140050230060864 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/ogbg_pytorch.
W0627 22:56:48.794373 140206553261888 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:56:48.795039 139830218024768 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:56:48.795410 140583656421184 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:56:48.796036 140001803814720 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:56:48.796080 140229532350272 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:56:48.797684 139854742710080 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0627 22:56:48.804440 140050230060864 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 22:56:48.809637 140050230060864 submission_runner.py:547] Using RNG seed 413940024
I0627 22:56:48.811071 140050230060864 submission_runner.py:556] --- Tuning run 1/1 ---
I0627 22:56:48.811185 140050230060864 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1.
I0627 22:56:48.811459 140050230060864 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/hparams.json.
I0627 22:56:48.812251 140050230060864 submission_runner.py:249] Initializing dataset.
I0627 22:56:48.812370 140050230060864 submission_runner.py:256] Initializing model.
W0627 22:56:48.830973 139869594580800 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0627 22:56:52.942393 140050230060864 submission_runner.py:268] Initializing optimizer.
I0627 22:56:52.943222 140050230060864 submission_runner.py:275] Initializing metrics bundle.
I0627 22:56:52.943337 140050230060864 submission_runner.py:292] Initializing checkpoint and logger.
I0627 22:56:52.944130 140050230060864 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0627 22:56:52.944242 140050230060864 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0627 22:56:53.573637 140050230060864 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0627 22:56:53.574551 140050230060864 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0627 22:56:53.623477 140050230060864 submission_runner.py:328] Starting training loop.
I0627 22:56:53.872218 140050230060864 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:56:53.877307 140050230060864 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0627 22:56:53.942316 140050230060864 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0627 22:56:53.999729 140050230060864 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:56:59.437330 140011924084480 logging_writer.py:48] [0] global_step=0, grad_norm=2.634073, loss=0.760814
I0627 22:56:59.451678 140050230060864 submission.py:119] 0) loss = 0.761, grad_norm = 2.634
I0627 22:56:59.467325 140050230060864 spec.py:298] Evaluating on the training split.
I0627 22:56:59.472366 140050230060864 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:56:59.476664 140050230060864 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0627 22:56:59.544120 140050230060864 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:57:57.145861 140050230060864 spec.py:310] Evaluating on the validation split.
I0627 22:57:57.149558 140050230060864 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:57:57.154325 140050230060864 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0627 22:57:57.222662 140050230060864 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:58:42.196445 140050230060864 spec.py:326] Evaluating on the test split.
I0627 22:58:42.200362 140050230060864 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:58:42.205453 140050230060864 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0627 22:58:42.273569 140050230060864 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0627 22:59:31.306167 140050230060864 submission_runner.py:424] Time since start: 157.68s, 	Step: 1, 	{'train/accuracy': 0.4557461334945714, 'train/loss': 0.7612821378845298, 'train/mean_average_precision': 0.021553852252134298, 'validation/accuracy': 0.4520364827024609, 'validation/loss': 0.7639259986344155, 'validation/mean_average_precision': 0.025606994461078756, 'validation/num_examples': 43793, 'test/accuracy': 0.45070737541377115, 'test/loss': 0.7634835475038108, 'test/mean_average_precision': 0.027067815955202305, 'test/num_examples': 43793, 'score': 5.843135118484497, 'total_duration': 157.68286299705505, 'accumulated_submission_time': 5.843135118484497, 'accumulated_eval_time': 151.83837485313416, 'accumulated_logging_time': 0}
I0627 22:59:31.316090 139998023714560 logging_writer.py:48] [1] accumulated_eval_time=151.838375, accumulated_logging_time=0, accumulated_submission_time=5.843135, global_step=1, preemption_count=0, score=5.843135, test/accuracy=0.450707, test/loss=0.763484, test/mean_average_precision=0.027068, test/num_examples=43793, total_duration=157.682863, train/accuracy=0.455746, train/loss=0.761282, train/mean_average_precision=0.021554, validation/accuracy=0.452036, validation/loss=0.763926, validation/mean_average_precision=0.025607, validation/num_examples=43793
I0627 22:59:31.658181 140050230060864 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.670250 139854742710080 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.670653 139830218024768 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.670808 139869594580800 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.671015 140229532350272 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.671024 140583656421184 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.671024 140001803814720 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.671041 140206553261888 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0627 22:59:31.710027 139998032107264 logging_writer.py:48] [1] global_step=1, grad_norm=2.653250, loss=0.762257
I0627 22:59:31.715760 140050230060864 submission.py:119] 1) loss = 0.762, grad_norm = 2.653
I0627 22:59:32.079381 139998023714560 logging_writer.py:48] [2] global_step=2, grad_norm=2.642796, loss=0.760398
I0627 22:59:32.084681 140050230060864 submission.py:119] 2) loss = 0.760, grad_norm = 2.643
I0627 22:59:32.432138 139998032107264 logging_writer.py:48] [3] global_step=3, grad_norm=2.630403, loss=0.757952
I0627 22:59:32.437497 140050230060864 submission.py:119] 3) loss = 0.758, grad_norm = 2.630
I0627 22:59:32.781764 139998023714560 logging_writer.py:48] [4] global_step=4, grad_norm=2.662228, loss=0.757572
I0627 22:59:32.787173 140050230060864 submission.py:119] 4) loss = 0.758, grad_norm = 2.662
I0627 22:59:33.128376 139998032107264 logging_writer.py:48] [5] global_step=5, grad_norm=2.651538, loss=0.753430
I0627 22:59:33.133311 140050230060864 submission.py:119] 5) loss = 0.753, grad_norm = 2.652
I0627 22:59:33.466053 139998023714560 logging_writer.py:48] [6] global_step=6, grad_norm=2.621044, loss=0.748915
I0627 22:59:33.471285 140050230060864 submission.py:119] 6) loss = 0.749, grad_norm = 2.621
I0627 22:59:33.812614 139998032107264 logging_writer.py:48] [7] global_step=7, grad_norm=2.624359, loss=0.745893
I0627 22:59:33.817561 140050230060864 submission.py:119] 7) loss = 0.746, grad_norm = 2.624
I0627 22:59:34.150842 139998023714560 logging_writer.py:48] [8] global_step=8, grad_norm=2.608070, loss=0.739205
I0627 22:59:34.156233 140050230060864 submission.py:119] 8) loss = 0.739, grad_norm = 2.608
I0627 22:59:34.488847 139998032107264 logging_writer.py:48] [9] global_step=9, grad_norm=2.583109, loss=0.733729
I0627 22:59:34.493910 140050230060864 submission.py:119] 9) loss = 0.734, grad_norm = 2.583
I0627 22:59:34.494688 140050230060864 spec.py:298] Evaluating on the training split.
I0627 23:00:38.935235 140050230060864 spec.py:310] Evaluating on the validation split.
I0627 23:00:42.544781 140050230060864 spec.py:326] Evaluating on the test split.
I0627 23:00:46.645438 140050230060864 submission_runner.py:424] Time since start: 233.02s, 	Step: 10, 	{'train/accuracy': 0.4804612088147615, 'train/loss': 0.7266717459025492, 'train/mean_average_precision': 0.022302747100053238, 'validation/accuracy': 0.47558936500320287, 'validation/loss': 0.7297720764759801, 'validation/mean_average_precision': 0.02505778488624171, 'validation/num_examples': 43793, 'test/accuracy': 0.4751236101101887, 'test/loss': 0.72945459710833, 'test/mean_average_precision': 0.027978533817471632, 'test/num_examples': 43793, 'score': 9.003802299499512, 'total_duration': 233.02224802970886, 'accumulated_submission_time': 9.003802299499512, 'accumulated_eval_time': 223.98872709274292, 'accumulated_logging_time': 0.024242877960205078}
I0627 23:00:46.654163 139998023714560 logging_writer.py:48] [10] accumulated_eval_time=223.988727, accumulated_logging_time=0.024243, accumulated_submission_time=9.003802, global_step=10, preemption_count=0, score=9.003802, test/accuracy=0.475124, test/loss=0.729455, test/mean_average_precision=0.027979, test/num_examples=43793, total_duration=233.022248, train/accuracy=0.480461, train/loss=0.726672, train/mean_average_precision=0.022303, validation/accuracy=0.475589, validation/loss=0.729772, validation/mean_average_precision=0.025058, validation/num_examples=43793
I0627 23:00:46.673074 139998032107264 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=9.003802
I0627 23:00:46.768270 140050230060864 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/checkpoint_10.
I0627 23:00:46.878198 140050230060864 submission_runner.py:587] Tuning trial 1/1
I0627 23:00:46.878416 140050230060864 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0627 23:00:46.879220 140050230060864 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4557461334945714, 'train/loss': 0.7612821378845298, 'train/mean_average_precision': 0.021553852252134298, 'validation/accuracy': 0.4520364827024609, 'validation/loss': 0.7639259986344155, 'validation/mean_average_precision': 0.025606994461078756, 'validation/num_examples': 43793, 'test/accuracy': 0.45070737541377115, 'test/loss': 0.7634835475038108, 'test/mean_average_precision': 0.027067815955202305, 'test/num_examples': 43793, 'score': 5.843135118484497, 'total_duration': 157.68286299705505, 'accumulated_submission_time': 5.843135118484497, 'accumulated_eval_time': 151.83837485313416, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.4804612088147615, 'train/loss': 0.7266717459025492, 'train/mean_average_precision': 0.022302747100053238, 'validation/accuracy': 0.47558936500320287, 'validation/loss': 0.7297720764759801, 'validation/mean_average_precision': 0.02505778488624171, 'validation/num_examples': 43793, 'test/accuracy': 0.4751236101101887, 'test/loss': 0.72945459710833, 'test/mean_average_precision': 0.027978533817471632, 'test/num_examples': 43793, 'score': 9.003802299499512, 'total_duration': 233.02224802970886, 'accumulated_submission_time': 9.003802299499512, 'accumulated_eval_time': 223.98872709274292, 'accumulated_logging_time': 0.024242877960205078, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0627 23:00:46.879331 140050230060864 submission_runner.py:590] Timing: 9.003802299499512
I0627 23:00:46.879384 140050230060864 submission_runner.py:591] ====================
I0627 23:00:46.879490 140050230060864 submission_runner.py:659] Final ogbg score: 9.003802299499512
