torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 2>&1 | tee -a /logs/ogbg_pytorch_06-29-2023-20-51-27.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-29 20:51:32.175239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 20:51:32.175239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0629 20:51:45.028059 139889650632512 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0629 20:51:45.050729 140115431098176 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0629 20:51:45.052014 140710857213760 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0629 20:51:45.052198 139664951904064 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0629 20:51:45.052499 140316709062464 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0629 20:51:46.024227 139687706572608 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0629 20:51:46.024254 140704843478848 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0629 20:51:46.031908 140390608717632 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0629 20:51:46.032253 140390608717632 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.034787 139687706572608 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.034804 140704843478848 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.040592 139889650632512 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.040637 140115431098176 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.040714 139664951904064 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.040745 140316709062464 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:46.041002 140710857213760 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 20:51:47.809031 140390608717632 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/ogbg_pytorch because --overwrite was set.
I0629 20:51:47.813844 140390608717632 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/ogbg_pytorch.
W0629 20:51:47.837075 140316709062464 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 20:51:47.838482 139687706572608 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 20:51:47.838521 139664951904064 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 20:51:47.839482 139889650632512 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 20:51:47.839529 140704843478848 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 20:51:47.840499 140115431098176 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 20:51:47.840574 140390608717632 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0629 20:51:47.845251 140390608717632 submission_runner.py:547] Using RNG seed 1188556336
I0629 20:51:47.846583 140390608717632 submission_runner.py:556] --- Tuning run 1/1 ---
I0629 20:51:47.846696 140390608717632 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1.
I0629 20:51:47.846960 140390608717632 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/hparams.json.
I0629 20:51:47.847760 140390608717632 submission_runner.py:249] Initializing dataset.
I0629 20:51:47.847873 140390608717632 submission_runner.py:256] Initializing model.
W0629 20:51:47.870655 140710857213760 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0629 20:51:51.922426 140390608717632 submission_runner.py:268] Initializing optimizer.
I0629 20:51:51.923303 140390608717632 submission_runner.py:275] Initializing metrics bundle.
I0629 20:51:51.923404 140390608717632 submission_runner.py:292] Initializing checkpoint and logger.
I0629 20:51:51.924983 140390608717632 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0629 20:51:51.925083 140390608717632 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0629 20:51:52.453499 140390608717632 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0629 20:51:52.454359 140390608717632 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0629 20:51:52.500776 140390608717632 submission_runner.py:328] Starting training loop.
I0629 20:51:53.072640 140390608717632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:51:53.077290 140390608717632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0629 20:51:53.157981 140390608717632 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0629 20:51:53.216651 140390608717632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:51:58.495406 140352770004736 logging_writer.py:48] [0] global_step=0, grad_norm=2.266524, loss=0.694482
I0629 20:51:58.508221 140390608717632 submission.py:119] 0) loss = 0.694, grad_norm = 2.267
I0629 20:51:58.525335 140390608717632 spec.py:298] Evaluating on the training split.
I0629 20:51:58.529907 140390608717632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:51:58.533925 140390608717632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0629 20:51:58.598885 140390608717632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:52:54.038909 140390608717632 spec.py:310] Evaluating on the validation split.
I0629 20:52:54.042312 140390608717632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:52:54.046917 140390608717632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0629 20:52:54.109459 140390608717632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:53:38.544227 140390608717632 spec.py:326] Evaluating on the test split.
I0629 20:53:38.547531 140390608717632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:53:38.551959 140390608717632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0629 20:53:38.613562 140390608717632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0629 20:54:24.488051 140390608717632 submission_runner.py:424] Time since start: 151.99s, 	Step: 1, 	{'train/accuracy': 0.5790898547059703, 'train/loss': 0.6946161169568391, 'train/mean_average_precision': 0.021523878010565274, 'validation/accuracy': 0.5830717045531121, 'validation/loss': 0.6950838348730664, 'validation/mean_average_precision': 0.02509589623801461, 'validation/num_examples': 43793, 'test/accuracy': 0.5837240265032544, 'test/loss': 0.6948744608181538, 'test/mean_average_precision': 0.025998408034251196, 'test/num_examples': 43793, 'score': 6.023759841918945, 'total_duration': 151.98752069473267, 'accumulated_submission_time': 6.023759841918945, 'accumulated_eval_time': 145.96247172355652, 'accumulated_logging_time': 0}
I0629 20:54:24.495343 140338399872768 logging_writer.py:48] [1] accumulated_eval_time=145.962472, accumulated_logging_time=0, accumulated_submission_time=6.023760, global_step=1, preemption_count=0, score=6.023760, test/accuracy=0.583724, test/loss=0.694874, test/mean_average_precision=0.025998, test/num_examples=43793, total_duration=151.987521, train/accuracy=0.579090, train/loss=0.694616, train/mean_average_precision=0.021524, validation/accuracy=0.583072, validation/loss=0.695084, validation/mean_average_precision=0.025096, validation/num_examples=43793
I0629 20:54:24.795538 140390608717632 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.803884 139889650632512 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.803891 140710857213760 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.804107 139687706572608 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.804321 140704843478848 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.804366 140316709062464 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.804562 140115431098176 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.804680 139664951904064 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 20:54:24.836680 140338408265472 logging_writer.py:48] [1] global_step=1, grad_norm=2.250466, loss=0.693827
I0629 20:54:24.841193 140390608717632 submission.py:119] 1) loss = 0.694, grad_norm = 2.250
I0629 20:54:25.184345 140338399872768 logging_writer.py:48] [2] global_step=2, grad_norm=2.237254, loss=0.694634
I0629 20:54:25.188780 140390608717632 submission.py:119] 2) loss = 0.695, grad_norm = 2.237
I0629 20:54:25.521866 140338408265472 logging_writer.py:48] [3] global_step=3, grad_norm=2.253313, loss=0.693971
I0629 20:54:25.526071 140390608717632 submission.py:119] 3) loss = 0.694, grad_norm = 2.253
I0629 20:54:25.851197 140338399872768 logging_writer.py:48] [4] global_step=4, grad_norm=2.232378, loss=0.690382
I0629 20:54:25.855430 140390608717632 submission.py:119] 4) loss = 0.690, grad_norm = 2.232
I0629 20:54:26.186548 140338408265472 logging_writer.py:48] [5] global_step=5, grad_norm=2.209376, loss=0.687894
I0629 20:54:26.190983 140390608717632 submission.py:119] 5) loss = 0.688, grad_norm = 2.209
I0629 20:54:26.515490 140338399872768 logging_writer.py:48] [6] global_step=6, grad_norm=2.210347, loss=0.685014
I0629 20:54:26.519983 140390608717632 submission.py:119] 6) loss = 0.685, grad_norm = 2.210
I0629 20:54:26.849019 140338408265472 logging_writer.py:48] [7] global_step=7, grad_norm=2.166474, loss=0.680770
I0629 20:54:26.853338 140390608717632 submission.py:119] 7) loss = 0.681, grad_norm = 2.166
I0629 20:54:27.183481 140338399872768 logging_writer.py:48] [8] global_step=8, grad_norm=2.112120, loss=0.676677
I0629 20:54:27.187885 140390608717632 submission.py:119] 8) loss = 0.677, grad_norm = 2.112
I0629 20:54:27.511879 140338408265472 logging_writer.py:48] [9] global_step=9, grad_norm=2.078998, loss=0.672277
I0629 20:54:27.516007 140390608717632 submission.py:119] 9) loss = 0.672, grad_norm = 2.079
I0629 20:54:27.516785 140390608717632 spec.py:298] Evaluating on the training split.
I0629 20:55:26.980287 140390608717632 spec.py:310] Evaluating on the validation split.
I0629 20:55:30.438982 140390608717632 spec.py:326] Evaluating on the test split.
I0629 20:55:33.815172 140390608717632 submission_runner.py:424] Time since start: 221.31s, 	Step: 10, 	{'train/accuracy': 0.6170104441495221, 'train/loss': 0.6664588625644109, 'train/mean_average_precision': 0.020972689349385475, 'validation/accuracy': 0.6126116032465513, 'validation/loss': 0.6680159120635021, 'validation/mean_average_precision': 0.024825340463950633, 'validation/num_examples': 43793, 'test/accuracy': 0.6118330856006594, 'test/loss': 0.6680527306262027, 'test/mean_average_precision': 0.025622676195709865, 'test/num_examples': 43793, 'score': 9.029508829116821, 'total_duration': 221.3147165775299, 'accumulated_submission_time': 9.029508829116821, 'accumulated_eval_time': 212.2605791091919, 'accumulated_logging_time': 0.01973104476928711}
I0629 20:55:33.822928 140338399872768 logging_writer.py:48] [10] accumulated_eval_time=212.260579, accumulated_logging_time=0.019731, accumulated_submission_time=9.029509, global_step=10, preemption_count=0, score=9.029509, test/accuracy=0.611833, test/loss=0.668053, test/mean_average_precision=0.025623, test/num_examples=43793, total_duration=221.314717, train/accuracy=0.617010, train/loss=0.666459, train/mean_average_precision=0.020973, validation/accuracy=0.612612, validation/loss=0.668016, validation/mean_average_precision=0.024825, validation/num_examples=43793
I0629 20:55:33.841714 140338408265472 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=9.029509
I0629 20:55:33.931502 140390608717632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/ogbg_pytorch/trial_1/checkpoint_10.
I0629 20:55:34.026095 140390608717632 submission_runner.py:587] Tuning trial 1/1
I0629 20:55:34.026369 140390608717632 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0629 20:55:34.027038 140390608717632 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5790898547059703, 'train/loss': 0.6946161169568391, 'train/mean_average_precision': 0.021523878010565274, 'validation/accuracy': 0.5830717045531121, 'validation/loss': 0.6950838348730664, 'validation/mean_average_precision': 0.02509589623801461, 'validation/num_examples': 43793, 'test/accuracy': 0.5837240265032544, 'test/loss': 0.6948744608181538, 'test/mean_average_precision': 0.025998408034251196, 'test/num_examples': 43793, 'score': 6.023759841918945, 'total_duration': 151.98752069473267, 'accumulated_submission_time': 6.023759841918945, 'accumulated_eval_time': 145.96247172355652, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.6170104441495221, 'train/loss': 0.6664588625644109, 'train/mean_average_precision': 0.020972689349385475, 'validation/accuracy': 0.6126116032465513, 'validation/loss': 0.6680159120635021, 'validation/mean_average_precision': 0.024825340463950633, 'validation/num_examples': 43793, 'test/accuracy': 0.6118330856006594, 'test/loss': 0.6680527306262027, 'test/mean_average_precision': 0.025622676195709865, 'test/num_examples': 43793, 'score': 9.029508829116821, 'total_duration': 221.3147165775299, 'accumulated_submission_time': 9.029508829116821, 'accumulated_eval_time': 212.2605791091919, 'accumulated_logging_time': 0.01973104476928711, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0629 20:55:34.027163 140390608717632 submission_runner.py:590] Timing: 9.029508829116821
I0629 20:55:34.027228 140390608717632 submission_runner.py:591] ====================
I0629 20:55:34.027341 140390608717632 submission_runner.py:659] Final ogbg score: 9.029508829116821
