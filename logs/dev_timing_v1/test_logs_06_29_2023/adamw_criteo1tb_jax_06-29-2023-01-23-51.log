python3 submission_runner.py --framework=jax --workload=criteo1tb --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 2>&1 | tee -a /logs/criteo1tb_jax_06-29-2023-01-23-51.log
2023-06-29 01:23:57.009246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0629 01:24:15.310758 140359018993472 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/criteo1tb_jax because --overwrite was set.
I0629 01:24:15.332682 140359018993472 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/criteo1tb_jax.
I0629 01:24:16.987231 140359018993472 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0629 01:24:16.988662 140359018993472 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0629 01:24:16.988821 140359018993472 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0629 01:24:16.993781 140359018993472 submission_runner.py:547] Using RNG seed 3034545734
I0629 01:24:23.205271 140359018993472 submission_runner.py:556] --- Tuning run 1/1 ---
I0629 01:24:23.205463 140359018993472 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1.
I0629 01:24:23.205660 140359018993472 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1/hparams.json.
I0629 01:24:23.386828 140359018993472 submission_runner.py:249] Initializing dataset.
I0629 01:24:23.387028 140359018993472 submission_runner.py:256] Initializing model.
I0629 01:24:28.614862 140359018993472 submission_runner.py:268] Initializing optimizer.
I0629 01:24:31.484933 140359018993472 submission_runner.py:275] Initializing metrics bundle.
I0629 01:24:31.485133 140359018993472 submission_runner.py:292] Initializing checkpoint and logger.
I0629 01:24:31.486290 140359018993472 checkpoints.py:915] Found no checkpoint files in /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1 with prefix checkpoint_
I0629 01:24:31.486614 140359018993472 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0629 01:24:31.486687 140359018993472 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0629 01:24:32.094192 140359018993472 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1/meta_data_0.json.
I0629 01:24:32.095151 140359018993472 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1/flags_0.json.
I0629 01:24:32.188850 140359018993472 submission_runner.py:328] Starting training loop.
I0629 01:24:58.824853 140194519959296 logging_writer.py:48] [0] global_step=0, grad_norm=13.520051002502441, loss=1.4844435453414917
I0629 01:24:58.835134 140359018993472 spec.py:298] Evaluating on the training split.
I0629 01:28:59.608826 140359018993472 spec.py:310] Evaluating on the validation split.
I0629 01:32:58.070630 140359018993472 spec.py:326] Evaluating on the test split.
I0629 01:36:56.648326 140359018993472 submission_runner.py:424] Time since start: 744.46s, 	Step: 1, 	{'train/loss': 1.484296282599954, 'validation/loss': 1.4732571685393259, 'validation/num_examples': 89000000, 'test/loss': 1.4828940945455762, 'test/num_examples': 89274637, 'score': 26.64605689048767, 'total_duration': 744.4593760967255, 'accumulated_submission_time': 26.64605689048767, 'accumulated_eval_time': 717.8131065368652, 'accumulated_logging_time': 0}
I0629 01:36:56.665649 140174956676864 logging_writer.py:48] [1] accumulated_eval_time=717.813107, accumulated_logging_time=0, accumulated_submission_time=26.646057, global_step=1, preemption_count=0, score=26.646057, test/loss=1.482894, test/num_examples=89274637, total_duration=744.459376, train/loss=1.484296, validation/loss=1.473257, validation/num_examples=89000000
I0629 01:36:57.460332 140359018993472 spec.py:298] Evaluating on the training split.
I0629 01:40:17.010113 140359018993472 spec.py:310] Evaluating on the validation split.
I0629 01:42:54.485184 140359018993472 spec.py:326] Evaluating on the test split.
I0629 01:45:22.489362 140359018993472 submission_runner.py:424] Time since start: 1250.30s, 	Step: 10, 	{'train/loss': 0.8495989631204044, 'validation/loss': 0.8572843146067416, 'validation/num_examples': 89000000, 'test/loss': 0.8562663996046268, 'test/num_examples': 89274637, 'score': 27.432461738586426, 'total_duration': 1250.3004276752472, 'accumulated_submission_time': 27.432461738586426, 'accumulated_eval_time': 1222.8420805931091, 'accumulated_logging_time': 0.025154829025268555}
I0629 01:45:22.497690 140174948284160 logging_writer.py:48] [10] accumulated_eval_time=1222.842081, accumulated_logging_time=0.025155, accumulated_submission_time=27.432462, global_step=10, preemption_count=0, score=27.432462, test/loss=0.856266, test/num_examples=89274637, total_duration=1250.300428, train/loss=0.849599, validation/loss=0.857284, validation/num_examples=89000000
I0629 01:45:22.510227 140174956676864 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=27.432462
I0629 01:45:25.942527 140359018993472 checkpoints.py:490] Saving checkpoint at step: 10
I0629 01:45:50.488564 140359018993472 checkpoints.py:422] Saved checkpoint at /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1/checkpoint_10
I0629 01:45:50.505981 140359018993472 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/criteo1tb_jax/trial_1/checkpoint_10.
I0629 01:45:50.551890 140359018993472 submission_runner.py:587] Tuning trial 1/1
I0629 01:45:50.552069 140359018993472 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0629 01:45:50.552476 140359018993472 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/loss': 1.484296282599954, 'validation/loss': 1.4732571685393259, 'validation/num_examples': 89000000, 'test/loss': 1.4828940945455762, 'test/num_examples': 89274637, 'score': 26.64605689048767, 'total_duration': 744.4593760967255, 'accumulated_submission_time': 26.64605689048767, 'accumulated_eval_time': 717.8131065368652, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/loss': 0.8495989631204044, 'validation/loss': 0.8572843146067416, 'validation/num_examples': 89000000, 'test/loss': 0.8562663996046268, 'test/num_examples': 89274637, 'score': 27.432461738586426, 'total_duration': 1250.3004276752472, 'accumulated_submission_time': 27.432461738586426, 'accumulated_eval_time': 1222.8420805931091, 'accumulated_logging_time': 0.025154829025268555, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0629 01:45:50.552567 140359018993472 submission_runner.py:590] Timing: 27.432461738586426
I0629 01:45:50.552623 140359018993472 submission_runner.py:591] ====================
I0629 01:45:50.552735 140359018993472 submission_runner.py:659] Final criteo1tb score: 27.432461738586426
