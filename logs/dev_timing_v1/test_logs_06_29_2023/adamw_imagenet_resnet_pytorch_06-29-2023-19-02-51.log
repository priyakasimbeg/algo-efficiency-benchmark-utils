torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=test_today/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_resnet_pytorch_06-29-2023-19-02-51.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-29 19:02:55.803869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.803869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.803869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.818311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.821407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.840162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.846287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-06-29 19:02:55.846819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0629 19:03:08.683817 139869660591936 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0629 19:03:09.622510 139843798284096 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0629 19:03:09.623445 140662413268800 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0629 19:03:09.673096 139635318024000 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0629 19:03:09.675937 139814030886720 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0629 19:03:09.680676 139664331335488 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0629 19:03:09.680718 139895043929920 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0629 19:03:09.686565 140115251844928 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0629 19:03:09.687021 140115251844928 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.691238 139664331335488 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.691220 139895043929920 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.694292 139635318024000 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.695410 139869660591936 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.695526 139843798284096 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.696073 140662413268800 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0629 19:03:09.696676 139814030886720 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0629 19:03:10.830071 140115251844928 logger_utils.py:61] Removing existing experiment directory /experiment_runs/test_today/adamw/imagenet_resnet_pytorch because --overwrite was set.
I0629 19:03:10.832320 140115251844928 logger_utils.py:76] Creating experiment directory at /experiment_runs/test_today/adamw/imagenet_resnet_pytorch.
W0629 19:03:10.856436 139843798284096 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.857051 140115251844928 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.858268 139869660591936 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.858763 139635318024000 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.859158 140662413268800 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.859380 139895043929920 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.859404 139664331335488 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0629 19:03:10.860368 139814030886720 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0629 19:03:10.862531 140115251844928 submission_runner.py:547] Using RNG seed 485725397
I0629 19:03:10.864003 140115251844928 submission_runner.py:556] --- Tuning run 1/1 ---
I0629 19:03:10.864123 140115251844928 submission_runner.py:561] Creating tuning directory at /experiment_runs/test_today/adamw/imagenet_resnet_pytorch/trial_1.
I0629 19:03:10.864386 140115251844928 logger_utils.py:92] Saving hparams to /experiment_runs/test_today/adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0629 19:03:10.865173 140115251844928 submission_runner.py:249] Initializing dataset.
I0629 19:03:17.356465 140115251844928 submission_runner.py:256] Initializing model.
I0629 19:03:21.731206 140115251844928 submission_runner.py:268] Initializing optimizer.
I0629 19:03:21.732450 140115251844928 submission_runner.py:275] Initializing metrics bundle.
I0629 19:03:21.732559 140115251844928 submission_runner.py:292] Initializing checkpoint and logger.
I0629 19:03:22.315336 140115251844928 submission_runner.py:313] Saving meta data to /experiment_runs/test_today/adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0629 19:03:22.316211 140115251844928 submission_runner.py:316] Saving flags to /experiment_runs/test_today/adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0629 19:03:22.359737 140115251844928 submission_runner.py:328] Starting training loop.
I0629 19:03:35.279237 140087320893184 logging_writer.py:48] [0] global_step=0, grad_norm=0.600055, loss=6.922347
I0629 19:03:35.650056 140115251844928 submission.py:119] 0) loss = 6.922, grad_norm = 0.600
I0629 19:03:35.651568 140115251844928 spec.py:298] Evaluating on the training split.
I0629 19:04:36.119535 140115251844928 spec.py:310] Evaluating on the validation split.
I0629 19:05:31.504159 140115251844928 spec.py:326] Evaluating on the test split.
I0629 19:05:31.520289 140115251844928 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0629 19:05:31.526398 140115251844928 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0629 19:05:31.594694 140115251844928 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
2023-06-29 19:05:32.476460: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:32.476382: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:32.476538: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:32.476690: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:32.475784: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:32.476276: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:32.477174: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.564471: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.580262: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.596045: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.681473: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.713681: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.929735: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:33.933307: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:34.687565: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:34.794141: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:34.810940: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:34.977754: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:35.010426: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:35.121970: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:35.187142: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.004043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.359191: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.367079: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.374213: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.383550: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.384362: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.390812: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:37.636267: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:38.374965: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:38.397380: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:38.432728: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:38.438093: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:38.579032: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
2023-06-29 19:05:38.703206: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 616562688 exceeds 10% of free system memory.
I0629 19:05:47.026749 140115251844928 submission_runner.py:424] Time since start: 144.67s, 	Step: 1, 	{'train/accuracy': 0.0012755102040816326, 'train/loss': 6.9248675910794, 'validation/accuracy': 0.0019, 'validation/loss': 6.924294375, 'validation/num_examples': 50000, 'test/accuracy': 0.0012, 'test/loss': 6.92830546875, 'test/num_examples': 10000, 'score': 13.290755033493042, 'total_duration': 144.66733765602112, 'accumulated_submission_time': 13.290755033493042, 'accumulated_eval_time': 131.37507319450378, 'accumulated_logging_time': 0}
I0629 19:05:47.035072 140071934916352 logging_writer.py:48] [1] accumulated_eval_time=131.375073, accumulated_logging_time=0, accumulated_submission_time=13.290755, global_step=1, preemption_count=0, score=13.290755, test/accuracy=0.001200, test/loss=6.928305, test/num_examples=10000, total_duration=144.667338, train/accuracy=0.001276, train/loss=6.924868, validation/accuracy=0.001900, validation/loss=6.924294, validation/num_examples=50000
I0629 19:05:47.075111 140115251844928 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.075292 139664331335488 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.076000 139843798284096 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.076017 140662413268800 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.076086 139895043929920 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.076848 139869660591936 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.076920 139635318024000 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0629 19:05:47.076957 139814030886720 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 1, 1]
bucket_view.sizes() = [512, 2048, 1, 1], strides() = [2048, 1, 2048, 2048] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I0629 19:05:47.573079 140071926523648 logging_writer.py:48] [1] global_step=1, grad_norm=0.617651, loss=6.919925
I0629 19:05:47.576598 140115251844928 submission.py:119] 1) loss = 6.920, grad_norm = 0.618
I0629 19:05:47.945851 140071934916352 logging_writer.py:48] [2] global_step=2, grad_norm=0.612242, loss=6.927114
I0629 19:05:47.949680 140115251844928 submission.py:119] 2) loss = 6.927, grad_norm = 0.612
I0629 19:05:48.320095 140071926523648 logging_writer.py:48] [3] global_step=3, grad_norm=0.610789, loss=6.924720
I0629 19:05:48.323896 140115251844928 submission.py:119] 3) loss = 6.925, grad_norm = 0.611
I0629 19:05:48.694938 140071934916352 logging_writer.py:48] [4] global_step=4, grad_norm=0.606696, loss=6.923808
I0629 19:05:48.701096 140115251844928 submission.py:119] 4) loss = 6.924, grad_norm = 0.607
I0629 19:05:49.081449 140071926523648 logging_writer.py:48] [5] global_step=5, grad_norm=0.605832, loss=6.936311
I0629 19:05:49.085412 140115251844928 submission.py:119] 5) loss = 6.936, grad_norm = 0.606
I0629 19:05:49.459299 140071934916352 logging_writer.py:48] [6] global_step=6, grad_norm=0.620595, loss=6.936229
I0629 19:05:49.464421 140115251844928 submission.py:119] 6) loss = 6.936, grad_norm = 0.621
I0629 19:05:49.844216 140071926523648 logging_writer.py:48] [7] global_step=7, grad_norm=0.619825, loss=6.938209
I0629 19:05:49.848722 140115251844928 submission.py:119] 7) loss = 6.938, grad_norm = 0.620
I0629 19:05:50.220973 140071934916352 logging_writer.py:48] [8] global_step=8, grad_norm=0.615898, loss=6.923210
I0629 19:05:50.225908 140115251844928 submission.py:119] 8) loss = 6.923, grad_norm = 0.616
I0629 19:05:50.600906 140071926523648 logging_writer.py:48] [9] global_step=9, grad_norm=0.632176, loss=6.935060
I0629 19:05:50.605515 140115251844928 submission.py:119] 9) loss = 6.935, grad_norm = 0.632
I0629 19:05:50.607445 140115251844928 spec.py:298] Evaluating on the training split.
I0629 19:06:42.367809 140115251844928 spec.py:310] Evaluating on the validation split.
I0629 19:07:37.843878 140115251844928 spec.py:326] Evaluating on the test split.
I0629 19:07:39.173883 140115251844928 submission_runner.py:424] Time since start: 256.81s, 	Step: 10, 	{'train/accuracy': 0.0013352997448979591, 'train/loss': 6.914421860052615, 'validation/accuracy': 0.00118, 'validation/loss': 6.9139775, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.9169171875, 'test/num_examples': 10000, 'score': 16.84600853919983, 'total_duration': 256.8145999908447, 'accumulated_submission_time': 16.84600853919983, 'accumulated_eval_time': 239.94173049926758, 'accumulated_logging_time': 0.01790904998779297}
I0629 19:07:39.181273 140071943309056 logging_writer.py:48] [10] accumulated_eval_time=239.941730, accumulated_logging_time=0.017909, accumulated_submission_time=16.846009, global_step=10, preemption_count=0, score=16.846009, test/accuracy=0.001000, test/loss=6.916917, test/num_examples=10000, total_duration=256.814600, train/accuracy=0.001335, train/loss=6.914422, validation/accuracy=0.001180, validation/loss=6.913977, validation/num_examples=50000
I0629 19:07:39.196735 140071951701760 logging_writer.py:48] [10] global_step=10, preemption_count=0, score=16.846009
I0629 19:07:39.729999 140115251844928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/test_today/adamw/imagenet_resnet_pytorch/trial_1/checkpoint_10.
I0629 19:07:40.046885 140115251844928 submission_runner.py:587] Tuning trial 1/1
I0629 19:07:40.047109 140115251844928 submission_runner.py:588] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0629 19:07:40.047695 140115251844928 submission_runner.py:589] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012755102040816326, 'train/loss': 6.9248675910794, 'validation/accuracy': 0.0019, 'validation/loss': 6.924294375, 'validation/num_examples': 50000, 'test/accuracy': 0.0012, 'test/loss': 6.92830546875, 'test/num_examples': 10000, 'score': 13.290755033493042, 'total_duration': 144.66733765602112, 'accumulated_submission_time': 13.290755033493042, 'accumulated_eval_time': 131.37507319450378, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (10, {'train/accuracy': 0.0013352997448979591, 'train/loss': 6.914421860052615, 'validation/accuracy': 0.00118, 'validation/loss': 6.9139775, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.9169171875, 'test/num_examples': 10000, 'score': 16.84600853919983, 'total_duration': 256.8145999908447, 'accumulated_submission_time': 16.84600853919983, 'accumulated_eval_time': 239.94173049926758, 'accumulated_logging_time': 0.01790904998779297, 'global_step': 10, 'preemption_count': 0})], 'global_step': 10}
I0629 19:07:40.047832 140115251844928 submission_runner.py:590] Timing: 16.84600853919983
I0629 19:07:40.047881 140115251844928 submission_runner.py:591] ====================
I0629 19:07:40.047966 140115251844928 submission_runner.py:659] Final imagenet_resnet score: 16.84600853919983
