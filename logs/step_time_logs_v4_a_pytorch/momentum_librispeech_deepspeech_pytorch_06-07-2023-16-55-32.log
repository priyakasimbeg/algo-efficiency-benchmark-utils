torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_06-07-2023-16-55-32.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 16:55:55.544582 140417234941760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 16:55:55.544604 140479323133760 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 16:55:55.544631 140639288481600 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 16:55:55.544668 140545111435072 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 16:55:55.545523 140040507991872 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 16:55:55.545525 140699356038976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 16:55:55.545840 140699356038976 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.545669 140539730544448 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 16:55:55.545859 140040507991872 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.545735 139705302714176 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 16:55:55.546044 139705302714176 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.546088 140539730544448 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.555149 140417234941760 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.555245 140639288481600 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.555276 140479323133760 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.555428 140545111435072 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 16:55:55.912011 140040507991872 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch because --overwrite was set.
I0607 16:55:55.927418 140040507991872 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch.
W0607 16:55:56.226659 140417234941760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.226942 140699356038976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.227681 140545111435072 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.227776 140479323133760 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.228337 140639288481600 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.228368 139705302714176 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.236923 140539730544448 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 16:55:56.261343 140040507991872 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 16:55:56.266607 140040507991872 submission_runner.py:541] Using RNG seed 4031652944
I0607 16:55:56.268013 140040507991872 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 16:55:56.268143 140040507991872 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch/trial_1.
I0607 16:55:56.269660 140040507991872 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0607 16:55:56.270752 140040507991872 submission_runner.py:255] Initializing dataset.
I0607 16:55:56.270884 140040507991872 input_pipeline.py:20] Loading split = train-clean-100
I0607 16:55:56.304974 140040507991872 input_pipeline.py:20] Loading split = train-clean-360
I0607 16:55:56.660066 140040507991872 input_pipeline.py:20] Loading split = train-other-500
I0607 16:55:57.101044 140040507991872 submission_runner.py:262] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0607 16:56:04.824174 140040507991872 submission_runner.py:272] Initializing optimizer.
I0607 16:56:05.289082 140040507991872 submission_runner.py:279] Initializing metrics bundle.
I0607 16:56:05.289314 140040507991872 submission_runner.py:297] Initializing checkpoint and logger.
I0607 16:56:05.290702 140040507991872 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 16:56:05.290841 140040507991872 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 16:56:05.914538 140040507991872 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0607 16:56:05.915593 140040507991872 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0607 16:56:05.922807 140040507991872 submission_runner.py:332] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0607 16:56:15.098767 140012724324096 logging_writer.py:48] [0] global_step=0, grad_norm=23.469412, loss=33.571091
I0607 16:56:15.119703 140040507991872 spec.py:298] Evaluating on the training split.
I0607 16:56:15.120835 140040507991872 input_pipeline.py:20] Loading split = train-clean-100
I0607 16:56:15.155557 140040507991872 input_pipeline.py:20] Loading split = train-clean-360
I0607 16:56:15.588300 140040507991872 input_pipeline.py:20] Loading split = train-other-500
I0607 16:56:34.598598 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 16:56:34.599949 140040507991872 input_pipeline.py:20] Loading split = dev-clean
I0607 16:56:34.604640 140040507991872 input_pipeline.py:20] Loading split = dev-other
I0607 16:56:47.114713 140040507991872 spec.py:326] Evaluating on the test split.
I0607 16:56:47.116196 140040507991872 input_pipeline.py:20] Loading split = test-clean
I0607 16:56:54.607249 140040507991872 submission_runner.py:419] Time since start: 48.68s, 	Step: 1, 	{'train/ctc_loss': 31.72076699427672, 'train/wer': 3.4918586806273604, 'validation/ctc_loss': 30.644656042796864, 'validation/wer': 3.2774392893352, 'validation/num_examples': 5348, 'test/ctc_loss': 30.775804890638106, 'test/wer': 3.4578433164747224, 'test/num_examples': 2472, 'score': 9.197114706039429, 'total_duration': 48.68477392196655, 'accumulated_submission_time': 9.197114706039429, 'accumulated_eval_time': 39.48726296424866, 'accumulated_logging_time': 0}
I0607 16:56:54.631147 140010808395520 logging_writer.py:48] [1] accumulated_eval_time=39.487263, accumulated_logging_time=0, accumulated_submission_time=9.197115, global_step=1, preemption_count=0, score=9.197115, test/ctc_loss=30.775805, test/num_examples=2472, test/wer=3.457843, total_duration=48.684774, train/ctc_loss=31.720767, train/wer=3.491859, validation/ctc_loss=30.644656, validation/num_examples=5348, validation/wer=3.277439
I0607 16:56:54.671552 140040507991872 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.672726 139705302714176 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.672905 140417234941760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.673257 140479323133760 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.674441 140699356038976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.674461 140539730544448 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.674475 140545111435072 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:54.674553 140639288481600 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 16:56:55.714514 140010800002816 logging_writer.py:48] [1] global_step=1, grad_norm=24.241426, loss=32.931412
I0607 16:56:56.670602 140010808395520 logging_writer.py:48] [2] global_step=2, grad_norm=28.502338, loss=33.397285
I0607 16:56:57.508138 140010800002816 logging_writer.py:48] [3] global_step=3, grad_norm=31.208179, loss=33.253113
I0607 16:56:58.327644 140010808395520 logging_writer.py:48] [4] global_step=4, grad_norm=34.450405, loss=32.276764
I0607 16:56:59.128231 140010800002816 logging_writer.py:48] [5] global_step=5, grad_norm=36.571148, loss=31.857573
I0607 16:56:59.961689 140010808395520 logging_writer.py:48] [6] global_step=6, grad_norm=39.086296, loss=30.679445
I0607 16:57:00.766446 140010800002816 logging_writer.py:48] [7] global_step=7, grad_norm=34.000980, loss=28.545555
I0607 16:57:01.575404 140010808395520 logging_writer.py:48] [8] global_step=8, grad_norm=35.717323, loss=27.544678
I0607 16:57:02.399933 140010800002816 logging_writer.py:48] [9] global_step=9, grad_norm=35.936771, loss=25.621666
I0607 16:57:03.229802 140010808395520 logging_writer.py:48] [10] global_step=10, grad_norm=35.005028, loss=23.683210
I0607 16:57:04.051094 140010800002816 logging_writer.py:48] [11] global_step=11, grad_norm=30.702864, loss=21.384350
I0607 16:57:04.855037 140010808395520 logging_writer.py:48] [12] global_step=12, grad_norm=24.504965, loss=19.590603
I0607 16:57:05.662503 140010800002816 logging_writer.py:48] [13] global_step=13, grad_norm=19.045185, loss=17.598240
I0607 16:57:06.507858 140010808395520 logging_writer.py:48] [14] global_step=14, grad_norm=15.389562, loss=16.719641
I0607 16:57:07.326700 140010800002816 logging_writer.py:48] [15] global_step=15, grad_norm=13.284247, loss=15.266919
I0607 16:57:08.150806 140010808395520 logging_writer.py:48] [16] global_step=16, grad_norm=12.491104, loss=14.762048
I0607 16:57:08.984467 140010800002816 logging_writer.py:48] [17] global_step=17, grad_norm=13.256309, loss=14.450658
I0607 16:57:09.806713 140010808395520 logging_writer.py:48] [18] global_step=18, grad_norm=10.760534, loss=13.569370
I0607 16:57:10.623466 140010800002816 logging_writer.py:48] [19] global_step=19, grad_norm=10.033644, loss=12.981235
I0607 16:57:11.472227 140010808395520 logging_writer.py:48] [20] global_step=20, grad_norm=10.064057, loss=12.229982
I0607 16:57:12.279665 140010800002816 logging_writer.py:48] [21] global_step=21, grad_norm=10.862700, loss=11.925056
I0607 16:57:13.121363 140010808395520 logging_writer.py:48] [22] global_step=22, grad_norm=9.602536, loss=11.689810
I0607 16:57:13.958944 140010800002816 logging_writer.py:48] [23] global_step=23, grad_norm=8.509575, loss=10.958283
I0607 16:57:14.811207 140010808395520 logging_writer.py:48] [24] global_step=24, grad_norm=9.275537, loss=10.866740
I0607 16:57:15.644964 140010800002816 logging_writer.py:48] [25] global_step=25, grad_norm=10.862634, loss=10.303912
I0607 16:57:16.476508 140010808395520 logging_writer.py:48] [26] global_step=26, grad_norm=10.800190, loss=9.904532
I0607 16:57:17.284193 140010800002816 logging_writer.py:48] [27] global_step=27, grad_norm=13.148622, loss=9.654977
I0607 16:57:18.096965 140010808395520 logging_writer.py:48] [28] global_step=28, grad_norm=11.498036, loss=9.282405
I0607 16:57:18.904567 140010800002816 logging_writer.py:48] [29] global_step=29, grad_norm=6.187070, loss=8.961635
I0607 16:57:19.713914 140010808395520 logging_writer.py:48] [30] global_step=30, grad_norm=5.129505, loss=8.537214
I0607 16:57:20.518058 140010800002816 logging_writer.py:48] [31] global_step=31, grad_norm=6.789941, loss=8.418380
I0607 16:57:21.321161 140010808395520 logging_writer.py:48] [32] global_step=32, grad_norm=6.514760, loss=8.215353
I0607 16:57:22.132204 140010800002816 logging_writer.py:48] [33] global_step=33, grad_norm=5.015613, loss=8.133368
I0607 16:57:22.939291 140010808395520 logging_writer.py:48] [34] global_step=34, grad_norm=3.771660, loss=8.027118
I0607 16:57:23.745573 140010800002816 logging_writer.py:48] [35] global_step=35, grad_norm=4.296781, loss=7.976919
I0607 16:57:24.548972 140010808395520 logging_writer.py:48] [36] global_step=36, grad_norm=4.170846, loss=7.800588
I0607 16:57:25.354737 140010800002816 logging_writer.py:48] [37] global_step=37, grad_norm=4.176736, loss=7.627660
I0607 16:57:26.159502 140010808395520 logging_writer.py:48] [38] global_step=38, grad_norm=4.407644, loss=7.659901
I0607 16:57:26.963462 140010800002816 logging_writer.py:48] [39] global_step=39, grad_norm=3.913628, loss=7.479757
I0607 16:57:27.772006 140010808395520 logging_writer.py:48] [40] global_step=40, grad_norm=4.086282, loss=7.396261
I0607 16:57:28.576800 140010800002816 logging_writer.py:48] [41] global_step=41, grad_norm=3.697402, loss=7.248980
I0607 16:57:29.398088 140010808395520 logging_writer.py:48] [42] global_step=42, grad_norm=3.221148, loss=7.207238
I0607 16:57:30.201961 140010800002816 logging_writer.py:48] [43] global_step=43, grad_norm=4.413620, loss=7.275909
I0607 16:57:31.014463 140010808395520 logging_writer.py:48] [44] global_step=44, grad_norm=5.521801, loss=7.245647
I0607 16:57:31.815678 140010800002816 logging_writer.py:48] [45] global_step=45, grad_norm=13.492188, loss=7.252053
I0607 16:57:32.633890 140010808395520 logging_writer.py:48] [46] global_step=46, grad_norm=3.311121, loss=6.991350
I0607 16:57:33.440209 140010800002816 logging_writer.py:48] [47] global_step=47, grad_norm=4.389334, loss=7.034481
I0607 16:57:34.253004 140010808395520 logging_writer.py:48] [48] global_step=48, grad_norm=3.030023, loss=6.917508
I0607 16:57:35.057868 140010800002816 logging_writer.py:48] [49] global_step=49, grad_norm=2.538095, loss=6.924793
I0607 16:57:35.874321 140010808395520 logging_writer.py:48] [50] global_step=50, grad_norm=2.000401, loss=6.859547
I0607 16:57:36.680469 140010800002816 logging_writer.py:48] [51] global_step=51, grad_norm=2.124385, loss=6.876330
I0607 16:57:37.484158 140010808395520 logging_writer.py:48] [52] global_step=52, grad_norm=2.612302, loss=6.789554
I0607 16:57:38.291954 140010800002816 logging_writer.py:48] [53] global_step=53, grad_norm=1.713748, loss=6.767834
I0607 16:57:39.097852 140010808395520 logging_writer.py:48] [54] global_step=54, grad_norm=2.426139, loss=6.808337
I0607 16:57:39.901824 140010800002816 logging_writer.py:48] [55] global_step=55, grad_norm=3.265935, loss=6.856762
I0607 16:57:40.713945 140010808395520 logging_writer.py:48] [56] global_step=56, grad_norm=2.801351, loss=6.739080
I0607 16:57:41.519364 140010800002816 logging_writer.py:48] [57] global_step=57, grad_norm=2.606691, loss=6.717000
I0607 16:57:42.322519 140010808395520 logging_writer.py:48] [58] global_step=58, grad_norm=2.084530, loss=6.610537
I0607 16:57:43.126562 140010800002816 logging_writer.py:48] [59] global_step=59, grad_norm=2.227624, loss=6.690321
I0607 16:57:43.931807 140010808395520 logging_writer.py:48] [60] global_step=60, grad_norm=2.012376, loss=6.604794
I0607 16:57:44.740797 140010800002816 logging_writer.py:48] [61] global_step=61, grad_norm=2.996820, loss=6.559835
I0607 16:57:45.552217 140010808395520 logging_writer.py:48] [62] global_step=62, grad_norm=4.171331, loss=6.503865
I0607 16:57:46.358110 140010800002816 logging_writer.py:48] [63] global_step=63, grad_norm=13.960478, loss=6.513540
I0607 16:57:47.161958 140010808395520 logging_writer.py:48] [64] global_step=64, grad_norm=3.881750, loss=6.428237
I0607 16:57:47.967887 140010800002816 logging_writer.py:48] [65] global_step=65, grad_norm=13.563126, loss=6.607502
I0607 16:57:48.775617 140010808395520 logging_writer.py:48] [66] global_step=66, grad_norm=2.751987, loss=6.349864
I0607 16:57:49.581450 140010800002816 logging_writer.py:48] [67] global_step=67, grad_norm=17.791618, loss=6.598675
I0607 16:57:50.391659 140010808395520 logging_writer.py:48] [68] global_step=68, grad_norm=4.565590, loss=6.514874
I0607 16:57:51.199230 140010800002816 logging_writer.py:48] [69] global_step=69, grad_norm=2.766664, loss=6.598007
I0607 16:57:52.006049 140010808395520 logging_writer.py:48] [70] global_step=70, grad_norm=2.111070, loss=6.497447
I0607 16:57:52.805521 140010800002816 logging_writer.py:48] [71] global_step=71, grad_norm=2.293581, loss=6.590011
I0607 16:57:53.613240 140010808395520 logging_writer.py:48] [72] global_step=72, grad_norm=1.657425, loss=6.545704
I0607 16:57:54.415862 140010800002816 logging_writer.py:48] [73] global_step=73, grad_norm=1.935658, loss=6.537902
I0607 16:57:55.222336 140010808395520 logging_writer.py:48] [74] global_step=74, grad_norm=1.664394, loss=6.504652
I0607 16:57:56.032718 140010800002816 logging_writer.py:48] [75] global_step=75, grad_norm=1.866753, loss=6.487953
I0607 16:57:56.845265 140010808395520 logging_writer.py:48] [76] global_step=76, grad_norm=2.005201, loss=6.499146
I0607 16:57:57.651932 140010800002816 logging_writer.py:48] [77] global_step=77, grad_norm=2.760017, loss=6.488619
I0607 16:57:58.463144 140010808395520 logging_writer.py:48] [78] global_step=78, grad_norm=3.866775, loss=6.411775
I0607 16:57:59.267315 140010800002816 logging_writer.py:48] [79] global_step=79, grad_norm=5.137229, loss=6.298361
I0607 16:58:00.074668 140010808395520 logging_writer.py:48] [80] global_step=80, grad_norm=6.320385, loss=6.240533
I0607 16:58:00.893980 140010800002816 logging_writer.py:48] [81] global_step=81, grad_norm=6.917278, loss=6.286661
I0607 16:58:01.717503 140010808395520 logging_writer.py:48] [82] global_step=82, grad_norm=6.190905, loss=6.254455
I0607 16:58:02.528637 140010800002816 logging_writer.py:48] [83] global_step=83, grad_norm=9.761561, loss=6.290479
I0607 16:58:03.335989 140010808395520 logging_writer.py:48] [84] global_step=84, grad_norm=2.942479, loss=6.227897
I0607 16:58:04.139551 140010800002816 logging_writer.py:48] [85] global_step=85, grad_norm=4.204429, loss=6.211356
I0607 16:58:04.948250 140010808395520 logging_writer.py:48] [86] global_step=86, grad_norm=2.407255, loss=6.199516
I0607 16:58:05.762352 140010800002816 logging_writer.py:48] [87] global_step=87, grad_norm=4.212480, loss=6.229231
I0607 16:58:06.570530 140010808395520 logging_writer.py:48] [88] global_step=88, grad_norm=2.920922, loss=6.216272
I0607 16:58:07.396226 140010800002816 logging_writer.py:48] [89] global_step=89, grad_norm=3.242260, loss=6.170125
I0607 16:58:08.209204 140010808395520 logging_writer.py:48] [90] global_step=90, grad_norm=3.385879, loss=6.141416
I0607 16:58:09.035836 140010800002816 logging_writer.py:48] [91] global_step=91, grad_norm=4.521172, loss=6.164237
I0607 16:58:09.873168 140010808395520 logging_writer.py:48] [92] global_step=92, grad_norm=2.205981, loss=6.137237
I0607 16:58:10.719525 140010800002816 logging_writer.py:48] [93] global_step=93, grad_norm=5.024725, loss=6.142129
I0607 16:58:11.552953 140010808395520 logging_writer.py:48] [94] global_step=94, grad_norm=3.098169, loss=6.108381
I0607 16:58:12.359027 140010800002816 logging_writer.py:48] [95] global_step=95, grad_norm=1.081852, loss=6.081594
I0607 16:58:13.165307 140010808395520 logging_writer.py:48] [96] global_step=96, grad_norm=2.519755, loss=6.100754
I0607 16:58:13.974167 140010800002816 logging_writer.py:48] [97] global_step=97, grad_norm=1.669264, loss=6.081894
I0607 16:58:14.783919 140010808395520 logging_writer.py:48] [98] global_step=98, grad_norm=1.229254, loss=6.075583
I0607 16:58:15.589476 140010800002816 logging_writer.py:48] [99] global_step=99, grad_norm=1.152652, loss=6.085524
I0607 16:58:16.397824 140010808395520 logging_writer.py:48] [100] global_step=100, grad_norm=2.892279, loss=6.089995
I0607 17:03:38.581988 140010800002816 logging_writer.py:48] [500] global_step=500, grad_norm=3.052443, loss=6.030914
I0607 17:10:21.264090 140010808395520 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.165162, loss=4.616249
I0607 17:17:05.264321 140010808395520 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.781547, loss=3.584724
I0607 17:23:48.674066 140010800002816 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.137559, loss=3.067312
I0607 17:30:30.406387 140010808395520 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.003296, loss=2.906736
I0607 17:36:54.746832 140040507991872 spec.py:298] Evaluating on the training split.
I0607 17:37:06.326717 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 17:37:15.766838 140040507991872 spec.py:326] Evaluating on the test split.
I0607 17:37:20.972881 140040507991872 submission_runner.py:419] Time since start: 2475.05s, 	Step: 2980, 	{'train/ctc_loss': 2.4927108318264626, 'train/wer': 0.5822839230710624, 'validation/ctc_loss': 2.6663546846745025, 'validation/wer': 0.5870226427847246, 'validation/num_examples': 5348, 'test/ctc_loss': 2.1990442931802052, 'test/wer': 0.5237543923790953, 'test/num_examples': 2472, 'score': 2407.819446325302, 'total_duration': 2475.050308227539, 'accumulated_submission_time': 2407.819446325302, 'accumulated_eval_time': 65.71296262741089, 'accumulated_logging_time': 0.032381296157836914}
I0607 17:37:20.998641 140010808395520 logging_writer.py:48] [2980] accumulated_eval_time=65.712963, accumulated_logging_time=0.032381, accumulated_submission_time=2407.819446, global_step=2980, preemption_count=0, score=2407.819446, test/ctc_loss=2.199044, test/num_examples=2472, test/wer=0.523754, total_duration=2475.050308, train/ctc_loss=2.492711, train/wer=0.582284, validation/ctc_loss=2.666355, validation/num_examples=5348, validation/wer=0.587023
I0607 17:37:37.881069 140010800002816 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.887958, loss=2.813738
I0607 17:44:19.681888 140010808395520 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.956413, loss=2.727585
I0607 17:51:00.711887 140010800002816 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.935784, loss=2.641078
I0607 17:57:41.728138 140010808395520 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.922427, loss=2.692474
I0607 18:04:23.249606 140010800002816 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.921200, loss=2.518955
I0607 18:11:05.411782 140010808395520 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.907992, loss=2.455913
I0607 18:17:21.630388 140040507991872 spec.py:298] Evaluating on the training split.
I0607 18:17:33.360477 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 18:17:43.187045 140040507991872 spec.py:326] Evaluating on the test split.
I0607 18:17:48.478385 140040507991872 submission_runner.py:419] Time since start: 4902.56s, 	Step: 5970, 	{'train/ctc_loss': 1.1622131998466554, 'train/wer': 0.3515134364421999, 'validation/ctc_loss': 1.405657724752612, 'validation/wer': 0.3837107130787428, 'validation/num_examples': 5348, 'test/ctc_loss': 1.025303245370669, 'test/wer': 0.3141998253204152, 'test/num_examples': 2472, 'score': 4806.796369314194, 'total_duration': 4902.5558824539185, 'accumulated_submission_time': 4806.796369314194, 'accumulated_eval_time': 92.56068205833435, 'accumulated_logging_time': 0.06749463081359863}
I0607 18:17:48.498476 140010808395520 logging_writer.py:48] [5970] accumulated_eval_time=92.560682, accumulated_logging_time=0.067495, accumulated_submission_time=4806.796369, global_step=5970, preemption_count=0, score=4806.796369, test/ctc_loss=1.025303, test/num_examples=2472, test/wer=0.314200, total_duration=4902.555882, train/ctc_loss=1.162213, train/wer=0.351513, validation/ctc_loss=1.405658, validation/num_examples=5348, validation/wer=0.383711
I0607 18:18:13.468943 140010800002816 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.685734, loss=2.400790
I0607 18:24:55.519575 140010808395520 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.736356, loss=2.293535
I0607 18:31:24.994265 140010800002816 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0607 18:37:54.340946 140010808395520 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0607 18:44:24.432154 140010800002816 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0607 18:50:55.269106 140010808395520 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0607 18:57:24.504766 140010800002816 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0607 18:57:48.589892 140040507991872 spec.py:298] Evaluating on the training split.
I0607 18:57:58.184916 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 18:58:07.076250 140040507991872 spec.py:326] Evaluating on the test split.
I0607 18:58:12.156891 140040507991872 submission_runner.py:419] Time since start: 7326.23s, 	Step: 9032, 	{'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7205.065991401672, 'total_duration': 7326.234359741211, 'accumulated_submission_time': 7205.065991401672, 'accumulated_eval_time': 116.13567996025085, 'accumulated_logging_time': 0.09842586517333984}
I0607 18:58:12.176273 140010808395520 logging_writer.py:48] [9032] accumulated_eval_time=116.135680, accumulated_logging_time=0.098426, accumulated_submission_time=7205.065991, global_step=9032, preemption_count=0, score=7205.065991, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7326.234360, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 19:04:19.153728 140010808395520 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0607 19:10:48.310678 140010800002816 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0607 19:17:18.774697 140010808395520 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0607 19:23:48.763468 140010800002816 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0607 19:30:18.388947 140010808395520 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0607 19:36:44.700251 140010800002816 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0607 19:38:12.706347 140040507991872 spec.py:298] Evaluating on the training split.
I0607 19:38:22.564491 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 19:38:31.455457 140040507991872 spec.py:326] Evaluating on the test split.
I0607 19:38:36.740640 140040507991872 submission_runner.py:419] Time since start: 9750.82s, 	Step: 12114, 	{'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9603.650729179382, 'total_duration': 9750.817978858948, 'accumulated_submission_time': 9603.650729179382, 'accumulated_eval_time': 140.17292261123657, 'accumulated_logging_time': 0.12703728675842285}
I0607 19:38:36.760625 140010808395520 logging_writer.py:48] [12114] accumulated_eval_time=140.172923, accumulated_logging_time=0.127037, accumulated_submission_time=9603.650729, global_step=12114, preemption_count=0, score=9603.650729, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=9750.817979, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 19:43:38.776283 140010808395520 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0607 19:50:07.078203 140010800002816 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0607 19:56:35.863242 140010808395520 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0607 20:03:03.652786 140010800002816 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0607 20:09:32.820565 140010808395520 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0607 20:16:03.906678 140010800002816 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0607 20:18:36.759175 140040507991872 spec.py:298] Evaluating on the training split.
I0607 20:18:46.543703 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 20:18:55.523064 140040507991872 spec.py:326] Evaluating on the test split.
I0607 20:19:00.426338 140040507991872 submission_runner.py:419] Time since start: 12174.50s, 	Step: 15199, 	{'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12001.654147863388, 'total_duration': 12174.50379705429, 'accumulated_submission_time': 12001.654147863388, 'accumulated_eval_time': 163.83977246284485, 'accumulated_logging_time': 0.15712642669677734}
I0607 20:19:00.446434 140010808395520 logging_writer.py:48] [15199] accumulated_eval_time=163.839772, accumulated_logging_time=0.157126, accumulated_submission_time=12001.654148, global_step=15199, preemption_count=0, score=12001.654148, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12174.503797, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 20:22:57.911069 140010808395520 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0607 20:29:25.442527 140040507991872 spec.py:298] Evaluating on the training split.
I0607 20:29:34.925346 140040507991872 spec.py:310] Evaluating on the validation split.
I0607 20:29:43.753457 140040507991872 spec.py:326] Evaluating on the test split.
I0607 20:29:48.744674 140040507991872 submission_runner.py:419] Time since start: 12822.82s, 	Step: 16000, 	{'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12626.024138689041, 'total_duration': 12822.820899009705, 'accumulated_submission_time': 12626.024138689041, 'accumulated_eval_time': 187.14095449447632, 'accumulated_logging_time': 0.18718242645263672}
I0607 20:29:48.770577 140010808395520 logging_writer.py:48] [16000] accumulated_eval_time=187.140954, accumulated_logging_time=0.187182, accumulated_submission_time=12626.024139, global_step=16000, preemption_count=0, score=12626.024139, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12822.820899, train/ctc_loss=nan, train/wer=0.941487, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.896722
I0607 20:29:48.795949 140010800002816 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12626.024139
I0607 20:29:49.050803 140040507991872 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/momentum/librispeech_deepspeech_pytorch/trial_1/checkpoint_16000.
I0607 20:29:49.150411 140040507991872 submission_runner.py:581] Tuning trial 1/1
I0607 20:29:49.150634 140040507991872 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 20:29:49.151046 140040507991872 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/ctc_loss': 31.72076699427672, 'train/wer': 3.4918586806273604, 'validation/ctc_loss': 30.644656042796864, 'validation/wer': 3.2774392893352, 'validation/num_examples': 5348, 'test/ctc_loss': 30.775804890638106, 'test/wer': 3.4578433164747224, 'test/num_examples': 2472, 'score': 9.197114706039429, 'total_duration': 48.68477392196655, 'accumulated_submission_time': 9.197114706039429, 'accumulated_eval_time': 39.48726296424866, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2980, {'train/ctc_loss': 2.4927108318264626, 'train/wer': 0.5822839230710624, 'validation/ctc_loss': 2.6663546846745025, 'validation/wer': 0.5870226427847246, 'validation/num_examples': 5348, 'test/ctc_loss': 2.1990442931802052, 'test/wer': 0.5237543923790953, 'test/num_examples': 2472, 'score': 2407.819446325302, 'total_duration': 2475.050308227539, 'accumulated_submission_time': 2407.819446325302, 'accumulated_eval_time': 65.71296262741089, 'accumulated_logging_time': 0.032381296157836914, 'global_step': 2980, 'preemption_count': 0}), (5970, {'train/ctc_loss': 1.1622131998466554, 'train/wer': 0.3515134364421999, 'validation/ctc_loss': 1.405657724752612, 'validation/wer': 0.3837107130787428, 'validation/num_examples': 5348, 'test/ctc_loss': 1.025303245370669, 'test/wer': 0.3141998253204152, 'test/num_examples': 2472, 'score': 4806.796369314194, 'total_duration': 4902.5558824539185, 'accumulated_submission_time': 4806.796369314194, 'accumulated_eval_time': 92.56068205833435, 'accumulated_logging_time': 0.06749463081359863, 'global_step': 5970, 'preemption_count': 0}), (9032, {'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7205.065991401672, 'total_duration': 7326.234359741211, 'accumulated_submission_time': 7205.065991401672, 'accumulated_eval_time': 116.13567996025085, 'accumulated_logging_time': 0.09842586517333984, 'global_step': 9032, 'preemption_count': 0}), (12114, {'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9603.650729179382, 'total_duration': 9750.817978858948, 'accumulated_submission_time': 9603.650729179382, 'accumulated_eval_time': 140.17292261123657, 'accumulated_logging_time': 0.12703728675842285, 'global_step': 12114, 'preemption_count': 0}), (15199, {'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12001.654147863388, 'total_duration': 12174.50379705429, 'accumulated_submission_time': 12001.654147863388, 'accumulated_eval_time': 163.83977246284485, 'accumulated_logging_time': 0.15712642669677734, 'global_step': 15199, 'preemption_count': 0}), (16000, {'train/ctc_loss': nan, 'train/wer': 0.9414869880383556, 'validation/ctc_loss': nan, 'validation/wer': 0.8967218654950997, 'validation/num_examples': 5348, 'test/ctc_loss': nan, 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12626.024138689041, 'total_duration': 12822.820899009705, 'accumulated_submission_time': 12626.024138689041, 'accumulated_eval_time': 187.14095449447632, 'accumulated_logging_time': 0.18718242645263672, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0607 20:29:49.151160 140040507991872 submission_runner.py:584] Timing: 12626.024138689041
I0607 20:29:49.151224 140040507991872 submission_runner.py:586] Total number of evals: 7
I0607 20:29:49.151279 140040507991872 submission_runner.py:587] ====================
I0607 20:29:49.151455 140040507991872 submission_runner.py:655] Final librispeech_deepspeech score: 12626.024138689041
