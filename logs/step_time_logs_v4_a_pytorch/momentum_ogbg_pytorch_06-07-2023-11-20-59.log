torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/momentum/pytorch/submission.py --tuning_search_space=baselines/momentum/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/momentum --overwrite=True --save_checkpoints=False --max_global_steps=12000 2>&1 | tee -a /logs/ogbg_pytorch_06-07-2023-11-20-59.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 11:21:22.884780 140469744969536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 11:21:22.884921 140194777798464 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 11:21:22.884955 140264778811200 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 11:21:22.884971 139879779452736 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 11:21:22.885513 140039460235072 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 11:21:22.885801 139646820644672 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 11:21:22.885962 140516485457728 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 11:21:22.886698 139883542857536 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 11:21:22.887051 139883542857536 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.895644 140469744969536 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.895665 140194777798464 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.895686 140264778811200 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.895703 139879779452736 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.896174 140039460235072 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.896342 139646820644672 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:22.896556 140516485457728 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 11:21:24.023105 139883542857536 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch because --overwrite was set.
I0607 11:21:24.033083 139883542857536 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch.
W0607 11:21:24.146485 139646820644672 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.146880 140469744969536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.147957 139879779452736 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.148869 139883542857536 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.149311 140194777798464 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.149586 140516485457728 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.149625 140039460235072 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 11:21:24.149940 140264778811200 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 11:21:24.154385 139883542857536 submission_runner.py:541] Using RNG seed 2559955633
I0607 11:21:24.155697 139883542857536 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 11:21:24.155812 139883542857536 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch/trial_1.
I0607 11:21:24.156147 139883542857536 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch/trial_1/hparams.json.
I0607 11:21:24.157168 139883542857536 submission_runner.py:255] Initializing dataset.
I0607 11:21:24.157285 139883542857536 submission_runner.py:262] Initializing model.
I0607 11:21:28.245367 139883542857536 submission_runner.py:272] Initializing optimizer.
I0607 11:21:28.717352 139883542857536 submission_runner.py:279] Initializing metrics bundle.
I0607 11:21:28.717580 139883542857536 submission_runner.py:297] Initializing checkpoint and logger.
I0607 11:21:28.721292 139883542857536 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 11:21:28.721450 139883542857536 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 11:21:29.176467 139883542857536 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch/trial_1/meta_data_0.json.
I0607 11:21:29.177432 139883542857536 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch/trial_1/flags_0.json.
I0607 11:21:29.230499 139883542857536 submission_runner.py:332] Starting training loop.
I0607 11:21:29.486814 139883542857536 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:21:29.493120 139883542857536 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:21:29.635890 139883542857536 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:21:34.253547 139844193871616 logging_writer.py:48] [0] global_step=0, grad_norm=2.922184, loss=0.770799
I0607 11:21:34.263644 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:21:34.269375 139883542857536 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:21:34.273319 139883542857536 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:21:34.327097 139883542857536 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:22:30.196552 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:22:30.199763 139883542857536 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:22:30.204317 139883542857536 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:22:30.259275 139883542857536 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:23:14.361209 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:23:14.364675 139883542857536 dataset_info.py:566] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:23:14.370123 139883542857536 dataset_builder.py:510] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0607 11:23:14.425485 139883542857536 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0607 11:23:59.412859 139883542857536 submission_runner.py:419] Time since start: 150.18s, 	Step: 1, 	{'train/accuracy': 0.5006755153966919, 'train/loss': 0.7710963960372651, 'train/mean_average_precision': 0.023039577043119073, 'validation/accuracy': 0.4962263752661956, 'validation/loss': 0.773006080179783, 'validation/mean_average_precision': 0.027755538417988838, 'validation/num_examples': 43793, 'test/accuracy': 0.49799869345408704, 'test/loss': 0.7711866191502903, 'test/mean_average_precision': 0.029201259784477025, 'test/num_examples': 43793, 'score': 5.033464431762695, 'total_duration': 150.18275809288025, 'accumulated_submission_time': 5.033464431762695, 'accumulated_eval_time': 145.1488757133484, 'accumulated_logging_time': 0}
I0607 11:23:59.431262 139831308527360 logging_writer.py:48] [1] accumulated_eval_time=145.148876, accumulated_logging_time=0, accumulated_submission_time=5.033464, global_step=1, preemption_count=0, score=5.033464, test/accuracy=0.497999, test/loss=0.771187, test/mean_average_precision=0.029201, test/num_examples=43793, total_duration=150.182758, train/accuracy=0.500676, train/loss=0.771096, train/mean_average_precision=0.023040, validation/accuracy=0.496226, validation/loss=0.773006, validation/mean_average_precision=0.027756, validation/num_examples=43793
I0607 11:23:59.697313 139883542857536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.703218 139646820644672 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.703920 139879779452736 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.703941 140264778811200 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.703942 140516485457728 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.703934 140469744969536 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.704199 140194777798464 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.704205 140039460235072 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 11:23:59.732741 139831316920064 logging_writer.py:48] [1] global_step=1, grad_norm=2.914586, loss=0.769886
I0607 11:24:00.021667 139831308527360 logging_writer.py:48] [2] global_step=2, grad_norm=2.918447, loss=0.770434
I0607 11:24:00.308656 139831316920064 logging_writer.py:48] [3] global_step=3, grad_norm=2.931076, loss=0.764326
I0607 11:24:00.598106 139831308527360 logging_writer.py:48] [4] global_step=4, grad_norm=2.918308, loss=0.751143
I0607 11:24:00.888219 139831316920064 logging_writer.py:48] [5] global_step=5, grad_norm=2.808345, loss=0.738462
I0607 11:24:01.178965 139831308527360 logging_writer.py:48] [6] global_step=6, grad_norm=2.616225, loss=0.715504
I0607 11:24:01.471091 139831316920064 logging_writer.py:48] [7] global_step=7, grad_norm=2.264669, loss=0.691039
I0607 11:24:01.757760 139831308527360 logging_writer.py:48] [8] global_step=8, grad_norm=2.002831, loss=0.668948
I0607 11:24:02.067024 139831316920064 logging_writer.py:48] [9] global_step=9, grad_norm=1.893070, loss=0.645197
I0607 11:24:02.420721 139831308527360 logging_writer.py:48] [10] global_step=10, grad_norm=1.688819, loss=0.625876
I0607 11:24:02.714862 139831316920064 logging_writer.py:48] [11] global_step=11, grad_norm=1.536478, loss=0.608440
I0607 11:24:03.014866 139831308527360 logging_writer.py:48] [12] global_step=12, grad_norm=1.439573, loss=0.593502
I0607 11:24:03.311181 139831316920064 logging_writer.py:48] [13] global_step=13, grad_norm=1.389222, loss=0.581118
I0607 11:24:03.606420 139831308527360 logging_writer.py:48] [14] global_step=14, grad_norm=1.306401, loss=0.565335
I0607 11:24:03.895282 139831316920064 logging_writer.py:48] [15] global_step=15, grad_norm=1.236766, loss=0.552898
I0607 11:24:04.184229 139831308527360 logging_writer.py:48] [16] global_step=16, grad_norm=1.127060, loss=0.539339
I0607 11:24:04.472638 139831316920064 logging_writer.py:48] [17] global_step=17, grad_norm=1.017464, loss=0.526332
I0607 11:24:04.763633 139831308527360 logging_writer.py:48] [18] global_step=18, grad_norm=0.926068, loss=0.515480
I0607 11:24:05.053743 139831316920064 logging_writer.py:48] [19] global_step=19, grad_norm=0.865164, loss=0.502873
I0607 11:24:05.341101 139831308527360 logging_writer.py:48] [20] global_step=20, grad_norm=0.824688, loss=0.491662
I0607 11:24:05.631865 139831316920064 logging_writer.py:48] [21] global_step=21, grad_norm=0.792720, loss=0.484190
I0607 11:24:05.921392 139831308527360 logging_writer.py:48] [22] global_step=22, grad_norm=0.756694, loss=0.470586
I0607 11:24:06.211210 139831316920064 logging_writer.py:48] [23] global_step=23, grad_norm=0.717288, loss=0.460998
I0607 11:24:06.504016 139831308527360 logging_writer.py:48] [24] global_step=24, grad_norm=0.685418, loss=0.452802
I0607 11:24:06.792702 139831316920064 logging_writer.py:48] [25] global_step=25, grad_norm=0.643030, loss=0.439143
I0607 11:24:07.081729 139831308527360 logging_writer.py:48] [26] global_step=26, grad_norm=0.629298, loss=0.429713
I0607 11:24:07.371907 139831316920064 logging_writer.py:48] [27] global_step=27, grad_norm=0.588917, loss=0.419844
I0607 11:24:07.666418 139831308527360 logging_writer.py:48] [28] global_step=28, grad_norm=0.547696, loss=0.408349
I0607 11:24:07.958887 139831316920064 logging_writer.py:48] [29] global_step=29, grad_norm=0.511360, loss=0.401864
I0607 11:24:08.254421 139831308527360 logging_writer.py:48] [30] global_step=30, grad_norm=0.489066, loss=0.392468
I0607 11:24:08.545892 139831316920064 logging_writer.py:48] [31] global_step=31, grad_norm=0.473593, loss=0.383806
I0607 11:24:08.840175 139831308527360 logging_writer.py:48] [32] global_step=32, grad_norm=0.463840, loss=0.374134
I0607 11:24:09.132849 139831316920064 logging_writer.py:48] [33] global_step=33, grad_norm=0.453577, loss=0.364604
I0607 11:24:09.426943 139831308527360 logging_writer.py:48] [34] global_step=34, grad_norm=0.440718, loss=0.356550
I0607 11:24:09.724432 139831316920064 logging_writer.py:48] [35] global_step=35, grad_norm=0.426652, loss=0.349529
I0607 11:24:10.014519 139831308527360 logging_writer.py:48] [36] global_step=36, grad_norm=0.413130, loss=0.343384
I0607 11:24:10.302943 139831316920064 logging_writer.py:48] [37] global_step=37, grad_norm=0.406368, loss=0.331917
I0607 11:24:10.592018 139831308527360 logging_writer.py:48] [38] global_step=38, grad_norm=0.394575, loss=0.322940
I0607 11:24:10.884164 139831316920064 logging_writer.py:48] [39] global_step=39, grad_norm=0.384562, loss=0.316051
I0607 11:24:11.173193 139831308527360 logging_writer.py:48] [40] global_step=40, grad_norm=0.370978, loss=0.310824
I0607 11:24:11.465744 139831316920064 logging_writer.py:48] [41] global_step=41, grad_norm=0.363891, loss=0.302467
I0607 11:24:11.753642 139831308527360 logging_writer.py:48] [42] global_step=42, grad_norm=0.353809, loss=0.292986
I0607 11:24:12.045809 139831316920064 logging_writer.py:48] [43] global_step=43, grad_norm=0.341242, loss=0.285269
I0607 11:24:12.336507 139831308527360 logging_writer.py:48] [44] global_step=44, grad_norm=0.333657, loss=0.279069
I0607 11:24:12.635210 139831316920064 logging_writer.py:48] [45] global_step=45, grad_norm=0.321216, loss=0.272325
I0607 11:24:12.932244 139831308527360 logging_writer.py:48] [46] global_step=46, grad_norm=0.310703, loss=0.264745
I0607 11:24:13.244237 139831316920064 logging_writer.py:48] [47] global_step=47, grad_norm=0.305290, loss=0.257097
I0607 11:24:13.538294 139831308527360 logging_writer.py:48] [48] global_step=48, grad_norm=0.294877, loss=0.249678
I0607 11:24:13.837536 139831316920064 logging_writer.py:48] [49] global_step=49, grad_norm=0.284409, loss=0.243949
I0607 11:24:14.129626 139831308527360 logging_writer.py:48] [50] global_step=50, grad_norm=0.278370, loss=0.237332
I0607 11:24:14.421674 139831316920064 logging_writer.py:48] [51] global_step=51, grad_norm=0.268200, loss=0.229765
I0607 11:24:14.708552 139831308527360 logging_writer.py:48] [52] global_step=52, grad_norm=0.260695, loss=0.225958
I0607 11:24:14.994802 139831316920064 logging_writer.py:48] [53] global_step=53, grad_norm=0.253951, loss=0.216850
I0607 11:24:15.279932 139831308527360 logging_writer.py:48] [54] global_step=54, grad_norm=0.241534, loss=0.216042
I0607 11:24:15.560680 139831316920064 logging_writer.py:48] [55] global_step=55, grad_norm=0.237155, loss=0.208810
I0607 11:24:15.848360 139831308527360 logging_writer.py:48] [56] global_step=56, grad_norm=0.232050, loss=0.201427
I0607 11:24:16.134495 139831316920064 logging_writer.py:48] [57] global_step=57, grad_norm=0.224136, loss=0.194414
I0607 11:24:16.424654 139831308527360 logging_writer.py:48] [58] global_step=58, grad_norm=0.215028, loss=0.189809
I0607 11:24:16.711383 139831316920064 logging_writer.py:48] [59] global_step=59, grad_norm=0.210788, loss=0.182861
I0607 11:24:17.004833 139831308527360 logging_writer.py:48] [60] global_step=60, grad_norm=0.201629, loss=0.184287
I0607 11:24:17.297552 139831316920064 logging_writer.py:48] [61] global_step=61, grad_norm=0.196925, loss=0.175347
I0607 11:24:17.582881 139831308527360 logging_writer.py:48] [62] global_step=62, grad_norm=0.190111, loss=0.171684
I0607 11:24:17.873685 139831316920064 logging_writer.py:48] [63] global_step=63, grad_norm=0.182989, loss=0.168910
I0607 11:24:18.159032 139831308527360 logging_writer.py:48] [64] global_step=64, grad_norm=0.177752, loss=0.164690
I0607 11:24:18.458835 139831316920064 logging_writer.py:48] [65] global_step=65, grad_norm=0.173831, loss=0.159666
I0607 11:24:18.753138 139831308527360 logging_writer.py:48] [66] global_step=66, grad_norm=0.167144, loss=0.156173
I0607 11:24:19.049022 139831316920064 logging_writer.py:48] [67] global_step=67, grad_norm=0.163725, loss=0.151890
I0607 11:24:19.337304 139831308527360 logging_writer.py:48] [68] global_step=68, grad_norm=0.153923, loss=0.149584
I0607 11:24:19.658754 139831316920064 logging_writer.py:48] [69] global_step=69, grad_norm=0.152054, loss=0.145041
I0607 11:24:19.978295 139831308527360 logging_writer.py:48] [70] global_step=70, grad_norm=0.147993, loss=0.140157
I0607 11:24:20.304231 139831316920064 logging_writer.py:48] [71] global_step=71, grad_norm=0.140038, loss=0.141933
I0607 11:24:20.644140 139831308527360 logging_writer.py:48] [72] global_step=72, grad_norm=0.139452, loss=0.131641
I0607 11:24:20.961267 139831316920064 logging_writer.py:48] [73] global_step=73, grad_norm=0.135523, loss=0.130473
I0607 11:24:21.284587 139831308527360 logging_writer.py:48] [74] global_step=74, grad_norm=0.129783, loss=0.132432
I0607 11:24:21.604076 139831316920064 logging_writer.py:48] [75] global_step=75, grad_norm=0.126670, loss=0.125078
I0607 11:24:21.926346 139831308527360 logging_writer.py:48] [76] global_step=76, grad_norm=0.120083, loss=0.129624
I0607 11:24:22.245050 139831316920064 logging_writer.py:48] [77] global_step=77, grad_norm=0.118272, loss=0.123976
I0607 11:24:22.561463 139831308527360 logging_writer.py:48] [78] global_step=78, grad_norm=0.115321, loss=0.122672
I0607 11:24:22.876692 139831316920064 logging_writer.py:48] [79] global_step=79, grad_norm=0.110776, loss=0.120236
I0607 11:24:23.190280 139831308527360 logging_writer.py:48] [80] global_step=80, grad_norm=0.110478, loss=0.116509
I0607 11:24:23.508044 139831316920064 logging_writer.py:48] [81] global_step=81, grad_norm=0.106312, loss=0.111668
I0607 11:24:23.834994 139831308527360 logging_writer.py:48] [82] global_step=82, grad_norm=0.101974, loss=0.115652
I0607 11:24:24.152687 139831316920064 logging_writer.py:48] [83] global_step=83, grad_norm=0.099312, loss=0.111129
I0607 11:24:24.452840 139831308527360 logging_writer.py:48] [84] global_step=84, grad_norm=0.098157, loss=0.108789
I0607 11:24:24.752119 139831316920064 logging_writer.py:48] [85] global_step=85, grad_norm=0.094299, loss=0.108809
I0607 11:24:25.069207 139831308527360 logging_writer.py:48] [86] global_step=86, grad_norm=0.091803, loss=0.107592
I0607 11:24:25.377652 139831316920064 logging_writer.py:48] [87] global_step=87, grad_norm=0.092036, loss=0.105081
I0607 11:24:25.710114 139831308527360 logging_writer.py:48] [88] global_step=88, grad_norm=0.086855, loss=0.106682
I0607 11:24:26.044068 139831316920064 logging_writer.py:48] [89] global_step=89, grad_norm=0.086601, loss=0.099633
I0607 11:24:26.360890 139831308527360 logging_writer.py:48] [90] global_step=90, grad_norm=0.083593, loss=0.095837
I0607 11:24:26.676667 139831316920064 logging_writer.py:48] [91] global_step=91, grad_norm=0.081102, loss=0.094126
I0607 11:24:26.988493 139831308527360 logging_writer.py:48] [92] global_step=92, grad_norm=0.077200, loss=0.103354
I0607 11:24:27.302630 139831316920064 logging_writer.py:48] [93] global_step=93, grad_norm=0.076707, loss=0.092975
I0607 11:24:27.612804 139831308527360 logging_writer.py:48] [94] global_step=94, grad_norm=0.077298, loss=0.093696
I0607 11:24:27.930777 139831316920064 logging_writer.py:48] [95] global_step=95, grad_norm=0.071756, loss=0.095594
I0607 11:24:28.246655 139831308527360 logging_writer.py:48] [96] global_step=96, grad_norm=0.072436, loss=0.088004
I0607 11:24:28.557996 139831316920064 logging_writer.py:48] [97] global_step=97, grad_norm=0.071042, loss=0.090270
I0607 11:24:28.869189 139831308527360 logging_writer.py:48] [98] global_step=98, grad_norm=0.067467, loss=0.088241
I0607 11:24:29.195510 139831316920064 logging_writer.py:48] [99] global_step=99, grad_norm=0.068650, loss=0.087853
I0607 11:24:29.512382 139831308527360 logging_writer.py:48] [100] global_step=100, grad_norm=0.064099, loss=0.084341
I0607 11:26:21.753775 139831316920064 logging_writer.py:48] [500] global_step=500, grad_norm=0.012762, loss=0.052937
I0607 11:27:59.623104 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:28:55.056981 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:28:58.237350 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:29:01.354139 139883542857536 submission_runner.py:419] Time since start: 452.12s, 	Step: 848, 	{'train/accuracy': 0.986677861630639, 'train/loss': 0.055731135813090034, 'train/mean_average_precision': 0.030809146908149686, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06577615709336718, 'validation/mean_average_precision': 0.034443011575041156, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06917948958785679, 'test/mean_average_precision': 0.0357865832761524, 'test/num_examples': 43793, 'score': 244.9859697818756, 'total_duration': 452.12412667274475, 'accumulated_submission_time': 244.9859697818756, 'accumulated_eval_time': 206.87965202331543, 'accumulated_logging_time': 0.028891324996948242}
I0607 11:29:01.364252 139831308527360 logging_writer.py:48] [848] accumulated_eval_time=206.879652, accumulated_logging_time=0.028891, accumulated_submission_time=244.985970, global_step=848, preemption_count=0, score=244.985970, test/accuracy=0.983142, test/loss=0.069179, test/mean_average_precision=0.035787, test/num_examples=43793, total_duration=452.124127, train/accuracy=0.986678, train/loss=0.055731, train/mean_average_precision=0.030809, validation/accuracy=0.984118, validation/loss=0.065776, validation/mean_average_precision=0.034443, validation/num_examples=43793
I0607 11:29:46.413587 139831316920064 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.009150, loss=0.053577
I0607 11:32:08.656456 139831308527360 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.026148, loss=0.053458
I0607 11:33:01.505950 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:33:59.362298 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:34:02.734853 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:34:06.019145 139883542857536 submission_runner.py:419] Time since start: 756.79s, 	Step: 1686, 	{'train/accuracy': 0.9867664580063166, 'train/loss': 0.053723268347611756, 'train/mean_average_precision': 0.045638217759605366, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06389688634350539, 'validation/mean_average_precision': 0.044529703747876216, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.0673127020944748, 'test/mean_average_precision': 0.044636594397591184, 'test/num_examples': 43793, 'score': 484.89826583862305, 'total_duration': 756.7891292572021, 'accumulated_submission_time': 484.89826583862305, 'accumulated_eval_time': 271.3925907611847, 'accumulated_logging_time': 0.04982566833496094}
I0607 11:34:06.032004 139831316920064 logging_writer.py:48] [1686] accumulated_eval_time=271.392591, accumulated_logging_time=0.049826, accumulated_submission_time=484.898266, global_step=1686, preemption_count=0, score=484.898266, test/accuracy=0.983142, test/loss=0.067313, test/mean_average_precision=0.044637, test/num_examples=43793, total_duration=756.789129, train/accuracy=0.986766, train/loss=0.053723, train/mean_average_precision=0.045638, validation/accuracy=0.984118, validation/loss=0.063897, validation/mean_average_precision=0.044530, validation/num_examples=43793
I0607 11:35:36.583203 139831308527360 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.022572, loss=0.054360
I0607 11:37:59.742177 139831316920064 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.026054, loss=0.048168
I0607 11:38:06.272959 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:39:02.389126 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:39:06.533802 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:39:09.721580 139883542857536 submission_runner.py:419] Time since start: 1060.49s, 	Step: 2523, 	{'train/accuracy': 0.986866066288613, 'train/loss': 0.05115165923126985, 'train/mean_average_precision': 0.06017793889961988, 'validation/accuracy': 0.9841719662224864, 'validation/loss': 0.06017317328715352, 'validation/mean_average_precision': 0.057403039859830536, 'validation/num_examples': 43793, 'test/accuracy': 0.9831766418905913, 'test/loss': 0.06322188162197019, 'test/mean_average_precision': 0.0592055688152516, 'test/num_examples': 43793, 'score': 724.9038140773773, 'total_duration': 1060.4915761947632, 'accumulated_submission_time': 724.9038140773773, 'accumulated_eval_time': 334.84096240997314, 'accumulated_logging_time': 0.07495975494384766}
I0607 11:39:09.732712 139831308527360 logging_writer.py:48] [2523] accumulated_eval_time=334.840962, accumulated_logging_time=0.074960, accumulated_submission_time=724.903814, global_step=2523, preemption_count=0, score=724.903814, test/accuracy=0.983177, test/loss=0.063222, test/mean_average_precision=0.059206, test/num_examples=43793, total_duration=1060.491576, train/accuracy=0.986866, train/loss=0.051152, train/mean_average_precision=0.060178, validation/accuracy=0.984172, validation/loss=0.060173, validation/mean_average_precision=0.057403, validation/num_examples=43793
I0607 11:41:27.329569 139831316920064 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.051432, loss=0.056062
I0607 11:43:09.780237 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:44:09.138712 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:44:12.397410 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:44:16.306566 139883542857536 submission_runner.py:419] Time since start: 1367.08s, 	Step: 3356, 	{'train/accuracy': 0.9869853392122176, 'train/loss': 0.04858620558966529, 'train/mean_average_precision': 0.08331823794182464, 'validation/accuracy': 0.9842389464377486, 'validation/loss': 0.057631540516535185, 'validation/mean_average_precision': 0.08287970644864162, 'validation/num_examples': 43793, 'test/accuracy': 0.983276465030943, 'test/loss': 0.06062366981453535, 'test/mean_average_precision': 0.08383150281864259, 'test/num_examples': 43793, 'score': 964.7192504405975, 'total_duration': 1367.0764825344086, 'accumulated_submission_time': 964.7192504405975, 'accumulated_eval_time': 401.36699962615967, 'accumulated_logging_time': 0.09861564636230469}
I0607 11:44:16.317035 139831308527360 logging_writer.py:48] [3356] accumulated_eval_time=401.367000, accumulated_logging_time=0.098616, accumulated_submission_time=964.719250, global_step=3356, preemption_count=0, score=964.719250, test/accuracy=0.983276, test/loss=0.060624, test/mean_average_precision=0.083832, test/num_examples=43793, total_duration=1367.076483, train/accuracy=0.986985, train/loss=0.048586, train/mean_average_precision=0.083318, validation/accuracy=0.984239, validation/loss=0.057632, validation/mean_average_precision=0.082880, validation/num_examples=43793
I0607 11:44:58.404071 139831316920064 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.050037, loss=0.052250
I0607 11:47:22.751936 139831308527360 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.064760, loss=0.044314
I0607 11:48:16.364511 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:49:14.464314 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:49:17.732643 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:49:20.888642 139883542857536 submission_runner.py:419] Time since start: 1671.66s, 	Step: 4186, 	{'train/accuracy': 0.9871203324272934, 'train/loss': 0.047363138396919545, 'train/mean_average_precision': 0.10799161340708438, 'validation/accuracy': 0.9845348772069981, 'validation/loss': 0.05621513730538188, 'validation/mean_average_precision': 0.10508848764595391, 'validation/num_examples': 43793, 'test/accuracy': 0.9836129995842808, 'test/loss': 0.059291654785466594, 'test/mean_average_precision': 0.10734050792360669, 'test/num_examples': 43793, 'score': 1204.5385072231293, 'total_duration': 1671.6586174964905, 'accumulated_submission_time': 1204.5385072231293, 'accumulated_eval_time': 465.8909101486206, 'accumulated_logging_time': 0.12276673316955566}
I0607 11:49:20.899533 139831316920064 logging_writer.py:48] [4186] accumulated_eval_time=465.890910, accumulated_logging_time=0.122767, accumulated_submission_time=1204.538507, global_step=4186, preemption_count=0, score=1204.538507, test/accuracy=0.983613, test/loss=0.059292, test/mean_average_precision=0.107341, test/num_examples=43793, total_duration=1671.658617, train/accuracy=0.987120, train/loss=0.047363, train/mean_average_precision=0.107992, validation/accuracy=0.984535, validation/loss=0.056215, validation/mean_average_precision=0.105088, validation/num_examples=43793
I0607 11:50:51.338690 139831308527360 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.092599, loss=0.045722
I0607 11:53:14.949296 139831316920064 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.055919, loss=0.048254
I0607 11:53:20.974811 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:54:21.023968 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:54:24.304193 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:54:27.494565 139883542857536 submission_runner.py:419] Time since start: 1978.26s, 	Step: 5022, 	{'train/accuracy': 0.9872949208260617, 'train/loss': 0.04538368922763406, 'train/mean_average_precision': 0.12702775491614565, 'validation/accuracy': 0.9846042930664517, 'validation/loss': 0.05476751471738003, 'validation/mean_average_precision': 0.11997565221439051, 'validation/num_examples': 43793, 'test/accuracy': 0.9836492223271933, 'test/loss': 0.05772898959817606, 'test/mean_average_precision': 0.12241629435280282, 'test/num_examples': 43793, 'score': 1444.3876831531525, 'total_duration': 1978.264571428299, 'accumulated_submission_time': 1444.3876831531525, 'accumulated_eval_time': 532.4104199409485, 'accumulated_logging_time': 0.14435410499572754}
I0607 11:54:27.504904 139831308527360 logging_writer.py:48] [5022] accumulated_eval_time=532.410420, accumulated_logging_time=0.144354, accumulated_submission_time=1444.387683, global_step=5022, preemption_count=0, score=1444.387683, test/accuracy=0.983649, test/loss=0.057729, test/mean_average_precision=0.122416, test/num_examples=43793, total_duration=1978.264571, train/accuracy=0.987295, train/loss=0.045384, train/mean_average_precision=0.127028, validation/accuracy=0.984604, validation/loss=0.054768, validation/mean_average_precision=0.119976, validation/num_examples=43793
I0607 11:56:46.124119 139831316920064 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.025687, loss=0.044804
I0607 11:58:27.505168 139883542857536 spec.py:298] Evaluating on the training split.
I0607 11:59:27.967546 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 11:59:31.218364 139883542857536 spec.py:326] Evaluating on the test split.
I0607 11:59:34.396467 139883542857536 submission_runner.py:419] Time since start: 2285.17s, 	Step: 5852, 	{'train/accuracy': 0.9875145974638749, 'train/loss': 0.044576240415806545, 'train/mean_average_precision': 0.14517832197586691, 'validation/accuracy': 0.9847646396423825, 'validation/loss': 0.05358248501875852, 'validation/mean_average_precision': 0.1392816794768868, 'validation/num_examples': 43793, 'test/accuracy': 0.9838155942277795, 'test/loss': 0.056320369785767745, 'test/mean_average_precision': 0.13711084334528859, 'test/num_examples': 43793, 'score': 1684.1492040157318, 'total_duration': 2285.1664323806763, 'accumulated_submission_time': 1684.1492040157318, 'accumulated_eval_time': 599.301433801651, 'accumulated_logging_time': 0.17257475852966309}
I0607 11:59:34.407577 139831308527360 logging_writer.py:48] [5852] accumulated_eval_time=599.301434, accumulated_logging_time=0.172575, accumulated_submission_time=1684.149204, global_step=5852, preemption_count=0, score=1684.149204, test/accuracy=0.983816, test/loss=0.056320, test/mean_average_precision=0.137111, test/num_examples=43793, total_duration=2285.166432, train/accuracy=0.987515, train/loss=0.044576, train/mean_average_precision=0.145178, validation/accuracy=0.984765, validation/loss=0.053582, validation/mean_average_precision=0.139282, validation/num_examples=43793
I0607 12:00:17.346268 139831316920064 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.087591, loss=0.045430
I0607 12:02:42.226191 139831308527360 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.043235, loss=0.048010
I0607 12:03:34.494782 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:04:34.174164 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:04:37.599374 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:04:40.932958 139883542857536 submission_runner.py:419] Time since start: 2591.70s, 	Step: 6681, 	{'train/accuracy': 0.98809895071412, 'train/loss': 0.04259153668565438, 'train/mean_average_precision': 0.16819375501846082, 'validation/accuracy': 0.9851470357804251, 'validation/loss': 0.05201970212883421, 'validation/mean_average_precision': 0.14919622092192367, 'validation/num_examples': 43793, 'test/accuracy': 0.9841837183824945, 'test/loss': 0.054809241716890625, 'test/mean_average_precision': 0.14787999824873294, 'test/num_examples': 43793, 'score': 1924.0024199485779, 'total_duration': 2591.7028527259827, 'accumulated_submission_time': 1924.0024199485779, 'accumulated_eval_time': 665.7392740249634, 'accumulated_logging_time': 0.1953113079071045}
I0607 12:04:40.944258 139831316920064 logging_writer.py:48] [6681] accumulated_eval_time=665.739274, accumulated_logging_time=0.195311, accumulated_submission_time=1924.002420, global_step=6681, preemption_count=0, score=1924.002420, test/accuracy=0.984184, test/loss=0.054809, test/mean_average_precision=0.147880, test/num_examples=43793, total_duration=2591.702853, train/accuracy=0.988099, train/loss=0.042592, train/mean_average_precision=0.168194, validation/accuracy=0.985147, validation/loss=0.052020, validation/mean_average_precision=0.149196, validation/num_examples=43793
I0607 12:06:15.261843 139831308527360 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.063259, loss=0.044847
I0607 12:08:39.652760 139831316920064 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.050549, loss=0.045775
I0607 12:08:41.111774 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:09:41.228586 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:09:44.617565 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:09:48.000766 139883542857536 submission_runner.py:419] Time since start: 2898.77s, 	Step: 7506, 	{'train/accuracy': 0.987769262154286, 'train/loss': 0.04275673528801351, 'train/mean_average_precision': 0.17318405341487086, 'validation/accuracy': 0.9850289070371444, 'validation/loss': 0.052800272178164125, 'validation/mean_average_precision': 0.1538506078521809, 'validation/num_examples': 43793, 'test/accuracy': 0.9840152405084831, 'test/loss': 0.055792685238263515, 'test/mean_average_precision': 0.15176544959580732, 'test/num_examples': 43793, 'score': 2163.931268453598, 'total_duration': 2898.770655155182, 'accumulated_submission_time': 2163.931268453598, 'accumulated_eval_time': 732.6279029846191, 'accumulated_logging_time': 0.2232367992401123}
I0607 12:09:48.012082 139831308527360 logging_writer.py:48] [7506] accumulated_eval_time=732.627903, accumulated_logging_time=0.223237, accumulated_submission_time=2163.931268, global_step=7506, preemption_count=0, score=2163.931268, test/accuracy=0.984015, test/loss=0.055793, test/mean_average_precision=0.151765, test/num_examples=43793, total_duration=2898.770655, train/accuracy=0.987769, train/loss=0.042757, train/mean_average_precision=0.173184, validation/accuracy=0.985029, validation/loss=0.052800, validation/mean_average_precision=0.153851, validation/num_examples=43793
I0607 12:12:11.352972 139831316920064 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.058240, loss=0.041873
I0607 12:13:48.165147 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:14:47.839394 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:14:51.063224 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:14:54.244788 139883542857536 submission_runner.py:419] Time since start: 3205.01s, 	Step: 8339, 	{'train/accuracy': 0.9883214598883885, 'train/loss': 0.04165052918630973, 'train/mean_average_precision': 0.1824362165961729, 'validation/accuracy': 0.9853873526739719, 'validation/loss': 0.05092257127709755, 'validation/mean_average_precision': 0.16538266906971288, 'validation/num_examples': 43793, 'test/accuracy': 0.9843749407694974, 'test/loss': 0.0535320214838773, 'test/mean_average_precision': 0.16002760155482212, 'test/num_examples': 43793, 'score': 2403.85449886322, 'total_duration': 3205.0147659778595, 'accumulated_submission_time': 2403.85449886322, 'accumulated_eval_time': 798.707277059555, 'accumulated_logging_time': 0.2474677562713623}
I0607 12:14:54.255285 139831308527360 logging_writer.py:48] [8339] accumulated_eval_time=798.707277, accumulated_logging_time=0.247468, accumulated_submission_time=2403.854499, global_step=8339, preemption_count=0, score=2403.854499, test/accuracy=0.984375, test/loss=0.053532, test/mean_average_precision=0.160028, test/num_examples=43793, total_duration=3205.014766, train/accuracy=0.988321, train/loss=0.041651, train/mean_average_precision=0.182436, validation/accuracy=0.985387, validation/loss=0.050923, validation/mean_average_precision=0.165383, validation/num_examples=43793
I0607 12:15:40.567699 139831316920064 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.032061, loss=0.041096
I0607 12:18:03.921588 139831308527360 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.067616, loss=0.042220
I0607 12:18:54.420639 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:19:54.167234 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:19:57.468516 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:20:00.691591 139883542857536 submission_runner.py:419] Time since start: 3511.46s, 	Step: 9177, 	{'train/accuracy': 0.9883112496619445, 'train/loss': 0.04021829813979891, 'train/mean_average_precision': 0.21320998615161335, 'validation/accuracy': 0.9854470259566601, 'validation/loss': 0.05024420314145328, 'validation/mean_average_precision': 0.1758924752146921, 'validation/num_examples': 43793, 'test/accuracy': 0.9844549677596528, 'test/loss': 0.053151738623215664, 'test/mean_average_precision': 0.17541628965163775, 'test/num_examples': 43793, 'score': 2643.789762020111, 'total_duration': 3511.461584329605, 'accumulated_submission_time': 2643.789762020111, 'accumulated_eval_time': 864.9779772758484, 'accumulated_logging_time': 0.26922035217285156}
I0607 12:20:00.702651 139831316920064 logging_writer.py:48] [9177] accumulated_eval_time=864.977977, accumulated_logging_time=0.269220, accumulated_submission_time=2643.789762, global_step=9177, preemption_count=0, score=2643.789762, test/accuracy=0.984455, test/loss=0.053152, test/mean_average_precision=0.175416, test/num_examples=43793, total_duration=3511.461584, train/accuracy=0.988311, train/loss=0.040218, train/mean_average_precision=0.213210, validation/accuracy=0.985447, validation/loss=0.050244, validation/mean_average_precision=0.175892, validation/num_examples=43793
I0607 12:21:33.590788 139831308527360 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.054424, loss=0.043134
I0607 12:23:57.593795 139831316920064 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.070830, loss=0.044482
I0607 12:24:00.734103 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:25:01.023367 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:25:04.331160 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:25:07.523051 139883542857536 submission_runner.py:419] Time since start: 3818.29s, 	Step: 10012, 	{'train/accuracy': 0.9886673240783934, 'train/loss': 0.03995534176552446, 'train/mean_average_precision': 0.2162103327654909, 'validation/accuracy': 0.9854807190346405, 'validation/loss': 0.04987303380085523, 'validation/mean_average_precision': 0.1865161122283142, 'validation/num_examples': 43793, 'test/accuracy': 0.9844225357689056, 'test/loss': 0.0525936688278868, 'test/mean_average_precision': 0.18121068397206636, 'test/num_examples': 43793, 'score': 2883.5915625095367, 'total_duration': 3818.2930438518524, 'accumulated_submission_time': 2883.5915625095367, 'accumulated_eval_time': 931.7666394710541, 'accumulated_logging_time': 0.2916090488433838}
I0607 12:25:07.533303 139831308527360 logging_writer.py:48] [10012] accumulated_eval_time=931.766639, accumulated_logging_time=0.291609, accumulated_submission_time=2883.591563, global_step=10012, preemption_count=0, score=2883.591563, test/accuracy=0.984423, test/loss=0.052594, test/mean_average_precision=0.181211, test/num_examples=43793, total_duration=3818.293044, train/accuracy=0.988667, train/loss=0.039955, train/mean_average_precision=0.216210, validation/accuracy=0.985481, validation/loss=0.049873, validation/mean_average_precision=0.186516, validation/num_examples=43793
I0607 12:27:27.944696 139831316920064 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.052890, loss=0.040967
I0607 12:29:07.553340 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:30:07.294390 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:30:10.556491 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:30:13.778528 139883542857536 submission_runner.py:419] Time since start: 4124.55s, 	Step: 10845, 	{'train/accuracy': 0.9884072000769084, 'train/loss': 0.039394597213882596, 'train/mean_average_precision': 0.22383906303161388, 'validation/accuracy': 0.9855018279509656, 'validation/loss': 0.04993085180769453, 'validation/mean_average_precision': 0.1888565937172506, 'validation/num_examples': 43793, 'test/accuracy': 0.984486978555715, 'test/loss': 0.052665035002331315, 'test/mean_average_precision': 0.18544040395111988, 'test/num_examples': 43793, 'score': 3123.377706050873, 'total_duration': 4124.548511981964, 'accumulated_submission_time': 3123.377706050873, 'accumulated_eval_time': 997.99156665802, 'accumulated_logging_time': 0.3149998188018799}
I0607 12:30:13.789142 139831308527360 logging_writer.py:48] [10845] accumulated_eval_time=997.991567, accumulated_logging_time=0.315000, accumulated_submission_time=3123.377706, global_step=10845, preemption_count=0, score=3123.377706, test/accuracy=0.984487, test/loss=0.052665, test/mean_average_precision=0.185440, test/num_examples=43793, total_duration=4124.548512, train/accuracy=0.988407, train/loss=0.039395, train/mean_average_precision=0.223839, validation/accuracy=0.985502, validation/loss=0.049931, validation/mean_average_precision=0.188857, validation/num_examples=43793
I0607 12:30:58.911409 139831316920064 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.076082, loss=0.043381
I0607 12:33:24.279929 139831308527360 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.059147, loss=0.042270
I0607 12:34:13.919256 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:35:15.151646 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:35:18.487315 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:35:21.766463 139883542857536 submission_runner.py:419] Time since start: 4432.54s, 	Step: 11672, 	{'train/accuracy': 0.9882713540594952, 'train/loss': 0.03936579620668667, 'train/mean_average_precision': 0.23295681231377144, 'validation/accuracy': 0.985291550669112, 'validation/loss': 0.05053657875817869, 'validation/mean_average_precision': 0.19520087278417395, 'validation/num_examples': 43793, 'test/accuracy': 0.9843441935574904, 'test/loss': 0.05349055684148633, 'test/mean_average_precision': 0.19068254020185324, 'test/num_examples': 43793, 'score': 3363.275000810623, 'total_duration': 4432.536464691162, 'accumulated_submission_time': 3363.275000810623, 'accumulated_eval_time': 1065.838543176651, 'accumulated_logging_time': 0.33846426010131836}
I0607 12:35:21.777557 139831316920064 logging_writer.py:48] [11672] accumulated_eval_time=1065.838543, accumulated_logging_time=0.338464, accumulated_submission_time=3363.275001, global_step=11672, preemption_count=0, score=3363.275001, test/accuracy=0.984344, test/loss=0.053491, test/mean_average_precision=0.190683, test/num_examples=43793, total_duration=4432.536465, train/accuracy=0.988271, train/loss=0.039366, train/mean_average_precision=0.232957, validation/accuracy=0.985292, validation/loss=0.050537, validation/mean_average_precision=0.195201, validation/num_examples=43793
I0607 12:36:57.002722 139883542857536 spec.py:298] Evaluating on the training split.
I0607 12:37:58.653956 139883542857536 spec.py:310] Evaluating on the validation split.
I0607 12:38:02.083757 139883542857536 spec.py:326] Evaluating on the test split.
I0607 12:38:05.309177 139883542857536 submission_runner.py:419] Time since start: 4596.08s, 	Step: 12000, 	{'train/accuracy': 0.9889561628443265, 'train/loss': 0.03780730287487588, 'train/mean_average_precision': 0.2344470142314207, 'validation/accuracy': 0.9858740755715442, 'validation/loss': 0.04834023026580185, 'validation/mean_average_precision': 0.1999531445707517, 'validation/num_examples': 43793, 'test/accuracy': 0.9849212302759794, 'test/loss': 0.05118119574222717, 'test/mean_average_precision': 0.19425171367888605, 'test/num_examples': 43793, 'score': 3458.4018244743347, 'total_duration': 4596.079164028168, 'accumulated_submission_time': 3458.4018244743347, 'accumulated_eval_time': 1134.1447713375092, 'accumulated_logging_time': 0.3612351417541504}
I0607 12:38:05.319675 139831308527360 logging_writer.py:48] [12000] accumulated_eval_time=1134.144771, accumulated_logging_time=0.361235, accumulated_submission_time=3458.401824, global_step=12000, preemption_count=0, score=3458.401824, test/accuracy=0.984921, test/loss=0.051181, test/mean_average_precision=0.194252, test/num_examples=43793, total_duration=4596.079164, train/accuracy=0.988956, train/loss=0.037807, train/mean_average_precision=0.234447, validation/accuracy=0.985874, validation/loss=0.048340, validation/mean_average_precision=0.199953, validation/num_examples=43793
I0607 12:38:05.340306 139831316920064 logging_writer.py:48] [12000] global_step=12000, preemption_count=0, score=3458.401824
I0607 12:38:05.400952 139883542857536 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/momentum/ogbg_pytorch/trial_1/checkpoint_12000.
I0607 12:38:05.586507 139883542857536 submission_runner.py:581] Tuning trial 1/1
I0607 12:38:05.586751 139883542857536 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0607 12:38:05.589510 139883542857536 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5006755153966919, 'train/loss': 0.7710963960372651, 'train/mean_average_precision': 0.023039577043119073, 'validation/accuracy': 0.4962263752661956, 'validation/loss': 0.773006080179783, 'validation/mean_average_precision': 0.027755538417988838, 'validation/num_examples': 43793, 'test/accuracy': 0.49799869345408704, 'test/loss': 0.7711866191502903, 'test/mean_average_precision': 0.029201259784477025, 'test/num_examples': 43793, 'score': 5.033464431762695, 'total_duration': 150.18275809288025, 'accumulated_submission_time': 5.033464431762695, 'accumulated_eval_time': 145.1488757133484, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (848, {'train/accuracy': 0.986677861630639, 'train/loss': 0.055731135813090034, 'train/mean_average_precision': 0.030809146908149686, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06577615709336718, 'validation/mean_average_precision': 0.034443011575041156, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06917948958785679, 'test/mean_average_precision': 0.0357865832761524, 'test/num_examples': 43793, 'score': 244.9859697818756, 'total_duration': 452.12412667274475, 'accumulated_submission_time': 244.9859697818756, 'accumulated_eval_time': 206.87965202331543, 'accumulated_logging_time': 0.028891324996948242, 'global_step': 848, 'preemption_count': 0}), (1686, {'train/accuracy': 0.9867664580063166, 'train/loss': 0.053723268347611756, 'train/mean_average_precision': 0.045638217759605366, 'validation/accuracy': 0.984117976109578, 'validation/loss': 0.06389688634350539, 'validation/mean_average_precision': 0.044529703747876216, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.0673127020944748, 'test/mean_average_precision': 0.044636594397591184, 'test/num_examples': 43793, 'score': 484.89826583862305, 'total_duration': 756.7891292572021, 'accumulated_submission_time': 484.89826583862305, 'accumulated_eval_time': 271.3925907611847, 'accumulated_logging_time': 0.04982566833496094, 'global_step': 1686, 'preemption_count': 0}), (2523, {'train/accuracy': 0.986866066288613, 'train/loss': 0.05115165923126985, 'train/mean_average_precision': 0.06017793889961988, 'validation/accuracy': 0.9841719662224864, 'validation/loss': 0.06017317328715352, 'validation/mean_average_precision': 0.057403039859830536, 'validation/num_examples': 43793, 'test/accuracy': 0.9831766418905913, 'test/loss': 0.06322188162197019, 'test/mean_average_precision': 0.0592055688152516, 'test/num_examples': 43793, 'score': 724.9038140773773, 'total_duration': 1060.4915761947632, 'accumulated_submission_time': 724.9038140773773, 'accumulated_eval_time': 334.84096240997314, 'accumulated_logging_time': 0.07495975494384766, 'global_step': 2523, 'preemption_count': 0}), (3356, {'train/accuracy': 0.9869853392122176, 'train/loss': 0.04858620558966529, 'train/mean_average_precision': 0.08331823794182464, 'validation/accuracy': 0.9842389464377486, 'validation/loss': 0.057631540516535185, 'validation/mean_average_precision': 0.08287970644864162, 'validation/num_examples': 43793, 'test/accuracy': 0.983276465030943, 'test/loss': 0.06062366981453535, 'test/mean_average_precision': 0.08383150281864259, 'test/num_examples': 43793, 'score': 964.7192504405975, 'total_duration': 1367.0764825344086, 'accumulated_submission_time': 964.7192504405975, 'accumulated_eval_time': 401.36699962615967, 'accumulated_logging_time': 0.09861564636230469, 'global_step': 3356, 'preemption_count': 0}), (4186, {'train/accuracy': 0.9871203324272934, 'train/loss': 0.047363138396919545, 'train/mean_average_precision': 0.10799161340708438, 'validation/accuracy': 0.9845348772069981, 'validation/loss': 0.05621513730538188, 'validation/mean_average_precision': 0.10508848764595391, 'validation/num_examples': 43793, 'test/accuracy': 0.9836129995842808, 'test/loss': 0.059291654785466594, 'test/mean_average_precision': 0.10734050792360669, 'test/num_examples': 43793, 'score': 1204.5385072231293, 'total_duration': 1671.6586174964905, 'accumulated_submission_time': 1204.5385072231293, 'accumulated_eval_time': 465.8909101486206, 'accumulated_logging_time': 0.12276673316955566, 'global_step': 4186, 'preemption_count': 0}), (5022, {'train/accuracy': 0.9872949208260617, 'train/loss': 0.04538368922763406, 'train/mean_average_precision': 0.12702775491614565, 'validation/accuracy': 0.9846042930664517, 'validation/loss': 0.05476751471738003, 'validation/mean_average_precision': 0.11997565221439051, 'validation/num_examples': 43793, 'test/accuracy': 0.9836492223271933, 'test/loss': 0.05772898959817606, 'test/mean_average_precision': 0.12241629435280282, 'test/num_examples': 43793, 'score': 1444.3876831531525, 'total_duration': 1978.264571428299, 'accumulated_submission_time': 1444.3876831531525, 'accumulated_eval_time': 532.4104199409485, 'accumulated_logging_time': 0.14435410499572754, 'global_step': 5022, 'preemption_count': 0}), (5852, {'train/accuracy': 0.9875145974638749, 'train/loss': 0.044576240415806545, 'train/mean_average_precision': 0.14517832197586691, 'validation/accuracy': 0.9847646396423825, 'validation/loss': 0.05358248501875852, 'validation/mean_average_precision': 0.1392816794768868, 'validation/num_examples': 43793, 'test/accuracy': 0.9838155942277795, 'test/loss': 0.056320369785767745, 'test/mean_average_precision': 0.13711084334528859, 'test/num_examples': 43793, 'score': 1684.1492040157318, 'total_duration': 2285.1664323806763, 'accumulated_submission_time': 1684.1492040157318, 'accumulated_eval_time': 599.301433801651, 'accumulated_logging_time': 0.17257475852966309, 'global_step': 5852, 'preemption_count': 0}), (6681, {'train/accuracy': 0.98809895071412, 'train/loss': 0.04259153668565438, 'train/mean_average_precision': 0.16819375501846082, 'validation/accuracy': 0.9851470357804251, 'validation/loss': 0.05201970212883421, 'validation/mean_average_precision': 0.14919622092192367, 'validation/num_examples': 43793, 'test/accuracy': 0.9841837183824945, 'test/loss': 0.054809241716890625, 'test/mean_average_precision': 0.14787999824873294, 'test/num_examples': 43793, 'score': 1924.0024199485779, 'total_duration': 2591.7028527259827, 'accumulated_submission_time': 1924.0024199485779, 'accumulated_eval_time': 665.7392740249634, 'accumulated_logging_time': 0.1953113079071045, 'global_step': 6681, 'preemption_count': 0}), (7506, {'train/accuracy': 0.987769262154286, 'train/loss': 0.04275673528801351, 'train/mean_average_precision': 0.17318405341487086, 'validation/accuracy': 0.9850289070371444, 'validation/loss': 0.052800272178164125, 'validation/mean_average_precision': 0.1538506078521809, 'validation/num_examples': 43793, 'test/accuracy': 0.9840152405084831, 'test/loss': 0.055792685238263515, 'test/mean_average_precision': 0.15176544959580732, 'test/num_examples': 43793, 'score': 2163.931268453598, 'total_duration': 2898.770655155182, 'accumulated_submission_time': 2163.931268453598, 'accumulated_eval_time': 732.6279029846191, 'accumulated_logging_time': 0.2232367992401123, 'global_step': 7506, 'preemption_count': 0}), (8339, {'train/accuracy': 0.9883214598883885, 'train/loss': 0.04165052918630973, 'train/mean_average_precision': 0.1824362165961729, 'validation/accuracy': 0.9853873526739719, 'validation/loss': 0.05092257127709755, 'validation/mean_average_precision': 0.16538266906971288, 'validation/num_examples': 43793, 'test/accuracy': 0.9843749407694974, 'test/loss': 0.0535320214838773, 'test/mean_average_precision': 0.16002760155482212, 'test/num_examples': 43793, 'score': 2403.85449886322, 'total_duration': 3205.0147659778595, 'accumulated_submission_time': 2403.85449886322, 'accumulated_eval_time': 798.707277059555, 'accumulated_logging_time': 0.2474677562713623, 'global_step': 8339, 'preemption_count': 0}), (9177, {'train/accuracy': 0.9883112496619445, 'train/loss': 0.04021829813979891, 'train/mean_average_precision': 0.21320998615161335, 'validation/accuracy': 0.9854470259566601, 'validation/loss': 0.05024420314145328, 'validation/mean_average_precision': 0.1758924752146921, 'validation/num_examples': 43793, 'test/accuracy': 0.9844549677596528, 'test/loss': 0.053151738623215664, 'test/mean_average_precision': 0.17541628965163775, 'test/num_examples': 43793, 'score': 2643.789762020111, 'total_duration': 3511.461584329605, 'accumulated_submission_time': 2643.789762020111, 'accumulated_eval_time': 864.9779772758484, 'accumulated_logging_time': 0.26922035217285156, 'global_step': 9177, 'preemption_count': 0}), (10012, {'train/accuracy': 0.9886673240783934, 'train/loss': 0.03995534176552446, 'train/mean_average_precision': 0.2162103327654909, 'validation/accuracy': 0.9854807190346405, 'validation/loss': 0.04987303380085523, 'validation/mean_average_precision': 0.1865161122283142, 'validation/num_examples': 43793, 'test/accuracy': 0.9844225357689056, 'test/loss': 0.0525936688278868, 'test/mean_average_precision': 0.18121068397206636, 'test/num_examples': 43793, 'score': 2883.5915625095367, 'total_duration': 3818.2930438518524, 'accumulated_submission_time': 2883.5915625095367, 'accumulated_eval_time': 931.7666394710541, 'accumulated_logging_time': 0.2916090488433838, 'global_step': 10012, 'preemption_count': 0}), (10845, {'train/accuracy': 0.9884072000769084, 'train/loss': 0.039394597213882596, 'train/mean_average_precision': 0.22383906303161388, 'validation/accuracy': 0.9855018279509656, 'validation/loss': 0.04993085180769453, 'validation/mean_average_precision': 0.1888565937172506, 'validation/num_examples': 43793, 'test/accuracy': 0.984486978555715, 'test/loss': 0.052665035002331315, 'test/mean_average_precision': 0.18544040395111988, 'test/num_examples': 43793, 'score': 3123.377706050873, 'total_duration': 4124.548511981964, 'accumulated_submission_time': 3123.377706050873, 'accumulated_eval_time': 997.99156665802, 'accumulated_logging_time': 0.3149998188018799, 'global_step': 10845, 'preemption_count': 0}), (11672, {'train/accuracy': 0.9882713540594952, 'train/loss': 0.03936579620668667, 'train/mean_average_precision': 0.23295681231377144, 'validation/accuracy': 0.985291550669112, 'validation/loss': 0.05053657875817869, 'validation/mean_average_precision': 0.19520087278417395, 'validation/num_examples': 43793, 'test/accuracy': 0.9843441935574904, 'test/loss': 0.05349055684148633, 'test/mean_average_precision': 0.19068254020185324, 'test/num_examples': 43793, 'score': 3363.275000810623, 'total_duration': 4432.536464691162, 'accumulated_submission_time': 3363.275000810623, 'accumulated_eval_time': 1065.838543176651, 'accumulated_logging_time': 0.33846426010131836, 'global_step': 11672, 'preemption_count': 0}), (12000, {'train/accuracy': 0.9889561628443265, 'train/loss': 0.03780730287487588, 'train/mean_average_precision': 0.2344470142314207, 'validation/accuracy': 0.9858740755715442, 'validation/loss': 0.04834023026580185, 'validation/mean_average_precision': 0.1999531445707517, 'validation/num_examples': 43793, 'test/accuracy': 0.9849212302759794, 'test/loss': 0.05118119574222717, 'test/mean_average_precision': 0.19425171367888605, 'test/num_examples': 43793, 'score': 3458.4018244743347, 'total_duration': 4596.079164028168, 'accumulated_submission_time': 3458.4018244743347, 'accumulated_eval_time': 1134.1447713375092, 'accumulated_logging_time': 0.3612351417541504, 'global_step': 12000, 'preemption_count': 0})], 'global_step': 12000}
I0607 12:38:05.589650 139883542857536 submission_runner.py:584] Timing: 3458.4018244743347
I0607 12:38:05.589699 139883542857536 submission_runner.py:586] Total number of evals: 16
I0607 12:38:05.589751 139883542857536 submission_runner.py:587] ====================
I0607 12:38:05.589879 139883542857536 submission_runner.py:655] Final ogbg score: 3458.4018244743347
