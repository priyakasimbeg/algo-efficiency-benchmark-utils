torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=baselines/nesterov/pytorch/submission.py --tuning_search_space=baselines/nesterov/tuning_search_space.json --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nesterov --overwrite=True --save_checkpoints=False --max_global_steps=1600 2>&1 | tee -a /logs/criteo1tb_pytorch_06-07-2023-20-32-40.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 20:33:05.372052 139818010933056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 20:33:05.372100 139924922414912 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 20:33:05.372079 140095327491904 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 20:33:05.372881 140458704885568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 20:33:05.373848 140172107470656 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 20:33:05.373903 139809674651456 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 20:33:05.374016 139714161289024 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 20:33:05.383812 139947217250112 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 20:33:05.384170 139947217250112 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.384439 140172107470656 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.384713 139809674651456 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.384690 139714161289024 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.393250 139818010933056 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.393277 139924922414912 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.393310 140095327491904 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.393690 140458704885568 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 20:33:05.411587 139947217250112 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch because --overwrite was set.
I0607 20:33:05.448874 139947217250112 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch.
W0607 20:33:05.555988 139818010933056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.556355 139947217250112 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.556781 140095327491904 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.556785 139714161289024 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.556899 140458704885568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.556895 140172107470656 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.558311 139924922414912 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 20:33:05.560401 139809674651456 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 20:33:05.562856 139947217250112 submission_runner.py:541] Using RNG seed 1789989201
I0607 20:33:05.564599 139947217250112 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 20:33:05.564718 139947217250112 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch/trial_1.
I0607 20:33:05.564942 139947217250112 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch/trial_1/hparams.json.
I0607 20:33:05.565886 139947217250112 submission_runner.py:255] Initializing dataset.
I0607 20:33:05.566031 139947217250112 submission_runner.py:262] Initializing model.
I0607 20:33:19.005764 139947217250112 submission_runner.py:272] Initializing optimizer.
I0607 20:33:19.556332 139947217250112 submission_runner.py:279] Initializing metrics bundle.
I0607 20:33:19.556531 139947217250112 submission_runner.py:297] Initializing checkpoint and logger.
I0607 20:33:19.560775 139947217250112 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0607 20:33:19.560895 139947217250112 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0607 20:33:20.016408 139947217250112 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0607 20:33:20.017395 139947217250112 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch/trial_1/flags_0.json.
I0607 20:33:20.076048 139947217250112 submission_runner.py:332] Starting training loop.
I0607 20:33:25.917041 139908693882624 logging_writer.py:48] [0] global_step=0, grad_norm=6.822846, loss=1.101175
I0607 20:33:25.923331 139947217250112 submission.py:139] 0) loss = 1.101, grad_norm = 6.823
I0607 20:33:25.924197 139947217250112 spec.py:298] Evaluating on the training split.
I0607 20:38:35.651780 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 20:43:27.564212 139947217250112 spec.py:326] Evaluating on the test split.
I0607 20:48:30.211431 139947217250112 submission_runner.py:419] Time since start: 910.14s, 	Step: 1, 	{'train/loss': 1.0982816021086277, 'validation/loss': 1.1031749213483146, 'validation/num_examples': 89000000, 'test/loss': 1.0988127792667475, 'test/num_examples': 89274637, 'score': 5.84829306602478, 'total_duration': 910.1357469558716, 'accumulated_submission_time': 5.84829306602478, 'accumulated_eval_time': 904.2871017456055, 'accumulated_logging_time': 0}
I0607 20:48:30.229061 139883678897920 logging_writer.py:48] [1] accumulated_eval_time=904.287102, accumulated_logging_time=0, accumulated_submission_time=5.848293, global_step=1, preemption_count=0, score=5.848293, test/loss=1.098813, test/num_examples=89274637, total_duration=910.135747, train/loss=1.098282, validation/loss=1.103175, validation/num_examples=89000000
I0607 20:48:30.254252 139947217250112 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254272 139924922414912 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254279 140095327491904 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254282 140172107470656 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254299 140458704885568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254287 139809674651456 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254302 139818010933056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:30.254292 139714161289024 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 20:48:31.434005 139883326732032 logging_writer.py:48] [1] global_step=1, grad_norm=6.839038, loss=1.101182
I0607 20:48:31.437410 139947217250112 submission.py:139] 1) loss = 1.101, grad_norm = 6.839
I0607 20:48:32.614182 139883678897920 logging_writer.py:48] [2] global_step=2, grad_norm=6.434963, loss=0.996919
I0607 20:48:32.617691 139947217250112 submission.py:139] 2) loss = 0.997, grad_norm = 6.435
I0607 20:48:33.793950 139883326732032 logging_writer.py:48] [3] global_step=3, grad_norm=5.714754, loss=0.768441
I0607 20:48:33.797301 139947217250112 submission.py:139] 3) loss = 0.768, grad_norm = 5.715
I0607 20:48:34.980161 139883678897920 logging_writer.py:48] [4] global_step=4, grad_norm=4.238300, loss=0.473937
I0607 20:48:34.983424 139947217250112 submission.py:139] 4) loss = 0.474, grad_norm = 4.238
I0607 20:48:36.138262 139883326732032 logging_writer.py:48] [5] global_step=5, grad_norm=1.972619, loss=0.256959
I0607 20:48:36.141489 139947217250112 submission.py:139] 5) loss = 0.257, grad_norm = 1.973
I0607 20:48:37.283204 139883678897920 logging_writer.py:48] [6] global_step=6, grad_norm=0.236651, loss=0.190362
I0607 20:48:37.286685 139947217250112 submission.py:139] 6) loss = 0.190, grad_norm = 0.237
I0607 20:48:38.433324 139883326732032 logging_writer.py:48] [7] global_step=7, grad_norm=1.000892, loss=0.218433
I0607 20:48:38.436657 139947217250112 submission.py:139] 7) loss = 0.218, grad_norm = 1.001
I0607 20:48:39.575222 139883678897920 logging_writer.py:48] [8] global_step=8, grad_norm=1.638869, loss=0.286177
I0607 20:48:39.578423 139947217250112 submission.py:139] 8) loss = 0.286, grad_norm = 1.639
I0607 20:48:40.708716 139883326732032 logging_writer.py:48] [9] global_step=9, grad_norm=2.213784, loss=0.373249
I0607 20:48:40.712226 139947217250112 submission.py:139] 9) loss = 0.373, grad_norm = 2.214
I0607 20:48:41.855011 139883678897920 logging_writer.py:48] [10] global_step=10, grad_norm=2.543634, loss=0.430250
I0607 20:48:41.858308 139947217250112 submission.py:139] 10) loss = 0.430, grad_norm = 2.544
I0607 20:48:43.005212 139883326732032 logging_writer.py:48] [11] global_step=11, grad_norm=2.458000, loss=0.416674
I0607 20:48:43.008454 139947217250112 submission.py:139] 11) loss = 0.417, grad_norm = 2.458
I0607 20:48:44.145169 139883678897920 logging_writer.py:48] [12] global_step=12, grad_norm=2.177295, loss=0.365619
I0607 20:48:44.148422 139947217250112 submission.py:139] 12) loss = 0.366, grad_norm = 2.177
I0607 20:48:45.289388 139883326732032 logging_writer.py:48] [13] global_step=13, grad_norm=1.720320, loss=0.290324
I0607 20:48:45.292698 139947217250112 submission.py:139] 13) loss = 0.290, grad_norm = 1.720
I0607 20:48:46.421741 139883678897920 logging_writer.py:48] [14] global_step=14, grad_norm=1.123975, loss=0.210735
I0607 20:48:46.425052 139947217250112 submission.py:139] 14) loss = 0.211, grad_norm = 1.124
I0607 20:48:47.556599 139883326732032 logging_writer.py:48] [15] global_step=15, grad_norm=0.410096, loss=0.163470
I0607 20:48:47.559714 139947217250112 submission.py:139] 15) loss = 0.163, grad_norm = 0.410
I0607 20:48:48.699827 139883678897920 logging_writer.py:48] [16] global_step=16, grad_norm=0.554466, loss=0.162502
I0607 20:48:48.703134 139947217250112 submission.py:139] 16) loss = 0.163, grad_norm = 0.554
I0607 20:48:49.823341 139883326732032 logging_writer.py:48] [17] global_step=17, grad_norm=1.138800, loss=0.184756
I0607 20:48:49.826793 139947217250112 submission.py:139] 17) loss = 0.185, grad_norm = 1.139
I0607 20:48:50.935744 139883678897920 logging_writer.py:48] [18] global_step=18, grad_norm=1.113910, loss=0.183711
I0607 20:48:50.939171 139947217250112 submission.py:139] 18) loss = 0.184, grad_norm = 1.114
I0607 20:48:52.057295 139883326732032 logging_writer.py:48] [19] global_step=19, grad_norm=0.596007, loss=0.164173
I0607 20:48:52.060731 139947217250112 submission.py:139] 19) loss = 0.164, grad_norm = 0.596
I0607 20:48:53.172047 139883678897920 logging_writer.py:48] [20] global_step=20, grad_norm=0.101860, loss=0.158054
I0607 20:48:53.175165 139947217250112 submission.py:139] 20) loss = 0.158, grad_norm = 0.102
I0607 20:48:54.298090 139883326732032 logging_writer.py:48] [21] global_step=21, grad_norm=0.282950, loss=0.155660
I0607 20:48:54.301460 139947217250112 submission.py:139] 21) loss = 0.156, grad_norm = 0.283
I0607 20:48:55.425466 139883678897920 logging_writer.py:48] [22] global_step=22, grad_norm=0.461578, loss=0.159909
I0607 20:48:55.428940 139947217250112 submission.py:139] 22) loss = 0.160, grad_norm = 0.462
I0607 20:48:56.545898 139883326732032 logging_writer.py:48] [23] global_step=23, grad_norm=0.489428, loss=0.162077
I0607 20:48:56.549085 139947217250112 submission.py:139] 23) loss = 0.162, grad_norm = 0.489
I0607 20:48:57.695370 139883678897920 logging_writer.py:48] [24] global_step=24, grad_norm=0.351348, loss=0.157319
I0607 20:48:57.698844 139947217250112 submission.py:139] 24) loss = 0.157, grad_norm = 0.351
I0607 20:48:58.831645 139883326732032 logging_writer.py:48] [25] global_step=25, grad_norm=0.163084, loss=0.154480
I0607 20:48:58.834919 139947217250112 submission.py:139] 25) loss = 0.154, grad_norm = 0.163
I0607 20:48:59.960338 139883678897920 logging_writer.py:48] [26] global_step=26, grad_norm=0.077289, loss=0.153792
I0607 20:48:59.963648 139947217250112 submission.py:139] 26) loss = 0.154, grad_norm = 0.077
I0607 20:49:01.115297 139883326732032 logging_writer.py:48] [27] global_step=27, grad_norm=0.168418, loss=0.151993
I0607 20:49:01.118591 139947217250112 submission.py:139] 27) loss = 0.152, grad_norm = 0.168
I0607 20:49:02.309385 139883678897920 logging_writer.py:48] [28] global_step=28, grad_norm=0.154969, loss=0.150608
I0607 20:49:02.312793 139947217250112 submission.py:139] 28) loss = 0.151, grad_norm = 0.155
I0607 20:49:03.473406 139883326732032 logging_writer.py:48] [29] global_step=29, grad_norm=0.081494, loss=0.150404
I0607 20:49:03.476945 139947217250112 submission.py:139] 29) loss = 0.150, grad_norm = 0.081
I0607 20:49:04.618051 139883678897920 logging_writer.py:48] [30] global_step=30, grad_norm=0.048117, loss=0.149676
I0607 20:49:04.621305 139947217250112 submission.py:139] 30) loss = 0.150, grad_norm = 0.048
I0607 20:49:05.753872 139883326732032 logging_writer.py:48] [31] global_step=31, grad_norm=0.057180, loss=0.149226
I0607 20:49:05.757222 139947217250112 submission.py:139] 31) loss = 0.149, grad_norm = 0.057
I0607 20:49:06.905099 139883678897920 logging_writer.py:48] [32] global_step=32, grad_norm=0.048702, loss=0.146286
I0607 20:49:06.908444 139947217250112 submission.py:139] 32) loss = 0.146, grad_norm = 0.049
I0607 20:49:08.050808 139883326732032 logging_writer.py:48] [33] global_step=33, grad_norm=0.052306, loss=0.146964
I0607 20:49:08.054122 139947217250112 submission.py:139] 33) loss = 0.147, grad_norm = 0.052
I0607 20:49:09.196142 139883678897920 logging_writer.py:48] [34] global_step=34, grad_norm=0.052861, loss=0.148813
I0607 20:49:09.199338 139947217250112 submission.py:139] 34) loss = 0.149, grad_norm = 0.053
I0607 20:49:10.354768 139883326732032 logging_writer.py:48] [35] global_step=35, grad_norm=0.033185, loss=0.147026
I0607 20:49:10.358136 139947217250112 submission.py:139] 35) loss = 0.147, grad_norm = 0.033
I0607 20:49:11.513933 139883678897920 logging_writer.py:48] [36] global_step=36, grad_norm=0.033168, loss=0.145532
I0607 20:49:11.517340 139947217250112 submission.py:139] 36) loss = 0.146, grad_norm = 0.033
I0607 20:49:12.670383 139883326732032 logging_writer.py:48] [37] global_step=37, grad_norm=0.027270, loss=0.146319
I0607 20:49:12.673752 139947217250112 submission.py:139] 37) loss = 0.146, grad_norm = 0.027
I0607 20:49:13.840418 139883678897920 logging_writer.py:48] [38] global_step=38, grad_norm=0.033008, loss=0.147525
I0607 20:49:13.843566 139947217250112 submission.py:139] 38) loss = 0.148, grad_norm = 0.033
I0607 20:49:15.005637 139883326732032 logging_writer.py:48] [39] global_step=39, grad_norm=0.026504, loss=0.146577
I0607 20:49:15.009012 139947217250112 submission.py:139] 39) loss = 0.147, grad_norm = 0.027
I0607 20:49:16.174037 139883678897920 logging_writer.py:48] [40] global_step=40, grad_norm=0.026350, loss=0.145322
I0607 20:49:16.177215 139947217250112 submission.py:139] 40) loss = 0.145, grad_norm = 0.026
I0607 20:49:17.354742 139883326732032 logging_writer.py:48] [41] global_step=41, grad_norm=0.033354, loss=0.145345
I0607 20:49:17.358108 139947217250112 submission.py:139] 41) loss = 0.145, grad_norm = 0.033
I0607 20:49:18.516247 139883678897920 logging_writer.py:48] [42] global_step=42, grad_norm=0.028148, loss=0.145161
I0607 20:49:18.519439 139947217250112 submission.py:139] 42) loss = 0.145, grad_norm = 0.028
I0607 20:49:19.674877 139883326732032 logging_writer.py:48] [43] global_step=43, grad_norm=0.036368, loss=0.147168
I0607 20:49:19.678297 139947217250112 submission.py:139] 43) loss = 0.147, grad_norm = 0.036
I0607 20:49:20.841825 139883678897920 logging_writer.py:48] [44] global_step=44, grad_norm=0.025029, loss=0.145435
I0607 20:49:20.845164 139947217250112 submission.py:139] 44) loss = 0.145, grad_norm = 0.025
I0607 20:49:22.008564 139883326732032 logging_writer.py:48] [45] global_step=45, grad_norm=0.027031, loss=0.144212
I0607 20:49:22.011840 139947217250112 submission.py:139] 45) loss = 0.144, grad_norm = 0.027
I0607 20:49:23.170975 139883678897920 logging_writer.py:48] [46] global_step=46, grad_norm=0.021151, loss=0.143373
I0607 20:49:23.174307 139947217250112 submission.py:139] 46) loss = 0.143, grad_norm = 0.021
I0607 20:49:24.326584 139883326732032 logging_writer.py:48] [47] global_step=47, grad_norm=0.022749, loss=0.142268
I0607 20:49:24.329941 139947217250112 submission.py:139] 47) loss = 0.142, grad_norm = 0.023
I0607 20:49:25.502361 139883678897920 logging_writer.py:48] [48] global_step=48, grad_norm=0.022219, loss=0.142421
I0607 20:49:25.505803 139947217250112 submission.py:139] 48) loss = 0.142, grad_norm = 0.022
I0607 20:49:26.699831 139883326732032 logging_writer.py:48] [49] global_step=49, grad_norm=0.025378, loss=0.143337
I0607 20:49:26.703403 139947217250112 submission.py:139] 49) loss = 0.143, grad_norm = 0.025
I0607 20:49:27.863334 139883678897920 logging_writer.py:48] [50] global_step=50, grad_norm=0.020899, loss=0.144041
I0607 20:49:27.866567 139947217250112 submission.py:139] 50) loss = 0.144, grad_norm = 0.021
I0607 20:49:29.026193 139883326732032 logging_writer.py:48] [51] global_step=51, grad_norm=0.027391, loss=0.141087
I0607 20:49:29.029795 139947217250112 submission.py:139] 51) loss = 0.141, grad_norm = 0.027
I0607 20:49:30.184184 139883678897920 logging_writer.py:48] [52] global_step=52, grad_norm=0.015391, loss=0.141471
I0607 20:49:30.187382 139947217250112 submission.py:139] 52) loss = 0.141, grad_norm = 0.015
I0607 20:49:31.351724 139883326732032 logging_writer.py:48] [53] global_step=53, grad_norm=0.015919, loss=0.141456
I0607 20:49:31.355328 139947217250112 submission.py:139] 53) loss = 0.141, grad_norm = 0.016
I0607 20:49:32.513947 139883678897920 logging_writer.py:48] [54] global_step=54, grad_norm=0.018248, loss=0.142326
I0607 20:49:32.517332 139947217250112 submission.py:139] 54) loss = 0.142, grad_norm = 0.018
I0607 20:49:33.662411 139883326732032 logging_writer.py:48] [55] global_step=55, grad_norm=0.021288, loss=0.144623
I0607 20:49:33.665855 139947217250112 submission.py:139] 55) loss = 0.145, grad_norm = 0.021
I0607 20:49:34.830200 139883678897920 logging_writer.py:48] [56] global_step=56, grad_norm=0.021406, loss=0.142145
I0607 20:49:34.833378 139947217250112 submission.py:139] 56) loss = 0.142, grad_norm = 0.021
I0607 20:49:35.991563 139883326732032 logging_writer.py:48] [57] global_step=57, grad_norm=0.035894, loss=0.133651
I0607 20:49:35.995087 139947217250112 submission.py:139] 57) loss = 0.134, grad_norm = 0.036
I0607 20:49:37.152523 139883678897920 logging_writer.py:48] [58] global_step=58, grad_norm=0.015207, loss=0.134128
I0607 20:49:37.155720 139947217250112 submission.py:139] 58) loss = 0.134, grad_norm = 0.015
I0607 20:49:38.334737 139883326732032 logging_writer.py:48] [59] global_step=59, grad_norm=0.018119, loss=0.133331
I0607 20:49:38.337871 139947217250112 submission.py:139] 59) loss = 0.133, grad_norm = 0.018
I0607 20:49:39.518686 139883678897920 logging_writer.py:48] [60] global_step=60, grad_norm=0.021475, loss=0.133463
I0607 20:49:39.521954 139947217250112 submission.py:139] 60) loss = 0.133, grad_norm = 0.021
I0607 20:49:40.707323 139883326732032 logging_writer.py:48] [61] global_step=61, grad_norm=0.015809, loss=0.134085
I0607 20:49:40.710604 139947217250112 submission.py:139] 61) loss = 0.134, grad_norm = 0.016
I0607 20:49:41.882985 139883678897920 logging_writer.py:48] [62] global_step=62, grad_norm=0.016020, loss=0.132613
I0607 20:49:41.886266 139947217250112 submission.py:139] 62) loss = 0.133, grad_norm = 0.016
I0607 20:49:43.039444 139883326732032 logging_writer.py:48] [63] global_step=63, grad_norm=0.025953, loss=0.129504
I0607 20:49:43.042804 139947217250112 submission.py:139] 63) loss = 0.130, grad_norm = 0.026
I0607 20:49:44.197675 139883678897920 logging_writer.py:48] [64] global_step=64, grad_norm=0.033682, loss=0.134312
I0607 20:49:44.201081 139947217250112 submission.py:139] 64) loss = 0.134, grad_norm = 0.034
I0607 20:49:45.343878 139883326732032 logging_writer.py:48] [65] global_step=65, grad_norm=0.010173, loss=0.133230
I0607 20:49:45.347188 139947217250112 submission.py:139] 65) loss = 0.133, grad_norm = 0.010
I0607 20:49:46.497806 139883678897920 logging_writer.py:48] [66] global_step=66, grad_norm=0.009479, loss=0.133896
I0607 20:49:46.501136 139947217250112 submission.py:139] 66) loss = 0.134, grad_norm = 0.009
I0607 20:49:47.661294 139883326732032 logging_writer.py:48] [67] global_step=67, grad_norm=0.022280, loss=0.131901
I0607 20:49:47.664728 139947217250112 submission.py:139] 67) loss = 0.132, grad_norm = 0.022
I0607 20:49:48.832115 139883678897920 logging_writer.py:48] [68] global_step=68, grad_norm=0.013404, loss=0.133542
I0607 20:49:48.835260 139947217250112 submission.py:139] 68) loss = 0.134, grad_norm = 0.013
I0607 20:49:49.988845 139883326732032 logging_writer.py:48] [69] global_step=69, grad_norm=0.011242, loss=0.131337
I0607 20:49:49.992125 139947217250112 submission.py:139] 69) loss = 0.131, grad_norm = 0.011
I0607 20:49:51.143898 139883678897920 logging_writer.py:48] [70] global_step=70, grad_norm=0.009042, loss=0.131790
I0607 20:49:51.147473 139947217250112 submission.py:139] 70) loss = 0.132, grad_norm = 0.009
I0607 20:49:52.300953 139883326732032 logging_writer.py:48] [71] global_step=71, grad_norm=0.008770, loss=0.131451
I0607 20:49:52.304010 139947217250112 submission.py:139] 71) loss = 0.131, grad_norm = 0.009
I0607 20:49:53.452620 139883678897920 logging_writer.py:48] [72] global_step=72, grad_norm=0.015402, loss=0.132896
I0607 20:49:53.455833 139947217250112 submission.py:139] 72) loss = 0.133, grad_norm = 0.015
I0607 20:49:54.621836 139883326732032 logging_writer.py:48] [73] global_step=73, grad_norm=0.008417, loss=0.132868
I0607 20:49:54.625047 139947217250112 submission.py:139] 73) loss = 0.133, grad_norm = 0.008
I0607 20:49:55.779271 139883678897920 logging_writer.py:48] [74] global_step=74, grad_norm=0.013121, loss=0.135750
I0607 20:49:55.782671 139947217250112 submission.py:139] 74) loss = 0.136, grad_norm = 0.013
I0607 20:49:56.934105 139883326732032 logging_writer.py:48] [75] global_step=75, grad_norm=0.011951, loss=0.135002
I0607 20:49:56.937341 139947217250112 submission.py:139] 75) loss = 0.135, grad_norm = 0.012
I0607 20:49:58.089783 139883678897920 logging_writer.py:48] [76] global_step=76, grad_norm=0.033978, loss=0.130984
I0607 20:49:58.093007 139947217250112 submission.py:139] 76) loss = 0.131, grad_norm = 0.034
I0607 20:49:59.240089 139883326732032 logging_writer.py:48] [77] global_step=77, grad_norm=0.010493, loss=0.131625
I0607 20:49:59.243373 139947217250112 submission.py:139] 77) loss = 0.132, grad_norm = 0.010
I0607 20:50:00.389928 139883678897920 logging_writer.py:48] [78] global_step=78, grad_norm=0.009796, loss=0.129050
I0607 20:50:00.393228 139947217250112 submission.py:139] 78) loss = 0.129, grad_norm = 0.010
I0607 20:50:01.549879 139883326732032 logging_writer.py:48] [79] global_step=79, grad_norm=0.024336, loss=0.131085
I0607 20:50:01.553147 139947217250112 submission.py:139] 79) loss = 0.131, grad_norm = 0.024
I0607 20:50:02.762300 139883678897920 logging_writer.py:48] [80] global_step=80, grad_norm=0.008803, loss=0.130738
I0607 20:50:02.765577 139947217250112 submission.py:139] 80) loss = 0.131, grad_norm = 0.009
I0607 20:50:03.926915 139883326732032 logging_writer.py:48] [81] global_step=81, grad_norm=0.008721, loss=0.130132
I0607 20:50:03.930979 139947217250112 submission.py:139] 81) loss = 0.130, grad_norm = 0.009
I0607 20:50:05.084615 139883678897920 logging_writer.py:48] [82] global_step=82, grad_norm=0.008936, loss=0.131526
I0607 20:50:05.087838 139947217250112 submission.py:139] 82) loss = 0.132, grad_norm = 0.009
I0607 20:50:06.241609 139883326732032 logging_writer.py:48] [83] global_step=83, grad_norm=0.012182, loss=0.129661
I0607 20:50:06.244823 139947217250112 submission.py:139] 83) loss = 0.130, grad_norm = 0.012
I0607 20:50:07.396811 139883678897920 logging_writer.py:48] [84] global_step=84, grad_norm=0.012301, loss=0.127904
I0607 20:50:07.400097 139947217250112 submission.py:139] 84) loss = 0.128, grad_norm = 0.012
I0607 20:50:08.552819 139883326732032 logging_writer.py:48] [85] global_step=85, grad_norm=0.022247, loss=0.130557
I0607 20:50:08.556251 139947217250112 submission.py:139] 85) loss = 0.131, grad_norm = 0.022
I0607 20:50:09.722695 139883678897920 logging_writer.py:48] [86] global_step=86, grad_norm=0.008647, loss=0.130873
I0607 20:50:09.726015 139947217250112 submission.py:139] 86) loss = 0.131, grad_norm = 0.009
I0607 20:50:10.884453 139883326732032 logging_writer.py:48] [87] global_step=87, grad_norm=0.023016, loss=0.127308
I0607 20:50:10.887655 139947217250112 submission.py:139] 87) loss = 0.127, grad_norm = 0.023
I0607 20:50:12.038023 139883678897920 logging_writer.py:48] [88] global_step=88, grad_norm=0.014301, loss=0.129890
I0607 20:50:12.041433 139947217250112 submission.py:139] 88) loss = 0.130, grad_norm = 0.014
I0607 20:50:13.196303 139883326732032 logging_writer.py:48] [89] global_step=89, grad_norm=0.006247, loss=0.129174
I0607 20:50:13.199457 139947217250112 submission.py:139] 89) loss = 0.129, grad_norm = 0.006
I0607 20:50:14.360627 139883678897920 logging_writer.py:48] [90] global_step=90, grad_norm=0.007704, loss=0.128088
I0607 20:50:14.363903 139947217250112 submission.py:139] 90) loss = 0.128, grad_norm = 0.008
I0607 20:50:15.549579 139883326732032 logging_writer.py:48] [91] global_step=91, grad_norm=0.007997, loss=0.127572
I0607 20:50:15.552989 139947217250112 submission.py:139] 91) loss = 0.128, grad_norm = 0.008
I0607 20:50:16.742351 139883678897920 logging_writer.py:48] [92] global_step=92, grad_norm=0.007603, loss=0.126887
I0607 20:50:16.745750 139947217250112 submission.py:139] 92) loss = 0.127, grad_norm = 0.008
I0607 20:50:17.928579 139883326732032 logging_writer.py:48] [93] global_step=93, grad_norm=0.006127, loss=0.127064
I0607 20:50:17.932065 139947217250112 submission.py:139] 93) loss = 0.127, grad_norm = 0.006
I0607 20:50:19.120925 139883678897920 logging_writer.py:48] [94] global_step=94, grad_norm=0.010327, loss=0.128399
I0607 20:50:19.124198 139947217250112 submission.py:139] 94) loss = 0.128, grad_norm = 0.010
I0607 20:50:20.301775 139883326732032 logging_writer.py:48] [95] global_step=95, grad_norm=0.017237, loss=0.137194
I0607 20:50:20.304971 139947217250112 submission.py:139] 95) loss = 0.137, grad_norm = 0.017
I0607 20:50:21.466368 139883678897920 logging_writer.py:48] [96] global_step=96, grad_norm=0.029210, loss=0.147910
I0607 20:50:21.469936 139947217250112 submission.py:139] 96) loss = 0.148, grad_norm = 0.029
I0607 20:50:22.628956 139883326732032 logging_writer.py:48] [97] global_step=97, grad_norm=0.033024, loss=0.146473
I0607 20:50:22.632679 139947217250112 submission.py:139] 97) loss = 0.146, grad_norm = 0.033
I0607 20:50:23.821980 139883678897920 logging_writer.py:48] [98] global_step=98, grad_norm=0.029242, loss=0.144409
I0607 20:50:23.825321 139947217250112 submission.py:139] 98) loss = 0.144, grad_norm = 0.029
I0607 20:50:24.983381 139883326732032 logging_writer.py:48] [99] global_step=99, grad_norm=0.025783, loss=0.147946
I0607 20:50:24.986657 139947217250112 submission.py:139] 99) loss = 0.148, grad_norm = 0.026
I0607 20:50:26.161749 139883678897920 logging_writer.py:48] [100] global_step=100, grad_norm=0.006631, loss=0.148699
I0607 20:50:26.165055 139947217250112 submission.py:139] 100) loss = 0.149, grad_norm = 0.007
I0607 20:50:30.793352 139947217250112 spec.py:298] Evaluating on the training split.
I0607 20:55:06.607949 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 20:59:35.018171 139947217250112 spec.py:326] Evaluating on the test split.
I0607 21:03:51.965766 139947217250112 submission_runner.py:419] Time since start: 1831.89s, 	Step: 105, 	{'train/loss': 0.13834888045624655, 'validation/loss': 0.1381338202247191, 'validation/num_examples': 89000000, 'test/loss': 0.14176683798781506, 'test/num_examples': 89274637, 'score': 126.36871337890625, 'total_duration': 1831.8900990486145, 'accumulated_submission_time': 126.36871337890625, 'accumulated_eval_time': 1705.459403038025, 'accumulated_logging_time': 0.025178194046020508}
I0607 21:03:51.977926 139883326732032 logging_writer.py:48] [105] accumulated_eval_time=1705.459403, accumulated_logging_time=0.025178, accumulated_submission_time=126.368713, global_step=105, preemption_count=0, score=126.368713, test/loss=0.141767, test/num_examples=89274637, total_duration=1831.890099, train/loss=0.138349, validation/loss=0.138134, validation/num_examples=89000000
I0607 21:05:52.508827 139947217250112 spec.py:298] Evaluating on the training split.
I0607 21:10:43.338799 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 21:15:15.203173 139947217250112 spec.py:326] Evaluating on the test split.
I0607 21:19:36.652821 139947217250112 submission_runner.py:419] Time since start: 2776.58s, 	Step: 213, 	{'train/loss': 0.13842237864109064, 'validation/loss': 0.1374721011235955, 'validation/num_examples': 89000000, 'test/loss': 0.14131796469808106, 'test/num_examples': 89274637, 'score': 246.84436798095703, 'total_duration': 2776.5771527290344, 'accumulated_submission_time': 246.84436798095703, 'accumulated_eval_time': 2529.6032695770264, 'accumulated_logging_time': 0.04415631294250488}
I0607 21:19:36.662514 139883678897920 logging_writer.py:48] [213] accumulated_eval_time=2529.603270, accumulated_logging_time=0.044156, accumulated_submission_time=246.844368, global_step=213, preemption_count=0, score=246.844368, test/loss=0.141318, test/num_examples=89274637, total_duration=2776.577153, train/loss=0.138422, validation/loss=0.137472, validation/num_examples=89000000
I0607 21:21:36.887780 139947217250112 spec.py:298] Evaluating on the training split.
I0607 21:26:22.478375 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 21:30:49.907030 139947217250112 spec.py:326] Evaluating on the test split.
I0607 21:35:06.999641 139947217250112 submission_runner.py:419] Time since start: 3706.92s, 	Step: 320, 	{'train/loss': 0.1372596454409837, 'validation/loss': 0.136774595505618, 'validation/num_examples': 89000000, 'test/loss': 0.1406222687861503, 'test/num_examples': 89274637, 'score': 367.0154905319214, 'total_duration': 3706.923928260803, 'accumulated_submission_time': 367.0154905319214, 'accumulated_eval_time': 3339.7152309417725, 'accumulated_logging_time': 0.060791730880737305}
I0607 21:35:07.021315 139883326732032 logging_writer.py:48] [320] accumulated_eval_time=3339.715231, accumulated_logging_time=0.060792, accumulated_submission_time=367.015491, global_step=320, preemption_count=0, score=367.015491, test/loss=0.140622, test/num_examples=89274637, total_duration=3706.923928, train/loss=0.137260, validation/loss=0.136775, validation/num_examples=89000000
I0607 21:37:07.520720 139947217250112 spec.py:298] Evaluating on the training split.
I0607 21:41:44.426556 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 21:46:13.497093 139947217250112 spec.py:326] Evaluating on the test split.
I0607 21:50:30.953950 139947217250112 submission_runner.py:419] Time since start: 4630.88s, 	Step: 431, 	{'train/loss': 0.1357058762977718, 'validation/loss': 0.13661767415730336, 'validation/num_examples': 89000000, 'test/loss': 0.14042511312591505, 'test/num_examples': 89274637, 'score': 487.4668912887573, 'total_duration': 4630.878333806992, 'accumulated_submission_time': 487.4668912887573, 'accumulated_eval_time': 4143.148376464844, 'accumulated_logging_time': 0.08957791328430176}
I0607 21:50:30.964670 139883678897920 logging_writer.py:48] [431] accumulated_eval_time=4143.148376, accumulated_logging_time=0.089578, accumulated_submission_time=487.466891, global_step=431, preemption_count=0, score=487.466891, test/loss=0.140425, test/num_examples=89274637, total_duration=4630.878334, train/loss=0.135706, validation/loss=0.136618, validation/num_examples=89000000
I0607 21:51:47.266323 139883326732032 logging_writer.py:48] [500] global_step=500, grad_norm=0.012046, loss=0.139038
I0607 21:51:47.269598 139947217250112 submission.py:139] 500) loss = 0.139, grad_norm = 0.012
I0607 21:52:31.532573 139947217250112 spec.py:298] Evaluating on the training split.
I0607 21:57:14.386781 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 22:01:45.290133 139947217250112 spec.py:326] Evaluating on the test split.
I0607 22:06:01.011460 139947217250112 submission_runner.py:419] Time since start: 5560.94s, 	Step: 542, 	{'train/loss': 0.13557538502000552, 'validation/loss': 0.13535873033707865, 'validation/num_examples': 89000000, 'test/loss': 0.13906496197794677, 'test/num_examples': 89274637, 'score': 607.9848237037659, 'total_duration': 5560.935802698135, 'accumulated_submission_time': 607.9848237037659, 'accumulated_eval_time': 4952.627123832703, 'accumulated_logging_time': 0.10733270645141602}
I0607 22:06:01.048006 139883678897920 logging_writer.py:48] [542] accumulated_eval_time=4952.627124, accumulated_logging_time=0.107333, accumulated_submission_time=607.984824, global_step=542, preemption_count=0, score=607.984824, test/loss=0.139065, test/num_examples=89274637, total_duration=5560.935803, train/loss=0.135575, validation/loss=0.135359, validation/num_examples=89000000
I0607 22:08:02.263447 139947217250112 spec.py:298] Evaluating on the training split.
I0607 22:12:45.209815 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 22:17:18.028543 139947217250112 spec.py:326] Evaluating on the test split.
I0607 22:21:43.377475 139947217250112 submission_runner.py:419] Time since start: 6503.30s, 	Step: 624, 	{'train/loss': 0.1383850569230305, 'validation/loss': 0.13494976404494383, 'validation/num_examples': 89000000, 'test/loss': 0.13861170894483726, 'test/num_examples': 89274637, 'score': 729.1586856842041, 'total_duration': 6503.301774740219, 'accumulated_submission_time': 729.1586856842041, 'accumulated_eval_time': 5773.740987300873, 'accumulated_logging_time': 0.1522829532623291}
I0607 22:21:43.387881 139883326732032 logging_writer.py:48] [624] accumulated_eval_time=5773.740987, accumulated_logging_time=0.152283, accumulated_submission_time=729.158686, global_step=624, preemption_count=0, score=729.158686, test/loss=0.138612, test/num_examples=89274637, total_duration=6503.301775, train/loss=0.138385, validation/loss=0.134950, validation/num_examples=89000000
I0607 22:23:43.613881 139947217250112 spec.py:298] Evaluating on the training split.
I0607 22:28:24.580007 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 22:32:55.662247 139947217250112 spec.py:326] Evaluating on the test split.
I0607 22:37:15.337215 139947217250112 submission_runner.py:419] Time since start: 7435.26s, 	Step: 727, 	{'train/loss': 0.13455041881165494, 'validation/loss': 0.13443660674157304, 'validation/num_examples': 89000000, 'test/loss': 0.13780489524701175, 'test/num_examples': 89274637, 'score': 849.3395309448242, 'total_duration': 7435.261584997177, 'accumulated_submission_time': 849.3395309448242, 'accumulated_eval_time': 6585.464201927185, 'accumulated_logging_time': 0.1696922779083252}
I0607 22:37:15.350395 139883678897920 logging_writer.py:48] [727] accumulated_eval_time=6585.464202, accumulated_logging_time=0.169692, accumulated_submission_time=849.339531, global_step=727, preemption_count=0, score=849.339531, test/loss=0.137805, test/num_examples=89274637, total_duration=7435.261585, train/loss=0.134550, validation/loss=0.134437, validation/num_examples=89000000
I0607 22:39:16.133179 139947217250112 spec.py:298] Evaluating on the training split.
I0607 22:43:56.994820 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 22:48:28.458612 139947217250112 spec.py:326] Evaluating on the test split.
I0607 22:52:53.357032 139947217250112 submission_runner.py:419] Time since start: 8373.28s, 	Step: 830, 	{'train/loss': 0.13633246348130518, 'validation/loss': 0.13421405617977528, 'validation/num_examples': 89000000, 'test/loss': 0.1377109603929277, 'test/num_examples': 89274637, 'score': 970.0747263431549, 'total_duration': 8373.281414270401, 'accumulated_submission_time': 970.0747263431549, 'accumulated_eval_time': 7402.687954425812, 'accumulated_logging_time': 0.19056010246276855}
I0607 22:52:53.367556 139883326732032 logging_writer.py:48] [830] accumulated_eval_time=7402.687954, accumulated_logging_time=0.190560, accumulated_submission_time=970.074726, global_step=830, preemption_count=0, score=970.074726, test/loss=0.137711, test/num_examples=89274637, total_duration=8373.281414, train/loss=0.136332, validation/loss=0.134214, validation/num_examples=89000000
I0607 22:54:53.694268 139947217250112 spec.py:298] Evaluating on the training split.
I0607 22:59:20.600697 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 23:03:54.283050 139947217250112 spec.py:326] Evaluating on the test split.
I0607 23:08:12.900412 139947217250112 submission_runner.py:419] Time since start: 9292.82s, 	Step: 932, 	{'train/loss': 0.13133111747421702, 'validation/loss': 0.13297338202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13609504791377644, 'test/num_examples': 89274637, 'score': 1090.357717037201, 'total_duration': 9292.82474398613, 'accumulated_submission_time': 1090.357717037201, 'accumulated_eval_time': 8201.893949270248, 'accumulated_logging_time': 0.2076272964477539}
I0607 23:08:12.914181 139883678897920 logging_writer.py:48] [932] accumulated_eval_time=8201.893949, accumulated_logging_time=0.207627, accumulated_submission_time=1090.357717, global_step=932, preemption_count=0, score=1090.357717, test/loss=0.136095, test/num_examples=89274637, total_duration=9292.824744, train/loss=0.131331, validation/loss=0.132973, validation/num_examples=89000000
I0607 23:09:29.788179 139883326732032 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.018083, loss=0.127382
I0607 23:09:29.792000 139947217250112 submission.py:139] 1000) loss = 0.127, grad_norm = 0.018
I0607 23:10:13.686108 139947217250112 spec.py:298] Evaluating on the training split.
I0607 23:14:52.647467 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 23:19:27.151651 139947217250112 spec.py:326] Evaluating on the test split.
I0607 23:23:53.456162 139947217250112 submission_runner.py:419] Time since start: 10233.38s, 	Step: 1041, 	{'train/loss': 0.13179821157560706, 'validation/loss': 0.13205370786516854, 'validation/num_examples': 89000000, 'test/loss': 0.13495858851825968, 'test/num_examples': 89274637, 'score': 1211.0848245620728, 'total_duration': 10233.380543231964, 'accumulated_submission_time': 1211.0848245620728, 'accumulated_eval_time': 9021.66390299797, 'accumulated_logging_time': 0.22860097885131836}
I0607 23:23:53.465984 139883678897920 logging_writer.py:48] [1041] accumulated_eval_time=9021.663903, accumulated_logging_time=0.228601, accumulated_submission_time=1211.084825, global_step=1041, preemption_count=0, score=1211.084825, test/loss=0.134959, test/num_examples=89274637, total_duration=10233.380543, train/loss=0.131798, validation/loss=0.132054, validation/num_examples=89000000
I0607 23:25:53.494875 139947217250112 spec.py:298] Evaluating on the training split.
I0607 23:30:24.984237 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 23:34:58.417143 139947217250112 spec.py:326] Evaluating on the test split.
I0607 23:39:17.122453 139947217250112 submission_runner.py:419] Time since start: 11157.05s, 	Step: 1151, 	{'train/loss': 0.13182655093071652, 'validation/loss': 0.1318211797752809, 'validation/num_examples': 89000000, 'test/loss': 0.13455023065509636, 'test/num_examples': 89274637, 'score': 1331.0657360553741, 'total_duration': 11157.046851158142, 'accumulated_submission_time': 1331.0657360553741, 'accumulated_eval_time': 9825.291404724121, 'accumulated_logging_time': 0.24524426460266113}
I0607 23:39:17.131989 139883326732032 logging_writer.py:48] [1151] accumulated_eval_time=9825.291405, accumulated_logging_time=0.245244, accumulated_submission_time=1331.065736, global_step=1151, preemption_count=0, score=1331.065736, test/loss=0.134550, test/num_examples=89274637, total_duration=11157.046851, train/loss=0.131827, validation/loss=0.131821, validation/num_examples=89000000
I0607 23:41:17.235696 139947217250112 spec.py:298] Evaluating on the training split.
I0607 23:45:57.401911 139947217250112 spec.py:310] Evaluating on the validation split.
I0607 23:50:30.639706 139947217250112 spec.py:326] Evaluating on the test split.
I0607 23:54:58.795465 139947217250112 submission_runner.py:419] Time since start: 12098.72s, 	Step: 1248, 	{'train/loss': 0.1313659016747436, 'validation/loss': 0.13100360674157302, 'validation/num_examples': 89000000, 'test/loss': 0.13381753655296297, 'test/num_examples': 89274637, 'score': 1451.1282398700714, 'total_duration': 12098.71983242035, 'accumulated_submission_time': 1451.1282398700714, 'accumulated_eval_time': 10646.851093292236, 'accumulated_logging_time': 0.2610602378845215}
I0607 23:54:58.805243 139883678897920 logging_writer.py:48] [1248] accumulated_eval_time=10646.851093, accumulated_logging_time=0.261060, accumulated_submission_time=1451.128240, global_step=1248, preemption_count=0, score=1451.128240, test/loss=0.133818, test/num_examples=89274637, total_duration=12098.719832, train/loss=0.131366, validation/loss=0.131004, validation/num_examples=89000000
I0607 23:56:59.395791 139947217250112 spec.py:298] Evaluating on the training split.
I0608 00:01:39.464715 139947217250112 spec.py:310] Evaluating on the validation split.
I0608 00:06:14.105286 139947217250112 spec.py:326] Evaluating on the test split.
I0608 00:10:34.380302 139947217250112 submission_runner.py:419] Time since start: 13034.30s, 	Step: 1340, 	{'train/loss': 0.12948756607426187, 'validation/loss': 0.1310783820224719, 'validation/num_examples': 89000000, 'test/loss': 0.13365394025629027, 'test/num_examples': 89274637, 'score': 1571.6791977882385, 'total_duration': 13034.304693937302, 'accumulated_submission_time': 1571.6791977882385, 'accumulated_eval_time': 11461.835547208786, 'accumulated_logging_time': 0.2772552967071533}
I0608 00:10:34.389918 139883326732032 logging_writer.py:48] [1340] accumulated_eval_time=11461.835547, accumulated_logging_time=0.277255, accumulated_submission_time=1571.679198, global_step=1340, preemption_count=0, score=1571.679198, test/loss=0.133654, test/num_examples=89274637, total_duration=13034.304694, train/loss=0.129488, validation/loss=0.131078, validation/num_examples=89000000
I0608 00:12:34.905576 139947217250112 spec.py:298] Evaluating on the training split.
I0608 00:17:03.643711 139947217250112 spec.py:310] Evaluating on the validation split.
I0608 00:21:38.623195 139947217250112 spec.py:326] Evaluating on the test split.
I0608 00:26:04.679149 139947217250112 submission_runner.py:419] Time since start: 13964.60s, 	Step: 1438, 	{'train/loss': 0.13098479124269913, 'validation/loss': 0.13187124719101123, 'validation/num_examples': 89000000, 'test/loss': 0.1344772872053235, 'test/num_examples': 89274637, 'score': 1692.152723789215, 'total_duration': 13964.60353899002, 'accumulated_submission_time': 1692.152723789215, 'accumulated_eval_time': 12271.60903096199, 'accumulated_logging_time': 0.2931537628173828}
I0608 00:26:04.688571 139883678897920 logging_writer.py:48] [1438] accumulated_eval_time=12271.609031, accumulated_logging_time=0.293154, accumulated_submission_time=1692.152724, global_step=1438, preemption_count=0, score=1692.152724, test/loss=0.134477, test/num_examples=89274637, total_duration=13964.603539, train/loss=0.130985, validation/loss=0.131871, validation/num_examples=89000000
I0608 00:27:23.788323 139883326732032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.020189, loss=0.123279
I0608 00:27:23.791399 139947217250112 submission.py:139] 1500) loss = 0.123, grad_norm = 0.020
I0608 00:28:05.391712 139947217250112 spec.py:298] Evaluating on the training split.
I0608 00:32:50.178146 139947217250112 spec.py:310] Evaluating on the validation split.
I0608 00:37:22.665116 139947217250112 spec.py:326] Evaluating on the test split.
I0608 00:41:46.610408 139947217250112 submission_runner.py:419] Time since start: 14906.53s, 	Step: 1535, 	{'train/loss': 0.12867068753161792, 'validation/loss': 0.12974478651685392, 'validation/num_examples': 89000000, 'test/loss': 0.13246504715555438, 'test/num_examples': 89274637, 'score': 1812.814180612564, 'total_duration': 14906.534751176834, 'accumulated_submission_time': 1812.814180612564, 'accumulated_eval_time': 13092.827597856522, 'accumulated_logging_time': 0.3098025321960449}
I0608 00:41:46.620412 139883678897920 logging_writer.py:48] [1535] accumulated_eval_time=13092.827598, accumulated_logging_time=0.309803, accumulated_submission_time=1812.814181, global_step=1535, preemption_count=0, score=1812.814181, test/loss=0.132465, test/num_examples=89274637, total_duration=14906.534751, train/loss=0.128671, validation/loss=0.129745, validation/num_examples=89000000
I0608 00:42:57.961428 139947217250112 spec.py:298] Evaluating on the training split.
I0608 00:47:28.784968 139947217250112 spec.py:310] Evaluating on the validation split.
I0608 00:52:01.992492 139947217250112 spec.py:326] Evaluating on the test split.
I0608 00:56:22.228493 139947217250112 submission_runner.py:419] Time since start: 15782.15s, 	Step: 1600, 	{'train/loss': 0.1285579139647719, 'validation/loss': 0.1297640449438202, 'validation/num_examples': 89000000, 'test/loss': 0.1326603769892674, 'test/num_examples': 89274637, 'score': 1884.1244649887085, 'total_duration': 15782.152899503708, 'accumulated_submission_time': 1884.1244649887085, 'accumulated_eval_time': 13897.094574689865, 'accumulated_logging_time': 0.32636094093322754}
I0608 00:56:22.238190 139883326732032 logging_writer.py:48] [1600] accumulated_eval_time=13897.094575, accumulated_logging_time=0.326361, accumulated_submission_time=1884.124465, global_step=1600, preemption_count=0, score=1884.124465, test/loss=0.132660, test/num_examples=89274637, total_duration=15782.152900, train/loss=0.128558, validation/loss=0.129764, validation/num_examples=89000000
I0608 00:56:22.253494 139883678897920 logging_writer.py:48] [1600] global_step=1600, preemption_count=0, score=1884.124465
I0608 00:56:29.528657 139947217250112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nesterov/criteo1tb_pytorch/trial_1/checkpoint_1600.
I0608 00:56:29.601995 139947217250112 submission_runner.py:581] Tuning trial 1/1
I0608 00:56:29.602245 139947217250112 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0608 00:56:29.603362 139947217250112 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/loss': 1.0982816021086277, 'validation/loss': 1.1031749213483146, 'validation/num_examples': 89000000, 'test/loss': 1.0988127792667475, 'test/num_examples': 89274637, 'score': 5.84829306602478, 'total_duration': 910.1357469558716, 'accumulated_submission_time': 5.84829306602478, 'accumulated_eval_time': 904.2871017456055, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (105, {'train/loss': 0.13834888045624655, 'validation/loss': 0.1381338202247191, 'validation/num_examples': 89000000, 'test/loss': 0.14176683798781506, 'test/num_examples': 89274637, 'score': 126.36871337890625, 'total_duration': 1831.8900990486145, 'accumulated_submission_time': 126.36871337890625, 'accumulated_eval_time': 1705.459403038025, 'accumulated_logging_time': 0.025178194046020508, 'global_step': 105, 'preemption_count': 0}), (213, {'train/loss': 0.13842237864109064, 'validation/loss': 0.1374721011235955, 'validation/num_examples': 89000000, 'test/loss': 0.14131796469808106, 'test/num_examples': 89274637, 'score': 246.84436798095703, 'total_duration': 2776.5771527290344, 'accumulated_submission_time': 246.84436798095703, 'accumulated_eval_time': 2529.6032695770264, 'accumulated_logging_time': 0.04415631294250488, 'global_step': 213, 'preemption_count': 0}), (320, {'train/loss': 0.1372596454409837, 'validation/loss': 0.136774595505618, 'validation/num_examples': 89000000, 'test/loss': 0.1406222687861503, 'test/num_examples': 89274637, 'score': 367.0154905319214, 'total_duration': 3706.923928260803, 'accumulated_submission_time': 367.0154905319214, 'accumulated_eval_time': 3339.7152309417725, 'accumulated_logging_time': 0.060791730880737305, 'global_step': 320, 'preemption_count': 0}), (431, {'train/loss': 0.1357058762977718, 'validation/loss': 0.13661767415730336, 'validation/num_examples': 89000000, 'test/loss': 0.14042511312591505, 'test/num_examples': 89274637, 'score': 487.4668912887573, 'total_duration': 4630.878333806992, 'accumulated_submission_time': 487.4668912887573, 'accumulated_eval_time': 4143.148376464844, 'accumulated_logging_time': 0.08957791328430176, 'global_step': 431, 'preemption_count': 0}), (542, {'train/loss': 0.13557538502000552, 'validation/loss': 0.13535873033707865, 'validation/num_examples': 89000000, 'test/loss': 0.13906496197794677, 'test/num_examples': 89274637, 'score': 607.9848237037659, 'total_duration': 5560.935802698135, 'accumulated_submission_time': 607.9848237037659, 'accumulated_eval_time': 4952.627123832703, 'accumulated_logging_time': 0.10733270645141602, 'global_step': 542, 'preemption_count': 0}), (624, {'train/loss': 0.1383850569230305, 'validation/loss': 0.13494976404494383, 'validation/num_examples': 89000000, 'test/loss': 0.13861170894483726, 'test/num_examples': 89274637, 'score': 729.1586856842041, 'total_duration': 6503.301774740219, 'accumulated_submission_time': 729.1586856842041, 'accumulated_eval_time': 5773.740987300873, 'accumulated_logging_time': 0.1522829532623291, 'global_step': 624, 'preemption_count': 0}), (727, {'train/loss': 0.13455041881165494, 'validation/loss': 0.13443660674157304, 'validation/num_examples': 89000000, 'test/loss': 0.13780489524701175, 'test/num_examples': 89274637, 'score': 849.3395309448242, 'total_duration': 7435.261584997177, 'accumulated_submission_time': 849.3395309448242, 'accumulated_eval_time': 6585.464201927185, 'accumulated_logging_time': 0.1696922779083252, 'global_step': 727, 'preemption_count': 0}), (830, {'train/loss': 0.13633246348130518, 'validation/loss': 0.13421405617977528, 'validation/num_examples': 89000000, 'test/loss': 0.1377109603929277, 'test/num_examples': 89274637, 'score': 970.0747263431549, 'total_duration': 8373.281414270401, 'accumulated_submission_time': 970.0747263431549, 'accumulated_eval_time': 7402.687954425812, 'accumulated_logging_time': 0.19056010246276855, 'global_step': 830, 'preemption_count': 0}), (932, {'train/loss': 0.13133111747421702, 'validation/loss': 0.13297338202247191, 'validation/num_examples': 89000000, 'test/loss': 0.13609504791377644, 'test/num_examples': 89274637, 'score': 1090.357717037201, 'total_duration': 9292.82474398613, 'accumulated_submission_time': 1090.357717037201, 'accumulated_eval_time': 8201.893949270248, 'accumulated_logging_time': 0.2076272964477539, 'global_step': 932, 'preemption_count': 0}), (1041, {'train/loss': 0.13179821157560706, 'validation/loss': 0.13205370786516854, 'validation/num_examples': 89000000, 'test/loss': 0.13495858851825968, 'test/num_examples': 89274637, 'score': 1211.0848245620728, 'total_duration': 10233.380543231964, 'accumulated_submission_time': 1211.0848245620728, 'accumulated_eval_time': 9021.66390299797, 'accumulated_logging_time': 0.22860097885131836, 'global_step': 1041, 'preemption_count': 0}), (1151, {'train/loss': 0.13182655093071652, 'validation/loss': 0.1318211797752809, 'validation/num_examples': 89000000, 'test/loss': 0.13455023065509636, 'test/num_examples': 89274637, 'score': 1331.0657360553741, 'total_duration': 11157.046851158142, 'accumulated_submission_time': 1331.0657360553741, 'accumulated_eval_time': 9825.291404724121, 'accumulated_logging_time': 0.24524426460266113, 'global_step': 1151, 'preemption_count': 0}), (1248, {'train/loss': 0.1313659016747436, 'validation/loss': 0.13100360674157302, 'validation/num_examples': 89000000, 'test/loss': 0.13381753655296297, 'test/num_examples': 89274637, 'score': 1451.1282398700714, 'total_duration': 12098.71983242035, 'accumulated_submission_time': 1451.1282398700714, 'accumulated_eval_time': 10646.851093292236, 'accumulated_logging_time': 0.2610602378845215, 'global_step': 1248, 'preemption_count': 0}), (1340, {'train/loss': 0.12948756607426187, 'validation/loss': 0.1310783820224719, 'validation/num_examples': 89000000, 'test/loss': 0.13365394025629027, 'test/num_examples': 89274637, 'score': 1571.6791977882385, 'total_duration': 13034.304693937302, 'accumulated_submission_time': 1571.6791977882385, 'accumulated_eval_time': 11461.835547208786, 'accumulated_logging_time': 0.2772552967071533, 'global_step': 1340, 'preemption_count': 0}), (1438, {'train/loss': 0.13098479124269913, 'validation/loss': 0.13187124719101123, 'validation/num_examples': 89000000, 'test/loss': 0.1344772872053235, 'test/num_examples': 89274637, 'score': 1692.152723789215, 'total_duration': 13964.60353899002, 'accumulated_submission_time': 1692.152723789215, 'accumulated_eval_time': 12271.60903096199, 'accumulated_logging_time': 0.2931537628173828, 'global_step': 1438, 'preemption_count': 0}), (1535, {'train/loss': 0.12867068753161792, 'validation/loss': 0.12974478651685392, 'validation/num_examples': 89000000, 'test/loss': 0.13246504715555438, 'test/num_examples': 89274637, 'score': 1812.814180612564, 'total_duration': 14906.534751176834, 'accumulated_submission_time': 1812.814180612564, 'accumulated_eval_time': 13092.827597856522, 'accumulated_logging_time': 0.3098025321960449, 'global_step': 1535, 'preemption_count': 0}), (1600, {'train/loss': 0.1285579139647719, 'validation/loss': 0.1297640449438202, 'validation/num_examples': 89000000, 'test/loss': 0.1326603769892674, 'test/num_examples': 89274637, 'score': 1884.1244649887085, 'total_duration': 15782.152899503708, 'accumulated_submission_time': 1884.1244649887085, 'accumulated_eval_time': 13897.094574689865, 'accumulated_logging_time': 0.32636094093322754, 'global_step': 1600, 'preemption_count': 0})], 'global_step': 1600}
I0608 00:56:29.603466 139947217250112 submission_runner.py:584] Timing: 1884.1244649887085
I0608 00:56:29.603515 139947217250112 submission_runner.py:586] Total number of evals: 17
I0608 00:56:29.603563 139947217250112 submission_runner.py:587] ====================
I0608 00:56:29.603688 139947217250112 submission_runner.py:655] Final criteo1tb score: 1884.1244649887085
