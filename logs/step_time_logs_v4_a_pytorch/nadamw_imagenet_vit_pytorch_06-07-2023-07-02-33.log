torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/nadamw/pytorch/submission.py --tuning_search_space=baselines/nadamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v4_a_pytorch/nadamw --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/pytorch 2>&1 | tee -a /logs/imagenet_vit_pytorch_06-07-2023-07-02-33.log
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0607 07:02:58.725086 140509995030336 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0607 07:02:58.725104 139861593544512 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0607 07:02:58.725053 140495915108160 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0607 07:02:58.725646 139983644149568 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0607 07:02:59.710975 140481181652800 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0607 07:02:59.711124 139911090489152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0607 07:02:59.711302 140360911750976 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0607 07:02:59.720258 140083107157824 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0607 07:02:59.720650 140083107157824 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.721586 140481181652800 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.721776 139911090489152 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.721893 140360911750976 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.722087 140509995030336 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.722117 139861593544512 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.728223 139983644149568 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0607 07:02:59.729301 140495915108160 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
W0607 07:03:02.314621 140481181652800 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 07:03:02.315095 139861593544512 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 07:03:02.315537 139911090489152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 07:03:02.317482 140360911750976 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 07:03:02.319843 140509995030336 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 07:03:02.339866 140083107157824 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch because --overwrite was set.
W0607 07:03:02.355427 140495915108160 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 07:03:02.356064 140083107157824 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch.
W0607 07:03:02.363639 139983644149568 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0607 07:03:02.386757 140083107157824 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0607 07:03:02.391762 140083107157824 submission_runner.py:541] Using RNG seed 1945534310
I0607 07:03:02.393265 140083107157824 submission_runner.py:550] --- Tuning run 1/1 ---
I0607 07:03:02.393378 140083107157824 submission_runner.py:555] Creating tuning directory at /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch/trial_1.
I0607 07:03:02.393633 140083107157824 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0607 07:03:02.394978 140083107157824 submission_runner.py:255] Initializing dataset.
I0607 07:03:08.674523 140083107157824 submission_runner.py:262] Initializing model.
I0607 07:03:13.181888 140083107157824 submission_runner.py:272] Initializing optimizer.
I0607 07:03:13.183579 140083107157824 submission_runner.py:279] Initializing metrics bundle.
I0607 07:03:13.183697 140083107157824 submission_runner.py:297] Initializing checkpoint and logger.
I0607 07:03:13.661237 140083107157824 submission_runner.py:318] Saving meta data to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0607 07:03:13.662119 140083107157824 submission_runner.py:321] Saving flags to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0607 07:03:13.717462 140083107157824 submission_runner.py:332] Starting training loop.
I0607 07:03:20.538057 140052634003200 logging_writer.py:48] [0] global_step=0, grad_norm=0.330334, loss=6.907756
I0607 07:03:20.559626 140083107157824 submission.py:296] 0) loss = 6.908, grad_norm = 0.330
I0607 07:03:20.560934 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:04:23.927931 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:05:22.683302 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:05:22.703183 140083107157824 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 07:05:22.709687 140083107157824 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0607 07:05:22.791011 140083107157824 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0607 07:05:34.946528 140083107157824 submission_runner.py:419] Time since start: 141.23s, 	Step: 1, 	{'train/accuracy': 0.00189453125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00232, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.002, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.84355616569519, 'total_duration': 141.2294020652771, 'accumulated_submission_time': 6.84355616569519, 'accumulated_eval_time': 134.38542771339417, 'accumulated_logging_time': 0}
I0607 07:05:34.966664 140049018517248 logging_writer.py:48] [1] accumulated_eval_time=134.385428, accumulated_logging_time=0, accumulated_submission_time=6.843556, global_step=1, preemption_count=0, score=6.843556, test/accuracy=0.002000, test/loss=6.907755, test/num_examples=10000, total_duration=141.229402, train/accuracy=0.001895, train/loss=6.907756, validation/accuracy=0.002320, validation/loss=6.907756, validation/num_examples=50000
I0607 07:05:34.985043 140083107157824 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.986616 139911090489152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.986621 139983644149568 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.986661 140495915108160 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.986816 140509995030336 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.986848 140360911750976 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.986876 139861593544512 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:34.987357 140481181652800 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0607 07:05:35.591552 140049010124544 logging_writer.py:48] [1] global_step=1, grad_norm=0.342166, loss=6.907756
I0607 07:05:35.595215 140083107157824 submission.py:296] 1) loss = 6.908, grad_norm = 0.342
I0607 07:05:36.030113 140049018517248 logging_writer.py:48] [2] global_step=2, grad_norm=0.341226, loss=6.907753
I0607 07:05:36.033874 140083107157824 submission.py:296] 2) loss = 6.908, grad_norm = 0.341
I0607 07:05:36.442325 140049010124544 logging_writer.py:48] [3] global_step=3, grad_norm=0.338141, loss=6.907751
I0607 07:05:36.445971 140083107157824 submission.py:296] 3) loss = 6.908, grad_norm = 0.338
I0607 07:05:36.853723 140049018517248 logging_writer.py:48] [4] global_step=4, grad_norm=0.334379, loss=6.907754
I0607 07:05:36.857773 140083107157824 submission.py:296] 4) loss = 6.908, grad_norm = 0.334
I0607 07:05:37.270350 140049010124544 logging_writer.py:48] [5] global_step=5, grad_norm=0.338175, loss=6.907752
I0607 07:05:37.275070 140083107157824 submission.py:296] 5) loss = 6.908, grad_norm = 0.338
I0607 07:05:37.689960 140049018517248 logging_writer.py:48] [6] global_step=6, grad_norm=0.344175, loss=6.907738
I0607 07:05:37.693839 140083107157824 submission.py:296] 6) loss = 6.908, grad_norm = 0.344
I0607 07:05:38.109066 140049010124544 logging_writer.py:48] [7] global_step=7, grad_norm=0.345844, loss=6.907743
I0607 07:05:38.112977 140083107157824 submission.py:296] 7) loss = 6.908, grad_norm = 0.346
I0607 07:05:38.520757 140049018517248 logging_writer.py:48] [8] global_step=8, grad_norm=0.343404, loss=6.907740
I0607 07:05:38.524761 140083107157824 submission.py:296] 8) loss = 6.908, grad_norm = 0.343
I0607 07:05:38.955755 140049010124544 logging_writer.py:48] [9] global_step=9, grad_norm=0.345425, loss=6.907734
I0607 07:05:38.959596 140083107157824 submission.py:296] 9) loss = 6.908, grad_norm = 0.345
I0607 07:05:39.373353 140049018517248 logging_writer.py:48] [10] global_step=10, grad_norm=0.338046, loss=6.907716
I0607 07:05:39.378174 140083107157824 submission.py:296] 10) loss = 6.908, grad_norm = 0.338
I0607 07:05:39.791578 140049010124544 logging_writer.py:48] [11] global_step=11, grad_norm=0.337036, loss=6.907709
I0607 07:05:39.796832 140083107157824 submission.py:296] 11) loss = 6.908, grad_norm = 0.337
I0607 07:05:40.208756 140049018517248 logging_writer.py:48] [12] global_step=12, grad_norm=0.335029, loss=6.907707
I0607 07:05:40.213480 140083107157824 submission.py:296] 12) loss = 6.908, grad_norm = 0.335
I0607 07:05:40.626681 140049010124544 logging_writer.py:48] [13] global_step=13, grad_norm=0.335313, loss=6.907670
I0607 07:05:40.633306 140083107157824 submission.py:296] 13) loss = 6.908, grad_norm = 0.335
I0607 07:05:41.044427 140049018517248 logging_writer.py:48] [14] global_step=14, grad_norm=0.340419, loss=6.907739
I0607 07:05:41.049398 140083107157824 submission.py:296] 14) loss = 6.908, grad_norm = 0.340
I0607 07:05:41.476609 140049010124544 logging_writer.py:48] [15] global_step=15, grad_norm=0.336158, loss=6.907650
I0607 07:05:41.481873 140083107157824 submission.py:296] 15) loss = 6.908, grad_norm = 0.336
I0607 07:05:41.902884 140049018517248 logging_writer.py:48] [16] global_step=16, grad_norm=0.330767, loss=6.907682
I0607 07:05:41.907743 140083107157824 submission.py:296] 16) loss = 6.908, grad_norm = 0.331
I0607 07:05:42.318447 140049010124544 logging_writer.py:48] [17] global_step=17, grad_norm=0.334568, loss=6.907671
I0607 07:05:42.323450 140083107157824 submission.py:296] 17) loss = 6.908, grad_norm = 0.335
I0607 07:05:42.734306 140049018517248 logging_writer.py:48] [18] global_step=18, grad_norm=0.339430, loss=6.907649
I0607 07:05:42.739185 140083107157824 submission.py:296] 18) loss = 6.908, grad_norm = 0.339
I0607 07:05:43.153757 140049010124544 logging_writer.py:48] [19] global_step=19, grad_norm=0.342729, loss=6.907651
I0607 07:05:43.158190 140083107157824 submission.py:296] 19) loss = 6.908, grad_norm = 0.343
I0607 07:05:43.573114 140049018517248 logging_writer.py:48] [20] global_step=20, grad_norm=0.343163, loss=6.907577
I0607 07:05:43.577955 140083107157824 submission.py:296] 20) loss = 6.908, grad_norm = 0.343
I0607 07:05:43.991579 140049010124544 logging_writer.py:48] [21] global_step=21, grad_norm=0.338038, loss=6.907543
I0607 07:05:43.996221 140083107157824 submission.py:296] 21) loss = 6.908, grad_norm = 0.338
I0607 07:05:44.407275 140049018517248 logging_writer.py:48] [22] global_step=22, grad_norm=0.344789, loss=6.907581
I0607 07:05:44.411571 140083107157824 submission.py:296] 22) loss = 6.908, grad_norm = 0.345
I0607 07:05:44.824347 140049010124544 logging_writer.py:48] [23] global_step=23, grad_norm=0.334479, loss=6.907528
I0607 07:05:44.828095 140083107157824 submission.py:296] 23) loss = 6.908, grad_norm = 0.334
I0607 07:05:45.246267 140049018517248 logging_writer.py:48] [24] global_step=24, grad_norm=0.337276, loss=6.907448
I0607 07:05:45.250048 140083107157824 submission.py:296] 24) loss = 6.907, grad_norm = 0.337
I0607 07:05:45.667061 140049010124544 logging_writer.py:48] [25] global_step=25, grad_norm=0.340672, loss=6.907402
I0607 07:05:45.672038 140083107157824 submission.py:296] 25) loss = 6.907, grad_norm = 0.341
I0607 07:05:46.084738 140049018517248 logging_writer.py:48] [26] global_step=26, grad_norm=0.339644, loss=6.907376
I0607 07:05:46.088489 140083107157824 submission.py:296] 26) loss = 6.907, grad_norm = 0.340
I0607 07:05:46.509836 140049010124544 logging_writer.py:48] [27] global_step=27, grad_norm=0.337129, loss=6.907460
I0607 07:05:46.514857 140083107157824 submission.py:296] 27) loss = 6.907, grad_norm = 0.337
I0607 07:05:46.925791 140049018517248 logging_writer.py:48] [28] global_step=28, grad_norm=0.341748, loss=6.907325
I0607 07:05:46.930941 140083107157824 submission.py:296] 28) loss = 6.907, grad_norm = 0.342
I0607 07:05:47.343039 140049010124544 logging_writer.py:48] [29] global_step=29, grad_norm=0.346278, loss=6.907390
I0607 07:05:47.348747 140083107157824 submission.py:296] 29) loss = 6.907, grad_norm = 0.346
I0607 07:05:47.787414 140049018517248 logging_writer.py:48] [30] global_step=30, grad_norm=0.353128, loss=6.907254
I0607 07:05:47.792875 140083107157824 submission.py:296] 30) loss = 6.907, grad_norm = 0.353
I0607 07:05:48.208177 140049010124544 logging_writer.py:48] [31] global_step=31, grad_norm=0.337311, loss=6.907312
I0607 07:05:48.212037 140083107157824 submission.py:296] 31) loss = 6.907, grad_norm = 0.337
I0607 07:05:48.629446 140049018517248 logging_writer.py:48] [32] global_step=32, grad_norm=0.330829, loss=6.907416
I0607 07:05:48.634610 140083107157824 submission.py:296] 32) loss = 6.907, grad_norm = 0.331
I0607 07:05:49.050413 140049010124544 logging_writer.py:48] [33] global_step=33, grad_norm=0.357632, loss=6.907066
I0607 07:05:49.054752 140083107157824 submission.py:296] 33) loss = 6.907, grad_norm = 0.358
I0607 07:05:49.486604 140049018517248 logging_writer.py:48] [34] global_step=34, grad_norm=0.347115, loss=6.907114
I0607 07:05:49.490964 140083107157824 submission.py:296] 34) loss = 6.907, grad_norm = 0.347
I0607 07:05:49.922272 140049010124544 logging_writer.py:48] [35] global_step=35, grad_norm=0.349005, loss=6.907138
I0607 07:05:49.927162 140083107157824 submission.py:296] 35) loss = 6.907, grad_norm = 0.349
I0607 07:05:50.342032 140049018517248 logging_writer.py:48] [36] global_step=36, grad_norm=0.354392, loss=6.907066
I0607 07:05:50.346419 140083107157824 submission.py:296] 36) loss = 6.907, grad_norm = 0.354
I0607 07:05:50.777566 140049010124544 logging_writer.py:48] [37] global_step=37, grad_norm=0.329042, loss=6.907231
I0607 07:05:50.781290 140083107157824 submission.py:296] 37) loss = 6.907, grad_norm = 0.329
I0607 07:05:51.193973 140049018517248 logging_writer.py:48] [38] global_step=38, grad_norm=0.338748, loss=6.906963
I0607 07:05:51.201485 140083107157824 submission.py:296] 38) loss = 6.907, grad_norm = 0.339
I0607 07:05:51.616368 140049010124544 logging_writer.py:48] [39] global_step=39, grad_norm=0.360466, loss=6.906826
I0607 07:05:51.620898 140083107157824 submission.py:296] 39) loss = 6.907, grad_norm = 0.360
I0607 07:05:52.049983 140049018517248 logging_writer.py:48] [40] global_step=40, grad_norm=0.362193, loss=6.906872
I0607 07:05:52.053841 140083107157824 submission.py:296] 40) loss = 6.907, grad_norm = 0.362
I0607 07:05:52.471760 140049010124544 logging_writer.py:48] [41] global_step=41, grad_norm=0.352208, loss=6.906712
I0607 07:05:52.476422 140083107157824 submission.py:296] 41) loss = 6.907, grad_norm = 0.352
I0607 07:05:52.891880 140049018517248 logging_writer.py:48] [42] global_step=42, grad_norm=0.362689, loss=6.906684
I0607 07:05:52.896042 140083107157824 submission.py:296] 42) loss = 6.907, grad_norm = 0.363
I0607 07:05:53.341399 140049010124544 logging_writer.py:48] [43] global_step=43, grad_norm=0.357183, loss=6.906576
I0607 07:05:53.345851 140083107157824 submission.py:296] 43) loss = 6.907, grad_norm = 0.357
I0607 07:05:53.766923 140049018517248 logging_writer.py:48] [44] global_step=44, grad_norm=0.364151, loss=6.906382
I0607 07:05:53.770908 140083107157824 submission.py:296] 44) loss = 6.906, grad_norm = 0.364
I0607 07:05:54.186389 140049010124544 logging_writer.py:48] [45] global_step=45, grad_norm=0.367284, loss=6.906315
I0607 07:05:54.190925 140083107157824 submission.py:296] 45) loss = 6.906, grad_norm = 0.367
I0607 07:05:54.617675 140049018517248 logging_writer.py:48] [46] global_step=46, grad_norm=0.372682, loss=6.906040
I0607 07:05:54.621950 140083107157824 submission.py:296] 46) loss = 6.906, grad_norm = 0.373
I0607 07:05:55.056813 140049010124544 logging_writer.py:48] [47] global_step=47, grad_norm=0.376017, loss=6.906044
I0607 07:05:55.064211 140083107157824 submission.py:296] 47) loss = 6.906, grad_norm = 0.376
I0607 07:05:55.476513 140049018517248 logging_writer.py:48] [48] global_step=48, grad_norm=0.359302, loss=6.905853
I0607 07:05:55.480758 140083107157824 submission.py:296] 48) loss = 6.906, grad_norm = 0.359
I0607 07:05:55.894258 140049010124544 logging_writer.py:48] [49] global_step=49, grad_norm=0.374532, loss=6.906159
I0607 07:05:55.899340 140083107157824 submission.py:296] 49) loss = 6.906, grad_norm = 0.375
I0607 07:05:56.325699 140049018517248 logging_writer.py:48] [50] global_step=50, grad_norm=0.367659, loss=6.905803
I0607 07:05:56.330291 140083107157824 submission.py:296] 50) loss = 6.906, grad_norm = 0.368
I0607 07:05:56.742517 140049010124544 logging_writer.py:48] [51] global_step=51, grad_norm=0.388985, loss=6.905576
I0607 07:05:56.747544 140083107157824 submission.py:296] 51) loss = 6.906, grad_norm = 0.389
I0607 07:05:57.162138 140049018517248 logging_writer.py:48] [52] global_step=52, grad_norm=0.371600, loss=6.905509
I0607 07:05:57.166944 140083107157824 submission.py:296] 52) loss = 6.906, grad_norm = 0.372
I0607 07:05:57.592376 140049010124544 logging_writer.py:48] [53] global_step=53, grad_norm=0.353844, loss=6.905550
I0607 07:05:57.598613 140083107157824 submission.py:296] 53) loss = 6.906, grad_norm = 0.354
I0607 07:05:58.013750 140049018517248 logging_writer.py:48] [54] global_step=54, grad_norm=0.382203, loss=6.904996
I0607 07:05:58.024951 140083107157824 submission.py:296] 54) loss = 6.905, grad_norm = 0.382
I0607 07:05:58.441679 140049010124544 logging_writer.py:48] [55] global_step=55, grad_norm=0.382892, loss=6.905801
I0607 07:05:58.447236 140083107157824 submission.py:296] 55) loss = 6.906, grad_norm = 0.383
I0607 07:05:58.864423 140049018517248 logging_writer.py:48] [56] global_step=56, grad_norm=0.381589, loss=6.904555
I0607 07:05:58.868648 140083107157824 submission.py:296] 56) loss = 6.905, grad_norm = 0.382
I0607 07:05:59.298875 140049010124544 logging_writer.py:48] [57] global_step=57, grad_norm=0.395139, loss=6.904913
I0607 07:05:59.303065 140083107157824 submission.py:296] 57) loss = 6.905, grad_norm = 0.395
I0607 07:05:59.720607 140049018517248 logging_writer.py:48] [58] global_step=58, grad_norm=0.393859, loss=6.904522
I0607 07:05:59.725748 140083107157824 submission.py:296] 58) loss = 6.905, grad_norm = 0.394
I0607 07:06:00.156030 140049010124544 logging_writer.py:48] [59] global_step=59, grad_norm=0.383096, loss=6.904522
I0607 07:06:00.160615 140083107157824 submission.py:296] 59) loss = 6.905, grad_norm = 0.383
I0607 07:06:00.574264 140049018517248 logging_writer.py:48] [60] global_step=60, grad_norm=0.383444, loss=6.904080
I0607 07:06:00.578455 140083107157824 submission.py:296] 60) loss = 6.904, grad_norm = 0.383
I0607 07:06:00.998142 140049010124544 logging_writer.py:48] [61] global_step=61, grad_norm=0.414903, loss=6.903840
I0607 07:06:01.003271 140083107157824 submission.py:296] 61) loss = 6.904, grad_norm = 0.415
I0607 07:06:01.420593 140049018517248 logging_writer.py:48] [62] global_step=62, grad_norm=0.389580, loss=6.904700
I0607 07:06:01.424248 140083107157824 submission.py:296] 62) loss = 6.905, grad_norm = 0.390
I0607 07:06:01.843036 140049010124544 logging_writer.py:48] [63] global_step=63, grad_norm=0.410942, loss=6.903759
I0607 07:06:01.847978 140083107157824 submission.py:296] 63) loss = 6.904, grad_norm = 0.411
I0607 07:06:02.279207 140049018517248 logging_writer.py:48] [64] global_step=64, grad_norm=0.421309, loss=6.903108
I0607 07:06:02.284048 140083107157824 submission.py:296] 64) loss = 6.903, grad_norm = 0.421
I0607 07:06:02.698010 140049010124544 logging_writer.py:48] [65] global_step=65, grad_norm=0.402778, loss=6.903816
I0607 07:06:02.702911 140083107157824 submission.py:296] 65) loss = 6.904, grad_norm = 0.403
I0607 07:06:03.133701 140049018517248 logging_writer.py:48] [66] global_step=66, grad_norm=0.405868, loss=6.903461
I0607 07:06:03.144464 140083107157824 submission.py:296] 66) loss = 6.903, grad_norm = 0.406
I0607 07:06:03.559683 140049010124544 logging_writer.py:48] [67] global_step=67, grad_norm=0.425025, loss=6.902766
I0607 07:06:03.564699 140083107157824 submission.py:296] 67) loss = 6.903, grad_norm = 0.425
I0607 07:06:04.005345 140049018517248 logging_writer.py:48] [68] global_step=68, grad_norm=0.417006, loss=6.903133
I0607 07:06:04.009817 140083107157824 submission.py:296] 68) loss = 6.903, grad_norm = 0.417
I0607 07:06:04.425382 140049010124544 logging_writer.py:48] [69] global_step=69, grad_norm=0.411109, loss=6.902862
I0607 07:06:04.429597 140083107157824 submission.py:296] 69) loss = 6.903, grad_norm = 0.411
I0607 07:06:04.856544 140049018517248 logging_writer.py:48] [70] global_step=70, grad_norm=0.400387, loss=6.903074
I0607 07:06:04.861479 140083107157824 submission.py:296] 70) loss = 6.903, grad_norm = 0.400
I0607 07:06:05.277652 140049010124544 logging_writer.py:48] [71] global_step=71, grad_norm=0.419832, loss=6.902076
I0607 07:06:05.282181 140083107157824 submission.py:296] 71) loss = 6.902, grad_norm = 0.420
I0607 07:06:05.709027 140049018517248 logging_writer.py:48] [72] global_step=72, grad_norm=0.422868, loss=6.901649
I0607 07:06:05.713615 140083107157824 submission.py:296] 72) loss = 6.902, grad_norm = 0.423
I0607 07:06:06.129013 140049010124544 logging_writer.py:48] [73] global_step=73, grad_norm=0.429788, loss=6.901001
I0607 07:06:06.132977 140083107157824 submission.py:296] 73) loss = 6.901, grad_norm = 0.430
I0607 07:06:06.544572 140049018517248 logging_writer.py:48] [74] global_step=74, grad_norm=0.433515, loss=6.901273
I0607 07:06:06.549939 140083107157824 submission.py:296] 74) loss = 6.901, grad_norm = 0.434
I0607 07:06:06.992775 140049010124544 logging_writer.py:48] [75] global_step=75, grad_norm=0.428172, loss=6.900283
I0607 07:06:06.997532 140083107157824 submission.py:296] 75) loss = 6.900, grad_norm = 0.428
I0607 07:06:07.424931 140049018517248 logging_writer.py:48] [76] global_step=76, grad_norm=0.425961, loss=6.901368
I0607 07:06:07.428691 140083107157824 submission.py:296] 76) loss = 6.901, grad_norm = 0.426
I0607 07:06:07.840998 140049010124544 logging_writer.py:48] [77] global_step=77, grad_norm=0.413599, loss=6.900485
I0607 07:06:07.846233 140083107157824 submission.py:296] 77) loss = 6.900, grad_norm = 0.414
I0607 07:06:08.268466 140049018517248 logging_writer.py:48] [78] global_step=78, grad_norm=0.430330, loss=6.899374
I0607 07:06:08.273818 140083107157824 submission.py:296] 78) loss = 6.899, grad_norm = 0.430
I0607 07:06:08.704250 140049010124544 logging_writer.py:48] [79] global_step=79, grad_norm=0.413748, loss=6.900027
I0607 07:06:08.708020 140083107157824 submission.py:296] 79) loss = 6.900, grad_norm = 0.414
I0607 07:06:09.121007 140049018517248 logging_writer.py:48] [80] global_step=80, grad_norm=0.435790, loss=6.899942
I0607 07:06:09.125494 140083107157824 submission.py:296] 80) loss = 6.900, grad_norm = 0.436
I0607 07:06:09.539182 140049010124544 logging_writer.py:48] [81] global_step=81, grad_norm=0.407377, loss=6.900794
I0607 07:06:09.544294 140083107157824 submission.py:296] 81) loss = 6.901, grad_norm = 0.407
I0607 07:06:09.965096 140049018517248 logging_writer.py:48] [82] global_step=82, grad_norm=0.424527, loss=6.898808
I0607 07:06:09.970430 140083107157824 submission.py:296] 82) loss = 6.899, grad_norm = 0.425
I0607 07:06:10.396987 140049010124544 logging_writer.py:48] [83] global_step=83, grad_norm=0.421099, loss=6.897855
I0607 07:06:10.401174 140083107157824 submission.py:296] 83) loss = 6.898, grad_norm = 0.421
I0607 07:06:10.820089 140049018517248 logging_writer.py:48] [84] global_step=84, grad_norm=0.415450, loss=6.898138
I0607 07:06:10.824575 140083107157824 submission.py:296] 84) loss = 6.898, grad_norm = 0.415
I0607 07:06:11.264830 140049010124544 logging_writer.py:48] [85] global_step=85, grad_norm=0.411864, loss=6.898561
I0607 07:06:11.268679 140083107157824 submission.py:296] 85) loss = 6.899, grad_norm = 0.412
I0607 07:06:11.685017 140049018517248 logging_writer.py:48] [86] global_step=86, grad_norm=0.447252, loss=6.895912
I0607 07:06:11.689749 140083107157824 submission.py:296] 86) loss = 6.896, grad_norm = 0.447
I0607 07:06:12.135443 140049010124544 logging_writer.py:48] [87] global_step=87, grad_norm=0.436979, loss=6.897084
I0607 07:06:12.139887 140083107157824 submission.py:296] 87) loss = 6.897, grad_norm = 0.437
I0607 07:06:12.558533 140049018517248 logging_writer.py:48] [88] global_step=88, grad_norm=0.433458, loss=6.899895
I0607 07:06:12.563832 140083107157824 submission.py:296] 88) loss = 6.900, grad_norm = 0.433
I0607 07:06:12.978502 140049010124544 logging_writer.py:48] [89] global_step=89, grad_norm=0.449695, loss=6.896110
I0607 07:06:12.982703 140083107157824 submission.py:296] 89) loss = 6.896, grad_norm = 0.450
I0607 07:06:13.403295 140049018517248 logging_writer.py:48] [90] global_step=90, grad_norm=0.413551, loss=6.897278
I0607 07:06:13.407661 140083107157824 submission.py:296] 90) loss = 6.897, grad_norm = 0.414
I0607 07:06:13.823160 140049010124544 logging_writer.py:48] [91] global_step=91, grad_norm=0.426764, loss=6.898632
I0607 07:06:13.827910 140083107157824 submission.py:296] 91) loss = 6.899, grad_norm = 0.427
I0607 07:06:14.249095 140049018517248 logging_writer.py:48] [92] global_step=92, grad_norm=0.446472, loss=6.896393
I0607 07:06:14.252990 140083107157824 submission.py:296] 92) loss = 6.896, grad_norm = 0.446
I0607 07:06:14.682840 140049010124544 logging_writer.py:48] [93] global_step=93, grad_norm=0.446632, loss=6.893668
I0607 07:06:14.686803 140083107157824 submission.py:296] 93) loss = 6.894, grad_norm = 0.447
I0607 07:06:15.117776 140049018517248 logging_writer.py:48] [94] global_step=94, grad_norm=0.417463, loss=6.897801
I0607 07:06:15.121795 140083107157824 submission.py:296] 94) loss = 6.898, grad_norm = 0.417
I0607 07:06:15.546523 140049010124544 logging_writer.py:48] [95] global_step=95, grad_norm=0.425758, loss=6.895837
I0607 07:06:15.550971 140083107157824 submission.py:296] 95) loss = 6.896, grad_norm = 0.426
I0607 07:06:15.964563 140049018517248 logging_writer.py:48] [96] global_step=96, grad_norm=0.430037, loss=6.892785
I0607 07:06:15.969175 140083107157824 submission.py:296] 96) loss = 6.893, grad_norm = 0.430
I0607 07:06:16.396287 140049010124544 logging_writer.py:48] [97] global_step=97, grad_norm=0.423494, loss=6.895955
I0607 07:06:16.406229 140083107157824 submission.py:296] 97) loss = 6.896, grad_norm = 0.423
I0607 07:06:16.823955 140049018517248 logging_writer.py:48] [98] global_step=98, grad_norm=0.433042, loss=6.894577
I0607 07:06:16.829442 140083107157824 submission.py:296] 98) loss = 6.895, grad_norm = 0.433
I0607 07:06:17.246852 140049010124544 logging_writer.py:48] [99] global_step=99, grad_norm=0.450953, loss=6.891376
I0607 07:06:17.251719 140083107157824 submission.py:296] 99) loss = 6.891, grad_norm = 0.451
I0607 07:06:17.710811 140049018517248 logging_writer.py:48] [100] global_step=100, grad_norm=0.434348, loss=6.893052
I0607 07:06:17.714875 140083107157824 submission.py:296] 100) loss = 6.893, grad_norm = 0.434
I0607 07:09:03.173726 140049010124544 logging_writer.py:48] [500] global_step=500, grad_norm=1.358302, loss=6.633197
I0607 07:09:03.179843 140083107157824 submission.py:296] 500) loss = 6.633, grad_norm = 1.358
I0607 07:12:29.760884 140049018517248 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.851064, loss=6.288933
I0607 07:12:29.766731 140083107157824 submission.py:296] 1000) loss = 6.289, grad_norm = 1.851
I0607 07:12:35.158072 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:13:18.005717 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:14:01.933638 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:14:03.399389 140083107157824 submission_runner.py:419] Time since start: 649.68s, 	Step: 1014, 	{'train/accuracy': 0.038828125, 'train/loss': 5.843416748046875, 'validation/accuracy': 0.03698, 'validation/loss': 5.8686975, 'validation/num_examples': 50000, 'test/accuracy': 0.0271, 'test/loss': 5.993223046875, 'test/num_examples': 10000, 'score': 426.43370366096497, 'total_duration': 649.6823070049286, 'accumulated_submission_time': 426.43370366096497, 'accumulated_eval_time': 222.62687230110168, 'accumulated_logging_time': 0.028340816497802734}
I0607 07:14:03.409080 140040755701504 logging_writer.py:48] [1014] accumulated_eval_time=222.626872, accumulated_logging_time=0.028341, accumulated_submission_time=426.433704, global_step=1014, preemption_count=0, score=426.433704, test/accuracy=0.027100, test/loss=5.993223, test/num_examples=10000, total_duration=649.682307, train/accuracy=0.038828, train/loss=5.843417, validation/accuracy=0.036980, validation/loss=5.868697, validation/num_examples=50000
I0607 07:17:27.323145 140040764094208 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.529068, loss=6.239800
I0607 07:17:27.328843 140083107157824 submission.py:296] 1500) loss = 6.240, grad_norm = 1.529
I0607 07:20:55.092763 140040755701504 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.272057, loss=6.084661
I0607 07:20:55.097348 140083107157824 submission.py:296] 2000) loss = 6.085, grad_norm = 1.272
I0607 07:21:03.796042 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:21:46.901722 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:22:33.842840 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:22:35.268983 140083107157824 submission_runner.py:419] Time since start: 1161.55s, 	Step: 2022, 	{'train/accuracy': 0.09099609375, 'train/loss': 5.165144653320312, 'validation/accuracy': 0.0851, 'validation/loss': 5.20816125, 'validation/num_examples': 50000, 'test/accuracy': 0.0654, 'test/loss': 5.421927734375, 'test/num_examples': 10000, 'score': 846.2307348251343, 'total_duration': 1161.5518922805786, 'accumulated_submission_time': 846.2307348251343, 'accumulated_eval_time': 314.0997107028961, 'accumulated_logging_time': 0.04536890983581543}
I0607 07:22:35.278378 140040764094208 logging_writer.py:48] [2022] accumulated_eval_time=314.099711, accumulated_logging_time=0.045369, accumulated_submission_time=846.230735, global_step=2022, preemption_count=0, score=846.230735, test/accuracy=0.065400, test/loss=5.421928, test/num_examples=10000, total_duration=1161.551892, train/accuracy=0.090996, train/loss=5.165145, validation/accuracy=0.085100, validation/loss=5.208161, validation/num_examples=50000
I0607 07:25:53.850681 140040755701504 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.312912, loss=5.695292
I0607 07:25:53.888196 140083107157824 submission.py:296] 2500) loss = 5.695, grad_norm = 1.313
I0607 07:29:23.685280 140040764094208 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.304245, loss=5.789111
I0607 07:29:23.689472 140083107157824 submission.py:296] 3000) loss = 5.789, grad_norm = 1.304
I0607 07:29:35.337809 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:30:19.379835 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:31:17.888905 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:31:19.318437 140083107157824 submission_runner.py:419] Time since start: 1685.60s, 	Step: 3029, 	{'train/accuracy': 0.15779296875, 'train/loss': 4.531780395507813, 'validation/accuracy': 0.14526, 'validation/loss': 4.60416, 'validation/num_examples': 50000, 'test/accuracy': 0.1102, 'test/loss': 4.9206578125, 'test/num_examples': 10000, 'score': 1265.7074403762817, 'total_duration': 1685.59991812706, 'accumulated_submission_time': 1265.7074403762817, 'accumulated_eval_time': 418.07883310317993, 'accumulated_logging_time': 0.06246638298034668}
I0607 07:31:19.328901 140040755701504 logging_writer.py:48] [3029] accumulated_eval_time=418.078833, accumulated_logging_time=0.062466, accumulated_submission_time=1265.707440, global_step=3029, preemption_count=0, score=1265.707440, test/accuracy=0.110200, test/loss=4.920658, test/num_examples=10000, total_duration=1685.599918, train/accuracy=0.157793, train/loss=4.531780, validation/accuracy=0.145260, validation/loss=4.604160, validation/num_examples=50000
I0607 07:34:35.281846 140040764094208 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.075188, loss=5.487312
I0607 07:34:35.286992 140083107157824 submission.py:296] 3500) loss = 5.487, grad_norm = 1.075
I0607 07:38:04.918106 140040755701504 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.128641, loss=5.343594
I0607 07:38:04.922418 140083107157824 submission.py:296] 4000) loss = 5.344, grad_norm = 1.129
I0607 07:38:19.436932 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:39:03.170511 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:39:48.605508 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:39:50.033268 140083107157824 submission_runner.py:419] Time since start: 2196.32s, 	Step: 4036, 	{'train/accuracy': 0.22349609375, 'train/loss': 3.990264587402344, 'validation/accuracy': 0.20674, 'validation/loss': 4.100036875, 'validation/num_examples': 50000, 'test/accuracy': 0.1588, 'test/loss': 4.475903125, 'test/num_examples': 10000, 'score': 1685.2327229976654, 'total_duration': 2196.316241502762, 'accumulated_submission_time': 1685.2327229976654, 'accumulated_eval_time': 508.6752543449402, 'accumulated_logging_time': 0.08076333999633789}
I0607 07:39:50.043980 140040764094208 logging_writer.py:48] [4036] accumulated_eval_time=508.675254, accumulated_logging_time=0.080763, accumulated_submission_time=1685.232723, global_step=4036, preemption_count=0, score=1685.232723, test/accuracy=0.158800, test/loss=4.475903, test/num_examples=10000, total_duration=2196.316242, train/accuracy=0.223496, train/loss=3.990265, validation/accuracy=0.206740, validation/loss=4.100037, validation/num_examples=50000
I0607 07:43:03.363938 140040755701504 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.159724, loss=5.259460
I0607 07:43:03.369690 140083107157824 submission.py:296] 4500) loss = 5.259, grad_norm = 1.160
I0607 07:46:30.237597 140040764094208 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.027718, loss=5.226200
I0607 07:46:30.241943 140083107157824 submission.py:296] 5000) loss = 5.226, grad_norm = 1.028
I0607 07:46:50.247555 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:47:33.713416 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:48:18.349364 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:48:19.778971 140083107157824 submission_runner.py:419] Time since start: 2706.06s, 	Step: 5044, 	{'train/accuracy': 0.2794140625, 'train/loss': 3.6068191528320312, 'validation/accuracy': 0.25616, 'validation/loss': 3.74033875, 'validation/num_examples': 50000, 'test/accuracy': 0.2031, 'test/loss': 4.16203046875, 'test/num_examples': 10000, 'score': 2104.855002641678, 'total_duration': 2706.061913251877, 'accumulated_submission_time': 2104.855002641678, 'accumulated_eval_time': 598.2066829204559, 'accumulated_logging_time': 0.09896636009216309}
I0607 07:48:19.790484 140040755701504 logging_writer.py:48] [5044] accumulated_eval_time=598.206683, accumulated_logging_time=0.098966, accumulated_submission_time=2104.855003, global_step=5044, preemption_count=0, score=2104.855003, test/accuracy=0.203100, test/loss=4.162030, test/num_examples=10000, total_duration=2706.061913, train/accuracy=0.279414, train/loss=3.606819, validation/accuracy=0.256160, validation/loss=3.740339, validation/num_examples=50000
I0607 07:51:29.533267 140040764094208 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.022896, loss=4.990675
I0607 07:51:29.537693 140083107157824 submission.py:296] 5500) loss = 4.991, grad_norm = 1.023
I0607 07:54:57.262758 140040755701504 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.146133, loss=4.949618
I0607 07:54:57.267855 140083107157824 submission.py:296] 6000) loss = 4.950, grad_norm = 1.146
I0607 07:55:19.796309 140083107157824 spec.py:298] Evaluating on the training split.
I0607 07:56:03.306505 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 07:56:47.694504 140083107157824 spec.py:326] Evaluating on the test split.
I0607 07:56:49.118568 140083107157824 submission_runner.py:419] Time since start: 3215.40s, 	Step: 6055, 	{'train/accuracy': 0.333515625, 'train/loss': 3.2463653564453123, 'validation/accuracy': 0.30754, 'validation/loss': 3.39476625, 'validation/num_examples': 50000, 'test/accuracy': 0.2369, 'test/loss': 3.8808171875, 'test/num_examples': 10000, 'score': 2524.2720992565155, 'total_duration': 3215.401571750641, 'accumulated_submission_time': 2524.2720992565155, 'accumulated_eval_time': 687.5289676189423, 'accumulated_logging_time': 0.11950230598449707}
I0607 07:56:49.129399 140040764094208 logging_writer.py:48] [6055] accumulated_eval_time=687.528968, accumulated_logging_time=0.119502, accumulated_submission_time=2524.272099, global_step=6055, preemption_count=0, score=2524.272099, test/accuracy=0.236900, test/loss=3.880817, test/num_examples=10000, total_duration=3215.401572, train/accuracy=0.333516, train/loss=3.246365, validation/accuracy=0.307540, validation/loss=3.394766, validation/num_examples=50000
I0607 07:59:55.984812 140040755701504 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.947781, loss=4.952155
I0607 07:59:55.990609 140083107157824 submission.py:296] 6500) loss = 4.952, grad_norm = 0.948
I0607 08:03:23.890425 140040764094208 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.076072, loss=4.828108
I0607 08:03:23.895146 140083107157824 submission.py:296] 7000) loss = 4.828, grad_norm = 1.076
I0607 08:03:49.246635 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:04:33.004732 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:05:17.966826 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:05:19.396040 140083107157824 submission_runner.py:419] Time since start: 3725.68s, 	Step: 7062, 	{'train/accuracy': 0.37861328125, 'train/loss': 2.9886663818359374, 'validation/accuracy': 0.3457, 'validation/loss': 3.1611471875, 'validation/num_examples': 50000, 'test/accuracy': 0.2731, 'test/loss': 3.656828515625, 'test/num_examples': 10000, 'score': 2943.807818174362, 'total_duration': 3725.6788613796234, 'accumulated_submission_time': 2943.807818174362, 'accumulated_eval_time': 777.6783640384674, 'accumulated_logging_time': 0.13805437088012695}
I0607 08:05:19.406213 140040755701504 logging_writer.py:48] [7062] accumulated_eval_time=777.678364, accumulated_logging_time=0.138054, accumulated_submission_time=2943.807818, global_step=7062, preemption_count=0, score=2943.807818, test/accuracy=0.273100, test/loss=3.656829, test/num_examples=10000, total_duration=3725.678861, train/accuracy=0.378613, train/loss=2.988666, validation/accuracy=0.345700, validation/loss=3.161147, validation/num_examples=50000
I0607 08:08:21.033174 140040764094208 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.900647, loss=4.558556
I0607 08:08:21.037538 140083107157824 submission.py:296] 7500) loss = 4.559, grad_norm = 0.901
I0607 08:11:50.966104 140040755701504 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.886630, loss=4.362392
I0607 08:11:50.970263 140083107157824 submission.py:296] 8000) loss = 4.362, grad_norm = 0.887
I0607 08:12:19.641860 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:13:03.738565 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:13:48.527777 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:13:49.948814 140083107157824 submission_runner.py:419] Time since start: 4236.23s, 	Step: 8070, 	{'train/accuracy': 0.413125, 'train/loss': 2.7620880126953127, 'validation/accuracy': 0.38454, 'validation/loss': 2.9400334375, 'validation/num_examples': 50000, 'test/accuracy': 0.2951, 'test/loss': 3.49085625, 'test/num_examples': 10000, 'score': 3363.4642367362976, 'total_duration': 4236.230208396912, 'accumulated_submission_time': 3363.4642367362976, 'accumulated_eval_time': 867.9837815761566, 'accumulated_logging_time': 0.1566169261932373}
I0607 08:13:49.959084 140040764094208 logging_writer.py:48] [8070] accumulated_eval_time=867.983782, accumulated_logging_time=0.156617, accumulated_submission_time=3363.464237, global_step=8070, preemption_count=0, score=3363.464237, test/accuracy=0.295100, test/loss=3.490856, test/num_examples=10000, total_duration=4236.230208, train/accuracy=0.413125, train/loss=2.762088, validation/accuracy=0.384540, validation/loss=2.940033, validation/num_examples=50000
I0607 08:16:48.943702 140040755701504 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.804441, loss=4.636168
I0607 08:16:48.949210 140083107157824 submission.py:296] 8500) loss = 4.636, grad_norm = 0.804
I0607 08:20:18.784873 140040764094208 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.747321, loss=4.455925
I0607 08:20:18.789708 140083107157824 submission.py:296] 9000) loss = 4.456, grad_norm = 0.747
I0607 08:20:49.954503 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:21:35.330709 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:22:21.431944 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:22:22.863360 140083107157824 submission_runner.py:419] Time since start: 4749.15s, 	Step: 9076, 	{'train/accuracy': 0.45080078125, 'train/loss': 2.5618402099609376, 'validation/accuracy': 0.414, 'validation/loss': 2.758578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3241, 'test/loss': 3.309755078125, 'test/num_examples': 10000, 'score': 3782.8806841373444, 'total_duration': 4749.146325349808, 'accumulated_submission_time': 3782.8806841373444, 'accumulated_eval_time': 960.892648935318, 'accumulated_logging_time': 0.1748521327972412}
I0607 08:22:22.873611 140040755701504 logging_writer.py:48] [9076] accumulated_eval_time=960.892649, accumulated_logging_time=0.174852, accumulated_submission_time=3782.880684, global_step=9076, preemption_count=0, score=3782.880684, test/accuracy=0.324100, test/loss=3.309755, test/num_examples=10000, total_duration=4749.146325, train/accuracy=0.450801, train/loss=2.561840, validation/accuracy=0.414000, validation/loss=2.758578, validation/num_examples=50000
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0607 08:25:19.427805 140040764094208 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.769274, loss=4.393977
I0607 08:25:19.433379 140083107157824 submission.py:296] 9500) loss = 4.394, grad_norm = 0.769
I0607 08:28:46.902909 140040755701504 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.783882, loss=4.152328
I0607 08:28:46.908532 140083107157824 submission.py:296] 10000) loss = 4.152, grad_norm = 0.784
I0607 08:29:22.948614 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:30:09.636530 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:30:54.509545 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:30:55.935466 140083107157824 submission_runner.py:419] Time since start: 5262.22s, 	Step: 10083, 	{'train/accuracy': 0.4814453125, 'train/loss': 2.424269104003906, 'validation/accuracy': 0.44412, 'validation/loss': 2.6137340625, 'validation/num_examples': 50000, 'test/accuracy': 0.3461, 'test/loss': 3.19464296875, 'test/num_examples': 10000, 'score': 4202.376667976379, 'total_duration': 5262.218409538269, 'accumulated_submission_time': 4202.376667976379, 'accumulated_eval_time': 1053.879543542862, 'accumulated_logging_time': 0.1942286491394043}
I0607 08:30:55.946630 140040764094208 logging_writer.py:48] [10083] accumulated_eval_time=1053.879544, accumulated_logging_time=0.194229, accumulated_submission_time=4202.376668, global_step=10083, preemption_count=0, score=4202.376668, test/accuracy=0.346100, test/loss=3.194643, test/num_examples=10000, total_duration=5262.218410, train/accuracy=0.481445, train/loss=2.424269, validation/accuracy=0.444120, validation/loss=2.613734, validation/num_examples=50000
I0607 08:33:49.637413 140040755701504 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.686528, loss=4.548463
I0607 08:33:49.642267 140083107157824 submission.py:296] 10500) loss = 4.548, grad_norm = 0.687
I0607 08:37:17.088620 140040764094208 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.713067, loss=4.183852
I0607 08:37:17.093137 140083107157824 submission.py:296] 11000) loss = 4.184, grad_norm = 0.713
I0607 08:37:56.348781 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:38:40.553872 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:39:25.462667 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:39:26.889746 140083107157824 submission_runner.py:419] Time since start: 5773.17s, 	Step: 11096, 	{'train/accuracy': 0.51248046875, 'train/loss': 2.280867614746094, 'validation/accuracy': 0.47072, 'validation/loss': 2.48386671875, 'validation/num_examples': 50000, 'test/accuracy': 0.3703, 'test/loss': 3.0490388671875, 'test/num_examples': 10000, 'score': 4622.193815469742, 'total_duration': 5773.172766923904, 'accumulated_submission_time': 4622.193815469742, 'accumulated_eval_time': 1144.4206216335297, 'accumulated_logging_time': 0.21381855010986328}
I0607 08:39:26.901449 140040755701504 logging_writer.py:48] [11096] accumulated_eval_time=1144.420622, accumulated_logging_time=0.213819, accumulated_submission_time=4622.193815, global_step=11096, preemption_count=0, score=4622.193815, test/accuracy=0.370300, test/loss=3.049039, test/num_examples=10000, total_duration=5773.172767, train/accuracy=0.512480, train/loss=2.280868, validation/accuracy=0.470720, validation/loss=2.483867, validation/num_examples=50000
I0607 08:42:16.447980 140040764094208 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.743019, loss=4.303425
I0607 08:42:16.453610 140083107157824 submission.py:296] 11500) loss = 4.303, grad_norm = 0.743
I0607 08:45:44.490471 140040755701504 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.770366, loss=3.827141
I0607 08:45:44.495463 140083107157824 submission.py:296] 12000) loss = 3.827, grad_norm = 0.770
I0607 08:46:26.892014 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:47:11.321284 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:47:56.181212 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:47:57.608430 140083107157824 submission_runner.py:419] Time since start: 6283.89s, 	Step: 12103, 	{'train/accuracy': 0.53423828125, 'train/loss': 2.1542283630371095, 'validation/accuracy': 0.48836, 'validation/loss': 2.37336265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3817, 'test/loss': 2.9487541015625, 'test/num_examples': 10000, 'score': 5041.6055698394775, 'total_duration': 6283.8914206027985, 'accumulated_submission_time': 5041.6055698394775, 'accumulated_eval_time': 1235.1370403766632, 'accumulated_logging_time': 0.2347583770751953}
I0607 08:47:57.619176 140040764094208 logging_writer.py:48] [12103] accumulated_eval_time=1235.137040, accumulated_logging_time=0.234758, accumulated_submission_time=5041.605570, global_step=12103, preemption_count=0, score=5041.605570, test/accuracy=0.381700, test/loss=2.948754, test/num_examples=10000, total_duration=6283.891421, train/accuracy=0.534238, train/loss=2.154228, validation/accuracy=0.488360, validation/loss=2.373363, validation/num_examples=50000
I0607 08:50:42.406242 140040755701504 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.881516, loss=4.186879
I0607 08:50:42.417533 140083107157824 submission.py:296] 12500) loss = 4.187, grad_norm = 0.882
I0607 08:54:12.028411 140040764094208 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.702064, loss=4.390220
I0607 08:54:12.033126 140083107157824 submission.py:296] 13000) loss = 4.390, grad_norm = 0.702
I0607 08:54:57.779505 140083107157824 spec.py:298] Evaluating on the training split.
I0607 08:55:42.567724 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 08:56:29.149717 140083107157824 spec.py:326] Evaluating on the test split.
I0607 08:56:30.577914 140083107157824 submission_runner.py:419] Time since start: 6796.86s, 	Step: 13111, 	{'train/accuracy': 0.5558984375, 'train/loss': 2.042594146728516, 'validation/accuracy': 0.50868, 'validation/loss': 2.26820140625, 'validation/num_examples': 50000, 'test/accuracy': 0.4028, 'test/loss': 2.8403251953125, 'test/num_examples': 10000, 'score': 5461.189966440201, 'total_duration': 6796.860877275467, 'accumulated_submission_time': 5461.189966440201, 'accumulated_eval_time': 1327.935367822647, 'accumulated_logging_time': 0.253279447555542}
I0607 08:56:30.588199 140040755701504 logging_writer.py:48] [13111] accumulated_eval_time=1327.935368, accumulated_logging_time=0.253279, accumulated_submission_time=5461.189966, global_step=13111, preemption_count=0, score=5461.189966, test/accuracy=0.402800, test/loss=2.840325, test/num_examples=10000, total_duration=6796.860877, train/accuracy=0.555898, train/loss=2.042594, validation/accuracy=0.508680, validation/loss=2.268201, validation/num_examples=50000
I0607 08:59:12.497138 140040764094208 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.747683, loss=3.842662
I0607 08:59:12.502587 140083107157824 submission.py:296] 13500) loss = 3.843, grad_norm = 0.748
I0607 09:02:41.647988 140040755701504 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.830100, loss=3.812388
I0607 09:02:41.653600 140083107157824 submission.py:296] 14000) loss = 3.812, grad_norm = 0.830
I0607 09:03:30.782467 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:04:15.088767 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:05:00.344449 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:05:01.769267 140083107157824 submission_runner.py:419] Time since start: 7308.05s, 	Step: 14119, 	{'train/accuracy': 0.5765234375, 'train/loss': 1.9274066162109376, 'validation/accuracy': 0.52476, 'validation/loss': 2.17199515625, 'validation/num_examples': 50000, 'test/accuracy': 0.422, 'test/loss': 2.7544607421875, 'test/num_examples': 10000, 'score': 5880.805162191391, 'total_duration': 7308.052047252655, 'accumulated_submission_time': 5880.805162191391, 'accumulated_eval_time': 1418.921957731247, 'accumulated_logging_time': 0.2713775634765625}
I0607 09:05:01.786372 140040764094208 logging_writer.py:48] [14119] accumulated_eval_time=1418.921958, accumulated_logging_time=0.271378, accumulated_submission_time=5880.805162, global_step=14119, preemption_count=0, score=5880.805162, test/accuracy=0.422000, test/loss=2.754461, test/num_examples=10000, total_duration=7308.052047, train/accuracy=0.576523, train/loss=1.927407, validation/accuracy=0.524760, validation/loss=2.171995, validation/num_examples=50000
I0607 09:07:40.684214 140040755701504 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.759538, loss=4.685034
I0607 09:07:40.690731 140083107157824 submission.py:296] 14500) loss = 4.685, grad_norm = 0.760
I0607 09:11:08.072250 140040764094208 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.670595, loss=4.551332
I0607 09:11:08.079477 140083107157824 submission.py:296] 15000) loss = 4.551, grad_norm = 0.671
I0607 09:12:01.897206 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:12:45.924222 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:13:31.337271 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:13:32.764418 140083107157824 submission_runner.py:419] Time since start: 7819.05s, 	Step: 15126, 	{'train/accuracy': 0.5925390625, 'train/loss': 1.8276060485839845, 'validation/accuracy': 0.54426, 'validation/loss': 2.06401, 'validation/num_examples': 50000, 'test/accuracy': 0.4317, 'test/loss': 2.683552734375, 'test/num_examples': 10000, 'score': 6300.33809876442, 'total_duration': 7819.047419309616, 'accumulated_submission_time': 6300.33809876442, 'accumulated_eval_time': 1509.789392232895, 'accumulated_logging_time': 0.2970716953277588}
I0607 09:13:32.775327 140040755701504 logging_writer.py:48] [15126] accumulated_eval_time=1509.789392, accumulated_logging_time=0.297072, accumulated_submission_time=6300.338099, global_step=15126, preemption_count=0, score=6300.338099, test/accuracy=0.431700, test/loss=2.683553, test/num_examples=10000, total_duration=7819.047419, train/accuracy=0.592539, train/loss=1.827606, validation/accuracy=0.544260, validation/loss=2.064010, validation/num_examples=50000
I0607 09:16:08.592746 140040764094208 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.715154, loss=4.107634
I0607 09:16:08.597687 140083107157824 submission.py:296] 15500) loss = 4.108, grad_norm = 0.715
I0607 09:19:36.471414 140040755701504 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.715036, loss=4.004019
I0607 09:19:36.477201 140083107157824 submission.py:296] 16000) loss = 4.004, grad_norm = 0.715
I0607 09:20:32.780354 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:21:18.240971 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:22:03.206909 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:22:04.634052 140083107157824 submission_runner.py:419] Time since start: 8330.92s, 	Step: 16137, 	{'train/accuracy': 0.60876953125, 'train/loss': 1.781832733154297, 'validation/accuracy': 0.55722, 'validation/loss': 2.02057671875, 'validation/num_examples': 50000, 'test/accuracy': 0.4415, 'test/loss': 2.61506953125, 'test/num_examples': 10000, 'score': 6719.760849237442, 'total_duration': 8330.916956663132, 'accumulated_submission_time': 6719.760849237442, 'accumulated_eval_time': 1601.6430187225342, 'accumulated_logging_time': 0.315929651260376}
I0607 09:22:04.646649 140040764094208 logging_writer.py:48] [16137] accumulated_eval_time=1601.643019, accumulated_logging_time=0.315930, accumulated_submission_time=6719.760849, global_step=16137, preemption_count=0, score=6719.760849, test/accuracy=0.441500, test/loss=2.615070, test/num_examples=10000, total_duration=8330.916957, train/accuracy=0.608770, train/loss=1.781833, validation/accuracy=0.557220, validation/loss=2.020577, validation/num_examples=50000
I0607 09:24:38.309292 140040755701504 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.750892, loss=4.270066
I0607 09:24:38.316743 140083107157824 submission.py:296] 16500) loss = 4.270, grad_norm = 0.751
I0607 09:28:06.487104 140040764094208 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.732547, loss=3.902890
I0607 09:28:06.492297 140083107157824 submission.py:296] 17000) loss = 3.903, grad_norm = 0.733
I0607 09:29:04.677239 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:29:49.612150 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:30:34.547543 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:30:35.976577 140083107157824 submission_runner.py:419] Time since start: 8842.26s, 	Step: 17141, 	{'train/accuracy': 0.62419921875, 'train/loss': 1.6723260498046875, 'validation/accuracy': 0.57236, 'validation/loss': 1.91899984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4544, 'test/loss': 2.54041875, 'test/num_examples': 10000, 'score': 7139.21347784996, 'total_duration': 8842.259424448013, 'accumulated_submission_time': 7139.21347784996, 'accumulated_eval_time': 1692.9422669410706, 'accumulated_logging_time': 0.33783388137817383}
I0607 09:30:35.989132 140040755701504 logging_writer.py:48] [17141] accumulated_eval_time=1692.942267, accumulated_logging_time=0.337834, accumulated_submission_time=7139.213478, global_step=17141, preemption_count=0, score=7139.213478, test/accuracy=0.454400, test/loss=2.540419, test/num_examples=10000, total_duration=8842.259424, train/accuracy=0.624199, train/loss=1.672326, validation/accuracy=0.572360, validation/loss=1.919000, validation/num_examples=50000
I0607 09:33:04.984683 140040764094208 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.749360, loss=3.858223
I0607 09:33:04.995149 140083107157824 submission.py:296] 17500) loss = 3.858, grad_norm = 0.749
I0607 09:36:34.628051 140040755701504 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.878821, loss=3.502691
I0607 09:36:34.633153 140083107157824 submission.py:296] 18000) loss = 3.503, grad_norm = 0.879
I0607 09:37:36.303439 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:38:20.439441 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:39:06.089951 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:39:07.510909 140083107157824 submission_runner.py:419] Time since start: 9353.79s, 	Step: 18149, 	{'train/accuracy': 0.63013671875, 'train/loss': 1.6741204833984376, 'validation/accuracy': 0.57848, 'validation/loss': 1.917489375, 'validation/num_examples': 50000, 'test/accuracy': 0.461, 'test/loss': 2.5456162109375, 'test/num_examples': 10000, 'score': 7558.941975355148, 'total_duration': 9353.793874263763, 'accumulated_submission_time': 7558.941975355148, 'accumulated_eval_time': 1784.1498746871948, 'accumulated_logging_time': 0.3585989475250244}
I0607 09:39:07.521173 140040764094208 logging_writer.py:48] [18149] accumulated_eval_time=1784.149875, accumulated_logging_time=0.358599, accumulated_submission_time=7558.941975, global_step=18149, preemption_count=0, score=7558.941975, test/accuracy=0.461000, test/loss=2.545616, test/num_examples=10000, total_duration=9353.793874, train/accuracy=0.630137, train/loss=1.674120, validation/accuracy=0.578480, validation/loss=1.917489, validation/num_examples=50000
I0607 09:41:33.606272 140040755701504 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.808931, loss=3.634106
I0607 09:41:33.611385 140083107157824 submission.py:296] 18500) loss = 3.634, grad_norm = 0.809
I0607 09:45:02.823404 140040764094208 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.853809, loss=3.720994
I0607 09:45:02.828485 140083107157824 submission.py:296] 19000) loss = 3.721, grad_norm = 0.854
I0607 09:46:07.828398 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:46:52.175778 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:47:37.533995 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:47:38.961936 140083107157824 submission_runner.py:419] Time since start: 9865.24s, 	Step: 19157, 	{'train/accuracy': 0.6454296875, 'train/loss': 1.5956845092773437, 'validation/accuracy': 0.59048, 'validation/loss': 1.8460928125, 'validation/num_examples': 50000, 'test/accuracy': 0.4731, 'test/loss': 2.454189453125, 'test/num_examples': 10000, 'score': 7978.669322252274, 'total_duration': 9865.244831562042, 'accumulated_submission_time': 7978.669322252274, 'accumulated_eval_time': 1875.2833297252655, 'accumulated_logging_time': 0.3771681785583496}
I0607 09:47:38.973007 140040755701504 logging_writer.py:48] [19157] accumulated_eval_time=1875.283330, accumulated_logging_time=0.377168, accumulated_submission_time=7978.669322, global_step=19157, preemption_count=0, score=7978.669322, test/accuracy=0.473100, test/loss=2.454189, test/num_examples=10000, total_duration=9865.244832, train/accuracy=0.645430, train/loss=1.595685, validation/accuracy=0.590480, validation/loss=1.846093, validation/num_examples=50000
I0607 09:50:02.243386 140040764094208 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.772548, loss=3.592342
I0607 09:50:02.247527 140083107157824 submission.py:296] 19500) loss = 3.592, grad_norm = 0.773
I0607 09:53:29.636126 140040755701504 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.859137, loss=4.104886
I0607 09:53:29.641417 140083107157824 submission.py:296] 20000) loss = 4.105, grad_norm = 0.859
I0607 09:54:39.254433 140083107157824 spec.py:298] Evaluating on the training split.
I0607 09:55:25.091230 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 09:56:10.494389 140083107157824 spec.py:326] Evaluating on the test split.
I0607 09:56:11.917280 140083107157824 submission_runner.py:419] Time since start: 10378.20s, 	Step: 20164, 	{'train/accuracy': 0.65451171875, 'train/loss': 1.5328373718261719, 'validation/accuracy': 0.59642, 'validation/loss': 1.79159421875, 'validation/num_examples': 50000, 'test/accuracy': 0.4732, 'test/loss': 2.431280859375, 'test/num_examples': 10000, 'score': 8398.366980791092, 'total_duration': 10378.200219869614, 'accumulated_submission_time': 8398.366980791092, 'accumulated_eval_time': 1967.9462401866913, 'accumulated_logging_time': 0.39628076553344727}
I0607 09:56:11.927558 140040764094208 logging_writer.py:48] [20164] accumulated_eval_time=1967.946240, accumulated_logging_time=0.396281, accumulated_submission_time=8398.366981, global_step=20164, preemption_count=0, score=8398.366981, test/accuracy=0.473200, test/loss=2.431281, test/num_examples=10000, total_duration=10378.200220, train/accuracy=0.654512, train/loss=1.532837, validation/accuracy=0.596420, validation/loss=1.791594, validation/num_examples=50000
I0607 09:58:32.264582 140040755701504 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.779469, loss=3.557900
I0607 09:58:32.270121 140083107157824 submission.py:296] 20500) loss = 3.558, grad_norm = 0.779
I0607 10:02:00.149119 140040764094208 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.757492, loss=3.923146
I0607 10:02:00.154764 140083107157824 submission.py:296] 21000) loss = 3.923, grad_norm = 0.757
I0607 10:03:12.217011 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:03:56.562917 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:04:43.190397 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:04:44.613569 140083107157824 submission_runner.py:419] Time since start: 10890.90s, 	Step: 21175, 	{'train/accuracy': 0.66447265625, 'train/loss': 1.4840493774414063, 'validation/accuracy': 0.60532, 'validation/loss': 1.74719859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4889, 'test/loss': 2.363407421875, 'test/num_examples': 10000, 'score': 8818.069005966187, 'total_duration': 10890.896516084671, 'accumulated_submission_time': 8818.069005966187, 'accumulated_eval_time': 2060.3429906368256, 'accumulated_logging_time': 0.41528916358947754}
I0607 10:04:44.626059 140040755701504 logging_writer.py:48] [21175] accumulated_eval_time=2060.342991, accumulated_logging_time=0.415289, accumulated_submission_time=8818.069006, global_step=21175, preemption_count=0, score=8818.069006, test/accuracy=0.488900, test/loss=2.363407, test/num_examples=10000, total_duration=10890.896516, train/accuracy=0.664473, train/loss=1.484049, validation/accuracy=0.605320, validation/loss=1.747199, validation/num_examples=50000
I0607 10:07:01.748616 140040764094208 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.789176, loss=3.987367
I0607 10:07:01.752942 140083107157824 submission.py:296] 21500) loss = 3.987, grad_norm = 0.789
I0607 10:10:29.954199 140040755701504 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.814992, loss=3.997287
I0607 10:10:29.958650 140083107157824 submission.py:296] 22000) loss = 3.997, grad_norm = 0.815
I0607 10:11:44.792900 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:12:29.050576 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:13:14.154791 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:13:15.578696 140083107157824 submission_runner.py:419] Time since start: 11401.86s, 	Step: 22181, 	{'train/accuracy': 0.6683203125, 'train/loss': 1.4947161865234375, 'validation/accuracy': 0.60774, 'validation/loss': 1.759760625, 'validation/num_examples': 50000, 'test/accuracy': 0.4895, 'test/loss': 2.376528515625, 'test/num_examples': 10000, 'score': 9237.651104211807, 'total_duration': 11401.861694812775, 'accumulated_submission_time': 9237.651104211807, 'accumulated_eval_time': 2151.1291811466217, 'accumulated_logging_time': 0.43657636642456055}
I0607 10:13:15.593213 140040764094208 logging_writer.py:48] [22181] accumulated_eval_time=2151.129181, accumulated_logging_time=0.436576, accumulated_submission_time=9237.651104, global_step=22181, preemption_count=0, score=9237.651104, test/accuracy=0.489500, test/loss=2.376529, test/num_examples=10000, total_duration=11401.861695, train/accuracy=0.668320, train/loss=1.494716, validation/accuracy=0.607740, validation/loss=1.759761, validation/num_examples=50000
I0607 10:15:27.898858 140040755701504 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.818641, loss=3.529260
I0607 10:15:27.904044 140083107157824 submission.py:296] 22500) loss = 3.529, grad_norm = 0.819
I0607 10:18:57.930454 140040764094208 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.816049, loss=3.734874
I0607 10:18:57.935200 140083107157824 submission.py:296] 23000) loss = 3.735, grad_norm = 0.816
I0607 10:20:15.912973 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:21:00.184032 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:21:45.928230 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:21:47.354341 140083107157824 submission_runner.py:419] Time since start: 11913.64s, 	Step: 23188, 	{'train/accuracy': 0.674140625, 'train/loss': 1.4539915466308593, 'validation/accuracy': 0.61606, 'validation/loss': 1.7176071875, 'validation/num_examples': 50000, 'test/accuracy': 0.493, 'test/loss': 2.33678125, 'test/num_examples': 10000, 'score': 9657.392753839493, 'total_duration': 11913.637330770493, 'accumulated_submission_time': 9657.392753839493, 'accumulated_eval_time': 2242.570745229721, 'accumulated_logging_time': 0.4588925838470459}
I0607 10:21:47.365157 140040755701504 logging_writer.py:48] [23188] accumulated_eval_time=2242.570745, accumulated_logging_time=0.458893, accumulated_submission_time=9657.392754, global_step=23188, preemption_count=0, score=9657.392754, test/accuracy=0.493000, test/loss=2.336781, test/num_examples=10000, total_duration=11913.637331, train/accuracy=0.674141, train/loss=1.453992, validation/accuracy=0.616060, validation/loss=1.717607, validation/num_examples=50000
I0607 10:23:57.321218 140040764094208 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.913228, loss=3.509891
I0607 10:23:57.325414 140083107157824 submission.py:296] 23500) loss = 3.510, grad_norm = 0.913
I0607 10:27:26.750452 140040755701504 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.817211, loss=3.699289
I0607 10:27:26.757049 140083107157824 submission.py:296] 24000) loss = 3.699, grad_norm = 0.817
I0607 10:28:47.653435 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:29:33.109318 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:30:19.920557 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:30:21.347672 140083107157824 submission_runner.py:419] Time since start: 12427.63s, 	Step: 24195, 	{'train/accuracy': 0.68171875, 'train/loss': 1.4300148010253906, 'validation/accuracy': 0.62146, 'validation/loss': 1.697539375, 'validation/num_examples': 50000, 'test/accuracy': 0.4943, 'test/loss': 2.3093939453125, 'test/num_examples': 10000, 'score': 10077.095980405807, 'total_duration': 12427.63063955307, 'accumulated_submission_time': 10077.095980405807, 'accumulated_eval_time': 2336.265119075775, 'accumulated_logging_time': 0.4773530960083008}
I0607 10:30:21.358904 140040764094208 logging_writer.py:48] [24195] accumulated_eval_time=2336.265119, accumulated_logging_time=0.477353, accumulated_submission_time=10077.095980, global_step=24195, preemption_count=0, score=10077.095980, test/accuracy=0.494300, test/loss=2.309394, test/num_examples=10000, total_duration=12427.630640, train/accuracy=0.681719, train/loss=1.430015, validation/accuracy=0.621460, validation/loss=1.697539, validation/num_examples=50000
I0607 10:32:28.748285 140040755701504 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.786802, loss=3.557941
I0607 10:32:28.752336 140083107157824 submission.py:296] 24500) loss = 3.558, grad_norm = 0.787
I0607 10:35:56.029503 140040764094208 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.826734, loss=3.584226
I0607 10:35:56.039507 140083107157824 submission.py:296] 25000) loss = 3.584, grad_norm = 0.827
I0607 10:37:21.613481 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:38:06.971129 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:38:53.035283 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:38:54.462302 140083107157824 submission_runner.py:419] Time since start: 12940.75s, 	Step: 25202, 	{'train/accuracy': 0.68990234375, 'train/loss': 1.387201690673828, 'validation/accuracy': 0.62968, 'validation/loss': 1.65968890625, 'validation/num_examples': 50000, 'test/accuracy': 0.5058, 'test/loss': 2.27465390625, 'test/num_examples': 10000, 'score': 10496.769646644592, 'total_duration': 12940.745275259018, 'accumulated_submission_time': 10496.769646644592, 'accumulated_eval_time': 2429.113988161087, 'accumulated_logging_time': 0.4966752529144287}
I0607 10:38:54.472829 140040755701504 logging_writer.py:48] [25202] accumulated_eval_time=2429.113988, accumulated_logging_time=0.496675, accumulated_submission_time=10496.769647, global_step=25202, preemption_count=0, score=10496.769647, test/accuracy=0.505800, test/loss=2.274654, test/num_examples=10000, total_duration=12940.745275, train/accuracy=0.689902, train/loss=1.387202, validation/accuracy=0.629680, validation/loss=1.659689, validation/num_examples=50000
I0607 10:40:58.693593 140040764094208 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.819069, loss=3.959384
I0607 10:40:58.697919 140083107157824 submission.py:296] 25500) loss = 3.959, grad_norm = 0.819
I0607 10:44:26.516709 140040755701504 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.886249, loss=3.916527
I0607 10:44:26.521345 140083107157824 submission.py:296] 26000) loss = 3.917, grad_norm = 0.886
I0607 10:45:54.761028 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:46:40.100453 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:47:25.218535 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:47:26.637070 140083107157824 submission_runner.py:419] Time since start: 13452.92s, 	Step: 26214, 	{'train/accuracy': 0.6950390625, 'train/loss': 1.362766571044922, 'validation/accuracy': 0.63424, 'validation/loss': 1.6346084375, 'validation/num_examples': 50000, 'test/accuracy': 0.5107, 'test/loss': 2.254859375, 'test/num_examples': 10000, 'score': 10916.472168684006, 'total_duration': 13452.920065641403, 'accumulated_submission_time': 10916.472168684006, 'accumulated_eval_time': 2520.9901778697968, 'accumulated_logging_time': 0.5149154663085938}
I0607 10:47:26.648404 140040764094208 logging_writer.py:48] [26214] accumulated_eval_time=2520.990178, accumulated_logging_time=0.514915, accumulated_submission_time=10916.472169, global_step=26214, preemption_count=0, score=10916.472169, test/accuracy=0.510700, test/loss=2.254859, test/num_examples=10000, total_duration=13452.920066, train/accuracy=0.695039, train/loss=1.362767, validation/accuracy=0.634240, validation/loss=1.634608, validation/num_examples=50000
I0607 10:49:27.710404 140040755701504 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.866770, loss=3.434788
I0607 10:49:27.715101 140083107157824 submission.py:296] 26500) loss = 3.435, grad_norm = 0.867
I0607 10:52:55.857124 140040764094208 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.818976, loss=3.075955
I0607 10:52:55.863589 140083107157824 submission.py:296] 27000) loss = 3.076, grad_norm = 0.819
I0607 10:54:26.881679 140083107157824 spec.py:298] Evaluating on the training split.
I0607 10:55:10.728420 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 10:55:55.745882 140083107157824 spec.py:326] Evaluating on the test split.
I0607 10:55:57.178054 140083107157824 submission_runner.py:419] Time since start: 13963.46s, 	Step: 27221, 	{'train/accuracy': 0.6984765625, 'train/loss': 1.3226051330566406, 'validation/accuracy': 0.6366, 'validation/loss': 1.59639859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5172, 'test/loss': 2.2201994140625, 'test/num_examples': 10000, 'score': 11336.119454145432, 'total_duration': 13963.4610517025, 'accumulated_submission_time': 11336.119454145432, 'accumulated_eval_time': 2611.286636829376, 'accumulated_logging_time': 0.5358114242553711}
I0607 10:55:57.189645 140040755701504 logging_writer.py:48] [27221] accumulated_eval_time=2611.286637, accumulated_logging_time=0.535811, accumulated_submission_time=11336.119454, global_step=27221, preemption_count=0, score=11336.119454, test/accuracy=0.517200, test/loss=2.220199, test/num_examples=10000, total_duration=13963.461052, train/accuracy=0.698477, train/loss=1.322605, validation/accuracy=0.636600, validation/loss=1.596399, validation/num_examples=50000
I0607 10:57:53.185789 140040764094208 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.797310, loss=3.847145
I0607 10:57:53.191900 140083107157824 submission.py:296] 27500) loss = 3.847, grad_norm = 0.797
I0607 11:01:22.982399 140083107157824 spec.py:298] Evaluating on the training split.
I0607 11:02:08.475161 140083107157824 spec.py:310] Evaluating on the validation split.
I0607 11:02:54.924446 140083107157824 spec.py:326] Evaluating on the test split.
I0607 11:02:56.350174 140083107157824 submission_runner.py:419] Time since start: 14382.63s, 	Step: 28000, 	{'train/accuracy': 0.703359375, 'train/loss': 1.3278759765625, 'validation/accuracy': 0.63922, 'validation/loss': 1.60778625, 'validation/num_examples': 50000, 'test/accuracy': 0.5195, 'test/loss': 2.201225390625, 'test/num_examples': 10000, 'score': 11661.458224534988, 'total_duration': 14382.633119106293, 'accumulated_submission_time': 11661.458224534988, 'accumulated_eval_time': 2704.6545536518097, 'accumulated_logging_time': 0.5554265975952148}
I0607 11:02:56.361348 140040755701504 logging_writer.py:48] [28000] accumulated_eval_time=2704.654554, accumulated_logging_time=0.555427, accumulated_submission_time=11661.458225, global_step=28000, preemption_count=0, score=11661.458225, test/accuracy=0.519500, test/loss=2.201225, test/num_examples=10000, total_duration=14382.633119, train/accuracy=0.703359, train/loss=1.327876, validation/accuracy=0.639220, validation/loss=1.607786, validation/num_examples=50000
I0607 11:02:56.378819 140040764094208 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11661.458225
I0607 11:02:57.116454 140083107157824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v4_a_pytorch/nadamw/imagenet_vit_pytorch/trial_1/checkpoint_28000.
I0607 11:02:57.417657 140083107157824 submission_runner.py:581] Tuning trial 1/1
I0607 11:02:57.417933 140083107157824 submission_runner.py:582] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0607 11:02:57.418949 140083107157824 submission_runner.py:583] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00189453125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00232, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.002, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.84355616569519, 'total_duration': 141.2294020652771, 'accumulated_submission_time': 6.84355616569519, 'accumulated_eval_time': 134.38542771339417, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1014, {'train/accuracy': 0.038828125, 'train/loss': 5.843416748046875, 'validation/accuracy': 0.03698, 'validation/loss': 5.8686975, 'validation/num_examples': 50000, 'test/accuracy': 0.0271, 'test/loss': 5.993223046875, 'test/num_examples': 10000, 'score': 426.43370366096497, 'total_duration': 649.6823070049286, 'accumulated_submission_time': 426.43370366096497, 'accumulated_eval_time': 222.62687230110168, 'accumulated_logging_time': 0.028340816497802734, 'global_step': 1014, 'preemption_count': 0}), (2022, {'train/accuracy': 0.09099609375, 'train/loss': 5.165144653320312, 'validation/accuracy': 0.0851, 'validation/loss': 5.20816125, 'validation/num_examples': 50000, 'test/accuracy': 0.0654, 'test/loss': 5.421927734375, 'test/num_examples': 10000, 'score': 846.2307348251343, 'total_duration': 1161.5518922805786, 'accumulated_submission_time': 846.2307348251343, 'accumulated_eval_time': 314.0997107028961, 'accumulated_logging_time': 0.04536890983581543, 'global_step': 2022, 'preemption_count': 0}), (3029, {'train/accuracy': 0.15779296875, 'train/loss': 4.531780395507813, 'validation/accuracy': 0.14526, 'validation/loss': 4.60416, 'validation/num_examples': 50000, 'test/accuracy': 0.1102, 'test/loss': 4.9206578125, 'test/num_examples': 10000, 'score': 1265.7074403762817, 'total_duration': 1685.59991812706, 'accumulated_submission_time': 1265.7074403762817, 'accumulated_eval_time': 418.07883310317993, 'accumulated_logging_time': 0.06246638298034668, 'global_step': 3029, 'preemption_count': 0}), (4036, {'train/accuracy': 0.22349609375, 'train/loss': 3.990264587402344, 'validation/accuracy': 0.20674, 'validation/loss': 4.100036875, 'validation/num_examples': 50000, 'test/accuracy': 0.1588, 'test/loss': 4.475903125, 'test/num_examples': 10000, 'score': 1685.2327229976654, 'total_duration': 2196.316241502762, 'accumulated_submission_time': 1685.2327229976654, 'accumulated_eval_time': 508.6752543449402, 'accumulated_logging_time': 0.08076333999633789, 'global_step': 4036, 'preemption_count': 0}), (5044, {'train/accuracy': 0.2794140625, 'train/loss': 3.6068191528320312, 'validation/accuracy': 0.25616, 'validation/loss': 3.74033875, 'validation/num_examples': 50000, 'test/accuracy': 0.2031, 'test/loss': 4.16203046875, 'test/num_examples': 10000, 'score': 2104.855002641678, 'total_duration': 2706.061913251877, 'accumulated_submission_time': 2104.855002641678, 'accumulated_eval_time': 598.2066829204559, 'accumulated_logging_time': 0.09896636009216309, 'global_step': 5044, 'preemption_count': 0}), (6055, {'train/accuracy': 0.333515625, 'train/loss': 3.2463653564453123, 'validation/accuracy': 0.30754, 'validation/loss': 3.39476625, 'validation/num_examples': 50000, 'test/accuracy': 0.2369, 'test/loss': 3.8808171875, 'test/num_examples': 10000, 'score': 2524.2720992565155, 'total_duration': 3215.401571750641, 'accumulated_submission_time': 2524.2720992565155, 'accumulated_eval_time': 687.5289676189423, 'accumulated_logging_time': 0.11950230598449707, 'global_step': 6055, 'preemption_count': 0}), (7062, {'train/accuracy': 0.37861328125, 'train/loss': 2.9886663818359374, 'validation/accuracy': 0.3457, 'validation/loss': 3.1611471875, 'validation/num_examples': 50000, 'test/accuracy': 0.2731, 'test/loss': 3.656828515625, 'test/num_examples': 10000, 'score': 2943.807818174362, 'total_duration': 3725.6788613796234, 'accumulated_submission_time': 2943.807818174362, 'accumulated_eval_time': 777.6783640384674, 'accumulated_logging_time': 0.13805437088012695, 'global_step': 7062, 'preemption_count': 0}), (8070, {'train/accuracy': 0.413125, 'train/loss': 2.7620880126953127, 'validation/accuracy': 0.38454, 'validation/loss': 2.9400334375, 'validation/num_examples': 50000, 'test/accuracy': 0.2951, 'test/loss': 3.49085625, 'test/num_examples': 10000, 'score': 3363.4642367362976, 'total_duration': 4236.230208396912, 'accumulated_submission_time': 3363.4642367362976, 'accumulated_eval_time': 867.9837815761566, 'accumulated_logging_time': 0.1566169261932373, 'global_step': 8070, 'preemption_count': 0}), (9076, {'train/accuracy': 0.45080078125, 'train/loss': 2.5618402099609376, 'validation/accuracy': 0.414, 'validation/loss': 2.758578125, 'validation/num_examples': 50000, 'test/accuracy': 0.3241, 'test/loss': 3.309755078125, 'test/num_examples': 10000, 'score': 3782.8806841373444, 'total_duration': 4749.146325349808, 'accumulated_submission_time': 3782.8806841373444, 'accumulated_eval_time': 960.892648935318, 'accumulated_logging_time': 0.1748521327972412, 'global_step': 9076, 'preemption_count': 0}), (10083, {'train/accuracy': 0.4814453125, 'train/loss': 2.424269104003906, 'validation/accuracy': 0.44412, 'validation/loss': 2.6137340625, 'validation/num_examples': 50000, 'test/accuracy': 0.3461, 'test/loss': 3.19464296875, 'test/num_examples': 10000, 'score': 4202.376667976379, 'total_duration': 5262.218409538269, 'accumulated_submission_time': 4202.376667976379, 'accumulated_eval_time': 1053.879543542862, 'accumulated_logging_time': 0.1942286491394043, 'global_step': 10083, 'preemption_count': 0}), (11096, {'train/accuracy': 0.51248046875, 'train/loss': 2.280867614746094, 'validation/accuracy': 0.47072, 'validation/loss': 2.48386671875, 'validation/num_examples': 50000, 'test/accuracy': 0.3703, 'test/loss': 3.0490388671875, 'test/num_examples': 10000, 'score': 4622.193815469742, 'total_duration': 5773.172766923904, 'accumulated_submission_time': 4622.193815469742, 'accumulated_eval_time': 1144.4206216335297, 'accumulated_logging_time': 0.21381855010986328, 'global_step': 11096, 'preemption_count': 0}), (12103, {'train/accuracy': 0.53423828125, 'train/loss': 2.1542283630371095, 'validation/accuracy': 0.48836, 'validation/loss': 2.37336265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3817, 'test/loss': 2.9487541015625, 'test/num_examples': 10000, 'score': 5041.6055698394775, 'total_duration': 6283.8914206027985, 'accumulated_submission_time': 5041.6055698394775, 'accumulated_eval_time': 1235.1370403766632, 'accumulated_logging_time': 0.2347583770751953, 'global_step': 12103, 'preemption_count': 0}), (13111, {'train/accuracy': 0.5558984375, 'train/loss': 2.042594146728516, 'validation/accuracy': 0.50868, 'validation/loss': 2.26820140625, 'validation/num_examples': 50000, 'test/accuracy': 0.4028, 'test/loss': 2.8403251953125, 'test/num_examples': 10000, 'score': 5461.189966440201, 'total_duration': 6796.860877275467, 'accumulated_submission_time': 5461.189966440201, 'accumulated_eval_time': 1327.935367822647, 'accumulated_logging_time': 0.253279447555542, 'global_step': 13111, 'preemption_count': 0}), (14119, {'train/accuracy': 0.5765234375, 'train/loss': 1.9274066162109376, 'validation/accuracy': 0.52476, 'validation/loss': 2.17199515625, 'validation/num_examples': 50000, 'test/accuracy': 0.422, 'test/loss': 2.7544607421875, 'test/num_examples': 10000, 'score': 5880.805162191391, 'total_duration': 7308.052047252655, 'accumulated_submission_time': 5880.805162191391, 'accumulated_eval_time': 1418.921957731247, 'accumulated_logging_time': 0.2713775634765625, 'global_step': 14119, 'preemption_count': 0}), (15126, {'train/accuracy': 0.5925390625, 'train/loss': 1.8276060485839845, 'validation/accuracy': 0.54426, 'validation/loss': 2.06401, 'validation/num_examples': 50000, 'test/accuracy': 0.4317, 'test/loss': 2.683552734375, 'test/num_examples': 10000, 'score': 6300.33809876442, 'total_duration': 7819.047419309616, 'accumulated_submission_time': 6300.33809876442, 'accumulated_eval_time': 1509.789392232895, 'accumulated_logging_time': 0.2970716953277588, 'global_step': 15126, 'preemption_count': 0}), (16137, {'train/accuracy': 0.60876953125, 'train/loss': 1.781832733154297, 'validation/accuracy': 0.55722, 'validation/loss': 2.02057671875, 'validation/num_examples': 50000, 'test/accuracy': 0.4415, 'test/loss': 2.61506953125, 'test/num_examples': 10000, 'score': 6719.760849237442, 'total_duration': 8330.916956663132, 'accumulated_submission_time': 6719.760849237442, 'accumulated_eval_time': 1601.6430187225342, 'accumulated_logging_time': 0.315929651260376, 'global_step': 16137, 'preemption_count': 0}), (17141, {'train/accuracy': 0.62419921875, 'train/loss': 1.6723260498046875, 'validation/accuracy': 0.57236, 'validation/loss': 1.91899984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4544, 'test/loss': 2.54041875, 'test/num_examples': 10000, 'score': 7139.21347784996, 'total_duration': 8842.259424448013, 'accumulated_submission_time': 7139.21347784996, 'accumulated_eval_time': 1692.9422669410706, 'accumulated_logging_time': 0.33783388137817383, 'global_step': 17141, 'preemption_count': 0}), (18149, {'train/accuracy': 0.63013671875, 'train/loss': 1.6741204833984376, 'validation/accuracy': 0.57848, 'validation/loss': 1.917489375, 'validation/num_examples': 50000, 'test/accuracy': 0.461, 'test/loss': 2.5456162109375, 'test/num_examples': 10000, 'score': 7558.941975355148, 'total_duration': 9353.793874263763, 'accumulated_submission_time': 7558.941975355148, 'accumulated_eval_time': 1784.1498746871948, 'accumulated_logging_time': 0.3585989475250244, 'global_step': 18149, 'preemption_count': 0}), (19157, {'train/accuracy': 0.6454296875, 'train/loss': 1.5956845092773437, 'validation/accuracy': 0.59048, 'validation/loss': 1.8460928125, 'validation/num_examples': 50000, 'test/accuracy': 0.4731, 'test/loss': 2.454189453125, 'test/num_examples': 10000, 'score': 7978.669322252274, 'total_duration': 9865.244831562042, 'accumulated_submission_time': 7978.669322252274, 'accumulated_eval_time': 1875.2833297252655, 'accumulated_logging_time': 0.3771681785583496, 'global_step': 19157, 'preemption_count': 0}), (20164, {'train/accuracy': 0.65451171875, 'train/loss': 1.5328373718261719, 'validation/accuracy': 0.59642, 'validation/loss': 1.79159421875, 'validation/num_examples': 50000, 'test/accuracy': 0.4732, 'test/loss': 2.431280859375, 'test/num_examples': 10000, 'score': 8398.366980791092, 'total_duration': 10378.200219869614, 'accumulated_submission_time': 8398.366980791092, 'accumulated_eval_time': 1967.9462401866913, 'accumulated_logging_time': 0.39628076553344727, 'global_step': 20164, 'preemption_count': 0}), (21175, {'train/accuracy': 0.66447265625, 'train/loss': 1.4840493774414063, 'validation/accuracy': 0.60532, 'validation/loss': 1.74719859375, 'validation/num_examples': 50000, 'test/accuracy': 0.4889, 'test/loss': 2.363407421875, 'test/num_examples': 10000, 'score': 8818.069005966187, 'total_duration': 10890.896516084671, 'accumulated_submission_time': 8818.069005966187, 'accumulated_eval_time': 2060.3429906368256, 'accumulated_logging_time': 0.41528916358947754, 'global_step': 21175, 'preemption_count': 0}), (22181, {'train/accuracy': 0.6683203125, 'train/loss': 1.4947161865234375, 'validation/accuracy': 0.60774, 'validation/loss': 1.759760625, 'validation/num_examples': 50000, 'test/accuracy': 0.4895, 'test/loss': 2.376528515625, 'test/num_examples': 10000, 'score': 9237.651104211807, 'total_duration': 11401.861694812775, 'accumulated_submission_time': 9237.651104211807, 'accumulated_eval_time': 2151.1291811466217, 'accumulated_logging_time': 0.43657636642456055, 'global_step': 22181, 'preemption_count': 0}), (23188, {'train/accuracy': 0.674140625, 'train/loss': 1.4539915466308593, 'validation/accuracy': 0.61606, 'validation/loss': 1.7176071875, 'validation/num_examples': 50000, 'test/accuracy': 0.493, 'test/loss': 2.33678125, 'test/num_examples': 10000, 'score': 9657.392753839493, 'total_duration': 11913.637330770493, 'accumulated_submission_time': 9657.392753839493, 'accumulated_eval_time': 2242.570745229721, 'accumulated_logging_time': 0.4588925838470459, 'global_step': 23188, 'preemption_count': 0}), (24195, {'train/accuracy': 0.68171875, 'train/loss': 1.4300148010253906, 'validation/accuracy': 0.62146, 'validation/loss': 1.697539375, 'validation/num_examples': 50000, 'test/accuracy': 0.4943, 'test/loss': 2.3093939453125, 'test/num_examples': 10000, 'score': 10077.095980405807, 'total_duration': 12427.63063955307, 'accumulated_submission_time': 10077.095980405807, 'accumulated_eval_time': 2336.265119075775, 'accumulated_logging_time': 0.4773530960083008, 'global_step': 24195, 'preemption_count': 0}), (25202, {'train/accuracy': 0.68990234375, 'train/loss': 1.387201690673828, 'validation/accuracy': 0.62968, 'validation/loss': 1.65968890625, 'validation/num_examples': 50000, 'test/accuracy': 0.5058, 'test/loss': 2.27465390625, 'test/num_examples': 10000, 'score': 10496.769646644592, 'total_duration': 12940.745275259018, 'accumulated_submission_time': 10496.769646644592, 'accumulated_eval_time': 2429.113988161087, 'accumulated_logging_time': 0.4966752529144287, 'global_step': 25202, 'preemption_count': 0}), (26214, {'train/accuracy': 0.6950390625, 'train/loss': 1.362766571044922, 'validation/accuracy': 0.63424, 'validation/loss': 1.6346084375, 'validation/num_examples': 50000, 'test/accuracy': 0.5107, 'test/loss': 2.254859375, 'test/num_examples': 10000, 'score': 10916.472168684006, 'total_duration': 13452.920065641403, 'accumulated_submission_time': 10916.472168684006, 'accumulated_eval_time': 2520.9901778697968, 'accumulated_logging_time': 0.5149154663085938, 'global_step': 26214, 'preemption_count': 0}), (27221, {'train/accuracy': 0.6984765625, 'train/loss': 1.3226051330566406, 'validation/accuracy': 0.6366, 'validation/loss': 1.59639859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5172, 'test/loss': 2.2201994140625, 'test/num_examples': 10000, 'score': 11336.119454145432, 'total_duration': 13963.4610517025, 'accumulated_submission_time': 11336.119454145432, 'accumulated_eval_time': 2611.286636829376, 'accumulated_logging_time': 0.5358114242553711, 'global_step': 27221, 'preemption_count': 0}), (28000, {'train/accuracy': 0.703359375, 'train/loss': 1.3278759765625, 'validation/accuracy': 0.63922, 'validation/loss': 1.60778625, 'validation/num_examples': 50000, 'test/accuracy': 0.5195, 'test/loss': 2.201225390625, 'test/num_examples': 10000, 'score': 11661.458224534988, 'total_duration': 14382.633119106293, 'accumulated_submission_time': 11661.458224534988, 'accumulated_eval_time': 2704.6545536518097, 'accumulated_logging_time': 0.5554265975952148, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0607 11:02:57.419092 140083107157824 submission_runner.py:584] Timing: 11661.458224534988
I0607 11:02:57.419147 140083107157824 submission_runner.py:586] Total number of evals: 29
I0607 11:02:57.419195 140083107157824 submission_runner.py:587] ====================
I0607 11:02:57.419325 140083107157824 submission_runner.py:655] Final imagenet_vit score: 11661.458224534988
