python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_adafactor/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_05-10-2023-03-52-32.log
I0510 03:52:53.844247 139643400587072 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax.
I0510 03:52:53.915916 139643400587072 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0510 03:52:54.740832 139643400587072 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0510 03:52:54.741545 139643400587072 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0510 03:52:54.746206 139643400587072 submission_runner.py:544] Using RNG seed 1893553634
I0510 03:52:57.419873 139643400587072 submission_runner.py:553] --- Tuning run 1/1 ---
I0510 03:52:57.420064 139643400587072 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1.
I0510 03:52:57.420254 139643400587072 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1/hparams.json.
I0510 03:52:57.556488 139643400587072 submission_runner.py:241] Initializing dataset.
I0510 03:52:57.572426 139643400587072 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0510 03:52:57.581375 139643400587072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 03:52:57.581487 139643400587072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 03:52:57.875917 139643400587072 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0510 03:53:06.243667 139643400587072 submission_runner.py:248] Initializing model.
I0510 03:53:18.731540 139643400587072 submission_runner.py:258] Initializing optimizer.
I0510 03:53:21.054769 139643400587072 submission_runner.py:265] Initializing metrics bundle.
I0510 03:53:21.054970 139643400587072 submission_runner.py:283] Initializing checkpoint and logger.
I0510 03:53:21.055822 139643400587072 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0510 03:53:21.862648 139643400587072 submission_runner.py:304] Saving meta data to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1/meta_data_0.json.
I0510 03:53:21.863765 139643400587072 submission_runner.py:307] Saving flags to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1/flags_0.json.
I0510 03:53:21.868197 139643400587072 submission_runner.py:319] Starting training loop.
I0510 03:55:15.397742 139466345805568 logging_writer.py:48] [0] global_step=0, grad_norm=0.32403936982154846, loss=6.907756805419922
I0510 03:55:15.421213 139643400587072 spec.py:298] Evaluating on the training split.
I0510 03:55:15.430361 139643400587072 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0510 03:55:15.438494 139643400587072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 03:55:15.438631 139643400587072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 03:55:15.518452 139643400587072 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0510 03:55:35.752564 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 03:55:35.763707 139643400587072 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0510 03:55:35.784844 139643400587072 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 03:55:35.785148 139643400587072 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 03:55:35.847311 139643400587072 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0510 03:55:55.226292 139643400587072 spec.py:326] Evaluating on the test split.
I0510 03:55:55.233604 139643400587072 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0510 03:55:55.238405 139643400587072 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0510 03:55:55.272592 139643400587072 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0510 03:56:06.882311 139643400587072 submission_runner.py:421] Time since start: 165.01s, 	Step: 1, 	{'train/accuracy': 0.0026367187965661287, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0025599999353289604, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.002300000051036477, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 113.5528347492218, 'total_duration': 165.01405382156372, 'accumulated_submission_time': 113.5528347492218, 'accumulated_eval_time': 51.46106433868408, 'accumulated_logging_time': 0}
I0510 03:56:06.899225 139414403528448 logging_writer.py:48] [1] accumulated_eval_time=51.461064, accumulated_logging_time=0, accumulated_submission_time=113.552835, global_step=1, preemption_count=0, score=113.552835, test/accuracy=0.002300, test/loss=6.907757, test/num_examples=10000, total_duration=165.014054, train/accuracy=0.002637, train/loss=6.907756, validation/accuracy=0.002560, validation/loss=6.907756, validation/num_examples=50000
I0510 03:58:14.945463 139461677299456 logging_writer.py:48] [100] global_step=100, grad_norm=0.43371662497520447, loss=6.899009704589844
I0510 03:59:03.402390 139461685692160 logging_writer.py:48] [200] global_step=200, grad_norm=0.5875821113586426, loss=6.834948539733887
I0510 03:59:51.740922 139461677299456 logging_writer.py:48] [300] global_step=300, grad_norm=0.6199197173118591, loss=6.830533981323242
I0510 04:00:40.375552 139461685692160 logging_writer.py:48] [400] global_step=400, grad_norm=0.5872254967689514, loss=6.793619155883789
I0510 04:01:28.829684 139461677299456 logging_writer.py:48] [500] global_step=500, grad_norm=0.6157564520835876, loss=6.64847469329834
I0510 04:02:17.471310 139461685692160 logging_writer.py:48] [600] global_step=600, grad_norm=1.2035053968429565, loss=6.575336456298828
I0510 04:03:05.880385 139461677299456 logging_writer.py:48] [700] global_step=700, grad_norm=0.7749467492103577, loss=6.61050271987915
I0510 04:03:07.344949 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:03:13.881860 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:03:20.897285 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:03:22.683525 139643400587072 submission_runner.py:421] Time since start: 600.82s, 	Step: 704, 	{'train/accuracy': 0.015683593228459358, 'train/loss': 6.279702186584473, 'validation/accuracy': 0.016739999875426292, 'validation/loss': 6.289986610412598, 'validation/num_examples': 50000, 'test/accuracy': 0.012800000607967377, 'test/loss': 6.35510778427124, 'test/num_examples': 10000, 'score': 533.9756042957306, 'total_duration': 600.8151960372925, 'accumulated_submission_time': 533.9756042957306, 'accumulated_eval_time': 66.7995822429657, 'accumulated_logging_time': 0.026096582412719727}
I0510 04:03:22.699050 139414571316992 logging_writer.py:48] [704] accumulated_eval_time=66.799582, accumulated_logging_time=0.026097, accumulated_submission_time=533.975604, global_step=704, preemption_count=0, score=533.975604, test/accuracy=0.012800, test/loss=6.355108, test/num_examples=10000, total_duration=600.815196, train/accuracy=0.015684, train/loss=6.279702, validation/accuracy=0.016740, validation/loss=6.289987, validation/num_examples=50000
I0510 04:04:10.172085 139414655178496 logging_writer.py:48] [800] global_step=800, grad_norm=1.0006352663040161, loss=6.531952381134033
I0510 04:04:58.601653 139414571316992 logging_writer.py:48] [900] global_step=900, grad_norm=0.9973722100257874, loss=6.580626964569092
I0510 04:05:47.465939 139414655178496 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.851706862449646, loss=6.57213830947876
I0510 04:06:36.000396 139414571316992 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2115075588226318, loss=6.345146656036377
I0510 04:07:24.745004 139414655178496 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2378041744232178, loss=6.288843154907227
I0510 04:08:13.176289 139414571316992 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7460742592811584, loss=6.323471546173096
I0510 04:09:01.831870 139414655178496 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9270495772361755, loss=6.737999439239502
I0510 04:09:50.289832 139414571316992 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.2628881931304932, loss=6.391183853149414
I0510 04:10:22.940612 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:10:29.508154 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:10:36.668547 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:10:38.348204 139643400587072 submission_runner.py:421] Time since start: 1036.48s, 	Step: 1568, 	{'train/accuracy': 0.04912109300494194, 'train/loss': 5.6229248046875, 'validation/accuracy': 0.0467199981212616, 'validation/loss': 5.673501968383789, 'validation/num_examples': 50000, 'test/accuracy': 0.03840000182390213, 'test/loss': 5.826052665710449, 'test/num_examples': 10000, 'score': 954.1862871646881, 'total_duration': 1036.4798901081085, 'accumulated_submission_time': 954.1862871646881, 'accumulated_eval_time': 82.20709347724915, 'accumulated_logging_time': 0.0557713508605957}
I0510 04:10:38.361543 139414655178496 logging_writer.py:48] [1568] accumulated_eval_time=82.207093, accumulated_logging_time=0.055771, accumulated_submission_time=954.186287, global_step=1568, preemption_count=0, score=954.186287, test/accuracy=0.038400, test/loss=5.826053, test/num_examples=10000, total_duration=1036.479890, train/accuracy=0.049121, train/loss=5.622925, validation/accuracy=0.046720, validation/loss=5.673502, validation/num_examples=50000
I0510 04:10:54.467010 139414571316992 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8637544512748718, loss=6.1186203956604
I0510 04:11:42.874550 139414655178496 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9365420341491699, loss=6.426507949829102
I0510 04:12:31.243230 139414571316992 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8401797413825989, loss=6.125736236572266
I0510 04:13:19.870499 139414655178496 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0374162197113037, loss=6.62294864654541
I0510 04:14:08.553230 139414571316992 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0290191173553467, loss=6.257637023925781
I0510 04:14:57.010705 139414655178496 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9880136847496033, loss=6.126036167144775
I0510 04:15:45.463801 139414571316992 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9822593331336975, loss=6.044773578643799
I0510 04:16:34.174307 139414655178496 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8548620343208313, loss=6.6137375831604
I0510 04:17:22.970314 139414571316992 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1147109270095825, loss=5.8525800704956055
I0510 04:17:38.576807 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:17:45.106664 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:17:52.245702 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:17:53.924993 139643400587072 submission_runner.py:421] Time since start: 1472.06s, 	Step: 2433, 	{'train/accuracy': 0.08671874552965164, 'train/loss': 5.211260795593262, 'validation/accuracy': 0.0828000009059906, 'validation/loss': 5.2506585121154785, 'validation/num_examples': 50000, 'test/accuracy': 0.06320000439882278, 'test/loss': 5.482518672943115, 'test/num_examples': 10000, 'score': 1374.3718881607056, 'total_duration': 1472.0566880702972, 'accumulated_submission_time': 1374.3718881607056, 'accumulated_eval_time': 97.55522584915161, 'accumulated_logging_time': 0.08239936828613281}
I0510 04:17:53.940667 139414655178496 logging_writer.py:48] [2433] accumulated_eval_time=97.555226, accumulated_logging_time=0.082399, accumulated_submission_time=1374.371888, global_step=2433, preemption_count=0, score=1374.371888, test/accuracy=0.063200, test/loss=5.482519, test/num_examples=10000, total_duration=1472.056688, train/accuracy=0.086719, train/loss=5.211261, validation/accuracy=0.082800, validation/loss=5.250659, validation/num_examples=50000
I0510 04:18:27.022144 139414571316992 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8954296112060547, loss=5.879571914672852
I0510 04:19:15.719664 139414655178496 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6677590608596802, loss=6.508289337158203
I0510 04:20:04.199059 139414571316992 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7358466982841492, loss=5.907041549682617
I0510 04:20:52.941461 139414655178496 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7591855525970459, loss=6.194500923156738
I0510 04:21:41.401013 139414571316992 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7910945415496826, loss=5.81298828125
I0510 04:22:30.130150 139414655178496 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8344293832778931, loss=5.698418140411377
I0510 04:23:18.599803 139414571316992 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7054813504219055, loss=6.530543804168701
I0510 04:24:07.218450 139414655178496 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8808916211128235, loss=5.669866561889648
I0510 04:24:54.324651 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:25:00.859034 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:25:07.882894 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:25:09.555053 139643400587072 submission_runner.py:421] Time since start: 1907.69s, 	Step: 3298, 	{'train/accuracy': 0.12818358838558197, 'train/loss': 4.738905429840088, 'validation/accuracy': 0.119159996509552, 'validation/loss': 4.799473285675049, 'validation/num_examples': 50000, 'test/accuracy': 0.09220000356435776, 'test/loss': 5.110001564025879, 'test/num_examples': 10000, 'score': 1794.7256660461426, 'total_duration': 1907.68674492836, 'accumulated_submission_time': 1794.7256660461426, 'accumulated_eval_time': 112.7855851650238, 'accumulated_logging_time': 0.11177897453308105}
I0510 04:25:09.569926 139414571316992 logging_writer.py:48] [3298] accumulated_eval_time=112.785585, accumulated_logging_time=0.111779, accumulated_submission_time=1794.725666, global_step=3298, preemption_count=0, score=1794.725666, test/accuracy=0.092200, test/loss=5.110002, test/num_examples=10000, total_duration=1907.686745, train/accuracy=0.128184, train/loss=4.738905, validation/accuracy=0.119160, validation/loss=4.799473, validation/num_examples=50000
I0510 04:25:11.319481 139414655178496 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7544398903846741, loss=6.041147232055664
I0510 04:26:00.149769 139414571316992 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8052214980125427, loss=5.692972183227539
I0510 04:26:48.546574 139414655178496 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0171257257461548, loss=5.678888320922852
I0510 04:27:37.490397 139414571316992 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7366515398025513, loss=5.665764808654785
I0510 04:28:25.973220 139414655178496 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9000820517539978, loss=5.513552188873291
I0510 04:29:14.816409 139414571316992 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6551985144615173, loss=6.356529235839844
I0510 04:30:03.343334 139414655178496 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9543876051902771, loss=5.491526126861572
I0510 04:30:52.055751 139414571316992 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8687177300453186, loss=5.50042724609375
I0510 04:31:40.662578 139414655178496 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6737988591194153, loss=6.426493167877197
I0510 04:32:09.601213 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:32:16.137997 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:32:23.225668 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:32:24.904749 139643400587072 submission_runner.py:421] Time since start: 2343.04s, 	Step: 4160, 	{'train/accuracy': 0.1864648461341858, 'train/loss': 4.322726726531982, 'validation/accuracy': 0.16561999917030334, 'validation/loss': 4.450417518615723, 'validation/num_examples': 50000, 'test/accuracy': 0.1282000094652176, 'test/loss': 4.791110992431641, 'test/num_examples': 10000, 'score': 2214.7247598171234, 'total_duration': 2343.036408185959, 'accumulated_submission_time': 2214.7247598171234, 'accumulated_eval_time': 128.08904123306274, 'accumulated_logging_time': 0.14190411567687988}
I0510 04:32:24.921081 139414571316992 logging_writer.py:48] [4160] accumulated_eval_time=128.089041, accumulated_logging_time=0.141904, accumulated_submission_time=2214.724760, global_step=4160, preemption_count=0, score=2214.724760, test/accuracy=0.128200, test/loss=4.791111, test/num_examples=10000, total_duration=2343.036408, train/accuracy=0.186465, train/loss=4.322727, validation/accuracy=0.165620, validation/loss=4.450418, validation/num_examples=50000
I0510 04:32:45.101943 139414655178496 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8134092688560486, loss=5.550539493560791
I0510 04:33:33.764263 139414571316992 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.651671290397644, loss=5.92377233505249
I0510 04:34:22.670572 139414655178496 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8098583817481995, loss=5.271110534667969
I0510 04:35:11.297271 139414571316992 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.815366804599762, loss=5.336713790893555
I0510 04:36:00.282930 139414655178496 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8577845096588135, loss=5.160162925720215
I0510 04:36:49.031676 139414571316992 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7537274360656738, loss=6.314493179321289
I0510 04:37:37.973972 139414655178496 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8287859559059143, loss=5.790093898773193
I0510 04:38:26.670510 139414571316992 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9134554266929626, loss=5.211850643157959
I0510 04:39:15.615213 139414655178496 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7724775075912476, loss=5.262432098388672
I0510 04:39:25.352968 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:39:31.914079 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:39:39.045352 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:39:40.730330 139643400587072 submission_runner.py:421] Time since start: 2778.86s, 	Step: 5021, 	{'train/accuracy': 0.2205859273672104, 'train/loss': 3.9992504119873047, 'validation/accuracy': 0.20273999869823456, 'validation/loss': 4.099082946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.1584000140428543, 'test/loss': 4.526467800140381, 'test/num_examples': 10000, 'score': 2635.1255753040314, 'total_duration': 2778.862047433853, 'accumulated_submission_time': 2635.1255753040314, 'accumulated_eval_time': 143.46635460853577, 'accumulated_logging_time': 0.17289423942565918}
I0510 04:39:40.742605 139414571316992 logging_writer.py:48] [5021] accumulated_eval_time=143.466355, accumulated_logging_time=0.172894, accumulated_submission_time=2635.125575, global_step=5021, preemption_count=0, score=2635.125575, test/accuracy=0.158400, test/loss=4.526468, test/num_examples=10000, total_duration=2778.862047, train/accuracy=0.220586, train/loss=3.999250, validation/accuracy=0.202740, validation/loss=4.099083, validation/num_examples=50000
I0510 04:40:19.706243 139414655178496 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8136389255523682, loss=5.0105133056640625
I0510 04:41:08.468325 139414571316992 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9229705929756165, loss=5.429540157318115
I0510 04:41:57.057394 139414655178496 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7781423330307007, loss=5.342087745666504
I0510 04:42:45.937360 139414571316992 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7396761178970337, loss=5.842256546020508
I0510 04:43:34.533976 139414655178496 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7468194365501404, loss=5.026849269866943
I0510 04:44:23.333938 139414571316992 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7332898378372192, loss=5.074866771697998
I0510 04:45:12.194736 139414655178496 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8620508313179016, loss=5.12402868270874
I0510 04:46:00.762076 139414571316992 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8300049901008606, loss=4.9397125244140625
I0510 04:46:41.123399 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:46:47.659307 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:46:54.817596 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:46:56.497250 139643400587072 submission_runner.py:421] Time since start: 3214.63s, 	Step: 5884, 	{'train/accuracy': 0.2549414038658142, 'train/loss': 3.747129440307617, 'validation/accuracy': 0.23347999155521393, 'validation/loss': 3.8836629390716553, 'validation/num_examples': 50000, 'test/accuracy': 0.18160000443458557, 'test/loss': 4.335049629211426, 'test/num_examples': 10000, 'score': 3055.4764370918274, 'total_duration': 3214.6289291381836, 'accumulated_submission_time': 3055.4764370918274, 'accumulated_eval_time': 158.84013962745667, 'accumulated_logging_time': 0.1985921859741211}
I0510 04:46:56.513564 139414655178496 logging_writer.py:48] [5884] accumulated_eval_time=158.840140, accumulated_logging_time=0.198592, accumulated_submission_time=3055.476437, global_step=5884, preemption_count=0, score=3055.476437, test/accuracy=0.181600, test/loss=4.335050, test/num_examples=10000, total_duration=3214.628929, train/accuracy=0.254941, train/loss=3.747129, validation/accuracy=0.233480, validation/loss=3.883663, validation/num_examples=50000
I0510 04:47:05.205802 139414571316992 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7741824984550476, loss=4.978021621704102
I0510 04:47:54.509830 139414655178496 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8063330054283142, loss=4.97690486907959
I0510 04:48:43.813978 139414571316992 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7194369435310364, loss=4.962024688720703
I0510 04:49:33.179322 139414571316992 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7944964170455933, loss=4.766328811645508
I0510 04:50:22.413285 139414655178496 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6780505776405334, loss=4.875093936920166
I0510 04:51:11.766655 139414571316992 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6838908791542053, loss=5.687921047210693
I0510 04:52:00.802808 139414655178496 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7333352565765381, loss=6.332225799560547
I0510 04:52:50.020619 139414571316992 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7293307185173035, loss=4.873268127441406
I0510 04:53:38.767731 139414655178496 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6533406972885132, loss=6.2695441246032715
I0510 04:53:56.585633 139643400587072 spec.py:298] Evaluating on the training split.
I0510 04:54:03.150463 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 04:54:10.555538 139643400587072 spec.py:326] Evaluating on the test split.
I0510 04:54:12.236896 139643400587072 submission_runner.py:421] Time since start: 3650.37s, 	Step: 6737, 	{'train/accuracy': 0.3078320324420929, 'train/loss': 3.4668655395507812, 'validation/accuracy': 0.2651199996471405, 'validation/loss': 3.694242477416992, 'validation/num_examples': 50000, 'test/accuracy': 0.19630001485347748, 'test/loss': 4.175069332122803, 'test/num_examples': 10000, 'score': 3475.51762843132, 'total_duration': 3650.368590116501, 'accumulated_submission_time': 3475.51762843132, 'accumulated_eval_time': 174.49133205413818, 'accumulated_logging_time': 0.22843647003173828}
I0510 04:54:12.254023 139414655178496 logging_writer.py:48] [6737] accumulated_eval_time=174.491332, accumulated_logging_time=0.228436, accumulated_submission_time=3475.517628, global_step=6737, preemption_count=0, score=3475.517628, test/accuracy=0.196300, test/loss=4.175069, test/num_examples=10000, total_duration=3650.368590, train/accuracy=0.307832, train/loss=3.466866, validation/accuracy=0.265120, validation/loss=3.694242, validation/num_examples=50000
I0510 04:54:44.017964 139414571316992 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6189296245574951, loss=5.07456111907959
I0510 04:55:33.868744 139414655178496 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6919316053390503, loss=4.902244567871094
I0510 04:56:23.625625 139414571316992 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7862420678138733, loss=4.737147808074951
I0510 04:57:13.195639 139414655178496 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7484447360038757, loss=5.038736343383789
I0510 04:58:03.312570 139414571316992 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8144083619117737, loss=4.78989839553833
I0510 04:58:53.101522 139414655178496 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.70624178647995, loss=4.702208995819092
I0510 04:59:42.547236 139414571316992 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.700014591217041, loss=4.865272521972656
I0510 05:00:32.451797 139414655178496 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.894756555557251, loss=4.622872352600098
I0510 05:01:12.543804 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:01:19.133124 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:01:26.401718 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:01:28.082744 139643400587072 submission_runner.py:421] Time since start: 4086.21s, 	Step: 7582, 	{'train/accuracy': 0.31507810950279236, 'train/loss': 3.3839008808135986, 'validation/accuracy': 0.2912199795246124, 'validation/loss': 3.5119004249572754, 'validation/num_examples': 50000, 'test/accuracy': 0.21880000829696655, 'test/loss': 4.030876159667969, 'test/num_examples': 10000, 'score': 3895.7772903442383, 'total_duration': 4086.2144305706024, 'accumulated_submission_time': 3895.7772903442383, 'accumulated_eval_time': 190.0302140712738, 'accumulated_logging_time': 0.2580409049987793}
I0510 05:01:28.098273 139414571316992 logging_writer.py:48] [7582] accumulated_eval_time=190.030214, accumulated_logging_time=0.258041, accumulated_submission_time=3895.777290, global_step=7582, preemption_count=0, score=3895.777290, test/accuracy=0.218800, test/loss=4.030876, test/num_examples=10000, total_duration=4086.214431, train/accuracy=0.315078, train/loss=3.383901, validation/accuracy=0.291220, validation/loss=3.511900, validation/num_examples=50000
I0510 05:01:37.683182 139414655178496 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7137110829353333, loss=5.434196949005127
I0510 05:02:27.669147 139414571316992 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5228531956672668, loss=5.883374214172363
I0510 05:03:17.875371 139414655178496 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6574378609657288, loss=4.704916954040527
I0510 05:04:08.259661 139414571316992 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7063083052635193, loss=4.664678573608398
I0510 05:04:58.048275 139414655178496 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6105147004127502, loss=5.027807235717773
I0510 05:05:48.342598 139414571316992 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7438555955886841, loss=5.125476360321045
I0510 05:06:38.034246 139414655178496 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7226114869117737, loss=5.297477722167969
I0510 05:07:28.590549 139414571316992 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9392673969268799, loss=4.679845809936523
I0510 05:08:18.419722 139414655178496 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5845088958740234, loss=5.025137901306152
I0510 05:08:28.403222 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:08:35.064839 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:08:42.073021 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:08:43.751376 139643400587072 submission_runner.py:421] Time since start: 4521.88s, 	Step: 8421, 	{'train/accuracy': 0.3431054651737213, 'train/loss': 3.189359426498413, 'validation/accuracy': 0.3113200068473816, 'validation/loss': 3.3457448482513428, 'validation/num_examples': 50000, 'test/accuracy': 0.2394000142812729, 'test/loss': 3.8773794174194336, 'test/num_examples': 10000, 'score': 4316.052321910858, 'total_duration': 4521.883055448532, 'accumulated_submission_time': 4316.052321910858, 'accumulated_eval_time': 205.37829613685608, 'accumulated_logging_time': 0.28595829010009766}
I0510 05:08:43.767394 139414571316992 logging_writer.py:48] [8421] accumulated_eval_time=205.378296, accumulated_logging_time=0.285958, accumulated_submission_time=4316.052322, global_step=8421, preemption_count=0, score=4316.052322, test/accuracy=0.239400, test/loss=3.877379, test/num_examples=10000, total_duration=4521.883055, train/accuracy=0.343105, train/loss=3.189359, validation/accuracy=0.311320, validation/loss=3.345745, validation/num_examples=50000
I0510 05:09:24.215674 139414655178496 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7533153891563416, loss=4.705422878265381
I0510 05:10:14.370810 139414571316992 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7499337792396545, loss=4.766704559326172
I0510 05:11:04.655299 139414655178496 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6521751284599304, loss=5.314389228820801
I0510 05:11:55.261734 139414571316992 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7802549600601196, loss=4.586355686187744
I0510 05:12:45.854325 139414655178496 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6953664422035217, loss=4.998041152954102
I0510 05:13:35.965393 139414571316992 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6064230799674988, loss=6.129768371582031
I0510 05:14:26.221025 139414655178496 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6785756945610046, loss=5.100681304931641
I0510 05:15:16.393587 139414571316992 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7251845002174377, loss=4.4922285079956055
I0510 05:15:44.078758 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:15:50.723478 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:15:58.244398 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:15:59.920777 139643400587072 submission_runner.py:421] Time since start: 4958.05s, 	Step: 9256, 	{'train/accuracy': 0.3556835949420929, 'train/loss': 3.126404047012329, 'validation/accuracy': 0.32923999428749084, 'validation/loss': 3.2531111240386963, 'validation/num_examples': 50000, 'test/accuracy': 0.249300017952919, 'test/loss': 3.7934415340423584, 'test/num_examples': 10000, 'score': 4736.33429980278, 'total_duration': 4958.052450895309, 'accumulated_submission_time': 4736.33429980278, 'accumulated_eval_time': 221.2202434539795, 'accumulated_logging_time': 0.3144559860229492}
I0510 05:15:59.933774 139414655178496 logging_writer.py:48] [9256] accumulated_eval_time=221.220243, accumulated_logging_time=0.314456, accumulated_submission_time=4736.334300, global_step=9256, preemption_count=0, score=4736.334300, test/accuracy=0.249300, test/loss=3.793442, test/num_examples=10000, total_duration=4958.052451, train/accuracy=0.355684, train/loss=3.126404, validation/accuracy=0.329240, validation/loss=3.253111, validation/num_examples=50000
I0510 05:16:23.164528 139414571316992 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7879558205604553, loss=5.851037502288818
I0510 05:17:13.487332 139414655178496 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7443636059761047, loss=4.501683235168457
I0510 05:18:03.948244 139414571316992 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.725841760635376, loss=4.466535568237305
I0510 05:18:54.391683 139414655178496 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8538018465042114, loss=4.5797882080078125
I0510 05:19:44.720459 139414571316992 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8192403316497803, loss=4.674881935119629
I0510 05:20:34.797697 139414655178496 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8450043797492981, loss=5.054806232452393
I0510 05:21:25.190394 139414571316992 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9011930823326111, loss=4.513669967651367
I0510 05:22:15.246873 139414655178496 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.749608039855957, loss=4.447531223297119
I0510 05:23:00.396631 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:23:07.308807 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:23:14.780379 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:23:16.465862 139643400587072 submission_runner.py:421] Time since start: 5394.60s, 	Step: 10091, 	{'train/accuracy': 0.3690820336341858, 'train/loss': 3.133815288543701, 'validation/accuracy': 0.3416999876499176, 'validation/loss': 3.2839529514312744, 'validation/num_examples': 50000, 'test/accuracy': 0.25620001554489136, 'test/loss': 3.820152521133423, 'test/num_examples': 10000, 'score': 5156.767928123474, 'total_duration': 5394.597569465637, 'accumulated_submission_time': 5156.767928123474, 'accumulated_eval_time': 237.28943991661072, 'accumulated_logging_time': 0.34005212783813477}
I0510 05:23:16.476828 139414571316992 logging_writer.py:48] [10091] accumulated_eval_time=237.289440, accumulated_logging_time=0.340052, accumulated_submission_time=5156.767928, global_step=10091, preemption_count=0, score=5156.767928, test/accuracy=0.256200, test/loss=3.820153, test/num_examples=10000, total_duration=5394.597569, train/accuracy=0.369082, train/loss=3.133815, validation/accuracy=0.341700, validation/loss=3.283953, validation/num_examples=50000
I0510 05:23:21.737312 139414655178496 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.7097193002700806, loss=6.228744029998779
I0510 05:24:11.919104 139414571316992 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8268738389015198, loss=4.294893264770508
I0510 05:25:02.519188 139414655178496 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0189779996871948, loss=4.6153130531311035
I0510 05:25:53.011653 139414571316992 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.8936077356338501, loss=4.342775821685791
I0510 05:26:44.011746 139414655178496 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8600698113441467, loss=4.343515872955322
I0510 05:27:34.058209 139414571316992 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7243139743804932, loss=5.998680114746094
I0510 05:28:24.672791 139414655178496 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9563367366790771, loss=5.010000228881836
I0510 05:29:14.657027 139414571316992 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8935087323188782, loss=4.363109111785889
I0510 05:30:05.153888 139414655178496 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7269091606140137, loss=4.177157878875732
I0510 05:30:16.681277 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:30:23.516080 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:30:30.883821 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:30:32.557960 139643400587072 submission_runner.py:421] Time since start: 5830.69s, 	Step: 10924, 	{'train/accuracy': 0.41859373450279236, 'train/loss': 2.7541611194610596, 'validation/accuracy': 0.37727999687194824, 'validation/loss': 2.95984148979187, 'validation/num_examples': 50000, 'test/accuracy': 0.290800005197525, 'test/loss': 3.53593373298645, 'test/num_examples': 10000, 'score': 5576.944191932678, 'total_duration': 5830.689658880234, 'accumulated_submission_time': 5576.944191932678, 'accumulated_eval_time': 253.16605687141418, 'accumulated_logging_time': 0.36259937286376953}
I0510 05:30:32.574865 139414571316992 logging_writer.py:48] [10924] accumulated_eval_time=253.166057, accumulated_logging_time=0.362599, accumulated_submission_time=5576.944192, global_step=10924, preemption_count=0, score=5576.944192, test/accuracy=0.290800, test/loss=3.535934, test/num_examples=10000, total_duration=5830.689659, train/accuracy=0.418594, train/loss=2.754161, validation/accuracy=0.377280, validation/loss=2.959841, validation/num_examples=50000
I0510 05:31:11.458668 139414655178496 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9058035016059875, loss=4.250614643096924
I0510 05:32:02.236676 139414571316992 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8766087293624878, loss=5.162654399871826
I0510 05:32:52.724520 139414655178496 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9217889308929443, loss=4.270139694213867
I0510 05:33:43.390221 139414571316992 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9639255404472351, loss=5.418412685394287
I0510 05:34:33.555495 139414655178496 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7686362862586975, loss=4.841403007507324
I0510 05:35:24.016721 139414571316992 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8776111602783203, loss=4.313729286193848
I0510 05:36:14.221368 139414655178496 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8695253133773804, loss=4.237044811248779
I0510 05:37:04.959362 139414571316992 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0913854837417603, loss=4.258567810058594
I0510 05:37:32.655146 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:37:39.549375 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:37:46.962276 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:37:48.638666 139643400587072 submission_runner.py:421] Time since start: 6266.77s, 	Step: 11756, 	{'train/accuracy': 0.4308398365974426, 'train/loss': 2.6695427894592285, 'validation/accuracy': 0.39375999569892883, 'validation/loss': 2.8504581451416016, 'validation/num_examples': 50000, 'test/accuracy': 0.30140000581741333, 'test/loss': 3.4531524181365967, 'test/num_examples': 10000, 'score': 5996.997510910034, 'total_duration': 6266.770356893539, 'accumulated_submission_time': 5996.997510910034, 'accumulated_eval_time': 269.1495158672333, 'accumulated_logging_time': 0.38939785957336426}
I0510 05:37:48.654131 139414655178496 logging_writer.py:48] [11756] accumulated_eval_time=269.149516, accumulated_logging_time=0.389398, accumulated_submission_time=5996.997511, global_step=11756, preemption_count=0, score=5996.997511, test/accuracy=0.301400, test/loss=3.453152, test/num_examples=10000, total_duration=6266.770357, train/accuracy=0.430840, train/loss=2.669543, validation/accuracy=0.393760, validation/loss=2.850458, validation/num_examples=50000
I0510 05:38:11.803828 139414571316992 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9932534098625183, loss=4.181695461273193
I0510 05:39:02.899474 139414655178496 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9928662180900574, loss=4.193130016326904
I0510 05:39:52.986579 139414571316992 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8921910524368286, loss=4.414620399475098
I0510 05:40:43.802390 139414655178496 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0262508392333984, loss=4.0852251052856445
I0510 05:41:33.827300 139414571316992 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.2879302501678467, loss=5.908627033233643
I0510 05:42:24.609048 139414655178496 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.8272401094436646, loss=5.883574962615967
I0510 05:43:14.894592 139414571316992 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0955647230148315, loss=5.436298847198486
I0510 05:44:05.249689 139414655178496 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8689300417900085, loss=4.084936141967773
I0510 05:44:49.062984 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:44:56.880291 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:45:04.629865 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:45:06.303169 139643400587072 submission_runner.py:421] Time since start: 6704.43s, 	Step: 12589, 	{'train/accuracy': 0.44462889432907104, 'train/loss': 2.623359203338623, 'validation/accuracy': 0.4078799784183502, 'validation/loss': 2.805159568786621, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.4135818481445312, 'test/num_examples': 10000, 'score': 6417.37339091301, 'total_duration': 6704.434841871262, 'accumulated_submission_time': 6417.37339091301, 'accumulated_eval_time': 286.3896324634552, 'accumulated_logging_time': 0.42110300064086914}
I0510 05:45:06.321556 139414571316992 logging_writer.py:48] [12589] accumulated_eval_time=286.389632, accumulated_logging_time=0.421103, accumulated_submission_time=6417.373391, global_step=12589, preemption_count=0, score=6417.373391, test/accuracy=0.311500, test/loss=3.413582, test/num_examples=10000, total_duration=6704.434842, train/accuracy=0.444629, train/loss=2.623359, validation/accuracy=0.407880, validation/loss=2.805160, validation/num_examples=50000
I0510 05:45:12.849215 139414655178496 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9211776852607727, loss=4.484893321990967
I0510 05:46:03.570257 139414571316992 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.8977310657501221, loss=4.154745578765869
I0510 05:46:53.708195 139414655178496 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8691126108169556, loss=4.081722736358643
I0510 05:47:43.750471 139414571316992 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9935757517814636, loss=5.267794132232666
I0510 05:48:33.709160 139414655178496 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.0092401504516602, loss=5.531442642211914
I0510 05:49:24.016957 139414571316992 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.92624431848526, loss=4.049722671508789
I0510 05:50:14.162383 139414655178496 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.077090859413147, loss=5.831274032592773
I0510 05:51:04.779540 139414571316992 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.8826456665992737, loss=3.942715644836426
I0510 05:51:55.067958 139414655178496 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0434322357177734, loss=4.738821506500244
I0510 05:52:06.582536 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:52:14.847442 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:52:22.679858 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:52:24.370156 139643400587072 submission_runner.py:421] Time since start: 7142.50s, 	Step: 13424, 	{'train/accuracy': 0.48179686069488525, 'train/loss': 2.428098440170288, 'validation/accuracy': 0.42183998227119446, 'validation/loss': 2.7164251804351807, 'validation/num_examples': 50000, 'test/accuracy': 0.3279000222682953, 'test/loss': 3.314305305480957, 'test/num_examples': 10000, 'score': 6837.590988636017, 'total_duration': 7142.501800537109, 'accumulated_submission_time': 6837.590988636017, 'accumulated_eval_time': 304.17714500427246, 'accumulated_logging_time': 0.4658012390136719}
I0510 05:52:24.385322 139414571316992 logging_writer.py:48] [13424] accumulated_eval_time=304.177145, accumulated_logging_time=0.465801, accumulated_submission_time=6837.590989, global_step=13424, preemption_count=0, score=6837.590989, test/accuracy=0.327900, test/loss=3.314305, test/num_examples=10000, total_duration=7142.501801, train/accuracy=0.481797, train/loss=2.428098, validation/accuracy=0.421840, validation/loss=2.716425, validation/num_examples=50000
I0510 05:53:04.510255 139414655178496 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.8591088056564331, loss=4.173599720001221
I0510 05:53:54.764243 139414571316992 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0513302087783813, loss=4.042222023010254
I0510 05:54:45.339515 139414655178496 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.028885006904602, loss=4.0813775062561035
I0510 05:55:35.604826 139414571316992 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.0627259016036987, loss=5.2387800216674805
I0510 05:56:26.142494 139414655178496 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9580530524253845, loss=4.47785758972168
I0510 05:57:16.405539 139414571316992 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.046543836593628, loss=4.285512447357178
I0510 05:58:07.256678 139414655178496 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.1370503902435303, loss=4.387741565704346
I0510 05:58:57.418981 139414571316992 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0720913410186768, loss=4.141838073730469
I0510 05:59:24.840361 139643400587072 spec.py:298] Evaluating on the training split.
I0510 05:59:33.351447 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 05:59:41.503138 139643400587072 spec.py:326] Evaluating on the test split.
I0510 05:59:43.198599 139643400587072 submission_runner.py:421] Time since start: 7581.33s, 	Step: 14255, 	{'train/accuracy': 0.47343748807907104, 'train/loss': 2.420116662979126, 'validation/accuracy': 0.4390399754047394, 'validation/loss': 2.599135637283325, 'validation/num_examples': 50000, 'test/accuracy': 0.33550000190734863, 'test/loss': 3.217409372329712, 'test/num_examples': 10000, 'score': 7258.014278650284, 'total_duration': 7581.330287694931, 'accumulated_submission_time': 7258.014278650284, 'accumulated_eval_time': 322.53532576560974, 'accumulated_logging_time': 0.4956676959991455}
I0510 05:59:43.213741 139414655178496 logging_writer.py:48] [14255] accumulated_eval_time=322.535326, accumulated_logging_time=0.495668, accumulated_submission_time=7258.014279, global_step=14255, preemption_count=0, score=7258.014279, test/accuracy=0.335500, test/loss=3.217409, test/num_examples=10000, total_duration=7581.330288, train/accuracy=0.473437, train/loss=2.420117, validation/accuracy=0.439040, validation/loss=2.599136, validation/num_examples=50000
I0510 06:00:07.122262 139414571316992 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0377545356750488, loss=3.928542375564575
I0510 06:00:57.942100 139414655178496 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.2551261186599731, loss=4.031478404998779
I0510 06:01:48.524239 139414571316992 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4116402864456177, loss=4.020124435424805
I0510 06:02:38.953691 139414655178496 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9478721022605896, loss=5.047846794128418
I0510 06:03:29.587065 139414571316992 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0888925790786743, loss=3.9104440212249756
I0510 06:04:20.182542 139414655178496 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.2499723434448242, loss=3.8774309158325195
I0510 06:05:10.548549 139414571316992 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.1831269264221191, loss=3.911470890045166
I0510 06:06:00.955749 139414655178496 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9890322685241699, loss=3.984156608581543
I0510 06:06:43.583868 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:06:51.443737 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:07:00.119746 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:07:01.788144 139643400587072 submission_runner.py:421] Time since start: 8019.92s, 	Step: 15086, 	{'train/accuracy': 0.4916015565395355, 'train/loss': 2.3632662296295166, 'validation/accuracy': 0.45093998312950134, 'validation/loss': 2.56736421585083, 'validation/num_examples': 50000, 'test/accuracy': 0.3477000296115875, 'test/loss': 3.204709768295288, 'test/num_examples': 10000, 'score': 7678.342448711395, 'total_duration': 8019.919867277145, 'accumulated_submission_time': 7678.342448711395, 'accumulated_eval_time': 340.7395739555359, 'accumulated_logging_time': 0.5357968807220459}
I0510 06:07:01.803030 139414571316992 logging_writer.py:48] [15086] accumulated_eval_time=340.739574, accumulated_logging_time=0.535797, accumulated_submission_time=7678.342449, global_step=15086, preemption_count=0, score=7678.342449, test/accuracy=0.347700, test/loss=3.204710, test/num_examples=10000, total_duration=8019.919867, train/accuracy=0.491602, train/loss=2.363266, validation/accuracy=0.450940, validation/loss=2.567364, validation/num_examples=50000
I0510 06:07:09.930198 139414655178496 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0678986310958862, loss=3.945897102355957
I0510 06:08:01.075195 139414571316992 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0509603023529053, loss=4.024032115936279
I0510 06:08:51.501085 139414655178496 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.1312460899353027, loss=3.7842211723327637
I0510 06:09:41.629070 139414571316992 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9901316165924072, loss=3.8479323387145996
I0510 06:10:32.419269 139414655178496 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.0312409400939941, loss=3.918692111968994
I0510 06:11:23.029076 139414571316992 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.1217461824417114, loss=4.05838680267334
I0510 06:12:13.143975 139414655178496 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.2963299751281738, loss=3.9140071868896484
I0510 06:13:03.777577 139414571316992 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.225360631942749, loss=3.850133180618286
I0510 06:13:54.101530 139414655178496 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.1867952346801758, loss=4.715677261352539
I0510 06:14:02.147580 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:14:10.812209 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:14:19.473925 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:14:21.139417 139643400587072 submission_runner.py:421] Time since start: 8459.27s, 	Step: 15917, 	{'train/accuracy': 0.4987109303474426, 'train/loss': 2.3268303871154785, 'validation/accuracy': 0.4606599807739258, 'validation/loss': 2.4965434074401855, 'validation/num_examples': 50000, 'test/accuracy': 0.3490000069141388, 'test/loss': 3.1463427543640137, 'test/num_examples': 10000, 'score': 8098.651120901108, 'total_duration': 8459.271131038666, 'accumulated_submission_time': 8098.651120901108, 'accumulated_eval_time': 359.7313690185547, 'accumulated_logging_time': 0.5695219039916992}
I0510 06:14:21.151677 139414571316992 logging_writer.py:48] [15917] accumulated_eval_time=359.731369, accumulated_logging_time=0.569522, accumulated_submission_time=8098.651121, global_step=15917, preemption_count=0, score=8098.651121, test/accuracy=0.349000, test/loss=3.146343, test/num_examples=10000, total_duration=8459.271131, train/accuracy=0.498711, train/loss=2.326830, validation/accuracy=0.460660, validation/loss=2.496543, validation/num_examples=50000
I0510 06:15:04.470654 139414655178496 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0790003538131714, loss=3.9100704193115234
I0510 06:15:54.548350 139414571316992 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.6543903350830078, loss=5.209866523742676
I0510 06:16:44.778896 139414655178496 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.1966462135314941, loss=3.828411340713501
I0510 06:17:34.777147 139414571316992 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.1942448616027832, loss=4.708843231201172
I0510 06:18:25.411526 139414655178496 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.1216626167297363, loss=3.8058552742004395
I0510 06:19:15.662888 139414571316992 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0628660917282104, loss=3.9094643592834473
I0510 06:20:06.383925 139414655178496 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1910064220428467, loss=4.514021873474121
I0510 06:20:56.775930 139414571316992 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.1302059888839722, loss=3.863957643508911
I0510 06:21:21.395518 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:21:29.113340 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:21:37.799198 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:21:39.476624 139643400587072 submission_runner.py:421] Time since start: 8897.61s, 	Step: 16750, 	{'train/accuracy': 0.5110741853713989, 'train/loss': 2.2409470081329346, 'validation/accuracy': 0.47095999121665955, 'validation/loss': 2.4478166103363037, 'validation/num_examples': 50000, 'test/accuracy': 0.3647000193595886, 'test/loss': 3.060823440551758, 'test/num_examples': 10000, 'score': 8518.864560127258, 'total_duration': 8897.608322143555, 'accumulated_submission_time': 8518.864560127258, 'accumulated_eval_time': 377.8124327659607, 'accumulated_logging_time': 0.5953128337860107}
I0510 06:21:39.498213 139414655178496 logging_writer.py:48] [16750] accumulated_eval_time=377.812433, accumulated_logging_time=0.595313, accumulated_submission_time=8518.864560, global_step=16750, preemption_count=0, score=8518.864560, test/accuracy=0.364700, test/loss=3.060823, test/num_examples=10000, total_duration=8897.608322, train/accuracy=0.511074, train/loss=2.240947, validation/accuracy=0.470960, validation/loss=2.447817, validation/num_examples=50000
I0510 06:22:06.161712 139414571316992 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.093588948249817, loss=3.9064455032348633
I0510 06:22:56.854470 139414655178496 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.095706582069397, loss=3.808650255203247
I0510 06:23:47.064976 139414571316992 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.329002857208252, loss=5.47910213470459
I0510 06:24:37.782693 139414655178496 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4283804893493652, loss=3.763915538787842
I0510 06:25:28.258976 139414571316992 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.2074168920516968, loss=3.75687313079834
I0510 06:26:18.639381 139414655178496 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.3263049125671387, loss=3.744295597076416
I0510 06:27:09.321048 139414571316992 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.3222581148147583, loss=5.840227127075195
I0510 06:27:59.842593 139414655178496 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.392756462097168, loss=5.537667751312256
I0510 06:28:39.956941 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:28:48.841151 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:28:58.000517 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:28:59.678939 139643400587072 submission_runner.py:421] Time since start: 9337.81s, 	Step: 17581, 	{'train/accuracy': 0.5305078029632568, 'train/loss': 2.195829391479492, 'validation/accuracy': 0.48325997591018677, 'validation/loss': 2.425340175628662, 'validation/num_examples': 50000, 'test/accuracy': 0.3783000111579895, 'test/loss': 3.0552666187286377, 'test/num_examples': 10000, 'score': 8939.292304039001, 'total_duration': 9337.810625553131, 'accumulated_submission_time': 8939.292304039001, 'accumulated_eval_time': 397.53436970710754, 'accumulated_logging_time': 0.6306376457214355}
I0510 06:28:59.695434 139414571316992 logging_writer.py:48] [17581] accumulated_eval_time=397.534370, accumulated_logging_time=0.630638, accumulated_submission_time=8939.292304, global_step=17581, preemption_count=0, score=8939.292304, test/accuracy=0.378300, test/loss=3.055267, test/num_examples=10000, total_duration=9337.810626, train/accuracy=0.530508, train/loss=2.195829, validation/accuracy=0.483260, validation/loss=2.425340, validation/num_examples=50000
I0510 06:29:09.873327 139414655178496 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.4847934246063232, loss=3.779873847961426
I0510 06:29:59.807416 139414571316992 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.1293363571166992, loss=3.8142378330230713
I0510 06:30:50.286293 139414655178496 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1001334190368652, loss=3.7072017192840576
I0510 06:31:40.313995 139414571316992 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.3010358810424805, loss=3.740560531616211
I0510 06:32:30.608816 139414655178496 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1492327451705933, loss=3.882336139678955
I0510 06:33:20.758728 139414571316992 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.5157909393310547, loss=5.012139320373535
I0510 06:34:11.377935 139414655178496 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.1957067251205444, loss=3.7311971187591553
I0510 06:35:01.423207 139414571316992 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.157968282699585, loss=5.290837287902832
I0510 06:35:51.864147 139414655178496 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1466715335845947, loss=4.1923017501831055
I0510 06:35:59.923664 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:36:08.400030 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:36:17.362233 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:36:19.039179 139643400587072 submission_runner.py:421] Time since start: 9777.17s, 	Step: 18417, 	{'train/accuracy': 0.5323437452316284, 'train/loss': 2.1378982067108154, 'validation/accuracy': 0.49173998832702637, 'validation/loss': 2.3323566913604736, 'validation/num_examples': 50000, 'test/accuracy': 0.3800000250339508, 'test/loss': 2.982506036758423, 'test/num_examples': 10000, 'score': 9359.492275714874, 'total_duration': 9777.17090010643, 'accumulated_submission_time': 9359.492275714874, 'accumulated_eval_time': 416.6498432159424, 'accumulated_logging_time': 0.6586251258850098}
I0510 06:36:19.053993 139414571316992 logging_writer.py:48] [18417] accumulated_eval_time=416.649843, accumulated_logging_time=0.658625, accumulated_submission_time=9359.492276, global_step=18417, preemption_count=0, score=9359.492276, test/accuracy=0.380000, test/loss=2.982506, test/num_examples=10000, total_duration=9777.170900, train/accuracy=0.532344, train/loss=2.137898, validation/accuracy=0.491740, validation/loss=2.332357, validation/num_examples=50000
I0510 06:37:02.381901 139414655178496 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1462022066116333, loss=3.6715128421783447
I0510 06:37:53.073339 139414571316992 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.260894536972046, loss=5.15140438079834
I0510 06:38:43.438803 139414655178496 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.3391947746276855, loss=3.5111000537872314
I0510 06:39:33.808104 139414571316992 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.2407487630844116, loss=3.7307016849517822
I0510 06:40:23.914673 139414655178496 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2074888944625854, loss=3.660054922103882
I0510 06:41:14.520711 139414571316992 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.2975265979766846, loss=3.895583152770996
I0510 06:42:04.998818 139414655178496 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.4242557287216187, loss=5.075937271118164
I0510 06:42:55.702919 139414571316992 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.2479761838912964, loss=3.696258544921875
I0510 06:43:19.223426 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:43:28.240114 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:43:36.945780 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:43:38.721758 139643400587072 submission_runner.py:421] Time since start: 10216.85s, 	Step: 19248, 	{'train/accuracy': 0.5361718535423279, 'train/loss': 2.154045343399048, 'validation/accuracy': 0.49281999468803406, 'validation/loss': 2.363997220993042, 'validation/num_examples': 50000, 'test/accuracy': 0.38920003175735474, 'test/loss': 2.987565279006958, 'test/num_examples': 10000, 'score': 9779.632402658463, 'total_duration': 10216.845138072968, 'accumulated_submission_time': 9779.632402658463, 'accumulated_eval_time': 436.13982462882996, 'accumulated_logging_time': 0.6858625411987305}
I0510 06:43:38.750135 139414655178496 logging_writer.py:48] [19248] accumulated_eval_time=436.139825, accumulated_logging_time=0.685863, accumulated_submission_time=9779.632403, global_step=19248, preemption_count=0, score=9779.632403, test/accuracy=0.389200, test/loss=2.987565, test/num_examples=10000, total_duration=10216.845138, train/accuracy=0.536172, train/loss=2.154045, validation/accuracy=0.492820, validation/loss=2.363997, validation/num_examples=50000
I0510 06:44:06.216873 139414571316992 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.1875431537628174, loss=3.8289289474487305
I0510 06:44:57.273286 139414655178496 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.2073613405227661, loss=3.809467315673828
I0510 06:45:47.724606 139414571316992 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.183447241783142, loss=3.746492624282837
I0510 06:46:38.486568 139414655178496 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.4473261833190918, loss=5.734172821044922
I0510 06:47:28.786243 139414571316992 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.1380484104156494, loss=3.932396650314331
I0510 06:48:19.558595 139414655178496 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.2716319561004639, loss=3.7139859199523926
I0510 06:49:09.830417 139414571316992 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.3923101425170898, loss=3.7335524559020996
I0510 06:50:00.642817 139414655178496 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.2956103086471558, loss=3.6052958965301514
I0510 06:50:39.264474 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:50:47.698474 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:50:56.138858 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:50:57.815590 139643400587072 submission_runner.py:421] Time since start: 10655.95s, 	Step: 20078, 	{'train/accuracy': 0.5722851157188416, 'train/loss': 1.9278665781021118, 'validation/accuracy': 0.5059999823570251, 'validation/loss': 2.2401540279388428, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.9006295204162598, 'test/num_examples': 10000, 'score': 10200.090380430222, 'total_duration': 10655.945546627045, 'accumulated_submission_time': 10200.090380430222, 'accumulated_eval_time': 454.6891474723816, 'accumulated_logging_time': 0.7537012100219727}
I0510 06:50:57.835404 139414571316992 logging_writer.py:48] [20078] accumulated_eval_time=454.689147, accumulated_logging_time=0.753701, accumulated_submission_time=10200.090380, global_step=20078, preemption_count=0, score=10200.090380, test/accuracy=0.392100, test/loss=2.900630, test/num_examples=10000, total_duration=10655.945547, train/accuracy=0.572285, train/loss=1.927867, validation/accuracy=0.506000, validation/loss=2.240154, validation/num_examples=50000
I0510 06:51:09.987157 139414655178496 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.3968573808670044, loss=3.657348871231079
I0510 06:52:01.180110 139414571316992 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.6414926052093506, loss=5.296505928039551
I0510 06:52:51.704246 139414655178496 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1842235326766968, loss=3.6733241081237793
I0510 06:53:42.590597 139414571316992 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.7898558378219604, loss=5.478278636932373
I0510 06:54:33.027817 139414655178496 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.293330430984497, loss=3.7366816997528076
I0510 06:55:23.720129 139414571316992 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.3456076383590698, loss=3.7325923442840576
I0510 06:56:14.067564 139414655178496 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.1914963722229004, loss=3.6659140586853027
I0510 06:57:04.553689 139414571316992 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.2996882200241089, loss=3.722297191619873
I0510 06:57:54.800767 139414655178496 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.2080076932907104, loss=4.009950160980225
I0510 06:57:57.871253 139643400587072 spec.py:298] Evaluating on the training split.
I0510 06:58:06.204565 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 06:58:14.936608 139643400587072 spec.py:326] Evaluating on the test split.
I0510 06:58:16.607734 139643400587072 submission_runner.py:421] Time since start: 11094.74s, 	Step: 20907, 	{'train/accuracy': 0.5478515625, 'train/loss': 2.08109974861145, 'validation/accuracy': 0.5068199634552002, 'validation/loss': 2.2877700328826904, 'validation/num_examples': 50000, 'test/accuracy': 0.39160001277923584, 'test/loss': 2.951646566390991, 'test/num_examples': 10000, 'score': 10620.099198102951, 'total_duration': 11094.737990379333, 'accumulated_submission_time': 10620.099198102951, 'accumulated_eval_time': 473.4241290092468, 'accumulated_logging_time': 0.7836754322052002}
I0510 06:58:16.622316 139414571316992 logging_writer.py:48] [20907] accumulated_eval_time=473.424129, accumulated_logging_time=0.783675, accumulated_submission_time=10620.099198, global_step=20907, preemption_count=0, score=10620.099198, test/accuracy=0.391600, test/loss=2.951647, test/num_examples=10000, total_duration=11094.737990, train/accuracy=0.547852, train/loss=2.081100, validation/accuracy=0.506820, validation/loss=2.287770, validation/num_examples=50000
I0510 06:59:04.762999 139414655178496 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.1869487762451172, loss=4.298000812530518
I0510 06:59:55.071031 139414571316992 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.267269253730774, loss=5.001386642456055
I0510 07:00:45.780360 139414655178496 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.1314125061035156, loss=3.584104537963867
I0510 07:01:36.015697 139414571316992 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.2450048923492432, loss=3.527066230773926
I0510 07:02:26.404907 139414655178496 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.2605667114257812, loss=3.704383134841919
I0510 07:03:16.669509 139414571316992 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.3835272789001465, loss=3.5036449432373047
I0510 07:04:07.388926 139414655178496 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.3109043836593628, loss=3.625758171081543
I0510 07:04:57.661310 139414571316992 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.4540812969207764, loss=4.284340858459473
I0510 07:05:17.065231 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:05:25.155301 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:05:33.786948 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:05:35.453317 139643400587072 submission_runner.py:421] Time since start: 11533.58s, 	Step: 21739, 	{'train/accuracy': 0.5675195455551147, 'train/loss': 1.9861010313034058, 'validation/accuracy': 0.5139200091362, 'validation/loss': 2.223585844039917, 'validation/num_examples': 50000, 'test/accuracy': 0.4034000337123871, 'test/loss': 2.8873274326324463, 'test/num_examples': 10000, 'score': 11040.511682748795, 'total_duration': 11533.583505153656, 'accumulated_submission_time': 11040.511682748795, 'accumulated_eval_time': 491.8106515407562, 'accumulated_logging_time': 0.8120110034942627}
I0510 07:05:35.472208 139414655178496 logging_writer.py:48] [21739] accumulated_eval_time=491.810652, accumulated_logging_time=0.812011, accumulated_submission_time=11040.511683, global_step=21739, preemption_count=0, score=11040.511683, test/accuracy=0.403400, test/loss=2.887327, test/num_examples=10000, total_duration=11533.583505, train/accuracy=0.567520, train/loss=1.986101, validation/accuracy=0.513920, validation/loss=2.223586, validation/num_examples=50000
I0510 07:06:07.158480 139414571316992 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2990198135375977, loss=3.5716028213500977
I0510 07:06:57.654496 139414655178496 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.2891901731491089, loss=3.5521159172058105
I0510 07:07:48.382890 139414571316992 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.4228148460388184, loss=3.561213493347168
I0510 07:08:38.721664 139414655178496 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.5195337533950806, loss=3.6647632122039795
I0510 07:09:29.633250 139414571316992 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2384774684906006, loss=3.500579357147217
I0510 07:10:19.944840 139414655178496 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.2167730331420898, loss=4.672708511352539
I0510 07:11:10.484214 139414571316992 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.368669033050537, loss=3.6515755653381348
I0510 07:12:00.805130 139414655178496 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.9977099895477295, loss=5.744197845458984
I0510 07:12:35.907168 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:12:43.975130 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:12:52.655649 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:12:54.320847 139643400587072 submission_runner.py:421] Time since start: 11972.45s, 	Step: 22570, 	{'train/accuracy': 0.5734961032867432, 'train/loss': 1.9080846309661865, 'validation/accuracy': 0.5288599729537964, 'validation/loss': 2.110729455947876, 'validation/num_examples': 50000, 'test/accuracy': 0.4099000096321106, 'test/loss': 2.7759528160095215, 'test/num_examples': 10000, 'score': 11460.916115760803, 'total_duration': 11972.450888872147, 'accumulated_submission_time': 11460.916115760803, 'accumulated_eval_time': 510.2226436138153, 'accumulated_logging_time': 0.8444089889526367}
I0510 07:12:54.338984 139414571316992 logging_writer.py:48] [22570] accumulated_eval_time=510.222644, accumulated_logging_time=0.844409, accumulated_submission_time=11460.916116, global_step=22570, preemption_count=0, score=11460.916116, test/accuracy=0.409900, test/loss=2.775953, test/num_examples=10000, total_duration=11972.450889, train/accuracy=0.573496, train/loss=1.908085, validation/accuracy=0.528860, validation/loss=2.110729, validation/num_examples=50000
I0510 07:13:10.187026 139414655178496 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.5375252962112427, loss=3.624631404876709
I0510 07:14:00.945424 139414571316992 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.225266456604004, loss=3.578922748565674
I0510 07:14:51.349109 139414655178496 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.3901206254959106, loss=3.5598857402801514
I0510 07:15:41.694133 139414571316992 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.2108362913131714, loss=3.5868847370147705
I0510 07:16:32.265442 139414655178496 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.1866436004638672, loss=4.004177093505859
I0510 07:17:22.830726 139414571316992 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.306475281715393, loss=3.5913352966308594
I0510 07:18:13.344557 139414655178496 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.325537919998169, loss=4.173226833343506
I0510 07:19:03.356114 139414571316992 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.248490333557129, loss=3.4744205474853516
I0510 07:19:53.879265 139414655178496 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.2114465236663818, loss=3.665037155151367
I0510 07:19:54.392427 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:20:02.562291 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:20:11.031876 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:20:12.700761 139643400587072 submission_runner.py:421] Time since start: 12410.83s, 	Step: 23402, 	{'train/accuracy': 0.5770507454872131, 'train/loss': 1.9056077003479004, 'validation/accuracy': 0.5333200097084045, 'validation/loss': 2.1202099323272705, 'validation/num_examples': 50000, 'test/accuracy': 0.4130000174045563, 'test/loss': 2.7872328758239746, 'test/num_examples': 10000, 'score': 11880.941533088684, 'total_duration': 12410.831207990646, 'accumulated_submission_time': 11880.941533088684, 'accumulated_eval_time': 528.5296852588654, 'accumulated_logging_time': 0.8736469745635986}
I0510 07:20:12.720435 139414571316992 logging_writer.py:48] [23402] accumulated_eval_time=528.529685, accumulated_logging_time=0.873647, accumulated_submission_time=11880.941533, global_step=23402, preemption_count=0, score=11880.941533, test/accuracy=0.413000, test/loss=2.787233, test/num_examples=10000, total_duration=12410.831208, train/accuracy=0.577051, train/loss=1.905608, validation/accuracy=0.533320, validation/loss=2.120210, validation/num_examples=50000
I0510 07:21:03.185450 139414655178496 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.34229576587677, loss=4.41902494430542
I0510 07:21:54.348663 139414571316992 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.0065102577209473, loss=5.615747928619385
I0510 07:22:44.847320 139414655178496 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3057951927185059, loss=3.488591432571411
I0510 07:23:35.577787 139414571316992 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.353015422821045, loss=4.399977684020996
I0510 07:24:25.967518 139414655178496 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.3414783477783203, loss=5.096223831176758
I0510 07:25:16.565443 139414571316992 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.500490427017212, loss=3.592013120651245
I0510 07:26:07.309175 139414655178496 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.557059407234192, loss=4.021546363830566
I0510 07:26:57.589583 139414571316992 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.318076729774475, loss=5.461621284484863
I0510 07:27:13.221151 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:27:21.426597 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:27:29.929557 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:27:31.595632 139643400587072 submission_runner.py:421] Time since start: 12849.73s, 	Step: 24232, 	{'train/accuracy': 0.5850390791893005, 'train/loss': 1.963027000427246, 'validation/accuracy': 0.5331799983978271, 'validation/loss': 2.2101948261260986, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.8468010425567627, 'test/num_examples': 10000, 'score': 12301.411336421967, 'total_duration': 12849.726059913635, 'accumulated_submission_time': 12301.411336421967, 'accumulated_eval_time': 546.9028518199921, 'accumulated_logging_time': 0.9074513912200928}
I0510 07:27:31.616196 139414655178496 logging_writer.py:48] [24232] accumulated_eval_time=546.902852, accumulated_logging_time=0.907451, accumulated_submission_time=12301.411336, global_step=24232, preemption_count=0, score=12301.411336, test/accuracy=0.415400, test/loss=2.846801, test/num_examples=10000, total_duration=12849.726060, train/accuracy=0.585039, train/loss=1.963027, validation/accuracy=0.533180, validation/loss=2.210195, validation/num_examples=50000
I0510 07:28:07.020668 139414571316992 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.3585140705108643, loss=3.6277456283569336
I0510 07:28:57.687254 139414655178496 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.312635064125061, loss=3.511636972427368
I0510 07:29:48.263493 139414571316992 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6761226654052734, loss=5.475071907043457
I0510 07:30:39.094038 139414655178496 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.134069561958313, loss=3.5380618572235107
I0510 07:31:29.902288 139414571316992 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.2755171060562134, loss=3.509068489074707
I0510 07:32:20.324083 139414655178496 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.2537161111831665, loss=3.518338441848755
I0510 07:33:10.377154 139414571316992 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.291296124458313, loss=3.797171115875244
I0510 07:34:01.209065 139414655178496 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.1958009004592896, loss=3.557041883468628
I0510 07:34:31.813017 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:34:40.093018 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:34:48.643697 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:34:50.310199 139643400587072 submission_runner.py:421] Time since start: 13288.44s, 	Step: 25062, 	{'train/accuracy': 0.5884765386581421, 'train/loss': 1.8750735521316528, 'validation/accuracy': 0.5419999957084656, 'validation/loss': 2.0917065143585205, 'validation/num_examples': 50000, 'test/accuracy': 0.4256000220775604, 'test/loss': 2.742830991744995, 'test/num_examples': 10000, 'score': 12721.578347921371, 'total_duration': 13288.440600633621, 'accumulated_submission_time': 12721.578347921371, 'accumulated_eval_time': 565.3986971378326, 'accumulated_logging_time': 0.9411191940307617}
I0510 07:34:50.323802 139414571316992 logging_writer.py:48] [25062] accumulated_eval_time=565.398697, accumulated_logging_time=0.941119, accumulated_submission_time=12721.578348, global_step=25062, preemption_count=0, score=12721.578348, test/accuracy=0.425600, test/loss=2.742831, test/num_examples=10000, total_duration=13288.440601, train/accuracy=0.588477, train/loss=1.875074, validation/accuracy=0.542000, validation/loss=2.091707, validation/num_examples=50000
I0510 07:35:10.443340 139414655178496 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.3569369316101074, loss=3.568627119064331
I0510 07:36:01.014528 139414571316992 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.234499454498291, loss=3.4871208667755127
I0510 07:36:51.591138 139414655178496 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.5576380491256714, loss=5.436429977416992
I0510 07:37:41.791975 139414571316992 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2861263751983643, loss=3.9154982566833496
I0510 07:38:32.522743 139414655178496 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.2705299854278564, loss=3.3940305709838867
I0510 07:39:22.705759 139414571316992 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.3257113695144653, loss=3.4120874404907227
I0510 07:40:13.563351 139414655178496 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.2877169847488403, loss=3.4439826011657715
I0510 07:41:04.157157 139414571316992 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.7324477434158325, loss=5.441906452178955
I0510 07:41:50.733953 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:41:58.921055 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:42:07.389266 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:42:09.056343 139643400587072 submission_runner.py:421] Time since start: 13727.19s, 	Step: 25893, 	{'train/accuracy': 0.6030663847923279, 'train/loss': 1.800011157989502, 'validation/accuracy': 0.5529199838638306, 'validation/loss': 2.0311379432678223, 'validation/num_examples': 50000, 'test/accuracy': 0.43060001730918884, 'test/loss': 2.67693829536438, 'test/num_examples': 10000, 'score': 13141.95744729042, 'total_duration': 13727.186617136002, 'accumulated_submission_time': 13141.95744729042, 'accumulated_eval_time': 583.7196209430695, 'accumulated_logging_time': 0.9688031673431396}
I0510 07:42:09.071972 139414655178496 logging_writer.py:48] [25893] accumulated_eval_time=583.719621, accumulated_logging_time=0.968803, accumulated_submission_time=13141.957447, global_step=25893, preemption_count=0, score=13141.957447, test/accuracy=0.430600, test/loss=2.676938, test/num_examples=10000, total_duration=13727.186617, train/accuracy=0.603066, train/loss=1.800011, validation/accuracy=0.552920, validation/loss=2.031138, validation/num_examples=50000
I0510 07:42:13.264717 139414571316992 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.466232180595398, loss=4.754337310791016
I0510 07:43:03.823130 139414655178496 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.344433069229126, loss=3.439539909362793
I0510 07:43:54.722032 139414571316992 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.3232001066207886, loss=3.493014335632324
I0510 07:44:44.967531 139414655178496 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.3995555639266968, loss=3.6549174785614014
I0510 07:45:35.464033 139414571316992 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.5129094123840332, loss=4.166197299957275
I0510 07:46:25.553740 139414655178496 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3132901191711426, loss=3.4657177925109863
I0510 07:47:15.858002 139414571316992 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.5959008932113647, loss=5.582189559936523
I0510 07:48:06.070956 139414655178496 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.8309340476989746, loss=5.296988487243652
I0510 07:48:56.624210 139414571316992 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.384691596031189, loss=3.4172327518463135
I0510 07:49:09.220575 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:49:17.376900 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:49:25.845047 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:49:27.516006 139643400587072 submission_runner.py:421] Time since start: 14165.65s, 	Step: 26726, 	{'train/accuracy': 0.6169335842132568, 'train/loss': 1.7284297943115234, 'validation/accuracy': 0.5488199591636658, 'validation/loss': 2.046231746673584, 'validation/num_examples': 50000, 'test/accuracy': 0.43410003185272217, 'test/loss': 2.6995279788970947, 'test/num_examples': 10000, 'score': 13562.074906349182, 'total_duration': 14165.646134853363, 'accumulated_submission_time': 13562.074906349182, 'accumulated_eval_time': 602.0134363174438, 'accumulated_logging_time': 0.9986917972564697}
I0510 07:49:27.537670 139414655178496 logging_writer.py:48] [26726] accumulated_eval_time=602.013436, accumulated_logging_time=0.998692, accumulated_submission_time=13562.074906, global_step=26726, preemption_count=0, score=13562.074906, test/accuracy=0.434100, test/loss=2.699528, test/num_examples=10000, total_duration=14165.646135, train/accuracy=0.616934, train/loss=1.728430, validation/accuracy=0.548820, validation/loss=2.046232, validation/num_examples=50000
I0510 07:50:05.560531 139414571316992 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.167487621307373, loss=3.832552909851074
I0510 07:50:56.556602 139414655178496 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2139244079589844, loss=4.404021263122559
I0510 07:51:46.987475 139414571316992 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.613554835319519, loss=4.519676685333252
I0510 07:52:37.604830 139414655178496 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.5016157627105713, loss=3.4540090560913086
I0510 07:53:27.819674 139414571316992 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.3084303140640259, loss=3.5241036415100098
I0510 07:54:18.408923 139414655178496 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.4635545015335083, loss=4.82302188873291
I0510 07:55:08.675610 139414571316992 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.2863199710845947, loss=3.4043140411376953
I0510 07:55:59.674666 139414655178496 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2312681674957275, loss=3.425969362258911
I0510 07:56:28.035246 139643400587072 spec.py:298] Evaluating on the training split.
I0510 07:56:36.263206 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 07:56:44.895703 139643400587072 spec.py:326] Evaluating on the test split.
I0510 07:56:46.564224 139643400587072 submission_runner.py:421] Time since start: 14604.69s, 	Step: 27557, 	{'train/accuracy': 0.6059374809265137, 'train/loss': 1.7797785997390747, 'validation/accuracy': 0.560259997844696, 'validation/loss': 2.0030548572540283, 'validation/num_examples': 50000, 'test/accuracy': 0.43810001015663147, 'test/loss': 2.6662871837615967, 'test/num_examples': 10000, 'score': 13982.541913032532, 'total_duration': 14604.694603204727, 'accumulated_submission_time': 13982.541913032532, 'accumulated_eval_time': 620.5410556793213, 'accumulated_logging_time': 1.0340726375579834}
I0510 07:56:46.585167 139414571316992 logging_writer.py:48] [27557] accumulated_eval_time=620.541056, accumulated_logging_time=1.034073, accumulated_submission_time=13982.541913, global_step=27557, preemption_count=0, score=13982.541913, test/accuracy=0.438100, test/loss=2.666287, test/num_examples=10000, total_duration=14604.694603, train/accuracy=0.605937, train/loss=1.779779, validation/accuracy=0.560260, validation/loss=2.003055, validation/num_examples=50000
I0510 07:57:08.955894 139414655178496 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.3050085306167603, loss=3.366891860961914
I0510 07:57:59.857044 139414571316992 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.4885462522506714, loss=4.705981731414795
I0510 07:58:50.313133 139414655178496 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.3174469470977783, loss=3.390026807785034
I0510 07:59:40.813121 139414571316992 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.0928871631622314, loss=5.5009379386901855
I0510 08:00:30.587490 139643400587072 spec.py:298] Evaluating on the training split.
I0510 08:00:38.897580 139643400587072 spec.py:310] Evaluating on the validation split.
I0510 08:00:47.589112 139643400587072 spec.py:326] Evaluating on the test split.
I0510 08:00:49.265385 139643400587072 submission_runner.py:421] Time since start: 14847.40s, 	Step: 28000, 	{'train/accuracy': 0.6208202838897705, 'train/loss': 1.7147682905197144, 'validation/accuracy': 0.5647599697113037, 'validation/loss': 1.9904669523239136, 'validation/num_examples': 50000, 'test/accuracy': 0.44540002942085266, 'test/loss': 2.6310150623321533, 'test/num_examples': 10000, 'score': 14206.521617650986, 'total_duration': 14847.395925998688, 'accumulated_submission_time': 14206.521617650986, 'accumulated_eval_time': 639.2177753448486, 'accumulated_logging_time': 1.068406581878662}
I0510 08:00:49.285998 139414655178496 logging_writer.py:48] [28000] accumulated_eval_time=639.217775, accumulated_logging_time=1.068407, accumulated_submission_time=14206.521618, global_step=28000, preemption_count=0, score=14206.521618, test/accuracy=0.445400, test/loss=2.631015, test/num_examples=10000, total_duration=14847.395926, train/accuracy=0.620820, train/loss=1.714768, validation/accuracy=0.564760, validation/loss=1.990467, validation/num_examples=50000
I0510 08:00:49.316908 139414571316992 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=14206.521618
I0510 08:00:49.455018 139643400587072 checkpoints.py:356] Saving checkpoint at step: 28000
I0510 08:00:50.087522 139643400587072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1/checkpoint_28000
I0510 08:00:50.097872 139643400587072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_vit_jax/trial_1/checkpoint_28000.
I0510 08:00:50.591530 139643400587072 submission_runner.py:584] Tuning trial 1/1
I0510 08:00:50.591768 139643400587072 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0510 08:00:50.605741 139643400587072 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0026367187965661287, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0025599999353289604, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.002300000051036477, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 113.5528347492218, 'total_duration': 165.01405382156372, 'accumulated_submission_time': 113.5528347492218, 'accumulated_eval_time': 51.46106433868408, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (704, {'train/accuracy': 0.015683593228459358, 'train/loss': 6.279702186584473, 'validation/accuracy': 0.016739999875426292, 'validation/loss': 6.289986610412598, 'validation/num_examples': 50000, 'test/accuracy': 0.012800000607967377, 'test/loss': 6.35510778427124, 'test/num_examples': 10000, 'score': 533.9756042957306, 'total_duration': 600.8151960372925, 'accumulated_submission_time': 533.9756042957306, 'accumulated_eval_time': 66.7995822429657, 'accumulated_logging_time': 0.026096582412719727, 'global_step': 704, 'preemption_count': 0}), (1568, {'train/accuracy': 0.04912109300494194, 'train/loss': 5.6229248046875, 'validation/accuracy': 0.0467199981212616, 'validation/loss': 5.673501968383789, 'validation/num_examples': 50000, 'test/accuracy': 0.03840000182390213, 'test/loss': 5.826052665710449, 'test/num_examples': 10000, 'score': 954.1862871646881, 'total_duration': 1036.4798901081085, 'accumulated_submission_time': 954.1862871646881, 'accumulated_eval_time': 82.20709347724915, 'accumulated_logging_time': 0.0557713508605957, 'global_step': 1568, 'preemption_count': 0}), (2433, {'train/accuracy': 0.08671874552965164, 'train/loss': 5.211260795593262, 'validation/accuracy': 0.0828000009059906, 'validation/loss': 5.2506585121154785, 'validation/num_examples': 50000, 'test/accuracy': 0.06320000439882278, 'test/loss': 5.482518672943115, 'test/num_examples': 10000, 'score': 1374.3718881607056, 'total_duration': 1472.0566880702972, 'accumulated_submission_time': 1374.3718881607056, 'accumulated_eval_time': 97.55522584915161, 'accumulated_logging_time': 0.08239936828613281, 'global_step': 2433, 'preemption_count': 0}), (3298, {'train/accuracy': 0.12818358838558197, 'train/loss': 4.738905429840088, 'validation/accuracy': 0.119159996509552, 'validation/loss': 4.799473285675049, 'validation/num_examples': 50000, 'test/accuracy': 0.09220000356435776, 'test/loss': 5.110001564025879, 'test/num_examples': 10000, 'score': 1794.7256660461426, 'total_duration': 1907.68674492836, 'accumulated_submission_time': 1794.7256660461426, 'accumulated_eval_time': 112.7855851650238, 'accumulated_logging_time': 0.11177897453308105, 'global_step': 3298, 'preemption_count': 0}), (4160, {'train/accuracy': 0.1864648461341858, 'train/loss': 4.322726726531982, 'validation/accuracy': 0.16561999917030334, 'validation/loss': 4.450417518615723, 'validation/num_examples': 50000, 'test/accuracy': 0.1282000094652176, 'test/loss': 4.791110992431641, 'test/num_examples': 10000, 'score': 2214.7247598171234, 'total_duration': 2343.036408185959, 'accumulated_submission_time': 2214.7247598171234, 'accumulated_eval_time': 128.08904123306274, 'accumulated_logging_time': 0.14190411567687988, 'global_step': 4160, 'preemption_count': 0}), (5021, {'train/accuracy': 0.2205859273672104, 'train/loss': 3.9992504119873047, 'validation/accuracy': 0.20273999869823456, 'validation/loss': 4.099082946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.1584000140428543, 'test/loss': 4.526467800140381, 'test/num_examples': 10000, 'score': 2635.1255753040314, 'total_duration': 2778.862047433853, 'accumulated_submission_time': 2635.1255753040314, 'accumulated_eval_time': 143.46635460853577, 'accumulated_logging_time': 0.17289423942565918, 'global_step': 5021, 'preemption_count': 0}), (5884, {'train/accuracy': 0.2549414038658142, 'train/loss': 3.747129440307617, 'validation/accuracy': 0.23347999155521393, 'validation/loss': 3.8836629390716553, 'validation/num_examples': 50000, 'test/accuracy': 0.18160000443458557, 'test/loss': 4.335049629211426, 'test/num_examples': 10000, 'score': 3055.4764370918274, 'total_duration': 3214.6289291381836, 'accumulated_submission_time': 3055.4764370918274, 'accumulated_eval_time': 158.84013962745667, 'accumulated_logging_time': 0.1985921859741211, 'global_step': 5884, 'preemption_count': 0}), (6737, {'train/accuracy': 0.3078320324420929, 'train/loss': 3.4668655395507812, 'validation/accuracy': 0.2651199996471405, 'validation/loss': 3.694242477416992, 'validation/num_examples': 50000, 'test/accuracy': 0.19630001485347748, 'test/loss': 4.175069332122803, 'test/num_examples': 10000, 'score': 3475.51762843132, 'total_duration': 3650.368590116501, 'accumulated_submission_time': 3475.51762843132, 'accumulated_eval_time': 174.49133205413818, 'accumulated_logging_time': 0.22843647003173828, 'global_step': 6737, 'preemption_count': 0}), (7582, {'train/accuracy': 0.31507810950279236, 'train/loss': 3.3839008808135986, 'validation/accuracy': 0.2912199795246124, 'validation/loss': 3.5119004249572754, 'validation/num_examples': 50000, 'test/accuracy': 0.21880000829696655, 'test/loss': 4.030876159667969, 'test/num_examples': 10000, 'score': 3895.7772903442383, 'total_duration': 4086.2144305706024, 'accumulated_submission_time': 3895.7772903442383, 'accumulated_eval_time': 190.0302140712738, 'accumulated_logging_time': 0.2580409049987793, 'global_step': 7582, 'preemption_count': 0}), (8421, {'train/accuracy': 0.3431054651737213, 'train/loss': 3.189359426498413, 'validation/accuracy': 0.3113200068473816, 'validation/loss': 3.3457448482513428, 'validation/num_examples': 50000, 'test/accuracy': 0.2394000142812729, 'test/loss': 3.8773794174194336, 'test/num_examples': 10000, 'score': 4316.052321910858, 'total_duration': 4521.883055448532, 'accumulated_submission_time': 4316.052321910858, 'accumulated_eval_time': 205.37829613685608, 'accumulated_logging_time': 0.28595829010009766, 'global_step': 8421, 'preemption_count': 0}), (9256, {'train/accuracy': 0.3556835949420929, 'train/loss': 3.126404047012329, 'validation/accuracy': 0.32923999428749084, 'validation/loss': 3.2531111240386963, 'validation/num_examples': 50000, 'test/accuracy': 0.249300017952919, 'test/loss': 3.7934415340423584, 'test/num_examples': 10000, 'score': 4736.33429980278, 'total_duration': 4958.052450895309, 'accumulated_submission_time': 4736.33429980278, 'accumulated_eval_time': 221.2202434539795, 'accumulated_logging_time': 0.3144559860229492, 'global_step': 9256, 'preemption_count': 0}), (10091, {'train/accuracy': 0.3690820336341858, 'train/loss': 3.133815288543701, 'validation/accuracy': 0.3416999876499176, 'validation/loss': 3.2839529514312744, 'validation/num_examples': 50000, 'test/accuracy': 0.25620001554489136, 'test/loss': 3.820152521133423, 'test/num_examples': 10000, 'score': 5156.767928123474, 'total_duration': 5394.597569465637, 'accumulated_submission_time': 5156.767928123474, 'accumulated_eval_time': 237.28943991661072, 'accumulated_logging_time': 0.34005212783813477, 'global_step': 10091, 'preemption_count': 0}), (10924, {'train/accuracy': 0.41859373450279236, 'train/loss': 2.7541611194610596, 'validation/accuracy': 0.37727999687194824, 'validation/loss': 2.95984148979187, 'validation/num_examples': 50000, 'test/accuracy': 0.290800005197525, 'test/loss': 3.53593373298645, 'test/num_examples': 10000, 'score': 5576.944191932678, 'total_duration': 5830.689658880234, 'accumulated_submission_time': 5576.944191932678, 'accumulated_eval_time': 253.16605687141418, 'accumulated_logging_time': 0.36259937286376953, 'global_step': 10924, 'preemption_count': 0}), (11756, {'train/accuracy': 0.4308398365974426, 'train/loss': 2.6695427894592285, 'validation/accuracy': 0.39375999569892883, 'validation/loss': 2.8504581451416016, 'validation/num_examples': 50000, 'test/accuracy': 0.30140000581741333, 'test/loss': 3.4531524181365967, 'test/num_examples': 10000, 'score': 5996.997510910034, 'total_duration': 6266.770356893539, 'accumulated_submission_time': 5996.997510910034, 'accumulated_eval_time': 269.1495158672333, 'accumulated_logging_time': 0.38939785957336426, 'global_step': 11756, 'preemption_count': 0}), (12589, {'train/accuracy': 0.44462889432907104, 'train/loss': 2.623359203338623, 'validation/accuracy': 0.4078799784183502, 'validation/loss': 2.805159568786621, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.4135818481445312, 'test/num_examples': 10000, 'score': 6417.37339091301, 'total_duration': 6704.434841871262, 'accumulated_submission_time': 6417.37339091301, 'accumulated_eval_time': 286.3896324634552, 'accumulated_logging_time': 0.42110300064086914, 'global_step': 12589, 'preemption_count': 0}), (13424, {'train/accuracy': 0.48179686069488525, 'train/loss': 2.428098440170288, 'validation/accuracy': 0.42183998227119446, 'validation/loss': 2.7164251804351807, 'validation/num_examples': 50000, 'test/accuracy': 0.3279000222682953, 'test/loss': 3.314305305480957, 'test/num_examples': 10000, 'score': 6837.590988636017, 'total_duration': 7142.501800537109, 'accumulated_submission_time': 6837.590988636017, 'accumulated_eval_time': 304.17714500427246, 'accumulated_logging_time': 0.4658012390136719, 'global_step': 13424, 'preemption_count': 0}), (14255, {'train/accuracy': 0.47343748807907104, 'train/loss': 2.420116662979126, 'validation/accuracy': 0.4390399754047394, 'validation/loss': 2.599135637283325, 'validation/num_examples': 50000, 'test/accuracy': 0.33550000190734863, 'test/loss': 3.217409372329712, 'test/num_examples': 10000, 'score': 7258.014278650284, 'total_duration': 7581.330287694931, 'accumulated_submission_time': 7258.014278650284, 'accumulated_eval_time': 322.53532576560974, 'accumulated_logging_time': 0.4956676959991455, 'global_step': 14255, 'preemption_count': 0}), (15086, {'train/accuracy': 0.4916015565395355, 'train/loss': 2.3632662296295166, 'validation/accuracy': 0.45093998312950134, 'validation/loss': 2.56736421585083, 'validation/num_examples': 50000, 'test/accuracy': 0.3477000296115875, 'test/loss': 3.204709768295288, 'test/num_examples': 10000, 'score': 7678.342448711395, 'total_duration': 8019.919867277145, 'accumulated_submission_time': 7678.342448711395, 'accumulated_eval_time': 340.7395739555359, 'accumulated_logging_time': 0.5357968807220459, 'global_step': 15086, 'preemption_count': 0}), (15917, {'train/accuracy': 0.4987109303474426, 'train/loss': 2.3268303871154785, 'validation/accuracy': 0.4606599807739258, 'validation/loss': 2.4965434074401855, 'validation/num_examples': 50000, 'test/accuracy': 0.3490000069141388, 'test/loss': 3.1463427543640137, 'test/num_examples': 10000, 'score': 8098.651120901108, 'total_duration': 8459.271131038666, 'accumulated_submission_time': 8098.651120901108, 'accumulated_eval_time': 359.7313690185547, 'accumulated_logging_time': 0.5695219039916992, 'global_step': 15917, 'preemption_count': 0}), (16750, {'train/accuracy': 0.5110741853713989, 'train/loss': 2.2409470081329346, 'validation/accuracy': 0.47095999121665955, 'validation/loss': 2.4478166103363037, 'validation/num_examples': 50000, 'test/accuracy': 0.3647000193595886, 'test/loss': 3.060823440551758, 'test/num_examples': 10000, 'score': 8518.864560127258, 'total_duration': 8897.608322143555, 'accumulated_submission_time': 8518.864560127258, 'accumulated_eval_time': 377.8124327659607, 'accumulated_logging_time': 0.5953128337860107, 'global_step': 16750, 'preemption_count': 0}), (17581, {'train/accuracy': 0.5305078029632568, 'train/loss': 2.195829391479492, 'validation/accuracy': 0.48325997591018677, 'validation/loss': 2.425340175628662, 'validation/num_examples': 50000, 'test/accuracy': 0.3783000111579895, 'test/loss': 3.0552666187286377, 'test/num_examples': 10000, 'score': 8939.292304039001, 'total_duration': 9337.810625553131, 'accumulated_submission_time': 8939.292304039001, 'accumulated_eval_time': 397.53436970710754, 'accumulated_logging_time': 0.6306376457214355, 'global_step': 17581, 'preemption_count': 0}), (18417, {'train/accuracy': 0.5323437452316284, 'train/loss': 2.1378982067108154, 'validation/accuracy': 0.49173998832702637, 'validation/loss': 2.3323566913604736, 'validation/num_examples': 50000, 'test/accuracy': 0.3800000250339508, 'test/loss': 2.982506036758423, 'test/num_examples': 10000, 'score': 9359.492275714874, 'total_duration': 9777.17090010643, 'accumulated_submission_time': 9359.492275714874, 'accumulated_eval_time': 416.6498432159424, 'accumulated_logging_time': 0.6586251258850098, 'global_step': 18417, 'preemption_count': 0}), (19248, {'train/accuracy': 0.5361718535423279, 'train/loss': 2.154045343399048, 'validation/accuracy': 0.49281999468803406, 'validation/loss': 2.363997220993042, 'validation/num_examples': 50000, 'test/accuracy': 0.38920003175735474, 'test/loss': 2.987565279006958, 'test/num_examples': 10000, 'score': 9779.632402658463, 'total_duration': 10216.845138072968, 'accumulated_submission_time': 9779.632402658463, 'accumulated_eval_time': 436.13982462882996, 'accumulated_logging_time': 0.6858625411987305, 'global_step': 19248, 'preemption_count': 0}), (20078, {'train/accuracy': 0.5722851157188416, 'train/loss': 1.9278665781021118, 'validation/accuracy': 0.5059999823570251, 'validation/loss': 2.2401540279388428, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.9006295204162598, 'test/num_examples': 10000, 'score': 10200.090380430222, 'total_duration': 10655.945546627045, 'accumulated_submission_time': 10200.090380430222, 'accumulated_eval_time': 454.6891474723816, 'accumulated_logging_time': 0.7537012100219727, 'global_step': 20078, 'preemption_count': 0}), (20907, {'train/accuracy': 0.5478515625, 'train/loss': 2.08109974861145, 'validation/accuracy': 0.5068199634552002, 'validation/loss': 2.2877700328826904, 'validation/num_examples': 50000, 'test/accuracy': 0.39160001277923584, 'test/loss': 2.951646566390991, 'test/num_examples': 10000, 'score': 10620.099198102951, 'total_duration': 11094.737990379333, 'accumulated_submission_time': 10620.099198102951, 'accumulated_eval_time': 473.4241290092468, 'accumulated_logging_time': 0.7836754322052002, 'global_step': 20907, 'preemption_count': 0}), (21739, {'train/accuracy': 0.5675195455551147, 'train/loss': 1.9861010313034058, 'validation/accuracy': 0.5139200091362, 'validation/loss': 2.223585844039917, 'validation/num_examples': 50000, 'test/accuracy': 0.4034000337123871, 'test/loss': 2.8873274326324463, 'test/num_examples': 10000, 'score': 11040.511682748795, 'total_duration': 11533.583505153656, 'accumulated_submission_time': 11040.511682748795, 'accumulated_eval_time': 491.8106515407562, 'accumulated_logging_time': 0.8120110034942627, 'global_step': 21739, 'preemption_count': 0}), (22570, {'train/accuracy': 0.5734961032867432, 'train/loss': 1.9080846309661865, 'validation/accuracy': 0.5288599729537964, 'validation/loss': 2.110729455947876, 'validation/num_examples': 50000, 'test/accuracy': 0.4099000096321106, 'test/loss': 2.7759528160095215, 'test/num_examples': 10000, 'score': 11460.916115760803, 'total_duration': 11972.450888872147, 'accumulated_submission_time': 11460.916115760803, 'accumulated_eval_time': 510.2226436138153, 'accumulated_logging_time': 0.8444089889526367, 'global_step': 22570, 'preemption_count': 0}), (23402, {'train/accuracy': 0.5770507454872131, 'train/loss': 1.9056077003479004, 'validation/accuracy': 0.5333200097084045, 'validation/loss': 2.1202099323272705, 'validation/num_examples': 50000, 'test/accuracy': 0.4130000174045563, 'test/loss': 2.7872328758239746, 'test/num_examples': 10000, 'score': 11880.941533088684, 'total_duration': 12410.831207990646, 'accumulated_submission_time': 11880.941533088684, 'accumulated_eval_time': 528.5296852588654, 'accumulated_logging_time': 0.8736469745635986, 'global_step': 23402, 'preemption_count': 0}), (24232, {'train/accuracy': 0.5850390791893005, 'train/loss': 1.963027000427246, 'validation/accuracy': 0.5331799983978271, 'validation/loss': 2.2101948261260986, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.8468010425567627, 'test/num_examples': 10000, 'score': 12301.411336421967, 'total_duration': 12849.726059913635, 'accumulated_submission_time': 12301.411336421967, 'accumulated_eval_time': 546.9028518199921, 'accumulated_logging_time': 0.9074513912200928, 'global_step': 24232, 'preemption_count': 0}), (25062, {'train/accuracy': 0.5884765386581421, 'train/loss': 1.8750735521316528, 'validation/accuracy': 0.5419999957084656, 'validation/loss': 2.0917065143585205, 'validation/num_examples': 50000, 'test/accuracy': 0.4256000220775604, 'test/loss': 2.742830991744995, 'test/num_examples': 10000, 'score': 12721.578347921371, 'total_duration': 13288.440600633621, 'accumulated_submission_time': 12721.578347921371, 'accumulated_eval_time': 565.3986971378326, 'accumulated_logging_time': 0.9411191940307617, 'global_step': 25062, 'preemption_count': 0}), (25893, {'train/accuracy': 0.6030663847923279, 'train/loss': 1.800011157989502, 'validation/accuracy': 0.5529199838638306, 'validation/loss': 2.0311379432678223, 'validation/num_examples': 50000, 'test/accuracy': 0.43060001730918884, 'test/loss': 2.67693829536438, 'test/num_examples': 10000, 'score': 13141.95744729042, 'total_duration': 13727.186617136002, 'accumulated_submission_time': 13141.95744729042, 'accumulated_eval_time': 583.7196209430695, 'accumulated_logging_time': 0.9688031673431396, 'global_step': 25893, 'preemption_count': 0}), (26726, {'train/accuracy': 0.6169335842132568, 'train/loss': 1.7284297943115234, 'validation/accuracy': 0.5488199591636658, 'validation/loss': 2.046231746673584, 'validation/num_examples': 50000, 'test/accuracy': 0.43410003185272217, 'test/loss': 2.6995279788970947, 'test/num_examples': 10000, 'score': 13562.074906349182, 'total_duration': 14165.646134853363, 'accumulated_submission_time': 13562.074906349182, 'accumulated_eval_time': 602.0134363174438, 'accumulated_logging_time': 0.9986917972564697, 'global_step': 26726, 'preemption_count': 0}), (27557, {'train/accuracy': 0.6059374809265137, 'train/loss': 1.7797785997390747, 'validation/accuracy': 0.560259997844696, 'validation/loss': 2.0030548572540283, 'validation/num_examples': 50000, 'test/accuracy': 0.43810001015663147, 'test/loss': 2.6662871837615967, 'test/num_examples': 10000, 'score': 13982.541913032532, 'total_duration': 14604.694603204727, 'accumulated_submission_time': 13982.541913032532, 'accumulated_eval_time': 620.5410556793213, 'accumulated_logging_time': 1.0340726375579834, 'global_step': 27557, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6208202838897705, 'train/loss': 1.7147682905197144, 'validation/accuracy': 0.5647599697113037, 'validation/loss': 1.9904669523239136, 'validation/num_examples': 50000, 'test/accuracy': 0.44540002942085266, 'test/loss': 2.6310150623321533, 'test/num_examples': 10000, 'score': 14206.521617650986, 'total_duration': 14847.395925998688, 'accumulated_submission_time': 14206.521617650986, 'accumulated_eval_time': 639.2177753448486, 'accumulated_logging_time': 1.068406581878662, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0510 08:00:50.605883 139643400587072 submission_runner.py:587] Timing: 14206.521617650986
I0510 08:00:50.605931 139643400587072 submission_runner.py:588] ====================
I0510 08:00:50.606085 139643400587072 submission_runner.py:651] Final imagenet_vit score: 14206.521617650986
