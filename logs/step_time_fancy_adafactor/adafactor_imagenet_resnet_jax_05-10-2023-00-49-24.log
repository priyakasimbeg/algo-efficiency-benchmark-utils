python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=baselines/adafactor/jax/submission.py --tuning_search_space=baselines/adafactor/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_fancy_adafactor/timing_adafactor --overwrite=True --save_checkpoints=False --max_global_steps=28000 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_05-10-2023-00-49-24.log
I0510 00:49:47.110057 140180531455808 logger_utils.py:61] Removing existing experiment directory /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax because --overwrite was set.
I0510 00:49:47.111741 140180531455808 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax.
I0510 00:49:47.177568 140180531455808 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0510 00:49:48.352684 140180531455808 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0510 00:49:48.353417 140180531455808 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0510 00:49:48.358699 140180531455808 submission_runner.py:544] Using RNG seed 3123015390
I0510 00:49:51.343134 140180531455808 submission_runner.py:553] --- Tuning run 1/1 ---
I0510 00:49:51.343362 140180531455808 submission_runner.py:558] Creating tuning directory at /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1.
I0510 00:49:51.343572 140180531455808 logger_utils.py:92] Saving hparams to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1/hparams.json.
I0510 00:49:51.475954 140180531455808 submission_runner.py:241] Initializing dataset.
I0510 00:49:51.491778 140180531455808 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0510 00:49:51.500751 140180531455808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 00:49:51.500868 140180531455808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 00:49:51.780764 140180531455808 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0510 00:49:53.104739 140180531455808 submission_runner.py:248] Initializing model.
I0510 00:50:06.180311 140180531455808 submission_runner.py:258] Initializing optimizer.
I0510 00:50:09.577350 140180531455808 submission_runner.py:265] Initializing metrics bundle.
I0510 00:50:09.577578 140180531455808 submission_runner.py:283] Initializing checkpoint and logger.
I0510 00:50:09.578503 140180531455808 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0510 00:50:10.769054 140180531455808 submission_runner.py:304] Saving meta data to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0510 00:50:10.769905 140180531455808 submission_runner.py:307] Saving flags to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1/flags_0.json.
I0510 00:50:10.774724 140180531455808 submission_runner.py:319] Starting training loop.
2023-05-10 00:50:13.234821: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-05-10 00:50:13.366429: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-05-10 00:50:13.711091: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-05-10 00:50:14.043544: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
2023-05-10 00:50:14.243912: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 616562688 exceeds 10% of free system memory.
I0510 00:51:46.697217 139995985708800 logging_writer.py:48] [0] global_step=0, grad_norm=0.5969241857528687, loss=6.926407337188721
I0510 00:51:46.719012 140180531455808 spec.py:298] Evaluating on the training split.
I0510 00:51:47.284813 140180531455808 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0510 00:51:47.292947 140180531455808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 00:51:47.293063 140180531455808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 00:51:47.369635 140180531455808 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0510 00:52:00.952175 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 00:52:02.670344 140180531455808 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0510 00:52:02.681275 140180531455808 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0510 00:52:02.681492 140180531455808 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0510 00:52:02.730974 140180531455808 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0510 00:52:22.799729 140180531455808 spec.py:326] Evaluating on the test split.
I0510 00:52:23.293756 140180531455808 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0510 00:52:23.297708 140180531455808 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0510 00:52:23.330334 140180531455808 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0510 00:52:33.188185 140180531455808 submission_runner.py:421] Time since start: 142.41s, 	Step: 1, 	{'train/accuracy': 0.0010961415246129036, 'train/loss': 6.912045001983643, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.912837505340576, 'validation/num_examples': 50000, 'test/accuracy': 0.0005000000237487257, 'test/loss': 6.912479400634766, 'test/num_examples': 10000, 'score': 95.94413018226624, 'total_duration': 142.4134018421173, 'accumulated_submission_time': 95.94413018226624, 'accumulated_eval_time': 46.46912693977356, 'accumulated_logging_time': 0}
I0510 00:52:33.205802 139974477321984 logging_writer.py:48] [1] accumulated_eval_time=46.469127, accumulated_logging_time=0, accumulated_submission_time=95.944130, global_step=1, preemption_count=0, score=95.944130, test/accuracy=0.000500, test/loss=6.912479, test/num_examples=10000, total_duration=142.413402, train/accuracy=0.001096, train/loss=6.912045, validation/accuracy=0.000760, validation/loss=6.912838, validation/num_examples=50000
I0510 00:53:09.461969 139974485714688 logging_writer.py:48] [100] global_step=100, grad_norm=0.5849817395210266, loss=6.874938011169434
I0510 00:53:45.633327 139974477321984 logging_writer.py:48] [200] global_step=200, grad_norm=0.6645925641059875, loss=6.738155364990234
I0510 00:54:21.862351 139974485714688 logging_writer.py:48] [300] global_step=300, grad_norm=0.7514280080795288, loss=6.53305196762085
I0510 00:54:58.044421 139974477321984 logging_writer.py:48] [400] global_step=400, grad_norm=0.7954034805297852, loss=6.386587142944336
I0510 00:55:34.272794 139974485714688 logging_writer.py:48] [500] global_step=500, grad_norm=1.1438840627670288, loss=6.240778923034668
I0510 00:56:10.454927 139974477321984 logging_writer.py:48] [600] global_step=600, grad_norm=1.5833849906921387, loss=6.129917144775391
I0510 00:56:46.718882 139974485714688 logging_writer.py:48] [700] global_step=700, grad_norm=3.042470693588257, loss=6.044764041900635
I0510 00:57:22.918302 139974477321984 logging_writer.py:48] [800] global_step=800, grad_norm=2.4971020221710205, loss=5.874089241027832
I0510 00:57:59.138598 139974485714688 logging_writer.py:48] [900] global_step=900, grad_norm=2.6407270431518555, loss=5.869988441467285
I0510 00:58:35.342367 139974477321984 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.536728620529175, loss=5.721476078033447
I0510 00:59:11.594070 139974485714688 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.370222806930542, loss=5.668003082275391
I0510 00:59:47.731034 139974477321984 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.5102391242980957, loss=5.5278520584106445
I0510 01:00:23.993562 139974485714688 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.02540922164917, loss=5.461201190948486
I0510 01:01:00.179303 139974477321984 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.602961540222168, loss=5.412282466888428
I0510 01:01:03.307093 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:01:10.318825 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:01:20.409704 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:01:22.634731 140180531455808 submission_runner.py:421] Time since start: 671.86s, 	Step: 1410, 	{'train/accuracy': 0.11850287020206451, 'train/loss': 4.740262508392334, 'validation/accuracy': 0.10325999557971954, 'validation/loss': 4.898953914642334, 'validation/num_examples': 50000, 'test/accuracy': 0.07400000095367432, 'test/loss': 5.212960720062256, 'test/num_examples': 10000, 'score': 606.0176303386688, 'total_duration': 671.8599333763123, 'accumulated_submission_time': 606.0176303386688, 'accumulated_eval_time': 65.79675078392029, 'accumulated_logging_time': 0.025952577590942383}
I0510 01:01:22.643435 139974812866304 logging_writer.py:48] [1410] accumulated_eval_time=65.796751, accumulated_logging_time=0.025953, accumulated_submission_time=606.017630, global_step=1410, preemption_count=0, score=606.017630, test/accuracy=0.074000, test/loss=5.212961, test/num_examples=10000, total_duration=671.859933, train/accuracy=0.118503, train/loss=4.740263, validation/accuracy=0.103260, validation/loss=4.898954, validation/num_examples=50000
I0510 01:01:55.698597 139974821259008 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.62823224067688, loss=5.29496431350708
I0510 01:02:31.824843 139974812866304 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.209314346313477, loss=5.172734260559082
I0510 01:03:08.028952 139974821259008 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.2804806232452393, loss=5.229705810546875
I0510 01:03:44.194652 139974812866304 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.920356512069702, loss=5.137174606323242
I0510 01:04:20.432535 139974821259008 logging_writer.py:48] [1900] global_step=1900, grad_norm=6.99013614654541, loss=5.047739505767822
I0510 01:04:56.594889 139974812866304 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.516770362854004, loss=4.966301441192627
I0510 01:05:32.863858 139974821259008 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.795924186706543, loss=4.7459611892700195
I0510 01:06:09.051618 139974812866304 logging_writer.py:48] [2200] global_step=2200, grad_norm=6.507896900177002, loss=4.769515514373779
I0510 01:06:45.287732 139974821259008 logging_writer.py:48] [2300] global_step=2300, grad_norm=7.496913909912109, loss=4.698454856872559
I0510 01:07:21.445443 139974812866304 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.237851142883301, loss=4.622413158416748
I0510 01:07:57.681687 139974821259008 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.963371992111206, loss=4.608898639678955
I0510 01:08:33.850986 139974812866304 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.673638343811035, loss=4.394208908081055
I0510 01:09:10.076710 139974821259008 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.886714220046997, loss=4.481046199798584
I0510 01:09:46.293766 139974812866304 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.0503034591674805, loss=4.360988616943359
I0510 01:09:52.661860 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:09:59.490788 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:10:09.361903 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:10:11.623251 140180531455808 submission_runner.py:421] Time since start: 1200.85s, 	Step: 2819, 	{'train/accuracy': 0.26949137449264526, 'train/loss': 3.5705699920654297, 'validation/accuracy': 0.2449999898672104, 'validation/loss': 3.74039626121521, 'validation/num_examples': 50000, 'test/accuracy': 0.18130001425743103, 'test/loss': 4.291834354400635, 'test/num_examples': 10000, 'score': 1116.0083949565887, 'total_duration': 1200.8484721183777, 'accumulated_submission_time': 1116.0083949565887, 'accumulated_eval_time': 84.75811791419983, 'accumulated_logging_time': 0.042806148529052734}
I0510 01:10:11.632070 139974821259008 logging_writer.py:48] [2819] accumulated_eval_time=84.758118, accumulated_logging_time=0.042806, accumulated_submission_time=1116.008395, global_step=2819, preemption_count=0, score=1116.008395, test/accuracy=0.181300, test/loss=4.291834, test/num_examples=10000, total_duration=1200.848472, train/accuracy=0.269491, train/loss=3.570570, validation/accuracy=0.245000, validation/loss=3.740396, validation/num_examples=50000
I0510 01:10:41.435636 139974812866304 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.649652004241943, loss=4.320331573486328
I0510 01:11:17.589232 139974821259008 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.841015338897705, loss=4.138773441314697
I0510 01:11:53.910046 139974812866304 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.4130356311798096, loss=4.264903545379639
I0510 01:12:30.076910 139974821259008 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.837751865386963, loss=4.112751007080078
I0510 01:13:06.277356 139974812866304 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.123741149902344, loss=4.158446311950684
I0510 01:13:42.444995 139974821259008 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.2890923023223877, loss=4.0440449714660645
I0510 01:14:18.694859 139974812866304 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.4628567695617676, loss=4.160627365112305
I0510 01:14:54.829120 139974821259008 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.1590805053710938, loss=4.037417888641357
I0510 01:15:31.030468 139974812866304 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.26840877532959, loss=4.078196048736572
I0510 01:16:07.150973 139974821259008 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.5889036655426025, loss=3.972869873046875
I0510 01:16:43.330798 139974812866304 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.8328800201416016, loss=3.9302515983581543
I0510 01:17:19.462934 139974821259008 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.5408735275268555, loss=3.8047311305999756
I0510 01:17:55.781730 139974812866304 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.804666519165039, loss=3.823291778564453
I0510 01:18:31.907810 139974821259008 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.7819135189056396, loss=3.7876453399658203
I0510 01:18:41.890391 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:18:48.682309 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:18:58.778297 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:19:01.017307 140180531455808 submission_runner.py:421] Time since start: 1730.24s, 	Step: 4229, 	{'train/accuracy': 0.3961256444454193, 'train/loss': 2.75467586517334, 'validation/accuracy': 0.3679199814796448, 'validation/loss': 2.927394390106201, 'validation/num_examples': 50000, 'test/accuracy': 0.2766000032424927, 'test/loss': 3.582864284515381, 'test/num_examples': 10000, 'score': 1626.2392609119415, 'total_duration': 1730.2425258159637, 'accumulated_submission_time': 1626.2392609119415, 'accumulated_eval_time': 103.88501000404358, 'accumulated_logging_time': 0.05928325653076172}
I0510 01:19:01.026084 139974812866304 logging_writer.py:48] [4229] accumulated_eval_time=103.885010, accumulated_logging_time=0.059283, accumulated_submission_time=1626.239261, global_step=4229, preemption_count=0, score=1626.239261, test/accuracy=0.276600, test/loss=3.582864, test/num_examples=10000, total_duration=1730.242526, train/accuracy=0.396126, train/loss=2.754676, validation/accuracy=0.367920, validation/loss=2.927394, validation/num_examples=50000
I0510 01:19:27.131088 139974821259008 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.7181203365325928, loss=3.709679126739502
I0510 01:20:03.276458 139974812866304 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.8907041549682617, loss=3.718627452850342
I0510 01:20:39.413408 139974821259008 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.527217149734497, loss=3.5811948776245117
I0510 01:21:15.550923 139974812866304 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.722944498062134, loss=3.5970091819763184
I0510 01:21:51.795944 139974821259008 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.6576459407806396, loss=3.686936378479004
I0510 01:22:27.876230 139974812866304 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.5322494506835938, loss=3.6034739017486572
I0510 01:23:04.121875 139974821259008 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.8829094171524048, loss=3.660161256790161
I0510 01:23:40.249073 139974812866304 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.0793204307556152, loss=3.64261531829834
I0510 01:24:16.335785 139974821259008 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.634232521057129, loss=3.4723427295684814
I0510 01:24:52.489260 139974812866304 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.1177520751953125, loss=3.4406394958496094
I0510 01:25:28.720424 139974821259008 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.319960355758667, loss=3.603689432144165
I0510 01:26:04.816196 139974812866304 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.0831315517425537, loss=3.539116382598877
I0510 01:26:41.070173 139974821259008 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.4840242862701416, loss=3.4913878440856934
I0510 01:27:17.116089 139974812866304 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.3702213764190674, loss=3.6007275581359863
I0510 01:27:31.061010 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:27:37.838017 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:27:48.169086 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:27:50.449056 140180531455808 submission_runner.py:421] Time since start: 2259.67s, 	Step: 5640, 	{'train/accuracy': 0.49844545125961304, 'train/loss': 2.2621564865112305, 'validation/accuracy': 0.45743998885154724, 'validation/loss': 2.478515863418579, 'validation/num_examples': 50000, 'test/accuracy': 0.35130003094673157, 'test/loss': 3.1392273902893066, 'test/num_examples': 10000, 'score': 2136.246258020401, 'total_duration': 2259.6742730140686, 'accumulated_submission_time': 2136.246258020401, 'accumulated_eval_time': 123.27302575111389, 'accumulated_logging_time': 0.07630085945129395}
I0510 01:27:50.458035 139974821259008 logging_writer.py:48] [5640] accumulated_eval_time=123.273026, accumulated_logging_time=0.076301, accumulated_submission_time=2136.246258, global_step=5640, preemption_count=0, score=2136.246258, test/accuracy=0.351300, test/loss=3.139227, test/num_examples=10000, total_duration=2259.674273, train/accuracy=0.498445, train/loss=2.262156, validation/accuracy=0.457440, validation/loss=2.478516, validation/num_examples=50000
I0510 01:28:12.600246 139974812866304 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.025527000427246, loss=3.4225919246673584
I0510 01:28:48.669348 139974821259008 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.2685041427612305, loss=3.413015604019165
I0510 01:29:24.723912 139974812866304 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1519427299499512, loss=3.25386643409729
I0510 01:30:00.862291 139974821259008 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.5686218738555908, loss=3.405437707901001
I0510 01:30:36.907899 139974812866304 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.5296599864959717, loss=3.3653547763824463
I0510 01:31:13.108195 139974821259008 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.2536598443984985, loss=3.35884428024292
I0510 01:31:49.322407 139974812866304 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9407846331596375, loss=3.4272961616516113
I0510 01:32:25.416445 139974821259008 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.8025813102722168, loss=3.3730380535125732
I0510 01:33:01.506777 139974812866304 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3456315994262695, loss=3.31540846824646
I0510 01:33:37.616697 139974821259008 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.3907461166381836, loss=3.3382837772369385
I0510 01:34:13.681635 139974812866304 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8406165242195129, loss=3.228604793548584
I0510 01:34:49.794435 139974821259008 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.031105399131775, loss=3.2474071979522705
I0510 01:35:25.932173 139974812866304 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.1683114767074585, loss=3.151052474975586
I0510 01:36:02.009652 139974821259008 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.1466978788375854, loss=3.229614734649658
I0510 01:36:20.605803 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:36:27.481152 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:36:37.606351 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:36:39.863163 140180531455808 submission_runner.py:421] Time since start: 2789.09s, 	Step: 7053, 	{'train/accuracy': 0.5594307780265808, 'train/loss': 1.9634349346160889, 'validation/accuracy': 0.513759970664978, 'validation/loss': 2.1818947792053223, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.8463857173919678, 'test/num_examples': 10000, 'score': 2646.366494655609, 'total_duration': 2789.0883781909943, 'accumulated_submission_time': 2646.366494655609, 'accumulated_eval_time': 142.5303556919098, 'accumulated_logging_time': 0.09299111366271973}
I0510 01:36:39.873177 139974812866304 logging_writer.py:48] [7053] accumulated_eval_time=142.530356, accumulated_logging_time=0.092991, accumulated_submission_time=2646.366495, global_step=7053, preemption_count=0, score=2646.366495, test/accuracy=0.399200, test/loss=2.846386, test/num_examples=10000, total_duration=2789.088378, train/accuracy=0.559431, train/loss=1.963435, validation/accuracy=0.513760, validation/loss=2.181895, validation/num_examples=50000
I0510 01:36:57.331642 139974821259008 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.0377670526504517, loss=3.298433303833008
I0510 01:37:33.419554 139974812866304 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8923144936561584, loss=3.221039056777954
I0510 01:38:09.475905 139974821259008 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.901441216468811, loss=3.0713002681732178
I0510 01:38:45.636861 139974812866304 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8614867925643921, loss=3.2612838745117188
I0510 01:39:21.668467 139974821259008 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6058965921401978, loss=3.122621536254883
I0510 01:39:57.845886 139974812866304 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9647186398506165, loss=3.0666275024414062
I0510 01:40:33.896919 139974821259008 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.1675567626953125, loss=3.2302961349487305
I0510 01:41:10.089955 139974812866304 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9297522306442261, loss=3.1126513481140137
I0510 01:41:46.156162 139974821259008 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0807255506515503, loss=3.0984277725219727
I0510 01:42:22.254935 139974812866304 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7376389503479004, loss=3.1749722957611084
I0510 01:42:58.330414 139974821259008 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8403933048248291, loss=3.056615114212036
I0510 01:43:34.477951 139974812866304 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8824400305747986, loss=3.0399184226989746
I0510 01:44:10.559591 139974821259008 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6413416266441345, loss=2.9231863021850586
I0510 01:44:46.787410 139974812866304 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0013647079467773, loss=3.107344627380371
I0510 01:45:10.091480 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:45:17.049258 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:45:27.418703 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:45:29.501340 140180531455808 submission_runner.py:421] Time since start: 3318.73s, 	Step: 8466, 	{'train/accuracy': 0.5884685516357422, 'train/loss': 1.81358003616333, 'validation/accuracy': 0.5415399670600891, 'validation/loss': 2.0500409603118896, 'validation/num_examples': 50000, 'test/accuracy': 0.421500027179718, 'test/loss': 2.728644847869873, 'test/num_examples': 10000, 'score': 3156.5567338466644, 'total_duration': 3318.726500749588, 'accumulated_submission_time': 3156.5567338466644, 'accumulated_eval_time': 161.9401376247406, 'accumulated_logging_time': 0.11182284355163574}
I0510 01:45:29.513316 139974821259008 logging_writer.py:48] [8466] accumulated_eval_time=161.940138, accumulated_logging_time=0.111823, accumulated_submission_time=3156.556734, global_step=8466, preemption_count=0, score=3156.556734, test/accuracy=0.421500, test/loss=2.728645, test/num_examples=10000, total_duration=3318.726501, train/accuracy=0.588469, train/loss=1.813580, validation/accuracy=0.541540, validation/loss=2.050041, validation/num_examples=50000
I0510 01:45:42.125055 139974812866304 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6150903105735779, loss=3.0480055809020996
I0510 01:46:18.264307 139974821259008 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0744494199752808, loss=3.069145917892456
I0510 01:46:54.344564 139974812866304 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0431091785430908, loss=3.0667598247528076
I0510 01:47:30.487349 139974821259008 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0483752489089966, loss=3.021735191345215
I0510 01:48:06.565814 139974812866304 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7270176410675049, loss=3.084428071975708
I0510 01:48:42.725301 139974821259008 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8051077127456665, loss=3.022157907485962
I0510 01:49:18.846509 139974812866304 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.661858081817627, loss=3.032700538635254
I0510 01:49:55.059847 139974821259008 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8337730169296265, loss=2.939054012298584
I0510 01:50:31.152739 139974812866304 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5420807003974915, loss=2.9717748165130615
I0510 01:51:07.224605 139974821259008 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0016531944274902, loss=3.004469871520996
I0510 01:51:43.382608 139974812866304 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8021510243415833, loss=2.976012945175171
I0510 01:52:19.554178 139974821259008 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5334809422492981, loss=2.87489652633667
I0510 01:52:55.630304 139974812866304 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5414432883262634, loss=2.954716444015503
I0510 01:53:31.783915 139974821259008 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6059303283691406, loss=2.960524082183838
I0510 01:53:59.796113 140180531455808 spec.py:298] Evaluating on the training split.
I0510 01:54:06.793145 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 01:54:17.388934 140180531455808 spec.py:326] Evaluating on the test split.
I0510 01:54:19.422499 140180531455808 submission_runner.py:421] Time since start: 3848.65s, 	Step: 9879, 	{'train/accuracy': 0.6167290806770325, 'train/loss': 1.7050678730010986, 'validation/accuracy': 0.5669199824333191, 'validation/loss': 1.9553146362304688, 'validation/num_examples': 50000, 'test/accuracy': 0.445000022649765, 'test/loss': 2.666539192199707, 'test/num_examples': 10000, 'score': 3666.8109295368195, 'total_duration': 3848.647720813751, 'accumulated_submission_time': 3666.8109295368195, 'accumulated_eval_time': 181.5665225982666, 'accumulated_logging_time': 0.13332223892211914}
I0510 01:54:19.432014 139974812866304 logging_writer.py:48] [9879] accumulated_eval_time=181.566523, accumulated_logging_time=0.133322, accumulated_submission_time=3666.810930, global_step=9879, preemption_count=0, score=3666.810930, test/accuracy=0.445000, test/loss=2.666539, test/num_examples=10000, total_duration=3848.647721, train/accuracy=0.616729, train/loss=1.705068, validation/accuracy=0.566920, validation/loss=1.955315, validation/num_examples=50000
I0510 01:54:27.377893 139974821259008 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5930935740470886, loss=2.8945276737213135
I0510 01:55:03.564325 139974812866304 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5926492810249329, loss=2.866058349609375
I0510 01:55:39.689097 139974821259008 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5950642228126526, loss=2.926691770553589
I0510 01:56:15.756086 139974812866304 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.5852562785148621, loss=2.918459892272949
I0510 01:56:51.914873 139974821259008 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.48586300015449524, loss=2.863150119781494
I0510 01:57:28.021790 139974812866304 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4866553246974945, loss=2.933537244796753
I0510 01:58:04.186950 139974821259008 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6823666095733643, loss=2.901869773864746
I0510 01:58:40.309154 139974812866304 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.42066851258277893, loss=2.803748369216919
I0510 01:59:16.382535 139974821259008 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4146186113357544, loss=2.799935817718506
I0510 01:59:52.482315 139974812866304 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4965085983276367, loss=2.786738872528076
I0510 02:00:28.653290 139974821259008 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4801810681819916, loss=2.8143858909606934
I0510 02:01:04.737800 139974812866304 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.46659374237060547, loss=2.884650945663452
I0510 02:01:40.947400 139974821259008 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.47646626830101013, loss=2.8825531005859375
I0510 02:02:17.186318 139974812866304 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4913356900215149, loss=2.8055667877197266
I0510 02:02:49.519334 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:02:56.534037 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:03:06.985706 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:03:09.167815 140180531455808 submission_runner.py:421] Time since start: 4378.39s, 	Step: 11291, 	{'train/accuracy': 0.6471619606018066, 'train/loss': 1.5356398820877075, 'validation/accuracy': 0.5917999744415283, 'validation/loss': 1.8010002374649048, 'validation/num_examples': 50000, 'test/accuracy': 0.4668000340461731, 'test/loss': 2.4797885417938232, 'test/num_examples': 10000, 'score': 4176.871681690216, 'total_duration': 4378.39169383049, 'accumulated_submission_time': 4176.871681690216, 'accumulated_eval_time': 201.21364188194275, 'accumulated_logging_time': 0.15044546127319336}
I0510 02:03:09.179197 139974821259008 logging_writer.py:48] [11291] accumulated_eval_time=201.213642, accumulated_logging_time=0.150445, accumulated_submission_time=4176.871682, global_step=11291, preemption_count=0, score=4176.871682, test/accuracy=0.466800, test/loss=2.479789, test/num_examples=10000, total_duration=4378.391694, train/accuracy=0.647162, train/loss=1.535640, validation/accuracy=0.591800, validation/loss=1.801000, validation/num_examples=50000
I0510 02:03:12.790431 139974812866304 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4545527398586273, loss=2.848325252532959
I0510 02:03:48.978861 139974821259008 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.45680364966392517, loss=2.83235239982605
I0510 02:04:25.028111 139974812866304 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.4895831048488617, loss=2.8122522830963135
I0510 02:05:01.130941 139974821259008 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.4719870388507843, loss=2.887388229370117
I0510 02:05:37.263354 139974812866304 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5520333051681519, loss=2.8456919193267822
I0510 02:06:13.342692 139974821259008 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.49777543544769287, loss=2.8077046871185303
I0510 02:06:49.552110 139974812866304 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.3655286133289337, loss=2.6505494117736816
I0510 02:07:25.603399 139974821259008 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.533692479133606, loss=2.7428693771362305
I0510 02:08:01.782871 139974812866304 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.39715003967285156, loss=2.8014800548553467
I0510 02:08:37.903606 139974821259008 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.43450692296028137, loss=2.726893901824951
I0510 02:09:14.035077 139974812866304 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.38189253211021423, loss=2.659463882446289
I0510 02:09:50.162384 139974821259008 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6213189363479614, loss=2.839198350906372
I0510 02:10:26.369227 139974812866304 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.43989238142967224, loss=2.741861581802368
I0510 02:11:02.430708 139974821259008 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4504737854003906, loss=2.8291513919830322
I0510 02:11:38.631698 139974812866304 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5641570687294006, loss=2.748006820678711
I0510 02:11:39.216527 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:11:46.770353 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:11:57.183938 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:11:59.436700 140180531455808 submission_runner.py:421] Time since start: 4908.66s, 	Step: 12703, 	{'train/accuracy': 0.6756815910339355, 'train/loss': 1.4451866149902344, 'validation/accuracy': 0.6180799603462219, 'validation/loss': 1.7174981832504272, 'validation/num_examples': 50000, 'test/accuracy': 0.48250001668930054, 'test/loss': 2.4316134452819824, 'test/num_examples': 10000, 'score': 4686.880864143372, 'total_duration': 4908.660715579987, 'accumulated_submission_time': 4686.880864143372, 'accumulated_eval_time': 221.43258142471313, 'accumulated_logging_time': 0.17119145393371582}
I0510 02:11:59.451570 139974821259008 logging_writer.py:48] [12703] accumulated_eval_time=221.432581, accumulated_logging_time=0.171191, accumulated_submission_time=4686.880864, global_step=12703, preemption_count=0, score=4686.880864, test/accuracy=0.482500, test/loss=2.431613, test/num_examples=10000, total_duration=4908.660716, train/accuracy=0.675682, train/loss=1.445187, validation/accuracy=0.618080, validation/loss=1.717498, validation/num_examples=50000
I0510 02:12:34.772805 139974812866304 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4240662753582001, loss=2.7735772132873535
I0510 02:13:10.936568 139974821259008 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.40890562534332275, loss=2.7287895679473877
I0510 02:13:47.042058 139974812866304 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5335816144943237, loss=2.8124403953552246
I0510 02:14:23.187017 139974821259008 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4620785117149353, loss=2.635061502456665
I0510 02:14:59.295721 139974812866304 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3565404415130615, loss=2.635577917098999
I0510 02:15:35.525057 139974821259008 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.41115257143974304, loss=2.720698118209839
I0510 02:16:11.589010 139974812866304 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.3734901547431946, loss=2.641998529434204
I0510 02:16:47.802095 139974821259008 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3271602690219879, loss=2.643932342529297
I0510 02:17:23.903406 139974812866304 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.38409659266471863, loss=2.676860809326172
I0510 02:18:00.064623 139974821259008 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.4179035425186157, loss=2.7304458618164062
I0510 02:18:36.182117 139974812866304 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3139278292655945, loss=2.5907468795776367
I0510 02:19:12.387940 139974821259008 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.42895644903182983, loss=2.6256470680236816
I0510 02:19:48.433935 139974812866304 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3547840416431427, loss=2.670599937438965
I0510 02:20:24.499373 139974821259008 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3479804992675781, loss=2.591736078262329
I0510 02:20:29.791606 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:20:37.048235 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:20:47.496443 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:20:49.548375 140180531455808 submission_runner.py:421] Time since start: 5438.77s, 	Step: 14116, 	{'train/accuracy': 0.6897520422935486, 'train/loss': 1.3505319356918335, 'validation/accuracy': 0.6247199773788452, 'validation/loss': 1.6474639177322388, 'validation/num_examples': 50000, 'test/accuracy': 0.4994000196456909, 'test/loss': 2.320878505706787, 'test/num_examples': 10000, 'score': 5197.19135260582, 'total_duration': 5438.772569894791, 'accumulated_submission_time': 5197.19135260582, 'accumulated_eval_time': 241.18829917907715, 'accumulated_logging_time': 0.1966872215270996}
I0510 02:20:49.558718 139974812866304 logging_writer.py:48] [14116] accumulated_eval_time=241.188299, accumulated_logging_time=0.196687, accumulated_submission_time=5197.191353, global_step=14116, preemption_count=0, score=5197.191353, test/accuracy=0.499400, test/loss=2.320879, test/num_examples=10000, total_duration=5438.772570, train/accuracy=0.689752, train/loss=1.350532, validation/accuracy=0.624720, validation/loss=1.647464, validation/num_examples=50000
I0510 02:21:20.230492 139974821259008 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.3432508409023285, loss=2.5839717388153076
I0510 02:21:56.304796 139974812866304 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.4266704022884369, loss=2.6234068870544434
I0510 02:22:32.382900 139974821259008 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.41818904876708984, loss=2.7474312782287598
I0510 02:23:08.459664 139974812866304 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.37403404712677, loss=2.7103097438812256
I0510 02:23:44.607042 139974821259008 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3418119251728058, loss=2.645318031311035
I0510 02:24:20.709671 139974812866304 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4090554118156433, loss=2.6015777587890625
I0510 02:24:56.865107 139974821259008 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.411405086517334, loss=2.6208207607269287
I0510 02:25:32.893807 139974812866304 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.412606418132782, loss=2.520758628845215
I0510 02:26:09.092974 139974821259008 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3932802379131317, loss=2.6344377994537354
I0510 02:26:45.171466 139974812866304 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.4573684334754944, loss=2.611798048019409
I0510 02:27:21.351736 139974821259008 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3875659704208374, loss=2.6118626594543457
I0510 02:27:57.413226 139974812866304 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.38520923256874084, loss=2.617892265319824
I0510 02:28:33.555313 139974821259008 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.4074571430683136, loss=2.67034912109375
I0510 02:29:09.608986 139974812866304 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3513748347759247, loss=2.650643825531006
I0510 02:29:19.916792 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:29:27.582040 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:29:38.155585 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:29:40.513439 140180531455808 submission_runner.py:421] Time since start: 5969.74s, 	Step: 15530, 	{'train/accuracy': 0.7140664458274841, 'train/loss': 1.2786846160888672, 'validation/accuracy': 0.6434999704360962, 'validation/loss': 1.600439190864563, 'validation/num_examples': 50000, 'test/accuracy': 0.513200044631958, 'test/loss': 2.263833999633789, 'test/num_examples': 10000, 'score': 5707.522194385529, 'total_duration': 5969.7373332977295, 'accumulated_submission_time': 5707.522194385529, 'accumulated_eval_time': 261.78359270095825, 'accumulated_logging_time': 0.2156062126159668}
I0510 02:29:40.527862 139974821259008 logging_writer.py:48] [15530] accumulated_eval_time=261.783593, accumulated_logging_time=0.215606, accumulated_submission_time=5707.522194, global_step=15530, preemption_count=0, score=5707.522194, test/accuracy=0.513200, test/loss=2.263834, test/num_examples=10000, total_duration=5969.737333, train/accuracy=0.714066, train/loss=1.278685, validation/accuracy=0.643500, validation/loss=1.600439, validation/num_examples=50000
I0510 02:30:06.154573 139974812866304 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.34864571690559387, loss=2.639967918395996
I0510 02:30:42.206356 139974821259008 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.477436363697052, loss=2.628660202026367
I0510 02:31:18.378379 139974812866304 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3965007960796356, loss=2.60982084274292
I0510 02:31:54.571115 139974821259008 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.32337474822998047, loss=2.539961099624634
I0510 02:32:30.592828 139974812866304 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3174751102924347, loss=2.605431318283081
I0510 02:33:06.671666 139974821259008 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.39159438014030457, loss=2.58736515045166
I0510 02:33:42.802376 139974812866304 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3341929316520691, loss=2.5670666694641113
I0510 02:34:18.831125 139974821259008 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3319269120693207, loss=2.5647332668304443
I0510 02:34:54.980568 139974812866304 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.30078747868537903, loss=2.5590662956237793
I0510 02:35:31.028548 139974821259008 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.34543853998184204, loss=2.5456912517547607
I0510 02:36:07.192696 139974812866304 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3890979588031769, loss=2.5942561626434326
I0510 02:36:43.313076 139974821259008 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.36908674240112305, loss=2.6247265338897705
I0510 02:37:19.397563 139974812866304 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3099249601364136, loss=2.5334229469299316
I0510 02:37:55.507703 139974821259008 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.31147319078445435, loss=2.6143691539764404
I0510 02:38:10.812965 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:38:18.004630 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:38:28.466709 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:38:30.584346 140180531455808 submission_runner.py:421] Time since start: 6499.81s, 	Step: 16944, 	{'train/accuracy': 0.7538663744926453, 'train/loss': 1.0930018424987793, 'validation/accuracy': 0.6424199938774109, 'validation/loss': 1.5768553018569946, 'validation/num_examples': 50000, 'test/accuracy': 0.5133000016212463, 'test/loss': 2.2820093631744385, 'test/num_examples': 10000, 'score': 6217.777619838715, 'total_duration': 6499.808264017105, 'accumulated_submission_time': 6217.777619838715, 'accumulated_eval_time': 281.553671836853, 'accumulated_logging_time': 0.24056577682495117}
I0510 02:38:30.595736 139974812866304 logging_writer.py:48] [16944] accumulated_eval_time=281.553672, accumulated_logging_time=0.240566, accumulated_submission_time=6217.777620, global_step=16944, preemption_count=0, score=6217.777620, test/accuracy=0.513300, test/loss=2.282009, test/num_examples=10000, total_duration=6499.808264, train/accuracy=0.753866, train/loss=1.093002, validation/accuracy=0.642420, validation/loss=1.576855, validation/num_examples=50000
I0510 02:38:51.302191 139974821259008 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.29706719517707825, loss=2.502070903778076
I0510 02:39:27.382842 139974812866304 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3966580033302307, loss=2.5553908348083496
I0510 02:40:03.479811 139974821259008 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3560198247432709, loss=2.522905111312866
I0510 02:40:39.603470 139974812866304 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.37164220213890076, loss=2.4873011112213135
I0510 02:41:15.638819 139974821259008 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3143587112426758, loss=2.5266809463500977
I0510 02:41:51.794844 139974812866304 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3098425269126892, loss=2.572554111480713
I0510 02:42:27.851019 139974821259008 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3167974650859833, loss=2.5191264152526855
I0510 02:43:04.105994 139974812866304 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.30640482902526855, loss=2.54194974899292
I0510 02:43:40.167656 139974821259008 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.38201239705085754, loss=2.509913444519043
I0510 02:44:16.306612 139974812866304 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3124159872531891, loss=2.4895265102386475
I0510 02:44:52.346175 139974821259008 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.29696062207221985, loss=2.5488979816436768
I0510 02:45:28.506541 139974812866304 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.2969491183757782, loss=2.470763921737671
I0510 02:46:04.590716 139974821259008 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3586810827255249, loss=2.555860996246338
I0510 02:46:40.795666 139974812866304 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.301358163356781, loss=2.6056461334228516
I0510 02:47:00.848891 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:47:08.516708 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:47:18.941912 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:47:20.940046 140180531455808 submission_runner.py:421] Time since start: 7030.16s, 	Step: 18357, 	{'train/accuracy': 0.7561582922935486, 'train/loss': 1.0842703580856323, 'validation/accuracy': 0.6499599814414978, 'validation/loss': 1.563425064086914, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.258106231689453, 'test/num_examples': 10000, 'score': 6728.002514362335, 'total_duration': 7030.163942575455, 'accumulated_submission_time': 6728.002514362335, 'accumulated_eval_time': 301.6434772014618, 'accumulated_logging_time': 0.2612724304199219}
I0510 02:47:20.955070 139974821259008 logging_writer.py:48] [18357] accumulated_eval_time=301.643477, accumulated_logging_time=0.261272, accumulated_submission_time=6728.002514, global_step=18357, preemption_count=0, score=6728.002514, test/accuracy=0.518300, test/loss=2.258106, test/num_examples=10000, total_duration=7030.163943, train/accuracy=0.756158, train/loss=1.084270, validation/accuracy=0.649960, validation/loss=1.563425, validation/num_examples=50000
I0510 02:47:36.827543 139974812866304 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.375861793756485, loss=2.511741876602173
I0510 02:48:12.934928 139974821259008 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.28950342535972595, loss=2.488847494125366
I0510 02:48:49.117383 139974812866304 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.33125039935112, loss=2.4350972175598145
I0510 02:49:25.227324 139974821259008 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.35472267866134644, loss=2.5150184631347656
I0510 02:50:01.424253 139974812866304 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3128892481327057, loss=2.4976675510406494
I0510 02:50:37.651204 139974821259008 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3228541910648346, loss=2.4317493438720703
I0510 02:51:13.717977 139974812866304 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2928975522518158, loss=2.4270317554473877
I0510 02:51:49.844449 139974821259008 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.31633034348487854, loss=2.490206003189087
I0510 02:52:25.981395 139974812866304 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.32410570979118347, loss=2.433779239654541
I0510 02:53:01.972400 139974821259008 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.29554665088653564, loss=2.5236101150512695
I0510 02:53:38.092793 139974812866304 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3005189299583435, loss=2.5214953422546387
I0510 02:54:14.189287 139974821259008 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3336777985095978, loss=2.479647159576416
I0510 02:54:50.426308 139974812866304 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.29446297883987427, loss=2.517813205718994
I0510 02:55:26.471696 139974821259008 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2965520918369293, loss=2.5772502422332764
I0510 02:55:51.233884 140180531455808 spec.py:298] Evaluating on the training split.
I0510 02:55:58.465991 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 02:56:08.805634 140180531455808 spec.py:326] Evaluating on the test split.
I0510 02:56:11.421068 140180531455808 submission_runner.py:421] Time since start: 7560.64s, 	Step: 19770, 	{'train/accuracy': 0.7606425285339355, 'train/loss': 1.047149658203125, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.4990339279174805, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.183821201324463, 'test/num_examples': 10000, 'score': 7238.2512130737305, 'total_duration': 7560.644819021225, 'accumulated_submission_time': 7238.2512130737305, 'accumulated_eval_time': 321.82916951179504, 'accumulated_logging_time': 0.2878072261810303}
I0510 02:56:11.432326 139974812866304 logging_writer.py:48] [19770] accumulated_eval_time=321.829170, accumulated_logging_time=0.287807, accumulated_submission_time=7238.251213, global_step=19770, preemption_count=0, score=7238.251213, test/accuracy=0.532200, test/loss=2.183821, test/num_examples=10000, total_duration=7560.644819, train/accuracy=0.760643, train/loss=1.047150, validation/accuracy=0.660320, validation/loss=1.499034, validation/num_examples=50000
I0510 02:56:22.590987 139974821259008 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.30873212218284607, loss=2.4020133018493652
I0510 02:56:58.631581 139974812866304 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3110850155353546, loss=2.433178663253784
I0510 02:57:34.822223 139974821259008 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3107987940311432, loss=2.4278130531311035
I0510 02:58:10.868360 139974812866304 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3177945613861084, loss=2.4256951808929443
I0510 02:58:47.058620 139974821259008 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.29572972655296326, loss=2.4812490940093994
I0510 02:59:23.114040 139974812866304 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3657231330871582, loss=2.570706844329834
I0510 02:59:59.155714 139974821259008 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.32558974623680115, loss=2.487516403198242
I0510 03:00:35.296897 139974812866304 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3009788393974304, loss=2.508042812347412
I0510 03:01:11.348836 139974821259008 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3067881762981415, loss=2.432504653930664
I0510 03:01:47.503509 139974812866304 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.30051225423812866, loss=2.462324619293213
I0510 03:02:23.710499 139974821259008 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.2891782820224762, loss=2.4462437629699707
I0510 03:02:59.790288 139974812866304 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3073592782020569, loss=2.40164852142334
I0510 03:03:35.857902 139974821259008 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.30918145179748535, loss=2.457386016845703
I0510 03:04:12.041847 139974812866304 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.27301615476608276, loss=2.4652347564697266
I0510 03:04:41.801226 140180531455808 spec.py:298] Evaluating on the training split.
I0510 03:04:48.948629 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 03:04:59.468873 140180531455808 spec.py:326] Evaluating on the test split.
I0510 03:05:01.470884 140180531455808 submission_runner.py:421] Time since start: 8090.69s, 	Step: 21184, 	{'train/accuracy': 0.7701889276504517, 'train/loss': 0.9905835390090942, 'validation/accuracy': 0.671999990940094, 'validation/loss': 1.4330065250396729, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.1261203289031982, 'test/num_examples': 10000, 'score': 7748.592979192734, 'total_duration': 8090.694789171219, 'accumulated_submission_time': 7748.592979192734, 'accumulated_eval_time': 341.49750876426697, 'accumulated_logging_time': 0.30765271186828613}
I0510 03:05:01.482680 139974821259008 logging_writer.py:48] [21184] accumulated_eval_time=341.497509, accumulated_logging_time=0.307653, accumulated_submission_time=7748.592979, global_step=21184, preemption_count=0, score=7748.592979, test/accuracy=0.541500, test/loss=2.126120, test/num_examples=10000, total_duration=8090.694789, train/accuracy=0.770189, train/loss=0.990584, validation/accuracy=0.672000, validation/loss=1.433007, validation/num_examples=50000
I0510 03:05:07.641674 139974812866304 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.28708311915397644, loss=2.3681867122650146
I0510 03:05:43.857111 139974821259008 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.292850524187088, loss=2.4365153312683105
I0510 03:06:19.952884 139974812866304 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3224157989025116, loss=2.537458896636963
I0510 03:06:56.185622 139974821259008 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2619614005088806, loss=2.4183766841888428
I0510 03:07:32.282044 139974812866304 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.2995329201221466, loss=2.421146869659424
I0510 03:08:08.477376 139974821259008 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.2764684855937958, loss=2.454651117324829
I0510 03:08:44.569341 139974812866304 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.27967819571495056, loss=2.363013505935669
I0510 03:09:20.695101 139974821259008 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.29963767528533936, loss=2.3591699600219727
I0510 03:09:56.791581 139974812866304 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2929282784461975, loss=2.3380470275878906
I0510 03:10:32.950388 139974821259008 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.28492674231529236, loss=2.447183132171631
I0510 03:11:09.040243 139974812866304 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3100219666957855, loss=2.3361053466796875
I0510 03:11:45.141118 139974821259008 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3162274658679962, loss=2.373154640197754
I0510 03:12:21.276032 139974812866304 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.2905977666378021, loss=2.3884530067443848
I0510 03:12:57.341079 139974821259008 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3158663511276245, loss=2.405632734298706
I0510 03:13:31.526713 140180531455808 spec.py:298] Evaluating on the training split.
I0510 03:13:38.598958 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 03:13:49.420007 140180531455808 spec.py:326] Evaluating on the test split.
I0510 03:13:51.665375 140180531455808 submission_runner.py:421] Time since start: 8620.89s, 	Step: 22596, 	{'train/accuracy': 0.7677375674247742, 'train/loss': 1.0469872951507568, 'validation/accuracy': 0.6655600070953369, 'validation/loss': 1.5032743215560913, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.1856842041015625, 'test/num_examples': 10000, 'score': 8258.606926441193, 'total_duration': 8620.890574455261, 'accumulated_submission_time': 8258.606926441193, 'accumulated_eval_time': 361.6361298561096, 'accumulated_logging_time': 0.33081722259521484}
I0510 03:13:51.677013 139974812866304 logging_writer.py:48] [22596] accumulated_eval_time=361.636130, accumulated_logging_time=0.330817, accumulated_submission_time=8258.606926, global_step=22596, preemption_count=0, score=8258.606926, test/accuracy=0.537600, test/loss=2.185684, test/num_examples=10000, total_duration=8620.890574, train/accuracy=0.767738, train/loss=1.046987, validation/accuracy=0.665560, validation/loss=1.503274, validation/num_examples=50000
I0510 03:13:53.502351 139974821259008 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.30519741773605347, loss=2.4656498432159424
I0510 03:14:29.665228 139974812866304 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.2839147448539734, loss=2.2925076484680176
I0510 03:15:05.722832 139974821259008 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.29732489585876465, loss=2.36639404296875
I0510 03:15:41.958264 139974812866304 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.30466392636299133, loss=2.4790892601013184
I0510 03:16:18.033590 139974821259008 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.2995852828025818, loss=2.380786418914795
I0510 03:16:54.151811 139974812866304 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.28326842188835144, loss=2.424071788787842
I0510 03:17:30.317526 139974821259008 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2657874822616577, loss=2.3498077392578125
I0510 03:18:06.418119 139974812866304 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.26457300782203674, loss=2.430323600769043
I0510 03:18:42.604077 139974821259008 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3433557450771332, loss=2.2977845668792725
I0510 03:19:18.837859 139974812866304 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.29325196146965027, loss=2.4713966846466064
I0510 03:19:54.930246 139974821259008 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2820791006088257, loss=2.3819384574890137
I0510 03:20:31.030441 139974812866304 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2825581431388855, loss=2.3658225536346436
I0510 03:21:07.290945 139974821259008 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.319048672914505, loss=2.303536891937256
I0510 03:21:43.381120 139974812866304 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.2854826748371124, loss=2.343778133392334
I0510 03:22:19.604667 139974821259008 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2858562171459198, loss=2.40301251411438
I0510 03:22:21.988770 140180531455808 spec.py:298] Evaluating on the training split.
I0510 03:22:29.279174 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 03:22:40.109067 140180531455808 spec.py:326] Evaluating on the test split.
I0510 03:22:42.136121 140180531455808 submission_runner.py:421] Time since start: 9151.36s, 	Step: 24008, 	{'train/accuracy': 0.7785195708274841, 'train/loss': 0.9540560841560364, 'validation/accuracy': 0.6762999892234802, 'validation/loss': 1.418950080871582, 'validation/num_examples': 50000, 'test/accuracy': 0.5419000387191772, 'test/loss': 2.1206202507019043, 'test/num_examples': 10000, 'score': 8768.891129493713, 'total_duration': 9151.360234498978, 'accumulated_submission_time': 8768.891129493713, 'accumulated_eval_time': 381.7823464870453, 'accumulated_logging_time': 0.35143494606018066}
I0510 03:22:42.147127 139974812866304 logging_writer.py:48] [24008] accumulated_eval_time=381.782346, accumulated_logging_time=0.351435, accumulated_submission_time=8768.891129, global_step=24008, preemption_count=0, score=8768.891129, test/accuracy=0.541900, test/loss=2.120620, test/num_examples=10000, total_duration=9151.360234, train/accuracy=0.778520, train/loss=0.954056, validation/accuracy=0.676300, validation/loss=1.418950, validation/num_examples=50000
I0510 03:23:15.694547 139974821259008 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.2761523127555847, loss=2.3473668098449707
I0510 03:23:51.864408 139974812866304 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.32480233907699585, loss=2.2954320907592773
I0510 03:24:27.950713 139974821259008 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.31440579891204834, loss=2.431905508041382
I0510 03:25:04.266706 139974812866304 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.27164050936698914, loss=2.3149843215942383
I0510 03:25:40.353118 139974821259008 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.30720072984695435, loss=2.453300714492798
I0510 03:26:16.548954 139974812866304 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.29105451703071594, loss=2.446528196334839
I0510 03:26:52.699115 139974821259008 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2888540029525757, loss=2.447039842605591
I0510 03:27:28.818063 139974812866304 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.25791725516319275, loss=2.3274428844451904
I0510 03:28:04.898517 139974821259008 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.2962651550769806, loss=2.44720196723938
I0510 03:28:41.105535 139974812866304 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2753337323665619, loss=2.362186908721924
I0510 03:29:17.225313 139974821259008 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.29108574986457825, loss=2.404822587966919
I0510 03:29:53.349954 139974812866304 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2999940812587738, loss=2.3945376873016357
I0510 03:30:29.561739 139974821259008 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.2517053186893463, loss=2.311458110809326
I0510 03:31:05.682823 139974812866304 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.27772778272628784, loss=2.356860876083374
I0510 03:31:12.168839 140180531455808 spec.py:298] Evaluating on the training split.
I0510 03:31:19.461347 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 03:31:30.230860 140180531455808 spec.py:326] Evaluating on the test split.
I0510 03:31:32.390112 140180531455808 submission_runner.py:421] Time since start: 9681.61s, 	Step: 25419, 	{'train/accuracy': 0.7716039419174194, 'train/loss': 1.0071545839309692, 'validation/accuracy': 0.6734600067138672, 'validation/loss': 1.4568926095962524, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.1423912048339844, 'test/num_examples': 10000, 'score': 9278.885692596436, 'total_duration': 9681.61410164833, 'accumulated_submission_time': 9278.885692596436, 'accumulated_eval_time': 402.00236916542053, 'accumulated_logging_time': 0.37114667892456055}
I0510 03:31:32.401722 139974821259008 logging_writer.py:48] [25419] accumulated_eval_time=402.002369, accumulated_logging_time=0.371147, accumulated_submission_time=9278.885693, global_step=25419, preemption_count=0, score=9278.885693, test/accuracy=0.543600, test/loss=2.142391, test/num_examples=10000, total_duration=9681.614102, train/accuracy=0.771604, train/loss=1.007155, validation/accuracy=0.673460, validation/loss=1.456893, validation/num_examples=50000
I0510 03:32:02.015552 139974812866304 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.31264790892601013, loss=2.4665231704711914
I0510 03:32:38.221724 139974821259008 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2865540683269501, loss=2.3968312740325928
I0510 03:33:14.333664 139974812866304 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.27386367321014404, loss=2.3565962314605713
I0510 03:33:50.438871 139974821259008 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2756810486316681, loss=2.3233721256256104
I0510 03:34:26.666779 139974812866304 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.29065635800361633, loss=2.4181127548217773
I0510 03:35:02.772281 139974821259008 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2810942530632019, loss=2.3837969303131104
I0510 03:35:38.991001 139974812866304 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3068245053291321, loss=2.323821783065796
I0510 03:36:15.128817 139974821259008 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.261731892824173, loss=2.3057782649993896
I0510 03:36:51.289993 139974812866304 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.28428730368614197, loss=2.351109504699707
I0510 03:37:27.384491 139974821259008 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.3132029175758362, loss=2.44403076171875
I0510 03:38:03.556093 139974812866304 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2598322629928589, loss=2.3725266456604004
I0510 03:38:39.658047 139974821259008 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2782095670700073, loss=2.4079833030700684
I0510 03:39:15.803063 139974812866304 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.27590590715408325, loss=2.34289813041687
I0510 03:39:51.865617 139974821259008 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.3338426947593689, loss=2.31890869140625
I0510 03:40:02.590406 140180531455808 spec.py:298] Evaluating on the training split.
I0510 03:40:09.873207 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 03:40:20.398016 140180531455808 spec.py:326] Evaluating on the test split.
I0510 03:40:22.652264 140180531455808 submission_runner.py:421] Time since start: 10211.88s, 	Step: 26831, 	{'train/accuracy': 0.7882652878761292, 'train/loss': 0.9323700666427612, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.3921185731887817, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.068401575088501, 'test/num_examples': 10000, 'score': 9789.047098875046, 'total_duration': 10211.875989437103, 'accumulated_submission_time': 9789.047098875046, 'accumulated_eval_time': 422.06271171569824, 'accumulated_logging_time': 0.39137744903564453}
I0510 03:40:22.664493 139974812866304 logging_writer.py:48] [26831] accumulated_eval_time=422.062712, accumulated_logging_time=0.391377, accumulated_submission_time=9789.047099, global_step=26831, preemption_count=0, score=9789.047099, test/accuracy=0.557300, test/loss=2.068402, test/num_examples=10000, total_duration=10211.875989, train/accuracy=0.788265, train/loss=0.932370, validation/accuracy=0.685000, validation/loss=1.392119, validation/num_examples=50000
I0510 03:40:48.055314 139974821259008 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2766363322734833, loss=2.3582770824432373
I0510 03:41:24.175672 139974812866304 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.32910075783729553, loss=2.431154251098633
I0510 03:42:00.262356 139974821259008 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2845507264137268, loss=2.3474180698394775
I0510 03:42:36.451464 139974812866304 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.28357136249542236, loss=2.329909324645996
I0510 03:43:12.562189 139974821259008 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.275734543800354, loss=2.3707494735717773
I0510 03:43:48.730485 139974812866304 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2922937870025635, loss=2.334160327911377
I0510 03:44:24.948105 139974821259008 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.29132726788520813, loss=2.396533966064453
I0510 03:45:01.040844 139974812866304 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.28901463747024536, loss=2.3375027179718018
I0510 03:45:37.148816 139974821259008 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.2696951627731323, loss=2.4034812450408936
I0510 03:46:13.393498 139974812866304 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.29083821177482605, loss=2.4051663875579834
I0510 03:46:49.482032 139974821259008 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.2846919894218445, loss=2.4685556888580322
I0510 03:47:25.193240 140180531455808 spec.py:298] Evaluating on the training split.
I0510 03:47:32.612704 140180531455808 spec.py:310] Evaluating on the validation split.
I0510 03:47:43.417793 140180531455808 spec.py:326] Evaluating on the test split.
I0510 03:47:45.664517 140180531455808 submission_runner.py:421] Time since start: 10654.89s, 	Step: 28000, 	{'train/accuracy': 0.7854352593421936, 'train/loss': 0.945590078830719, 'validation/accuracy': 0.6778799891471863, 'validation/loss': 1.428815245628357, 'validation/num_examples': 50000, 'test/accuracy': 0.5534000396728516, 'test/loss': 2.1190450191497803, 'test/num_examples': 10000, 'score': 10211.551881790161, 'total_duration': 10654.888427972794, 'accumulated_submission_time': 10211.551881790161, 'accumulated_eval_time': 442.53265404701233, 'accumulated_logging_time': 0.41214704513549805}
I0510 03:47:45.676370 139974812866304 logging_writer.py:48] [28000] accumulated_eval_time=442.532654, accumulated_logging_time=0.412147, accumulated_submission_time=10211.551882, global_step=28000, preemption_count=0, score=10211.551882, test/accuracy=0.553400, test/loss=2.119045, test/num_examples=10000, total_duration=10654.888428, train/accuracy=0.785435, train/loss=0.945590, validation/accuracy=0.677880, validation/loss=1.428815, validation/num_examples=50000
I0510 03:47:45.696224 139974821259008 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=10211.551882
I0510 03:47:45.894776 140180531455808 checkpoints.py:356] Saving checkpoint at step: 28000
I0510 03:47:46.541249 140180531455808 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1/checkpoint_28000
I0510 03:47:46.556667 140180531455808 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_fancy_adafactor/timing_adafactor/imagenet_resnet_jax/trial_1/checkpoint_28000.
I0510 03:47:46.940207 140180531455808 submission_runner.py:584] Tuning trial 1/1
I0510 03:47:46.940451 140180531455808 submission_runner.py:585] Hyperparameters: Hyperparameters(learning_rate=0.0032594519610942875, one_minus_beta1=0.03999478140191344, warmup_factor=0.05, weight_decay=0.002578922011395245, label_smoothing=0.1, dropout_rate=0.0)
I0510 03:47:46.945356 140180531455808 submission_runner.py:586] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010961415246129036, 'train/loss': 6.912045001983643, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.912837505340576, 'validation/num_examples': 50000, 'test/accuracy': 0.0005000000237487257, 'test/loss': 6.912479400634766, 'test/num_examples': 10000, 'score': 95.94413018226624, 'total_duration': 142.4134018421173, 'accumulated_submission_time': 95.94413018226624, 'accumulated_eval_time': 46.46912693977356, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1410, {'train/accuracy': 0.11850287020206451, 'train/loss': 4.740262508392334, 'validation/accuracy': 0.10325999557971954, 'validation/loss': 4.898953914642334, 'validation/num_examples': 50000, 'test/accuracy': 0.07400000095367432, 'test/loss': 5.212960720062256, 'test/num_examples': 10000, 'score': 606.0176303386688, 'total_duration': 671.8599333763123, 'accumulated_submission_time': 606.0176303386688, 'accumulated_eval_time': 65.79675078392029, 'accumulated_logging_time': 0.025952577590942383, 'global_step': 1410, 'preemption_count': 0}), (2819, {'train/accuracy': 0.26949137449264526, 'train/loss': 3.5705699920654297, 'validation/accuracy': 0.2449999898672104, 'validation/loss': 3.74039626121521, 'validation/num_examples': 50000, 'test/accuracy': 0.18130001425743103, 'test/loss': 4.291834354400635, 'test/num_examples': 10000, 'score': 1116.0083949565887, 'total_duration': 1200.8484721183777, 'accumulated_submission_time': 1116.0083949565887, 'accumulated_eval_time': 84.75811791419983, 'accumulated_logging_time': 0.042806148529052734, 'global_step': 2819, 'preemption_count': 0}), (4229, {'train/accuracy': 0.3961256444454193, 'train/loss': 2.75467586517334, 'validation/accuracy': 0.3679199814796448, 'validation/loss': 2.927394390106201, 'validation/num_examples': 50000, 'test/accuracy': 0.2766000032424927, 'test/loss': 3.582864284515381, 'test/num_examples': 10000, 'score': 1626.2392609119415, 'total_duration': 1730.2425258159637, 'accumulated_submission_time': 1626.2392609119415, 'accumulated_eval_time': 103.88501000404358, 'accumulated_logging_time': 0.05928325653076172, 'global_step': 4229, 'preemption_count': 0}), (5640, {'train/accuracy': 0.49844545125961304, 'train/loss': 2.2621564865112305, 'validation/accuracy': 0.45743998885154724, 'validation/loss': 2.478515863418579, 'validation/num_examples': 50000, 'test/accuracy': 0.35130003094673157, 'test/loss': 3.1392273902893066, 'test/num_examples': 10000, 'score': 2136.246258020401, 'total_duration': 2259.6742730140686, 'accumulated_submission_time': 2136.246258020401, 'accumulated_eval_time': 123.27302575111389, 'accumulated_logging_time': 0.07630085945129395, 'global_step': 5640, 'preemption_count': 0}), (7053, {'train/accuracy': 0.5594307780265808, 'train/loss': 1.9634349346160889, 'validation/accuracy': 0.513759970664978, 'validation/loss': 2.1818947792053223, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.8463857173919678, 'test/num_examples': 10000, 'score': 2646.366494655609, 'total_duration': 2789.0883781909943, 'accumulated_submission_time': 2646.366494655609, 'accumulated_eval_time': 142.5303556919098, 'accumulated_logging_time': 0.09299111366271973, 'global_step': 7053, 'preemption_count': 0}), (8466, {'train/accuracy': 0.5884685516357422, 'train/loss': 1.81358003616333, 'validation/accuracy': 0.5415399670600891, 'validation/loss': 2.0500409603118896, 'validation/num_examples': 50000, 'test/accuracy': 0.421500027179718, 'test/loss': 2.728644847869873, 'test/num_examples': 10000, 'score': 3156.5567338466644, 'total_duration': 3318.726500749588, 'accumulated_submission_time': 3156.5567338466644, 'accumulated_eval_time': 161.9401376247406, 'accumulated_logging_time': 0.11182284355163574, 'global_step': 8466, 'preemption_count': 0}), (9879, {'train/accuracy': 0.6167290806770325, 'train/loss': 1.7050678730010986, 'validation/accuracy': 0.5669199824333191, 'validation/loss': 1.9553146362304688, 'validation/num_examples': 50000, 'test/accuracy': 0.445000022649765, 'test/loss': 2.666539192199707, 'test/num_examples': 10000, 'score': 3666.8109295368195, 'total_duration': 3848.647720813751, 'accumulated_submission_time': 3666.8109295368195, 'accumulated_eval_time': 181.5665225982666, 'accumulated_logging_time': 0.13332223892211914, 'global_step': 9879, 'preemption_count': 0}), (11291, {'train/accuracy': 0.6471619606018066, 'train/loss': 1.5356398820877075, 'validation/accuracy': 0.5917999744415283, 'validation/loss': 1.8010002374649048, 'validation/num_examples': 50000, 'test/accuracy': 0.4668000340461731, 'test/loss': 2.4797885417938232, 'test/num_examples': 10000, 'score': 4176.871681690216, 'total_duration': 4378.39169383049, 'accumulated_submission_time': 4176.871681690216, 'accumulated_eval_time': 201.21364188194275, 'accumulated_logging_time': 0.15044546127319336, 'global_step': 11291, 'preemption_count': 0}), (12703, {'train/accuracy': 0.6756815910339355, 'train/loss': 1.4451866149902344, 'validation/accuracy': 0.6180799603462219, 'validation/loss': 1.7174981832504272, 'validation/num_examples': 50000, 'test/accuracy': 0.48250001668930054, 'test/loss': 2.4316134452819824, 'test/num_examples': 10000, 'score': 4686.880864143372, 'total_duration': 4908.660715579987, 'accumulated_submission_time': 4686.880864143372, 'accumulated_eval_time': 221.43258142471313, 'accumulated_logging_time': 0.17119145393371582, 'global_step': 12703, 'preemption_count': 0}), (14116, {'train/accuracy': 0.6897520422935486, 'train/loss': 1.3505319356918335, 'validation/accuracy': 0.6247199773788452, 'validation/loss': 1.6474639177322388, 'validation/num_examples': 50000, 'test/accuracy': 0.4994000196456909, 'test/loss': 2.320878505706787, 'test/num_examples': 10000, 'score': 5197.19135260582, 'total_duration': 5438.772569894791, 'accumulated_submission_time': 5197.19135260582, 'accumulated_eval_time': 241.18829917907715, 'accumulated_logging_time': 0.1966872215270996, 'global_step': 14116, 'preemption_count': 0}), (15530, {'train/accuracy': 0.7140664458274841, 'train/loss': 1.2786846160888672, 'validation/accuracy': 0.6434999704360962, 'validation/loss': 1.600439190864563, 'validation/num_examples': 50000, 'test/accuracy': 0.513200044631958, 'test/loss': 2.263833999633789, 'test/num_examples': 10000, 'score': 5707.522194385529, 'total_duration': 5969.7373332977295, 'accumulated_submission_time': 5707.522194385529, 'accumulated_eval_time': 261.78359270095825, 'accumulated_logging_time': 0.2156062126159668, 'global_step': 15530, 'preemption_count': 0}), (16944, {'train/accuracy': 0.7538663744926453, 'train/loss': 1.0930018424987793, 'validation/accuracy': 0.6424199938774109, 'validation/loss': 1.5768553018569946, 'validation/num_examples': 50000, 'test/accuracy': 0.5133000016212463, 'test/loss': 2.2820093631744385, 'test/num_examples': 10000, 'score': 6217.777619838715, 'total_duration': 6499.808264017105, 'accumulated_submission_time': 6217.777619838715, 'accumulated_eval_time': 281.553671836853, 'accumulated_logging_time': 0.24056577682495117, 'global_step': 16944, 'preemption_count': 0}), (18357, {'train/accuracy': 0.7561582922935486, 'train/loss': 1.0842703580856323, 'validation/accuracy': 0.6499599814414978, 'validation/loss': 1.563425064086914, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.258106231689453, 'test/num_examples': 10000, 'score': 6728.002514362335, 'total_duration': 7030.163942575455, 'accumulated_submission_time': 6728.002514362335, 'accumulated_eval_time': 301.6434772014618, 'accumulated_logging_time': 0.2612724304199219, 'global_step': 18357, 'preemption_count': 0}), (19770, {'train/accuracy': 0.7606425285339355, 'train/loss': 1.047149658203125, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.4990339279174805, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.183821201324463, 'test/num_examples': 10000, 'score': 7238.2512130737305, 'total_duration': 7560.644819021225, 'accumulated_submission_time': 7238.2512130737305, 'accumulated_eval_time': 321.82916951179504, 'accumulated_logging_time': 0.2878072261810303, 'global_step': 19770, 'preemption_count': 0}), (21184, {'train/accuracy': 0.7701889276504517, 'train/loss': 0.9905835390090942, 'validation/accuracy': 0.671999990940094, 'validation/loss': 1.4330065250396729, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.1261203289031982, 'test/num_examples': 10000, 'score': 7748.592979192734, 'total_duration': 8090.694789171219, 'accumulated_submission_time': 7748.592979192734, 'accumulated_eval_time': 341.49750876426697, 'accumulated_logging_time': 0.30765271186828613, 'global_step': 21184, 'preemption_count': 0}), (22596, {'train/accuracy': 0.7677375674247742, 'train/loss': 1.0469872951507568, 'validation/accuracy': 0.6655600070953369, 'validation/loss': 1.5032743215560913, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.1856842041015625, 'test/num_examples': 10000, 'score': 8258.606926441193, 'total_duration': 8620.890574455261, 'accumulated_submission_time': 8258.606926441193, 'accumulated_eval_time': 361.6361298561096, 'accumulated_logging_time': 0.33081722259521484, 'global_step': 22596, 'preemption_count': 0}), (24008, {'train/accuracy': 0.7785195708274841, 'train/loss': 0.9540560841560364, 'validation/accuracy': 0.6762999892234802, 'validation/loss': 1.418950080871582, 'validation/num_examples': 50000, 'test/accuracy': 0.5419000387191772, 'test/loss': 2.1206202507019043, 'test/num_examples': 10000, 'score': 8768.891129493713, 'total_duration': 9151.360234498978, 'accumulated_submission_time': 8768.891129493713, 'accumulated_eval_time': 381.7823464870453, 'accumulated_logging_time': 0.35143494606018066, 'global_step': 24008, 'preemption_count': 0}), (25419, {'train/accuracy': 0.7716039419174194, 'train/loss': 1.0071545839309692, 'validation/accuracy': 0.6734600067138672, 'validation/loss': 1.4568926095962524, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.1423912048339844, 'test/num_examples': 10000, 'score': 9278.885692596436, 'total_duration': 9681.61410164833, 'accumulated_submission_time': 9278.885692596436, 'accumulated_eval_time': 402.00236916542053, 'accumulated_logging_time': 0.37114667892456055, 'global_step': 25419, 'preemption_count': 0}), (26831, {'train/accuracy': 0.7882652878761292, 'train/loss': 0.9323700666427612, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.3921185731887817, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.068401575088501, 'test/num_examples': 10000, 'score': 9789.047098875046, 'total_duration': 10211.875989437103, 'accumulated_submission_time': 9789.047098875046, 'accumulated_eval_time': 422.06271171569824, 'accumulated_logging_time': 0.39137744903564453, 'global_step': 26831, 'preemption_count': 0}), (28000, {'train/accuracy': 0.7854352593421936, 'train/loss': 0.945590078830719, 'validation/accuracy': 0.6778799891471863, 'validation/loss': 1.428815245628357, 'validation/num_examples': 50000, 'test/accuracy': 0.5534000396728516, 'test/loss': 2.1190450191497803, 'test/num_examples': 10000, 'score': 10211.551881790161, 'total_duration': 10654.888427972794, 'accumulated_submission_time': 10211.551881790161, 'accumulated_eval_time': 442.53265404701233, 'accumulated_logging_time': 0.41214704513549805, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0510 03:47:46.945496 140180531455808 submission_runner.py:587] Timing: 10211.551881790161
I0510 03:47:46.945552 140180531455808 submission_runner.py:588] ====================
I0510 03:47:46.945679 140180531455808 submission_runner.py:651] Final imagenet_resnet score: 10211.551881790161
