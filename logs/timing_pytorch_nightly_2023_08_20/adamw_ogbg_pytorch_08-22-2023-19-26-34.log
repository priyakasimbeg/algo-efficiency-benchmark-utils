torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_nightly_2023_08_20/adamw --overwrite=True --save_checkpoints=False --max_global_steps=6000 --torch_compile=true 2>&1 | tee -a /logs/ogbg_pytorch_08-22-2023-19-26-34.log
[2023-08-22 19:26:38,322] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-08-22 19:26:38,322] torch.distributed.run: [WARNING] 
[2023-08-22 19:26:38,322] torch.distributed.run: [WARNING] *****************************************
[2023-08-22 19:26:38,322] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-08-22 19:26:38,322] torch.distributed.run: [WARNING] *****************************************
2023-08-22 19:26:43.168281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 19:26:43.168288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0822 19:26:59.482664 139760196605760 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/ogbg_pytorch.
W0822 19:26:59.525561 139760196605760 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.525592 139643997411136 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.525592 139802368943936 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.526315 140486329931584 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.526395 139670045562688 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.526544 139848053798720 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.527070 139843147327296 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 19:26:59.527431 140304599869248 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0822 19:26:59.531885 139760196605760 submission_runner.py:494] Using RNG seed 1768243300
I0822 19:26:59.533733 139760196605760 submission_runner.py:503] --- Tuning run 1/1 ---
I0822 19:26:59.533853 139760196605760 submission_runner.py:508] Creating tuning directory at /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/ogbg_pytorch/trial_1.
I0822 19:26:59.534102 139760196605760 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/ogbg_pytorch/trial_1/hparams.json.
I0822 19:26:59.534916 139760196605760 submission_runner.py:177] Initializing dataset.
I0822 19:26:59.535093 139760196605760 submission_runner.py:184] Initializing model.
W0822 19:27:03.977054 140304599869248 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977057 139843147327296 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977059 139848053798720 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977055 140486329931584 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977056 139643997411136 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977074 139802368943936 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977178 139760196605760 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0822 19:27:03.977190 139670045562688 submission_runner.py:201] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0822 19:27:03.977380 139760196605760 submission_runner.py:218] Initializing optimizer.
I0822 19:27:04.761384 139760196605760 submission_runner.py:225] Initializing metrics bundle.
I0822 19:27:04.761563 139760196605760 submission_runner.py:243] Initializing checkpoint and logger.
I0822 19:27:04.762192 139760196605760 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0822 19:27:04.762311 139760196605760 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0822 19:27:05.239813 139760196605760 submission_runner.py:264] Saving meta data to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0822 19:27:05.240836 139760196605760 submission_runner.py:267] Saving flags to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0822 19:27:05.327563 139760196605760 submission_runner.py:277] Starting training loop.
I0822 19:27:05.879336 139760196605760 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:27:05.885354 139760196605760 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0822 19:27:05.992244 139760196605760 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0822 19:27:06.056317 139760196605760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:27:10.824084 139721560815360 logging_writer.py:48] [0] global_step=0, grad_norm=3.105666, loss=0.766511
I0822 19:27:10.837658 139760196605760 submission.py:120] 0) loss = 0.767, grad_norm = 3.106
I0822 19:27:10.846986 139760196605760 spec.py:320] Evaluating on the training split.
I0822 19:27:10.852118 139760196605760 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:27:10.856658 139760196605760 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0822 19:27:10.926738 139760196605760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:28:07.403241 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 19:28:07.407083 139760196605760 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:28:07.412649 139760196605760 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0822 19:28:07.485275 139760196605760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:28:51.205382 139760196605760 spec.py:348] Evaluating on the test split.
I0822 19:28:51.209171 139760196605760 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:28:51.213926 139760196605760 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0822 19:28:51.290511 139760196605760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0822 19:29:37.069218 139760196605760 submission_runner.py:365] Time since start: 151.74s, 	Step: 1, 	{'train/accuracy': 0.46622092718489266, 'train/loss': 0.7659106056384047, 'train/mean_average_precision': 0.021994273129527744, 'validation/accuracy': 0.4772809198940982, 'validation/loss': 0.7598563923887742, 'validation/mean_average_precision': 0.026346153208646184, 'validation/num_examples': 43793, 'test/accuracy': 0.480313149824425, 'test/loss': 0.7579515238613107, 'test/mean_average_precision': 0.027121493056662383, 'test/num_examples': 43793, 'score': 5.519641876220703, 'total_duration': 151.74199795722961, 'accumulated_submission_time': 5.519641876220703, 'accumulated_eval_time': 146.22201204299927, 'accumulated_logging_time': 0}
I0822 19:29:37.090591 139707081611008 logging_writer.py:48] [1] accumulated_eval_time=146.222012, accumulated_logging_time=0, accumulated_submission_time=5.519642, global_step=1, preemption_count=0, score=5.519642, test/accuracy=0.480313, test/loss=0.757952, test/mean_average_precision=0.027121, test/num_examples=43793, total_duration=151.741998, train/accuracy=0.466221, train/loss=0.765911, train/mean_average_precision=0.021994, validation/accuracy=0.477281, validation/loss=0.759856, validation/mean_average_precision=0.026346, validation/num_examples=43793
I0822 19:29:37.524722 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 19:29:37.911277 139707090003712 logging_writer.py:48] [1] global_step=1, grad_norm=3.112103, loss=0.767525
I0822 19:29:37.916671 139760196605760 submission.py:120] 1) loss = 0.768, grad_norm = 3.112
I0822 19:29:38.255704 139707081611008 logging_writer.py:48] [2] global_step=2, grad_norm=3.123134, loss=0.765812
I0822 19:29:38.260679 139760196605760 submission.py:120] 2) loss = 0.766, grad_norm = 3.123
I0822 19:29:38.593459 139707090003712 logging_writer.py:48] [3] global_step=3, grad_norm=3.110415, loss=0.764320
I0822 19:29:38.598182 139760196605760 submission.py:120] 3) loss = 0.764, grad_norm = 3.110
I0822 19:29:38.934064 139707081611008 logging_writer.py:48] [4] global_step=4, grad_norm=3.087165, loss=0.761014
I0822 19:29:38.939252 139760196605760 submission.py:120] 4) loss = 0.761, grad_norm = 3.087
I0822 19:29:39.275686 139707090003712 logging_writer.py:48] [5] global_step=5, grad_norm=3.005910, loss=0.756102
I0822 19:29:39.280499 139760196605760 submission.py:120] 5) loss = 0.756, grad_norm = 3.006
I0822 19:29:39.631912 139707081611008 logging_writer.py:48] [6] global_step=6, grad_norm=2.878449, loss=0.751906
I0822 19:29:39.636939 139760196605760 submission.py:120] 6) loss = 0.752, grad_norm = 2.878
I0822 19:29:39.970191 139707090003712 logging_writer.py:48] [7] global_step=7, grad_norm=2.778736, loss=0.745856
I0822 19:29:39.975267 139760196605760 submission.py:120] 7) loss = 0.746, grad_norm = 2.779
I0822 19:29:40.306650 139707081611008 logging_writer.py:48] [8] global_step=8, grad_norm=2.698841, loss=0.743356
I0822 19:29:40.311521 139760196605760 submission.py:120] 8) loss = 0.743, grad_norm = 2.699
I0822 19:29:40.647133 139707090003712 logging_writer.py:48] [9] global_step=9, grad_norm=2.610455, loss=0.735942
I0822 19:29:40.652186 139760196605760 submission.py:120] 9) loss = 0.736, grad_norm = 2.610
I0822 19:29:40.982431 139707081611008 logging_writer.py:48] [10] global_step=10, grad_norm=2.569786, loss=0.729997
I0822 19:29:40.987119 139760196605760 submission.py:120] 10) loss = 0.730, grad_norm = 2.570
I0822 19:32:22.567904 139707090003712 logging_writer.py:48] [500] global_step=500, grad_norm=0.051309, loss=0.060670
I0822 19:32:22.572813 139760196605760 submission.py:120] 500) loss = 0.061, grad_norm = 0.051
I0822 19:33:37.354892 139760196605760 spec.py:320] Evaluating on the training split.
I0822 19:34:36.368961 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 19:34:39.734221 139760196605760 spec.py:348] Evaluating on the test split.
I0822 19:34:43.024564 139760196605760 submission_runner.py:365] Time since start: 457.70s, 	Step: 726, 	{'train/accuracy': 0.9867161540763137, 'train/loss': 0.05389862831884537, 'train/mean_average_precision': 0.04688688340228962, 'validation/accuracy': 0.9841175701688795, 'validation/loss': 0.06319835120081319, 'validation/mean_average_precision': 0.04810083967958095, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06628598081921523, 'test/mean_average_precision': 0.04946182017245257, 'test/num_examples': 43793, 'score': 245.11628651618958, 'total_duration': 457.6973223686218, 'accumulated_submission_time': 245.11628651618958, 'accumulated_eval_time': 211.8913700580597, 'accumulated_logging_time': 0.4554903507232666}
I0822 19:34:43.042773 139707081611008 logging_writer.py:48] [726] accumulated_eval_time=211.891370, accumulated_logging_time=0.455490, accumulated_submission_time=245.116287, global_step=726, preemption_count=0, score=245.116287, test/accuracy=0.983142, test/loss=0.066286, test/mean_average_precision=0.049462, test/num_examples=43793, total_duration=457.697322, train/accuracy=0.986716, train/loss=0.053899, train/mean_average_precision=0.046887, validation/accuracy=0.984118, validation/loss=0.063198, validation/mean_average_precision=0.048101, validation/num_examples=43793
I0822 19:34:43.481558 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 19:36:15.263582 139707090003712 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.019003, loss=0.053684
I0822 19:36:15.268736 139760196605760 submission.py:120] 1000) loss = 0.054, grad_norm = 0.019
I0822 19:38:43.290053 139760196605760 spec.py:320] Evaluating on the training split.
I0822 19:39:43.460746 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 19:39:46.787716 139760196605760 spec.py:348] Evaluating on the test split.
I0822 19:39:50.046389 139760196605760 submission_runner.py:365] Time since start: 764.72s, 	Step: 1449, 	{'train/accuracy': 0.9871134720164144, 'train/loss': 0.048702205091279, 'train/mean_average_precision': 0.08639300425724779, 'validation/accuracy': 0.9844443524312194, 'validation/loss': 0.05754053368617699, 'validation/mean_average_precision': 0.08889121106431917, 'validation/num_examples': 43793, 'test/accuracy': 0.9834542091880251, 'test/loss': 0.06073521401323141, 'test/mean_average_precision': 0.08755603499784494, 'test/num_examples': 43793, 'score': 484.6616539955139, 'total_duration': 764.7192132472992, 'accumulated_submission_time': 484.6616539955139, 'accumulated_eval_time': 278.6474826335907, 'accumulated_logging_time': 0.9392831325531006}
I0822 19:39:50.061216 139707081611008 logging_writer.py:48] [1449] accumulated_eval_time=278.647483, accumulated_logging_time=0.939283, accumulated_submission_time=484.661654, global_step=1449, preemption_count=0, score=484.661654, test/accuracy=0.983454, test/loss=0.060735, test/mean_average_precision=0.087556, test/num_examples=43793, total_duration=764.719213, train/accuracy=0.987113, train/loss=0.048702, train/mean_average_precision=0.086393, validation/accuracy=0.984444, validation/loss=0.057541, validation/mean_average_precision=0.088891, validation/num_examples=43793
I0822 19:39:50.512938 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 19:40:07.997075 139707090003712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.043752, loss=0.051789
I0822 19:40:08.002171 139760196605760 submission.py:120] 1500) loss = 0.052, grad_norm = 0.044
I0822 19:42:56.087719 139707081611008 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.022980, loss=0.046165
I0822 19:42:56.093411 139760196605760 submission.py:120] 2000) loss = 0.046, grad_norm = 0.023
I0822 19:43:50.258346 139760196605760 spec.py:320] Evaluating on the training split.
I0822 19:44:52.288723 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 19:44:55.604854 139760196605760 spec.py:348] Evaluating on the test split.
I0822 19:44:58.872701 139760196605760 submission_runner.py:365] Time since start: 1073.55s, 	Step: 2166, 	{'train/accuracy': 0.9875926967123368, 'train/loss': 0.04574967140630703, 'train/mean_average_precision': 0.12007729674272365, 'validation/accuracy': 0.9847849366773105, 'validation/loss': 0.05524402566925413, 'validation/mean_average_precision': 0.11267912161807485, 'validation/num_examples': 43793, 'test/accuracy': 0.9838113822809292, 'test/loss': 0.05827248210870277, 'test/mean_average_precision': 0.11229102051619379, 'test/num_examples': 43793, 'score': 724.1096820831299, 'total_duration': 1073.5455062389374, 'accumulated_submission_time': 724.1096820831299, 'accumulated_eval_time': 347.2615919113159, 'accumulated_logging_time': 1.468698263168335}
I0822 19:44:58.888116 139707090003712 logging_writer.py:48] [2166] accumulated_eval_time=347.261592, accumulated_logging_time=1.468698, accumulated_submission_time=724.109682, global_step=2166, preemption_count=0, score=724.109682, test/accuracy=0.983811, test/loss=0.058272, test/mean_average_precision=0.112291, test/num_examples=43793, total_duration=1073.545506, train/accuracy=0.987593, train/loss=0.045750, train/mean_average_precision=0.120077, validation/accuracy=0.984785, validation/loss=0.055244, validation/mean_average_precision=0.112679, validation/num_examples=43793
I0822 19:44:59.385297 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 19:46:51.135877 139707081611008 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.026344, loss=0.047966
I0822 19:46:51.141619 139760196605760 submission.py:120] 2500) loss = 0.048, grad_norm = 0.026
I0822 19:48:59.059102 139760196605760 spec.py:320] Evaluating on the training split.
I0822 19:50:02.690041 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 19:50:06.042822 139760196605760 spec.py:348] Evaluating on the test split.
I0822 19:50:09.290832 139760196605760 submission_runner.py:365] Time since start: 1383.96s, 	Step: 2882, 	{'train/accuracy': 0.9877435382075416, 'train/loss': 0.044246721686908, 'train/mean_average_precision': 0.13671871068881722, 'validation/accuracy': 0.9849046891833854, 'validation/loss': 0.053681230093682995, 'validation/mean_average_precision': 0.1342739862928798, 'validation/num_examples': 43793, 'test/accuracy': 0.983925526040572, 'test/loss': 0.05676909349216304, 'test/mean_average_precision': 0.13217006404824969, 'test/num_examples': 43793, 'score': 963.5242903232574, 'total_duration': 1383.963627576828, 'accumulated_submission_time': 963.5242903232574, 'accumulated_eval_time': 417.4931015968323, 'accumulated_logging_time': 2.012341260910034}
I0822 19:50:09.307567 139707090003712 logging_writer.py:48] [2882] accumulated_eval_time=417.493102, accumulated_logging_time=2.012341, accumulated_submission_time=963.524290, global_step=2882, preemption_count=0, score=963.524290, test/accuracy=0.983926, test/loss=0.056769, test/mean_average_precision=0.132170, test/num_examples=43793, total_duration=1383.963628, train/accuracy=0.987744, train/loss=0.044247, train/mean_average_precision=0.136719, validation/accuracy=0.984905, validation/loss=0.053681, validation/mean_average_precision=0.134274, validation/num_examples=43793
I0822 19:50:09.834241 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 19:50:49.556365 139707081611008 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.020940, loss=0.048760
I0822 19:50:49.561672 139760196605760 submission.py:120] 3000) loss = 0.049, grad_norm = 0.021
I0822 19:53:36.966976 139707090003712 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.016483, loss=0.046703
I0822 19:53:36.972992 139760196605760 submission.py:120] 3500) loss = 0.047, grad_norm = 0.016
I0822 19:54:09.319987 139760196605760 spec.py:320] Evaluating on the training split.
I0822 19:55:14.504533 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 19:55:17.917289 139760196605760 spec.py:348] Evaluating on the test split.
I0822 19:55:21.306766 139760196605760 submission_runner.py:365] Time since start: 1695.98s, 	Step: 3597, 	{'train/accuracy': 0.9880371892252494, 'train/loss': 0.04175983623854898, 'train/mean_average_precision': 0.1793658699010806, 'validation/accuracy': 0.9851998080712377, 'validation/loss': 0.051135074889969774, 'validation/mean_average_precision': 0.15397476294447626, 'validation/num_examples': 43793, 'test/accuracy': 0.9842692209035553, 'test/loss': 0.05395344650974918, 'test/mean_average_precision': 0.1519311718800423, 'test/num_examples': 43793, 'score': 1202.755440711975, 'total_duration': 1695.9795582294464, 'accumulated_submission_time': 1202.755440711975, 'accumulated_eval_time': 489.4795889854431, 'accumulated_logging_time': 2.580101728439331}
I0822 19:55:21.322215 139707081611008 logging_writer.py:48] [3597] accumulated_eval_time=489.479589, accumulated_logging_time=2.580102, accumulated_submission_time=1202.755441, global_step=3597, preemption_count=0, score=1202.755441, test/accuracy=0.984269, test/loss=0.053953, test/mean_average_precision=0.151931, test/num_examples=43793, total_duration=1695.979558, train/accuracy=0.988037, train/loss=0.041760, train/mean_average_precision=0.179366, validation/accuracy=0.985200, validation/loss=0.051135, validation/mean_average_precision=0.153975, validation/num_examples=43793
I0822 19:55:21.878477 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0822 19:57:38.684448 139707090003712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.026731, loss=0.046207
I0822 19:57:38.690366 139760196605760 submission.py:120] 4000) loss = 0.046, grad_norm = 0.027
I0822 19:59:21.331576 139760196605760 spec.py:320] Evaluating on the training split.
I0822 20:00:28.653104 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 20:00:32.137888 139760196605760 spec.py:348] Evaluating on the test split.
I0822 20:00:35.506295 139760196605760 submission_runner.py:365] Time since start: 2010.18s, 	Step: 4308, 	{'train/accuracy': 0.9883939328283337, 'train/loss': 0.04000909243787935, 'train/mean_average_precision': 0.199701747756519, 'validation/accuracy': 0.9855172536975109, 'validation/loss': 0.04985950138709937, 'validation/mean_average_precision': 0.1729184807709904, 'validation/num_examples': 43793, 'test/accuracy': 0.9845476305903591, 'test/loss': 0.052784163995941366, 'test/mean_average_precision': 0.17265510041636634, 'test/num_examples': 43793, 'score': 1441.9497179985046, 'total_duration': 2010.1790902614594, 'accumulated_submission_time': 1441.9497179985046, 'accumulated_eval_time': 563.6540303230286, 'accumulated_logging_time': 3.1824069023132324}
I0822 20:00:35.523102 139712534574848 logging_writer.py:48] [4308] accumulated_eval_time=563.654030, accumulated_logging_time=3.182407, accumulated_submission_time=1441.949718, global_step=4308, preemption_count=0, score=1441.949718, test/accuracy=0.984548, test/loss=0.052784, test/mean_average_precision=0.172655, test/num_examples=43793, total_duration=2010.179090, train/accuracy=0.988394, train/loss=0.040009, train/mean_average_precision=0.199702, validation/accuracy=0.985517, validation/loss=0.049860, validation/mean_average_precision=0.172918, validation/num_examples=43793
I0822 20:00:36.116252 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 20:01:40.594163 139712542967552 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016236, loss=0.045699
I0822 20:01:40.600159 139760196605760 submission.py:120] 4500) loss = 0.046, grad_norm = 0.016
I0822 20:04:28.045740 139712534574848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.011362, loss=0.038330
I0822 20:04:28.052057 139760196605760 submission.py:120] 5000) loss = 0.038, grad_norm = 0.011
I0822 20:04:35.791664 139760196605760 spec.py:320] Evaluating on the training split.
I0822 20:05:43.244973 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 20:05:46.623244 139760196605760 spec.py:348] Evaluating on the test split.
I0822 20:05:50.211488 139760196605760 submission_runner.py:365] Time since start: 2324.88s, 	Step: 5024, 	{'train/accuracy': 0.9884613991375482, 'train/loss': 0.040033510126643086, 'train/mean_average_precision': 0.2186825431591602, 'validation/accuracy': 0.9855655606406394, 'validation/loss': 0.04928371890595734, 'validation/mean_average_precision': 0.1830481512735323, 'validation/num_examples': 43793, 'test/accuracy': 0.98470852696004, 'test/loss': 0.05196817168653512, 'test/mean_average_precision': 0.1874048845046727, 'test/num_examples': 43793, 'score': 1681.3753895759583, 'total_duration': 2324.884206056595, 'accumulated_submission_time': 1681.3753895759583, 'accumulated_eval_time': 638.0734860897064, 'accumulated_logging_time': 3.8143155574798584}
I0822 20:05:50.226840 139712542967552 logging_writer.py:48] [5024] accumulated_eval_time=638.073486, accumulated_logging_time=3.814316, accumulated_submission_time=1681.375390, global_step=5024, preemption_count=0, score=1681.375390, test/accuracy=0.984709, test/loss=0.051968, test/mean_average_precision=0.187405, test/num_examples=43793, total_duration=2324.884206, train/accuracy=0.988461, train/loss=0.040034, train/mean_average_precision=0.218683, validation/accuracy=0.985566, validation/loss=0.049284, validation/mean_average_precision=0.183048, validation/num_examples=43793
I0822 20:05:50.807935 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 20:08:30.953661 139712534574848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.012737, loss=0.041843
I0822 20:08:30.959634 139760196605760 submission.py:120] 5500) loss = 0.042, grad_norm = 0.013
I0822 20:09:50.512815 139760196605760 spec.py:320] Evaluating on the training split.
I0822 20:10:58.837206 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 20:11:02.478234 139760196605760 spec.py:348] Evaluating on the test split.
I0822 20:11:05.919168 139760196605760 submission_runner.py:365] Time since start: 2640.59s, 	Step: 5737, 	{'train/accuracy': 0.9888109066647248, 'train/loss': 0.038133139503988445, 'train/mean_average_precision': 0.24268419931619992, 'validation/accuracy': 0.9857872042620526, 'validation/loss': 0.04791420184548761, 'validation/mean_average_precision': 0.20138320263389284, 'validation/num_examples': 43793, 'test/accuracy': 0.9849806187265684, 'test/loss': 0.0505546916823316, 'test/mean_average_precision': 0.2012989136301116, 'test/num_examples': 43793, 'score': 1920.7139356136322, 'total_duration': 2640.5919439792633, 'accumulated_submission_time': 1920.7139356136322, 'accumulated_eval_time': 713.4795372486115, 'accumulated_logging_time': 4.546701669692993}
I0822 20:11:05.936088 139712542967552 logging_writer.py:48] [5737] accumulated_eval_time=713.479537, accumulated_logging_time=4.546702, accumulated_submission_time=1920.713936, global_step=5737, preemption_count=0, score=1920.713936, test/accuracy=0.984981, test/loss=0.050555, test/mean_average_precision=0.201299, test/num_examples=43793, total_duration=2640.591944, train/accuracy=0.988811, train/loss=0.038133, train/mean_average_precision=0.242684, validation/accuracy=0.985787, validation/loss=0.047914, validation/mean_average_precision=0.201383, validation/num_examples=43793
I0822 20:11:06.552160 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 20:12:36.573167 139760196605760 spec.py:320] Evaluating on the training split.
I0822 20:13:45.554621 139760196605760 spec.py:332] Evaluating on the validation split.
I0822 20:13:49.067141 139760196605760 spec.py:348] Evaluating on the test split.
I0822 20:13:52.432103 139760196605760 submission_runner.py:365] Time since start: 2807.10s, 	Step: 6000, 	{'train/accuracy': 0.9891089741636033, 'train/loss': 0.037589298354882505, 'train/mean_average_precision': 0.25349511341714276, 'validation/accuracy': 0.9858716399273528, 'validation/loss': 0.047754673493777336, 'validation/mean_average_precision': 0.19871236987927127, 'validation/num_examples': 43793, 'test/accuracy': 0.9850374800090472, 'test/loss': 0.05017792645540665, 'test/mean_average_precision': 0.20307275957985907, 'test/num_examples': 43793, 'score': 2010.6181144714355, 'total_duration': 2807.1048669815063, 'accumulated_submission_time': 2010.6181144714355, 'accumulated_eval_time': 789.3381910324097, 'accumulated_logging_time': 5.213672637939453}
I0822 20:13:52.448080 139712534574848 logging_writer.py:48] [6000] accumulated_eval_time=789.338191, accumulated_logging_time=5.213673, accumulated_submission_time=2010.618114, global_step=6000, preemption_count=0, score=2010.618114, test/accuracy=0.985037, test/loss=0.050178, test/mean_average_precision=0.203073, test/num_examples=43793, total_duration=2807.104867, train/accuracy=0.989109, train/loss=0.037589, train/mean_average_precision=0.253495, validation/accuracy=0.985872, validation/loss=0.047755, validation/mean_average_precision=0.198712, validation/num_examples=43793
I0822 20:13:53.085265 139760196605760 submission_runner.py:396] Released all unoccupied cached memory.
I0822 20:13:53.199621 139712542967552 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=2010.618114
I0822 20:13:53.289502 139760196605760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0822 20:13:53.464825 139760196605760 submission_runner.py:534] Tuning trial 1/1
I0822 20:13:53.465048 139760196605760 submission_runner.py:535] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0822 20:13:53.466083 139760196605760 submission_runner.py:536] Metrics: {'eval_results': [(1, {'train/accuracy': 0.46622092718489266, 'train/loss': 0.7659106056384047, 'train/mean_average_precision': 0.021994273129527744, 'validation/accuracy': 0.4772809198940982, 'validation/loss': 0.7598563923887742, 'validation/mean_average_precision': 0.026346153208646184, 'validation/num_examples': 43793, 'test/accuracy': 0.480313149824425, 'test/loss': 0.7579515238613107, 'test/mean_average_precision': 0.027121493056662383, 'test/num_examples': 43793, 'score': 5.519641876220703, 'total_duration': 151.74199795722961, 'accumulated_submission_time': 5.519641876220703, 'accumulated_eval_time': 146.22201204299927, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (726, {'train/accuracy': 0.9867161540763137, 'train/loss': 0.05389862831884537, 'train/mean_average_precision': 0.04688688340228962, 'validation/accuracy': 0.9841175701688795, 'validation/loss': 0.06319835120081319, 'validation/mean_average_precision': 0.04810083967958095, 'validation/num_examples': 43793, 'test/accuracy': 0.983142103926419, 'test/loss': 0.06628598081921523, 'test/mean_average_precision': 0.04946182017245257, 'test/num_examples': 43793, 'score': 245.11628651618958, 'total_duration': 457.6973223686218, 'accumulated_submission_time': 245.11628651618958, 'accumulated_eval_time': 211.8913700580597, 'accumulated_logging_time': 0.4554903507232666, 'global_step': 726, 'preemption_count': 0}), (1449, {'train/accuracy': 0.9871134720164144, 'train/loss': 0.048702205091279, 'train/mean_average_precision': 0.08639300425724779, 'validation/accuracy': 0.9844443524312194, 'validation/loss': 0.05754053368617699, 'validation/mean_average_precision': 0.08889121106431917, 'validation/num_examples': 43793, 'test/accuracy': 0.9834542091880251, 'test/loss': 0.06073521401323141, 'test/mean_average_precision': 0.08755603499784494, 'test/num_examples': 43793, 'score': 484.6616539955139, 'total_duration': 764.7192132472992, 'accumulated_submission_time': 484.6616539955139, 'accumulated_eval_time': 278.6474826335907, 'accumulated_logging_time': 0.9392831325531006, 'global_step': 1449, 'preemption_count': 0}), (2166, {'train/accuracy': 0.9875926967123368, 'train/loss': 0.04574967140630703, 'train/mean_average_precision': 0.12007729674272365, 'validation/accuracy': 0.9847849366773105, 'validation/loss': 0.05524402566925413, 'validation/mean_average_precision': 0.11267912161807485, 'validation/num_examples': 43793, 'test/accuracy': 0.9838113822809292, 'test/loss': 0.05827248210870277, 'test/mean_average_precision': 0.11229102051619379, 'test/num_examples': 43793, 'score': 724.1096820831299, 'total_duration': 1073.5455062389374, 'accumulated_submission_time': 724.1096820831299, 'accumulated_eval_time': 347.2615919113159, 'accumulated_logging_time': 1.468698263168335, 'global_step': 2166, 'preemption_count': 0}), (2882, {'train/accuracy': 0.9877435382075416, 'train/loss': 0.044246721686908, 'train/mean_average_precision': 0.13671871068881722, 'validation/accuracy': 0.9849046891833854, 'validation/loss': 0.053681230093682995, 'validation/mean_average_precision': 0.1342739862928798, 'validation/num_examples': 43793, 'test/accuracy': 0.983925526040572, 'test/loss': 0.05676909349216304, 'test/mean_average_precision': 0.13217006404824969, 'test/num_examples': 43793, 'score': 963.5242903232574, 'total_duration': 1383.963627576828, 'accumulated_submission_time': 963.5242903232574, 'accumulated_eval_time': 417.4931015968323, 'accumulated_logging_time': 2.012341260910034, 'global_step': 2882, 'preemption_count': 0}), (3597, {'train/accuracy': 0.9880371892252494, 'train/loss': 0.04175983623854898, 'train/mean_average_precision': 0.1793658699010806, 'validation/accuracy': 0.9851998080712377, 'validation/loss': 0.051135074889969774, 'validation/mean_average_precision': 0.15397476294447626, 'validation/num_examples': 43793, 'test/accuracy': 0.9842692209035553, 'test/loss': 0.05395344650974918, 'test/mean_average_precision': 0.1519311718800423, 'test/num_examples': 43793, 'score': 1202.755440711975, 'total_duration': 1695.9795582294464, 'accumulated_submission_time': 1202.755440711975, 'accumulated_eval_time': 489.4795889854431, 'accumulated_logging_time': 2.580101728439331, 'global_step': 3597, 'preemption_count': 0}), (4308, {'train/accuracy': 0.9883939328283337, 'train/loss': 0.04000909243787935, 'train/mean_average_precision': 0.199701747756519, 'validation/accuracy': 0.9855172536975109, 'validation/loss': 0.04985950138709937, 'validation/mean_average_precision': 0.1729184807709904, 'validation/num_examples': 43793, 'test/accuracy': 0.9845476305903591, 'test/loss': 0.052784163995941366, 'test/mean_average_precision': 0.17265510041636634, 'test/num_examples': 43793, 'score': 1441.9497179985046, 'total_duration': 2010.1790902614594, 'accumulated_submission_time': 1441.9497179985046, 'accumulated_eval_time': 563.6540303230286, 'accumulated_logging_time': 3.1824069023132324, 'global_step': 4308, 'preemption_count': 0}), (5024, {'train/accuracy': 0.9884613991375482, 'train/loss': 0.040033510126643086, 'train/mean_average_precision': 0.2186825431591602, 'validation/accuracy': 0.9855655606406394, 'validation/loss': 0.04928371890595734, 'validation/mean_average_precision': 0.1830481512735323, 'validation/num_examples': 43793, 'test/accuracy': 0.98470852696004, 'test/loss': 0.05196817168653512, 'test/mean_average_precision': 0.1874048845046727, 'test/num_examples': 43793, 'score': 1681.3753895759583, 'total_duration': 2324.884206056595, 'accumulated_submission_time': 1681.3753895759583, 'accumulated_eval_time': 638.0734860897064, 'accumulated_logging_time': 3.8143155574798584, 'global_step': 5024, 'preemption_count': 0}), (5737, {'train/accuracy': 0.9888109066647248, 'train/loss': 0.038133139503988445, 'train/mean_average_precision': 0.24268419931619992, 'validation/accuracy': 0.9857872042620526, 'validation/loss': 0.04791420184548761, 'validation/mean_average_precision': 0.20138320263389284, 'validation/num_examples': 43793, 'test/accuracy': 0.9849806187265684, 'test/loss': 0.0505546916823316, 'test/mean_average_precision': 0.2012989136301116, 'test/num_examples': 43793, 'score': 1920.7139356136322, 'total_duration': 2640.5919439792633, 'accumulated_submission_time': 1920.7139356136322, 'accumulated_eval_time': 713.4795372486115, 'accumulated_logging_time': 4.546701669692993, 'global_step': 5737, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9891089741636033, 'train/loss': 0.037589298354882505, 'train/mean_average_precision': 0.25349511341714276, 'validation/accuracy': 0.9858716399273528, 'validation/loss': 0.047754673493777336, 'validation/mean_average_precision': 0.19871236987927127, 'validation/num_examples': 43793, 'test/accuracy': 0.9850374800090472, 'test/loss': 0.05017792645540665, 'test/mean_average_precision': 0.20307275957985907, 'test/num_examples': 43793, 'score': 2010.6181144714355, 'total_duration': 2807.1048669815063, 'accumulated_submission_time': 2010.6181144714355, 'accumulated_eval_time': 789.3381910324097, 'accumulated_logging_time': 5.213672637939453, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0822 20:13:53.466202 139760196605760 submission_runner.py:537] Timing: 2010.6181144714355
I0822 20:13:53.466251 139760196605760 submission_runner.py:539] Total number of evals: 10
I0822 20:13:53.466294 139760196605760 submission_runner.py:540] ====================
I0822 20:13:53.466407 139760196605760 submission_runner.py:608] Final ogbg score: 2010.6181144714355
