torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_nightly_2023_08_20/adamw --overwrite=True --save_checkpoints=False --max_global_steps=10000 --torch_compile=true 2>&1 | tee -a /logs/wmt_pytorch_08-22-2023-20-16-49.log
[2023-08-22 20:16:53,518] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2023-08-22 20:16:53,518] torch.distributed.run: [WARNING] 
[2023-08-22 20:16:53,518] torch.distributed.run: [WARNING] *****************************************
[2023-08-22 20:16:53,518] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-08-22 20:16:53,518] torch.distributed.run: [WARNING] *****************************************
2023-08-22 20:16:58.502017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-22 20:16:58.502003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0822 20:17:18.424092 140373593519936 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/wmt_pytorch.
W0822 20:17:18.462037 140382184900416 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.462056 140492932548416 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.462039 140206071400256 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.462038 140606393198400 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.462131 139780189009728 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.462167 140373593519936 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.462556 140685654820672 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0822 20:17:18.463395 140313008236352 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0822 20:17:18.468286 140373593519936 submission_runner.py:494] Using RNG seed 1818093334
I0822 20:17:18.470433 140373593519936 submission_runner.py:503] --- Tuning run 1/1 ---
I0822 20:17:18.470580 140373593519936 submission_runner.py:508] Creating tuning directory at /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/wmt_pytorch/trial_1.
I0822 20:17:18.470943 140373593519936 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/wmt_pytorch/trial_1/hparams.json.
I0822 20:17:18.471969 140373593519936 submission_runner.py:177] Initializing dataset.
I0822 20:17:18.472147 140373593519936 submission_runner.py:184] Initializing model.
I0822 20:17:21.720139 140373593519936 submission_runner.py:215] Performing `torch.compile`.
I0822 20:17:27.258186 140373593519936 submission_runner.py:218] Initializing optimizer.
I0822 20:17:27.260936 140373593519936 submission_runner.py:225] Initializing metrics bundle.
I0822 20:17:27.261080 140373593519936 submission_runner.py:243] Initializing checkpoint and logger.
I0822 20:17:27.262242 140373593519936 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0822 20:17:27.262356 140373593519936 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0822 20:17:27.814617 140373593519936 submission_runner.py:264] Saving meta data to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0822 20:17:27.815738 140373593519936 submission_runner.py:267] Saving flags to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/wmt_pytorch/trial_1/flags_0.json.
I0822 20:17:27.913680 140373593519936 submission_runner.py:277] Starting training loop.
I0822 20:17:27.932205 140373593519936 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0822 20:17:27.939911 140373593519936 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0822 20:17:28.028675 140373593519936 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
[rank5]:[2023-08-22 20:17:30,030] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:[2023-08-22 20:17:30,030] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank7]:[2023-08-22 20:17:30,030] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:[2023-08-22 20:17:30,030] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:[2023-08-22 20:17:30,031] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:[2023-08-22 20:17:30,036] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:[2023-08-22 20:17:30,054] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:[2023-08-22 20:17:30,559] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
I0822 20:18:29.221939 140333518137088 logging_writer.py:48] [0] global_step=0, grad_norm=5.641861, loss=11.003184
I0822 20:18:29.234540 140373593519936 submission.py:120] 0) loss = 11.003, grad_norm = 5.642
I0822 20:18:29.236376 140373593519936 spec.py:320] Evaluating on the training split.
I0822 20:18:29.239464 140373593519936 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0822 20:18:29.243232 140373593519936 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0822 20:18:29.284652 140373593519936 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
[rank7]:[2023-08-22 20:18:29,628] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:[2023-08-22 20:18:29,628] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:[2023-08-22 20:18:29,628] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:[2023-08-22 20:18:29,628] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:[2023-08-22 20:18:29,628] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:[2023-08-22 20:18:29,634] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:[2023-08-22 20:18:29,635] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:[2023-08-22 20:18:29,645] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
I0822 20:18:51.126273 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 20:23:35.177859 140373593519936 spec.py:332] Evaluating on the validation split.
I0822 20:23:35.181340 140373593519936 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0822 20:23:35.185035 140373593519936 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0822 20:23:35.223256 140373593519936 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
[rank0]:[2023-08-22 20:23:38,847] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:[2023-08-22 20:23:38,919] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:[2023-08-22 20:23:38,924] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:[2023-08-22 20:23:39,440] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:[2023-08-22 20:23:39,442] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:[2023-08-22 20:23:39,442] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:[2023-08-22 20:23:39,445] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
[rank7]:[2023-08-22 20:23:39,549] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
  torch.has_cuda,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
  torch.has_cudnn,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
  torch.has_mps,
/usr/local/lib/python3.8/dist-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
  torch.has_mkldnn,
I0822 20:24:09.600893 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 20:28:48.299514 140373593519936 spec.py:348] Evaluating on the test split.
I0822 20:28:48.302382 140373593519936 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0822 20:28:48.306197 140373593519936 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0822 20:28:48.345227 140373593519936 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0822 20:28:52.147314 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 20:33:36.342100 140373593519936 submission_runner.py:365] Time since start: 968.43s, 	Step: 1, 	{'train/accuracy': 0.0006354826263589115, 'train/loss': 11.001906447879076, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.033981289754621, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.028322003369938, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 61.32317328453064, 'total_duration': 968.4293344020844, 'accumulated_submission_time': 61.32317328453064, 'accumulated_eval_time': 907.1056416034698, 'accumulated_logging_time': 0}
I0822 20:33:36.372888 140314852267776 logging_writer.py:48] [1] accumulated_eval_time=907.105642, accumulated_logging_time=0, accumulated_submission_time=61.323173, global_step=1, preemption_count=0, score=61.323173, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.028322, test/num_examples=3003, total_duration=968.429334, train/accuracy=0.000635, train/bleu=0.000000, train/loss=11.001906, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.033981, validation/num_examples=3000
I0822 20:33:37.080034 140373593519936 submission_runner.py:396] Released all unoccupied cached memory.
I0822 20:33:37.768616 140314843875072 logging_writer.py:48] [1] global_step=1, grad_norm=5.550072, loss=11.008617
I0822 20:33:37.772005 140373593519936 submission.py:120] 1) loss = 11.009, grad_norm = 5.550
I0822 20:33:38.195901 140314852267776 logging_writer.py:48] [2] global_step=2, grad_norm=5.631650, loss=10.998197
I0822 20:33:38.199743 140373593519936 submission.py:120] 2) loss = 10.998, grad_norm = 5.632
I0822 20:33:38.621074 140314843875072 logging_writer.py:48] [3] global_step=3, grad_norm=5.547876, loss=10.986278
I0822 20:33:38.624570 140373593519936 submission.py:120] 3) loss = 10.986, grad_norm = 5.548
I0822 20:33:39.045654 140314852267776 logging_writer.py:48] [4] global_step=4, grad_norm=5.454346, loss=10.957262
I0822 20:33:39.049832 140373593519936 submission.py:120] 4) loss = 10.957, grad_norm = 5.454
I0822 20:33:39.474714 140314843875072 logging_writer.py:48] [5] global_step=5, grad_norm=5.434286, loss=10.927804
I0822 20:33:39.478434 140373593519936 submission.py:120] 5) loss = 10.928, grad_norm = 5.434
I0822 20:33:39.894775 140314852267776 logging_writer.py:48] [6] global_step=6, grad_norm=5.422608, loss=10.885461
I0822 20:33:39.898421 140373593519936 submission.py:120] 6) loss = 10.885, grad_norm = 5.423
I0822 20:33:40.320780 140314843875072 logging_writer.py:48] [7] global_step=7, grad_norm=5.219940, loss=10.844080
I0822 20:33:40.324248 140373593519936 submission.py:120] 7) loss = 10.844, grad_norm = 5.220
I0822 20:33:40.745442 140314852267776 logging_writer.py:48] [8] global_step=8, grad_norm=5.142043, loss=10.782040
I0822 20:33:40.748973 140373593519936 submission.py:120] 8) loss = 10.782, grad_norm = 5.142
I0822 20:33:41.172526 140314843875072 logging_writer.py:48] [9] global_step=9, grad_norm=4.878633, loss=10.731359
I0822 20:33:41.176086 140373593519936 submission.py:120] 9) loss = 10.731, grad_norm = 4.879
I0822 20:33:41.594381 140314852267776 logging_writer.py:48] [10] global_step=10, grad_norm=4.723836, loss=10.664450
I0822 20:33:41.597855 140373593519936 submission.py:120] 10) loss = 10.664, grad_norm = 4.724
I0822 20:37:03.892316 140314843875072 logging_writer.py:48] [500] global_step=500, grad_norm=0.551244, loss=6.904703
I0822 20:37:03.896483 140373593519936 submission.py:120] 500) loss = 6.905, grad_norm = 0.551
I0822 20:40:30.792335 140314852267776 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.339765, loss=8.099967
I0822 20:40:30.796010 140373593519936 submission.py:120] 1000) loss = 8.100, grad_norm = 1.340
I0822 20:43:57.031758 140314843875072 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.094309, loss=6.433517
I0822 20:43:57.035942 140373593519936 submission.py:120] 1500) loss = 6.434, grad_norm = 1.094
I0822 20:47:23.687984 140314852267776 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.723339, loss=5.767788
I0822 20:47:23.692003 140373593519936 submission.py:120] 2000) loss = 5.768, grad_norm = 0.723
I0822 20:47:36.496151 140373593519936 spec.py:320] Evaluating on the training split.
I0822 20:47:40.192056 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 20:52:24.648968 140373593519936 spec.py:332] Evaluating on the validation split.
I0822 20:52:28.260991 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 20:57:06.494260 140373593519936 spec.py:348] Evaluating on the test split.
I0822 20:57:10.170452 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:01:54.250736 140373593519936 submission_runner.py:365] Time since start: 2666.34s, 	Step: 2032, 	{'train/accuracy': 0.27990345790009497, 'train/loss': 4.996286732324445, 'train/bleu': 3.485640944901242, 'validation/accuracy': 0.24886238236351688, 'validation/loss': 5.274623454761875, 'validation/bleu': 1.2661157120669724, 'validation/num_examples': 3000, 'test/accuracy': 0.23113125326825867, 'test/loss': 5.546610815757364, 'test/bleu': 0.8796833409741861, 'test/num_examples': 3003, 'score': 899.8910636901855, 'total_duration': 2666.3379492759705, 'accumulated_submission_time': 899.8910636901855, 'accumulated_eval_time': 1764.860092163086, 'accumulated_logging_time': 0.7459840774536133}
I0822 21:01:54.268265 140314843875072 logging_writer.py:48] [2032] accumulated_eval_time=1764.860092, accumulated_logging_time=0.745984, accumulated_submission_time=899.891064, global_step=2032, preemption_count=0, score=899.891064, test/accuracy=0.231131, test/bleu=0.879683, test/loss=5.546611, test/num_examples=3003, total_duration=2666.337949, train/accuracy=0.279903, train/bleu=3.485641, train/loss=4.996287, validation/accuracy=0.248862, validation/bleu=1.266116, validation/loss=5.274623, validation/num_examples=3000
I0822 21:01:55.133525 140373593519936 submission_runner.py:396] Released all unoccupied cached memory.
I0822 21:05:09.252339 140314852267776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.429930, loss=4.776852
I0822 21:05:09.256321 140373593519936 submission.py:120] 2500) loss = 4.777, grad_norm = 0.430
I0822 21:08:35.897477 140314843875072 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.088197, loss=7.096973
I0822 21:08:35.901205 140373593519936 submission.py:120] 3000) loss = 7.097, grad_norm = 0.088
I0822 21:12:02.241846 140314852267776 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.222527, loss=5.636606
I0822 21:12:02.245945 140373593519936 submission.py:120] 3500) loss = 5.637, grad_norm = 0.223
I0822 21:15:28.939559 140314843875072 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.376619, loss=4.454218
I0822 21:15:28.943608 140373593519936 submission.py:120] 4000) loss = 4.454, grad_norm = 0.377
I0822 21:15:54.617510 140373593519936 spec.py:320] Evaluating on the training split.
I0822 21:15:58.301218 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:18:37.376960 140373593519936 spec.py:332] Evaluating on the validation split.
I0822 21:18:40.985231 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:21:15.669637 140373593519936 spec.py:348] Evaluating on the test split.
I0822 21:21:19.341184 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:23:56.268624 140373593519936 submission_runner.py:365] Time since start: 3988.36s, 	Step: 4063, 	{'train/accuracy': 0.45795846964740095, 'train/loss': 3.356152095260814, 'train/bleu': 14.697666638775827, 'validation/accuracy': 0.4491946783052907, 'validation/loss': 3.4526106309903164, 'validation/bleu': 11.010690938157387, 'validation/num_examples': 3000, 'test/accuracy': 0.4386845621985939, 'test/loss': 3.5862783975364594, 'test/bleu': 9.523200444068582, 'test/num_examples': 3003, 'score': 1738.5285358428955, 'total_duration': 3988.355861186981, 'accumulated_submission_time': 1738.5285358428955, 'accumulated_eval_time': 2246.5111231803894, 'accumulated_logging_time': 1.653170108795166}
I0822 21:23:56.285081 140314852267776 logging_writer.py:48] [4063] accumulated_eval_time=2246.511123, accumulated_logging_time=1.653170, accumulated_submission_time=1738.528536, global_step=4063, preemption_count=0, score=1738.528536, test/accuracy=0.438685, test/bleu=9.523200, test/loss=3.586278, test/num_examples=3003, total_duration=3988.355861, train/accuracy=0.457958, train/bleu=14.697667, train/loss=3.356152, validation/accuracy=0.449195, validation/bleu=11.010691, validation/loss=3.452611, validation/num_examples=3000
I0822 21:23:57.122730 140373593519936 submission_runner.py:396] Released all unoccupied cached memory.
I0822 21:26:58.153584 140314843875072 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.297781, loss=3.969535
I0822 21:26:58.157459 140373593519936 submission.py:120] 4500) loss = 3.970, grad_norm = 0.298
I0822 21:30:24.546135 140314852267776 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.264732, loss=3.761574
I0822 21:30:24.549793 140373593519936 submission.py:120] 5000) loss = 3.762, grad_norm = 0.265
I0822 21:33:51.094378 140314843875072 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.213503, loss=3.590529
I0822 21:33:51.097973 140373593519936 submission.py:120] 5500) loss = 3.591, grad_norm = 0.214
I0822 21:37:17.603284 140314852267776 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.245338, loss=3.589861
I0822 21:37:17.606980 140373593519936 submission.py:120] 6000) loss = 3.590, grad_norm = 0.245
I0822 21:37:56.405727 140373593519936 spec.py:320] Evaluating on the training split.
I0822 21:38:00.105595 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:40:35.988449 140373593519936 spec.py:332] Evaluating on the validation split.
I0822 21:40:39.589264 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:43:05.038627 140373593519936 spec.py:348] Evaluating on the test split.
I0822 21:43:08.705695 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 21:45:26.397607 140373593519936 submission_runner.py:365] Time since start: 5278.48s, 	Step: 6095, 	{'train/accuracy': 0.5622192684939041, 'train/loss': 2.491438405215877, 'train/bleu': 25.674626744719067, 'validation/accuracy': 0.5705570916665633, 'validation/loss': 2.424331959616124, 'validation/bleu': 22.270409336252882, 'validation/num_examples': 3000, 'test/accuracy': 0.5721341002847017, 'test/loss': 2.4282579091859855, 'test/bleu': 20.806326489531237, 'test/num_examples': 3003, 'score': 2576.997967004776, 'total_duration': 5278.484850645065, 'accumulated_submission_time': 2576.997967004776, 'accumulated_eval_time': 2696.502896308899, 'accumulated_logging_time': 2.513371229171753}
I0822 21:45:26.414218 140314843875072 logging_writer.py:48] [6095] accumulated_eval_time=2696.502896, accumulated_logging_time=2.513371, accumulated_submission_time=2576.997967, global_step=6095, preemption_count=0, score=2576.997967, test/accuracy=0.572134, test/bleu=20.806326, test/loss=2.428258, test/num_examples=3003, total_duration=5278.484851, train/accuracy=0.562219, train/bleu=25.674627, train/loss=2.491438, validation/accuracy=0.570557, validation/bleu=22.270409, validation/loss=2.424332, validation/num_examples=3000
I0822 21:45:27.245814 140373593519936 submission_runner.py:396] Released all unoccupied cached memory.
I0822 21:48:15.068525 140314852267776 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.264918, loss=3.513287
I0822 21:48:15.072188 140373593519936 submission.py:120] 6500) loss = 3.513, grad_norm = 0.265
I0822 21:51:41.610367 140314843875072 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.159351, loss=3.445179
I0822 21:51:41.614078 140373593519936 submission.py:120] 7000) loss = 3.445, grad_norm = 0.159
I0822 21:55:08.053689 140314852267776 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.184699, loss=3.308166
I0822 21:55:08.057490 140373593519936 submission.py:120] 7500) loss = 3.308, grad_norm = 0.185
I0822 21:58:34.619199 140314843875072 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.142262, loss=3.295540
I0822 21:58:34.622864 140373593519936 submission.py:120] 8000) loss = 3.296, grad_norm = 0.142
I0822 21:59:26.657980 140373593519936 spec.py:320] Evaluating on the training split.
I0822 21:59:30.336211 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 22:02:05.277083 140373593519936 spec.py:332] Evaluating on the validation split.
I0822 22:02:08.861191 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 22:04:24.906554 140373593519936 spec.py:348] Evaluating on the test split.
I0822 22:04:28.575263 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 22:06:42.242347 140373593519936 submission_runner.py:365] Time since start: 6554.33s, 	Step: 8127, 	{'train/accuracy': 0.5924991939569804, 'train/loss': 2.2208090081064897, 'train/bleu': 28.150173880736887, 'validation/accuracy': 0.6043570445499746, 'validation/loss': 2.1299480167635862, 'validation/bleu': 24.053906125857097, 'validation/num_examples': 3000, 'test/accuracy': 0.6094242054500029, 'test/loss': 2.104489316425542, 'test/bleu': 23.390670854784183, 'test/num_examples': 3003, 'score': 3415.5900571346283, 'total_duration': 6554.32959151268, 'accumulated_submission_time': 3415.5900571346283, 'accumulated_eval_time': 3132.0871634483337, 'accumulated_logging_time': 3.3678994178771973}
I0822 22:06:42.258309 140314852267776 logging_writer.py:48] [8127] accumulated_eval_time=3132.087163, accumulated_logging_time=3.367899, accumulated_submission_time=3415.590057, global_step=8127, preemption_count=0, score=3415.590057, test/accuracy=0.609424, test/bleu=23.390671, test/loss=2.104489, test/num_examples=3003, total_duration=6554.329592, train/accuracy=0.592499, train/bleu=28.150174, train/loss=2.220809, validation/accuracy=0.604357, validation/bleu=24.053906, validation/loss=2.129948, validation/num_examples=3000
I0822 22:06:43.067699 140373593519936 submission_runner.py:396] Released all unoccupied cached memory.
I0822 22:09:17.717887 140314843875072 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.137168, loss=3.288181
I0822 22:09:17.721697 140373593519936 submission.py:120] 8500) loss = 3.288, grad_norm = 0.137
I0822 22:12:44.382191 140314852267776 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.161215, loss=3.280070
I0822 22:12:44.386083 140373593519936 submission.py:120] 9000) loss = 3.280, grad_norm = 0.161
I0822 22:16:11.024751 140314843875072 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.130500, loss=3.208227
I0822 22:16:11.028819 140373593519936 submission.py:120] 9500) loss = 3.208, grad_norm = 0.131
I0822 22:19:37.196771 140373593519936 spec.py:320] Evaluating on the training split.
I0822 22:19:40.889414 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 22:22:00.857880 140373593519936 spec.py:332] Evaluating on the validation split.
I0822 22:22:04.466247 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 22:24:21.276008 140373593519936 spec.py:348] Evaluating on the test split.
I0822 22:24:24.944181 140373593519936 workload.py:141] Translating evaluation dataset.
I0822 22:26:38.025116 140373593519936 submission_runner.py:365] Time since start: 7750.11s, 	Step: 10000, 	{'train/accuracy': 0.6074355273177773, 'train/loss': 2.107541549219736, 'train/bleu': 29.271223299729353, 'validation/accuracy': 0.6228069087798044, 'validation/loss': 1.9948634781341832, 'validation/bleu': 25.70140711804895, 'validation/num_examples': 3000, 'test/accuracy': 0.6293417000755331, 'test/loss': 1.9526701673348439, 'test/bleu': 24.82913838832102, 'test/num_examples': 3003, 'score': 4188.988482713699, 'total_duration': 7750.112265825272, 'accumulated_submission_time': 4188.988482713699, 'accumulated_eval_time': 3552.915351867676, 'accumulated_logging_time': 4.195118188858032}
I0822 22:26:38.043313 140314852267776 logging_writer.py:48] [10000] accumulated_eval_time=3552.915352, accumulated_logging_time=4.195118, accumulated_submission_time=4188.988483, global_step=10000, preemption_count=0, score=4188.988483, test/accuracy=0.629342, test/bleu=24.829138, test/loss=1.952670, test/num_examples=3003, total_duration=7750.112266, train/accuracy=0.607436, train/bleu=29.271223, train/loss=2.107542, validation/accuracy=0.622807, validation/bleu=25.701407, validation/loss=1.994863, validation/num_examples=3000
I0822 22:26:38.819282 140373593519936 submission_runner.py:396] Released all unoccupied cached memory.
I0822 22:26:38.828539 140314843875072 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4188.988483
I0822 22:26:41.613851 140373593519936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_nightly_2023_08_20/adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0822 22:26:41.638914 140373593519936 submission_runner.py:534] Tuning trial 1/1
I0822 22:26:41.639171 140373593519936 submission_runner.py:535] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0822 22:26:41.640025 140373593519936 submission_runner.py:536] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006354826263589115, 'train/loss': 11.001906447879076, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.033981289754621, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.028322003369938, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 61.32317328453064, 'total_duration': 968.4293344020844, 'accumulated_submission_time': 61.32317328453064, 'accumulated_eval_time': 907.1056416034698, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2032, {'train/accuracy': 0.27990345790009497, 'train/loss': 4.996286732324445, 'train/bleu': 3.485640944901242, 'validation/accuracy': 0.24886238236351688, 'validation/loss': 5.274623454761875, 'validation/bleu': 1.2661157120669724, 'validation/num_examples': 3000, 'test/accuracy': 0.23113125326825867, 'test/loss': 5.546610815757364, 'test/bleu': 0.8796833409741861, 'test/num_examples': 3003, 'score': 899.8910636901855, 'total_duration': 2666.3379492759705, 'accumulated_submission_time': 899.8910636901855, 'accumulated_eval_time': 1764.860092163086, 'accumulated_logging_time': 0.7459840774536133, 'global_step': 2032, 'preemption_count': 0}), (4063, {'train/accuracy': 0.45795846964740095, 'train/loss': 3.356152095260814, 'train/bleu': 14.697666638775827, 'validation/accuracy': 0.4491946783052907, 'validation/loss': 3.4526106309903164, 'validation/bleu': 11.010690938157387, 'validation/num_examples': 3000, 'test/accuracy': 0.4386845621985939, 'test/loss': 3.5862783975364594, 'test/bleu': 9.523200444068582, 'test/num_examples': 3003, 'score': 1738.5285358428955, 'total_duration': 3988.355861186981, 'accumulated_submission_time': 1738.5285358428955, 'accumulated_eval_time': 2246.5111231803894, 'accumulated_logging_time': 1.653170108795166, 'global_step': 4063, 'preemption_count': 0}), (6095, {'train/accuracy': 0.5622192684939041, 'train/loss': 2.491438405215877, 'train/bleu': 25.674626744719067, 'validation/accuracy': 0.5705570916665633, 'validation/loss': 2.424331959616124, 'validation/bleu': 22.270409336252882, 'validation/num_examples': 3000, 'test/accuracy': 0.5721341002847017, 'test/loss': 2.4282579091859855, 'test/bleu': 20.806326489531237, 'test/num_examples': 3003, 'score': 2576.997967004776, 'total_duration': 5278.484850645065, 'accumulated_submission_time': 2576.997967004776, 'accumulated_eval_time': 2696.502896308899, 'accumulated_logging_time': 2.513371229171753, 'global_step': 6095, 'preemption_count': 0}), (8127, {'train/accuracy': 0.5924991939569804, 'train/loss': 2.2208090081064897, 'train/bleu': 28.150173880736887, 'validation/accuracy': 0.6043570445499746, 'validation/loss': 2.1299480167635862, 'validation/bleu': 24.053906125857097, 'validation/num_examples': 3000, 'test/accuracy': 0.6094242054500029, 'test/loss': 2.104489316425542, 'test/bleu': 23.390670854784183, 'test/num_examples': 3003, 'score': 3415.5900571346283, 'total_duration': 6554.32959151268, 'accumulated_submission_time': 3415.5900571346283, 'accumulated_eval_time': 3132.0871634483337, 'accumulated_logging_time': 3.3678994178771973, 'global_step': 8127, 'preemption_count': 0}), (10000, {'train/accuracy': 0.6074355273177773, 'train/loss': 2.107541549219736, 'train/bleu': 29.271223299729353, 'validation/accuracy': 0.6228069087798044, 'validation/loss': 1.9948634781341832, 'validation/bleu': 25.70140711804895, 'validation/num_examples': 3000, 'test/accuracy': 0.6293417000755331, 'test/loss': 1.9526701673348439, 'test/bleu': 24.82913838832102, 'test/num_examples': 3003, 'score': 4188.988482713699, 'total_duration': 7750.112265825272, 'accumulated_submission_time': 4188.988482713699, 'accumulated_eval_time': 3552.915351867676, 'accumulated_logging_time': 4.195118188858032, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0822 22:26:41.640141 140373593519936 submission_runner.py:537] Timing: 4188.988482713699
I0822 22:26:41.640197 140373593519936 submission_runner.py:539] Total number of evals: 6
I0822 22:26:41.640260 140373593519936 submission_runner.py:540] ====================
I0822 22:26:41.640380 140373593519936 submission_runner.py:608] Final wmt score: 4188.988482713699
