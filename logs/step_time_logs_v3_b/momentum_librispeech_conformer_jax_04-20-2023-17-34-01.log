I0420 17:34:21.561177 140442543011648 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax.
I0420 17:34:21.626562 140442543011648 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 17:34:22.455983 140442543011648 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0420 17:34:22.456613 140442543011648 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 17:34:22.460675 140442543011648 submission_runner.py:528] Using RNG seed 914029960
I0420 17:34:24.994560 140442543011648 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 17:34:24.994762 140442543011648 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1.
I0420 17:34:24.994962 140442543011648 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/hparams.json.
I0420 17:34:25.118752 140442543011648 submission_runner.py:232] Initializing dataset.
I0420 17:34:25.118950 140442543011648 submission_runner.py:239] Initializing model.
I0420 17:34:30.651942 140442543011648 submission_runner.py:249] Initializing optimizer.
I0420 17:34:31.312832 140442543011648 submission_runner.py:256] Initializing metrics bundle.
I0420 17:34:31.313016 140442543011648 submission_runner.py:273] Initializing checkpoint and logger.
I0420 17:34:31.313993 140442543011648 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0420 17:34:31.314231 140442543011648 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 17:34:31.314292 140442543011648 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 17:34:31.958078 140442543011648 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0420 17:34:31.958938 140442543011648 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/flags_0.json.
I0420 17:34:31.965212 140442543011648 submission_runner.py:309] Starting training loop.
I0420 17:34:32.156904 140442543011648 input_pipeline.py:20] Loading split = train-clean-100
I0420 17:34:32.186846 140442543011648 input_pipeline.py:20] Loading split = train-clean-360
I0420 17:34:32.477315 140442543011648 input_pipeline.py:20] Loading split = train-other-500
2023-04-20 17:35:31.392740: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-20 17:35:31.430794: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0420 17:35:33.156851 140265268438784 logging_writer.py:48] [0] global_step=0, grad_norm=44.794063568115234, loss=31.577852249145508
I0420 17:35:33.180867 140442543011648 spec.py:298] Evaluating on the training split.
I0420 17:35:33.280559 140442543011648 input_pipeline.py:20] Loading split = train-clean-100
I0420 17:35:33.305521 140442543011648 input_pipeline.py:20] Loading split = train-clean-360
I0420 17:35:33.572795 140442543011648 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0420 17:36:27.814149 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 17:36:27.874551 140442543011648 input_pipeline.py:20] Loading split = dev-clean
I0420 17:36:27.879559 140442543011648 input_pipeline.py:20] Loading split = dev-other
I0420 17:37:11.198007 140442543011648 spec.py:326] Evaluating on the test split.
I0420 17:37:11.258798 140442543011648 input_pipeline.py:20] Loading split = test-clean
I0420 17:37:40.923043 140442543011648 submission_runner.py:406] Time since start: 188.96s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.020954, dtype=float32), 'train/wer': 1.4414447244028903, 'validation/ctc_loss': DeviceArray(30.870548, dtype=float32), 'validation/wer': 1.3574757112948508, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.020004, dtype=float32), 'test/wer': 1.361749233237869, 'test/num_examples': 2472, 'score': 61.21548104286194, 'total_duration': 188.9565737247467, 'accumulated_submission_time': 61.21548104286194, 'accumulated_eval_time': 127.7409417629242, 'accumulated_logging_time': 0}
I0420 17:37:40.943161 140263666218752 logging_writer.py:48] [1] accumulated_eval_time=127.740942, accumulated_logging_time=0, accumulated_submission_time=61.215481, global_step=1, preemption_count=0, score=61.215481, test/ctc_loss=31.020004272460938, test/num_examples=2472, test/wer=1.361749, total_duration=188.956574, train/ctc_loss=32.02095413208008, train/wer=1.441445, validation/ctc_loss=30.870548248291016, validation/num_examples=5348, validation/wer=1.357476
I0420 17:37:41.109396 140442543011648 checkpoints.py:356] Saving checkpoint at step: 1
I0420 17:37:41.646673 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_1
I0420 17:37:41.647633 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_1.
I0420 17:39:09.473880 140269203830528 logging_writer.py:48] [100] global_step=100, grad_norm=0.8670235872268677, loss=5.938551902770996
I0420 17:40:25.439613 140269212223232 logging_writer.py:48] [200] global_step=200, grad_norm=33.897560119628906, loss=18.691181182861328
I0420 17:41:40.291955 140269203830528 logging_writer.py:48] [300] global_step=300, grad_norm=0.0, loss=1859.6336669921875
I0420 17:42:53.804354 140269212223232 logging_writer.py:48] [400] global_step=400, grad_norm=0.0, loss=1829.858154296875
I0420 17:44:07.890896 140269203830528 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1895.011962890625
I0420 17:45:25.361362 140269212223232 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1813.7818603515625
I0420 17:46:48.155635 140269203830528 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1745.5145263671875
I0420 17:48:10.721962 140269212223232 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1771.4840087890625
I0420 17:49:34.329331 140269203830528 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1858.28369140625
I0420 17:50:56.324242 140269212223232 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1810.0625
I0420 17:52:13.423765 140264991610624 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1863.8306884765625
I0420 17:53:26.929585 140264874178304 logging_writer.py:48] [1200] global_step=1200, grad_norm=nan, loss=nan
I0420 17:54:39.015143 140264991610624 logging_writer.py:48] [1300] global_step=1300, grad_norm=nan, loss=nan
I0420 17:55:53.466738 140264874178304 logging_writer.py:48] [1400] global_step=1400, grad_norm=nan, loss=nan
I0420 17:57:05.767822 140264991610624 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0420 17:58:30.334099 140264874178304 logging_writer.py:48] [1600] global_step=1600, grad_norm=nan, loss=nan
I0420 17:59:55.673489 140264991610624 logging_writer.py:48] [1700] global_step=1700, grad_norm=nan, loss=nan
I0420 18:01:14.537959 140264874178304 logging_writer.py:48] [1800] global_step=1800, grad_norm=nan, loss=nan
I0420 18:02:40.428419 140264991610624 logging_writer.py:48] [1900] global_step=1900, grad_norm=nan, loss=nan
I0420 18:04:05.382889 140264874178304 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0420 18:05:27.609270 140269354899200 logging_writer.py:48] [2100] global_step=2100, grad_norm=nan, loss=nan
I0420 18:06:39.677139 140269346506496 logging_writer.py:48] [2200] global_step=2200, grad_norm=nan, loss=nan
I0420 18:07:52.443664 140269354899200 logging_writer.py:48] [2300] global_step=2300, grad_norm=nan, loss=nan
I0420 18:09:05.408104 140269346506496 logging_writer.py:48] [2400] global_step=2400, grad_norm=nan, loss=nan
I0420 18:10:18.345540 140269354899200 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0420 18:11:36.420687 140269346506496 logging_writer.py:48] [2600] global_step=2600, grad_norm=nan, loss=nan
I0420 18:12:58.489161 140269354899200 logging_writer.py:48] [2700] global_step=2700, grad_norm=nan, loss=nan
I0420 18:14:18.760476 140269346506496 logging_writer.py:48] [2800] global_step=2800, grad_norm=nan, loss=nan
I0420 18:15:36.548206 140269354899200 logging_writer.py:48] [2900] global_step=2900, grad_norm=nan, loss=nan
I0420 18:17:00.047562 140269346506496 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0420 18:17:41.672834 140442543011648 spec.py:298] Evaluating on the training split.
I0420 18:18:07.943349 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 18:18:42.318444 140442543011648 spec.py:326] Evaluating on the test split.
I0420 18:18:59.082884 140442543011648 submission_runner.py:406] Time since start: 2667.11s, 	Step: 3057, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2461.201361179352, 'total_duration': 2667.1144416332245, 'accumulated_submission_time': 2461.201361179352, 'accumulated_eval_time': 205.14784145355225, 'accumulated_logging_time': 0.7300925254821777}
I0420 18:18:59.101582 140269354899200 logging_writer.py:48] [3057] accumulated_eval_time=205.147841, accumulated_logging_time=0.730093, accumulated_submission_time=2461.201361, global_step=3057, preemption_count=0, score=2461.201361, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2667.114442, train/ctc_loss=nan, train/wer=0.944636, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 18:18:59.277482 140442543011648 checkpoints.py:356] Saving checkpoint at step: 3057
I0420 18:19:00.154573 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_3057
I0420 18:19:00.172761 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_3057.
I0420 18:19:35.346125 140269027219200 logging_writer.py:48] [3100] global_step=3100, grad_norm=nan, loss=nan
I0420 18:20:48.189497 140269018826496 logging_writer.py:48] [3200] global_step=3200, grad_norm=nan, loss=nan
I0420 18:22:01.050458 140269027219200 logging_writer.py:48] [3300] global_step=3300, grad_norm=nan, loss=nan
I0420 18:23:13.444653 140269018826496 logging_writer.py:48] [3400] global_step=3400, grad_norm=nan, loss=nan
I0420 18:24:27.993358 140269027219200 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0420 18:25:42.628262 140269018826496 logging_writer.py:48] [3600] global_step=3600, grad_norm=nan, loss=nan
I0420 18:26:59.667225 140269027219200 logging_writer.py:48] [3700] global_step=3700, grad_norm=nan, loss=nan
I0420 18:28:22.372554 140269018826496 logging_writer.py:48] [3800] global_step=3800, grad_norm=nan, loss=nan
I0420 18:29:42.858113 140269027219200 logging_writer.py:48] [3900] global_step=3900, grad_norm=nan, loss=nan
I0420 18:31:02.286718 140269018826496 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0420 18:32:23.491029 140269027219200 logging_writer.py:48] [4100] global_step=4100, grad_norm=nan, loss=nan
I0420 18:33:38.864142 140269027219200 logging_writer.py:48] [4200] global_step=4200, grad_norm=nan, loss=nan
I0420 18:34:51.836809 140269018826496 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0420 18:36:05.865664 140269027219200 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0420 18:37:26.685621 140269018826496 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0420 18:38:47.491984 140269027219200 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0420 18:40:10.773300 140269018826496 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0420 18:41:35.792670 140269027219200 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0420 18:42:56.995570 140269018826496 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0420 18:44:13.849667 140269027219200 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0420 18:45:32.939437 140269018826496 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0420 18:46:55.029251 140269027219200 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0420 18:48:07.692166 140269018826496 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0420 18:49:20.506252 140269027219200 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0420 18:50:39.821095 140269018826496 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0420 18:52:00.401557 140269027219200 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0420 18:53:25.492468 140269018826496 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0420 18:54:50.964806 140269027219200 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0420 18:56:17.536764 140269018826496 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0420 18:57:37.609689 140269027219200 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0420 18:59:00.343235 140442543011648 spec.py:298] Evaluating on the training split.
I0420 18:59:26.394339 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 19:00:02.356812 140442543011648 spec.py:326] Evaluating on the test split.
I0420 19:00:20.968261 140442543011648 submission_runner.py:406] Time since start: 5149.00s, 	Step: 6100, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4861.330677270889, 'total_duration': 5148.999841451645, 'accumulated_submission_time': 4861.330677270889, 'accumulated_eval_time': 285.76971220970154, 'accumulated_logging_time': 1.8268394470214844}
I0420 19:00:20.987751 140268735379200 logging_writer.py:48] [6100] accumulated_eval_time=285.769712, accumulated_logging_time=1.826839, accumulated_submission_time=4861.330677, global_step=6100, preemption_count=0, score=4861.330677, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5148.999841, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 19:00:21.167203 140442543011648 checkpoints.py:356] Saving checkpoint at step: 6100
I0420 19:00:21.993570 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_6100
I0420 19:00:22.011773 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_6100.
I0420 19:00:22.810797 140268726986496 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0420 19:01:39.767612 140268735379200 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0420 19:02:53.937554 140268726986496 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0420 19:04:08.206175 140268735379200 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0420 19:05:20.278117 140268726986496 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0420 19:06:42.179556 140268735379200 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0420 19:07:59.138459 140268726986496 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0420 19:09:14.644273 140268735379200 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0420 19:10:39.298440 140268726986496 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0420 19:11:54.745078 140268735379200 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0420 19:13:16.760184 140268726986496 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0420 19:14:38.477244 140268735379200 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0420 19:15:54.721124 140268735379200 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0420 19:17:07.468758 140268726986496 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0420 19:18:20.139428 140268735379200 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0420 19:19:41.739418 140268726986496 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0420 19:20:59.504697 140268735379200 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0420 19:22:23.139870 140268726986496 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0420 19:23:44.592859 140268735379200 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0420 19:25:04.955600 140268726986496 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0420 19:26:22.785479 140268735379200 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0420 19:27:47.588435 140268726986496 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0420 19:29:08.446375 140268735379200 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0420 19:30:21.589288 140268726986496 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0420 19:31:38.226789 140268735379200 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0420 19:32:58.173218 140268726986496 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0420 19:34:16.652167 140268735379200 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0420 19:35:38.036674 140268726986496 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0420 19:36:57.801514 140268735379200 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0420 19:38:19.458020 140268726986496 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0420 19:39:37.455533 140268735379200 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0420 19:40:22.228886 140442543011648 spec.py:298] Evaluating on the training split.
I0420 19:40:49.249661 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 19:41:24.827234 140442543011648 spec.py:326] Evaluating on the test split.
I0420 19:41:44.663474 140442543011648 submission_runner.py:406] Time since start: 7632.70s, 	Step: 9154, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7261.506939172745, 'total_duration': 7632.695374250412, 'accumulated_submission_time': 7261.506939172745, 'accumulated_eval_time': 368.20146894454956, 'accumulated_logging_time': 2.8776209354400635}
I0420 19:41:44.682897 140268812179200 logging_writer.py:48] [9154] accumulated_eval_time=368.201469, accumulated_logging_time=2.877621, accumulated_submission_time=7261.506939, global_step=9154, preemption_count=0, score=7261.506939, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7632.695374, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 19:41:44.858977 140442543011648 checkpoints.py:356] Saving checkpoint at step: 9154
I0420 19:41:45.744622 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_9154
I0420 19:41:45.762879 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_9154.
I0420 19:42:19.709564 140268803786496 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0420 19:43:35.227877 140268812179200 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0420 19:44:47.906708 140268803786496 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0420 19:46:00.339752 140268812179200 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0420 19:47:12.895106 140268803786496 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0420 19:48:31.631857 140268812179200 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0420 19:49:50.594316 140268803786496 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0420 19:51:10.980247 140268812179200 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0420 19:52:31.842933 140268803786496 logging_writer.py:48] [10000] global_step=10000, grad_norm=nan, loss=nan
I0420 19:53:53.507570 140268812179200 logging_writer.py:48] [10100] global_step=10100, grad_norm=nan, loss=nan
I0420 19:55:17.212892 140268803786496 logging_writer.py:48] [10200] global_step=10200, grad_norm=nan, loss=nan
I0420 19:56:37.865871 140270010259200 logging_writer.py:48] [10300] global_step=10300, grad_norm=nan, loss=nan
I0420 19:57:49.978816 140270001866496 logging_writer.py:48] [10400] global_step=10400, grad_norm=nan, loss=nan
I0420 19:59:02.071900 140270010259200 logging_writer.py:48] [10500] global_step=10500, grad_norm=nan, loss=nan
I0420 20:00:18.101077 140270001866496 logging_writer.py:48] [10600] global_step=10600, grad_norm=nan, loss=nan
I0420 20:01:34.445866 140270010259200 logging_writer.py:48] [10700] global_step=10700, grad_norm=nan, loss=nan
I0420 20:02:54.507450 140270001866496 logging_writer.py:48] [10800] global_step=10800, grad_norm=nan, loss=nan
I0420 20:04:20.965595 140270010259200 logging_writer.py:48] [10900] global_step=10900, grad_norm=nan, loss=nan
I0420 20:05:46.049216 140270001866496 logging_writer.py:48] [11000] global_step=11000, grad_norm=nan, loss=nan
I0420 20:07:13.005819 140270010259200 logging_writer.py:48] [11100] global_step=11100, grad_norm=nan, loss=nan
I0420 20:08:33.005457 140270001866496 logging_writer.py:48] [11200] global_step=11200, grad_norm=nan, loss=nan
I0420 20:09:57.253990 140270010259200 logging_writer.py:48] [11300] global_step=11300, grad_norm=nan, loss=nan
I0420 20:11:16.615854 140270010259200 logging_writer.py:48] [11400] global_step=11400, grad_norm=nan, loss=nan
I0420 20:12:28.710952 140270001866496 logging_writer.py:48] [11500] global_step=11500, grad_norm=nan, loss=nan
I0420 20:13:41.957512 140270010259200 logging_writer.py:48] [11600] global_step=11600, grad_norm=nan, loss=nan
I0420 20:14:54.860886 140270001866496 logging_writer.py:48] [11700] global_step=11700, grad_norm=nan, loss=nan
I0420 20:16:15.533648 140270010259200 logging_writer.py:48] [11800] global_step=11800, grad_norm=nan, loss=nan
I0420 20:17:39.182891 140270001866496 logging_writer.py:48] [11900] global_step=11900, grad_norm=nan, loss=nan
I0420 20:18:58.254827 140270010259200 logging_writer.py:48] [12000] global_step=12000, grad_norm=nan, loss=nan
I0420 20:20:17.952203 140270001866496 logging_writer.py:48] [12100] global_step=12100, grad_norm=nan, loss=nan
I0420 20:21:37.315846 140270010259200 logging_writer.py:48] [12200] global_step=12200, grad_norm=nan, loss=nan
I0420 20:21:46.240295 140442543011648 spec.py:298] Evaluating on the training split.
I0420 20:22:13.621593 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 20:22:50.017515 140442543011648 spec.py:326] Evaluating on the test split.
I0420 20:23:08.130287 140442543011648 submission_runner.py:406] Time since start: 10116.16s, 	Step: 12213, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9661.942535877228, 'total_duration': 10116.162129163742, 'accumulated_submission_time': 9661.942535877228, 'accumulated_eval_time': 450.0885646343231, 'accumulated_logging_time': 3.984492301940918}
I0420 20:23:08.149750 140270010259200 logging_writer.py:48] [12213] accumulated_eval_time=450.088565, accumulated_logging_time=3.984492, accumulated_submission_time=9661.942536, global_step=12213, preemption_count=0, score=9661.942536, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=10116.162129, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 20:23:08.329430 140442543011648 checkpoints.py:356] Saving checkpoint at step: 12213
I0420 20:23:09.185937 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_12213
I0420 20:23:09.204236 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_12213.
I0420 20:24:13.275071 140270001866496 logging_writer.py:48] [12300] global_step=12300, grad_norm=nan, loss=nan
I0420 20:25:29.530275 140270010259200 logging_writer.py:48] [12400] global_step=12400, grad_norm=nan, loss=nan
I0420 20:26:42.018901 140270001866496 logging_writer.py:48] [12500] global_step=12500, grad_norm=nan, loss=nan
I0420 20:27:54.807022 140270010259200 logging_writer.py:48] [12600] global_step=12600, grad_norm=nan, loss=nan
I0420 20:29:07.648078 140270001866496 logging_writer.py:48] [12700] global_step=12700, grad_norm=nan, loss=nan
I0420 20:30:21.464368 140270010259200 logging_writer.py:48] [12800] global_step=12800, grad_norm=nan, loss=nan
I0420 20:31:48.469336 140270001866496 logging_writer.py:48] [12900] global_step=12900, grad_norm=nan, loss=nan
I0420 20:33:13.808754 140270010259200 logging_writer.py:48] [13000] global_step=13000, grad_norm=nan, loss=nan
I0420 20:34:36.350839 140270001866496 logging_writer.py:48] [13100] global_step=13100, grad_norm=nan, loss=nan
I0420 20:35:56.693601 140270010259200 logging_writer.py:48] [13200] global_step=13200, grad_norm=nan, loss=nan
I0420 20:37:20.247285 140270001866496 logging_writer.py:48] [13300] global_step=13300, grad_norm=nan, loss=nan
I0420 20:38:45.213020 140270010259200 logging_writer.py:48] [13400] global_step=13400, grad_norm=nan, loss=nan
I0420 20:39:57.453743 140270001866496 logging_writer.py:48] [13500] global_step=13500, grad_norm=nan, loss=nan
I0420 20:41:10.267477 140270010259200 logging_writer.py:48] [13600] global_step=13600, grad_norm=nan, loss=nan
I0420 20:42:28.075580 140270001866496 logging_writer.py:48] [13700] global_step=13700, grad_norm=nan, loss=nan
I0420 20:43:50.752751 140270010259200 logging_writer.py:48] [13800] global_step=13800, grad_norm=nan, loss=nan
I0420 20:45:14.250441 140270001866496 logging_writer.py:48] [13900] global_step=13900, grad_norm=nan, loss=nan
I0420 20:46:36.843899 140270010259200 logging_writer.py:48] [14000] global_step=14000, grad_norm=nan, loss=nan
I0420 20:48:02.534173 140270001866496 logging_writer.py:48] [14100] global_step=14100, grad_norm=nan, loss=nan
I0420 20:49:26.308563 140270010259200 logging_writer.py:48] [14200] global_step=14200, grad_norm=nan, loss=nan
I0420 20:50:44.188287 140270001866496 logging_writer.py:48] [14300] global_step=14300, grad_norm=nan, loss=nan
I0420 20:52:02.938480 140270010259200 logging_writer.py:48] [14400] global_step=14400, grad_norm=nan, loss=nan
I0420 20:53:19.211242 140270010259200 logging_writer.py:48] [14500] global_step=14500, grad_norm=nan, loss=nan
I0420 20:54:32.111634 140270001866496 logging_writer.py:48] [14600] global_step=14600, grad_norm=nan, loss=nan
I0420 20:55:44.926704 140270010259200 logging_writer.py:48] [14700] global_step=14700, grad_norm=nan, loss=nan
I0420 20:56:57.717012 140270001866496 logging_writer.py:48] [14800] global_step=14800, grad_norm=nan, loss=nan
I0420 20:58:10.763174 140270010259200 logging_writer.py:48] [14900] global_step=14900, grad_norm=nan, loss=nan
I0420 20:59:28.044370 140270001866496 logging_writer.py:48] [15000] global_step=15000, grad_norm=nan, loss=nan
I0420 21:00:46.028473 140270010259200 logging_writer.py:48] [15100] global_step=15100, grad_norm=nan, loss=nan
I0420 21:02:05.848201 140270001866496 logging_writer.py:48] [15200] global_step=15200, grad_norm=nan, loss=nan
I0420 21:03:09.406339 140442543011648 spec.py:298] Evaluating on the training split.
I0420 21:03:36.717004 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 21:04:13.219309 140442543011648 spec.py:326] Evaluating on the test split.
I0420 21:04:30.718983 140442543011648 submission_runner.py:406] Time since start: 12598.75s, 	Step: 15276, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12062.104198217392, 'total_duration': 12598.750331640244, 'accumulated_submission_time': 12062.104198217392, 'accumulated_eval_time': 531.3978159427643, 'accumulated_logging_time': 5.065887451171875}
I0420 21:04:30.743220 140270010259200 logging_writer.py:48] [15276] accumulated_eval_time=531.397816, accumulated_logging_time=5.065887, accumulated_submission_time=12062.104198, global_step=15276, preemption_count=0, score=12062.104198, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=12598.750332, train/ctc_loss=nan, train/wer=0.941551, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 21:04:30.961820 140442543011648 checkpoints.py:356] Saving checkpoint at step: 15276
I0420 21:04:32.111582 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_15276
I0420 21:04:32.135297 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_15276.
I0420 21:04:50.248572 140270001866496 logging_writer.py:48] [15300] global_step=15300, grad_norm=nan, loss=nan
I0420 21:06:02.907938 140269091772160 logging_writer.py:48] [15400] global_step=15400, grad_norm=nan, loss=nan
I0420 21:07:19.122094 140270010259200 logging_writer.py:48] [15500] global_step=15500, grad_norm=nan, loss=nan
I0420 21:08:31.241724 140270001866496 logging_writer.py:48] [15600] global_step=15600, grad_norm=nan, loss=nan
I0420 21:09:43.459036 140270010259200 logging_writer.py:48] [15700] global_step=15700, grad_norm=nan, loss=nan
I0420 21:10:58.518556 140270001866496 logging_writer.py:48] [15800] global_step=15800, grad_norm=nan, loss=nan
I0420 21:12:21.821322 140270010259200 logging_writer.py:48] [15900] global_step=15900, grad_norm=nan, loss=nan
I0420 21:13:45.470499 140270001866496 logging_writer.py:48] [16000] global_step=16000, grad_norm=nan, loss=nan
I0420 21:15:08.231541 140270010259200 logging_writer.py:48] [16100] global_step=16100, grad_norm=nan, loss=nan
I0420 21:16:33.981414 140270001866496 logging_writer.py:48] [16200] global_step=16200, grad_norm=nan, loss=nan
I0420 21:17:53.012789 140270010259200 logging_writer.py:48] [16300] global_step=16300, grad_norm=nan, loss=nan
I0420 21:19:14.782248 140270001866496 logging_writer.py:48] [16400] global_step=16400, grad_norm=nan, loss=nan
I0420 21:20:37.106318 140270010259200 logging_writer.py:48] [16500] global_step=16500, grad_norm=nan, loss=nan
I0420 21:21:49.564017 140270001866496 logging_writer.py:48] [16600] global_step=16600, grad_norm=nan, loss=nan
I0420 21:23:03.694240 140270010259200 logging_writer.py:48] [16700] global_step=16700, grad_norm=nan, loss=nan
I0420 21:24:20.629101 140270001866496 logging_writer.py:48] [16800] global_step=16800, grad_norm=nan, loss=nan
I0420 21:25:41.624956 140270010259200 logging_writer.py:48] [16900] global_step=16900, grad_norm=nan, loss=nan
I0420 21:26:58.125848 140270001866496 logging_writer.py:48] [17000] global_step=17000, grad_norm=nan, loss=nan
I0420 21:28:22.824506 140270010259200 logging_writer.py:48] [17100] global_step=17100, grad_norm=nan, loss=nan
I0420 21:29:50.200810 140270001866496 logging_writer.py:48] [17200] global_step=17200, grad_norm=nan, loss=nan
I0420 21:31:16.874145 140270010259200 logging_writer.py:48] [17300] global_step=17300, grad_norm=nan, loss=nan
I0420 21:32:43.756001 140270001866496 logging_writer.py:48] [17400] global_step=17400, grad_norm=nan, loss=nan
I0420 21:34:03.819196 140270010259200 logging_writer.py:48] [17500] global_step=17500, grad_norm=nan, loss=nan
I0420 21:35:21.420246 140270010259200 logging_writer.py:48] [17600] global_step=17600, grad_norm=nan, loss=nan
I0420 21:36:34.660795 140270001866496 logging_writer.py:48] [17700] global_step=17700, grad_norm=nan, loss=nan
I0420 21:37:49.268842 140270010259200 logging_writer.py:48] [17800] global_step=17800, grad_norm=nan, loss=nan
I0420 21:39:08.864019 140270001866496 logging_writer.py:48] [17900] global_step=17900, grad_norm=nan, loss=nan
I0420 21:40:34.388656 140270010259200 logging_writer.py:48] [18000] global_step=18000, grad_norm=nan, loss=nan
I0420 21:41:52.215647 140270001866496 logging_writer.py:48] [18100] global_step=18100, grad_norm=nan, loss=nan
I0420 21:43:13.065165 140270010259200 logging_writer.py:48] [18200] global_step=18200, grad_norm=nan, loss=nan
I0420 21:44:32.725134 140442543011648 spec.py:298] Evaluating on the training split.
I0420 21:45:00.075832 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 21:45:36.033309 140442543011648 spec.py:326] Evaluating on the test split.
I0420 21:45:54.824858 140442543011648 submission_runner.py:406] Time since start: 15082.86s, 	Step: 18298, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14462.648379802704, 'total_duration': 15082.856575250626, 'accumulated_submission_time': 14462.648379802704, 'accumulated_eval_time': 613.4945187568665, 'accumulated_logging_time': 6.494167804718018}
I0420 21:45:54.848652 140270010259200 logging_writer.py:48] [18298] accumulated_eval_time=613.494519, accumulated_logging_time=6.494168, accumulated_submission_time=14462.648380, global_step=18298, preemption_count=0, score=14462.648380, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=15082.856575, train/ctc_loss=nan, train/wer=0.942641, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 21:45:55.032345 140442543011648 checkpoints.py:356] Saving checkpoint at step: 18298
I0420 21:45:55.956495 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_18298
I0420 21:45:55.974592 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_18298.
I0420 21:45:58.240593 140270001866496 logging_writer.py:48] [18300] global_step=18300, grad_norm=nan, loss=nan
I0420 21:47:10.740392 140268755699456 logging_writer.py:48] [18400] global_step=18400, grad_norm=nan, loss=nan
I0420 21:48:22.792445 140270001866496 logging_writer.py:48] [18500] global_step=18500, grad_norm=nan, loss=nan
I0420 21:49:39.046020 140270010259200 logging_writer.py:48] [18600] global_step=18600, grad_norm=nan, loss=nan
I0420 21:50:51.520995 140270001866496 logging_writer.py:48] [18700] global_step=18700, grad_norm=nan, loss=nan
I0420 21:52:08.017347 140270010259200 logging_writer.py:48] [18800] global_step=18800, grad_norm=nan, loss=nan
I0420 21:53:23.435757 140270001866496 logging_writer.py:48] [18900] global_step=18900, grad_norm=nan, loss=nan
I0420 21:54:47.805014 140270010259200 logging_writer.py:48] [19000] global_step=19000, grad_norm=nan, loss=nan
I0420 21:56:14.552611 140270001866496 logging_writer.py:48] [19100] global_step=19100, grad_norm=nan, loss=nan
I0420 21:57:33.368479 140270010259200 logging_writer.py:48] [19200] global_step=19200, grad_norm=nan, loss=nan
I0420 21:58:55.438262 140270001866496 logging_writer.py:48] [19300] global_step=19300, grad_norm=nan, loss=nan
I0420 22:00:19.001743 140270010259200 logging_writer.py:48] [19400] global_step=19400, grad_norm=nan, loss=nan
I0420 22:01:43.749710 140270001866496 logging_writer.py:48] [19500] global_step=19500, grad_norm=nan, loss=nan
I0420 22:03:10.646132 140270010259200 logging_writer.py:48] [19600] global_step=19600, grad_norm=nan, loss=nan
I0420 22:04:23.532138 140270001866496 logging_writer.py:48] [19700] global_step=19700, grad_norm=nan, loss=nan
I0420 22:05:36.283113 140270010259200 logging_writer.py:48] [19800] global_step=19800, grad_norm=nan, loss=nan
I0420 22:06:49.143683 140270001866496 logging_writer.py:48] [19900] global_step=19900, grad_norm=nan, loss=nan
I0420 22:08:10.124807 140442543011648 spec.py:298] Evaluating on the training split.
I0420 22:08:37.314958 140442543011648 spec.py:310] Evaluating on the validation split.
I0420 22:09:11.840401 140442543011648 spec.py:326] Evaluating on the test split.
I0420 22:09:30.466669 140442543011648 submission_runner.py:406] Time since start: 16498.50s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15796.770457267761, 'total_duration': 16498.499616861343, 'accumulated_submission_time': 15796.770457267761, 'accumulated_eval_time': 693.8345944881439, 'accumulated_logging_time': 7.653445243835449}
I0420 22:09:30.487498 140269580179200 logging_writer.py:48] [20000] accumulated_eval_time=693.834594, accumulated_logging_time=7.653445, accumulated_submission_time=15796.770457, global_step=20000, preemption_count=0, score=15796.770457, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=16498.499617, train/ctc_loss=nan, train/wer=0.942824, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0420 22:09:30.673871 140442543011648 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 22:09:31.557192 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000
I0420 22:09:31.575495 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0420 22:09:31.591312 140269571786496 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15796.770457
I0420 22:09:31.752525 140442543011648 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 22:09:32.848911 140442543011648 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000
I0420 22:09:32.867191 140442543011648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0420 22:09:33.989127 140442543011648 submission_runner.py:567] Tuning trial 1/1
I0420 22:09:33.989369 140442543011648 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0420 22:09:33.994399 140442543011648 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.020954, dtype=float32), 'train/wer': 1.4414447244028903, 'validation/ctc_loss': DeviceArray(30.870548, dtype=float32), 'validation/wer': 1.3574757112948508, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.020004, dtype=float32), 'test/wer': 1.361749233237869, 'test/num_examples': 2472, 'score': 61.21548104286194, 'total_duration': 188.9565737247467, 'accumulated_submission_time': 61.21548104286194, 'accumulated_eval_time': 127.7409417629242, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3057, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2461.201361179352, 'total_duration': 2667.1144416332245, 'accumulated_submission_time': 2461.201361179352, 'accumulated_eval_time': 205.14784145355225, 'accumulated_logging_time': 0.7300925254821777, 'global_step': 3057, 'preemption_count': 0}), (6100, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4861.330677270889, 'total_duration': 5148.999841451645, 'accumulated_submission_time': 4861.330677270889, 'accumulated_eval_time': 285.76971220970154, 'accumulated_logging_time': 1.8268394470214844, 'global_step': 6100, 'preemption_count': 0}), (9154, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7261.506939172745, 'total_duration': 7632.695374250412, 'accumulated_submission_time': 7261.506939172745, 'accumulated_eval_time': 368.20146894454956, 'accumulated_logging_time': 2.8776209354400635, 'global_step': 9154, 'preemption_count': 0}), (12213, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9661.942535877228, 'total_duration': 10116.162129163742, 'accumulated_submission_time': 9661.942535877228, 'accumulated_eval_time': 450.0885646343231, 'accumulated_logging_time': 3.984492301940918, 'global_step': 12213, 'preemption_count': 0}), (15276, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12062.104198217392, 'total_duration': 12598.750331640244, 'accumulated_submission_time': 12062.104198217392, 'accumulated_eval_time': 531.3978159427643, 'accumulated_logging_time': 5.065887451171875, 'global_step': 15276, 'preemption_count': 0}), (18298, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14462.648379802704, 'total_duration': 15082.856575250626, 'accumulated_submission_time': 14462.648379802704, 'accumulated_eval_time': 613.4945187568665, 'accumulated_logging_time': 6.494167804718018, 'global_step': 18298, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15796.770457267761, 'total_duration': 16498.499616861343, 'accumulated_submission_time': 15796.770457267761, 'accumulated_eval_time': 693.8345944881439, 'accumulated_logging_time': 7.653445243835449, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0420 22:09:33.994592 140442543011648 submission_runner.py:570] Timing: 15796.770457267761
I0420 22:09:33.994671 140442543011648 submission_runner.py:571] ====================
I0420 22:09:33.995212 140442543011648 submission_runner.py:631] Final librispeech_conformer score: 15796.770457267761
