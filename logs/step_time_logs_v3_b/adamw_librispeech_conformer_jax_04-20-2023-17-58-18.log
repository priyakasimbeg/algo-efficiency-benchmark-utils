I0420 17:58:38.642331 140126101780288 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax.
I0420 17:58:38.707648 140126101780288 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 17:58:39.505532 140126101780288 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0420 17:58:39.506172 140126101780288 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 17:58:39.510217 140126101780288 submission_runner.py:528] Using RNG seed 2912809369
I0420 17:58:42.167554 140126101780288 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 17:58:42.167816 140126101780288 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1.
I0420 17:58:42.168068 140126101780288 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/hparams.json.
I0420 17:58:42.298741 140126101780288 submission_runner.py:232] Initializing dataset.
I0420 17:58:42.299001 140126101780288 submission_runner.py:239] Initializing model.
I0420 17:58:48.573338 140126101780288 submission_runner.py:249] Initializing optimizer.
I0420 17:58:49.388803 140126101780288 submission_runner.py:256] Initializing metrics bundle.
I0420 17:58:49.389065 140126101780288 submission_runner.py:273] Initializing checkpoint and logger.
I0420 17:58:49.390301 140126101780288 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0420 17:58:49.390684 140126101780288 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 17:58:49.390763 140126101780288 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 17:58:50.235364 140126101780288 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0420 17:58:50.236386 140126101780288 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/flags_0.json.
I0420 17:58:50.243238 140126101780288 submission_runner.py:309] Starting training loop.
I0420 17:58:50.447854 140126101780288 input_pipeline.py:20] Loading split = train-clean-100
I0420 17:58:50.483160 140126101780288 input_pipeline.py:20] Loading split = train-clean-360
I0420 17:58:50.882461 140126101780288 input_pipeline.py:20] Loading split = train-other-500
2023-04-20 17:59:53.505458: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-20 17:59:53.535856: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0420 17:59:55.507636 139949856777984 logging_writer.py:48] [0] global_step=0, grad_norm=36.14596939086914, loss=32.02219772338867
I0420 17:59:55.538599 140126101780288 spec.py:298] Evaluating on the training split.
I0420 17:59:55.636451 140126101780288 input_pipeline.py:20] Loading split = train-clean-100
I0420 17:59:55.662503 140126101780288 input_pipeline.py:20] Loading split = train-clean-360
I0420 17:59:55.918700 140126101780288 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0420 18:00:51.548542 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 18:00:51.608461 140126101780288 input_pipeline.py:20] Loading split = dev-clean
I0420 18:00:51.612641 140126101780288 input_pipeline.py:20] Loading split = dev-other
I0420 18:01:32.851827 140126101780288 spec.py:326] Evaluating on the test split.
I0420 18:01:32.910501 140126101780288 input_pipeline.py:20] Loading split = test-clean
I0420 18:02:02.646051 140126101780288 submission_runner.py:406] Time since start: 192.40s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.492025, dtype=float32), 'train/wer': 1.4972148762051516, 'validation/ctc_loss': DeviceArray(30.649307, dtype=float32), 'validation/wer': 1.0822390954085424, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.738192, dtype=float32), 'test/wer': 1.1321471370828509, 'test/num_examples': 2472, 'score': 65.29518342018127, 'total_duration': 192.4014823436737, 'accumulated_submission_time': 65.29518342018127, 'accumulated_eval_time': 127.10613012313843, 'accumulated_logging_time': 0}
I0420 18:02:02.669022 139946761377536 logging_writer.py:48] [1] accumulated_eval_time=127.106130, accumulated_logging_time=0, accumulated_submission_time=65.295183, global_step=1, preemption_count=0, score=65.295183, test/ctc_loss=30.738191604614258, test/num_examples=2472, test/wer=1.132147, total_duration=192.401482, train/ctc_loss=31.49202537536621, train/wer=1.497215, validation/ctc_loss=30.649307250976562, validation/num_examples=5348, validation/wer=1.082239
I0420 18:02:02.935449 140126101780288 checkpoints.py:356] Saving checkpoint at step: 1
I0420 18:02:03.728879 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_1
I0420 18:02:03.730033 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_1.
I0420 18:03:35.874977 139951751644928 logging_writer.py:48] [100] global_step=100, grad_norm=1.9857810735702515, loss=6.346652030944824
I0420 18:04:52.437660 139951760037632 logging_writer.py:48] [200] global_step=200, grad_norm=1.560339093208313, loss=5.8507466316223145
I0420 18:06:09.122910 139951751644928 logging_writer.py:48] [300] global_step=300, grad_norm=1.4673070907592773, loss=5.825323581695557
I0420 18:07:25.506819 139951760037632 logging_writer.py:48] [400] global_step=400, grad_norm=1.0058350563049316, loss=5.813530921936035
I0420 18:08:41.869374 139951751644928 logging_writer.py:48] [500] global_step=500, grad_norm=3.049083948135376, loss=5.784374713897705
I0420 18:09:58.352611 139951760037632 logging_writer.py:48] [600] global_step=600, grad_norm=0.6161346435546875, loss=5.775665283203125
I0420 18:11:15.356799 139951751644928 logging_writer.py:48] [700] global_step=700, grad_norm=0.6871655583381653, loss=5.782374382019043
I0420 18:12:32.163723 139951760037632 logging_writer.py:48] [800] global_step=800, grad_norm=0.5801939368247986, loss=5.801845073699951
I0420 18:13:48.726031 139951751644928 logging_writer.py:48] [900] global_step=900, grad_norm=1.0214849710464478, loss=5.769526958465576
I0420 18:15:08.931843 139951760037632 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0188820362091064, loss=5.656209945678711
I0420 18:16:28.127581 139953860400896 logging_writer.py:48] [1100] global_step=1100, grad_norm=5.059133052825928, loss=5.618041515350342
I0420 18:17:44.351384 139953852008192 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2810722589492798, loss=5.475882530212402
I0420 18:19:00.476130 139953860400896 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.997940719127655, loss=4.823318958282471
I0420 18:20:16.676914 139953852008192 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.6425660848617554, loss=4.239048004150391
I0420 18:21:33.324719 139953860400896 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.6668730974197388, loss=3.824397087097168
I0420 18:22:49.515004 139953852008192 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8369305729866028, loss=3.600003242492676
I0420 18:24:06.043008 139953860400896 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.937832772731781, loss=3.3765110969543457
I0420 18:25:22.123013 139953852008192 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.016329288482666, loss=3.2677671909332275
I0420 18:26:38.166747 139953860400896 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9615703821182251, loss=3.1601853370666504
I0420 18:27:54.244421 139953852008192 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.5996489524841309, loss=3.01192045211792
I0420 18:29:13.689584 139952549680896 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9342783689498901, loss=2.949467658996582
I0420 18:30:29.682114 139952541288192 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.7536380290985107, loss=2.8953936100006104
I0420 18:31:45.635964 139952549680896 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9017800092697144, loss=2.839587688446045
I0420 18:33:01.597964 139952541288192 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0662940740585327, loss=2.760672092437744
I0420 18:34:17.595647 139952549680896 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9302290678024292, loss=2.7165558338165283
I0420 18:35:33.426594 139952541288192 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.3000379800796509, loss=2.647273540496826
I0420 18:36:49.328414 139952549680896 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.119725227355957, loss=2.6273770332336426
I0420 18:38:05.399303 139952541288192 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9387955665588379, loss=2.5414161682128906
I0420 18:39:21.487804 139952549680896 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7685185670852661, loss=2.431657075881958
I0420 18:40:37.427026 139952541288192 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8736453056335449, loss=2.4555931091308594
I0420 18:41:56.042258 139952549680896 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7914010882377625, loss=2.4164280891418457
I0420 18:42:04.064944 140126101780288 spec.py:298] Evaluating on the training split.
I0420 18:42:40.160110 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 18:43:18.384604 140126101780288 spec.py:326] Evaluating on the test split.
I0420 18:43:36.759521 140126101780288 submission_runner.py:406] Time since start: 2686.51s, 	Step: 3112, 	{'train/ctc_loss': DeviceArray(2.2769244, dtype=float32), 'train/wer': 0.5094694239047599, 'validation/ctc_loss': DeviceArray(2.6861687, dtype=float32), 'validation/wer': 0.5539657883819429, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.3674603, dtype=float32), 'test/wer': 0.5131314362317958, 'test/num_examples': 2472, 'score': 2465.595070838928, 'total_duration': 2686.513051509857, 'accumulated_submission_time': 2465.595070838928, 'accumulated_eval_time': 219.79750776290894, 'accumulated_logging_time': 1.0860676765441895}
I0420 18:43:36.777978 139953276720896 logging_writer.py:48] [3112] accumulated_eval_time=219.797508, accumulated_logging_time=1.086068, accumulated_submission_time=2465.595071, global_step=3112, preemption_count=0, score=2465.595071, test/ctc_loss=2.367460250854492, test/num_examples=2472, test/wer=0.513131, total_duration=2686.513052, train/ctc_loss=2.2769243717193604, train/wer=0.509469, validation/ctc_loss=2.686168670654297, validation/num_examples=5348, validation/wer=0.553966
I0420 18:43:37.057868 140126101780288 checkpoints.py:356] Saving checkpoint at step: 3112
I0420 18:43:38.353566 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_3112
I0420 18:43:38.376471 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_3112.
I0420 18:44:45.814761 139953268328192 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.011910080909729, loss=2.3760268688201904
I0420 18:46:01.733045 139953226364672 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8131176829338074, loss=2.341761589050293
I0420 18:47:17.682307 139953268328192 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9381418824195862, loss=2.341020107269287
I0420 18:48:33.632447 139953226364672 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8739580512046814, loss=2.249523878097534
I0420 18:49:49.386878 139953268328192 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9556592702865601, loss=2.2543179988861084
I0420 18:51:05.169160 139953226364672 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7784319519996643, loss=2.20662260055542
I0420 18:52:20.911508 139953268328192 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8268436193466187, loss=2.1449296474456787
I0420 18:53:36.690044 139953226364672 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.1861417293548584, loss=2.1969001293182373
I0420 18:54:52.855264 139953268328192 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1630645990371704, loss=2.175736904144287
I0420 18:56:15.326958 139953226364672 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7775524258613586, loss=2.10160756111145
I0420 18:57:34.728888 139951966000896 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9671969413757324, loss=2.0801889896392822
I0420 18:58:50.618208 139951957608192 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.0507702827453613, loss=2.062516927719116
I0420 19:00:06.407804 139951966000896 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6954763531684875, loss=2.0126760005950928
I0420 19:01:22.089351 139951957608192 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6515053510665894, loss=1.9625442028045654
I0420 19:02:37.945353 139951966000896 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9337266683578491, loss=2.023221492767334
I0420 19:03:54.015162 139951957608192 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6517234444618225, loss=1.9389281272888184
I0420 19:05:09.940646 139951966000896 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7853249907493591, loss=1.9491446018218994
I0420 19:06:25.542626 139951957608192 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6607178449630737, loss=1.938220500946045
I0420 19:07:41.221510 139951966000896 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.752864420413971, loss=1.9093427658081055
I0420 19:08:56.756923 139951957608192 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5434218645095825, loss=1.8957653045654297
I0420 19:10:15.361204 139953276720896 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8621317744255066, loss=1.8606462478637695
I0420 19:11:30.882738 139953268328192 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7621456980705261, loss=1.8708888292312622
I0420 19:12:46.470978 139953276720896 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7866858243942261, loss=1.9037176370620728
I0420 19:14:02.124400 139953268328192 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6004408001899719, loss=1.8870031833648682
I0420 19:15:17.753137 139953276720896 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6636971831321716, loss=1.8538897037506104
I0420 19:16:33.525855 139953268328192 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.0302857160568237, loss=1.8644670248031616
I0420 19:17:49.078295 139953276720896 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6787122488021851, loss=1.8522652387619019
I0420 19:19:04.689625 139953268328192 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6228504180908203, loss=1.9061319828033447
I0420 19:20:20.155009 139953276720896 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5267152190208435, loss=1.771201729774475
I0420 19:21:35.537472 139953268328192 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7044117450714111, loss=1.8078304529190063
I0420 19:22:54.303323 139951966000896 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6145115494728088, loss=1.7963868379592896
I0420 19:23:38.398579 140126101780288 spec.py:298] Evaluating on the training split.
I0420 19:24:14.978304 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 19:24:51.944541 140126101780288 spec.py:326] Evaluating on the test split.
I0420 19:25:10.915024 140126101780288 submission_runner.py:406] Time since start: 5180.67s, 	Step: 6260, 	{'train/ctc_loss': DeviceArray(0.5434191, dtype=float32), 'train/wer': 0.18594145379964308, 'validation/ctc_loss': DeviceArray(0.87846535, dtype=float32), 'validation/wer': 0.25604685042788644, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59064984, dtype=float32), 'test/wer': 0.1929600064997055, 'test/num_examples': 2472, 'score': 4865.572159051895, 'total_duration': 5180.668483257294, 'accumulated_submission_time': 4865.572159051895, 'accumulated_eval_time': 312.3106791973114, 'accumulated_logging_time': 2.715363025665283}
I0420 19:25:10.933701 139951423276800 logging_writer.py:48] [6260] accumulated_eval_time=312.310679, accumulated_logging_time=2.715363, accumulated_submission_time=4865.572159, global_step=6260, preemption_count=0, score=4865.572159, test/ctc_loss=0.5906498432159424, test/num_examples=2472, test/wer=0.192960, total_duration=5180.668483, train/ctc_loss=0.5434191226959229, train/wer=0.185941, validation/ctc_loss=0.8784653544425964, validation/num_examples=5348, validation/wer=0.256047
I0420 19:25:11.199099 140126101780288 checkpoints.py:356] Saving checkpoint at step: 6260
I0420 19:25:12.524653 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_6260
I0420 19:25:12.550763 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_6260.
I0420 19:25:43.520328 139951414884096 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6292977929115295, loss=1.7687922716140747
I0420 19:26:58.930015 139951364527872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8366546630859375, loss=1.777278184890747
I0420 19:28:14.337401 139951414884096 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6786743998527527, loss=1.7530118227005005
I0420 19:29:29.966686 139951364527872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6693475842475891, loss=1.7709962129592896
I0420 19:30:45.510177 139951414884096 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5492285490036011, loss=1.7410964965820312
I0420 19:32:01.003939 139951364527872 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6804770231246948, loss=1.8011834621429443
I0420 19:33:16.495326 139951414884096 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8594623804092407, loss=1.7000573873519897
I0420 19:34:32.118346 139951364527872 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6211638450622559, loss=1.7324414253234863
I0420 19:35:52.223320 139951414884096 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5285779237747192, loss=1.7058125734329224
I0420 19:37:13.421172 139951364527872 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.47580665349960327, loss=1.6690419912338257
I0420 19:38:32.206689 139951423276800 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8774622678756714, loss=1.6868573427200317
I0420 19:39:47.435927 139951414884096 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7935356497764587, loss=1.7839175462722778
I0420 19:41:02.683411 139951423276800 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5828433036804199, loss=1.7574913501739502
I0420 19:42:18.107029 139951414884096 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4432166516780853, loss=1.695395827293396
I0420 19:43:33.361978 139951423276800 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.47117167711257935, loss=1.7145161628723145
I0420 19:44:48.712356 139951414884096 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5103321671485901, loss=1.663727045059204
I0420 19:46:04.420459 139951423276800 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.47683507204055786, loss=1.6988837718963623
I0420 19:47:20.230220 139951414884096 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5650578141212463, loss=1.695473313331604
I0420 19:48:35.675941 139951423276800 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5793245434761047, loss=1.5704131126403809
I0420 19:49:51.575128 139951414884096 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6134675741195679, loss=1.7065603733062744
I0420 19:51:09.884898 139951423276800 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5134586095809937, loss=1.621171236038208
I0420 19:52:25.290629 139951414884096 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4397214353084564, loss=1.6000280380249023
I0420 19:53:40.623237 139951423276800 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.45445960760116577, loss=1.6582140922546387
I0420 19:54:56.307988 139951414884096 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6025823354721069, loss=1.6439828872680664
I0420 19:56:11.631238 139951423276800 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4477623999118805, loss=1.587766408920288
I0420 19:57:26.880332 139951414884096 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.43542155623435974, loss=1.5321394205093384
I0420 19:58:42.120213 139951423276800 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5445258021354675, loss=1.5845168828964233
I0420 19:59:57.348152 139951414884096 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.45148372650146484, loss=1.6017627716064453
I0420 20:01:12.828396 139951423276800 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.45955711603164673, loss=1.5879759788513184
I0420 20:02:30.594770 139951414884096 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6044507622718811, loss=1.598129391670227
I0420 20:03:49.757530 139953276720896 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5039804577827454, loss=1.5283101797103882
I0420 20:05:05.113703 139953268328192 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4546799063682556, loss=1.528416633605957
I0420 20:05:13.065949 140126101780288 spec.py:298] Evaluating on the training split.
I0420 20:05:49.906877 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 20:06:27.126420 140126101780288 spec.py:326] Evaluating on the test split.
I0420 20:06:46.181418 140126101780288 submission_runner.py:406] Time since start: 7675.93s, 	Step: 9412, 	{'train/ctc_loss': DeviceArray(0.35719657, dtype=float32), 'train/wer': 0.13311299819198677, 'validation/ctc_loss': DeviceArray(0.69747907, dtype=float32), 'validation/wer': 0.20919642254146206, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44192302, dtype=float32), 'test/wer': 0.1497166534641399, 'test/num_examples': 2472, 'score': 7266.045335531235, 'total_duration': 7675.9344391822815, 'accumulated_submission_time': 7266.045335531235, 'accumulated_eval_time': 405.42244362831116, 'accumulated_logging_time': 4.361053705215454}
I0420 20:06:46.203146 139953860400896 logging_writer.py:48] [9412] accumulated_eval_time=405.422444, accumulated_logging_time=4.361054, accumulated_submission_time=7266.045336, global_step=9412, preemption_count=0, score=7266.045336, test/ctc_loss=0.44192302227020264, test/num_examples=2472, test/wer=0.149717, total_duration=7675.934439, train/ctc_loss=0.357196569442749, train/wer=0.133113, validation/ctc_loss=0.6974790692329407, validation/num_examples=5348, validation/wer=0.209196
I0420 20:06:46.457401 140126101780288 checkpoints.py:356] Saving checkpoint at step: 9412
I0420 20:06:47.759310 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_9412
I0420 20:06:47.786601 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_9412.
I0420 20:07:54.865196 139953852008192 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4737584888935089, loss=1.5279344320297241
I0420 20:09:10.190517 139953793259264 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5818862318992615, loss=1.5198395252227783
I0420 20:10:25.416071 139953852008192 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5443726778030396, loss=1.5535541772842407
I0420 20:11:40.743448 139953793259264 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.38490015268325806, loss=1.5715628862380981
I0420 20:12:56.008050 139953852008192 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5749099254608154, loss=1.5626143217086792
I0420 20:14:11.201800 139953793259264 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4346143901348114, loss=1.5131449699401855
I0420 20:15:29.250337 139953852008192 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.4359089732170105, loss=1.5249115228652954
I0420 20:16:51.230655 139953793259264 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.43436962366104126, loss=1.5626510381698608
I0420 20:18:14.887117 139953317680896 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5264759063720703, loss=1.497016429901123
I0420 20:19:30.284425 139953309288192 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.49278414249420166, loss=1.6111862659454346
I0420 20:20:45.901125 139953317680896 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5800195336341858, loss=1.5081911087036133
I0420 20:22:01.751994 139953309288192 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5196093916893005, loss=1.505698323249817
I0420 20:23:17.667846 139953317680896 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.37257105112075806, loss=1.5216954946517944
I0420 20:24:33.062541 139953309288192 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3568323850631714, loss=1.4644509553909302
I0420 20:25:48.258493 139953317680896 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.4138692319393158, loss=1.5534099340438843
I0420 20:27:03.530522 139953309288192 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.4161830544471741, loss=1.4850139617919922
I0420 20:28:24.196156 139953317680896 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.35108956694602966, loss=1.4842609167099
I0420 20:29:45.049974 139953309288192 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.40504276752471924, loss=1.5233629941940308
I0420 20:31:06.370546 139953317680896 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4563688337802887, loss=1.49863600730896
I0420 20:32:27.438959 139953860400896 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.4187272787094116, loss=1.4911339282989502
I0420 20:33:43.013348 139953852008192 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.46706411242485046, loss=1.4615554809570312
I0420 20:34:58.644636 139953860400896 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5955253839492798, loss=1.502382516860962
I0420 20:36:13.839024 139953852008192 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.73866868019104, loss=1.5232563018798828
I0420 20:37:29.351462 139953860400896 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.45853549242019653, loss=1.5024455785751343
I0420 20:38:44.638867 139953852008192 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.40792402625083923, loss=1.5148565769195557
I0420 20:40:00.210977 139953860400896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.42553696036338806, loss=1.5058320760726929
I0420 20:41:16.876395 139953852008192 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3640044331550598, loss=1.4417287111282349
I0420 20:42:37.242485 139953860400896 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.42412644624710083, loss=1.4819926023483276
I0420 20:43:55.271135 139953852008192 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6888009905815125, loss=1.4498368501663208
I0420 20:45:16.225112 139953317680896 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3273134231567383, loss=1.4420671463012695
I0420 20:46:31.605319 139953309288192 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3867965638637543, loss=1.4292429685592651
I0420 20:46:47.801424 140126101780288 spec.py:298] Evaluating on the training split.
I0420 20:47:25.068265 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 20:48:02.788809 140126101780288 spec.py:326] Evaluating on the test split.
I0420 20:48:22.284402 140126101780288 submission_runner.py:406] Time since start: 10172.04s, 	Step: 12523, 	{'train/ctc_loss': DeviceArray(0.3082053, dtype=float32), 'train/wer': 0.11479856318528836, 'validation/ctc_loss': DeviceArray(0.6235066, dtype=float32), 'validation/wer': 0.18523092359791218, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38520232, dtype=float32), 'test/wer': 0.1291613348770134, 'test/num_examples': 2472, 'score': 9666.019686937332, 'total_duration': 10172.037778615952, 'accumulated_submission_time': 9666.019686937332, 'accumulated_eval_time': 499.90205335617065, 'accumulated_logging_time': 5.973283767700195}
I0420 20:48:22.304923 139952518960896 logging_writer.py:48] [12523] accumulated_eval_time=499.902053, accumulated_logging_time=5.973284, accumulated_submission_time=9666.019687, global_step=12523, preemption_count=0, score=9666.019687, test/ctc_loss=0.3852023184299469, test/num_examples=2472, test/wer=0.129161, total_duration=10172.037779, train/ctc_loss=0.3082053065299988, train/wer=0.114799, validation/ctc_loss=0.6235066056251526, validation/num_examples=5348, validation/wer=0.185231
I0420 20:48:22.558844 140126101780288 checkpoints.py:356] Saving checkpoint at step: 12523
I0420 20:48:24.210993 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_12523
I0420 20:48:24.246106 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_12523.
I0420 20:49:22.849762 139952510568192 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.43760448694229126, loss=1.4243212938308716
I0420 20:50:38.123342 139952443426560 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3927524983882904, loss=1.4455996751785278
I0420 20:51:53.489027 139952510568192 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4394012689590454, loss=1.4750218391418457
I0420 20:53:08.722631 139952443426560 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.4486115574836731, loss=1.4862279891967773
I0420 20:54:24.202191 139952510568192 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.38689687848091125, loss=1.412562608718872
I0420 20:55:39.497326 139952443426560 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4821397364139557, loss=1.4628381729125977
I0420 20:56:54.905276 139952510568192 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4440944790840149, loss=1.429690957069397
I0420 20:58:11.845806 139952443426560 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.48039573431015015, loss=1.4818000793457031
I0420 20:59:35.555334 139952518960896 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.36011460423469543, loss=1.4037970304489136
I0420 21:00:50.933216 139952510568192 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5377477407455444, loss=1.4026823043823242
I0420 21:02:06.156361 139952518960896 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4239538609981537, loss=1.4093773365020752
I0420 21:03:21.683830 139952510568192 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.4680425822734833, loss=1.4204859733581543
I0420 21:04:36.975599 139952518960896 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.47913500666618347, loss=1.468241810798645
I0420 21:05:52.298233 139952510568192 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5377029776573181, loss=1.3730807304382324
I0420 21:07:07.635023 139952518960896 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.43599674105644226, loss=1.4224568605422974
I0420 21:08:23.413022 139952510568192 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2929244339466095, loss=1.4231886863708496
I0420 21:09:38.791731 139952518960896 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.33983880281448364, loss=1.3541533946990967
I0420 21:10:54.436718 139952510568192 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.33589911460876465, loss=1.2956092357635498
I0420 21:12:17.831179 139952518960896 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3489755094051361, loss=1.427817702293396
I0420 21:13:37.124358 139952518960896 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.36218640208244324, loss=1.4327601194381714
I0420 21:14:52.819764 139952510568192 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3496401011943817, loss=1.3973183631896973
I0420 21:16:08.234971 139952518960896 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3756332993507385, loss=1.3808236122131348
I0420 21:17:23.560405 139952510568192 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3570738434791565, loss=1.3571828603744507
I0420 21:18:38.873384 139952518960896 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.454142689704895, loss=1.386950135231018
I0420 21:19:54.081537 139952510568192 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3439342975616455, loss=1.3688324689865112
I0420 21:21:09.466441 139952518960896 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3795808255672455, loss=1.3639625310897827
I0420 21:22:25.074333 139952510568192 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.49303585290908813, loss=1.409218668937683
I0420 21:23:42.940745 139952518960896 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3678853213787079, loss=1.4133070707321167
I0420 21:25:03.284128 139952510568192 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.36542749404907227, loss=1.3782840967178345
I0420 21:26:23.465860 139952518960896 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.367294579744339, loss=1.4028455018997192
I0420 21:27:38.641367 139952510568192 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.39035287499427795, loss=1.3904317617416382
I0420 21:28:24.854884 140126101780288 spec.py:298] Evaluating on the training split.
I0420 21:29:01.434365 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 21:29:39.986757 140126101780288 spec.py:326] Evaluating on the test split.
I0420 21:29:59.187661 140126101780288 submission_runner.py:406] Time since start: 12668.94s, 	Step: 15663, 	{'train/ctc_loss': DeviceArray(0.25956455, dtype=float32), 'train/wer': 0.09692535171454043, 'validation/ctc_loss': DeviceArray(0.5569345, dtype=float32), 'validation/wer': 0.16854962421248637, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.33750063, dtype=float32), 'test/wer': 0.11177462271240834, 'test/num_examples': 2472, 'score': 12066.58435869217, 'total_duration': 12668.941144227982, 'accumulated_submission_time': 12066.58435869217, 'accumulated_eval_time': 594.2315948009491, 'accumulated_logging_time': 7.945818662643433}
I0420 21:29:59.207801 139952518960896 logging_writer.py:48] [15663] accumulated_eval_time=594.231595, accumulated_logging_time=7.945819, accumulated_submission_time=12066.584359, global_step=15663, preemption_count=0, score=12066.584359, test/ctc_loss=0.3375006318092346, test/num_examples=2472, test/wer=0.111775, total_duration=12668.941144, train/ctc_loss=0.2595645487308502, train/wer=0.096925, validation/ctc_loss=0.5569344758987427, validation/num_examples=5348, validation/wer=0.168550
I0420 21:29:59.471003 140126101780288 checkpoints.py:356] Saving checkpoint at step: 15663
I0420 21:30:00.795238 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_15663
I0420 21:30:00.822312 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_15663.
I0420 21:30:29.435497 139952510568192 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.37135738134384155, loss=1.3062688112258911
I0420 21:31:44.597188 139952107353856 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2981235384941101, loss=1.389784574508667
I0420 21:32:59.864225 139952510568192 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3324107825756073, loss=1.340339183807373
I0420 21:34:15.211203 139952107353856 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.32873037457466125, loss=1.3048083782196045
I0420 21:35:30.502923 139952510568192 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.40443554520606995, loss=1.3272173404693604
I0420 21:36:47.295957 139952107353856 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.38011643290519714, loss=1.2771496772766113
I0420 21:38:09.886838 139952510568192 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.4067979156970978, loss=1.343517541885376
I0420 21:39:33.192989 139952107353856 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.34905391931533813, loss=1.3846253156661987
I0420 21:40:53.359194 139952518960896 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.4079465866088867, loss=1.3725529909133911
I0420 21:42:08.622472 139952510568192 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.4027039408683777, loss=1.399696707725525
I0420 21:43:23.991345 139952518960896 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3668147623538971, loss=1.3359909057617188
I0420 21:44:39.555974 139952510568192 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.33893799781799316, loss=1.3013780117034912
I0420 21:45:54.899659 139952518960896 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.4321550130844116, loss=1.322609782218933
I0420 21:47:10.200809 139952510568192 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3506646156311035, loss=1.432266116142273
I0420 21:48:25.460525 139952518960896 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.4340672492980957, loss=1.3144562244415283
I0420 21:49:40.887140 139952510568192 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.41922077536582947, loss=1.3737484216690063
I0420 21:50:56.674740 139952518960896 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4243921637535095, loss=1.3911724090576172
I0420 21:52:12.420612 139952510568192 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.40550896525382996, loss=1.382027506828308
I0420 21:53:29.751983 139952518960896 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.4530460238456726, loss=1.3688666820526123
I0420 21:54:48.519981 139952518960896 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.33974480628967285, loss=1.2967947721481323
I0420 21:56:03.981496 139952510568192 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3802725076675415, loss=1.3546948432922363
I0420 21:57:19.281507 139952518960896 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.44646018743515015, loss=1.3404123783111572
I0420 21:58:34.679028 139952510568192 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3471916913986206, loss=1.3196247816085815
I0420 21:59:50.010600 139952518960896 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3387940526008606, loss=1.3417876958847046
I0420 22:01:05.274515 139952510568192 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.33032599091529846, loss=1.3483309745788574
I0420 22:02:20.640905 139952518960896 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3099791407585144, loss=1.307196021080017
I0420 22:03:39.588138 139952510568192 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4998426139354706, loss=1.313046932220459
I0420 22:05:02.430485 139952518960896 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3966197967529297, loss=1.243041753768921
I0420 22:06:21.449731 139952510568192 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3059508502483368, loss=1.2722315788269043
I0420 22:07:39.879140 139952518960896 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.2770397663116455, loss=1.2636501789093018
I0420 22:08:55.271825 139952510568192 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3616461753845215, loss=1.302668571472168
I0420 22:10:01.002391 140126101780288 spec.py:298] Evaluating on the training split.
I0420 22:10:37.867821 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 22:11:15.723568 140126101780288 spec.py:326] Evaluating on the test split.
I0420 22:11:36.324099 140126101780288 submission_runner.py:406] Time since start: 15166.08s, 	Step: 18789, 	{'train/ctc_loss': DeviceArray(0.25302166, dtype=float32), 'train/wer': 0.09207206149873834, 'validation/ctc_loss': DeviceArray(0.5270991, dtype=float32), 'validation/wer': 0.15748342965199857, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31017742, dtype=float32), 'test/wer': 0.10566083724331242, 'test/num_examples': 2472, 'score': 14466.722465753555, 'total_duration': 15166.077390909195, 'accumulated_submission_time': 14466.722465753555, 'accumulated_eval_time': 689.549859046936, 'accumulated_logging_time': 9.590147972106934}
I0420 22:11:36.347366 139953102640896 logging_writer.py:48] [18789] accumulated_eval_time=689.549859, accumulated_logging_time=9.590148, accumulated_submission_time=14466.722466, global_step=18789, preemption_count=0, score=14466.722466, test/ctc_loss=0.31017741560935974, test/num_examples=2472, test/wer=0.105661, total_duration=15166.077391, train/ctc_loss=0.2530216574668884, train/wer=0.092072, validation/ctc_loss=0.527099072933197, validation/num_examples=5348, validation/wer=0.157483
I0420 22:11:36.605729 140126101780288 checkpoints.py:356] Saving checkpoint at step: 18789
I0420 22:11:37.944478 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_18789
I0420 22:11:37.971540 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_18789.
I0420 22:11:47.062882 139953094248192 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3749016225337982, loss=1.3544138669967651
I0420 22:13:02.253315 139953010321152 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3301543593406677, loss=1.2876535654067993
I0420 22:14:17.405387 139953094248192 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3370915353298187, loss=1.2811570167541504
I0420 22:15:32.649856 139953010321152 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.31946438550949097, loss=1.2737246751785278
I0420 22:16:48.584547 139953094248192 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.40274518728256226, loss=1.3051204681396484
I0420 22:18:06.830295 139953010321152 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5127620100975037, loss=1.284878134727478
I0420 22:19:30.410685 139953094248192 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.2461363822221756, loss=1.3109586238861084
I0420 22:20:50.336920 139953010321152 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.34378311038017273, loss=1.2893450260162354
I0420 22:22:12.872536 139952774960896 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.42442071437835693, loss=1.2724287509918213
I0420 22:23:28.262299 139952766568192 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4186364412307739, loss=1.2421576976776123
I0420 22:24:43.863378 139952774960896 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4014820456504822, loss=1.2978808879852295
I0420 22:25:59.162668 139952766568192 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.42550379037857056, loss=1.3407832384109497
I0420 22:27:13.275163 140126101780288 spec.py:298] Evaluating on the training split.
I0420 22:27:49.648289 140126101780288 spec.py:310] Evaluating on the validation split.
I0420 22:28:28.678723 140126101780288 spec.py:326] Evaluating on the test split.
I0420 22:28:48.331002 140126101780288 submission_runner.py:406] Time since start: 16198.08s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.22151864, dtype=float32), 'train/wer': 0.08533278029481207, 'validation/ctc_loss': DeviceArray(0.51295966, dtype=float32), 'validation/wer': 0.15445397447153372, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3065624, dtype=float32), 'test/wer': 0.10218755712631772, 'test/num_examples': 2472, 'score': 15402.003957509995, 'total_duration': 16198.084981918335, 'accumulated_submission_time': 15402.003957509995, 'accumulated_eval_time': 784.6029562950134, 'accumulated_logging_time': 11.246610164642334}
I0420 22:28:48.351832 139953138480896 logging_writer.py:48] [20000] accumulated_eval_time=784.602956, accumulated_logging_time=11.246610, accumulated_submission_time=15402.003958, global_step=20000, preemption_count=0, score=15402.003958, test/ctc_loss=0.3065623939037323, test/num_examples=2472, test/wer=0.102188, total_duration=16198.084982, train/ctc_loss=0.2215186357498169, train/wer=0.085333, validation/ctc_loss=0.5129596590995789, validation/num_examples=5348, validation/wer=0.154454
I0420 22:28:48.601762 140126101780288 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 22:28:49.875950 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0420 22:28:49.902880 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0420 22:28:49.917684 139953130088192 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15402.003958
I0420 22:28:50.098083 140126101780288 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 22:28:51.791111 140126101780288 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000
I0420 22:28:51.818024 140126101780288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0420 22:28:53.096155 140126101780288 submission_runner.py:567] Tuning trial 1/1
I0420 22:28:53.096385 140126101780288 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 22:28:53.102118 140126101780288 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.492025, dtype=float32), 'train/wer': 1.4972148762051516, 'validation/ctc_loss': DeviceArray(30.649307, dtype=float32), 'validation/wer': 1.0822390954085424, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.738192, dtype=float32), 'test/wer': 1.1321471370828509, 'test/num_examples': 2472, 'score': 65.29518342018127, 'total_duration': 192.4014823436737, 'accumulated_submission_time': 65.29518342018127, 'accumulated_eval_time': 127.10613012313843, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3112, {'train/ctc_loss': DeviceArray(2.2769244, dtype=float32), 'train/wer': 0.5094694239047599, 'validation/ctc_loss': DeviceArray(2.6861687, dtype=float32), 'validation/wer': 0.5539657883819429, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(2.3674603, dtype=float32), 'test/wer': 0.5131314362317958, 'test/num_examples': 2472, 'score': 2465.595070838928, 'total_duration': 2686.513051509857, 'accumulated_submission_time': 2465.595070838928, 'accumulated_eval_time': 219.79750776290894, 'accumulated_logging_time': 1.0860676765441895, 'global_step': 3112, 'preemption_count': 0}), (6260, {'train/ctc_loss': DeviceArray(0.5434191, dtype=float32), 'train/wer': 0.18594145379964308, 'validation/ctc_loss': DeviceArray(0.87846535, dtype=float32), 'validation/wer': 0.25604685042788644, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.59064984, dtype=float32), 'test/wer': 0.1929600064997055, 'test/num_examples': 2472, 'score': 4865.572159051895, 'total_duration': 5180.668483257294, 'accumulated_submission_time': 4865.572159051895, 'accumulated_eval_time': 312.3106791973114, 'accumulated_logging_time': 2.715363025665283, 'global_step': 6260, 'preemption_count': 0}), (9412, {'train/ctc_loss': DeviceArray(0.35719657, dtype=float32), 'train/wer': 0.13311299819198677, 'validation/ctc_loss': DeviceArray(0.69747907, dtype=float32), 'validation/wer': 0.20919642254146206, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.44192302, dtype=float32), 'test/wer': 0.1497166534641399, 'test/num_examples': 2472, 'score': 7266.045335531235, 'total_duration': 7675.9344391822815, 'accumulated_submission_time': 7266.045335531235, 'accumulated_eval_time': 405.42244362831116, 'accumulated_logging_time': 4.361053705215454, 'global_step': 9412, 'preemption_count': 0}), (12523, {'train/ctc_loss': DeviceArray(0.3082053, dtype=float32), 'train/wer': 0.11479856318528836, 'validation/ctc_loss': DeviceArray(0.6235066, dtype=float32), 'validation/wer': 0.18523092359791218, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.38520232, dtype=float32), 'test/wer': 0.1291613348770134, 'test/num_examples': 2472, 'score': 9666.019686937332, 'total_duration': 10172.037778615952, 'accumulated_submission_time': 9666.019686937332, 'accumulated_eval_time': 499.90205335617065, 'accumulated_logging_time': 5.973283767700195, 'global_step': 12523, 'preemption_count': 0}), (15663, {'train/ctc_loss': DeviceArray(0.25956455, dtype=float32), 'train/wer': 0.09692535171454043, 'validation/ctc_loss': DeviceArray(0.5569345, dtype=float32), 'validation/wer': 0.16854962421248637, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.33750063, dtype=float32), 'test/wer': 0.11177462271240834, 'test/num_examples': 2472, 'score': 12066.58435869217, 'total_duration': 12668.941144227982, 'accumulated_submission_time': 12066.58435869217, 'accumulated_eval_time': 594.2315948009491, 'accumulated_logging_time': 7.945818662643433, 'global_step': 15663, 'preemption_count': 0}), (18789, {'train/ctc_loss': DeviceArray(0.25302166, dtype=float32), 'train/wer': 0.09207206149873834, 'validation/ctc_loss': DeviceArray(0.5270991, dtype=float32), 'validation/wer': 0.15748342965199857, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.31017742, dtype=float32), 'test/wer': 0.10566083724331242, 'test/num_examples': 2472, 'score': 14466.722465753555, 'total_duration': 15166.077390909195, 'accumulated_submission_time': 14466.722465753555, 'accumulated_eval_time': 689.549859046936, 'accumulated_logging_time': 9.590147972106934, 'global_step': 18789, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.22151864, dtype=float32), 'train/wer': 0.08533278029481207, 'validation/ctc_loss': DeviceArray(0.51295966, dtype=float32), 'validation/wer': 0.15445397447153372, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.3065624, dtype=float32), 'test/wer': 0.10218755712631772, 'test/num_examples': 2472, 'score': 15402.003957509995, 'total_duration': 16198.084981918335, 'accumulated_submission_time': 15402.003957509995, 'accumulated_eval_time': 784.6029562950134, 'accumulated_logging_time': 11.246610164642334, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0420 22:28:53.102290 140126101780288 submission_runner.py:570] Timing: 15402.003957509995
I0420 22:28:53.102339 140126101780288 submission_runner.py:571] ====================
I0420 22:28:53.102890 140126101780288 submission_runner.py:631] Final librispeech_conformer score: 15402.003957509995
