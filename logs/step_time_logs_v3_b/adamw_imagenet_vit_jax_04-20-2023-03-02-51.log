I0420 03:03:12.706590 139634154121024 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax.
I0420 03:03:12.770160 139634154121024 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 03:03:13.593397 139634154121024 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0420 03:03:13.594068 139634154121024 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 03:03:13.597965 139634154121024 submission_runner.py:528] Using RNG seed 3887363512
I0420 03:03:16.234697 139634154121024 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 03:03:16.234929 139634154121024 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1.
I0420 03:03:16.235105 139634154121024 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/hparams.json.
I0420 03:03:16.355035 139634154121024 submission_runner.py:232] Initializing dataset.
I0420 03:03:16.366474 139634154121024 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 03:03:16.373620 139634154121024 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 03:03:16.373735 139634154121024 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 03:03:16.631062 139634154121024 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 03:03:23.212046 139634154121024 submission_runner.py:239] Initializing model.
I0420 03:03:33.965139 139634154121024 submission_runner.py:249] Initializing optimizer.
I0420 03:03:34.619526 139634154121024 submission_runner.py:256] Initializing metrics bundle.
I0420 03:03:34.619700 139634154121024 submission_runner.py:273] Initializing checkpoint and logger.
I0420 03:03:34.620612 139634154121024 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0420 03:03:35.359110 139634154121024 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0420 03:03:35.360054 139634154121024 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/flags_0.json.
I0420 03:03:35.365514 139634154121024 submission_runner.py:309] Starting training loop.
I0420 03:04:27.252471 139457940412160 logging_writer.py:48] [0] global_step=0, grad_norm=0.3189859986305237, loss=6.907756805419922
I0420 03:04:27.267668 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:04:27.273640 139634154121024 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 03:04:27.279776 139634154121024 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 03:04:27.279893 139634154121024 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 03:04:27.339916 139634154121024 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 03:04:46.943123 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:04:46.951330 139634154121024 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0420 03:04:46.962224 139634154121024 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 03:04:46.962471 139634154121024 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 03:04:47.016571 139634154121024 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0420 03:05:04.995196 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:05:05.002508 139634154121024 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 03:05:05.007426 139634154121024 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0420 03:05:05.040757 139634154121024 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0420 03:05:15.853969 139634154121024 submission_runner.py:406] Time since start: 100.49s, 	Step: 1, 	{'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 51.90200400352478, 'total_duration': 100.48839139938354, 'accumulated_submission_time': 51.90200400352478, 'accumulated_eval_time': 48.5862500667572, 'accumulated_logging_time': 0}
I0420 03:05:15.871347 139394480596736 logging_writer.py:48] [1] accumulated_eval_time=48.586250, accumulated_logging_time=0, accumulated_submission_time=51.902004, global_step=1, preemption_count=0, score=51.902004, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=100.488391, train/accuracy=0.000859, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0420 03:05:16.076351 139634154121024 checkpoints.py:356] Saving checkpoint at step: 1
I0420 03:05:16.588188 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1
I0420 03:05:16.589088 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_1.
I0420 03:06:12.919959 139452470982400 logging_writer.py:48] [100] global_step=100, grad_norm=0.46589672565460205, loss=6.895674228668213
I0420 03:06:52.628671 139452479375104 logging_writer.py:48] [200] global_step=200, grad_norm=0.5130423307418823, loss=6.8541107177734375
I0420 03:07:32.404483 139452470982400 logging_writer.py:48] [300] global_step=300, grad_norm=0.5597947835922241, loss=6.806299209594727
I0420 03:08:12.639651 139452479375104 logging_writer.py:48] [400] global_step=400, grad_norm=0.6703774929046631, loss=6.756648063659668
I0420 03:08:53.766413 139452470982400 logging_writer.py:48] [500] global_step=500, grad_norm=0.7431609630584717, loss=6.680700778961182
I0420 03:09:34.601318 139452479375104 logging_writer.py:48] [600] global_step=600, grad_norm=1.0938981771469116, loss=6.62691593170166
I0420 03:10:15.772239 139452470982400 logging_writer.py:48] [700] global_step=700, grad_norm=0.6523046493530273, loss=6.721721172332764
I0420 03:10:56.825983 139452479375104 logging_writer.py:48] [800] global_step=800, grad_norm=0.9918351769447327, loss=6.612194538116455
I0420 03:11:38.063565 139452470982400 logging_writer.py:48] [900] global_step=900, grad_norm=0.7087054252624512, loss=6.551886558532715
I0420 03:12:16.609494 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:12:27.358552 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:12:33.893461 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:12:35.868214 139634154121024 submission_runner.py:406] Time since start: 540.50s, 	Step: 995, 	{'train/accuracy': 0.02812499925494194, 'train/loss': 6.073142051696777, 'validation/accuracy': 0.026359999552369118, 'validation/loss': 6.097471714019775, 'validation/num_examples': 50000, 'test/accuracy': 0.020500000566244125, 'test/loss': 6.181626319885254, 'test/num_examples': 10000, 'score': 471.8989188671112, 'total_duration': 540.5025823116302, 'accumulated_submission_time': 471.8989188671112, 'accumulated_eval_time': 67.84494185447693, 'accumulated_logging_time': 0.7367057800292969}
I0420 03:12:35.885680 139396032485120 logging_writer.py:48] [995] accumulated_eval_time=67.844942, accumulated_logging_time=0.736706, accumulated_submission_time=471.898919, global_step=995, preemption_count=0, score=471.898919, test/accuracy=0.020500, test/loss=6.181626, test/num_examples=10000, total_duration=540.502582, train/accuracy=0.028125, train/loss=6.073142, validation/accuracy=0.026360, validation/loss=6.097472, validation/num_examples=50000
I0420 03:12:37.576263 139634154121024 checkpoints.py:356] Saving checkpoint at step: 995
I0420 03:12:38.544213 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_995
I0420 03:12:38.545109 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_995.
I0420 03:12:40.961056 139396040877824 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0814331769943237, loss=6.389519214630127
I0420 03:13:20.757879 139455826482944 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1889045238494873, loss=6.474746227264404
I0420 03:14:00.643283 139396040877824 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8400888442993164, loss=6.541831016540527
I0420 03:14:40.984954 139455826482944 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.860002338886261, loss=6.717292308807373
I0420 03:15:22.047795 139396040877824 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9337859153747559, loss=6.239899635314941
I0420 03:16:02.935529 139455826482944 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8842772841453552, loss=6.152428150177002
I0420 03:16:43.754352 139396040877824 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.515420913696289, loss=6.176859378814697
I0420 03:17:24.645251 139455826482944 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9218276143074036, loss=6.104589462280273
I0420 03:18:05.792971 139396040877824 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.004148244857788, loss=6.055150508880615
I0420 03:18:46.851403 139455826482944 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7875878810882568, loss=6.291751861572266
I0420 03:19:27.753144 139396040877824 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1281951665878296, loss=6.599747657775879
I0420 03:19:38.676299 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:19:49.428650 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:19:55.964869 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:19:57.695671 139634154121024 submission_runner.py:406] Time since start: 982.33s, 	Step: 2028, 	{'train/accuracy': 0.05998046696186066, 'train/loss': 5.488440990447998, 'validation/accuracy': 0.05657999962568283, 'validation/loss': 5.528353214263916, 'validation/num_examples': 50000, 'test/accuracy': 0.04130000248551369, 'test/loss': 5.7072906494140625, 'test/num_examples': 10000, 'score': 892.0069506168365, 'total_duration': 982.3300426006317, 'accumulated_submission_time': 892.0069506168365, 'accumulated_eval_time': 86.86427545547485, 'accumulated_logging_time': 3.415088415145874}
I0420 03:19:57.707226 139455826482944 logging_writer.py:48] [2028] accumulated_eval_time=86.864275, accumulated_logging_time=3.415088, accumulated_submission_time=892.006951, global_step=2028, preemption_count=0, score=892.006951, test/accuracy=0.041300, test/loss=5.707291, test/num_examples=10000, total_duration=982.330043, train/accuracy=0.059980, train/loss=5.488441, validation/accuracy=0.056580, validation/loss=5.528353, validation/num_examples=50000
I0420 03:19:57.872184 139634154121024 checkpoints.py:356] Saving checkpoint at step: 2028
I0420 03:20:01.019685 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_2028
I0420 03:20:01.033776 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_2028.
I0420 03:20:30.019123 139396040877824 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1855942010879517, loss=6.018041610717773
I0420 03:21:09.903896 139455792912128 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8980762958526611, loss=6.009124755859375
I0420 03:21:50.025862 139396040877824 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0969653129577637, loss=6.006365776062012
I0420 03:22:31.236472 139455792912128 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0564919710159302, loss=5.977774620056152
I0420 03:23:12.254541 139396040877824 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.0250447988510132, loss=6.193692207336426
I0420 03:23:53.334012 139455792912128 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.2685449123382568, loss=5.861085414886475
I0420 03:24:34.057403 139396040877824 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8510279059410095, loss=5.826923847198486
I0420 03:25:15.031996 139455792912128 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9178870320320129, loss=6.056130409240723
I0420 03:25:55.869178 139396040877824 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9024989604949951, loss=5.801617622375488
I0420 03:26:36.970348 139455792912128 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9969676733016968, loss=5.858409404754639
I0420 03:27:01.305325 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:27:12.269659 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:27:19.043041 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:27:20.739552 139634154121024 submission_runner.py:406] Time since start: 1425.37s, 	Step: 3061, 	{'train/accuracy': 0.10968749970197678, 'train/loss': 5.010592937469482, 'validation/accuracy': 0.10305999964475632, 'validation/loss': 5.071018218994141, 'validation/num_examples': 50000, 'test/accuracy': 0.07690000534057617, 'test/loss': 5.341904163360596, 'test/num_examples': 10000, 'score': 1312.2549996376038, 'total_duration': 1425.3739075660706, 'accumulated_submission_time': 1312.2549996376038, 'accumulated_eval_time': 106.29844236373901, 'accumulated_logging_time': 6.754659414291382}
I0420 03:27:20.752800 139396040877824 logging_writer.py:48] [3061] accumulated_eval_time=106.298442, accumulated_logging_time=6.754659, accumulated_submission_time=1312.255000, global_step=3061, preemption_count=0, score=1312.255000, test/accuracy=0.076900, test/loss=5.341904, test/num_examples=10000, total_duration=1425.373908, train/accuracy=0.109687, train/loss=5.010593, validation/accuracy=0.103060, validation/loss=5.071018, validation/num_examples=50000
I0420 03:27:21.408591 139634154121024 checkpoints.py:356] Saving checkpoint at step: 3061
I0420 03:27:24.528525 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_3061
I0420 03:27:24.541863 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_3061.
I0420 03:27:40.554249 139455792912128 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8764802813529968, loss=5.773686408996582
I0420 03:28:20.583179 139455172179712 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7983725070953369, loss=5.832674503326416
I0420 03:29:00.634055 139455792912128 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0312297344207764, loss=5.647428035736084
I0420 03:29:41.082877 139455172179712 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7009238600730896, loss=6.599437713623047
I0420 03:30:22.143966 139455792912128 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8090174198150635, loss=5.620092391967773
I0420 03:31:02.899441 139455172179712 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.5175063610076904, loss=5.571531295776367
I0420 03:31:44.069276 139455792912128 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.695536732673645, loss=6.199016094207764
I0420 03:32:24.722570 139455172179712 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7962614297866821, loss=6.047206401824951
I0420 03:33:05.734082 139455792912128 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8040031790733337, loss=5.723294734954834
I0420 03:33:46.658408 139455172179712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.787736177444458, loss=5.565727710723877
I0420 03:34:24.933460 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:34:35.837564 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:34:43.025896 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:34:44.711074 139634154121024 submission_runner.py:406] Time since start: 1869.35s, 	Step: 4095, 	{'train/accuracy': 0.16121093928813934, 'train/loss': 4.548720359802246, 'validation/accuracy': 0.1471799910068512, 'validation/loss': 4.648599147796631, 'validation/num_examples': 50000, 'test/accuracy': 0.11170000582933426, 'test/loss': 4.955526351928711, 'test/num_examples': 10000, 'score': 1732.6223714351654, 'total_duration': 1869.3454902172089, 'accumulated_submission_time': 1732.6223714351654, 'accumulated_eval_time': 126.07607507705688, 'accumulated_logging_time': 10.55923843383789}
I0420 03:34:44.720468 139455792912128 logging_writer.py:48] [4095] accumulated_eval_time=126.076075, accumulated_logging_time=10.559238, accumulated_submission_time=1732.622371, global_step=4095, preemption_count=0, score=1732.622371, test/accuracy=0.111700, test/loss=4.955526, test/num_examples=10000, total_duration=1869.345490, train/accuracy=0.161211, train/loss=4.548720, validation/accuracy=0.147180, validation/loss=4.648599, validation/num_examples=50000
I0420 03:34:44.866107 139634154121024 checkpoints.py:356] Saving checkpoint at step: 4095
I0420 03:34:48.429230 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_4095
I0420 03:34:48.434087 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_4095.
I0420 03:34:50.950256 139455172179712 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9101976752281189, loss=5.437889575958252
I0420 03:35:31.129544 139455113434880 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.015149474143982, loss=5.405046463012695
I0420 03:36:11.151466 139455172179712 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7463934421539307, loss=6.292580604553223
I0420 03:36:51.704483 139455113434880 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7484068274497986, loss=6.334565162658691
I0420 03:37:33.204257 139455172179712 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7342497706413269, loss=6.42032527923584
I0420 03:38:14.386823 139455113434880 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.891605019569397, loss=5.372732162475586
I0420 03:38:55.409546 139455172179712 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9161351919174194, loss=5.636668682098389
I0420 03:39:36.614234 139455113434880 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8014264106750488, loss=5.287147045135498
I0420 03:40:17.909050 139455172179712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.836117684841156, loss=6.603343486785889
I0420 03:40:58.786904 139455113434880 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8510583639144897, loss=5.141363143920898
I0420 03:41:39.941273 139455172179712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8586410880088806, loss=5.324774742126465
I0420 03:41:48.490629 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:41:59.579822 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:42:06.365985 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:42:08.055047 139634154121024 submission_runner.py:406] Time since start: 2312.69s, 	Step: 5123, 	{'train/accuracy': 0.19539062678813934, 'train/loss': 4.199929237365723, 'validation/accuracy': 0.18063999712467194, 'validation/loss': 4.287660121917725, 'validation/num_examples': 50000, 'test/accuracy': 0.1347000002861023, 'test/loss': 4.6761980056762695, 'test/num_examples': 10000, 'score': 2152.6536116600037, 'total_duration': 2312.6894397735596, 'accumulated_submission_time': 2152.6536116600037, 'accumulated_eval_time': 145.6404550075531, 'accumulated_logging_time': 14.285023927688599}
I0420 03:42:08.066613 139455113434880 logging_writer.py:48] [5123] accumulated_eval_time=145.640455, accumulated_logging_time=14.285024, accumulated_submission_time=2152.653612, global_step=5123, preemption_count=0, score=2152.653612, test/accuracy=0.134700, test/loss=4.676198, test/num_examples=10000, total_duration=2312.689440, train/accuracy=0.195391, train/loss=4.199929, validation/accuracy=0.180640, validation/loss=4.287660, validation/num_examples=50000
I0420 03:42:08.215078 139634154121024 checkpoints.py:356] Saving checkpoint at step: 5123
I0420 03:42:11.859942 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_5123
I0420 03:42:11.877189 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_5123.
I0420 03:42:42.951109 139455172179712 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8685110807418823, loss=5.291982173919678
I0420 03:43:23.063139 139455105042176 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8660858869552612, loss=5.182009696960449
I0420 03:44:03.188620 139455172179712 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8672358989715576, loss=5.1935014724731445
I0420 03:44:44.067975 139455105042176 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8033756017684937, loss=5.590746879577637
I0420 03:45:25.372466 139455172179712 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6058066487312317, loss=6.409392356872559
I0420 03:46:06.537376 139455105042176 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6634718775749207, loss=5.835916042327881
I0420 03:46:47.727416 139455172179712 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8154371380805969, loss=5.031843185424805
I0420 03:47:28.984754 139455105042176 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6593493223190308, loss=6.438361644744873
I0420 03:48:09.902832 139455172179712 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7127517461776733, loss=5.156374931335449
I0420 03:48:50.856975 139455105042176 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.0044629573822021, loss=5.144964694976807
I0420 03:49:12.248329 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:49:23.269707 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:49:30.849543 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:49:32.526446 139634154121024 submission_runner.py:406] Time since start: 2757.16s, 	Step: 6154, 	{'train/accuracy': 0.24339842796325684, 'train/loss': 3.873344898223877, 'validation/accuracy': 0.2291799932718277, 'validation/loss': 3.9699673652648926, 'validation/num_examples': 50000, 'test/accuracy': 0.17080001533031464, 'test/loss': 4.4077606201171875, 'test/num_examples': 10000, 'score': 2573.000037908554, 'total_duration': 2757.160853624344, 'accumulated_submission_time': 2573.000037908554, 'accumulated_eval_time': 165.9185597896576, 'accumulated_logging_time': 18.109528303146362}
I0420 03:49:32.538714 139455172179712 logging_writer.py:48] [6154] accumulated_eval_time=165.918560, accumulated_logging_time=18.109528, accumulated_submission_time=2573.000038, global_step=6154, preemption_count=0, score=2573.000038, test/accuracy=0.170800, test/loss=4.407761, test/num_examples=10000, total_duration=2757.160854, train/accuracy=0.243398, train/loss=3.873345, validation/accuracy=0.229180, validation/loss=3.969967, validation/num_examples=50000
I0420 03:49:32.683143 139634154121024 checkpoints.py:356] Saving checkpoint at step: 6154
I0420 03:49:35.393112 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_6154
I0420 03:49:35.412020 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_6154.
I0420 03:49:54.934989 139455105042176 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7365401983261108, loss=4.975285530090332
I0420 03:50:34.810980 139455096649472 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8222160935401917, loss=4.946352005004883
I0420 03:51:15.371761 139455105042176 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7462472915649414, loss=5.016948699951172
I0420 03:51:56.671118 139455096649472 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7656948566436768, loss=4.925719738006592
I0420 03:52:38.028562 139455105042176 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9664551615715027, loss=5.283195972442627
I0420 03:53:19.160715 139455096649472 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8168413043022156, loss=5.237983226776123
I0420 03:54:00.644006 139455105042176 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8606110215187073, loss=4.868976593017578
I0420 03:54:41.550621 139455096649472 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6487092971801758, loss=5.101475715637207
I0420 03:55:23.020338 139455105042176 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6860421299934387, loss=4.956605911254883
I0420 03:56:04.011458 139455096649472 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7942523956298828, loss=4.880406379699707
I0420 03:56:35.647386 139634154121024 spec.py:298] Evaluating on the training split.
I0420 03:56:47.938549 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 03:56:57.539937 139634154121024 spec.py:326] Evaluating on the test split.
I0420 03:56:59.234759 139634154121024 submission_runner.py:406] Time since start: 3203.87s, 	Step: 7179, 	{'train/accuracy': 0.2899218797683716, 'train/loss': 3.5608651638031006, 'validation/accuracy': 0.26646000146865845, 'validation/loss': 3.6901631355285645, 'validation/num_examples': 50000, 'test/accuracy': 0.2053000032901764, 'test/loss': 4.165830612182617, 'test/num_examples': 10000, 'score': 2993.211090564728, 'total_duration': 3203.869146347046, 'accumulated_submission_time': 2993.211090564728, 'accumulated_eval_time': 189.50590872764587, 'accumulated_logging_time': 20.99738049507141}
I0420 03:56:59.249481 139455105042176 logging_writer.py:48] [7179] accumulated_eval_time=189.505909, accumulated_logging_time=20.997380, accumulated_submission_time=2993.211091, global_step=7179, preemption_count=0, score=2993.211091, test/accuracy=0.205300, test/loss=4.165831, test/num_examples=10000, total_duration=3203.869146, train/accuracy=0.289922, train/loss=3.560865, validation/accuracy=0.266460, validation/loss=3.690163, validation/num_examples=50000
I0420 03:56:59.408980 139634154121024 checkpoints.py:356] Saving checkpoint at step: 7179
I0420 03:57:00.747460 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_7179
I0420 03:57:00.762457 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_7179.
I0420 03:57:09.594130 139455096649472 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.689548909664154, loss=4.844955921173096
I0420 03:57:49.743745 139453771257600 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7286275029182434, loss=4.767106533050537
I0420 03:58:29.949496 139455096649472 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.732869565486908, loss=5.053918838500977
I0420 03:59:11.351594 139453771257600 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.48817065358161926, loss=6.090160369873047
I0420 03:59:52.633045 139455096649472 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7867326140403748, loss=4.664629936218262
I0420 04:00:34.016000 139453771257600 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5504612326622009, loss=5.951025485992432
I0420 04:01:15.281367 139455096649472 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7070886492729187, loss=4.74204158782959
I0420 04:01:56.314337 139453771257600 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7965511679649353, loss=4.72283935546875
I0420 04:02:37.731026 139455096649472 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6641388535499573, loss=5.1050872802734375
I0420 04:03:19.318996 139453771257600 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5336998701095581, loss=6.262487888336182
I0420 04:04:00.567327 139455096649472 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6848152279853821, loss=4.685548305511475
I0420 04:04:01.116990 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:04:14.665744 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:04:24.511104 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:04:26.186455 139634154121024 submission_runner.py:406] Time since start: 3650.82s, 	Step: 8203, 	{'train/accuracy': 0.3360937535762787, 'train/loss': 3.207644462585449, 'validation/accuracy': 0.30225998163223267, 'validation/loss': 3.3761558532714844, 'validation/num_examples': 50000, 'test/accuracy': 0.23600001633167267, 'test/loss': 3.899517059326172, 'test/num_examples': 10000, 'score': 3413.54154753685, 'total_duration': 3650.8208429813385, 'accumulated_submission_time': 3413.54154753685, 'accumulated_eval_time': 214.5753390789032, 'accumulated_logging_time': 22.52751350402832}
I0420 04:04:26.200630 139453771257600 logging_writer.py:48] [8203] accumulated_eval_time=214.575339, accumulated_logging_time=22.527514, accumulated_submission_time=3413.541548, global_step=8203, preemption_count=0, score=3413.541548, test/accuracy=0.236000, test/loss=3.899517, test/num_examples=10000, total_duration=3650.820843, train/accuracy=0.336094, train/loss=3.207644, validation/accuracy=0.302260, validation/loss=3.376156, validation/num_examples=50000
I0420 04:04:26.358402 139634154121024 checkpoints.py:356] Saving checkpoint at step: 8203
I0420 04:04:27.567242 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_8203
I0420 04:04:27.585590 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_8203.
I0420 04:05:06.844443 139455096649472 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7381556034088135, loss=4.540480613708496
I0420 04:05:47.433132 139453762864896 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5822755098342896, loss=6.115339279174805
I0420 04:06:28.565082 139455096649472 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5735705494880676, loss=5.505640983581543
I0420 04:07:09.573452 139453762864896 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6858212947845459, loss=4.632613182067871
I0420 04:07:51.472026 139455096649472 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6708112955093384, loss=4.766199111938477
I0420 04:08:33.013156 139453762864896 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7741075158119202, loss=4.530519962310791
I0420 04:09:14.609754 139455096649472 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6312893033027649, loss=4.474356651306152
I0420 04:09:55.812975 139453762864896 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6880275011062622, loss=4.397856712341309
I0420 04:10:36.631594 139455096649472 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6432282328605652, loss=4.381619930267334
I0420 04:11:18.928940 139453762864896 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8534457087516785, loss=5.0474677085876465
I0420 04:11:27.609301 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:11:41.442992 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:11:51.765620 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:11:53.462333 139634154121024 submission_runner.py:406] Time since start: 4098.10s, 	Step: 9223, 	{'train/accuracy': 0.3788085877895355, 'train/loss': 2.9512648582458496, 'validation/accuracy': 0.34125998616218567, 'validation/loss': 3.1582469940185547, 'validation/num_examples': 50000, 'test/accuracy': 0.26020002365112305, 'test/loss': 3.696072816848755, 'test/num_examples': 10000, 'score': 3833.537258386612, 'total_duration': 4098.096709251404, 'accumulated_submission_time': 3833.537258386612, 'accumulated_eval_time': 240.42833709716797, 'accumulated_logging_time': 23.933500289916992}
I0420 04:11:53.476099 139455096649472 logging_writer.py:48] [9223] accumulated_eval_time=240.428337, accumulated_logging_time=23.933500, accumulated_submission_time=3833.537258, global_step=9223, preemption_count=0, score=3833.537258, test/accuracy=0.260200, test/loss=3.696073, test/num_examples=10000, total_duration=4098.096709, train/accuracy=0.378809, train/loss=2.951265, validation/accuracy=0.341260, validation/loss=3.158247, validation/num_examples=50000
I0420 04:11:53.670415 139634154121024 checkpoints.py:356] Saving checkpoint at step: 9223
I0420 04:11:54.805116 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_9223
I0420 04:11:54.820359 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_9223.
I0420 04:12:25.866039 139453762864896 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7419213056564331, loss=4.4823222160339355
I0420 04:13:06.326049 139453754472192 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6548750400543213, loss=6.221632957458496
I0420 04:13:47.395848 139453762864896 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6963731646537781, loss=4.6874799728393555
I0420 04:14:28.579256 139453754472192 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.61579829454422, loss=4.977553367614746
I0420 04:15:09.675850 139453762864896 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7021828293800354, loss=5.226950168609619
I0420 04:15:51.456299 139453754472192 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7468405365943909, loss=4.384143829345703
I0420 04:16:32.896169 139453762864896 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7148858904838562, loss=4.750021457672119
I0420 04:17:14.527753 139453754472192 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6175177097320557, loss=4.458514213562012
I0420 04:17:55.783137 139453762864896 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7258414626121521, loss=4.318105697631836
I0420 04:18:37.164007 139453754472192 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6772084832191467, loss=4.629603385925293
I0420 04:18:55.136575 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:19:09.237823 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:19:20.021734 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:19:21.694340 139634154121024 submission_runner.py:406] Time since start: 4546.33s, 	Step: 10245, 	{'train/accuracy': 0.4022460877895355, 'train/loss': 2.934783458709717, 'validation/accuracy': 0.37459999322891235, 'validation/loss': 3.0687639713287354, 'validation/num_examples': 50000, 'test/accuracy': 0.28280001878738403, 'test/loss': 3.6166794300079346, 'test/num_examples': 10000, 'score': 4253.82599902153, 'total_duration': 4546.328699350357, 'accumulated_submission_time': 4253.82599902153, 'accumulated_eval_time': 266.98603224754333, 'accumulated_logging_time': 25.29740834236145}
I0420 04:19:21.704850 139453762864896 logging_writer.py:48] [10245] accumulated_eval_time=266.986032, accumulated_logging_time=25.297408, accumulated_submission_time=4253.825999, global_step=10245, preemption_count=0, score=4253.825999, test/accuracy=0.282800, test/loss=3.616679, test/num_examples=10000, total_duration=4546.328699, train/accuracy=0.402246, train/loss=2.934783, validation/accuracy=0.374600, validation/loss=3.068764, validation/num_examples=50000
I0420 04:19:21.875043 139634154121024 checkpoints.py:356] Saving checkpoint at step: 10245
I0420 04:19:23.055686 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_10245
I0420 04:19:23.072769 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_10245.
I0420 04:19:45.494510 139453754472192 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.670406699180603, loss=4.289156913757324
I0420 04:20:26.007113 139453746079488 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7312547564506531, loss=4.304503917694092
I0420 04:21:07.258377 139453754472192 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7030819654464722, loss=4.195718765258789
I0420 04:21:48.338887 139453746079488 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5885094404220581, loss=5.249181747436523
I0420 04:22:30.016894 139453754472192 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6155046820640564, loss=4.302185535430908
I0420 04:23:11.713175 139453746079488 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6167828440666199, loss=4.526386260986328
I0420 04:23:53.182045 139453754472192 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8046151995658875, loss=4.4902753829956055
I0420 04:24:34.353512 139453746079488 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7963425517082214, loss=4.124216556549072
I0420 04:25:16.047049 139453754472192 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5542579889297485, loss=4.888519763946533
I0420 04:25:57.761998 139453746079488 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6677783727645874, loss=4.193068504333496
I0420 04:26:23.443607 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:26:37.206217 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:26:48.153800 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:26:49.833659 139634154121024 submission_runner.py:406] Time since start: 4994.47s, 	Step: 11264, 	{'train/accuracy': 0.43550780415534973, 'train/loss': 2.6335062980651855, 'validation/accuracy': 0.402679979801178, 'validation/loss': 2.802704334259033, 'validation/num_examples': 50000, 'test/accuracy': 0.3061000108718872, 'test/loss': 3.4120912551879883, 'test/num_examples': 10000, 'score': 4674.169599056244, 'total_duration': 4994.468029975891, 'accumulated_submission_time': 4674.169599056244, 'accumulated_eval_time': 293.3760635852814, 'accumulated_logging_time': 26.6821870803833}
I0420 04:26:49.848705 139453754472192 logging_writer.py:48] [11264] accumulated_eval_time=293.376064, accumulated_logging_time=26.682187, accumulated_submission_time=4674.169599, global_step=11264, preemption_count=0, score=4674.169599, test/accuracy=0.306100, test/loss=3.412091, test/num_examples=10000, total_duration=4994.468030, train/accuracy=0.435508, train/loss=2.633506, validation/accuracy=0.402680, validation/loss=2.802704, validation/num_examples=50000
I0420 04:26:50.013250 139634154121024 checkpoints.py:356] Saving checkpoint at step: 11264
I0420 04:26:51.367334 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_11264
I0420 04:26:51.383439 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_11264.
I0420 04:27:06.156617 139453746079488 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7308830618858337, loss=4.171750068664551
I0420 04:27:47.251925 139453737686784 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.8208177089691162, loss=4.20612907409668
I0420 04:28:28.983844 139453746079488 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7361948490142822, loss=4.037213325500488
I0420 04:29:10.425948 139453737686784 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6430282592773438, loss=4.150773048400879
I0420 04:29:52.185323 139453746079488 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.652262806892395, loss=4.193026065826416
I0420 04:30:33.639154 139453737686784 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6750026345252991, loss=4.0429253578186035
I0420 04:31:15.357964 139453746079488 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5028260350227356, loss=6.014105319976807
I0420 04:31:57.024714 139453737686784 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4934598207473755, loss=5.977431774139404
I0420 04:32:38.630908 139453746079488 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5974968075752258, loss=4.5732903480529785
I0420 04:33:19.917654 139453737686784 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6913290619850159, loss=4.254616737365723
I0420 04:33:51.575500 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:34:05.698036 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:34:16.629250 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:34:18.294458 139634154121024 submission_runner.py:406] Time since start: 5442.93s, 	Step: 12278, 	{'train/accuracy': 0.4640234410762787, 'train/loss': 2.504469633102417, 'validation/accuracy': 0.42649999260902405, 'validation/loss': 2.704127550125122, 'validation/num_examples': 50000, 'test/accuracy': 0.32340002059936523, 'test/loss': 3.3191914558410645, 'test/num_examples': 10000, 'score': 5094.338372707367, 'total_duration': 5442.928847789764, 'accumulated_submission_time': 5094.338372707367, 'accumulated_eval_time': 320.094975233078, 'accumulated_logging_time': 28.23476767539978}
I0420 04:34:18.309305 139453746079488 logging_writer.py:48] [12278] accumulated_eval_time=320.094975, accumulated_logging_time=28.234768, accumulated_submission_time=5094.338373, global_step=12278, preemption_count=0, score=5094.338373, test/accuracy=0.323400, test/loss=3.319191, test/num_examples=10000, total_duration=5442.928848, train/accuracy=0.464023, train/loss=2.504470, validation/accuracy=0.426500, validation/loss=2.704128, validation/num_examples=50000
I0420 04:34:18.465780 139634154121024 checkpoints.py:356] Saving checkpoint at step: 12278
I0420 04:34:19.766052 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12278
I0420 04:34:19.783011 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_12278.
I0420 04:34:28.989217 139453737686784 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5843192338943481, loss=5.191561698913574
I0420 04:35:10.145118 139453729294080 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6267406940460205, loss=3.9983766078948975
I0420 04:35:53.174765 139453737686784 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6472973823547363, loss=3.9706053733825684
I0420 04:36:35.036328 139453729294080 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9517509937286377, loss=4.011266231536865
I0420 04:37:17.515861 139453737686784 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5294451117515564, loss=6.062749862670898
I0420 04:37:59.519477 139453729294080 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.699786365032196, loss=4.032550811767578
I0420 04:38:41.084119 139453737686784 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6124776005744934, loss=4.5469231605529785
I0420 04:39:23.183774 139453729294080 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5962494015693665, loss=4.388494968414307
I0420 04:40:05.392963 139453737686784 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7058518528938293, loss=3.8874869346618652
I0420 04:40:46.607630 139453729294080 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.73973548412323, loss=4.024561882019043
I0420 04:41:20.191439 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:41:34.665807 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:41:45.383986 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:41:47.063036 139634154121024 submission_runner.py:406] Time since start: 5891.70s, 	Step: 13282, 	{'train/accuracy': 0.5072070360183716, 'train/loss': 2.3172781467437744, 'validation/accuracy': 0.45027998089790344, 'validation/loss': 2.5977280139923096, 'validation/num_examples': 50000, 'test/accuracy': 0.3428000211715698, 'test/loss': 3.2109899520874023, 'test/num_examples': 10000, 'score': 5514.724670648575, 'total_duration': 5891.697426080704, 'accumulated_submission_time': 5514.724670648575, 'accumulated_eval_time': 346.9665296077728, 'accumulated_logging_time': 29.725385189056396}
I0420 04:41:47.077525 139453737686784 logging_writer.py:48] [13282] accumulated_eval_time=346.966530, accumulated_logging_time=29.725385, accumulated_submission_time=5514.724671, global_step=13282, preemption_count=0, score=5514.724671, test/accuracy=0.342800, test/loss=3.210990, test/num_examples=10000, total_duration=5891.697426, train/accuracy=0.507207, train/loss=2.317278, validation/accuracy=0.450280, validation/loss=2.597728, validation/num_examples=50000
I0420 04:41:47.296291 139634154121024 checkpoints.py:356] Saving checkpoint at step: 13282
I0420 04:41:48.580563 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_13282
I0420 04:41:48.595847 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_13282.
I0420 04:41:56.251339 139453729294080 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6522648930549622, loss=4.359612464904785
I0420 04:42:37.437401 139453720901376 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6098518967628479, loss=5.648103713989258
I0420 04:43:18.937218 139453729294080 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6497410535812378, loss=3.9709248542785645
I0420 04:44:01.000737 139453720901376 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.725502610206604, loss=4.147610187530518
I0420 04:44:42.796058 139453729294080 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5716898441314697, loss=5.862875461578369
I0420 04:45:25.224509 139453720901376 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7451931834220886, loss=4.0880513191223145
I0420 04:46:07.378299 139453729294080 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5644588470458984, loss=5.544243812561035
I0420 04:46:48.915932 139453720901376 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5875407457351685, loss=5.110862731933594
I0420 04:47:30.499364 139453729294080 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6360754370689392, loss=4.326661586761475
I0420 04:48:12.199314 139453720901376 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6686467528343201, loss=3.800045967102051
I0420 04:48:48.943968 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:49:02.984272 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:49:13.866277 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:49:15.535353 139634154121024 submission_runner.py:406] Time since start: 6340.17s, 	Step: 14289, 	{'train/accuracy': 0.5003710985183716, 'train/loss': 2.3247897624969482, 'validation/accuracy': 0.46173998713493347, 'validation/loss': 2.4987082481384277, 'validation/num_examples': 50000, 'test/accuracy': 0.3663000166416168, 'test/loss': 3.079174518585205, 'test/num_examples': 10000, 'score': 5935.051446914673, 'total_duration': 6340.169740438461, 'accumulated_submission_time': 5935.051446914673, 'accumulated_eval_time': 373.55786418914795, 'accumulated_logging_time': 31.259744882583618}
I0420 04:49:15.547696 139453729294080 logging_writer.py:48] [14289] accumulated_eval_time=373.557864, accumulated_logging_time=31.259745, accumulated_submission_time=5935.051447, global_step=14289, preemption_count=0, score=5935.051447, test/accuracy=0.366300, test/loss=3.079175, test/num_examples=10000, total_duration=6340.169740, train/accuracy=0.500371, train/loss=2.324790, validation/accuracy=0.461740, validation/loss=2.498708, validation/num_examples=50000
I0420 04:49:15.796291 139634154121024 checkpoints.py:356] Saving checkpoint at step: 14289
I0420 04:49:16.932713 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14289
I0420 04:49:16.952873 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_14289.
I0420 04:49:21.870061 139453720901376 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7353075742721558, loss=3.916991710662842
I0420 04:50:02.877327 139453712508672 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7375757694244385, loss=3.858506202697754
I0420 04:50:45.003934 139453720901376 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.656928539276123, loss=5.417484283447266
I0420 04:51:27.389152 139453712508672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7052691578865051, loss=3.8983306884765625
I0420 04:52:10.229889 139453720901376 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6626461744308472, loss=5.2684407234191895
I0420 04:52:52.322471 139453712508672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7659168839454651, loss=3.7681634426116943
I0420 04:53:34.991529 139453720901376 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6771610975265503, loss=3.854055643081665
I0420 04:54:17.334604 139453712508672 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6446698307991028, loss=4.962366104125977
I0420 04:54:59.637378 139453720901376 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8235964775085449, loss=3.8173534870147705
I0420 04:55:41.839241 139453712508672 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6292253136634827, loss=4.667903900146484
I0420 04:56:17.071921 139634154121024 spec.py:298] Evaluating on the training split.
I0420 04:56:31.332909 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 04:56:42.564424 139634154121024 spec.py:326] Evaluating on the test split.
I0420 04:56:44.225523 139634154121024 submission_runner.py:406] Time since start: 6788.86s, 	Step: 15285, 	{'train/accuracy': 0.5218554735183716, 'train/loss': 2.1704061031341553, 'validation/accuracy': 0.4879799783229828, 'validation/loss': 2.3612325191497803, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.9920032024383545, 'test/num_examples': 10000, 'score': 6355.149173736572, 'total_duration': 6788.859912872314, 'accumulated_submission_time': 6355.149173736572, 'accumulated_eval_time': 400.71142745018005, 'accumulated_logging_time': 32.67931628227234}
I0420 04:56:44.238958 139453720901376 logging_writer.py:48] [15285] accumulated_eval_time=400.711427, accumulated_logging_time=32.679316, accumulated_submission_time=6355.149174, global_step=15285, preemption_count=0, score=6355.149174, test/accuracy=0.377500, test/loss=2.992003, test/num_examples=10000, total_duration=6788.859913, train/accuracy=0.521855, train/loss=2.170406, validation/accuracy=0.487980, validation/loss=2.361233, validation/num_examples=50000
I0420 04:56:44.487751 139634154121024 checkpoints.py:356] Saving checkpoint at step: 15285
I0420 04:56:45.840132 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_15285
I0420 04:56:45.861614 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_15285.
I0420 04:56:52.328685 139453712508672 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9366720914840698, loss=3.8434715270996094
I0420 04:57:33.091804 139453637052160 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6135429739952087, loss=5.144565582275391
I0420 04:58:15.077042 139453712508672 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8244155049324036, loss=3.7301549911499023
I0420 04:58:56.564502 139453637052160 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6876589059829712, loss=4.490531921386719
I0420 04:59:38.158198 139453712508672 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6486899852752686, loss=4.605117321014404
I0420 05:00:19.991280 139453637052160 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7794049382209778, loss=3.828577995300293
I0420 05:01:01.655415 139453712508672 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6898302435874939, loss=3.731013774871826
I0420 05:01:43.986118 139453637052160 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6559706330299377, loss=4.463418006896973
I0420 05:02:26.619266 139453712508672 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.687834620475769, loss=4.4638776779174805
I0420 05:03:08.917777 139453637052160 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6562686562538147, loss=3.6107661724090576
I0420 05:03:46.239450 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:04:00.326704 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:04:12.159886 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:04:13.842812 139634154121024 submission_runner.py:406] Time since start: 7238.48s, 	Step: 16291, 	{'train/accuracy': 0.5483984351158142, 'train/loss': 2.0730888843536377, 'validation/accuracy': 0.5008599758148193, 'validation/loss': 2.300245761871338, 'validation/num_examples': 50000, 'test/accuracy': 0.39100003242492676, 'test/loss': 2.9199233055114746, 'test/num_examples': 10000, 'score': 6775.505286693573, 'total_duration': 7238.477157354355, 'accumulated_submission_time': 6775.505286693573, 'accumulated_eval_time': 428.31474685668945, 'accumulated_logging_time': 34.31773328781128}
I0420 05:04:13.858413 139453712508672 logging_writer.py:48] [16291] accumulated_eval_time=428.314747, accumulated_logging_time=34.317733, accumulated_submission_time=6775.505287, global_step=16291, preemption_count=0, score=6775.505287, test/accuracy=0.391000, test/loss=2.919923, test/num_examples=10000, total_duration=7238.477157, train/accuracy=0.548398, train/loss=2.073089, validation/accuracy=0.500860, validation/loss=2.300246, validation/num_examples=50000
I0420 05:04:14.179104 139634154121024 checkpoints.py:356] Saving checkpoint at step: 16291
I0420 05:04:15.685209 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_16291
I0420 05:04:15.707209 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_16291.
I0420 05:04:19.802891 139453637052160 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6586387753486633, loss=3.898588180541992
I0420 05:05:00.201977 139453628659456 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6763238310813904, loss=3.893313407897949
I0420 05:05:41.717226 139453637052160 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6373094916343689, loss=4.178755760192871
I0420 05:06:23.278599 139453628659456 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6393744945526123, loss=4.0590949058532715
I0420 05:07:05.385938 139453637052160 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6882225275039673, loss=4.166154384613037
I0420 05:07:47.507329 139453628659456 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7010473012924194, loss=3.7176127433776855
I0420 05:08:28.729430 139453637052160 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6613526940345764, loss=5.700739860534668
I0420 05:09:10.410720 139453628659456 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8042665123939514, loss=3.6299073696136475
I0420 05:09:51.879879 139453637052160 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.705530047416687, loss=3.716768264770508
I0420 05:10:33.280373 139453628659456 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6727344989776611, loss=4.1848554611206055
I0420 05:11:14.768607 139453637052160 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.8069307804107666, loss=3.562239646911621
I0420 05:11:15.744989 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:11:30.246900 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:11:42.267907 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:11:43.954449 139634154121024 submission_runner.py:406] Time since start: 7688.59s, 	Step: 17304, 	{'train/accuracy': 0.5660351514816284, 'train/loss': 1.962159514427185, 'validation/accuracy': 0.5111799836158752, 'validation/loss': 2.2268118858337402, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.8737308979034424, 'test/num_examples': 10000, 'score': 7195.521427631378, 'total_duration': 7688.588857412338, 'accumulated_submission_time': 7195.521427631378, 'accumulated_eval_time': 456.5241746902466, 'accumulated_logging_time': 36.18405222892761}
I0420 05:11:43.966156 139453628659456 logging_writer.py:48] [17304] accumulated_eval_time=456.524175, accumulated_logging_time=36.184052, accumulated_submission_time=7195.521428, global_step=17304, preemption_count=0, score=7195.521428, test/accuracy=0.402800, test/loss=2.873731, test/num_examples=10000, total_duration=7688.588857, train/accuracy=0.566035, train/loss=1.962160, validation/accuracy=0.511180, validation/loss=2.226812, validation/num_examples=50000
I0420 05:11:44.272521 139634154121024 checkpoints.py:356] Saving checkpoint at step: 17304
I0420 05:11:45.661015 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_17304
I0420 05:11:45.680557 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_17304.
I0420 05:12:24.426121 139453637052160 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7508355379104614, loss=3.660287380218506
I0420 05:13:06.592317 139453620266752 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9188234210014343, loss=3.6047019958496094
I0420 05:13:47.899699 139453637052160 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7526692748069763, loss=3.925004243850708
I0420 05:14:30.234894 139453620266752 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7589253783226013, loss=3.542567253112793
I0420 05:15:12.020778 139453637052160 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7177490592002869, loss=3.928793430328369
I0420 05:15:53.309608 139453620266752 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.783944845199585, loss=4.169888496398926
I0420 05:16:35.214505 139453637052160 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6861305236816406, loss=3.9515843391418457
I0420 05:17:16.867642 139453620266752 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.8492748141288757, loss=3.529148817062378
I0420 05:17:58.126158 139453637052160 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6795141100883484, loss=5.712793827056885
I0420 05:18:39.345567 139453620266752 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7648507952690125, loss=4.693485260009766
I0420 05:18:45.925124 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:19:00.550424 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:19:11.906728 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:19:13.579616 139634154121024 submission_runner.py:406] Time since start: 8138.21s, 	Step: 18317, 	{'train/accuracy': 0.5642773509025574, 'train/loss': 1.942944884300232, 'validation/accuracy': 0.524679958820343, 'validation/loss': 2.141568899154663, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.7986931800842285, 'test/num_examples': 10000, 'score': 7615.744054317474, 'total_duration': 8138.213966369629, 'accumulated_submission_time': 7615.744054317474, 'accumulated_eval_time': 484.17857789993286, 'accumulated_logging_time': 37.9122474193573}
I0420 05:19:13.594579 139453637052160 logging_writer.py:48] [18317] accumulated_eval_time=484.178578, accumulated_logging_time=37.912247, accumulated_submission_time=7615.744054, global_step=18317, preemption_count=0, score=7615.744054, test/accuracy=0.406100, test/loss=2.798693, test/num_examples=10000, total_duration=8138.213966, train/accuracy=0.564277, train/loss=1.942945, validation/accuracy=0.524680, validation/loss=2.141569, validation/num_examples=50000
I0420 05:19:13.877415 139634154121024 checkpoints.py:356] Saving checkpoint at step: 18317
I0420 05:19:15.049975 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_18317
I0420 05:19:15.068395 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_18317.
I0420 05:19:48.746524 139453620266752 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.797909677028656, loss=3.519608974456787
I0420 05:20:30.633298 139453611874048 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8496129512786865, loss=3.515594482421875
I0420 05:21:12.757247 139453620266752 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.124060869216919, loss=3.50006365776062
I0420 05:21:54.535831 139453611874048 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6358296275138855, loss=4.250974178314209
I0420 05:22:36.592792 139453620266752 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9784843921661377, loss=3.590665578842163
I0420 05:23:18.045997 139453611874048 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7677465677261353, loss=3.702180862426758
I0420 05:23:59.897296 139453620266752 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8881382346153259, loss=5.080704689025879
I0420 05:24:42.133588 139453611874048 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7526998519897461, loss=4.846268177032471
I0420 05:25:23.804228 139453620266752 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8897027969360352, loss=3.42706036567688
I0420 05:26:05.094825 139453611874048 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8661536574363708, loss=5.793911933898926
I0420 05:26:15.248848 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:26:29.817697 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:26:41.295815 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:26:42.981665 139634154121024 submission_runner.py:406] Time since start: 8587.62s, 	Step: 19326, 	{'train/accuracy': 0.5783789157867432, 'train/loss': 1.9520848989486694, 'validation/accuracy': 0.5354999899864197, 'validation/loss': 2.1538939476013184, 'validation/num_examples': 50000, 'test/accuracy': 0.4174000322818756, 'test/loss': 2.7942354679107666, 'test/num_examples': 10000, 'score': 8035.903555631638, 'total_duration': 8587.616047143936, 'accumulated_submission_time': 8035.903555631638, 'accumulated_eval_time': 511.91134095191956, 'accumulated_logging_time': 39.4027681350708}
I0420 05:26:42.998843 139453620266752 logging_writer.py:48] [19326] accumulated_eval_time=511.911341, accumulated_logging_time=39.402768, accumulated_submission_time=8035.903556, global_step=19326, preemption_count=0, score=8035.903556, test/accuracy=0.417400, test/loss=2.794235, test/num_examples=10000, total_duration=8587.616047, train/accuracy=0.578379, train/loss=1.952085, validation/accuracy=0.535500, validation/loss=2.153894, validation/num_examples=50000
I0420 05:26:43.253479 139634154121024 checkpoints.py:356] Saving checkpoint at step: 19326
I0420 05:26:44.516179 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_19326
I0420 05:26:44.537524 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_19326.
I0420 05:27:14.394602 139453611874048 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.8740406036376953, loss=3.614694595336914
I0420 05:27:55.574479 139453486049024 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7822442650794983, loss=3.3921635150909424
I0420 05:28:36.956911 139453611874048 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7770916819572449, loss=3.6218435764312744
I0420 05:29:18.589553 139453486049024 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7010191082954407, loss=3.4433159828186035
I0420 05:30:00.057059 139453611874048 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7229951024055481, loss=4.390405178070068
I0420 05:30:41.386964 139453486049024 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7474470138549805, loss=4.327921390533447
I0420 05:31:23.526871 139453611874048 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6937509775161743, loss=4.4531779289245605
I0420 05:32:05.330206 139453486049024 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.7901067137718201, loss=4.411792755126953
I0420 05:32:47.203082 139453611874048 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.789534330368042, loss=4.698045253753662
I0420 05:33:28.447218 139453486049024 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7506505250930786, loss=5.29945707321167
I0420 05:33:44.747446 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:33:58.964486 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:34:11.348082 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:34:13.044473 139634154121024 submission_runner.py:406] Time since start: 9037.68s, 	Step: 20341, 	{'train/accuracy': 0.6015819907188416, 'train/loss': 1.78476083278656, 'validation/accuracy': 0.5509999990463257, 'validation/loss': 2.0242514610290527, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.675971269607544, 'test/num_examples': 10000, 'score': 8456.092192411423, 'total_duration': 9037.67885351181, 'accumulated_submission_time': 8456.092192411423, 'accumulated_eval_time': 540.208366394043, 'accumulated_logging_time': 40.96019172668457}
I0420 05:34:13.061339 139453611874048 logging_writer.py:48] [20341] accumulated_eval_time=540.208366, accumulated_logging_time=40.960192, accumulated_submission_time=8456.092192, global_step=20341, preemption_count=0, score=8456.092192, test/accuracy=0.431200, test/loss=2.675971, test/num_examples=10000, total_duration=9037.678854, train/accuracy=0.601582, train/loss=1.784761, validation/accuracy=0.551000, validation/loss=2.024251, validation/num_examples=50000
I0420 05:34:13.400717 139634154121024 checkpoints.py:356] Saving checkpoint at step: 20341
I0420 05:34:14.808845 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_20341
I0420 05:34:14.830531 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_20341.
I0420 05:34:38.814736 139453486049024 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7950549721717834, loss=5.541198253631592
I0420 05:35:19.425281 139453477656320 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7080432176589966, loss=3.592655897140503
I0420 05:36:00.760152 139453486049024 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7643886208534241, loss=3.747051477432251
I0420 05:36:42.884333 139453477656320 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7912095189094543, loss=4.089912414550781
I0420 05:37:24.155169 139453486049024 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8096510171890259, loss=3.4596920013427734
I0420 05:38:05.537180 139453477656320 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7718480825424194, loss=3.652275800704956
I0420 05:38:47.183101 139453486049024 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8305178284645081, loss=3.4341046810150146
I0420 05:39:29.061799 139453477656320 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7493624091148376, loss=3.5128467082977295
I0420 05:40:10.544789 139453486049024 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.812901496887207, loss=4.066964626312256
I0420 05:40:52.926659 139453477656320 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7486361861228943, loss=4.948031902313232
I0420 05:41:15.195403 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:41:29.389605 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:41:41.903845 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:41:43.571950 139634154121024 submission_runner.py:406] Time since start: 9488.21s, 	Step: 21355, 	{'train/accuracy': 0.6152929663658142, 'train/loss': 1.734614372253418, 'validation/accuracy': 0.5588200092315674, 'validation/loss': 1.9915305376052856, 'validation/num_examples': 50000, 'test/accuracy': 0.4402000308036804, 'test/loss': 2.6341724395751953, 'test/num_examples': 10000, 'score': 8876.418927431107, 'total_duration': 9488.206291437149, 'accumulated_submission_time': 8876.418927431107, 'accumulated_eval_time': 568.5848226547241, 'accumulated_logging_time': 42.764538049697876}
I0420 05:41:43.588802 139453486049024 logging_writer.py:48] [21355] accumulated_eval_time=568.584823, accumulated_logging_time=42.764538, accumulated_submission_time=8876.418927, global_step=21355, preemption_count=0, score=8876.418927, test/accuracy=0.440200, test/loss=2.634172, test/num_examples=10000, total_duration=9488.206291, train/accuracy=0.615293, train/loss=1.734614, validation/accuracy=0.558820, validation/loss=1.991531, validation/num_examples=50000
I0420 05:41:43.977138 139634154121024 checkpoints.py:356] Saving checkpoint at step: 21355
I0420 05:41:45.326733 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_21355
I0420 05:41:45.346591 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_21355.
I0420 05:42:03.661230 139453477656320 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7248865365982056, loss=3.5847034454345703
I0420 05:42:44.308717 139453469263616 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8539273142814636, loss=3.674053192138672
I0420 05:43:26.181803 139453477656320 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7757465839385986, loss=4.64797830581665
I0420 05:44:07.968099 139453469263616 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8926685452461243, loss=3.632018566131592
I0420 05:44:49.301065 139453477656320 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7635306119918823, loss=4.897179126739502
I0420 05:45:31.111279 139453469263616 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8594139218330383, loss=3.5559353828430176
I0420 05:46:12.931369 139453477656320 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7494821548461914, loss=4.5595197677612305
I0420 05:46:54.662409 139453469263616 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.8039975762367249, loss=3.558868408203125
I0420 05:47:36.472911 139453477656320 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7997802495956421, loss=3.459993839263916
I0420 05:48:18.329005 139453469263616 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8910505175590515, loss=5.520071506500244
I0420 05:48:45.468526 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:48:59.481081 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:49:11.629046 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:49:13.305882 139634154121024 submission_runner.py:406] Time since start: 9937.94s, 	Step: 22366, 	{'train/accuracy': 0.6175585985183716, 'train/loss': 1.6707730293273926, 'validation/accuracy': 0.5679000020027161, 'validation/loss': 1.9088568687438965, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.5545949935913086, 'test/num_examples': 10000, 'score': 9296.501121044159, 'total_duration': 9937.94027686119, 'accumulated_submission_time': 9296.501121044159, 'accumulated_eval_time': 596.4221653938293, 'accumulated_logging_time': 44.55964112281799}
I0420 05:49:13.322716 139453477656320 logging_writer.py:48] [22366] accumulated_eval_time=596.422165, accumulated_logging_time=44.559641, accumulated_submission_time=9296.501121, global_step=22366, preemption_count=0, score=9296.501121, test/accuracy=0.447900, test/loss=2.554595, test/num_examples=10000, total_duration=9937.940277, train/accuracy=0.617559, train/loss=1.670773, validation/accuracy=0.567900, validation/loss=1.908857, validation/num_examples=50000
I0420 05:49:13.727297 139634154121024 checkpoints.py:356] Saving checkpoint at step: 22366
I0420 05:49:15.211397 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_22366
I0420 05:49:15.238602 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_22366.
I0420 05:49:29.142359 139453469263616 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7915220260620117, loss=4.638244152069092
I0420 05:50:10.409174 139453460870912 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.785254955291748, loss=3.9700310230255127
I0420 05:50:53.060432 139453469263616 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0496771335601807, loss=5.677826881408691
I0420 05:51:34.727767 139453460870912 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7310267686843872, loss=3.760916233062744
I0420 05:52:16.604804 139453469263616 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8593236804008484, loss=3.4037208557128906
I0420 05:52:58.520917 139453460870912 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7787240743637085, loss=4.942465782165527
I0420 05:53:40.594852 139453469263616 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7248708605766296, loss=3.6205387115478516
I0420 05:54:23.174958 139453460870912 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.9412805438041687, loss=3.369175672531128
I0420 05:55:05.075385 139453469263616 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.787350058555603, loss=3.3019111156463623
I0420 05:55:47.135450 139453460870912 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8486455678939819, loss=3.2937495708465576
I0420 05:56:15.601628 139634154121024 spec.py:298] Evaluating on the training split.
I0420 05:56:29.975666 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 05:56:41.487111 139634154121024 spec.py:326] Evaluating on the test split.
I0420 05:56:43.177753 139634154121024 submission_runner.py:406] Time since start: 10387.81s, 	Step: 23370, 	{'train/accuracy': 0.624707043170929, 'train/loss': 1.713015079498291, 'validation/accuracy': 0.573419988155365, 'validation/loss': 1.939420461654663, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.585891008377075, 'test/num_examples': 10000, 'score': 9716.842607975006, 'total_duration': 10387.812150239944, 'accumulated_submission_time': 9716.842607975006, 'accumulated_eval_time': 623.9982669353485, 'accumulated_logging_time': 46.49494504928589}
I0420 05:56:43.196108 139453469263616 logging_writer.py:48] [23370] accumulated_eval_time=623.998267, accumulated_logging_time=46.494945, accumulated_submission_time=9716.842608, global_step=23370, preemption_count=0, score=9716.842608, test/accuracy=0.460600, test/loss=2.585891, test/num_examples=10000, total_duration=10387.812150, train/accuracy=0.624707, train/loss=1.713015, validation/accuracy=0.573420, validation/loss=1.939420, validation/num_examples=50000
I0420 05:56:44.117962 139634154121024 checkpoints.py:356] Saving checkpoint at step: 23370
I0420 05:56:45.468690 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_23370
I0420 05:56:45.490315 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_23370.
I0420 05:56:57.842207 139453460870912 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8680200576782227, loss=3.573857307434082
I0420 05:57:39.149727 139453452478208 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8092349767684937, loss=3.283189296722412
I0420 05:58:21.425849 139453460870912 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.8344994783401489, loss=3.327792167663574
I0420 05:59:03.934755 139453452478208 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7443515062332153, loss=4.275934219360352
I0420 05:59:46.318121 139453460870912 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8202580809593201, loss=3.308056592941284
I0420 06:00:29.372996 139453452478208 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.780497670173645, loss=3.404155969619751
I0420 06:01:11.791313 139453460870912 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7644280195236206, loss=4.392645835876465
I0420 06:01:54.130522 139453452478208 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7421071529388428, loss=3.460360288619995
I0420 06:02:36.629232 139453460870912 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9130488038063049, loss=5.448277950286865
I0420 06:03:18.512124 139453452478208 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7777784466743469, loss=3.9498698711395264
I0420 06:03:45.723557 139634154121024 spec.py:298] Evaluating on the training split.
I0420 06:03:56.487267 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 06:04:08.862334 139634154121024 spec.py:326] Evaluating on the test split.
I0420 06:04:10.524220 139634154121024 submission_runner.py:406] Time since start: 10835.16s, 	Step: 24365, 	{'train/accuracy': 0.6304101347923279, 'train/loss': 1.6804702281951904, 'validation/accuracy': 0.5785999894142151, 'validation/loss': 1.9160630702972412, 'validation/num_examples': 50000, 'test/accuracy': 0.45350003242492676, 'test/loss': 2.588242530822754, 'test/num_examples': 10000, 'score': 10137.035184621811, 'total_duration': 10835.158628702164, 'accumulated_submission_time': 10137.035184621811, 'accumulated_eval_time': 648.7989227771759, 'accumulated_logging_time': 48.828855991363525}
I0420 06:04:10.535785 139453460870912 logging_writer.py:48] [24365] accumulated_eval_time=648.798923, accumulated_logging_time=48.828856, accumulated_submission_time=10137.035185, global_step=24365, preemption_count=0, score=10137.035185, test/accuracy=0.453500, test/loss=2.588243, test/num_examples=10000, total_duration=10835.158629, train/accuracy=0.630410, train/loss=1.680470, validation/accuracy=0.578600, validation/loss=1.916063, validation/num_examples=50000
I0420 06:04:10.743700 139634154121024 checkpoints.py:356] Saving checkpoint at step: 24365
I0420 06:04:12.140009 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_24365
I0420 06:04:12.162709 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_24365.
I0420 06:04:26.492304 139453452478208 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7018592953681946, loss=4.150853633880615
I0420 06:05:07.167165 139453444085504 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.81535804271698, loss=4.39856481552124
I0420 06:05:48.562629 139453452478208 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.7305572032928467, loss=3.6677212715148926
I0420 06:06:30.884896 139453444085504 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8014618158340454, loss=3.3644464015960693
I0420 06:07:12.136374 139453452478208 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7034755349159241, loss=4.053365707397461
I0420 06:07:53.462913 139453444085504 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8362252116203308, loss=3.184016704559326
I0420 06:08:34.883136 139453452478208 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8970214128494263, loss=4.872138977050781
I0420 06:09:16.807366 139453444085504 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8651825785636902, loss=3.3117988109588623
I0420 06:09:58.646864 139453452478208 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0073862075805664, loss=3.3196940422058105
I0420 06:10:40.481575 139453444085504 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8272531032562256, loss=3.63496994972229
I0420 06:11:12.290838 139634154121024 spec.py:298] Evaluating on the training split.
I0420 06:11:22.372483 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 06:11:35.185533 139634154121024 spec.py:326] Evaluating on the test split.
I0420 06:11:36.835753 139634154121024 submission_runner.py:406] Time since start: 11281.47s, 	Step: 25378, 	{'train/accuracy': 0.6426171660423279, 'train/loss': 1.614936351776123, 'validation/accuracy': 0.5890399813652039, 'validation/loss': 1.8615516424179077, 'validation/num_examples': 50000, 'test/accuracy': 0.4627000093460083, 'test/loss': 2.5205576419830322, 'test/num_examples': 10000, 'score': 10557.13270187378, 'total_duration': 11281.470165014267, 'accumulated_submission_time': 10557.13270187378, 'accumulated_eval_time': 673.3438484668732, 'accumulated_logging_time': 50.47846269607544}
I0420 06:11:36.849967 139453452478208 logging_writer.py:48] [25378] accumulated_eval_time=673.343848, accumulated_logging_time=50.478463, accumulated_submission_time=10557.132702, global_step=25378, preemption_count=0, score=10557.132702, test/accuracy=0.462700, test/loss=2.520558, test/num_examples=10000, total_duration=11281.470165, train/accuracy=0.642617, train/loss=1.614936, validation/accuracy=0.589040, validation/loss=1.861552, validation/num_examples=50000
I0420 06:11:37.133082 139634154121024 checkpoints.py:356] Saving checkpoint at step: 25378
I0420 06:11:38.385550 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_25378
I0420 06:11:38.406009 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_25378.
I0420 06:11:47.563324 139453444085504 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7544991970062256, loss=4.331748008728027
I0420 06:12:28.121483 139453192451840 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9295344352722168, loss=5.498793601989746
I0420 06:13:10.221106 139453444085504 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8513849973678589, loss=3.210885524749756
I0420 06:13:52.624719 139453192451840 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.7939133048057556, loss=3.3689610958099365
I0420 06:14:34.812580 139453444085504 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7850509881973267, loss=4.61033821105957
I0420 06:15:16.481493 139453192451840 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8946605920791626, loss=3.783802032470703
I0420 06:15:58.474325 139453444085504 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7707095742225647, loss=3.540799379348755
I0420 06:16:40.042110 139453192451840 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8384771943092346, loss=4.840837001800537
I0420 06:17:21.928684 139453444085504 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8344506025314331, loss=3.2618892192840576
I0420 06:18:04.751715 139453192451840 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7586999535560608, loss=3.5690841674804688
I0420 06:18:38.806475 139634154121024 spec.py:298] Evaluating on the training split.
I0420 06:18:49.034438 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 06:19:01.801932 139634154121024 spec.py:326] Evaluating on the test split.
I0420 06:19:03.486969 139634154121024 submission_runner.py:406] Time since start: 11728.12s, 	Step: 26384, 	{'train/accuracy': 0.672558605670929, 'train/loss': 1.4435343742370605, 'validation/accuracy': 0.5943999886512756, 'validation/loss': 1.7993324995040894, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.4525508880615234, 'test/num_examples': 10000, 'score': 10977.496617555618, 'total_duration': 11728.121235847473, 'accumulated_submission_time': 10977.496617555618, 'accumulated_eval_time': 698.0242011547089, 'accumulated_logging_time': 52.06649827957153}
I0420 06:19:03.504993 139453444085504 logging_writer.py:48] [26384] accumulated_eval_time=698.024201, accumulated_logging_time=52.066498, accumulated_submission_time=10977.496618, global_step=26384, preemption_count=0, score=10977.496618, test/accuracy=0.467400, test/loss=2.452551, test/num_examples=10000, total_duration=11728.121236, train/accuracy=0.672559, train/loss=1.443534, validation/accuracy=0.594400, validation/loss=1.799332, validation/num_examples=50000
I0420 06:19:03.901458 139634154121024 checkpoints.py:356] Saving checkpoint at step: 26384
I0420 06:19:05.068249 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_26384
I0420 06:19:05.089055 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_26384.
I0420 06:19:11.904929 139453192451840 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8746671080589294, loss=3.358431339263916
I0420 06:19:51.985368 139453184059136 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.8275668025016785, loss=3.322085380554199
I0420 06:20:33.513986 139453192451840 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7918702960014343, loss=3.920954704284668
I0420 06:21:15.506410 139453184059136 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9615909457206726, loss=3.1788110733032227
I0420 06:21:57.533877 139453192451840 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7922037243843079, loss=3.1971797943115234
I0420 06:22:39.353956 139453184059136 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.8685779571533203, loss=3.2146358489990234
I0420 06:23:21.710153 139453192451840 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.884283721446991, loss=3.188629150390625
I0420 06:24:03.662868 139453184059136 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8447819352149963, loss=3.153878927230835
I0420 06:24:45.606555 139453192451840 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8060881495475769, loss=3.278449296951294
I0420 06:25:27.094819 139453184059136 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8984205722808838, loss=3.1914730072021484
I0420 06:26:05.195970 139634154121024 spec.py:298] Evaluating on the training split.
I0420 06:26:15.337901 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 06:26:27.833574 139634154121024 spec.py:326] Evaluating on the test split.
I0420 06:26:29.492370 139634154121024 submission_runner.py:406] Time since start: 12174.13s, 	Step: 27392, 	{'train/accuracy': 0.6496288776397705, 'train/loss': 1.5612359046936035, 'validation/accuracy': 0.5991599559783936, 'validation/loss': 1.7951539754867554, 'validation/num_examples': 50000, 'test/accuracy': 0.47140002250671387, 'test/loss': 2.453737735748291, 'test/num_examples': 10000, 'score': 11397.563924312592, 'total_duration': 12174.12678194046, 'accumulated_submission_time': 11397.563924312592, 'accumulated_eval_time': 722.3205797672272, 'accumulated_logging_time': 53.68946599960327}
I0420 06:26:29.504776 139453192451840 logging_writer.py:48] [27392] accumulated_eval_time=722.320580, accumulated_logging_time=53.689466, accumulated_submission_time=11397.563924, global_step=27392, preemption_count=0, score=11397.563924, test/accuracy=0.471400, test/loss=2.453738, test/num_examples=10000, total_duration=12174.126782, train/accuracy=0.649629, train/loss=1.561236, validation/accuracy=0.599160, validation/loss=1.795154, validation/num_examples=50000
I0420 06:26:29.835322 139634154121024 checkpoints.py:356] Saving checkpoint at step: 27392
I0420 06:26:31.266296 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_27392
I0420 06:26:31.288595 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_27392.
I0420 06:26:34.903632 139453184059136 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9763460159301758, loss=5.489879608154297
I0420 06:27:14.877130 139453175666432 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.9528980255126953, loss=4.554422855377197
I0420 06:27:56.428476 139453184059136 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8044832348823547, loss=3.2387712001800537
I0420 06:28:38.464526 139453175666432 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8646512031555176, loss=3.1156716346740723
I0420 06:29:20.301573 139453184059136 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8066275715827942, loss=3.6573972702026367
I0420 06:30:02.361010 139453175666432 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8838907480239868, loss=3.2151172161102295
I0420 06:30:43.082022 139634154121024 spec.py:298] Evaluating on the training split.
I0420 06:30:53.130707 139634154121024 spec.py:310] Evaluating on the validation split.
I0420 06:31:06.034134 139634154121024 spec.py:326] Evaluating on the test split.
I0420 06:31:07.709724 139634154121024 submission_runner.py:406] Time since start: 12452.34s, 	Step: 28000, 	{'train/accuracy': 0.6583593487739563, 'train/loss': 1.5350534915924072, 'validation/accuracy': 0.6018199920654297, 'validation/loss': 1.8021916151046753, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.4410178661346436, 'test/num_examples': 10000, 'score': 11649.32623744011, 'total_duration': 12452.344131708145, 'accumulated_submission_time': 11649.32623744011, 'accumulated_eval_time': 746.9482498168945, 'accumulated_logging_time': 55.50487017631531}
I0420 06:31:07.722320 139453184059136 logging_writer.py:48] [28000] accumulated_eval_time=746.948250, accumulated_logging_time=55.504870, accumulated_submission_time=11649.326237, global_step=28000, preemption_count=0, score=11649.326237, test/accuracy=0.475900, test/loss=2.441018, test/num_examples=10000, total_duration=12452.344132, train/accuracy=0.658359, train/loss=1.535053, validation/accuracy=0.601820, validation/loss=1.802192, validation/num_examples=50000
I0420 06:31:07.965365 139634154121024 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 06:31:09.218816 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000
I0420 06:31:09.238425 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000.
I0420 06:31:09.250383 139453175666432 logging_writer.py:48] [28000] global_step=28000, preemption_count=0, score=11649.326237
I0420 06:31:09.519906 139634154121024 checkpoints.py:356] Saving checkpoint at step: 28000
I0420 06:31:10.670540 139634154121024 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000
I0420 06:31:10.685891 139634154121024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/imagenet_vit_jax/trial_1/checkpoint_28000.
I0420 06:31:11.543402 139634154121024 submission_runner.py:567] Tuning trial 1/1
I0420 06:31:11.544251 139634154121024 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 06:31:11.548473 139634154121024 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 51.90200400352478, 'total_duration': 100.48839139938354, 'accumulated_submission_time': 51.90200400352478, 'accumulated_eval_time': 48.5862500667572, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (995, {'train/accuracy': 0.02812499925494194, 'train/loss': 6.073142051696777, 'validation/accuracy': 0.026359999552369118, 'validation/loss': 6.097471714019775, 'validation/num_examples': 50000, 'test/accuracy': 0.020500000566244125, 'test/loss': 6.181626319885254, 'test/num_examples': 10000, 'score': 471.8989188671112, 'total_duration': 540.5025823116302, 'accumulated_submission_time': 471.8989188671112, 'accumulated_eval_time': 67.84494185447693, 'accumulated_logging_time': 0.7367057800292969, 'global_step': 995, 'preemption_count': 0}), (2028, {'train/accuracy': 0.05998046696186066, 'train/loss': 5.488440990447998, 'validation/accuracy': 0.05657999962568283, 'validation/loss': 5.528353214263916, 'validation/num_examples': 50000, 'test/accuracy': 0.04130000248551369, 'test/loss': 5.7072906494140625, 'test/num_examples': 10000, 'score': 892.0069506168365, 'total_duration': 982.3300426006317, 'accumulated_submission_time': 892.0069506168365, 'accumulated_eval_time': 86.86427545547485, 'accumulated_logging_time': 3.415088415145874, 'global_step': 2028, 'preemption_count': 0}), (3061, {'train/accuracy': 0.10968749970197678, 'train/loss': 5.010592937469482, 'validation/accuracy': 0.10305999964475632, 'validation/loss': 5.071018218994141, 'validation/num_examples': 50000, 'test/accuracy': 0.07690000534057617, 'test/loss': 5.341904163360596, 'test/num_examples': 10000, 'score': 1312.2549996376038, 'total_duration': 1425.3739075660706, 'accumulated_submission_time': 1312.2549996376038, 'accumulated_eval_time': 106.29844236373901, 'accumulated_logging_time': 6.754659414291382, 'global_step': 3061, 'preemption_count': 0}), (4095, {'train/accuracy': 0.16121093928813934, 'train/loss': 4.548720359802246, 'validation/accuracy': 0.1471799910068512, 'validation/loss': 4.648599147796631, 'validation/num_examples': 50000, 'test/accuracy': 0.11170000582933426, 'test/loss': 4.955526351928711, 'test/num_examples': 10000, 'score': 1732.6223714351654, 'total_duration': 1869.3454902172089, 'accumulated_submission_time': 1732.6223714351654, 'accumulated_eval_time': 126.07607507705688, 'accumulated_logging_time': 10.55923843383789, 'global_step': 4095, 'preemption_count': 0}), (5123, {'train/accuracy': 0.19539062678813934, 'train/loss': 4.199929237365723, 'validation/accuracy': 0.18063999712467194, 'validation/loss': 4.287660121917725, 'validation/num_examples': 50000, 'test/accuracy': 0.1347000002861023, 'test/loss': 4.6761980056762695, 'test/num_examples': 10000, 'score': 2152.6536116600037, 'total_duration': 2312.6894397735596, 'accumulated_submission_time': 2152.6536116600037, 'accumulated_eval_time': 145.6404550075531, 'accumulated_logging_time': 14.285023927688599, 'global_step': 5123, 'preemption_count': 0}), (6154, {'train/accuracy': 0.24339842796325684, 'train/loss': 3.873344898223877, 'validation/accuracy': 0.2291799932718277, 'validation/loss': 3.9699673652648926, 'validation/num_examples': 50000, 'test/accuracy': 0.17080001533031464, 'test/loss': 4.4077606201171875, 'test/num_examples': 10000, 'score': 2573.000037908554, 'total_duration': 2757.160853624344, 'accumulated_submission_time': 2573.000037908554, 'accumulated_eval_time': 165.9185597896576, 'accumulated_logging_time': 18.109528303146362, 'global_step': 6154, 'preemption_count': 0}), (7179, {'train/accuracy': 0.2899218797683716, 'train/loss': 3.5608651638031006, 'validation/accuracy': 0.26646000146865845, 'validation/loss': 3.6901631355285645, 'validation/num_examples': 50000, 'test/accuracy': 0.2053000032901764, 'test/loss': 4.165830612182617, 'test/num_examples': 10000, 'score': 2993.211090564728, 'total_duration': 3203.869146347046, 'accumulated_submission_time': 2993.211090564728, 'accumulated_eval_time': 189.50590872764587, 'accumulated_logging_time': 20.99738049507141, 'global_step': 7179, 'preemption_count': 0}), (8203, {'train/accuracy': 0.3360937535762787, 'train/loss': 3.207644462585449, 'validation/accuracy': 0.30225998163223267, 'validation/loss': 3.3761558532714844, 'validation/num_examples': 50000, 'test/accuracy': 0.23600001633167267, 'test/loss': 3.899517059326172, 'test/num_examples': 10000, 'score': 3413.54154753685, 'total_duration': 3650.8208429813385, 'accumulated_submission_time': 3413.54154753685, 'accumulated_eval_time': 214.5753390789032, 'accumulated_logging_time': 22.52751350402832, 'global_step': 8203, 'preemption_count': 0}), (9223, {'train/accuracy': 0.3788085877895355, 'train/loss': 2.9512648582458496, 'validation/accuracy': 0.34125998616218567, 'validation/loss': 3.1582469940185547, 'validation/num_examples': 50000, 'test/accuracy': 0.26020002365112305, 'test/loss': 3.696072816848755, 'test/num_examples': 10000, 'score': 3833.537258386612, 'total_duration': 4098.096709251404, 'accumulated_submission_time': 3833.537258386612, 'accumulated_eval_time': 240.42833709716797, 'accumulated_logging_time': 23.933500289916992, 'global_step': 9223, 'preemption_count': 0}), (10245, {'train/accuracy': 0.4022460877895355, 'train/loss': 2.934783458709717, 'validation/accuracy': 0.37459999322891235, 'validation/loss': 3.0687639713287354, 'validation/num_examples': 50000, 'test/accuracy': 0.28280001878738403, 'test/loss': 3.6166794300079346, 'test/num_examples': 10000, 'score': 4253.82599902153, 'total_duration': 4546.328699350357, 'accumulated_submission_time': 4253.82599902153, 'accumulated_eval_time': 266.98603224754333, 'accumulated_logging_time': 25.29740834236145, 'global_step': 10245, 'preemption_count': 0}), (11264, {'train/accuracy': 0.43550780415534973, 'train/loss': 2.6335062980651855, 'validation/accuracy': 0.402679979801178, 'validation/loss': 2.802704334259033, 'validation/num_examples': 50000, 'test/accuracy': 0.3061000108718872, 'test/loss': 3.4120912551879883, 'test/num_examples': 10000, 'score': 4674.169599056244, 'total_duration': 4994.468029975891, 'accumulated_submission_time': 4674.169599056244, 'accumulated_eval_time': 293.3760635852814, 'accumulated_logging_time': 26.6821870803833, 'global_step': 11264, 'preemption_count': 0}), (12278, {'train/accuracy': 0.4640234410762787, 'train/loss': 2.504469633102417, 'validation/accuracy': 0.42649999260902405, 'validation/loss': 2.704127550125122, 'validation/num_examples': 50000, 'test/accuracy': 0.32340002059936523, 'test/loss': 3.3191914558410645, 'test/num_examples': 10000, 'score': 5094.338372707367, 'total_duration': 5442.928847789764, 'accumulated_submission_time': 5094.338372707367, 'accumulated_eval_time': 320.094975233078, 'accumulated_logging_time': 28.23476767539978, 'global_step': 12278, 'preemption_count': 0}), (13282, {'train/accuracy': 0.5072070360183716, 'train/loss': 2.3172781467437744, 'validation/accuracy': 0.45027998089790344, 'validation/loss': 2.5977280139923096, 'validation/num_examples': 50000, 'test/accuracy': 0.3428000211715698, 'test/loss': 3.2109899520874023, 'test/num_examples': 10000, 'score': 5514.724670648575, 'total_duration': 5891.697426080704, 'accumulated_submission_time': 5514.724670648575, 'accumulated_eval_time': 346.9665296077728, 'accumulated_logging_time': 29.725385189056396, 'global_step': 13282, 'preemption_count': 0}), (14289, {'train/accuracy': 0.5003710985183716, 'train/loss': 2.3247897624969482, 'validation/accuracy': 0.46173998713493347, 'validation/loss': 2.4987082481384277, 'validation/num_examples': 50000, 'test/accuracy': 0.3663000166416168, 'test/loss': 3.079174518585205, 'test/num_examples': 10000, 'score': 5935.051446914673, 'total_duration': 6340.169740438461, 'accumulated_submission_time': 5935.051446914673, 'accumulated_eval_time': 373.55786418914795, 'accumulated_logging_time': 31.259744882583618, 'global_step': 14289, 'preemption_count': 0}), (15285, {'train/accuracy': 0.5218554735183716, 'train/loss': 2.1704061031341553, 'validation/accuracy': 0.4879799783229828, 'validation/loss': 2.3612325191497803, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.9920032024383545, 'test/num_examples': 10000, 'score': 6355.149173736572, 'total_duration': 6788.859912872314, 'accumulated_submission_time': 6355.149173736572, 'accumulated_eval_time': 400.71142745018005, 'accumulated_logging_time': 32.67931628227234, 'global_step': 15285, 'preemption_count': 0}), (16291, {'train/accuracy': 0.5483984351158142, 'train/loss': 2.0730888843536377, 'validation/accuracy': 0.5008599758148193, 'validation/loss': 2.300245761871338, 'validation/num_examples': 50000, 'test/accuracy': 0.39100003242492676, 'test/loss': 2.9199233055114746, 'test/num_examples': 10000, 'score': 6775.505286693573, 'total_duration': 7238.477157354355, 'accumulated_submission_time': 6775.505286693573, 'accumulated_eval_time': 428.31474685668945, 'accumulated_logging_time': 34.31773328781128, 'global_step': 16291, 'preemption_count': 0}), (17304, {'train/accuracy': 0.5660351514816284, 'train/loss': 1.962159514427185, 'validation/accuracy': 0.5111799836158752, 'validation/loss': 2.2268118858337402, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.8737308979034424, 'test/num_examples': 10000, 'score': 7195.521427631378, 'total_duration': 7688.588857412338, 'accumulated_submission_time': 7195.521427631378, 'accumulated_eval_time': 456.5241746902466, 'accumulated_logging_time': 36.18405222892761, 'global_step': 17304, 'preemption_count': 0}), (18317, {'train/accuracy': 0.5642773509025574, 'train/loss': 1.942944884300232, 'validation/accuracy': 0.524679958820343, 'validation/loss': 2.141568899154663, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.7986931800842285, 'test/num_examples': 10000, 'score': 7615.744054317474, 'total_duration': 8138.213966369629, 'accumulated_submission_time': 7615.744054317474, 'accumulated_eval_time': 484.17857789993286, 'accumulated_logging_time': 37.9122474193573, 'global_step': 18317, 'preemption_count': 0}), (19326, {'train/accuracy': 0.5783789157867432, 'train/loss': 1.9520848989486694, 'validation/accuracy': 0.5354999899864197, 'validation/loss': 2.1538939476013184, 'validation/num_examples': 50000, 'test/accuracy': 0.4174000322818756, 'test/loss': 2.7942354679107666, 'test/num_examples': 10000, 'score': 8035.903555631638, 'total_duration': 8587.616047143936, 'accumulated_submission_time': 8035.903555631638, 'accumulated_eval_time': 511.91134095191956, 'accumulated_logging_time': 39.4027681350708, 'global_step': 19326, 'preemption_count': 0}), (20341, {'train/accuracy': 0.6015819907188416, 'train/loss': 1.78476083278656, 'validation/accuracy': 0.5509999990463257, 'validation/loss': 2.0242514610290527, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.675971269607544, 'test/num_examples': 10000, 'score': 8456.092192411423, 'total_duration': 9037.67885351181, 'accumulated_submission_time': 8456.092192411423, 'accumulated_eval_time': 540.208366394043, 'accumulated_logging_time': 40.96019172668457, 'global_step': 20341, 'preemption_count': 0}), (21355, {'train/accuracy': 0.6152929663658142, 'train/loss': 1.734614372253418, 'validation/accuracy': 0.5588200092315674, 'validation/loss': 1.9915305376052856, 'validation/num_examples': 50000, 'test/accuracy': 0.4402000308036804, 'test/loss': 2.6341724395751953, 'test/num_examples': 10000, 'score': 8876.418927431107, 'total_duration': 9488.206291437149, 'accumulated_submission_time': 8876.418927431107, 'accumulated_eval_time': 568.5848226547241, 'accumulated_logging_time': 42.764538049697876, 'global_step': 21355, 'preemption_count': 0}), (22366, {'train/accuracy': 0.6175585985183716, 'train/loss': 1.6707730293273926, 'validation/accuracy': 0.5679000020027161, 'validation/loss': 1.9088568687438965, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.5545949935913086, 'test/num_examples': 10000, 'score': 9296.501121044159, 'total_duration': 9937.94027686119, 'accumulated_submission_time': 9296.501121044159, 'accumulated_eval_time': 596.4221653938293, 'accumulated_logging_time': 44.55964112281799, 'global_step': 22366, 'preemption_count': 0}), (23370, {'train/accuracy': 0.624707043170929, 'train/loss': 1.713015079498291, 'validation/accuracy': 0.573419988155365, 'validation/loss': 1.939420461654663, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.585891008377075, 'test/num_examples': 10000, 'score': 9716.842607975006, 'total_duration': 10387.812150239944, 'accumulated_submission_time': 9716.842607975006, 'accumulated_eval_time': 623.9982669353485, 'accumulated_logging_time': 46.49494504928589, 'global_step': 23370, 'preemption_count': 0}), (24365, {'train/accuracy': 0.6304101347923279, 'train/loss': 1.6804702281951904, 'validation/accuracy': 0.5785999894142151, 'validation/loss': 1.9160630702972412, 'validation/num_examples': 50000, 'test/accuracy': 0.45350003242492676, 'test/loss': 2.588242530822754, 'test/num_examples': 10000, 'score': 10137.035184621811, 'total_duration': 10835.158628702164, 'accumulated_submission_time': 10137.035184621811, 'accumulated_eval_time': 648.7989227771759, 'accumulated_logging_time': 48.828855991363525, 'global_step': 24365, 'preemption_count': 0}), (25378, {'train/accuracy': 0.6426171660423279, 'train/loss': 1.614936351776123, 'validation/accuracy': 0.5890399813652039, 'validation/loss': 1.8615516424179077, 'validation/num_examples': 50000, 'test/accuracy': 0.4627000093460083, 'test/loss': 2.5205576419830322, 'test/num_examples': 10000, 'score': 10557.13270187378, 'total_duration': 11281.470165014267, 'accumulated_submission_time': 10557.13270187378, 'accumulated_eval_time': 673.3438484668732, 'accumulated_logging_time': 50.47846269607544, 'global_step': 25378, 'preemption_count': 0}), (26384, {'train/accuracy': 0.672558605670929, 'train/loss': 1.4435343742370605, 'validation/accuracy': 0.5943999886512756, 'validation/loss': 1.7993324995040894, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.4525508880615234, 'test/num_examples': 10000, 'score': 10977.496617555618, 'total_duration': 11728.121235847473, 'accumulated_submission_time': 10977.496617555618, 'accumulated_eval_time': 698.0242011547089, 'accumulated_logging_time': 52.06649827957153, 'global_step': 26384, 'preemption_count': 0}), (27392, {'train/accuracy': 0.6496288776397705, 'train/loss': 1.5612359046936035, 'validation/accuracy': 0.5991599559783936, 'validation/loss': 1.7951539754867554, 'validation/num_examples': 50000, 'test/accuracy': 0.47140002250671387, 'test/loss': 2.453737735748291, 'test/num_examples': 10000, 'score': 11397.563924312592, 'total_duration': 12174.12678194046, 'accumulated_submission_time': 11397.563924312592, 'accumulated_eval_time': 722.3205797672272, 'accumulated_logging_time': 53.68946599960327, 'global_step': 27392, 'preemption_count': 0}), (28000, {'train/accuracy': 0.6583593487739563, 'train/loss': 1.5350534915924072, 'validation/accuracy': 0.6018199920654297, 'validation/loss': 1.8021916151046753, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.4410178661346436, 'test/num_examples': 10000, 'score': 11649.32623744011, 'total_duration': 12452.344131708145, 'accumulated_submission_time': 11649.32623744011, 'accumulated_eval_time': 746.9482498168945, 'accumulated_logging_time': 55.50487017631531, 'global_step': 28000, 'preemption_count': 0})], 'global_step': 28000}
I0420 06:31:11.548591 139634154121024 submission_runner.py:570] Timing: 11649.32623744011
I0420 06:31:11.548650 139634154121024 submission_runner.py:571] ====================
I0420 06:31:11.548764 139634154121024 submission_runner.py:631] Final imagenet_vit score: 11649.32623744011
