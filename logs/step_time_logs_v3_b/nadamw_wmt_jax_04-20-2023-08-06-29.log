I0420 08:06:50.638691 140233344304960 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax.
I0420 08:06:50.708246 140233344304960 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 08:06:51.547249 140233344304960 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0420 08:06:51.548170 140233344304960 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 08:06:51.551980 140233344304960 submission_runner.py:528] Using RNG seed 15855281
I0420 08:06:54.174996 140233344304960 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 08:06:54.175215 140233344304960 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1.
I0420 08:06:54.175474 140233344304960 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/hparams.json.
I0420 08:06:54.296892 140233344304960 submission_runner.py:232] Initializing dataset.
I0420 08:06:54.306041 140233344304960 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:06:54.309708 140233344304960 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:06:54.309821 140233344304960 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:06:54.425382 140233344304960 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:06:56.305398 140233344304960 submission_runner.py:239] Initializing model.
I0420 08:07:08.235943 140233344304960 submission_runner.py:249] Initializing optimizer.
I0420 08:07:09.206523 140233344304960 submission_runner.py:256] Initializing metrics bundle.
I0420 08:07:09.206721 140233344304960 submission_runner.py:273] Initializing checkpoint and logger.
I0420 08:07:09.207665 140233344304960 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1 with prefix checkpoint_
I0420 08:07:09.207927 140233344304960 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 08:07:09.207990 140233344304960 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 08:07:10.124401 140233344304960 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/meta_data_0.json.
I0420 08:07:10.125490 140233344304960 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/flags_0.json.
I0420 08:07:10.129710 140233344304960 submission_runner.py:309] Starting training loop.
I0420 08:07:44.283144 140057096742656 logging_writer.py:48] [0] global_step=0, grad_norm=5.609912872314453, loss=11.033411026000977
I0420 08:07:44.297019 140233344304960 spec.py:298] Evaluating on the training split.
I0420 08:07:44.299562 140233344304960 dataset_info.py:566] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:07:44.302062 140233344304960 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:07:44.302170 140233344304960 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:07:44.332436 140233344304960 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0420 08:07:52.263008 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 08:12:56.664481 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 08:12:56.668353 140233344304960 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:12:56.671595 140233344304960 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:12:56.671704 140233344304960 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:12:56.701752 140233344304960 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:13:03.968285 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 08:18:01.228603 140233344304960 spec.py:326] Evaluating on the test split.
I0420 08:18:01.231033 140233344304960 dataset_info.py:566] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:18:01.233581 140233344304960 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0420 08:18:01.233688 140233344304960 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0420 08:18:01.262665 140233344304960 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0420 08:18:08.149521 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 08:22:59.709133 140233344304960 submission_runner.py:406] Time since start: 949.58s, 	Step: 1, 	{'train/accuracy': 0.000654179893899709, 'train/loss': 11.03438949584961, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.04201889038086, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.034158706665039, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 34.16710019111633, 'total_duration': 949.5793123245239, 'accumulated_submission_time': 34.16710019111633, 'accumulated_eval_time': 915.412011384964, 'accumulated_logging_time': 0}
I0420 08:22:59.727826 140046014912256 logging_writer.py:48] [1] accumulated_eval_time=915.412011, accumulated_logging_time=0, accumulated_submission_time=34.167100, global_step=1, preemption_count=0, score=34.167100, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.034159, test/num_examples=3003, total_duration=949.579312, train/accuracy=0.000654, train/bleu=0.000000, train/loss=11.034389, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.042019, validation/num_examples=3000
I0420 08:23:00.776000 140233344304960 checkpoints.py:356] Saving checkpoint at step: 1
I0420 08:23:04.612039 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_1
I0420 08:23:04.616464 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_1.
I0420 08:23:40.531075 140046023304960 logging_writer.py:48] [100] global_step=100, grad_norm=0.1998254507780075, loss=8.678055763244629
I0420 08:24:16.479732 140046115624704 logging_writer.py:48] [200] global_step=200, grad_norm=0.41245555877685547, loss=8.191524505615234
I0420 08:24:52.409654 140046023304960 logging_writer.py:48] [300] global_step=300, grad_norm=1.0786927938461304, loss=7.639920711517334
I0420 08:25:28.409321 140046115624704 logging_writer.py:48] [400] global_step=400, grad_norm=0.431077241897583, loss=7.274783134460449
I0420 08:26:04.424255 140046023304960 logging_writer.py:48] [500] global_step=500, grad_norm=0.6097431778907776, loss=6.897676944732666
I0420 08:26:40.416922 140046115624704 logging_writer.py:48] [600] global_step=600, grad_norm=0.8550600409507751, loss=6.62746000289917
I0420 08:27:16.393077 140046023304960 logging_writer.py:48] [700] global_step=700, grad_norm=0.6476503610610962, loss=6.3330793380737305
I0420 08:27:52.382039 140046115624704 logging_writer.py:48] [800] global_step=800, grad_norm=0.6078975796699524, loss=6.138942718505859
I0420 08:28:28.380033 140046023304960 logging_writer.py:48] [900] global_step=900, grad_norm=0.8272196054458618, loss=5.898545742034912
I0420 08:29:04.335672 140046115624704 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.683708667755127, loss=5.707281589508057
I0420 08:29:40.304493 140046023304960 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7181958556175232, loss=5.609237194061279
I0420 08:30:16.321822 140046115624704 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7298682332038879, loss=5.356523036956787
I0420 08:30:52.287179 140046023304960 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6497300863265991, loss=5.253881931304932
I0420 08:31:28.285972 140046115624704 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6508438587188721, loss=5.012280464172363
I0420 08:32:04.298028 140046023304960 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9101191163063049, loss=4.877928733825684
I0420 08:32:40.364518 140046115624704 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7390294075012207, loss=4.742735862731934
I0420 08:33:16.389310 140046023304960 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6157726645469666, loss=4.572994709014893
I0420 08:33:52.392903 140046115624704 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9331234693527222, loss=4.560553550720215
I0420 08:34:28.450838 140046023304960 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7022603750228882, loss=4.414177894592285
I0420 08:35:04.458079 140046115624704 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5826794505119324, loss=4.395997047424316
I0420 08:35:40.476956 140046023304960 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6403067708015442, loss=4.324546813964844
I0420 08:36:16.429943 140046115624704 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5307523012161255, loss=4.2787885665893555
I0420 08:36:52.474786 140046023304960 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5810022354125977, loss=4.161046981811523
I0420 08:37:04.790064 140233344304960 spec.py:298] Evaluating on the training split.
I0420 08:37:07.786569 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 08:39:47.576937 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 08:39:50.235499 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 08:42:14.594628 140233344304960 spec.py:326] Evaluating on the test split.
I0420 08:42:17.305830 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 08:44:42.387620 140233344304960 submission_runner.py:406] Time since start: 2252.26s, 	Step: 2336, 	{'train/accuracy': 0.4938582181930542, 'train/loss': 3.1620736122131348, 'train/bleu': 19.578788915280832, 'validation/accuracy': 0.490260511636734, 'validation/loss': 3.1897687911987305, 'validation/bleu': 15.95483346289313, 'validation/num_examples': 3000, 'test/accuracy': 0.4865841865539551, 'test/loss': 3.2703359127044678, 'test/bleu': 14.091131277353982, 'test/num_examples': 3003, 'score': 874.3044197559357, 'total_duration': 2252.2578303813934, 'accumulated_submission_time': 874.3044197559357, 'accumulated_eval_time': 1373.0095331668854, 'accumulated_logging_time': 4.911365270614624}
I0420 08:44:42.395445 140046115624704 logging_writer.py:48] [2336] accumulated_eval_time=1373.009533, accumulated_logging_time=4.911365, accumulated_submission_time=874.304420, global_step=2336, preemption_count=0, score=874.304420, test/accuracy=0.486584, test/bleu=14.091131, test/loss=3.270336, test/num_examples=3003, total_duration=2252.257830, train/accuracy=0.493858, train/bleu=19.578789, train/loss=3.162074, validation/accuracy=0.490261, validation/bleu=15.954833, validation/loss=3.189769, validation/num_examples=3000
I0420 08:44:43.435731 140233344304960 checkpoints.py:356] Saving checkpoint at step: 2336
I0420 08:44:47.115986 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_2336
I0420 08:44:47.120318 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_2336.
I0420 08:45:10.555338 140046023304960 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5620898008346558, loss=4.09086275100708
I0420 08:45:46.558505 140046090446592 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5004940032958984, loss=4.0651068687438965
I0420 08:46:22.532834 140046023304960 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.44738948345184326, loss=3.9230406284332275
I0420 08:46:58.529519 140046090446592 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.47764885425567627, loss=4.0294013023376465
I0420 08:47:34.521814 140046023304960 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.51790851354599, loss=3.941206455230713
I0420 08:48:10.506681 140046090446592 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5406662821769714, loss=3.9127197265625
I0420 08:48:46.492936 140046023304960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.45330101251602173, loss=3.832092761993408
I0420 08:49:22.495030 140046090446592 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.455816388130188, loss=3.805788040161133
I0420 08:49:58.426718 140046023304960 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.4699190557003021, loss=3.799952268600464
I0420 08:50:34.387803 140046090446592 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.616416335105896, loss=3.8615779876708984
I0420 08:51:10.347684 140046023304960 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.41752856969833374, loss=3.877751350402832
I0420 08:51:46.328309 140046090446592 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.398672491312027, loss=3.735355854034424
I0420 08:52:22.276325 140046023304960 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.45941004157066345, loss=3.7712087631225586
I0420 08:52:58.254455 140046090446592 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.3901238739490509, loss=3.683713674545288
I0420 08:53:34.253970 140046023304960 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.38534781336784363, loss=3.6767561435699463
I0420 08:54:10.161152 140046090446592 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8026434183120728, loss=3.7989561557769775
I0420 08:54:46.128774 140046023304960 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5268799066543579, loss=3.638209342956543
I0420 08:55:22.070845 140046090446592 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.3241878151893616, loss=3.5837349891662598
I0420 08:55:58.029077 140046023304960 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.352912575006485, loss=3.661896228790283
I0420 08:56:33.997840 140046090446592 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.4728866219520569, loss=3.568786859512329
I0420 08:57:09.941645 140046023304960 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3996449410915375, loss=3.5915658473968506
I0420 08:57:45.900354 140046090446592 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.4909517467021942, loss=3.5487849712371826
I0420 08:58:21.789680 140046023304960 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.37714579701423645, loss=3.5676662921905518
I0420 08:58:47.392602 140233344304960 spec.py:298] Evaluating on the training split.
I0420 08:58:50.384199 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:01:26.687437 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 09:01:29.332450 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:03:56.826994 140233344304960 spec.py:326] Evaluating on the test split.
I0420 09:03:59.537677 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:06:19.669217 140233344304960 submission_runner.py:406] Time since start: 3549.54s, 	Step: 4673, 	{'train/accuracy': 0.5655911564826965, 'train/loss': 2.4734439849853516, 'train/bleu': 25.964784450724483, 'validation/accuracy': 0.5752315521240234, 'validation/loss': 2.38930606842041, 'validation/bleu': 22.356922669564238, 'validation/num_examples': 3000, 'test/accuracy': 0.5750043988227844, 'test/loss': 2.4071476459503174, 'test/bleu': 20.99551454134878, 'test/num_examples': 3003, 'score': 1714.5421273708344, 'total_duration': 3549.539385318756, 'accumulated_submission_time': 1714.5421273708344, 'accumulated_eval_time': 1825.2860560417175, 'accumulated_logging_time': 9.64766526222229}
I0420 09:06:19.677541 140046090446592 logging_writer.py:48] [4673] accumulated_eval_time=1825.286056, accumulated_logging_time=9.647665, accumulated_submission_time=1714.542127, global_step=4673, preemption_count=0, score=1714.542127, test/accuracy=0.575004, test/bleu=20.995515, test/loss=2.407148, test/num_examples=3003, total_duration=3549.539385, train/accuracy=0.565591, train/bleu=25.964784, train/loss=2.473444, validation/accuracy=0.575232, validation/bleu=22.356923, validation/loss=2.389306, validation/num_examples=3000
I0420 09:06:20.720500 140233344304960 checkpoints.py:356] Saving checkpoint at step: 4673
I0420 09:06:24.412723 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_4673
I0420 09:06:24.416985 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_4673.
I0420 09:06:34.479159 140046023304960 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.38580140471458435, loss=3.5370404720306396
I0420 09:07:10.439669 140046073661184 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.32165226340293884, loss=3.4781675338745117
I0420 09:07:46.396245 140046023304960 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.42771223187446594, loss=3.5632708072662354
I0420 09:08:22.343631 140046073661184 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.29086342453956604, loss=3.426288366317749
I0420 09:08:58.288100 140046023304960 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.33428049087524414, loss=3.462076425552368
I0420 09:09:34.216772 140046073661184 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.2859615981578827, loss=3.5192911624908447
I0420 09:10:10.165267 140046023304960 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.24970048666000366, loss=3.459947347640991
I0420 09:10:46.095043 140046073661184 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.2970515191555023, loss=3.5036299228668213
I0420 09:11:22.062049 140046023304960 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.27172258496284485, loss=3.4740076065063477
I0420 09:11:58.036888 140046073661184 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.3152002692222595, loss=3.4685282707214355
I0420 09:12:33.983732 140046023304960 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.24573738873004913, loss=3.3971283435821533
I0420 09:13:09.866003 140046073661184 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2481757402420044, loss=3.4318954944610596
I0420 09:13:45.812291 140046023304960 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.2919479310512543, loss=3.3895316123962402
I0420 09:14:21.730951 140046073661184 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.2684999704360962, loss=3.3431386947631836
I0420 09:14:57.680059 140046023304960 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.24913614988327026, loss=3.3891305923461914
I0420 09:15:33.620691 140046073661184 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.25235822796821594, loss=3.4025189876556396
I0420 09:16:09.561628 140046023304960 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.23566114902496338, loss=3.355942487716675
I0420 09:16:45.490453 140046073661184 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.24513103067874908, loss=3.4411494731903076
I0420 09:17:21.442648 140046023304960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.2488509714603424, loss=3.304534673690796
I0420 09:17:57.342156 140046073661184 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.20367810130119324, loss=3.2925431728363037
I0420 09:18:33.294977 140046023304960 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.21338091790676117, loss=3.405008316040039
I0420 09:19:09.207706 140046073661184 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.2098332941532135, loss=3.376538038253784
I0420 09:19:45.158685 140046023304960 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.20260508358478546, loss=3.3365535736083984
I0420 09:20:21.094327 140046073661184 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.19283097982406616, loss=3.3410940170288086
I0420 09:20:24.752245 140233344304960 spec.py:298] Evaluating on the training split.
I0420 09:20:27.741528 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:23:30.742740 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 09:23:33.379333 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:25:59.420245 140233344304960 spec.py:326] Evaluating on the test split.
I0420 09:26:02.125217 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:28:14.918574 140233344304960 submission_runner.py:406] Time since start: 4864.79s, 	Step: 7012, 	{'train/accuracy': 0.6015288829803467, 'train/loss': 2.156787872314453, 'train/bleu': 28.79796470922906, 'validation/accuracy': 0.6079403758049011, 'validation/loss': 2.1234517097473145, 'validation/bleu': 24.973602677191664, 'validation/num_examples': 3000, 'test/accuracy': 0.6100749373435974, 'test/loss': 2.1012003421783447, 'test/bleu': 23.37076061352592, 'test/num_examples': 3003, 'score': 2554.8441903591156, 'total_duration': 4864.788790225983, 'accumulated_submission_time': 2554.8441903591156, 'accumulated_eval_time': 2295.4523346424103, 'accumulated_logging_time': 14.398738622665405}
I0420 09:28:14.927272 140046023304960 logging_writer.py:48] [7012] accumulated_eval_time=2295.452335, accumulated_logging_time=14.398739, accumulated_submission_time=2554.844190, global_step=7012, preemption_count=0, score=2554.844190, test/accuracy=0.610075, test/bleu=23.370761, test/loss=2.101200, test/num_examples=3003, total_duration=4864.788790, train/accuracy=0.601529, train/bleu=28.797965, train/loss=2.156788, validation/accuracy=0.607940, validation/bleu=24.973603, validation/loss=2.123452, validation/num_examples=3000
I0420 09:28:15.967166 140233344304960 checkpoints.py:356] Saving checkpoint at step: 7012
I0420 09:28:19.668059 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_7012
I0420 09:28:19.672298 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_7012.
I0420 09:28:51.672412 140046073661184 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.18244877457618713, loss=3.354947566986084
I0420 09:29:27.667460 140046065268480 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.18912525475025177, loss=3.3238208293914795
I0420 09:30:03.617100 140046073661184 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.17191797494888306, loss=3.3022360801696777
I0420 09:30:39.551820 140046065268480 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.17619816958904266, loss=3.3582773208618164
I0420 09:31:15.460183 140046073661184 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.1774877905845642, loss=3.307912588119507
I0420 09:31:51.402460 140046065268480 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.16704049706459045, loss=3.333874464035034
I0420 09:32:27.364615 140046073661184 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.2255610078573227, loss=3.2480924129486084
I0420 09:33:03.308079 140046065268480 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.199219211935997, loss=3.310861825942993
I0420 09:33:39.261959 140046073661184 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.17014020681381226, loss=3.2763848304748535
I0420 09:34:15.198339 140046065268480 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.16475073993206024, loss=3.2646169662475586
I0420 09:34:51.115568 140046073661184 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.19123095273971558, loss=3.2569310665130615
I0420 09:35:27.042426 140046065268480 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.15810026228427887, loss=3.270657777786255
I0420 09:36:02.982984 140046073661184 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.16174103319644928, loss=3.224699020385742
I0420 09:36:38.886337 140046065268480 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.19988574087619781, loss=3.161480665206909
I0420 09:37:14.791517 140046073661184 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1789143830537796, loss=3.2951300144195557
I0420 09:37:50.700354 140046065268480 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.14597535133361816, loss=3.3028080463409424
I0420 09:38:26.588748 140046073661184 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.158024400472641, loss=3.154038429260254
I0420 09:39:02.565430 140046065268480 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.16258366405963898, loss=3.134758234024048
I0420 09:39:38.480540 140046073661184 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.16674020886421204, loss=3.1201717853546143
I0420 09:40:14.412808 140046065268480 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1520126461982727, loss=3.213486671447754
I0420 09:40:50.331649 140046073661184 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.15303152799606323, loss=3.2381324768066406
I0420 09:41:26.289896 140046065268480 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.159307062625885, loss=3.280071496963501
I0420 09:42:02.191625 140046073661184 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.14778858423233032, loss=3.139016628265381
I0420 09:42:19.867027 140233344304960 spec.py:298] Evaluating on the training split.
I0420 09:42:22.848918 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:46:50.430420 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 09:46:53.068425 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:49:29.386689 140233344304960 spec.py:326] Evaluating on the test split.
I0420 09:49:32.083304 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 09:51:51.403390 140233344304960 submission_runner.py:406] Time since start: 6281.27s, 	Step: 9351, 	{'train/accuracy': 0.6065051555633545, 'train/loss': 2.1066224575042725, 'train/bleu': 29.159817413153558, 'validation/accuracy': 0.6268118023872375, 'validation/loss': 1.9603495597839355, 'validation/bleu': 25.803536849039023, 'validation/num_examples': 3000, 'test/accuracy': 0.6319795846939087, 'test/loss': 1.9205944538116455, 'test/bleu': 25.013935694776066, 'test/num_examples': 3003, 'score': 3395.0064322948456, 'total_duration': 6281.273523569107, 'accumulated_submission_time': 3395.0064322948456, 'accumulated_eval_time': 2866.988565683365, 'accumulated_logging_time': 19.155814170837402}
I0420 09:51:51.413350 140046065268480 logging_writer.py:48] [9351] accumulated_eval_time=2866.988566, accumulated_logging_time=19.155814, accumulated_submission_time=3395.006432, global_step=9351, preemption_count=0, score=3395.006432, test/accuracy=0.631980, test/bleu=25.013936, test/loss=1.920594, test/num_examples=3003, total_duration=6281.273524, train/accuracy=0.606505, train/bleu=29.159817, train/loss=2.106622, validation/accuracy=0.626812, validation/bleu=25.803537, validation/loss=1.960350, validation/num_examples=3000
I0420 09:51:52.453346 140233344304960 checkpoints.py:356] Saving checkpoint at step: 9351
I0420 09:51:56.154522 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_9351
I0420 09:51:56.158738 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_9351.
I0420 09:52:14.127430 140046073661184 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.14694823324680328, loss=3.18340802192688
I0420 09:52:50.069940 140045655254784 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.14931142330169678, loss=3.1195366382598877
I0420 09:53:25.989291 140046073661184 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.16703161597251892, loss=3.1961426734924316
I0420 09:54:01.889168 140045655254784 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1813582181930542, loss=3.1680943965911865
I0420 09:54:37.805921 140046073661184 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.15219669044017792, loss=3.329354763031006
I0420 09:55:13.724374 140045655254784 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.1410820335149765, loss=3.1674516201019287
I0420 09:55:49.670665 140046073661184 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.15168029069900513, loss=3.1950292587280273
I0420 09:56:25.572960 140045655254784 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.15753577649593353, loss=3.157487392425537
I0420 09:57:01.480985 140046073661184 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.16806888580322266, loss=3.1755778789520264
I0420 09:57:37.427681 140045655254784 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.15500371158123016, loss=3.1429519653320312
I0420 09:58:13.308582 140046073661184 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18091386556625366, loss=3.0978329181671143
I0420 09:58:49.246431 140045655254784 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.14679285883903503, loss=3.1583375930786133
I0420 09:59:25.150479 140046073661184 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.14516066014766693, loss=3.1680428981781006
I0420 10:00:01.031066 140045655254784 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.15285459160804749, loss=3.1615447998046875
I0420 10:00:36.915592 140046073661184 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.14112341403961182, loss=3.0365655422210693
I0420 10:01:12.822339 140045655254784 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1783217191696167, loss=3.2084131240844727
I0420 10:01:48.745018 140046073661184 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.1407247632741928, loss=3.208554983139038
I0420 10:02:24.651317 140045655254784 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.16456687450408936, loss=3.0587286949157715
I0420 10:03:00.566136 140046073661184 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.15067504346370697, loss=3.0641133785247803
I0420 10:03:36.496542 140045655254784 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.14645415544509888, loss=3.1789801120758057
I0420 10:04:12.386619 140046073661184 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1620979756116867, loss=3.069993019104004
I0420 10:04:48.306115 140045655254784 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1582021415233612, loss=3.115636110305786
I0420 10:05:24.256063 140046073661184 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.169945627450943, loss=3.160949230194092
I0420 10:05:56.329465 140233344304960 spec.py:298] Evaluating on the training split.
I0420 10:05:59.321491 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:09:46.590458 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 10:09:49.244198 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:12:10.687007 140233344304960 spec.py:326] Evaluating on the test split.
I0420 10:12:13.387562 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:14:25.350192 140233344304960 submission_runner.py:406] Time since start: 7635.22s, 	Step: 11691, 	{'train/accuracy': 0.6200766563415527, 'train/loss': 1.9994512796401978, 'train/bleu': 29.954990632316843, 'validation/accuracy': 0.6402648091316223, 'validation/loss': 1.8624681234359741, 'validation/bleu': 26.78654318906322, 'validation/num_examples': 3000, 'test/accuracy': 0.6461797952651978, 'test/loss': 1.815356731414795, 'test/bleu': 25.64648400880459, 'test/num_examples': 3003, 'score': 4235.14500951767, 'total_duration': 7635.220397472382, 'accumulated_submission_time': 4235.14500951767, 'accumulated_eval_time': 3376.0092358589172, 'accumulated_logging_time': 23.914429664611816}
I0420 10:14:25.358979 140045655254784 logging_writer.py:48] [11691] accumulated_eval_time=3376.009236, accumulated_logging_time=23.914430, accumulated_submission_time=4235.145010, global_step=11691, preemption_count=0, score=4235.145010, test/accuracy=0.646180, test/bleu=25.646484, test/loss=1.815357, test/num_examples=3003, total_duration=7635.220397, train/accuracy=0.620077, train/bleu=29.954991, train/loss=1.999451, validation/accuracy=0.640265, validation/bleu=26.786543, validation/loss=1.862468, validation/num_examples=3000
I0420 10:14:26.393287 140233344304960 checkpoints.py:356] Saving checkpoint at step: 11691
I0420 10:14:30.078479 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_11691
I0420 10:14:30.082724 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_11691.
I0420 10:14:33.680799 140046073661184 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.14933373034000397, loss=3.1350371837615967
I0420 10:15:09.638518 140045646862080 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.17571622133255005, loss=3.0868265628814697
I0420 10:15:45.589422 140046073661184 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.20247633755207062, loss=3.1276028156280518
I0420 10:16:21.520903 140045646862080 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.14534148573875427, loss=3.1258082389831543
I0420 10:16:57.444113 140046073661184 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.14216403663158417, loss=3.1107594966888428
I0420 10:17:33.384539 140045646862080 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1473603993654251, loss=3.1188223361968994
I0420 10:18:09.344820 140046073661184 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.17867442965507507, loss=3.1259868144989014
I0420 10:18:45.292577 140045646862080 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.15102951228618622, loss=3.100743293762207
I0420 10:19:21.193526 140046073661184 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.15409401059150696, loss=3.0638327598571777
I0420 10:19:57.084667 140045646862080 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.15006369352340698, loss=3.180893659591675
I0420 10:20:33.031559 140046073661184 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1572868674993515, loss=2.9562079906463623
I0420 10:21:08.962956 140045646862080 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.148668110370636, loss=3.074350595474243
I0420 10:21:44.928436 140046073661184 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.1904936581850052, loss=3.02299165725708
I0420 10:22:20.848783 140045646862080 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.1432858407497406, loss=3.080247640609741
I0420 10:22:56.776173 140046073661184 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.17274782061576843, loss=3.105022430419922
I0420 10:23:32.711436 140045646862080 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.19501365721225739, loss=3.07660174369812
I0420 10:24:08.609009 140046073661184 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.17694291472434998, loss=3.0772881507873535
I0420 10:24:44.553223 140045646862080 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1625288724899292, loss=3.1257286071777344
I0420 10:25:20.492158 140046073661184 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17213042080402374, loss=3.1199727058410645
I0420 10:25:56.413169 140045646862080 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.20726074278354645, loss=3.1553397178649902
I0420 10:26:32.317520 140046073661184 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.18730777502059937, loss=3.086446523666382
I0420 10:27:08.265988 140045646862080 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.20121951401233673, loss=3.0149004459381104
I0420 10:27:44.223080 140046073661184 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.15266524255275726, loss=3.0529773235321045
I0420 10:28:20.133627 140045646862080 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1967577338218689, loss=3.017192840576172
I0420 10:28:30.299608 140233344304960 spec.py:298] Evaluating on the training split.
I0420 10:28:33.279621 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:31:19.803530 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 10:31:22.470796 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:33:53.970422 140233344304960 spec.py:326] Evaluating on the test split.
I0420 10:33:56.677026 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:36:08.642941 140233344304960 submission_runner.py:406] Time since start: 8938.51s, 	Step: 14030, 	{'train/accuracy': 0.6319608092308044, 'train/loss': 1.8993852138519287, 'train/bleu': 30.900683487243228, 'validation/accuracy': 0.648423433303833, 'validation/loss': 1.7948167324066162, 'validation/bleu': 27.689290241174735, 'validation/num_examples': 3000, 'test/accuracy': 0.6574748754501343, 'test/loss': 1.7408850193023682, 'test/bleu': 26.802222416611276, 'test/num_examples': 3003, 'score': 5075.329639434814, 'total_duration': 8938.513160705566, 'accumulated_submission_time': 5075.329639434814, 'accumulated_eval_time': 3834.352524280548, 'accumulated_logging_time': 28.650205612182617}
I0420 10:36:08.652061 140046073661184 logging_writer.py:48] [14030] accumulated_eval_time=3834.352524, accumulated_logging_time=28.650206, accumulated_submission_time=5075.329639, global_step=14030, preemption_count=0, score=5075.329639, test/accuracy=0.657475, test/bleu=26.802222, test/loss=1.740885, test/num_examples=3003, total_duration=8938.513161, train/accuracy=0.631961, train/bleu=30.900683, train/loss=1.899385, validation/accuracy=0.648423, validation/bleu=27.689290, validation/loss=1.794817, validation/num_examples=3000
I0420 10:36:09.695324 140233344304960 checkpoints.py:356] Saving checkpoint at step: 14030
I0420 10:36:13.358588 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_14030
I0420 10:36:13.362988 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_14030.
I0420 10:36:38.901658 140045646862080 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.1669187694787979, loss=3.053063154220581
I0420 10:37:14.818784 140045638469376 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.21754805743694305, loss=3.0226926803588867
I0420 10:37:50.757834 140045646862080 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.15721920132637024, loss=2.974135398864746
I0420 10:38:26.690883 140045638469376 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.19169694185256958, loss=2.99784779548645
I0420 10:39:02.653274 140045646862080 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.14502133429050446, loss=2.994344472885132
I0420 10:39:38.575712 140045638469376 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.17073073983192444, loss=3.0146453380584717
I0420 10:40:14.498073 140045646862080 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.15690846741199493, loss=3.018876791000366
I0420 10:40:50.441090 140045638469376 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.19333775341510773, loss=3.062009334564209
I0420 10:41:26.362705 140045646862080 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.17815862596035004, loss=2.9593405723571777
I0420 10:42:02.298619 140045638469376 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.15866254270076752, loss=3.1428210735321045
I0420 10:42:38.200550 140045646862080 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.17607952654361725, loss=3.1247284412384033
I0420 10:43:14.154811 140045638469376 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.1495598703622818, loss=3.0272250175476074
I0420 10:43:50.109719 140045646862080 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.1601257622241974, loss=3.0233030319213867
I0420 10:44:26.037754 140045638469376 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.16043850779533386, loss=3.1295509338378906
I0420 10:45:01.955487 140045646862080 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18311206996440887, loss=2.977858543395996
I0420 10:45:37.871455 140045638469376 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.17298603057861328, loss=3.0756773948669434
I0420 10:46:13.788058 140045646862080 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.21708038449287415, loss=3.0469868183135986
I0420 10:46:49.656596 140045638469376 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.1721055656671524, loss=2.994235038757324
I0420 10:47:25.582965 140045646862080 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1629791408777237, loss=2.9325404167175293
I0420 10:48:01.540068 140045638469376 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1828993707895279, loss=3.078749179840088
I0420 10:48:37.471065 140045646862080 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.25945934653282166, loss=2.9726972579956055
I0420 10:49:13.371770 140045638469376 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.1571895331144333, loss=2.9952430725097656
I0420 10:49:49.324267 140045646862080 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.17303480207920074, loss=2.9899723529815674
I0420 10:50:13.457535 140233344304960 spec.py:298] Evaluating on the training split.
I0420 10:50:16.433182 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:53:17.943290 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 10:53:20.578670 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:55:38.587230 140233344304960 spec.py:326] Evaluating on the test split.
I0420 10:55:41.294096 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 10:57:53.478580 140233344304960 submission_runner.py:406] Time since start: 10243.35s, 	Step: 16369, 	{'train/accuracy': 0.6387855410575867, 'train/loss': 1.8642685413360596, 'train/bleu': 31.317779041253367, 'validation/accuracy': 0.6536682844161987, 'validation/loss': 1.7516402006149292, 'validation/bleu': 27.854721517937474, 'validation/num_examples': 3000, 'test/accuracy': 0.6626692414283752, 'test/loss': 1.6914116144180298, 'test/bleu': 27.325665235114343, 'test/num_examples': 3003, 'score': 5915.3916015625, 'total_duration': 10243.348803043365, 'accumulated_submission_time': 5915.3916015625, 'accumulated_eval_time': 4294.373533964157, 'accumulated_logging_time': 33.37369775772095}
I0420 10:57:53.487498 140045638469376 logging_writer.py:48] [16369] accumulated_eval_time=4294.373534, accumulated_logging_time=33.373698, accumulated_submission_time=5915.391602, global_step=16369, preemption_count=0, score=5915.391602, test/accuracy=0.662669, test/bleu=27.325665, test/loss=1.691412, test/num_examples=3003, total_duration=10243.348803, train/accuracy=0.638786, train/bleu=31.317779, train/loss=1.864269, validation/accuracy=0.653668, validation/bleu=27.854722, validation/loss=1.751640, validation/num_examples=3000
I0420 10:57:54.526996 140233344304960 checkpoints.py:356] Saving checkpoint at step: 16369
I0420 10:57:58.222247 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_16369
I0420 10:57:58.226597 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_16369.
I0420 10:58:09.735432 140045646862080 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.16928359866142273, loss=3.002220392227173
I0420 10:58:45.677497 140045630076672 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.1721440702676773, loss=3.085043430328369
I0420 10:59:21.614104 140045646862080 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.1628057360649109, loss=2.917551279067993
I0420 10:59:57.515851 140045630076672 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.26235243678092957, loss=3.0134594440460205
I0420 11:00:33.392886 140045646862080 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.24625128507614136, loss=3.06784987449646
I0420 11:01:09.284126 140045630076672 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.17427675426006317, loss=2.980587959289551
I0420 11:01:45.209209 140045646862080 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17084328830242157, loss=3.038533926010132
I0420 11:02:21.117949 140045630076672 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.16652877628803253, loss=3.0000908374786377
I0420 11:02:57.038696 140045646862080 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.21663938462734222, loss=3.0197319984436035
I0420 11:03:32.984695 140045630076672 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.25419166684150696, loss=3.081259250640869
I0420 11:04:08.878285 140045646862080 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.1987636685371399, loss=3.0098466873168945
I0420 11:04:44.776446 140045630076672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.18956314027309418, loss=3.0585265159606934
I0420 11:05:20.724721 140045646862080 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.16690725088119507, loss=2.969149112701416
I0420 11:05:56.612446 140045630076672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.20874899625778198, loss=3.0880239009857178
I0420 11:06:32.569403 140045646862080 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2888341546058655, loss=3.003283977508545
I0420 11:07:08.469670 140045630076672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1789531260728836, loss=2.9449703693389893
I0420 11:07:44.413384 140045646862080 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.17465338110923767, loss=2.975414514541626
I0420 11:08:20.361486 140045630076672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.165655255317688, loss=3.029648542404175
I0420 11:08:56.273118 140045646862080 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1689019650220871, loss=2.975341558456421
I0420 11:09:32.187247 140045630076672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.18334119021892548, loss=2.9660751819610596
I0420 11:10:08.131535 140045646862080 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.17449317872524261, loss=2.9165518283843994
I0420 11:10:44.042698 140045630076672 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.24265088140964508, loss=2.9431724548339844
I0420 11:11:19.941437 140045646862080 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1801682710647583, loss=2.903043508529663
I0420 11:11:55.924674 140045630076672 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.18857064843177795, loss=2.9540188312530518
I0420 11:11:58.511992 140233344304960 spec.py:298] Evaluating on the training split.
I0420 11:12:01.506829 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 11:15:10.737458 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 11:15:13.393516 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 11:17:31.766652 140233344304960 spec.py:326] Evaluating on the test split.
I0420 11:17:34.471077 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 11:19:42.864499 140233344304960 submission_runner.py:406] Time since start: 11552.73s, 	Step: 18709, 	{'train/accuracy': 0.6407297253608704, 'train/loss': 1.8476550579071045, 'train/bleu': 31.42979299089432, 'validation/accuracy': 0.6589626669883728, 'validation/loss': 1.7180213928222656, 'validation/bleu': 28.34567620200068, 'validation/num_examples': 3000, 'test/accuracy': 0.6699088215827942, 'test/loss': 1.6498134136199951, 'test/bleu': 27.53444328395565, 'test/num_examples': 3003, 'score': 6755.64435505867, 'total_duration': 11552.734676837921, 'accumulated_submission_time': 6755.64435505867, 'accumulated_eval_time': 4758.7259521484375, 'accumulated_logging_time': 38.12495946884155}
I0420 11:19:42.874082 140045646862080 logging_writer.py:48] [18709] accumulated_eval_time=4758.725952, accumulated_logging_time=38.124959, accumulated_submission_time=6755.644355, global_step=18709, preemption_count=0, score=6755.644355, test/accuracy=0.669909, test/bleu=27.534443, test/loss=1.649813, test/num_examples=3003, total_duration=11552.734677, train/accuracy=0.640730, train/bleu=31.429793, train/loss=1.847655, validation/accuracy=0.658963, validation/bleu=28.345676, validation/loss=1.718021, validation/num_examples=3000
I0420 11:19:43.923490 140233344304960 checkpoints.py:356] Saving checkpoint at step: 18709
I0420 11:19:47.628027 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_18709
I0420 11:19:47.632232 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_18709.
I0420 11:20:20.664069 140045630076672 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.1733158826828003, loss=3.029954195022583
I0420 11:20:56.611820 140045621683968 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.1682131141424179, loss=2.9969704151153564
I0420 11:21:32.518624 140045630076672 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.1879691481590271, loss=2.9125399589538574
I0420 11:22:08.427868 140045621683968 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.1716432273387909, loss=2.9193170070648193
I0420 11:22:44.340981 140045630076672 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.1829429268836975, loss=2.960707664489746
I0420 11:23:20.286760 140045621683968 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.1687624752521515, loss=2.9050869941711426
I0420 11:23:56.181044 140045630076672 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.23680949211120605, loss=2.925992012023926
I0420 11:24:32.111511 140045621683968 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.17893049120903015, loss=2.9608869552612305
I0420 11:25:08.046780 140045630076672 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.18074144423007965, loss=2.8896727561950684
I0420 11:25:43.944052 140045621683968 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.19029952585697174, loss=2.975801706314087
I0420 11:26:19.896864 140045630076672 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.23453286290168762, loss=2.9343199729919434
I0420 11:26:55.840196 140045621683968 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.18140433728694916, loss=2.963738203048706
I0420 11:27:31.124548 140233344304960 spec.py:298] Evaluating on the training split.
I0420 11:27:34.119878 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 11:30:23.454672 140233344304960 spec.py:310] Evaluating on the validation split.
I0420 11:30:26.105899 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 11:32:51.004111 140233344304960 spec.py:326] Evaluating on the test split.
I0420 11:32:53.709602 140233344304960 workload.py:179] Translating evaluation dataset.
I0420 11:35:09.774031 140233344304960 submission_runner.py:406] Time since start: 12479.64s, 	Step: 20000, 	{'train/accuracy': 0.6518198251724243, 'train/loss': 1.7600075006484985, 'train/bleu': 32.51973853924899, 'validation/accuracy': 0.6620996594429016, 'validation/loss': 1.6969603300094604, 'validation/bleu': 28.765707639714858, 'validation/num_examples': 3000, 'test/accuracy': 0.671221911907196, 'test/loss': 1.6319975852966309, 'test/bleu': 27.895292460172936, 'test/num_examples': 3003, 'score': 7219.116750717163, 'total_duration': 12479.644171476364, 'accumulated_submission_time': 7219.116750717163, 'accumulated_eval_time': 5217.375326633453, 'accumulated_logging_time': 42.896119356155396}
I0420 11:35:09.783671 140045630076672 logging_writer.py:48] [20000] accumulated_eval_time=5217.375327, accumulated_logging_time=42.896119, accumulated_submission_time=7219.116751, global_step=20000, preemption_count=0, score=7219.116751, test/accuracy=0.671222, test/bleu=27.895292, test/loss=1.631998, test/num_examples=3003, total_duration=12479.644171, train/accuracy=0.651820, train/bleu=32.519739, train/loss=1.760008, validation/accuracy=0.662100, validation/bleu=28.765708, validation/loss=1.696960, validation/num_examples=3000
I0420 11:35:10.822752 140233344304960 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 11:35:14.553735 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_20000
I0420 11:35:14.557985 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_20000.
I0420 11:35:14.569008 140045621683968 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=7219.116751
I0420 11:35:15.136744 140233344304960 checkpoints.py:356] Saving checkpoint at step: 20000
I0420 11:35:20.692886 140233344304960 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_20000
I0420 11:35:20.697039 140233344304960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/wmt_jax/trial_1/checkpoint_20000.
I0420 11:35:20.768987 140233344304960 submission_runner.py:567] Tuning trial 1/1
I0420 11:35:20.769186 140233344304960 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 11:35:20.770386 140233344304960 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000654179893899709, 'train/loss': 11.03438949584961, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.04201889038086, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.034158706665039, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 34.16710019111633, 'total_duration': 949.5793123245239, 'accumulated_submission_time': 34.16710019111633, 'accumulated_eval_time': 915.412011384964, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2336, {'train/accuracy': 0.4938582181930542, 'train/loss': 3.1620736122131348, 'train/bleu': 19.578788915280832, 'validation/accuracy': 0.490260511636734, 'validation/loss': 3.1897687911987305, 'validation/bleu': 15.95483346289313, 'validation/num_examples': 3000, 'test/accuracy': 0.4865841865539551, 'test/loss': 3.2703359127044678, 'test/bleu': 14.091131277353982, 'test/num_examples': 3003, 'score': 874.3044197559357, 'total_duration': 2252.2578303813934, 'accumulated_submission_time': 874.3044197559357, 'accumulated_eval_time': 1373.0095331668854, 'accumulated_logging_time': 4.911365270614624, 'global_step': 2336, 'preemption_count': 0}), (4673, {'train/accuracy': 0.5655911564826965, 'train/loss': 2.4734439849853516, 'train/bleu': 25.964784450724483, 'validation/accuracy': 0.5752315521240234, 'validation/loss': 2.38930606842041, 'validation/bleu': 22.356922669564238, 'validation/num_examples': 3000, 'test/accuracy': 0.5750043988227844, 'test/loss': 2.4071476459503174, 'test/bleu': 20.99551454134878, 'test/num_examples': 3003, 'score': 1714.5421273708344, 'total_duration': 3549.539385318756, 'accumulated_submission_time': 1714.5421273708344, 'accumulated_eval_time': 1825.2860560417175, 'accumulated_logging_time': 9.64766526222229, 'global_step': 4673, 'preemption_count': 0}), (7012, {'train/accuracy': 0.6015288829803467, 'train/loss': 2.156787872314453, 'train/bleu': 28.79796470922906, 'validation/accuracy': 0.6079403758049011, 'validation/loss': 2.1234517097473145, 'validation/bleu': 24.973602677191664, 'validation/num_examples': 3000, 'test/accuracy': 0.6100749373435974, 'test/loss': 2.1012003421783447, 'test/bleu': 23.37076061352592, 'test/num_examples': 3003, 'score': 2554.8441903591156, 'total_duration': 4864.788790225983, 'accumulated_submission_time': 2554.8441903591156, 'accumulated_eval_time': 2295.4523346424103, 'accumulated_logging_time': 14.398738622665405, 'global_step': 7012, 'preemption_count': 0}), (9351, {'train/accuracy': 0.6065051555633545, 'train/loss': 2.1066224575042725, 'train/bleu': 29.159817413153558, 'validation/accuracy': 0.6268118023872375, 'validation/loss': 1.9603495597839355, 'validation/bleu': 25.803536849039023, 'validation/num_examples': 3000, 'test/accuracy': 0.6319795846939087, 'test/loss': 1.9205944538116455, 'test/bleu': 25.013935694776066, 'test/num_examples': 3003, 'score': 3395.0064322948456, 'total_duration': 6281.273523569107, 'accumulated_submission_time': 3395.0064322948456, 'accumulated_eval_time': 2866.988565683365, 'accumulated_logging_time': 19.155814170837402, 'global_step': 9351, 'preemption_count': 0}), (11691, {'train/accuracy': 0.6200766563415527, 'train/loss': 1.9994512796401978, 'train/bleu': 29.954990632316843, 'validation/accuracy': 0.6402648091316223, 'validation/loss': 1.8624681234359741, 'validation/bleu': 26.78654318906322, 'validation/num_examples': 3000, 'test/accuracy': 0.6461797952651978, 'test/loss': 1.815356731414795, 'test/bleu': 25.64648400880459, 'test/num_examples': 3003, 'score': 4235.14500951767, 'total_duration': 7635.220397472382, 'accumulated_submission_time': 4235.14500951767, 'accumulated_eval_time': 3376.0092358589172, 'accumulated_logging_time': 23.914429664611816, 'global_step': 11691, 'preemption_count': 0}), (14030, {'train/accuracy': 0.6319608092308044, 'train/loss': 1.8993852138519287, 'train/bleu': 30.900683487243228, 'validation/accuracy': 0.648423433303833, 'validation/loss': 1.7948167324066162, 'validation/bleu': 27.689290241174735, 'validation/num_examples': 3000, 'test/accuracy': 0.6574748754501343, 'test/loss': 1.7408850193023682, 'test/bleu': 26.802222416611276, 'test/num_examples': 3003, 'score': 5075.329639434814, 'total_duration': 8938.513160705566, 'accumulated_submission_time': 5075.329639434814, 'accumulated_eval_time': 3834.352524280548, 'accumulated_logging_time': 28.650205612182617, 'global_step': 14030, 'preemption_count': 0}), (16369, {'train/accuracy': 0.6387855410575867, 'train/loss': 1.8642685413360596, 'train/bleu': 31.317779041253367, 'validation/accuracy': 0.6536682844161987, 'validation/loss': 1.7516402006149292, 'validation/bleu': 27.854721517937474, 'validation/num_examples': 3000, 'test/accuracy': 0.6626692414283752, 'test/loss': 1.6914116144180298, 'test/bleu': 27.325665235114343, 'test/num_examples': 3003, 'score': 5915.3916015625, 'total_duration': 10243.348803043365, 'accumulated_submission_time': 5915.3916015625, 'accumulated_eval_time': 4294.373533964157, 'accumulated_logging_time': 33.37369775772095, 'global_step': 16369, 'preemption_count': 0}), (18709, {'train/accuracy': 0.6407297253608704, 'train/loss': 1.8476550579071045, 'train/bleu': 31.42979299089432, 'validation/accuracy': 0.6589626669883728, 'validation/loss': 1.7180213928222656, 'validation/bleu': 28.34567620200068, 'validation/num_examples': 3000, 'test/accuracy': 0.6699088215827942, 'test/loss': 1.6498134136199951, 'test/bleu': 27.53444328395565, 'test/num_examples': 3003, 'score': 6755.64435505867, 'total_duration': 11552.734676837921, 'accumulated_submission_time': 6755.64435505867, 'accumulated_eval_time': 4758.7259521484375, 'accumulated_logging_time': 38.12495946884155, 'global_step': 18709, 'preemption_count': 0}), (20000, {'train/accuracy': 0.6518198251724243, 'train/loss': 1.7600075006484985, 'train/bleu': 32.51973853924899, 'validation/accuracy': 0.6620996594429016, 'validation/loss': 1.6969603300094604, 'validation/bleu': 28.765707639714858, 'validation/num_examples': 3000, 'test/accuracy': 0.671221911907196, 'test/loss': 1.6319975852966309, 'test/bleu': 27.895292460172936, 'test/num_examples': 3003, 'score': 7219.116750717163, 'total_duration': 12479.644171476364, 'accumulated_submission_time': 7219.116750717163, 'accumulated_eval_time': 5217.375326633453, 'accumulated_logging_time': 42.896119356155396, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0420 11:35:20.770501 140233344304960 submission_runner.py:570] Timing: 7219.116750717163
I0420 11:35:20.770542 140233344304960 submission_runner.py:571] ====================
I0420 11:35:20.770638 140233344304960 submission_runner.py:631] Final wmt score: 7219.116750717163
