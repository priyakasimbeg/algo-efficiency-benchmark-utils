I0426 04:44:23.176493 140662253995840 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax.
I0426 04:44:23.250159 140662253995840 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0426 04:44:24.123409 140662253995840 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0426 04:44:24.123992 140662253995840 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0426 04:44:24.127784 140662253995840 submission_runner.py:528] Using RNG seed 3343109423
I0426 04:44:26.736535 140662253995840 submission_runner.py:537] --- Tuning run 1/1 ---
I0426 04:44:26.736730 140662253995840 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1.
I0426 04:44:26.736968 140662253995840 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/hparams.json.
I0426 04:44:26.859530 140662253995840 submission_runner.py:232] Initializing dataset.
I0426 04:44:26.859739 140662253995840 submission_runner.py:239] Initializing model.
I0426 04:44:32.609282 140662253995840 submission_runner.py:249] Initializing optimizer.
I0426 04:44:33.282406 140662253995840 submission_runner.py:256] Initializing metrics bundle.
I0426 04:44:33.282593 140662253995840 submission_runner.py:273] Initializing checkpoint and logger.
I0426 04:44:33.283509 140662253995840 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0426 04:44:33.283767 140662253995840 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0426 04:44:33.283830 140662253995840 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0426 04:44:33.984121 140662253995840 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0426 04:44:33.985050 140662253995840 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/flags_0.json.
I0426 04:44:33.992038 140662253995840 submission_runner.py:309] Starting training loop.
I0426 04:44:34.202640 140662253995840 input_pipeline.py:20] Loading split = train-clean-100
I0426 04:44:34.235080 140662253995840 input_pipeline.py:20] Loading split = train-clean-360
I0426 04:44:34.556180 140662253995840 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0426 04:45:36.131443 140487195494144 logging_writer.py:48] [0] global_step=0, grad_norm=49.16309356689453, loss=31.53667449951172
I0426 04:45:36.156427 140662253995840 spec.py:298] Evaluating on the training split.
I0426 04:45:36.256731 140662253995840 input_pipeline.py:20] Loading split = train-clean-100
I0426 04:45:36.283208 140662253995840 input_pipeline.py:20] Loading split = train-clean-360
I0426 04:45:36.575163 140662253995840 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0426 04:46:29.695922 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 04:46:29.758172 140662253995840 input_pipeline.py:20] Loading split = dev-clean
I0426 04:46:29.762969 140662253995840 input_pipeline.py:20] Loading split = dev-other
I0426 04:47:10.879878 140662253995840 spec.py:326] Evaluating on the test split.
I0426 04:47:10.942471 140662253995840 input_pipeline.py:20] Loading split = test-clean
I0426 04:47:40.557091 140662253995840 submission_runner.py:406] Time since start: 186.56s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.15589, dtype=float32), 'train/wer': 1.247829165191679, 'validation/ctc_loss': DeviceArray(30.062668, dtype=float32), 'validation/wer': 1.1601269669750793, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.051409, dtype=float32), 'test/wer': 1.1669814961509557, 'test/num_examples': 2472, 'score': 62.164222717285156, 'total_duration': 186.56355381011963, 'accumulated_submission_time': 62.164222717285156, 'accumulated_eval_time': 124.3991870880127, 'accumulated_logging_time': 0}
I0426 04:47:40.580645 140483313530624 logging_writer.py:48] [1] accumulated_eval_time=124.399187, accumulated_logging_time=0, accumulated_submission_time=62.164223, global_step=1, preemption_count=0, score=62.164223, test/ctc_loss=30.051408767700195, test/num_examples=2472, test/wer=1.166981, total_duration=186.563554, train/ctc_loss=31.1558895111084, train/wer=1.247829, validation/ctc_loss=30.062667846679688, validation/num_examples=5348, validation/wer=1.160127
I0426 04:47:40.770646 140662253995840 checkpoints.py:356] Saving checkpoint at step: 1
I0426 04:47:41.384367 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_1
I0426 04:47:41.385335 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_1.
I0426 04:49:10.629280 140487733356288 logging_writer.py:48] [100] global_step=100, grad_norm=61.831886291503906, loss=23.12375259399414
I0426 04:50:25.770270 140487741748992 logging_writer.py:48] [200] global_step=200, grad_norm=0.0, loss=1779.24072265625
I0426 04:51:39.250894 140487733356288 logging_writer.py:48] [300] global_step=300, grad_norm=0.0, loss=1826.4638671875
I0426 04:52:52.795023 140487741748992 logging_writer.py:48] [400] global_step=400, grad_norm=0.0, loss=1834.9735107421875
I0426 04:54:06.400974 140487733356288 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1817.5162353515625
I0426 04:55:19.888803 140487741748992 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1814.038818359375
I0426 04:56:38.153415 140487733356288 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1805.84912109375
I0426 04:57:57.751146 140487741748992 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1856.8011474609375
I0426 04:59:17.782461 140487733356288 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1864.3736572265625
I0426 05:00:39.766435 140487741748992 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1768.1805419921875
I0426 05:01:58.452785 140489858897664 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1855.32080078125
I0426 05:03:12.214332 140489850504960 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1760.8829345703125
I0426 05:04:25.851647 140489858897664 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1789.564208984375
I0426 05:05:39.405968 140489850504960 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1828.5511474609375
I0426 05:06:52.993330 140489858897664 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1880.3963623046875
I0426 05:08:06.556233 140489850504960 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1836.4215087890625
I0426 05:09:26.855390 140489858897664 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1801.655029296875
I0426 05:10:50.047901 140489850504960 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1799.2490234375
I0426 05:12:12.558609 140489858897664 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1766.4722900390625
I0426 05:13:35.466760 140489850504960 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1815.0677490234375
I0426 05:14:57.394316 140489203537664 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1853.5743408203125
I0426 05:16:10.915317 140489195144960 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1845.29052734375
I0426 05:17:24.428718 140489203537664 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1833.79052734375
I0426 05:18:37.962704 140489195144960 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1763.1873779296875
I0426 05:19:51.476924 140489203537664 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1771.4840087890625
I0426 05:21:05.016463 140489195144960 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1752.6849365234375
I0426 05:22:25.613378 140489203537664 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1844.2271728515625
I0426 05:23:46.955168 140489195144960 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1818.5491943359375
I0426 05:25:07.431476 140489203537664 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1849.9578857421875
I0426 05:26:29.383402 140489195144960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1817.000244140625
I0426 05:27:42.022093 140662253995840 spec.py:298] Evaluating on the training split.
I0426 05:28:08.919529 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 05:28:44.610646 140662253995840 spec.py:326] Evaluating on the test split.
I0426 05:29:02.579498 140662253995840 submission_runner.py:406] Time since start: 2668.58s, 	Step: 3090, 	{'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2462.755706548691, 'total_duration': 2668.584280729294, 'accumulated_submission_time': 2462.755706548691, 'accumulated_eval_time': 204.9535026550293, 'accumulated_logging_time': 0.834252119064331}
I0426 05:29:02.600204 140489275217664 logging_writer.py:48] [3090] accumulated_eval_time=204.953503, accumulated_logging_time=0.834252, accumulated_submission_time=2462.755707, global_step=3090, preemption_count=0, score=2462.755707, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2668.584281, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 05:29:02.795023 140662253995840 checkpoints.py:356] Saving checkpoint at step: 3090
I0426 05:29:03.788605 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_3090
I0426 05:29:03.810004 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_3090.
I0426 05:29:15.548411 140488619857664 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1765.2542724609375
I0426 05:30:29.010944 140488611464960 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1785.943603515625
I0426 05:31:42.502445 140488619857664 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1830.512451171875
I0426 05:32:55.978114 140488611464960 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1880.1201171875
I0426 05:34:09.460632 140488619857664 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1851.4295654296875
I0426 05:35:22.998991 140488611464960 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1787.065673828125
I0426 05:36:43.279309 140488619857664 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1800.767822265625
I0426 05:38:07.038290 140488611464960 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1797.2279052734375
I0426 05:39:29.635017 140488619857664 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1825.6822509765625
I0426 05:40:52.267379 140488611464960 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1766.716064453125
I0426 05:42:14.201117 140488619857664 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1825.5521240234375
I0426 05:43:31.859253 140488619857664 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1749.9293212890625
I0426 05:44:45.427626 140488611464960 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1797.6065673828125
I0426 05:45:59.002043 140488619857664 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1830.512451171875
I0426 05:47:12.692613 140488611464960 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1851.5634765625
I0426 05:48:26.252526 140488619857664 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1873.6524658203125
I0426 05:49:39.768113 140488611464960 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1813.396484375
I0426 05:50:56.490041 140488619857664 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1873.515380859375
I0426 05:52:17.273126 140488611464960 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1843.0321044921875
I0426 05:53:36.907288 140488619857664 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1807.25146484375
I0426 05:54:56.794716 140488611464960 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1815.196533203125
I0426 05:56:16.702190 140488619857664 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1848.75537109375
I0426 05:57:30.262739 140488611464960 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1792.195068359375
I0426 05:58:43.830385 140488619857664 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1831.5601806640625
I0426 05:59:57.368047 140488611464960 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1785.1962890625
I0426 06:01:10.876794 140488619857664 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1860.5797119140625
I0426 06:02:24.513642 140488611464960 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1805.339599609375
I0426 06:03:43.443374 140488619857664 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1792.57177734375
I0426 06:05:06.834450 140488611464960 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1770.6263427734375
I0426 06:06:32.970330 140488619857664 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1756.65380859375
I0426 06:07:57.850858 140488611464960 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1826.9852294921875
I0426 06:09:04.077084 140662253995840 spec.py:298] Evaluating on the training split.
I0426 06:09:30.473612 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 06:10:06.440637 140662253995840 spec.py:326] Evaluating on the test split.
I0426 06:10:25.205826 140662253995840 submission_runner.py:406] Time since start: 5151.21s, 	Step: 6177, 	{'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4862.974407196045, 'total_duration': 5151.210730552673, 'accumulated_submission_time': 4862.974407196045, 'accumulated_eval_time': 286.0792419910431, 'accumulated_logging_time': 2.0744779109954834}
I0426 06:10:25.227399 140488619857664 logging_writer.py:48] [6177] accumulated_eval_time=286.079242, accumulated_logging_time=2.074478, accumulated_submission_time=4862.974407, global_step=6177, preemption_count=0, score=4862.974407, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5151.210731, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 06:10:25.419950 140662253995840 checkpoints.py:356] Saving checkpoint at step: 6177
I0426 06:10:26.411057 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_6177
I0426 06:10:26.432631 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_6177.
I0426 06:10:47.425338 140487534417664 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1781.09765625
I0426 06:12:00.931434 140487526024960 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1819.8419189453125
I0426 06:13:14.496592 140487534417664 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1826.8548583984375
I0426 06:14:28.036442 140487526024960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1919.306884765625
I0426 06:15:42.726701 140487534417664 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1815.582763671875
I0426 06:17:05.570546 140487526024960 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1878.740234375
I0426 06:18:28.066741 140487534417664 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1752.445068359375
I0426 06:19:49.847065 140487526024960 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1913.99755859375
I0426 06:21:10.592408 140487534417664 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1831.4290771484375
I0426 06:22:31.496829 140487526024960 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1763.673095703125
I0426 06:23:52.351568 140487534417664 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1785.4453125
I0426 06:25:09.449991 140487534417664 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1838.927978515625
I0426 06:26:23.016683 140487526024960 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1855.85888671875
I0426 06:27:36.615519 140487534417664 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1759.3096923828125
I0426 06:28:50.294170 140487526024960 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1776.6475830078125
I0426 06:30:04.000501 140487534417664 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1777.5111083984375
I0426 06:31:19.036502 140487526024960 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1814.681884765625
I0426 06:32:40.420169 140487534417664 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1848.3548583984375
I0426 06:34:01.970763 140487526024960 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1820.747802734375
I0426 06:35:22.470729 140487534417664 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1865.32470703125
I0426 06:36:43.286153 140487526024960 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1790.8160400390625
I0426 06:38:03.504511 140487534417664 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1847.6878662109375
I0426 06:39:17.051510 140487526024960 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1790.94140625
I0426 06:40:30.617499 140487534417664 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1824.2510986328125
I0426 06:41:44.190509 140487526024960 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1851.5634765625
I0426 06:42:58.601304 140487534417664 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1750.288330078125
I0426 06:44:23.690260 140487526024960 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1859.7686767578125
I0426 06:45:48.057947 140487534417664 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1821.9141845703125
I0426 06:47:10.864003 140487526024960 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1808.1448974609375
I0426 06:48:33.656796 140487534417664 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1839.9853515625
I0426 06:49:57.462205 140487526024960 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1776.7708740234375
I0426 06:50:26.671745 140662253995840 spec.py:298] Evaluating on the training split.
I0426 06:50:54.715919 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 06:51:31.012646 140662253995840 spec.py:326] Evaluating on the test split.
I0426 06:51:49.531736 140662253995840 submission_runner.py:406] Time since start: 7635.54s, 	Step: 9237, 	{'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7263.166621685028, 'total_duration': 7635.536442041397, 'accumulated_submission_time': 7263.166621685028, 'accumulated_eval_time': 368.9360771179199, 'accumulated_logging_time': 3.30911922454834}
I0426 06:51:49.552889 140487964497664 logging_writer.py:48] [9237] accumulated_eval_time=368.936077, accumulated_logging_time=3.309119, accumulated_submission_time=7263.166622, global_step=9237, preemption_count=0, score=7263.166622, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7635.536442, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 06:51:49.749409 140662253995840 checkpoints.py:356] Saving checkpoint at step: 9237
I0426 06:51:50.724227 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_9237
I0426 06:51:50.745721 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_9237.
I0426 06:52:41.210329 140488619857664 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1818.936767578125
I0426 06:53:54.859282 140488611464960 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1807.25146484375
I0426 06:55:08.457657 140488619857664 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1869.1380615234375
I0426 06:56:22.007172 140488611464960 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1843.82861328125
I0426 06:57:35.587750 140488619857664 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1784.07666015625
I0426 06:58:53.545585 140488611464960 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1810.958984375
I0426 07:00:15.390739 140488619857664 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1856.5318603515625
I0426 07:01:38.037880 140488611464960 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1816.2266845703125
I0426 07:03:01.622359 140488619857664 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1847.5545654296875
I0426 07:04:26.827754 140488611464960 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1753.2850341796875
I0426 07:05:53.146933 140487964497664 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1857.070556640625
I0426 07:07:06.675705 140487956104960 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1803.1778564453125
I0426 07:08:20.142481 140487964497664 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1882.1937255859375
I0426 07:09:33.615571 140487956104960 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1829.4658203125
I0426 07:10:47.120584 140487964497664 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1869.8206787109375
I0426 07:12:04.386676 140487956104960 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1841.4412841796875
I0426 07:13:27.733855 140487964497664 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1805.0850830078125
I0426 07:14:51.510990 140487956104960 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1896.134765625
I0426 07:16:15.280545 140487964497664 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1896.2752685546875
I0426 07:17:39.818202 140487956104960 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1895.29248046875
I0426 07:19:02.266139 140487964497664 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1780.478271484375
I0426 07:20:21.556822 140488619857664 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1843.2974853515625
I0426 07:21:35.176751 140488611464960 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1830.9051513671875
I0426 07:22:48.699052 140488619857664 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1794.9598388671875
I0426 07:24:02.230568 140488611464960 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1828.159423828125
I0426 07:25:15.759626 140488619857664 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1797.73291015625
I0426 07:26:36.383112 140488611464960 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1840.9117431640625
I0426 07:27:58.278275 140488619857664 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1847.1546630859375
I0426 07:29:19.884998 140488611464960 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1783.8280029296875
I0426 07:30:42.433041 140488619857664 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1867.6380615234375
I0426 07:31:51.664709 140662253995840 spec.py:298] Evaluating on the training split.
I0426 07:32:19.815729 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 07:32:55.712217 140662253995840 spec.py:326] Evaluating on the test split.
I0426 07:33:14.077892 140662253995840 submission_runner.py:406] Time since start: 10120.08s, 	Step: 12287, 	{'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9664.040581941605, 'total_duration': 10120.082686901093, 'accumulated_submission_time': 9664.040581941605, 'accumulated_eval_time': 451.34616708755493, 'accumulated_logging_time': 4.5301737785339355}
I0426 07:33:14.101087 140488619857664 logging_writer.py:48] [12287] accumulated_eval_time=451.346167, accumulated_logging_time=4.530174, accumulated_submission_time=9664.040582, global_step=12287, preemption_count=0, score=9664.040582, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=10120.082687, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 07:33:14.287457 140662253995840 checkpoints.py:356] Saving checkpoint at step: 12287
I0426 07:33:15.288983 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_12287
I0426 07:33:15.310903 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_12287.
I0426 07:33:25.669841 140488611464960 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0, loss=1862.4747314453125
I0426 07:34:42.527329 140487309137664 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1901.48681640625
I0426 07:35:56.085035 140487300744960 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1819.1953125
I0426 07:37:09.601040 140487309137664 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1801.7818603515625
I0426 07:38:23.189519 140487300744960 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1843.0321044921875
I0426 07:39:36.823262 140487309137664 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1890.95263671875
I0426 07:40:55.680661 140487300744960 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1803.9403076171875
I0426 07:42:17.918396 140487309137664 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1858.5535888671875
I0426 07:43:41.377921 140487300744960 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1788.31396484375
I0426 07:45:03.373800 140487309137664 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1833.1339111328125
I0426 07:46:27.546753 140487300744960 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1794.8338623046875
I0426 07:47:54.280454 140487964497664 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1737.222900390625
I0426 07:49:07.862912 140487956104960 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1840.38232421875
I0426 07:50:21.437759 140487964497664 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1805.339599609375
I0426 07:51:34.974588 140487956104960 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1898.3846435546875
I0426 07:52:48.500034 140487964497664 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1832.08447265625
I0426 07:54:07.952088 140487956104960 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1842.766845703125
I0426 07:55:31.462135 140487964497664 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1815.582763671875
I0426 07:56:56.162859 140487956104960 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1864.9169921875
I0426 07:58:20.904014 140487964497664 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1763.9161376953125
I0426 07:59:44.793385 140487956104960 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1822.0438232421875
I0426 08:01:10.044826 140487964497664 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1794.330810546875
I0426 08:02:29.191301 140487964497664 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1834.71044921875
I0426 08:03:42.789176 140487956104960 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1736.39794921875
I0426 08:04:56.443327 140487964497664 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1761.60986328125
I0426 08:06:10.086633 140487956104960 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1849.28955078125
I0426 08:07:29.126315 140487964497664 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1890.8128662109375
I0426 08:08:52.482921 140487956104960 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1849.556884765625
I0426 08:10:15.932327 140487964497664 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1809.5509033203125
I0426 08:11:39.770247 140487956104960 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1869.6842041015625
I0426 08:13:03.734042 140487964497664 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1748.4951171875
I0426 08:13:15.660835 140662253995840 spec.py:298] Evaluating on the training split.
I0426 08:13:44.089291 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 08:14:20.370222 140662253995840 spec.py:326] Evaluating on the test split.
I0426 08:14:39.229926 140662253995840 submission_runner.py:406] Time since start: 12605.23s, 	Step: 15316, 	{'train/ctc_loss': DeviceArray(1832.9214, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12064.342221975327, 'total_duration': 12605.234751939774, 'accumulated_submission_time': 12064.342221975327, 'accumulated_eval_time': 534.9121971130371, 'accumulated_logging_time': 5.772064447402954}
I0426 08:14:39.252938 140489275217664 logging_writer.py:48] [15316] accumulated_eval_time=534.912197, accumulated_logging_time=5.772064, accumulated_submission_time=12064.342222, global_step=15316, preemption_count=0, score=12064.342222, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=12605.234752, train/ctc_loss=1832.92138671875, train/wer=0.941551, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 08:14:39.449432 140662253995840 checkpoints.py:356] Saving checkpoint at step: 15316
I0426 08:14:40.474716 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_15316
I0426 08:14:40.496592 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_15316.
I0426 08:15:42.930841 140489266824960 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1877.224853515625
I0426 08:16:59.970904 140488619857664 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1774.6771240234375
I0426 08:18:13.504488 140488611464960 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1821.9141845703125
I0426 08:19:26.947893 140488619857664 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1810.190673828125
I0426 08:20:40.454029 140488611464960 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1859.90380859375
I0426 08:21:53.990898 140488619857664 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1846.0889892578125
I0426 08:23:13.275034 140488611464960 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1859.90380859375
I0426 08:24:35.951057 140488619857664 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.0, loss=1840.6468505859375
I0426 08:25:59.176214 140488611464960 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0, loss=1783.206787109375
I0426 08:27:23.445057 140488619857664 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.0, loss=1779.61181640625
I0426 08:28:47.983120 140488611464960 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0, loss=1780.7259521484375
I0426 08:30:12.724975 140488619857664 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1807.63427734375
I0426 08:31:26.264250 140488611464960 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.0, loss=1820.87744140625
I0426 08:32:39.811911 140488619857664 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.0, loss=1790.69091796875
I0426 08:33:53.350586 140488611464960 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.0, loss=1799.881591796875
I0426 08:35:06.926422 140488619857664 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0, loss=1850.49267578125
I0426 08:36:29.341581 140488611464960 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1865.460693359375
I0426 08:37:53.604413 140488619857664 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.0, loss=1778.3756103515625
I0426 08:39:17.481905 140488611464960 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.0, loss=1828.681640625
I0426 08:40:41.578810 140488619857664 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0, loss=1818.8074951171875
I0426 08:42:05.136985 140488611464960 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.0, loss=1828.8125
I0426 08:43:29.649319 140488619857664 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1782.089599609375
I0426 08:44:47.147348 140489275217664 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0, loss=1858.013916015625
I0426 08:46:00.687035 140489266824960 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0, loss=1777.1409912109375
I0426 08:47:14.203740 140489275217664 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.0, loss=1794.0791015625
I0426 08:48:27.740126 140489266824960 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.0, loss=1862.068359375
I0426 08:49:49.130085 140489275217664 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1763.06591796875
I0426 08:51:15.568446 140489266824960 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.0, loss=1802.035400390625
I0426 08:52:41.618445 140489275217664 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.0, loss=1770.38134765625
I0426 08:54:06.552610 140489266824960 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.0, loss=1768.91357421875
I0426 08:54:40.823693 140662253995840 spec.py:298] Evaluating on the training split.
I0426 08:55:09.294156 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 08:55:45.441480 140662253995840 spec.py:326] Evaluating on the test split.
I0426 08:56:04.122826 140662253995840 submission_runner.py:406] Time since start: 15090.13s, 	Step: 18342, 	{'train/ctc_loss': DeviceArray(1752.7933, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14464.619918823242, 'total_duration': 15090.127621412277, 'accumulated_submission_time': 14464.619918823242, 'accumulated_eval_time': 618.2082343101501, 'accumulated_logging_time': 7.0500853061676025}
I0426 08:56:04.146668 140489275217664 logging_writer.py:48] [18342] accumulated_eval_time=618.208234, accumulated_logging_time=7.050085, accumulated_submission_time=14464.619919, global_step=18342, preemption_count=0, score=14464.619919, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=15090.127621, train/ctc_loss=1752.7933349609375, train/wer=0.942641, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 08:56:04.339467 140662253995840 checkpoints.py:356] Saving checkpoint at step: 18342
I0426 08:56:05.367330 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_18342
I0426 08:56:05.388993 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_18342.
I0426 08:56:48.750656 140489266824960 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0, loss=1851.0279541015625
I0426 08:58:02.288092 140488527537920 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1780.478271484375
I0426 08:59:19.336454 140489275217664 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.0, loss=1833.9219970703125
I0426 09:00:32.881602 140489266824960 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0, loss=1881.501953125
I0426 09:01:46.459914 140489275217664 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0, loss=1824.771240234375
I0426 09:02:59.985531 140489266824960 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.0, loss=1779.1170654296875
I0426 09:04:15.406509 140489275217664 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1812.75439453125
I0426 09:05:37.842066 140489266824960 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.0, loss=1844.625732421875
I0426 09:07:01.806892 140489275217664 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0, loss=1786.1927490234375
I0426 09:08:26.076032 140489266824960 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.0, loss=1815.9691162109375
I0426 09:09:51.546240 140489275217664 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.0, loss=1849.556884765625
I0426 09:11:17.676894 140489266824960 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1758.4638671875
I0426 09:12:42.785071 140489275217664 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.0, loss=1834.316162109375
I0426 09:13:56.378154 140489266824960 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.0, loss=1847.1546630859375
I0426 09:15:09.959017 140489275217664 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0, loss=1910.5692138671875
I0426 09:16:23.526677 140489266824960 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.0, loss=1786.9407958984375
I0426 09:17:35.924892 140662253995840 spec.py:298] Evaluating on the training split.
I0426 09:18:04.229339 140662253995840 spec.py:310] Evaluating on the validation split.
I0426 09:18:41.082039 140662253995840 spec.py:326] Evaluating on the test split.
I0426 09:18:59.807735 140662253995840 submission_runner.py:406] Time since start: 16465.81s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15755.123582839966, 'total_duration': 16465.812519311905, 'accumulated_submission_time': 15755.123582839966, 'accumulated_eval_time': 702.0879828929901, 'accumulated_logging_time': 8.3266921043396}
I0426 09:18:59.832369 140489275217664 logging_writer.py:48] [20000] accumulated_eval_time=702.087983, accumulated_logging_time=8.326692, accumulated_submission_time=15755.123583, global_step=20000, preemption_count=0, score=15755.123583, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=16465.812519, train/ctc_loss=1746.1038818359375, train/wer=0.942824, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 09:19:00.028447 140662253995840 checkpoints.py:356] Saving checkpoint at step: 20000
I0426 09:19:01.046472 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000
I0426 09:19:01.068174 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0426 09:19:01.087122 140489266824960 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15755.123583
I0426 09:19:01.254743 140662253995840 checkpoints.py:356] Saving checkpoint at step: 20000
I0426 09:19:02.563957 140662253995840 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000
I0426 09:19:02.585813 140662253995840 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0426 09:19:03.922281 140662253995840 submission_runner.py:567] Tuning trial 1/1
I0426 09:19:03.922575 140662253995840 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0426 09:19:03.928339 140662253995840 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.15589, dtype=float32), 'train/wer': 1.247829165191679, 'validation/ctc_loss': DeviceArray(30.062668, dtype=float32), 'validation/wer': 1.1601269669750793, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.051409, dtype=float32), 'test/wer': 1.1669814961509557, 'test/num_examples': 2472, 'score': 62.164222717285156, 'total_duration': 186.56355381011963, 'accumulated_submission_time': 62.164222717285156, 'accumulated_eval_time': 124.3991870880127, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3090, {'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2462.755706548691, 'total_duration': 2668.584280729294, 'accumulated_submission_time': 2462.755706548691, 'accumulated_eval_time': 204.9535026550293, 'accumulated_logging_time': 0.834252119064331, 'global_step': 3090, 'preemption_count': 0}), (6177, {'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4862.974407196045, 'total_duration': 5151.210730552673, 'accumulated_submission_time': 4862.974407196045, 'accumulated_eval_time': 286.0792419910431, 'accumulated_logging_time': 2.0744779109954834, 'global_step': 6177, 'preemption_count': 0}), (9237, {'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7263.166621685028, 'total_duration': 7635.536442041397, 'accumulated_submission_time': 7263.166621685028, 'accumulated_eval_time': 368.9360771179199, 'accumulated_logging_time': 3.30911922454834, 'global_step': 9237, 'preemption_count': 0}), (12287, {'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9664.040581941605, 'total_duration': 10120.082686901093, 'accumulated_submission_time': 9664.040581941605, 'accumulated_eval_time': 451.34616708755493, 'accumulated_logging_time': 4.5301737785339355, 'global_step': 12287, 'preemption_count': 0}), (15316, {'train/ctc_loss': DeviceArray(1832.9214, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12064.342221975327, 'total_duration': 12605.234751939774, 'accumulated_submission_time': 12064.342221975327, 'accumulated_eval_time': 534.9121971130371, 'accumulated_logging_time': 5.772064447402954, 'global_step': 15316, 'preemption_count': 0}), (18342, {'train/ctc_loss': DeviceArray(1752.7933, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14464.619918823242, 'total_duration': 15090.127621412277, 'accumulated_submission_time': 14464.619918823242, 'accumulated_eval_time': 618.2082343101501, 'accumulated_logging_time': 7.0500853061676025, 'global_step': 18342, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15755.123582839966, 'total_duration': 16465.812519311905, 'accumulated_submission_time': 15755.123582839966, 'accumulated_eval_time': 702.0879828929901, 'accumulated_logging_time': 8.3266921043396, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0426 09:19:03.928502 140662253995840 submission_runner.py:570] Timing: 15755.123582839966
I0426 09:19:03.928553 140662253995840 submission_runner.py:571] ====================
I0426 09:19:03.929039 140662253995840 submission_runner.py:631] Final librispeech_conformer score: 15755.123582839966
