I0420 12:23:04.814415 139917992580928 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax.
I0420 12:23:04.886149 139917992580928 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 12:23:05.704655 139917992580928 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0420 12:23:05.705401 139917992580928 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 12:23:05.709412 139917992580928 submission_runner.py:528] Using RNG seed 1111612996
I0420 12:23:08.231436 139917992580928 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 12:23:08.231631 139917992580928 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1.
I0420 12:23:08.231878 139917992580928 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0420 12:23:08.350554 139917992580928 submission_runner.py:232] Initializing dataset.
I0420 12:23:08.350739 139917992580928 submission_runner.py:239] Initializing model.
I0420 12:23:24.630722 139917992580928 submission_runner.py:249] Initializing optimizer.
I0420 12:23:25.262130 139917992580928 submission_runner.py:256] Initializing metrics bundle.
I0420 12:23:25.262313 139917992580928 submission_runner.py:273] Initializing checkpoint and logger.
I0420 12:23:25.263328 139917992580928 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0420 12:23:25.263602 139917992580928 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 12:23:25.263689 139917992580928 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 12:23:26.022104 139917992580928 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0420 12:23:26.023007 139917992580928 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0420 12:23:26.029606 139917992580928 submission_runner.py:309] Starting training loop.
I0420 12:23:26.218482 139917992580928 input_pipeline.py:20] Loading split = train-clean-100
I0420 12:23:26.250360 139917992580928 input_pipeline.py:20] Loading split = train-clean-360
I0420 12:23:26.532860 139917992580928 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0420 12:24:22.595618 139742146451200 logging_writer.py:48] [0] global_step=0, grad_norm=23.489686965942383, loss=32.54134750366211
I0420 12:24:22.617100 139917992580928 spec.py:298] Evaluating on the training split.
I0420 12:24:22.737033 139917992580928 input_pipeline.py:20] Loading split = train-clean-100
I0420 12:24:22.763020 139917992580928 input_pipeline.py:20] Loading split = train-clean-360
I0420 12:24:23.019187 139917992580928 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0420 12:25:47.151742 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 12:25:47.476894 139917992580928 input_pipeline.py:20] Loading split = dev-clean
I0420 12:25:47.481317 139917992580928 input_pipeline.py:20] Loading split = dev-other
I0420 12:26:39.593894 139917992580928 spec.py:326] Evaluating on the test split.
I0420 12:26:39.684993 139917992580928 input_pipeline.py:20] Loading split = test-clean
I0420 12:27:14.016501 139917992580928 submission_runner.py:406] Time since start: 227.99s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.72194, dtype=float32), 'train/wer': 3.884211731878475, 'validation/ctc_loss': DeviceArray(31.736477, dtype=float32), 'validation/wer': 3.737768815907534, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.82975, dtype=float32), 'test/wer': 3.719618954766112, 'test/num_examples': 2472, 'score': 56.587342262268066, 'total_duration': 227.9857313632965, 'accumulated_submission_time': 56.587342262268066, 'accumulated_eval_time': 171.39825773239136, 'accumulated_logging_time': 0}
I0420 12:27:14.037935 139740082861824 logging_writer.py:48] [1] accumulated_eval_time=171.398258, accumulated_logging_time=0, accumulated_submission_time=56.587342, global_step=1, preemption_count=0, score=56.587342, test/ctc_loss=31.829750061035156, test/num_examples=2472, test/wer=3.719619, total_duration=227.985731, train/ctc_loss=32.72193908691406, train/wer=3.884212, validation/ctc_loss=31.73647689819336, validation/num_examples=5348, validation/wer=3.737769
I0420 12:27:14.171478 139917992580928 checkpoints.py:356] Saving checkpoint at step: 1
I0420 12:27:14.530866 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0420 12:27:14.531672 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0420 12:29:26.063141 139743784412928 logging_writer.py:48] [100] global_step=100, grad_norm=4.3373332023620605, loss=7.458837032318115
I0420 12:31:23.596879 139743792805632 logging_writer.py:48] [200] global_step=200, grad_norm=1.5674560070037842, loss=6.234828472137451
I0420 12:33:21.182817 139743784412928 logging_writer.py:48] [300] global_step=300, grad_norm=1.1516417264938354, loss=5.943124294281006
I0420 12:35:17.368181 139743792805632 logging_writer.py:48] [400] global_step=400, grad_norm=0.469760537147522, loss=5.839599132537842
I0420 12:37:14.131147 139743784412928 logging_writer.py:48] [500] global_step=500, grad_norm=0.7067213654518127, loss=5.806063175201416
I0420 12:39:11.480955 139743792805632 logging_writer.py:48] [600] global_step=600, grad_norm=0.5384879112243652, loss=5.678997039794922
I0420 12:41:05.481841 139743784412928 logging_writer.py:48] [700] global_step=700, grad_norm=1.165966272354126, loss=5.542491436004639
I0420 12:42:58.538435 139743792805632 logging_writer.py:48] [800] global_step=800, grad_norm=0.8684929609298706, loss=5.3030242919921875
I0420 12:44:53.373818 139743784412928 logging_writer.py:48] [900] global_step=900, grad_norm=1.3750890493392944, loss=4.948925018310547
I0420 12:46:51.620244 139743792805632 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.8889760971069336, loss=4.578432083129883
I0420 12:48:52.695367 139741416646400 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0720807313919067, loss=4.297859191894531
I0420 12:50:50.892499 139741408253696 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.852993130683899, loss=4.117410659790039
I0420 12:52:43.680012 139741416646400 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.7116575241088867, loss=3.9065678119659424
I0420 12:54:41.423499 139741408253696 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.3181910514831543, loss=3.691594362258911
I0420 12:56:37.068822 139741416646400 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.7751502990722656, loss=3.554582118988037
I0420 12:58:32.484111 139741408253696 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.5941145420074463, loss=3.4481356143951416
I0420 13:00:26.587487 139741416646400 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.1272220611572266, loss=3.3347554206848145
I0420 13:02:18.928742 139741408253696 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.6780238151550293, loss=3.286850929260254
I0420 13:04:13.604079 139741416646400 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.9476282596588135, loss=3.1660914421081543
I0420 13:06:06.747905 139741408253696 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.2788920402526855, loss=3.03525710105896
I0420 13:07:16.831171 139917992580928 spec.py:298] Evaluating on the training split.
I0420 13:07:46.439387 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 13:08:22.021104 139917992580928 spec.py:326] Evaluating on the test split.
I0420 13:08:39.748551 139917992580928 submission_runner.py:406] Time since start: 2713.72s, 	Step: 2061, 	{'train/ctc_loss': DeviceArray(6.269429, dtype=float32), 'train/wer': 0.9379260983295916, 'validation/ctc_loss': DeviceArray(6.1423116, dtype=float32), 'validation/wer': 0.8929174425223592, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.0254493, dtype=float32), 'test/wer': 0.8960453354457376, 'test/num_examples': 2472, 'score': 2458.8603105545044, 'total_duration': 2713.715287208557, 'accumulated_submission_time': 2458.8603105545044, 'accumulated_eval_time': 254.31217861175537, 'accumulated_logging_time': 0.5197093486785889}
I0420 13:08:39.767502 139741416646400 logging_writer.py:48] [2061] accumulated_eval_time=254.312179, accumulated_logging_time=0.519709, accumulated_submission_time=2458.860311, global_step=2061, preemption_count=0, score=2458.860311, test/ctc_loss=6.025449275970459, test/num_examples=2472, test/wer=0.896045, total_duration=2713.715287, train/ctc_loss=6.2694292068481445, train/wer=0.937926, validation/ctc_loss=6.1423115730285645, validation/num_examples=5348, validation/wer=0.892917
I0420 13:08:39.921643 139917992580928 checkpoints.py:356] Saving checkpoint at step: 2061
I0420 13:08:40.454599 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_2061
I0420 13:08:40.467526 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_2061.
I0420 13:09:25.375652 139741408253696 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.702255964279175, loss=3.022692918777466
I0420 13:11:17.461623 139741081102080 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.008960247039795, loss=2.985456943511963
I0420 13:13:09.504103 139741408253696 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.9900712966918945, loss=2.908282995223999
I0420 13:15:04.060430 139741081102080 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.8181591033935547, loss=2.783792018890381
I0420 13:16:59.243589 139741408253696 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.242543935775757, loss=2.7930352687835693
I0420 13:18:57.748131 139741081102080 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.865131139755249, loss=2.683527946472168
I0420 13:20:54.759179 139741408253696 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.09216570854187, loss=2.7023138999938965
I0420 13:22:50.532833 139741081102080 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.8141207695007324, loss=2.6145570278167725
I0420 13:24:44.420909 139741408253696 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.3436810970306396, loss=2.5654942989349365
I0420 13:26:36.565109 139741081102080 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.4763357639312744, loss=2.6062939167022705
I0420 13:28:37.946482 139741416646400 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.0513510704040527, loss=2.5736260414123535
I0420 13:30:34.066362 139741408253696 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.4223122596740723, loss=2.5702767372131348
I0420 13:32:27.746080 139741416646400 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.103576898574829, loss=2.5177507400512695
I0420 13:34:23.889129 139741408253696 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.761608600616455, loss=2.484422445297241
I0420 13:36:21.978884 139741416646400 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.612948179244995, loss=2.43424916267395
I0420 13:38:20.480320 139741408253696 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.2989118099212646, loss=2.3986666202545166
I0420 13:40:19.490752 139741416646400 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.8888537883758545, loss=2.389953136444092
I0420 13:42:17.432763 139741408253696 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.604212522506714, loss=2.256453037261963
I0420 13:44:13.759742 139741416646400 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.563866138458252, loss=2.3286643028259277
I0420 13:46:10.428156 139741408253696 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.245197296142578, loss=2.2695741653442383
I0420 13:48:06.194137 139741416646400 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.8792781829833984, loss=2.292865514755249
I0420 13:48:41.514617 139917992580928 spec.py:298] Evaluating on the training split.
I0420 13:49:19.396685 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 13:49:57.862109 139917992580928 spec.py:326] Evaluating on the test split.
I0420 13:50:16.986592 139917992580928 submission_runner.py:406] Time since start: 5210.95s, 	Step: 4130, 	{'train/ctc_loss': DeviceArray(1.3599398, dtype=float32), 'train/wer': 0.37822763231494555, 'validation/ctc_loss': DeviceArray(1.8146176, dtype=float32), 'validation/wer': 0.44111375893641036, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3643454, dtype=float32), 'test/wer': 0.3712347409258018, 'test/num_examples': 2472, 'score': 4859.881725549698, 'total_duration': 5210.953722715378, 'accumulated_submission_time': 4859.881725549698, 'accumulated_eval_time': 349.7811191082001, 'accumulated_logging_time': 1.2431936264038086}
I0420 13:50:17.004653 139741416646400 logging_writer.py:48] [4130] accumulated_eval_time=349.781119, accumulated_logging_time=1.243194, accumulated_submission_time=4859.881726, global_step=4130, preemption_count=0, score=4859.881726, test/ctc_loss=1.3643454313278198, test/num_examples=2472, test/wer=0.371235, total_duration=5210.953723, train/ctc_loss=1.3599398136138916, train/wer=0.378228, validation/ctc_loss=1.81461763381958, validation/num_examples=5348, validation/wer=0.441114
I0420 13:50:17.155347 139917992580928 checkpoints.py:356] Saving checkpoint at step: 4130
I0420 13:50:17.728445 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_4130
I0420 13:50:17.741262 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_4130.
I0420 13:51:41.469313 139741408253696 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.831315755844116, loss=2.274674892425537
I0420 13:53:36.866858 139740544231168 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.728957176208496, loss=2.2056074142456055
I0420 13:55:28.949034 139741408253696 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.235279083251953, loss=2.262173891067505
I0420 13:57:21.444201 139740544231168 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.393120288848877, loss=2.1634843349456787
I0420 13:59:19.065500 139741408253696 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.9480342864990234, loss=2.181450605392456
I0420 14:01:16.762626 139740544231168 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.7738269567489624, loss=2.1242032051086426
I0420 14:03:13.412718 139741408253696 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.520659923553467, loss=2.0875275135040283
I0420 14:05:08.935716 139740544231168 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.2917630672454834, loss=2.2188355922698975
I0420 14:07:03.485054 139741408253696 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.326923847198486, loss=2.2008795738220215
I0420 14:08:57.379860 139740544231168 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.107483386993408, loss=2.1400887966156006
I0420 14:10:56.069843 139741416646400 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.182565927505493, loss=2.066446304321289
I0420 14:12:48.070700 139741408253696 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.203303813934326, loss=2.0629048347473145
I0420 14:14:40.090457 139741416646400 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.2346608638763428, loss=2.13175106048584
I0420 14:16:33.550104 139741408253696 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.7994885444641113, loss=2.061934471130371
I0420 14:18:31.860581 139741416646400 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.521646738052368, loss=2.1095473766326904
I0420 14:20:29.130943 139741408253696 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.653895854949951, loss=2.0663440227508545
I0420 14:22:27.453243 139741416646400 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.2287731170654297, loss=2.08766508102417
I0420 14:24:26.026854 139741408253696 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.3611462116241455, loss=2.0064499378204346
I0420 14:26:23.902846 139741416646400 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.9691998958587646, loss=2.0630998611450195
I0420 14:28:16.910473 139741408253696 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.071532964706421, loss=1.9701370000839233
I0420 14:30:12.386805 139741416646400 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.6830391883850098, loss=1.9564502239227295
I0420 14:30:17.817052 139917992580928 spec.py:298] Evaluating on the training split.
I0420 14:30:56.696585 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 14:31:33.708283 139917992580928 spec.py:326] Evaluating on the test split.
I0420 14:31:52.881217 139917992580928 submission_runner.py:406] Time since start: 7706.85s, 	Step: 6206, 	{'train/ctc_loss': DeviceArray(0.64907223, dtype=float32), 'train/wer': 0.22258066216508268, 'validation/ctc_loss': DeviceArray(1.0638685, dtype=float32), 'validation/wer': 0.2979575297397949, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.714808, dtype=float32), 'test/wer': 0.22899274876607154, 'test/num_examples': 2472, 'score': 7259.9311439991, 'total_duration': 7706.8482401371, 'accumulated_submission_time': 7259.9311439991, 'accumulated_eval_time': 444.8419623374939, 'accumulated_logging_time': 2.0026235580444336}
I0420 14:31:52.899854 139743943874304 logging_writer.py:48] [6206] accumulated_eval_time=444.841962, accumulated_logging_time=2.002624, accumulated_submission_time=7259.931144, global_step=6206, preemption_count=0, score=7259.931144, test/ctc_loss=0.7148079872131348, test/num_examples=2472, test/wer=0.228993, total_duration=7706.848240, train/ctc_loss=0.6490722298622131, train/wer=0.222581, validation/ctc_loss=1.063868522644043, validation/num_examples=5348, validation/wer=0.297958
I0420 14:31:53.053357 139917992580928 checkpoints.py:356] Saving checkpoint at step: 6206
I0420 14:31:53.652325 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_6206
I0420 14:31:53.665153 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_6206.
I0420 14:33:46.231543 139743935481600 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.5238711833953857, loss=1.9729982614517212
I0420 14:35:44.831978 139743876732672 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.5014952421188354, loss=2.0199403762817383
I0420 14:37:40.620272 139743935481600 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.250044107437134, loss=1.9787757396697998
I0420 14:39:34.915509 139743876732672 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.4171411991119385, loss=1.9153047800064087
I0420 14:41:26.871462 139743935481600 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.4793405532836914, loss=1.938722848892212
I0420 14:43:24.382732 139743876732672 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.4945240020751953, loss=1.9188185930252075
I0420 14:45:22.707781 139743935481600 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.615527868270874, loss=1.920624017715454
I0420 14:47:19.935719 139743876732672 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.7502421140670776, loss=1.9597193002700806
I0420 14:49:15.105733 139743935481600 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.728238582611084, loss=1.9730207920074463
I0420 14:51:07.088295 139743876732672 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.858013391494751, loss=1.9316856861114502
I0420 14:53:02.339203 139741416646400 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.5551834106445312, loss=1.9319168329238892
I0420 14:54:56.004127 139741408253696 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.87823224067688, loss=1.8903919458389282
I0420 14:56:52.225286 139741416646400 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.844222903251648, loss=1.9411067962646484
I0420 14:58:44.315129 139741408253696 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.4786434173583984, loss=1.8688832521438599
I0420 15:00:38.348223 139741416646400 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.335059881210327, loss=1.9051446914672852
I0420 15:02:37.571285 139741408253696 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5588431358337402, loss=5.962520122528076
I0420 15:04:35.027384 139741416646400 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.3408961296081543, loss=5.800527095794678
I0420 15:06:32.336087 139741408253696 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.4703644514083862, loss=4.900538444519043
I0420 15:08:28.276900 139741416646400 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.3280184268951416, loss=3.6815054416656494
I0420 15:10:26.132648 139741408253696 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.389097213745117, loss=2.483229398727417
I0420 15:11:54.001469 139917992580928 spec.py:298] Evaluating on the training split.
I0420 15:12:22.848517 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 15:12:58.806416 139917992580928 spec.py:326] Evaluating on the test split.
I0420 15:13:17.065498 139917992580928 submission_runner.py:406] Time since start: 10191.03s, 	Step: 8273, 	{'train/ctc_loss': DeviceArray(5.700151, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(5.7379537, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.5954595, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9660.241299152374, 'total_duration': 10191.032596588135, 'accumulated_submission_time': 9660.241299152374, 'accumulated_eval_time': 527.9029009342194, 'accumulated_logging_time': 2.791566848754883}
I0420 15:13:17.084393 139741416646400 logging_writer.py:48] [8273] accumulated_eval_time=527.902901, accumulated_logging_time=2.791567, accumulated_submission_time=9660.241299, global_step=8273, preemption_count=0, score=9660.241299, test/ctc_loss=5.595459461212158, test/num_examples=2472, test/wer=0.899580, total_duration=10191.032597, train/ctc_loss=5.700150966644287, train/wer=0.943700, validation/ctc_loss=5.7379536628723145, validation/num_examples=5348, validation/wer=0.895995
I0420 15:13:17.239142 139917992580928 checkpoints.py:356] Saving checkpoint at step: 8273
I0420 15:13:17.828952 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8273
I0420 15:13:17.841985 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_8273.
I0420 15:13:49.889709 139741408253696 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.592797040939331, loss=2.233584403991699
I0420 15:15:43.599660 139740351297280 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.5648927688598633, loss=2.1456942558288574
I0420 15:17:41.584432 139741408253696 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.4252769947052, loss=2.0815024375915527
I0420 15:19:33.710316 139740351297280 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.3266994953155518, loss=2.028324842453003
I0420 15:21:25.932624 139741408253696 logging_writer.py:48] [8700] global_step=8700, grad_norm=4.650741100311279, loss=2.0317749977111816
I0420 15:23:20.465574 139740351297280 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.0244314670562744, loss=1.9986568689346313
I0420 15:25:14.583657 139741408253696 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.8702285289764404, loss=1.92501962184906
I0420 15:27:12.176337 139740351297280 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.671051263809204, loss=1.9626246690750122
I0420 15:29:05.687441 139741408253696 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.979570150375366, loss=1.872976541519165
I0420 15:31:01.302061 139740351297280 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.505885601043701, loss=1.9086612462997437
I0420 15:32:56.315396 139741416646400 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.3265504837036133, loss=1.8958905935287476
I0420 15:34:50.593920 139741408253696 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.229299545288086, loss=1.851159691810608
I0420 15:36:48.183548 139741416646400 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.3463134765625, loss=1.8765230178833008
I0420 15:38:42.047654 139741408253696 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.6745645999908447, loss=1.8834292888641357
I0420 15:40:33.800657 139741416646400 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8917672634124756, loss=1.8538103103637695
I0420 15:42:31.527895 139741408253696 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.610867977142334, loss=1.8881456851959229
I0420 15:44:25.760108 139741416646400 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.576572895050049, loss=1.8060948848724365
I0420 15:46:18.258079 139741408253696 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.0639381408691406, loss=1.8588738441467285
I0420 15:48:12.323352 139741416646400 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.183586835861206, loss=1.8145368099212646
I0420 15:50:03.916780 139741408253696 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.509040594100952, loss=1.8193717002868652
I0420 15:52:03.205033 139741416646400 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.468090295791626, loss=1.7865283489227295
I0420 15:53:17.941901 139917992580928 spec.py:298] Evaluating on the training split.
I0420 15:53:55.823196 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 15:54:33.548894 139917992580928 spec.py:326] Evaluating on the test split.
I0420 15:54:52.618443 139917992580928 submission_runner.py:406] Time since start: 12686.58s, 	Step: 10368, 	{'train/ctc_loss': DeviceArray(0.6898364, dtype=float32), 'train/wer': 0.21638326556350432, 'validation/ctc_loss': DeviceArray(1.0474675, dtype=float32), 'validation/wer': 0.28466265955291414, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7015874, dtype=float32), 'test/wer': 0.21400280299798916, 'test/num_examples': 2472, 'score': 12060.316445350647, 'total_duration': 12686.584991931915, 'accumulated_submission_time': 12060.316445350647, 'accumulated_eval_time': 622.5756497383118, 'accumulated_logging_time': 3.569930076599121}
I0420 15:54:52.638131 139741416646400 logging_writer.py:48] [10368] accumulated_eval_time=622.575650, accumulated_logging_time=3.569930, accumulated_submission_time=12060.316445, global_step=10368, preemption_count=0, score=12060.316445, test/ctc_loss=0.7015873789787292, test/num_examples=2472, test/wer=0.214003, total_duration=12686.584992, train/ctc_loss=0.6898363828659058, train/wer=0.216383, validation/ctc_loss=1.0474674701690674, validation/num_examples=5348, validation/wer=0.284663
I0420 15:54:52.794840 139917992580928 checkpoints.py:356] Saving checkpoint at step: 10368
I0420 15:54:53.441745 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_10368
I0420 15:54:53.454567 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_10368.
I0420 15:55:32.212075 139741408253696 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.673593282699585, loss=1.8199509382247925
I0420 15:57:27.024743 139740342904576 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.1655731201171875, loss=1.8343074321746826
I0420 15:59:25.517364 139741408253696 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.262509346008301, loss=1.7579660415649414
I0420 16:01:22.730724 139740342904576 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.1491336822509766, loss=1.8021942377090454
I0420 16:03:18.300250 139741408253696 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.910759210586548, loss=1.8037749528884888
I0420 16:05:15.937472 139740342904576 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.7339396476745605, loss=1.7381477355957031
I0420 16:07:09.011356 139741408253696 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.8939805030822754, loss=1.7823830842971802
I0420 16:09:04.220357 139740342904576 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.8938558101654053, loss=1.789435863494873
I0420 16:10:56.205095 139741408253696 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.913261651992798, loss=1.8224713802337646
I0420 16:12:49.882720 139740342904576 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.654177665710449, loss=1.8479866981506348
I0420 16:14:46.691507 139741416646400 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.2516391277313232, loss=1.7696336507797241
I0420 16:16:39.653897 139741408253696 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.132903575897217, loss=1.697603702545166
I0420 16:18:35.697903 139741416646400 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.2978408336639404, loss=1.7745742797851562
I0420 16:20:30.587065 139741408253696 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.8289246559143066, loss=1.747799277305603
I0420 16:22:26.855254 139741416646400 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.5531609058380127, loss=1.7922006845474243
I0420 16:24:21.982926 139741408253696 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.2681806087493896, loss=1.7336452007293701
I0420 16:26:17.063081 139741416646400 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.8707666397094727, loss=1.7643673419952393
I0420 16:28:11.894154 139741408253696 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.9888172149658203, loss=1.7696993350982666
I0420 16:30:09.789569 139741416646400 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.6437723636627197, loss=1.7225698232650757
I0420 16:32:07.500258 139741408253696 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.288093328475952, loss=1.7066017389297485
I0420 16:34:06.070031 139741416646400 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.0309512615203857, loss=1.7061278820037842
I0420 16:34:54.095618 139917992580928 spec.py:298] Evaluating on the training split.
I0420 16:35:33.036044 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 16:36:10.051831 139917992580928 spec.py:326] Evaluating on the test split.
I0420 16:36:29.084319 139917992580928 submission_runner.py:406] Time since start: 15183.05s, 	Step: 12444, 	{'train/ctc_loss': DeviceArray(0.48838204, dtype=float32), 'train/wer': 0.16173826760059962, 'validation/ctc_loss': DeviceArray(0.8057666, dtype=float32), 'validation/wer': 0.23038331291184672, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5016021, dtype=float32), 'test/wer': 0.16194422440233178, 'test/num_examples': 2472, 'score': 14460.93118596077, 'total_duration': 15183.051533460617, 'accumulated_submission_time': 14460.93118596077, 'accumulated_eval_time': 717.5612359046936, 'accumulated_logging_time': 4.411169052124023}
I0420 16:36:29.103627 139743943874304 logging_writer.py:48] [12444] accumulated_eval_time=717.561236, accumulated_logging_time=4.411169, accumulated_submission_time=14460.931186, global_step=12444, preemption_count=0, score=14460.931186, test/ctc_loss=0.5016021132469177, test/num_examples=2472, test/wer=0.161944, total_duration=15183.051533, train/ctc_loss=0.4883820414543152, train/wer=0.161738, validation/ctc_loss=0.8057665824890137, validation/num_examples=5348, validation/wer=0.230383
I0420 16:36:29.246193 139917992580928 checkpoints.py:356] Saving checkpoint at step: 12444
I0420 16:36:29.820022 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_12444
I0420 16:36:29.833141 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_12444.
I0420 16:37:36.461893 139743935481600 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.2957701683044434, loss=1.6880998611450195
I0420 16:39:29.283102 139743851554560 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.0437448024749756, loss=1.7047864198684692
I0420 16:41:21.925343 139743935481600 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.769029378890991, loss=1.7345505952835083
I0420 16:43:17.819324 139743851554560 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.967604160308838, loss=1.718164324760437
I0420 16:45:12.754694 139743935481600 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.267744541168213, loss=1.7002153396606445
I0420 16:47:04.572601 139743851554560 logging_writer.py:48] [13000] global_step=13000, grad_norm=6.661744117736816, loss=1.697776436805725
I0420 16:49:00.127267 139743935481600 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.729640007019043, loss=1.7320241928100586
I0420 16:50:57.025673 139743851554560 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.7595021724700928, loss=1.7360609769821167
I0420 16:52:50.065933 139743935481600 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.2955267429351807, loss=1.7562183141708374
I0420 16:54:45.380423 139743943874304 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.9382596015930176, loss=1.7460688352584839
I0420 16:56:41.762791 139743935481600 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.3111047744750977, loss=1.7376123666763306
I0420 16:58:34.930157 139743943874304 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.693887233734131, loss=1.7121493816375732
I0420 17:00:27.623628 139743935481600 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.467189311981201, loss=1.7624200582504272
I0420 17:02:23.511489 139743943874304 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.5980722904205322, loss=1.7459454536437988
I0420 17:04:20.596210 139743935481600 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.8202362060546875, loss=1.652942419052124
I0420 17:06:18.026640 139743943874304 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.5194005966186523, loss=1.6643234491348267
I0420 17:08:15.019860 139743935481600 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.2962758541107178, loss=1.7109209299087524
I0420 17:10:12.032127 139743943874304 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.403063774108887, loss=1.7652502059936523
I0420 17:12:10.389499 139743935481600 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.2185020446777344, loss=1.638704538345337
I0420 17:14:06.082578 139743943874304 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.119957208633423, loss=1.6502346992492676
I0420 17:16:05.115341 139741416646400 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.5053954124450684, loss=1.6905395984649658
I0420 17:16:30.120045 139917992580928 spec.py:298] Evaluating on the training split.
I0420 17:17:09.321241 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 17:17:48.227358 139917992580928 spec.py:326] Evaluating on the test split.
I0420 17:18:07.956207 139917992580928 submission_runner.py:406] Time since start: 17681.92s, 	Step: 14523, 	{'train/ctc_loss': DeviceArray(0.42855743, dtype=float32), 'train/wer': 0.1448429157892498, 'validation/ctc_loss': DeviceArray(0.7710321, dtype=float32), 'validation/wer': 0.22068712674507232, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4713283, dtype=float32), 'test/wer': 0.15142282615318994, 'test/num_examples': 2472, 'score': 16861.19521832466, 'total_duration': 17681.922884702682, 'accumulated_submission_time': 16861.19521832466, 'accumulated_eval_time': 815.3937335014343, 'accumulated_logging_time': 5.161632537841797}
I0420 17:18:07.979804 139743360194304 logging_writer.py:48] [14523] accumulated_eval_time=815.393734, accumulated_logging_time=5.161633, accumulated_submission_time=16861.195218, global_step=14523, preemption_count=0, score=16861.195218, test/ctc_loss=0.4713282883167267, test/num_examples=2472, test/wer=0.151423, total_duration=17681.922885, train/ctc_loss=0.428557425737381, train/wer=0.144843, validation/ctc_loss=0.7710320949554443, validation/num_examples=5348, validation/wer=0.220687
I0420 17:18:08.136769 139917992580928 checkpoints.py:356] Saving checkpoint at step: 14523
I0420 17:18:08.871032 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_14523
I0420 17:18:08.888070 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_14523.
I0420 17:19:36.733130 139743351801600 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.4731481075286865, loss=1.644266963005066
I0420 17:21:31.018103 139741206927104 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.4192659854888916, loss=1.680433750152588
I0420 17:23:28.563783 139743351801600 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.7838902473449707, loss=1.6895291805267334
I0420 17:25:24.541104 139741206927104 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.4787354469299316, loss=1.6610691547393799
I0420 17:27:17.613322 139743351801600 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.163168907165527, loss=1.6894789934158325
I0420 17:29:09.516500 139741206927104 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.91758131980896, loss=1.753653645515442
I0420 17:31:01.468743 139743351801600 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.4884259700775146, loss=1.6422260999679565
I0420 17:32:54.218043 139741206927104 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.848068952560425, loss=1.574506402015686
I0420 17:34:48.072719 139743351801600 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.7644872665405273, loss=1.6624654531478882
I0420 17:36:43.800193 139743360194304 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.07492995262146, loss=1.6227941513061523
I0420 17:38:35.940323 139743351801600 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.5783677101135254, loss=1.6660878658294678
I0420 17:40:27.683120 139743360194304 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.8236355781555176, loss=1.656431794166565
I0420 17:42:19.457746 139743351801600 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.3167459964752197, loss=1.6802572011947632
I0420 17:44:12.327485 139743360194304 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.218601942062378, loss=1.6113841533660889
I0420 17:46:03.227952 139917992580928 spec.py:298] Evaluating on the training split.
I0420 17:46:43.006264 139917992580928 spec.py:310] Evaluating on the validation split.
I0420 17:47:22.075044 139917992580928 spec.py:326] Evaluating on the test split.
I0420 17:47:42.683917 139917992580928 submission_runner.py:406] Time since start: 19456.65s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.3744063, dtype=float32), 'train/wer': 0.13300917860578798, 'validation/ctc_loss': DeviceArray(0.7581941, dtype=float32), 'validation/wer': 0.21672182076045113, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.461176, dtype=float32), 'test/wer': 0.1477667418195113, 'test/num_examples': 2472, 'score': 18535.515899658203, 'total_duration': 19456.650873422623, 'accumulated_submission_time': 18535.515899658203, 'accumulated_eval_time': 914.8463470935822, 'accumulated_logging_time': 6.0958452224731445}
I0420 17:47:42.708832 139743513794304 logging_writer.py:48] [16000] accumulated_eval_time=914.846347, accumulated_logging_time=6.095845, accumulated_submission_time=18535.515900, global_step=16000, preemption_count=0, score=18535.515900, test/ctc_loss=0.4611760079860687, test/num_examples=2472, test/wer=0.147767, total_duration=19456.650873, train/ctc_loss=0.3744063079357147, train/wer=0.133009, validation/ctc_loss=0.758194088935852, validation/num_examples=5348, validation/wer=0.216722
I0420 17:47:42.859729 139917992580928 checkpoints.py:356] Saving checkpoint at step: 16000
I0420 17:47:43.666744 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0420 17:47:43.683577 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0420 17:47:43.696883 139743505401600 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18535.515900
I0420 17:47:43.771706 139917992580928 checkpoints.py:356] Saving checkpoint at step: 16000
I0420 17:47:44.843479 139917992580928 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0420 17:47:44.860774 139917992580928 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0420 17:47:46.104948 139917992580928 submission_runner.py:567] Tuning trial 1/1
I0420 17:47:46.105246 139917992580928 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 17:47:46.111963 139917992580928 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.72194, dtype=float32), 'train/wer': 3.884211731878475, 'validation/ctc_loss': DeviceArray(31.736477, dtype=float32), 'validation/wer': 3.737768815907534, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.82975, dtype=float32), 'test/wer': 3.719618954766112, 'test/num_examples': 2472, 'score': 56.587342262268066, 'total_duration': 227.9857313632965, 'accumulated_submission_time': 56.587342262268066, 'accumulated_eval_time': 171.39825773239136, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2061, {'train/ctc_loss': DeviceArray(6.269429, dtype=float32), 'train/wer': 0.9379260983295916, 'validation/ctc_loss': DeviceArray(6.1423116, dtype=float32), 'validation/wer': 0.8929174425223592, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.0254493, dtype=float32), 'test/wer': 0.8960453354457376, 'test/num_examples': 2472, 'score': 2458.8603105545044, 'total_duration': 2713.715287208557, 'accumulated_submission_time': 2458.8603105545044, 'accumulated_eval_time': 254.31217861175537, 'accumulated_logging_time': 0.5197093486785889, 'global_step': 2061, 'preemption_count': 0}), (4130, {'train/ctc_loss': DeviceArray(1.3599398, dtype=float32), 'train/wer': 0.37822763231494555, 'validation/ctc_loss': DeviceArray(1.8146176, dtype=float32), 'validation/wer': 0.44111375893641036, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.3643454, dtype=float32), 'test/wer': 0.3712347409258018, 'test/num_examples': 2472, 'score': 4859.881725549698, 'total_duration': 5210.953722715378, 'accumulated_submission_time': 4859.881725549698, 'accumulated_eval_time': 349.7811191082001, 'accumulated_logging_time': 1.2431936264038086, 'global_step': 4130, 'preemption_count': 0}), (6206, {'train/ctc_loss': DeviceArray(0.64907223, dtype=float32), 'train/wer': 0.22258066216508268, 'validation/ctc_loss': DeviceArray(1.0638685, dtype=float32), 'validation/wer': 0.2979575297397949, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.714808, dtype=float32), 'test/wer': 0.22899274876607154, 'test/num_examples': 2472, 'score': 7259.9311439991, 'total_duration': 7706.8482401371, 'accumulated_submission_time': 7259.9311439991, 'accumulated_eval_time': 444.8419623374939, 'accumulated_logging_time': 2.0026235580444336, 'global_step': 6206, 'preemption_count': 0}), (8273, {'train/ctc_loss': DeviceArray(5.700151, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(5.7379537, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.5954595, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9660.241299152374, 'total_duration': 10191.032596588135, 'accumulated_submission_time': 9660.241299152374, 'accumulated_eval_time': 527.9029009342194, 'accumulated_logging_time': 2.791566848754883, 'global_step': 8273, 'preemption_count': 0}), (10368, {'train/ctc_loss': DeviceArray(0.6898364, dtype=float32), 'train/wer': 0.21638326556350432, 'validation/ctc_loss': DeviceArray(1.0474675, dtype=float32), 'validation/wer': 0.28466265955291414, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7015874, dtype=float32), 'test/wer': 0.21400280299798916, 'test/num_examples': 2472, 'score': 12060.316445350647, 'total_duration': 12686.584991931915, 'accumulated_submission_time': 12060.316445350647, 'accumulated_eval_time': 622.5756497383118, 'accumulated_logging_time': 3.569930076599121, 'global_step': 10368, 'preemption_count': 0}), (12444, {'train/ctc_loss': DeviceArray(0.48838204, dtype=float32), 'train/wer': 0.16173826760059962, 'validation/ctc_loss': DeviceArray(0.8057666, dtype=float32), 'validation/wer': 0.23038331291184672, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5016021, dtype=float32), 'test/wer': 0.16194422440233178, 'test/num_examples': 2472, 'score': 14460.93118596077, 'total_duration': 15183.051533460617, 'accumulated_submission_time': 14460.93118596077, 'accumulated_eval_time': 717.5612359046936, 'accumulated_logging_time': 4.411169052124023, 'global_step': 12444, 'preemption_count': 0}), (14523, {'train/ctc_loss': DeviceArray(0.42855743, dtype=float32), 'train/wer': 0.1448429157892498, 'validation/ctc_loss': DeviceArray(0.7710321, dtype=float32), 'validation/wer': 0.22068712674507232, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.4713283, dtype=float32), 'test/wer': 0.15142282615318994, 'test/num_examples': 2472, 'score': 16861.19521832466, 'total_duration': 17681.922884702682, 'accumulated_submission_time': 16861.19521832466, 'accumulated_eval_time': 815.3937335014343, 'accumulated_logging_time': 5.161632537841797, 'global_step': 14523, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.3744063, dtype=float32), 'train/wer': 0.13300917860578798, 'validation/ctc_loss': DeviceArray(0.7581941, dtype=float32), 'validation/wer': 0.21672182076045113, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.461176, dtype=float32), 'test/wer': 0.1477667418195113, 'test/num_examples': 2472, 'score': 18535.515899658203, 'total_duration': 19456.650873422623, 'accumulated_submission_time': 18535.515899658203, 'accumulated_eval_time': 914.8463470935822, 'accumulated_logging_time': 6.0958452224731445, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0420 17:47:46.112153 139917992580928 submission_runner.py:570] Timing: 18535.515899658203
I0420 17:47:46.112208 139917992580928 submission_runner.py:571] ====================
I0420 17:47:46.112787 139917992580928 submission_runner.py:631] Final librispeech_deepspeech score: 18535.515899658203
