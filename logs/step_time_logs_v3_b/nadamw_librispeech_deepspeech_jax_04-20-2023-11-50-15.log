I0420 11:50:36.269520 139802087286592 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax.
I0420 11:50:36.346217 139802087286592 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 11:50:37.222570 139802087286592 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0420 11:50:37.223200 139802087286592 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0420 11:50:37.227164 139802087286592 submission_runner.py:528] Using RNG seed 3738863218
I0420 11:50:39.981468 139802087286592 submission_runner.py:537] --- Tuning run 1/1 ---
I0420 11:50:39.981659 139802087286592 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1.
I0420 11:50:39.981935 139802087286592 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0420 11:50:40.107280 139802087286592 submission_runner.py:232] Initializing dataset.
I0420 11:50:40.107470 139802087286592 submission_runner.py:239] Initializing model.
I0420 11:50:57.226706 139802087286592 submission_runner.py:249] Initializing optimizer.
I0420 11:50:57.870846 139802087286592 submission_runner.py:256] Initializing metrics bundle.
I0420 11:50:57.871043 139802087286592 submission_runner.py:273] Initializing checkpoint and logger.
I0420 11:50:57.872305 139802087286592 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0420 11:50:57.872570 139802087286592 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0420 11:50:57.872631 139802087286592 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0420 11:50:58.825034 139802087286592 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0420 11:50:58.825944 139802087286592 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0420 11:50:58.833797 139802087286592 submission_runner.py:309] Starting training loop.
I0420 11:50:59.037693 139802087286592 input_pipeline.py:20] Loading split = train-clean-100
I0420 11:50:59.069104 139802087286592 input_pipeline.py:20] Loading split = train-clean-360
I0420 11:50:59.375014 139802087286592 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0420 11:51:56.825008 139626459145984 logging_writer.py:48] [0] global_step=0, grad_norm=21.822467803955078, loss=32.95185089111328
I0420 11:51:56.850072 139802087286592 spec.py:298] Evaluating on the training split.
I0420 11:51:56.978229 139802087286592 input_pipeline.py:20] Loading split = train-clean-100
I0420 11:51:57.004515 139802087286592 input_pipeline.py:20] Loading split = train-clean-360
I0420 11:51:57.279959 139802087286592 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0420 11:53:28.297879 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 11:53:28.393311 139802087286592 input_pipeline.py:20] Loading split = dev-clean
I0420 11:53:28.398265 139802087286592 input_pipeline.py:20] Loading split = dev-other
I0420 11:54:22.947263 139802087286592 spec.py:326] Evaluating on the test split.
I0420 11:54:23.040831 139802087286592 input_pipeline.py:20] Loading split = test-clean
I0420 11:54:57.870701 139802087286592 submission_runner.py:406] Time since start: 239.04s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.27572, dtype=float32), 'train/wer': 4.338395143993503, 'validation/ctc_loss': DeviceArray(31.099588, dtype=float32), 'validation/wer': 3.883848372873834, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.214, dtype=float32), 'test/wer': 4.165356569780432, 'test/num_examples': 2472, 'score': 58.016090631484985, 'total_duration': 239.03518199920654, 'accumulated_submission_time': 58.016090631484985, 'accumulated_eval_time': 181.0189299583435, 'accumulated_logging_time': 0}
I0420 11:54:57.896018 139623363761920 logging_writer.py:48] [1] accumulated_eval_time=181.018930, accumulated_logging_time=0, accumulated_submission_time=58.016091, global_step=1, preemption_count=0, score=58.016091, test/ctc_loss=31.214000701904297, test/num_examples=2472, test/wer=4.165357, total_duration=239.035182, train/ctc_loss=32.275718688964844, train/wer=4.338395, validation/ctc_loss=31.09958839416504, validation/num_examples=5348, validation/wer=3.883848
I0420 11:54:58.044465 139802087286592 checkpoints.py:356] Saving checkpoint at step: 1
I0420 11:54:58.446094 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0420 11:54:58.447036 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0420 11:57:12.278782 139627114612480 logging_writer.py:48] [100] global_step=100, grad_norm=5.382140636444092, loss=9.070680618286133
I0420 11:59:08.582946 139627123005184 logging_writer.py:48] [200] global_step=200, grad_norm=0.822883665561676, loss=6.173194885253906
I0420 12:01:04.597577 139627114612480 logging_writer.py:48] [300] global_step=300, grad_norm=0.382285475730896, loss=5.902701377868652
I0420 12:03:00.639770 139627123005184 logging_writer.py:48] [400] global_step=400, grad_norm=0.41757944226264954, loss=5.820046424865723
I0420 12:04:56.814639 139627114612480 logging_writer.py:48] [500] global_step=500, grad_norm=0.42361581325531006, loss=5.781426906585693
I0420 12:06:53.560466 139627123005184 logging_writer.py:48] [600] global_step=600, grad_norm=0.9612606763839722, loss=5.732216835021973
I0420 12:08:50.002173 139627114612480 logging_writer.py:48] [700] global_step=700, grad_norm=0.3975503146648407, loss=5.643712520599365
I0420 12:10:46.399627 139627123005184 logging_writer.py:48] [800] global_step=800, grad_norm=0.6533264517784119, loss=5.435786724090576
I0420 12:12:42.816401 139627114612480 logging_writer.py:48] [900] global_step=900, grad_norm=0.9610376954078674, loss=5.107685089111328
I0420 12:14:39.145030 139627123005184 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7826916575431824, loss=4.762308120727539
I0420 12:16:38.232726 139627912648448 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5310038328170776, loss=4.452386856079102
I0420 12:18:33.962392 139627904255744 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1835178136825562, loss=4.151994705200195
I0420 12:20:29.704181 139627912648448 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.4171109199523926, loss=3.9173693656921387
I0420 12:22:25.677247 139627904255744 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7772321701049805, loss=3.729836940765381
I0420 12:24:21.617630 139627912648448 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.227858066558838, loss=3.5704143047332764
I0420 12:26:17.729199 139627904255744 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.947852611541748, loss=3.4682703018188477
I0420 12:28:14.078358 139627912648448 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.7881245613098145, loss=3.3601222038269043
I0420 12:30:09.664279 139627904255744 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.7911409139633179, loss=3.3223042488098145
I0420 12:32:04.994051 139627912648448 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.7441916465759277, loss=3.166863203048706
I0420 12:34:00.787011 139627904255744 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.850328207015991, loss=3.0354394912719727
I0420 12:34:59.384722 139802087286592 spec.py:298] Evaluating on the training split.
I0420 12:35:29.198924 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 12:36:04.181438 139802087286592 spec.py:326] Evaluating on the test split.
I0420 12:36:21.906970 139802087286592 submission_runner.py:406] Time since start: 2723.07s, 	Step: 2052, 	{'train/ctc_loss': DeviceArray(5.794315, dtype=float32), 'train/wer': 0.9284309283021976, 'validation/ctc_loss': DeviceArray(5.7590656, dtype=float32), 'validation/wer': 0.8875821281440246, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6509447, dtype=float32), 'test/wer': 0.8885097394024333, 'test/num_examples': 2472, 'score': 2458.924425125122, 'total_duration': 2723.0697429180145, 'accumulated_submission_time': 2458.924425125122, 'accumulated_eval_time': 263.53778862953186, 'accumulated_logging_time': 0.5816538333892822}
I0420 12:36:21.926522 139627912648448 logging_writer.py:48] [2052] accumulated_eval_time=263.537789, accumulated_logging_time=0.581654, accumulated_submission_time=2458.924425, global_step=2052, preemption_count=0, score=2458.924425, test/ctc_loss=5.650944709777832, test/num_examples=2472, test/wer=0.888510, total_duration=2723.069743, train/ctc_loss=5.794314861297607, train/wer=0.928431, validation/ctc_loss=5.759065628051758, validation/num_examples=5348, validation/wer=0.887582
I0420 12:36:22.079575 139802087286592 checkpoints.py:356] Saving checkpoint at step: 2052
I0420 12:36:22.715216 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_2052
I0420 12:36:22.727001 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_2052.
I0420 12:37:22.530905 139627257288448 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.4151523113250732, loss=2.8998868465423584
I0420 12:39:17.847076 139627248895744 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.15305233001709, loss=2.9019293785095215
I0420 12:41:13.443100 139627257288448 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.0703792572021484, loss=2.812812089920044
I0420 12:43:08.793770 139627248895744 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.3872265815734863, loss=2.8165297508239746
I0420 12:45:04.984440 139627257288448 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.1718037128448486, loss=2.783099889755249
I0420 12:47:01.003424 139627248895744 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.07460355758667, loss=2.7292234897613525
I0420 12:48:57.653173 139627257288448 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.5466086864471436, loss=2.6360790729522705
I0420 12:50:53.576545 139627248895744 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.933882713317871, loss=2.6420416831970215
I0420 12:52:49.618289 139627257288448 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.445744037628174, loss=2.598414659500122
I0420 12:54:45.599772 139627248895744 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.334522247314453, loss=2.4913933277130127
I0420 12:56:44.269710 139627912648448 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.9769880771636963, loss=2.490631580352783
I0420 12:58:39.983475 139627904255744 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.679902076721191, loss=2.493110418319702
I0420 13:00:35.418115 139627912648448 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.525916814804077, loss=2.4124808311462402
I0420 13:02:31.230271 139627904255744 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.9502389430999756, loss=2.3675248622894287
I0420 13:04:27.045852 139627912648448 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.184969186782837, loss=2.384589672088623
I0420 13:06:23.068851 139627904255744 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.193518877029419, loss=2.2808573246002197
I0420 13:08:19.063787 139627912648448 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.140997886657715, loss=2.335423469543457
I0420 13:10:15.220381 139627904255744 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.7151782512664795, loss=2.3398029804229736
I0420 13:12:10.733840 139627912648448 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.085757255554199, loss=2.3264639377593994
I0420 13:14:06.426164 139627904255744 logging_writer.py:48] [4000] global_step=4000, grad_norm=8.782259941101074, loss=2.2757837772369385
I0420 13:16:02.146768 139627912648448 logging_writer.py:48] [4100] global_step=4100, grad_norm=8.73527717590332, loss=2.210911512374878
I0420 13:16:23.743556 139802087286592 spec.py:298] Evaluating on the training split.
I0420 13:17:01.626023 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 13:17:38.501167 139802087286592 spec.py:326] Evaluating on the test split.
I0420 13:17:57.368999 139802087286592 submission_runner.py:406] Time since start: 5218.53s, 	Step: 4120, 	{'train/ctc_loss': DeviceArray(1.7810379, dtype=float32), 'train/wer': 0.450949577817436, 'validation/ctc_loss': DeviceArray(2.234496, dtype=float32), 'validation/wer': 0.5060444384412778, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.7670053, dtype=float32), 'test/wer': 0.4392175979525928, 'test/num_examples': 2472, 'score': 4859.910271406174, 'total_duration': 5218.531920433044, 'accumulated_submission_time': 4859.910271406174, 'accumulated_eval_time': 357.15998935699463, 'accumulated_logging_time': 1.4071693420410156}
I0420 13:17:57.388788 139627912648448 logging_writer.py:48] [4120] accumulated_eval_time=357.159989, accumulated_logging_time=1.407169, accumulated_submission_time=4859.910271, global_step=4120, preemption_count=0, score=4859.910271, test/ctc_loss=1.7670053243637085, test/num_examples=2472, test/wer=0.439218, total_duration=5218.531920, train/ctc_loss=1.7810379266738892, train/wer=0.450950, validation/ctc_loss=2.2344961166381836, validation/num_examples=5348, validation/wer=0.506044
I0420 13:17:57.539064 139802087286592 checkpoints.py:356] Saving checkpoint at step: 4120
I0420 13:17:57.988454 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_4120
I0420 13:17:57.989426 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_4120.
I0420 13:19:34.606064 139627912648448 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.69981050491333, loss=2.186906576156616
I0420 13:21:29.860536 139627904255744 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.1389787197113037, loss=2.1657514572143555
I0420 13:23:25.293582 139627912648448 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.502188682556152, loss=2.1292781829833984
I0420 13:25:21.360749 139627904255744 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.52006459236145, loss=2.1076486110687256
I0420 13:27:17.257776 139627912648448 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.447756767272949, loss=2.1810734272003174
I0420 13:29:13.302267 139627904255744 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.638882637023926, loss=2.1451492309570312
I0420 13:31:09.269217 139627912648448 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.0676095485687256, loss=2.1746737957000732
I0420 13:33:05.079120 139627904255744 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.086345672607422, loss=2.1083383560180664
I0420 13:35:01.043834 139627912648448 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.9788706302642822, loss=2.0580592155456543
I0420 13:36:56.692928 139627904255744 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.4081692695617676, loss=2.1123504638671875
I0420 13:38:55.561762 139627912648448 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.373605489730835, loss=2.0690438747406006
I0420 13:40:51.263866 139627904255744 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.0438575744628906, loss=2.023879289627075
I0420 13:42:46.692571 139627912648448 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.049208641052246, loss=2.0594844818115234
I0420 13:44:42.590082 139627904255744 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.3389675617218018, loss=2.011758327484131
I0420 13:46:39.165568 139627912648448 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.5698862075805664, loss=1.9572415351867676
I0420 13:48:34.749114 139627904255744 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.794137477874756, loss=1.975050449371338
I0420 13:50:30.427141 139627912648448 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.1306416988372803, loss=1.903071403503418
I0420 13:52:26.088179 139627904255744 logging_writer.py:48] [5900] global_step=5900, grad_norm=4.423792362213135, loss=1.996654748916626
I0420 13:54:21.723946 139627912648448 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.760422468185425, loss=1.929980754852295
I0420 13:56:17.501717 139627904255744 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.32970929145813, loss=1.9353382587432861
I0420 13:57:58.963514 139802087286592 spec.py:298] Evaluating on the training split.
I0420 13:58:38.766275 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 13:59:17.666408 139802087286592 spec.py:326] Evaluating on the test split.
I0420 13:59:37.424900 139802087286592 submission_runner.py:406] Time since start: 7718.59s, 	Step: 6186, 	{'train/ctc_loss': DeviceArray(0.70387995, dtype=float32), 'train/wer': 0.24401326227788583, 'validation/ctc_loss': DeviceArray(1.1258361, dtype=float32), 'validation/wer': 0.3135486111781107, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7779258, dtype=float32), 'test/wer': 0.25036053053845997, 'test/num_examples': 2472, 'score': 7260.854313135147, 'total_duration': 7718.586970567703, 'accumulated_submission_time': 7260.854313135147, 'accumulated_eval_time': 455.61749720573425, 'accumulated_logging_time': 2.0326685905456543}
I0420 13:59:37.445068 139626673608448 logging_writer.py:48] [6186] accumulated_eval_time=455.617497, accumulated_logging_time=2.032669, accumulated_submission_time=7260.854313, global_step=6186, preemption_count=0, score=7260.854313, test/ctc_loss=0.7779257893562317, test/num_examples=2472, test/wer=0.250361, total_duration=7718.586971, train/ctc_loss=0.7038799524307251, train/wer=0.244013, validation/ctc_loss=1.1258361339569092, validation/num_examples=5348, validation/wer=0.313549
I0420 13:59:37.599221 139802087286592 checkpoints.py:356] Saving checkpoint at step: 6186
I0420 13:59:38.222279 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_6186
I0420 13:59:38.237932 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_6186.
I0420 13:59:55.575434 139626665215744 logging_writer.py:48] [6200] global_step=6200, grad_norm=4.6263227462768555, loss=1.9786059856414795
I0420 14:01:50.717841 139624521398016 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.664917230606079, loss=1.9519957304000854
I0420 14:03:46.167620 139626665215744 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.6014368534088135, loss=1.89495050907135
I0420 14:05:41.538136 139624521398016 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.4067132472991943, loss=1.926408290863037
I0420 14:07:37.574187 139626665215744 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.3293826580047607, loss=1.9235728979110718
I0420 14:09:33.222572 139624521398016 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.52779483795166, loss=1.8917747735977173
I0420 14:11:29.042731 139626665215744 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.076397895812988, loss=1.96345055103302
I0420 14:13:24.928138 139624521398016 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.8735802173614502, loss=1.8759808540344238
I0420 14:15:20.767074 139626665215744 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.43206524848938, loss=1.7889504432678223
I0420 14:17:16.752280 139624521398016 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.1889290809631348, loss=1.8171651363372803
I0420 14:19:12.307407 139626665215744 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.9319260120391846, loss=1.8572499752044678
I0420 14:21:11.852246 139626673608448 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.1443254947662354, loss=1.841557264328003
I0420 14:23:07.078027 139626665215744 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.860100030899048, loss=1.848331093788147
I0420 14:25:03.263987 139626673608448 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.047599792480469, loss=1.8354740142822266
I0420 14:26:59.324313 139626665215744 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.2833306789398193, loss=1.8285493850708008
I0420 14:28:55.181492 139626673608448 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.7309412956237793, loss=1.828129768371582
I0420 14:30:51.656661 139626665215744 logging_writer.py:48] [7800] global_step=7800, grad_norm=4.14990234375, loss=1.953218698501587
I0420 14:32:48.248059 139626673608448 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.2515807151794434, loss=1.8394333124160767
I0420 14:34:44.136098 139626665215744 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.44746732711792, loss=1.806710124015808
I0420 14:36:39.893270 139626673608448 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.316263675689697, loss=1.8927897214889526
I0420 14:38:35.763098 139626665215744 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.308593273162842, loss=1.8162939548492432
I0420 14:39:38.833539 139802087286592 spec.py:298] Evaluating on the training split.
I0420 14:40:19.387343 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 14:40:57.323126 139802087286592 spec.py:326] Evaluating on the test split.
I0420 14:41:17.413610 139802087286592 submission_runner.py:406] Time since start: 10218.58s, 	Step: 8253, 	{'train/ctc_loss': DeviceArray(0.5554383, dtype=float32), 'train/wer': 0.18907937039868467, 'validation/ctc_loss': DeviceArray(0.92947006, dtype=float32), 'validation/wer': 0.264141477486517, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5974571, dtype=float32), 'test/wer': 0.19409745495907216, 'test/num_examples': 2472, 'score': 9661.420640945435, 'total_duration': 10218.576330423355, 'accumulated_submission_time': 9661.420640945435, 'accumulated_eval_time': 554.1943180561066, 'accumulated_logging_time': 2.8508787155151367}
I0420 14:41:17.435595 139627912648448 logging_writer.py:48] [8253] accumulated_eval_time=554.194318, accumulated_logging_time=2.850879, accumulated_submission_time=9661.420641, global_step=8253, preemption_count=0, score=9661.420641, test/ctc_loss=0.5974571108818054, test/num_examples=2472, test/wer=0.194097, total_duration=10218.576330, train/ctc_loss=0.5554382801055908, train/wer=0.189079, validation/ctc_loss=0.9294700622558594, validation/num_examples=5348, validation/wer=0.264141
I0420 14:41:17.592691 139802087286592 checkpoints.py:356] Saving checkpoint at step: 8253
I0420 14:41:18.242265 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8253
I0420 14:41:18.257655 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_8253.
I0420 14:42:14.011536 139627904255744 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.134230613708496, loss=1.7581276893615723
I0420 14:44:09.309071 139627837114112 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.2894787788391113, loss=1.8249064683914185
I0420 14:46:05.202436 139627904255744 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.7390739917755127, loss=1.7867330312728882
I0420 14:48:01.870259 139627837114112 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.948967456817627, loss=1.7569489479064941
I0420 14:49:57.798981 139627904255744 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.6555309295654297, loss=1.763244390487671
I0420 14:51:53.787444 139627837114112 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.777761936187744, loss=1.7707451581954956
I0420 14:53:50.504841 139627904255744 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.836554527282715, loss=1.797163486480713
I0420 14:55:47.470451 139627837114112 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.6128172874450684, loss=1.7875639200210571
I0420 14:57:43.284606 139627904255744 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.9841244220733643, loss=1.861755132675171
I0420 14:59:39.323360 139627837114112 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.7025229930877686, loss=1.7289955615997314
I0420 15:01:38.766314 139626673608448 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.599524736404419, loss=1.7668054103851318
I0420 15:03:34.213518 139626665215744 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.399700164794922, loss=1.745435357093811
I0420 15:05:29.587089 139626673608448 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.636390447616577, loss=1.748608112335205
I0420 15:07:25.248913 139626665215744 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.7457711696624756, loss=1.7594748735427856
I0420 15:09:21.524142 139626673608448 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.6987977027893066, loss=1.6639341115951538
I0420 15:11:18.734385 139626665215744 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.9734272956848145, loss=1.696496605873108
I0420 15:13:15.282603 139626673608448 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.87619686126709, loss=1.7568995952606201
I0420 15:15:11.463679 139626665215744 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.406437873840332, loss=1.8120594024658203
I0420 15:17:07.007750 139626673608448 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.944110155105591, loss=1.769187331199646
I0420 15:19:03.848026 139626665215744 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.721503257751465, loss=1.697832465171814
I0420 15:21:02.896642 139627912648448 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.6702659130096436, loss=1.6815593242645264
I0420 15:21:18.949408 139802087286592 spec.py:298] Evaluating on the training split.
I0420 15:21:58.160223 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 15:22:36.033867 139802087286592 spec.py:326] Evaluating on the test split.
I0420 15:22:56.130736 139802087286592 submission_runner.py:406] Time since start: 12717.29s, 	Step: 10315, 	{'train/ctc_loss': DeviceArray(0.4516884, dtype=float32), 'train/wer': 0.15722237926460675, 'validation/ctc_loss': DeviceArray(0.8084371, dtype=float32), 'validation/wer': 0.22977549228646682, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5046656, dtype=float32), 'test/wer': 0.16614872138606218, 'test/num_examples': 2472, 'score': 12062.082273721695, 'total_duration': 12717.29378604889, 'accumulated_submission_time': 12062.082273721695, 'accumulated_eval_time': 651.3725554943085, 'accumulated_logging_time': 3.700324296951294}
I0420 15:22:56.150878 139627482568448 logging_writer.py:48] [10315] accumulated_eval_time=651.372555, accumulated_logging_time=3.700324, accumulated_submission_time=12062.082274, global_step=10315, preemption_count=0, score=12062.082274, test/ctc_loss=0.5046656131744385, test/num_examples=2472, test/wer=0.166149, total_duration=12717.293786, train/ctc_loss=0.45168840885162354, train/wer=0.157222, validation/ctc_loss=0.8084371089935303, validation/num_examples=5348, validation/wer=0.229775
I0420 15:22:56.308044 139802087286592 checkpoints.py:356] Saving checkpoint at step: 10315
I0420 15:22:56.855902 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_10315
I0420 15:22:56.871270 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_10315.
I0420 15:24:35.928633 139627474175744 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.616175651550293, loss=1.6629631519317627
I0420 15:26:31.022618 139627398641408 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.904358386993408, loss=1.7277625799179077
I0420 15:28:26.542616 139627474175744 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.4445066452026367, loss=1.7122114896774292
I0420 15:30:22.299004 139627398641408 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.274207592010498, loss=1.7226332426071167
I0420 15:32:17.949472 139627474175744 logging_writer.py:48] [10800] global_step=10800, grad_norm=4.3714189529418945, loss=1.7553143501281738
I0420 15:34:13.882019 139627398641408 logging_writer.py:48] [10900] global_step=10900, grad_norm=6.303901672363281, loss=1.671818494796753
I0420 15:36:09.877393 139627474175744 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.676098346710205, loss=1.722200632095337
I0420 15:38:05.823505 139627398641408 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.5637221336364746, loss=1.6892461776733398
I0420 15:40:01.898074 139627474175744 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.9532833099365234, loss=1.7104445695877075
I0420 15:41:58.070525 139627398641408 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.556058406829834, loss=1.7420713901519775
I0420 15:43:57.096038 139627482568448 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.4977867603302, loss=1.604237675666809
I0420 15:45:52.309502 139627474175744 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.9260005950927734, loss=1.697675108909607
I0420 15:47:47.466904 139627482568448 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.180192232131958, loss=1.698934555053711
I0420 15:49:42.843862 139627474175744 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.6698102951049805, loss=1.6569132804870605
I0420 15:51:38.708171 139627482568448 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.9595351219177246, loss=1.6557766199111938
I0420 15:53:34.533413 139627474175744 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.5837156772613525, loss=1.6427875757217407
I0420 15:55:30.403491 139627482568448 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.746549606323242, loss=1.6688835620880127
I0420 15:57:27.133692 139627474175744 logging_writer.py:48] [12100] global_step=12100, grad_norm=4.835149765014648, loss=1.6867307424545288
I0420 15:59:22.974241 139627482568448 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.877376079559326, loss=1.6648280620574951
I0420 16:01:18.770518 139627474175744 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.658807754516602, loss=1.6702284812927246
I0420 16:02:57.615795 139802087286592 spec.py:298] Evaluating on the training split.
I0420 16:03:37.488545 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 16:04:15.399266 139802087286592 spec.py:326] Evaluating on the test split.
I0420 16:04:35.564321 139802087286592 submission_runner.py:406] Time since start: 15216.73s, 	Step: 12384, 	{'train/ctc_loss': DeviceArray(0.43703285, dtype=float32), 'train/wer': 0.1484761350966386, 'validation/ctc_loss': DeviceArray(0.7608442, dtype=float32), 'validation/wer': 0.21663498924253974, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46055803, dtype=float32), 'test/wer': 0.1513618914142953, 'test/num_examples': 2472, 'score': 14462.799909591675, 'total_duration': 15216.72711777687, 'accumulated_submission_time': 14462.799909591675, 'accumulated_eval_time': 749.317911863327, 'accumulated_logging_time': 4.443044900894165}
I0420 16:04:35.585309 139627912648448 logging_writer.py:48] [12384] accumulated_eval_time=749.317912, accumulated_logging_time=4.443045, accumulated_submission_time=14462.799910, global_step=12384, preemption_count=0, score=14462.799910, test/ctc_loss=0.4605580270290375, test/num_examples=2472, test/wer=0.151362, total_duration=15216.727118, train/ctc_loss=0.4370328485965729, train/wer=0.148476, validation/ctc_loss=0.7608441710472107, validation/num_examples=5348, validation/wer=0.216635
I0420 16:04:35.739804 139802087286592 checkpoints.py:356] Saving checkpoint at step: 12384
I0420 16:04:36.438636 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_12384
I0420 16:04:36.454945 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_12384.
I0420 16:04:56.097044 139627904255744 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.8542494773864746, loss=1.56534743309021
I0420 16:06:51.739333 139627820328704 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.6745805740356445, loss=1.6518263816833496
I0420 16:08:46.815679 139627904255744 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.877222061157227, loss=1.6651781797409058
I0420 16:10:42.610754 139627820328704 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.347214698791504, loss=1.622997760772705
I0420 16:12:38.640908 139627904255744 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.867953300476074, loss=1.6325637102127075
I0420 16:14:34.589454 139627820328704 logging_writer.py:48] [12900] global_step=12900, grad_norm=4.353846549987793, loss=1.6591551303863525
I0420 16:16:30.384736 139627904255744 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.627610206604004, loss=1.68153977394104
I0420 16:18:26.064568 139627820328704 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.808540344238281, loss=1.6509180068969727
I0420 16:20:21.728372 139627904255744 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.12755012512207, loss=1.6083568334579468
I0420 16:22:17.308421 139627820328704 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.986979961395264, loss=1.6935198307037354
I0420 16:24:16.089452 139627584968448 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.63098406791687, loss=1.6797038316726685
I0420 16:26:11.616543 139627576575744 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.054510116577148, loss=1.6274923086166382
I0420 16:28:06.629457 139627584968448 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.290017604827881, loss=1.6151963472366333
I0420 16:30:01.685986 139627576575744 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.507214546203613, loss=1.6446645259857178
I0420 16:31:56.826032 139627584968448 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.1940202713012695, loss=1.6727312803268433
I0420 16:33:52.826927 139627576575744 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.392574310302734, loss=1.5706859827041626
I0420 16:35:48.764263 139627584968448 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.167566299438477, loss=1.649903416633606
I0420 16:37:45.387485 139627576575744 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.402449131011963, loss=1.6602177619934082
I0420 16:39:41.091111 139627584968448 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.579894781112671, loss=1.621453046798706
I0420 16:41:37.139023 139627576575744 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.591193437576294, loss=1.5752900838851929
I0420 16:43:32.627180 139627584968448 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.680233955383301, loss=1.6062989234924316
I0420 16:44:37.366684 139802087286592 spec.py:298] Evaluating on the training split.
I0420 16:45:17.297648 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 16:45:55.141165 139802087286592 spec.py:326] Evaluating on the test split.
I0420 16:46:15.222438 139802087286592 submission_runner.py:406] Time since start: 17716.39s, 	Step: 14454, 	{'train/ctc_loss': DeviceArray(0.37520102, dtype=float32), 'train/wer': 0.12992682874949482, 'validation/ctc_loss': DeviceArray(0.69923735, dtype=float32), 'validation/wer': 0.20148771334021554, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.42496568, dtype=float32), 'test/wer': 0.13947961732983974, 'test/num_examples': 2472, 'score': 16863.680718183517, 'total_duration': 17716.385192155838, 'accumulated_submission_time': 16863.680718183517, 'accumulated_eval_time': 847.170530796051, 'accumulated_logging_time': 5.339754343032837}
I0420 16:46:15.242985 139627001288448 logging_writer.py:48] [14454] accumulated_eval_time=847.170531, accumulated_logging_time=5.339754, accumulated_submission_time=16863.680718, global_step=14454, preemption_count=0, score=16863.680718, test/ctc_loss=0.42496567964553833, test/num_examples=2472, test/wer=0.139480, total_duration=17716.385192, train/ctc_loss=0.375201016664505, train/wer=0.129927, validation/ctc_loss=0.6992373466491699, validation/num_examples=5348, validation/wer=0.201488
I0420 16:46:15.388378 139802087286592 checkpoints.py:356] Saving checkpoint at step: 14454
I0420 16:46:16.026243 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_14454
I0420 16:46:16.041623 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_14454.
I0420 16:47:10.330267 139626992895744 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.7332282066345215, loss=1.6022484302520752
I0420 16:49:05.264523 139626900576000 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.5584449768066406, loss=1.6708197593688965
I0420 16:51:00.100452 139626992895744 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.3417463302612305, loss=1.529563307762146
I0420 16:52:55.237636 139626900576000 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.7149922847747803, loss=1.5644009113311768
I0420 16:54:51.734641 139626992895744 logging_writer.py:48] [14900] global_step=14900, grad_norm=4.164835453033447, loss=1.641720175743103
I0420 16:56:47.661255 139626900576000 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.520954608917236, loss=1.6182705163955688
I0420 16:58:43.205816 139626992895744 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.589172124862671, loss=1.5650997161865234
I0420 17:00:39.145646 139626900576000 logging_writer.py:48] [15200] global_step=15200, grad_norm=5.499513626098633, loss=1.5924227237701416
I0420 17:02:35.336942 139626992895744 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.700089931488037, loss=1.5830559730529785
I0420 17:04:30.937131 139626900576000 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.4817872047424316, loss=1.588463306427002
I0420 17:06:29.653189 139627001288448 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.684946298599243, loss=1.6121814250946045
I0420 17:08:24.738400 139626992895744 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.118757247924805, loss=1.649912714958191
I0420 17:10:20.236925 139627001288448 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.260059356689453, loss=1.6406368017196655
I0420 17:12:15.185504 139626992895744 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.4844651222229, loss=1.6280772686004639
I0420 17:14:10.395278 139627001288448 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.179701089859009, loss=1.6059391498565674
I0420 17:16:05.054374 139802087286592 spec.py:298] Evaluating on the training split.
I0420 17:16:44.714607 139802087286592 spec.py:310] Evaluating on the validation split.
I0420 17:17:22.819733 139802087286592 spec.py:326] Evaluating on the test split.
I0420 17:17:42.373516 139802087286592 submission_runner.py:406] Time since start: 19603.54s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(0.32596782, dtype=float32), 'train/wer': 0.11659169505187342, 'validation/ctc_loss': DeviceArray(0.68479526, dtype=float32), 'validation/wer': 0.19812058003453964, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41277495, dtype=float32), 'test/wer': 0.13444234558121584, 'test/num_examples': 2472, 'score': 18652.673414945602, 'total_duration': 19603.53784751892, 'accumulated_submission_time': 18652.673414945602, 'accumulated_eval_time': 944.4878647327423, 'accumulated_logging_time': 6.160890579223633}
I0420 17:17:42.394654 139627620808448 logging_writer.py:48] [16000] accumulated_eval_time=944.487865, accumulated_logging_time=6.160891, accumulated_submission_time=18652.673415, global_step=16000, preemption_count=0, score=18652.673415, test/ctc_loss=0.4127749502658844, test/num_examples=2472, test/wer=0.134442, total_duration=19603.537848, train/ctc_loss=0.32596781849861145, train/wer=0.116592, validation/ctc_loss=0.6847952604293823, validation/num_examples=5348, validation/wer=0.198121
I0420 17:17:42.533942 139802087286592 checkpoints.py:356] Saving checkpoint at step: 16000
I0420 17:17:43.213054 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0420 17:17:43.228503 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0420 17:17:43.240163 139627612415744 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18652.673415
I0420 17:17:43.319186 139802087286592 checkpoints.py:356] Saving checkpoint at step: 16000
I0420 17:17:44.237272 139802087286592 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0420 17:17:44.252849 139802087286592 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nadamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0420 17:17:45.515255 139802087286592 submission_runner.py:567] Tuning trial 1/1
I0420 17:17:45.515495 139802087286592 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0420 17:17:45.521524 139802087286592 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.27572, dtype=float32), 'train/wer': 4.338395143993503, 'validation/ctc_loss': DeviceArray(31.099588, dtype=float32), 'validation/wer': 3.883848372873834, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.214, dtype=float32), 'test/wer': 4.165356569780432, 'test/num_examples': 2472, 'score': 58.016090631484985, 'total_duration': 239.03518199920654, 'accumulated_submission_time': 58.016090631484985, 'accumulated_eval_time': 181.0189299583435, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2052, {'train/ctc_loss': DeviceArray(5.794315, dtype=float32), 'train/wer': 0.9284309283021976, 'validation/ctc_loss': DeviceArray(5.7590656, dtype=float32), 'validation/wer': 0.8875821281440246, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.6509447, dtype=float32), 'test/wer': 0.8885097394024333, 'test/num_examples': 2472, 'score': 2458.924425125122, 'total_duration': 2723.0697429180145, 'accumulated_submission_time': 2458.924425125122, 'accumulated_eval_time': 263.53778862953186, 'accumulated_logging_time': 0.5816538333892822, 'global_step': 2052, 'preemption_count': 0}), (4120, {'train/ctc_loss': DeviceArray(1.7810379, dtype=float32), 'train/wer': 0.450949577817436, 'validation/ctc_loss': DeviceArray(2.234496, dtype=float32), 'validation/wer': 0.5060444384412778, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.7670053, dtype=float32), 'test/wer': 0.4392175979525928, 'test/num_examples': 2472, 'score': 4859.910271406174, 'total_duration': 5218.531920433044, 'accumulated_submission_time': 4859.910271406174, 'accumulated_eval_time': 357.15998935699463, 'accumulated_logging_time': 1.4071693420410156, 'global_step': 4120, 'preemption_count': 0}), (6186, {'train/ctc_loss': DeviceArray(0.70387995, dtype=float32), 'train/wer': 0.24401326227788583, 'validation/ctc_loss': DeviceArray(1.1258361, dtype=float32), 'validation/wer': 0.3135486111781107, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.7779258, dtype=float32), 'test/wer': 0.25036053053845997, 'test/num_examples': 2472, 'score': 7260.854313135147, 'total_duration': 7718.586970567703, 'accumulated_submission_time': 7260.854313135147, 'accumulated_eval_time': 455.61749720573425, 'accumulated_logging_time': 2.0326685905456543, 'global_step': 6186, 'preemption_count': 0}), (8253, {'train/ctc_loss': DeviceArray(0.5554383, dtype=float32), 'train/wer': 0.18907937039868467, 'validation/ctc_loss': DeviceArray(0.92947006, dtype=float32), 'validation/wer': 0.264141477486517, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5974571, dtype=float32), 'test/wer': 0.19409745495907216, 'test/num_examples': 2472, 'score': 9661.420640945435, 'total_duration': 10218.576330423355, 'accumulated_submission_time': 9661.420640945435, 'accumulated_eval_time': 554.1943180561066, 'accumulated_logging_time': 2.8508787155151367, 'global_step': 8253, 'preemption_count': 0}), (10315, {'train/ctc_loss': DeviceArray(0.4516884, dtype=float32), 'train/wer': 0.15722237926460675, 'validation/ctc_loss': DeviceArray(0.8084371, dtype=float32), 'validation/wer': 0.22977549228646682, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5046656, dtype=float32), 'test/wer': 0.16614872138606218, 'test/num_examples': 2472, 'score': 12062.082273721695, 'total_duration': 12717.29378604889, 'accumulated_submission_time': 12062.082273721695, 'accumulated_eval_time': 651.3725554943085, 'accumulated_logging_time': 3.700324296951294, 'global_step': 10315, 'preemption_count': 0}), (12384, {'train/ctc_loss': DeviceArray(0.43703285, dtype=float32), 'train/wer': 0.1484761350966386, 'validation/ctc_loss': DeviceArray(0.7608442, dtype=float32), 'validation/wer': 0.21663498924253974, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.46055803, dtype=float32), 'test/wer': 0.1513618914142953, 'test/num_examples': 2472, 'score': 14462.799909591675, 'total_duration': 15216.72711777687, 'accumulated_submission_time': 14462.799909591675, 'accumulated_eval_time': 749.317911863327, 'accumulated_logging_time': 4.443044900894165, 'global_step': 12384, 'preemption_count': 0}), (14454, {'train/ctc_loss': DeviceArray(0.37520102, dtype=float32), 'train/wer': 0.12992682874949482, 'validation/ctc_loss': DeviceArray(0.69923735, dtype=float32), 'validation/wer': 0.20148771334021554, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.42496568, dtype=float32), 'test/wer': 0.13947961732983974, 'test/num_examples': 2472, 'score': 16863.680718183517, 'total_duration': 17716.385192155838, 'accumulated_submission_time': 16863.680718183517, 'accumulated_eval_time': 847.170530796051, 'accumulated_logging_time': 5.339754343032837, 'global_step': 14454, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(0.32596782, dtype=float32), 'train/wer': 0.11659169505187342, 'validation/ctc_loss': DeviceArray(0.68479526, dtype=float32), 'validation/wer': 0.19812058003453964, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.41277495, dtype=float32), 'test/wer': 0.13444234558121584, 'test/num_examples': 2472, 'score': 18652.673414945602, 'total_duration': 19603.53784751892, 'accumulated_submission_time': 18652.673414945602, 'accumulated_eval_time': 944.4878647327423, 'accumulated_logging_time': 6.160890579223633, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0420 17:17:45.521746 139802087286592 submission_runner.py:570] Timing: 18652.673414945602
I0420 17:17:45.521800 139802087286592 submission_runner.py:571] ====================
I0420 17:17:45.522351 139802087286592 submission_runner.py:631] Final librispeech_deepspeech score: 18652.673414945602
