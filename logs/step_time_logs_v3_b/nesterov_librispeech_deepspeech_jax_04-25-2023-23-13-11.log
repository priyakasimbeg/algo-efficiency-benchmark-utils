I0425 23:13:31.345122 139942137968448 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax.
I0425 23:13:31.419179 139942137968448 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0425 23:13:32.326758 139942137968448 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0425 23:13:32.327465 139942137968448 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0425 23:13:32.331428 139942137968448 submission_runner.py:528] Using RNG seed 2251676048
I0425 23:13:34.992070 139942137968448 submission_runner.py:537] --- Tuning run 1/1 ---
I0425 23:13:34.992263 139942137968448 submission_runner.py:542] Creating tuning directory at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1.
I0425 23:13:34.992539 139942137968448 logger_utils.py:83] Saving hparams to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/hparams.json.
I0425 23:13:35.113861 139942137968448 submission_runner.py:232] Initializing dataset.
I0425 23:13:35.114076 139942137968448 submission_runner.py:239] Initializing model.
I0425 23:13:51.886836 139942137968448 submission_runner.py:249] Initializing optimizer.
I0425 23:13:52.443548 139942137968448 submission_runner.py:256] Initializing metrics bundle.
I0425 23:13:52.443744 139942137968448 submission_runner.py:273] Initializing checkpoint and logger.
I0425 23:13:52.444763 139942137968448 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0425 23:13:52.445036 139942137968448 logger_utils.py:230] Unable to record workload.train_mean information. Continuing without it.
I0425 23:13:52.445101 139942137968448 logger_utils.py:230] Unable to record workload.train_stddev information. Continuing without it.
I0425 23:13:53.325574 139942137968448 submission_runner.py:294] Saving meta data to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0425 23:13:53.326507 139942137968448 submission_runner.py:297] Saving flags to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0425 23:13:53.332881 139942137968448 submission_runner.py:309] Starting training loop.
I0425 23:13:53.529779 139942137968448 input_pipeline.py:20] Loading split = train-clean-100
I0425 23:13:53.569840 139942137968448 input_pipeline.py:20] Loading split = train-clean-360
I0425 23:13:53.890583 139942137968448 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0425 23:14:49.872192 139765716035328 logging_writer.py:48] [0] global_step=0, grad_norm=21.769384384155273, loss=32.80571365356445
I0425 23:14:49.894179 139942137968448 spec.py:298] Evaluating on the training split.
I0425 23:14:50.022020 139942137968448 input_pipeline.py:20] Loading split = train-clean-100
I0425 23:14:50.048149 139942137968448 input_pipeline.py:20] Loading split = train-clean-360
I0425 23:14:50.336288 139942137968448 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0425 23:16:06.261932 139942137968448 spec.py:310] Evaluating on the validation split.
I0425 23:16:06.356722 139942137968448 input_pipeline.py:20] Loading split = dev-clean
I0425 23:16:06.361460 139942137968448 input_pipeline.py:20] Loading split = dev-other
I0425 23:16:56.397420 139942137968448 spec.py:326] Evaluating on the test split.
I0425 23:16:56.492421 139942137968448 input_pipeline.py:20] Loading split = test-clean
I0425 23:17:29.122923 139942137968448 submission_runner.py:406] Time since start: 215.79s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.332123, dtype=float32), 'train/wer': 2.8998917185515274, 'validation/ctc_loss': DeviceArray(31.170574, dtype=float32), 'validation/wer': 2.765825044139355, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.257626, dtype=float32), 'test/wer': 2.7578859707919485, 'test/num_examples': 2472, 'score': 56.561121225357056, 'total_duration': 215.7885844707489, 'accumulated_submission_time': 56.561121225357056, 'accumulated_eval_time': 159.22730159759521, 'accumulated_logging_time': 0}
I0425 23:17:29.146203 139762950199040 logging_writer.py:48] [1] accumulated_eval_time=159.227302, accumulated_logging_time=0, accumulated_submission_time=56.561121, global_step=1, preemption_count=0, score=56.561121, test/ctc_loss=31.257625579833984, test/num_examples=2472, test/wer=2.757886, total_duration=215.788584, train/ctc_loss=32.332122802734375, train/wer=2.899892, validation/ctc_loss=31.170574188232422, validation/num_examples=5348, validation/wer=2.765825
I0425 23:17:29.243972 139942137968448 checkpoints.py:356] Saving checkpoint at step: 1
I0425 23:17:29.533315 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_1
I0425 23:17:29.534007 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_1.
I0425 23:19:38.001447 139766448342784 logging_writer.py:48] [100] global_step=100, grad_norm=1.650061011314392, loss=6.35524845123291
I0425 23:21:31.139130 139766456735488 logging_writer.py:48] [200] global_step=200, grad_norm=0.6015563011169434, loss=6.609129428863525
I0425 23:23:24.715789 139766448342784 logging_writer.py:48] [300] global_step=300, grad_norm=0.9149544835090637, loss=7.546745777130127
I0425 23:25:17.929033 139766456735488 logging_writer.py:48] [400] global_step=400, grad_norm=0.732477068901062, loss=7.099267482757568
I0425 23:27:11.091041 139766448342784 logging_writer.py:48] [500] global_step=500, grad_norm=0.4280334413051605, loss=5.96469783782959
I0425 23:29:03.924145 139766456735488 logging_writer.py:48] [600] global_step=600, grad_norm=3.738224744796753, loss=6.2118635177612305
I0425 23:30:57.232414 139766448342784 logging_writer.py:48] [700] global_step=700, grad_norm=2.399078130722046, loss=5.987442493438721
I0425 23:32:50.455240 139766456735488 logging_writer.py:48] [800] global_step=800, grad_norm=1.4090135097503662, loss=5.9446797370910645
I0425 23:34:43.380551 139766448342784 logging_writer.py:48] [900] global_step=900, grad_norm=0.28937485814094543, loss=5.895876884460449
I0425 23:36:36.187309 139766456735488 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.4085968732833862, loss=5.928744792938232
I0425 23:38:32.138089 139767901738752 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0870527029037476, loss=5.900852203369141
I0425 23:40:24.815321 139767893346048 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1822811365127563, loss=5.92070198059082
I0425 23:42:17.592196 139767901738752 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7046363353729248, loss=5.878204345703125
I0425 23:44:10.472349 139767893346048 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5953636765480042, loss=5.850812911987305
I0425 23:46:03.900736 139767901738752 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.28641277551651, loss=5.870318412780762
I0425 23:47:57.359543 139767893346048 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.1413416862487793, loss=5.835589408874512
I0425 23:49:51.044911 139767901738752 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8675323128700256, loss=5.848487854003906
I0425 23:51:44.890675 139767893346048 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.1834055334329605, loss=5.8116912841796875
I0425 23:53:38.252735 139767901738752 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.10528608411550522, loss=5.800941467285156
I0425 23:55:32.003623 139767893346048 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.16680802404880524, loss=5.7988786697387695
I0425 23:57:28.940996 139766591018752 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.41026076674461365, loss=1789.0125732421875
I0425 23:57:29.919054 139942137968448 spec.py:298] Evaluating on the training split.
I0425 23:57:58.874738 139942137968448 spec.py:310] Evaluating on the validation split.
I0425 23:58:34.043194 139942137968448 spec.py:326] Evaluating on the test split.
I0425 23:58:51.732792 139942137968448 submission_runner.py:406] Time since start: 2698.40s, 	Step: 2102, 	{'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2456.912838459015, 'total_duration': 2698.3968262672424, 'accumulated_submission_time': 2456.912838459015, 'accumulated_eval_time': 241.03802371025085, 'accumulated_logging_time': 0.4143674373626709}
I0425 23:58:51.751987 139766591018752 logging_writer.py:48] [2102] accumulated_eval_time=241.038024, accumulated_logging_time=0.414367, accumulated_submission_time=2456.912838, global_step=2102, preemption_count=0, score=2456.912838, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2698.396826, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0425 23:58:51.853565 139942137968448 checkpoints.py:356] Saving checkpoint at step: 2102
I0425 23:58:52.238945 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_2102
I0425 23:58:52.249127 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_2102.
I0426 00:00:43.512931 139766582626048 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1861.120849609375
I0426 00:02:36.373830 139764166551296 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.08074809610843658, loss=1848.7220458984375
I0426 00:04:29.492873 139766582626048 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1796.34521484375
I0426 00:06:22.172015 139764166551296 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1821.9141845703125
I0426 00:08:15.115688 139766582626048 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1775.661865234375
I0426 00:10:07.986448 139764166551296 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1861.3914794921875
I0426 00:12:00.664061 139766582626048 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1877.637939453125
I0426 00:13:53.373211 139764166551296 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1794.204833984375
I0426 00:15:46.176287 139766582626048 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1751.365966796875
I0426 00:17:42.317180 139767901738752 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1825.291748046875
I0426 00:19:35.128417 139767893346048 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1905.875244140625
I0426 00:21:27.504413 139767901738752 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1860.5797119140625
I0426 00:23:20.316778 139767893346048 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1742.4256591796875
I0426 00:25:12.919428 139767901738752 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1834.71044921875
I0426 00:27:05.763348 139767893346048 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1811.4715576171875
I0426 00:28:58.152243 139767901738752 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1778.7462158203125
I0426 00:30:50.898613 139767893346048 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1820.3594970703125
I0426 00:32:43.547147 139767901738752 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1800.6412353515625
I0426 00:34:36.172423 139767893346048 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1889.9754638671875
I0426 00:36:29.313136 139767901738752 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1840.9117431640625
I0426 00:38:25.244138 139766263338752 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1858.1488037109375
I0426 00:38:53.113102 139942137968448 spec.py:298] Evaluating on the training split.
I0426 00:39:21.976211 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 00:39:57.736044 139942137968448 spec.py:326] Evaluating on the test split.
I0426 00:40:16.182195 139942137968448 submission_runner.py:406] Time since start: 5182.85s, 	Step: 4226, 	{'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4857.742379903793, 'total_duration': 5182.84611082077, 'accumulated_submission_time': 4857.742379903793, 'accumulated_eval_time': 324.10399198532104, 'accumulated_logging_time': 0.933495044708252}
I0426 00:40:16.202215 139767318058752 logging_writer.py:48] [4226] accumulated_eval_time=324.103992, accumulated_logging_time=0.933495, accumulated_submission_time=4857.742380, global_step=4226, preemption_count=0, score=4857.742380, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5182.846111, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 00:40:16.304699 139942137968448 checkpoints.py:356] Saving checkpoint at step: 4226
I0426 00:40:16.763385 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_4226
I0426 00:40:16.773949 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_4226.
I0426 00:41:40.854777 139767309666048 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1870.230712890625
I0426 00:43:33.121554 139767259309824 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1773.4476318359375
I0426 00:45:25.980697 139767309666048 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1815.453857421875
I0426 00:47:18.838330 139767259309824 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1866.1405029296875
I0426 00:49:11.595259 139767309666048 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1838.3997802734375
I0426 00:51:04.105588 139767259309824 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1810.4466552734375
I0426 00:52:56.705162 139767309666048 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1833.9219970703125
I0426 00:54:49.524240 139767259309824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1842.3690185546875
I0426 00:56:42.627448 139767309666048 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1881.501953125
I0426 00:58:38.713495 139767318058752 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1818.67822265625
I0426 01:00:31.140066 139767309666048 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1865.596435546875
I0426 01:02:23.766081 139767318058752 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1888.4417724609375
I0426 01:04:16.525301 139767309666048 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1820.4888916015625
I0426 01:06:09.305605 139767318058752 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1861.120849609375
I0426 01:08:01.832417 139767309666048 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1767.325927734375
I0426 01:09:54.755015 139767318058752 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1819.3245849609375
I0426 01:11:47.925281 139767309666048 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1772.342529296875
I0426 01:13:40.904655 139767318058752 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1816.871337890625
I0426 01:15:33.990737 139767309666048 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1847.2879638671875
I0426 01:17:30.565743 139767318058752 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1845.8226318359375
I0426 01:19:23.247443 139767309666048 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1823.341552734375
I0426 01:20:17.010489 139942137968448 spec.py:298] Evaluating on the training split.
I0426 01:20:46.254848 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 01:21:21.972342 139942137968448 spec.py:326] Evaluating on the test split.
I0426 01:21:40.260380 139942137968448 submission_runner.py:406] Time since start: 7666.92s, 	Step: 6349, 	{'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7257.943964719772, 'total_duration': 7666.924389839172, 'accumulated_submission_time': 7257.943964719772, 'accumulated_eval_time': 407.35083651542664, 'accumulated_logging_time': 1.5286853313446045}
I0426 01:21:40.280607 139767318058752 logging_writer.py:48] [6349] accumulated_eval_time=407.350837, accumulated_logging_time=1.528685, accumulated_submission_time=7257.943965, global_step=6349, preemption_count=0, score=7257.943965, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7666.924390, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 01:21:40.386208 139942137968448 checkpoints.py:356] Saving checkpoint at step: 6349
I0426 01:21:40.803661 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_6349
I0426 01:21:40.814094 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_6349.
I0426 01:22:39.370429 139767309666048 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1822.822265625
I0426 01:24:32.316496 139766048298752 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1762.4588623046875
I0426 01:26:25.093249 139767309666048 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1806.2313232421875
I0426 01:28:18.270161 139766048298752 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1772.5880126953125
I0426 01:30:10.999247 139767309666048 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1776.401123046875
I0426 01:32:03.582380 139766048298752 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1859.7686767578125
I0426 01:33:56.972467 139767309666048 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1890.67333984375
I0426 01:35:49.817154 139766048298752 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1844.625732421875
I0426 01:37:42.254539 139767309666048 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1860.85009765625
I0426 01:39:38.971334 139767318058752 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1813.396484375
I0426 01:41:31.721117 139767309666048 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1877.9134521484375
I0426 01:43:24.708911 139767318058752 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1839.720947265625
I0426 01:45:17.321508 139767309666048 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1846.8880615234375
I0426 01:47:10.505075 139767318058752 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1834.9735107421875
I0426 01:49:03.859681 139767309666048 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1781.841552734375
I0426 01:50:56.853689 139767318058752 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1788.189208984375
I0426 01:52:49.600269 139767309666048 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1829.07373046875
I0426 01:54:42.380825 139767318058752 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1912.2818603515625
I0426 01:56:35.282416 139767309666048 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1877.9134521484375
I0426 01:58:31.466732 139767318058752 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1867.6380615234375
I0426 02:00:23.842170 139767309666048 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1860.985595703125
I0426 02:01:41.359850 139942137968448 spec.py:298] Evaluating on the training split.
I0426 02:02:11.131822 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 02:02:47.009775 139942137968448 spec.py:326] Evaluating on the test split.
I0426 02:03:05.597243 139942137968448 submission_runner.py:406] Time since start: 10152.26s, 	Step: 8470, 	{'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9658.455670118332, 'total_duration': 10152.261192798615, 'accumulated_submission_time': 9658.455670118332, 'accumulated_eval_time': 491.58512783050537, 'accumulated_logging_time': 2.084970712661743}
I0426 02:03:05.618190 139767686698752 logging_writer.py:48] [8470] accumulated_eval_time=491.585128, accumulated_logging_time=2.084971, accumulated_submission_time=9658.455670, global_step=8470, preemption_count=0, score=9658.455670, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=10152.261193, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 02:03:05.723295 139942137968448 checkpoints.py:356] Saving checkpoint at step: 8470
I0426 02:03:06.196844 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8470
I0426 02:03:06.207287 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_8470.
I0426 02:03:41.112432 139767678306048 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1903.32470703125
I0426 02:05:33.699822 139767611164416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1831.167236328125
I0426 02:07:26.057789 139767678306048 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1857.6094970703125
I0426 02:09:18.415955 139767611164416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1752.804931640625
I0426 02:11:10.766810 139767678306048 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1814.9390869140625
I0426 02:13:03.330348 139767611164416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1837.871826171875
I0426 02:14:55.926280 139767678306048 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1789.564208984375
I0426 02:16:48.518309 139767611164416 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1876.6744384765625
I0426 02:18:44.854721 139767686698752 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1791.6934814453125
I0426 02:20:37.867681 139767678306048 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1854.514404296875
I0426 02:22:30.558440 139767686698752 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1877.362548828125
I0426 02:24:23.298860 139767678306048 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1813.0111083984375
I0426 02:26:16.106741 139767686698752 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1868.3197021484375
I0426 02:28:09.207087 139767678306048 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1785.8189697265625
I0426 02:30:02.012096 139767686698752 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1817.5162353515625
I0426 02:31:54.747536 139767678306048 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1822.4329833984375
I0426 02:33:47.305112 139767686698752 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0, loss=1848.488525390625
I0426 02:35:40.152839 139767678306048 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0, loss=1764.4024658203125
I0426 02:37:36.068829 139767686698752 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0, loss=1748.3756103515625
I0426 02:39:29.456824 139767678306048 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.0, loss=1840.38232421875
I0426 02:41:21.842770 139767686698752 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1778.0048828125
I0426 02:43:06.296801 139942137968448 spec.py:298] Evaluating on the training split.
I0426 02:43:35.918312 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 02:44:11.743593 139942137968448 spec.py:326] Evaluating on the test split.
I0426 02:44:30.186156 139942137968448 submission_runner.py:406] Time since start: 12636.85s, 	Step: 10594, 	{'train/ctc_loss': DeviceArray(1832.9214, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12058.508462429047, 'total_duration': 12636.84992647171, 'accumulated_submission_time': 12058.508462429047, 'accumulated_eval_time': 575.4712131023407, 'accumulated_logging_time': 2.6988255977630615}
I0426 02:44:30.206743 139766887978752 logging_writer.py:48] [10594] accumulated_eval_time=575.471213, accumulated_logging_time=2.698826, accumulated_submission_time=12058.508462, global_step=10594, preemption_count=0, score=12058.508462, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=12636.849926, train/ctc_loss=1832.92138671875, train/wer=0.941551, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 02:44:30.310318 139942137968448 checkpoints.py:356] Saving checkpoint at step: 10594
I0426 02:44:30.771952 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_10594
I0426 02:44:30.782673 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_10594.
I0426 02:44:38.680539 139766879586048 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0, loss=1840.11767578125
I0426 02:46:30.654114 139766804051712 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0, loss=1802.035400390625
I0426 02:48:23.052693 139766879586048 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0, loss=1778.993408203125
I0426 02:50:15.790342 139766804051712 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0, loss=1833.9219970703125
I0426 02:52:08.627470 139766879586048 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1822.0438232421875
I0426 02:54:01.472524 139766804051712 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.0, loss=1822.9520263671875
I0426 02:55:54.457786 139766879586048 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0, loss=1809.1671142578125
I0426 02:57:47.122724 139766804051712 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0, loss=1848.3548583984375
I0426 02:59:43.227675 139766232618752 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0, loss=1834.1845703125
I0426 03:01:35.354451 139766224226048 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1860.5797119140625
I0426 03:03:27.634029 139766232618752 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.0, loss=1826.4638671875
I0426 03:05:20.231709 139766224226048 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0, loss=1796.34521484375
I0426 03:07:12.723609 139766232618752 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0, loss=1869.6842041015625
I0426 03:09:05.060006 139766224226048 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0, loss=1828.5511474609375
I0426 03:10:57.647643 139766232618752 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1831.298095703125
I0426 03:12:50.502142 139766224226048 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0, loss=1857.6094970703125
I0426 03:14:43.342422 139766232618752 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0, loss=1874.475830078125
I0426 03:16:35.688714 139766224226048 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0, loss=1836.81689453125
I0426 03:18:31.747560 139766232618752 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0, loss=1871.32421875
I0426 03:20:24.388898 139766224226048 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1905.4495849609375
I0426 03:22:17.080403 139766232618752 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0, loss=1799.37548828125
I0426 03:24:09.821808 139766224226048 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0, loss=1820.747802734375
I0426 03:24:31.116713 139942137968448 spec.py:298] Evaluating on the training split.
I0426 03:25:00.808894 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 03:25:36.918434 139942137968448 spec.py:326] Evaluating on the test split.
I0426 03:25:55.423448 139942137968448 submission_runner.py:406] Time since start: 15122.09s, 	Step: 12720, 	{'train/ctc_loss': DeviceArray(1752.7933, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14458.80634713173, 'total_duration': 15122.087109804153, 'accumulated_submission_time': 14458.80634713173, 'accumulated_eval_time': 659.7745571136475, 'accumulated_logging_time': 3.2981576919555664}
I0426 03:25:55.444919 139766232618752 logging_writer.py:48] [12720] accumulated_eval_time=659.774557, accumulated_logging_time=3.298158, accumulated_submission_time=14458.806347, global_step=12720, preemption_count=0, score=14458.806347, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=15122.087110, train/ctc_loss=1752.7933349609375, train/wer=0.942641, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 03:25:55.553184 139942137968448 checkpoints.py:356] Saving checkpoint at step: 12720
I0426 03:25:56.003410 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_12720
I0426 03:25:56.013998 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_12720.
I0426 03:27:27.546336 139766224226048 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0, loss=1821.6549072265625
I0426 03:29:20.257273 139766140299008 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0, loss=1805.4669189453125
I0426 03:31:13.078192 139766224226048 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1841.308837890625
I0426 03:33:06.018418 139766140299008 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.0, loss=1880.25830078125
I0426 03:34:58.667034 139766224226048 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0, loss=1902.4759521484375
I0426 03:36:51.447760 139766140299008 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0, loss=1872.96728515625
I0426 03:38:47.553581 139766887978752 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0, loss=1880.25830078125
I0426 03:40:40.159139 139766879586048 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1804.1944580078125
I0426 03:42:33.021718 139766887978752 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0, loss=1886.9105224609375
I0426 03:44:25.998213 139766879586048 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0, loss=1833.3966064453125
I0426 03:46:18.792443 139766887978752 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0, loss=1788.56396484375
I0426 03:48:12.139417 139766879586048 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0, loss=1796.7235107421875
I0426 03:50:04.844755 139766887978752 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1811.4715576171875
I0426 03:51:57.663386 139766879586048 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0, loss=1769.2803955078125
I0426 03:53:50.508433 139766887978752 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0, loss=1817.000244140625
I0426 03:55:43.177753 139766879586048 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.0, loss=1833.2652587890625
I0426 03:57:35.795529 139766887978752 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0, loss=1884.271728515625
I0426 03:59:32.170513 139766232618752 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1872.145263671875
I0426 04:01:24.791704 139766224226048 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0, loss=1830.6434326171875
I0426 04:03:17.610052 139766232618752 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.0, loss=1894.450927734375
I0426 04:05:10.410217 139766224226048 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0, loss=1864.102294921875
I0426 04:05:56.276980 139942137968448 spec.py:298] Evaluating on the training split.
I0426 04:06:26.293552 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 04:07:02.790785 139942137968448 spec.py:326] Evaluating on the test split.
I0426 04:07:21.866593 139942137968448 submission_runner.py:406] Time since start: 17608.53s, 	Step: 14842, 	{'train/ctc_loss': DeviceArray(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16859.03346824646, 'total_duration': 17608.53052020073, 'accumulated_submission_time': 16859.03346824646, 'accumulated_eval_time': 745.3610491752625, 'accumulated_logging_time': 3.892012596130371}
I0426 04:07:21.888265 139767471658752 logging_writer.py:48] [14842] accumulated_eval_time=745.361049, accumulated_logging_time=3.892013, accumulated_submission_time=16859.033468, global_step=14842, preemption_count=0, score=16859.033468, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=17608.530520, train/ctc_loss=1746.1038818359375, train/wer=0.942824, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 04:07:21.994688 139942137968448 checkpoints.py:356] Saving checkpoint at step: 14842
I0426 04:07:22.420248 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_14842
I0426 04:07:22.430855 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_14842.
I0426 04:08:28.963340 139767463266048 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0, loss=1817.129150390625
I0426 04:10:21.340074 139767370946304 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1820.3594970703125
I0426 04:12:13.843197 139767463266048 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0, loss=1893.190185546875
I0426 04:14:06.218454 139767370946304 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0, loss=1842.5015869140625
I0426 04:15:58.562370 139767463266048 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.0, loss=1871.8717041015625
I0426 04:17:51.158948 139767370946304 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.0, loss=1829.4658203125
I0426 04:19:47.014241 139767471658752 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1866.2764892578125
I0426 04:21:39.284394 139767463266048 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.0, loss=1836.68505859375
I0426 04:23:31.903074 139767471658752 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0, loss=1820.22998046875
I0426 04:25:24.504043 139767463266048 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0, loss=1860.85009765625
I0426 04:27:17.049834 139767471658752 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0, loss=1803.68603515625
I0426 04:29:08.108810 139942137968448 spec.py:298] Evaluating on the training split.
I0426 04:29:38.111525 139942137968448 spec.py:310] Evaluating on the validation split.
I0426 04:30:14.065264 139942137968448 spec.py:326] Evaluating on the test split.
I0426 04:30:32.646328 139942137968448 submission_runner.py:406] Time since start: 18999.31s, 	Step: 16000, 	{'train/ctc_loss': DeviceArray(1733.7323, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18164.689826488495, 'total_duration': 18999.311536073685, 'accumulated_submission_time': 18164.689826488495, 'accumulated_eval_time': 829.8967523574829, 'accumulated_logging_time': 4.459317922592163}
I0426 04:30:32.668344 139767471658752 logging_writer.py:48] [16000] accumulated_eval_time=829.896752, accumulated_logging_time=4.459318, accumulated_submission_time=18164.689826, global_step=16000, preemption_count=0, score=18164.689826, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=18999.311536, train/ctc_loss=1733.7322998046875, train/wer=0.944086, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0426 04:30:32.770155 139942137968448 checkpoints.py:356] Saving checkpoint at step: 16000
I0426 04:30:33.204192 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0426 04:30:33.214554 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0426 04:30:33.226545 139767463266048 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=18164.689826
I0426 04:30:33.297440 139942137968448 checkpoints.py:356] Saving checkpoint at step: 16000
I0426 04:30:33.809223 139942137968448 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0426 04:30:33.819653 139942137968448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3_b/timing_nesterov/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0426 04:30:35.113491 139942137968448 submission_runner.py:567] Tuning trial 1/1
I0426 04:30:35.113772 139942137968448 submission_runner.py:568] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0426 04:30:35.118827 139942137968448 submission_runner.py:569] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.332123, dtype=float32), 'train/wer': 2.8998917185515274, 'validation/ctc_loss': DeviceArray(31.170574, dtype=float32), 'validation/wer': 2.765825044139355, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.257626, dtype=float32), 'test/wer': 2.7578859707919485, 'test/num_examples': 2472, 'score': 56.561121225357056, 'total_duration': 215.7885844707489, 'accumulated_submission_time': 56.561121225357056, 'accumulated_eval_time': 159.22730159759521, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2102, {'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2456.912838459015, 'total_duration': 2698.3968262672424, 'accumulated_submission_time': 2456.912838459015, 'accumulated_eval_time': 241.03802371025085, 'accumulated_logging_time': 0.4143674373626709, 'global_step': 2102, 'preemption_count': 0}), (4226, {'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4857.742379903793, 'total_duration': 5182.84611082077, 'accumulated_submission_time': 4857.742379903793, 'accumulated_eval_time': 324.10399198532104, 'accumulated_logging_time': 0.933495044708252, 'global_step': 4226, 'preemption_count': 0}), (6349, {'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7257.943964719772, 'total_duration': 7666.924389839172, 'accumulated_submission_time': 7257.943964719772, 'accumulated_eval_time': 407.35083651542664, 'accumulated_logging_time': 1.5286853313446045, 'global_step': 6349, 'preemption_count': 0}), (8470, {'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 9658.455670118332, 'total_duration': 10152.261192798615, 'accumulated_submission_time': 9658.455670118332, 'accumulated_eval_time': 491.58512783050537, 'accumulated_logging_time': 2.084970712661743, 'global_step': 8470, 'preemption_count': 0}), (10594, {'train/ctc_loss': DeviceArray(1832.9214, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 12058.508462429047, 'total_duration': 12636.84992647171, 'accumulated_submission_time': 12058.508462429047, 'accumulated_eval_time': 575.4712131023407, 'accumulated_logging_time': 2.6988255977630615, 'global_step': 10594, 'preemption_count': 0}), (12720, {'train/ctc_loss': DeviceArray(1752.7933, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14458.80634713173, 'total_duration': 15122.087109804153, 'accumulated_submission_time': 14458.80634713173, 'accumulated_eval_time': 659.7745571136475, 'accumulated_logging_time': 3.2981576919555664, 'global_step': 12720, 'preemption_count': 0}), (14842, {'train/ctc_loss': DeviceArray(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 16859.03346824646, 'total_duration': 17608.53052020073, 'accumulated_submission_time': 16859.03346824646, 'accumulated_eval_time': 745.3610491752625, 'accumulated_logging_time': 3.892012596130371, 'global_step': 14842, 'preemption_count': 0}), (16000, {'train/ctc_loss': DeviceArray(1733.7323, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18164.689826488495, 'total_duration': 18999.311536073685, 'accumulated_submission_time': 18164.689826488495, 'accumulated_eval_time': 829.8967523574829, 'accumulated_logging_time': 4.459317922592163, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0426 04:30:35.119017 139942137968448 submission_runner.py:570] Timing: 18164.689826488495
I0426 04:30:35.119069 139942137968448 submission_runner.py:571] ====================
I0426 04:30:35.119628 139942137968448 submission_runner.py:631] Final librispeech_deepspeech score: 18164.689826488495
