torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_resnet --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_2/adamw --overwrite=true --save_checkpoints=false --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=True 2>&1 | tee -a /logs/imagenet_resnet_pytorch_08-10-2023-01-30-53.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-10 01:31:03.924792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 01:31:03.924798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0810 01:31:18.453562 139851907209024 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0810 01:31:18.453582 139746523662144 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0810 01:31:18.453603 140018384090944 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0810 01:31:18.454542 139665565120320 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0810 01:31:18.454703 140104604628800 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0810 01:31:18.454762 140505787062080 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0810 01:31:18.455029 140104604628800 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.455062 140505787062080 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.454959 140199859279680 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0810 01:31:18.454949 140427484497728 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0810 01:31:18.455287 140427484497728 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.455309 140199859279680 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.464173 139851907209024 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.464201 139746523662144 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.464232 140018384090944 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 01:31:18.465074 139665565120320 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0810 01:31:19.741916 140199859279680 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_resnet_pytorch.
W0810 01:31:19.784599 139746523662144 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.784599 139851907209024 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.784644 140427484497728 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.785274 140018384090944 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.785299 140104604628800 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.785477 140199859279680 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.785990 140505787062080 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 01:31:19.786035 139665565120320 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 01:31:19.791320 140199859279680 submission_runner.py:488] Using RNG seed 268543200
I0810 01:31:19.793147 140199859279680 submission_runner.py:497] --- Tuning run 1/1 ---
I0810 01:31:19.793329 140199859279680 submission_runner.py:502] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_resnet_pytorch/trial_1.
I0810 01:31:19.793754 140199859279680 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_resnet_pytorch/trial_1/hparams.json.
I0810 01:31:19.794668 140199859279680 submission_runner.py:176] Initializing dataset.
I0810 01:31:26.296596 140199859279680 submission_runner.py:183] Initializing model.
I0810 01:31:31.073058 140199859279680 submission_runner.py:212] Performing `torch.compile`.
I0810 01:31:31.663776 140199859279680 submission_runner.py:215] Initializing optimizer.
I0810 01:31:31.665159 140199859279680 submission_runner.py:222] Initializing metrics bundle.
I0810 01:31:31.665296 140199859279680 submission_runner.py:240] Initializing checkpoint and logger.
I0810 01:31:32.163823 140199859279680 submission_runner.py:261] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_resnet_pytorch/trial_1/meta_data_0.json.
I0810 01:31:32.164734 140199859279680 submission_runner.py:264] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_resnet_pytorch/trial_1/flags_0.json.
I0810 01:31:32.260012 140199859279680 submission_runner.py:274] Starting training loop.
[2023-08-10 01:31:34,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,660] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,781] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,786] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,796] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:34,798] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:31:36,477] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,503] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,506] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,507] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,530] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,555] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,559] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,560] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,609] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,612] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,613] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,694] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,698] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,733] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,759] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,768] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,821] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:36,839] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,842] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,843] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,846] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:36,849] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:36,855] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:36,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:31:37,000] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:31:37,003] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:31:37,008] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:31:47,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:47,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:47,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:47,930] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:47,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:47,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:48,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:48,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 01:31:57,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:31:57,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 01:32:01,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:01,731] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:01,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:01,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:01,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:01,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:02,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:02,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 01:32:03,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:03,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:03,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:03,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:03,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:04,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:04,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:04,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 01:32:04,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:04,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:04,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:05,029] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:05,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:05,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:05,428] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:05,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 01:32:05,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:05,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:05,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:05,990] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:06,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:06,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:06,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:06,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 01:32:07,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:07,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:07,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:07,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:07,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:07,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:08,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:08,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 01:32:08,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:08,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:08,833] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:08,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:08,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:08,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:08,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:08,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:08,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:08,914] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:08,961] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:08,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:08,988] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:08,991] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:08,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:08,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:09,004] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:09,008] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:09,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:09,112] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:09,114] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:09,118] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:09,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:09,128] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:09,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:09,467] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:09,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:09,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:09,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 01:32:09,755] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 01:32:09,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 01:32:09,771] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:15,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 01:32:15,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:15,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:15,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:15,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:15,937] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:15,949] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:16,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:16,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:16,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:16,015] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:16,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 01:32:16,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:16,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:16,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:16,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:16,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 01:32:16,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,155] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:17,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 01:32:18,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:18,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 01:32:19,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,447] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,454] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:19,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 01:32:20,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:20,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:20,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:20,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:20,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:21,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:21,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:21,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 01:32:22,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,358] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,581] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:23,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 01:32:24,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,826] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:24,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 01:32:29,438] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,174] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 01:32:30,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0810 01:32:33.709703 140172825978624 logging_writer.py:48] [0] global_step=0, grad_norm=0.592335, loss=6.934749
I0810 01:32:33.736753 140199859279680 submission.py:120] 0) loss = 6.935, grad_norm = 0.592
I0810 01:32:33.756900 140199859279680 spec.py:320] Evaluating on the training split.
[2023-08-10 01:32:43,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:43,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:43,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:43,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:43,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:43,818] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:44,033] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:44,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:32:45,015] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,035] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,039] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,039] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:45,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,093] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,096] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,096] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,097] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:45,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,119] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,120] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:45,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,446] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,449] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,450] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:45,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,702] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,705] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,706] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:45,713] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,716] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:45,808] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:45,829] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:45,832] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:45,833] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:46,307] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:32:46,328] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:32:46,332] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:32:46,332] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:32:48,260] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:48,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:48,329] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:48,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:48,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:48,493] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:48,655] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:49,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 01:32:51,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:51,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:51,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:51,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:51,847] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:51,887] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:52,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:52,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 01:32:53,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:53,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:53,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:53,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:53,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:54,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:54,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:54,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 01:32:55,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:55,838] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:55,848] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:55,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:55,921] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:55,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:56,075] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:56,169] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 01:32:56,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:56,464] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:56,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:56,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:56,553] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:56,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:56,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:56,868] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 01:32:56,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:57,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,123] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 01:32:57,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:57,712] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:57,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:57,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:57,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:57,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:57,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:57,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:57,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:57,840] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:57,845] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:57,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:57,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:57,870] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:57,884] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:57,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:57,944] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:57,950] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:57,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:58,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:58,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:58,012] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:58,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 01:32:58,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:58,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:58,106] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:58,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:58,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:58,346] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:58,352] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:32:58,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 01:32:58,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 01:32:58,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 01:32:58,852] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0810 01:33:51.607917 140199859279680 spec.py:332] Evaluating on the validation split.
[2023-08-10 01:34:42,310] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:43,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:43,526] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:43,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:43,723] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:43,742] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:43,746] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:43,746] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:44,740] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:44,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:44,763] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:44,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:44,881] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:44,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:44,900] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:44,904] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:44,904] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:44,919] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:44,923] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:44,923] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:45,166] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:46,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:46,522] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:46,541] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:46,545] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:46,545] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:47,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:47,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:47,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:47,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:48,137] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:48,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:34:49,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:49,181] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:49,201] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:49,204] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:49,205] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:49,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:49,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:49,719] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:49,724] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:49,725] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:49,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:34:49,773] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:34:49,778] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:34:49,779] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:34:49,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:50,090] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:50,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:51,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:52,135] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:52,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:52,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:52,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:52,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:52,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 01:34:52,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:52,798] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:53,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:54,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:54,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:34:54,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:54,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:54,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:54,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:34:54,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:54,761] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:54,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:55,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:55,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:55,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:34:55,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:34:55,459] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:34:55,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:34:55,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:34:55,535] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:34:55,598] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:34:55,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 01:34:55,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:56,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:34:56,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:34:56,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:34:56,580] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:56,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:34:56,738] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:34:56,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:34:56,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:34:56,786] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:34:56,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:34:56,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:34:56,862] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:34:56,868] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:34:56,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:34:56,972] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:34:56,978] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:34:57,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:34:57,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:57,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:34:57,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:57,971] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 01:34:58,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:34:58,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:34:58,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:34:58,525] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:34:58,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:59,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:59,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 01:34:59,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:59,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:34:59,980] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:34:59,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 01:35:00,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:35:00,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 01:35:00,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:35:01,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:35:01,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 01:35:01,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:35:01,453] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:35:01,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:35:01,473] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:01,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:35:01,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:35:01,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:35:01,863] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:01,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 01:35:02,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 01:35:02,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 01:35:02,053] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0810 01:35:03.342620 140199859279680 spec.py:348] Evaluating on the test split.
I0810 01:35:03.358151 140199859279680 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0810 01:35:03.364494 140199859279680 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0810 01:35:03.443677 140199859279680 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-08-10 01:35:05,420] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:05,426] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:05,460] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:05,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:05,591] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:06,138] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:06,234] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:06,302] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:08,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,575] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,578] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,615] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,623] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,633] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,637] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,637] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,642] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,646] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,646] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,663] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,666] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,667] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,671] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,692] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,695] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,696] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,826] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,845] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,848] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,946] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,949] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,950] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:08,967] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:08,986] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:08,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:08,989] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:11,237] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,574] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:11,621] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 01:35:18,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,266] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,310] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:18,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 01:35:20,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,511] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:20,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 01:35:25,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,437] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,565] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 01:35:25,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:25,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:25,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:25,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:25,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:26,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:26,137] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:26,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 01:35:26,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,570] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 01:35:26,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,172] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,339] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:27,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 01:35:29,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:29,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:29,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:29,758] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:29,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:29,767] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:29,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:29,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:29,806] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:29,869] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:29,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:29,879] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:30,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:30,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:30,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:30,185] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:30,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:30,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:30,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:30,435] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:30,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:30,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:30,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:30,616] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:30,621] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:30,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 01:35:30,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:30,658] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:30,663] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:30,759] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 01:35:30,762] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 01:35:30,768] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:41,876] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:41,974] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:41,981] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:42,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:42,350] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:42,509] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:42,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:42,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 01:35:43,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,499] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,502] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,503] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:43,547] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,570] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,570] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:43,661] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,681] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,684] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,685] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:43,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,782] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,785] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,785] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:43,931] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,952] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:43,954] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,955] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:43,955] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,958] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,958] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:43,973] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:43,976] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:43,977] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:44,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 01:35:44,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 01:35:44,157] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 01:35:44,158] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 01:35:46,288] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,594] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:46,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 01:35:50,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,326] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,360] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:51,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 01:35:52,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,355] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,394] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:53,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 01:35:55,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,275] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,371] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,440] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:56,641] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 01:35:56,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,030] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,104] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 01:35:57,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:57,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,815] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:57,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 01:35:58,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:58,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:58,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:58,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:58,404] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:58,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:58,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 01:35:59,433] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:35:59,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:35:59,555] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:35:59,561] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:59,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:35:59,835] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:35:59,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:35:59,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:35:59,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:59,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:35:59,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:35:59,961] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:35:59,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:36:00,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:36:00,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:36:00,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:36:00,146] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:36:00,152] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:36:00,157] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:36:00,160] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:36:00,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:36:00,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:36:00,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:36:00,170] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:36:00,171] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 01:36:00,172] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:36:00,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:36:00,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:36:00,232] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 01:36:00,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 01:36:00,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 01:36:00,289] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0810 01:36:04.867565 140199859279680 submission_runner.py:362] Time since start: 272.61s, 	Step: 1, 	{'train/accuracy': 0.0007971938775510204, 'train/loss': 6.916522590481505, 'validation/accuracy': 0.00096, 'validation/loss': 6.91603625, 'validation/num_examples': 50000, 'test/accuracy': 0.0012, 'test/loss': 6.9179359375, 'test/num_examples': 10000, 'score': 61.496670722961426, 'total_duration': 272.6078770160675, 'accumulated_submission_time': 61.496670722961426, 'accumulated_eval_time': 211.11052703857422, 'accumulated_logging_time': 0}
I0810 01:36:04.888285 140156585633536 logging_writer.py:48] [1] accumulated_eval_time=211.110527, accumulated_logging_time=0, accumulated_submission_time=61.496671, global_step=1, preemption_count=0, score=61.496671, test/accuracy=0.001200, test/loss=6.917936, test/num_examples=10000, total_duration=272.607877, train/accuracy=0.000797, train/loss=6.916523, validation/accuracy=0.000960, validation/loss=6.916036, validation/num_examples=50000
I0810 01:36:04.916916 140199859279680 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.916919 140505787062080 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.920038 139746523662144 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.920067 139851907209024 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.920055 140104604628800 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.920093 140018384090944 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.920114 139665565120320 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 01:36:04.920566 140427484497728 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1, 1]
bucket_view.sizes() = [2048, 1024, 1, 1], strides() = [1024, 1, 1024, 1024] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:323.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I0810 01:36:05.380511 140156577240832 logging_writer.py:48] [1] global_step=1, grad_norm=0.607795, loss=6.930198
I0810 01:36:05.384260 140199859279680 submission.py:120] 1) loss = 6.930, grad_norm = 0.608
I0810 01:36:05.737295 140156585633536 logging_writer.py:48] [2] global_step=2, grad_norm=0.606922, loss=6.933972
I0810 01:36:05.741356 140199859279680 submission.py:120] 2) loss = 6.934, grad_norm = 0.607
I0810 01:36:06.095077 140156577240832 logging_writer.py:48] [3] global_step=3, grad_norm=0.604284, loss=6.928546
I0810 01:36:06.099280 140199859279680 submission.py:120] 3) loss = 6.929, grad_norm = 0.604
I0810 01:36:06.451732 140156585633536 logging_writer.py:48] [4] global_step=4, grad_norm=0.603163, loss=6.927111
I0810 01:36:06.457400 140199859279680 submission.py:120] 4) loss = 6.927, grad_norm = 0.603
I0810 01:36:06.814087 140156577240832 logging_writer.py:48] [5] global_step=5, grad_norm=0.599304, loss=6.933515
I0810 01:36:06.819401 140199859279680 submission.py:120] 5) loss = 6.934, grad_norm = 0.599
I0810 01:36:07.200138 140156585633536 logging_writer.py:48] [6] global_step=6, grad_norm=0.608237, loss=6.922774
I0810 01:36:07.204146 140199859279680 submission.py:120] 6) loss = 6.923, grad_norm = 0.608
I0810 01:36:07.561448 140156577240832 logging_writer.py:48] [7] global_step=7, grad_norm=0.616368, loss=6.926626
I0810 01:36:07.566124 140199859279680 submission.py:120] 7) loss = 6.927, grad_norm = 0.616
I0810 01:36:07.919678 140156585633536 logging_writer.py:48] [8] global_step=8, grad_norm=0.607164, loss=6.930974
I0810 01:36:07.924487 140199859279680 submission.py:120] 8) loss = 6.931, grad_norm = 0.607
I0810 01:36:08.279492 140156577240832 logging_writer.py:48] [9] global_step=9, grad_norm=0.618228, loss=6.934031
I0810 01:36:08.283802 140199859279680 submission.py:120] 9) loss = 6.934, grad_norm = 0.618
I0810 01:36:08.641646 140156585633536 logging_writer.py:48] [10] global_step=10, grad_norm=0.601050, loss=6.929240
I0810 01:36:08.646806 140199859279680 submission.py:120] 10) loss = 6.929, grad_norm = 0.601
I0810 01:36:09.002521 140156577240832 logging_writer.py:48] [11] global_step=11, grad_norm=0.590859, loss=6.918987
I0810 01:36:09.009377 140199859279680 submission.py:120] 11) loss = 6.919, grad_norm = 0.591
I0810 01:36:09.368927 140156585633536 logging_writer.py:48] [12] global_step=12, grad_norm=0.594707, loss=6.926454
I0810 01:36:09.373750 140199859279680 submission.py:120] 12) loss = 6.926, grad_norm = 0.595
I0810 01:36:09.739566 140156577240832 logging_writer.py:48] [13] global_step=13, grad_norm=0.600587, loss=6.925228
I0810 01:36:09.745284 140199859279680 submission.py:120] 13) loss = 6.925, grad_norm = 0.601
I0810 01:36:10.158135 140156585633536 logging_writer.py:48] [14] global_step=14, grad_norm=0.607932, loss=6.927145
I0810 01:36:10.163885 140199859279680 submission.py:120] 14) loss = 6.927, grad_norm = 0.608
I0810 01:36:10.530414 140156577240832 logging_writer.py:48] [15] global_step=15, grad_norm=0.586196, loss=6.914658
I0810 01:36:10.537090 140199859279680 submission.py:120] 15) loss = 6.915, grad_norm = 0.586
I0810 01:36:10.892167 140156585633536 logging_writer.py:48] [16] global_step=16, grad_norm=0.588898, loss=6.928778
I0810 01:36:10.896965 140199859279680 submission.py:120] 16) loss = 6.929, grad_norm = 0.589
I0810 01:36:11.251875 140156577240832 logging_writer.py:48] [17] global_step=17, grad_norm=0.597222, loss=6.926363
I0810 01:36:11.256326 140199859279680 submission.py:120] 17) loss = 6.926, grad_norm = 0.597
I0810 01:36:11.610492 140156585633536 logging_writer.py:48] [18] global_step=18, grad_norm=0.602917, loss=6.927906
I0810 01:36:11.616035 140199859279680 submission.py:120] 18) loss = 6.928, grad_norm = 0.603
I0810 01:36:11.975796 140156577240832 logging_writer.py:48] [19] global_step=19, grad_norm=0.596630, loss=6.923167
I0810 01:36:11.981679 140199859279680 submission.py:120] 19) loss = 6.923, grad_norm = 0.597
I0810 01:36:12.339599 140156585633536 logging_writer.py:48] [20] global_step=20, grad_norm=0.601049, loss=6.919100
I0810 01:36:12.345150 140199859279680 submission.py:120] 20) loss = 6.919, grad_norm = 0.601
I0810 01:36:12.702316 140156577240832 logging_writer.py:48] [21] global_step=21, grad_norm=0.593864, loss=6.925923
I0810 01:36:12.707250 140199859279680 submission.py:120] 21) loss = 6.926, grad_norm = 0.594
I0810 01:36:13.070559 140156585633536 logging_writer.py:48] [22] global_step=22, grad_norm=0.603656, loss=6.931885
I0810 01:36:13.075975 140199859279680 submission.py:120] 22) loss = 6.932, grad_norm = 0.604
I0810 01:36:13.441096 140156577240832 logging_writer.py:48] [23] global_step=23, grad_norm=0.590640, loss=6.925755
I0810 01:36:13.445720 140199859279680 submission.py:120] 23) loss = 6.926, grad_norm = 0.591
I0810 01:36:13.810129 140156585633536 logging_writer.py:48] [24] global_step=24, grad_norm=0.601661, loss=6.920595
I0810 01:36:13.814680 140199859279680 submission.py:120] 24) loss = 6.921, grad_norm = 0.602
I0810 01:36:14.168824 140156577240832 logging_writer.py:48] [25] global_step=25, grad_norm=0.593261, loss=6.920019
I0810 01:36:14.173652 140199859279680 submission.py:120] 25) loss = 6.920, grad_norm = 0.593
I0810 01:36:14.530590 140156585633536 logging_writer.py:48] [26] global_step=26, grad_norm=0.608621, loss=6.922769
I0810 01:36:14.536947 140199859279680 submission.py:120] 26) loss = 6.923, grad_norm = 0.609
I0810 01:36:14.897428 140156577240832 logging_writer.py:48] [27] global_step=27, grad_norm=0.592139, loss=6.922262
I0810 01:36:14.902773 140199859279680 submission.py:120] 27) loss = 6.922, grad_norm = 0.592
I0810 01:36:15.268285 140156585633536 logging_writer.py:48] [28] global_step=28, grad_norm=0.602553, loss=6.928583
I0810 01:36:15.273774 140199859279680 submission.py:120] 28) loss = 6.929, grad_norm = 0.603
I0810 01:36:15.632326 140156577240832 logging_writer.py:48] [29] global_step=29, grad_norm=0.600394, loss=6.924206
I0810 01:36:15.637388 140199859279680 submission.py:120] 29) loss = 6.924, grad_norm = 0.600
I0810 01:36:15.992255 140156585633536 logging_writer.py:48] [30] global_step=30, grad_norm=0.602635, loss=6.925374
I0810 01:36:15.996722 140199859279680 submission.py:120] 30) loss = 6.925, grad_norm = 0.603
I0810 01:36:16.352408 140156577240832 logging_writer.py:48] [31] global_step=31, grad_norm=0.586281, loss=6.926647
I0810 01:36:16.422516 140199859279680 submission.py:120] 31) loss = 6.927, grad_norm = 0.586
I0810 01:36:16.784286 140156585633536 logging_writer.py:48] [32] global_step=32, grad_norm=0.581416, loss=6.923218
I0810 01:36:16.788679 140199859279680 submission.py:120] 32) loss = 6.923, grad_norm = 0.581
I0810 01:36:17.154905 140156577240832 logging_writer.py:48] [33] global_step=33, grad_norm=0.618427, loss=6.917427
I0810 01:36:17.159992 140199859279680 submission.py:120] 33) loss = 6.917, grad_norm = 0.618
I0810 01:36:17.516913 140156585633536 logging_writer.py:48] [34] global_step=34, grad_norm=0.590623, loss=6.918042
I0810 01:36:17.521198 140199859279680 submission.py:120] 34) loss = 6.918, grad_norm = 0.591
I0810 01:36:17.879010 140156577240832 logging_writer.py:48] [35] global_step=35, grad_norm=0.592385, loss=6.921622
I0810 01:36:17.883536 140199859279680 submission.py:120] 35) loss = 6.922, grad_norm = 0.592
I0810 01:36:18.241165 140156585633536 logging_writer.py:48] [36] global_step=36, grad_norm=0.604837, loss=6.917678
I0810 01:36:18.245265 140199859279680 submission.py:120] 36) loss = 6.918, grad_norm = 0.605
I0810 01:36:18.602262 140156577240832 logging_writer.py:48] [37] global_step=37, grad_norm=0.603447, loss=6.923177
I0810 01:36:18.607310 140199859279680 submission.py:120] 37) loss = 6.923, grad_norm = 0.603
I0810 01:36:18.964626 140156585633536 logging_writer.py:48] [38] global_step=38, grad_norm=0.585176, loss=6.919897
I0810 01:36:18.970726 140199859279680 submission.py:120] 38) loss = 6.920, grad_norm = 0.585
I0810 01:36:19.329025 140156577240832 logging_writer.py:48] [39] global_step=39, grad_norm=0.592840, loss=6.912204
I0810 01:36:19.334213 140199859279680 submission.py:120] 39) loss = 6.912, grad_norm = 0.593
I0810 01:36:19.689761 140156585633536 logging_writer.py:48] [40] global_step=40, grad_norm=0.599080, loss=6.911526
I0810 01:36:19.695117 140199859279680 submission.py:120] 40) loss = 6.912, grad_norm = 0.599
I0810 01:36:20.054552 140156577240832 logging_writer.py:48] [41] global_step=41, grad_norm=0.579833, loss=6.924126
I0810 01:36:20.058694 140199859279680 submission.py:120] 41) loss = 6.924, grad_norm = 0.580
I0810 01:36:20.414105 140156585633536 logging_writer.py:48] [42] global_step=42, grad_norm=0.590111, loss=6.907583
I0810 01:36:20.418650 140199859279680 submission.py:120] 42) loss = 6.908, grad_norm = 0.590
I0810 01:36:20.775131 140156577240832 logging_writer.py:48] [43] global_step=43, grad_norm=0.603675, loss=6.918759
I0810 01:36:20.780996 140199859279680 submission.py:120] 43) loss = 6.919, grad_norm = 0.604
I0810 01:36:21.144067 140156585633536 logging_writer.py:48] [44] global_step=44, grad_norm=0.590050, loss=6.915968
I0810 01:36:21.151063 140199859279680 submission.py:120] 44) loss = 6.916, grad_norm = 0.590
I0810 01:36:21.506425 140156577240832 logging_writer.py:48] [45] global_step=45, grad_norm=0.583455, loss=6.925560
I0810 01:36:21.514525 140199859279680 submission.py:120] 45) loss = 6.926, grad_norm = 0.583
I0810 01:36:21.886090 140156585633536 logging_writer.py:48] [46] global_step=46, grad_norm=0.609658, loss=6.919465
I0810 01:36:21.892732 140199859279680 submission.py:120] 46) loss = 6.919, grad_norm = 0.610
I0810 01:36:22.250990 140156577240832 logging_writer.py:48] [47] global_step=47, grad_norm=0.594775, loss=6.913037
I0810 01:36:22.255999 140199859279680 submission.py:120] 47) loss = 6.913, grad_norm = 0.595
I0810 01:36:22.629061 140156585633536 logging_writer.py:48] [48] global_step=48, grad_norm=0.588491, loss=6.910890
I0810 01:36:22.634447 140199859279680 submission.py:120] 48) loss = 6.911, grad_norm = 0.588
I0810 01:36:22.994217 140156577240832 logging_writer.py:48] [49] global_step=49, grad_norm=0.599835, loss=6.914671
I0810 01:36:23.000209 140199859279680 submission.py:120] 49) loss = 6.915, grad_norm = 0.600
I0810 01:36:23.366217 140156585633536 logging_writer.py:48] [50] global_step=50, grad_norm=0.586470, loss=6.912975
I0810 01:36:23.370636 140199859279680 submission.py:120] 50) loss = 6.913, grad_norm = 0.586
I0810 01:36:23.729321 140156577240832 logging_writer.py:48] [51] global_step=51, grad_norm=0.591733, loss=6.915673
I0810 01:36:23.734388 140199859279680 submission.py:120] 51) loss = 6.916, grad_norm = 0.592
I0810 01:36:24.089994 140156585633536 logging_writer.py:48] [52] global_step=52, grad_norm=0.583051, loss=6.911358
I0810 01:36:24.094375 140199859279680 submission.py:120] 52) loss = 6.911, grad_norm = 0.583
I0810 01:36:24.455560 140156577240832 logging_writer.py:48] [53] global_step=53, grad_norm=0.580322, loss=6.908111
I0810 01:36:24.460745 140199859279680 submission.py:120] 53) loss = 6.908, grad_norm = 0.580
I0810 01:36:24.822420 140156585633536 logging_writer.py:48] [54] global_step=54, grad_norm=0.603708, loss=6.915507
I0810 01:36:24.827301 140199859279680 submission.py:120] 54) loss = 6.916, grad_norm = 0.604
I0810 01:36:25.182206 140156577240832 logging_writer.py:48] [55] global_step=55, grad_norm=0.581601, loss=6.919918
I0810 01:36:25.188752 140199859279680 submission.py:120] 55) loss = 6.920, grad_norm = 0.582
I0810 01:36:25.564884 140156585633536 logging_writer.py:48] [56] global_step=56, grad_norm=0.594777, loss=6.913187
I0810 01:36:25.570832 140199859279680 submission.py:120] 56) loss = 6.913, grad_norm = 0.595
I0810 01:36:25.927062 140156577240832 logging_writer.py:48] [57] global_step=57, grad_norm=0.598648, loss=6.909486
I0810 01:36:25.931379 140199859279680 submission.py:120] 57) loss = 6.909, grad_norm = 0.599
I0810 01:36:26.292206 140156585633536 logging_writer.py:48] [58] global_step=58, grad_norm=0.610179, loss=6.911372
I0810 01:36:26.297103 140199859279680 submission.py:120] 58) loss = 6.911, grad_norm = 0.610
I0810 01:36:26.655498 140156577240832 logging_writer.py:48] [59] global_step=59, grad_norm=0.602095, loss=6.901635
I0810 01:36:26.661439 140199859279680 submission.py:120] 59) loss = 6.902, grad_norm = 0.602
I0810 01:36:27.024435 140156585633536 logging_writer.py:48] [60] global_step=60, grad_norm=0.594019, loss=6.902379
I0810 01:36:27.028996 140199859279680 submission.py:120] 60) loss = 6.902, grad_norm = 0.594
I0810 01:36:27.389082 140156577240832 logging_writer.py:48] [61] global_step=61, grad_norm=0.574824, loss=6.909645
I0810 01:36:27.396352 140199859279680 submission.py:120] 61) loss = 6.910, grad_norm = 0.575
I0810 01:36:27.753140 140156585633536 logging_writer.py:48] [62] global_step=62, grad_norm=0.578635, loss=6.911564
I0810 01:36:27.758350 140199859279680 submission.py:120] 62) loss = 6.912, grad_norm = 0.579
I0810 01:36:28.113606 140156577240832 logging_writer.py:48] [63] global_step=63, grad_norm=0.601743, loss=6.903473
I0810 01:36:28.119824 140199859279680 submission.py:120] 63) loss = 6.903, grad_norm = 0.602
I0810 01:36:28.478576 140156585633536 logging_writer.py:48] [64] global_step=64, grad_norm=0.620402, loss=6.911524
I0810 01:36:28.483138 140199859279680 submission.py:120] 64) loss = 6.912, grad_norm = 0.620
I0810 01:36:28.843484 140156577240832 logging_writer.py:48] [65] global_step=65, grad_norm=0.587390, loss=6.903368
I0810 01:36:28.849407 140199859279680 submission.py:120] 65) loss = 6.903, grad_norm = 0.587
I0810 01:36:29.209418 140156585633536 logging_writer.py:48] [66] global_step=66, grad_norm=0.586910, loss=6.906043
I0810 01:36:29.213756 140199859279680 submission.py:120] 66) loss = 6.906, grad_norm = 0.587
I0810 01:36:29.575048 140156577240832 logging_writer.py:48] [67] global_step=67, grad_norm=0.599282, loss=6.904418
I0810 01:36:29.579499 140199859279680 submission.py:120] 67) loss = 6.904, grad_norm = 0.599
I0810 01:36:29.945430 140156585633536 logging_writer.py:48] [68] global_step=68, grad_norm=0.593893, loss=6.896468
I0810 01:36:29.949562 140199859279680 submission.py:120] 68) loss = 6.896, grad_norm = 0.594
I0810 01:36:30.340844 140156577240832 logging_writer.py:48] [69] global_step=69, grad_norm=0.572133, loss=6.898271
I0810 01:36:30.345200 140199859279680 submission.py:120] 69) loss = 6.898, grad_norm = 0.572
I0810 01:36:30.703542 140156585633536 logging_writer.py:48] [70] global_step=70, grad_norm=0.587343, loss=6.898225
I0810 01:36:30.709082 140199859279680 submission.py:120] 70) loss = 6.898, grad_norm = 0.587
I0810 01:36:31.070809 140156577240832 logging_writer.py:48] [71] global_step=71, grad_norm=0.586477, loss=6.900551
I0810 01:36:31.075174 140199859279680 submission.py:120] 71) loss = 6.901, grad_norm = 0.586
I0810 01:36:31.443339 140156585633536 logging_writer.py:48] [72] global_step=72, grad_norm=0.594217, loss=6.898499
I0810 01:36:31.447654 140199859279680 submission.py:120] 72) loss = 6.898, grad_norm = 0.594
I0810 01:36:31.856878 140156577240832 logging_writer.py:48] [73] global_step=73, grad_norm=0.610503, loss=6.893178
I0810 01:36:31.862075 140199859279680 submission.py:120] 73) loss = 6.893, grad_norm = 0.611
I0810 01:36:32.219044 140156585633536 logging_writer.py:48] [74] global_step=74, grad_norm=0.576262, loss=6.899547
I0810 01:36:32.223676 140199859279680 submission.py:120] 74) loss = 6.900, grad_norm = 0.576
I0810 01:36:32.579330 140156577240832 logging_writer.py:48] [75] global_step=75, grad_norm=0.579109, loss=6.893462
I0810 01:36:32.583464 140199859279680 submission.py:120] 75) loss = 6.893, grad_norm = 0.579
I0810 01:36:32.940591 140156585633536 logging_writer.py:48] [76] global_step=76, grad_norm=0.584094, loss=6.893392
I0810 01:36:32.946414 140199859279680 submission.py:120] 76) loss = 6.893, grad_norm = 0.584
I0810 01:36:33.301937 140156577240832 logging_writer.py:48] [77] global_step=77, grad_norm=0.587363, loss=6.897445
I0810 01:36:33.308281 140199859279680 submission.py:120] 77) loss = 6.897, grad_norm = 0.587
I0810 01:36:33.675667 140156585633536 logging_writer.py:48] [78] global_step=78, grad_norm=0.603105, loss=6.896602
I0810 01:36:33.680294 140199859279680 submission.py:120] 78) loss = 6.897, grad_norm = 0.603
I0810 01:36:34.044356 140156577240832 logging_writer.py:48] [79] global_step=79, grad_norm=0.612240, loss=6.900024
I0810 01:36:34.048838 140199859279680 submission.py:120] 79) loss = 6.900, grad_norm = 0.612
I0810 01:36:34.405495 140156585633536 logging_writer.py:48] [80] global_step=80, grad_norm=0.576002, loss=6.893939
I0810 01:36:34.411394 140199859279680 submission.py:120] 80) loss = 6.894, grad_norm = 0.576
I0810 01:36:34.769725 140156577240832 logging_writer.py:48] [81] global_step=81, grad_norm=0.582344, loss=6.893847
I0810 01:36:34.775098 140199859279680 submission.py:120] 81) loss = 6.894, grad_norm = 0.582
I0810 01:36:35.132443 140156585633536 logging_writer.py:48] [82] global_step=82, grad_norm=0.582556, loss=6.892363
I0810 01:36:35.137620 140199859279680 submission.py:120] 82) loss = 6.892, grad_norm = 0.583
I0810 01:36:35.496679 140156577240832 logging_writer.py:48] [83] global_step=83, grad_norm=0.597416, loss=6.896391
I0810 01:36:35.500646 140199859279680 submission.py:120] 83) loss = 6.896, grad_norm = 0.597
I0810 01:36:35.869399 140156585633536 logging_writer.py:48] [84] global_step=84, grad_norm=0.587675, loss=6.895084
I0810 01:36:35.874012 140199859279680 submission.py:120] 84) loss = 6.895, grad_norm = 0.588
I0810 01:36:36.239385 140156577240832 logging_writer.py:48] [85] global_step=85, grad_norm=0.576794, loss=6.889331
I0810 01:36:36.246609 140199859279680 submission.py:120] 85) loss = 6.889, grad_norm = 0.577
I0810 01:36:36.605991 140156585633536 logging_writer.py:48] [86] global_step=86, grad_norm=0.606900, loss=6.889292
I0810 01:36:36.610483 140199859279680 submission.py:120] 86) loss = 6.889, grad_norm = 0.607
I0810 01:36:36.977138 140156577240832 logging_writer.py:48] [87] global_step=87, grad_norm=0.591152, loss=6.891784
I0810 01:36:36.983691 140199859279680 submission.py:120] 87) loss = 6.892, grad_norm = 0.591
I0810 01:36:37.341548 140156585633536 logging_writer.py:48] [88] global_step=88, grad_norm=0.589644, loss=6.884268
I0810 01:36:37.345670 140199859279680 submission.py:120] 88) loss = 6.884, grad_norm = 0.590
I0810 01:36:37.703969 140156577240832 logging_writer.py:48] [89] global_step=89, grad_norm=0.587328, loss=6.881471
I0810 01:36:37.709413 140199859279680 submission.py:120] 89) loss = 6.881, grad_norm = 0.587
I0810 01:36:38.070899 140156585633536 logging_writer.py:48] [90] global_step=90, grad_norm=0.602023, loss=6.893006
I0810 01:36:38.075764 140199859279680 submission.py:120] 90) loss = 6.893, grad_norm = 0.602
I0810 01:36:38.452393 140156577240832 logging_writer.py:48] [91] global_step=91, grad_norm=0.584884, loss=6.879613
I0810 01:36:38.456954 140199859279680 submission.py:120] 91) loss = 6.880, grad_norm = 0.585
I0810 01:36:38.822505 140156585633536 logging_writer.py:48] [92] global_step=92, grad_norm=0.602907, loss=6.882949
I0810 01:36:38.826715 140199859279680 submission.py:120] 92) loss = 6.883, grad_norm = 0.603
I0810 01:36:39.191545 140156577240832 logging_writer.py:48] [93] global_step=93, grad_norm=0.590764, loss=6.882388
I0810 01:36:39.196885 140199859279680 submission.py:120] 93) loss = 6.882, grad_norm = 0.591
I0810 01:36:39.569570 140156585633536 logging_writer.py:48] [94] global_step=94, grad_norm=0.578998, loss=6.887684
I0810 01:36:39.574539 140199859279680 submission.py:120] 94) loss = 6.888, grad_norm = 0.579
I0810 01:36:39.930767 140156577240832 logging_writer.py:48] [95] global_step=95, grad_norm=0.591609, loss=6.882155
I0810 01:36:39.935293 140199859279680 submission.py:120] 95) loss = 6.882, grad_norm = 0.592
I0810 01:36:40.290138 140156585633536 logging_writer.py:48] [96] global_step=96, grad_norm=0.585986, loss=6.882454
I0810 01:36:40.295546 140199859279680 submission.py:120] 96) loss = 6.882, grad_norm = 0.586
I0810 01:36:40.651687 140156577240832 logging_writer.py:48] [97] global_step=97, grad_norm=0.599345, loss=6.877069
I0810 01:36:40.656358 140199859279680 submission.py:120] 97) loss = 6.877, grad_norm = 0.599
I0810 01:36:41.024874 140156585633536 logging_writer.py:48] [98] global_step=98, grad_norm=0.596560, loss=6.871088
I0810 01:36:41.032076 140199859279680 submission.py:120] 98) loss = 6.871, grad_norm = 0.597
I0810 01:36:41.398950 140156577240832 logging_writer.py:48] [99] global_step=99, grad_norm=0.597136, loss=6.876346
I0810 01:36:41.403114 140199859279680 submission.py:120] 99) loss = 6.876, grad_norm = 0.597
I0810 01:36:41.765312 140156585633536 logging_writer.py:48] [100] global_step=100, grad_norm=0.591534, loss=6.881032
I0810 01:36:41.771608 140199859279680 submission.py:120] 100) loss = 6.881, grad_norm = 0.592
I0810 01:39:04.046268 140156577240832 logging_writer.py:48] [500] global_step=500, grad_norm=1.961910, loss=6.275135
I0810 01:39:04.052155 140199859279680 submission.py:120] 500) loss = 6.275, grad_norm = 1.962
I0810 01:42:01.667877 140156585633536 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.404903, loss=5.670541
I0810 01:42:01.675226 140199859279680 submission.py:120] 1000) loss = 5.671, grad_norm = 2.405
I0810 01:44:34.878343 140199859279680 spec.py:320] Evaluating on the training split.
I0810 01:45:18.437791 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 01:46:14.787822 140199859279680 spec.py:348] Evaluating on the test split.
I0810 01:46:15.936439 140199859279680 submission_runner.py:362] Time since start: 883.68s, 	Step: 1428, 	{'train/accuracy': 0.11134805484693877, 'train/loss': 4.849654528559471, 'validation/accuracy': 0.10236, 'validation/loss': 4.9383471875, 'validation/num_examples': 50000, 'test/accuracy': 0.0674, 'test/loss': 5.42650078125, 'test/num_examples': 10000, 'score': 570.5793006420135, 'total_duration': 883.6768410205841, 'accumulated_submission_time': 570.5793006420135, 'accumulated_eval_time': 312.16871333122253, 'accumulated_logging_time': 0.03105902671813965}
I0810 01:46:15.958955 140156594026240 logging_writer.py:48] [1428] accumulated_eval_time=312.168713, accumulated_logging_time=0.031059, accumulated_submission_time=570.579301, global_step=1428, preemption_count=0, score=570.579301, test/accuracy=0.067400, test/loss=5.426501, test/num_examples=10000, total_duration=883.676841, train/accuracy=0.111348, train/loss=4.849655, validation/accuracy=0.102360, validation/loss=4.938347, validation/num_examples=50000
I0810 01:46:42.010766 140156677904128 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.361351, loss=5.276858
I0810 01:46:42.016155 140199859279680 submission.py:120] 1500) loss = 5.277, grad_norm = 4.361
I0810 01:49:38.027356 140156594026240 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.473996, loss=4.933497
I0810 01:49:38.033052 140199859279680 submission.py:120] 2000) loss = 4.933, grad_norm = 3.474
I0810 01:52:33.977534 140156677904128 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.544375, loss=4.683681
I0810 01:52:33.984999 140199859279680 submission.py:120] 2500) loss = 4.684, grad_norm = 4.544
I0810 01:54:46.223995 140199859279680 spec.py:320] Evaluating on the training split.
I0810 01:55:32.167962 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 01:56:29.303929 140199859279680 spec.py:348] Evaluating on the test split.
I0810 01:56:30.423172 140199859279680 submission_runner.py:362] Time since start: 1498.16s, 	Step: 2873, 	{'train/accuracy': 0.22907366071428573, 'train/loss': 3.850658183195153, 'validation/accuracy': 0.21002, 'validation/loss': 3.9867896875, 'validation/num_examples': 50000, 'test/accuracy': 0.1381, 'test/loss': 4.678465625, 'test/num_examples': 10000, 'score': 1079.560331106186, 'total_duration': 1498.1634321212769, 'accumulated_submission_time': 1079.560331106186, 'accumulated_eval_time': 416.36783742904663, 'accumulated_logging_time': 0.4551842212677002}
I0810 01:56:30.448111 140156594026240 logging_writer.py:48] [2873] accumulated_eval_time=416.367837, accumulated_logging_time=0.455184, accumulated_submission_time=1079.560331, global_step=2873, preemption_count=0, score=1079.560331, test/accuracy=0.138100, test/loss=4.678466, test/num_examples=10000, total_duration=1498.163432, train/accuracy=0.229074, train/loss=3.850658, validation/accuracy=0.210020, validation/loss=3.986790, validation/num_examples=50000
I0810 01:57:15.859565 140156677904128 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.790134, loss=4.352109
I0810 01:57:15.864115 140199859279680 submission.py:120] 3000) loss = 4.352, grad_norm = 4.790
I0810 02:00:11.668031 140156594026240 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.632101, loss=4.038262
I0810 02:00:11.673052 140199859279680 submission.py:120] 3500) loss = 4.038, grad_norm = 2.632
I0810 02:03:08.726219 140156677904128 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.736601, loss=3.958100
I0810 02:03:08.733283 140199859279680 submission.py:120] 4000) loss = 3.958, grad_norm = 2.737
I0810 02:05:00.622158 140199859279680 spec.py:320] Evaluating on the training split.
I0810 02:05:44.875857 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 02:06:40.813082 140199859279680 spec.py:348] Evaluating on the test split.
I0810 02:06:41.925187 140199859279680 submission_runner.py:362] Time since start: 2109.67s, 	Step: 4319, 	{'train/accuracy': 0.382991868622449, 'train/loss': 2.892535462671397, 'validation/accuracy': 0.35406, 'validation/loss': 3.0495034375, 'validation/num_examples': 50000, 'test/accuracy': 0.2624, 'test/loss': 3.67705625, 'test/num_examples': 10000, 'score': 1588.4398415088654, 'total_duration': 2109.665498495102, 'accumulated_submission_time': 1588.4398415088654, 'accumulated_eval_time': 517.670770406723, 'accumulated_logging_time': 0.8954789638519287}
I0810 02:06:41.947137 140156594026240 logging_writer.py:48] [4319] accumulated_eval_time=517.670770, accumulated_logging_time=0.895479, accumulated_submission_time=1588.439842, global_step=4319, preemption_count=0, score=1588.439842, test/accuracy=0.262400, test/loss=3.677056, test/num_examples=10000, total_duration=2109.665498, train/accuracy=0.382992, train/loss=2.892535, validation/accuracy=0.354060, validation/loss=3.049503, validation/num_examples=50000
I0810 02:07:46.329529 140156677904128 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.927058, loss=3.644182
I0810 02:07:46.334580 140199859279680 submission.py:120] 4500) loss = 3.644, grad_norm = 1.927
I0810 02:10:42.094663 140156594026240 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.536554, loss=3.651581
I0810 02:10:42.099539 140199859279680 submission.py:120] 5000) loss = 3.652, grad_norm = 3.537
I0810 02:13:39.524948 140156677904128 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.805651, loss=3.575328
I0810 02:13:39.529557 140199859279680 submission.py:120] 5500) loss = 3.575, grad_norm = 3.806
I0810 02:15:12.201292 140199859279680 spec.py:320] Evaluating on the training split.
I0810 02:15:57.468460 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 02:16:52.959948 140199859279680 spec.py:348] Evaluating on the test split.
I0810 02:16:54.075457 140199859279680 submission_runner.py:362] Time since start: 2721.82s, 	Step: 5764, 	{'train/accuracy': 0.45210857780612246, 'train/loss': 2.5112753109056123, 'validation/accuracy': 0.4117, 'validation/loss': 2.728236875, 'validation/num_examples': 50000, 'test/accuracy': 0.308, 'test/loss': 3.42851328125, 'test/num_examples': 10000, 'score': 2097.4035930633545, 'total_duration': 2721.815848350525, 'accumulated_submission_time': 2097.4035930633545, 'accumulated_eval_time': 619.5450084209442, 'accumulated_logging_time': 1.3322434425354004}
I0810 02:16:54.096613 140156594026240 logging_writer.py:48] [5764] accumulated_eval_time=619.545008, accumulated_logging_time=1.332243, accumulated_submission_time=2097.403593, global_step=5764, preemption_count=0, score=2097.403593, test/accuracy=0.308000, test/loss=3.428513, test/num_examples=10000, total_duration=2721.815848, train/accuracy=0.452109, train/loss=2.511275, validation/accuracy=0.411700, validation/loss=2.728237, validation/num_examples=50000
I0810 02:18:17.749944 140156677904128 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.360684, loss=3.348849
I0810 02:18:17.756147 140199859279680 submission.py:120] 6000) loss = 3.349, grad_norm = 1.361
I0810 02:21:14.856200 140156594026240 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.517400, loss=3.366849
I0810 02:21:14.861298 140199859279680 submission.py:120] 6500) loss = 3.367, grad_norm = 1.517
I0810 02:24:10.559133 140156677904128 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.443778, loss=3.219091
I0810 02:24:10.563674 140199859279680 submission.py:120] 7000) loss = 3.219, grad_norm = 1.444
I0810 02:25:24.306206 140199859279680 spec.py:320] Evaluating on the training split.
I0810 02:26:09.239486 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 02:27:07.464366 140199859279680 spec.py:348] Evaluating on the test split.
I0810 02:27:08.571421 140199859279680 submission_runner.py:362] Time since start: 3336.31s, 	Step: 7211, 	{'train/accuracy': 0.5068957270408163, 'train/loss': 2.235393290617028, 'validation/accuracy': 0.46676, 'validation/loss': 2.45919296875, 'validation/num_examples': 50000, 'test/accuracy': 0.3604, 'test/loss': 3.116198828125, 'test/num_examples': 10000, 'score': 2606.3233728408813, 'total_duration': 3336.3106327056885, 'accumulated_submission_time': 2606.3233728408813, 'accumulated_eval_time': 723.8091795444489, 'accumulated_logging_time': 1.760321855545044}
I0810 02:27:08.592104 140156594026240 logging_writer.py:48] [7211] accumulated_eval_time=723.809180, accumulated_logging_time=1.760322, accumulated_submission_time=2606.323373, global_step=7211, preemption_count=0, score=2606.323373, test/accuracy=0.360400, test/loss=3.116199, test/num_examples=10000, total_duration=3336.310633, train/accuracy=0.506896, train/loss=2.235393, validation/accuracy=0.466760, validation/loss=2.459193, validation/num_examples=50000
I0810 02:28:50.711610 140156677904128 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.296693, loss=3.151855
I0810 02:28:50.716337 140199859279680 submission.py:120] 7500) loss = 3.152, grad_norm = 1.297
I0810 02:31:48.153258 140156594026240 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.215083, loss=3.159247
I0810 02:31:48.158416 140199859279680 submission.py:120] 8000) loss = 3.159, grad_norm = 1.215
I0810 02:34:43.826954 140156677904128 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.172079, loss=3.110819
I0810 02:34:43.835024 140199859279680 submission.py:120] 8500) loss = 3.111, grad_norm = 1.172
I0810 02:35:38.619370 140199859279680 spec.py:320] Evaluating on the training split.
I0810 02:36:22.644949 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 02:37:18.537319 140199859279680 spec.py:348] Evaluating on the test split.
I0810 02:37:19.646909 140199859279680 submission_runner.py:362] Time since start: 3947.39s, 	Step: 8657, 	{'train/accuracy': 0.5609853316326531, 'train/loss': 1.9595878756776148, 'validation/accuracy': 0.506, 'validation/loss': 2.22082265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3819, 'test/loss': 3.00467578125, 'test/num_examples': 10000, 'score': 3115.06165766716, 'total_duration': 3947.3873014450073, 'accumulated_submission_time': 3115.06165766716, 'accumulated_eval_time': 824.837128162384, 'accumulated_logging_time': 2.1825380325317383}
I0810 02:37:19.670082 140156594026240 logging_writer.py:48] [8657] accumulated_eval_time=824.837128, accumulated_logging_time=2.182538, accumulated_submission_time=3115.061658, global_step=8657, preemption_count=0, score=3115.061658, test/accuracy=0.381900, test/loss=3.004676, test/num_examples=10000, total_duration=3947.387301, train/accuracy=0.560985, train/loss=1.959588, validation/accuracy=0.506000, validation/loss=2.220823, validation/num_examples=50000
I0810 02:39:22.600657 140156677904128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.979709, loss=3.050960
I0810 02:39:22.608926 140199859279680 submission.py:120] 9000) loss = 3.051, grad_norm = 0.980
I0810 02:42:18.291559 140156594026240 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.866605, loss=2.998257
I0810 02:42:18.299751 140199859279680 submission.py:120] 9500) loss = 2.998, grad_norm = 0.867
I0810 02:45:13.744642 140156677904128 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.946871, loss=2.949239
I0810 02:45:13.749613 140199859279680 submission.py:120] 10000) loss = 2.949, grad_norm = 0.947
I0810 02:45:49.958112 140199859279680 spec.py:320] Evaluating on the training split.
I0810 02:46:33.803452 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 02:47:32.405760 140199859279680 spec.py:348] Evaluating on the test split.
I0810 02:47:33.514793 140199859279680 submission_runner.py:362] Time since start: 4561.26s, 	Step: 10100, 	{'train/accuracy': 0.6074816645408163, 'train/loss': 1.7587233562858737, 'validation/accuracy': 0.54956, 'validation/loss': 2.0357709375, 'validation/num_examples': 50000, 'test/accuracy': 0.4173, 'test/loss': 2.803833203125, 'test/num_examples': 10000, 'score': 3624.0718228816986, 'total_duration': 4561.255175828934, 'accumulated_submission_time': 3624.0718228816986, 'accumulated_eval_time': 928.3938958644867, 'accumulated_logging_time': 2.603935480117798}
I0810 02:47:33.532449 140156594026240 logging_writer.py:48] [10100] accumulated_eval_time=928.393896, accumulated_logging_time=2.603935, accumulated_submission_time=3624.071823, global_step=10100, preemption_count=0, score=3624.071823, test/accuracy=0.417300, test/loss=2.803833, test/num_examples=10000, total_duration=4561.255176, train/accuracy=0.607482, train/loss=1.758723, validation/accuracy=0.549560, validation/loss=2.035771, validation/num_examples=50000
I0810 02:49:54.928828 140156677904128 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.824604, loss=2.931277
I0810 02:49:54.934484 140199859279680 submission.py:120] 10500) loss = 2.931, grad_norm = 0.825
I0810 02:52:50.614742 140156594026240 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.803011, loss=2.852417
I0810 02:52:50.621778 140199859279680 submission.py:120] 11000) loss = 2.852, grad_norm = 0.803
I0810 02:55:47.761501 140156677904128 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.674322, loss=2.734110
I0810 02:55:47.768557 140199859279680 submission.py:120] 11500) loss = 2.734, grad_norm = 0.674
I0810 02:56:03.620439 140199859279680 spec.py:320] Evaluating on the training split.
I0810 02:56:48.285726 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 02:57:44.769752 140199859279680 spec.py:348] Evaluating on the test split.
I0810 02:57:45.878771 140199859279680 submission_runner.py:362] Time since start: 5173.62s, 	Step: 11546, 	{'train/accuracy': 0.6476203762755102, 'train/loss': 1.52553433788066, 'validation/accuracy': 0.58698, 'validation/loss': 1.82519328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4454, 'test/loss': 2.5933595703125, 'test/num_examples': 10000, 'score': 4132.879248380661, 'total_duration': 5173.619136333466, 'accumulated_submission_time': 4132.879248380661, 'accumulated_eval_time': 1030.6523780822754, 'accumulated_logging_time': 3.0133228302001953}
I0810 02:57:45.901557 140156594026240 logging_writer.py:48] [11546] accumulated_eval_time=1030.652378, accumulated_logging_time=3.013323, accumulated_submission_time=4132.879248, global_step=11546, preemption_count=0, score=4132.879248, test/accuracy=0.445400, test/loss=2.593360, test/num_examples=10000, total_duration=5173.619136, train/accuracy=0.647620, train/loss=1.525534, validation/accuracy=0.586980, validation/loss=1.825193, validation/num_examples=50000
I0810 03:00:26.087486 140156677904128 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.386614, loss=2.773494
I0810 03:00:26.092813 140199859279680 submission.py:120] 12000) loss = 2.773, grad_norm = 1.387
I0810 03:03:21.450225 140156594026240 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.618168, loss=2.796896
I0810 03:03:21.461127 140199859279680 submission.py:120] 12500) loss = 2.797, grad_norm = 0.618
I0810 03:06:15.921219 140199859279680 spec.py:320] Evaluating on the training split.
I0810 03:07:00.887746 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 03:07:58.199008 140199859279680 spec.py:348] Evaluating on the test split.
I0810 03:07:59.302724 140199859279680 submission_runner.py:362] Time since start: 5787.04s, 	Step: 12993, 	{'train/accuracy': 0.6516262755102041, 'train/loss': 1.5083411080496651, 'validation/accuracy': 0.58582, 'validation/loss': 1.82770390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4537, 'test/loss': 2.5629427734375, 'test/num_examples': 10000, 'score': 4641.636296749115, 'total_duration': 5787.043079376221, 'accumulated_submission_time': 4641.636296749115, 'accumulated_eval_time': 1134.033985376358, 'accumulated_logging_time': 3.4261820316314697}
I0810 03:07:59.326294 140156677904128 logging_writer.py:48] [12993] accumulated_eval_time=1134.033985, accumulated_logging_time=3.426182, accumulated_submission_time=4641.636297, global_step=12993, preemption_count=0, score=4641.636297, test/accuracy=0.453700, test/loss=2.562943, test/num_examples=10000, total_duration=5787.043079, train/accuracy=0.651626, train/loss=1.508341, validation/accuracy=0.585820, validation/loss=1.827704, validation/num_examples=50000
I0810 03:08:02.652297 140156594026240 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.598238, loss=2.672481
I0810 03:08:02.656592 140199859279680 submission.py:120] 13000) loss = 2.672, grad_norm = 0.598
I0810 03:10:58.079769 140156677904128 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.588889, loss=2.751843
I0810 03:10:58.090433 140199859279680 submission.py:120] 13500) loss = 2.752, grad_norm = 0.589
I0810 03:13:57.046630 140199859279680 spec.py:320] Evaluating on the training split.
I0810 03:14:40.715080 140199859279680 spec.py:332] Evaluating on the validation split.
I0810 03:15:37.395886 140199859279680 spec.py:348] Evaluating on the test split.
I0810 03:15:38.507946 140199859279680 submission_runner.py:362] Time since start: 6246.25s, 	Step: 14000, 	{'train/accuracy': 0.6518056441326531, 'train/loss': 1.523495421117666, 'validation/accuracy': 0.58158, 'validation/loss': 1.85739265625, 'validation/num_examples': 50000, 'test/accuracy': 0.4484, 'test/loss': 2.61544921875, 'test/num_examples': 10000, 'score': 4998.353546142578, 'total_duration': 6246.246742010117, 'accumulated_submission_time': 4998.353546142578, 'accumulated_eval_time': 1235.4936935901642, 'accumulated_logging_time': 3.84303879737854}
I0810 03:15:38.528482 140156594026240 logging_writer.py:48] [14000] accumulated_eval_time=1235.493694, accumulated_logging_time=3.843039, accumulated_submission_time=4998.353546, global_step=14000, preemption_count=0, score=4998.353546, test/accuracy=0.448400, test/loss=2.615449, test/num_examples=10000, total_duration=6246.246742, train/accuracy=0.651806, train/loss=1.523495, validation/accuracy=0.581580, validation/loss=1.857393, validation/num_examples=50000
I0810 03:15:38.945747 140156677904128 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4998.353546
I0810 03:15:39.708547 140199859279680 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_resnet_pytorch/trial_1/checkpoint_14000.
I0810 03:15:40.059892 140199859279680 submission_runner.py:528] Tuning trial 1/1
I0810 03:15:40.060109 140199859279680 submission_runner.py:529] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0810 03:15:40.060819 140199859279680 submission_runner.py:530] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007971938775510204, 'train/loss': 6.916522590481505, 'validation/accuracy': 0.00096, 'validation/loss': 6.91603625, 'validation/num_examples': 50000, 'test/accuracy': 0.0012, 'test/loss': 6.9179359375, 'test/num_examples': 10000, 'score': 61.496670722961426, 'total_duration': 272.6078770160675, 'accumulated_submission_time': 61.496670722961426, 'accumulated_eval_time': 211.11052703857422, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1428, {'train/accuracy': 0.11134805484693877, 'train/loss': 4.849654528559471, 'validation/accuracy': 0.10236, 'validation/loss': 4.9383471875, 'validation/num_examples': 50000, 'test/accuracy': 0.0674, 'test/loss': 5.42650078125, 'test/num_examples': 10000, 'score': 570.5793006420135, 'total_duration': 883.6768410205841, 'accumulated_submission_time': 570.5793006420135, 'accumulated_eval_time': 312.16871333122253, 'accumulated_logging_time': 0.03105902671813965, 'global_step': 1428, 'preemption_count': 0}), (2873, {'train/accuracy': 0.22907366071428573, 'train/loss': 3.850658183195153, 'validation/accuracy': 0.21002, 'validation/loss': 3.9867896875, 'validation/num_examples': 50000, 'test/accuracy': 0.1381, 'test/loss': 4.678465625, 'test/num_examples': 10000, 'score': 1079.560331106186, 'total_duration': 1498.1634321212769, 'accumulated_submission_time': 1079.560331106186, 'accumulated_eval_time': 416.36783742904663, 'accumulated_logging_time': 0.4551842212677002, 'global_step': 2873, 'preemption_count': 0}), (4319, {'train/accuracy': 0.382991868622449, 'train/loss': 2.892535462671397, 'validation/accuracy': 0.35406, 'validation/loss': 3.0495034375, 'validation/num_examples': 50000, 'test/accuracy': 0.2624, 'test/loss': 3.67705625, 'test/num_examples': 10000, 'score': 1588.4398415088654, 'total_duration': 2109.665498495102, 'accumulated_submission_time': 1588.4398415088654, 'accumulated_eval_time': 517.670770406723, 'accumulated_logging_time': 0.8954789638519287, 'global_step': 4319, 'preemption_count': 0}), (5764, {'train/accuracy': 0.45210857780612246, 'train/loss': 2.5112753109056123, 'validation/accuracy': 0.4117, 'validation/loss': 2.728236875, 'validation/num_examples': 50000, 'test/accuracy': 0.308, 'test/loss': 3.42851328125, 'test/num_examples': 10000, 'score': 2097.4035930633545, 'total_duration': 2721.815848350525, 'accumulated_submission_time': 2097.4035930633545, 'accumulated_eval_time': 619.5450084209442, 'accumulated_logging_time': 1.3322434425354004, 'global_step': 5764, 'preemption_count': 0}), (7211, {'train/accuracy': 0.5068957270408163, 'train/loss': 2.235393290617028, 'validation/accuracy': 0.46676, 'validation/loss': 2.45919296875, 'validation/num_examples': 50000, 'test/accuracy': 0.3604, 'test/loss': 3.116198828125, 'test/num_examples': 10000, 'score': 2606.3233728408813, 'total_duration': 3336.3106327056885, 'accumulated_submission_time': 2606.3233728408813, 'accumulated_eval_time': 723.8091795444489, 'accumulated_logging_time': 1.760321855545044, 'global_step': 7211, 'preemption_count': 0}), (8657, {'train/accuracy': 0.5609853316326531, 'train/loss': 1.9595878756776148, 'validation/accuracy': 0.506, 'validation/loss': 2.22082265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3819, 'test/loss': 3.00467578125, 'test/num_examples': 10000, 'score': 3115.06165766716, 'total_duration': 3947.3873014450073, 'accumulated_submission_time': 3115.06165766716, 'accumulated_eval_time': 824.837128162384, 'accumulated_logging_time': 2.1825380325317383, 'global_step': 8657, 'preemption_count': 0}), (10100, {'train/accuracy': 0.6074816645408163, 'train/loss': 1.7587233562858737, 'validation/accuracy': 0.54956, 'validation/loss': 2.0357709375, 'validation/num_examples': 50000, 'test/accuracy': 0.4173, 'test/loss': 2.803833203125, 'test/num_examples': 10000, 'score': 3624.0718228816986, 'total_duration': 4561.255175828934, 'accumulated_submission_time': 3624.0718228816986, 'accumulated_eval_time': 928.3938958644867, 'accumulated_logging_time': 2.603935480117798, 'global_step': 10100, 'preemption_count': 0}), (11546, {'train/accuracy': 0.6476203762755102, 'train/loss': 1.52553433788066, 'validation/accuracy': 0.58698, 'validation/loss': 1.82519328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4454, 'test/loss': 2.5933595703125, 'test/num_examples': 10000, 'score': 4132.879248380661, 'total_duration': 5173.619136333466, 'accumulated_submission_time': 4132.879248380661, 'accumulated_eval_time': 1030.6523780822754, 'accumulated_logging_time': 3.0133228302001953, 'global_step': 11546, 'preemption_count': 0}), (12993, {'train/accuracy': 0.6516262755102041, 'train/loss': 1.5083411080496651, 'validation/accuracy': 0.58582, 'validation/loss': 1.82770390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4537, 'test/loss': 2.5629427734375, 'test/num_examples': 10000, 'score': 4641.636296749115, 'total_duration': 5787.043079376221, 'accumulated_submission_time': 4641.636296749115, 'accumulated_eval_time': 1134.033985376358, 'accumulated_logging_time': 3.4261820316314697, 'global_step': 12993, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6518056441326531, 'train/loss': 1.523495421117666, 'validation/accuracy': 0.58158, 'validation/loss': 1.85739265625, 'validation/num_examples': 50000, 'test/accuracy': 0.4484, 'test/loss': 2.61544921875, 'test/num_examples': 10000, 'score': 4998.353546142578, 'total_duration': 6246.246742010117, 'accumulated_submission_time': 4998.353546142578, 'accumulated_eval_time': 1235.4936935901642, 'accumulated_logging_time': 3.84303879737854, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0810 03:15:40.060931 140199859279680 submission_runner.py:531] Timing: 4998.353546142578
I0810 03:15:40.060984 140199859279680 submission_runner.py:533] Total number of evals: 11
I0810 03:15:40.061030 140199859279680 submission_runner.py:534] ====================
I0810 03:15:40.061144 140199859279680 submission_runner.py:602] Final imagenet_resnet score: 4998.353546142578
