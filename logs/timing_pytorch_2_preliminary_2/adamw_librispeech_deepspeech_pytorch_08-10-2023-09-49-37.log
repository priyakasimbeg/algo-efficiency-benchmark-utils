torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_2/adamw --overwrite=true --save_checkpoints=false --max_global_steps=8000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --torch_compile=True 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_08-10-2023-09-49-37.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-10 09:49:47.577560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 09:49:47.577572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0810 09:50:02.497724 139703354869568 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0810 09:50:02.497730 140299333195584 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0810 09:50:02.497753 140147633821504 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0810 09:50:02.497789 140098374854464 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0810 09:50:02.498645 139633711261504 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0810 09:50:02.498772 140019564005184 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0810 09:50:02.498822 139960920717120 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0810 09:50:02.498853 139634825914176 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0810 09:50:02.498961 139633711261504 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.499077 140019564005184 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.499095 139960920717120 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.499142 139634825914176 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.508410 139703354869568 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.508433 140299333195584 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.508494 140098374854464 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:02.508512 140147633821504 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 09:50:03.040897 139634825914176 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/librispeech_deepspeech_pytorch.
W0810 09:50:03.080257 139634825914176 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.080256 140147633821504 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.080261 139703354869568 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.080302 140299333195584 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.080317 139633711261504 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.081196 140098374854464 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.081499 139960920717120 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 09:50:03.083018 140019564005184 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 09:50:03.086867 139634825914176 submission_runner.py:488] Using RNG seed 2619008487
I0810 09:50:03.088928 139634825914176 submission_runner.py:497] --- Tuning run 1/1 ---
I0810 09:50:03.089069 139634825914176 submission_runner.py:502] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/librispeech_deepspeech_pytorch/trial_1.
I0810 09:50:03.089531 139634825914176 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/librispeech_deepspeech_pytorch/trial_1/hparams.json.
I0810 09:50:03.090497 139634825914176 submission_runner.py:176] Initializing dataset.
I0810 09:50:03.090650 139634825914176 input_pipeline.py:20] Loading split = train-clean-100
I0810 09:50:03.125987 139634825914176 input_pipeline.py:20] Loading split = train-clean-360
I0810 09:50:03.527632 139634825914176 input_pipeline.py:20] Loading split = train-other-500
I0810 09:50:04.015304 139634825914176 submission_runner.py:183] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
W0810 09:50:12.306061 140147633821504 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 09:50:12.306509 140299333195584 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 09:50:12.308434 139703354869568 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 09:50:12.308495 139634825914176 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0810 09:50:12.308685 139634825914176 submission_runner.py:215] Initializing optimizer.
W0810 09:50:12.308722 139960920717120 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0810 09:50:12.309474 139634825914176 submission_runner.py:222] Initializing metrics bundle.
I0810 09:50:12.309600 139634825914176 submission_runner.py:240] Initializing checkpoint and logger.
I0810 09:50:12.310360 139634825914176 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0810 09:50:12.310486 139634825914176 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
W0810 09:50:12.326281 140098374854464 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 09:50:12.328229 140019564005184 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 09:50:12.333770 139633711261504 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0810 09:50:12.986000 139634825914176 submission_runner.py:261] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/librispeech_deepspeech_pytorch/trial_1/meta_data_0.json.
I0810 09:50:12.987316 139634825914176 submission_runner.py:264] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/librispeech_deepspeech_pytorch/trial_1/flags_0.json.
I0810 09:50:12.999697 139634825914176 submission_runner.py:274] Starting training loop.
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  spectrum = torch.abs(spectrum)
I0810 09:50:47.404474 139609161844480 logging_writer.py:48] [0] global_step=0, grad_norm=19.946661, loss=33.468704
I0810 09:50:47.436334 139634825914176 submission.py:120] 0) loss = 33.469, grad_norm = 19.947
I0810 09:50:47.438162 139634825914176 spec.py:320] Evaluating on the training split.
I0810 09:50:47.439461 139634825914176 input_pipeline.py:20] Loading split = train-clean-100
I0810 09:50:47.476327 139634825914176 input_pipeline.py:20] Loading split = train-clean-360
I0810 09:50:47.961919 139634825914176 input_pipeline.py:20] Loading split = train-other-500
I0810 09:51:10.028079 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 09:51:10.029647 139634825914176 input_pipeline.py:20] Loading split = dev-clean
I0810 09:51:10.033610 139634825914176 input_pipeline.py:20] Loading split = dev-other
I0810 09:51:25.448531 139634825914176 spec.py:348] Evaluating on the test split.
I0810 09:51:25.450040 139634825914176 input_pipeline.py:20] Loading split = test-clean
I0810 09:51:34.586489 139634825914176 submission_runner.py:362] Time since start: 81.59s, 	Step: 1, 	{'train/ctc_loss': 30.675214336666713, 'train/wer': 4.270925893974033, 'validation/ctc_loss': 29.309482996785214, 'validation/wer': 3.935016656206247, 'validation/num_examples': 5348, 'test/ctc_loss': 29.5897509516743, 'test/wer': 4.1791278207706215, 'test/num_examples': 2472, 'score': 34.43839430809021, 'total_duration': 81.58680367469788, 'accumulated_submission_time': 34.43839430809021, 'accumulated_eval_time': 47.1478054523468, 'accumulated_logging_time': 0}
I0810 09:51:34.615866 139594807408384 logging_writer.py:48] [1] accumulated_eval_time=47.147805, accumulated_logging_time=0, accumulated_submission_time=34.438394, global_step=1, preemption_count=0, score=34.438394, test/ctc_loss=29.589751, test/num_examples=2472, test/wer=4.179128, total_duration=81.586804, train/ctc_loss=30.675214, train/wer=4.270926, validation/ctc_loss=29.309483, validation/num_examples=5348, validation/wer=3.935017
I0810 09:51:34.698873 139634825914176 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.699044 140098374854464 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.699199 140147633821504 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.699654 139633711261504 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.699610 139703354869568 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.699688 140299333195584 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.700032 140019564005184 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:34.700089 139960920717120 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 09:51:35.679674 139594799015680 logging_writer.py:48] [1] global_step=1, grad_norm=20.513458, loss=32.775364
I0810 09:51:35.684209 139634825914176 submission.py:120] 1) loss = 32.775, grad_norm = 20.513
I0810 09:51:36.597930 139594807408384 logging_writer.py:48] [2] global_step=2, grad_norm=20.562403, loss=33.378933
I0810 09:51:36.601657 139634825914176 submission.py:120] 2) loss = 33.379, grad_norm = 20.562
I0810 09:51:37.523628 139594799015680 logging_writer.py:48] [3] global_step=3, grad_norm=20.849394, loss=33.408543
I0810 09:51:37.527238 139634825914176 submission.py:120] 3) loss = 33.409, grad_norm = 20.849
I0810 09:51:38.446747 139594807408384 logging_writer.py:48] [4] global_step=4, grad_norm=22.150984, loss=32.874443
I0810 09:51:38.450678 139634825914176 submission.py:120] 4) loss = 32.874, grad_norm = 22.151
I0810 09:51:39.378369 139594799015680 logging_writer.py:48] [5] global_step=5, grad_norm=21.313080, loss=33.193825
I0810 09:51:39.381928 139634825914176 submission.py:120] 5) loss = 33.194, grad_norm = 21.313
I0810 09:51:40.294831 139594807408384 logging_writer.py:48] [6] global_step=6, grad_norm=23.420328, loss=33.204979
I0810 09:51:40.298186 139634825914176 submission.py:120] 6) loss = 33.205, grad_norm = 23.420
I0810 09:51:41.208797 139594799015680 logging_writer.py:48] [7] global_step=7, grad_norm=24.592066, loss=32.190327
I0810 09:51:41.212469 139634825914176 submission.py:120] 7) loss = 32.190, grad_norm = 24.592
I0810 09:51:42.128178 139594807408384 logging_writer.py:48] [8] global_step=8, grad_norm=26.599604, loss=32.527199
I0810 09:51:42.131855 139634825914176 submission.py:120] 8) loss = 32.527, grad_norm = 26.600
I0810 09:51:43.040073 139594799015680 logging_writer.py:48] [9] global_step=9, grad_norm=28.299330, loss=32.131634
I0810 09:51:43.043570 139634825914176 submission.py:120] 9) loss = 32.132, grad_norm = 28.299
I0810 09:51:43.954027 139594807408384 logging_writer.py:48] [10] global_step=10, grad_norm=32.569740, loss=31.933132
I0810 09:51:43.957418 139634825914176 submission.py:120] 10) loss = 31.933, grad_norm = 32.570
I0810 09:51:44.866976 139594799015680 logging_writer.py:48] [11] global_step=11, grad_norm=37.521706, loss=31.908157
I0810 09:51:44.870547 139634825914176 submission.py:120] 11) loss = 31.908, grad_norm = 37.522
I0810 09:51:45.784246 139594807408384 logging_writer.py:48] [12] global_step=12, grad_norm=40.766457, loss=31.779219
I0810 09:51:45.787774 139634825914176 submission.py:120] 12) loss = 31.779, grad_norm = 40.766
I0810 09:51:46.700343 139594799015680 logging_writer.py:48] [13] global_step=13, grad_norm=43.930107, loss=30.992840
I0810 09:51:46.703872 139634825914176 submission.py:120] 13) loss = 30.993, grad_norm = 43.930
I0810 09:51:47.616624 139594807408384 logging_writer.py:48] [14] global_step=14, grad_norm=46.627735, loss=30.378832
I0810 09:51:47.620232 139634825914176 submission.py:120] 14) loss = 30.379, grad_norm = 46.628
I0810 09:51:48.527205 139594799015680 logging_writer.py:48] [15] global_step=15, grad_norm=45.322723, loss=29.176702
I0810 09:51:48.531091 139634825914176 submission.py:120] 15) loss = 29.177, grad_norm = 45.323
I0810 09:51:49.442749 139594807408384 logging_writer.py:48] [16] global_step=16, grad_norm=44.311661, loss=29.147833
I0810 09:51:49.446644 139634825914176 submission.py:120] 16) loss = 29.148, grad_norm = 44.312
I0810 09:51:50.358319 139594799015680 logging_writer.py:48] [17] global_step=17, grad_norm=43.589352, loss=28.397804
I0810 09:51:50.361812 139634825914176 submission.py:120] 17) loss = 28.398, grad_norm = 43.589
I0810 09:51:51.271396 139594807408384 logging_writer.py:48] [18] global_step=18, grad_norm=43.266987, loss=27.599443
I0810 09:51:51.275214 139634825914176 submission.py:120] 18) loss = 27.599, grad_norm = 43.267
I0810 09:51:52.189156 139594799015680 logging_writer.py:48] [19] global_step=19, grad_norm=41.628429, loss=26.561371
I0810 09:51:52.192776 139634825914176 submission.py:120] 19) loss = 26.561, grad_norm = 41.628
I0810 09:51:53.108502 139594807408384 logging_writer.py:48] [20] global_step=20, grad_norm=39.293861, loss=25.651991
I0810 09:51:53.112028 139634825914176 submission.py:120] 20) loss = 25.652, grad_norm = 39.294
I0810 09:51:54.028183 139594799015680 logging_writer.py:48] [21] global_step=21, grad_norm=38.434269, loss=24.880619
I0810 09:51:54.031893 139634825914176 submission.py:120] 21) loss = 24.881, grad_norm = 38.434
I0810 09:51:54.944777 139594807408384 logging_writer.py:48] [22] global_step=22, grad_norm=38.613506, loss=24.776117
I0810 09:51:54.948023 139634825914176 submission.py:120] 22) loss = 24.776, grad_norm = 38.614
I0810 09:51:55.864705 139594799015680 logging_writer.py:48] [23] global_step=23, grad_norm=36.205963, loss=23.478271
I0810 09:51:55.867959 139634825914176 submission.py:120] 23) loss = 23.478, grad_norm = 36.206
I0810 09:51:56.783175 139594807408384 logging_writer.py:48] [24] global_step=24, grad_norm=35.575863, loss=22.919991
I0810 09:51:56.786567 139634825914176 submission.py:120] 24) loss = 22.920, grad_norm = 35.576
I0810 09:51:57.705253 139594799015680 logging_writer.py:48] [25] global_step=25, grad_norm=34.329781, loss=22.518156
I0810 09:51:57.708479 139634825914176 submission.py:120] 25) loss = 22.518, grad_norm = 34.330
I0810 09:51:58.625740 139594807408384 logging_writer.py:48] [26] global_step=26, grad_norm=32.915001, loss=21.309025
I0810 09:51:58.629343 139634825914176 submission.py:120] 26) loss = 21.309, grad_norm = 32.915
I0810 09:51:59.549506 139594799015680 logging_writer.py:48] [27] global_step=27, grad_norm=31.523602, loss=20.940598
I0810 09:51:59.552814 139634825914176 submission.py:120] 27) loss = 20.941, grad_norm = 31.524
I0810 09:52:00.469689 139594807408384 logging_writer.py:48] [28] global_step=28, grad_norm=29.469118, loss=20.267662
I0810 09:52:00.473161 139634825914176 submission.py:120] 28) loss = 20.268, grad_norm = 29.469
I0810 09:52:01.383801 139594799015680 logging_writer.py:48] [29] global_step=29, grad_norm=27.874929, loss=19.350712
I0810 09:52:01.387179 139634825914176 submission.py:120] 29) loss = 19.351, grad_norm = 27.875
I0810 09:52:02.298028 139594807408384 logging_writer.py:48] [30] global_step=30, grad_norm=26.261259, loss=18.856733
I0810 09:52:02.301487 139634825914176 submission.py:120] 30) loss = 18.857, grad_norm = 26.261
I0810 09:52:03.213560 139594799015680 logging_writer.py:48] [31] global_step=31, grad_norm=24.973799, loss=18.265491
I0810 09:52:03.217002 139634825914176 submission.py:120] 31) loss = 18.265, grad_norm = 24.974
I0810 09:52:04.131624 139594807408384 logging_writer.py:48] [32] global_step=32, grad_norm=23.333584, loss=17.863813
I0810 09:52:04.135250 139634825914176 submission.py:120] 32) loss = 17.864, grad_norm = 23.334
I0810 09:52:05.051812 139594799015680 logging_writer.py:48] [33] global_step=33, grad_norm=22.782080, loss=17.590185
I0810 09:52:05.055384 139634825914176 submission.py:120] 33) loss = 17.590, grad_norm = 22.782
I0810 09:52:05.968173 139594807408384 logging_writer.py:48] [34] global_step=34, grad_norm=21.529367, loss=17.059519
I0810 09:52:05.971534 139634825914176 submission.py:120] 34) loss = 17.060, grad_norm = 21.529
I0810 09:52:06.885560 139594799015680 logging_writer.py:48] [35] global_step=35, grad_norm=20.520468, loss=16.699806
I0810 09:52:06.888870 139634825914176 submission.py:120] 35) loss = 16.700, grad_norm = 20.520
I0810 09:52:07.796947 139594807408384 logging_writer.py:48] [36] global_step=36, grad_norm=19.821848, loss=16.021214
I0810 09:52:07.800344 139634825914176 submission.py:120] 36) loss = 16.021, grad_norm = 19.822
I0810 09:52:08.708357 139594799015680 logging_writer.py:48] [37] global_step=37, grad_norm=18.137522, loss=15.354171
I0810 09:52:08.711848 139634825914176 submission.py:120] 37) loss = 15.354, grad_norm = 18.138
I0810 09:52:09.624007 139594807408384 logging_writer.py:48] [38] global_step=38, grad_norm=17.569632, loss=15.372946
I0810 09:52:09.627468 139634825914176 submission.py:120] 38) loss = 15.373, grad_norm = 17.570
I0810 09:52:10.533841 139594799015680 logging_writer.py:48] [39] global_step=39, grad_norm=16.944719, loss=15.122000
I0810 09:52:10.537260 139634825914176 submission.py:120] 39) loss = 15.122, grad_norm = 16.945
I0810 09:52:11.448032 139594807408384 logging_writer.py:48] [40] global_step=40, grad_norm=15.288530, loss=14.479461
I0810 09:52:11.451714 139634825914176 submission.py:120] 40) loss = 14.479, grad_norm = 15.289
I0810 09:52:12.363270 139594799015680 logging_writer.py:48] [41] global_step=41, grad_norm=14.979230, loss=14.048974
I0810 09:52:12.367052 139634825914176 submission.py:120] 41) loss = 14.049, grad_norm = 14.979
I0810 09:52:13.283609 139594807408384 logging_writer.py:48] [42] global_step=42, grad_norm=14.952062, loss=13.842924
I0810 09:52:13.287060 139634825914176 submission.py:120] 42) loss = 13.843, grad_norm = 14.952
I0810 09:52:14.204401 139594799015680 logging_writer.py:48] [43] global_step=43, grad_norm=13.342010, loss=13.492837
I0810 09:52:14.207772 139634825914176 submission.py:120] 43) loss = 13.493, grad_norm = 13.342
I0810 09:52:15.123600 139594807408384 logging_writer.py:48] [44] global_step=44, grad_norm=12.766487, loss=13.168150
I0810 09:52:15.127320 139634825914176 submission.py:120] 44) loss = 13.168, grad_norm = 12.766
I0810 09:52:16.038968 139594799015680 logging_writer.py:48] [45] global_step=45, grad_norm=12.134970, loss=12.785947
I0810 09:52:16.042698 139634825914176 submission.py:120] 45) loss = 12.786, grad_norm = 12.135
I0810 09:52:16.974652 139594807408384 logging_writer.py:48] [46] global_step=46, grad_norm=11.372329, loss=12.409893
I0810 09:52:16.978324 139634825914176 submission.py:120] 46) loss = 12.410, grad_norm = 11.372
I0810 09:52:17.893059 139594799015680 logging_writer.py:48] [47] global_step=47, grad_norm=11.340239, loss=12.458198
I0810 09:52:17.896485 139634825914176 submission.py:120] 47) loss = 12.458, grad_norm = 11.340
I0810 09:52:18.807253 139594807408384 logging_writer.py:48] [48] global_step=48, grad_norm=11.525941, loss=12.259891
I0810 09:52:18.810955 139634825914176 submission.py:120] 48) loss = 12.260, grad_norm = 11.526
I0810 09:52:19.723285 139594799015680 logging_writer.py:48] [49] global_step=49, grad_norm=10.578276, loss=12.185547
I0810 09:52:19.726868 139634825914176 submission.py:120] 49) loss = 12.186, grad_norm = 10.578
I0810 09:52:20.635561 139594807408384 logging_writer.py:48] [50] global_step=50, grad_norm=10.028083, loss=11.824182
I0810 09:52:20.639157 139634825914176 submission.py:120] 50) loss = 11.824, grad_norm = 10.028
I0810 09:52:21.551551 139594799015680 logging_writer.py:48] [51] global_step=51, grad_norm=9.907616, loss=11.380877
I0810 09:52:21.555165 139634825914176 submission.py:120] 51) loss = 11.381, grad_norm = 9.908
I0810 09:52:22.466683 139594807408384 logging_writer.py:48] [52] global_step=52, grad_norm=9.677318, loss=11.098013
I0810 09:52:22.470107 139634825914176 submission.py:120] 52) loss = 11.098, grad_norm = 9.677
I0810 09:52:23.384316 139594799015680 logging_writer.py:48] [53] global_step=53, grad_norm=9.577116, loss=10.972495
I0810 09:52:23.387597 139634825914176 submission.py:120] 53) loss = 10.972, grad_norm = 9.577
I0810 09:52:24.298888 139594807408384 logging_writer.py:48] [54] global_step=54, grad_norm=10.777787, loss=11.066886
I0810 09:52:24.302469 139634825914176 submission.py:120] 54) loss = 11.067, grad_norm = 10.778
I0810 09:52:25.211048 139594799015680 logging_writer.py:48] [55] global_step=55, grad_norm=11.414848, loss=10.896830
I0810 09:52:25.214642 139634825914176 submission.py:120] 55) loss = 10.897, grad_norm = 11.415
I0810 09:52:26.131900 139594807408384 logging_writer.py:48] [56] global_step=56, grad_norm=11.742988, loss=10.620949
I0810 09:52:26.135452 139634825914176 submission.py:120] 56) loss = 10.621, grad_norm = 11.743
I0810 09:52:27.051708 139594799015680 logging_writer.py:48] [57] global_step=57, grad_norm=12.732989, loss=10.459970
I0810 09:52:27.055142 139634825914176 submission.py:120] 57) loss = 10.460, grad_norm = 12.733
I0810 09:52:27.979333 139594807408384 logging_writer.py:48] [58] global_step=58, grad_norm=12.676377, loss=10.005512
I0810 09:52:27.982997 139634825914176 submission.py:120] 58) loss = 10.006, grad_norm = 12.676
I0810 09:52:28.899131 139594799015680 logging_writer.py:48] [59] global_step=59, grad_norm=13.021965, loss=9.729177
I0810 09:52:28.902734 139634825914176 submission.py:120] 59) loss = 9.729, grad_norm = 13.022
I0810 09:52:29.812290 139594807408384 logging_writer.py:48] [60] global_step=60, grad_norm=13.630677, loss=9.513724
I0810 09:52:29.815775 139634825914176 submission.py:120] 60) loss = 9.514, grad_norm = 13.631
I0810 09:52:30.735734 139594799015680 logging_writer.py:48] [61] global_step=61, grad_norm=11.445613, loss=8.966091
I0810 09:52:30.739261 139634825914176 submission.py:120] 61) loss = 8.966, grad_norm = 11.446
I0810 09:52:31.658827 139594807408384 logging_writer.py:48] [62] global_step=62, grad_norm=11.066664, loss=8.832838
I0810 09:52:31.662897 139634825914176 submission.py:120] 62) loss = 8.833, grad_norm = 11.067
I0810 09:52:32.577336 139594799015680 logging_writer.py:48] [63] global_step=63, grad_norm=10.147313, loss=8.677792
I0810 09:52:32.580784 139634825914176 submission.py:120] 63) loss = 8.678, grad_norm = 10.147
I0810 09:52:33.493559 139594807408384 logging_writer.py:48] [64] global_step=64, grad_norm=8.459492, loss=8.326504
I0810 09:52:33.497017 139634825914176 submission.py:120] 64) loss = 8.327, grad_norm = 8.459
I0810 09:52:34.417526 139594799015680 logging_writer.py:48] [65] global_step=65, grad_norm=7.173119, loss=8.241693
I0810 09:52:34.421372 139634825914176 submission.py:120] 65) loss = 8.242, grad_norm = 7.173
I0810 09:52:35.333155 139594807408384 logging_writer.py:48] [66] global_step=66, grad_norm=7.088989, loss=8.144542
I0810 09:52:35.336662 139634825914176 submission.py:120] 66) loss = 8.145, grad_norm = 7.089
I0810 09:52:36.252229 139594799015680 logging_writer.py:48] [67] global_step=67, grad_norm=5.824441, loss=7.916564
I0810 09:52:36.255809 139634825914176 submission.py:120] 67) loss = 7.917, grad_norm = 5.824
I0810 09:52:37.171051 139594807408384 logging_writer.py:48] [68] global_step=68, grad_norm=5.327943, loss=7.812142
I0810 09:52:37.174701 139634825914176 submission.py:120] 68) loss = 7.812, grad_norm = 5.328
I0810 09:52:38.090909 139594799015680 logging_writer.py:48] [69] global_step=69, grad_norm=4.906097, loss=7.846843
I0810 09:52:38.094572 139634825914176 submission.py:120] 69) loss = 7.847, grad_norm = 4.906
I0810 09:52:39.017950 139594807408384 logging_writer.py:48] [70] global_step=70, grad_norm=4.415500, loss=7.651291
I0810 09:52:39.021840 139634825914176 submission.py:120] 70) loss = 7.651, grad_norm = 4.416
I0810 09:52:39.930862 139594799015680 logging_writer.py:48] [71] global_step=71, grad_norm=4.506819, loss=7.730789
I0810 09:52:39.934476 139634825914176 submission.py:120] 71) loss = 7.731, grad_norm = 4.507
I0810 09:52:40.844355 139594807408384 logging_writer.py:48] [72] global_step=72, grad_norm=3.901021, loss=7.549142
I0810 09:52:40.847987 139634825914176 submission.py:120] 72) loss = 7.549, grad_norm = 3.901
I0810 09:52:41.762925 139594799015680 logging_writer.py:48] [73] global_step=73, grad_norm=3.862626, loss=7.536564
I0810 09:52:41.766416 139634825914176 submission.py:120] 73) loss = 7.537, grad_norm = 3.863
I0810 09:52:42.683762 139594807408384 logging_writer.py:48] [74] global_step=74, grad_norm=3.834996, loss=7.522450
I0810 09:52:42.687242 139634825914176 submission.py:120] 74) loss = 7.522, grad_norm = 3.835
I0810 09:52:43.606425 139594799015680 logging_writer.py:48] [75] global_step=75, grad_norm=3.848314, loss=7.477463
I0810 09:52:43.609787 139634825914176 submission.py:120] 75) loss = 7.477, grad_norm = 3.848
I0810 09:52:44.536336 139594807408384 logging_writer.py:48] [76] global_step=76, grad_norm=3.553583, loss=7.349237
I0810 09:52:44.539858 139634825914176 submission.py:120] 76) loss = 7.349, grad_norm = 3.554
I0810 09:52:45.456015 139594799015680 logging_writer.py:48] [77] global_step=77, grad_norm=3.422935, loss=7.315584
I0810 09:52:45.459552 139634825914176 submission.py:120] 77) loss = 7.316, grad_norm = 3.423
I0810 09:52:46.373807 139594807408384 logging_writer.py:48] [78] global_step=78, grad_norm=3.256057, loss=7.324139
I0810 09:52:46.377217 139634825914176 submission.py:120] 78) loss = 7.324, grad_norm = 3.256
I0810 09:52:47.292823 139594799015680 logging_writer.py:48] [79] global_step=79, grad_norm=3.072721, loss=7.257571
I0810 09:52:47.296416 139634825914176 submission.py:120] 79) loss = 7.258, grad_norm = 3.073
I0810 09:52:48.210190 139594807408384 logging_writer.py:48] [80] global_step=80, grad_norm=3.261989, loss=7.244963
I0810 09:52:48.213717 139634825914176 submission.py:120] 80) loss = 7.245, grad_norm = 3.262
I0810 09:52:49.128634 139594799015680 logging_writer.py:48] [81] global_step=81, grad_norm=2.823728, loss=7.184811
I0810 09:52:49.132145 139634825914176 submission.py:120] 81) loss = 7.185, grad_norm = 2.824
I0810 09:52:50.061748 139594807408384 logging_writer.py:48] [82] global_step=82, grad_norm=2.752965, loss=7.147913
I0810 09:52:50.065438 139634825914176 submission.py:120] 82) loss = 7.148, grad_norm = 2.753
I0810 09:52:50.975366 139594799015680 logging_writer.py:48] [83] global_step=83, grad_norm=2.660252, loss=7.175880
I0810 09:52:50.978999 139634825914176 submission.py:120] 83) loss = 7.176, grad_norm = 2.660
I0810 09:52:51.894770 139594807408384 logging_writer.py:48] [84] global_step=84, grad_norm=2.567821, loss=7.138781
I0810 09:52:51.898350 139634825914176 submission.py:120] 84) loss = 7.139, grad_norm = 2.568
I0810 09:52:52.816445 139594799015680 logging_writer.py:48] [85] global_step=85, grad_norm=2.517496, loss=7.074972
I0810 09:52:52.819948 139634825914176 submission.py:120] 85) loss = 7.075, grad_norm = 2.517
I0810 09:52:53.739395 139594807408384 logging_writer.py:48] [86] global_step=86, grad_norm=2.531996, loss=6.967264
I0810 09:52:53.743191 139634825914176 submission.py:120] 86) loss = 6.967, grad_norm = 2.532
I0810 09:52:54.650196 139594799015680 logging_writer.py:48] [87] global_step=87, grad_norm=2.292895, loss=7.004599
I0810 09:52:54.653883 139634825914176 submission.py:120] 87) loss = 7.005, grad_norm = 2.293
I0810 09:52:55.597217 139594807408384 logging_writer.py:48] [88] global_step=88, grad_norm=2.492165, loss=6.964730
I0810 09:52:55.600646 139634825914176 submission.py:120] 88) loss = 6.965, grad_norm = 2.492
I0810 09:52:56.516036 139594799015680 logging_writer.py:48] [89] global_step=89, grad_norm=2.315488, loss=7.021801
I0810 09:52:56.519864 139634825914176 submission.py:120] 89) loss = 7.022, grad_norm = 2.315
I0810 09:52:57.433405 139594807408384 logging_writer.py:48] [90] global_step=90, grad_norm=2.144905, loss=6.871937
I0810 09:52:57.436997 139634825914176 submission.py:120] 90) loss = 6.872, grad_norm = 2.145
I0810 09:52:58.350025 139594799015680 logging_writer.py:48] [91] global_step=91, grad_norm=2.106122, loss=6.912326
I0810 09:52:58.353520 139634825914176 submission.py:120] 91) loss = 6.912, grad_norm = 2.106
I0810 09:52:59.273390 139594807408384 logging_writer.py:48] [92] global_step=92, grad_norm=1.989268, loss=6.907944
I0810 09:52:59.276917 139634825914176 submission.py:120] 92) loss = 6.908, grad_norm = 1.989
I0810 09:53:00.193660 139594799015680 logging_writer.py:48] [93] global_step=93, grad_norm=2.040896, loss=6.834790
I0810 09:53:00.197318 139634825914176 submission.py:120] 93) loss = 6.835, grad_norm = 2.041
I0810 09:53:01.126455 139594807408384 logging_writer.py:48] [94] global_step=94, grad_norm=1.738324, loss=6.835196
I0810 09:53:01.130198 139634825914176 submission.py:120] 94) loss = 6.835, grad_norm = 1.738
I0810 09:53:02.083250 139594799015680 logging_writer.py:48] [95] global_step=95, grad_norm=1.793845, loss=6.796051
I0810 09:53:02.086830 139634825914176 submission.py:120] 95) loss = 6.796, grad_norm = 1.794
I0810 09:53:03.004371 139594807408384 logging_writer.py:48] [96] global_step=96, grad_norm=1.940754, loss=6.858750
I0810 09:53:03.007960 139634825914176 submission.py:120] 96) loss = 6.859, grad_norm = 1.941
I0810 09:53:03.925628 139594799015680 logging_writer.py:48] [97] global_step=97, grad_norm=1.828384, loss=6.787127
I0810 09:53:03.929176 139634825914176 submission.py:120] 97) loss = 6.787, grad_norm = 1.828
I0810 09:53:04.848423 139594807408384 logging_writer.py:48] [98] global_step=98, grad_norm=1.682252, loss=6.772821
I0810 09:53:04.852135 139634825914176 submission.py:120] 98) loss = 6.773, grad_norm = 1.682
I0810 09:53:05.766916 139594799015680 logging_writer.py:48] [99] global_step=99, grad_norm=1.740679, loss=6.786073
I0810 09:53:05.770449 139634825914176 submission.py:120] 99) loss = 6.786, grad_norm = 1.741
I0810 09:53:06.696090 139594807408384 logging_writer.py:48] [100] global_step=100, grad_norm=1.623451, loss=6.755220
I0810 09:53:06.699618 139634825914176 submission.py:120] 100) loss = 6.755, grad_norm = 1.623
I0810 09:59:11.018516 139594799015680 logging_writer.py:48] [500] global_step=500, grad_norm=1.629540, loss=5.660281
I0810 09:59:11.022963 139634825914176 submission.py:120] 500) loss = 5.660, grad_norm = 1.630
I0810 10:06:45.830623 139594807408384 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.479033, loss=3.957960
I0810 10:06:45.835789 139634825914176 submission.py:120] 1000) loss = 3.958, grad_norm = 1.479
I0810 10:14:20.476521 139594807408384 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.070258, loss=3.101634
I0810 10:14:20.484728 139634825914176 submission.py:120] 1500) loss = 3.102, grad_norm = 3.070
I0810 10:15:34.729936 139634825914176 spec.py:320] Evaluating on the training split.
I0810 10:15:46.309035 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 10:15:56.622617 139634825914176 spec.py:348] Evaluating on the test split.
I0810 10:16:02.315400 139634825914176 submission_runner.py:362] Time since start: 1549.32s, 	Step: 1583, 	{'train/ctc_loss': 6.4248223394662105, 'train/wer': 0.9066233878272872, 'validation/ctc_loss': 6.261542548849708, 'validation/wer': 0.8704388548254719, 'validation/num_examples': 5348, 'test/ctc_loss': 6.19902413058907, 'test/wer': 0.8679544208153068, 'test/num_examples': 2472, 'score': 1473.648729801178, 'total_duration': 1549.3158571720123, 'accumulated_submission_time': 1473.648729801178, 'accumulated_eval_time': 74.73293566703796, 'accumulated_logging_time': 0.07440948486328125}
I0810 10:16:02.347256 139594807408384 logging_writer.py:48] [1583] accumulated_eval_time=74.732936, accumulated_logging_time=0.074409, accumulated_submission_time=1473.648730, global_step=1583, preemption_count=0, score=1473.648730, test/ctc_loss=6.199024, test/num_examples=2472, test/wer=0.867954, total_duration=1549.315857, train/ctc_loss=6.424822, train/wer=0.906623, validation/ctc_loss=6.261543, validation/num_examples=5348, validation/wer=0.870439
I0810 10:22:20.993260 139594799015680 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.153950, loss=2.610810
I0810 10:22:20.998329 139634825914176 submission.py:120] 2000) loss = 2.611, grad_norm = 2.154
I0810 10:29:54.360568 139594807408384 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.742881, loss=2.401871
I0810 10:29:54.369535 139634825914176 submission.py:120] 2500) loss = 2.402, grad_norm = 1.743
I0810 10:37:25.544495 139594799015680 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.936215, loss=2.281163
I0810 10:37:25.553849 139634825914176 submission.py:120] 3000) loss = 2.281, grad_norm = 1.936
I0810 10:40:03.197526 139634825914176 spec.py:320] Evaluating on the training split.
I0810 10:40:16.314139 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 10:40:27.135350 139634825914176 spec.py:348] Evaluating on the test split.
I0810 10:40:33.439801 139634825914176 submission_runner.py:362] Time since start: 3020.44s, 	Step: 3174, 	{'train/ctc_loss': 1.4094827151105667, 'train/wer': 0.39165013156192036, 'validation/ctc_loss': 1.6367250288828612, 'validation/wer': 0.41413604982378216, 'validation/num_examples': 5348, 'test/ctc_loss': 1.1915786905606813, 'test/wer': 0.3380253082282209, 'test/num_examples': 2472, 'score': 2913.431603908539, 'total_duration': 3020.440285682678, 'accumulated_submission_time': 2913.431603908539, 'accumulated_eval_time': 104.97510528564453, 'accumulated_logging_time': 0.22552490234375}
I0810 10:40:33.472089 139594807408384 logging_writer.py:48] [3174] accumulated_eval_time=104.975105, accumulated_logging_time=0.225525, accumulated_submission_time=2913.431604, global_step=3174, preemption_count=0, score=2913.431604, test/ctc_loss=1.191579, test/num_examples=2472, test/wer=0.338025, total_duration=3020.440286, train/ctc_loss=1.409483, train/wer=0.391650, validation/ctc_loss=1.636725, validation/num_examples=5348, validation/wer=0.414136
I0810 10:45:28.121970 139594799015680 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.533448, loss=2.176176
I0810 10:45:28.126583 139634825914176 submission.py:120] 3500) loss = 2.176, grad_norm = 1.533
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0810 10:52:58.607899 139594807408384 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.886335, loss=2.083979
I0810 10:52:58.615539 139634825914176 submission.py:120] 4000) loss = 2.084, grad_norm = 1.886
I0810 11:00:30.980345 139594807408384 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.218904, loss=1.976701
I0810 11:00:30.989427 139634825914176 submission.py:120] 4500) loss = 1.977, grad_norm = 2.219
I0810 11:04:34.007005 139634825914176 spec.py:320] Evaluating on the training split.
I0810 11:04:47.327966 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 11:04:58.253084 139634825914176 spec.py:348] Evaluating on the test split.
I0810 11:05:04.164448 139634825914176 submission_runner.py:362] Time since start: 4491.16s, 	Step: 4771, 	{'train/ctc_loss': 0.8398431191032181, 'train/wer': 0.26930832937928656, 'validation/ctc_loss': 1.0983879360684148, 'validation/wer': 0.30610727562400425, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7349448956384282, 'test/wer': 0.23658927954827047, 'test/num_examples': 2472, 'score': 4352.862879753113, 'total_duration': 4491.164002895355, 'accumulated_submission_time': 4352.862879753113, 'accumulated_eval_time': 135.13134121894836, 'accumulated_logging_time': 0.386624813079834}
I0810 11:05:04.199465 139594807408384 logging_writer.py:48] [4771] accumulated_eval_time=135.131341, accumulated_logging_time=0.386625, accumulated_submission_time=4352.862880, global_step=4771, preemption_count=0, score=4352.862880, test/ctc_loss=0.734945, test/num_examples=2472, test/wer=0.236589, total_duration=4491.164003, train/ctc_loss=0.839843, train/wer=0.269308, validation/ctc_loss=1.098388, validation/num_examples=5348, validation/wer=0.306107
I0810 11:08:31.796322 139594799015680 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.276306, loss=1.878752
I0810 11:08:31.802366 139634825914176 submission.py:120] 5000) loss = 1.879, grad_norm = 3.276
I0810 11:16:03.170155 139594807408384 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.105733, loss=1.912552
I0810 11:16:03.177546 139634825914176 submission.py:120] 5500) loss = 1.913, grad_norm = 3.106
I0810 11:23:33.436761 139594799015680 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.212191, loss=1.833702
I0810 11:23:33.441582 139634825914176 submission.py:120] 6000) loss = 1.834, grad_norm = 2.212
I0810 11:29:04.355273 139634825914176 spec.py:320] Evaluating on the training split.
I0810 11:29:17.836125 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 11:29:29.817759 139634825914176 spec.py:348] Evaluating on the test split.
I0810 11:29:35.805657 139634825914176 submission_runner.py:362] Time since start: 5962.81s, 	Step: 6366, 	{'train/ctc_loss': 0.6194317050027591, 'train/wer': 0.20320924815597635, 'validation/ctc_loss': 0.8687885170220514, 'validation/wer': 0.24570076763385312, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5498686407187561, 'test/wer': 0.17756382913899213, 'test/num_examples': 2472, 'score': 5791.97611618042, 'total_duration': 5962.806046962738, 'accumulated_submission_time': 5791.97611618042, 'accumulated_eval_time': 166.5814607143402, 'accumulated_logging_time': 0.5439853668212891}
I0810 11:29:35.841459 139594807408384 logging_writer.py:48] [6366] accumulated_eval_time=166.581461, accumulated_logging_time=0.543985, accumulated_submission_time=5791.976116, global_step=6366, preemption_count=0, score=5791.976116, test/ctc_loss=0.549869, test/num_examples=2472, test/wer=0.177564, total_duration=5962.806047, train/ctc_loss=0.619432, train/wer=0.203209, validation/ctc_loss=0.868789, validation/num_examples=5348, validation/wer=0.245701
I0810 11:31:38.025566 139594799015680 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.475404, loss=1.799594
I0810 11:31:38.031093 139634825914176 submission.py:120] 6500) loss = 1.800, grad_norm = 2.475
I0810 11:39:09.057755 139594807408384 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.178905, loss=1.770632
I0810 11:39:09.063123 139634825914176 submission.py:120] 7000) loss = 1.771, grad_norm = 2.179
I0810 11:46:41.129565 139594807408384 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.403306, loss=1.712347
I0810 11:46:41.137587 139634825914176 submission.py:120] 7500) loss = 1.712, grad_norm = 1.403
I0810 11:53:35.905917 139634825914176 spec.py:320] Evaluating on the training split.
I0810 11:53:49.076988 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 11:54:00.083302 139634825914176 spec.py:348] Evaluating on the test split.
I0810 11:54:06.237986 139634825914176 submission_runner.py:362] Time since start: 7433.24s, 	Step: 7961, 	{'train/ctc_loss': 0.5490128463202735, 'train/wer': 0.1813829098908683, 'validation/ctc_loss': 0.7996156342613522, 'validation/wer': 0.2270940954955825, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4918246237660494, 'test/wer': 0.16133487701338534, 'test/num_examples': 2472, 'score': 7230.923235654831, 'total_duration': 7433.238354921341, 'accumulated_submission_time': 7230.923235654831, 'accumulated_eval_time': 196.91309905052185, 'accumulated_logging_time': 0.7057557106018066}
I0810 11:54:06.270357 139594807408384 logging_writer.py:48] [7961] accumulated_eval_time=196.913099, accumulated_logging_time=0.705756, accumulated_submission_time=7230.923236, global_step=7961, preemption_count=0, score=7230.923236, test/ctc_loss=0.491825, test/num_examples=2472, test/wer=0.161335, total_duration=7433.238355, train/ctc_loss=0.549013, train/wer=0.181383, validation/ctc_loss=0.799616, validation/num_examples=5348, validation/wer=0.227094
I0810 11:54:41.905686 139634825914176 spec.py:320] Evaluating on the training split.
I0810 11:54:54.476333 139634825914176 spec.py:332] Evaluating on the validation split.
I0810 11:55:04.979604 139634825914176 spec.py:348] Evaluating on the test split.
I0810 11:55:10.446029 139634825914176 submission_runner.py:362] Time since start: 7497.45s, 	Step: 8000, 	{'train/ctc_loss': 0.5465567125735205, 'train/wer': 0.1831999741189665, 'validation/ctc_loss': 0.8072935797637885, 'validation/wer': 0.23216337565779946, 'validation/num_examples': 5348, 'test/ctc_loss': 0.49123587610491, 'test/wer': 0.15952714642617757, 'test/num_examples': 2472, 'score': 7266.4074149131775, 'total_duration': 7497.446504831314, 'accumulated_submission_time': 7266.4074149131775, 'accumulated_eval_time': 225.4530861377716, 'accumulated_logging_time': 0.8664255142211914}
I0810 11:55:10.469163 139594807408384 logging_writer.py:48] [8000] accumulated_eval_time=225.453086, accumulated_logging_time=0.866426, accumulated_submission_time=7266.407415, global_step=8000, preemption_count=0, score=7266.407415, test/ctc_loss=0.491236, test/num_examples=2472, test/wer=0.159527, total_duration=7497.446505, train/ctc_loss=0.546557, train/wer=0.183200, validation/ctc_loss=0.807294, validation/num_examples=5348, validation/wer=0.232163
I0810 11:55:10.613579 139594799015680 logging_writer.py:48] [8000] global_step=8000, preemption_count=0, score=7266.407415
I0810 11:55:11.027410 139634825914176 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/librispeech_deepspeech_pytorch/trial_1/checkpoint_8000.
I0810 11:55:11.138259 139634825914176 submission_runner.py:528] Tuning trial 1/1
I0810 11:55:11.138545 139634825914176 submission_runner.py:529] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0810 11:55:11.139162 139634825914176 submission_runner.py:530] Metrics: {'eval_results': [(1, {'train/ctc_loss': 30.675214336666713, 'train/wer': 4.270925893974033, 'validation/ctc_loss': 29.309482996785214, 'validation/wer': 3.935016656206247, 'validation/num_examples': 5348, 'test/ctc_loss': 29.5897509516743, 'test/wer': 4.1791278207706215, 'test/num_examples': 2472, 'score': 34.43839430809021, 'total_duration': 81.58680367469788, 'accumulated_submission_time': 34.43839430809021, 'accumulated_eval_time': 47.1478054523468, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1583, {'train/ctc_loss': 6.4248223394662105, 'train/wer': 0.9066233878272872, 'validation/ctc_loss': 6.261542548849708, 'validation/wer': 0.8704388548254719, 'validation/num_examples': 5348, 'test/ctc_loss': 6.19902413058907, 'test/wer': 0.8679544208153068, 'test/num_examples': 2472, 'score': 1473.648729801178, 'total_duration': 1549.3158571720123, 'accumulated_submission_time': 1473.648729801178, 'accumulated_eval_time': 74.73293566703796, 'accumulated_logging_time': 0.07440948486328125, 'global_step': 1583, 'preemption_count': 0}), (3174, {'train/ctc_loss': 1.4094827151105667, 'train/wer': 0.39165013156192036, 'validation/ctc_loss': 1.6367250288828612, 'validation/wer': 0.41413604982378216, 'validation/num_examples': 5348, 'test/ctc_loss': 1.1915786905606813, 'test/wer': 0.3380253082282209, 'test/num_examples': 2472, 'score': 2913.431603908539, 'total_duration': 3020.440285682678, 'accumulated_submission_time': 2913.431603908539, 'accumulated_eval_time': 104.97510528564453, 'accumulated_logging_time': 0.22552490234375, 'global_step': 3174, 'preemption_count': 0}), (4771, {'train/ctc_loss': 0.8398431191032181, 'train/wer': 0.26930832937928656, 'validation/ctc_loss': 1.0983879360684148, 'validation/wer': 0.30610727562400425, 'validation/num_examples': 5348, 'test/ctc_loss': 0.7349448956384282, 'test/wer': 0.23658927954827047, 'test/num_examples': 2472, 'score': 4352.862879753113, 'total_duration': 4491.164002895355, 'accumulated_submission_time': 4352.862879753113, 'accumulated_eval_time': 135.13134121894836, 'accumulated_logging_time': 0.386624813079834, 'global_step': 4771, 'preemption_count': 0}), (6366, {'train/ctc_loss': 0.6194317050027591, 'train/wer': 0.20320924815597635, 'validation/ctc_loss': 0.8687885170220514, 'validation/wer': 0.24570076763385312, 'validation/num_examples': 5348, 'test/ctc_loss': 0.5498686407187561, 'test/wer': 0.17756382913899213, 'test/num_examples': 2472, 'score': 5791.97611618042, 'total_duration': 5962.806046962738, 'accumulated_submission_time': 5791.97611618042, 'accumulated_eval_time': 166.5814607143402, 'accumulated_logging_time': 0.5439853668212891, 'global_step': 6366, 'preemption_count': 0}), (7961, {'train/ctc_loss': 0.5490128463202735, 'train/wer': 0.1813829098908683, 'validation/ctc_loss': 0.7996156342613522, 'validation/wer': 0.2270940954955825, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4918246237660494, 'test/wer': 0.16133487701338534, 'test/num_examples': 2472, 'score': 7230.923235654831, 'total_duration': 7433.238354921341, 'accumulated_submission_time': 7230.923235654831, 'accumulated_eval_time': 196.91309905052185, 'accumulated_logging_time': 0.7057557106018066, 'global_step': 7961, 'preemption_count': 0}), (8000, {'train/ctc_loss': 0.5465567125735205, 'train/wer': 0.1831999741189665, 'validation/ctc_loss': 0.8072935797637885, 'validation/wer': 0.23216337565779946, 'validation/num_examples': 5348, 'test/ctc_loss': 0.49123587610491, 'test/wer': 0.15952714642617757, 'test/num_examples': 2472, 'score': 7266.4074149131775, 'total_duration': 7497.446504831314, 'accumulated_submission_time': 7266.4074149131775, 'accumulated_eval_time': 225.4530861377716, 'accumulated_logging_time': 0.8664255142211914, 'global_step': 8000, 'preemption_count': 0})], 'global_step': 8000}
I0810 11:55:11.139266 139634825914176 submission_runner.py:531] Timing: 7266.4074149131775
I0810 11:55:11.139318 139634825914176 submission_runner.py:533] Total number of evals: 7
I0810 11:55:11.139384 139634825914176 submission_runner.py:534] ====================
I0810 11:55:11.139580 139634825914176 submission_runner.py:602] Final librispeech_deepspeech score: 7266.4074149131775
