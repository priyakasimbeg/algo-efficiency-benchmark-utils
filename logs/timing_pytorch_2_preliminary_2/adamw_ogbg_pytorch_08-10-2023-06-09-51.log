torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=ogbg --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_2/adamw --overwrite=true --save_checkpoints=false --max_global_steps=6000 --torch_compile=True 2>&1 | tee -a /logs/ogbg_pytorch_08-10-2023-06-09-51.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-10 06:10:01.279910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 06:10:01.279936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0810 06:10:16.293398 140501243746112 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0810 06:10:16.293432 140656512235328 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0810 06:10:16.293481 140434983634752 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0810 06:10:17.281474 139962410080064 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0810 06:10:17.281683 140348342982464 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0810 06:10:17.282022 139761169434432 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0810 06:10:17.282063 139691564787520 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0810 06:10:17.282373 139691564787520 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.282401 139761169434432 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.282268 139966470805312 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0810 06:10:17.282695 139966470805312 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.286018 140501243746112 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.286045 140656512235328 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.292246 139962410080064 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.292301 140348342982464 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 06:10:17.292293 140434983634752 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
W0810 06:10:19.293615 139761169434432 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 06:10:19.294239 140434983634752 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 06:10:19.294629 139691564787520 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 06:10:19.294754 140501243746112 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 06:10:19.295160 140348342982464 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 06:10:19.295473 139962410080064 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 06:10:19.303081 139966470805312 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/ogbg_pytorch.
W0810 06:10:19.328138 140656512235328 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 06:10:19.330095 139966470805312 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 06:10:19.334901 139966470805312 submission_runner.py:488] Using RNG seed 4281053998
I0810 06:10:19.336370 139966470805312 submission_runner.py:497] --- Tuning run 1/1 ---
I0810 06:10:19.336506 139966470805312 submission_runner.py:502] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/ogbg_pytorch/trial_1.
I0810 06:10:19.336941 139966470805312 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/ogbg_pytorch/trial_1/hparams.json.
I0810 06:10:19.337921 139966470805312 submission_runner.py:176] Initializing dataset.
I0810 06:10:19.338051 139966470805312 submission_runner.py:183] Initializing model.
W0810 06:10:23.552537 139962410080064 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552536 139761169434432 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552524 140501243746112 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552517 139966470805312 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552547 140434983634752 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552535 139691564787520 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552541 140348342982464 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 06:10:23.552586 140656512235328 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0810 06:10:23.552731 139966470805312 submission_runner.py:215] Initializing optimizer.
I0810 06:10:23.553495 139966470805312 submission_runner.py:222] Initializing metrics bundle.
I0810 06:10:23.553605 139966470805312 submission_runner.py:240] Initializing checkpoint and logger.
I0810 06:10:23.554400 139966470805312 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0810 06:10:23.554518 139966470805312 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0810 06:10:24.101458 139966470805312 submission_runner.py:261] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/ogbg_pytorch/trial_1/meta_data_0.json.
I0810 06:10:24.102308 139966470805312 submission_runner.py:264] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/ogbg_pytorch/trial_1/flags_0.json.
I0810 06:10:24.196260 139966470805312 submission_runner.py:274] Starting training loop.
I0810 06:10:24.729587 139966470805312 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:10:24.735351 139966470805312 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0810 06:10:24.837922 139966470805312 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0810 06:10:24.900290 139966470805312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:10:29.656642 139928113506048 logging_writer.py:48] [0] global_step=0, grad_norm=3.087746, loss=0.769894
I0810 06:10:29.670260 139966470805312 submission.py:120] 0) loss = 0.770, grad_norm = 3.088
I0810 06:10:29.671288 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:10:29.675940 139966470805312 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:10:29.680549 139966470805312 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0810 06:10:29.748096 139966470805312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:11:25.868870 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:11:25.872300 139966470805312 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:11:25.877074 139966470805312 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0810 06:11:25.944042 139966470805312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:12:10.518735 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:12:10.522066 139966470805312 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:12:10.526742 139966470805312 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0810 06:12:10.592333 139966470805312 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0810 06:12:57.031515 139966470805312 submission_runner.py:362] Time since start: 152.84s, 	Step: 1, 	{'train/accuracy': 0.5079313337109175, 'train/loss': 0.7704198915917613, 'train/mean_average_precision': 0.019777809757215506, 'validation/accuracy': 0.5008313665506489, 'validation/loss': 0.7763156132099598, 'validation/mean_average_precision': 0.02383655911985128, 'validation/num_examples': 43793, 'test/accuracy': 0.49733973436935996, 'test/loss': 0.7795949286475143, 'test/mean_average_precision': 0.025727453796939883, 'test/num_examples': 43793, 'score': 5.4752020835876465, 'total_duration': 152.83542776107788, 'accumulated_submission_time': 5.4752020835876465, 'accumulated_eval_time': 147.35985565185547, 'accumulated_logging_time': 0}
I0810 06:12:57.052849 139913760143104 logging_writer.py:48] [1] accumulated_eval_time=147.359856, accumulated_logging_time=0, accumulated_submission_time=5.475202, global_step=1, preemption_count=0, score=5.475202, test/accuracy=0.497340, test/loss=0.779595, test/mean_average_precision=0.025727, test/num_examples=43793, total_duration=152.835428, train/accuracy=0.507931, train/loss=0.770420, train/mean_average_precision=0.019778, validation/accuracy=0.500831, validation/loss=0.776316, validation/mean_average_precision=0.023837, validation/num_examples=43793
I0810 06:12:57.479364 139966470805312 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.485462 139761169434432 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.486624 140501243746112 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.486628 139691564787520 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.486639 140434983634752 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.486667 140656512235328 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.486671 140348342982464 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.486676 139962410080064 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 06:12:57.555104 139913768535808 logging_writer.py:48] [1] global_step=1, grad_norm=3.077062, loss=0.769936
I0810 06:12:57.560371 139966470805312 submission.py:120] 1) loss = 0.770, grad_norm = 3.077
I0810 06:12:57.905399 139913760143104 logging_writer.py:48] [2] global_step=2, grad_norm=3.064533, loss=0.769221
I0810 06:12:57.911192 139966470805312 submission.py:120] 2) loss = 0.769, grad_norm = 3.065
I0810 06:12:58.263935 139913768535808 logging_writer.py:48] [3] global_step=3, grad_norm=3.063216, loss=0.767117
I0810 06:12:58.269437 139966470805312 submission.py:120] 3) loss = 0.767, grad_norm = 3.063
I0810 06:12:58.611200 139913760143104 logging_writer.py:48] [4] global_step=4, grad_norm=3.020812, loss=0.762247
I0810 06:12:58.616487 139966470805312 submission.py:120] 4) loss = 0.762, grad_norm = 3.021
I0810 06:12:58.955090 139913768535808 logging_writer.py:48] [5] global_step=5, grad_norm=2.997627, loss=0.760130
I0810 06:12:58.960502 139966470805312 submission.py:120] 5) loss = 0.760, grad_norm = 2.998
I0810 06:12:59.292128 139913760143104 logging_writer.py:48] [6] global_step=6, grad_norm=2.970193, loss=0.758003
I0810 06:12:59.296997 139966470805312 submission.py:120] 6) loss = 0.758, grad_norm = 2.970
I0810 06:12:59.637207 139913768535808 logging_writer.py:48] [7] global_step=7, grad_norm=2.934719, loss=0.752584
I0810 06:12:59.642290 139966470805312 submission.py:120] 7) loss = 0.753, grad_norm = 2.935
I0810 06:12:59.985567 139913760143104 logging_writer.py:48] [8] global_step=8, grad_norm=2.886791, loss=0.744796
I0810 06:12:59.990595 139966470805312 submission.py:120] 8) loss = 0.745, grad_norm = 2.887
I0810 06:13:00.325636 139913768535808 logging_writer.py:48] [9] global_step=9, grad_norm=2.844228, loss=0.738063
I0810 06:13:00.330684 139966470805312 submission.py:120] 9) loss = 0.738, grad_norm = 2.844
I0810 06:13:00.670014 139913760143104 logging_writer.py:48] [10] global_step=10, grad_norm=2.759409, loss=0.731785
I0810 06:13:00.674957 139966470805312 submission.py:120] 10) loss = 0.732, grad_norm = 2.759
I0810 06:13:01.018615 139913768535808 logging_writer.py:48] [11] global_step=11, grad_norm=2.692316, loss=0.724694
I0810 06:13:01.023527 139966470805312 submission.py:120] 11) loss = 0.725, grad_norm = 2.692
I0810 06:13:01.355731 139913760143104 logging_writer.py:48] [12] global_step=12, grad_norm=2.611318, loss=0.716491
I0810 06:13:01.360744 139966470805312 submission.py:120] 12) loss = 0.716, grad_norm = 2.611
I0810 06:13:01.704911 139913768535808 logging_writer.py:48] [13] global_step=13, grad_norm=2.509553, loss=0.707723
I0810 06:13:01.709932 139966470805312 submission.py:120] 13) loss = 0.708, grad_norm = 2.510
I0810 06:13:02.095187 139913760143104 logging_writer.py:48] [14] global_step=14, grad_norm=2.504644, loss=0.700986
I0810 06:13:02.100890 139966470805312 submission.py:120] 14) loss = 0.701, grad_norm = 2.505
I0810 06:13:02.459007 139913768535808 logging_writer.py:48] [15] global_step=15, grad_norm=2.486146, loss=0.690692
I0810 06:13:02.464303 139966470805312 submission.py:120] 15) loss = 0.691, grad_norm = 2.486
I0810 06:13:02.801878 139913760143104 logging_writer.py:48] [16] global_step=16, grad_norm=2.411454, loss=0.679644
I0810 06:13:02.806834 139966470805312 submission.py:120] 16) loss = 0.680, grad_norm = 2.411
I0810 06:13:03.145568 139913768535808 logging_writer.py:48] [17] global_step=17, grad_norm=2.347860, loss=0.669577
I0810 06:13:03.150652 139966470805312 submission.py:120] 17) loss = 0.670, grad_norm = 2.348
I0810 06:13:03.479472 139913760143104 logging_writer.py:48] [18] global_step=18, grad_norm=2.274354, loss=0.659701
I0810 06:13:03.484456 139966470805312 submission.py:120] 18) loss = 0.660, grad_norm = 2.274
I0810 06:13:03.819260 139913768535808 logging_writer.py:48] [19] global_step=19, grad_norm=2.213277, loss=0.648544
I0810 06:13:03.824384 139966470805312 submission.py:120] 19) loss = 0.649, grad_norm = 2.213
I0810 06:13:04.168757 139913760143104 logging_writer.py:48] [20] global_step=20, grad_norm=2.213719, loss=0.636735
I0810 06:13:04.173743 139966470805312 submission.py:120] 20) loss = 0.637, grad_norm = 2.214
I0810 06:13:04.509818 139913768535808 logging_writer.py:48] [21] global_step=21, grad_norm=2.079241, loss=0.625718
I0810 06:13:04.515080 139966470805312 submission.py:120] 21) loss = 0.626, grad_norm = 2.079
I0810 06:13:04.850569 139913760143104 logging_writer.py:48] [22] global_step=22, grad_norm=1.904787, loss=0.614568
I0810 06:13:04.855782 139966470805312 submission.py:120] 22) loss = 0.615, grad_norm = 1.905
I0810 06:13:05.195961 139913768535808 logging_writer.py:48] [23] global_step=23, grad_norm=1.738196, loss=0.604805
I0810 06:13:05.201258 139966470805312 submission.py:120] 23) loss = 0.605, grad_norm = 1.738
I0810 06:13:05.541648 139913760143104 logging_writer.py:48] [24] global_step=24, grad_norm=1.636834, loss=0.594911
I0810 06:13:05.546521 139966470805312 submission.py:120] 24) loss = 0.595, grad_norm = 1.637
I0810 06:13:05.907024 139913768535808 logging_writer.py:48] [25] global_step=25, grad_norm=1.585267, loss=0.584132
I0810 06:13:05.912141 139966470805312 submission.py:120] 25) loss = 0.584, grad_norm = 1.585
I0810 06:13:06.251375 139913760143104 logging_writer.py:48] [26] global_step=26, grad_norm=1.524324, loss=0.576220
I0810 06:13:06.256146 139966470805312 submission.py:120] 26) loss = 0.576, grad_norm = 1.524
I0810 06:13:06.585255 139913768535808 logging_writer.py:48] [27] global_step=27, grad_norm=1.421640, loss=0.566867
I0810 06:13:06.589842 139966470805312 submission.py:120] 27) loss = 0.567, grad_norm = 1.422
I0810 06:13:06.924978 139913760143104 logging_writer.py:48] [28] global_step=28, grad_norm=1.339378, loss=0.559651
I0810 06:13:06.930033 139966470805312 submission.py:120] 28) loss = 0.560, grad_norm = 1.339
I0810 06:13:07.269063 139913768535808 logging_writer.py:48] [29] global_step=29, grad_norm=1.245663, loss=0.551057
I0810 06:13:07.274118 139966470805312 submission.py:120] 29) loss = 0.551, grad_norm = 1.246
I0810 06:13:07.602189 139913760143104 logging_writer.py:48] [30] global_step=30, grad_norm=1.211635, loss=0.542903
I0810 06:13:07.606913 139966470805312 submission.py:120] 30) loss = 0.543, grad_norm = 1.212
I0810 06:13:07.940597 139913768535808 logging_writer.py:48] [31] global_step=31, grad_norm=1.187557, loss=0.533170
I0810 06:13:07.945230 139966470805312 submission.py:120] 31) loss = 0.533, grad_norm = 1.188
I0810 06:13:08.277798 139913760143104 logging_writer.py:48] [32] global_step=32, grad_norm=1.133864, loss=0.526691
I0810 06:13:08.282506 139966470805312 submission.py:120] 32) loss = 0.527, grad_norm = 1.134
I0810 06:13:08.606950 139913768535808 logging_writer.py:48] [33] global_step=33, grad_norm=1.093672, loss=0.522704
I0810 06:13:08.611718 139966470805312 submission.py:120] 33) loss = 0.523, grad_norm = 1.094
I0810 06:13:08.948076 139913760143104 logging_writer.py:48] [34] global_step=34, grad_norm=1.079254, loss=0.513231
I0810 06:13:08.953088 139966470805312 submission.py:120] 34) loss = 0.513, grad_norm = 1.079
I0810 06:13:09.286070 139913768535808 logging_writer.py:48] [35] global_step=35, grad_norm=1.077115, loss=0.506428
I0810 06:13:09.290984 139966470805312 submission.py:120] 35) loss = 0.506, grad_norm = 1.077
I0810 06:13:09.621387 139913760143104 logging_writer.py:48] [36] global_step=36, grad_norm=1.023494, loss=0.501253
I0810 06:13:09.626027 139966470805312 submission.py:120] 36) loss = 0.501, grad_norm = 1.023
I0810 06:13:09.960994 139913768535808 logging_writer.py:48] [37] global_step=37, grad_norm=0.997715, loss=0.490547
I0810 06:13:09.966066 139966470805312 submission.py:120] 37) loss = 0.491, grad_norm = 0.998
I0810 06:13:10.297116 139913760143104 logging_writer.py:48] [38] global_step=38, grad_norm=0.990237, loss=0.485687
I0810 06:13:10.302116 139966470805312 submission.py:120] 38) loss = 0.486, grad_norm = 0.990
I0810 06:13:10.640957 139913768535808 logging_writer.py:48] [39] global_step=39, grad_norm=0.902568, loss=0.479152
I0810 06:13:10.645874 139966470805312 submission.py:120] 39) loss = 0.479, grad_norm = 0.903
I0810 06:13:10.987705 139913760143104 logging_writer.py:48] [40] global_step=40, grad_norm=0.804358, loss=0.472760
I0810 06:13:10.992484 139966470805312 submission.py:120] 40) loss = 0.473, grad_norm = 0.804
I0810 06:13:11.324809 139913768535808 logging_writer.py:48] [41] global_step=41, grad_norm=0.799336, loss=0.466327
I0810 06:13:11.329555 139966470805312 submission.py:120] 41) loss = 0.466, grad_norm = 0.799
I0810 06:13:11.661479 139913760143104 logging_writer.py:48] [42] global_step=42, grad_norm=0.782499, loss=0.461461
I0810 06:13:11.666283 139966470805312 submission.py:120] 42) loss = 0.461, grad_norm = 0.782
I0810 06:13:12.004732 139913768535808 logging_writer.py:48] [43] global_step=43, grad_norm=0.731580, loss=0.459951
I0810 06:13:12.009576 139966470805312 submission.py:120] 43) loss = 0.460, grad_norm = 0.732
I0810 06:13:12.355009 139913760143104 logging_writer.py:48] [44] global_step=44, grad_norm=0.699884, loss=0.453216
I0810 06:13:12.360263 139966470805312 submission.py:120] 44) loss = 0.453, grad_norm = 0.700
I0810 06:13:12.688750 139913768535808 logging_writer.py:48] [45] global_step=45, grad_norm=0.665035, loss=0.450775
I0810 06:13:12.693827 139966470805312 submission.py:120] 45) loss = 0.451, grad_norm = 0.665
I0810 06:13:13.028144 139913760143104 logging_writer.py:48] [46] global_step=46, grad_norm=0.651436, loss=0.444170
I0810 06:13:13.032990 139966470805312 submission.py:120] 46) loss = 0.444, grad_norm = 0.651
I0810 06:13:13.362721 139913768535808 logging_writer.py:48] [47] global_step=47, grad_norm=0.626424, loss=0.440527
I0810 06:13:13.367220 139966470805312 submission.py:120] 47) loss = 0.441, grad_norm = 0.626
I0810 06:13:13.696114 139913760143104 logging_writer.py:48] [48] global_step=48, grad_norm=0.620974, loss=0.435115
I0810 06:13:13.700608 139966470805312 submission.py:120] 48) loss = 0.435, grad_norm = 0.621
I0810 06:13:14.029629 139913768535808 logging_writer.py:48] [49] global_step=49, grad_norm=0.587601, loss=0.433503
I0810 06:13:14.034663 139966470805312 submission.py:120] 49) loss = 0.434, grad_norm = 0.588
I0810 06:13:14.363227 139913760143104 logging_writer.py:48] [50] global_step=50, grad_norm=0.570166, loss=0.430357
I0810 06:13:14.367757 139966470805312 submission.py:120] 50) loss = 0.430, grad_norm = 0.570
I0810 06:13:14.709436 139913768535808 logging_writer.py:48] [51] global_step=51, grad_norm=0.571105, loss=0.426834
I0810 06:13:14.714392 139966470805312 submission.py:120] 51) loss = 0.427, grad_norm = 0.571
I0810 06:13:15.046044 139913760143104 logging_writer.py:48] [52] global_step=52, grad_norm=0.553849, loss=0.422197
I0810 06:13:15.050822 139966470805312 submission.py:120] 52) loss = 0.422, grad_norm = 0.554
I0810 06:13:15.389949 139913768535808 logging_writer.py:48] [53] global_step=53, grad_norm=0.545644, loss=0.421902
I0810 06:13:15.394717 139966470805312 submission.py:120] 53) loss = 0.422, grad_norm = 0.546
I0810 06:13:15.729596 139913760143104 logging_writer.py:48] [54] global_step=54, grad_norm=0.520047, loss=0.417309
I0810 06:13:15.734505 139966470805312 submission.py:120] 54) loss = 0.417, grad_norm = 0.520
I0810 06:13:16.069094 139913768535808 logging_writer.py:48] [55] global_step=55, grad_norm=0.517185, loss=0.414475
I0810 06:13:16.074010 139966470805312 submission.py:120] 55) loss = 0.414, grad_norm = 0.517
I0810 06:13:16.418038 139913760143104 logging_writer.py:48] [56] global_step=56, grad_norm=0.516309, loss=0.409919
I0810 06:13:16.422819 139966470805312 submission.py:120] 56) loss = 0.410, grad_norm = 0.516
I0810 06:13:16.770059 139913768535808 logging_writer.py:48] [57] global_step=57, grad_norm=0.512500, loss=0.408175
I0810 06:13:16.774900 139966470805312 submission.py:120] 57) loss = 0.408, grad_norm = 0.513
I0810 06:13:17.116429 139913760143104 logging_writer.py:48] [58] global_step=58, grad_norm=0.497671, loss=0.404210
I0810 06:13:17.120969 139966470805312 submission.py:120] 58) loss = 0.404, grad_norm = 0.498
I0810 06:13:17.459098 139913768535808 logging_writer.py:48] [59] global_step=59, grad_norm=0.495262, loss=0.404025
I0810 06:13:17.463737 139966470805312 submission.py:120] 59) loss = 0.404, grad_norm = 0.495
I0810 06:13:17.810953 139913760143104 logging_writer.py:48] [60] global_step=60, grad_norm=0.492236, loss=0.399043
I0810 06:13:17.815824 139966470805312 submission.py:120] 60) loss = 0.399, grad_norm = 0.492
I0810 06:13:18.156852 139913768535808 logging_writer.py:48] [61] global_step=61, grad_norm=0.476311, loss=0.397469
I0810 06:13:18.161568 139966470805312 submission.py:120] 61) loss = 0.397, grad_norm = 0.476
I0810 06:13:18.498487 139913760143104 logging_writer.py:48] [62] global_step=62, grad_norm=0.469277, loss=0.393739
I0810 06:13:18.503888 139966470805312 submission.py:120] 62) loss = 0.394, grad_norm = 0.469
I0810 06:13:18.832875 139913768535808 logging_writer.py:48] [63] global_step=63, grad_norm=0.469124, loss=0.391681
I0810 06:13:18.837440 139966470805312 submission.py:120] 63) loss = 0.392, grad_norm = 0.469
I0810 06:13:19.187258 139913760143104 logging_writer.py:48] [64] global_step=64, grad_norm=0.455246, loss=0.390356
I0810 06:13:19.191967 139966470805312 submission.py:120] 64) loss = 0.390, grad_norm = 0.455
I0810 06:13:19.537653 139913768535808 logging_writer.py:48] [65] global_step=65, grad_norm=0.452600, loss=0.387744
I0810 06:13:19.542174 139966470805312 submission.py:120] 65) loss = 0.388, grad_norm = 0.453
I0810 06:13:19.878775 139913760143104 logging_writer.py:48] [66] global_step=66, grad_norm=0.444163, loss=0.386576
I0810 06:13:19.884108 139966470805312 submission.py:120] 66) loss = 0.387, grad_norm = 0.444
I0810 06:13:20.233462 139913768535808 logging_writer.py:48] [67] global_step=67, grad_norm=0.439392, loss=0.385520
I0810 06:13:20.238566 139966470805312 submission.py:120] 67) loss = 0.386, grad_norm = 0.439
I0810 06:13:20.583399 139913760143104 logging_writer.py:48] [68] global_step=68, grad_norm=0.463349, loss=0.381262
I0810 06:13:20.588260 139966470805312 submission.py:120] 68) loss = 0.381, grad_norm = 0.463
I0810 06:13:20.929517 139913768535808 logging_writer.py:48] [69] global_step=69, grad_norm=0.546319, loss=0.377708
I0810 06:13:20.934598 139966470805312 submission.py:120] 69) loss = 0.378, grad_norm = 0.546
I0810 06:13:21.276846 139913760143104 logging_writer.py:48] [70] global_step=70, grad_norm=0.503606, loss=0.374587
I0810 06:13:21.281451 139966470805312 submission.py:120] 70) loss = 0.375, grad_norm = 0.504
I0810 06:13:21.612769 139913768535808 logging_writer.py:48] [71] global_step=71, grad_norm=0.469602, loss=0.370810
I0810 06:13:21.617792 139966470805312 submission.py:120] 71) loss = 0.371, grad_norm = 0.470
I0810 06:13:21.957543 139913760143104 logging_writer.py:48] [72] global_step=72, grad_norm=0.462852, loss=0.368019
I0810 06:13:21.962490 139966470805312 submission.py:120] 72) loss = 0.368, grad_norm = 0.463
I0810 06:13:22.301501 139913768535808 logging_writer.py:48] [73] global_step=73, grad_norm=0.448475, loss=0.364228
I0810 06:13:22.306223 139966470805312 submission.py:120] 73) loss = 0.364, grad_norm = 0.448
I0810 06:13:22.650228 139913760143104 logging_writer.py:48] [74] global_step=74, grad_norm=0.437733, loss=0.361996
I0810 06:13:22.654944 139966470805312 submission.py:120] 74) loss = 0.362, grad_norm = 0.438
I0810 06:13:22.994254 139913768535808 logging_writer.py:48] [75] global_step=75, grad_norm=0.428320, loss=0.358889
I0810 06:13:22.998982 139966470805312 submission.py:120] 75) loss = 0.359, grad_norm = 0.428
I0810 06:13:23.337149 139913760143104 logging_writer.py:48] [76] global_step=76, grad_norm=0.426009, loss=0.356900
I0810 06:13:23.341911 139966470805312 submission.py:120] 76) loss = 0.357, grad_norm = 0.426
I0810 06:13:23.685553 139913768535808 logging_writer.py:48] [77] global_step=77, grad_norm=0.417686, loss=0.355862
I0810 06:13:23.690397 139966470805312 submission.py:120] 77) loss = 0.356, grad_norm = 0.418
I0810 06:13:24.027793 139913760143104 logging_writer.py:48] [78] global_step=78, grad_norm=0.414311, loss=0.352800
I0810 06:13:24.032842 139966470805312 submission.py:120] 78) loss = 0.353, grad_norm = 0.414
I0810 06:13:24.376859 139913768535808 logging_writer.py:48] [79] global_step=79, grad_norm=0.406431, loss=0.351835
I0810 06:13:24.381523 139966470805312 submission.py:120] 79) loss = 0.352, grad_norm = 0.406
I0810 06:13:24.716677 139913760143104 logging_writer.py:48] [80] global_step=80, grad_norm=0.401551, loss=0.348347
I0810 06:13:24.721473 139966470805312 submission.py:120] 80) loss = 0.348, grad_norm = 0.402
I0810 06:13:25.061541 139913768535808 logging_writer.py:48] [81] global_step=81, grad_norm=0.403708, loss=0.346706
I0810 06:13:25.066249 139966470805312 submission.py:120] 81) loss = 0.347, grad_norm = 0.404
I0810 06:13:25.411235 139913760143104 logging_writer.py:48] [82] global_step=82, grad_norm=0.396741, loss=0.346338
I0810 06:13:25.416136 139966470805312 submission.py:120] 82) loss = 0.346, grad_norm = 0.397
I0810 06:13:25.748836 139913768535808 logging_writer.py:48] [83] global_step=83, grad_norm=0.396598, loss=0.344191
I0810 06:13:25.753415 139966470805312 submission.py:120] 83) loss = 0.344, grad_norm = 0.397
I0810 06:13:26.088966 139913760143104 logging_writer.py:48] [84] global_step=84, grad_norm=0.391458, loss=0.343365
I0810 06:13:26.093969 139966470805312 submission.py:120] 84) loss = 0.343, grad_norm = 0.391
I0810 06:13:26.420970 139913768535808 logging_writer.py:48] [85] global_step=85, grad_norm=0.387906, loss=0.343104
I0810 06:13:26.425746 139966470805312 submission.py:120] 85) loss = 0.343, grad_norm = 0.388
I0810 06:13:26.754831 139913760143104 logging_writer.py:48] [86] global_step=86, grad_norm=0.383615, loss=0.338758
I0810 06:13:26.759448 139966470805312 submission.py:120] 86) loss = 0.339, grad_norm = 0.384
I0810 06:13:27.091246 139913768535808 logging_writer.py:48] [87] global_step=87, grad_norm=0.387941, loss=0.336804
I0810 06:13:27.096465 139966470805312 submission.py:120] 87) loss = 0.337, grad_norm = 0.388
I0810 06:13:27.426393 139913760143104 logging_writer.py:48] [88] global_step=88, grad_norm=0.380223, loss=0.334531
I0810 06:13:27.431230 139966470805312 submission.py:120] 88) loss = 0.335, grad_norm = 0.380
I0810 06:13:27.768880 139913768535808 logging_writer.py:48] [89] global_step=89, grad_norm=0.380817, loss=0.336263
I0810 06:13:27.773766 139966470805312 submission.py:120] 89) loss = 0.336, grad_norm = 0.381
I0810 06:13:28.118331 139913760143104 logging_writer.py:48] [90] global_step=90, grad_norm=0.384161, loss=0.332577
I0810 06:13:28.123297 139966470805312 submission.py:120] 90) loss = 0.333, grad_norm = 0.384
I0810 06:13:28.462842 139913768535808 logging_writer.py:48] [91] global_step=91, grad_norm=0.381263, loss=0.331366
I0810 06:13:28.467535 139966470805312 submission.py:120] 91) loss = 0.331, grad_norm = 0.381
I0810 06:13:28.804400 139913760143104 logging_writer.py:48] [92] global_step=92, grad_norm=0.375386, loss=0.330364
I0810 06:13:28.808955 139966470805312 submission.py:120] 92) loss = 0.330, grad_norm = 0.375
I0810 06:13:29.144868 139913768535808 logging_writer.py:48] [93] global_step=93, grad_norm=0.375641, loss=0.327508
I0810 06:13:29.149466 139966470805312 submission.py:120] 93) loss = 0.328, grad_norm = 0.376
I0810 06:13:29.491422 139913760143104 logging_writer.py:48] [94] global_step=94, grad_norm=0.372278, loss=0.325775
I0810 06:13:29.496409 139966470805312 submission.py:120] 94) loss = 0.326, grad_norm = 0.372
I0810 06:13:29.829397 139913768535808 logging_writer.py:48] [95] global_step=95, grad_norm=0.370785, loss=0.325458
I0810 06:13:29.833979 139966470805312 submission.py:120] 95) loss = 0.325, grad_norm = 0.371
I0810 06:13:30.166064 139913760143104 logging_writer.py:48] [96] global_step=96, grad_norm=0.371236, loss=0.326191
I0810 06:13:30.170841 139966470805312 submission.py:120] 96) loss = 0.326, grad_norm = 0.371
I0810 06:13:30.509910 139913768535808 logging_writer.py:48] [97] global_step=97, grad_norm=0.372618, loss=0.323787
I0810 06:13:30.514604 139966470805312 submission.py:120] 97) loss = 0.324, grad_norm = 0.373
I0810 06:13:30.855322 139913760143104 logging_writer.py:48] [98] global_step=98, grad_norm=0.371823, loss=0.321277
I0810 06:13:30.860608 139966470805312 submission.py:120] 98) loss = 0.321, grad_norm = 0.372
I0810 06:13:31.194177 139913768535808 logging_writer.py:48] [99] global_step=99, grad_norm=0.373306, loss=0.317175
I0810 06:13:31.199070 139966470805312 submission.py:120] 99) loss = 0.317, grad_norm = 0.373
I0810 06:13:31.538850 139913760143104 logging_writer.py:48] [100] global_step=100, grad_norm=0.366841, loss=0.317865
I0810 06:13:31.543652 139966470805312 submission.py:120] 100) loss = 0.318, grad_norm = 0.367
I0810 06:15:43.918936 139913768535808 logging_writer.py:48] [500] global_step=500, grad_norm=0.063018, loss=0.061173
I0810 06:15:43.923817 139966470805312 submission.py:120] 500) loss = 0.061, grad_norm = 0.063
I0810 06:16:57.132557 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:17:57.690225 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:18:01.136439 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:18:04.642809 139966470805312 submission_runner.py:362] Time since start: 460.45s, 	Step: 721, 	{'train/accuracy': 0.9865629014192676, 'train/loss': 0.05514131856534467, 'train/mean_average_precision': 0.047045180651805074, 'validation/accuracy': 0.9841187879909751, 'validation/loss': 0.06402281041676308, 'validation/mean_average_precision': 0.047410283914460824, 'validation/num_examples': 43793, 'test/accuracy': 0.9831370495901987, 'test/loss': 0.06720510659595089, 'test/mean_average_precision': 0.048797128832584205, 'test/num_examples': 43793, 'score': 245.21889686584473, 'total_duration': 460.4468505382538, 'accumulated_submission_time': 245.21889686584473, 'accumulated_eval_time': 214.86986327171326, 'accumulated_logging_time': 0.13502168655395508}
I0810 06:18:04.660122 139913760143104 logging_writer.py:48] [721] accumulated_eval_time=214.869863, accumulated_logging_time=0.135022, accumulated_submission_time=245.218897, global_step=721, preemption_count=0, score=245.218897, test/accuracy=0.983137, test/loss=0.067205, test/mean_average_precision=0.048797, test/num_examples=43793, total_duration=460.446851, train/accuracy=0.986563, train/loss=0.055141, train/mean_average_precision=0.047045, validation/accuracy=0.984119, validation/loss=0.064023, validation/mean_average_precision=0.047410, validation/num_examples=43793
I0810 06:19:37.728101 139913768535808 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.024552, loss=0.048676
I0810 06:19:37.734469 139966470805312 submission.py:120] 1000) loss = 0.049, grad_norm = 0.025
I0810 06:22:04.919369 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:23:08.185730 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:23:11.675680 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:23:15.064846 139966470805312 submission_runner.py:362] Time since start: 770.87s, 	Step: 1442, 	{'train/accuracy': 0.9872697419369948, 'train/loss': 0.047790411656614254, 'train/mean_average_precision': 0.08982930542522222, 'validation/accuracy': 0.9842275800981889, 'validation/loss': 0.05741432418586563, 'validation/mean_average_precision': 0.09132391521775894, 'validation/num_examples': 43793, 'test/accuracy': 0.9831913837045673, 'test/loss': 0.06069554932000224, 'test/mean_average_precision': 0.088599298419814, 'test/num_examples': 43793, 'score': 485.14320158958435, 'total_duration': 770.8689312934875, 'accumulated_submission_time': 485.14320158958435, 'accumulated_eval_time': 285.0151455402374, 'accumulated_logging_time': 0.2667274475097656}
I0810 06:23:15.081738 139913760143104 logging_writer.py:48] [1442] accumulated_eval_time=285.015146, accumulated_logging_time=0.266727, accumulated_submission_time=485.143202, global_step=1442, preemption_count=0, score=485.143202, test/accuracy=0.983191, test/loss=0.060696, test/mean_average_precision=0.088599, test/num_examples=43793, total_duration=770.868931, train/accuracy=0.987270, train/loss=0.047790, train/mean_average_precision=0.089829, validation/accuracy=0.984228, validation/loss=0.057414, validation/mean_average_precision=0.091324, validation/num_examples=43793
I0810 06:23:35.111998 139913768535808 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.019473, loss=0.052778
I0810 06:23:35.117237 139966470805312 submission.py:120] 1500) loss = 0.053, grad_norm = 0.019
I0810 06:26:21.568074 139913760143104 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.048303, loss=0.045684
I0810 06:26:21.574403 139966470805312 submission.py:120] 2000) loss = 0.046, grad_norm = 0.048
I0810 06:27:15.307833 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:28:20.194594 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:28:23.689755 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:28:27.132242 139966470805312 submission_runner.py:362] Time since start: 1082.94s, 	Step: 2160, 	{'train/accuracy': 0.9874215456540928, 'train/loss': 0.04604501881881807, 'train/mean_average_precision': 0.11396813282697371, 'validation/accuracy': 0.9846818277398764, 'validation/loss': 0.055646928155397346, 'validation/mean_average_precision': 0.112870534232086, 'validation/num_examples': 43793, 'test/accuracy': 0.9836917629903812, 'test/loss': 0.058833098815642665, 'test/mean_average_precision': 0.10835791709671096, 'test/num_examples': 43793, 'score': 725.0415494441986, 'total_duration': 1082.9362525939941, 'accumulated_submission_time': 725.0415494441986, 'accumulated_eval_time': 356.8393123149872, 'accumulated_logging_time': 0.3942258358001709}
I0810 06:28:27.148544 139913768535808 logging_writer.py:48] [2160] accumulated_eval_time=356.839312, accumulated_logging_time=0.394226, accumulated_submission_time=725.041549, global_step=2160, preemption_count=0, score=725.041549, test/accuracy=0.983692, test/loss=0.058833, test/mean_average_precision=0.108358, test/num_examples=43793, total_duration=1082.936253, train/accuracy=0.987422, train/loss=0.046045, train/mean_average_precision=0.113968, validation/accuracy=0.984682, validation/loss=0.055647, validation/mean_average_precision=0.112871, validation/num_examples=43793
I0810 06:30:23.436907 139913760143104 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.037555, loss=0.050183
I0810 06:30:23.443211 139966470805312 submission.py:120] 2500) loss = 0.050, grad_norm = 0.038
I0810 06:32:27.415233 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:33:32.281490 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:33:36.261807 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:33:39.707827 139966470805312 submission_runner.py:362] Time since start: 1395.51s, 	Step: 2866, 	{'train/accuracy': 0.9877184442312449, 'train/loss': 0.044532122605246284, 'train/mean_average_precision': 0.13340078162835312, 'validation/accuracy': 0.984943659490447, 'validation/loss': 0.05416215565065393, 'validation/mean_average_precision': 0.12846748187521004, 'validation/num_examples': 43793, 'test/accuracy': 0.9839672243143899, 'test/loss': 0.05720712938342574, 'test/mean_average_precision': 0.12549124401127718, 'test/num_examples': 43793, 'score': 964.9911663532257, 'total_duration': 1395.5118265151978, 'accumulated_submission_time': 964.9911663532257, 'accumulated_eval_time': 429.13165307044983, 'accumulated_logging_time': 0.518953800201416}
I0810 06:33:39.725795 139913768535808 logging_writer.py:48] [2866] accumulated_eval_time=429.131653, accumulated_logging_time=0.518954, accumulated_submission_time=964.991166, global_step=2866, preemption_count=0, score=964.991166, test/accuracy=0.983967, test/loss=0.057207, test/mean_average_precision=0.125491, test/num_examples=43793, total_duration=1395.511827, train/accuracy=0.987718, train/loss=0.044532, train/mean_average_precision=0.133401, validation/accuracy=0.984944, validation/loss=0.054162, validation/mean_average_precision=0.128467, validation/num_examples=43793
I0810 06:34:25.184638 139913760143104 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.044861, loss=0.049958
I0810 06:34:25.191683 139966470805312 submission.py:120] 3000) loss = 0.050, grad_norm = 0.045
I0810 06:37:13.489459 139913768535808 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.019101, loss=0.041028
I0810 06:37:13.497271 139966470805312 submission.py:120] 3500) loss = 0.041, grad_norm = 0.019
I0810 06:37:39.786838 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:38:47.472878 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:38:51.087806 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:38:54.688852 139966470805312 submission_runner.py:362] Time since start: 1710.49s, 	Step: 3579, 	{'train/accuracy': 0.9880858293698727, 'train/loss': 0.04227317223713763, 'train/mean_average_precision': 0.17239404750201004, 'validation/accuracy': 0.9852168575805772, 'validation/loss': 0.051567826703103906, 'validation/mean_average_precision': 0.14683989345013113, 'validation/num_examples': 43793, 'test/accuracy': 0.9843147099295383, 'test/loss': 0.05414854849467125, 'test/mean_average_precision': 0.15202470797157003, 'test/num_examples': 43793, 'score': 1204.732923746109, 'total_duration': 1710.4927518367767, 'accumulated_submission_time': 1204.732923746109, 'accumulated_eval_time': 504.0333044528961, 'accumulated_logging_time': 0.6462557315826416}
I0810 06:38:54.707097 139913760143104 logging_writer.py:48] [3579] accumulated_eval_time=504.033304, accumulated_logging_time=0.646256, accumulated_submission_time=1204.732924, global_step=3579, preemption_count=0, score=1204.732924, test/accuracy=0.984315, test/loss=0.054149, test/mean_average_precision=0.152025, test/num_examples=43793, total_duration=1710.492752, train/accuracy=0.988086, train/loss=0.042273, train/mean_average_precision=0.172394, validation/accuracy=0.985217, validation/loss=0.051568, validation/mean_average_precision=0.146840, validation/num_examples=43793
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0810 06:41:16.822773 139913768535808 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.021515, loss=0.044307
I0810 06:41:16.829555 139966470805312 submission.py:120] 4000) loss = 0.044, grad_norm = 0.022
I0810 06:42:54.769078 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:44:05.752620 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:44:09.283632 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:44:12.771064 139966470805312 submission_runner.py:362] Time since start: 2028.58s, 	Step: 4292, 	{'train/accuracy': 0.9883424685303047, 'train/loss': 0.040661998558337346, 'train/mean_average_precision': 0.1927635664647842, 'validation/accuracy': 0.9854843725009276, 'validation/loss': 0.049951329613089804, 'validation/mean_average_precision': 0.1694825102653003, 'validation/num_examples': 43793, 'test/accuracy': 0.9845842745279566, 'test/loss': 0.05265355415658081, 'test/mean_average_precision': 0.16937413406260654, 'test/num_examples': 43793, 'score': 1444.46311211586, 'total_duration': 2028.5750901699066, 'accumulated_submission_time': 1444.46311211586, 'accumulated_eval_time': 582.0350384712219, 'accumulated_logging_time': 0.7775490283966064}
I0810 06:44:12.787541 139919213123328 logging_writer.py:48] [4292] accumulated_eval_time=582.035038, accumulated_logging_time=0.777549, accumulated_submission_time=1444.463112, global_step=4292, preemption_count=0, score=1444.463112, test/accuracy=0.984584, test/loss=0.052654, test/mean_average_precision=0.169374, test/num_examples=43793, total_duration=2028.575090, train/accuracy=0.988342, train/loss=0.040662, train/mean_average_precision=0.192764, validation/accuracy=0.985484, validation/loss=0.049951, validation/mean_average_precision=0.169483, validation/num_examples=43793
I0810 06:45:22.880573 139919221516032 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013972, loss=0.042140
I0810 06:45:22.887185 139966470805312 submission.py:120] 4500) loss = 0.042, grad_norm = 0.014
I0810 06:48:10.141825 139919213123328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.024426, loss=0.044014
I0810 06:48:10.148581 139966470805312 submission.py:120] 5000) loss = 0.044, grad_norm = 0.024
I0810 06:48:12.848470 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:49:24.247125 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:49:27.802987 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:49:31.303041 139966470805312 submission_runner.py:362] Time since start: 2347.11s, 	Step: 5009, 	{'train/accuracy': 0.988445439275795, 'train/loss': 0.03979597624095294, 'train/mean_average_precision': 0.21759987882718612, 'validation/accuracy': 0.9856053428290982, 'validation/loss': 0.04924073993449741, 'validation/mean_average_precision': 0.18200892627921372, 'validation/num_examples': 43793, 'test/accuracy': 0.9847321138624016, 'test/loss': 0.05211811699440527, 'test/mean_average_precision': 0.18170750700324387, 'test/num_examples': 43793, 'score': 1684.2023293972015, 'total_duration': 2347.1070551872253, 'accumulated_submission_time': 1684.2023293972015, 'accumulated_eval_time': 660.489334821701, 'accumulated_logging_time': 0.9032316207885742}
I0810 06:49:31.320278 139919221516032 logging_writer.py:48] [5009] accumulated_eval_time=660.489335, accumulated_logging_time=0.903232, accumulated_submission_time=1684.202329, global_step=5009, preemption_count=0, score=1684.202329, test/accuracy=0.984732, test/loss=0.052118, test/mean_average_precision=0.181708, test/num_examples=43793, total_duration=2347.107055, train/accuracy=0.988445, train/loss=0.039796, train/mean_average_precision=0.217600, validation/accuracy=0.985605, validation/loss=0.049241, validation/mean_average_precision=0.182009, validation/num_examples=43793
I0810 06:52:17.220099 139919213123328 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.010060, loss=0.039202
I0810 06:52:17.227232 139966470805312 submission.py:120] 5500) loss = 0.039, grad_norm = 0.010
I0810 06:53:31.335071 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:54:43.341323 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:54:46.904054 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:54:50.446507 139966470805312 submission_runner.py:362] Time since start: 2666.25s, 	Step: 5720, 	{'train/accuracy': 0.9889241848133519, 'train/loss': 0.03793585247199909, 'train/mean_average_precision': 0.2426269070525726, 'validation/accuracy': 0.9859146696414001, 'validation/loss': 0.04790611157422179, 'validation/mean_average_precision': 0.203543368375288, 'validation/num_examples': 43793, 'test/accuracy': 0.9850202110269611, 'test/loss': 0.050469903217885276, 'test/mean_average_precision': 0.2053993925496678, 'test/num_examples': 43793, 'score': 1923.8910374641418, 'total_duration': 2666.2504873275757, 'accumulated_submission_time': 1923.8910374641418, 'accumulated_eval_time': 739.6004867553711, 'accumulated_logging_time': 1.033203125}
I0810 06:54:50.473217 139919221516032 logging_writer.py:48] [5720] accumulated_eval_time=739.600487, accumulated_logging_time=1.033203, accumulated_submission_time=1923.891037, global_step=5720, preemption_count=0, score=1923.891037, test/accuracy=0.985020, test/loss=0.050470, test/mean_average_precision=0.205399, test/num_examples=43793, total_duration=2666.250487, train/accuracy=0.988924, train/loss=0.037936, train/mean_average_precision=0.242627, validation/accuracy=0.985915, validation/loss=0.047906, validation/mean_average_precision=0.203543, validation/num_examples=43793
I0810 06:56:25.195469 139966470805312 spec.py:320] Evaluating on the training split.
I0810 06:57:36.127505 139966470805312 spec.py:332] Evaluating on the validation split.
I0810 06:57:39.653573 139966470805312 spec.py:348] Evaluating on the test split.
I0810 06:57:43.184834 139966470805312 submission_runner.py:362] Time since start: 2838.99s, 	Step: 6000, 	{'train/accuracy': 0.9888351427995694, 'train/loss': 0.03801871433696865, 'train/mean_average_precision': 0.24446142460445966, 'validation/accuracy': 0.9858968082506635, 'validation/loss': 0.047743893865383566, 'validation/mean_average_precision': 0.20581218382202407, 'validation/num_examples': 43793, 'test/accuracy': 0.9849700888594427, 'test/loss': 0.05039513458012576, 'test/mean_average_precision': 0.20651685886708276, 'test/num_examples': 43793, 'score': 2018.425853729248, 'total_duration': 2838.9886934757233, 'accumulated_submission_time': 2018.425853729248, 'accumulated_eval_time': 817.5894513130188, 'accumulated_logging_time': 1.1624915599822998}
I0810 06:57:43.203720 139919213123328 logging_writer.py:48] [6000] accumulated_eval_time=817.589451, accumulated_logging_time=1.162492, accumulated_submission_time=2018.425854, global_step=6000, preemption_count=0, score=2018.425854, test/accuracy=0.984970, test/loss=0.050395, test/mean_average_precision=0.206517, test/num_examples=43793, total_duration=2838.988693, train/accuracy=0.988835, train/loss=0.038019, train/mean_average_precision=0.244461, validation/accuracy=0.985897, validation/loss=0.047744, validation/mean_average_precision=0.205812, validation/num_examples=43793
I0810 06:57:43.329195 139919221516032 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=2018.425854
I0810 06:57:43.421181 139966470805312 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/ogbg_pytorch/trial_1/checkpoint_6000.
I0810 06:57:43.584893 139966470805312 submission_runner.py:528] Tuning trial 1/1
I0810 06:57:43.585116 139966470805312 submission_runner.py:529] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0810 06:57:43.586490 139966470805312 submission_runner.py:530] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5079313337109175, 'train/loss': 0.7704198915917613, 'train/mean_average_precision': 0.019777809757215506, 'validation/accuracy': 0.5008313665506489, 'validation/loss': 0.7763156132099598, 'validation/mean_average_precision': 0.02383655911985128, 'validation/num_examples': 43793, 'test/accuracy': 0.49733973436935996, 'test/loss': 0.7795949286475143, 'test/mean_average_precision': 0.025727453796939883, 'test/num_examples': 43793, 'score': 5.4752020835876465, 'total_duration': 152.83542776107788, 'accumulated_submission_time': 5.4752020835876465, 'accumulated_eval_time': 147.35985565185547, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (721, {'train/accuracy': 0.9865629014192676, 'train/loss': 0.05514131856534467, 'train/mean_average_precision': 0.047045180651805074, 'validation/accuracy': 0.9841187879909751, 'validation/loss': 0.06402281041676308, 'validation/mean_average_precision': 0.047410283914460824, 'validation/num_examples': 43793, 'test/accuracy': 0.9831370495901987, 'test/loss': 0.06720510659595089, 'test/mean_average_precision': 0.048797128832584205, 'test/num_examples': 43793, 'score': 245.21889686584473, 'total_duration': 460.4468505382538, 'accumulated_submission_time': 245.21889686584473, 'accumulated_eval_time': 214.86986327171326, 'accumulated_logging_time': 0.13502168655395508, 'global_step': 721, 'preemption_count': 0}), (1442, {'train/accuracy': 0.9872697419369948, 'train/loss': 0.047790411656614254, 'train/mean_average_precision': 0.08982930542522222, 'validation/accuracy': 0.9842275800981889, 'validation/loss': 0.05741432418586563, 'validation/mean_average_precision': 0.09132391521775894, 'validation/num_examples': 43793, 'test/accuracy': 0.9831913837045673, 'test/loss': 0.06069554932000224, 'test/mean_average_precision': 0.088599298419814, 'test/num_examples': 43793, 'score': 485.14320158958435, 'total_duration': 770.8689312934875, 'accumulated_submission_time': 485.14320158958435, 'accumulated_eval_time': 285.0151455402374, 'accumulated_logging_time': 0.2667274475097656, 'global_step': 1442, 'preemption_count': 0}), (2160, {'train/accuracy': 0.9874215456540928, 'train/loss': 0.04604501881881807, 'train/mean_average_precision': 0.11396813282697371, 'validation/accuracy': 0.9846818277398764, 'validation/loss': 0.055646928155397346, 'validation/mean_average_precision': 0.112870534232086, 'validation/num_examples': 43793, 'test/accuracy': 0.9836917629903812, 'test/loss': 0.058833098815642665, 'test/mean_average_precision': 0.10835791709671096, 'test/num_examples': 43793, 'score': 725.0415494441986, 'total_duration': 1082.9362525939941, 'accumulated_submission_time': 725.0415494441986, 'accumulated_eval_time': 356.8393123149872, 'accumulated_logging_time': 0.3942258358001709, 'global_step': 2160, 'preemption_count': 0}), (2866, {'train/accuracy': 0.9877184442312449, 'train/loss': 0.044532122605246284, 'train/mean_average_precision': 0.13340078162835312, 'validation/accuracy': 0.984943659490447, 'validation/loss': 0.05416215565065393, 'validation/mean_average_precision': 0.12846748187521004, 'validation/num_examples': 43793, 'test/accuracy': 0.9839672243143899, 'test/loss': 0.05720712938342574, 'test/mean_average_precision': 0.12549124401127718, 'test/num_examples': 43793, 'score': 964.9911663532257, 'total_duration': 1395.5118265151978, 'accumulated_submission_time': 964.9911663532257, 'accumulated_eval_time': 429.13165307044983, 'accumulated_logging_time': 0.518953800201416, 'global_step': 2866, 'preemption_count': 0}), (3579, {'train/accuracy': 0.9880858293698727, 'train/loss': 0.04227317223713763, 'train/mean_average_precision': 0.17239404750201004, 'validation/accuracy': 0.9852168575805772, 'validation/loss': 0.051567826703103906, 'validation/mean_average_precision': 0.14683989345013113, 'validation/num_examples': 43793, 'test/accuracy': 0.9843147099295383, 'test/loss': 0.05414854849467125, 'test/mean_average_precision': 0.15202470797157003, 'test/num_examples': 43793, 'score': 1204.732923746109, 'total_duration': 1710.4927518367767, 'accumulated_submission_time': 1204.732923746109, 'accumulated_eval_time': 504.0333044528961, 'accumulated_logging_time': 0.6462557315826416, 'global_step': 3579, 'preemption_count': 0}), (4292, {'train/accuracy': 0.9883424685303047, 'train/loss': 0.040661998558337346, 'train/mean_average_precision': 0.1927635664647842, 'validation/accuracy': 0.9854843725009276, 'validation/loss': 0.049951329613089804, 'validation/mean_average_precision': 0.1694825102653003, 'validation/num_examples': 43793, 'test/accuracy': 0.9845842745279566, 'test/loss': 0.05265355415658081, 'test/mean_average_precision': 0.16937413406260654, 'test/num_examples': 43793, 'score': 1444.46311211586, 'total_duration': 2028.5750901699066, 'accumulated_submission_time': 1444.46311211586, 'accumulated_eval_time': 582.0350384712219, 'accumulated_logging_time': 0.7775490283966064, 'global_step': 4292, 'preemption_count': 0}), (5009, {'train/accuracy': 0.988445439275795, 'train/loss': 0.03979597624095294, 'train/mean_average_precision': 0.21759987882718612, 'validation/accuracy': 0.9856053428290982, 'validation/loss': 0.04924073993449741, 'validation/mean_average_precision': 0.18200892627921372, 'validation/num_examples': 43793, 'test/accuracy': 0.9847321138624016, 'test/loss': 0.05211811699440527, 'test/mean_average_precision': 0.18170750700324387, 'test/num_examples': 43793, 'score': 1684.2023293972015, 'total_duration': 2347.1070551872253, 'accumulated_submission_time': 1684.2023293972015, 'accumulated_eval_time': 660.489334821701, 'accumulated_logging_time': 0.9032316207885742, 'global_step': 5009, 'preemption_count': 0}), (5720, {'train/accuracy': 0.9889241848133519, 'train/loss': 0.03793585247199909, 'train/mean_average_precision': 0.2426269070525726, 'validation/accuracy': 0.9859146696414001, 'validation/loss': 0.04790611157422179, 'validation/mean_average_precision': 0.203543368375288, 'validation/num_examples': 43793, 'test/accuracy': 0.9850202110269611, 'test/loss': 0.050469903217885276, 'test/mean_average_precision': 0.2053993925496678, 'test/num_examples': 43793, 'score': 1923.8910374641418, 'total_duration': 2666.2504873275757, 'accumulated_submission_time': 1923.8910374641418, 'accumulated_eval_time': 739.6004867553711, 'accumulated_logging_time': 1.033203125, 'global_step': 5720, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9888351427995694, 'train/loss': 0.03801871433696865, 'train/mean_average_precision': 0.24446142460445966, 'validation/accuracy': 0.9858968082506635, 'validation/loss': 0.047743893865383566, 'validation/mean_average_precision': 0.20581218382202407, 'validation/num_examples': 43793, 'test/accuracy': 0.9849700888594427, 'test/loss': 0.05039513458012576, 'test/mean_average_precision': 0.20651685886708276, 'test/num_examples': 43793, 'score': 2018.425853729248, 'total_duration': 2838.9886934757233, 'accumulated_submission_time': 2018.425853729248, 'accumulated_eval_time': 817.5894513130188, 'accumulated_logging_time': 1.1624915599822998, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0810 06:57:43.586644 139966470805312 submission_runner.py:531] Timing: 2018.425853729248
I0810 06:57:43.586696 139966470805312 submission_runner.py:533] Total number of evals: 10
I0810 06:57:43.586740 139966470805312 submission_runner.py:534] ====================
I0810 06:57:43.586840 139966470805312 submission_runner.py:602] Final ogbg score: 2018.425853729248
