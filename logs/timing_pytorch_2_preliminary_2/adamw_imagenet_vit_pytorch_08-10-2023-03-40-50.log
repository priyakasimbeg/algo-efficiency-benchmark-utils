torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=imagenet_vit --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/imagenet/pytorch --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_2/adamw --overwrite=true --save_checkpoints=false --max_global_steps=14000 --imagenet_v2_data_dir=/data/imagenet/pytorch --torch_compile=True 2>&1 | tee -a /logs/imagenet_vit_pytorch_08-10-2023-03-40-50.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-10 03:40:59.743131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 03:40:59.743221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0810 03:41:14.441610 139902298187584 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0810 03:41:14.441640 139780404025152 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0810 03:41:14.441630 139895319234368 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0810 03:41:14.442498 140150877022016 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0810 03:41:14.442553 140112051132224 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0810 03:41:14.442791 139735904175936 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0810 03:41:14.443027 139716144174912 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0810 03:41:14.443350 139716144174912 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.443283 140546297386816 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0810 03:41:14.443610 140546297386816 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.452385 139902298187584 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.452415 139895319234368 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.452446 139780404025152 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.453109 140150877022016 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.453138 140112051132224 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 03:41:14.453393 139735904175936 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0810 03:41:15.659886 139735904175936 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_vit_pytorch.
W0810 03:41:15.704109 140150877022016 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704107 139735904175936 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704120 139895319234368 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704126 140546297386816 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704150 139716144174912 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704231 139902298187584 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704405 139780404025152 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 03:41:15.704924 140112051132224 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 03:41:15.710530 139735904175936 submission_runner.py:488] Using RNG seed 330293680
I0810 03:41:15.712641 139735904175936 submission_runner.py:497] --- Tuning run 1/1 ---
I0810 03:41:15.712758 139735904175936 submission_runner.py:502] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_vit_pytorch/trial_1.
I0810 03:41:15.717573 139735904175936 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_vit_pytorch/trial_1/hparams.json.
I0810 03:41:15.718415 139735904175936 submission_runner.py:176] Initializing dataset.
I0810 03:41:22.106230 139735904175936 submission_runner.py:183] Initializing model.
I0810 03:41:26.898029 139735904175936 submission_runner.py:212] Performing `torch.compile`.
I0810 03:41:27.485018 139735904175936 submission_runner.py:215] Initializing optimizer.
I0810 03:41:27.486714 139735904175936 submission_runner.py:222] Initializing metrics bundle.
I0810 03:41:27.486839 139735904175936 submission_runner.py:240] Initializing checkpoint and logger.
I0810 03:41:27.995452 139735904175936 submission_runner.py:261] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0810 03:41:27.996500 139735904175936 submission_runner.py:264] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_vit_pytorch/trial_1/flags_0.json.
I0810 03:41:28.083199 139735904175936 submission_runner.py:274] Starting training loop.
[2023-08-10 03:41:30,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,722] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,835] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:30,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:41:33,080] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,157] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,161] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,168] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,190] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,191] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,235] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,246] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,250] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,251] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,264] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,270] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,284] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,288] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,289] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,295] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:41:33,312] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,316] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,318] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,318] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,322] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,323] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,338] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,342] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,342] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:41:33,343] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:33,346] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:41:33,347] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:41:39,049] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,050] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,052] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:39,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 03:41:45,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,531] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:45,635] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 03:41:49,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,216] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,226] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,282] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,284] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:49,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 03:41:56,220] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:56,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 03:41:59,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,721] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,770] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:41:59,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 03:42:00,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:00,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:01,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:01,027] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:01,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:01,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:01,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:01,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 03:42:04,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,539] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:04,717] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 03:42:08,210] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,344] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,471] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,487] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,552] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,601] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 03:42:08,603] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,660] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,708] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,722] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,783] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,796] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,836] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:08,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 03:42:08,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 03:42:08,916] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:42:17,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,737] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:17,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 03:42:18,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,264] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 03:42:18,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:18,318] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 03:42:23,311] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,345] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:23,402] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 03:42:25,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:25,423] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 03:42:30,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,639] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,656] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,761] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:30,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 03:42:31,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:31,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:31,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:31,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:31,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:31,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:31,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:32,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 03:42:34,151] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,179] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,285] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,457] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 03:42:34,474] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 03:42:34,721] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:34,744] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:34,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:34,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:34,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:34,901] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:35,044] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 03:42:35,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0810 03:42:35.545454 139705748285184 logging_writer.py:48] [0] global_step=0, grad_norm=0.330972, loss=6.907756
I0810 03:42:35.572091 139735904175936 submission.py:120] 0) loss = 6.908, grad_norm = 0.331
I0810 03:42:35.581705 139735904175936 spec.py:320] Evaluating on the training split.
[2023-08-10 03:42:47,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:47,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:47,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:47,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:47,585] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:47,629] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:47,900] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:48,413] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:42:49,176] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,219] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,221] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,223] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,224] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,225] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,226] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,259] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,274] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,303] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,303] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,307] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,307] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,315] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,317] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,319] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,321] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,322] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,347] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,351] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,352] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,550] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:49,593] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:49,597] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:49,597] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:49,726] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:49,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:49,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:49,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:49,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:49,851] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:50,087] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:50,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:42:50,219] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:42:50,223] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:42:50,223] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:42:50,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 03:42:52,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:52,325] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:52,366] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:52,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:52,396] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:52,501] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:52,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:53,352] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 03:42:53,832] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:53,865] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:53,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:53,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:53,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:54,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:54,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:54,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 03:42:57,533] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:57,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:57,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:57,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:57,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:57,870] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:57,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:58,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 03:42:59,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:42:59,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:42:59,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:42:59,429] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:42:59,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:42:59,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:42:59,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:43:00,217] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:00,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 03:43:00,315] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:00,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:00,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:00,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:00,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:00,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:01,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 03:43:01,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:01,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:01,879] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:02,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:02,116] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:02,138] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:02,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:02,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 03:43:05,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:05,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:05,790] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:05,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:05,804] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:05,810] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:05,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:05,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:05,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:05,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:05,990] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:05,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:06,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:06,007] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:06,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:06,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:06,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:06,180] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:06,185] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:06,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:06,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:06,191] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:06,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:06,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:06,239] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:06,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:06,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:06,282] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:43:06,634] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 03:43:06,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 03:43:06,890] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 03:43:06,895] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0810 03:43:59.029011 139735904175936 spec.py:332] Evaluating on the validation split.
[2023-08-10 03:44:50,732] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:50,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:50,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:51,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:51,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:51,659] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:52,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:52,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:52,640] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:52,644] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:52,645] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:52,656] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:52,680] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:52,698] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:52,702] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:52,702] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:52,722] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:52,726] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:52,726] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:52,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:52,927] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:52,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:52,932] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:53,032] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:53,073] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:53,077] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:53,078] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:53,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:53,196] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:53,223] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:53,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:53,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:53,538] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:53,541] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:53,542] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:53,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:54,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:54,622] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:54,670] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:54,674] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:54,675] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:55,042] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:44:55,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:55,643] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:55,660] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:55,689] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:55,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:56,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:56,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:56,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:44:56,994] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:44:56,998] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:44:56,998] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:44:57,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:44:57,232] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:44:57,268] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:44:57,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:44:57,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:44:57,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 03:44:57,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:44:58,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:44:59,047] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:45:00,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 03:45:01,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:01,609] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:01,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:01,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:02,007] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 03:45:02,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:02,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:03,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:03,342] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:03,384] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:03,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:03,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:03,761] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:04,178] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:04,401] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:04,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:04,462] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:04,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:04,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:05,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:05,218] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:05,572] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 03:45:05,860] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:05,891] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:05,912] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:06,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:06,101] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:06,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:06,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:07,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 03:45:07,600] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:08,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 03:45:08,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:08,720] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:08,793] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:08,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:08,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:08,913] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:08,913] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:08,933] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:08,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:08,943] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:09,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:09,008] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:09,013] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:09,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:09,127] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:09,133] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:09,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:09,392] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:09,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:09,402] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:09,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:09,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:09,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:09,685] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:09,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 03:45:10,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:10,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:10,569] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:10,574] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:12,605] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 03:45:12,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 03:45:12,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 03:45:12,811] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0810 03:45:15.877325 139735904175936 spec.py:348] Evaluating on the test split.
I0810 03:45:15.893038 139735904175936 dataset_info.py:578] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0810 03:45:15.899262 139735904175936 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0810 03:45:15.976458 139735904175936 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
[2023-08-10 03:45:19,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,215] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,365] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,432] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,622] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,642] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:19,693] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:24,631] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,640] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,671] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,675] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,675] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,680] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,683] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,684] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,758] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,762] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,763] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,794] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,798] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,799] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,803] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,805] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,809] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,809] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,829] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:24,843] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,868] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,869] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:24,872] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,872] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:24,873] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:24,873] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:25,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,278] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,357] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 03:45:25,984] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:25,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:26,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:26,157] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:26,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:26,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:26,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:26,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 03:45:27,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:27,679] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 03:45:28,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:28,877] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:28,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:29,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:29,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:29,055] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:29,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:29,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 03:45:30,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,591] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:30,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 03:45:31,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,627] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:31,674] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 03:45:32,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:32,931] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:32,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:32,979] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:33,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:33,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:33,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:33,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 03:45:34,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,472] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,494] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:34,508] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,568] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,628] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:34,638] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,642] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,647] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:34,683] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,692] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:34,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 03:45:34,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,735] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,745] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:34,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,756] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,762] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:34,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:34,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:34,803] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:35,016] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 03:45:35,024] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 03:45:35,035] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:39,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,264] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,339] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,589] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:39,614] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 03:45:40,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:40,914] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:40,918] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:40,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:40,937] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:40,961] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:40,980] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:40,984] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:40,984] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,004] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:41,008] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:41,009] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,032] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:41,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:41,079] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:41,079] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,115] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:41,158] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:41,162] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:41,162] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,173] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:41,215] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:41,219] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:41,220] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:41,317] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:41,321] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:41,321] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 03:45:41,405] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,422] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 03:45:41,426] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 03:45:41,426] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 03:45:41,481] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,505] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,578] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,817] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:41,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 03:45:42,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,687] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,859] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:42,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 03:45:43,921] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:43,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:44,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:44,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:44,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:44,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:44,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:44,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 03:45:45,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:45,518] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:45,557] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:45,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:45,725] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:45,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:45,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:46,079] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 03:45:47,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,483] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,661] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:47,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 03:45:48,056] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,200] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,281] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,334] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,441] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:48,714] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 03:45:49,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:49,615] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:49,648] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:49,661] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:49,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:49,839] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:50,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:50,202] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 03:45:52,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,439] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,479] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:52,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:52,521] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:52,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:52,619] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:52,624] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:52,644] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:52,649] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:52,654] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:52,663] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:52,730] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:52,738] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:52,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:52,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:52,774] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:52,856] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:52,917] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:52,922] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:52,927] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:53,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:53,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:53,071] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 03:45:53,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 03:45:53,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 03:45:53,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 03:45:53,327] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
I0810 03:45:54.117033 139735904175936 submission_runner.py:362] Time since start: 266.03s, 	Step: 1, 	{'train/accuracy': 0.002578125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00242, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.002, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 67.49856781959534, 'total_duration': 266.03423857688904, 'accumulated_submission_time': 67.49856781959534, 'accumulated_eval_time': 198.53520894050598, 'accumulated_logging_time': 0}
I0810 03:45:54.137798 139701730146048 logging_writer.py:48] [1] accumulated_eval_time=198.535209, accumulated_logging_time=0, accumulated_submission_time=67.498568, global_step=1, preemption_count=0, score=67.498568, test/accuracy=0.002000, test/loss=6.907755, test/num_examples=10000, total_duration=266.034239, train/accuracy=0.002578, train/loss=6.907756, validation/accuracy=0.002420, validation/loss=6.907756, validation/num_examples=50000
I0810 03:45:54.170234 139735904175936 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172712 139902298187584 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172758 140150877022016 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172771 139895319234368 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172789 140112051132224 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172754 139780404025152 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172831 140546297386816 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.172834 139716144174912 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 03:45:54.810975 139701721753344 logging_writer.py:48] [1] global_step=1, grad_norm=0.342142, loss=6.907755
I0810 03:45:54.814561 139735904175936 submission.py:120] 1) loss = 6.908, grad_norm = 0.342
I0810 03:45:55.208406 139701730146048 logging_writer.py:48] [2] global_step=2, grad_norm=0.348640, loss=6.907754
I0810 03:45:55.212462 139735904175936 submission.py:120] 2) loss = 6.908, grad_norm = 0.349
I0810 03:45:55.600012 139701721753344 logging_writer.py:48] [3] global_step=3, grad_norm=0.344304, loss=6.907752
I0810 03:45:55.604142 139735904175936 submission.py:120] 3) loss = 6.908, grad_norm = 0.344
I0810 03:45:55.996401 139701730146048 logging_writer.py:48] [4] global_step=4, grad_norm=0.335612, loss=6.907754
I0810 03:45:56.001711 139735904175936 submission.py:120] 4) loss = 6.908, grad_norm = 0.336
I0810 03:45:56.389141 139701721753344 logging_writer.py:48] [5] global_step=5, grad_norm=0.342438, loss=6.907753
I0810 03:45:56.394042 139735904175936 submission.py:120] 5) loss = 6.908, grad_norm = 0.342
I0810 03:45:56.787194 139701730146048 logging_writer.py:48] [6] global_step=6, grad_norm=0.353205, loss=6.907741
I0810 03:45:56.796386 139735904175936 submission.py:120] 6) loss = 6.908, grad_norm = 0.353
I0810 03:45:57.187557 139701721753344 logging_writer.py:48] [7] global_step=7, grad_norm=0.352520, loss=6.907741
I0810 03:45:57.192842 139735904175936 submission.py:120] 7) loss = 6.908, grad_norm = 0.353
I0810 03:45:57.588109 139701730146048 logging_writer.py:48] [8] global_step=8, grad_norm=0.345914, loss=6.907735
I0810 03:45:57.592819 139735904175936 submission.py:120] 8) loss = 6.908, grad_norm = 0.346
I0810 03:45:57.985862 139701721753344 logging_writer.py:48] [9] global_step=9, grad_norm=0.350267, loss=6.907739
I0810 03:45:57.990696 139735904175936 submission.py:120] 9) loss = 6.908, grad_norm = 0.350
I0810 03:45:58.384248 139701730146048 logging_writer.py:48] [10] global_step=10, grad_norm=0.344381, loss=6.907724
I0810 03:45:58.389899 139735904175936 submission.py:120] 10) loss = 6.908, grad_norm = 0.344
I0810 03:45:58.781868 139701721753344 logging_writer.py:48] [11] global_step=11, grad_norm=0.347601, loss=6.907714
I0810 03:45:58.786817 139735904175936 submission.py:120] 11) loss = 6.908, grad_norm = 0.348
I0810 03:45:59.201596 139701730146048 logging_writer.py:48] [12] global_step=12, grad_norm=0.339378, loss=6.907711
I0810 03:45:59.206701 139735904175936 submission.py:120] 12) loss = 6.908, grad_norm = 0.339
I0810 03:45:59.605736 139701721753344 logging_writer.py:48] [13] global_step=13, grad_norm=0.344564, loss=6.907674
I0810 03:45:59.612060 139735904175936 submission.py:120] 13) loss = 6.908, grad_norm = 0.345
I0810 03:46:00.005981 139701730146048 logging_writer.py:48] [14] global_step=14, grad_norm=0.349631, loss=6.907704
I0810 03:46:00.010262 139735904175936 submission.py:120] 14) loss = 6.908, grad_norm = 0.350
I0810 03:46:00.405891 139701721753344 logging_writer.py:48] [15] global_step=15, grad_norm=0.337302, loss=6.907702
I0810 03:46:00.411945 139735904175936 submission.py:120] 15) loss = 6.908, grad_norm = 0.337
I0810 03:46:00.818441 139701730146048 logging_writer.py:48] [16] global_step=16, grad_norm=0.331171, loss=6.907677
I0810 03:46:00.823269 139735904175936 submission.py:120] 16) loss = 6.908, grad_norm = 0.331
I0810 03:46:01.248020 139701721753344 logging_writer.py:48] [17] global_step=17, grad_norm=0.340245, loss=6.907639
I0810 03:46:01.252860 139735904175936 submission.py:120] 17) loss = 6.908, grad_norm = 0.340
I0810 03:46:01.653502 139701730146048 logging_writer.py:48] [18] global_step=18, grad_norm=0.343168, loss=6.907636
I0810 03:46:01.659025 139735904175936 submission.py:120] 18) loss = 6.908, grad_norm = 0.343
I0810 03:46:02.064868 139701721753344 logging_writer.py:48] [19] global_step=19, grad_norm=0.345485, loss=6.907657
I0810 03:46:02.071801 139735904175936 submission.py:120] 19) loss = 6.908, grad_norm = 0.345
I0810 03:46:02.468196 139701730146048 logging_writer.py:48] [20] global_step=20, grad_norm=0.341327, loss=6.907564
I0810 03:46:02.472729 139735904175936 submission.py:120] 20) loss = 6.908, grad_norm = 0.341
I0810 03:46:02.864702 139701721753344 logging_writer.py:48] [21] global_step=21, grad_norm=0.354080, loss=6.907523
I0810 03:46:02.870126 139735904175936 submission.py:120] 21) loss = 6.908, grad_norm = 0.354
I0810 03:46:03.269793 139701730146048 logging_writer.py:48] [22] global_step=22, grad_norm=0.347901, loss=6.907616
I0810 03:46:03.276023 139735904175936 submission.py:120] 22) loss = 6.908, grad_norm = 0.348
I0810 03:46:03.675785 139701721753344 logging_writer.py:48] [23] global_step=23, grad_norm=0.335134, loss=6.907476
I0810 03:46:03.681705 139735904175936 submission.py:120] 23) loss = 6.907, grad_norm = 0.335
I0810 03:46:04.073098 139701730146048 logging_writer.py:48] [24] global_step=24, grad_norm=0.347929, loss=6.907434
I0810 03:46:04.078216 139735904175936 submission.py:120] 24) loss = 6.907, grad_norm = 0.348
I0810 03:46:04.478090 139701721753344 logging_writer.py:48] [25] global_step=25, grad_norm=0.343598, loss=6.907420
I0810 03:46:04.483351 139735904175936 submission.py:120] 25) loss = 6.907, grad_norm = 0.344
I0810 03:46:04.886141 139701730146048 logging_writer.py:48] [26] global_step=26, grad_norm=0.355292, loss=6.907425
I0810 03:46:04.892330 139735904175936 submission.py:120] 26) loss = 6.907, grad_norm = 0.355
I0810 03:46:05.299011 139701721753344 logging_writer.py:48] [27] global_step=27, grad_norm=0.345214, loss=6.907417
I0810 03:46:05.305084 139735904175936 submission.py:120] 27) loss = 6.907, grad_norm = 0.345
I0810 03:46:05.698231 139701730146048 logging_writer.py:48] [28] global_step=28, grad_norm=0.355251, loss=6.907339
I0810 03:46:05.703673 139735904175936 submission.py:120] 28) loss = 6.907, grad_norm = 0.355
I0810 03:46:06.105605 139701721753344 logging_writer.py:48] [29] global_step=29, grad_norm=0.354725, loss=6.907278
I0810 03:46:06.114101 139735904175936 submission.py:120] 29) loss = 6.907, grad_norm = 0.355
I0810 03:46:06.506376 139701730146048 logging_writer.py:48] [30] global_step=30, grad_norm=0.358619, loss=6.907207
I0810 03:46:06.512200 139735904175936 submission.py:120] 30) loss = 6.907, grad_norm = 0.359
I0810 03:46:06.910634 139701721753344 logging_writer.py:48] [31] global_step=31, grad_norm=0.358967, loss=6.907096
I0810 03:46:06.915897 139735904175936 submission.py:120] 31) loss = 6.907, grad_norm = 0.359
I0810 03:46:07.326420 139701730146048 logging_writer.py:48] [32] global_step=32, grad_norm=0.364253, loss=6.907199
I0810 03:46:07.333363 139735904175936 submission.py:120] 32) loss = 6.907, grad_norm = 0.364
I0810 03:46:07.742595 139701721753344 logging_writer.py:48] [33] global_step=33, grad_norm=0.353894, loss=6.907188
I0810 03:46:07.748614 139735904175936 submission.py:120] 33) loss = 6.907, grad_norm = 0.354
I0810 03:46:08.144646 139701730146048 logging_writer.py:48] [34] global_step=34, grad_norm=0.359830, loss=6.906971
I0810 03:46:08.148951 139735904175936 submission.py:120] 34) loss = 6.907, grad_norm = 0.360
I0810 03:46:08.553322 139701721753344 logging_writer.py:48] [35] global_step=35, grad_norm=0.368475, loss=6.906770
I0810 03:46:08.558900 139735904175936 submission.py:120] 35) loss = 6.907, grad_norm = 0.368
I0810 03:46:08.952820 139701730146048 logging_writer.py:48] [36] global_step=36, grad_norm=0.354819, loss=6.906799
I0810 03:46:08.957885 139735904175936 submission.py:120] 36) loss = 6.907, grad_norm = 0.355
I0810 03:46:09.351231 139701721753344 logging_writer.py:48] [37] global_step=37, grad_norm=0.361933, loss=6.906839
I0810 03:46:09.356403 139735904175936 submission.py:120] 37) loss = 6.907, grad_norm = 0.362
I0810 03:46:09.748259 139701730146048 logging_writer.py:48] [38] global_step=38, grad_norm=0.367319, loss=6.906649
I0810 03:46:09.752936 139735904175936 submission.py:120] 38) loss = 6.907, grad_norm = 0.367
I0810 03:46:10.153518 139701721753344 logging_writer.py:48] [39] global_step=39, grad_norm=0.373464, loss=6.906436
I0810 03:46:10.158129 139735904175936 submission.py:120] 39) loss = 6.906, grad_norm = 0.373
I0810 03:46:10.555002 139701730146048 logging_writer.py:48] [40] global_step=40, grad_norm=0.360771, loss=6.906528
I0810 03:46:10.561080 139735904175936 submission.py:120] 40) loss = 6.907, grad_norm = 0.361
I0810 03:46:10.952540 139701721753344 logging_writer.py:48] [41] global_step=41, grad_norm=0.362918, loss=6.906587
I0810 03:46:10.957427 139735904175936 submission.py:120] 41) loss = 6.907, grad_norm = 0.363
I0810 03:46:11.359382 139701730146048 logging_writer.py:48] [42] global_step=42, grad_norm=0.390523, loss=6.906433
I0810 03:46:11.365869 139735904175936 submission.py:120] 42) loss = 6.906, grad_norm = 0.391
I0810 03:46:11.757754 139701721753344 logging_writer.py:48] [43] global_step=43, grad_norm=0.388484, loss=6.905910
I0810 03:46:11.762583 139735904175936 submission.py:120] 43) loss = 6.906, grad_norm = 0.388
I0810 03:46:12.162215 139701730146048 logging_writer.py:48] [44] global_step=44, grad_norm=0.398104, loss=6.905441
I0810 03:46:12.167709 139735904175936 submission.py:120] 44) loss = 6.905, grad_norm = 0.398
I0810 03:46:12.573925 139701721753344 logging_writer.py:48] [45] global_step=45, grad_norm=0.398361, loss=6.905590
I0810 03:46:12.579461 139735904175936 submission.py:120] 45) loss = 6.906, grad_norm = 0.398
I0810 03:46:12.971377 139701730146048 logging_writer.py:48] [46] global_step=46, grad_norm=0.402031, loss=6.905768
I0810 03:46:12.976259 139735904175936 submission.py:120] 46) loss = 6.906, grad_norm = 0.402
I0810 03:46:13.373838 139701721753344 logging_writer.py:48] [47] global_step=47, grad_norm=0.405964, loss=6.905614
I0810 03:46:13.380263 139735904175936 submission.py:120] 47) loss = 6.906, grad_norm = 0.406
I0810 03:46:13.789024 139701730146048 logging_writer.py:48] [48] global_step=48, grad_norm=0.422858, loss=6.904798
I0810 03:46:13.794058 139735904175936 submission.py:120] 48) loss = 6.905, grad_norm = 0.423
I0810 03:46:14.188739 139701721753344 logging_writer.py:48] [49] global_step=49, grad_norm=0.406644, loss=6.905610
I0810 03:46:14.194351 139735904175936 submission.py:120] 49) loss = 6.906, grad_norm = 0.407
I0810 03:46:14.591153 139701730146048 logging_writer.py:48] [50] global_step=50, grad_norm=0.382395, loss=6.905776
I0810 03:46:14.596582 139735904175936 submission.py:120] 50) loss = 6.906, grad_norm = 0.382
I0810 03:46:14.990116 139701721753344 logging_writer.py:48] [51] global_step=51, grad_norm=0.401484, loss=6.905025
I0810 03:46:14.994500 139735904175936 submission.py:120] 51) loss = 6.905, grad_norm = 0.401
I0810 03:46:15.386772 139701730146048 logging_writer.py:48] [52] global_step=52, grad_norm=0.387426, loss=6.905342
I0810 03:46:15.392172 139735904175936 submission.py:120] 52) loss = 6.905, grad_norm = 0.387
I0810 03:46:15.785983 139701721753344 logging_writer.py:48] [53] global_step=53, grad_norm=0.411032, loss=6.904723
I0810 03:46:15.791929 139735904175936 submission.py:120] 53) loss = 6.905, grad_norm = 0.411
I0810 03:46:16.186852 139701730146048 logging_writer.py:48] [54] global_step=54, grad_norm=0.402751, loss=6.904491
I0810 03:46:16.192228 139735904175936 submission.py:120] 54) loss = 6.904, grad_norm = 0.403
I0810 03:46:16.585459 139701721753344 logging_writer.py:48] [55] global_step=55, grad_norm=0.395252, loss=6.904344
I0810 03:46:16.591490 139735904175936 submission.py:120] 55) loss = 6.904, grad_norm = 0.395
I0810 03:46:16.983107 139701730146048 logging_writer.py:48] [56] global_step=56, grad_norm=0.422657, loss=6.904051
I0810 03:46:16.989844 139735904175936 submission.py:120] 56) loss = 6.904, grad_norm = 0.423
I0810 03:46:17.385389 139701721753344 logging_writer.py:48] [57] global_step=57, grad_norm=0.427554, loss=6.903349
I0810 03:46:17.390600 139735904175936 submission.py:120] 57) loss = 6.903, grad_norm = 0.428
I0810 03:46:17.788920 139701730146048 logging_writer.py:48] [58] global_step=58, grad_norm=0.423862, loss=6.903799
I0810 03:46:17.800224 139735904175936 submission.py:120] 58) loss = 6.904, grad_norm = 0.424
I0810 03:46:18.197208 139701721753344 logging_writer.py:48] [59] global_step=59, grad_norm=0.424143, loss=6.903205
I0810 03:46:18.202450 139735904175936 submission.py:120] 59) loss = 6.903, grad_norm = 0.424
I0810 03:46:18.598830 139701730146048 logging_writer.py:48] [60] global_step=60, grad_norm=0.446134, loss=6.903028
I0810 03:46:18.605206 139735904175936 submission.py:120] 60) loss = 6.903, grad_norm = 0.446
I0810 03:46:19.000833 139701721753344 logging_writer.py:48] [61] global_step=61, grad_norm=0.438129, loss=6.901556
I0810 03:46:19.006381 139735904175936 submission.py:120] 61) loss = 6.902, grad_norm = 0.438
I0810 03:46:19.401979 139701730146048 logging_writer.py:48] [62] global_step=62, grad_norm=0.389241, loss=6.903750
I0810 03:46:19.408159 139735904175936 submission.py:120] 62) loss = 6.904, grad_norm = 0.389
I0810 03:46:19.802976 139701721753344 logging_writer.py:48] [63] global_step=63, grad_norm=0.424200, loss=6.901774
I0810 03:46:19.808434 139735904175936 submission.py:120] 63) loss = 6.902, grad_norm = 0.424
I0810 03:46:20.203805 139701730146048 logging_writer.py:48] [64] global_step=64, grad_norm=0.405659, loss=6.902814
I0810 03:46:20.208329 139735904175936 submission.py:120] 64) loss = 6.903, grad_norm = 0.406
I0810 03:46:20.607136 139701721753344 logging_writer.py:48] [65] global_step=65, grad_norm=0.430379, loss=6.902479
I0810 03:46:20.611770 139735904175936 submission.py:120] 65) loss = 6.902, grad_norm = 0.430
I0810 03:46:21.018246 139701730146048 logging_writer.py:48] [66] global_step=66, grad_norm=0.420385, loss=6.901839
I0810 03:46:21.022549 139735904175936 submission.py:120] 66) loss = 6.902, grad_norm = 0.420
I0810 03:46:21.430965 139701721753344 logging_writer.py:48] [67] global_step=67, grad_norm=0.433733, loss=6.901320
I0810 03:46:21.435519 139735904175936 submission.py:120] 67) loss = 6.901, grad_norm = 0.434
I0810 03:46:21.833525 139701730146048 logging_writer.py:48] [68] global_step=68, grad_norm=0.433188, loss=6.900767
I0810 03:46:21.839148 139735904175936 submission.py:120] 68) loss = 6.901, grad_norm = 0.433
I0810 03:46:22.236497 139701721753344 logging_writer.py:48] [69] global_step=69, grad_norm=0.415189, loss=6.901386
I0810 03:46:22.242182 139735904175936 submission.py:120] 69) loss = 6.901, grad_norm = 0.415
I0810 03:46:22.636063 139701730146048 logging_writer.py:48] [70] global_step=70, grad_norm=0.420155, loss=6.900009
I0810 03:46:22.640084 139735904175936 submission.py:120] 70) loss = 6.900, grad_norm = 0.420
I0810 03:46:23.035536 139701721753344 logging_writer.py:48] [71] global_step=71, grad_norm=0.455294, loss=6.901219
I0810 03:46:23.041278 139735904175936 submission.py:120] 71) loss = 6.901, grad_norm = 0.455
I0810 03:46:23.438218 139701730146048 logging_writer.py:48] [72] global_step=72, grad_norm=0.465170, loss=6.897845
I0810 03:46:23.443431 139735904175936 submission.py:120] 72) loss = 6.898, grad_norm = 0.465
I0810 03:46:23.839610 139701721753344 logging_writer.py:48] [73] global_step=73, grad_norm=0.441897, loss=6.900233
I0810 03:46:23.845533 139735904175936 submission.py:120] 73) loss = 6.900, grad_norm = 0.442
I0810 03:46:24.242630 139701730146048 logging_writer.py:48] [74] global_step=74, grad_norm=0.418140, loss=6.901224
I0810 03:46:24.248563 139735904175936 submission.py:120] 74) loss = 6.901, grad_norm = 0.418
I0810 03:46:24.646730 139701721753344 logging_writer.py:48] [75] global_step=75, grad_norm=0.437977, loss=6.898579
I0810 03:46:24.658581 139735904175936 submission.py:120] 75) loss = 6.899, grad_norm = 0.438
I0810 03:46:25.067008 139701730146048 logging_writer.py:48] [76] global_step=76, grad_norm=0.462801, loss=6.898726
I0810 03:46:25.072768 139735904175936 submission.py:120] 76) loss = 6.899, grad_norm = 0.463
I0810 03:46:25.467618 139701721753344 logging_writer.py:48] [77] global_step=77, grad_norm=0.447206, loss=6.897429
I0810 03:46:25.473408 139735904175936 submission.py:120] 77) loss = 6.897, grad_norm = 0.447
I0810 03:46:25.875586 139701730146048 logging_writer.py:48] [78] global_step=78, grad_norm=0.432245, loss=6.898581
I0810 03:46:25.882650 139735904175936 submission.py:120] 78) loss = 6.899, grad_norm = 0.432
I0810 03:46:26.279347 139701721753344 logging_writer.py:48] [79] global_step=79, grad_norm=0.455116, loss=6.899700
I0810 03:46:26.285861 139735904175936 submission.py:120] 79) loss = 6.900, grad_norm = 0.455
I0810 03:46:26.689324 139701730146048 logging_writer.py:48] [80] global_step=80, grad_norm=0.480531, loss=6.895918
I0810 03:46:26.693844 139735904175936 submission.py:120] 80) loss = 6.896, grad_norm = 0.481
I0810 03:46:27.092822 139701721753344 logging_writer.py:48] [81] global_step=81, grad_norm=0.462273, loss=6.895973
I0810 03:46:27.100743 139735904175936 submission.py:120] 81) loss = 6.896, grad_norm = 0.462
I0810 03:46:27.517931 139701730146048 logging_writer.py:48] [82] global_step=82, grad_norm=0.445295, loss=6.896323
I0810 03:46:27.522620 139735904175936 submission.py:120] 82) loss = 6.896, grad_norm = 0.445
I0810 03:46:27.929559 139701721753344 logging_writer.py:48] [83] global_step=83, grad_norm=0.464566, loss=6.897726
I0810 03:46:27.935025 139735904175936 submission.py:120] 83) loss = 6.898, grad_norm = 0.465
I0810 03:46:28.332016 139701730146048 logging_writer.py:48] [84] global_step=84, grad_norm=0.460131, loss=6.895686
I0810 03:46:28.342949 139735904175936 submission.py:120] 84) loss = 6.896, grad_norm = 0.460
I0810 03:46:28.743658 139701721753344 logging_writer.py:48] [85] global_step=85, grad_norm=0.443986, loss=6.896175
I0810 03:46:28.749592 139735904175936 submission.py:120] 85) loss = 6.896, grad_norm = 0.444
I0810 03:46:29.143478 139701730146048 logging_writer.py:48] [86] global_step=86, grad_norm=0.441402, loss=6.897203
I0810 03:46:29.148903 139735904175936 submission.py:120] 86) loss = 6.897, grad_norm = 0.441
I0810 03:46:29.541846 139701721753344 logging_writer.py:48] [87] global_step=87, grad_norm=0.434866, loss=6.898022
I0810 03:46:29.546226 139735904175936 submission.py:120] 87) loss = 6.898, grad_norm = 0.435
I0810 03:46:29.951992 139701730146048 logging_writer.py:48] [88] global_step=88, grad_norm=0.455346, loss=6.894580
I0810 03:46:29.957629 139735904175936 submission.py:120] 88) loss = 6.895, grad_norm = 0.455
I0810 03:46:30.364800 139701721753344 logging_writer.py:48] [89] global_step=89, grad_norm=0.460364, loss=6.892106
I0810 03:46:30.371061 139735904175936 submission.py:120] 89) loss = 6.892, grad_norm = 0.460
I0810 03:46:30.783441 139701730146048 logging_writer.py:48] [90] global_step=90, grad_norm=0.479185, loss=6.893165
I0810 03:46:30.789382 139735904175936 submission.py:120] 90) loss = 6.893, grad_norm = 0.479
I0810 03:46:31.187498 139701721753344 logging_writer.py:48] [91] global_step=91, grad_norm=0.457325, loss=6.891839
I0810 03:46:31.192738 139735904175936 submission.py:120] 91) loss = 6.892, grad_norm = 0.457
I0810 03:46:31.586172 139701730146048 logging_writer.py:48] [92] global_step=92, grad_norm=0.481270, loss=6.890592
I0810 03:46:31.592132 139735904175936 submission.py:120] 92) loss = 6.891, grad_norm = 0.481
I0810 03:46:32.003109 139701721753344 logging_writer.py:48] [93] global_step=93, grad_norm=0.467350, loss=6.892278
I0810 03:46:32.008114 139735904175936 submission.py:120] 93) loss = 6.892, grad_norm = 0.467
I0810 03:46:32.407518 139701730146048 logging_writer.py:48] [94] global_step=94, grad_norm=0.453214, loss=6.892759
I0810 03:46:32.412624 139735904175936 submission.py:120] 94) loss = 6.893, grad_norm = 0.453
I0810 03:46:32.823621 139701721753344 logging_writer.py:48] [95] global_step=95, grad_norm=0.469412, loss=6.891612
I0810 03:46:32.830385 139735904175936 submission.py:120] 95) loss = 6.892, grad_norm = 0.469
I0810 03:46:33.249740 139701730146048 logging_writer.py:48] [96] global_step=96, grad_norm=0.452301, loss=6.893085
I0810 03:46:33.254397 139735904175936 submission.py:120] 96) loss = 6.893, grad_norm = 0.452
I0810 03:46:33.666015 139701721753344 logging_writer.py:48] [97] global_step=97, grad_norm=0.467143, loss=6.894829
I0810 03:46:33.670789 139735904175936 submission.py:120] 97) loss = 6.895, grad_norm = 0.467
I0810 03:46:34.073038 139701730146048 logging_writer.py:48] [98] global_step=98, grad_norm=0.461417, loss=6.890241
I0810 03:46:34.077889 139735904175936 submission.py:120] 98) loss = 6.890, grad_norm = 0.461
I0810 03:46:34.471035 139701721753344 logging_writer.py:48] [99] global_step=99, grad_norm=0.476701, loss=6.890924
I0810 03:46:34.476978 139735904175936 submission.py:120] 99) loss = 6.891, grad_norm = 0.477
I0810 03:46:34.885436 139701730146048 logging_writer.py:48] [100] global_step=100, grad_norm=0.469349, loss=6.889908
I0810 03:46:34.891000 139735904175936 submission.py:120] 100) loss = 6.890, grad_norm = 0.469
I0810 03:49:17.228754 139701721753344 logging_writer.py:48] [500] global_step=500, grad_norm=0.753719, loss=6.717290
I0810 03:49:17.236057 139735904175936 submission.py:120] 500) loss = 6.717, grad_norm = 0.754
I0810 03:52:39.476277 139701730146048 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.232328, loss=6.465036
I0810 03:52:39.485616 139735904175936 submission.py:120] 1000) loss = 6.465, grad_norm = 1.232
I0810 03:52:54.432685 139735904175936 spec.py:320] Evaluating on the training split.
I0810 03:53:41.059329 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 03:54:35.698798 139735904175936 spec.py:348] Evaluating on the test split.
I0810 03:54:37.123923 139735904175936 submission_runner.py:362] Time since start: 789.04s, 	Step: 1036, 	{'train/accuracy': 0.03462890625, 'train/loss': 5.95049072265625, 'validation/accuracy': 0.032, 'validation/loss': 5.9781525, 'validation/num_examples': 50000, 'test/accuracy': 0.0256, 'test/loss': 6.064193359375, 'test/num_examples': 10000, 'score': 487.0942919254303, 'total_duration': 789.0410706996918, 'accumulated_submission_time': 487.0942919254303, 'accumulated_eval_time': 301.2265577316284, 'accumulated_logging_time': 0.03258657455444336}
I0810 03:54:37.142553 139692236805888 logging_writer.py:48] [1036] accumulated_eval_time=301.226558, accumulated_logging_time=0.032587, accumulated_submission_time=487.094292, global_step=1036, preemption_count=0, score=487.094292, test/accuracy=0.025600, test/loss=6.064193, test/num_examples=10000, total_duration=789.041071, train/accuracy=0.034629, train/loss=5.950491, validation/accuracy=0.032000, validation/loss=5.978153, validation/num_examples=50000
I0810 03:57:45.151425 139692245198592 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.043059, loss=6.191224
I0810 03:57:45.156761 139735904175936 submission.py:120] 1500) loss = 6.191, grad_norm = 1.043
I0810 04:01:01.149396 139692236805888 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.027521, loss=6.089983
I0810 04:01:01.164927 139735904175936 submission.py:120] 2000) loss = 6.090, grad_norm = 1.028
I0810 04:01:37.184617 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:02:21.825921 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:03:07.200086 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:03:08.591279 139735904175936 submission_runner.py:362] Time since start: 1300.51s, 	Step: 2092, 	{'train/accuracy': 0.08234375, 'train/loss': 5.27327880859375, 'validation/accuracy': 0.07774, 'validation/loss': 5.318429375, 'validation/num_examples': 50000, 'test/accuracy': 0.0574, 'test/loss': 5.549546875, 'test/num_examples': 10000, 'score': 905.9109473228455, 'total_duration': 1300.5085935592651, 'accumulated_submission_time': 905.9109473228455, 'accumulated_eval_time': 392.6334009170532, 'accumulated_logging_time': 0.5851695537567139}
I0810 04:03:08.607716 139692245198592 logging_writer.py:48] [2092] accumulated_eval_time=392.633401, accumulated_logging_time=0.585170, accumulated_submission_time=905.910947, global_step=2092, preemption_count=0, score=905.910947, test/accuracy=0.057400, test/loss=5.549547, test/num_examples=10000, total_duration=1300.508594, train/accuracy=0.082344, train/loss=5.273279, validation/accuracy=0.077740, validation/loss=5.318429, validation/num_examples=50000
I0810 04:05:57.084368 139692236805888 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.014286, loss=5.999456
I0810 04:05:57.089944 139735904175936 submission.py:120] 2500) loss = 5.999, grad_norm = 1.014
I0810 04:09:16.058363 139692245198592 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.915795, loss=6.068266
I0810 04:09:16.064008 139735904175936 submission.py:120] 3000) loss = 6.068, grad_norm = 0.916
I0810 04:10:08.778241 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:10:53.257822 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:11:38.519489 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:11:39.912698 139735904175936 submission_runner.py:362] Time since start: 1811.83s, 	Step: 3134, 	{'train/accuracy': 0.14150390625, 'train/loss': 4.688422546386719, 'validation/accuracy': 0.13, 'validation/loss': 4.7481584375, 'validation/num_examples': 50000, 'test/accuracy': 0.0989, 'test/loss': 5.04826484375, 'test/num_examples': 10000, 'score': 1324.8571407794952, 'total_duration': 1811.8299634456635, 'accumulated_submission_time': 1324.8571407794952, 'accumulated_eval_time': 483.767826795578, 'accumulated_logging_time': 1.1294574737548828}
I0810 04:11:39.930058 139692236805888 logging_writer.py:48] [3134] accumulated_eval_time=483.767827, accumulated_logging_time=1.129457, accumulated_submission_time=1324.857141, global_step=3134, preemption_count=0, score=1324.857141, test/accuracy=0.098900, test/loss=5.048265, test/num_examples=10000, total_duration=1811.829963, train/accuracy=0.141504, train/loss=4.688423, validation/accuracy=0.130000, validation/loss=4.748158, validation/num_examples=50000
I0810 04:14:05.309561 139692245198592 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.905020, loss=5.880190
I0810 04:14:05.316489 139735904175936 submission.py:120] 3500) loss = 5.880, grad_norm = 0.905
I0810 04:17:26.908175 139692236805888 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.794771, loss=5.691355
I0810 04:17:26.915715 139735904175936 submission.py:120] 4000) loss = 5.691, grad_norm = 0.795
I0810 04:18:39.936454 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:19:27.539910 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:20:14.127879 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:20:15.517709 139735904175936 submission_runner.py:362] Time since start: 2327.43s, 	Step: 4187, 	{'train/accuracy': 0.193515625, 'train/loss': 4.262715148925781, 'validation/accuracy': 0.18148, 'validation/loss': 4.3486190625, 'validation/num_examples': 50000, 'test/accuracy': 0.138, 'test/loss': 4.70346328125, 'test/num_examples': 10000, 'score': 1743.64759683609, 'total_duration': 2327.43332695961, 'accumulated_submission_time': 1743.64759683609, 'accumulated_eval_time': 579.3476147651672, 'accumulated_logging_time': 1.6668717861175537}
I0810 04:20:15.538077 139692245198592 logging_writer.py:48] [4187] accumulated_eval_time=579.347615, accumulated_logging_time=1.666872, accumulated_submission_time=1743.647597, global_step=4187, preemption_count=0, score=1743.647597, test/accuracy=0.138000, test/loss=4.703463, test/num_examples=10000, total_duration=2327.433327, train/accuracy=0.193516, train/loss=4.262715, validation/accuracy=0.181480, validation/loss=4.348619, validation/num_examples=50000
I0810 04:22:19.546173 139692236805888 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.947563, loss=5.365844
I0810 04:22:19.550571 139735904175936 submission.py:120] 4500) loss = 5.366, grad_norm = 0.948
I0810 04:25:40.895594 139692245198592 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.755491, loss=5.191640
I0810 04:25:40.900366 139735904175936 submission.py:120] 5000) loss = 5.192, grad_norm = 0.755
I0810 04:27:15.873458 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:28:00.559538 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:28:45.435228 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:28:46.825613 139735904175936 submission_runner.py:362] Time since start: 2838.74s, 	Step: 5237, 	{'train/accuracy': 0.2476171875, 'train/loss': 3.83951904296875, 'validation/accuracy': 0.22886, 'validation/loss': 3.946846875, 'validation/num_examples': 50000, 'test/accuracy': 0.1738, 'test/loss': 4.37698203125, 'test/num_examples': 10000, 'score': 2162.7474913597107, 'total_duration': 2838.742928981781, 'accumulated_submission_time': 2162.7474913597107, 'accumulated_eval_time': 670.2998406887054, 'accumulated_logging_time': 2.224851608276367}
I0810 04:28:46.845343 139692236805888 logging_writer.py:48] [5237] accumulated_eval_time=670.299841, accumulated_logging_time=2.224852, accumulated_submission_time=2162.747491, global_step=5237, preemption_count=0, score=2162.747491, test/accuracy=0.173800, test/loss=4.376982, test/num_examples=10000, total_duration=2838.742929, train/accuracy=0.247617, train/loss=3.839519, validation/accuracy=0.228860, validation/loss=3.946847, validation/num_examples=50000
I0810 04:30:31.129764 139692245198592 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.933486, loss=5.300401
I0810 04:30:31.135700 139735904175936 submission.py:120] 5500) loss = 5.300, grad_norm = 0.933
I0810 04:33:50.380304 139692236805888 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.788518, loss=5.589695
I0810 04:33:50.386573 139735904175936 submission.py:120] 6000) loss = 5.590, grad_norm = 0.789
I0810 04:35:47.153360 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:36:32.804102 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:37:29.552997 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:37:30.949020 139735904175936 submission_runner.py:362] Time since start: 3362.86s, 	Step: 6280, 	{'train/accuracy': 0.28974609375, 'train/loss': 3.5317092895507813, 'validation/accuracy': 0.26546, 'validation/loss': 3.661364375, 'validation/num_examples': 50000, 'test/accuracy': 0.206, 'test/loss': 4.09978203125, 'test/num_examples': 10000, 'score': 2581.8367042541504, 'total_duration': 3362.8643465042114, 'accumulated_submission_time': 2581.8367042541504, 'accumulated_eval_time': 774.0936694145203, 'accumulated_logging_time': 2.773371934890747}
I0810 04:37:30.965872 139692245198592 logging_writer.py:48] [6280] accumulated_eval_time=774.093669, accumulated_logging_time=2.773372, accumulated_submission_time=2581.836704, global_step=6280, preemption_count=0, score=2581.836704, test/accuracy=0.206000, test/loss=4.099782, test/num_examples=10000, total_duration=3362.864347, train/accuracy=0.289746, train/loss=3.531709, validation/accuracy=0.265460, validation/loss=3.661364, validation/num_examples=50000
I0810 04:38:58.468089 139692236805888 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.816262, loss=5.037342
I0810 04:38:58.472928 139735904175936 submission.py:120] 6500) loss = 5.037, grad_norm = 0.816
I0810 04:42:15.562844 139692245198592 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.701424, loss=4.990126
I0810 04:42:15.570145 139735904175936 submission.py:120] 7000) loss = 4.990, grad_norm = 0.701
I0810 04:44:31.201454 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:45:16.560574 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:46:03.580148 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:46:04.976892 139735904175936 submission_runner.py:362] Time since start: 3876.89s, 	Step: 7336, 	{'train/accuracy': 0.33802734375, 'train/loss': 3.2290850830078126, 'validation/accuracy': 0.31188, 'validation/loss': 3.3718340625, 'validation/num_examples': 50000, 'test/accuracy': 0.2427, 'test/loss': 3.85165078125, 'test/num_examples': 10000, 'score': 3000.8093717098236, 'total_duration': 3876.894100189209, 'accumulated_submission_time': 3000.8093717098236, 'accumulated_eval_time': 867.8691403865814, 'accumulated_logging_time': 3.3547871112823486}
I0810 04:46:04.994567 139692236805888 logging_writer.py:48] [7336] accumulated_eval_time=867.869140, accumulated_logging_time=3.354787, accumulated_submission_time=3000.809372, global_step=7336, preemption_count=0, score=3000.809372, test/accuracy=0.242700, test/loss=3.851651, test/num_examples=10000, total_duration=3876.894100, train/accuracy=0.338027, train/loss=3.229085, validation/accuracy=0.311880, validation/loss=3.371834, validation/num_examples=50000
I0810 04:47:12.987081 139692245198592 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.790087, loss=4.624276
I0810 04:47:12.991067 139735904175936 submission.py:120] 7500) loss = 4.624, grad_norm = 0.790
I0810 04:50:31.188722 139692236805888 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.925688, loss=4.649048
I0810 04:50:31.195321 139735904175936 submission.py:120] 8000) loss = 4.649, grad_norm = 0.926
I0810 04:53:05.208655 139735904175936 spec.py:320] Evaluating on the training split.
I0810 04:53:49.884752 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 04:54:36.312989 139735904175936 spec.py:348] Evaluating on the test split.
I0810 04:54:37.712392 139735904175936 submission_runner.py:362] Time since start: 4389.63s, 	Step: 8390, 	{'train/accuracy': 0.371640625, 'train/loss': 3.020064392089844, 'validation/accuracy': 0.34376, 'validation/loss': 3.184846875, 'validation/num_examples': 50000, 'test/accuracy': 0.2681, 'test/loss': 3.71076484375, 'test/num_examples': 10000, 'score': 3419.7532732486725, 'total_duration': 4389.628029584885, 'accumulated_submission_time': 3419.7532732486725, 'accumulated_eval_time': 960.371298789978, 'accumulated_logging_time': 3.944474458694458}
I0810 04:54:37.732092 139692245198592 logging_writer.py:48] [8390] accumulated_eval_time=960.371299, accumulated_logging_time=3.944474, accumulated_submission_time=3419.753273, global_step=8390, preemption_count=0, score=3419.753273, test/accuracy=0.268100, test/loss=3.710765, test/num_examples=10000, total_duration=4389.628030, train/accuracy=0.371641, train/loss=3.020064, validation/accuracy=0.343760, validation/loss=3.184847, validation/num_examples=50000
I0810 04:55:22.971632 139692236805888 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.694391, loss=4.793151
I0810 04:55:22.978543 139735904175936 submission.py:120] 8500) loss = 4.793, grad_norm = 0.694
I0810 04:58:45.169330 139692245198592 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.563402, loss=5.044547
I0810 04:58:45.174613 139735904175936 submission.py:120] 9000) loss = 5.045, grad_norm = 0.563
I0810 05:01:38.030977 139735904175936 spec.py:320] Evaluating on the training split.
I0810 05:02:25.018844 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 05:03:10.952050 139735904175936 spec.py:348] Evaluating on the test split.
I0810 05:03:12.354520 139735904175936 submission_runner.py:362] Time since start: 4904.27s, 	Step: 9441, 	{'train/accuracy': 0.398984375, 'train/loss': 2.8604669189453125, 'validation/accuracy': 0.36646, 'validation/loss': 3.0260815625, 'validation/num_examples': 50000, 'test/accuracy': 0.2847, 'test/loss': 3.5651125, 'test/num_examples': 10000, 'score': 3838.762129545212, 'total_duration': 4904.271735429764, 'accumulated_submission_time': 3838.762129545212, 'accumulated_eval_time': 1054.6949138641357, 'accumulated_logging_time': 4.558684349060059}
I0810 05:03:12.374398 139692236805888 logging_writer.py:48] [9441] accumulated_eval_time=1054.694914, accumulated_logging_time=4.558684, accumulated_submission_time=3838.762130, global_step=9441, preemption_count=0, score=3838.762130, test/accuracy=0.284700, test/loss=3.565113, test/num_examples=10000, total_duration=4904.271735, train/accuracy=0.398984, train/loss=2.860467, validation/accuracy=0.366460, validation/loss=3.026082, validation/num_examples=50000
I0810 05:03:36.564803 139692245198592 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.621537, loss=4.667973
I0810 05:03:36.569322 139735904175936 submission.py:120] 9500) loss = 4.668, grad_norm = 0.622
I0810 05:07:00.753281 139692236805888 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.685260, loss=4.884323
I0810 05:07:00.767956 139735904175936 submission.py:120] 10000) loss = 4.884, grad_norm = 0.685
I0810 05:10:12.653625 139735904175936 spec.py:320] Evaluating on the training split.
I0810 05:10:57.909864 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 05:11:43.446425 139735904175936 spec.py:348] Evaluating on the test split.
I0810 05:11:44.841784 139735904175936 submission_runner.py:362] Time since start: 5416.76s, 	Step: 10484, 	{'train/accuracy': 0.449296875, 'train/loss': 2.5556100463867186, 'validation/accuracy': 0.413, 'validation/loss': 2.7363659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3216, 'test/loss': 3.307839453125, 'test/num_examples': 10000, 'score': 4257.808800458908, 'total_duration': 5416.759051322937, 'accumulated_submission_time': 4257.808800458908, 'accumulated_eval_time': 1146.8835639953613, 'accumulated_logging_time': 5.122627019882202}
I0810 05:11:44.860741 139692245198592 logging_writer.py:48] [10484] accumulated_eval_time=1146.883564, accumulated_logging_time=5.122627, accumulated_submission_time=4257.808800, global_step=10484, preemption_count=0, score=4257.808800, test/accuracy=0.321600, test/loss=3.307839, test/num_examples=10000, total_duration=5416.759051, train/accuracy=0.449297, train/loss=2.555610, validation/accuracy=0.413000, validation/loss=2.736366, validation/num_examples=50000
I0810 05:11:52.317440 139692236805888 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.691017, loss=4.539589
I0810 05:11:52.322768 139735904175936 submission.py:120] 10500) loss = 4.540, grad_norm = 0.691
I0810 05:15:11.239897 139692245198592 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.600125, loss=4.357847
I0810 05:15:11.246853 139735904175936 submission.py:120] 11000) loss = 4.358, grad_norm = 0.600
I0810 05:18:34.808670 139692236805888 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.625629, loss=4.383447
I0810 05:18:34.815822 139735904175936 submission.py:120] 11500) loss = 4.383, grad_norm = 0.626
I0810 05:18:45.030222 139735904175936 spec.py:320] Evaluating on the training split.
I0810 05:19:31.799758 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 05:20:18.838304 139735904175936 spec.py:348] Evaluating on the test split.
I0810 05:20:20.228908 139735904175936 submission_runner.py:362] Time since start: 5932.15s, 	Step: 11527, 	{'train/accuracy': 0.469296875, 'train/loss': 2.5130189514160155, 'validation/accuracy': 0.42948, 'validation/loss': 2.6997659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3356, 'test/loss': 3.2742798828125, 'test/num_examples': 10000, 'score': 4676.730163812637, 'total_duration': 5932.1459608078, 'accumulated_submission_time': 4676.730163812637, 'accumulated_eval_time': 1242.0822477340698, 'accumulated_logging_time': 5.70567512512207}
I0810 05:20:20.254768 139692245198592 logging_writer.py:48] [11527] accumulated_eval_time=1242.082248, accumulated_logging_time=5.705675, accumulated_submission_time=4676.730164, global_step=11527, preemption_count=0, score=4676.730164, test/accuracy=0.335600, test/loss=3.274280, test/num_examples=10000, total_duration=5932.145961, train/accuracy=0.469297, train/loss=2.513019, validation/accuracy=0.429480, validation/loss=2.699766, validation/num_examples=50000
I0810 05:23:27.835709 139692236805888 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.620153, loss=4.094521
I0810 05:23:27.841685 139735904175936 submission.py:120] 12000) loss = 4.095, grad_norm = 0.620
I0810 05:26:50.783589 139692245198592 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.721113, loss=4.466949
I0810 05:26:50.793112 139735904175936 submission.py:120] 12500) loss = 4.467, grad_norm = 0.721
I0810 05:27:20.535259 139735904175936 spec.py:320] Evaluating on the training split.
I0810 05:28:07.394734 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 05:28:55.179361 139735904175936 spec.py:348] Evaluating on the test split.
I0810 05:28:56.579194 139735904175936 submission_runner.py:362] Time since start: 6448.49s, 	Step: 12571, 	{'train/accuracy': 0.50107421875, 'train/loss': 2.296060791015625, 'validation/accuracy': 0.46088, 'validation/loss': 2.49200453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3564, 'test/loss': 3.106426953125, 'test/num_examples': 10000, 'score': 5095.722487688065, 'total_duration': 6448.49462890625, 'accumulated_submission_time': 5095.722487688065, 'accumulated_eval_time': 1338.1244540214539, 'accumulated_logging_time': 6.319035053253174}
I0810 05:28:56.596837 139692236805888 logging_writer.py:48] [12571] accumulated_eval_time=1338.124454, accumulated_logging_time=6.319035, accumulated_submission_time=5095.722488, global_step=12571, preemption_count=0, score=5095.722488, test/accuracy=0.356400, test/loss=3.106427, test/num_examples=10000, total_duration=6448.494629, train/accuracy=0.501074, train/loss=2.296061, validation/accuracy=0.460880, validation/loss=2.492005, validation/num_examples=50000
I0810 05:31:46.068430 139692245198592 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.657626, loss=4.269727
I0810 05:31:46.073910 139735904175936 submission.py:120] 13000) loss = 4.270, grad_norm = 0.658
I0810 05:35:04.783244 139692236805888 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.682315, loss=4.275585
I0810 05:35:04.791872 139735904175936 submission.py:120] 13500) loss = 4.276, grad_norm = 0.682
I0810 05:35:56.641820 139735904175936 spec.py:320] Evaluating on the training split.
I0810 05:36:41.864802 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 05:37:38.204010 139735904175936 spec.py:348] Evaluating on the test split.
I0810 05:37:39.606618 139735904175936 submission_runner.py:362] Time since start: 6971.52s, 	Step: 13629, 	{'train/accuracy': 0.5262890625, 'train/loss': 2.1913856506347655, 'validation/accuracy': 0.47932, 'validation/loss': 2.40763265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3767, 'test/loss': 2.977025, 'test/num_examples': 10000, 'score': 5514.524749040604, 'total_duration': 6971.523854017258, 'accumulated_submission_time': 5514.524749040604, 'accumulated_eval_time': 1441.0891845226288, 'accumulated_logging_time': 6.885109186172485}
I0810 05:37:39.624961 139692245198592 logging_writer.py:48] [13629] accumulated_eval_time=1441.089185, accumulated_logging_time=6.885109, accumulated_submission_time=5514.524749, global_step=13629, preemption_count=0, score=5514.524749, test/accuracy=0.376700, test/loss=2.977025, test/num_examples=10000, total_duration=6971.523854, train/accuracy=0.526289, train/loss=2.191386, validation/accuracy=0.479320, validation/loss=2.407633, validation/num_examples=50000
I0810 05:40:09.586244 139735904175936 spec.py:320] Evaluating on the training split.
I0810 05:40:55.386693 139735904175936 spec.py:332] Evaluating on the validation split.
I0810 05:41:41.414981 139735904175936 spec.py:348] Evaluating on the test split.
I0810 05:41:42.818733 139735904175936 submission_runner.py:362] Time since start: 7214.74s, 	Step: 14000, 	{'train/accuracy': 0.53904296875, 'train/loss': 2.1271002197265627, 'validation/accuracy': 0.49102, 'validation/loss': 2.34867609375, 'validation/num_examples': 50000, 'test/accuracy': 0.3896, 'test/loss': 2.937223828125, 'test/num_examples': 10000, 'score': 5663.719955921173, 'total_duration': 7214.735929012299, 'accumulated_submission_time': 5663.719955921173, 'accumulated_eval_time': 1534.3217194080353, 'accumulated_logging_time': 7.4243364334106445}
I0810 05:41:42.835110 139692236805888 logging_writer.py:48] [14000] accumulated_eval_time=1534.321719, accumulated_logging_time=7.424336, accumulated_submission_time=5663.719956, global_step=14000, preemption_count=0, score=5663.719956, test/accuracy=0.389600, test/loss=2.937224, test/num_examples=10000, total_duration=7214.735929, train/accuracy=0.539043, train/loss=2.127100, validation/accuracy=0.491020, validation/loss=2.348676, validation/num_examples=50000
I0810 05:41:43.418112 139692245198592 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5663.719956
I0810 05:41:44.151677 139735904175936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0810 05:41:44.524919 139735904175936 submission_runner.py:528] Tuning trial 1/1
I0810 05:41:44.525146 139735904175936 submission_runner.py:529] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0810 05:41:44.526141 139735904175936 submission_runner.py:530] Metrics: {'eval_results': [(1, {'train/accuracy': 0.002578125, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.00242, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.002, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 67.49856781959534, 'total_duration': 266.03423857688904, 'accumulated_submission_time': 67.49856781959534, 'accumulated_eval_time': 198.53520894050598, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1036, {'train/accuracy': 0.03462890625, 'train/loss': 5.95049072265625, 'validation/accuracy': 0.032, 'validation/loss': 5.9781525, 'validation/num_examples': 50000, 'test/accuracy': 0.0256, 'test/loss': 6.064193359375, 'test/num_examples': 10000, 'score': 487.0942919254303, 'total_duration': 789.0410706996918, 'accumulated_submission_time': 487.0942919254303, 'accumulated_eval_time': 301.2265577316284, 'accumulated_logging_time': 0.03258657455444336, 'global_step': 1036, 'preemption_count': 0}), (2092, {'train/accuracy': 0.08234375, 'train/loss': 5.27327880859375, 'validation/accuracy': 0.07774, 'validation/loss': 5.318429375, 'validation/num_examples': 50000, 'test/accuracy': 0.0574, 'test/loss': 5.549546875, 'test/num_examples': 10000, 'score': 905.9109473228455, 'total_duration': 1300.5085935592651, 'accumulated_submission_time': 905.9109473228455, 'accumulated_eval_time': 392.6334009170532, 'accumulated_logging_time': 0.5851695537567139, 'global_step': 2092, 'preemption_count': 0}), (3134, {'train/accuracy': 0.14150390625, 'train/loss': 4.688422546386719, 'validation/accuracy': 0.13, 'validation/loss': 4.7481584375, 'validation/num_examples': 50000, 'test/accuracy': 0.0989, 'test/loss': 5.04826484375, 'test/num_examples': 10000, 'score': 1324.8571407794952, 'total_duration': 1811.8299634456635, 'accumulated_submission_time': 1324.8571407794952, 'accumulated_eval_time': 483.767826795578, 'accumulated_logging_time': 1.1294574737548828, 'global_step': 3134, 'preemption_count': 0}), (4187, {'train/accuracy': 0.193515625, 'train/loss': 4.262715148925781, 'validation/accuracy': 0.18148, 'validation/loss': 4.3486190625, 'validation/num_examples': 50000, 'test/accuracy': 0.138, 'test/loss': 4.70346328125, 'test/num_examples': 10000, 'score': 1743.64759683609, 'total_duration': 2327.43332695961, 'accumulated_submission_time': 1743.64759683609, 'accumulated_eval_time': 579.3476147651672, 'accumulated_logging_time': 1.6668717861175537, 'global_step': 4187, 'preemption_count': 0}), (5237, {'train/accuracy': 0.2476171875, 'train/loss': 3.83951904296875, 'validation/accuracy': 0.22886, 'validation/loss': 3.946846875, 'validation/num_examples': 50000, 'test/accuracy': 0.1738, 'test/loss': 4.37698203125, 'test/num_examples': 10000, 'score': 2162.7474913597107, 'total_duration': 2838.742928981781, 'accumulated_submission_time': 2162.7474913597107, 'accumulated_eval_time': 670.2998406887054, 'accumulated_logging_time': 2.224851608276367, 'global_step': 5237, 'preemption_count': 0}), (6280, {'train/accuracy': 0.28974609375, 'train/loss': 3.5317092895507813, 'validation/accuracy': 0.26546, 'validation/loss': 3.661364375, 'validation/num_examples': 50000, 'test/accuracy': 0.206, 'test/loss': 4.09978203125, 'test/num_examples': 10000, 'score': 2581.8367042541504, 'total_duration': 3362.8643465042114, 'accumulated_submission_time': 2581.8367042541504, 'accumulated_eval_time': 774.0936694145203, 'accumulated_logging_time': 2.773371934890747, 'global_step': 6280, 'preemption_count': 0}), (7336, {'train/accuracy': 0.33802734375, 'train/loss': 3.2290850830078126, 'validation/accuracy': 0.31188, 'validation/loss': 3.3718340625, 'validation/num_examples': 50000, 'test/accuracy': 0.2427, 'test/loss': 3.85165078125, 'test/num_examples': 10000, 'score': 3000.8093717098236, 'total_duration': 3876.894100189209, 'accumulated_submission_time': 3000.8093717098236, 'accumulated_eval_time': 867.8691403865814, 'accumulated_logging_time': 3.3547871112823486, 'global_step': 7336, 'preemption_count': 0}), (8390, {'train/accuracy': 0.371640625, 'train/loss': 3.020064392089844, 'validation/accuracy': 0.34376, 'validation/loss': 3.184846875, 'validation/num_examples': 50000, 'test/accuracy': 0.2681, 'test/loss': 3.71076484375, 'test/num_examples': 10000, 'score': 3419.7532732486725, 'total_duration': 4389.628029584885, 'accumulated_submission_time': 3419.7532732486725, 'accumulated_eval_time': 960.371298789978, 'accumulated_logging_time': 3.944474458694458, 'global_step': 8390, 'preemption_count': 0}), (9441, {'train/accuracy': 0.398984375, 'train/loss': 2.8604669189453125, 'validation/accuracy': 0.36646, 'validation/loss': 3.0260815625, 'validation/num_examples': 50000, 'test/accuracy': 0.2847, 'test/loss': 3.5651125, 'test/num_examples': 10000, 'score': 3838.762129545212, 'total_duration': 4904.271735429764, 'accumulated_submission_time': 3838.762129545212, 'accumulated_eval_time': 1054.6949138641357, 'accumulated_logging_time': 4.558684349060059, 'global_step': 9441, 'preemption_count': 0}), (10484, {'train/accuracy': 0.449296875, 'train/loss': 2.5556100463867186, 'validation/accuracy': 0.413, 'validation/loss': 2.7363659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3216, 'test/loss': 3.307839453125, 'test/num_examples': 10000, 'score': 4257.808800458908, 'total_duration': 5416.759051322937, 'accumulated_submission_time': 4257.808800458908, 'accumulated_eval_time': 1146.8835639953613, 'accumulated_logging_time': 5.122627019882202, 'global_step': 10484, 'preemption_count': 0}), (11527, {'train/accuracy': 0.469296875, 'train/loss': 2.5130189514160155, 'validation/accuracy': 0.42948, 'validation/loss': 2.6997659375, 'validation/num_examples': 50000, 'test/accuracy': 0.3356, 'test/loss': 3.2742798828125, 'test/num_examples': 10000, 'score': 4676.730163812637, 'total_duration': 5932.1459608078, 'accumulated_submission_time': 4676.730163812637, 'accumulated_eval_time': 1242.0822477340698, 'accumulated_logging_time': 5.70567512512207, 'global_step': 11527, 'preemption_count': 0}), (12571, {'train/accuracy': 0.50107421875, 'train/loss': 2.296060791015625, 'validation/accuracy': 0.46088, 'validation/loss': 2.49200453125, 'validation/num_examples': 50000, 'test/accuracy': 0.3564, 'test/loss': 3.106426953125, 'test/num_examples': 10000, 'score': 5095.722487688065, 'total_duration': 6448.49462890625, 'accumulated_submission_time': 5095.722487688065, 'accumulated_eval_time': 1338.1244540214539, 'accumulated_logging_time': 6.319035053253174, 'global_step': 12571, 'preemption_count': 0}), (13629, {'train/accuracy': 0.5262890625, 'train/loss': 2.1913856506347655, 'validation/accuracy': 0.47932, 'validation/loss': 2.40763265625, 'validation/num_examples': 50000, 'test/accuracy': 0.3767, 'test/loss': 2.977025, 'test/num_examples': 10000, 'score': 5514.524749040604, 'total_duration': 6971.523854017258, 'accumulated_submission_time': 5514.524749040604, 'accumulated_eval_time': 1441.0891845226288, 'accumulated_logging_time': 6.885109186172485, 'global_step': 13629, 'preemption_count': 0}), (14000, {'train/accuracy': 0.53904296875, 'train/loss': 2.1271002197265627, 'validation/accuracy': 0.49102, 'validation/loss': 2.34867609375, 'validation/num_examples': 50000, 'test/accuracy': 0.3896, 'test/loss': 2.937223828125, 'test/num_examples': 10000, 'score': 5663.719955921173, 'total_duration': 7214.735929012299, 'accumulated_submission_time': 5663.719955921173, 'accumulated_eval_time': 1534.3217194080353, 'accumulated_logging_time': 7.4243364334106445, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0810 05:41:44.526256 139735904175936 submission_runner.py:531] Timing: 5663.719955921173
I0810 05:41:44.526305 139735904175936 submission_runner.py:533] Total number of evals: 15
I0810 05:41:44.526369 139735904175936 submission_runner.py:534] ====================
I0810 05:41:44.526520 139735904175936 submission_runner.py:602] Final imagenet_vit score: 5663.719955921173
