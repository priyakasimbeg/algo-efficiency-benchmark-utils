torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_2/adamw --overwrite=true --save_checkpoints=false --max_global_steps=10000 --torch_compile=True 2>&1 | tee -a /logs/wmt_pytorch_08-10-2023-07-00-05.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-10 07:00:15.677975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 07:00:15.678078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0810 07:00:30.349607 140588010506048 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0810 07:00:30.349651 139867582850880 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0810 07:00:30.349662 140393539254080 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0810 07:00:30.349803 140576940013376 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0810 07:00:30.350936 139632913553216 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0810 07:00:30.350957 140491808188224 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0810 07:00:30.351202 139632913553216 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.351221 140491808188224 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.351083 140085202220864 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0810 07:00:30.351232 140065023153984 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0810 07:00:30.351486 140065023153984 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.351475 140085202220864 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.360404 140393539254080 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.360434 140588010506048 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.360455 139867582850880 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:30.360483 140576940013376 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 07:00:35.169259 140065023153984 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/wmt_pytorch.
W0810 07:00:35.202966 140491808188224 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.203052 140085202220864 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.203238 140576940013376 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.204065 140588010506048 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.204150 139632913553216 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.204290 139867582850880 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.204338 140393539254080 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 07:00:35.206707 140065023153984 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 07:00:35.211886 140065023153984 submission_runner.py:488] Using RNG seed 3345879980
I0810 07:00:35.213466 140065023153984 submission_runner.py:497] --- Tuning run 1/1 ---
I0810 07:00:35.213594 140065023153984 submission_runner.py:502] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/wmt_pytorch/trial_1.
I0810 07:00:35.214075 140065023153984 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/wmt_pytorch/trial_1/hparams.json.
I0810 07:00:35.214998 140065023153984 submission_runner.py:176] Initializing dataset.
I0810 07:00:35.215139 140065023153984 submission_runner.py:183] Initializing model.
W0810 07:00:38.734786 139867582850880 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.734931 140393539254080 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.735004 140576940013376 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.735070 140491808188224 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.735171 140085202220864 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.735230 140588010506048 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.735401 139632913553216 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0810 07:00:38.735628 140065023153984 submission_runner.py:198] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0810 07:00:38.735877 140065023153984 submission_runner.py:215] Initializing optimizer.
I0810 07:00:38.737325 140065023153984 submission_runner.py:222] Initializing metrics bundle.
I0810 07:00:38.737445 140065023153984 submission_runner.py:240] Initializing checkpoint and logger.
I0810 07:00:38.738423 140065023153984 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0810 07:00:38.738567 140065023153984 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0810 07:00:39.291159 140065023153984 submission_runner.py:261] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/wmt_pytorch/trial_1/meta_data_0.json.
I0810 07:00:39.292132 140065023153984 submission_runner.py:264] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/wmt_pytorch/trial_1/flags_0.json.
I0810 07:00:39.385140 140065023153984 submission_runner.py:274] Starting training loop.
I0810 07:00:39.400245 140065023153984 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0810 07:00:39.404589 140065023153984 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0810 07:00:39.484423 140065023153984 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0810 07:00:43.640757 140017729001216 logging_writer.py:48] [0] global_step=0, grad_norm=5.683928, loss=11.028594
I0810 07:00:43.651383 140065023153984 submission.py:120] 0) loss = 11.029, grad_norm = 5.684
I0810 07:00:43.652818 140065023153984 spec.py:320] Evaluating on the training split.
I0810 07:00:43.655288 140065023153984 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0810 07:00:43.658525 140065023153984 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0810 07:00:43.695854 140065023153984 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0810 07:00:47.895930 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 07:05:32.814903 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 07:05:32.818211 140065023153984 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0810 07:05:32.821965 140065023153984 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0810 07:05:32.858526 140065023153984 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0810 07:05:36.728868 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 07:10:17.216691 140065023153984 spec.py:348] Evaluating on the test split.
I0810 07:10:17.219353 140065023153984 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0810 07:10:17.222992 140065023153984 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0810 07:10:17.259823 140065023153984 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0810 07:10:21.216997 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 07:15:06.016739 140065023153984 submission_runner.py:362] Time since start: 866.63s, 	Step: 1, 	{'train/accuracy': 0.000638060296698038, 'train/loss': 11.017881354966615, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.005854701119638, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.026332723258381, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.2677836418151855, 'total_duration': 866.6320533752441, 'accumulated_submission_time': 4.2677836418151855, 'accumulated_eval_time': 862.3638000488281, 'accumulated_logging_time': 0}
I0810 07:15:06.039377 140006680004352 logging_writer.py:48] [1] accumulated_eval_time=862.363800, accumulated_logging_time=0, accumulated_submission_time=4.267784, global_step=1, preemption_count=0, score=4.267784, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.026333, test/num_examples=3003, total_duration=866.632053, train/accuracy=0.000638, train/bleu=0.000000, train/loss=11.017881, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.005855, validation/num_examples=3000
I0810 07:15:06.324906 140065023153984 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.324905 139632913553216 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.324961 140588010506048 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.324977 140576940013376 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.324940 139867582850880 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.325031 140393539254080 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.325036 140491808188224 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:06.325391 140085202220864 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 07:15:07.116162 140006671611648 logging_writer.py:48] [1] global_step=1, grad_norm=5.575920, loss=11.022713
I0810 07:15:07.119861 140065023153984 submission.py:120] 1) loss = 11.023, grad_norm = 5.576
I0810 07:15:07.564181 140006680004352 logging_writer.py:48] [2] global_step=2, grad_norm=5.622807, loss=11.009809
I0810 07:15:07.567646 140065023153984 submission.py:120] 2) loss = 11.010, grad_norm = 5.623
I0810 07:15:08.009781 140006671611648 logging_writer.py:48] [3] global_step=3, grad_norm=5.529409, loss=10.997403
I0810 07:15:08.013357 140065023153984 submission.py:120] 3) loss = 10.997, grad_norm = 5.529
I0810 07:15:08.455040 140006680004352 logging_writer.py:48] [4] global_step=4, grad_norm=5.489195, loss=10.987454
I0810 07:15:08.458928 140065023153984 submission.py:120] 4) loss = 10.987, grad_norm = 5.489
I0810 07:15:08.901406 140006671611648 logging_writer.py:48] [5] global_step=5, grad_norm=5.390344, loss=10.952185
I0810 07:15:08.904698 140065023153984 submission.py:120] 5) loss = 10.952, grad_norm = 5.390
I0810 07:15:09.353347 140006680004352 logging_writer.py:48] [6] global_step=6, grad_norm=5.333482, loss=10.904350
I0810 07:15:09.356948 140065023153984 submission.py:120] 6) loss = 10.904, grad_norm = 5.333
I0810 07:15:09.796908 140006671611648 logging_writer.py:48] [7] global_step=7, grad_norm=5.147633, loss=10.862002
I0810 07:15:09.800202 140065023153984 submission.py:120] 7) loss = 10.862, grad_norm = 5.148
I0810 07:15:10.244759 140006680004352 logging_writer.py:48] [8] global_step=8, grad_norm=5.069372, loss=10.801391
I0810 07:15:10.248316 140065023153984 submission.py:120] 8) loss = 10.801, grad_norm = 5.069
I0810 07:15:10.688777 140006671611648 logging_writer.py:48] [9] global_step=9, grad_norm=4.902057, loss=10.750361
I0810 07:15:10.692167 140065023153984 submission.py:120] 9) loss = 10.750, grad_norm = 4.902
I0810 07:15:11.133991 140006680004352 logging_writer.py:48] [10] global_step=10, grad_norm=4.721019, loss=10.685763
I0810 07:15:11.138226 140065023153984 submission.py:120] 10) loss = 10.686, grad_norm = 4.721
I0810 07:15:11.581911 140006671611648 logging_writer.py:48] [11] global_step=11, grad_norm=4.510962, loss=10.633053
I0810 07:15:11.586040 140065023153984 submission.py:120] 11) loss = 10.633, grad_norm = 4.511
I0810 07:15:12.025924 140006680004352 logging_writer.py:48] [12] global_step=12, grad_norm=4.317156, loss=10.552754
I0810 07:15:12.029856 140065023153984 submission.py:120] 12) loss = 10.553, grad_norm = 4.317
I0810 07:15:12.472942 140006671611648 logging_writer.py:48] [13] global_step=13, grad_norm=4.110009, loss=10.485566
I0810 07:15:12.477026 140065023153984 submission.py:120] 13) loss = 10.486, grad_norm = 4.110
I0810 07:15:12.918836 140006680004352 logging_writer.py:48] [14] global_step=14, grad_norm=3.890511, loss=10.407025
I0810 07:15:12.923079 140065023153984 submission.py:120] 14) loss = 10.407, grad_norm = 3.891
I0810 07:15:13.372482 140006671611648 logging_writer.py:48] [15] global_step=15, grad_norm=3.625993, loss=10.351405
I0810 07:15:13.376585 140065023153984 submission.py:120] 15) loss = 10.351, grad_norm = 3.626
I0810 07:15:13.819262 140006680004352 logging_writer.py:48] [16] global_step=16, grad_norm=3.429889, loss=10.273111
I0810 07:15:13.823306 140065023153984 submission.py:120] 16) loss = 10.273, grad_norm = 3.430
I0810 07:15:14.266225 140006671611648 logging_writer.py:48] [17] global_step=17, grad_norm=3.218123, loss=10.205391
I0810 07:15:14.270284 140065023153984 submission.py:120] 17) loss = 10.205, grad_norm = 3.218
I0810 07:15:14.709710 140006680004352 logging_writer.py:48] [18] global_step=18, grad_norm=2.995783, loss=10.125062
I0810 07:15:14.713758 140065023153984 submission.py:120] 18) loss = 10.125, grad_norm = 2.996
I0810 07:15:15.153574 140006671611648 logging_writer.py:48] [19] global_step=19, grad_norm=2.796306, loss=10.077640
I0810 07:15:15.157613 140065023153984 submission.py:120] 19) loss = 10.078, grad_norm = 2.796
I0810 07:15:15.598149 140006680004352 logging_writer.py:48] [20] global_step=20, grad_norm=2.621625, loss=10.012839
I0810 07:15:15.602218 140065023153984 submission.py:120] 20) loss = 10.013, grad_norm = 2.622
I0810 07:15:16.041725 140006671611648 logging_writer.py:48] [21] global_step=21, grad_norm=2.478368, loss=9.933081
I0810 07:15:16.045039 140065023153984 submission.py:120] 21) loss = 9.933, grad_norm = 2.478
I0810 07:15:16.486801 140006680004352 logging_writer.py:48] [22] global_step=22, grad_norm=2.304147, loss=9.887239
I0810 07:15:16.490232 140065023153984 submission.py:120] 22) loss = 9.887, grad_norm = 2.304
I0810 07:15:16.933938 140006671611648 logging_writer.py:48] [23] global_step=23, grad_norm=2.159740, loss=9.822504
I0810 07:15:16.938199 140065023153984 submission.py:120] 23) loss = 9.823, grad_norm = 2.160
I0810 07:15:17.379185 140006680004352 logging_writer.py:48] [24] global_step=24, grad_norm=2.000098, loss=9.761152
I0810 07:15:17.383387 140065023153984 submission.py:120] 24) loss = 9.761, grad_norm = 2.000
I0810 07:15:17.826235 140006671611648 logging_writer.py:48] [25] global_step=25, grad_norm=1.860774, loss=9.716208
I0810 07:15:17.830356 140065023153984 submission.py:120] 25) loss = 9.716, grad_norm = 1.861
I0810 07:15:18.283658 140006680004352 logging_writer.py:48] [26] global_step=26, grad_norm=1.765898, loss=9.652917
I0810 07:15:18.287663 140065023153984 submission.py:120] 26) loss = 9.653, grad_norm = 1.766
I0810 07:15:18.727965 140006671611648 logging_writer.py:48] [27] global_step=27, grad_norm=1.649852, loss=9.617185
I0810 07:15:18.732014 140065023153984 submission.py:120] 27) loss = 9.617, grad_norm = 1.650
I0810 07:15:19.174665 140006680004352 logging_writer.py:48] [28] global_step=28, grad_norm=1.536582, loss=9.569769
I0810 07:15:19.178290 140065023153984 submission.py:120] 28) loss = 9.570, grad_norm = 1.537
I0810 07:15:19.618596 140006671611648 logging_writer.py:48] [29] global_step=29, grad_norm=1.445975, loss=9.524673
I0810 07:15:19.622206 140065023153984 submission.py:120] 29) loss = 9.525, grad_norm = 1.446
I0810 07:15:20.061110 140006680004352 logging_writer.py:48] [30] global_step=30, grad_norm=1.370878, loss=9.475364
I0810 07:15:20.064852 140065023153984 submission.py:120] 30) loss = 9.475, grad_norm = 1.371
I0810 07:15:20.504178 140006671611648 logging_writer.py:48] [31] global_step=31, grad_norm=1.296846, loss=9.429598
I0810 07:15:20.507585 140065023153984 submission.py:120] 31) loss = 9.430, grad_norm = 1.297
I0810 07:15:20.948756 140006680004352 logging_writer.py:48] [32] global_step=32, grad_norm=1.198125, loss=9.402852
I0810 07:15:20.952346 140065023153984 submission.py:120] 32) loss = 9.403, grad_norm = 1.198
I0810 07:15:21.390695 140006671611648 logging_writer.py:48] [33] global_step=33, grad_norm=1.146941, loss=9.371008
I0810 07:15:21.394107 140065023153984 submission.py:120] 33) loss = 9.371, grad_norm = 1.147
I0810 07:15:21.838744 140006680004352 logging_writer.py:48] [34] global_step=34, grad_norm=1.073778, loss=9.347966
I0810 07:15:21.842098 140065023153984 submission.py:120] 34) loss = 9.348, grad_norm = 1.074
I0810 07:15:22.293754 140006671611648 logging_writer.py:48] [35] global_step=35, grad_norm=1.001877, loss=9.316458
I0810 07:15:22.297153 140065023153984 submission.py:120] 35) loss = 9.316, grad_norm = 1.002
I0810 07:15:22.737023 140006680004352 logging_writer.py:48] [36] global_step=36, grad_norm=0.948639, loss=9.297603
I0810 07:15:22.740511 140065023153984 submission.py:120] 36) loss = 9.298, grad_norm = 0.949
I0810 07:15:23.181802 140006671611648 logging_writer.py:48] [37] global_step=37, grad_norm=0.896066, loss=9.287796
I0810 07:15:23.185379 140065023153984 submission.py:120] 37) loss = 9.288, grad_norm = 0.896
I0810 07:15:23.630316 140006680004352 logging_writer.py:48] [38] global_step=38, grad_norm=0.860339, loss=9.240913
I0810 07:15:23.634166 140065023153984 submission.py:120] 38) loss = 9.241, grad_norm = 0.860
I0810 07:15:24.075276 140006671611648 logging_writer.py:48] [39] global_step=39, grad_norm=0.829016, loss=9.199550
I0810 07:15:24.079086 140065023153984 submission.py:120] 39) loss = 9.200, grad_norm = 0.829
I0810 07:15:24.520756 140006680004352 logging_writer.py:48] [40] global_step=40, grad_norm=0.777722, loss=9.202287
I0810 07:15:24.524376 140065023153984 submission.py:120] 40) loss = 9.202, grad_norm = 0.778
I0810 07:15:24.966242 140006671611648 logging_writer.py:48] [41] global_step=41, grad_norm=0.746779, loss=9.169961
I0810 07:15:24.969904 140065023153984 submission.py:120] 41) loss = 9.170, grad_norm = 0.747
I0810 07:15:25.410526 140006680004352 logging_writer.py:48] [42] global_step=42, grad_norm=0.716041, loss=9.119242
I0810 07:15:25.414425 140065023153984 submission.py:120] 42) loss = 9.119, grad_norm = 0.716
I0810 07:15:25.857833 140006671611648 logging_writer.py:48] [43] global_step=43, grad_norm=0.687193, loss=9.095926
I0810 07:15:25.861701 140065023153984 submission.py:120] 43) loss = 9.096, grad_norm = 0.687
I0810 07:15:26.305751 140006680004352 logging_writer.py:48] [44] global_step=44, grad_norm=0.653741, loss=9.098141
I0810 07:15:26.309771 140065023153984 submission.py:120] 44) loss = 9.098, grad_norm = 0.654
I0810 07:15:26.749578 140006671611648 logging_writer.py:48] [45] global_step=45, grad_norm=0.617649, loss=9.056603
I0810 07:15:26.753664 140065023153984 submission.py:120] 45) loss = 9.057, grad_norm = 0.618
I0810 07:15:27.197290 140006680004352 logging_writer.py:48] [46] global_step=46, grad_norm=0.599019, loss=9.028468
I0810 07:15:27.201475 140065023153984 submission.py:120] 46) loss = 9.028, grad_norm = 0.599
I0810 07:15:27.641913 140006671611648 logging_writer.py:48] [47] global_step=47, grad_norm=0.567875, loss=9.050540
I0810 07:15:27.645930 140065023153984 submission.py:120] 47) loss = 9.051, grad_norm = 0.568
I0810 07:15:28.086191 140006680004352 logging_writer.py:48] [48] global_step=48, grad_norm=0.542529, loss=9.023123
I0810 07:15:28.090447 140065023153984 submission.py:120] 48) loss = 9.023, grad_norm = 0.543
I0810 07:15:28.532894 140006671611648 logging_writer.py:48] [49] global_step=49, grad_norm=0.526797, loss=8.980461
I0810 07:15:28.536937 140065023153984 submission.py:120] 49) loss = 8.980, grad_norm = 0.527
I0810 07:15:28.975874 140006680004352 logging_writer.py:48] [50] global_step=50, grad_norm=0.513414, loss=8.969501
I0810 07:15:28.979370 140065023153984 submission.py:120] 50) loss = 8.970, grad_norm = 0.513
I0810 07:15:29.417290 140006671611648 logging_writer.py:48] [51] global_step=51, grad_norm=0.491562, loss=8.968021
I0810 07:15:29.420634 140065023153984 submission.py:120] 51) loss = 8.968, grad_norm = 0.492
I0810 07:15:29.862218 140006680004352 logging_writer.py:48] [52] global_step=52, grad_norm=0.465725, loss=8.976593
I0810 07:15:29.866103 140065023153984 submission.py:120] 52) loss = 8.977, grad_norm = 0.466
I0810 07:15:30.308348 140006671611648 logging_writer.py:48] [53] global_step=53, grad_norm=0.445227, loss=8.946680
I0810 07:15:30.312410 140065023153984 submission.py:120] 53) loss = 8.947, grad_norm = 0.445
I0810 07:15:30.759397 140006680004352 logging_writer.py:48] [54] global_step=54, grad_norm=0.438732, loss=8.946007
I0810 07:15:30.762825 140065023153984 submission.py:120] 54) loss = 8.946, grad_norm = 0.439
I0810 07:15:31.206072 140006671611648 logging_writer.py:48] [55] global_step=55, grad_norm=0.409505, loss=8.944264
I0810 07:15:31.209907 140065023153984 submission.py:120] 55) loss = 8.944, grad_norm = 0.410
I0810 07:15:31.648799 140006680004352 logging_writer.py:48] [56] global_step=56, grad_norm=0.403212, loss=8.894953
I0810 07:15:31.652857 140065023153984 submission.py:120] 56) loss = 8.895, grad_norm = 0.403
I0810 07:15:32.095232 140006671611648 logging_writer.py:48] [57] global_step=57, grad_norm=0.379159, loss=8.888298
I0810 07:15:32.099329 140065023153984 submission.py:120] 57) loss = 8.888, grad_norm = 0.379
I0810 07:15:32.543595 140006680004352 logging_writer.py:48] [58] global_step=58, grad_norm=0.368055, loss=8.879707
I0810 07:15:32.547598 140065023153984 submission.py:120] 58) loss = 8.880, grad_norm = 0.368
I0810 07:15:32.987267 140006671611648 logging_writer.py:48] [59] global_step=59, grad_norm=0.360208, loss=8.886583
I0810 07:15:32.991434 140065023153984 submission.py:120] 59) loss = 8.887, grad_norm = 0.360
I0810 07:15:33.435070 140006680004352 logging_writer.py:48] [60] global_step=60, grad_norm=0.350467, loss=8.859418
I0810 07:15:33.439087 140065023153984 submission.py:120] 60) loss = 8.859, grad_norm = 0.350
I0810 07:15:33.884825 140006671611648 logging_writer.py:48] [61] global_step=61, grad_norm=0.334911, loss=8.886943
I0810 07:15:33.888653 140065023153984 submission.py:120] 61) loss = 8.887, grad_norm = 0.335
I0810 07:15:34.331581 140006680004352 logging_writer.py:48] [62] global_step=62, grad_norm=0.325302, loss=8.849787
I0810 07:15:34.334973 140065023153984 submission.py:120] 62) loss = 8.850, grad_norm = 0.325
I0810 07:15:34.792150 140006671611648 logging_writer.py:48] [63] global_step=63, grad_norm=0.328267, loss=8.849449
I0810 07:15:34.795543 140065023153984 submission.py:120] 63) loss = 8.849, grad_norm = 0.328
I0810 07:15:35.241075 140006680004352 logging_writer.py:48] [64] global_step=64, grad_norm=0.311409, loss=8.832947
I0810 07:15:35.244779 140065023153984 submission.py:120] 64) loss = 8.833, grad_norm = 0.311
I0810 07:15:35.686099 140006671611648 logging_writer.py:48] [65] global_step=65, grad_norm=0.300954, loss=8.841133
I0810 07:15:35.689637 140065023153984 submission.py:120] 65) loss = 8.841, grad_norm = 0.301
I0810 07:15:36.133134 140006680004352 logging_writer.py:48] [66] global_step=66, grad_norm=0.302736, loss=8.809730
I0810 07:15:36.136716 140065023153984 submission.py:120] 66) loss = 8.810, grad_norm = 0.303
I0810 07:15:36.580690 140006671611648 logging_writer.py:48] [67] global_step=67, grad_norm=0.291951, loss=8.777024
I0810 07:15:36.583926 140065023153984 submission.py:120] 67) loss = 8.777, grad_norm = 0.292
I0810 07:15:37.027155 140006680004352 logging_writer.py:48] [68] global_step=68, grad_norm=0.289810, loss=8.764225
I0810 07:15:37.030445 140065023153984 submission.py:120] 68) loss = 8.764, grad_norm = 0.290
I0810 07:15:37.475753 140006671611648 logging_writer.py:48] [69] global_step=69, grad_norm=0.292531, loss=8.824171
I0810 07:15:37.479085 140065023153984 submission.py:120] 69) loss = 8.824, grad_norm = 0.293
I0810 07:15:37.924499 140006680004352 logging_writer.py:48] [70] global_step=70, grad_norm=0.272727, loss=8.749718
I0810 07:15:37.927803 140065023153984 submission.py:120] 70) loss = 8.750, grad_norm = 0.273
I0810 07:15:38.376307 140006671611648 logging_writer.py:48] [71] global_step=71, grad_norm=0.272654, loss=8.791658
I0810 07:15:38.379860 140065023153984 submission.py:120] 71) loss = 8.792, grad_norm = 0.273
I0810 07:15:38.824089 140006680004352 logging_writer.py:48] [72] global_step=72, grad_norm=0.266344, loss=8.778934
I0810 07:15:38.827311 140065023153984 submission.py:120] 72) loss = 8.779, grad_norm = 0.266
I0810 07:15:39.273149 140006671611648 logging_writer.py:48] [73] global_step=73, grad_norm=0.255895, loss=8.755816
I0810 07:15:39.276632 140065023153984 submission.py:120] 73) loss = 8.756, grad_norm = 0.256
I0810 07:15:39.719055 140006680004352 logging_writer.py:48] [74] global_step=74, grad_norm=0.266072, loss=8.718549
I0810 07:15:39.723224 140065023153984 submission.py:120] 74) loss = 8.719, grad_norm = 0.266
I0810 07:15:40.165561 140006671611648 logging_writer.py:48] [75] global_step=75, grad_norm=0.260049, loss=8.756397
I0810 07:15:40.169584 140065023153984 submission.py:120] 75) loss = 8.756, grad_norm = 0.260
I0810 07:15:40.615417 140006680004352 logging_writer.py:48] [76] global_step=76, grad_norm=0.244117, loss=8.719450
I0810 07:15:40.619383 140065023153984 submission.py:120] 76) loss = 8.719, grad_norm = 0.244
I0810 07:15:41.058983 140006671611648 logging_writer.py:48] [77] global_step=77, grad_norm=0.246481, loss=8.749283
I0810 07:15:41.063232 140065023153984 submission.py:120] 77) loss = 8.749, grad_norm = 0.246
I0810 07:15:41.503771 140006680004352 logging_writer.py:48] [78] global_step=78, grad_norm=0.262534, loss=8.687861
I0810 07:15:41.507931 140065023153984 submission.py:120] 78) loss = 8.688, grad_norm = 0.263
I0810 07:15:41.952619 140006671611648 logging_writer.py:48] [79] global_step=79, grad_norm=0.234967, loss=8.712746
I0810 07:15:41.956679 140065023153984 submission.py:120] 79) loss = 8.713, grad_norm = 0.235
I0810 07:15:42.398336 140006680004352 logging_writer.py:48] [80] global_step=80, grad_norm=0.244144, loss=8.770206
I0810 07:15:42.402350 140065023153984 submission.py:120] 80) loss = 8.770, grad_norm = 0.244
I0810 07:15:42.847662 140006671611648 logging_writer.py:48] [81] global_step=81, grad_norm=0.249459, loss=8.700356
I0810 07:15:42.851368 140065023153984 submission.py:120] 81) loss = 8.700, grad_norm = 0.249
I0810 07:15:43.294276 140006680004352 logging_writer.py:48] [82] global_step=82, grad_norm=0.226603, loss=8.699780
I0810 07:15:43.297891 140065023153984 submission.py:120] 82) loss = 8.700, grad_norm = 0.227
I0810 07:15:43.740500 140006671611648 logging_writer.py:48] [83] global_step=83, grad_norm=0.243974, loss=8.678624
I0810 07:15:43.743931 140065023153984 submission.py:120] 83) loss = 8.679, grad_norm = 0.244
I0810 07:15:44.189411 140006680004352 logging_writer.py:48] [84] global_step=84, grad_norm=0.231754, loss=8.703045
I0810 07:15:44.192914 140065023153984 submission.py:120] 84) loss = 8.703, grad_norm = 0.232
I0810 07:15:44.637041 140006671611648 logging_writer.py:48] [85] global_step=85, grad_norm=0.250867, loss=8.688148
I0810 07:15:44.640868 140065023153984 submission.py:120] 85) loss = 8.688, grad_norm = 0.251
I0810 07:15:45.091077 140006680004352 logging_writer.py:48] [86] global_step=86, grad_norm=0.244927, loss=8.657652
I0810 07:15:45.094649 140065023153984 submission.py:120] 86) loss = 8.658, grad_norm = 0.245
I0810 07:15:45.535916 140006671611648 logging_writer.py:48] [87] global_step=87, grad_norm=0.245900, loss=8.652124
I0810 07:15:45.539532 140065023153984 submission.py:120] 87) loss = 8.652, grad_norm = 0.246
I0810 07:15:45.983707 140006680004352 logging_writer.py:48] [88] global_step=88, grad_norm=0.231173, loss=8.644904
I0810 07:15:45.987314 140065023153984 submission.py:120] 88) loss = 8.645, grad_norm = 0.231
I0810 07:15:46.428280 140006671611648 logging_writer.py:48] [89] global_step=89, grad_norm=0.228716, loss=8.627735
I0810 07:15:46.432002 140065023153984 submission.py:120] 89) loss = 8.628, grad_norm = 0.229
I0810 07:15:46.877379 140006680004352 logging_writer.py:48] [90] global_step=90, grad_norm=0.233428, loss=8.646604
I0810 07:15:46.880984 140065023153984 submission.py:120] 90) loss = 8.647, grad_norm = 0.233
I0810 07:15:47.327585 140006671611648 logging_writer.py:48] [91] global_step=91, grad_norm=0.253482, loss=8.681833
I0810 07:15:47.331343 140065023153984 submission.py:120] 91) loss = 8.682, grad_norm = 0.253
I0810 07:15:47.774146 140006680004352 logging_writer.py:48] [92] global_step=92, grad_norm=0.222680, loss=8.614757
I0810 07:15:47.777738 140065023153984 submission.py:120] 92) loss = 8.615, grad_norm = 0.223
I0810 07:15:48.223438 140006671611648 logging_writer.py:48] [93] global_step=93, grad_norm=0.242301, loss=8.639266
I0810 07:15:48.227996 140065023153984 submission.py:120] 93) loss = 8.639, grad_norm = 0.242
I0810 07:15:48.673004 140006680004352 logging_writer.py:48] [94] global_step=94, grad_norm=0.220112, loss=8.635926
I0810 07:15:48.677087 140065023153984 submission.py:120] 94) loss = 8.636, grad_norm = 0.220
I0810 07:15:49.122022 140006671611648 logging_writer.py:48] [95] global_step=95, grad_norm=0.238694, loss=8.604860
I0810 07:15:49.125996 140065023153984 submission.py:120] 95) loss = 8.605, grad_norm = 0.239
I0810 07:15:49.569842 140006680004352 logging_writer.py:48] [96] global_step=96, grad_norm=0.207575, loss=8.628262
I0810 07:15:49.574011 140065023153984 submission.py:120] 96) loss = 8.628, grad_norm = 0.208
I0810 07:15:50.020897 140006671611648 logging_writer.py:48] [97] global_step=97, grad_norm=0.240516, loss=8.616439
I0810 07:15:50.025269 140065023153984 submission.py:120] 97) loss = 8.616, grad_norm = 0.241
I0810 07:15:50.474687 140006680004352 logging_writer.py:48] [98] global_step=98, grad_norm=0.246047, loss=8.602829
I0810 07:15:50.478910 140065023153984 submission.py:120] 98) loss = 8.603, grad_norm = 0.246
I0810 07:15:50.931051 140006671611648 logging_writer.py:48] [99] global_step=99, grad_norm=0.273354, loss=8.576202
I0810 07:15:50.935257 140065023153984 submission.py:120] 99) loss = 8.576, grad_norm = 0.273
I0810 07:15:51.381876 140006680004352 logging_writer.py:48] [100] global_step=100, grad_norm=0.222089, loss=8.616353
I0810 07:15:51.385953 140065023153984 submission.py:120] 100) loss = 8.616, grad_norm = 0.222
I0810 07:18:46.384236 140006671611648 logging_writer.py:48] [500] global_step=500, grad_norm=0.773820, loss=6.889452
I0810 07:18:46.388117 140065023153984 submission.py:120] 500) loss = 6.889, grad_norm = 0.774
I0810 07:22:25.350874 140006680004352 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.673518, loss=5.846684
I0810 07:22:25.355530 140065023153984 submission.py:120] 1000) loss = 5.847, grad_norm = 0.674
I0810 07:26:03.965389 140006671611648 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.946851, loss=6.580058
I0810 07:26:03.969301 140065023153984 submission.py:120] 1500) loss = 6.580, grad_norm = 0.947
I0810 07:29:06.058826 140065023153984 spec.py:320] Evaluating on the training split.
I0810 07:29:09.873906 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 07:33:54.131415 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 07:33:57.854307 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 07:38:35.955722 140065023153984 spec.py:348] Evaluating on the test split.
I0810 07:38:39.743657 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 07:43:23.697920 140065023153984 submission_runner.py:362] Time since start: 2564.31s, 	Step: 1917, 	{'train/accuracy': 0.1337695675093397, 'train/loss': 6.458865256354427, 'train/bleu': 0.008602074257434374, 'validation/accuracy': 0.13362512554091083, 'validation/loss': 6.501780821068555, 'validation/bleu': 0.00972662074284031, 'validation/num_examples': 3000, 'test/accuracy': 0.12751147521933648, 'test/loss': 6.70122450758236, 'test/bleu': 0.008188075912792704, 'test/num_examples': 3003, 'score': 843.1898283958435, 'total_duration': 2564.3132150173187, 'accumulated_submission_time': 843.1898283958435, 'accumulated_eval_time': 1720.0027911663055, 'accumulated_logging_time': 0.2940502166748047}
I0810 07:43:23.714077 140006680004352 logging_writer.py:48] [1917] accumulated_eval_time=1720.002791, accumulated_logging_time=0.294050, accumulated_submission_time=843.189828, global_step=1917, preemption_count=0, score=843.189828, test/accuracy=0.127511, test/bleu=0.008188, test/loss=6.701225, test/num_examples=3003, total_duration=2564.313215, train/accuracy=0.133770, train/bleu=0.008602, train/loss=6.458865, validation/accuracy=0.133625, validation/bleu=0.009727, validation/loss=6.501781, validation/num_examples=3000
I0810 07:44:01.145613 140006671611648 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.485706, loss=6.264721
I0810 07:44:01.149204 140065023153984 submission.py:120] 2000) loss = 6.265, grad_norm = 0.486
I0810 07:47:40.126828 140006680004352 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.502680, loss=5.683379
I0810 07:47:40.130631 140065023153984 submission.py:120] 2500) loss = 5.683, grad_norm = 0.503
I0810 07:51:19.111024 140006671611648 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.388935, loss=5.115779
I0810 07:51:19.114939 140065023153984 submission.py:120] 3000) loss = 5.116, grad_norm = 0.389
I0810 07:54:58.088688 140006680004352 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.407899, loss=4.549713
I0810 07:54:58.093158 140065023153984 submission.py:120] 3500) loss = 4.550, grad_norm = 0.408
I0810 07:57:23.938951 140065023153984 spec.py:320] Evaluating on the training split.
I0810 07:57:27.756345 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:00:55.276680 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 08:00:59.024679 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:04:22.145902 140065023153984 spec.py:348] Evaluating on the test split.
I0810 08:04:25.948354 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:07:27.160543 140065023153984 submission_runner.py:362] Time since start: 4007.78s, 	Step: 3834, 	{'train/accuracy': 0.44151308304891923, 'train/loss': 3.4865617889647327, 'train/bleu': 15.571475247160558, 'validation/accuracy': 0.4328526614673098, 'validation/loss': 3.624027445412952, 'validation/bleu': 11.539359336475597, 'validation/num_examples': 3000, 'test/accuracy': 0.42113764452966124, 'test/loss': 3.762319156353495, 'test/bleu': 9.680546508683822, 'test/num_examples': 3003, 'score': 1682.163651227951, 'total_duration': 4007.7758860588074, 'accumulated_submission_time': 1682.163651227951, 'accumulated_eval_time': 2323.224311351776, 'accumulated_logging_time': 0.7296514511108398}
I0810 08:07:27.177059 140006671611648 logging_writer.py:48] [3834] accumulated_eval_time=2323.224311, accumulated_logging_time=0.729651, accumulated_submission_time=1682.163651, global_step=3834, preemption_count=0, score=1682.163651, test/accuracy=0.421138, test/bleu=9.680547, test/loss=3.762319, test/num_examples=3003, total_duration=4007.775886, train/accuracy=0.441513, train/bleu=15.571475, train/loss=3.486562, validation/accuracy=0.432853, validation/bleu=11.539359, validation/loss=3.624027, validation/num_examples=3000
I0810 08:08:40.855901 140006680004352 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.643027, loss=4.544181
I0810 08:08:40.860648 140065023153984 submission.py:120] 4000) loss = 4.544, grad_norm = 0.643
I0810 08:12:19.928582 140006671611648 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.075090, loss=4.714363
I0810 08:12:19.933355 140065023153984 submission.py:120] 4500) loss = 4.714, grad_norm = 2.075
I0810 08:15:58.935336 140006680004352 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.495059, loss=4.884603
I0810 08:15:58.939378 140065023153984 submission.py:120] 5000) loss = 4.885, grad_norm = 2.495
I0810 08:19:37.783050 140006671611648 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.098432, loss=4.898795
I0810 08:19:37.786704 140065023153984 submission.py:120] 5500) loss = 4.899, grad_norm = 5.098
I0810 08:21:27.216074 140065023153984 spec.py:320] Evaluating on the training split.
I0810 08:21:31.034630 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:25:04.544353 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 08:25:08.290724 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:28:53.657164 140065023153984 spec.py:348] Evaluating on the test split.
I0810 08:28:57.478515 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:33:09.851641 140065023153984 submission_runner.py:362] Time since start: 5550.47s, 	Step: 5751, 	{'train/accuracy': 0.36026433113811757, 'train/loss': 3.945869613800439, 'train/bleu': 7.518079377044725, 'validation/accuracy': 0.3321099552392407, 'validation/loss': 4.221701373820536, 'validation/bleu': 4.061206438491127, 'validation/num_examples': 3000, 'test/accuracy': 0.3132299111033641, 'test/loss': 4.467999026785195, 'test/bleu': 2.85361385363975, 'test/num_examples': 3003, 'score': 2521.103744983673, 'total_duration': 5550.46692276001, 'accumulated_submission_time': 2521.103744983673, 'accumulated_eval_time': 3025.8597650527954, 'accumulated_logging_time': 1.0359251499176025}
I0810 08:33:09.869390 140006680004352 logging_writer.py:48] [5751] accumulated_eval_time=3025.859765, accumulated_logging_time=1.035925, accumulated_submission_time=2521.103745, global_step=5751, preemption_count=0, score=2521.103745, test/accuracy=0.313230, test/bleu=2.853614, test/loss=4.467999, test/num_examples=3003, total_duration=5550.466923, train/accuracy=0.360264, train/bleu=7.518079, train/loss=3.945870, validation/accuracy=0.332110, validation/bleu=4.061206, validation/loss=4.221701, validation/num_examples=3000
I0810 08:34:59.724558 140006671611648 logging_writer.py:48] [6000] global_step=6000, grad_norm=6.288304, loss=4.872653
I0810 08:34:59.728391 140065023153984 submission.py:120] 6000) loss = 4.873, grad_norm = 6.288
I0810 08:38:38.687489 140006680004352 logging_writer.py:48] [6500] global_step=6500, grad_norm=7.343848, loss=5.033945
I0810 08:38:38.692093 140065023153984 submission.py:120] 6500) loss = 5.034, grad_norm = 7.344
I0810 08:42:17.468412 140006671611648 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.654135, loss=4.893386
I0810 08:42:17.472007 140065023153984 submission.py:120] 7000) loss = 4.893, grad_norm = 5.654
I0810 08:45:56.420346 140006680004352 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.723734, loss=4.828781
I0810 08:45:56.424984 140065023153984 submission.py:120] 7500) loss = 4.829, grad_norm = 6.724
I0810 08:47:09.976664 140065023153984 spec.py:320] Evaluating on the training split.
I0810 08:47:13.802433 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:51:10.792102 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 08:51:14.521278 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:54:51.283378 140065023153984 spec.py:348] Evaluating on the test split.
I0810 08:54:55.091459 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 08:58:28.137846 140065023153984 submission_runner.py:362] Time since start: 7068.75s, 	Step: 7669, 	{'train/accuracy': 0.36624332398980286, 'train/loss': 3.868467819612186, 'train/bleu': 6.88944904903742, 'validation/accuracy': 0.3320727579323257, 'validation/loss': 4.209756388637462, 'validation/bleu': 3.05387891642738, 'validation/num_examples': 3000, 'test/accuracy': 0.31273023066643424, 'test/loss': 4.453419324850386, 'test/bleu': 2.1790434741977647, 'test/num_examples': 3003, 'score': 3360.112795352936, 'total_duration': 7068.753155231476, 'accumulated_submission_time': 3360.112795352936, 'accumulated_eval_time': 3704.020843267441, 'accumulated_logging_time': 1.3382742404937744}
I0810 08:58:28.154044 140006671611648 logging_writer.py:48] [7669] accumulated_eval_time=3704.020843, accumulated_logging_time=1.338274, accumulated_submission_time=3360.112795, global_step=7669, preemption_count=0, score=3360.112795, test/accuracy=0.312730, test/bleu=2.179043, test/loss=4.453419, test/num_examples=3003, total_duration=7068.753155, train/accuracy=0.366243, train/bleu=6.889449, train/loss=3.868468, validation/accuracy=0.332073, validation/bleu=3.053879, validation/loss=4.209756, validation/num_examples=3000
I0810 09:00:53.946911 140006680004352 logging_writer.py:48] [8000] global_step=8000, grad_norm=7.561722, loss=4.754017
I0810 09:00:53.951263 140065023153984 submission.py:120] 8000) loss = 4.754, grad_norm = 7.562
I0810 09:04:32.774314 140006671611648 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.369332, loss=4.728670
I0810 09:04:32.777996 140065023153984 submission.py:120] 8500) loss = 4.729, grad_norm = 3.369
I0810 09:08:11.603716 140006680004352 logging_writer.py:48] [9000] global_step=9000, grad_norm=7.955623, loss=4.719368
I0810 09:08:11.607336 140065023153984 submission.py:120] 9000) loss = 4.719, grad_norm = 7.956
I0810 09:11:50.314411 140006671611648 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.410206, loss=4.692039
I0810 09:11:50.318130 140065023153984 submission.py:120] 9500) loss = 4.692, grad_norm = 2.410
I0810 09:12:28.363782 140065023153984 spec.py:320] Evaluating on the training split.
I0810 09:12:32.198808 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 09:15:33.282836 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 09:15:37.021261 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 09:18:42.112385 140065023153984 spec.py:348] Evaluating on the test split.
I0810 09:18:45.925877 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 09:21:59.763631 140065023153984 submission_runner.py:362] Time since start: 8480.38s, 	Step: 9588, 	{'train/accuracy': 0.37559866312924234, 'train/loss': 3.7579545389288955, 'train/bleu': 5.992806293212731, 'validation/accuracy': 0.3473236537674672, 'validation/loss': 4.0711460490260505, 'validation/bleu': 2.899263259634424, 'validation/num_examples': 3000, 'test/accuracy': 0.3272790657137877, 'test/loss': 4.3148934547673, 'test/bleu': 2.2610505758314385, 'test/num_examples': 3003, 'score': 4199.195156812668, 'total_duration': 8480.378928422928, 'accumulated_submission_time': 4199.195156812668, 'accumulated_eval_time': 4275.420564651489, 'accumulated_logging_time': 1.6565840244293213}
I0810 09:21:59.780042 140006680004352 logging_writer.py:48] [9588] accumulated_eval_time=4275.420565, accumulated_logging_time=1.656584, accumulated_submission_time=4199.195157, global_step=9588, preemption_count=0, score=4199.195157, test/accuracy=0.327279, test/bleu=2.261051, test/loss=4.314893, test/num_examples=3003, total_duration=8480.378928, train/accuracy=0.375599, train/bleu=5.992806, train/loss=3.757955, validation/accuracy=0.347324, validation/bleu=2.899263, validation/loss=4.071146, validation/num_examples=3000
I0810 09:25:00.148471 140065023153984 spec.py:320] Evaluating on the training split.
I0810 09:25:03.973875 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 09:29:03.297754 140065023153984 spec.py:332] Evaluating on the validation split.
I0810 09:29:07.039752 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 09:32:54.859638 140065023153984 spec.py:348] Evaluating on the test split.
I0810 09:32:58.675823 140065023153984 workload.py:131] Translating evaluation dataset.
I0810 09:36:50.069524 140065023153984 submission_runner.py:362] Time since start: 9370.68s, 	Step: 10000, 	{'train/accuracy': 0.3881262829554707, 'train/loss': 3.6719267840965126, 'train/bleu': 6.8442408403394746, 'validation/accuracy': 0.3583464557166061, 'validation/loss': 3.9813808105293176, 'validation/bleu': 3.504991798759946, 'validation/num_examples': 3000, 'test/accuracy': 0.337586427284876, 'test/loss': 4.22291121956888, 'test/bleu': 2.5612223257387603, 'test/num_examples': 3003, 'score': 4379.084274291992, 'total_duration': 9370.68479013443, 'accumulated_submission_time': 4379.084274291992, 'accumulated_eval_time': 4985.341482400894, 'accumulated_logging_time': 1.9730076789855957}
I0810 09:36:50.085870 140006671611648 logging_writer.py:48] [10000] accumulated_eval_time=4985.341482, accumulated_logging_time=1.973008, accumulated_submission_time=4379.084274, global_step=10000, preemption_count=0, score=4379.084274, test/accuracy=0.337586, test/bleu=2.561222, test/loss=4.222911, test/num_examples=3003, total_duration=9370.684790, train/accuracy=0.388126, train/bleu=6.844241, train/loss=3.671927, validation/accuracy=0.358346, validation/bleu=3.504992, validation/loss=3.981381, validation/num_examples=3000
I0810 09:36:50.365493 140006680004352 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=4379.084274
I0810 09:36:52.679827 140065023153984 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/wmt_pytorch/trial_1/checkpoint_10000.
I0810 09:36:52.702225 140065023153984 submission_runner.py:528] Tuning trial 1/1
I0810 09:36:52.702399 140065023153984 submission_runner.py:529] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0810 09:36:52.703196 140065023153984 submission_runner.py:530] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000638060296698038, 'train/loss': 11.017881354966615, 'train/bleu': 0.0, 'validation/accuracy': 0.00048356498989473163, 'validation/loss': 11.005854701119638, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489919237697, 'test/loss': 11.026332723258381, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 4.2677836418151855, 'total_duration': 866.6320533752441, 'accumulated_submission_time': 4.2677836418151855, 'accumulated_eval_time': 862.3638000488281, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1917, {'train/accuracy': 0.1337695675093397, 'train/loss': 6.458865256354427, 'train/bleu': 0.008602074257434374, 'validation/accuracy': 0.13362512554091083, 'validation/loss': 6.501780821068555, 'validation/bleu': 0.00972662074284031, 'validation/num_examples': 3000, 'test/accuracy': 0.12751147521933648, 'test/loss': 6.70122450758236, 'test/bleu': 0.008188075912792704, 'test/num_examples': 3003, 'score': 843.1898283958435, 'total_duration': 2564.3132150173187, 'accumulated_submission_time': 843.1898283958435, 'accumulated_eval_time': 1720.0027911663055, 'accumulated_logging_time': 0.2940502166748047, 'global_step': 1917, 'preemption_count': 0}), (3834, {'train/accuracy': 0.44151308304891923, 'train/loss': 3.4865617889647327, 'train/bleu': 15.571475247160558, 'validation/accuracy': 0.4328526614673098, 'validation/loss': 3.624027445412952, 'validation/bleu': 11.539359336475597, 'validation/num_examples': 3000, 'test/accuracy': 0.42113764452966124, 'test/loss': 3.762319156353495, 'test/bleu': 9.680546508683822, 'test/num_examples': 3003, 'score': 1682.163651227951, 'total_duration': 4007.7758860588074, 'accumulated_submission_time': 1682.163651227951, 'accumulated_eval_time': 2323.224311351776, 'accumulated_logging_time': 0.7296514511108398, 'global_step': 3834, 'preemption_count': 0}), (5751, {'train/accuracy': 0.36026433113811757, 'train/loss': 3.945869613800439, 'train/bleu': 7.518079377044725, 'validation/accuracy': 0.3321099552392407, 'validation/loss': 4.221701373820536, 'validation/bleu': 4.061206438491127, 'validation/num_examples': 3000, 'test/accuracy': 0.3132299111033641, 'test/loss': 4.467999026785195, 'test/bleu': 2.85361385363975, 'test/num_examples': 3003, 'score': 2521.103744983673, 'total_duration': 5550.46692276001, 'accumulated_submission_time': 2521.103744983673, 'accumulated_eval_time': 3025.8597650527954, 'accumulated_logging_time': 1.0359251499176025, 'global_step': 5751, 'preemption_count': 0}), (7669, {'train/accuracy': 0.36624332398980286, 'train/loss': 3.868467819612186, 'train/bleu': 6.88944904903742, 'validation/accuracy': 0.3320727579323257, 'validation/loss': 4.209756388637462, 'validation/bleu': 3.05387891642738, 'validation/num_examples': 3000, 'test/accuracy': 0.31273023066643424, 'test/loss': 4.453419324850386, 'test/bleu': 2.1790434741977647, 'test/num_examples': 3003, 'score': 3360.112795352936, 'total_duration': 7068.753155231476, 'accumulated_submission_time': 3360.112795352936, 'accumulated_eval_time': 3704.020843267441, 'accumulated_logging_time': 1.3382742404937744, 'global_step': 7669, 'preemption_count': 0}), (9588, {'train/accuracy': 0.37559866312924234, 'train/loss': 3.7579545389288955, 'train/bleu': 5.992806293212731, 'validation/accuracy': 0.3473236537674672, 'validation/loss': 4.0711460490260505, 'validation/bleu': 2.899263259634424, 'validation/num_examples': 3000, 'test/accuracy': 0.3272790657137877, 'test/loss': 4.3148934547673, 'test/bleu': 2.2610505758314385, 'test/num_examples': 3003, 'score': 4199.195156812668, 'total_duration': 8480.378928422928, 'accumulated_submission_time': 4199.195156812668, 'accumulated_eval_time': 4275.420564651489, 'accumulated_logging_time': 1.6565840244293213, 'global_step': 9588, 'preemption_count': 0}), (10000, {'train/accuracy': 0.3881262829554707, 'train/loss': 3.6719267840965126, 'train/bleu': 6.8442408403394746, 'validation/accuracy': 0.3583464557166061, 'validation/loss': 3.9813808105293176, 'validation/bleu': 3.504991798759946, 'validation/num_examples': 3000, 'test/accuracy': 0.337586427284876, 'test/loss': 4.22291121956888, 'test/bleu': 2.5612223257387603, 'test/num_examples': 3003, 'score': 4379.084274291992, 'total_duration': 9370.68479013443, 'accumulated_submission_time': 4379.084274291992, 'accumulated_eval_time': 4985.341482400894, 'accumulated_logging_time': 1.9730076789855957, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0810 09:36:52.703318 140065023153984 submission_runner.py:531] Timing: 4379.084274291992
I0810 09:36:52.703367 140065023153984 submission_runner.py:533] Total number of evals: 7
I0810 09:36:52.703416 140065023153984 submission_runner.py:534] ====================
I0810 09:36:52.703506 140065023153984 submission_runner.py:602] Final wmt score: 4379.084274291992
