torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=baselines/adamw/pytorch/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_pytorch_2_preliminary_2/adamw --overwrite=true --save_checkpoints=false --max_global_steps=2714 --torch_compile=True 2>&1 | tee -a /logs/fastmri_pytorch_08-10-2023-05-44-33.log
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-10 05:44:43.686059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-10 05:44:43.686082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0810 05:44:58.173445 140556381808448 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 3
I0810 05:44:58.173490 140480425027392 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 4
I0810 05:44:58.173439 140029818570560 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 7
I0810 05:44:58.173563 140619559941952 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 5
I0810 05:44:58.174781 140014022567744 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 6
I0810 05:44:58.174905 139638027228992 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 2
I0810 05:44:58.174950 140190517720896 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 1
I0810 05:44:58.175119 140014022567744 distributed_c10d.py:476] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.175065 139925592147776 distributed_c10d.py:442] Added key: store_based_barrier_key:1 to store for rank: 0
I0810 05:44:58.175369 139638027228992 distributed_c10d.py:476] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.175409 139925592147776 distributed_c10d.py:476] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.175395 140190517720896 distributed_c10d.py:476] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.184121 140556381808448 distributed_c10d.py:476] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.184146 140480425027392 distributed_c10d.py:476] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.184118 140029818570560 distributed_c10d.py:476] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.184233 140619559941952 distributed_c10d.py:476] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0810 05:44:58.502342 139925592147776 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/fastmri_pytorch.
W0810 05:44:58.537822 140190517720896 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.537884 140619559941952 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.537832 140480425027392 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.537870 140029818570560 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.538120 139925592147776 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.538737 140014022567744 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.539750 140556381808448 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0810 05:44:58.540749 139638027228992 xla_bridge.py:463] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0810 05:44:58.545518 139925592147776 submission_runner.py:488] Using RNG seed 721696921
I0810 05:44:58.547319 139925592147776 submission_runner.py:497] --- Tuning run 1/1 ---
I0810 05:44:58.547496 139925592147776 submission_runner.py:502] Creating tuning directory at /experiment_runs/timing_pytorch_2_preliminary_2/adamw/fastmri_pytorch/trial_1.
I0810 05:44:58.549150 139925592147776 logger_utils.py:92] Saving hparams to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/fastmri_pytorch/trial_1/hparams.json.
I0810 05:44:58.557934 139925592147776 submission_runner.py:176] Initializing dataset.
I0810 05:44:58.558079 139925592147776 submission_runner.py:183] Initializing model.
I0810 05:45:03.168462 139925592147776 submission_runner.py:212] Performing `torch.compile`.
I0810 05:45:03.455449 139925592147776 submission_runner.py:215] Initializing optimizer.
I0810 05:45:03.456296 139925592147776 submission_runner.py:222] Initializing metrics bundle.
I0810 05:45:03.456393 139925592147776 submission_runner.py:240] Initializing checkpoint and logger.
I0810 05:45:03.457055 139925592147776 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0810 05:45:03.457162 139925592147776 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0810 05:45:03.933220 139925592147776 submission_runner.py:261] Saving meta data to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/fastmri_pytorch/trial_1/meta_data_0.json.
I0810 05:45:03.934103 139925592147776 submission_runner.py:264] Saving flags to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/fastmri_pytorch/trial_1/flags_0.json.
I0810 05:45:04.023855 139925592147776 submission_runner.py:274] Starting training loop.
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,340] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,342] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,345] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,347] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,348] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:04,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,355] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:04,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,388] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,388] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,388] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,388] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,388] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:04,388] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,389] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,389] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:04,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,391] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,391] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,391] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:04,391] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:04,392] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,394] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,395] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,395] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:04,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:04,399] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,400] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,400] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:04,401] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:04,401] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:04,401] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:09,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:09,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:09,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:09,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:09,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:09,252] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:09,254] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:50,045] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:50,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,083] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,083] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,084] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,084] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,085] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:45:50,087] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:50,489] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __iter__
[2023-08-10 05:45:50,495] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:50,555] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:50,558] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:50,558] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:50,559] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,638] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,641] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,692] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,694] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,694] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,695] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,706] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,706] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,707] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,739] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,739] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,739] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,740] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,741] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,741] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,742] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,742] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,744] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,744] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,745] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:53,795] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:53,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:53,847] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:53,847] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:53,848] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:54,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:54,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:54,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:54,201] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:54,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:54,317] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:54,534] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:45:55,797] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:55,798] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:55,810] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:55,811] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:55,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:55,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:55,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:55,843] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:55,858] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:55,866] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:55,867] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:55,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:55,885] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:55,886] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:55,890] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:55,907] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:55,909] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:55,910] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:55,910] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:55,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:55,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:55,921] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:55,921] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:55,922] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:55,935] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:55,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:55,939] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:55,939] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:55,939] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:55,951] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:55,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:55,968] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:55,969] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:55,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:55,984] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:55,986] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:55,987] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:55,987] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:55,996] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:55,999] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:55,999] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:56,000] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:56,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:45:56,052] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:56,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:56,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:56,154] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:56,154] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:56,155] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:56,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:56,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:56,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:56,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:56,452] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:56,456] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:56,612] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:45:57,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,304] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,304] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,307] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,349] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,350] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,386] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,387] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,387] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,415] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,432] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,433] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,435] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,435] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,435] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,435] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,435] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,436] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,439] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,439] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,440] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,461] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,463] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,463] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,463] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,488] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,491] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,493] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,493] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,493] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:45:57,552] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:57,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,599] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,601] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,601] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,602] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,606] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:57,652] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:57,655] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:57,655] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:57,655] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:57,678] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,692] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,701] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,729] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,857] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:45:57,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-08-10 05:45:58,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,532] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,596] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,637] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,743] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:58,747] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:58,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:45:58,806] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:58,864] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:58,902] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:58,928] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:58,945] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:58,992] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:59,009] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:59,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:59,011] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,020] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:59,027] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:59,058] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,060] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,060] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,061] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,084] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:45:59,111] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,114] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,114] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,114] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:59,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,136] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,136] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,137] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,140] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:59,147] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:59,158] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:59,202] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:59,239] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:59,250] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,252] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,252] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,253] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,265] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,265] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:45:59,298] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,305] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:45:59,330] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:45:59,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:45:59,366] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,369] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,370] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:45:59,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:45:59,413] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:45:59,414] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:45:59,414] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:45:59,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:45:59,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:45:59,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:45:59,662] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:45:59,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:46:00,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-08-10 05:46:00,479] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:00,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,632] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,789] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:00,874] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:00,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:00,894] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:00,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:00,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:00,978] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:00,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,057] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:01,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:01,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:01,078] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,079] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,080] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,080] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,090] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,094] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,156] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:01,162] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:01,162] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,163] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:01,163] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,179] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,180] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,185] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,186] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,186] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,186] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,187] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,187] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,187] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,261] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,262] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,262] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:01,280] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,287] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,288] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,289] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,289] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,289] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,291] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,291] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,291] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,341] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,347] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,369] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,369] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,370] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:01,453] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,520] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,553] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,555] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,555] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,555] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,583] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,592] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:01,592] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:01,699] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,767] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,774] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:01,774] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,776] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:01,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,781] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:01,782] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,812] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,815] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,815] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,815] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,824] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:01,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:01,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:01,854] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,870] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,871] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,899] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,901] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,901] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,902] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,917] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,917] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,918] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:01,924] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,924] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,924] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:01,972] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:01,972] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:01,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:01,976] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:02,044] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,058] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:02,150] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,173] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,181] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,201] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:02,204] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:02,205] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:02,206] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:02,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,257] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,411] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,455] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:02,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:02,536] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:02,595] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:02,613] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,670] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:02,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:02,690] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:02,695] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:02,697] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:02,697] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:02,698] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:02,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:02,713] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:02,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:02,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:02,771] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:02,773] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:02,773] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:02,773] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:02,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:02,841] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:02,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:02,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,863] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:02,871] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,881] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,916] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:02,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:02,937] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:02,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:02,942] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:02,942] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:02,943] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:02,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:02,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:02,959] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:02,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:02,971] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:02,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:02,971] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:02,971] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:02,972] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:02,973] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:02,973] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:02,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,005] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:03,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,021] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,031] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:03,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:03,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,204] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,204] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,205] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:03,223] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,229] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:03,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:03,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,248] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,248] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,249] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,249] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,249] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:03,321] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,325] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,327] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,328] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,328] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,369] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,371] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,371] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,372] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,425] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,426] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,444] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,445] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,447] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,488] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,492] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:03,525] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,536] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,538] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,539] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,539] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,552] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,568] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,570] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,571] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,571] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,577] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,580] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,580] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,580] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,597] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,597] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,598] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:03,694] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:03,757] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:03,784] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:03,800] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:03,802] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:03,802] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:03,803] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:03,818] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:03,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:04,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:04,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:04,060] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:04,062] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,063] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,092] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,093] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,122] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,145] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,145] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,146] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,172] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,174] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,174] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,174] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,245] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:04,269] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,270] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,301] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,329] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,332] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,359] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,360] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,361] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,361] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,362] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,362] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,385] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,387] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,387] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,388] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,388] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,391] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,414] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,414] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,415] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,447] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,449] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,449] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,449] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,527] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:04,528] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,564] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,567] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,568] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,569] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,581] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,590] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:04,590] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,604] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:04,605] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,605] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,606] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,606] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,620] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,684] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,717] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,717] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,717] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,726] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,728] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,729] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:04,804] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,816] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:04,817] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,834] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:04,845] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:04,846] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,884] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,897] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,899] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:04,899] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:04,926] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:04,929] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,931] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,931] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,932] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,939] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,941] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,941] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,942] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,969] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:04,972] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:04,972] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:04,972] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:04,980] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,026] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,028] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,028] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,028] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,053] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:05,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,164] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,177] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,179] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,179] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,179] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,214] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-08-10 05:46:05,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,390] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,419] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,442] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,443] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,443] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,503] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,523] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,525] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,525] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,525] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,526] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,528] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,528] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,528] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:05,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,664] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,669] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,670] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,698] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,728] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,750] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,752] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,752] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,752] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,753] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:05,755] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,755] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,756] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,760] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,760] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:05,779] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,780] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,784] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,784] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,784] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,846] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,886] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,889] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,889] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,890] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:05,905] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,955] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:05,978] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:05,980] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:05,980] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:05,981] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:05,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:05,995] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:05,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:06,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:06,006] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:06,006] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:06,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:06,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,140] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,180] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,182] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,182] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,183] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,194] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,196] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,196] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,197] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:06,222] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:06,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:06,257] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:06,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:06,275] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:06,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:06,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:06,389] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,408] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,445] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,446] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,446] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,464] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,467] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,467] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,467] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,477] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:06,478] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:06,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,481] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,482] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,482] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:06,496] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:06,611] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,632] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:06,664] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,667] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,667] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:06,690] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:06,693] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:06,693] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:06,694] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:06,697] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:06,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:06,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:06,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:07,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:07,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:07,304] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-08-10 05:46:07,305] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,368] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:07,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,380] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,410] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,438] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:07,440] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:07,441] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:07,441] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:07,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:07,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:07,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,648] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,680] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,681] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,698] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,698] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:07,754] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:07,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:07,861] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,862] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:07,942] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:07,948] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:08,018] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:08,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-08-10 05:46:09,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-08-10 05:46:09,564] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:09,622] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:09,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:09,685] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:09,685] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:09,686] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:09,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-08-10 05:46:11,125] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-08-10 05:46:11,451] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-08-10 05:46:11,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-08-10 05:46:11,662] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:11,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:11,773] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:11,775] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:11,775] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:11,775] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:12,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-08-10 05:46:14,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-08-10 05:46:14,446] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-08-10 05:46:14,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-08-10 05:46:14,648] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:14,708] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:14,729] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:14,730] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:14,730] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:14,731] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:14,960] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-08-10 05:46:15,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-08-10 05:46:15,143] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:15,213] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:15,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:15,259] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:15,260] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:15,260] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:15,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-08-10 05:46:15,676] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-08-10 05:46:15,920] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-08-10 05:46:16,039] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-08-10 05:46:16,054] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:16,107] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:16,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:16,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:16,129] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:16,130] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:16,354] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-08-10 05:46:16,549] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-08-10 05:46:16,549] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:16,612] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:16,653] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:16,655] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:16,656] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:16,656] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:17,089] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-08-10 05:46:17,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-08-10 05:46:17,371] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:17,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:17,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:17,445] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:17,445] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:17,445] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:17,673] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-08-10 05:46:17,867] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-08-10 05:46:17,867] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:17,946] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:17,988] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:17,990] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:17,990] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:17,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:18,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-08-10 05:46:18,733] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-08-10 05:46:18,734] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:18,782] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:18,804] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:18,805] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:18,806] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:18,806] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:19,033] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-08-10 05:46:19,290] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-08-10 05:46:19,291] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:19,421] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:46:19,471] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:46:19,473] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:46:19,473] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:46:19,474] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:46:19,932] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-08-10 05:46:21,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-08-10 05:46:21,125] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:46:21,198] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15
[2023-08-10 05:46:21,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,236] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:21,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:21,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:21,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:21,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:21,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:21,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:21,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:22,602] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15
[2023-08-10 05:46:22,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,667] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,668] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,670] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,672] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,675] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,702] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,703] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,707] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,709] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,710] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,732] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:22,764] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:22,999] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14
[2023-08-10 05:46:23,244] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,280] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,283] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,322] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,341] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:23,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,694] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:23,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,750] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:23,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,827] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:23,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:23,895] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:23,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:23,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:23,967] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:23,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:23,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:23,982] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:24,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:24,026] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:24,312] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:24,320] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,351] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:24,453] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14
[2023-08-10 05:46:24,480] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,525] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,577] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,626] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:24,631] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 13
[2023-08-10 05:46:24,734] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:24,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:24,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:24,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:24,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:24,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:24,970] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:25,010] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:25,071] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:25,072] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:25,073] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:25,086] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:25,113] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:25,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:25,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:25,272] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:25,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:25,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:25,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:25,413] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:25,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:25,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 13
[2023-08-10 05:46:25,705] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:25,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:25,880] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 12
[2023-08-10 05:46:25,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:25,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:25,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:25,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 12
[2023-08-10 05:46:26,025] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 11
[2023-08-10 05:46:26,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:26,065] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:26,066] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:26,067] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:26,103] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:26,139] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:26,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:26,142] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:26,144] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:26,153] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,166] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,183] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,186] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:26,206] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:26,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:26,243] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:26,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:26,289] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:26,308] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:26,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,398] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,403] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,443] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:26,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,503] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,535] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,541] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,550] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:26,556] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,580] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:26,924] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 11
[2023-08-10 05:46:27,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,231] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 10
[2023-08-10 05:46:27,259] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,263] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,299] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,332] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:27,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 10
[2023-08-10 05:46:27,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,367] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,369] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,370] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9
[2023-08-10 05:46:27,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,415] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,417] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:27,470] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,471] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,499] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,500] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,516] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,526] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,528] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:27,529] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,564] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:27,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,610] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,611] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,614] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,639] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,647] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:27,652] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:27,661] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,761] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:27,767] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,769] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,772] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,773] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,800] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:27,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:27,842] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:27,872] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:27,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:27,950] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:27,952] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:27,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:27,956] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:27,981] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:28,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:28,001] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:28,012] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:28,014] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:28,028] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:28,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,124] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,143] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9
[2023-08-10 05:46:28,184] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 8
[2023-08-10 05:46:28,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,211] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:28,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,265] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,270] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,273] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,274] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:28,301] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 8
[2023-08-10 05:46:28,363] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,386] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7
[2023-08-10 05:46:28,393] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,490] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7
[2023-08-10 05:46:28,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,512] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,513] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,519] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
[2023-08-10 05:46:28,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 6
[2023-08-10 05:46:29,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 6
[2023-08-10 05:46:29,975] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5
[2023-08-10 05:46:30,077] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5
[2023-08-10 05:46:30,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 4
[2023-08-10 05:46:30,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 4
[2023-08-10 05:46:30,195] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3
[2023-08-10 05:46:30,298] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3
[2023-08-10 05:46:30,331] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 2
[2023-08-10 05:46:30,510] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 2
[2023-08-10 05:46:30,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1
[2023-08-10 05:46:30,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1
[2023-08-10 05:46:30,782] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0
[2023-08-10 05:46:31,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0
I0810 05:46:31.064958 139883385448192 logging_writer.py:48] [0] global_step=0, grad_norm=4.013397, loss=0.788651
I0810 05:46:31.077733 139925592147776 submission.py:120] 0) loss = 0.789, grad_norm = 4.013
I0810 05:46:31.079121 139925592147776 spec.py:320] Evaluating on the training split.
[2023-08-10 05:47:25,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:25,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:26,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,018] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,018] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,019] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,019] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,020] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,020] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,021] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,021] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,021] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,022] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,038] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:26,040] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:26,041] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:26,041] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:26,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,227] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,228] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,230] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-08-10 05:47:26,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,872] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:26,872] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:26,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,873] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:26,878] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,879] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:26,881] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,882] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:26,883] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,884] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:26,939] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:26,940] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:27,212] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,214] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,217] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,227] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,229] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,265] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,266] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,267] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,267] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,268] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,269] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,269] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,270] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,275] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,276] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,277] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,278] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,278] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,278] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,279] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,279] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,279] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,279] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,279] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,280] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,287] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,287] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,287] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-08-10 05:47:27,419] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:27,473] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,476] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,482] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,486] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,491] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:27,610] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:27,676] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:27,678] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:27,678] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:27,679] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:27,954] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-08-10 05:47:28,105] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,105] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,106] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,107] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,107] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,109] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,110] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,110] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,213] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:28,214] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:28,373] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,378] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,393] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,397] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,405] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,414] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,416] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,418] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:28,419] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,419] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,419] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,424] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,425] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,425] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,437] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,439] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,440] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,440] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,442] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,444] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,445] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,445] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,452] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,454] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,454] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,455] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,459] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,462] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:28,462] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,462] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,464] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:28,464] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:28,464] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:28,608] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:28,617] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:28,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:28,650] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:28,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:28,654] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:28,671] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:29,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,272] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,278] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,306] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,306] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,307] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,314] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,315] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,360] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-08-10 05:47:29,369] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:29,382] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:29,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,523] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,524] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,524] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,524] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,526] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,526] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,527] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,559] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,597] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,607] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,607] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,608] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,608] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,608] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,609] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,609] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,610] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,635] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:29,640] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,644] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,647] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,647] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,647] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,684] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,684] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,684] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,692] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:29,697] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:29,697] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:29,699] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:29,719] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,723] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,799] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:29,989] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-08-10 05:47:30,167] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,170] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,247] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,249] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,292] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,302] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,328] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,374] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,377] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,380] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:30,410] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,420] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,424] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,426] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,437] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,492] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,496] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,507] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:30,509] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,523] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,546] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,564] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,608] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,628] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:30,630] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,645] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:30,654] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,655] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,656] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,657] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,657] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,657] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,658] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,679] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,714] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,725] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,727] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,728] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,728] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,764] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,766] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,767] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,767] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,775] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:30,810] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,812] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,813] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,813] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,825] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:30,834] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,837] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:30,843] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:30,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,883] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,884] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:30,892] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:30,895] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:30,895] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:30,895] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:30,935] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:30,998] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-08-10 05:47:30,999] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:31,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:31,011] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:31,159] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:31,236] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:31,239] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:31,239] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:31,240] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:31,406] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-08-10 05:47:31,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,618] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,625] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,688] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,787] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:31,807] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:31,822] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,824] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:31,876] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:31,892] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:31,903] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:31,907] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,925] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:31,942] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:31,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:31,959] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:31,983] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,006] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,008] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,008] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,009] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,036] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:32,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:32,056] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,057] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,059] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,081] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:32,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,085] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,085] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,085] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,100] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,117] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,120] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,129] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,129] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,130] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,144] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,146] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,146] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,146] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,238] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,241] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,285] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,306] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,308] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,308] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,308] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,314] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,315] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,315] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,316] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,317] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,319] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,319] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,319] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,349] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,365] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,366] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,382] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,383] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,397] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,397] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,398] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,399] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-08-10 05:47:32,421] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,424] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,427] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,435] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:32,470] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,472] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,473] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,473] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,509] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,518] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,520] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,520] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,521] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,548] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,549] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,558] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,560] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,561] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,561] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,561] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:32,561] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,562] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,562] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,584] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-08-10 05:47:32,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:32,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,681] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,685] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,687] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:32,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,704] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,704] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,705] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,715] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-08-10 05:47:32,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,729] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,729] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,730] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,734] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,735] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,736] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,737] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,737] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:32,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:32,808] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:32,819] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:32,844] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,852] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:32,869] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,871] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:32,873] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:32,896] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:32,929] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:32,938] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:32,941] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:32,947] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:32,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:32,966] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:32,968] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:32,969] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:32,969] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:32,973] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:32,988] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,035] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:33,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:33,058] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,061] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:33,061] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,083] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,084] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,084] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,085] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,091] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:33,122] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-08-10 05:47:33,128] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,151] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,153] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,153] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,153] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,158] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:33,165] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:33,169] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,174] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,188] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,190] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:33,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,193] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,194] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,194] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,207] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,258] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:33,265] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,266] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,266] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,267] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,277] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,297] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,314] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,375] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,381] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,381] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,422] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,422] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,423] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,423] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,445] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,447] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,447] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,447] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,447] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,448] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,448] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,450] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,462] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,486] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,487] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,488] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,488] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,496] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,499] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,499] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,499] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,504] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,505] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,528] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,548] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,567] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,573] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,576] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,578] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,578] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,579] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,595] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,597] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:33,597] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,597] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,598] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,639] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,687] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,688] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,688] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,700] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,701] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,705] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,706] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:33,728] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:33,728] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:33,791] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:33,802] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:33,840] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,844] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,851] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:33,886] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:33,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,891] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,892] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,892] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,900] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,900] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,900] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:33,908] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:33,908] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:33,908] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:33,909] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:33,909] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,040] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:34,041] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,046] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:34,047] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,091] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,095] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:34,098] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:34,099] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,115] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,116] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,117] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,128] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:34,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
[2023-08-10 05:47:34,224] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,244] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,267] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,269] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,269] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,269] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,270] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,270] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,270] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,303] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:34,304] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,305] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,307] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,307] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,307] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:34,341] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,359] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:34,360] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,361] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,361] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,378] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,379] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,412] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,460] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-08-10 05:47:34,494] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,514] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,515] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,517] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,517] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,519] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,519] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,519] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,544] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,551] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,573] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,575] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,575] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,576] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,582] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,589] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,605] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,606] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,607] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,607] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,623] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22
[2023-08-10 05:47:34,633] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,635] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,636] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,636] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,711] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:34,737] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,746] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,747] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,749] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,750] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,757] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22
[2023-08-10 05:47:34,777] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,781] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,783] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,783] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,783] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,791] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,793] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,793] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,793] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,794] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,795] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,795] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,813] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,829] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:34,840] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:34,841] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:34,944] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:34,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,989] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,989] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,990] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,995] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:34,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:34,996] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:34,997] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:34,997] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:34,998] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:34,998] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:34,998] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,025] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,033] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,044] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,046] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,046] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,047] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,072] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,075] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,075] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,132] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23
[2023-08-10 05:47:35,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,189] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,189] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,192] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:35,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,200] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,239] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:35,253] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23
[2023-08-10 05:47:35,253] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,276] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:35,279] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,280] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,281] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,281] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,370] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,387] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,395] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,396] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,435] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,437] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,437] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,438] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,448] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,449] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,458] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,481] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,481] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,482] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,483] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,483] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,483] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,483] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,483] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,484] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:35,485] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,497] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,498] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,499] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,499] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,523] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,603] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24
[2023-08-10 05:47:35,607] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,642] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,647] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,665] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,666] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,667] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,667] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,670] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,672] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,672] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,672] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,685] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,707] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,709] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,709] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,709] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,716] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,716] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,724] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24
[2023-08-10 05:47:35,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,740] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,771] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,778] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,814] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:35,830] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:35,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:35,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:35,883] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:35,883] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:35,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25
[2023-08-10 05:47:35,897] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,898] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,904] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,904] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,945] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:35,945] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:35,993] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25
[2023-08-10 05:47:36,015] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,024] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,054] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,057] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,074] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,077] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,077] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,078] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,085] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,106] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,109] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,109] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,110] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,110] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,112] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,113] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,113] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,219] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,228] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,238] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,241] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,253] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,255] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,255] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,280] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,282] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,283] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,283] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,290] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,292] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,292] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,293] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,293] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,294] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,295] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,295] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,296] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,321] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,324] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,397] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26
[2023-08-10 05:47:36,485] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,485] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,487] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,495] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,502] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:36,524] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26
[2023-08-10 05:47:36,525] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,657] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:36,699] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,700] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,719] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:36,721] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:36,721] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:36,722] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:36,741] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,742] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,751] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,752] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-10 05:47:36,906] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,906] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,915] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:36,915] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:36,916] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:37,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-10 05:47:37,215] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27
[2023-08-10 05:47:37,217] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
[2023-08-10 05:47:37,412] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:37,449] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:37,450] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:37,451] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:37,451] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:37,588] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28
[2023-08-10 05:47:37,722] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28
[2023-08-10 05:47:37,723] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:37,904] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:37,973] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:37,976] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:37,977] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:37,978] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:38,256] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29
[2023-08-10 05:47:38,463] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29
[2023-08-10 05:47:38,463] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:38,658] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:38,689] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:38,691] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:38,691] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:38,692] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:38,828] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30
[2023-08-10 05:47:38,957] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30
[2023-08-10 05:47:38,958] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
[2023-08-10 05:47:39,254] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-08-10 05:47:39,324] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-08-10 05:47:39,326] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compile_fn
[2023-08-10 05:47:39,327] torch._dynamo.backends.distributed: [INFO] DDPOptimizer used bucket cap 26214400 and produced the following buckets:
[2023-08-10 05:47:39,327] torch._dynamo.backends.distributed: [INFO] Please `pip install tabulate` in order to pretty-print ddp bucket sizes
[2023-08-10 05:47:39,613] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31
[2023-08-10 05:47:40,368] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31
[2023-08-10 05:47:40,369] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compile_fn
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:48:08.157513 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:49:10.673233 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:50:10.865356 139925592147776 submission_runner.py:362] Time since start: 306.84s, 	Step: 1, 	{'train/ssim': 0.310445989881243, 'train/loss': 0.8099030085972377, 'validation/ssim': 0.3026097973344295, 'validation/loss': 0.8176845878983188, 'validation/num_examples': 3554, 'test/ssim': 0.32400648196427323, 'test/loss': 0.819239467523911, 'test/num_examples': 3581, 'score': 87.05508780479431, 'total_duration': 306.8419408798218, 'accumulated_submission_time': 87.05508780479431, 'accumulated_eval_time': 219.78624653816223, 'accumulated_logging_time': 0}
I0810 05:50:10.888843 139861013018368 logging_writer.py:48] [1] accumulated_eval_time=219.786247, accumulated_logging_time=0, accumulated_submission_time=87.055088, global_step=1, preemption_count=0, score=87.055088, test/loss=0.819239, test/num_examples=3581, test/ssim=0.324006, total_duration=306.841941, train/loss=0.809903, train/ssim=0.310446, validation/loss=0.817685, validation/num_examples=3554, validation/ssim=0.302610
I0810 05:50:10.986707 140556381808448 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986687 140029818570560 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986678 140619559941952 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986717 140480425027392 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986561 139925592147776 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986775 140014022567744 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986846 140190517720896 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:10.986865 139638027228992 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
I0810 05:50:11.168731 139861004625664 logging_writer.py:48] [1] global_step=1, grad_norm=3.901849, loss=0.762141
I0810 05:50:11.172610 139925592147776 submission.py:120] 1) loss = 0.762, grad_norm = 3.902
I0810 05:50:11.233570 139861013018368 logging_writer.py:48] [2] global_step=2, grad_norm=3.837759, loss=0.779860
I0810 05:50:11.237271 139925592147776 submission.py:120] 2) loss = 0.780, grad_norm = 3.838
I0810 05:50:11.320699 139861004625664 logging_writer.py:48] [3] global_step=3, grad_norm=3.515881, loss=0.822190
I0810 05:50:11.327538 139925592147776 submission.py:120] 3) loss = 0.822, grad_norm = 3.516
I0810 05:50:11.420114 139861013018368 logging_writer.py:48] [4] global_step=4, grad_norm=3.922111, loss=0.749543
I0810 05:50:11.425167 139925592147776 submission.py:120] 4) loss = 0.750, grad_norm = 3.922
I0810 05:50:11.513593 139861004625664 logging_writer.py:48] [5] global_step=5, grad_norm=3.297282, loss=0.775699
I0810 05:50:11.518039 139925592147776 submission.py:120] 5) loss = 0.776, grad_norm = 3.297
I0810 05:50:11.599302 139861013018368 logging_writer.py:48] [6] global_step=6, grad_norm=3.341489, loss=0.886787
I0810 05:50:11.605298 139925592147776 submission.py:120] 6) loss = 0.887, grad_norm = 3.341
I0810 05:50:11.697739 139861004625664 logging_writer.py:48] [7] global_step=7, grad_norm=3.101686, loss=0.825375
I0810 05:50:11.703228 139925592147776 submission.py:120] 7) loss = 0.825, grad_norm = 3.102
I0810 05:50:11.777023 139861013018368 logging_writer.py:48] [8] global_step=8, grad_norm=3.122133, loss=0.826555
I0810 05:50:11.783071 139925592147776 submission.py:120] 8) loss = 0.827, grad_norm = 3.122
I0810 05:50:11.869742 139861004625664 logging_writer.py:48] [9] global_step=9, grad_norm=3.594639, loss=0.769715
I0810 05:50:11.875329 139925592147776 submission.py:120] 9) loss = 0.770, grad_norm = 3.595
I0810 05:50:11.948551 139861013018368 logging_writer.py:48] [10] global_step=10, grad_norm=3.047715, loss=0.763900
I0810 05:50:11.954390 139925592147776 submission.py:120] 10) loss = 0.764, grad_norm = 3.048
I0810 05:50:12.033407 139861004625664 logging_writer.py:48] [11] global_step=11, grad_norm=3.020273, loss=0.736226
I0810 05:50:12.039717 139925592147776 submission.py:120] 11) loss = 0.736, grad_norm = 3.020
I0810 05:50:12.137207 139861013018368 logging_writer.py:48] [12] global_step=12, grad_norm=2.829434, loss=0.780999
I0810 05:50:12.142282 139925592147776 submission.py:120] 12) loss = 0.781, grad_norm = 2.829
I0810 05:50:12.204233 139861004625664 logging_writer.py:48] [13] global_step=13, grad_norm=3.264241, loss=0.688261
I0810 05:50:12.208561 139925592147776 submission.py:120] 13) loss = 0.688, grad_norm = 3.264
I0810 05:50:12.273455 139861013018368 logging_writer.py:48] [14] global_step=14, grad_norm=2.823664, loss=0.705815
I0810 05:50:12.279095 139925592147776 submission.py:120] 14) loss = 0.706, grad_norm = 2.824
I0810 05:50:12.469088 139861004625664 logging_writer.py:48] [15] global_step=15, grad_norm=2.593001, loss=0.706905
I0810 05:50:12.474559 139925592147776 submission.py:120] 15) loss = 0.707, grad_norm = 2.593
I0810 05:50:12.774637 139861013018368 logging_writer.py:48] [16] global_step=16, grad_norm=2.725900, loss=0.718555
I0810 05:50:12.778463 139925592147776 submission.py:120] 16) loss = 0.719, grad_norm = 2.726
I0810 05:50:13.027562 139861004625664 logging_writer.py:48] [17] global_step=17, grad_norm=2.550267, loss=0.683341
I0810 05:50:13.033843 139925592147776 submission.py:120] 17) loss = 0.683, grad_norm = 2.550
I0810 05:50:13.317993 139861013018368 logging_writer.py:48] [18] global_step=18, grad_norm=2.305454, loss=0.708391
I0810 05:50:13.323860 139925592147776 submission.py:120] 18) loss = 0.708, grad_norm = 2.305
I0810 05:50:13.543154 139861004625664 logging_writer.py:48] [19] global_step=19, grad_norm=2.296574, loss=0.646293
I0810 05:50:13.549353 139925592147776 submission.py:120] 19) loss = 0.646, grad_norm = 2.297
I0810 05:50:13.861851 139861013018368 logging_writer.py:48] [20] global_step=20, grad_norm=2.246884, loss=0.630786
I0810 05:50:13.868920 139925592147776 submission.py:120] 20) loss = 0.631, grad_norm = 2.247
I0810 05:50:14.096634 139861004625664 logging_writer.py:48] [21] global_step=21, grad_norm=1.920452, loss=0.789076
I0810 05:50:14.107283 139925592147776 submission.py:120] 21) loss = 0.789, grad_norm = 1.920
I0810 05:50:14.392056 139861013018368 logging_writer.py:48] [22] global_step=22, grad_norm=1.905596, loss=0.657089
I0810 05:50:14.398072 139925592147776 submission.py:120] 22) loss = 0.657, grad_norm = 1.906
I0810 05:50:14.684377 139861004625664 logging_writer.py:48] [23] global_step=23, grad_norm=1.857944, loss=0.684315
I0810 05:50:14.691130 139925592147776 submission.py:120] 23) loss = 0.684, grad_norm = 1.858
I0810 05:50:15.000570 139861013018368 logging_writer.py:48] [24] global_step=24, grad_norm=1.880556, loss=0.604494
I0810 05:50:15.007372 139925592147776 submission.py:120] 24) loss = 0.604, grad_norm = 1.881
I0810 05:50:15.236311 139861004625664 logging_writer.py:48] [25] global_step=25, grad_norm=1.697344, loss=0.583227
I0810 05:50:15.243402 139925592147776 submission.py:120] 25) loss = 0.583, grad_norm = 1.697
I0810 05:50:15.485804 139861013018368 logging_writer.py:48] [26] global_step=26, grad_norm=1.563668, loss=0.683702
I0810 05:50:15.489904 139925592147776 submission.py:120] 26) loss = 0.684, grad_norm = 1.564
I0810 05:50:15.773219 139861004625664 logging_writer.py:48] [27] global_step=27, grad_norm=1.759118, loss=0.540954
I0810 05:50:15.776929 139925592147776 submission.py:120] 27) loss = 0.541, grad_norm = 1.759
I0810 05:50:16.089414 139861013018368 logging_writer.py:48] [28] global_step=28, grad_norm=1.510042, loss=0.608753
I0810 05:50:16.093154 139925592147776 submission.py:120] 28) loss = 0.609, grad_norm = 1.510
I0810 05:50:16.418207 139861004625664 logging_writer.py:48] [29] global_step=29, grad_norm=1.486663, loss=0.626170
I0810 05:50:16.424375 139925592147776 submission.py:120] 29) loss = 0.626, grad_norm = 1.487
I0810 05:50:16.660156 139861013018368 logging_writer.py:48] [30] global_step=30, grad_norm=1.481210, loss=0.632619
I0810 05:50:16.664504 139925592147776 submission.py:120] 30) loss = 0.633, grad_norm = 1.481
I0810 05:50:16.941640 139861004625664 logging_writer.py:48] [31] global_step=31, grad_norm=1.427925, loss=0.577243
I0810 05:50:16.947418 139925592147776 submission.py:120] 31) loss = 0.577, grad_norm = 1.428
I0810 05:50:17.206332 139861013018368 logging_writer.py:48] [32] global_step=32, grad_norm=1.320351, loss=0.600503
I0810 05:50:17.210072 139925592147776 submission.py:120] 32) loss = 0.601, grad_norm = 1.320
I0810 05:50:17.502083 139861004625664 logging_writer.py:48] [33] global_step=33, grad_norm=1.419147, loss=0.564898
I0810 05:50:17.506317 139925592147776 submission.py:120] 33) loss = 0.565, grad_norm = 1.419
I0810 05:50:17.771446 139861013018368 logging_writer.py:48] [34] global_step=34, grad_norm=1.196581, loss=0.621418
I0810 05:50:17.775554 139925592147776 submission.py:120] 34) loss = 0.621, grad_norm = 1.197
I0810 05:50:18.064896 139861004625664 logging_writer.py:48] [35] global_step=35, grad_norm=1.284841, loss=0.531280
I0810 05:50:18.076334 139925592147776 submission.py:120] 35) loss = 0.531, grad_norm = 1.285
I0810 05:50:18.292136 139861013018368 logging_writer.py:48] [36] global_step=36, grad_norm=1.286260, loss=0.521106
I0810 05:50:18.299180 139925592147776 submission.py:120] 36) loss = 0.521, grad_norm = 1.286
I0810 05:50:18.556424 139861004625664 logging_writer.py:48] [37] global_step=37, grad_norm=1.221163, loss=0.544835
I0810 05:50:18.563225 139925592147776 submission.py:120] 37) loss = 0.545, grad_norm = 1.221
I0810 05:50:18.826812 139861013018368 logging_writer.py:48] [38] global_step=38, grad_norm=1.245641, loss=0.579978
I0810 05:50:18.833625 139925592147776 submission.py:120] 38) loss = 0.580, grad_norm = 1.246
I0810 05:50:19.133332 139861004625664 logging_writer.py:48] [39] global_step=39, grad_norm=1.189379, loss=0.521379
I0810 05:50:19.138974 139925592147776 submission.py:120] 39) loss = 0.521, grad_norm = 1.189
I0810 05:50:19.403683 139861013018368 logging_writer.py:48] [40] global_step=40, grad_norm=1.182627, loss=0.581285
I0810 05:50:19.409902 139925592147776 submission.py:120] 40) loss = 0.581, grad_norm = 1.183
I0810 05:50:19.649453 139861004625664 logging_writer.py:48] [41] global_step=41, grad_norm=1.180509, loss=0.539508
I0810 05:50:19.656029 139925592147776 submission.py:120] 41) loss = 0.540, grad_norm = 1.181
I0810 05:50:19.953892 139861013018368 logging_writer.py:48] [42] global_step=42, grad_norm=1.137148, loss=0.519886
I0810 05:50:19.959261 139925592147776 submission.py:120] 42) loss = 0.520, grad_norm = 1.137
I0810 05:50:20.179559 139861004625664 logging_writer.py:48] [43] global_step=43, grad_norm=1.143949, loss=0.524281
I0810 05:50:20.184479 139925592147776 submission.py:120] 43) loss = 0.524, grad_norm = 1.144
I0810 05:50:20.468730 139861013018368 logging_writer.py:48] [44] global_step=44, grad_norm=1.058711, loss=0.593860
I0810 05:50:20.472946 139925592147776 submission.py:120] 44) loss = 0.594, grad_norm = 1.059
I0810 05:50:20.757281 139861004625664 logging_writer.py:48] [45] global_step=45, grad_norm=1.159064, loss=0.463866
I0810 05:50:20.761134 139925592147776 submission.py:120] 45) loss = 0.464, grad_norm = 1.159
I0810 05:50:21.032721 139861013018368 logging_writer.py:48] [46] global_step=46, grad_norm=1.107485, loss=0.475446
I0810 05:50:21.036699 139925592147776 submission.py:120] 46) loss = 0.475, grad_norm = 1.107
I0810 05:50:21.273685 139861004625664 logging_writer.py:48] [47] global_step=47, grad_norm=1.055074, loss=0.504089
I0810 05:50:21.277562 139925592147776 submission.py:120] 47) loss = 0.504, grad_norm = 1.055
I0810 05:50:21.554510 139861013018368 logging_writer.py:48] [48] global_step=48, grad_norm=1.033457, loss=0.552066
I0810 05:50:21.558304 139925592147776 submission.py:120] 48) loss = 0.552, grad_norm = 1.033
I0810 05:50:21.846255 139861004625664 logging_writer.py:48] [49] global_step=49, grad_norm=1.000432, loss=0.646531
I0810 05:50:21.850802 139925592147776 submission.py:120] 49) loss = 0.647, grad_norm = 1.000
I0810 05:50:22.128172 139861013018368 logging_writer.py:48] [50] global_step=50, grad_norm=1.003511, loss=0.648265
I0810 05:50:22.133563 139925592147776 submission.py:120] 50) loss = 0.648, grad_norm = 1.004
I0810 05:50:22.367298 139861004625664 logging_writer.py:48] [51] global_step=51, grad_norm=0.988277, loss=0.550541
I0810 05:50:22.373491 139925592147776 submission.py:120] 51) loss = 0.551, grad_norm = 0.988
I0810 05:50:22.665708 139861013018368 logging_writer.py:48] [52] global_step=52, grad_norm=1.043963, loss=0.592572
I0810 05:50:22.672286 139925592147776 submission.py:120] 52) loss = 0.593, grad_norm = 1.044
I0810 05:50:22.926778 139861004625664 logging_writer.py:48] [53] global_step=53, grad_norm=0.997407, loss=0.440555
I0810 05:50:22.934564 139925592147776 submission.py:120] 53) loss = 0.441, grad_norm = 0.997
I0810 05:50:23.205275 139861013018368 logging_writer.py:48] [54] global_step=54, grad_norm=1.032429, loss=0.493217
I0810 05:50:23.212250 139925592147776 submission.py:120] 54) loss = 0.493, grad_norm = 1.032
I0810 05:50:23.460782 139861004625664 logging_writer.py:48] [55] global_step=55, grad_norm=1.001225, loss=0.483060
I0810 05:50:23.465270 139925592147776 submission.py:120] 55) loss = 0.483, grad_norm = 1.001
I0810 05:50:23.716374 139861013018368 logging_writer.py:48] [56] global_step=56, grad_norm=0.950830, loss=0.483345
I0810 05:50:23.722622 139925592147776 submission.py:120] 56) loss = 0.483, grad_norm = 0.951
I0810 05:50:23.987244 139861004625664 logging_writer.py:48] [57] global_step=57, grad_norm=0.918338, loss=0.528528
I0810 05:50:23.993846 139925592147776 submission.py:120] 57) loss = 0.529, grad_norm = 0.918
I0810 05:50:24.282442 139861013018368 logging_writer.py:48] [58] global_step=58, grad_norm=0.875464, loss=0.483401
I0810 05:50:24.288192 139925592147776 submission.py:120] 58) loss = 0.483, grad_norm = 0.875
I0810 05:50:24.544899 139861004625664 logging_writer.py:48] [59] global_step=59, grad_norm=0.942625, loss=0.517422
I0810 05:50:24.549970 139925592147776 submission.py:120] 59) loss = 0.517, grad_norm = 0.943
I0810 05:50:24.847036 139861013018368 logging_writer.py:48] [60] global_step=60, grad_norm=0.931013, loss=0.493993
I0810 05:50:24.852719 139925592147776 submission.py:120] 60) loss = 0.494, grad_norm = 0.931
I0810 05:50:25.132186 139861004625664 logging_writer.py:48] [61] global_step=61, grad_norm=0.905474, loss=0.438688
I0810 05:50:25.138303 139925592147776 submission.py:120] 61) loss = 0.439, grad_norm = 0.905
I0810 05:50:25.368243 139861013018368 logging_writer.py:48] [62] global_step=62, grad_norm=0.918546, loss=0.516639
I0810 05:50:25.372059 139925592147776 submission.py:120] 62) loss = 0.517, grad_norm = 0.919
I0810 05:50:25.643777 139861004625664 logging_writer.py:48] [63] global_step=63, grad_norm=0.935152, loss=0.370886
I0810 05:50:25.647334 139925592147776 submission.py:120] 63) loss = 0.371, grad_norm = 0.935
I0810 05:50:25.924304 139861013018368 logging_writer.py:48] [64] global_step=64, grad_norm=0.844364, loss=0.489656
I0810 05:50:25.928138 139925592147776 submission.py:120] 64) loss = 0.490, grad_norm = 0.844
I0810 05:50:26.203552 139861004625664 logging_writer.py:48] [65] global_step=65, grad_norm=0.842226, loss=0.387015
I0810 05:50:26.208534 139925592147776 submission.py:120] 65) loss = 0.387, grad_norm = 0.842
I0810 05:50:26.483525 139861013018368 logging_writer.py:48] [66] global_step=66, grad_norm=0.804825, loss=0.373689
I0810 05:50:26.489791 139925592147776 submission.py:120] 66) loss = 0.374, grad_norm = 0.805
I0810 05:50:26.749006 139861004625664 logging_writer.py:48] [67] global_step=67, grad_norm=0.887989, loss=0.422067
I0810 05:50:26.754755 139925592147776 submission.py:120] 67) loss = 0.422, grad_norm = 0.888
I0810 05:50:26.998888 139861013018368 logging_writer.py:48] [68] global_step=68, grad_norm=0.801661, loss=0.356672
I0810 05:50:27.003575 139925592147776 submission.py:120] 68) loss = 0.357, grad_norm = 0.802
I0810 05:50:27.259402 139861004625664 logging_writer.py:48] [69] global_step=69, grad_norm=0.784196, loss=0.509112
I0810 05:50:27.263071 139925592147776 submission.py:120] 69) loss = 0.509, grad_norm = 0.784
I0810 05:50:27.566663 139861013018368 logging_writer.py:48] [70] global_step=70, grad_norm=0.766665, loss=0.422541
I0810 05:50:27.570368 139925592147776 submission.py:120] 70) loss = 0.423, grad_norm = 0.767
I0810 05:50:27.820806 139861004625664 logging_writer.py:48] [71] global_step=71, grad_norm=0.742968, loss=0.403301
I0810 05:50:27.824615 139925592147776 submission.py:120] 71) loss = 0.403, grad_norm = 0.743
I0810 05:50:28.050103 139861013018368 logging_writer.py:48] [72] global_step=72, grad_norm=0.746744, loss=0.449571
I0810 05:50:28.056396 139925592147776 submission.py:120] 72) loss = 0.450, grad_norm = 0.747
I0810 05:50:28.334555 139861004625664 logging_writer.py:48] [73] global_step=73, grad_norm=0.723037, loss=0.406075
I0810 05:50:28.339893 139925592147776 submission.py:120] 73) loss = 0.406, grad_norm = 0.723
I0810 05:50:28.604553 139861013018368 logging_writer.py:48] [74] global_step=74, grad_norm=0.676287, loss=0.436516
I0810 05:50:28.610393 139925592147776 submission.py:120] 74) loss = 0.437, grad_norm = 0.676
I0810 05:50:28.899781 139861004625664 logging_writer.py:48] [75] global_step=75, grad_norm=0.705983, loss=0.379583
I0810 05:50:28.905726 139925592147776 submission.py:120] 75) loss = 0.380, grad_norm = 0.706
I0810 05:50:29.164332 139861013018368 logging_writer.py:48] [76] global_step=76, grad_norm=0.722958, loss=0.402115
I0810 05:50:29.170391 139925592147776 submission.py:120] 76) loss = 0.402, grad_norm = 0.723
I0810 05:50:29.422372 139861004625664 logging_writer.py:48] [77] global_step=77, grad_norm=0.640833, loss=0.361030
I0810 05:50:29.428107 139925592147776 submission.py:120] 77) loss = 0.361, grad_norm = 0.641
I0810 05:50:29.685033 139861013018368 logging_writer.py:48] [78] global_step=78, grad_norm=0.648316, loss=0.316452
I0810 05:50:29.691374 139925592147776 submission.py:120] 78) loss = 0.316, grad_norm = 0.648
I0810 05:50:29.954315 139861004625664 logging_writer.py:48] [79] global_step=79, grad_norm=0.650267, loss=0.323314
I0810 05:50:29.961468 139925592147776 submission.py:120] 79) loss = 0.323, grad_norm = 0.650
I0810 05:50:30.213257 139861013018368 logging_writer.py:48] [80] global_step=80, grad_norm=0.591848, loss=0.403895
I0810 05:50:30.217849 139925592147776 submission.py:120] 80) loss = 0.404, grad_norm = 0.592
I0810 05:50:30.504814 139861004625664 logging_writer.py:48] [81] global_step=81, grad_norm=0.588964, loss=0.349900
I0810 05:50:30.509131 139925592147776 submission.py:120] 81) loss = 0.350, grad_norm = 0.589
I0810 05:50:30.779711 139861013018368 logging_writer.py:48] [82] global_step=82, grad_norm=0.554424, loss=0.378835
I0810 05:50:30.783458 139925592147776 submission.py:120] 82) loss = 0.379, grad_norm = 0.554
I0810 05:50:31.054587 139861004625664 logging_writer.py:48] [83] global_step=83, grad_norm=0.511798, loss=0.296089
I0810 05:50:31.059655 139925592147776 submission.py:120] 83) loss = 0.296, grad_norm = 0.512
I0810 05:50:31.329547 139861013018368 logging_writer.py:48] [84] global_step=84, grad_norm=0.546182, loss=0.408582
I0810 05:50:31.335348 139925592147776 submission.py:120] 84) loss = 0.409, grad_norm = 0.546
I0810 05:50:31.565029 139861004625664 logging_writer.py:48] [85] global_step=85, grad_norm=0.570873, loss=0.285473
I0810 05:50:31.571242 139925592147776 submission.py:120] 85) loss = 0.285, grad_norm = 0.571
I0810 05:50:31.803999 139861013018368 logging_writer.py:48] [86] global_step=86, grad_norm=0.585883, loss=0.359310
I0810 05:50:31.808024 139925592147776 submission.py:120] 86) loss = 0.359, grad_norm = 0.586
I0810 05:50:32.112924 139861004625664 logging_writer.py:48] [87] global_step=87, grad_norm=0.500173, loss=0.292376
I0810 05:50:32.118718 139925592147776 submission.py:120] 87) loss = 0.292, grad_norm = 0.500
I0810 05:50:32.342551 139861013018368 logging_writer.py:48] [88] global_step=88, grad_norm=0.502913, loss=0.287249
I0810 05:50:32.346360 139925592147776 submission.py:120] 88) loss = 0.287, grad_norm = 0.503
I0810 05:50:32.612698 139861004625664 logging_writer.py:48] [89] global_step=89, grad_norm=0.473593, loss=0.326248
I0810 05:50:32.616541 139925592147776 submission.py:120] 89) loss = 0.326, grad_norm = 0.474
I0810 05:50:32.881232 139861013018368 logging_writer.py:48] [90] global_step=90, grad_norm=0.459759, loss=0.312098
I0810 05:50:32.887652 139925592147776 submission.py:120] 90) loss = 0.312, grad_norm = 0.460
I0810 05:50:33.172638 139861004625664 logging_writer.py:48] [91] global_step=91, grad_norm=0.416718, loss=0.368909
I0810 05:50:33.179556 139925592147776 submission.py:120] 91) loss = 0.369, grad_norm = 0.417
I0810 05:50:33.368602 139861013018368 logging_writer.py:48] [92] global_step=92, grad_norm=0.355519, loss=0.318293
I0810 05:50:33.374241 139925592147776 submission.py:120] 92) loss = 0.318, grad_norm = 0.356
I0810 05:50:33.720310 139861004625664 logging_writer.py:48] [93] global_step=93, grad_norm=0.385155, loss=0.298899
I0810 05:50:33.725843 139925592147776 submission.py:120] 93) loss = 0.299, grad_norm = 0.385
I0810 05:50:33.934768 139861013018368 logging_writer.py:48] [94] global_step=94, grad_norm=0.375342, loss=0.269706
I0810 05:50:33.938236 139925592147776 submission.py:120] 94) loss = 0.270, grad_norm = 0.375
I0810 05:50:34.206762 139861004625664 logging_writer.py:48] [95] global_step=95, grad_norm=0.410220, loss=0.345202
I0810 05:50:34.210556 139925592147776 submission.py:120] 95) loss = 0.345, grad_norm = 0.410
I0810 05:50:34.463108 139861013018368 logging_writer.py:48] [96] global_step=96, grad_norm=0.365044, loss=0.401187
I0810 05:50:34.467481 139925592147776 submission.py:120] 96) loss = 0.401, grad_norm = 0.365
I0810 05:50:34.775924 139861004625664 logging_writer.py:48] [97] global_step=97, grad_norm=0.332432, loss=0.281793
I0810 05:50:34.782649 139925592147776 submission.py:120] 97) loss = 0.282, grad_norm = 0.332
I0810 05:50:35.018688 139861013018368 logging_writer.py:48] [98] global_step=98, grad_norm=0.319703, loss=0.439352
I0810 05:50:35.025012 139925592147776 submission.py:120] 98) loss = 0.439, grad_norm = 0.320
I0810 05:50:35.274549 139861004625664 logging_writer.py:48] [99] global_step=99, grad_norm=0.304652, loss=0.290751
I0810 05:50:35.280947 139925592147776 submission.py:120] 99) loss = 0.291, grad_norm = 0.305
I0810 05:50:35.545324 139861013018368 logging_writer.py:48] [100] global_step=100, grad_norm=0.318477, loss=0.315405
I0810 05:50:35.551947 139925592147776 submission.py:120] 100) loss = 0.315, grad_norm = 0.318
I0810 05:51:30.935775 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:51:33.170100 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:51:35.440024 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:51:37.661593 139925592147776 submission_runner.py:362] Time since start: 393.64s, 	Step: 304, 	{'train/ssim': 0.7005562782287598, 'train/loss': 0.3109074320111956, 'validation/ssim': 0.6819865710291221, 'validation/loss': 0.3252768529957442, 'validation/num_examples': 3554, 'test/ssim': 0.700016348763439, 'test/loss': 0.32750404696662944, 'test/num_examples': 3581, 'score': 166.82454133033752, 'total_duration': 393.6382083892822, 'accumulated_submission_time': 166.82454133033752, 'accumulated_eval_time': 226.51240611076355, 'accumulated_logging_time': 0.09405279159545898}
I0810 05:51:37.684954 139861004625664 logging_writer.py:48] [304] accumulated_eval_time=226.512406, accumulated_logging_time=0.094053, accumulated_submission_time=166.824541, global_step=304, preemption_count=0, score=166.824541, test/loss=0.327504, test/num_examples=3581, test/ssim=0.700016, total_duration=393.638208, train/loss=0.310907, train/ssim=0.700556, validation/loss=0.325277, validation/num_examples=3554, validation/ssim=0.681987
I0810 05:52:45.053592 139861013018368 logging_writer.py:48] [500] global_step=500, grad_norm=0.477819, loss=0.228171
I0810 05:52:45.060868 139925592147776 submission.py:120] 500) loss = 0.228, grad_norm = 0.478
I0810 05:52:57.970392 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:53:00.117936 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:53:02.384190 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:53:04.586157 139925592147776 submission_runner.py:362] Time since start: 480.56s, 	Step: 537, 	{'train/ssim': 0.7156028066362653, 'train/loss': 0.2977255753108433, 'validation/ssim': 0.6964126440938028, 'validation/loss': 0.3121408989540834, 'validation/num_examples': 3554, 'test/ssim': 0.7137059496954412, 'test/loss': 0.3145854899883936, 'test/num_examples': 3581, 'score': 246.7932460308075, 'total_duration': 480.5627598762512, 'accumulated_submission_time': 246.7932460308075, 'accumulated_eval_time': 233.12836289405823, 'accumulated_logging_time': 0.27129220962524414}
I0810 05:53:04.604878 139861004625664 logging_writer.py:48] [537] accumulated_eval_time=233.128363, accumulated_logging_time=0.271292, accumulated_submission_time=246.793246, global_step=537, preemption_count=0, score=246.793246, test/loss=0.314585, test/num_examples=3581, test/ssim=0.713706, total_duration=480.562760, train/loss=0.297726, train/ssim=0.715603, validation/loss=0.312141, validation/num_examples=3554, validation/ssim=0.696413
I0810 05:54:24.676644 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:54:26.803924 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:54:29.054328 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:54:31.256211 139925592147776 submission_runner.py:362] Time since start: 567.23s, 	Step: 765, 	{'train/ssim': 0.7208949497767857, 'train/loss': 0.2929708276476179, 'validation/ssim': 0.7024379165640827, 'validation/loss': 0.3068432397364765, 'validation/num_examples': 3554, 'test/ssim': 0.7197712182133134, 'test/loss': 0.3090341371452632, 'test/num_examples': 3581, 'score': 326.52577686309814, 'total_duration': 567.2327833175659, 'accumulated_submission_time': 326.52577686309814, 'accumulated_eval_time': 239.70779633522034, 'accumulated_logging_time': 0.4669365882873535}
I0810 05:54:31.279501 139861013018368 logging_writer.py:48] [765] accumulated_eval_time=239.707796, accumulated_logging_time=0.466937, accumulated_submission_time=326.525777, global_step=765, preemption_count=0, score=326.525777, test/loss=0.309034, test/num_examples=3581, test/ssim=0.719771, total_duration=567.232783, train/loss=0.292971, train/ssim=0.720895, validation/loss=0.306843, validation/num_examples=3554, validation/ssim=0.702438
I0810 05:55:51.463251 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:55:53.662604 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:55:57.606517 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:55:59.676142 139925592147776 submission_runner.py:362] Time since start: 655.65s, 	Step: 990, 	{'train/ssim': 0.7262475831168038, 'train/loss': 0.28947843824114117, 'validation/ssim': 0.7084004021656936, 'validation/loss': 0.3034041473955402, 'validation/num_examples': 3554, 'test/ssim': 0.7256078903850182, 'test/loss': 0.3054942685244171, 'test/num_examples': 3581, 'score': 406.39382910728455, 'total_duration': 655.6527743339539, 'accumulated_submission_time': 406.39382910728455, 'accumulated_eval_time': 247.92072129249573, 'accumulated_logging_time': 0.6484041213989258}
I0810 05:55:59.699390 139861004625664 logging_writer.py:48] [990] accumulated_eval_time=247.920721, accumulated_logging_time=0.648404, accumulated_submission_time=406.393829, global_step=990, preemption_count=0, score=406.393829, test/loss=0.305494, test/num_examples=3581, test/ssim=0.725608, total_duration=655.652774, train/loss=0.289478, train/ssim=0.726248, validation/loss=0.303404, validation/num_examples=3554, validation/ssim=0.708400
I0810 05:56:00.585289 139861013018368 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.248450, loss=0.284664
I0810 05:56:00.589901 139925592147776 submission.py:120] 1000) loss = 0.285, grad_norm = 0.248
I0810 05:57:19.986706 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:57:22.077077 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:57:24.317228 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:57:26.512278 139925592147776 submission_runner.py:362] Time since start: 742.49s, 	Step: 1300, 	{'train/ssim': 0.7287156922476632, 'train/loss': 0.2852769068309239, 'validation/ssim': 0.7101835080191333, 'validation/loss': 0.29916246164093274, 'validation/num_examples': 3554, 'test/ssim': 0.7273300328556968, 'test/loss': 0.3011670958160256, 'test/num_examples': 3581, 'score': 486.384033203125, 'total_duration': 742.488951921463, 'accumulated_submission_time': 486.384033203125, 'accumulated_eval_time': 254.44630789756775, 'accumulated_logging_time': 0.8408277034759521}
I0810 05:57:26.529377 139861004625664 logging_writer.py:48] [1300] accumulated_eval_time=254.446308, accumulated_logging_time=0.840828, accumulated_submission_time=486.384033, global_step=1300, preemption_count=0, score=486.384033, test/loss=0.301167, test/num_examples=3581, test/ssim=0.727330, total_duration=742.488952, train/loss=0.285277, train/ssim=0.728716, validation/loss=0.299162, validation/num_examples=3554, validation/ssim=0.710184
I0810 05:58:17.813238 139861013018368 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.109929, loss=0.286747
I0810 05:58:17.817030 139925592147776 submission.py:120] 1500) loss = 0.287, grad_norm = 0.110
I0810 05:58:46.710873 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 05:58:48.781078 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 05:58:50.958002 139925592147776 spec.py:348] Evaluating on the test split.
I0810 05:58:53.082375 139925592147776 submission_runner.py:362] Time since start: 829.06s, 	Step: 1609, 	{'train/ssim': 0.7289133071899414, 'train/loss': 0.28502796377454487, 'validation/ssim': 0.7096492700786086, 'validation/loss': 0.299383280447647, 'validation/num_examples': 3554, 'test/ssim': 0.7265948157375733, 'test/loss': 0.3014476086899609, 'test/num_examples': 3581, 'score': 566.2631950378418, 'total_duration': 829.0590381622314, 'accumulated_submission_time': 566.2631950378418, 'accumulated_eval_time': 260.8178081512451, 'accumulated_logging_time': 1.0308380126953125}
I0810 05:58:53.098278 139861004625664 logging_writer.py:48] [1609] accumulated_eval_time=260.817808, accumulated_logging_time=1.030838, accumulated_submission_time=566.263195, global_step=1609, preemption_count=0, score=566.263195, test/loss=0.301448, test/num_examples=3581, test/ssim=0.726595, total_duration=829.059038, train/loss=0.285028, train/ssim=0.728913, validation/loss=0.299383, validation/num_examples=3554, validation/ssim=0.709649
I0810 06:00:13.380857 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 06:00:15.488972 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 06:00:17.662018 139925592147776 spec.py:348] Evaluating on the test split.
I0810 06:00:19.805245 139925592147776 submission_runner.py:362] Time since start: 915.78s, 	Step: 1919, 	{'train/ssim': 0.7355875968933105, 'train/loss': 0.2802081618990217, 'validation/ssim': 0.7162448451568655, 'validation/loss': 0.2946417384021525, 'validation/num_examples': 3554, 'test/ssim': 0.7333162164461743, 'test/loss': 0.2964682580372103, 'test/num_examples': 3581, 'score': 646.2301194667816, 'total_duration': 915.781857252121, 'accumulated_submission_time': 646.2301194667816, 'accumulated_eval_time': 267.24221086502075, 'accumulated_logging_time': 1.2335209846496582}
I0810 06:00:19.822583 139861013018368 logging_writer.py:48] [1919] accumulated_eval_time=267.242211, accumulated_logging_time=1.233521, accumulated_submission_time=646.230119, global_step=1919, preemption_count=0, score=646.230119, test/loss=0.296468, test/num_examples=3581, test/ssim=0.733316, total_duration=915.781857, train/loss=0.280208, train/ssim=0.735588, validation/loss=0.294642, validation/num_examples=3554, validation/ssim=0.716245
I0810 06:00:39.544313 139861004625664 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.073820, loss=0.315466
I0810 06:00:39.548068 139925592147776 submission.py:120] 2000) loss = 0.315, grad_norm = 0.074
I0810 06:01:40.002215 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 06:01:42.090143 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 06:01:44.293232 139925592147776 spec.py:348] Evaluating on the test split.
I0810 06:01:46.422466 139925592147776 submission_runner.py:362] Time since start: 1002.40s, 	Step: 2224, 	{'train/ssim': 0.7366048949105399, 'train/loss': 0.27916877610342844, 'validation/ssim': 0.7164764833638154, 'validation/loss': 0.2941859840474641, 'validation/num_examples': 3554, 'test/ssim': 0.7335473353288188, 'test/loss': 0.296009667723314, 'test/num_examples': 3581, 'score': 726.116669178009, 'total_duration': 1002.3991317749023, 'accumulated_submission_time': 726.116669178009, 'accumulated_eval_time': 273.6624393463135, 'accumulated_logging_time': 1.4155004024505615}
I0810 06:01:46.439122 139861013018368 logging_writer.py:48] [2224] accumulated_eval_time=273.662439, accumulated_logging_time=1.415500, accumulated_submission_time=726.116669, global_step=2224, preemption_count=0, score=726.116669, test/loss=0.296010, test/num_examples=3581, test/ssim=0.733547, total_duration=1002.399132, train/loss=0.279169, train/ssim=0.736605, validation/loss=0.294186, validation/num_examples=3554, validation/ssim=0.716476
I0810 06:02:57.809863 139861004625664 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.251874, loss=0.298579
I0810 06:02:57.813718 139925592147776 submission.py:120] 2500) loss = 0.299, grad_norm = 0.252
I0810 06:03:06.561416 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 06:03:08.640723 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 06:03:10.808667 139925592147776 spec.py:348] Evaluating on the test split.
I0810 06:03:12.943003 139925592147776 submission_runner.py:362] Time since start: 1088.92s, 	Step: 2534, 	{'train/ssim': 0.738417489188058, 'train/loss': 0.27806070872715544, 'validation/ssim': 0.7186231897597777, 'validation/loss': 0.2929373222183631, 'validation/num_examples': 3554, 'test/ssim': 0.7357467826069882, 'test/loss': 0.29463952135620286, 'test/num_examples': 3581, 'score': 805.933347940445, 'total_duration': 1088.9196608066559, 'accumulated_submission_time': 805.933347940445, 'accumulated_eval_time': 280.0441982746124, 'accumulated_logging_time': 1.6090195178985596}
I0810 06:03:12.959058 139861013018368 logging_writer.py:48] [2534] accumulated_eval_time=280.044198, accumulated_logging_time=1.609020, accumulated_submission_time=805.933348, global_step=2534, preemption_count=0, score=805.933348, test/loss=0.294640, test/num_examples=3581, test/ssim=0.735747, total_duration=1088.919661, train/loss=0.278061, train/ssim=0.738417, validation/loss=0.292937, validation/num_examples=3554, validation/ssim=0.718623
I0810 06:03:58.443690 139925592147776 spec.py:320] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html
  warn_deprecated('vmap', 'torch.vmap')
I0810 06:04:00.511005 139925592147776 spec.py:332] Evaluating on the validation split.
I0810 06:04:02.683246 139925592147776 spec.py:348] Evaluating on the test split.
I0810 06:04:04.756002 139925592147776 submission_runner.py:362] Time since start: 1140.73s, 	Step: 2714, 	{'train/ssim': 0.7303018569946289, 'train/loss': 0.27964612415858675, 'validation/ssim': 0.7125635012925576, 'validation/loss': 0.29364113279051773, 'validation/num_examples': 3554, 'test/ssim': 0.7293906724509565, 'test/loss': 0.29519764959595785, 'test/num_examples': 3581, 'score': 851.1671271324158, 'total_duration': 1140.7326662540436, 'accumulated_submission_time': 851.1671271324158, 'accumulated_eval_time': 286.35650420188904, 'accumulated_logging_time': 1.7999389171600342}
I0810 06:04:04.772249 139861004625664 logging_writer.py:48] [2714] accumulated_eval_time=286.356504, accumulated_logging_time=1.799939, accumulated_submission_time=851.167127, global_step=2714, preemption_count=0, score=851.167127, test/loss=0.295198, test/num_examples=3581, test/ssim=0.729391, total_duration=1140.732666, train/loss=0.279646, train/ssim=0.730302, validation/loss=0.293641, validation/num_examples=3554, validation/ssim=0.712564
I0810 06:04:04.962544 139861013018368 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=851.167127
I0810 06:04:05.115686 139925592147776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_pytorch_2_preliminary_2/adamw/fastmri_pytorch/trial_1/checkpoint_2714.
I0810 06:04:06.315679 139925592147776 submission_runner.py:528] Tuning trial 1/1
I0810 06:04:06.315906 139925592147776 submission_runner.py:529] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0810 06:04:06.321116 139925592147776 submission_runner.py:530] Metrics: {'eval_results': [(1, {'train/ssim': 0.310445989881243, 'train/loss': 0.8099030085972377, 'validation/ssim': 0.3026097973344295, 'validation/loss': 0.8176845878983188, 'validation/num_examples': 3554, 'test/ssim': 0.32400648196427323, 'test/loss': 0.819239467523911, 'test/num_examples': 3581, 'score': 87.05508780479431, 'total_duration': 306.8419408798218, 'accumulated_submission_time': 87.05508780479431, 'accumulated_eval_time': 219.78624653816223, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (304, {'train/ssim': 0.7005562782287598, 'train/loss': 0.3109074320111956, 'validation/ssim': 0.6819865710291221, 'validation/loss': 0.3252768529957442, 'validation/num_examples': 3554, 'test/ssim': 0.700016348763439, 'test/loss': 0.32750404696662944, 'test/num_examples': 3581, 'score': 166.82454133033752, 'total_duration': 393.6382083892822, 'accumulated_submission_time': 166.82454133033752, 'accumulated_eval_time': 226.51240611076355, 'accumulated_logging_time': 0.09405279159545898, 'global_step': 304, 'preemption_count': 0}), (537, {'train/ssim': 0.7156028066362653, 'train/loss': 0.2977255753108433, 'validation/ssim': 0.6964126440938028, 'validation/loss': 0.3121408989540834, 'validation/num_examples': 3554, 'test/ssim': 0.7137059496954412, 'test/loss': 0.3145854899883936, 'test/num_examples': 3581, 'score': 246.7932460308075, 'total_duration': 480.5627598762512, 'accumulated_submission_time': 246.7932460308075, 'accumulated_eval_time': 233.12836289405823, 'accumulated_logging_time': 0.27129220962524414, 'global_step': 537, 'preemption_count': 0}), (765, {'train/ssim': 0.7208949497767857, 'train/loss': 0.2929708276476179, 'validation/ssim': 0.7024379165640827, 'validation/loss': 0.3068432397364765, 'validation/num_examples': 3554, 'test/ssim': 0.7197712182133134, 'test/loss': 0.3090341371452632, 'test/num_examples': 3581, 'score': 326.52577686309814, 'total_duration': 567.2327833175659, 'accumulated_submission_time': 326.52577686309814, 'accumulated_eval_time': 239.70779633522034, 'accumulated_logging_time': 0.4669365882873535, 'global_step': 765, 'preemption_count': 0}), (990, {'train/ssim': 0.7262475831168038, 'train/loss': 0.28947843824114117, 'validation/ssim': 0.7084004021656936, 'validation/loss': 0.3034041473955402, 'validation/num_examples': 3554, 'test/ssim': 0.7256078903850182, 'test/loss': 0.3054942685244171, 'test/num_examples': 3581, 'score': 406.39382910728455, 'total_duration': 655.6527743339539, 'accumulated_submission_time': 406.39382910728455, 'accumulated_eval_time': 247.92072129249573, 'accumulated_logging_time': 0.6484041213989258, 'global_step': 990, 'preemption_count': 0}), (1300, {'train/ssim': 0.7287156922476632, 'train/loss': 0.2852769068309239, 'validation/ssim': 0.7101835080191333, 'validation/loss': 0.29916246164093274, 'validation/num_examples': 3554, 'test/ssim': 0.7273300328556968, 'test/loss': 0.3011670958160256, 'test/num_examples': 3581, 'score': 486.384033203125, 'total_duration': 742.488951921463, 'accumulated_submission_time': 486.384033203125, 'accumulated_eval_time': 254.44630789756775, 'accumulated_logging_time': 0.8408277034759521, 'global_step': 1300, 'preemption_count': 0}), (1609, {'train/ssim': 0.7289133071899414, 'train/loss': 0.28502796377454487, 'validation/ssim': 0.7096492700786086, 'validation/loss': 0.299383280447647, 'validation/num_examples': 3554, 'test/ssim': 0.7265948157375733, 'test/loss': 0.3014476086899609, 'test/num_examples': 3581, 'score': 566.2631950378418, 'total_duration': 829.0590381622314, 'accumulated_submission_time': 566.2631950378418, 'accumulated_eval_time': 260.8178081512451, 'accumulated_logging_time': 1.0308380126953125, 'global_step': 1609, 'preemption_count': 0}), (1919, {'train/ssim': 0.7355875968933105, 'train/loss': 0.2802081618990217, 'validation/ssim': 0.7162448451568655, 'validation/loss': 0.2946417384021525, 'validation/num_examples': 3554, 'test/ssim': 0.7333162164461743, 'test/loss': 0.2964682580372103, 'test/num_examples': 3581, 'score': 646.2301194667816, 'total_duration': 915.781857252121, 'accumulated_submission_time': 646.2301194667816, 'accumulated_eval_time': 267.24221086502075, 'accumulated_logging_time': 1.2335209846496582, 'global_step': 1919, 'preemption_count': 0}), (2224, {'train/ssim': 0.7366048949105399, 'train/loss': 0.27916877610342844, 'validation/ssim': 0.7164764833638154, 'validation/loss': 0.2941859840474641, 'validation/num_examples': 3554, 'test/ssim': 0.7335473353288188, 'test/loss': 0.296009667723314, 'test/num_examples': 3581, 'score': 726.116669178009, 'total_duration': 1002.3991317749023, 'accumulated_submission_time': 726.116669178009, 'accumulated_eval_time': 273.6624393463135, 'accumulated_logging_time': 1.4155004024505615, 'global_step': 2224, 'preemption_count': 0}), (2534, {'train/ssim': 0.738417489188058, 'train/loss': 0.27806070872715544, 'validation/ssim': 0.7186231897597777, 'validation/loss': 0.2929373222183631, 'validation/num_examples': 3554, 'test/ssim': 0.7357467826069882, 'test/loss': 0.29463952135620286, 'test/num_examples': 3581, 'score': 805.933347940445, 'total_duration': 1088.9196608066559, 'accumulated_submission_time': 805.933347940445, 'accumulated_eval_time': 280.0441982746124, 'accumulated_logging_time': 1.6090195178985596, 'global_step': 2534, 'preemption_count': 0}), (2714, {'train/ssim': 0.7303018569946289, 'train/loss': 0.27964612415858675, 'validation/ssim': 0.7125635012925576, 'validation/loss': 0.29364113279051773, 'validation/num_examples': 3554, 'test/ssim': 0.7293906724509565, 'test/loss': 0.29519764959595785, 'test/num_examples': 3581, 'score': 851.1671271324158, 'total_duration': 1140.7326662540436, 'accumulated_submission_time': 851.1671271324158, 'accumulated_eval_time': 286.35650420188904, 'accumulated_logging_time': 1.7999389171600342, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0810 06:04:06.321292 139925592147776 submission_runner.py:531] Timing: 851.1671271324158
I0810 06:04:06.321341 139925592147776 submission_runner.py:533] Total number of evals: 11
I0810 06:04:06.321384 139925592147776 submission_runner.py:534] ====================
I0810 06:04:06.321523 139925592147776 submission_runner.py:602] Final fastmri score: 851.1671271324158
