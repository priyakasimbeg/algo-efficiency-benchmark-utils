I0410 23:23:47.849980 140083301586752 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax.
I0410 23:23:47.916970 140083301586752 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0410 23:23:48.857456 140083301586752 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0410 23:23:48.858139 140083301586752 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0410 23:23:48.862209 140083301586752 submission_runner.py:511] Using RNG seed 128905863
I0410 23:23:51.436717 140083301586752 submission_runner.py:520] --- Tuning run 1/1 ---
I0410 23:23:51.436901 140083301586752 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1.
I0410 23:23:51.437084 140083301586752 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/hparams.json.
I0410 23:23:51.561270 140083301586752 submission_runner.py:230] Starting train once: RAM USED (GB) 4.285722624
I0410 23:23:51.561450 140083301586752 submission_runner.py:231] Initializing dataset.
I0410 23:23:51.561617 140083301586752 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.285722624
I0410 23:23:51.561676 140083301586752 submission_runner.py:240] Initializing model.
I0410 23:23:57.025866 140083301586752 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.74250496
I0410 23:23:57.026058 140083301586752 submission_runner.py:252] Initializing optimizer.
I0410 23:23:57.684542 140083301586752 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.742713856
I0410 23:23:57.684708 140083301586752 submission_runner.py:261] Initializing metrics bundle.
I0410 23:23:57.684755 140083301586752 submission_runner.py:276] Initializing checkpoint and logger.
I0410 23:23:57.685656 140083301586752 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0410 23:23:57.685884 140083301586752 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0410 23:23:57.685943 140083301586752 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0410 23:23:58.304010 140083301586752 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0410 23:23:58.304862 140083301586752 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/flags_0.json.
I0410 23:23:58.310962 140083301586752 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.742435328
I0410 23:23:58.311177 140083301586752 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.742435328
I0410 23:23:58.311263 140083301586752 submission_runner.py:313] Starting training loop.
I0410 23:23:58.502443 140083301586752 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:23:58.534084 140083301586752 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:23:58.850555 140083301586752 input_pipeline.py:20] Loading split = train-other-500
I0410 23:24:03.391878 140083301586752 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.865079296
2023-04-10 23:24:54.183270: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-10 23:24:54.461841: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0410 23:24:56.294106 139908173780736 logging_writer.py:48] [0] global_step=0, grad_norm=90.76520538330078, loss=31.96085548400879
I0410 23:24:56.319606 140083301586752 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.119451136
I0410 23:24:56.319846 140083301586752 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.119451136
I0410 23:24:56.319922 140083301586752 spec.py:298] Evaluating on the training split.
I0410 23:24:56.418734 140083301586752 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:24:56.443899 140083301586752 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:24:56.724225 140083301586752 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0410 23:25:56.968633 140083301586752 spec.py:310] Evaluating on the validation split.
I0410 23:25:57.028282 140083301586752 input_pipeline.py:20] Loading split = dev-clean
I0410 23:25:57.033936 140083301586752 input_pipeline.py:20] Loading split = dev-other
I0410 23:26:39.726524 140083301586752 spec.py:326] Evaluating on the test split.
I0410 23:26:39.786873 140083301586752 input_pipeline.py:20] Loading split = test-clean
I0410 23:27:09.129217 140083301586752 submission_runner.py:382] Time since start: 58.01s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.013662, dtype=float32), 'train/wer': 1.8845136705328696, 'validation/ctc_loss': DeviceArray(29.171618, dtype=float32), 'validation/wer': 1.5327692500651235, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.328405, dtype=float32), 'test/wer': 1.588832693518575, 'test/num_examples': 2472}
I0410 23:27:09.130300 140083301586752 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.715040768
I0410 23:27:09.149449 139904499578624 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=57.817373, test/ctc_loss=29.328405380249023, test/num_examples=2472, test/wer=1.588833, total_duration=58.008672, train/ctc_loss=31.013662338256836, train/wer=1.884514, validation/ctc_loss=29.17161750793457, validation/num_examples=5348, validation/wer=1.532769
I0410 23:27:09.318700 140083301586752 checkpoints.py:356] Saving checkpoint at step: 1
I0410 23:27:09.885218 140083301586752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_1
I0410 23:27:09.886396 140083301586752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_1.
I0410 23:27:09.894197 140083301586752 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.669083648
I0410 23:27:09.929907 140083301586752 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.666785792
I0410 23:27:23.095104 140083301586752 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.993069056
I0410 23:28:38.831720 139909961688832 logging_writer.py:48] [100] global_step=100, grad_norm=77.27749633789062, loss=11.828872680664062
I0410 23:29:55.747114 139909970081536 logging_writer.py:48] [200] global_step=200, grad_norm=1.6039515733718872, loss=5.936753749847412
I0410 23:31:12.735064 139909961688832 logging_writer.py:48] [300] global_step=300, grad_norm=0.34864309430122375, loss=5.828203201293945
I0410 23:32:29.692487 139909970081536 logging_writer.py:48] [400] global_step=400, grad_norm=0.8247246742248535, loss=5.8206892013549805
I0410 23:33:46.698853 139909961688832 logging_writer.py:48] [500] global_step=500, grad_norm=0.671023428440094, loss=5.824085235595703
I0410 23:35:03.704126 139909970081536 logging_writer.py:48] [600] global_step=600, grad_norm=2.131375789642334, loss=5.830258369445801
I0410 23:36:20.722596 139909961688832 logging_writer.py:48] [700] global_step=700, grad_norm=2.67659068107605, loss=5.853797435760498
I0410 23:37:37.746054 139909970081536 logging_writer.py:48] [800] global_step=800, grad_norm=161.80747985839844, loss=252.85824584960938
I0410 23:38:52.071212 139909961688832 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1836.4290771484375
I0410 23:40:06.124597 139909970081536 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1834.5865478515625
I0410 23:41:22.836788 139910776510208 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1821.5328369140625
I0410 23:42:37.003290 139910768117504 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1814.6893310546875
I0410 23:43:50.982183 139910776510208 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1845.830322265625
I0410 23:45:05.042201 139910768117504 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1792.2025146484375
I0410 23:46:19.213737 139910776510208 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1869.1458740234375
I0410 23:47:33.358804 139910768117504 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1822.3109130859375
I0410 23:48:49.771877 139910776510208 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1868.8729248046875
I0410 23:50:11.531825 139910768117504 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1832.3543701171875
I0410 23:51:33.327592 139910776510208 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1868.60009765625
I0410 23:52:51.421313 139910768117504 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1843.703369140625
I0410 23:54:09.778862 139910121150208 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1887.0574951171875
I0410 23:55:23.865268 139910112757504 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1794.715576171875
I0410 23:56:37.920673 139910121150208 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1818.5567626953125
I0410 23:57:52.005901 139910112757504 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1803.820556640625
I0410 23:59:06.110509 139910121150208 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1886.22314453125
I0411 00:00:20.208585 139910112757504 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1806.4937744140625
I0411 00:01:34.268124 139910121150208 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1829.212158203125
I0411 00:02:52.041269 139910112757504 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1807.0037841796875
I0411 00:04:11.779526 139910121150208 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1753.17236328125
I0411 00:05:32.322813 139910112757504 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1777.8887939453125
I0411 00:06:53.218885 139910776510208 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1884.69580078125
I0411 00:07:09.923182 140083301586752 submission_runner.py:373] Before eval at step 3124: RAM USED (GB) 22.434336768
I0411 00:07:09.923376 140083301586752 spec.py:298] Evaluating on the training split.
I0411 00:07:35.725891 140083301586752 spec.py:310] Evaluating on the validation split.
I0411 00:08:09.703892 140083301586752 spec.py:326] Evaluating on the test split.
I0411 00:08:27.432398 140083301586752 submission_runner.py:382] Time since start: 2591.61s, 	Step: 3124, 	{'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:08:27.433807 140083301586752 submission_runner.py:396] After eval at step 3124: RAM USED (GB) 21.162442752
I0411 00:08:27.453006 139910776510208 logging_writer.py:48] [3124] global_step=3124, preemption_count=0, score=2452.214880, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=2591.609010, train/ctc_loss=1767.6815185546875, train/wer=0.944636, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 00:08:27.656731 140083301586752 checkpoints.py:356] Saving checkpoint at step: 3124
I0411 00:08:28.521585 140083301586752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_3124
I0411 00:08:28.543179 140083301586752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_3124.
I0411 00:08:28.551328 140083301586752 submission_runner.py:416] After logging and checkpointing eval at step 3124: RAM USED (GB) 21.1852288
I0411 00:09:25.495833 139910768117504 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1804.07470703125
I0411 00:10:39.600176 139906177300224 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1825.1690673828125
I0411 00:11:53.671225 139910768117504 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1801.028564453125
I0411 00:13:07.782372 139906177300224 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1821.7921142578125
I0411 00:14:21.902801 139910768117504 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1799.5093994140625
I0411 00:15:36.089888 139906177300224 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1826.0804443359375
I0411 00:16:52.494103 139910768117504 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1862.8890380859375
I0411 00:18:12.010012 139906177300224 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1813.4039306640625
I0411 00:19:30.670021 139910768117504 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1828.297607421875
I0411 00:20:50.439841 139906177300224 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1763.9234619140625
I0411 00:22:08.645859 139910121150208 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1819.5906982421875
I0411 00:23:22.585335 139910112757504 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1755.0955810546875
I0411 00:24:36.566086 139910121150208 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1788.8212890625
I0411 00:25:50.553944 139910112757504 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1795.0931396484375
I0411 00:27:04.544272 139910121150208 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1776.2852783203125
I0411 00:28:18.498022 139910112757504 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1793.081298828125
I0411 00:29:32.513391 139910121150208 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1865.604248046875
I0411 00:30:47.479118 139910112757504 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1885.9453125
I0411 00:32:07.368252 139910121150208 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1836.692626953125
I0411 00:33:28.792496 139910112757504 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1841.8463134765625
I0411 00:34:49.012389 139909793470208 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1866.0123291015625
I0411 00:36:03.138701 139909785077504 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1839.067626953125
I0411 00:37:17.277308 139909793470208 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1789.3214111328125
I0411 00:38:31.362722 139909785077504 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1845.5643310546875
I0411 00:39:45.437170 139909793470208 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1892.4981689453125
I0411 00:40:59.536198 139909785077504 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1775.7923583984375
I0411 00:42:13.586948 139909793470208 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1849.0301513671875
I0411 00:43:29.185663 139909785077504 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1775.91552734375
I0411 00:44:49.616748 139909793470208 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1823.73876953125
I0411 00:46:11.416859 139909785077504 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1807.8970947265625
I0411 00:47:32.644304 139910121150208 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1818.8150634765625
I0411 00:48:29.268603 140083301586752 submission_runner.py:373] Before eval at step 6278: RAM USED (GB) 21.597433856
I0411 00:48:29.268827 140083301586752 spec.py:298] Evaluating on the training split.
I0411 00:48:55.233298 140083301586752 spec.py:310] Evaluating on the validation split.
I0411 00:49:29.835225 140083301586752 spec.py:326] Evaluating on the test split.
I0411 00:49:47.491715 140083301586752 submission_runner.py:382] Time since start: 5070.95s, 	Step: 6278, 	{'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:49:47.493031 140083301586752 submission_runner.py:396] After eval at step 6278: RAM USED (GB) 21.484711936
I0411 00:49:47.513146 139910121150208 logging_writer.py:48] [6278] global_step=6278, preemption_count=0, score=4847.138377, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=5070.953568, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 00:49:47.703940 140083301586752 checkpoints.py:356] Saving checkpoint at step: 6278
I0411 00:49:48.674071 140083301586752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_6278
I0411 00:49:48.695769 140083301586752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_6278.
I0411 00:49:48.706991 140083301586752 submission_runner.py:416] After logging and checkpointing eval at step 6278: RAM USED (GB) 21.498994688
I0411 00:50:05.728922 139910112757504 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1776.2852783203125
I0411 00:51:19.226394 139906168907520 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1782.4691162109375
I0411 00:52:33.286213 139910112757504 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1916.010986328125
I0411 00:53:47.348559 139906168907520 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1833.666748046875
I0411 00:55:01.481661 139910112757504 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1869.009521484375
I0411 00:56:15.647478 139906168907520 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1902.3424072265625
I0411 00:57:29.726800 139910112757504 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1857.88671875
I0411 00:58:43.849062 139906168907520 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1854.3875732421875
I0411 01:00:04.032456 139910112757504 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1782.2210693359375
I0411 01:01:22.830528 139906168907520 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1788.6962890625
I0411 01:02:40.622217 139910121150208 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1761.7384033203125
I0411 01:03:54.756859 139910112757504 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1768.43212890625
I0411 01:05:08.911010 139910121150208 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1729.7177734375
I0411 01:06:23.039774 139910112757504 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1817.7818603515625
I0411 01:07:37.187953 139910121150208 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1837.0880126953125
I0411 01:08:51.191968 139910112757504 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1810.582275390625
I0411 01:10:07.578496 139910121150208 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1727.616455078125
I0411 01:11:27.596300 139910112757504 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1815.847900390625
I0411 01:12:48.510570 139910121150208 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1784.954833984375
I0411 01:14:10.520752 139910112757504 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1809.8143310546875
I0411 01:15:29.742302 139910776510208 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1893.758056640625
I0411 01:16:43.820546 139910768117504 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1871.87939453125
I0411 01:17:57.954850 139910776510208 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1791.1993408203125
I0411 01:19:12.274867 139910768117504 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1827.9058837890625
I0411 01:20:26.382734 139910776510208 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1829.7349853515625
I0411 01:21:40.456970 139910768117504 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1786.9482421875
I0411 01:22:57.606776 139910776510208 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1835.375732421875
I0411 01:24:19.670798 139910768117504 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1785.701904296875
I0411 01:25:42.896912 139910776510208 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1856.53955078125
I0411 01:27:05.657435 139910768117504 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1845.6973876953125
I0411 01:28:26.959084 139910776510208 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1877.508056640625
I0411 01:29:41.112469 139910768117504 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1806.4937744140625
I0411 01:29:48.997485 140083301586752 submission_runner.py:373] Before eval at step 9412: RAM USED (GB) 22.531452928
I0411 01:29:48.997704 140083301586752 spec.py:298] Evaluating on the training split.
I0411 01:30:15.694994 140083301586752 spec.py:310] Evaluating on the validation split.
I0411 01:30:50.672381 140083301586752 spec.py:326] Evaluating on the test split.
I0411 01:31:08.297098 140083301586752 submission_runner.py:382] Time since start: 7550.68s, 	Step: 9412, 	{'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:31:08.298458 140083301586752 submission_runner.py:396] After eval at step 9412: RAM USED (GB) 21.333417984
I0411 01:31:08.318445 139910776510208 logging_writer.py:48] [9412] global_step=9412, preemption_count=0, score=7241.573849, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7550.682779, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 01:31:08.513813 140083301586752 checkpoints.py:356] Saving checkpoint at step: 9412
I0411 01:31:09.416657 140083301586752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_9412
I0411 01:31:09.437918 140083301586752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_9412.
I0411 01:31:09.445933 140083301586752 submission_runner.py:416] After logging and checkpointing eval at step 9412: RAM USED (GB) 21.349167104
I0411 01:32:15.311213 139910768117504 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1885.2508544921875
I0411 01:33:29.407152 139906160514816 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1856.4049072265625
I0411 01:34:43.516621 139910768117504 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1823.998779296875
I0411 01:35:57.622518 139906160514816 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1808.53564453125
I0411 01:37:13.087742 139910768117504 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1842.50927734375
I0411 01:38:34.635243 140083301586752 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.529835008
I0411 01:38:34.635469 140083301586752 spec.py:298] Evaluating on the training split.
I0411 01:39:00.879966 140083301586752 spec.py:310] Evaluating on the validation split.
I0411 01:39:36.296588 140083301586752 spec.py:326] Evaluating on the test split.
I0411 01:39:54.233316 140083301586752 submission_runner.py:382] Time since start: 8076.32s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:39:54.234825 140083301586752 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 22.356738048
I0411 01:39:54.251082 139910776510208 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7685.657964, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=8076.323069, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.895995
I0411 01:39:54.453532 140083301586752 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:39:55.392989 140083301586752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:39:55.414278 140083301586752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:39:55.427205 140083301586752 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.3725568
I0411 01:39:55.434478 139910768117504 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7685.657964
I0411 01:39:55.603345 140083301586752 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:39:56.909085 140083301586752 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:39:56.930678 140083301586752 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:39:58.521607 140083301586752 submission_runner.py:550] Tuning trial 1/1
I0411 01:39:58.521862 140083301586752 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 01:39:58.527100 140083301586752 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.013662, dtype=float32), 'train/wer': 1.8845136705328696, 'validation/ctc_loss': DeviceArray(29.171618, dtype=float32), 'validation/wer': 1.5327692500651235, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.328405, dtype=float32), 'test/wer': 1.588832693518575, 'test/num_examples': 2472, 'score': 57.817373275756836, 'total_duration': 58.0086715221405, 'global_step': 1, 'preemption_count': 0}), (3124, {'train/ctc_loss': DeviceArray(1767.6815, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2452.2148797512054, 'total_duration': 2591.6090095043182, 'global_step': 3124, 'preemption_count': 0}), (6278, {'train/ctc_loss': DeviceArray(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4847.138377428055, 'total_duration': 5070.953568220139, 'global_step': 6278, 'preemption_count': 0}), (9412, {'train/ctc_loss': DeviceArray(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7241.573848962784, 'total_duration': 7550.682778835297, 'global_step': 9412, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.9224, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7685.657963991165, 'total_duration': 8076.323068618774, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 01:39:58.527260 140083301586752 submission_runner.py:553] Timing: 7685.657963991165
I0411 01:39:58.527330 140083301586752 submission_runner.py:554] ====================
I0411 01:39:58.527776 140083301586752 submission_runner.py:613] Final librispeech_conformer score: 7685.657963991165
