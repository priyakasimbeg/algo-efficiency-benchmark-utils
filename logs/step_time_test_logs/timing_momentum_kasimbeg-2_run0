I0410 23:23:22.576785 140687375083328 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax.
I0410 23:23:22.649463 140687375083328 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0410 23:23:23.685823 140687375083328 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0410 23:23:23.686486 140687375083328 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0410 23:23:23.690560 140687375083328 submission_runner.py:511] Using RNG seed 2716315558
I0410 23:23:26.179556 140687375083328 submission_runner.py:520] --- Tuning run 1/1 ---
I0410 23:23:26.179773 140687375083328 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1.
I0410 23:23:26.179962 140687375083328 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/hparams.json.
I0410 23:23:26.300482 140687375083328 submission_runner.py:230] Starting train once: RAM USED (GB) 4.367826944
I0410 23:23:26.300634 140687375083328 submission_runner.py:231] Initializing dataset.
I0410 23:23:26.300786 140687375083328 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.367826944
I0410 23:23:26.300843 140687375083328 submission_runner.py:240] Initializing model.
I0410 23:23:32.411563 140687375083328 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.804956672
I0410 23:23:32.411829 140687375083328 submission_runner.py:252] Initializing optimizer.
I0410 23:23:33.073463 140687375083328 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.806287872
I0410 23:23:33.073662 140687375083328 submission_runner.py:261] Initializing metrics bundle.
I0410 23:23:33.073710 140687375083328 submission_runner.py:276] Initializing checkpoint and logger.
I0410 23:23:33.074551 140687375083328 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0410 23:23:33.074846 140687375083328 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0410 23:23:33.074914 140687375083328 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0410 23:23:33.728668 140687375083328 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0410 23:23:33.729545 140687375083328 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/flags_0.json.
I0410 23:23:33.736500 140687375083328 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.80412928
I0410 23:23:33.736691 140687375083328 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.80412928
I0410 23:23:33.736750 140687375083328 submission_runner.py:313] Starting training loop.
I0410 23:23:33.928474 140687375083328 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:23:33.961128 140687375083328 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:23:34.316392 140687375083328 input_pipeline.py:20] Loading split = train-other-500
I0410 23:23:39.241547 140687375083328 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.733011968
2023-04-10 23:24:29.886110: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-10 23:24:30.176836: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0410 23:24:31.881520 140512036124416 logging_writer.py:48] [0] global_step=0, grad_norm=101.22785949707031, loss=30.947431564331055
I0410 23:24:31.907385 140687375083328 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.909819392
I0410 23:24:31.907687 140687375083328 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.909819392
I0410 23:24:31.907777 140687375083328 spec.py:298] Evaluating on the training split.
I0410 23:24:32.010164 140687375083328 input_pipeline.py:20] Loading split = train-clean-100
I0410 23:24:32.035798 140687375083328 input_pipeline.py:20] Loading split = train-clean-360
I0410 23:24:32.325378 140687375083328 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0410 23:25:15.159775 140687375083328 spec.py:310] Evaluating on the validation split.
I0410 23:25:15.220902 140687375083328 input_pipeline.py:20] Loading split = dev-clean
I0410 23:25:15.226241 140687375083328 input_pipeline.py:20] Loading split = dev-other
I0410 23:25:53.163012 140687375083328 spec.py:326] Evaluating on the test split.
I0410 23:25:53.226321 140687375083328 input_pipeline.py:20] Loading split = test-clean
I0410 23:26:20.764835 140687375083328 submission_runner.py:382] Time since start: 58.17s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(29.91766, dtype=float32), 'train/wer': 0.9454480144930554, 'validation/ctc_loss': DeviceArray(29.020447, dtype=float32), 'validation/wer': 0.9127632683383342, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.188421, dtype=float32), 'test/wer': 0.9162147340198648, 'test/num_examples': 2472}
I0410 23:26:20.765836 140687375083328 submission_runner.py:396] After eval at step 1: RAM USED (GB) 21.385158656
I0410 23:26:20.783993 140508470961920 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=57.979160, test/ctc_loss=29.18842124938965, test/num_examples=2472, test/wer=0.916215, total_duration=58.170942, train/ctc_loss=29.917659759521484, train/wer=0.945448, validation/ctc_loss=29.02044677734375, validation/num_examples=5348, validation/wer=0.912763
I0410 23:26:20.964687 140687375083328 checkpoints.py:356] Saving checkpoint at step: 1
I0410 23:26:21.522561 140687375083328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_1
I0410 23:26:21.523606 140687375083328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_1.
I0410 23:26:21.532122 140687375083328 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 21.343408128
I0410 23:26:21.568960 140687375083328 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 21.34151168
I0410 23:26:34.868114 140687375083328 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 21.700419584
I0410 23:27:50.192609 140514251863808 logging_writer.py:48] [100] global_step=100, grad_norm=37.288116455078125, loss=7.574113845825195
I0410 23:29:06.311932 140514260256512 logging_writer.py:48] [200] global_step=200, grad_norm=4.034768104553223, loss=8.923959732055664
I0410 23:30:22.466670 140514251863808 logging_writer.py:48] [300] global_step=300, grad_norm=0.36066800355911255, loss=8.363012313842773
I0410 23:31:38.653943 140514260256512 logging_writer.py:48] [400] global_step=400, grad_norm=0.6117107272148132, loss=8.346025466918945
I0410 23:32:54.173026 140514251863808 logging_writer.py:48] [500] global_step=500, grad_norm=nan, loss=nan
I0410 23:34:07.177745 140514260256512 logging_writer.py:48] [600] global_step=600, grad_norm=nan, loss=nan
I0410 23:35:20.866025 140514251863808 logging_writer.py:48] [700] global_step=700, grad_norm=nan, loss=nan
I0410 23:36:42.514755 140514260256512 logging_writer.py:48] [800] global_step=800, grad_norm=nan, loss=nan
I0410 23:38:03.572890 140514251863808 logging_writer.py:48] [900] global_step=900, grad_norm=nan, loss=nan
I0410 23:39:26.223653 140514260256512 logging_writer.py:48] [1000] global_step=1000, grad_norm=nan, loss=nan
I0410 23:40:43.829719 140511021098752 logging_writer.py:48] [1100] global_step=1100, grad_norm=nan, loss=nan
I0410 23:41:56.758845 140511012706048 logging_writer.py:48] [1200] global_step=1200, grad_norm=nan, loss=nan
I0410 23:43:09.625693 140511021098752 logging_writer.py:48] [1300] global_step=1300, grad_norm=nan, loss=nan
I0410 23:44:22.512381 140511012706048 logging_writer.py:48] [1400] global_step=1400, grad_norm=nan, loss=nan
I0410 23:45:37.560741 140511021098752 logging_writer.py:48] [1500] global_step=1500, grad_norm=nan, loss=nan
I0410 23:46:58.847244 140511012706048 logging_writer.py:48] [1600] global_step=1600, grad_norm=nan, loss=nan
I0410 23:48:18.099935 140511021098752 logging_writer.py:48] [1700] global_step=1700, grad_norm=nan, loss=nan
I0410 23:49:42.039544 140511012706048 logging_writer.py:48] [1800] global_step=1800, grad_norm=nan, loss=nan
I0410 23:51:03.883017 140511021098752 logging_writer.py:48] [1900] global_step=1900, grad_norm=nan, loss=nan
I0410 23:52:25.520479 140511012706048 logging_writer.py:48] [2000] global_step=2000, grad_norm=nan, loss=nan
I0410 23:53:43.207015 140515024721664 logging_writer.py:48] [2100] global_step=2100, grad_norm=nan, loss=nan
I0410 23:54:56.204029 140515016328960 logging_writer.py:48] [2200] global_step=2200, grad_norm=nan, loss=nan
I0410 23:56:09.219008 140515024721664 logging_writer.py:48] [2300] global_step=2300, grad_norm=nan, loss=nan
I0410 23:57:22.191053 140515016328960 logging_writer.py:48] [2400] global_step=2400, grad_norm=nan, loss=nan
I0410 23:58:35.087378 140515024721664 logging_writer.py:48] [2500] global_step=2500, grad_norm=nan, loss=nan
I0410 23:59:48.036277 140515016328960 logging_writer.py:48] [2600] global_step=2600, grad_norm=nan, loss=nan
I0411 00:01:03.402449 140515024721664 logging_writer.py:48] [2700] global_step=2700, grad_norm=nan, loss=nan
I0411 00:02:25.696700 140515016328960 logging_writer.py:48] [2800] global_step=2800, grad_norm=nan, loss=nan
I0411 00:03:48.972716 140515024721664 logging_writer.py:48] [2900] global_step=2900, grad_norm=nan, loss=nan
I0411 00:05:11.361219 140515016328960 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0411 00:06:22.417610 140687375083328 submission_runner.py:373] Before eval at step 3088: RAM USED (GB) 21.780901888
I0411 00:06:22.417822 140687375083328 spec.py:298] Evaluating on the training split.
I0411 00:06:47.989634 140687375083328 spec.py:310] Evaluating on the validation split.
I0411 00:07:23.491022 140687375083328 spec.py:326] Evaluating on the test split.
I0411 00:07:40.969641 140687375083328 submission_runner.py:382] Time since start: 2568.68s, 	Step: 3088, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:07:40.970901 140687375083328 submission_runner.py:396] After eval at step 3088: RAM USED (GB) 21.305339904
I0411 00:07:40.989239 140515024721664 logging_writer.py:48] [3088] global_step=3088, preemption_count=0, score=2453.431024, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2568.675355, train/ctc_loss=nan, train/wer=0.944636, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 00:07:41.202892 140687375083328 checkpoints.py:356] Saving checkpoint at step: 3088
I0411 00:07:42.142494 140687375083328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_3088
I0411 00:07:42.162940 140687375083328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_3088.
I0411 00:07:42.175402 140687375083328 submission_runner.py:416] After logging and checkpointing eval at step 3088: RAM USED (GB) 21.325553664
I0411 00:07:54.895108 140514369361664 logging_writer.py:48] [3100] global_step=3100, grad_norm=nan, loss=nan
I0411 00:09:07.807269 140514360968960 logging_writer.py:48] [3200] global_step=3200, grad_norm=nan, loss=nan
I0411 00:10:20.689254 140514369361664 logging_writer.py:48] [3300] global_step=3300, grad_norm=nan, loss=nan
I0411 00:11:33.590116 140514360968960 logging_writer.py:48] [3400] global_step=3400, grad_norm=nan, loss=nan
I0411 00:12:46.486110 140514369361664 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0411 00:14:06.522592 140514360968960 logging_writer.py:48] [3600] global_step=3600, grad_norm=nan, loss=nan
I0411 00:15:29.309109 140514369361664 logging_writer.py:48] [3700] global_step=3700, grad_norm=nan, loss=nan
I0411 00:16:50.906309 140514360968960 logging_writer.py:48] [3800] global_step=3800, grad_norm=nan, loss=nan
I0411 00:18:11.524171 140514369361664 logging_writer.py:48] [3900] global_step=3900, grad_norm=nan, loss=nan
I0411 00:19:34.595112 140514360968960 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0411 00:20:56.380270 140514369361664 logging_writer.py:48] [4100] global_step=4100, grad_norm=nan, loss=nan
I0411 00:22:13.287155 140514041681664 logging_writer.py:48] [4200] global_step=4200, grad_norm=nan, loss=nan
I0411 00:23:26.414613 140514033288960 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0411 00:24:39.492054 140514041681664 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0411 00:25:52.546703 140514033288960 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0411 00:27:05.605690 140514041681664 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0411 00:28:24.466845 140514033288960 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0411 00:29:47.186024 140514041681664 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0411 00:31:10.086222 140514033288960 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0411 00:32:33.562486 140514041681664 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0411 00:33:55.848279 140514033288960 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0411 00:35:16.148542 140514369361664 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0411 00:36:29.217910 140514360968960 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0411 00:37:42.260500 140514369361664 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0411 00:38:55.187706 140514360968960 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0411 00:40:08.221426 140514369361664 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0411 00:41:21.241961 140514360968960 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0411 00:42:37.481235 140514369361664 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0411 00:43:59.817923 140514360968960 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0411 00:45:22.606669 140514369361664 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0411 00:46:44.458267 140514360968960 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0411 00:47:42.316699 140687375083328 submission_runner.py:373] Before eval at step 6174: RAM USED (GB) 22.855266304
I0411 00:47:42.316936 140687375083328 spec.py:298] Evaluating on the training split.
I0411 00:48:07.703820 140687375083328 spec.py:310] Evaluating on the validation split.
I0411 00:48:43.041543 140687375083328 spec.py:326] Evaluating on the test split.
I0411 00:49:00.799597 140687375083328 submission_runner.py:382] Time since start: 5048.58s, 	Step: 6174, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 00:49:00.800885 140687375083328 submission_runner.py:396] After eval at step 6174: RAM USED (GB) 21.490622464
I0411 00:49:00.820188 140514369361664 logging_writer.py:48] [6174] global_step=6174, preemption_count=0, score=4847.844953, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5048.576541, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 00:49:01.013832 140687375083328 checkpoints.py:356] Saving checkpoint at step: 6174
I0411 00:49:01.981080 140687375083328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_6174
I0411 00:49:02.002747 140687375083328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_6174.
I0411 00:49:02.014247 140687375083328 submission_runner.py:416] After logging and checkpointing eval at step 6174: RAM USED (GB) 21.520453632
I0411 00:49:25.140891 140514369361664 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0411 00:50:38.311507 140514360968960 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0411 00:51:51.479084 140514369361664 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0411 00:53:04.448613 140514360968960 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0411 00:54:17.357199 140514369361664 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0411 00:55:34.040725 140514360968960 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0411 00:56:53.823395 140514369361664 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0411 00:58:11.866254 140514360968960 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0411 00:59:29.958766 140514369361664 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0411 01:00:51.824066 140514360968960 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0411 01:02:15.343486 140514369361664 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0411 01:03:31.928276 140514369361664 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0411 01:04:44.932488 140514360968960 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0411 01:05:57.911504 140514369361664 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0411 01:07:10.824048 140514360968960 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0411 01:08:23.732669 140514369361664 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0411 01:09:43.211916 140514360968960 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0411 01:11:07.176511 140514369361664 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0411 01:12:28.413533 140514360968960 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0411 01:13:50.680357 140514369361664 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0411 01:15:14.161773 140514360968960 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0411 01:16:33.324318 140514369361664 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0411 01:17:46.393952 140514360968960 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0411 01:18:59.375053 140514369361664 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0411 01:20:12.394297 140514360968960 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0411 01:21:25.384896 140514369361664 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0411 01:22:39.831059 140514360968960 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0411 01:24:03.725788 140514369361664 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0411 01:25:28.148094 140514360968960 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0411 01:26:48.529828 140514369361664 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0411 01:28:11.480624 140514360968960 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0411 01:29:02.576618 140687375083328 submission_runner.py:373] Before eval at step 9266: RAM USED (GB) 21.707411456
I0411 01:29:02.576870 140687375083328 spec.py:298] Evaluating on the training split.
I0411 01:29:28.275821 140687375083328 spec.py:310] Evaluating on the validation split.
I0411 01:30:02.857289 140687375083328 spec.py:326] Evaluating on the test split.
I0411 01:30:20.572993 140687375083328 submission_runner.py:382] Time since start: 7528.84s, 	Step: 9266, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:30:20.574316 140687375083328 submission_runner.py:396] After eval at step 9266: RAM USED (GB) 21.555257344
I0411 01:30:20.594210 140514441041664 logging_writer.py:48] [9266] global_step=9266, preemption_count=0, score=7242.483152, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7528.835085, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 01:30:20.803943 140687375083328 checkpoints.py:356] Saving checkpoint at step: 9266
I0411 01:30:21.768804 140687375083328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_9266
I0411 01:30:21.790057 140687375083328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_9266.
I0411 01:30:21.803661 140687375083328 submission_runner.py:416] After logging and checkpointing eval at step 9266: RAM USED (GB) 21.56926976
I0411 01:30:50.530090 140514441041664 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0411 01:32:03.404934 140514432648960 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0411 01:33:16.422173 140514441041664 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0411 01:34:29.760313 140514432648960 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0411 01:35:48.708063 140514441041664 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0411 01:37:14.303517 140514432648960 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0411 01:38:40.101921 140514441041664 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0411 01:40:02.165030 140687375083328 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 23.61327616
I0411 01:40:02.165294 140687375083328 spec.py:298] Evaluating on the training split.
I0411 01:40:28.025994 140687375083328 spec.py:310] Evaluating on the validation split.
I0411 01:41:04.311828 140687375083328 spec.py:326] Evaluating on the test split.
I0411 01:41:22.086473 140687375083328 submission_runner.py:382] Time since start: 8188.43s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 01:41:22.087791 140687375083328 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.877071872
I0411 01:41:22.103208 140514441041664 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7821.429289, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=8188.427095, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 01:41:22.293329 140687375083328 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:41:23.227756 140687375083328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:41:23.248882 140687375083328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:41:23.260609 140687375083328 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.889880064
I0411 01:41:23.267449 140514432648960 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7821.429289
I0411 01:41:23.427656 140687375083328 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 01:41:24.640676 140687375083328 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 01:41:24.661941 140687375083328 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-2_run0/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 01:41:26.074638 140687375083328 submission_runner.py:550] Tuning trial 1/1
I0411 01:41:26.074907 140687375083328 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 01:41:26.079443 140687375083328 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(29.91766, dtype=float32), 'train/wer': 0.9454480144930554, 'validation/ctc_loss': DeviceArray(29.020447, dtype=float32), 'validation/wer': 0.9127632683383342, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(29.188421, dtype=float32), 'test/wer': 0.9162147340198648, 'test/num_examples': 2472, 'score': 57.97916030883789, 'total_duration': 58.17094159126282, 'global_step': 1, 'preemption_count': 0}), (3088, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2453.4310235977173, 'total_duration': 2568.675354719162, 'global_step': 3088, 'preemption_count': 0}), (6174, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4847.844953298569, 'total_duration': 5048.576541185379, 'global_step': 6174, 'preemption_count': 0}), (9266, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.483151912689, 'total_duration': 7528.835084915161, 'global_step': 9266, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7821.429288864136, 'total_duration': 8188.427094697952, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 01:41:26.079601 140687375083328 submission_runner.py:553] Timing: 7821.429288864136
I0411 01:41:26.079651 140687375083328 submission_runner.py:554] ====================
I0411 01:41:26.080089 140687375083328 submission_runner.py:613] Final librispeech_conformer score: 7821.429288864136
