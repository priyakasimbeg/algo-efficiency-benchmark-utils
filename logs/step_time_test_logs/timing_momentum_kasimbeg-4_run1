I0411 01:53:51.428608 140157788407616 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax.
I0411 01:53:51.485123 140157788407616 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0411 01:53:52.401375 140157788407616 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0411 01:53:52.402065 140157788407616 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0411 01:53:52.407619 140157788407616 submission_runner.py:511] Using RNG seed 3901197056
I0411 01:53:54.794932 140157788407616 submission_runner.py:520] --- Tuning run 1/1 ---
I0411 01:53:54.795134 140157788407616 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1.
I0411 01:53:54.795316 140157788407616 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/hparams.json.
I0411 01:53:54.925123 140157788407616 submission_runner.py:230] Starting train once: RAM USED (GB) 4.325388288
I0411 01:53:54.925292 140157788407616 submission_runner.py:231] Initializing dataset.
I0411 01:53:54.925445 140157788407616 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.325388288
I0411 01:53:54.925503 140157788407616 submission_runner.py:240] Initializing model.
I0411 01:54:00.548465 140157788407616 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.764647936
I0411 01:54:00.548677 140157788407616 submission_runner.py:252] Initializing optimizer.
I0411 01:54:01.238131 140157788407616 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.815999488
I0411 01:54:01.238323 140157788407616 submission_runner.py:261] Initializing metrics bundle.
I0411 01:54:01.238377 140157788407616 submission_runner.py:276] Initializing checkpoint and logger.
I0411 01:54:01.239441 140157788407616 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0411 01:54:01.239711 140157788407616 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0411 01:54:01.239776 140157788407616 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0411 01:54:01.934177 140157788407616 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0411 01:54:01.935060 140157788407616 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/flags_0.json.
I0411 01:54:01.940011 140157788407616 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.76247296
I0411 01:54:01.940207 140157788407616 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.76247296
I0411 01:54:01.940269 140157788407616 submission_runner.py:313] Starting training loop.
I0411 01:54:02.131982 140157788407616 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:54:02.162508 140157788407616 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:54:02.488896 140157788407616 input_pipeline.py:20] Loading split = train-other-500
I0411 01:54:05.784214 140157788407616 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 9.611620352
2023-04-11 01:54:58.516028: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-11 01:54:58.635466: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0411 01:55:00.307682 139982731728640 logging_writer.py:48] [0] global_step=0, grad_norm=66.25446319580078, loss=31.635204315185547
I0411 01:55:00.324983 140157788407616 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.116448768
I0411 01:55:00.325252 140157788407616 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.116448768
I0411 01:55:00.325336 140157788407616 spec.py:298] Evaluating on the training split.
I0411 01:55:00.428311 140157788407616 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:55:00.454861 140157788407616 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:55:00.755913 140157788407616 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0411 01:55:55.356392 140157788407616 spec.py:310] Evaluating on the validation split.
I0411 01:55:55.421830 140157788407616 input_pipeline.py:20] Loading split = dev-clean
I0411 01:55:55.427134 140157788407616 input_pipeline.py:20] Loading split = dev-other
I0411 01:56:34.156151 140157788407616 spec.py:326] Evaluating on the test split.
I0411 01:56:34.233408 140157788407616 input_pipeline.py:20] Loading split = test-clean
I0411 01:57:02.130748 140157788407616 submission_runner.py:382] Time since start: 58.39s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.362164, dtype=float32), 'train/wer': 1.5019886304479104, 'validation/ctc_loss': DeviceArray(30.539703, dtype=float32), 'validation/wer': 1.3959034819438683, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.570782, dtype=float32), 'test/wer': 1.4307070460869742, 'test/num_examples': 2472}
I0411 01:57:02.132095 140157788407616 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.614946816
I0411 01:57:02.145699 139979460179712 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=58.193241, test/ctc_loss=30.570781707763672, test/num_examples=2472, test/wer=1.430707, total_duration=58.385011, train/ctc_loss=31.362163543701172, train/wer=1.501989, validation/ctc_loss=30.539703369140625, validation/num_examples=5348, validation/wer=1.395903
I0411 01:57:02.376756 140157788407616 checkpoints.py:356] Saving checkpoint at step: 1
I0411 01:57:03.309511 140157788407616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_1
I0411 01:57:03.327810 140157788407616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_1.
I0411 01:57:03.342736 140157788407616 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.607758336
I0411 01:57:03.395548 140157788407616 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.607082496
I0411 01:57:17.193088 140157788407616 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.885635072
I0411 01:58:32.741637 139984486098688 logging_writer.py:48] [100] global_step=100, grad_norm=15.973198890686035, loss=6.592584133148193
I0411 01:59:49.099350 139984494491392 logging_writer.py:48] [200] global_step=200, grad_norm=2.144763469696045, loss=8.583688735961914
I0411 02:01:05.610630 139984486098688 logging_writer.py:48] [300] global_step=300, grad_norm=64.11009979248047, loss=37.460472106933594
I0411 02:02:20.170246 139984494491392 logging_writer.py:48] [400] global_step=400, grad_norm=0.0, loss=1913.99755859375
I0411 02:03:33.844935 139984486098688 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1866.4127197265625
I0411 02:04:47.120584 139984494491392 logging_writer.py:48] [600] global_step=600, grad_norm=0.0, loss=1778.2520751953125
I0411 02:06:00.366601 139984486098688 logging_writer.py:48] [700] global_step=700, grad_norm=0.0, loss=1832.4779052734375
I0411 02:07:21.867767 139984494491392 logging_writer.py:48] [800] global_step=800, grad_norm=0.0, loss=1781.09765625
I0411 02:08:48.097168 139984486098688 logging_writer.py:48] [900] global_step=900, grad_norm=0.0, loss=1831.4290771484375
I0411 02:10:13.810180 139984494491392 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1839.8531494140625
I0411 02:11:32.930077 139981360305920 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0, loss=1814.810546875
I0411 02:12:46.634624 139981351913216 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0, loss=1845.556640625
I0411 02:14:00.415851 139981360305920 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0, loss=1820.87744140625
I0411 02:15:14.317240 139981351913216 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0, loss=1807.63427734375
I0411 02:16:28.024817 139981360305920 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1819.0660400390625
I0411 02:17:47.089264 139981351913216 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0, loss=1789.939453125
I0411 02:19:06.114592 139981360305920 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0, loss=1759.4306640625
I0411 02:20:25.434604 139981351913216 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0, loss=1864.3736572265625
I0411 02:21:47.970982 139981360305920 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0, loss=1844.2271728515625
I0411 02:23:11.724149 139981351913216 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1710.867919921875
I0411 02:24:35.293353 139984611989248 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0, loss=1827.7679443359375
I0411 02:25:49.031301 139984603596544 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.0, loss=1863.5595703125
I0411 02:27:02.743701 139984611989248 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.0, loss=1824.9013671875
I0411 02:28:16.501000 139984603596544 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0, loss=1860.1741943359375
I0411 02:29:30.402637 139984611989248 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1830.6434326171875
I0411 02:30:49.403326 139984603596544 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0, loss=1931.762451171875
I0411 02:32:12.975466 139984611989248 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0, loss=1828.5511474609375
I0411 02:33:35.261387 139984603596544 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0, loss=1804.1944580078125
I0411 02:34:56.733306 139984611989248 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0, loss=1836.9486083984375
I0411 02:36:20.477745 139984603596544 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1812.6258544921875
I0411 02:37:03.636852 140157788407616 submission_runner.py:373] Before eval at step 3052: RAM USED (GB) 22.883954688
I0411 02:37:03.637248 140157788407616 spec.py:298] Evaluating on the training split.
I0411 02:37:30.005869 140157788407616 spec.py:310] Evaluating on the validation split.
I0411 02:38:05.459571 140157788407616 spec.py:326] Evaluating on the test split.
I0411 02:38:24.114502 140157788407616 submission_runner.py:382] Time since start: 2581.69s, 	Step: 3052, 	{'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 02:38:24.115845 140157788407616 submission_runner.py:396] After eval at step 3052: RAM USED (GB) 20.529061888
I0411 02:38:24.132366 139984611989248 logging_writer.py:48] [3052] global_step=3052, preemption_count=0, score=2452.489950, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=2581.694787, train/ctc_loss=1767.6744384765625, train/wer=0.944636, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 02:38:24.359127 140157788407616 checkpoints.py:356] Saving checkpoint at step: 3052
I0411 02:38:25.308942 140157788407616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_3052
I0411 02:38:25.327421 140157788407616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_3052.
I0411 02:38:25.341651 140157788407616 submission_runner.py:416] After logging and checkpointing eval at step 3052: RAM USED (GB) 20.550455296
I0411 02:39:05.032779 139985267349248 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0, loss=1831.953369140625
I0411 02:40:18.781350 139985258956544 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0, loss=1829.20458984375
I0411 02:41:32.621976 139985267349248 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.0, loss=1811.343505859375
I0411 02:42:46.508103 139985258956544 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0, loss=1833.2652587890625
I0411 02:44:00.332115 139985267349248 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1822.9520263671875
I0411 02:45:19.323224 139985258956544 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0, loss=1778.2520751953125
I0411 02:46:40.824116 139985267349248 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.0, loss=1832.3468017578125
I0411 02:48:03.502770 139985258956544 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0, loss=1840.6468505859375
I0411 02:49:22.477475 139985267349248 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.0, loss=1866.2764892578125
I0411 02:50:43.666973 139985258956544 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1847.95458984375
I0411 02:52:06.109540 139985267349248 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0, loss=1858.28369140625
I0411 02:53:24.199829 139985267349248 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0, loss=1823.8612060546875
I0411 02:54:38.062017 139985258956544 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0, loss=1772.09716796875
I0411 02:55:51.877766 139985267349248 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0, loss=1836.1580810546875
I0411 02:57:05.635807 139985258956544 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1804.575927734375
I0411 02:58:24.053103 139985267349248 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0, loss=1794.45654296875
I0411 02:59:45.455620 139985258956544 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0, loss=1810.190673828125
I0411 03:01:10.174890 139985267349248 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0, loss=1828.681640625
I0411 03:02:33.917028 139985258956544 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0, loss=1847.6878662109375
I0411 03:03:55.260202 139985267349248 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1862.4747314453125
I0411 03:05:17.757825 139985258956544 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0, loss=1769.0357666015625
I0411 03:06:39.199103 139984611989248 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0, loss=1795.71533203125
I0411 03:07:52.989600 139984603596544 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0, loss=1790.18994140625
I0411 03:09:06.767391 139984611989248 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0, loss=1896.69677734375
I0411 03:10:20.514637 139984603596544 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1878.18896484375
I0411 03:11:34.235127 139984611989248 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0, loss=1820.4888916015625
I0411 03:12:55.068870 139984603596544 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0, loss=1868.0469970703125
I0411 03:14:20.468109 139984611989248 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0, loss=1823.9912109375
I0411 03:15:41.662126 139984603596544 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0, loss=1782.4617919921875
I0411 03:17:05.729089 139984611989248 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1809.6788330078125
I0411 03:18:26.064758 140157788407616 submission_runner.py:373] Before eval at step 6098: RAM USED (GB) 22.481936384
I0411 03:18:26.064995 140157788407616 spec.py:298] Evaluating on the training split.
I0411 03:18:52.986231 140157788407616 spec.py:310] Evaluating on the validation split.
I0411 03:19:29.096827 140157788407616 spec.py:326] Evaluating on the test split.
I0411 03:19:47.605343 140157788407616 submission_runner.py:382] Time since start: 5064.12s, 	Step: 6098, 	{'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 03:19:47.606639 140157788407616 submission_runner.py:396] After eval at step 6098: RAM USED (GB) 20.86414336
I0411 03:19:47.661793 139984611989248 logging_writer.py:48] [6098] global_step=6098, preemption_count=0, score=4847.239883, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=5064.120633, train/ctc_loss=1761.5635986328125, train/wer=0.942722, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 03:19:47.926593 140157788407616 checkpoints.py:356] Saving checkpoint at step: 6098
I0411 03:19:48.870644 140157788407616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_6098
I0411 03:19:48.889321 140157788407616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_6098.
I0411 03:19:48.903771 140157788407616 submission_runner.py:416] After logging and checkpointing eval at step 6098: RAM USED (GB) 20.879110144
I0411 03:19:51.176729 139984603596544 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.0, loss=1852.099365234375
I0411 03:21:08.590115 139984611989248 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0, loss=1843.0321044921875
I0411 03:22:22.360548 139984603596544 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0, loss=1763.5516357421875
I0411 03:23:36.158301 139984611989248 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0, loss=1886.3544921875
I0411 03:24:49.827790 139984603596544 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1854.91748046875
I0411 03:26:03.519243 139984611989248 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0, loss=1825.291748046875
I0411 03:27:25.136622 139984603596544 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.0, loss=1885.6597900390625
I0411 03:28:45.866438 139984611989248 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0, loss=1806.358642578125
I0411 03:30:05.639397 139984603596544 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0, loss=1781.345458984375
I0411 03:31:26.264484 139984611989248 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1818.936767578125
I0411 03:32:48.188825 139984603596544 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0, loss=1829.3350830078125
I0411 03:34:09.763909 139984611989248 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0, loss=1836.5533447265625
I0411 03:35:27.422861 139984284309248 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0, loss=1828.159423828125
I0411 03:36:41.269358 139984275916544 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0, loss=1846.62158203125
I0411 03:37:55.000380 139984284309248 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1790.315185546875
I0411 03:39:08.795238 139984275916544 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.0, loss=1782.213623046875
I0411 03:40:22.534815 139984284309248 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0, loss=1849.423095703125
I0411 03:41:44.700609 139984275916544 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0, loss=1801.9085693359375
I0411 03:43:07.355077 139984284309248 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0, loss=1855.993408203125
I0411 03:44:30.198996 139984275916544 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1868.7288818359375
I0411 03:45:52.131219 139984284309248 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0, loss=1763.673095703125
I0411 03:47:16.246821 139984275916544 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0, loss=1762.094970703125
I0411 03:48:37.905086 139985267349248 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0, loss=1867.09326171875
I0411 03:49:51.574827 139985258956544 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0, loss=1797.9854736328125
I0411 03:51:05.286098 139985267349248 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1839.45654296875
I0411 03:52:19.050728 139985258956544 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0, loss=1825.9427490234375
I0411 03:53:37.768081 139985267349248 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0, loss=1847.421142578125
I0411 03:54:59.378737 139985258956544 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.0, loss=1851.4295654296875
I0411 03:56:20.880298 139985267349248 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.0, loss=1821.0069580078125
I0411 03:57:39.738409 139985258956544 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1816.4844970703125
I0411 03:58:59.960707 139985267349248 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.0, loss=1850.35888671875
I0411 03:59:49.527065 140157788407616 submission_runner.py:373] Before eval at step 9163: RAM USED (GB) 22.02093568
I0411 03:59:49.527287 140157788407616 spec.py:298] Evaluating on the training split.
I0411 04:00:16.699962 140157788407616 spec.py:310] Evaluating on the validation split.
I0411 04:00:51.405308 140157788407616 spec.py:326] Evaluating on the test split.
I0411 04:01:10.263570 140157788407616 submission_runner.py:382] Time since start: 7547.58s, 	Step: 9163, 	{'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:01:10.264786 140157788407616 submission_runner.py:396] After eval at step 9163: RAM USED (GB) 20.899823616
I0411 04:01:10.283895 139985267349248 logging_writer.py:48] [9163] global_step=9163, preemption_count=0, score=7241.959775, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=7547.583514, train/ctc_loss=1741.2908935546875, train/wer=0.943324, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 04:01:10.473511 140157788407616 checkpoints.py:356] Saving checkpoint at step: 9163
I0411 04:01:11.432334 140157788407616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_9163
I0411 04:01:11.450878 140157788407616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_9163.
I0411 04:01:11.460707 140157788407616 submission_runner.py:416] After logging and checkpointing eval at step 9163: RAM USED (GB) 20.9135616
I0411 04:01:39.415189 139985258956544 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0, loss=1887.327880859375
I0411 04:02:56.572364 139985267349248 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0, loss=1863.2880859375
I0411 04:04:10.361464 139985258956544 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.0, loss=1853.7086181640625
I0411 04:05:24.216421 139985267349248 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1884.5491943359375
I0411 04:06:38.034445 139985258956544 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0, loss=1859.2283935546875
I0411 04:07:51.688820 139985267349248 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.0, loss=1752.3250732421875
I0411 04:09:10.205965 139985258956544 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0, loss=1815.3251953125
I0411 04:10:33.222739 139985267349248 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.0, loss=1866.9571533203125
I0411 04:11:51.894368 140157788407616 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 21.857673216
I0411 04:11:51.894586 140157788407616 spec.py:298] Evaluating on the training split.
I0411 04:12:18.551172 140157788407616 spec.py:310] Evaluating on the validation split.
I0411 04:12:55.177072 140157788407616 spec.py:326] Evaluating on the test split.
I0411 04:13:13.847454 140157788407616 submission_runner.py:382] Time since start: 8269.95s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:13:13.848598 140157788407616 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 22.21897728
I0411 04:13:13.864000 139985267349248 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7880.787815, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=8269.953022, train/ctc_loss=1724.8543701171875, train/wer=0.943700, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.895995
I0411 04:13:14.081763 140157788407616 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:13:15.016474 140157788407616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:13:15.034886 140157788407616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:13:15.047951 140157788407616 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 22.228484096
I0411 04:13:15.055255 139985258956544 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7880.787815
I0411 04:13:15.471909 140157788407616 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:13:16.767769 140157788407616 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:13:16.786351 140157788407616 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-4_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:13:18.066696 140157788407616 submission_runner.py:550] Tuning trial 1/1
I0411 04:13:18.066943 140157788407616 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 04:13:18.071035 140157788407616 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.362164, dtype=float32), 'train/wer': 1.5019886304479104, 'validation/ctc_loss': DeviceArray(30.539703, dtype=float32), 'validation/wer': 1.3959034819438683, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(30.570782, dtype=float32), 'test/wer': 1.4307070460869742, 'test/num_examples': 2472, 'score': 58.193241119384766, 'total_duration': 58.385010719299316, 'global_step': 1, 'preemption_count': 0}), (3052, {'train/ctc_loss': DeviceArray(1767.6744, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2452.4899504184723, 'total_duration': 2581.6947870254517, 'global_step': 3052, 'preemption_count': 0}), (6098, {'train/ctc_loss': DeviceArray(1761.5636, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4847.239882946014, 'total_duration': 5064.120633125305, 'global_step': 6098, 'preemption_count': 0}), (9163, {'train/ctc_loss': DeviceArray(1741.2909, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7241.95977473259, 'total_duration': 7547.583514451981, 'global_step': 9163, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(1724.8544, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(3357.909, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7880.787815332413, 'total_duration': 8269.953022241592, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 04:13:18.071200 140157788407616 submission_runner.py:553] Timing: 7880.787815332413
I0411 04:13:18.071267 140157788407616 submission_runner.py:554] ====================
I0411 04:13:18.071553 140157788407616 submission_runner.py:613] Final librispeech_conformer score: 7880.787815332413
