I0411 01:54:11.489061 140199567791936 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax.
I0411 01:54:11.555956 140199567791936 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0411 01:54:12.443509 140199567791936 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0411 01:54:12.444280 140199567791936 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0411 01:54:12.450319 140199567791936 submission_runner.py:511] Using RNG seed 3863503417
I0411 01:54:14.949499 140199567791936 submission_runner.py:520] --- Tuning run 1/1 ---
I0411 01:54:14.949713 140199567791936 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1.
I0411 01:54:14.949929 140199567791936 logger_utils.py:84] Saving hparams to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/hparams.json.
I0411 01:54:15.072736 140199567791936 submission_runner.py:230] Starting train once: RAM USED (GB) 4.276187136
I0411 01:54:15.072899 140199567791936 submission_runner.py:231] Initializing dataset.
I0411 01:54:15.073058 140199567791936 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.276187136
I0411 01:54:15.073132 140199567791936 submission_runner.py:240] Initializing model.
I0411 01:54:21.092640 140199567791936 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.733743616
I0411 01:54:21.092877 140199567791936 submission_runner.py:252] Initializing optimizer.
I0411 01:54:21.797226 140199567791936 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.733977088
I0411 01:54:21.797442 140199567791936 submission_runner.py:261] Initializing metrics bundle.
I0411 01:54:21.797493 140199567791936 submission_runner.py:276] Initializing checkpoint and logger.
I0411 01:54:21.798803 140199567791936 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0411 01:54:21.799435 140199567791936 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0411 01:54:21.799515 140199567791936 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0411 01:54:22.543725 140199567791936 submission_runner.py:297] Saving meta data to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0411 01:54:22.544775 140199567791936 submission_runner.py:300] Saving flags to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/flags_0.json.
I0411 01:54:22.550279 140199567791936 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.73357568
I0411 01:54:22.550476 140199567791936 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.73357568
I0411 01:54:22.550536 140199567791936 submission_runner.py:313] Starting training loop.
I0411 01:54:22.749377 140199567791936 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:54:22.781323 140199567791936 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:54:23.137446 140199567791936 input_pipeline.py:20] Loading split = train-other-500
I0411 01:54:26.585787 140199567791936 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.194502656
2023-04-11 01:55:20.825845: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-11 01:55:20.864304: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0411 01:55:22.631128 140024414725888 logging_writer.py:48] [0] global_step=0, grad_norm=44.25423812866211, loss=32.7008056640625
I0411 01:55:22.650971 140199567791936 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.194002432
I0411 01:55:22.651238 140199567791936 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.194002432
I0411 01:55:22.651334 140199567791936 spec.py:298] Evaluating on the training split.
I0411 01:55:22.757536 140199567791936 input_pipeline.py:20] Loading split = train-clean-100
I0411 01:55:22.783450 140199567791936 input_pipeline.py:20] Loading split = train-clean-360
I0411 01:55:23.086595 140199567791936 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0411 01:56:13.041860 140199567791936 spec.py:310] Evaluating on the validation split.
I0411 01:56:13.107117 140199567791936 input_pipeline.py:20] Loading split = dev-clean
I0411 01:56:13.112018 140199567791936 input_pipeline.py:20] Loading split = dev-other
I0411 01:56:53.531580 140199567791936 spec.py:326] Evaluating on the test split.
I0411 01:56:53.599107 140199567791936 input_pipeline.py:20] Loading split = test-clean
I0411 01:57:21.687032 140199567791936 submission_runner.py:382] Time since start: 60.10s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.854635, dtype=float32), 'train/wer': 1.0603617016846094, 'validation/ctc_loss': DeviceArray(31.106913, dtype=float32), 'validation/wer': 1.245530588814171, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.271856, dtype=float32), 'test/wer': 1.2572258444539233, 'test/num_examples': 2472}
I0411 01:57:21.688120 140199567791936 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.680220672
I0411 01:57:21.700226 140020262369024 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=59.901810, test/ctc_loss=31.2718563079834, test/num_examples=2472, test/wer=1.257226, total_duration=60.100717, train/ctc_loss=31.85463523864746, train/wer=1.060362, validation/ctc_loss=31.10691261291504, validation/num_examples=5348, validation/wer=1.245531
I0411 01:57:21.926299 140199567791936 checkpoints.py:356] Saving checkpoint at step: 1
I0411 01:57:22.852519 140199567791936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_1
I0411 01:57:22.874218 140199567791936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_1.
I0411 01:57:22.891151 140199567791936 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.69110784
I0411 01:57:22.947122 140199567791936 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.68957184
I0411 01:57:36.893919 140199567791936 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.974862336
I0411 01:58:52.048971 140025193920256 logging_writer.py:48] [100] global_step=100, grad_norm=19.973665237426758, loss=10.725321769714355
I0411 02:00:07.797739 140025202312960 logging_writer.py:48] [200] global_step=200, grad_norm=4.942763328552246, loss=6.270003318786621
I0411 02:01:23.435028 140025193920256 logging_writer.py:48] [300] global_step=300, grad_norm=0.21901756525039673, loss=5.8280487060546875
I0411 02:02:39.201020 140025202312960 logging_writer.py:48] [400] global_step=400, grad_norm=0.8450660109519958, loss=5.833791732788086
I0411 02:03:54.763623 140025193920256 logging_writer.py:48] [500] global_step=500, grad_norm=1.6002686023712158, loss=5.837645530700684
I0411 02:05:10.237279 140025202312960 logging_writer.py:48] [600] global_step=600, grad_norm=0.85515296459198, loss=5.82423210144043
I0411 02:06:25.742021 140025193920256 logging_writer.py:48] [700] global_step=700, grad_norm=0.628365695476532, loss=5.803109645843506
I0411 02:07:41.294309 140025202312960 logging_writer.py:48] [800] global_step=800, grad_norm=2.888545513153076, loss=5.913338661193848
I0411 02:09:02.912403 140025193920256 logging_writer.py:48] [900] global_step=900, grad_norm=0.28127992153167725, loss=5.822591304779053
I0411 02:10:24.804413 140025202312960 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.33866190910339355, loss=5.817558288574219
I0411 02:11:47.006537 140027277498112 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.3067407011985779, loss=5.791131019592285
I0411 02:13:02.464402 140027269105408 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.25817593932151794, loss=5.7809062004089355
I0411 02:14:18.019785 140027277498112 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.4840412139892578, loss=5.850047588348389
I0411 02:15:33.434621 140027269105408 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5579232573509216, loss=5.801479816436768
I0411 02:16:48.994281 140027277498112 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6107295155525208, loss=5.801337718963623
I0411 02:18:04.468231 140027269105408 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.9328161478042603, loss=5.8832316398620605
I0411 02:19:24.967111 140027277498112 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.17180107533931732, loss=5.771296977996826
I0411 02:20:47.347412 140027269105408 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6523168087005615, loss=5.8403120040893555
I0411 02:22:11.404508 140027277498112 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.3210570812225342, loss=5.775301456451416
I0411 02:23:34.430272 140027269105408 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9649927020072937, loss=5.79791784286499
I0411 02:24:58.496464 140026622138112 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.505747377872467, loss=5.798103332519531
I0411 02:26:14.012922 140026613745408 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.19240745902061462, loss=5.797021389007568
I0411 02:27:29.596506 140026622138112 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.38529813289642334, loss=5.781274318695068
I0411 02:28:45.321802 140026613745408 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0863566398620605, loss=5.829349517822266
I0411 02:30:01.042160 140026622138112 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9145944714546204, loss=5.808609962463379
I0411 02:31:16.888345 140026613745408 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.17938341200351715, loss=5.796751976013184
I0411 02:32:33.298093 140026622138112 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.24800938367843628, loss=5.78450870513916
I0411 02:33:55.354322 140026613745408 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.07246825844049454, loss=5.798882484436035
I0411 02:35:18.650642 140026622138112 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2195091247558594, loss=5.846034049987793
I0411 02:36:42.800020 140026613745408 logging_writer.py:48] [3000] global_step=3000, grad_norm=nan, loss=nan
I0411 02:37:23.190239 140199567791936 submission_runner.py:373] Before eval at step 3049: RAM USED (GB) 22.614151168
I0411 02:37:23.190463 140199567791936 spec.py:298] Evaluating on the training split.
I0411 02:37:49.426651 140199567791936 spec.py:310] Evaluating on the validation split.
I0411 02:38:24.142612 140199567791936 spec.py:326] Evaluating on the test split.
I0411 02:38:42.545509 140199567791936 submission_runner.py:382] Time since start: 2580.64s, 	Step: 3049, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 02:38:42.546930 140199567791936 submission_runner.py:396] After eval at step 3049: RAM USED (GB) 20.530282496
I0411 02:38:42.563220 140026622138112 logging_writer.py:48] [3049] global_step=3049, preemption_count=0, score=2454.295345, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=2580.637909, train/ctc_loss=nan, train/wer=0.944636, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 02:38:42.775902 140199567791936 checkpoints.py:356] Saving checkpoint at step: 3049
I0411 02:38:43.718873 140199567791936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_3049
I0411 02:38:43.740562 140199567791936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_3049.
I0411 02:38:43.754029 140199567791936 submission_runner.py:416] After logging and checkpointing eval at step 3049: RAM USED (GB) 20.5428736
I0411 02:39:25.634784 140026622138112 logging_writer.py:48] [3100] global_step=3100, grad_norm=nan, loss=nan
I0411 02:40:38.172703 140026613745408 logging_writer.py:48] [3200] global_step=3200, grad_norm=nan, loss=nan
I0411 02:41:50.651056 140026622138112 logging_writer.py:48] [3300] global_step=3300, grad_norm=nan, loss=nan
I0411 02:43:03.261708 140026613745408 logging_writer.py:48] [3400] global_step=3400, grad_norm=nan, loss=nan
I0411 02:44:16.875808 140026622138112 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0411 02:45:36.293895 140026613745408 logging_writer.py:48] [3600] global_step=3600, grad_norm=nan, loss=nan
I0411 02:46:56.755335 140026622138112 logging_writer.py:48] [3700] global_step=3700, grad_norm=nan, loss=nan
I0411 02:48:19.658309 140026613745408 logging_writer.py:48] [3800] global_step=3800, grad_norm=nan, loss=nan
I0411 02:49:39.096434 140026622138112 logging_writer.py:48] [3900] global_step=3900, grad_norm=nan, loss=nan
I0411 02:51:00.486390 140026613745408 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0411 02:52:24.189131 140026622138112 logging_writer.py:48] [4100] global_step=4100, grad_norm=nan, loss=nan
I0411 02:53:41.659751 140026622138112 logging_writer.py:48] [4200] global_step=4200, grad_norm=nan, loss=nan
I0411 02:54:54.198889 140026613745408 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0411 02:56:06.698549 140026622138112 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0411 02:57:19.198885 140026613745408 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0411 02:58:35.194371 140026622138112 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0411 02:59:59.182186 140026613745408 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0411 03:01:20.480504 140026622138112 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0411 03:02:45.265809 140026613745408 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0411 03:04:09.216701 140026622138112 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0411 03:05:29.111051 140026613745408 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0411 03:06:50.229942 140026622138112 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0411 03:08:02.648622 140026613745408 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0411 03:09:15.083896 140026622138112 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0411 03:10:27.520976 140026613745408 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0411 03:11:43.574176 140026622138112 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0411 03:13:02.177322 140026613745408 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0411 03:14:26.382031 140026622138112 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0411 03:15:49.631596 140026613745408 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0411 03:17:13.549596 140026622138112 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0411 03:18:33.404633 140026613745408 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0411 03:18:44.159518 140199567791936 submission_runner.py:373] Before eval at step 6115: RAM USED (GB) 22.310096896
I0411 03:18:44.159749 140199567791936 spec.py:298] Evaluating on the training split.
I0411 03:19:11.465042 140199567791936 spec.py:310] Evaluating on the validation split.
I0411 03:19:47.761665 140199567791936 spec.py:326] Evaluating on the test split.
I0411 03:20:06.643450 140199567791936 submission_runner.py:382] Time since start: 5061.61s, 	Step: 6115, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 03:20:06.644723 140199567791936 submission_runner.py:396] After eval at step 6115: RAM USED (GB) 21.077409792
I0411 03:20:06.662901 140025383089920 logging_writer.py:48] [6115] global_step=6115, preemption_count=0, score=4848.851472, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5061.606194, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 03:20:06.880120 140199567791936 checkpoints.py:356] Saving checkpoint at step: 6115
I0411 03:20:07.841914 140199567791936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_6115
I0411 03:20:07.863171 140199567791936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_6115.
I0411 03:20:07.877913 140199567791936 submission_runner.py:416] After logging and checkpointing eval at step 6115: RAM USED (GB) 21.08452864
I0411 03:21:14.097141 140025383089920 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0411 03:22:26.562191 140025374697216 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0411 03:23:39.121036 140025383089920 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0411 03:24:51.767402 140025374697216 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0411 03:26:04.499500 140025383089920 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0411 03:27:17.144412 140025374697216 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0411 03:28:33.838448 140025383089920 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0411 03:29:54.528525 140025374697216 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0411 03:31:16.190431 140025383089920 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0411 03:32:38.323702 140025374697216 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0411 03:33:58.665635 140025383089920 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0411 03:35:15.439123 140025055409920 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0411 03:36:28.101506 140025047017216 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0411 03:37:40.579322 140025055409920 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0411 03:38:53.132564 140025047017216 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0411 03:40:05.704087 140025055409920 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0411 03:41:23.547562 140025047017216 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0411 03:42:44.918387 140025055409920 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0411 03:44:05.351871 140025047017216 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0411 03:45:28.507920 140025055409920 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0411 03:46:52.878096 140025047017216 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0411 03:48:12.763062 140025055409920 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0411 03:49:25.327757 140025047017216 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0411 03:50:37.816448 140025055409920 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0411 03:51:50.298287 140025047017216 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0411 03:53:09.124607 140025055409920 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0411 03:54:28.104762 140025047017216 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0411 03:55:47.765604 140025055409920 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0411 03:57:07.341985 140025047017216 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0411 03:58:30.992092 140025055409920 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0411 03:59:53.495722 140025047017216 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0411 04:00:07.968344 140199567791936 submission_runner.py:373] Before eval at step 9220: RAM USED (GB) 22.272229376
I0411 04:00:07.968581 140199567791936 spec.py:298] Evaluating on the training split.
I0411 04:00:34.816318 140199567791936 spec.py:310] Evaluating on the validation split.
I0411 04:01:09.856106 140199567791936 spec.py:326] Evaluating on the test split.
I0411 04:01:28.496787 140199567791936 submission_runner.py:382] Time since start: 7545.41s, 	Step: 9220, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:01:28.498123 140199567791936 submission_runner.py:396] After eval at step 9220: RAM USED (GB) 21.246275584
I0411 04:01:28.518987 140026263738112 logging_writer.py:48] [9220] global_step=9220, preemption_count=0, score=7242.941189, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7545.412776, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 04:01:28.729014 140199567791936 checkpoints.py:356] Saving checkpoint at step: 9220
I0411 04:01:29.692159 140199567791936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_9220
I0411 04:01:29.713665 140199567791936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_9220.
I0411 04:01:29.727882 140199567791936 submission_runner.py:416] After logging and checkpointing eval at step 9220: RAM USED (GB) 21.25510656
I0411 04:02:32.184934 140026263738112 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0411 04:03:44.619675 140026255345408 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0411 04:04:57.191532 140026263738112 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0411 04:06:09.889092 140026255345408 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0411 04:07:23.494529 140026263738112 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0411 04:08:45.680125 140026255345408 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0411 04:10:06.633872 140026263738112 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0411 04:11:27.760776 140199567791936 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.274015232
I0411 04:11:27.760996 140199567791936 spec.py:298] Evaluating on the training split.
I0411 04:11:54.990921 140199567791936 spec.py:310] Evaluating on the validation split.
I0411 04:12:30.173822 140199567791936 spec.py:326] Evaluating on the test split.
I0411 04:12:48.731377 140199567791936 submission_runner.py:382] Time since start: 8225.21s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0411 04:12:48.732792 140199567791936 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.525807104
I0411 04:12:48.769235 140026693818112 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7839.478188, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=8225.209107, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0411 04:12:48.996979 140199567791936 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:12:49.916871 140199567791936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:12:49.938640 140199567791936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:12:49.953122 140199567791936 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.532319744
I0411 04:12:49.960493 140026685425408 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7839.478188
I0411 04:12:50.132874 140199567791936 checkpoints.py:356] Saving checkpoint at step: 10000
I0411 04:12:51.358857 140199567791936 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_10000
I0411 04:12:51.380608 140199567791936 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_test/timing_momentum_kasimbeg-3_run1/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0411 04:12:52.597326 140199567791936 submission_runner.py:550] Tuning trial 1/1
I0411 04:12:52.597582 140199567791936 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0411 04:12:52.602204 140199567791936 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.854635, dtype=float32), 'train/wer': 1.0603617016846094, 'validation/ctc_loss': DeviceArray(31.106913, dtype=float32), 'validation/wer': 1.245530588814171, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.271856, dtype=float32), 'test/wer': 1.2572258444539233, 'test/num_examples': 2472, 'score': 59.90180993080139, 'total_duration': 60.10071659088135, 'global_step': 1, 'preemption_count': 0}), (3049, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2454.2953445911407, 'total_duration': 2580.6379091739655, 'global_step': 3049, 'preemption_count': 0}), (6115, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4848.851471662521, 'total_duration': 5061.606193780899, 'global_step': 6115, 'preemption_count': 0}), (9220, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.941189289093, 'total_duration': 7545.41277551651, 'global_step': 9220, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7839.478187799454, 'total_duration': 8225.20910692215, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0411 04:12:52.602361 140199567791936 submission_runner.py:553] Timing: 7839.478187799454
I0411 04:12:52.602411 140199567791936 submission_runner.py:554] ====================
I0411 04:12:52.602845 140199567791936 submission_runner.py:613] Final librispeech_conformer score: 7839.478187799454
