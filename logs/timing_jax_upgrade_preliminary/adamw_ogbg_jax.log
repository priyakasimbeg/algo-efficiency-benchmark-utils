python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_jax_upgrade_preliminary/adamw --overwrite=True --save_checkpoints=False --max_global_steps=6000 2>&1 | tee -a /logs/ogbg_jax_08-08-2023-00-15-02.log
2023-08-08 00:15:07.272084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0808 00:15:26.252997 140240288372544 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax.
I0808 00:15:27.152703 140240288372544 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0808 00:15:27.153458 140240288372544 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0808 00:15:27.153616 140240288372544 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0808 00:15:27.159499 140240288372544 submission_runner.py:490] Using RNG seed 1315357753
I0808 00:15:32.470089 140240288372544 submission_runner.py:499] --- Tuning run 1/1 ---
I0808 00:15:32.470288 140240288372544 submission_runner.py:504] Creating tuning directory at /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1.
I0808 00:15:32.471950 140240288372544 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1/hparams.json.
I0808 00:15:32.652736 140240288372544 submission_runner.py:176] Initializing dataset.
I0808 00:15:32.766190 140240288372544 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:15:32.771413 140240288372544 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0808 00:15:33.008141 140240288372544 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0808 00:15:33.065310 140240288372544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:15:33.139588 140240288372544 submission_runner.py:183] Initializing model.
I0808 00:15:37.749256 140240288372544 submission_runner.py:217] Initializing optimizer.
I0808 00:15:38.385236 140240288372544 submission_runner.py:224] Initializing metrics bundle.
I0808 00:15:38.385432 140240288372544 submission_runner.py:242] Initializing checkpoint and logger.
I0808 00:15:38.386371 140240288372544 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1 with prefix checkpoint_
I0808 00:15:38.386641 140240288372544 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0808 00:15:38.386713 140240288372544 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
I0808 00:15:39.218018 140240288372544 submission_runner.py:263] Saving meta data to /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1/meta_data_0.json.
I0808 00:15:39.219085 140240288372544 submission_runner.py:266] Saving flags to /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1/flags_0.json.
I0808 00:15:39.228091 140240288372544 submission_runner.py:276] Starting training loop.
I0808 00:16:03.364418 140076021442304 logging_writer.py:48] [0] global_step=0, grad_norm=2.071784019470215, loss=0.7222486138343811
I0808 00:16:03.379321 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:16:03.385334 140240288372544 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:16:03.390105 140240288372544 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0808 00:16:03.455662 140240288372544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:17:55.462856 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:17:55.466816 140240288372544 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:17:55.471110 140240288372544 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0808 00:17:55.540534 140240288372544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:19:29.240899 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:19:29.244342 140240288372544 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:19:29.248269 140240288372544 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0808 00:19:29.314713 140240288372544 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0808 00:21:02.557329 140240288372544 submission_runner.py:364] Time since start: 323.33s, 	Step: 1, 	{'train/accuracy': 0.5046198964118958, 'train/loss': 0.7227272391319275, 'train/mean_average_precision': 0.021590107274601145, 'validation/accuracy': 0.5105925798416138, 'validation/loss': 0.7194507718086243, 'validation/mean_average_precision': 0.026683119495966856, 'validation/num_examples': 43793, 'test/accuracy': 0.5130106806755066, 'test/loss': 0.7191532254219055, 'test/mean_average_precision': 0.028235916743085274, 'test/num_examples': 43793, 'score': 24.151200532913208, 'total_duration': 323.3291618824005, 'accumulated_submission_time': 24.151200532913208, 'accumulated_eval_time': 299.177939414978, 'accumulated_logging_time': 0}
I0808 00:21:02.575605 140064948020992 logging_writer.py:48] [1] accumulated_eval_time=299.177939, accumulated_logging_time=0, accumulated_submission_time=24.151201, global_step=1, preemption_count=0, score=24.151201, test/accuracy=0.513011, test/loss=0.719153, test/mean_average_precision=0.028236, test/num_examples=43793, total_duration=323.329162, train/accuracy=0.504620, train/loss=0.722727, train/mean_average_precision=0.021590, validation/accuracy=0.510593, validation/loss=0.719451, validation/mean_average_precision=0.026683, validation/num_examples=43793
I0808 00:21:32.062715 140066097723136 logging_writer.py:48] [100] global_step=100, grad_norm=0.41153016686439514, loss=0.3564402461051941
I0808 00:22:01.630183 140064948020992 logging_writer.py:48] [200] global_step=200, grad_norm=0.2469121813774109, loss=0.20578455924987793
I0808 00:22:31.253751 140066097723136 logging_writer.py:48] [300] global_step=300, grad_norm=0.13634520769119263, loss=0.11617889255285263
I0808 00:23:00.527134 140064948020992 logging_writer.py:48] [400] global_step=400, grad_norm=0.08053533732891083, loss=0.07523621618747711
I0808 00:23:30.010693 140066097723136 logging_writer.py:48] [500] global_step=500, grad_norm=0.08142963796854019, loss=0.06308794766664505
I0808 00:23:59.286662 140064948020992 logging_writer.py:48] [600] global_step=600, grad_norm=0.022976523265242577, loss=0.058972496539354324
I0808 00:24:29.007526 140066097723136 logging_writer.py:48] [700] global_step=700, grad_norm=0.047952257096767426, loss=0.04812045022845268
I0808 00:24:58.815775 140064948020992 logging_writer.py:48] [800] global_step=800, grad_norm=0.01854935847222805, loss=0.05695313960313797
I0808 00:25:02.578300 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:26:50.191918 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:26:53.093930 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:26:55.959700 140240288372544 submission_runner.py:364] Time since start: 676.73s, 	Step: 813, 	{'train/accuracy': 0.9866933822631836, 'train/loss': 0.05387502908706665, 'train/mean_average_precision': 0.04291431957925572, 'validation/accuracy': 0.9841272830963135, 'validation/loss': 0.06371606886386871, 'validation/mean_average_precision': 0.04190304384825048, 'validation/num_examples': 43793, 'test/accuracy': 0.9831513166427612, 'test/loss': 0.06698337197303772, 'test/mean_average_precision': 0.04182226764566002, 'test/num_examples': 43793, 'score': 264.13414096832275, 'total_duration': 676.7315173149109, 'accumulated_submission_time': 264.13414096832275, 'accumulated_eval_time': 412.5592677593231, 'accumulated_logging_time': 0.028700828552246094}
I0808 00:26:55.975460 140066156226304 logging_writer.py:48] [813] accumulated_eval_time=412.559268, accumulated_logging_time=0.028701, accumulated_submission_time=264.134141, global_step=813, preemption_count=0, score=264.134141, test/accuracy=0.983151, test/loss=0.066983, test/mean_average_precision=0.041822, test/num_examples=43793, total_duration=676.731517, train/accuracy=0.986693, train/loss=0.053875, train/mean_average_precision=0.042914, validation/accuracy=0.984127, validation/loss=0.063716, validation/mean_average_precision=0.041903, validation/num_examples=43793
I0808 00:27:21.888115 140066164619008 logging_writer.py:48] [900] global_step=900, grad_norm=0.012449153698980808, loss=0.053295571357011795
I0808 00:27:51.169069 140066156226304 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.06776145100593567, loss=0.056384313851594925
I0808 00:28:21.177391 140066164619008 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.05104599520564079, loss=0.05077274143695831
I0808 00:28:51.331244 140066156226304 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.06558812409639359, loss=0.056874752044677734
I0808 00:29:21.150570 140066164619008 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.017511514946818352, loss=0.04547787085175514
I0808 00:29:50.653049 140066156226304 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.021860450506210327, loss=0.05051174387335777
I0808 00:30:20.297361 140066164619008 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0333201102912426, loss=0.05304712802171707
I0808 00:30:49.610445 140066156226304 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.03462997078895569, loss=0.05085277929902077
I0808 00:30:55.963615 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:32:45.274425 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:32:48.246690 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:32:51.121593 140240288372544 submission_runner.py:364] Time since start: 1031.89s, 	Step: 1623, 	{'train/accuracy': 0.9873326420783997, 'train/loss': 0.04758629947900772, 'train/mean_average_precision': 0.09075100418657536, 'validation/accuracy': 0.9845096468925476, 'validation/loss': 0.05719640851020813, 'validation/mean_average_precision': 0.09496654902664603, 'validation/num_examples': 43793, 'test/accuracy': 0.9835299849510193, 'test/loss': 0.060526877641677856, 'test/mean_average_precision': 0.09155810964602416, 'test/num_examples': 43793, 'score': 504.1020448207855, 'total_duration': 1031.8934330940247, 'accumulated_submission_time': 504.1020448207855, 'accumulated_eval_time': 527.717206954956, 'accumulated_logging_time': 0.05543017387390137}
I0808 00:32:51.136206 140066089330432 logging_writer.py:48] [1623] accumulated_eval_time=527.717207, accumulated_logging_time=0.055430, accumulated_submission_time=504.102045, global_step=1623, preemption_count=0, score=504.102045, test/accuracy=0.983530, test/loss=0.060527, test/mean_average_precision=0.091558, test/num_examples=43793, total_duration=1031.893433, train/accuracy=0.987333, train/loss=0.047586, train/mean_average_precision=0.090751, validation/accuracy=0.984510, validation/loss=0.057196, validation/mean_average_precision=0.094967, validation/num_examples=43793
I0808 00:33:14.125623 140066147833600 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.04219648241996765, loss=0.05173876881599426
I0808 00:33:43.743305 140066089330432 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.020356465131044388, loss=0.05081465467810631
I0808 00:34:13.335253 140066147833600 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.019863735884428024, loss=0.050432153046131134
I0808 00:34:42.651730 140066089330432 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.024113520979881287, loss=0.04211943596601486
I0808 00:35:13.110848 140066147833600 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.055382080376148224, loss=0.04662550613284111
I0808 00:35:42.823717 140066089330432 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.02374209091067314, loss=0.0493493527173996
I0808 00:36:12.267813 140066147833600 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04095470532774925, loss=0.050249580293893814
I0808 00:36:41.710782 140066089330432 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.020793618634343147, loss=0.05120076984167099
I0808 00:36:51.379259 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:38:41.710644 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:38:44.605720 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:38:47.459859 140240288372544 submission_runner.py:364] Time since start: 1388.23s, 	Step: 2434, 	{'train/accuracy': 0.9873964190483093, 'train/loss': 0.04586842283606529, 'train/mean_average_precision': 0.12195921378003309, 'validation/accuracy': 0.9846959710121155, 'validation/loss': 0.05539641156792641, 'validation/mean_average_precision': 0.11976489089627063, 'validation/num_examples': 43793, 'test/accuracy': 0.9836854338645935, 'test/loss': 0.05860842391848564, 'test/mean_average_precision': 0.11799772484473646, 'test/num_examples': 43793, 'score': 744.325510263443, 'total_duration': 1388.2317054271698, 'accumulated_submission_time': 744.325510263443, 'accumulated_eval_time': 643.7977638244629, 'accumulated_logging_time': 0.08050012588500977}
I0808 00:38:47.475166 140066097723136 logging_writer.py:48] [2434] accumulated_eval_time=643.797764, accumulated_logging_time=0.080500, accumulated_submission_time=744.325510, global_step=2434, preemption_count=0, score=744.325510, test/accuracy=0.983685, test/loss=0.058608, test/mean_average_precision=0.117998, test/num_examples=43793, total_duration=1388.231705, train/accuracy=0.987396, train/loss=0.045868, train/mean_average_precision=0.121959, validation/accuracy=0.984696, validation/loss=0.055396, validation/mean_average_precision=0.119765, validation/num_examples=43793
I0808 00:39:07.464749 140066156226304 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.017952319234609604, loss=0.046357251703739166
I0808 00:39:36.965934 140066097723136 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.018570877611637115, loss=0.046753693372011185
I0808 00:40:06.761039 140066156226304 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.022448472678661346, loss=0.04533364996314049
I0808 00:40:36.373380 140066097723136 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.022172870114445686, loss=0.0444178506731987
I0808 00:41:06.313137 140066156226304 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.02414717897772789, loss=0.04749003052711487
I0808 00:41:35.828910 140066097723136 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01427699625492096, loss=0.04397449642419815
I0808 00:42:05.282368 140066156226304 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01703227497637272, loss=0.04570429027080536
I0808 00:42:34.630173 140066097723136 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.02058139257133007, loss=0.04378362372517586
I0808 00:42:47.541989 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:44:37.871793 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:44:40.864750 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:44:43.756479 140240288372544 submission_runner.py:364] Time since start: 1744.53s, 	Step: 3245, 	{'train/accuracy': 0.9878032803535461, 'train/loss': 0.04346046969294548, 'train/mean_average_precision': 0.15754756636709868, 'validation/accuracy': 0.9851222038269043, 'validation/loss': 0.052109986543655396, 'validation/mean_average_precision': 0.14698940588253046, 'validation/num_examples': 43793, 'test/accuracy': 0.9841980338096619, 'test/loss': 0.054697971791028976, 'test/mean_average_precision': 0.1438010743765955, 'test/num_examples': 43793, 'score': 984.371369600296, 'total_duration': 1744.5283246040344, 'accumulated_submission_time': 984.371369600296, 'accumulated_eval_time': 760.0122117996216, 'accumulated_logging_time': 0.10749506950378418}
I0808 00:44:43.772237 140066089330432 logging_writer.py:48] [3245] accumulated_eval_time=760.012212, accumulated_logging_time=0.107495, accumulated_submission_time=984.371370, global_step=3245, preemption_count=0, score=984.371370, test/accuracy=0.984198, test/loss=0.054698, test/mean_average_precision=0.143801, test/num_examples=43793, total_duration=1744.528325, train/accuracy=0.987803, train/loss=0.043460, train/mean_average_precision=0.157548, validation/accuracy=0.985122, validation/loss=0.052110, validation/mean_average_precision=0.146989, validation/num_examples=43793
I0808 00:45:00.177688 140066147833600 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.014376227743923664, loss=0.04490887001156807
I0808 00:45:29.611917 140066089330432 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.016401382163167, loss=0.050360072404146194
I0808 00:45:58.867991 140066147833600 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.018243560567498207, loss=0.043744951486587524
I0808 00:46:28.308172 140066089330432 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.011564543470740318, loss=0.0395328663289547
I0808 00:46:57.678556 140066147833600 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.02235105074942112, loss=0.04402632638812065
I0808 00:47:27.127111 140066089330432 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.014173791743814945, loss=0.04317767918109894
I0808 00:47:56.399662 140066147833600 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.012855117209255695, loss=0.05246197059750557
I0808 00:48:25.849528 140066089330432 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.02045270800590515, loss=0.04407675936818123
I0808 00:48:43.960085 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:50:34.987613 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:50:37.944398 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:50:40.916580 140240288372544 submission_runner.py:364] Time since start: 2101.69s, 	Step: 4063, 	{'train/accuracy': 0.9881353378295898, 'train/loss': 0.04135793447494507, 'train/mean_average_precision': 0.1829953593373077, 'validation/accuracy': 0.9851396679878235, 'validation/loss': 0.05070439353585243, 'validation/mean_average_precision': 0.16543952954513488, 'validation/num_examples': 43793, 'test/accuracy': 0.9842885732650757, 'test/loss': 0.05322302132844925, 'test/mean_average_precision': 0.1637908300792072, 'test/num_examples': 43793, 'score': 1224.5396528244019, 'total_duration': 2101.688425064087, 'accumulated_submission_time': 1224.5396528244019, 'accumulated_eval_time': 876.9686615467072, 'accumulated_logging_time': 0.13369035720825195}
I0808 00:50:40.933078 140066097723136 logging_writer.py:48] [4063] accumulated_eval_time=876.968662, accumulated_logging_time=0.133690, accumulated_submission_time=1224.539653, global_step=4063, preemption_count=0, score=1224.539653, test/accuracy=0.984289, test/loss=0.053223, test/mean_average_precision=0.163791, test/num_examples=43793, total_duration=2101.688425, train/accuracy=0.988135, train/loss=0.041358, train/mean_average_precision=0.182995, validation/accuracy=0.985140, validation/loss=0.050704, validation/mean_average_precision=0.165440, validation/num_examples=43793
I0808 00:50:52.195261 140066156226304 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.014422540552914143, loss=0.044796500355005264
I0808 00:51:21.946957 140066097723136 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.012329328805208206, loss=0.045153047889471054
I0808 00:51:52.057524 140066156226304 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.019743094220757484, loss=0.04675263166427612
I0808 00:52:21.904151 140066097723136 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.014662787318229675, loss=0.045029085129499435
I0808 00:52:51.310055 140066156226304 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.023635946214199066, loss=0.04357660934329033
I0808 00:53:20.650226 140066097723136 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.015501686371862888, loss=0.04279400408267975
I0808 00:53:49.826152 140066156226304 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.011848545633256435, loss=0.04114483669400215
I0808 00:54:19.687186 140066097723136 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.012157498858869076, loss=0.04512718319892883
I0808 00:54:41.023065 140240288372544 spec.py:320] Evaluating on the training split.
I0808 00:56:36.138653 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 00:56:39.410881 140240288372544 spec.py:348] Evaluating on the test split.
I0808 00:56:42.702032 140240288372544 submission_runner.py:364] Time since start: 2463.47s, 	Step: 4874, 	{'train/accuracy': 0.988304615020752, 'train/loss': 0.0400959849357605, 'train/mean_average_precision': 0.2133099315213401, 'validation/accuracy': 0.9855878353118896, 'validation/loss': 0.04914826527237892, 'validation/mean_average_precision': 0.18747076735709783, 'validation/num_examples': 43793, 'test/accuracy': 0.984661340713501, 'test/loss': 0.051862891763448715, 'test/mean_average_precision': 0.1848702120730487, 'test/num_examples': 43793, 'score': 1464.6099455356598, 'total_duration': 2463.4738550186157, 'accumulated_submission_time': 1464.6099455356598, 'accumulated_eval_time': 998.6475627422333, 'accumulated_logging_time': 0.16056585311889648}
I0808 00:56:42.720087 140066181404416 logging_writer.py:48] [4874] accumulated_eval_time=998.647563, accumulated_logging_time=0.160566, accumulated_submission_time=1464.609946, global_step=4874, preemption_count=0, score=1464.609946, test/accuracy=0.984661, test/loss=0.051863, test/mean_average_precision=0.184870, test/num_examples=43793, total_duration=2463.473855, train/accuracy=0.988305, train/loss=0.040096, train/mean_average_precision=0.213310, validation/accuracy=0.985588, validation/loss=0.049148, validation/mean_average_precision=0.187471, validation/num_examples=43793
I0808 00:56:51.041103 140175912986368 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.011412200517952442, loss=0.04507417976856232
I0808 00:57:21.037634 140066181404416 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.01096666231751442, loss=0.04019671306014061
I0808 00:57:51.300286 140175912986368 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.010518396273255348, loss=0.041653823107481
I0808 00:58:21.624620 140066181404416 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.012986620888113976, loss=0.04110775515437126
I0808 00:58:51.924937 140175912986368 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.010745727457106113, loss=0.037328287959098816
I0808 00:59:22.149563 140066181404416 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.010518603026866913, loss=0.041024547070264816
I0808 00:59:52.373796 140175912986368 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.009870518930256367, loss=0.038957104086875916
I0808 01:00:22.654557 140066181404416 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.010622345842421055, loss=0.041319653391838074
I0808 01:00:42.985171 140240288372544 spec.py:320] Evaluating on the training split.
I0808 01:02:46.542932 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 01:02:49.888788 140240288372544 spec.py:348] Evaluating on the test split.
I0808 01:02:53.092777 140240288372544 submission_runner.py:364] Time since start: 2833.86s, 	Step: 5668, 	{'train/accuracy': 0.9889119267463684, 'train/loss': 0.03829492628574371, 'train/mean_average_precision': 0.25122407387542056, 'validation/accuracy': 0.9858127236366272, 'validation/loss': 0.04794996976852417, 'validation/mean_average_precision': 0.20042399930457336, 'validation/num_examples': 43793, 'test/accuracy': 0.9849317073822021, 'test/loss': 0.05053669586777687, 'test/mean_average_precision': 0.2001815375264897, 'test/num_examples': 43793, 'score': 1704.8532152175903, 'total_duration': 2833.8645961284637, 'accumulated_submission_time': 1704.8532152175903, 'accumulated_eval_time': 1128.7551126480103, 'accumulated_logging_time': 0.18965792655944824}
I0808 01:02:53.110458 140066173011712 logging_writer.py:48] [5668] accumulated_eval_time=1128.755113, accumulated_logging_time=0.189658, accumulated_submission_time=1704.853215, global_step=5668, preemption_count=0, score=1704.853215, test/accuracy=0.984932, test/loss=0.050537, test/mean_average_precision=0.200182, test/num_examples=43793, total_duration=2833.864596, train/accuracy=0.988912, train/loss=0.038295, train/mean_average_precision=0.251224, validation/accuracy=0.985813, validation/loss=0.047950, validation/mean_average_precision=0.200424, validation/num_examples=43793
I0808 01:03:03.225547 140176005240576 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01994633860886097, loss=0.0437631718814373
I0808 01:03:33.594055 140066173011712 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.012713732197880745, loss=0.03980616480112076
I0808 01:04:03.747926 140176005240576 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.015160750597715378, loss=0.03892127424478531
I0808 01:04:33.773396 140240288372544 spec.py:320] Evaluating on the training split.
I0808 01:06:31.898664 140240288372544 spec.py:332] Evaluating on the validation split.
I0808 01:06:34.851223 140240288372544 spec.py:348] Evaluating on the test split.
I0808 01:06:37.815863 140240288372544 submission_runner.py:364] Time since start: 3058.59s, 	Step: 6000, 	{'train/accuracy': 0.9892973899841309, 'train/loss': 0.03681842237710953, 'train/mean_average_precision': 0.27682318067163353, 'validation/accuracy': 0.9859678149223328, 'validation/loss': 0.04736611992120743, 'validation/mean_average_precision': 0.20814808803727852, 'validation/num_examples': 43793, 'test/accuracy': 0.9849852323532104, 'test/loss': 0.05015521124005318, 'test/mean_average_precision': 0.20688498719981022, 'test/num_examples': 43793, 'score': 1805.5004451274872, 'total_duration': 3058.587692975998, 'accumulated_submission_time': 1805.5004451274872, 'accumulated_eval_time': 1252.7975256443024, 'accumulated_logging_time': 0.21857357025146484}
I0808 01:06:37.831284 140077075326720 logging_writer.py:48] [6000] accumulated_eval_time=1252.797526, accumulated_logging_time=0.218574, accumulated_submission_time=1805.500445, global_step=6000, preemption_count=0, score=1805.500445, test/accuracy=0.984985, test/loss=0.050155, test/mean_average_precision=0.206885, test/num_examples=43793, total_duration=3058.587693, train/accuracy=0.989297, train/loss=0.036818, train/mean_average_precision=0.276823, validation/accuracy=0.985968, validation/loss=0.047366, validation/mean_average_precision=0.208148, validation/num_examples=43793
I0808 01:06:37.847472 140175912986368 logging_writer.py:48] [6000] global_step=6000, preemption_count=0, score=1805.500445
I0808 01:06:37.892061 140240288372544 checkpoints.py:490] Saving checkpoint at step: 6000
I0808 01:06:38.006229 140240288372544 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1/checkpoint_6000
I0808 01:06:38.007472 140240288372544 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_upgrade_preliminary/adamw/ogbg_jax/trial_1/checkpoint_6000.
I0808 01:06:38.161439 140240288372544 submission_runner.py:530] Tuning trial 1/1
I0808 01:06:38.161707 140240288372544 submission_runner.py:531] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0808 01:06:38.163321 140240288372544 submission_runner.py:532] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5046198964118958, 'train/loss': 0.7227272391319275, 'train/mean_average_precision': 0.021590107274601145, 'validation/accuracy': 0.5105925798416138, 'validation/loss': 0.7194507718086243, 'validation/mean_average_precision': 0.026683119495966856, 'validation/num_examples': 43793, 'test/accuracy': 0.5130106806755066, 'test/loss': 0.7191532254219055, 'test/mean_average_precision': 0.028235916743085274, 'test/num_examples': 43793, 'score': 24.151200532913208, 'total_duration': 323.3291618824005, 'accumulated_submission_time': 24.151200532913208, 'accumulated_eval_time': 299.177939414978, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (813, {'train/accuracy': 0.9866933822631836, 'train/loss': 0.05387502908706665, 'train/mean_average_precision': 0.04291431957925572, 'validation/accuracy': 0.9841272830963135, 'validation/loss': 0.06371606886386871, 'validation/mean_average_precision': 0.04190304384825048, 'validation/num_examples': 43793, 'test/accuracy': 0.9831513166427612, 'test/loss': 0.06698337197303772, 'test/mean_average_precision': 0.04182226764566002, 'test/num_examples': 43793, 'score': 264.13414096832275, 'total_duration': 676.7315173149109, 'accumulated_submission_time': 264.13414096832275, 'accumulated_eval_time': 412.5592677593231, 'accumulated_logging_time': 0.028700828552246094, 'global_step': 813, 'preemption_count': 0}), (1623, {'train/accuracy': 0.9873326420783997, 'train/loss': 0.04758629947900772, 'train/mean_average_precision': 0.09075100418657536, 'validation/accuracy': 0.9845096468925476, 'validation/loss': 0.05719640851020813, 'validation/mean_average_precision': 0.09496654902664603, 'validation/num_examples': 43793, 'test/accuracy': 0.9835299849510193, 'test/loss': 0.060526877641677856, 'test/mean_average_precision': 0.09155810964602416, 'test/num_examples': 43793, 'score': 504.1020448207855, 'total_duration': 1031.8934330940247, 'accumulated_submission_time': 504.1020448207855, 'accumulated_eval_time': 527.717206954956, 'accumulated_logging_time': 0.05543017387390137, 'global_step': 1623, 'preemption_count': 0}), (2434, {'train/accuracy': 0.9873964190483093, 'train/loss': 0.04586842283606529, 'train/mean_average_precision': 0.12195921378003309, 'validation/accuracy': 0.9846959710121155, 'validation/loss': 0.05539641156792641, 'validation/mean_average_precision': 0.11976489089627063, 'validation/num_examples': 43793, 'test/accuracy': 0.9836854338645935, 'test/loss': 0.05860842391848564, 'test/mean_average_precision': 0.11799772484473646, 'test/num_examples': 43793, 'score': 744.325510263443, 'total_duration': 1388.2317054271698, 'accumulated_submission_time': 744.325510263443, 'accumulated_eval_time': 643.7977638244629, 'accumulated_logging_time': 0.08050012588500977, 'global_step': 2434, 'preemption_count': 0}), (3245, {'train/accuracy': 0.9878032803535461, 'train/loss': 0.04346046969294548, 'train/mean_average_precision': 0.15754756636709868, 'validation/accuracy': 0.9851222038269043, 'validation/loss': 0.052109986543655396, 'validation/mean_average_precision': 0.14698940588253046, 'validation/num_examples': 43793, 'test/accuracy': 0.9841980338096619, 'test/loss': 0.054697971791028976, 'test/mean_average_precision': 0.1438010743765955, 'test/num_examples': 43793, 'score': 984.371369600296, 'total_duration': 1744.5283246040344, 'accumulated_submission_time': 984.371369600296, 'accumulated_eval_time': 760.0122117996216, 'accumulated_logging_time': 0.10749506950378418, 'global_step': 3245, 'preemption_count': 0}), (4063, {'train/accuracy': 0.9881353378295898, 'train/loss': 0.04135793447494507, 'train/mean_average_precision': 0.1829953593373077, 'validation/accuracy': 0.9851396679878235, 'validation/loss': 0.05070439353585243, 'validation/mean_average_precision': 0.16543952954513488, 'validation/num_examples': 43793, 'test/accuracy': 0.9842885732650757, 'test/loss': 0.05322302132844925, 'test/mean_average_precision': 0.1637908300792072, 'test/num_examples': 43793, 'score': 1224.5396528244019, 'total_duration': 2101.688425064087, 'accumulated_submission_time': 1224.5396528244019, 'accumulated_eval_time': 876.9686615467072, 'accumulated_logging_time': 0.13369035720825195, 'global_step': 4063, 'preemption_count': 0}), (4874, {'train/accuracy': 0.988304615020752, 'train/loss': 0.0400959849357605, 'train/mean_average_precision': 0.2133099315213401, 'validation/accuracy': 0.9855878353118896, 'validation/loss': 0.04914826527237892, 'validation/mean_average_precision': 0.18747076735709783, 'validation/num_examples': 43793, 'test/accuracy': 0.984661340713501, 'test/loss': 0.051862891763448715, 'test/mean_average_precision': 0.1848702120730487, 'test/num_examples': 43793, 'score': 1464.6099455356598, 'total_duration': 2463.4738550186157, 'accumulated_submission_time': 1464.6099455356598, 'accumulated_eval_time': 998.6475627422333, 'accumulated_logging_time': 0.16056585311889648, 'global_step': 4874, 'preemption_count': 0}), (5668, {'train/accuracy': 0.9889119267463684, 'train/loss': 0.03829492628574371, 'train/mean_average_precision': 0.25122407387542056, 'validation/accuracy': 0.9858127236366272, 'validation/loss': 0.04794996976852417, 'validation/mean_average_precision': 0.20042399930457336, 'validation/num_examples': 43793, 'test/accuracy': 0.9849317073822021, 'test/loss': 0.05053669586777687, 'test/mean_average_precision': 0.2001815375264897, 'test/num_examples': 43793, 'score': 1704.8532152175903, 'total_duration': 2833.8645961284637, 'accumulated_submission_time': 1704.8532152175903, 'accumulated_eval_time': 1128.7551126480103, 'accumulated_logging_time': 0.18965792655944824, 'global_step': 5668, 'preemption_count': 0}), (6000, {'train/accuracy': 0.9892973899841309, 'train/loss': 0.03681842237710953, 'train/mean_average_precision': 0.27682318067163353, 'validation/accuracy': 0.9859678149223328, 'validation/loss': 0.04736611992120743, 'validation/mean_average_precision': 0.20814808803727852, 'validation/num_examples': 43793, 'test/accuracy': 0.9849852323532104, 'test/loss': 0.05015521124005318, 'test/mean_average_precision': 0.20688498719981022, 'test/num_examples': 43793, 'score': 1805.5004451274872, 'total_duration': 3058.587692975998, 'accumulated_submission_time': 1805.5004451274872, 'accumulated_eval_time': 1252.7975256443024, 'accumulated_logging_time': 0.21857357025146484, 'global_step': 6000, 'preemption_count': 0})], 'global_step': 6000}
I0808 01:06:38.163475 140240288372544 submission_runner.py:533] Timing: 1805.5004451274872
I0808 01:06:38.163573 140240288372544 submission_runner.py:535] Total number of evals: 9
I0808 01:06:38.163680 140240288372544 submission_runner.py:536] ====================
I0808 01:06:38.163886 140240288372544 submission_runner.py:604] Final ogbg score: 1805.5004451274872
