python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=baselines/adamw/jax/submission.py --tuning_search_space=baselines/adamw/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_jax_upgrade_a/adamw --overwrite=True --save_checkpoints=False --max_global_steps=16000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_05-26-2023-18-00-38.log
/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ../c10/hip/HIPFunctions.cpp:110.)
  return torch._C._cuda_getDeviceCount() > 0
I0526 18:01:01.940576 140665212061504 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax.
I0526 18:01:02.956210 140665212061504 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0526 18:01:02.957166 140665212061504 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0526 18:01:02.957327 140665212061504 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0526 18:01:02.963037 140665212061504 submission_runner.py:549] Using RNG seed 2414030229
I0526 18:01:08.464022 140665212061504 submission_runner.py:558] --- Tuning run 1/1 ---
I0526 18:01:08.464261 140665212061504 submission_runner.py:563] Creating tuning directory at /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1.
I0526 18:01:08.466340 140665212061504 logger_utils.py:92] Saving hparams to /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1/hparams.json.
I0526 18:01:08.652927 140665212061504 submission_runner.py:243] Initializing dataset.
I0526 18:01:08.653172 140665212061504 submission_runner.py:250] Initializing model.
I0526 18:01:11.136640 140665212061504 submission_runner.py:260] Initializing optimizer.
I0526 18:01:11.849514 140665212061504 submission_runner.py:267] Initializing metrics bundle.
I0526 18:01:11.849803 140665212061504 submission_runner.py:285] Initializing checkpoint and logger.
I0526 18:01:11.851104 140665212061504 checkpoints.py:915] Found no checkpoint files in /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0526 18:01:11.851462 140665212061504 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0526 18:01:11.851544 140665212061504 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0526 18:01:12.632179 140665212061504 submission_runner.py:306] Saving meta data to /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0526 18:01:12.633254 140665212061504 submission_runner.py:309] Saving flags to /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0526 18:01:12.639938 140665212061504 submission_runner.py:321] Starting training loop.
I0526 18:01:12.945569 140665212061504 input_pipeline.py:20] Loading split = train-clean-100
I0526 18:01:12.987902 140665212061504 input_pipeline.py:20] Loading split = train-clean-360
I0526 18:01:13.405033 140665212061504 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0526 18:02:13.885153 140498475890432 logging_writer.py:48] [0] global_step=0, grad_norm=42.48813247680664, loss=32.60034942626953
I0526 18:02:13.906037 140665212061504 spec.py:298] Evaluating on the training split.
I0526 18:02:14.168379 140665212061504 input_pipeline.py:20] Loading split = train-clean-100
I0526 18:02:14.203303 140665212061504 input_pipeline.py:20] Loading split = train-clean-360
I0526 18:02:14.533922 140665212061504 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0526 18:03:43.391212 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 18:03:43.585669 140665212061504 input_pipeline.py:20] Loading split = dev-clean
I0526 18:03:43.591114 140665212061504 input_pipeline.py:20] Loading split = dev-other
I0526 18:04:35.335060 140665212061504 spec.py:326] Evaluating on the test split.
I0526 18:04:35.539190 140665212061504 input_pipeline.py:20] Loading split = test-clean
I0526 18:05:11.351865 140665212061504 submission_runner.py:426] Time since start: 238.71s, 	Step: 1, 	{'train/ctc_loss': Array(31.605133, dtype=float32), 'train/wer': 2.2168544239218706, 'validation/ctc_loss': Array(30.501688, dtype=float32), 'validation/wer': 2.0145298073305096, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.64987, dtype=float32), 'test/wer': 2.104767127739524, 'test/num_examples': 2472, 'score': 61.26592206954956, 'total_duration': 238.70971512794495, 'accumulated_submission_time': 61.26592206954956, 'accumulated_data_selection_time': 5.116481304168701, 'accumulated_eval_time': 177.44363236427307, 'accumulated_logging_time': 0}
I0526 18:05:11.377116 140496794019584 logging_writer.py:48] [1] accumulated_data_selection_time=5.116481, accumulated_eval_time=177.443632, accumulated_logging_time=0, accumulated_submission_time=61.265922, global_step=1, preemption_count=0, score=61.265922, test/ctc_loss=30.649869918823242, test/num_examples=2472, test/wer=2.104767, total_duration=238.709715, train/ctc_loss=31.605133056640625, train/wer=2.216854, validation/ctc_loss=30.50168800354004, validation/num_examples=5348, validation/wer=2.014530
I0526 18:06:35.828998 140505268770560 logging_writer.py:48] [100] global_step=100, grad_norm=5.168108940124512, loss=8.398422241210938
I0526 18:07:53.228306 140505277163264 logging_writer.py:48] [200] global_step=200, grad_norm=1.1287388801574707, loss=6.005085468292236
I0526 18:09:09.787284 140505268770560 logging_writer.py:48] [300] global_step=300, grad_norm=1.163962483406067, loss=5.831446647644043
I0526 18:10:25.941199 140505277163264 logging_writer.py:48] [400] global_step=400, grad_norm=0.8270374536514282, loss=5.776876449584961
I0526 18:11:42.092852 140505268770560 logging_writer.py:48] [500] global_step=500, grad_norm=0.5549586415290833, loss=5.738245487213135
I0526 18:12:59.105700 140505277163264 logging_writer.py:48] [600] global_step=600, grad_norm=0.5616191029548645, loss=5.621891975402832
I0526 18:14:15.119508 140505268770560 logging_writer.py:48] [700] global_step=700, grad_norm=3.939145803451538, loss=5.422478675842285
I0526 18:15:33.348943 140505277163264 logging_writer.py:48] [800] global_step=800, grad_norm=1.2693445682525635, loss=5.023624420166016
I0526 18:16:49.768468 140505268770560 logging_writer.py:48] [900] global_step=900, grad_norm=2.1231017112731934, loss=4.582744598388672
I0526 18:18:08.867983 140505277163264 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2854557037353516, loss=4.3071818351745605
I0526 18:19:29.151531 140507260028672 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.0275979042053223, loss=4.026120662689209
I0526 18:20:44.864712 140507251635968 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.708699107170105, loss=3.839512348175049
I0526 18:22:00.929298 140507260028672 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.3831450939178467, loss=3.639406204223633
I0526 18:23:16.215259 140507251635968 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.414553165435791, loss=3.5112147331237793
I0526 18:24:31.602171 140507260028672 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.6984550952911377, loss=3.3911995887756348
I0526 18:25:48.384561 140507251635968 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.711592435836792, loss=3.337351083755493
I0526 18:27:07.821909 140507260028672 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.4508635997772217, loss=3.140378713607788
I0526 18:28:28.929772 140507251635968 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.8031601905822754, loss=3.0935709476470947
I0526 18:29:51.452426 140507260028672 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.629474401473999, loss=3.0427722930908203
I0526 18:31:14.582664 140507251635968 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.8683998584747314, loss=2.9574341773986816
I0526 18:32:37.855326 140507260028672 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.0746870040893555, loss=2.8778038024902344
I0526 18:33:54.037431 140507251635968 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.280059337615967, loss=2.741422176361084
I0526 18:35:10.581808 140507260028672 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.6120004653930664, loss=2.715515375137329
I0526 18:36:25.745500 140507251635968 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.462519645690918, loss=2.697031021118164
I0526 18:37:42.060026 140507260028672 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.862858533859253, loss=2.65053653717041
I0526 18:38:58.296068 140507251635968 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.6039717197418213, loss=2.6124141216278076
I0526 18:40:15.568128 140507260028672 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.394547939300537, loss=2.494727611541748
I0526 18:41:35.608528 140507251635968 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.842435598373413, loss=2.4419775009155273
I0526 18:42:54.095947 140507260028672 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.677063226699829, loss=2.416822910308838
I0526 18:44:16.032730 140507251635968 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.044830799102783, loss=2.3849809169769287
I0526 18:45:11.838729 140665212061504 spec.py:298] Evaluating on the training split.
I0526 18:45:51.183529 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 18:46:30.404376 140665212061504 spec.py:326] Evaluating on the test split.
I0526 18:46:50.222346 140665212061504 submission_runner.py:426] Time since start: 2737.58s, 	Step: 3074, 	{'train/ctc_loss': Array(3.522665, dtype=float32), 'train/wer': 0.7530380424708039, 'validation/ctc_loss': Array(3.7186372, dtype=float32), 'validation/wer': 0.7569006936873487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.3067698, dtype=float32), 'test/wer': 0.7088741291410233, 'test/num_examples': 2472, 'score': 2461.676515340805, 'total_duration': 2737.578357696533, 'accumulated_submission_time': 2461.676515340805, 'accumulated_data_selection_time': 465.97851943969727, 'accumulated_eval_time': 275.8232674598694, 'accumulated_logging_time': 0.03723740577697754}
I0526 18:46:50.242281 140507260028672 logging_writer.py:48] [3074] accumulated_data_selection_time=465.978519, accumulated_eval_time=275.823267, accumulated_logging_time=0.037237, accumulated_submission_time=2461.676515, global_step=3074, preemption_count=0, score=2461.676515, test/ctc_loss=3.306769847869873, test/num_examples=2472, test/wer=0.708874, total_duration=2737.578358, train/ctc_loss=3.522665023803711, train/wer=0.753038, validation/ctc_loss=3.718637228012085, validation/num_examples=5348, validation/wer=0.756901
I0526 18:47:14.271622 140507260028672 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.3543927669525146, loss=2.325331687927246
I0526 18:48:31.318427 140507251635968 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.9361060857772827, loss=2.342473030090332
I0526 18:49:47.787164 140507260028672 logging_writer.py:48] [3300] global_step=3300, grad_norm=7.354928970336914, loss=2.360511064529419
I0526 18:51:02.490356 140507251635968 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.2420287132263184, loss=2.283175230026245
I0526 18:52:18.405291 140507260028672 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.231114387512207, loss=2.259592056274414
I0526 18:53:33.054266 140507251635968 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.621229648590088, loss=2.284497022628784
I0526 18:54:49.433561 140507260028672 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.186605453491211, loss=2.293741226196289
I0526 18:56:07.539054 140507251635968 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.3934032917022705, loss=2.147637128829956
I0526 18:57:28.025609 140507260028672 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.4751741886138916, loss=2.172360420227051
I0526 18:58:48.073238 140507251635968 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.029446601867676, loss=2.158447742462158
I0526 19:00:09.027366 140507260028672 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.5482399463653564, loss=2.166111946105957
I0526 19:01:28.633438 140507260028672 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.650940418243408, loss=2.1044561862945557
I0526 19:02:44.841716 140507251635968 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.9486770629882812, loss=2.0631778240203857
I0526 19:04:01.124612 140507260028672 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.7944178581237793, loss=1.9956306219100952
I0526 19:05:16.181344 140507251635968 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.718697190284729, loss=2.053739070892334
I0526 19:06:32.579431 140507260028672 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.7948930263519287, loss=2.0446724891662598
I0526 19:07:48.995755 140507251635968 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.5676491260528564, loss=1.9683319330215454
I0526 19:09:07.744621 140507260028672 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.132915735244751, loss=1.9743332862854004
I0526 19:10:28.755406 140507251635968 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.1512348651885986, loss=2.016601800918579
I0526 19:11:51.464234 140507260028672 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.1843881607055664, loss=1.9998140335083008
I0526 19:13:13.314521 140507251635968 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.593074083328247, loss=1.962767243385315
I0526 19:14:35.442471 140507260028672 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.6195943355560303, loss=1.9182474613189697
I0526 19:15:51.541689 140507251635968 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.153459310531616, loss=1.9252102375030518
I0526 19:17:07.608659 140507260028672 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.2093522548675537, loss=1.876833438873291
I0526 19:18:23.486527 140507251635968 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.155712842941284, loss=1.8757814168930054
I0526 19:19:38.613790 140507260028672 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.9272805452346802, loss=1.9395960569381714
I0526 19:20:53.292600 140507251635968 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.6818103790283203, loss=1.8848357200622559
I0526 19:22:15.068888 140507260028672 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.2343697547912598, loss=1.9085522890090942
I0526 19:23:36.326127 140507251635968 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.311816930770874, loss=1.972813606262207
I0526 19:24:57.575956 140507260028672 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.6653298139572144, loss=1.8440157175064087
I0526 19:26:19.595167 140507251635968 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.852797031402588, loss=1.9184110164642334
I0526 19:26:50.568054 140665212061504 spec.py:298] Evaluating on the training split.
I0526 19:27:38.708522 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 19:28:21.938583 140665212061504 spec.py:326] Evaluating on the test split.
I0526 19:28:44.459211 140665212061504 submission_runner.py:426] Time since start: 5251.82s, 	Step: 6140, 	{'train/ctc_loss': Array(0.65123934, dtype=float32), 'train/wer': 0.22124497243161176, 'validation/ctc_loss': Array(1.0540591, dtype=float32), 'validation/wer': 0.2951403293808913, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.69123757, dtype=float32), 'test/wer': 0.22497105599902503, 'test/num_examples': 2472, 'score': 4861.946359872818, 'total_duration': 5251.815352201462, 'accumulated_submission_time': 4861.946359872818, 'accumulated_data_selection_time': 1010.0378971099854, 'accumulated_eval_time': 389.7105870246887, 'accumulated_logging_time': 0.07214975357055664}
I0526 19:28:44.481013 140507260028672 logging_writer.py:48] [6140] accumulated_data_selection_time=1010.037897, accumulated_eval_time=389.710587, accumulated_logging_time=0.072150, accumulated_submission_time=4861.946360, global_step=6140, preemption_count=0, score=4861.946360, test/ctc_loss=0.6912375688552856, test/num_examples=2472, test/wer=0.224971, total_duration=5251.815352, train/ctc_loss=0.6512393355369568, train/wer=0.221245, validation/ctc_loss=1.0540591478347778, validation/num_examples=5348, validation/wer=0.295140
I0526 19:29:33.935969 140505949308672 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.626235246658325, loss=1.9082682132720947
I0526 19:30:48.515569 140505940915968 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.3093795776367188, loss=1.8683425188064575
I0526 19:32:03.118385 140505949308672 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.073117256164551, loss=1.8580631017684937
I0526 19:33:19.106870 140505940915968 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.049783229827881, loss=1.8413002490997314
I0526 19:34:35.986416 140505949308672 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.169532299041748, loss=1.9223566055297852
I0526 19:35:50.945360 140505940915968 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.4773824214935303, loss=1.8107430934906006
I0526 19:37:10.274597 140505949308672 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.792774200439453, loss=1.93912935256958
I0526 19:38:31.516334 140505940915968 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.7077466249465942, loss=1.8389486074447632
I0526 19:39:53.698990 140505949308672 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.12815260887146, loss=1.8741239309310913
I0526 19:41:15.455738 140505940915968 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.1610052585601807, loss=1.7663081884384155
I0526 19:42:37.507919 140505949308672 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.3374059200286865, loss=1.8579858541488647
I0526 19:43:55.853867 140506604668672 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.5000009536743164, loss=1.7879230976104736
I0526 19:45:11.392325 140506596275968 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.0201168060302734, loss=1.8317527770996094
I0526 19:46:26.551803 140506604668672 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.2345876693725586, loss=1.843119740486145
I0526 19:47:41.893107 140506596275968 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.6242140531539917, loss=1.7777190208435059
I0526 19:48:57.662692 140506604668672 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1590561866760254, loss=1.808831810951233
I0526 19:50:20.782229 140506596275968 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.0066771507263184, loss=1.8348878622055054
I0526 19:51:44.154858 140506604668672 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.037309169769287, loss=1.8572052717208862
I0526 19:53:07.562874 140506596275968 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.7380645275115967, loss=1.771659255027771
I0526 19:54:28.510792 140506604668672 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.0287322998046875, loss=1.7642700672149658
I0526 19:55:49.127746 140506596275968 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.9309576749801636, loss=1.7882177829742432
I0526 19:57:08.009353 140507260028672 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.54465651512146, loss=1.704868197441101
I0526 19:58:22.128811 140507251635968 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.6731791496276855, loss=1.8120269775390625
I0526 19:59:38.238498 140507260028672 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.5799214839935303, loss=1.802161693572998
I0526 20:00:52.685162 140507251635968 logging_writer.py:48] [8600] global_step=8600, grad_norm=4.076695919036865, loss=1.7747793197631836
I0526 20:02:08.084209 140507260028672 logging_writer.py:48] [8700] global_step=8700, grad_norm=7.471029758453369, loss=1.7027003765106201
I0526 20:03:25.545219 140507251635968 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.432446002960205, loss=1.7777397632598877
I0526 20:04:49.633747 140507260028672 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.7144827842712402, loss=1.7486004829406738
I0526 20:06:14.172768 140507251635968 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.5796778202056885, loss=1.6808725595474243
I0526 20:07:37.101686 140507260028672 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.846937894821167, loss=1.6909027099609375
I0526 20:08:44.531888 140665212061504 spec.py:298] Evaluating on the training split.
I0526 20:09:34.595977 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 20:10:18.341298 140665212061504 spec.py:326] Evaluating on the test split.
I0526 20:10:40.581109 140665212061504 submission_runner.py:426] Time since start: 7767.94s, 	Step: 9182, 	{'train/ctc_loss': Array(0.45406228, dtype=float32), 'train/wer': 0.16109239843762355, 'validation/ctc_loss': Array(0.8647607, dtype=float32), 'validation/wer': 0.2453376298854789, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5298119, dtype=float32), 'test/wer': 0.1753904901184165, 'test/num_examples': 2472, 'score': 7261.939821958542, 'total_duration': 7767.937376737595, 'accumulated_submission_time': 7261.939821958542, 'accumulated_data_selection_time': 1587.919186592102, 'accumulated_eval_time': 505.7560770511627, 'accumulated_logging_time': 0.11017441749572754}
I0526 20:10:40.602986 140507260028672 logging_writer.py:48] [9182] accumulated_data_selection_time=1587.919187, accumulated_eval_time=505.756077, accumulated_logging_time=0.110174, accumulated_submission_time=7261.939822, global_step=9182, preemption_count=0, score=7261.939822, test/ctc_loss=0.5298119187355042, test/num_examples=2472, test/wer=0.175390, total_duration=7767.937377, train/ctc_loss=0.454062283039093, train/wer=0.161092, validation/ctc_loss=0.86476069688797, validation/num_examples=5348, validation/wer=0.245338
I0526 20:10:55.047078 140507251635968 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.8456766605377197, loss=1.8072118759155273
I0526 20:12:13.971342 140505949308672 logging_writer.py:48] [9300] global_step=9300, grad_norm=6.981875419616699, loss=1.8546704053878784
I0526 20:13:28.913936 140505940915968 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.2327630519866943, loss=1.745901346206665
I0526 20:14:44.174872 140505949308672 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.1273889541625977, loss=1.8262938261032104
I0526 20:16:00.534014 140505940915968 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.42461895942688, loss=1.7676175832748413
I0526 20:17:16.452424 140505949308672 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.846919059753418, loss=1.809544563293457
I0526 20:18:31.516916 140505940915968 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.8971714973449707, loss=1.791189193725586
I0526 20:19:50.695853 140505949308672 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.7227513790130615, loss=1.7425758838653564
I0526 20:21:13.975608 140505940915968 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.204958915710449, loss=1.8073251247406006
I0526 20:22:34.105585 140505949308672 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.5139777660369873, loss=1.7115857601165771
I0526 20:23:55.555756 140505940915968 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.75198221206665, loss=1.8655445575714111
I0526 20:25:19.789360 140506604668672 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.2824559211730957, loss=1.728546142578125
I0526 20:26:33.913182 140506596275968 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.6347079277038574, loss=1.7684190273284912
I0526 20:27:48.240988 140506604668672 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.895958662033081, loss=2.258826732635498
I0526 20:29:03.726935 140506596275968 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.1106560230255127, loss=1.7892402410507202
I0526 20:30:18.066739 140506604668672 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.8593177795410156, loss=1.6843138933181763
I0526 20:31:35.095009 140506596275968 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.7035584449768066, loss=1.7711162567138672
I0526 20:32:56.905028 140506604668672 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.7926106452941895, loss=1.6644933223724365
I0526 20:34:19.723160 140506596275968 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.402494192123413, loss=1.7546699047088623
I0526 20:35:42.060976 140506604668672 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.516883134841919, loss=1.7160701751708984
I0526 20:37:05.011236 140506596275968 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.675379753112793, loss=1.734540343284607
I0526 20:38:28.822259 140506604668672 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.019647121429443, loss=1.845611810684204
I0526 20:39:49.365912 140507260028672 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.9101192951202393, loss=1.7361012697219849
I0526 20:41:05.919332 140507251635968 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.6897311210632324, loss=1.673618197441101
I0526 20:42:21.645624 140507260028672 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.596862554550171, loss=1.6426283121109009
I0526 20:43:36.871321 140507251635968 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.033755302429199, loss=1.7251043319702148
I0526 20:44:52.550984 140507260028672 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.303389072418213, loss=1.7198671102523804
I0526 20:46:06.932158 140507251635968 logging_writer.py:48] [11900] global_step=11900, grad_norm=7.7922868728637695, loss=1.8006176948547363
I0526 20:47:30.691578 140507260028672 logging_writer.py:48] [12000] global_step=12000, grad_norm=13.107100486755371, loss=1.6848905086517334
I0526 20:48:54.819093 140507251635968 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.7556068897247314, loss=1.7159401178359985
I0526 20:50:16.610306 140507260028672 logging_writer.py:48] [12200] global_step=12200, grad_norm=6.092728614807129, loss=1.7088713645935059
I0526 20:50:40.983418 140665212061504 spec.py:298] Evaluating on the training split.
I0526 20:51:31.842787 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 20:52:16.441802 140665212061504 spec.py:326] Evaluating on the test split.
I0526 20:52:39.491146 140665212061504 submission_runner.py:426] Time since start: 10286.85s, 	Step: 12230, 	{'train/ctc_loss': Array(0.4588607, dtype=float32), 'train/wer': 0.15624836368586958, 'validation/ctc_loss': Array(0.8318703, dtype=float32), 'validation/wer': 0.2343003791642949, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.502279, dtype=float32), 'test/wer': 0.16230983283569964, 'test/num_examples': 2472, 'score': 9662.264205932617, 'total_duration': 10286.847130060196, 'accumulated_submission_time': 9662.264205932617, 'accumulated_data_selection_time': 2159.5501165390015, 'accumulated_eval_time': 624.2598078250885, 'accumulated_logging_time': 0.14711451530456543}
I0526 20:52:39.512706 140507260028672 logging_writer.py:48] [12230] accumulated_data_selection_time=2159.550117, accumulated_eval_time=624.259808, accumulated_logging_time=0.147115, accumulated_submission_time=9662.264206, global_step=12230, preemption_count=0, score=9662.264206, test/ctc_loss=0.5022789835929871, test/num_examples=2472, test/wer=0.162310, total_duration=10286.847130, train/ctc_loss=0.45886069536209106, train/wer=0.156248, validation/ctc_loss=0.8318703174591064, validation/num_examples=5348, validation/wer=0.234300
I0526 20:53:32.227113 140507251635968 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.893414258956909, loss=1.73129403591156
I0526 20:54:50.431824 140506604668672 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.804596424102783, loss=1.8055857419967651
I0526 20:56:05.373196 140506596275968 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.8568997383117676, loss=1.6937154531478882
I0526 20:57:20.845087 140506604668672 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.821308135986328, loss=1.6560150384902954
I0526 20:58:37.123562 140506596275968 logging_writer.py:48] [12700] global_step=12700, grad_norm=5.129011631011963, loss=1.7388324737548828
I0526 20:59:51.239667 140506604668672 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.8861327171325684, loss=1.6714693307876587
I0526 21:01:08.304620 140506596275968 logging_writer.py:48] [12900] global_step=12900, grad_norm=5.758816242218018, loss=1.667250394821167
I0526 21:02:31.652477 140506604668672 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.978017330169678, loss=1.705580711364746
I0526 21:03:54.451417 140506596275968 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.6598589420318604, loss=1.6486060619354248
I0526 21:05:18.948475 140506604668672 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.7996270656585693, loss=1.7625387907028198
I0526 21:06:41.477231 140506596275968 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.6392738819122314, loss=1.7035530805587769
I0526 21:08:07.466124 140505949308672 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.299036502838135, loss=1.6715260744094849
I0526 21:09:22.685453 140505940915968 logging_writer.py:48] [13500] global_step=13500, grad_norm=16.1571102142334, loss=1.9642751216888428
I0526 21:10:38.109753 140505949308672 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.122581720352173, loss=1.785718321800232
I0526 21:11:53.655987 140505940915968 logging_writer.py:48] [13700] global_step=13700, grad_norm=325.6844482421875, loss=3.1262433528900146
I0526 21:13:09.604162 140505949308672 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.64250111579895, loss=1.689767837524414
I0526 21:14:25.051210 140505940915968 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.2386748790740967, loss=1.680151343345642
I0526 21:15:40.758271 140505949308672 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.493985652923584, loss=1.7533268928527832
I0526 21:16:55.778761 140505940915968 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.217944860458374, loss=1.689757227897644
I0526 21:18:17.571011 140505949308672 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.7697196006774902, loss=1.7197784185409546
I0526 21:19:37.105477 140505940915968 logging_writer.py:48] [14300] global_step=14300, grad_norm=7.149017333984375, loss=1.84031081199646
I0526 21:20:59.210368 140505949308672 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.1598100662231445, loss=1.7644009590148926
I0526 21:22:18.890780 140505949308672 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6694753170013428, loss=1.6626474857330322
I0526 21:23:34.471167 140505940915968 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.4075253009796143, loss=1.8052550554275513
I0526 21:24:50.047729 140505949308672 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.805757761001587, loss=1.6641418933868408
I0526 21:26:06.042776 140505940915968 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.9751474857330322, loss=1.6633533239364624
I0526 21:27:22.067855 140505949308672 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.982542037963867, loss=1.6194885969161987
I0526 21:28:37.301275 140505940915968 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.230426788330078, loss=1.8357694149017334
I0526 21:29:53.184906 140505949308672 logging_writer.py:48] [15100] global_step=15100, grad_norm=7.431095123291016, loss=1.7185128927230835
I0526 21:31:09.256784 140505940915968 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.904672861099243, loss=1.6923161745071411
I0526 21:32:25.018485 140505949308672 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.598689317703247, loss=1.6510785818099976
I0526 21:32:40.075383 140665212061504 spec.py:298] Evaluating on the training split.
I0526 21:33:28.996272 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 21:34:11.976013 140665212061504 spec.py:326] Evaluating on the test split.
I0526 21:34:34.457449 140665212061504 submission_runner.py:426] Time since start: 12801.81s, 	Step: 15321, 	{'train/ctc_loss': Array(0.4120031, dtype=float32), 'train/wer': 0.142066581138968, 'validation/ctc_loss': Array(0.76329786, dtype=float32), 'validation/wer': 0.21881542513675964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4575008, dtype=float32), 'test/wer': 0.14760424918245893, 'test/num_examples': 2472, 'score': 12062.76822733879, 'total_duration': 12801.813318014145, 'accumulated_submission_time': 12062.76822733879, 'accumulated_data_selection_time': 2689.4030039310455, 'accumulated_eval_time': 738.6377646923065, 'accumulated_logging_time': 0.18582606315612793}
I0526 21:34:34.478145 140505949308672 logging_writer.py:48] [15321] accumulated_data_selection_time=2689.403004, accumulated_eval_time=738.637765, accumulated_logging_time=0.185826, accumulated_submission_time=12062.768227, global_step=15321, preemption_count=0, score=12062.768227, test/ctc_loss=0.45750078558921814, test/num_examples=2472, test/wer=0.147604, total_duration=12801.813318, train/ctc_loss=0.4120030999183655, train/wer=0.142067, validation/ctc_loss=0.7632978558540344, validation/num_examples=5348, validation/wer=0.218815
I0526 21:35:35.385782 140505940915968 logging_writer.py:48] [15400] global_step=15400, grad_norm=7.0108137130737305, loss=1.8535306453704834
I0526 21:36:53.778503 140507260028672 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.220913887023926, loss=1.6120257377624512
I0526 21:38:08.606343 140507251635968 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.451960802078247, loss=1.6374324560165405
I0526 21:39:24.544519 140507260028672 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.9561386108398438, loss=1.630015254020691
I0526 21:40:39.267278 140507251635968 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.7326157093048096, loss=1.6154536008834839
I0526 21:41:55.341509 140507260028672 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.2394423484802246, loss=1.6944794654846191
I0526 21:43:09.968558 140665212061504 spec.py:298] Evaluating on the training split.
I0526 21:43:58.677112 140665212061504 spec.py:310] Evaluating on the validation split.
I0526 21:44:42.236955 140665212061504 spec.py:326] Evaluating on the test split.
I0526 21:45:04.727080 140665212061504 submission_runner.py:426] Time since start: 13432.08s, 	Step: 16000, 	{'train/ctc_loss': Array(0.39524344, dtype=float32), 'train/wer': 0.13185845901062143, 'validation/ctc_loss': Array(0.7023348, dtype=float32), 'validation/wer': 0.20038784744667099, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41991663, dtype=float32), 'test/wer': 0.1359047793146873, 'test/num_examples': 2472, 'score': 12578.234146356583, 'total_duration': 13432.084242343903, 'accumulated_submission_time': 12578.234146356583, 'accumulated_data_selection_time': 2789.466808319092, 'accumulated_eval_time': 853.3934607505798, 'accumulated_logging_time': 0.22128558158874512}
I0526 21:45:04.748383 140507260028672 logging_writer.py:48] [16000] accumulated_data_selection_time=2789.466808, accumulated_eval_time=853.393461, accumulated_logging_time=0.221286, accumulated_submission_time=12578.234146, global_step=16000, preemption_count=0, score=12578.234146, test/ctc_loss=0.41991662979125977, test/num_examples=2472, test/wer=0.135905, total_duration=13432.084242, train/ctc_loss=0.39524343609809875, train/wer=0.131858, validation/ctc_loss=0.7023348212242126, validation/num_examples=5348, validation/wer=0.200388
I0526 21:45:04.768865 140507251635968 logging_writer.py:48] [16000] global_step=16000, preemption_count=0, score=12578.234146
I0526 21:45:04.957129 140665212061504 checkpoints.py:490] Saving checkpoint at step: 16000
I0526 21:45:05.861037 140665212061504 checkpoints.py:422] Saved checkpoint at /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000
I0526 21:45:05.882786 140665212061504 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_jax_upgrade_a/adamw/librispeech_deepspeech_jax/trial_1/checkpoint_16000.
I0526 21:45:07.088952 140665212061504 submission_runner.py:589] Tuning trial 1/1
I0526 21:45:07.089201 140665212061504 submission_runner.py:590] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0526 21:45:07.094477 140665212061504 submission_runner.py:591] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.605133, dtype=float32), 'train/wer': 2.2168544239218706, 'validation/ctc_loss': Array(30.501688, dtype=float32), 'validation/wer': 2.0145298073305096, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.64987, dtype=float32), 'test/wer': 2.104767127739524, 'test/num_examples': 2472, 'score': 61.26592206954956, 'total_duration': 238.70971512794495, 'accumulated_submission_time': 61.26592206954956, 'accumulated_data_selection_time': 5.116481304168701, 'accumulated_eval_time': 177.44363236427307, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3074, {'train/ctc_loss': Array(3.522665, dtype=float32), 'train/wer': 0.7530380424708039, 'validation/ctc_loss': Array(3.7186372, dtype=float32), 'validation/wer': 0.7569006936873487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.3067698, dtype=float32), 'test/wer': 0.7088741291410233, 'test/num_examples': 2472, 'score': 2461.676515340805, 'total_duration': 2737.578357696533, 'accumulated_submission_time': 2461.676515340805, 'accumulated_data_selection_time': 465.97851943969727, 'accumulated_eval_time': 275.8232674598694, 'accumulated_logging_time': 0.03723740577697754, 'global_step': 3074, 'preemption_count': 0}), (6140, {'train/ctc_loss': Array(0.65123934, dtype=float32), 'train/wer': 0.22124497243161176, 'validation/ctc_loss': Array(1.0540591, dtype=float32), 'validation/wer': 0.2951403293808913, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.69123757, dtype=float32), 'test/wer': 0.22497105599902503, 'test/num_examples': 2472, 'score': 4861.946359872818, 'total_duration': 5251.815352201462, 'accumulated_submission_time': 4861.946359872818, 'accumulated_data_selection_time': 1010.0378971099854, 'accumulated_eval_time': 389.7105870246887, 'accumulated_logging_time': 0.07214975357055664, 'global_step': 6140, 'preemption_count': 0}), (9182, {'train/ctc_loss': Array(0.45406228, dtype=float32), 'train/wer': 0.16109239843762355, 'validation/ctc_loss': Array(0.8647607, dtype=float32), 'validation/wer': 0.2453376298854789, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5298119, dtype=float32), 'test/wer': 0.1753904901184165, 'test/num_examples': 2472, 'score': 7261.939821958542, 'total_duration': 7767.937376737595, 'accumulated_submission_time': 7261.939821958542, 'accumulated_data_selection_time': 1587.919186592102, 'accumulated_eval_time': 505.7560770511627, 'accumulated_logging_time': 0.11017441749572754, 'global_step': 9182, 'preemption_count': 0}), (12230, {'train/ctc_loss': Array(0.4588607, dtype=float32), 'train/wer': 0.15624836368586958, 'validation/ctc_loss': Array(0.8318703, dtype=float32), 'validation/wer': 0.2343003791642949, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.502279, dtype=float32), 'test/wer': 0.16230983283569964, 'test/num_examples': 2472, 'score': 9662.264205932617, 'total_duration': 10286.847130060196, 'accumulated_submission_time': 9662.264205932617, 'accumulated_data_selection_time': 2159.5501165390015, 'accumulated_eval_time': 624.2598078250885, 'accumulated_logging_time': 0.14711451530456543, 'global_step': 12230, 'preemption_count': 0}), (15321, {'train/ctc_loss': Array(0.4120031, dtype=float32), 'train/wer': 0.142066581138968, 'validation/ctc_loss': Array(0.76329786, dtype=float32), 'validation/wer': 0.21881542513675964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4575008, dtype=float32), 'test/wer': 0.14760424918245893, 'test/num_examples': 2472, 'score': 12062.76822733879, 'total_duration': 12801.813318014145, 'accumulated_submission_time': 12062.76822733879, 'accumulated_data_selection_time': 2689.4030039310455, 'accumulated_eval_time': 738.6377646923065, 'accumulated_logging_time': 0.18582606315612793, 'global_step': 15321, 'preemption_count': 0}), (16000, {'train/ctc_loss': Array(0.39524344, dtype=float32), 'train/wer': 0.13185845901062143, 'validation/ctc_loss': Array(0.7023348, dtype=float32), 'validation/wer': 0.20038784744667099, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41991663, dtype=float32), 'test/wer': 0.1359047793146873, 'test/num_examples': 2472, 'score': 12578.234146356583, 'total_duration': 13432.084242343903, 'accumulated_submission_time': 12578.234146356583, 'accumulated_data_selection_time': 2789.466808319092, 'accumulated_eval_time': 853.3934607505798, 'accumulated_logging_time': 0.22128558158874512, 'global_step': 16000, 'preemption_count': 0})], 'global_step': 16000}
I0526 21:45:07.094606 140665212061504 submission_runner.py:592] Timing: 12578.234146356583
I0526 21:45:07.094673 140665212061504 submission_runner.py:593] ====================
I0526 21:45:07.095842 140665212061504 submission_runner.py:661] Final librispeech_deepspeech score: 12578.234146356583
