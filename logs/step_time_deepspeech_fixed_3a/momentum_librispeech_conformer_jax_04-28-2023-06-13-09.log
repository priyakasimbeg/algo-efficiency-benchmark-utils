python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=baselines/momentum/jax/submission.py --tuning_search_space=baselines/momentum/tuning_search_space_conformer.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=timing_v3/timing_momentum --overwrite=False --save_checkpoints=True --max_global_steps=20000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_04-28-2023-06-13-09.log
I0428 06:13:30.787531 140520195979072 logger_utils.py:76] Creating experiment directory at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax.
I0428 06:13:30.867278 140520195979072 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0428 06:13:31.708423 140520195979072 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0428 06:13:31.709828 140520195979072 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0428 06:13:31.714149 140520195979072 submission_runner.py:538] Using RNG seed 41687355
I0428 06:13:34.550018 140520195979072 submission_runner.py:547] --- Tuning run 1/1 ---
I0428 06:13:34.550256 140520195979072 submission_runner.py:552] Creating tuning directory at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1.
I0428 06:13:34.550578 140520195979072 logger_utils.py:92] Saving hparams to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/hparams.json.
I0428 06:13:34.678297 140520195979072 submission_runner.py:241] Initializing dataset.
I0428 06:13:34.678495 140520195979072 submission_runner.py:248] Initializing model.
I0428 06:13:40.584592 140520195979072 submission_runner.py:258] Initializing optimizer.
I0428 06:13:41.262745 140520195979072 submission_runner.py:265] Initializing metrics bundle.
I0428 06:13:41.262923 140520195979072 submission_runner.py:282] Initializing checkpoint and logger.
I0428 06:13:41.264175 140520195979072 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0428 06:13:41.264434 140520195979072 logger_utils.py:239] Unable to record workload.train_mean information. Continuing without it.
I0428 06:13:41.264499 140520195979072 logger_utils.py:239] Unable to record workload.train_stddev information. Continuing without it.
I0428 06:13:41.984330 140520195979072 submission_runner.py:303] Saving meta data to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0428 06:13:41.985263 140520195979072 submission_runner.py:306] Saving flags to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/flags_0.json.
I0428 06:13:41.992586 140520195979072 submission_runner.py:318] Starting training loop.
I0428 06:13:42.187699 140520195979072 input_pipeline.py:20] Loading split = train-clean-100
I0428 06:13:42.222965 140520195979072 input_pipeline.py:20] Loading split = train-clean-360
I0428 06:13:42.545440 140520195979072 input_pipeline.py:20] Loading split = train-other-500
2023-04-28 06:14:43.302013: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-28 06:14:43.657874: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0428 06:14:45.371565 140344255571712 logging_writer.py:48] [0] global_step=0, grad_norm=29.453792572021484, loss=33.31920623779297
I0428 06:14:45.398587 140520195979072 spec.py:298] Evaluating on the training split.
I0428 06:14:45.499947 140520195979072 input_pipeline.py:20] Loading split = train-clean-100
I0428 06:14:45.527482 140520195979072 input_pipeline.py:20] Loading split = train-clean-360
I0428 06:14:45.818605 140520195979072 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0428 06:15:25.769061 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 06:15:25.836206 140520195979072 input_pipeline.py:20] Loading split = dev-clean
I0428 06:15:25.840648 140520195979072 input_pipeline.py:20] Loading split = dev-other
I0428 06:16:03.895508 140520195979072 spec.py:326] Evaluating on the test split.
I0428 06:16:03.957398 140520195979072 input_pipeline.py:20] Loading split = test-clean
I0428 06:16:32.013814 140520195979072 submission_runner.py:415] Time since start: 170.02s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(32.140953, dtype=float32), 'train/wer': 0.9480717572829686, 'validation/ctc_loss': DeviceArray(31.378462, dtype=float32), 'validation/wer': 0.961707300601067, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.495008, dtype=float32), 'test/wer': 0.9620376576686369, 'test/num_examples': 2472, 'score': 63.40580916404724, 'total_duration': 170.01994276046753, 'accumulated_submission_time': 63.40580916404724, 'accumulated_eval_time': 106.61396765708923, 'accumulated_logging_time': 0}
I0428 06:16:32.034746 140340707194624 logging_writer.py:48] [1] accumulated_eval_time=106.613968, accumulated_logging_time=0, accumulated_submission_time=63.405809, global_step=1, preemption_count=0, score=63.405809, test/ctc_loss=31.49500846862793, test/num_examples=2472, test/wer=0.962038, total_duration=170.019943, train/ctc_loss=32.140953063964844, train/wer=0.948072, validation/ctc_loss=31.378461837768555, validation/num_examples=5348, validation/wer=0.961707
I0428 06:16:32.220982 140520195979072 checkpoints.py:356] Saving checkpoint at step: 1
I0428 06:16:32.846886 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_1
I0428 06:16:32.847897 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_1.
I0428 06:18:02.633444 140346126317312 logging_writer.py:48] [100] global_step=100, grad_norm=48.71158218383789, loss=8.779129028320312
I0428 06:19:18.289401 140346134710016 logging_writer.py:48] [200] global_step=200, grad_norm=4.053994178771973, loss=6.096956253051758
I0428 06:20:33.864879 140346126317312 logging_writer.py:48] [300] global_step=300, grad_norm=1.0716205835342407, loss=6.160700798034668
I0428 06:21:49.437425 140346134710016 logging_writer.py:48] [400] global_step=400, grad_norm=2.604811668395996, loss=5.995185375213623
I0428 06:23:04.989336 140346126317312 logging_writer.py:48] [500] global_step=500, grad_norm=0.47082066535949707, loss=5.925817012786865
I0428 06:24:20.592629 140346134710016 logging_writer.py:48] [600] global_step=600, grad_norm=0.33924755454063416, loss=5.909250259399414
I0428 06:25:36.181625 140346126317312 logging_writer.py:48] [700] global_step=700, grad_norm=0.2938137352466583, loss=5.906857490539551
I0428 06:26:55.691313 140346134710016 logging_writer.py:48] [800] global_step=800, grad_norm=2.043168544769287, loss=5.910008430480957
I0428 06:28:15.951317 140346126317312 logging_writer.py:48] [900] global_step=900, grad_norm=0.30652040243148804, loss=5.9210028648376465
I0428 06:29:35.958248 140346134710016 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.32877254486084, loss=5.829237461090088
I0428 06:30:55.212065 140347588105984 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5397565364837646, loss=5.818036079406738
I0428 06:32:10.867452 140347579713280 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.5226240158081055, loss=5.8523454666137695
I0428 06:33:26.571652 140347588105984 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5250449776649475, loss=5.8314971923828125
I0428 06:34:42.266727 140347579713280 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.5272512435913086, loss=5.819770336151123
I0428 06:35:57.943315 140347588105984 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.26148539781570435, loss=5.801403045654297
I0428 06:37:13.626709 140347579713280 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1994041204452515, loss=5.8173370361328125
I0428 06:38:36.220921 140347588105984 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.7466601133346558, loss=5.8336944580078125
I0428 06:39:59.431500 140347579713280 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.21027135848999, loss=5.867465019226074
I0428 06:41:21.606977 140347588105984 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.5989916324615479, loss=5.821841716766357
I0428 06:42:44.806773 140347579713280 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.6194581985473633, loss=5.85180139541626
I0428 06:44:09.491789 140346932745984 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.3822543621063232, loss=5.830629348754883
I0428 06:45:25.174049 140346924353280 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8143994808197021, loss=5.8088603019714355
I0428 06:46:40.727840 140346932745984 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.115670680999756, loss=5.815461158752441
I0428 06:47:56.351379 140346924353280 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9750339388847351, loss=5.81327486038208
I0428 06:49:12.024369 140346932745984 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.061409592628479, loss=5.793228626251221
I0428 06:50:27.691780 140346924353280 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.2861106395721436, loss=5.805813789367676
I0428 06:51:46.360477 140346932745984 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.19958773255348206, loss=5.811959266662598
I0428 06:53:10.706211 140346924353280 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9903963208198547, loss=5.795213222503662
I0428 06:54:34.459523 140346932745984 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.367161989212036, loss=5.812323570251465
I0428 06:55:58.433428 140346924353280 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.496256947517395, loss=5.823606014251709
I0428 06:56:33.528587 140520195979072 spec.py:298] Evaluating on the training split.
I0428 06:57:00.772694 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 06:57:36.590071 140520195979072 spec.py:326] Evaluating on the test split.
I0428 06:57:54.645382 140520195979072 submission_runner.py:415] Time since start: 2652.65s, 	Step: 3044, 	{'train/ctc_loss': DeviceArray(6.0822186, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.1345134, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.1199045, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2464.032116651535, 'total_duration': 2652.6490638256073, 'accumulated_submission_time': 2464.032116651535, 'accumulated_eval_time': 187.72709226608276, 'accumulated_logging_time': 0.8401167392730713}
I0428 06:57:54.666898 140346932745984 logging_writer.py:48] [3044] accumulated_eval_time=187.727092, accumulated_logging_time=0.840117, accumulated_submission_time=2464.032117, global_step=3044, preemption_count=0, score=2464.032117, test/ctc_loss=6.119904518127441, test/num_examples=2472, test/wer=0.899580, total_duration=2652.649064, train/ctc_loss=6.082218647003174, train/wer=0.944636, validation/ctc_loss=6.1345133781433105, validation/num_examples=5348, validation/wer=0.895995
I0428 06:57:54.882743 140520195979072 checkpoints.py:356] Saving checkpoint at step: 3044
I0428 06:57:55.917997 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_3044
I0428 06:57:55.939535 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_3044.
I0428 06:58:42.720663 140346932745984 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1037286520004272, loss=5.799157619476318
I0428 06:59:58.409835 140346924353280 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.20225019752979279, loss=5.831820011138916
I0428 07:01:14.039556 140346932745984 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.2193801999092102, loss=5.782633304595947
I0428 07:02:29.717494 140346924353280 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.145442396402359, loss=5.779628753662109
I0428 07:03:45.370586 140346932745984 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.538641095161438, loss=5.771811008453369
I0428 07:05:03.124979 140346924353280 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.3591396808624268, loss=5.815877914428711
I0428 07:06:25.975974 140346932745984 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9361611008644104, loss=5.833124160766602
I0428 07:07:49.345970 140346924353280 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.12671934068202972, loss=5.788727283477783
I0428 07:09:11.865368 140346932745984 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.26869264245033264, loss=5.780926704406738
I0428 07:10:36.667470 140346924353280 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.30776894092559814, loss=5.78162145614624
I0428 07:12:00.035332 140346932745984 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0774611234664917, loss=5.7921271324157715
I0428 07:13:20.466389 140346932745984 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9663704037666321, loss=5.7989182472229
I0428 07:14:36.053489 140346924353280 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.22154143452644348, loss=5.811620235443115
I0428 07:15:51.703519 140346932745984 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.1809133291244507, loss=5.808749198913574
I0428 07:17:07.304484 140346924353280 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.34400475025177, loss=5.7738494873046875
I0428 07:18:22.937868 140346932745984 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.2238830029964447, loss=5.775418281555176
I0428 07:19:40.304539 140346924353280 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.896871030330658, loss=5.806344032287598
I0428 07:21:02.678775 140346932745984 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.4452717304229736, loss=5.804103374481201
I0428 07:22:24.253727 140346924353280 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5880376100540161, loss=5.795629978179932
I0428 07:23:45.146596 140346932745984 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.14148284494876862, loss=5.787058353424072
I0428 07:25:05.968984 140346924353280 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.2215440422296524, loss=5.784267902374268
I0428 07:26:27.577809 140347588105984 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0369935035705566, loss=5.811882495880127
I0428 07:27:43.239591 140347579713280 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.24036112427711487, loss=5.799444675445557
I0428 07:28:58.915890 140347588105984 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4885857105255127, loss=5.770232677459717
I0428 07:30:14.594285 140347579713280 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5971499085426331, loss=5.80124044418335
I0428 07:31:30.234934 140347588105984 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.2376624345779419, loss=5.77083158493042
I0428 07:32:45.881673 140347579713280 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7738438248634338, loss=5.7773823738098145
I0428 07:34:01.532232 140347588105984 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.9092986583709717, loss=5.715472221374512
I0428 07:35:21.674740 140347579713280 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7299267649650574, loss=5.6609907150268555
I0428 07:36:43.340893 140347588105984 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7538845539093018, loss=5.649692058563232
I0428 07:37:56.221128 140520195979072 spec.py:298] Evaluating on the training split.
I0428 07:38:23.519941 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 07:38:59.180095 140520195979072 spec.py:326] Evaluating on the test split.
I0428 07:39:17.195201 140520195979072 submission_runner.py:415] Time since start: 5135.20s, 	Step: 6090, 	{'train/ctc_loss': DeviceArray(9.919225, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(9.65895, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(9.63769, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4864.253120183945, 'total_duration': 5135.199232816696, 'accumulated_submission_time': 4864.253120183945, 'accumulated_eval_time': 268.69786834716797, 'accumulated_logging_time': 2.1474766731262207}
I0428 07:39:17.218149 140347588105984 logging_writer.py:48] [6090] accumulated_eval_time=268.697868, accumulated_logging_time=2.147477, accumulated_submission_time=4864.253120, global_step=6090, preemption_count=0, score=4864.253120, test/ctc_loss=9.637689590454102, test/num_examples=2472, test/wer=0.899580, total_duration=5135.199233, train/ctc_loss=9.919224739074707, train/wer=0.942722, validation/ctc_loss=9.658949851989746, validation/num_examples=5348, validation/wer=0.895995
I0428 07:39:17.420576 140520195979072 checkpoints.py:356] Saving checkpoint at step: 6090
I0428 07:39:18.395781 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_6090
I0428 07:39:18.417547 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_6090.
I0428 07:39:26.789463 140347579713280 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5191413164138794, loss=5.641392230987549
I0428 07:40:45.818915 140347588105984 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.42885822057724, loss=5.625674247741699
I0428 07:42:01.485171 140347579713280 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5332581400871277, loss=5.60870361328125
I0428 07:43:17.217969 140347588105984 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5132708549499512, loss=5.650152206420898
I0428 07:44:32.891224 140347579713280 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8909463882446289, loss=5.642024993896484
I0428 07:45:48.622816 140347588105984 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.46045926213264465, loss=5.617582321166992
I0428 07:47:04.339829 140347579713280 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.2782338261604309, loss=5.649497985839844
I0428 07:48:22.170472 140347588105984 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5350922346115112, loss=5.618189334869385
I0428 07:49:46.497287 140347579713280 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.39988043904304504, loss=5.593415260314941
I0428 07:51:09.844841 140347588105984 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8257678747177124, loss=5.618764400482178
I0428 07:52:34.259951 140347579713280 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6919776201248169, loss=5.5889387130737305
I0428 07:53:59.432357 140347588105984 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9750670194625854, loss=5.629655361175537
I0428 07:55:18.838262 140347588105984 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.39673882722854614, loss=5.612421035766602
I0428 07:56:34.616196 140347579713280 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9071573615074158, loss=5.583235740661621
I0428 07:57:50.385669 140347588105984 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7361736297607422, loss=5.615464210510254
I0428 07:59:06.144296 140347579713280 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.37095487117767334, loss=5.589451313018799
I0428 08:00:21.957740 140347588105984 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.3719194829463959, loss=5.599438190460205
I0428 08:01:39.530727 140347579713280 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5018811225891113, loss=5.572606086730957
I0428 08:03:05.067448 140347588105984 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.2946571111679077, loss=5.571040630340576
I0428 08:04:30.960144 140347579713280 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5298546552658081, loss=5.583393096923828
I0428 08:05:54.854975 140347588105984 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5622021555900574, loss=5.5901031494140625
I0428 08:07:20.275549 140347579713280 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4901403784751892, loss=5.555698394775391
I0428 08:08:43.187950 140347588105984 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4776758849620819, loss=5.55821418762207
I0428 08:09:58.957535 140347579713280 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.28630533814430237, loss=5.559543609619141
I0428 08:11:14.698534 140347588105984 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6152040958404541, loss=5.536830902099609
I0428 08:12:30.473402 140347579713280 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6786396503448486, loss=5.51082181930542
I0428 08:13:46.142706 140347588105984 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4598403573036194, loss=5.498607158660889
I0428 08:15:04.012303 140347579713280 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.835422933101654, loss=5.509571552276611
I0428 08:16:28.805156 140347588105984 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6008877754211426, loss=5.48150634765625
I0428 08:17:52.947951 140347579713280 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5030245780944824, loss=5.500164985656738
I0428 08:19:17.894400 140347588105984 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.35247185826301575, loss=5.500113487243652
I0428 08:19:18.676980 140520195979072 spec.py:298] Evaluating on the training split.
I0428 08:19:46.556403 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 08:20:22.875712 140520195979072 spec.py:326] Evaluating on the test split.
I0428 08:20:41.528329 140520195979072 submission_runner.py:415] Time since start: 7619.53s, 	Step: 9102, 	{'train/ctc_loss': DeviceArray(6.458328, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(6.396484, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.362341, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7264.4573357105255, 'total_duration': 7619.532325267792, 'accumulated_submission_time': 7264.4573357105255, 'accumulated_eval_time': 351.5458610057831, 'accumulated_logging_time': 3.377991199493408}
I0428 08:20:41.550487 140347158025984 logging_writer.py:48] [9102] accumulated_eval_time=351.545861, accumulated_logging_time=3.377991, accumulated_submission_time=7264.457336, global_step=9102, preemption_count=0, score=7264.457336, test/ctc_loss=6.362340927124023, test/num_examples=2472, test/wer=0.899580, total_duration=7619.532325, train/ctc_loss=6.458327770233154, train/wer=0.943324, validation/ctc_loss=6.396483898162842, validation/num_examples=5348, validation/wer=0.895995
I0428 08:20:41.751200 140520195979072 checkpoints.py:356] Saving checkpoint at step: 9102
I0428 08:20:42.770276 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_9102
I0428 08:20:42.791943 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_9102.
I0428 08:21:57.611871 140347149633280 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6468861103057861, loss=5.4568328857421875
I0428 08:23:16.531491 140347158025984 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4106602966785431, loss=5.468555927276611
I0428 08:24:32.149357 140347149633280 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.42495203018188477, loss=5.447872638702393
I0428 08:25:47.748785 140347158025984 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6036986708641052, loss=5.451519966125488
I0428 08:27:03.319800 140347149633280 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4725932478904724, loss=5.431811809539795
I0428 08:28:18.945667 140347158025984 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.44255000352859497, loss=5.421436309814453
I0428 08:29:40.372816 140347149633280 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.40818941593170166, loss=5.402169227600098
I0428 08:31:07.196310 140347158025984 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.906730592250824, loss=5.355013847351074
I0428 08:32:33.847844 140347149633280 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7538011074066162, loss=5.215425968170166
I0428 08:33:57.978013 140347158025984 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6111854314804077, loss=5.076101779937744
I0428 08:35:22.648976 140347149633280 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9579676985740662, loss=4.7965474128723145
I0428 08:36:49.658618 140347158025984 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.1675643920898438, loss=4.444027423858643
I0428 08:38:05.444367 140347149633280 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.1114509105682373, loss=4.175999641418457
I0428 08:39:21.091888 140347158025984 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7599942684173584, loss=3.9027929306030273
I0428 08:40:36.776191 140347149633280 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7804791927337646, loss=3.816606283187866
I0428 08:41:52.461897 140347158025984 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6923678517341614, loss=3.654062271118164
I0428 08:43:08.588594 140347149633280 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7365090847015381, loss=3.545381784439087
I0428 08:44:33.925986 140347158025984 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.0672812461853027, loss=3.5096917152404785
I0428 08:46:00.674086 140347149633280 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8033052086830139, loss=3.385136604309082
I0428 08:47:24.339069 140347158025984 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7500711679458618, loss=3.3680710792541504
I0428 08:48:50.682413 140347149633280 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8465499877929688, loss=3.2564053535461426
I0428 08:50:16.926491 140347158025984 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7244802117347717, loss=3.202733278274536
I0428 08:51:38.313579 140347158025984 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7250706553459167, loss=3.122141122817993
I0428 08:52:53.962864 140347149633280 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8577345013618469, loss=3.1195626258850098
I0428 08:54:09.548672 140347158025984 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8508203625679016, loss=3.0281224250793457
I0428 08:55:25.069742 140347149633280 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9099714756011963, loss=2.9891152381896973
I0428 08:56:40.621403 140347158025984 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8039553165435791, loss=3.0082759857177734
I0428 08:57:57.972300 140347149633280 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7204028964042664, loss=2.9232168197631836
I0428 08:59:22.118770 140347158025984 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7298874258995056, loss=2.875309944152832
I0428 09:00:43.571824 140520195979072 spec.py:298] Evaluating on the training split.
I0428 09:01:21.707088 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 09:02:01.038479 140520195979072 spec.py:326] Evaluating on the test split.
I0428 09:02:21.290424 140520195979072 submission_runner.py:415] Time since start: 10119.29s, 	Step: 12097, 	{'train/ctc_loss': DeviceArray(1.7382253, dtype=float32), 'train/wer': 0.4654672265915446, 'validation/ctc_loss': DeviceArray(2.0513983, dtype=float32), 'validation/wer': 0.5004293336163398, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.7614521, dtype=float32), 'test/wer': 0.4598541628582455, 'test/num_examples': 2472, 'score': 9665.180943965912, 'total_duration': 10119.29407119751, 'accumulated_submission_time': 9665.180943965912, 'accumulated_eval_time': 449.2607924938202, 'accumulated_logging_time': 4.652899265289307}
I0428 09:02:21.312264 140347588105984 logging_writer.py:48] [12097] accumulated_eval_time=449.260792, accumulated_logging_time=4.652899, accumulated_submission_time=9665.180944, global_step=12097, preemption_count=0, score=9665.180944, test/ctc_loss=1.761452078819275, test/num_examples=2472, test/wer=0.459854, total_duration=10119.294071, train/ctc_loss=1.7382253408432007, train/wer=0.465467, validation/ctc_loss=2.051398277282715, validation/num_examples=5348, validation/wer=0.500429
I0428 09:02:21.515747 140520195979072 checkpoints.py:356] Saving checkpoint at step: 12097
I0428 09:02:22.429967 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_12097
I0428 09:02:22.451748 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_12097.
I0428 09:02:25.538681 140347579713280 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8641126751899719, loss=2.8773536682128906
I0428 09:03:41.090338 140347512571648 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.1134113073349, loss=2.794435501098633
I0428 09:04:56.661783 140347579713280 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7669823169708252, loss=2.8193321228027344
I0428 09:06:15.428947 140347260425984 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6945403814315796, loss=2.7948391437530518
I0428 09:07:30.989308 140347252033280 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.769895613193512, loss=2.7015795707702637
I0428 09:08:46.530648 140347260425984 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.688250720500946, loss=2.770792245864868
I0428 09:10:02.070841 140347252033280 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7388388514518738, loss=2.6544837951660156
I0428 09:11:17.629369 140347260425984 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7923222780227661, loss=2.6525888442993164
I0428 09:12:38.770694 140347252033280 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6631323099136353, loss=2.6877620220184326
I0428 09:14:04.473330 140347260425984 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6942993998527527, loss=2.654883861541748
I0428 09:15:29.278128 140347252033280 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9407426118850708, loss=2.606092929840088
I0428 09:16:53.563462 140347260425984 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.764875590801239, loss=2.6038620471954346
I0428 09:18:17.774733 140347252033280 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7614744901657104, loss=2.503129720687866
I0428 09:19:44.289578 140347260425984 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7727359533309937, loss=2.539180278778076
I0428 09:20:59.881724 140347252033280 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7208312749862671, loss=2.5181727409362793
I0428 09:22:15.535212 140347260425984 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6615593433380127, loss=2.488224744796753
I0428 09:23:31.148463 140347252033280 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7070906162261963, loss=2.4851040840148926
I0428 09:24:46.702787 140347260425984 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.067758321762085, loss=2.4668564796447754
I0428 09:26:05.276125 140347252033280 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7475952506065369, loss=2.4200260639190674
I0428 09:27:32.553858 140347260425984 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9433208703994751, loss=2.44582462310791
I0428 09:29:00.563102 140347252033280 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6551612615585327, loss=2.4140496253967285
I0428 09:30:26.193764 140347260425984 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.723298966884613, loss=2.424116611480713
I0428 09:31:51.633514 140347252033280 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7117795348167419, loss=2.379682779312134
I0428 09:33:15.738899 140347260425984 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6526876091957092, loss=2.3776328563690186
I0428 09:34:36.120419 140347588105984 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.620323657989502, loss=2.367875576019287
I0428 09:35:51.803861 140347579713280 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8425485491752625, loss=2.343975305557251
I0428 09:37:07.360476 140347588105984 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6884921789169312, loss=2.336596727371216
I0428 09:38:23.015099 140347579713280 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6779447197914124, loss=2.265691041946411
I0428 09:39:38.557049 140347588105984 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7527114152908325, loss=2.305816173553467
I0428 09:40:56.405937 140347579713280 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6469343304634094, loss=2.3279800415039062
I0428 09:42:18.164276 140347588105984 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6419869065284729, loss=2.304241180419922
I0428 09:42:22.568036 140520195979072 spec.py:298] Evaluating on the training split.
I0428 09:43:00.142177 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 09:43:39.208331 140520195979072 spec.py:326] Evaluating on the test split.
I0428 09:43:59.572643 140520195979072 submission_runner.py:415] Time since start: 12617.58s, 	Step: 15107, 	{'train/ctc_loss': DeviceArray(0.96463966, dtype=float32), 'train/wer': 0.3130585574811368, 'validation/ctc_loss': DeviceArray(1.2988716, dtype=float32), 'validation/wer': 0.36840683460525425, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.00281, dtype=float32), 'test/wer': 0.3110312188978937, 'test/num_examples': 2472, 'score': 12065.24285531044, 'total_duration': 12617.576254844666, 'accumulated_submission_time': 12065.24285531044, 'accumulated_eval_time': 546.2616612911224, 'accumulated_logging_time': 5.82207989692688}
I0428 09:43:59.594380 140347588105984 logging_writer.py:48] [15107] accumulated_eval_time=546.261661, accumulated_logging_time=5.822080, accumulated_submission_time=12065.242855, global_step=15107, preemption_count=0, score=12065.242855, test/ctc_loss=1.002810001373291, test/num_examples=2472, test/wer=0.311031, total_duration=12617.576255, train/ctc_loss=0.9646396636962891, train/wer=0.313059, validation/ctc_loss=1.298871636390686, validation/num_examples=5348, validation/wer=0.368407
I0428 09:43:59.788083 140520195979072 checkpoints.py:356] Saving checkpoint at step: 15107
I0428 09:44:00.732483 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_15107
I0428 09:44:00.754359 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_15107.
I0428 09:45:11.594892 140347579713280 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7510825991630554, loss=2.23109769821167
I0428 09:46:27.061498 140346162738944 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7977819442749023, loss=2.214653968811035
I0428 09:47:42.581067 140347579713280 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6807698607444763, loss=2.1890294551849365
I0428 09:49:02.394155 140347588105984 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.694121778011322, loss=2.1666688919067383
I0428 09:50:18.039047 140347579713280 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.8122618198394775, loss=2.136899948120117
I0428 09:51:33.585675 140347588105984 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0688831806182861, loss=2.153006076812744
I0428 09:52:49.112425 140347579713280 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6976515650749207, loss=2.202793598175049
I0428 09:54:04.715210 140347588105984 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7061519622802734, loss=2.1666438579559326
I0428 09:55:20.384090 140347579713280 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6875162124633789, loss=2.11194109916687
I0428 09:56:44.279368 140347588105984 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7466194033622742, loss=2.0966989994049072
I0428 09:58:08.001964 140347579713280 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7067813873291016, loss=2.1450583934783936
I0428 09:59:33.527071 140347588105984 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.7362667322158813, loss=2.0457651615142822
I0428 10:00:58.543917 140347579713280 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7179427742958069, loss=2.0830109119415283
I0428 10:02:23.806300 140347260425984 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6819500923156738, loss=2.06294584274292
I0428 10:03:39.339562 140347252033280 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6009036302566528, loss=2.035376787185669
I0428 10:04:54.898286 140347260425984 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6786049008369446, loss=2.0573606491088867
I0428 10:06:10.393151 140347252033280 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6202237606048584, loss=2.0525877475738525
I0428 10:07:25.921220 140347260425984 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6655577421188354, loss=1.999583125114441
I0428 10:08:47.074006 140347252033280 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6738077998161316, loss=2.0032713413238525
I0428 10:10:11.591388 140347260425984 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6892057657241821, loss=2.0566153526306152
I0428 10:11:34.444468 140347252033280 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6325650811195374, loss=1.9852579832077026
I0428 10:12:58.282186 140347260425984 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6057531833648682, loss=2.0334293842315674
I0428 10:14:20.599147 140347252033280 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8543705940246582, loss=1.967621088027954
I0428 10:15:43.948813 140347260425984 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7307546734809875, loss=2.0109033584594727
I0428 10:17:03.185571 140347260425984 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.607969343662262, loss=1.9664485454559326
I0428 10:18:18.701458 140347252033280 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6278906464576721, loss=1.9931185245513916
I0428 10:19:34.159174 140347260425984 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6380199790000916, loss=1.9205522537231445
I0428 10:20:49.622776 140347252033280 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.731156587600708, loss=1.9644391536712646
I0428 10:22:06.405534 140347260425984 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6448530554771423, loss=1.9736019372940063
I0428 10:23:31.525602 140347252033280 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6539462804794312, loss=1.9191333055496216
I0428 10:24:01.240922 140520195979072 spec.py:298] Evaluating on the training split.
I0428 10:24:39.302540 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 10:25:19.202150 140520195979072 spec.py:326] Evaluating on the test split.
I0428 10:25:39.589476 140520195979072 submission_runner.py:415] Time since start: 15117.59s, 	Step: 18137, 	{'train/ctc_loss': DeviceArray(0.6511061, dtype=float32), 'train/wer': 0.22193533243354263, 'validation/ctc_loss': DeviceArray(0.95807743, dtype=float32), 'validation/wer': 0.28329265115920077, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.67948425, dtype=float32), 'test/wer': 0.22220868116913453, 'test/num_examples': 2472, 'score': 14465.675067901611, 'total_duration': 15117.593477725983, 'accumulated_submission_time': 14465.675067901611, 'accumulated_eval_time': 644.6068787574768, 'accumulated_logging_time': 7.01200270652771}
I0428 10:25:39.614571 140347045385984 logging_writer.py:48] [18137] accumulated_eval_time=644.606879, accumulated_logging_time=7.012003, accumulated_submission_time=14465.675068, global_step=18137, preemption_count=0, score=14465.675068, test/ctc_loss=0.6794842481613159, test/num_examples=2472, test/wer=0.222209, total_duration=15117.593478, train/ctc_loss=0.6511061191558838, train/wer=0.221935, validation/ctc_loss=0.9580774307250977, validation/num_examples=5348, validation/wer=0.283293
I0428 10:25:39.812034 140520195979072 checkpoints.py:356] Saving checkpoint at step: 18137
I0428 10:25:40.764607 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_18137
I0428 10:25:40.786597 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_18137.
I0428 10:26:28.992166 140347036993280 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6639441251754761, loss=1.8996011018753052
I0428 10:27:44.388683 140346953066240 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8603233098983765, loss=1.960372805595398
I0428 10:28:59.773579 140347036993280 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6972365975379944, loss=1.9726728200912476
I0428 10:30:18.777042 140346953066240 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6487677097320557, loss=1.880937933921814
I0428 10:31:41.472534 140347588105984 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6362189054489136, loss=1.9030472040176392
I0428 10:32:57.047471 140347579713280 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.714706540107727, loss=1.8527002334594727
I0428 10:34:12.550605 140347588105984 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6067079901695251, loss=1.8115952014923096
I0428 10:35:27.993308 140347579713280 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6415671110153198, loss=1.866047978401184
I0428 10:36:43.474945 140347588105984 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7152162790298462, loss=1.8602397441864014
I0428 10:38:06.758545 140347579713280 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.665667712688446, loss=1.844421148300171
I0428 10:39:33.407117 140347588105984 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6485174894332886, loss=1.8597650527954102
I0428 10:41:00.751530 140347579713280 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6464464068412781, loss=1.9068857431411743
I0428 10:42:26.187492 140347588105984 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6707382798194885, loss=1.900375247001648
I0428 10:43:50.303942 140347579713280 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.622835636138916, loss=1.8703243732452393
I0428 10:45:14.003304 140347588105984 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.62975013256073, loss=1.8111106157302856
I0428 10:46:29.534635 140347579713280 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8399492502212524, loss=1.7620004415512085
I0428 10:47:45.041867 140347588105984 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.61581951379776, loss=1.7656688690185547
I0428 10:49:00.610117 140347579713280 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6441481709480286, loss=1.793698787689209
I0428 10:50:14.968987 140520195979072 spec.py:298] Evaluating on the training split.
I0428 10:50:52.969739 140520195979072 spec.py:310] Evaluating on the validation split.
I0428 10:51:32.477195 140520195979072 spec.py:326] Evaluating on the test split.
I0428 10:51:52.397551 140520195979072 submission_runner.py:415] Time since start: 16690.40s, 	Step: 20000, 	{'train/ctc_loss': DeviceArray(0.5348848, dtype=float32), 'train/wer': 0.18625699274668708, 'validation/ctc_loss': DeviceArray(0.84864396, dtype=float32), 'validation/wer': 0.2558249476598906, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5799034, dtype=float32), 'test/wer': 0.1938537160034936, 'test/num_examples': 2472, 'score': 15939.820055007935, 'total_duration': 16690.402992725372, 'accumulated_submission_time': 15939.820055007935, 'accumulated_eval_time': 742.0335381031036, 'accumulated_logging_time': 8.217254638671875}
I0428 10:51:52.420888 140347081225984 logging_writer.py:48] [20000] accumulated_eval_time=742.033538, accumulated_logging_time=8.217255, accumulated_submission_time=15939.820055, global_step=20000, preemption_count=0, score=15939.820055, test/ctc_loss=0.5799034237861633, test/num_examples=2472, test/wer=0.193854, total_duration=16690.402993, train/ctc_loss=0.5348848104476929, train/wer=0.186257, validation/ctc_loss=0.848643958568573, validation/num_examples=5348, validation/wer=0.255825
I0428 10:51:52.616044 140520195979072 checkpoints.py:356] Saving checkpoint at step: 20000
I0428 10:51:53.576402 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000
I0428 10:51:53.598112 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0428 10:51:53.615760 140347072833280 logging_writer.py:48] [20000] global_step=20000, preemption_count=0, score=15939.820055
I0428 10:51:53.790595 140520195979072 checkpoints.py:356] Saving checkpoint at step: 20000
I0428 10:51:55.105988 140520195979072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000
I0428 10:51:55.127602 140520195979072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_v3/timing_momentum/librispeech_conformer_jax/trial_1/checkpoint_20000.
I0428 10:51:56.386526 140520195979072 submission_runner.py:578] Tuning trial 1/1
I0428 10:51:56.386771 140520195979072 submission_runner.py:579] Hyperparameters: Hyperparameters(learning_rate=0.02294064372630481, one_minus_beta1=0.01594265061, warmup_steps=4999, warmup_factor=0.05, decay_steps_factor=0.9249881733126807, end_factor=0.01, weight_decay=2.388370133081662e-07, label_smoothing=0.0)
I0428 10:51:56.392615 140520195979072 submission_runner.py:580] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(32.140953, dtype=float32), 'train/wer': 0.9480717572829686, 'validation/ctc_loss': DeviceArray(31.378462, dtype=float32), 'validation/wer': 0.961707300601067, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.495008, dtype=float32), 'test/wer': 0.9620376576686369, 'test/num_examples': 2472, 'score': 63.40580916404724, 'total_duration': 170.01994276046753, 'accumulated_submission_time': 63.40580916404724, 'accumulated_eval_time': 106.61396765708923, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (3044, {'train/ctc_loss': DeviceArray(6.0822186, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(6.1345134, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.1199045, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2464.032116651535, 'total_duration': 2652.6490638256073, 'accumulated_submission_time': 2464.032116651535, 'accumulated_eval_time': 187.72709226608276, 'accumulated_logging_time': 0.8401167392730713, 'global_step': 3044, 'preemption_count': 0}), (6090, {'train/ctc_loss': DeviceArray(9.919225, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(9.65895, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(9.63769, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4864.253120183945, 'total_duration': 5135.199232816696, 'accumulated_submission_time': 4864.253120183945, 'accumulated_eval_time': 268.69786834716797, 'accumulated_logging_time': 2.1474766731262207, 'global_step': 6090, 'preemption_count': 0}), (9102, {'train/ctc_loss': DeviceArray(6.458328, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(6.396484, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(6.362341, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7264.4573357105255, 'total_duration': 7619.532325267792, 'accumulated_submission_time': 7264.4573357105255, 'accumulated_eval_time': 351.5458610057831, 'accumulated_logging_time': 3.377991199493408, 'global_step': 9102, 'preemption_count': 0}), (12097, {'train/ctc_loss': DeviceArray(1.7382253, dtype=float32), 'train/wer': 0.4654672265915446, 'validation/ctc_loss': DeviceArray(2.0513983, dtype=float32), 'validation/wer': 0.5004293336163398, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.7614521, dtype=float32), 'test/wer': 0.4598541628582455, 'test/num_examples': 2472, 'score': 9665.180943965912, 'total_duration': 10119.29407119751, 'accumulated_submission_time': 9665.180943965912, 'accumulated_eval_time': 449.2607924938202, 'accumulated_logging_time': 4.652899265289307, 'global_step': 12097, 'preemption_count': 0}), (15107, {'train/ctc_loss': DeviceArray(0.96463966, dtype=float32), 'train/wer': 0.3130585574811368, 'validation/ctc_loss': DeviceArray(1.2988716, dtype=float32), 'validation/wer': 0.36840683460525425, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(1.00281, dtype=float32), 'test/wer': 0.3110312188978937, 'test/num_examples': 2472, 'score': 12065.24285531044, 'total_duration': 12617.576254844666, 'accumulated_submission_time': 12065.24285531044, 'accumulated_eval_time': 546.2616612911224, 'accumulated_logging_time': 5.82207989692688, 'global_step': 15107, 'preemption_count': 0}), (18137, {'train/ctc_loss': DeviceArray(0.6511061, dtype=float32), 'train/wer': 0.22193533243354263, 'validation/ctc_loss': DeviceArray(0.95807743, dtype=float32), 'validation/wer': 0.28329265115920077, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.67948425, dtype=float32), 'test/wer': 0.22220868116913453, 'test/num_examples': 2472, 'score': 14465.675067901611, 'total_duration': 15117.593477725983, 'accumulated_submission_time': 14465.675067901611, 'accumulated_eval_time': 644.6068787574768, 'accumulated_logging_time': 7.01200270652771, 'global_step': 18137, 'preemption_count': 0}), (20000, {'train/ctc_loss': DeviceArray(0.5348848, dtype=float32), 'train/wer': 0.18625699274668708, 'validation/ctc_loss': DeviceArray(0.84864396, dtype=float32), 'validation/wer': 0.2558249476598906, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(0.5799034, dtype=float32), 'test/wer': 0.1938537160034936, 'test/num_examples': 2472, 'score': 15939.820055007935, 'total_duration': 16690.402992725372, 'accumulated_submission_time': 15939.820055007935, 'accumulated_eval_time': 742.0335381031036, 'accumulated_logging_time': 8.217254638671875, 'global_step': 20000, 'preemption_count': 0})], 'global_step': 20000}
I0428 10:51:56.392808 140520195979072 submission_runner.py:581] Timing: 15939.820055007935
I0428 10:51:56.392863 140520195979072 submission_runner.py:582] ====================
I0428 10:51:56.393461 140520195979072 submission_runner.py:645] Final librispeech_conformer score: 15939.820055007935
