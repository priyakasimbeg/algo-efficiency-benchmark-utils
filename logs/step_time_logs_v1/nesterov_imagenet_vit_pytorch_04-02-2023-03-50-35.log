WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 03:50:56.447933 140417562826560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 03:50:56.447971 140230802581312 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 03:50:56.448011 140080308438848 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 03:50:56.448709 139849151493952 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 03:50:56.448837 140462101018432 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 03:50:56.449410 140052673640256 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 03:50:56.449647 139898468734784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 03:50:56.459473 140462101018432 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.459475 140318046459712 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 03:50:56.460017 140318046459712 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.460073 140052673640256 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.460401 139898468734784 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.469074 140417562826560 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.469107 140230802581312 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.469132 140080308438848 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:56.469589 139849151493952 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 03:50:58.490847 140318046459712 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/imagenet_vit_pytorch.
W0402 03:50:58.599236 139898468734784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.599237 140318046459712 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.600175 140230802581312 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.601581 140052673640256 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.602238 140080308438848 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.602476 140417562826560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.602716 140462101018432 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 03:50:58.602764 139849151493952 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 03:50:58.605628 140318046459712 submission_runner.py:511] Using RNG seed 2106639606
I0402 03:50:58.606858 140318046459712 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 03:50:58.606976 140318046459712 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1.
I0402 03:50:58.607187 140318046459712 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/hparams.json.
I0402 03:50:58.608150 140318046459712 submission_runner.py:230] Starting train once: RAM USED (GB) 5.797318656
I0402 03:50:58.608244 140318046459712 submission_runner.py:231] Initializing dataset.
I0402 03:51:02.860590 140318046459712 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 7.844491264
I0402 03:51:02.860773 140318046459712 submission_runner.py:240] Initializing model.
I0402 03:51:07.236092 140318046459712 submission_runner.py:251] After Initializing model: RAM USED (GB) 18.064621568
I0402 03:51:07.236266 140318046459712 submission_runner.py:252] Initializing optimizer.
I0402 03:51:07.779434 140318046459712 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 18.068557824
I0402 03:51:07.779612 140318046459712 submission_runner.py:261] Initializing metrics bundle.
I0402 03:51:07.779700 140318046459712 submission_runner.py:276] Initializing checkpoint and logger.
I0402 03:51:08.462454 140318046459712 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/meta_data_0.json.
I0402 03:51:08.463396 140318046459712 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/flags_0.json.
I0402 03:51:08.504866 140318046459712 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 18.119901184
I0402 03:51:08.505988 140318046459712 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 18.119897088
I0402 03:51:08.506112 140318046459712 submission_runner.py:313] Starting training loop.
I0402 03:51:11.167299 140318046459712 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 23.677976576
I0402 03:51:15.110176 140289058301696 logging_writer.py:48] [0] global_step=0, grad_norm=0.301487, loss=6.907755
I0402 03:51:15.136152 140318046459712 submission.py:139] 0) loss = 6.908, grad_norm = 0.301
I0402 03:51:15.136999 140318046459712 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 31.881678848
I0402 03:51:15.137627 140318046459712 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 31.882227712
I0402 03:51:15.137758 140318046459712 spec.py:298] Evaluating on the training split.
I0402 03:52:04.917497 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 03:52:49.507752 140318046459712 spec.py:326] Evaluating on the test split.
I0402 03:52:49.524080 140318046459712 dataset_info.py:566] Load dataset info from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0402 03:52:49.530481 140318046459712 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0)
I0402 03:52:49.612503 140318046459712 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/pytorch/imagenet_v2/matched-frequency/3.0.0
I0402 03:53:02.799506 140318046459712 submission_runner.py:382] Time since start: 6.63s, 	Step: 1, 	{'train/accuracy': 0.00087890625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000}
I0402 03:53:02.800848 140318046459712 submission_runner.py:396] After eval at step 1: RAM USED (GB) 92.636643328
I0402 03:53:02.811360 140284134422272 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.630068, test/accuracy=0.001000, test/loss=6.907755, test/num_examples=10000, total_duration=6.632102, train/accuracy=0.000879, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0402 03:53:03.094113 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_1.
I0402 03:53:03.094746 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 92.638183424
I0402 03:53:03.100143 140318046459712 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 92.639666176
I0402 03:53:03.107087 140318046459712 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107393 140052673640256 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107404 140230802581312 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107420 139898468734784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107424 140417562826560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107420 139849151493952 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107423 140080308438848 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.107831 140462101018432 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 03:53:03.669916 140284126029568 logging_writer.py:48] [1] global_step=1, grad_norm=0.308062, loss=6.907755
I0402 03:53:03.673131 140318046459712 submission.py:139] 1) loss = 6.908, grad_norm = 0.308
I0402 03:53:03.674071 140318046459712 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 92.670537728
I0402 03:53:04.080387 140284134422272 logging_writer.py:48] [2] global_step=2, grad_norm=0.305657, loss=6.907754
I0402 03:53:04.084007 140318046459712 submission.py:139] 2) loss = 6.908, grad_norm = 0.306
I0402 03:53:04.477408 140284126029568 logging_writer.py:48] [3] global_step=3, grad_norm=0.302959, loss=6.907754
I0402 03:53:04.481892 140318046459712 submission.py:139] 3) loss = 6.908, grad_norm = 0.303
I0402 03:53:04.870094 140284134422272 logging_writer.py:48] [4] global_step=4, grad_norm=0.299770, loss=6.907754
I0402 03:53:04.873753 140318046459712 submission.py:139] 4) loss = 6.908, grad_norm = 0.300
I0402 03:53:05.266201 140284126029568 logging_writer.py:48] [5] global_step=5, grad_norm=0.301218, loss=6.907755
I0402 03:53:05.272062 140318046459712 submission.py:139] 5) loss = 6.908, grad_norm = 0.301
I0402 03:53:05.665129 140284134422272 logging_writer.py:48] [6] global_step=6, grad_norm=0.305107, loss=6.907747
I0402 03:53:05.669723 140318046459712 submission.py:139] 6) loss = 6.908, grad_norm = 0.305
I0402 03:53:06.063536 140284126029568 logging_writer.py:48] [7] global_step=7, grad_norm=0.311332, loss=6.907744
I0402 03:53:06.068864 140318046459712 submission.py:139] 7) loss = 6.908, grad_norm = 0.311
I0402 03:53:06.463133 140284134422272 logging_writer.py:48] [8] global_step=8, grad_norm=0.309172, loss=6.907743
I0402 03:53:06.470032 140318046459712 submission.py:139] 8) loss = 6.908, grad_norm = 0.309
I0402 03:53:06.867825 140284126029568 logging_writer.py:48] [9] global_step=9, grad_norm=0.311853, loss=6.907745
I0402 03:53:06.871691 140318046459712 submission.py:139] 9) loss = 6.908, grad_norm = 0.312
I0402 03:53:07.266748 140284134422272 logging_writer.py:48] [10] global_step=10, grad_norm=0.311446, loss=6.907734
I0402 03:53:07.272003 140318046459712 submission.py:139] 10) loss = 6.908, grad_norm = 0.311
I0402 03:53:07.676595 140284126029568 logging_writer.py:48] [11] global_step=11, grad_norm=0.303246, loss=6.907729
I0402 03:53:07.681864 140318046459712 submission.py:139] 11) loss = 6.908, grad_norm = 0.303
I0402 03:53:08.083300 140284134422272 logging_writer.py:48] [12] global_step=12, grad_norm=0.302713, loss=6.907740
I0402 03:53:08.089626 140318046459712 submission.py:139] 12) loss = 6.908, grad_norm = 0.303
I0402 03:53:08.488702 140284126029568 logging_writer.py:48] [13] global_step=13, grad_norm=0.303228, loss=6.907716
I0402 03:53:08.492784 140318046459712 submission.py:139] 13) loss = 6.908, grad_norm = 0.303
I0402 03:53:08.888099 140284134422272 logging_writer.py:48] [14] global_step=14, grad_norm=0.309701, loss=6.907746
I0402 03:53:08.899814 140318046459712 submission.py:139] 14) loss = 6.908, grad_norm = 0.310
I0402 03:53:09.298249 140284126029568 logging_writer.py:48] [15] global_step=15, grad_norm=0.303087, loss=6.907713
I0402 03:53:09.303750 140318046459712 submission.py:139] 15) loss = 6.908, grad_norm = 0.303
I0402 03:53:09.696559 140284134422272 logging_writer.py:48] [16] global_step=16, grad_norm=0.303476, loss=6.907702
I0402 03:53:09.700942 140318046459712 submission.py:139] 16) loss = 6.908, grad_norm = 0.303
I0402 03:53:10.096076 140284126029568 logging_writer.py:48] [17] global_step=17, grad_norm=0.302678, loss=6.907670
I0402 03:53:10.099966 140318046459712 submission.py:139] 17) loss = 6.908, grad_norm = 0.303
I0402 03:53:10.494196 140284134422272 logging_writer.py:48] [18] global_step=18, grad_norm=0.312396, loss=6.907685
I0402 03:53:10.502665 140318046459712 submission.py:139] 18) loss = 6.908, grad_norm = 0.312
I0402 03:53:10.895006 140284126029568 logging_writer.py:48] [19] global_step=19, grad_norm=0.307257, loss=6.907697
I0402 03:53:10.898753 140318046459712 submission.py:139] 19) loss = 6.908, grad_norm = 0.307
I0402 03:53:11.297202 140284134422272 logging_writer.py:48] [20] global_step=20, grad_norm=0.312453, loss=6.907656
I0402 03:53:11.302793 140318046459712 submission.py:139] 20) loss = 6.908, grad_norm = 0.312
I0402 03:53:11.712903 140284126029568 logging_writer.py:48] [21] global_step=21, grad_norm=0.306291, loss=6.907617
I0402 03:53:11.718028 140318046459712 submission.py:139] 21) loss = 6.908, grad_norm = 0.306
I0402 03:53:12.114808 140284134422272 logging_writer.py:48] [22] global_step=22, grad_norm=0.304383, loss=6.907636
I0402 03:53:12.118605 140318046459712 submission.py:139] 22) loss = 6.908, grad_norm = 0.304
I0402 03:53:12.513500 140284126029568 logging_writer.py:48] [23] global_step=23, grad_norm=0.302892, loss=6.907616
I0402 03:53:12.517213 140318046459712 submission.py:139] 23) loss = 6.908, grad_norm = 0.303
I0402 03:53:12.910984 140284134422272 logging_writer.py:48] [24] global_step=24, grad_norm=0.309296, loss=6.907547
I0402 03:53:12.914665 140318046459712 submission.py:139] 24) loss = 6.908, grad_norm = 0.309
I0402 03:53:13.306305 140284126029568 logging_writer.py:48] [25] global_step=25, grad_norm=0.304570, loss=6.907541
I0402 03:53:13.314789 140318046459712 submission.py:139] 25) loss = 6.908, grad_norm = 0.305
I0402 03:53:13.720600 140284134422272 logging_writer.py:48] [26] global_step=26, grad_norm=0.309491, loss=6.907480
I0402 03:53:13.724588 140318046459712 submission.py:139] 26) loss = 6.907, grad_norm = 0.309
I0402 03:53:14.130877 140284126029568 logging_writer.py:48] [27] global_step=27, grad_norm=0.303191, loss=6.907561
I0402 03:53:14.136301 140318046459712 submission.py:139] 27) loss = 6.908, grad_norm = 0.303
I0402 03:53:14.531406 140284134422272 logging_writer.py:48] [28] global_step=28, grad_norm=0.308625, loss=6.907462
I0402 03:53:14.535145 140318046459712 submission.py:139] 28) loss = 6.907, grad_norm = 0.309
I0402 03:53:14.931522 140284126029568 logging_writer.py:48] [29] global_step=29, grad_norm=0.315151, loss=6.907579
I0402 03:53:14.935230 140318046459712 submission.py:139] 29) loss = 6.908, grad_norm = 0.315
I0402 03:53:15.327022 140284134422272 logging_writer.py:48] [30] global_step=30, grad_norm=0.313021, loss=6.907355
I0402 03:53:15.330948 140318046459712 submission.py:139] 30) loss = 6.907, grad_norm = 0.313
I0402 03:53:15.731557 140284126029568 logging_writer.py:48] [31] global_step=31, grad_norm=0.299729, loss=6.907510
I0402 03:53:15.736117 140318046459712 submission.py:139] 31) loss = 6.908, grad_norm = 0.300
I0402 03:53:16.137832 140284134422272 logging_writer.py:48] [32] global_step=32, grad_norm=0.305059, loss=6.907481
I0402 03:53:16.142901 140318046459712 submission.py:139] 32) loss = 6.907, grad_norm = 0.305
I0402 03:53:16.545154 140284126029568 logging_writer.py:48] [33] global_step=33, grad_norm=0.316638, loss=6.907209
I0402 03:53:16.549724 140318046459712 submission.py:139] 33) loss = 6.907, grad_norm = 0.317
I0402 03:53:16.948141 140284134422272 logging_writer.py:48] [34] global_step=34, grad_norm=0.303608, loss=6.907381
I0402 03:53:16.952909 140318046459712 submission.py:139] 34) loss = 6.907, grad_norm = 0.304
I0402 03:53:17.361705 140284126029568 logging_writer.py:48] [35] global_step=35, grad_norm=0.300572, loss=6.907400
I0402 03:53:17.366067 140318046459712 submission.py:139] 35) loss = 6.907, grad_norm = 0.301
I0402 03:53:17.760251 140284134422272 logging_writer.py:48] [36] global_step=36, grad_norm=0.309615, loss=6.907420
I0402 03:53:17.765290 140318046459712 submission.py:139] 36) loss = 6.907, grad_norm = 0.310
I0402 03:53:18.156785 140284126029568 logging_writer.py:48] [37] global_step=37, grad_norm=0.301665, loss=6.907445
I0402 03:53:18.160858 140318046459712 submission.py:139] 37) loss = 6.907, grad_norm = 0.302
I0402 03:53:18.569012 140284134422272 logging_writer.py:48] [38] global_step=38, grad_norm=0.302060, loss=6.907204
I0402 03:53:18.573217 140318046459712 submission.py:139] 38) loss = 6.907, grad_norm = 0.302
I0402 03:53:18.976303 140284126029568 logging_writer.py:48] [39] global_step=39, grad_norm=0.302849, loss=6.907467
I0402 03:53:18.986706 140318046459712 submission.py:139] 39) loss = 6.907, grad_norm = 0.303
I0402 03:53:19.390113 140284134422272 logging_writer.py:48] [40] global_step=40, grad_norm=0.314529, loss=6.907090
I0402 03:53:19.398200 140318046459712 submission.py:139] 40) loss = 6.907, grad_norm = 0.315
I0402 03:53:19.815344 140284126029568 logging_writer.py:48] [41] global_step=41, grad_norm=0.302905, loss=6.907251
I0402 03:53:19.819736 140318046459712 submission.py:139] 41) loss = 6.907, grad_norm = 0.303
I0402 03:53:20.223652 140284134422272 logging_writer.py:48] [42] global_step=42, grad_norm=0.299685, loss=6.907461
I0402 03:53:20.228132 140318046459712 submission.py:139] 42) loss = 6.907, grad_norm = 0.300
I0402 03:53:20.630386 140284126029568 logging_writer.py:48] [43] global_step=43, grad_norm=0.307443, loss=6.906984
I0402 03:53:20.634360 140318046459712 submission.py:139] 43) loss = 6.907, grad_norm = 0.307
I0402 03:53:21.041431 140284134422272 logging_writer.py:48] [44] global_step=44, grad_norm=0.310567, loss=6.906820
I0402 03:53:21.050746 140318046459712 submission.py:139] 44) loss = 6.907, grad_norm = 0.311
I0402 03:53:21.461925 140284126029568 logging_writer.py:48] [45] global_step=45, grad_norm=0.291609, loss=6.907070
I0402 03:53:21.466285 140318046459712 submission.py:139] 45) loss = 6.907, grad_norm = 0.292
I0402 03:53:21.869277 140284134422272 logging_writer.py:48] [46] global_step=46, grad_norm=0.312127, loss=6.906733
I0402 03:53:21.873306 140318046459712 submission.py:139] 46) loss = 6.907, grad_norm = 0.312
I0402 03:53:22.269408 140284126029568 logging_writer.py:48] [47] global_step=47, grad_norm=0.304921, loss=6.906929
I0402 03:53:22.273302 140318046459712 submission.py:139] 47) loss = 6.907, grad_norm = 0.305
I0402 03:53:22.663465 140284134422272 logging_writer.py:48] [48] global_step=48, grad_norm=0.300809, loss=6.906868
I0402 03:53:22.667143 140318046459712 submission.py:139] 48) loss = 6.907, grad_norm = 0.301
I0402 03:53:23.060852 140284126029568 logging_writer.py:48] [49] global_step=49, grad_norm=0.306887, loss=6.907050
I0402 03:53:23.064926 140318046459712 submission.py:139] 49) loss = 6.907, grad_norm = 0.307
I0402 03:53:23.459774 140284134422272 logging_writer.py:48] [50] global_step=50, grad_norm=0.300741, loss=6.906750
I0402 03:53:23.465322 140318046459712 submission.py:139] 50) loss = 6.907, grad_norm = 0.301
I0402 03:53:23.859860 140284126029568 logging_writer.py:48] [51] global_step=51, grad_norm=0.308800, loss=6.907043
I0402 03:53:23.864237 140318046459712 submission.py:139] 51) loss = 6.907, grad_norm = 0.309
I0402 03:53:24.274237 140284134422272 logging_writer.py:48] [52] global_step=52, grad_norm=0.299983, loss=6.907113
I0402 03:53:24.278324 140318046459712 submission.py:139] 52) loss = 6.907, grad_norm = 0.300
I0402 03:53:24.673802 140284126029568 logging_writer.py:48] [53] global_step=53, grad_norm=0.297994, loss=6.906521
I0402 03:53:24.678673 140318046459712 submission.py:139] 53) loss = 6.907, grad_norm = 0.298
I0402 03:53:25.075660 140284134422272 logging_writer.py:48] [54] global_step=54, grad_norm=0.304506, loss=6.906605
I0402 03:53:25.079192 140318046459712 submission.py:139] 54) loss = 6.907, grad_norm = 0.305
I0402 03:53:25.479360 140284126029568 logging_writer.py:48] [55] global_step=55, grad_norm=0.299810, loss=6.907100
I0402 03:53:25.484568 140318046459712 submission.py:139] 55) loss = 6.907, grad_norm = 0.300
I0402 03:53:25.891512 140284134422272 logging_writer.py:48] [56] global_step=56, grad_norm=0.306558, loss=6.906634
I0402 03:53:25.895226 140318046459712 submission.py:139] 56) loss = 6.907, grad_norm = 0.307
I0402 03:53:26.288306 140284126029568 logging_writer.py:48] [57] global_step=57, grad_norm=0.305153, loss=6.907103
I0402 03:53:26.292593 140318046459712 submission.py:139] 57) loss = 6.907, grad_norm = 0.305
I0402 03:53:26.686610 140284134422272 logging_writer.py:48] [58] global_step=58, grad_norm=0.314831, loss=6.906384
I0402 03:53:26.690256 140318046459712 submission.py:139] 58) loss = 6.906, grad_norm = 0.315
I0402 03:53:27.093137 140284126029568 logging_writer.py:48] [59] global_step=59, grad_norm=0.306563, loss=6.906956
I0402 03:53:27.097327 140318046459712 submission.py:139] 59) loss = 6.907, grad_norm = 0.307
I0402 03:53:27.504742 140284134422272 logging_writer.py:48] [60] global_step=60, grad_norm=0.307914, loss=6.905831
I0402 03:53:27.508693 140318046459712 submission.py:139] 60) loss = 6.906, grad_norm = 0.308
I0402 03:53:27.903479 140284126029568 logging_writer.py:48] [61] global_step=61, grad_norm=0.307890, loss=6.905855
I0402 03:53:27.907888 140318046459712 submission.py:139] 61) loss = 6.906, grad_norm = 0.308
I0402 03:53:28.308239 140284134422272 logging_writer.py:48] [62] global_step=62, grad_norm=0.299496, loss=6.906605
I0402 03:53:28.312625 140318046459712 submission.py:139] 62) loss = 6.907, grad_norm = 0.299
I0402 03:53:28.708060 140284126029568 logging_writer.py:48] [63] global_step=63, grad_norm=0.310205, loss=6.906015
I0402 03:53:28.712081 140318046459712 submission.py:139] 63) loss = 6.906, grad_norm = 0.310
I0402 03:53:29.104379 140284134422272 logging_writer.py:48] [64] global_step=64, grad_norm=0.312286, loss=6.906154
I0402 03:53:29.110723 140318046459712 submission.py:139] 64) loss = 6.906, grad_norm = 0.312
I0402 03:53:29.507735 140284126029568 logging_writer.py:48] [65] global_step=65, grad_norm=0.303648, loss=6.906111
I0402 03:53:29.511693 140318046459712 submission.py:139] 65) loss = 6.906, grad_norm = 0.304
I0402 03:53:29.904875 140284134422272 logging_writer.py:48] [66] global_step=66, grad_norm=0.299896, loss=6.906601
I0402 03:53:29.909690 140318046459712 submission.py:139] 66) loss = 6.907, grad_norm = 0.300
I0402 03:53:30.312764 140284126029568 logging_writer.py:48] [67] global_step=67, grad_norm=0.311387, loss=6.907108
I0402 03:53:30.319119 140318046459712 submission.py:139] 67) loss = 6.907, grad_norm = 0.311
I0402 03:53:30.717859 140284134422272 logging_writer.py:48] [68] global_step=68, grad_norm=0.307428, loss=6.906078
I0402 03:53:30.727092 140318046459712 submission.py:139] 68) loss = 6.906, grad_norm = 0.307
I0402 03:53:31.129801 140284126029568 logging_writer.py:48] [69] global_step=69, grad_norm=0.300954, loss=6.905997
I0402 03:53:31.137426 140318046459712 submission.py:139] 69) loss = 6.906, grad_norm = 0.301
I0402 03:53:31.548653 140284134422272 logging_writer.py:48] [70] global_step=70, grad_norm=0.305961, loss=6.905562
I0402 03:53:31.553819 140318046459712 submission.py:139] 70) loss = 6.906, grad_norm = 0.306
I0402 03:53:31.949814 140284126029568 logging_writer.py:48] [71] global_step=71, grad_norm=0.306167, loss=6.905766
I0402 03:53:31.953389 140318046459712 submission.py:139] 71) loss = 6.906, grad_norm = 0.306
I0402 03:53:32.350320 140284134422272 logging_writer.py:48] [72] global_step=72, grad_norm=0.315372, loss=6.904934
I0402 03:53:32.354149 140318046459712 submission.py:139] 72) loss = 6.905, grad_norm = 0.315
I0402 03:53:32.748533 140284126029568 logging_writer.py:48] [73] global_step=73, grad_norm=0.304506, loss=6.905315
I0402 03:53:32.752279 140318046459712 submission.py:139] 73) loss = 6.905, grad_norm = 0.305
I0402 03:53:33.144528 140284134422272 logging_writer.py:48] [74] global_step=74, grad_norm=0.301541, loss=6.905396
I0402 03:53:33.148604 140318046459712 submission.py:139] 74) loss = 6.905, grad_norm = 0.302
I0402 03:53:33.540078 140284126029568 logging_writer.py:48] [75] global_step=75, grad_norm=0.300067, loss=6.905301
I0402 03:53:33.544044 140318046459712 submission.py:139] 75) loss = 6.905, grad_norm = 0.300
I0402 03:53:33.936884 140284134422272 logging_writer.py:48] [76] global_step=76, grad_norm=0.303555, loss=6.905796
I0402 03:53:33.941183 140318046459712 submission.py:139] 76) loss = 6.906, grad_norm = 0.304
I0402 03:53:34.339169 140284126029568 logging_writer.py:48] [77] global_step=77, grad_norm=0.305613, loss=6.905171
I0402 03:53:34.343034 140318046459712 submission.py:139] 77) loss = 6.905, grad_norm = 0.306
I0402 03:53:34.735794 140284134422272 logging_writer.py:48] [78] global_step=78, grad_norm=0.308105, loss=6.905236
I0402 03:53:34.739857 140318046459712 submission.py:139] 78) loss = 6.905, grad_norm = 0.308
I0402 03:53:35.139814 140284126029568 logging_writer.py:48] [79] global_step=79, grad_norm=0.313804, loss=6.905912
I0402 03:53:35.143598 140318046459712 submission.py:139] 79) loss = 6.906, grad_norm = 0.314
I0402 03:53:35.536952 140284134422272 logging_writer.py:48] [80] global_step=80, grad_norm=0.302776, loss=6.904822
I0402 03:53:35.541626 140318046459712 submission.py:139] 80) loss = 6.905, grad_norm = 0.303
I0402 03:53:35.952556 140284126029568 logging_writer.py:48] [81] global_step=81, grad_norm=0.299699, loss=6.905675
I0402 03:53:35.956581 140318046459712 submission.py:139] 81) loss = 6.906, grad_norm = 0.300
I0402 03:53:36.350820 140284134422272 logging_writer.py:48] [82] global_step=82, grad_norm=0.308728, loss=6.904891
I0402 03:53:36.361198 140318046459712 submission.py:139] 82) loss = 6.905, grad_norm = 0.309
I0402 03:53:36.752608 140284126029568 logging_writer.py:48] [83] global_step=83, grad_norm=0.307917, loss=6.905340
I0402 03:53:36.757416 140318046459712 submission.py:139] 83) loss = 6.905, grad_norm = 0.308
I0402 03:53:37.149616 140284134422272 logging_writer.py:48] [84] global_step=84, grad_norm=0.304914, loss=6.905408
I0402 03:53:37.153331 140318046459712 submission.py:139] 84) loss = 6.905, grad_norm = 0.305
I0402 03:53:37.554261 140284126029568 logging_writer.py:48] [85] global_step=85, grad_norm=0.304172, loss=6.904941
I0402 03:53:37.558545 140318046459712 submission.py:139] 85) loss = 6.905, grad_norm = 0.304
I0402 03:53:37.952280 140284134422272 logging_writer.py:48] [86] global_step=86, grad_norm=0.310516, loss=6.905424
I0402 03:53:37.956194 140318046459712 submission.py:139] 86) loss = 6.905, grad_norm = 0.311
I0402 03:53:38.352899 140284126029568 logging_writer.py:48] [87] global_step=87, grad_norm=0.298203, loss=6.904747
I0402 03:53:38.356742 140318046459712 submission.py:139] 87) loss = 6.905, grad_norm = 0.298
I0402 03:53:38.760808 140284134422272 logging_writer.py:48] [88] global_step=88, grad_norm=0.310147, loss=6.903197
I0402 03:53:38.766740 140318046459712 submission.py:139] 88) loss = 6.903, grad_norm = 0.310
I0402 03:53:39.161388 140284126029568 logging_writer.py:48] [89] global_step=89, grad_norm=0.313387, loss=6.903482
I0402 03:53:39.165324 140318046459712 submission.py:139] 89) loss = 6.903, grad_norm = 0.313
I0402 03:53:39.559182 140284134422272 logging_writer.py:48] [90] global_step=90, grad_norm=0.306460, loss=6.905615
I0402 03:53:39.563224 140318046459712 submission.py:139] 90) loss = 6.906, grad_norm = 0.306
I0402 03:53:39.957549 140284126029568 logging_writer.py:48] [91] global_step=91, grad_norm=0.300732, loss=6.903465
I0402 03:53:39.961322 140318046459712 submission.py:139] 91) loss = 6.903, grad_norm = 0.301
I0402 03:53:40.358537 140284134422272 logging_writer.py:48] [92] global_step=92, grad_norm=0.309771, loss=6.903239
I0402 03:53:40.362924 140318046459712 submission.py:139] 92) loss = 6.903, grad_norm = 0.310
I0402 03:53:40.755112 140284126029568 logging_writer.py:48] [93] global_step=93, grad_norm=0.307618, loss=6.904782
I0402 03:53:40.759027 140318046459712 submission.py:139] 93) loss = 6.905, grad_norm = 0.308
I0402 03:53:41.163949 140284134422272 logging_writer.py:48] [94] global_step=94, grad_norm=0.304642, loss=6.904814
I0402 03:53:41.167734 140318046459712 submission.py:139] 94) loss = 6.905, grad_norm = 0.305
I0402 03:53:41.579554 140284126029568 logging_writer.py:48] [95] global_step=95, grad_norm=0.303684, loss=6.903175
I0402 03:53:41.584149 140318046459712 submission.py:139] 95) loss = 6.903, grad_norm = 0.304
I0402 03:53:41.977197 140284134422272 logging_writer.py:48] [96] global_step=96, grad_norm=0.306250, loss=6.903811
I0402 03:53:41.983143 140318046459712 submission.py:139] 96) loss = 6.904, grad_norm = 0.306
I0402 03:53:42.378717 140284126029568 logging_writer.py:48] [97] global_step=97, grad_norm=0.308340, loss=6.902719
I0402 03:53:42.382174 140318046459712 submission.py:139] 97) loss = 6.903, grad_norm = 0.308
I0402 03:53:42.777866 140284134422272 logging_writer.py:48] [98] global_step=98, grad_norm=0.302754, loss=6.905000
I0402 03:53:42.782712 140318046459712 submission.py:139] 98) loss = 6.905, grad_norm = 0.303
I0402 03:53:43.178520 140284126029568 logging_writer.py:48] [99] global_step=99, grad_norm=0.305030, loss=6.903212
I0402 03:53:43.182886 140318046459712 submission.py:139] 99) loss = 6.903, grad_norm = 0.305
I0402 03:53:43.574678 140284134422272 logging_writer.py:48] [100] global_step=100, grad_norm=0.303841, loss=6.903227
I0402 03:53:43.579410 140318046459712 submission.py:139] 100) loss = 6.903, grad_norm = 0.304
I0402 03:56:17.865782 140284126029568 logging_writer.py:48] [500] global_step=500, grad_norm=0.919983, loss=6.750151
I0402 03:56:17.870978 140318046459712 submission.py:139] 500) loss = 6.750, grad_norm = 0.920
I0402 03:59:30.466051 140284134422272 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.151728, loss=6.498142
I0402 03:59:30.471241 140318046459712 submission.py:139] 1000) loss = 6.498, grad_norm = 1.152
I0402 04:00:03.231498 140318046459712 submission_runner.py:373] Before eval at step 1086: RAM USED (GB) 98.752569344
I0402 04:00:03.231744 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:00:47.149871 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:01:31.476929 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:01:32.913613 140318046459712 submission_runner.py:382] Time since start: 534.73s, 	Step: 1086, 	{'train/accuracy': 0.04888671875, 'train/loss': 5.88658447265625, 'validation/accuracy': 0.04692, 'validation/loss': 5.9145125, 'validation/num_examples': 50000, 'test/accuracy': 0.0364, 'test/loss': 6.028406640625, 'test/num_examples': 10000}
I0402 04:01:32.913956 140318046459712 submission_runner.py:396] After eval at step 1086: RAM USED (GB) 98.776100864
I0402 04:01:32.921941 140273262769920 logging_writer.py:48] [1086] global_step=1086, preemption_count=0, score=424.320442, test/accuracy=0.036400, test/loss=6.028407, test/num_examples=10000, total_duration=534.725374, train/accuracy=0.048887, train/loss=5.886584, validation/accuracy=0.046920, validation/loss=5.914512, validation/num_examples=50000
I0402 04:01:33.202848 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_1086.
I0402 04:01:33.203689 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 1086: RAM USED (GB) 98.773991424
I0402 04:04:16.222733 140273271162624 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.096472, loss=6.414803
I0402 04:04:16.227083 140318046459712 submission.py:139] 1500) loss = 6.415, grad_norm = 1.096
I0402 04:07:28.987923 140273262769920 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.932983, loss=6.305472
I0402 04:07:28.992834 140318046459712 submission.py:139] 2000) loss = 6.305, grad_norm = 0.933
I0402 04:08:33.255180 140318046459712 submission_runner.py:373] Before eval at step 2168: RAM USED (GB) 100.28670976
I0402 04:08:33.255411 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:09:16.584966 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:10:00.713416 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:10:02.137329 140318046459712 submission_runner.py:382] Time since start: 1044.75s, 	Step: 2168, 	{'train/accuracy': 0.09189453125, 'train/loss': 5.308607177734375, 'validation/accuracy': 0.08532, 'validation/loss': 5.367700625, 'validation/num_examples': 50000, 'test/accuracy': 0.0656, 'test/loss': 5.5681, 'test/num_examples': 10000}
I0402 04:10:02.137688 140318046459712 submission_runner.py:396] After eval at step 2168: RAM USED (GB) 100.356804608
I0402 04:10:02.145492 140273271162624 logging_writer.py:48] [2168] global_step=2168, preemption_count=0, score=841.922299, test/accuracy=0.065600, test/loss=5.568100, test/num_examples=10000, total_duration=1044.749125, train/accuracy=0.091895, train/loss=5.308607, validation/accuracy=0.085320, validation/loss=5.367701, validation/num_examples=50000
I0402 04:10:02.428480 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_2168.
I0402 04:10:02.429158 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 2168: RAM USED (GB) 100.356165632
I0402 04:12:12.391045 140273262769920 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.765688, loss=6.170695
I0402 04:12:12.394533 140318046459712 submission.py:139] 2500) loss = 6.171, grad_norm = 0.766
I0402 04:15:27.410767 140273271162624 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.873143, loss=6.172781
I0402 04:15:27.416167 140318046459712 submission.py:139] 3000) loss = 6.173, grad_norm = 0.873
I0402 04:17:02.605039 140318046459712 submission_runner.py:373] Before eval at step 3248: RAM USED (GB) 100.890607616
I0402 04:17:02.605278 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:17:45.925882 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:18:30.079174 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:18:31.499754 140318046459712 submission_runner.py:382] Time since start: 1554.10s, 	Step: 3248, 	{'train/accuracy': 0.123828125, 'train/loss': 4.954647827148437, 'validation/accuracy': 0.11478, 'validation/loss': 5.03891125, 'validation/num_examples': 50000, 'test/accuracy': 0.0862, 'test/loss': 5.3061859375, 'test/num_examples': 10000}
I0402 04:18:31.500081 140318046459712 submission_runner.py:396] After eval at step 3248: RAM USED (GB) 100.849610752
I0402 04:18:31.507753 140273262769920 logging_writer.py:48] [3248] global_step=3248, preemption_count=0, score=1259.636549, test/accuracy=0.086200, test/loss=5.306186, test/num_examples=10000, total_duration=1554.098954, train/accuracy=0.123828, train/loss=4.954648, validation/accuracy=0.114780, validation/loss=5.038911, validation/num_examples=50000
I0402 04:18:31.788005 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_3248.
I0402 04:18:31.788725 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 3248: RAM USED (GB) 100.848480256
I0402 04:20:09.378232 140273271162624 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.678185, loss=6.319765
I0402 04:20:09.383418 140318046459712 submission.py:139] 3500) loss = 6.320, grad_norm = 0.678
I0402 04:23:25.871989 140273262769920 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.844299, loss=6.162643
I0402 04:23:25.878446 140318046459712 submission.py:139] 4000) loss = 6.163, grad_norm = 0.844
I0402 04:25:31.960371 140318046459712 submission_runner.py:373] Before eval at step 4328: RAM USED (GB) 100.698386432
I0402 04:25:31.960848 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:26:15.066637 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:26:59.976296 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:27:01.399128 140318046459712 submission_runner.py:382] Time since start: 2063.45s, 	Step: 4328, 	{'train/accuracy': 0.163671875, 'train/loss': 4.581618957519531, 'validation/accuracy': 0.15, 'validation/loss': 4.68083, 'validation/num_examples': 50000, 'test/accuracy': 0.1153, 'test/loss': 4.990866796875, 'test/num_examples': 10000}
I0402 04:27:01.399542 140318046459712 submission_runner.py:396] After eval at step 4328: RAM USED (GB) 100.689862656
I0402 04:27:01.408642 140273271162624 logging_writer.py:48] [4328] global_step=4328, preemption_count=0, score=1677.382023, test/accuracy=0.115300, test/loss=4.990867, test/num_examples=10000, total_duration=2063.454026, train/accuracy=0.163672, train/loss=4.581619, validation/accuracy=0.150000, validation/loss=4.680830, validation/num_examples=50000
I0402 04:27:01.701795 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_4328.
I0402 04:27:01.702466 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 4328: RAM USED (GB) 100.688162816
I0402 04:28:08.564119 140273262769920 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.721307, loss=5.846287
I0402 04:28:08.568214 140318046459712 submission.py:139] 4500) loss = 5.846, grad_norm = 0.721
I0402 04:31:23.686180 140273271162624 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.712058, loss=5.903663
I0402 04:31:23.690409 140318046459712 submission.py:139] 5000) loss = 5.904, grad_norm = 0.712
I0402 04:34:02.081052 140318046459712 submission_runner.py:373] Before eval at step 5406: RAM USED (GB) 101.065080832
I0402 04:34:02.081313 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:34:45.222305 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:35:29.859221 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:35:31.277437 140318046459712 submission_runner.py:382] Time since start: 2573.57s, 	Step: 5406, 	{'train/accuracy': 0.20154296875, 'train/loss': 4.322109375, 'validation/accuracy': 0.18486, 'validation/loss': 4.423338125, 'validation/num_examples': 50000, 'test/accuracy': 0.1412, 'test/loss': 4.75633828125, 'test/num_examples': 10000}
I0402 04:35:31.277775 140318046459712 submission_runner.py:396] After eval at step 5406: RAM USED (GB) 101.04109056
I0402 04:35:31.286633 140273262769920 logging_writer.py:48] [5406] global_step=5406, preemption_count=0, score=2095.346474, test/accuracy=0.141200, test/loss=4.756338, test/num_examples=10000, total_duration=2573.574904, train/accuracy=0.201543, train/loss=4.322109, validation/accuracy=0.184860, validation/loss=4.423338, validation/num_examples=50000
I0402 04:35:31.567421 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_5406.
I0402 04:35:31.568103 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 5406: RAM USED (GB) 101.039968256
I0402 04:36:08.167451 140273271162624 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.635390, loss=5.941774
I0402 04:36:08.175285 140318046459712 submission.py:139] 5500) loss = 5.942, grad_norm = 0.635
I0402 04:39:21.244809 140273262769920 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.650175, loss=5.797546
I0402 04:39:21.249307 140318046459712 submission.py:139] 6000) loss = 5.798, grad_norm = 0.650
I0402 04:42:31.844871 140318046459712 submission_runner.py:373] Before eval at step 6484: RAM USED (GB) 100.880850944
I0402 04:42:31.845106 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:43:15.313761 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:43:59.458577 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:44:00.877672 140318046459712 submission_runner.py:382] Time since start: 3083.34s, 	Step: 6484, 	{'train/accuracy': 0.2448828125, 'train/loss': 3.965003662109375, 'validation/accuracy': 0.22386, 'validation/loss': 4.088123125, 'validation/num_examples': 50000, 'test/accuracy': 0.1675, 'test/loss': 4.47933515625, 'test/num_examples': 10000}
I0402 04:44:00.878067 140318046459712 submission_runner.py:396] After eval at step 6484: RAM USED (GB) 100.992262144
I0402 04:44:00.886088 140273271162624 logging_writer.py:48] [6484] global_step=6484, preemption_count=0, score=2513.213015, test/accuracy=0.167500, test/loss=4.479335, test/num_examples=10000, total_duration=3083.338677, train/accuracy=0.244883, train/loss=3.965004, validation/accuracy=0.223860, validation/loss=4.088123, validation/num_examples=50000
I0402 04:44:01.169894 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_6484.
I0402 04:44:01.170657 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 6484: RAM USED (GB) 100.991393792
I0402 04:44:07.732325 140273262769920 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.655654, loss=5.823627
I0402 04:44:07.736385 140318046459712 submission.py:139] 6500) loss = 5.824, grad_norm = 0.656
I0402 04:47:20.675450 140273271162624 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.717784, loss=5.547496
I0402 04:47:20.681429 140318046459712 submission.py:139] 7000) loss = 5.547, grad_norm = 0.718
I0402 04:50:35.568883 140273262769920 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.682472, loss=5.552877
I0402 04:50:35.572769 140318046459712 submission.py:139] 7500) loss = 5.553, grad_norm = 0.682
I0402 04:51:01.469554 140318046459712 submission_runner.py:373] Before eval at step 7563: RAM USED (GB) 101.016276992
I0402 04:51:01.469788 140318046459712 spec.py:298] Evaluating on the training split.
I0402 04:51:45.311331 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 04:52:29.473411 140318046459712 spec.py:326] Evaluating on the test split.
I0402 04:52:30.894956 140318046459712 submission_runner.py:382] Time since start: 3592.96s, 	Step: 7563, 	{'train/accuracy': 0.28994140625, 'train/loss': 3.637658996582031, 'validation/accuracy': 0.26588, 'validation/loss': 3.77862, 'validation/num_examples': 50000, 'test/accuracy': 0.2078, 'test/loss': 4.211125, 'test/num_examples': 10000}
I0402 04:52:30.895298 140318046459712 submission_runner.py:396] After eval at step 7563: RAM USED (GB) 100.961558528
I0402 04:52:30.903473 140273271162624 logging_writer.py:48] [7563] global_step=7563, preemption_count=0, score=2931.101516, test/accuracy=0.207800, test/loss=4.211125, test/num_examples=10000, total_duration=3592.963399, train/accuracy=0.289941, train/loss=3.637659, validation/accuracy=0.265880, validation/loss=3.778620, validation/num_examples=50000
I0402 04:52:31.185014 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_7563.
I0402 04:52:31.185696 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 7563: RAM USED (GB) 100.960436224
I0402 04:55:20.262704 140273262769920 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.754129, loss=5.377388
I0402 04:55:20.268549 140318046459712 submission.py:139] 8000) loss = 5.377, grad_norm = 0.754
I0402 04:58:33.024487 140273271162624 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.587519, loss=5.417029
I0402 04:58:33.029420 140318046459712 submission.py:139] 8500) loss = 5.417, grad_norm = 0.588
I0402 04:59:31.538525 140318046459712 submission_runner.py:373] Before eval at step 8650: RAM USED (GB) 100.931919872
I0402 04:59:31.538741 140318046459712 spec.py:298] Evaluating on the training split.
I0402 05:00:14.908627 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 05:01:00.757557 140318046459712 spec.py:326] Evaluating on the test split.
I0402 05:01:02.179382 140318046459712 submission_runner.py:382] Time since start: 4103.03s, 	Step: 8650, 	{'train/accuracy': 0.3250390625, 'train/loss': 3.4433575439453126, 'validation/accuracy': 0.29816, 'validation/loss': 3.593865, 'validation/num_examples': 50000, 'test/accuracy': 0.2285, 'test/loss': 4.05041484375, 'test/num_examples': 10000}
I0402 05:01:02.179817 140318046459712 submission_runner.py:396] After eval at step 8650: RAM USED (GB) 100.938571776
I0402 05:01:02.188159 140273262769920 logging_writer.py:48] [8650] global_step=8650, preemption_count=0, score=3349.057635, test/accuracy=0.228500, test/loss=4.050415, test/num_examples=10000, total_duration=4103.032350, train/accuracy=0.325039, train/loss=3.443358, validation/accuracy=0.298160, validation/loss=3.593865, validation/num_examples=50000
I0402 05:01:02.480253 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_8650.
I0402 05:01:02.480973 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 8650: RAM USED (GB) 100.938653696
I0402 05:03:20.803289 140273271162624 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.628110, loss=5.323610
I0402 05:03:20.807734 140318046459712 submission.py:139] 9000) loss = 5.324, grad_norm = 0.628
I0402 05:06:33.803984 140273262769920 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.573073, loss=5.554344
I0402 05:06:33.808024 140318046459712 submission.py:139] 9500) loss = 5.554, grad_norm = 0.573
I0402 05:08:02.858916 140318046459712 submission_runner.py:373] Before eval at step 9732: RAM USED (GB) 100.933165056
I0402 05:08:02.859152 140318046459712 spec.py:298] Evaluating on the training split.
I0402 05:08:46.220428 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 05:09:32.608782 140318046459712 spec.py:326] Evaluating on the test split.
I0402 05:09:34.028105 140318046459712 submission_runner.py:382] Time since start: 4614.35s, 	Step: 9732, 	{'train/accuracy': 0.35884765625, 'train/loss': 3.2429742431640625, 'validation/accuracy': 0.32506, 'validation/loss': 3.4045528125, 'validation/num_examples': 50000, 'test/accuracy': 0.2543, 'test/loss': 3.90951484375, 'test/num_examples': 10000}
I0402 05:09:34.028441 140318046459712 submission_runner.py:396] After eval at step 9732: RAM USED (GB) 101.01387264
I0402 05:09:34.035893 140273271162624 logging_writer.py:48] [9732] global_step=9732, preemption_count=0, score=3767.028032, test/accuracy=0.254300, test/loss=3.909515, test/num_examples=10000, total_duration=4614.352316, train/accuracy=0.358848, train/loss=3.242974, validation/accuracy=0.325060, validation/loss=3.404553, validation/num_examples=50000
I0402 05:09:34.316918 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_9732.
I0402 05:09:34.317612 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 9732: RAM USED (GB) 101.013266432
I0402 05:11:20.697893 140273262769920 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.661047, loss=5.082608
I0402 05:11:20.701678 140318046459712 submission.py:139] 10000) loss = 5.083, grad_norm = 0.661
I0402 05:14:35.535805 140273271162624 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.575365, loss=5.255467
I0402 05:14:35.541927 140318046459712 submission.py:139] 10500) loss = 5.255, grad_norm = 0.575
I0402 05:16:34.429598 140318046459712 submission_runner.py:373] Before eval at step 10809: RAM USED (GB) 101.013999616
I0402 05:16:34.429832 140318046459712 spec.py:298] Evaluating on the training split.
I0402 05:17:17.763430 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 05:18:03.245290 140318046459712 spec.py:326] Evaluating on the test split.
I0402 05:18:04.664664 140318046459712 submission_runner.py:382] Time since start: 5125.92s, 	Step: 10809, 	{'train/accuracy': 0.3924609375, 'train/loss': 3.04691162109375, 'validation/accuracy': 0.35706, 'validation/loss': 3.2251484375, 'validation/num_examples': 50000, 'test/accuracy': 0.2795, 'test/loss': 3.739496875, 'test/num_examples': 10000}
I0402 05:18:04.664989 140318046459712 submission_runner.py:396] After eval at step 10809: RAM USED (GB) 101.035876352
I0402 05:18:04.673806 140273262769920 logging_writer.py:48] [10809] global_step=10809, preemption_count=0, score=4184.749270, test/accuracy=0.279500, test/loss=3.739497, test/num_examples=10000, total_duration=5125.923606, train/accuracy=0.392461, train/loss=3.046912, validation/accuracy=0.357060, validation/loss=3.225148, validation/num_examples=50000
I0402 05:18:04.956379 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_10809.
I0402 05:18:04.957031 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 10809: RAM USED (GB) 101.034754048
I0402 05:19:18.894606 140273271162624 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.652773, loss=5.259958
I0402 05:19:18.899442 140318046459712 submission.py:139] 11000) loss = 5.260, grad_norm = 0.653
I0402 05:22:36.756451 140273262769920 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.835781, loss=4.876355
I0402 05:22:36.762614 140318046459712 submission.py:139] 11500) loss = 4.876, grad_norm = 0.836
I0402 05:25:05.296165 140318046459712 submission_runner.py:373] Before eval at step 11886: RAM USED (GB) 101.01200896
I0402 05:25:05.296385 140318046459712 spec.py:298] Evaluating on the training split.
I0402 05:25:48.073917 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 05:26:33.653857 140318046459712 spec.py:326] Evaluating on the test split.
I0402 05:26:35.074579 140318046459712 submission_runner.py:382] Time since start: 5636.79s, 	Step: 11886, 	{'train/accuracy': 0.41880859375, 'train/loss': 2.9218707275390625, 'validation/accuracy': 0.38258, 'validation/loss': 3.0969409375, 'validation/num_examples': 50000, 'test/accuracy': 0.2932, 'test/loss': 3.61540390625, 'test/num_examples': 10000}
I0402 05:26:35.074922 140318046459712 submission_runner.py:396] After eval at step 11886: RAM USED (GB) 100.998836224
I0402 05:26:35.083563 140273271162624 logging_writer.py:48] [11886] global_step=11886, preemption_count=0, score=4602.716186, test/accuracy=0.293200, test/loss=3.615404, test/num_examples=10000, total_duration=5636.789861, train/accuracy=0.418809, train/loss=2.921871, validation/accuracy=0.382580, validation/loss=3.096941, validation/num_examples=50000
I0402 05:26:35.370176 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_11886.
I0402 05:26:35.370836 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 11886: RAM USED (GB) 100.99771392
I0402 05:27:19.748395 140273262769920 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.602371, loss=4.965272
I0402 05:27:19.752137 140318046459712 submission.py:139] 12000) loss = 4.965, grad_norm = 0.602
I0402 05:30:34.953976 140273271162624 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.632206, loss=5.075850
I0402 05:30:34.959014 140318046459712 submission.py:139] 12500) loss = 5.076, grad_norm = 0.632
I0402 05:33:35.678603 140318046459712 submission_runner.py:373] Before eval at step 12964: RAM USED (GB) 100.749656064
I0402 05:33:35.678883 140318046459712 spec.py:298] Evaluating on the training split.
I0402 05:34:19.191713 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 05:35:04.583876 140318046459712 spec.py:326] Evaluating on the test split.
I0402 05:35:06.008253 140318046459712 submission_runner.py:382] Time since start: 6147.17s, 	Step: 12964, 	{'train/accuracy': 0.44775390625, 'train/loss': 2.7214312744140625, 'validation/accuracy': 0.40772, 'validation/loss': 2.9132921875, 'validation/num_examples': 50000, 'test/accuracy': 0.3218, 'test/loss': 3.4499203125, 'test/num_examples': 10000}
I0402 05:35:06.008589 140318046459712 submission_runner.py:396] After eval at step 12964: RAM USED (GB) 100.920684544
I0402 05:35:06.016893 140273262769920 logging_writer.py:48] [12964] global_step=12964, preemption_count=0, score=5020.636513, test/accuracy=0.321800, test/loss=3.449920, test/num_examples=10000, total_duration=6147.172546, train/accuracy=0.447754, train/loss=2.721431, validation/accuracy=0.407720, validation/loss=2.913292, validation/num_examples=50000
I0402 05:35:06.315622 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_12964.
I0402 05:35:06.316301 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 12964: RAM USED (GB) 100.919554048
I0402 05:35:20.723084 140273271162624 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.653086, loss=5.000206
I0402 05:35:20.728455 140318046459712 submission.py:139] 13000) loss = 5.000, grad_norm = 0.653
I0402 05:38:33.619317 140273262769920 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.623009, loss=4.783950
I0402 05:38:33.624548 140318046459712 submission.py:139] 13500) loss = 4.784, grad_norm = 0.623
I0402 05:41:51.117501 140318046459712 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 100.991127552
I0402 05:41:51.117741 140318046459712 spec.py:298] Evaluating on the training split.
I0402 05:42:34.927245 140318046459712 spec.py:310] Evaluating on the validation split.
I0402 05:43:19.002800 140318046459712 spec.py:326] Evaluating on the test split.
I0402 05:43:20.426076 140318046459712 submission_runner.py:382] Time since start: 6642.61s, 	Step: 14000, 	{'train/accuracy': 0.476875, 'train/loss': 2.5310903930664064, 'validation/accuracy': 0.43482, 'validation/loss': 2.7351628125, 'validation/num_examples': 50000, 'test/accuracy': 0.338, 'test/loss': 3.30964296875, 'test/num_examples': 10000}
I0402 05:43:20.426412 140318046459712 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 100.972445696
I0402 05:43:20.434461 140273271162624 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5423.155259, test/accuracy=0.338000, test/loss=3.309643, test/num_examples=10000, total_duration=6642.611205, train/accuracy=0.476875, train/loss=2.531090, validation/accuracy=0.434820, validation/loss=2.735163, validation/num_examples=50000
I0402 05:43:20.718843 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0402 05:43:20.719551 140318046459712 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 100.971573248
I0402 05:43:20.727820 140273262769920 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5423.155259
I0402 05:43:21.520239 140318046459712 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/imagenet_vit_pytorch/trial_1/checkpoint_14000.
I0402 05:43:21.827413 140318046459712 submission_runner.py:550] Tuning trial 1/1
I0402 05:43:21.827626 140318046459712 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 05:43:21.828314 140318046459712 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00087890625, 'train/loss': 6.90775634765625, 'validation/accuracy': 0.001, 'validation/loss': 6.907755625, 'validation/num_examples': 50000, 'test/accuracy': 0.001, 'test/loss': 6.90775546875, 'test/num_examples': 10000, 'score': 6.630067586898804, 'total_duration': 6.632102012634277, 'global_step': 1, 'preemption_count': 0}), (1086, {'train/accuracy': 0.04888671875, 'train/loss': 5.88658447265625, 'validation/accuracy': 0.04692, 'validation/loss': 5.9145125, 'validation/num_examples': 50000, 'test/accuracy': 0.0364, 'test/loss': 6.028406640625, 'test/num_examples': 10000, 'score': 424.3204417228699, 'total_duration': 534.7253739833832, 'global_step': 1086, 'preemption_count': 0}), (2168, {'train/accuracy': 0.09189453125, 'train/loss': 5.308607177734375, 'validation/accuracy': 0.08532, 'validation/loss': 5.367700625, 'validation/num_examples': 50000, 'test/accuracy': 0.0656, 'test/loss': 5.5681, 'test/num_examples': 10000, 'score': 841.9222993850708, 'total_duration': 1044.7491254806519, 'global_step': 2168, 'preemption_count': 0}), (3248, {'train/accuracy': 0.123828125, 'train/loss': 4.954647827148437, 'validation/accuracy': 0.11478, 'validation/loss': 5.03891125, 'validation/num_examples': 50000, 'test/accuracy': 0.0862, 'test/loss': 5.3061859375, 'test/num_examples': 10000, 'score': 1259.6365492343903, 'total_duration': 1554.0989544391632, 'global_step': 3248, 'preemption_count': 0}), (4328, {'train/accuracy': 0.163671875, 'train/loss': 4.581618957519531, 'validation/accuracy': 0.15, 'validation/loss': 4.68083, 'validation/num_examples': 50000, 'test/accuracy': 0.1153, 'test/loss': 4.990866796875, 'test/num_examples': 10000, 'score': 1677.3820233345032, 'total_duration': 2063.4540264606476, 'global_step': 4328, 'preemption_count': 0}), (5406, {'train/accuracy': 0.20154296875, 'train/loss': 4.322109375, 'validation/accuracy': 0.18486, 'validation/loss': 4.423338125, 'validation/num_examples': 50000, 'test/accuracy': 0.1412, 'test/loss': 4.75633828125, 'test/num_examples': 10000, 'score': 2095.346474170685, 'total_duration': 2573.5749039649963, 'global_step': 5406, 'preemption_count': 0}), (6484, {'train/accuracy': 0.2448828125, 'train/loss': 3.965003662109375, 'validation/accuracy': 0.22386, 'validation/loss': 4.088123125, 'validation/num_examples': 50000, 'test/accuracy': 0.1675, 'test/loss': 4.47933515625, 'test/num_examples': 10000, 'score': 2513.2130148410797, 'total_duration': 3083.3386766910553, 'global_step': 6484, 'preemption_count': 0}), (7563, {'train/accuracy': 0.28994140625, 'train/loss': 3.637658996582031, 'validation/accuracy': 0.26588, 'validation/loss': 3.77862, 'validation/num_examples': 50000, 'test/accuracy': 0.2078, 'test/loss': 4.211125, 'test/num_examples': 10000, 'score': 2931.10151553154, 'total_duration': 3592.963399171829, 'global_step': 7563, 'preemption_count': 0}), (8650, {'train/accuracy': 0.3250390625, 'train/loss': 3.4433575439453126, 'validation/accuracy': 0.29816, 'validation/loss': 3.593865, 'validation/num_examples': 50000, 'test/accuracy': 0.2285, 'test/loss': 4.05041484375, 'test/num_examples': 10000, 'score': 3349.0576345920563, 'total_duration': 4103.032350301743, 'global_step': 8650, 'preemption_count': 0}), (9732, {'train/accuracy': 0.35884765625, 'train/loss': 3.2429742431640625, 'validation/accuracy': 0.32506, 'validation/loss': 3.4045528125, 'validation/num_examples': 50000, 'test/accuracy': 0.2543, 'test/loss': 3.90951484375, 'test/num_examples': 10000, 'score': 3767.0280315876007, 'total_duration': 4614.352315664291, 'global_step': 9732, 'preemption_count': 0}), (10809, {'train/accuracy': 0.3924609375, 'train/loss': 3.04691162109375, 'validation/accuracy': 0.35706, 'validation/loss': 3.2251484375, 'validation/num_examples': 50000, 'test/accuracy': 0.2795, 'test/loss': 3.739496875, 'test/num_examples': 10000, 'score': 4184.749269723892, 'total_duration': 5125.923605918884, 'global_step': 10809, 'preemption_count': 0}), (11886, {'train/accuracy': 0.41880859375, 'train/loss': 2.9218707275390625, 'validation/accuracy': 0.38258, 'validation/loss': 3.0969409375, 'validation/num_examples': 50000, 'test/accuracy': 0.2932, 'test/loss': 3.61540390625, 'test/num_examples': 10000, 'score': 4602.716185569763, 'total_duration': 5636.78986120224, 'global_step': 11886, 'preemption_count': 0}), (12964, {'train/accuracy': 0.44775390625, 'train/loss': 2.7214312744140625, 'validation/accuracy': 0.40772, 'validation/loss': 2.9132921875, 'validation/num_examples': 50000, 'test/accuracy': 0.3218, 'test/loss': 3.4499203125, 'test/num_examples': 10000, 'score': 5020.636513233185, 'total_duration': 6147.172545671463, 'global_step': 12964, 'preemption_count': 0}), (14000, {'train/accuracy': 0.476875, 'train/loss': 2.5310903930664064, 'validation/accuracy': 0.43482, 'validation/loss': 2.7351628125, 'validation/num_examples': 50000, 'test/accuracy': 0.338, 'test/loss': 3.30964296875, 'test/num_examples': 10000, 'score': 5423.155259370804, 'total_duration': 6642.611204862595, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0402 05:43:21.828418 140318046459712 submission_runner.py:553] Timing: 5423.155259370804
I0402 05:43:21.828463 140318046459712 submission_runner.py:554] ====================
I0402 05:43:21.828555 140318046459712 submission_runner.py:613] Final imagenet_vit score: 5423.155259370804
