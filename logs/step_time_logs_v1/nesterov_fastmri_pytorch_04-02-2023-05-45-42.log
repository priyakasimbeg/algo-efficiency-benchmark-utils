WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0402 05:46:03.900157 140112236312384 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0402 05:46:03.900187 140287585130304 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0402 05:46:03.900691 140472367200064 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0402 05:46:03.901032 139834018232128 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0402 05:46:03.901070 140036926846784 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0402 05:46:03.901127 139978644035392 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0402 05:46:03.901717 140531736139584 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0402 05:46:03.911292 140472367200064 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.911290 139915587815232 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0402 05:46:03.911568 139915587815232 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.911615 140036926846784 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.911648 139834018232128 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.911706 139978644035392 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.912202 140531736139584 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.921186 140112236312384 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:03.921210 140287585130304 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0402 05:46:04.462271 139915587815232 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/fastmri_pytorch.
W0402 05:46:04.579314 140112236312384 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.579863 140472367200064 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.580066 140531736139584 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.580740 140036926846784 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.580785 139978644035392 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.581082 139915587815232 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.581087 140287585130304 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0402 05:46:04.581561 139834018232128 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0402 05:46:04.586242 139915587815232 submission_runner.py:511] Using RNG seed 2056840072
I0402 05:46:04.587153 139915587815232 submission_runner.py:520] --- Tuning run 1/1 ---
I0402 05:46:04.587267 139915587815232 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1.
I0402 05:46:04.587446 139915587815232 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/hparams.json.
I0402 05:46:04.588347 139915587815232 submission_runner.py:230] Starting train once: RAM USED (GB) 5.664444416
I0402 05:46:04.588441 139915587815232 submission_runner.py:231] Initializing dataset.
I0402 05:46:04.588594 139915587815232 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.664444416
I0402 05:46:04.588650 139915587815232 submission_runner.py:240] Initializing model.
I0402 05:46:08.720536 139915587815232 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.303389184
I0402 05:46:08.720749 139915587815232 submission_runner.py:252] Initializing optimizer.
I0402 05:46:09.298888 139915587815232 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.309819904
I0402 05:46:09.299069 139915587815232 submission_runner.py:261] Initializing metrics bundle.
I0402 05:46:09.299136 139915587815232 submission_runner.py:276] Initializing checkpoint and logger.
I0402 05:46:09.302081 139915587815232 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0402 05:46:09.302223 139915587815232 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0402 05:46:09.889993 139915587815232 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/meta_data_0.json.
I0402 05:46:09.890939 139915587815232 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/flags_0.json.
I0402 05:46:09.933183 139915587815232 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 15.360720896
I0402 05:46:09.934289 139915587815232 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.360720896
I0402 05:46:09.934403 139915587815232 submission_runner.py:313] Starting training loop.
I0402 05:46:53.651950 139915587815232 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 24.051482624
I0402 05:46:57.722468 139873553999616 logging_writer.py:48] [0] global_step=0, grad_norm=2.954432, loss=0.777221
I0402 05:46:57.728838 139915587815232 submission.py:139] 0) loss = 0.777, grad_norm = 2.954
I0402 05:46:57.730039 139915587815232 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 32.828317696
I0402 05:46:57.730789 139915587815232 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 32.828424192
I0402 05:46:57.730935 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:48:37.244825 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:49:39.337374 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:50:40.275082 139915587815232 submission_runner.py:382] Time since start: 47.80s, 	Step: 1, 	{'train/ssim': 0.2415801456996373, 'train/loss': 0.8037490844726562, 'validation/ssim': 0.2333616000182453, 'validation/loss': 0.8096762395804024, 'validation/num_examples': 3554, 'test/ssim': 0.25698948838596936, 'test/loss': 0.8089340197177813, 'test/num_examples': 3581}
I0402 05:50:40.275522 139915587815232 submission_runner.py:396] After eval at step 1: RAM USED (GB) 68.952055808
I0402 05:50:40.284329 139847977113344 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=47.794957, test/loss=0.808934, test/num_examples=3581, test/ssim=0.256989, total_duration=47.796866, train/loss=0.803749, train/ssim=0.241580, validation/loss=0.809676, validation/num_examples=3554, validation/ssim=0.233362
I0402 05:50:40.385187 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1.
I0402 05:50:40.385670 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 68.953096192
I0402 05:50:40.396334 139915587815232 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 68.967796736
I0402 05:50:40.402096 140472367200064 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402103 140531736139584 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402113 139834018232128 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402229 139915587815232 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402160 140112236312384 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402162 139978644035392 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402199 140036926846784 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.402205 140287585130304 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0402 05:50:40.464048 139847826142976 logging_writer.py:48] [1] global_step=1, grad_norm=2.885581, loss=0.822551
I0402 05:50:40.469838 139915587815232 submission.py:139] 1) loss = 0.823, grad_norm = 2.886
I0402 05:50:40.470894 139915587815232 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 68.970201088
I0402 05:50:40.543227 139847977113344 logging_writer.py:48] [2] global_step=2, grad_norm=2.792937, loss=0.790179
I0402 05:50:40.548269 139915587815232 submission.py:139] 2) loss = 0.790, grad_norm = 2.793
I0402 05:50:40.620139 139847826142976 logging_writer.py:48] [3] global_step=3, grad_norm=2.518557, loss=0.822201
I0402 05:50:40.623553 139915587815232 submission.py:139] 3) loss = 0.822, grad_norm = 2.519
I0402 05:50:40.689796 139847977113344 logging_writer.py:48] [4] global_step=4, grad_norm=2.687681, loss=0.764399
I0402 05:50:40.692987 139915587815232 submission.py:139] 4) loss = 0.764, grad_norm = 2.688
I0402 05:50:40.760685 139847826142976 logging_writer.py:48] [5] global_step=5, grad_norm=2.370782, loss=0.719469
I0402 05:50:40.765082 139915587815232 submission.py:139] 5) loss = 0.719, grad_norm = 2.371
I0402 05:50:40.835123 139847977113344 logging_writer.py:48] [6] global_step=6, grad_norm=2.101124, loss=0.737384
I0402 05:50:40.840276 139915587815232 submission.py:139] 6) loss = 0.737, grad_norm = 2.101
I0402 05:50:40.909546 139847826142976 logging_writer.py:48] [7] global_step=7, grad_norm=1.591758, loss=0.667246
I0402 05:50:40.915371 139915587815232 submission.py:139] 7) loss = 0.667, grad_norm = 1.592
I0402 05:50:40.980760 139847977113344 logging_writer.py:48] [8] global_step=8, grad_norm=1.375449, loss=0.615370
I0402 05:50:40.984966 139915587815232 submission.py:139] 8) loss = 0.615, grad_norm = 1.375
I0402 05:50:41.051225 139847826142976 logging_writer.py:48] [9] global_step=9, grad_norm=0.965881, loss=0.627273
I0402 05:50:41.056831 139915587815232 submission.py:139] 9) loss = 0.627, grad_norm = 0.966
I0402 05:50:41.129718 139847977113344 logging_writer.py:48] [10] global_step=10, grad_norm=0.950738, loss=0.543343
I0402 05:50:41.135085 139915587815232 submission.py:139] 10) loss = 0.543, grad_norm = 0.951
I0402 05:50:41.209019 139847826142976 logging_writer.py:48] [11] global_step=11, grad_norm=1.020252, loss=0.600259
I0402 05:50:41.214297 139915587815232 submission.py:139] 11) loss = 0.600, grad_norm = 1.020
I0402 05:50:41.292386 139847977113344 logging_writer.py:48] [12] global_step=12, grad_norm=1.265479, loss=0.564635
I0402 05:50:41.297835 139915587815232 submission.py:139] 12) loss = 0.565, grad_norm = 1.265
I0402 05:50:41.369886 139847826142976 logging_writer.py:48] [13] global_step=13, grad_norm=1.468396, loss=0.505887
I0402 05:50:41.375300 139915587815232 submission.py:139] 13) loss = 0.506, grad_norm = 1.468
I0402 05:50:41.490099 139847977113344 logging_writer.py:48] [14] global_step=14, grad_norm=1.525691, loss=0.524965
I0402 05:50:41.493536 139915587815232 submission.py:139] 14) loss = 0.525, grad_norm = 1.526
I0402 05:50:41.724720 139847826142976 logging_writer.py:48] [15] global_step=15, grad_norm=1.465815, loss=0.532595
I0402 05:50:41.730942 139915587815232 submission.py:139] 15) loss = 0.533, grad_norm = 1.466
I0402 05:50:42.005579 139847977113344 logging_writer.py:48] [16] global_step=16, grad_norm=1.424924, loss=0.487287
I0402 05:50:42.008691 139915587815232 submission.py:139] 16) loss = 0.487, grad_norm = 1.425
I0402 05:50:42.288063 139847826142976 logging_writer.py:48] [17] global_step=17, grad_norm=1.024692, loss=0.436071
I0402 05:50:42.293326 139915587815232 submission.py:139] 17) loss = 0.436, grad_norm = 1.025
I0402 05:50:42.542050 139847977113344 logging_writer.py:48] [18] global_step=18, grad_norm=0.730185, loss=0.446865
I0402 05:50:42.547427 139915587815232 submission.py:139] 18) loss = 0.447, grad_norm = 0.730
I0402 05:50:42.774846 139847826142976 logging_writer.py:48] [19] global_step=19, grad_norm=0.483911, loss=0.425538
I0402 05:50:42.780158 139915587815232 submission.py:139] 19) loss = 0.426, grad_norm = 0.484
I0402 05:50:43.120209 139847977113344 logging_writer.py:48] [20] global_step=20, grad_norm=0.412552, loss=0.398893
I0402 05:50:43.125876 139915587815232 submission.py:139] 20) loss = 0.399, grad_norm = 0.413
I0402 05:50:43.364628 139847826142976 logging_writer.py:48] [21] global_step=21, grad_norm=0.559669, loss=0.378580
I0402 05:50:43.370232 139915587815232 submission.py:139] 21) loss = 0.379, grad_norm = 0.560
I0402 05:50:43.622230 139847977113344 logging_writer.py:48] [22] global_step=22, grad_norm=0.739225, loss=0.399766
I0402 05:50:43.627902 139915587815232 submission.py:139] 22) loss = 0.400, grad_norm = 0.739
I0402 05:50:43.870862 139847826142976 logging_writer.py:48] [23] global_step=23, grad_norm=0.854452, loss=0.365148
I0402 05:50:43.876012 139915587815232 submission.py:139] 23) loss = 0.365, grad_norm = 0.854
I0402 05:50:44.134231 139847977113344 logging_writer.py:48] [24] global_step=24, grad_norm=0.799836, loss=0.378270
I0402 05:50:44.139654 139915587815232 submission.py:139] 24) loss = 0.378, grad_norm = 0.800
I0402 05:50:44.396824 139847826142976 logging_writer.py:48] [25] global_step=25, grad_norm=0.811807, loss=0.375680
I0402 05:50:44.402572 139915587815232 submission.py:139] 25) loss = 0.376, grad_norm = 0.812
I0402 05:50:44.687221 139847977113344 logging_writer.py:48] [26] global_step=26, grad_norm=0.856028, loss=0.331121
I0402 05:50:44.692517 139915587815232 submission.py:139] 26) loss = 0.331, grad_norm = 0.856
I0402 05:50:44.944740 139847826142976 logging_writer.py:48] [27] global_step=27, grad_norm=0.729447, loss=0.347262
I0402 05:50:44.949671 139915587815232 submission.py:139] 27) loss = 0.347, grad_norm = 0.729
I0402 05:50:45.194534 139847977113344 logging_writer.py:48] [28] global_step=28, grad_norm=0.539147, loss=0.292768
I0402 05:50:45.197889 139915587815232 submission.py:139] 28) loss = 0.293, grad_norm = 0.539
I0402 05:50:45.463103 139847826142976 logging_writer.py:48] [29] global_step=29, grad_norm=0.390037, loss=0.430929
I0402 05:50:45.466273 139915587815232 submission.py:139] 29) loss = 0.431, grad_norm = 0.390
I0402 05:50:45.704669 139847977113344 logging_writer.py:48] [30] global_step=30, grad_norm=0.448363, loss=0.415419
I0402 05:50:45.707813 139915587815232 submission.py:139] 30) loss = 0.415, grad_norm = 0.448
I0402 05:50:46.034829 139847826142976 logging_writer.py:48] [31] global_step=31, grad_norm=0.407451, loss=0.441880
I0402 05:50:46.038081 139915587815232 submission.py:139] 31) loss = 0.442, grad_norm = 0.407
I0402 05:50:46.316689 139847977113344 logging_writer.py:48] [32] global_step=32, grad_norm=0.343892, loss=0.313355
I0402 05:50:46.320134 139915587815232 submission.py:139] 32) loss = 0.313, grad_norm = 0.344
I0402 05:50:46.580318 139847826142976 logging_writer.py:48] [33] global_step=33, grad_norm=0.443094, loss=0.468245
I0402 05:50:46.584932 139915587815232 submission.py:139] 33) loss = 0.468, grad_norm = 0.443
I0402 05:50:46.851312 139847977113344 logging_writer.py:48] [34] global_step=34, grad_norm=0.304956, loss=0.451554
I0402 05:50:46.855004 139915587815232 submission.py:139] 34) loss = 0.452, grad_norm = 0.305
I0402 05:50:47.106735 139847826142976 logging_writer.py:48] [35] global_step=35, grad_norm=0.329739, loss=0.348843
I0402 05:50:47.111877 139915587815232 submission.py:139] 35) loss = 0.349, grad_norm = 0.330
I0402 05:50:47.363399 139847977113344 logging_writer.py:48] [36] global_step=36, grad_norm=0.246653, loss=0.323611
I0402 05:50:47.367967 139915587815232 submission.py:139] 36) loss = 0.324, grad_norm = 0.247
I0402 05:50:47.660187 139847826142976 logging_writer.py:48] [37] global_step=37, grad_norm=0.193320, loss=0.346488
I0402 05:50:47.665160 139915587815232 submission.py:139] 37) loss = 0.346, grad_norm = 0.193
I0402 05:50:47.934452 139847977113344 logging_writer.py:48] [38] global_step=38, grad_norm=0.392281, loss=0.319127
I0402 05:50:47.939170 139915587815232 submission.py:139] 38) loss = 0.319, grad_norm = 0.392
I0402 05:50:48.172166 139847826142976 logging_writer.py:48] [39] global_step=39, grad_norm=0.164677, loss=0.382996
I0402 05:50:48.178710 139915587815232 submission.py:139] 39) loss = 0.383, grad_norm = 0.165
I0402 05:50:48.438799 139847977113344 logging_writer.py:48] [40] global_step=40, grad_norm=0.560500, loss=0.285948
I0402 05:50:48.444742 139915587815232 submission.py:139] 40) loss = 0.286, grad_norm = 0.560
I0402 05:50:48.730273 139847826142976 logging_writer.py:48] [41] global_step=41, grad_norm=0.296672, loss=0.316499
I0402 05:50:48.735040 139915587815232 submission.py:139] 41) loss = 0.316, grad_norm = 0.297
I0402 05:50:48.953026 139847977113344 logging_writer.py:48] [42] global_step=42, grad_norm=0.226881, loss=0.363610
I0402 05:50:48.958664 139915587815232 submission.py:139] 42) loss = 0.364, grad_norm = 0.227
I0402 05:50:49.230588 139847826142976 logging_writer.py:48] [43] global_step=43, grad_norm=0.095611, loss=0.369967
I0402 05:50:49.237424 139915587815232 submission.py:139] 43) loss = 0.370, grad_norm = 0.096
I0402 05:50:49.482625 139847977113344 logging_writer.py:48] [44] global_step=44, grad_norm=0.131514, loss=0.457708
I0402 05:50:49.486093 139915587815232 submission.py:139] 44) loss = 0.458, grad_norm = 0.132
I0402 05:50:49.749421 139847826142976 logging_writer.py:48] [45] global_step=45, grad_norm=0.133557, loss=0.328823
I0402 05:50:49.752719 139915587815232 submission.py:139] 45) loss = 0.329, grad_norm = 0.134
I0402 05:50:50.062047 139847977113344 logging_writer.py:48] [46] global_step=46, grad_norm=0.150166, loss=0.370495
I0402 05:50:50.065254 139915587815232 submission.py:139] 46) loss = 0.370, grad_norm = 0.150
I0402 05:50:50.330916 139847826142976 logging_writer.py:48] [47] global_step=47, grad_norm=0.223749, loss=0.362595
I0402 05:50:50.334882 139915587815232 submission.py:139] 47) loss = 0.363, grad_norm = 0.224
I0402 05:50:50.605528 139847977113344 logging_writer.py:48] [48] global_step=48, grad_norm=0.259610, loss=0.401552
I0402 05:50:50.609317 139915587815232 submission.py:139] 48) loss = 0.402, grad_norm = 0.260
I0402 05:50:50.884793 139847826142976 logging_writer.py:48] [49] global_step=49, grad_norm=0.149356, loss=0.321855
I0402 05:50:50.888582 139915587815232 submission.py:139] 49) loss = 0.322, grad_norm = 0.149
I0402 05:50:51.155992 139847977113344 logging_writer.py:48] [50] global_step=50, grad_norm=0.124118, loss=0.379951
I0402 05:50:51.159646 139915587815232 submission.py:139] 50) loss = 0.380, grad_norm = 0.124
I0402 05:50:51.430073 139847826142976 logging_writer.py:48] [51] global_step=51, grad_norm=0.151162, loss=0.334556
I0402 05:50:51.435631 139915587815232 submission.py:139] 51) loss = 0.335, grad_norm = 0.151
I0402 05:50:51.669354 139847977113344 logging_writer.py:48] [52] global_step=52, grad_norm=0.111694, loss=0.367038
I0402 05:50:51.673879 139915587815232 submission.py:139] 52) loss = 0.367, grad_norm = 0.112
I0402 05:50:51.972661 139847826142976 logging_writer.py:48] [53] global_step=53, grad_norm=0.099592, loss=0.395434
I0402 05:50:51.992343 139915587815232 submission.py:139] 53) loss = 0.395, grad_norm = 0.100
I0402 05:50:52.235967 139847977113344 logging_writer.py:48] [54] global_step=54, grad_norm=0.097483, loss=0.406924
I0402 05:50:52.241788 139915587815232 submission.py:139] 54) loss = 0.407, grad_norm = 0.097
I0402 05:50:52.533396 139847826142976 logging_writer.py:48] [55] global_step=55, grad_norm=0.166095, loss=0.376934
I0402 05:50:52.539798 139915587815232 submission.py:139] 55) loss = 0.377, grad_norm = 0.166
I0402 05:50:52.795212 139847977113344 logging_writer.py:48] [56] global_step=56, grad_norm=0.080868, loss=0.363419
I0402 05:50:52.800388 139915587815232 submission.py:139] 56) loss = 0.363, grad_norm = 0.081
I0402 05:50:53.037579 139847826142976 logging_writer.py:48] [57] global_step=57, grad_norm=0.083074, loss=0.383157
I0402 05:50:53.041908 139915587815232 submission.py:139] 57) loss = 0.383, grad_norm = 0.083
I0402 05:50:53.321069 139847977113344 logging_writer.py:48] [58] global_step=58, grad_norm=0.110306, loss=0.283644
I0402 05:50:53.326262 139915587815232 submission.py:139] 58) loss = 0.284, grad_norm = 0.110
I0402 05:50:53.593065 139847826142976 logging_writer.py:48] [59] global_step=59, grad_norm=0.073407, loss=0.315021
I0402 05:50:53.598774 139915587815232 submission.py:139] 59) loss = 0.315, grad_norm = 0.073
I0402 05:50:53.856401 139847977113344 logging_writer.py:48] [60] global_step=60, grad_norm=0.102256, loss=0.287539
I0402 05:50:53.861444 139915587815232 submission.py:139] 60) loss = 0.288, grad_norm = 0.102
I0402 05:50:54.143071 139847826142976 logging_writer.py:48] [61] global_step=61, grad_norm=0.188888, loss=0.354396
I0402 05:50:54.149921 139915587815232 submission.py:139] 61) loss = 0.354, grad_norm = 0.189
I0402 05:50:54.430467 139847977113344 logging_writer.py:48] [62] global_step=62, grad_norm=0.113889, loss=0.311732
I0402 05:50:54.434784 139915587815232 submission.py:139] 62) loss = 0.312, grad_norm = 0.114
I0402 05:50:54.684818 139847826142976 logging_writer.py:48] [63] global_step=63, grad_norm=0.117503, loss=0.379606
I0402 05:50:54.687938 139915587815232 submission.py:139] 63) loss = 0.380, grad_norm = 0.118
I0402 05:50:54.904190 139847977113344 logging_writer.py:48] [64] global_step=64, grad_norm=0.095268, loss=0.345059
I0402 05:50:54.907329 139915587815232 submission.py:139] 64) loss = 0.345, grad_norm = 0.095
I0402 05:50:55.171128 139847826142976 logging_writer.py:48] [65] global_step=65, grad_norm=0.107921, loss=0.338516
I0402 05:50:55.174647 139915587815232 submission.py:139] 65) loss = 0.339, grad_norm = 0.108
I0402 05:50:55.410950 139847977113344 logging_writer.py:48] [66] global_step=66, grad_norm=0.119923, loss=0.284121
I0402 05:50:55.416412 139915587815232 submission.py:139] 66) loss = 0.284, grad_norm = 0.120
I0402 05:50:55.731433 139847826142976 logging_writer.py:48] [67] global_step=67, grad_norm=0.101707, loss=0.299282
I0402 05:50:55.736012 139915587815232 submission.py:139] 67) loss = 0.299, grad_norm = 0.102
I0402 05:50:55.942096 139847977113344 logging_writer.py:48] [68] global_step=68, grad_norm=0.147604, loss=0.377113
I0402 05:50:55.947255 139915587815232 submission.py:139] 68) loss = 0.377, grad_norm = 0.148
I0402 05:50:56.230450 139847826142976 logging_writer.py:48] [69] global_step=69, grad_norm=0.109250, loss=0.371623
I0402 05:50:56.235927 139915587815232 submission.py:139] 69) loss = 0.372, grad_norm = 0.109
I0402 05:50:56.511727 139847977113344 logging_writer.py:48] [70] global_step=70, grad_norm=0.062234, loss=0.299814
I0402 05:50:56.516953 139915587815232 submission.py:139] 70) loss = 0.300, grad_norm = 0.062
I0402 05:50:56.754220 139847826142976 logging_writer.py:48] [71] global_step=71, grad_norm=0.125172, loss=0.276655
I0402 05:50:56.757629 139915587815232 submission.py:139] 71) loss = 0.277, grad_norm = 0.125
I0402 05:50:57.030061 139847977113344 logging_writer.py:48] [72] global_step=72, grad_norm=0.094548, loss=0.418829
I0402 05:50:57.033321 139915587815232 submission.py:139] 72) loss = 0.419, grad_norm = 0.095
I0402 05:50:57.301412 139847826142976 logging_writer.py:48] [73] global_step=73, grad_norm=0.058180, loss=0.309433
I0402 05:50:57.307415 139915587815232 submission.py:139] 73) loss = 0.309, grad_norm = 0.058
I0402 05:50:57.588299 139847977113344 logging_writer.py:48] [74] global_step=74, grad_norm=0.185517, loss=0.445747
I0402 05:50:57.592790 139915587815232 submission.py:139] 74) loss = 0.446, grad_norm = 0.186
I0402 05:50:57.857982 139847826142976 logging_writer.py:48] [75] global_step=75, grad_norm=0.183924, loss=0.307349
I0402 05:50:57.863128 139915587815232 submission.py:139] 75) loss = 0.307, grad_norm = 0.184
I0402 05:50:58.118287 139847977113344 logging_writer.py:48] [76] global_step=76, grad_norm=0.316871, loss=0.259692
I0402 05:50:58.123691 139915587815232 submission.py:139] 76) loss = 0.260, grad_norm = 0.317
I0402 05:50:58.402838 139847826142976 logging_writer.py:48] [77] global_step=77, grad_norm=0.184100, loss=0.370943
I0402 05:50:58.408647 139915587815232 submission.py:139] 77) loss = 0.371, grad_norm = 0.184
I0402 05:50:58.670711 139847977113344 logging_writer.py:48] [78] global_step=78, grad_norm=0.088111, loss=0.270539
I0402 05:50:58.675759 139915587815232 submission.py:139] 78) loss = 0.271, grad_norm = 0.088
I0402 05:50:58.879938 139847826142976 logging_writer.py:48] [79] global_step=79, grad_norm=0.143966, loss=0.265652
I0402 05:50:58.883368 139915587815232 submission.py:139] 79) loss = 0.266, grad_norm = 0.144
I0402 05:50:59.165082 139847977113344 logging_writer.py:48] [80] global_step=80, grad_norm=0.078381, loss=0.316022
I0402 05:50:59.168432 139915587815232 submission.py:139] 80) loss = 0.316, grad_norm = 0.078
I0402 05:50:59.446573 139847826142976 logging_writer.py:48] [81] global_step=81, grad_norm=0.184134, loss=0.295207
I0402 05:50:59.449974 139915587815232 submission.py:139] 81) loss = 0.295, grad_norm = 0.184
I0402 05:50:59.696155 139847977113344 logging_writer.py:48] [82] global_step=82, grad_norm=0.120580, loss=0.268084
I0402 05:50:59.702409 139915587815232 submission.py:139] 82) loss = 0.268, grad_norm = 0.121
I0402 05:50:59.950554 139847826142976 logging_writer.py:48] [83] global_step=83, grad_norm=0.137176, loss=0.292650
I0402 05:50:59.956008 139915587815232 submission.py:139] 83) loss = 0.293, grad_norm = 0.137
I0402 05:51:00.221348 139847977113344 logging_writer.py:48] [84] global_step=84, grad_norm=0.116851, loss=0.247804
I0402 05:51:00.225674 139915587815232 submission.py:139] 84) loss = 0.248, grad_norm = 0.117
I0402 05:51:00.524012 139847826142976 logging_writer.py:48] [85] global_step=85, grad_norm=0.233248, loss=0.274915
I0402 05:51:00.529117 139915587815232 submission.py:139] 85) loss = 0.275, grad_norm = 0.233
I0402 05:51:00.808046 139847977113344 logging_writer.py:48] [86] global_step=86, grad_norm=0.118201, loss=0.388087
I0402 05:51:00.812762 139915587815232 submission.py:139] 86) loss = 0.388, grad_norm = 0.118
I0402 05:51:01.102368 139847826142976 logging_writer.py:48] [87] global_step=87, grad_norm=0.140658, loss=0.265887
I0402 05:51:01.109925 139915587815232 submission.py:139] 87) loss = 0.266, grad_norm = 0.141
I0402 05:51:01.348207 139847977113344 logging_writer.py:48] [88] global_step=88, grad_norm=0.108181, loss=0.303426
I0402 05:51:01.351423 139915587815232 submission.py:139] 88) loss = 0.303, grad_norm = 0.108
I0402 05:51:01.633634 139847826142976 logging_writer.py:48] [89] global_step=89, grad_norm=0.086604, loss=0.308598
I0402 05:51:01.636770 139915587815232 submission.py:139] 89) loss = 0.309, grad_norm = 0.087
I0402 05:51:01.886319 139847977113344 logging_writer.py:48] [90] global_step=90, grad_norm=0.121867, loss=0.277471
I0402 05:51:01.891135 139915587815232 submission.py:139] 90) loss = 0.277, grad_norm = 0.122
I0402 05:51:02.195482 139847826142976 logging_writer.py:48] [91] global_step=91, grad_norm=0.228109, loss=0.318395
I0402 05:51:02.198689 139915587815232 submission.py:139] 91) loss = 0.318, grad_norm = 0.228
I0402 05:51:02.490741 139847977113344 logging_writer.py:48] [92] global_step=92, grad_norm=0.157303, loss=0.253991
I0402 05:51:02.496071 139915587815232 submission.py:139] 92) loss = 0.254, grad_norm = 0.157
I0402 05:51:02.685155 139847826142976 logging_writer.py:48] [93] global_step=93, grad_norm=0.091520, loss=0.319954
I0402 05:51:02.689975 139915587815232 submission.py:139] 93) loss = 0.320, grad_norm = 0.092
I0402 05:51:02.990718 139847977113344 logging_writer.py:48] [94] global_step=94, grad_norm=0.093845, loss=0.283794
I0402 05:51:02.994039 139915587815232 submission.py:139] 94) loss = 0.284, grad_norm = 0.094
I0402 05:51:03.212157 139847826142976 logging_writer.py:48] [95] global_step=95, grad_norm=0.076570, loss=0.243190
I0402 05:51:03.215231 139915587815232 submission.py:139] 95) loss = 0.243, grad_norm = 0.077
I0402 05:51:03.454321 139847977113344 logging_writer.py:48] [96] global_step=96, grad_norm=0.117949, loss=0.312921
I0402 05:51:03.457779 139915587815232 submission.py:139] 96) loss = 0.313, grad_norm = 0.118
I0402 05:51:03.757396 139847826142976 logging_writer.py:48] [97] global_step=97, grad_norm=0.079010, loss=0.242769
I0402 05:51:03.763118 139915587815232 submission.py:139] 97) loss = 0.243, grad_norm = 0.079
I0402 05:51:03.995133 139847977113344 logging_writer.py:48] [98] global_step=98, grad_norm=0.077076, loss=0.252518
I0402 05:51:03.999552 139915587815232 submission.py:139] 98) loss = 0.253, grad_norm = 0.077
I0402 05:51:04.314788 139847826142976 logging_writer.py:48] [99] global_step=99, grad_norm=0.113808, loss=0.277335
I0402 05:51:04.320185 139915587815232 submission.py:139] 99) loss = 0.277, grad_norm = 0.114
I0402 05:51:04.671265 139847977113344 logging_writer.py:48] [100] global_step=100, grad_norm=0.127697, loss=0.288482
I0402 05:51:04.675709 139915587815232 submission.py:139] 100) loss = 0.288, grad_norm = 0.128
I0402 05:52:00.407960 139915587815232 submission_runner.py:373] Before eval at step 309: RAM USED (GB) 86.905602048
I0402 05:52:00.408195 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:52:02.542576 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:52:06.053822 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:52:08.263950 139915587815232 submission_runner.py:382] Time since start: 350.46s, 	Step: 309, 	{'train/ssim': 0.7049869809831891, 'train/loss': 0.30833782468523296, 'validation/ssim': 0.6848600663425014, 'validation/loss': 0.3293337502637873, 'validation/num_examples': 3554, 'test/ssim': 0.7031311358995392, 'test/loss': 0.3307962979527367, 'test/num_examples': 3581}
I0402 05:52:08.264353 139915587815232 submission_runner.py:396] After eval at step 309: RAM USED (GB) 87.931949056
I0402 05:52:08.277136 139847826142976 logging_writer.py:48] [309] global_step=309, preemption_count=0, score=123.798127, test/loss=0.330796, test/num_examples=3581, test/ssim=0.703131, total_duration=350.460194, train/loss=0.308338, train/ssim=0.704987, validation/loss=0.329334, validation/num_examples=3554, validation/ssim=0.684860
I0402 05:52:08.385586 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_309.
I0402 05:52:08.386553 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 309: RAM USED (GB) 87.94796032
I0402 05:53:15.062727 139847977113344 logging_writer.py:48] [500] global_step=500, grad_norm=0.517478, loss=0.254942
I0402 05:53:15.067633 139915587815232 submission.py:139] 500) loss = 0.255, grad_norm = 0.517
I0402 05:53:28.484088 139915587815232 submission_runner.py:373] Before eval at step 535: RAM USED (GB) 105.254281216
I0402 05:53:28.484278 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:53:30.601189 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:53:33.562160 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:53:35.725658 139915587815232 submission_runner.py:382] Time since start: 438.53s, 	Step: 535, 	{'train/ssim': 0.7054862976074219, 'train/loss': 0.30438266481672016, 'validation/ssim': 0.6896361961214477, 'validation/loss': 0.32253222876072735, 'validation/num_examples': 3554, 'test/ssim': 0.7057318028309132, 'test/loss': 0.3247646064406765, 'test/num_examples': 3581}
I0402 05:53:35.726116 139915587815232 submission_runner.py:396] After eval at step 535: RAM USED (GB) 106.454536192
I0402 05:53:35.736377 139847826142976 logging_writer.py:48] [535] global_step=535, preemption_count=0, score=199.509997, test/loss=0.324765, test/num_examples=3581, test/ssim=0.705732, total_duration=438.527573, train/loss=0.304383, train/ssim=0.705486, validation/loss=0.322532, validation/num_examples=3554, validation/ssim=0.689636
I0402 05:53:35.844918 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_535.
I0402 05:53:35.846543 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 535: RAM USED (GB) 106.459246592
I0402 05:54:56.081879 139915587815232 submission_runner.py:373] Before eval at step 768: RAM USED (GB) 123.999133696
I0402 05:54:56.082073 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:54:58.178062 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:55:00.955665 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:55:03.166945 139915587815232 submission_runner.py:382] Time since start: 526.13s, 	Step: 768, 	{'train/ssim': 0.7192912101745605, 'train/loss': 0.29730258669172016, 'validation/ssim': 0.6994507316799733, 'validation/loss': 0.31704346115292625, 'validation/num_examples': 3554, 'test/ssim': 0.7167340842947152, 'test/loss': 0.3189693515908615, 'test/num_examples': 3581}
I0402 05:55:03.167291 139915587815232 submission_runner.py:396] After eval at step 768: RAM USED (GB) 125.194481664
I0402 05:55:03.176689 139847977113344 logging_writer.py:48] [768] global_step=768, preemption_count=0, score=275.234271, test/loss=0.318969, test/num_examples=3581, test/ssim=0.716734, total_duration=526.128688, train/loss=0.297303, train/ssim=0.719291, validation/loss=0.317043, validation/num_examples=3554, validation/ssim=0.699451
I0402 05:55:03.293282 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_768.
I0402 05:55:03.293825 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 768: RAM USED (GB) 125.213282304
I0402 05:56:21.432471 139847826142976 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.474606, loss=0.296787
I0402 05:56:21.440939 139915587815232 submission.py:139] 1000) loss = 0.297, grad_norm = 0.475
I0402 05:56:23.554410 139915587815232 submission_runner.py:373] Before eval at step 1009: RAM USED (GB) 142.813663232
I0402 05:56:23.554642 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:56:25.638505 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:56:28.060261 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:56:30.155880 139915587815232 submission_runner.py:382] Time since start: 613.60s, 	Step: 1009, 	{'train/ssim': 0.7121045248849052, 'train/loss': 0.29122304916381836, 'validation/ssim': 0.6948699006621061, 'validation/loss': 0.31016029611221513, 'validation/num_examples': 3554, 'test/ssim': 0.7124002984501536, 'test/loss': 0.31161424881318067, 'test/num_examples': 3581}
I0402 05:56:30.156273 139915587815232 submission_runner.py:396] After eval at step 1009: RAM USED (GB) 142.881304576
I0402 05:56:30.164231 139847977113344 logging_writer.py:48] [1009] global_step=1009, preemption_count=0, score=350.986705, test/loss=0.311614, test/num_examples=3581, test/ssim=0.712400, total_duration=613.599641, train/loss=0.291223, train/ssim=0.712105, validation/loss=0.310160, validation/num_examples=3554, validation/ssim=0.694870
I0402 05:56:30.265200 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1009.
I0402 05:56:30.265694 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 1009: RAM USED (GB) 142.890151936
I0402 05:57:50.528845 139915587815232 submission_runner.py:373] Before eval at step 1323: RAM USED (GB) 143.213867008
I0402 05:57:50.529063 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:57:52.609304 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:57:54.789063 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:57:56.886196 139915587815232 submission_runner.py:382] Time since start: 700.57s, 	Step: 1323, 	{'train/ssim': 0.7158542360578265, 'train/loss': 0.2968353543962751, 'validation/ssim': 0.6974837990644345, 'validation/loss': 0.31432480347847497, 'validation/num_examples': 3554, 'test/ssim': 0.7136976321427325, 'test/loss': 0.31728552440397234, 'test/num_examples': 3581}
I0402 05:57:56.886543 139915587815232 submission_runner.py:396] After eval at step 1323: RAM USED (GB) 143.218577408
I0402 05:57:56.894619 139847826142976 logging_writer.py:48] [1323] global_step=1323, preemption_count=0, score=423.957286, test/loss=0.317286, test/num_examples=3581, test/ssim=0.713698, total_duration=700.574660, train/loss=0.296835, train/ssim=0.715854, validation/loss=0.314325, validation/num_examples=3554, validation/ssim=0.697484
I0402 05:57:56.991003 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1323.
I0402 05:57:56.991509 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 1323: RAM USED (GB) 143.218036736
I0402 05:58:41.462173 139847977113344 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.520456, loss=0.301753
I0402 05:58:41.466197 139915587815232 submission.py:139] 1500) loss = 0.302, grad_norm = 0.520
I0402 05:59:17.257768 139915587815232 submission_runner.py:373] Before eval at step 1637: RAM USED (GB) 143.266316288
I0402 05:59:17.258081 139915587815232 spec.py:298] Evaluating on the training split.
I0402 05:59:19.332170 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 05:59:21.539358 139915587815232 spec.py:326] Evaluating on the test split.
I0402 05:59:23.632478 139915587815232 submission_runner.py:382] Time since start: 787.30s, 	Step: 1637, 	{'train/ssim': 0.6882866450718471, 'train/loss': 0.3352555547441755, 'validation/ssim': 0.6735762218846723, 'validation/loss': 0.35131908066834905, 'validation/num_examples': 3554, 'test/ssim': 0.6899931523361142, 'test/loss': 0.35434598264931233, 'test/num_examples': 3581}
I0402 05:59:23.632839 139915587815232 submission_runner.py:396] After eval at step 1637: RAM USED (GB) 143.263080448
I0402 05:59:23.640824 139847826142976 logging_writer.py:48] [1637] global_step=1637, preemption_count=0, score=496.982018, test/loss=0.354346, test/num_examples=3581, test/ssim=0.689993, total_duration=787.299356, train/loss=0.335256, train/ssim=0.688287, validation/loss=0.351319, validation/num_examples=3554, validation/ssim=0.673576
I0402 05:59:23.736322 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1637.
I0402 05:59:23.736854 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 1637: RAM USED (GB) 143.26360064
I0402 06:00:43.862579 139915587815232 submission_runner.py:373] Before eval at step 1950: RAM USED (GB) 143.29470976
I0402 06:00:43.862796 139915587815232 spec.py:298] Evaluating on the training split.
I0402 06:00:45.911912 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 06:00:48.078557 139915587815232 spec.py:326] Evaluating on the test split.
I0402 06:00:50.168824 139915587815232 submission_runner.py:382] Time since start: 873.91s, 	Step: 1950, 	{'train/ssim': 0.6737407956804548, 'train/loss': 0.3695085048675537, 'validation/ssim': 0.6609158749384496, 'validation/loss': 0.383117984906971, 'validation/num_examples': 3554, 'test/ssim': 0.6768030818032672, 'test/loss': 0.3870068304833671, 'test/num_examples': 3581}
I0402 06:00:50.169203 139915587815232 submission_runner.py:396] After eval at step 1950: RAM USED (GB) 143.315456
I0402 06:00:50.177386 139847977113344 logging_writer.py:48] [1950] global_step=1950, preemption_count=0, score=569.863750, test/loss=0.387007, test/num_examples=3581, test/ssim=0.676803, total_duration=873.905332, train/loss=0.369509, train/ssim=0.673741, validation/loss=0.383118, validation/num_examples=3554, validation/ssim=0.660916
I0402 06:00:50.272286 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_1950.
I0402 06:00:50.272775 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 1950: RAM USED (GB) 143.314878464
I0402 06:01:01.257829 139847826142976 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.381070, loss=0.330650
I0402 06:01:01.261093 139915587815232 submission.py:139] 2000) loss = 0.331, grad_norm = 0.381
I0402 06:02:10.434026 139915587815232 submission_runner.py:373] Before eval at step 2259: RAM USED (GB) 143.41920768
I0402 06:02:10.434295 139915587815232 spec.py:298] Evaluating on the training split.
I0402 06:02:12.493278 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 06:02:14.696780 139915587815232 spec.py:326] Evaluating on the test split.
I0402 06:02:16.784885 139915587815232 submission_runner.py:382] Time since start: 960.48s, 	Step: 2259, 	{'train/ssim': 0.5493340492248535, 'train/loss': 0.37208168847220285, 'validation/ssim': 0.5564254048035664, 'validation/loss': 0.3836455251235404, 'validation/num_examples': 3554, 'test/ssim': 0.5732363924837685, 'test/loss': 0.38445876184637673, 'test/num_examples': 3581}
I0402 06:02:16.785281 139915587815232 submission_runner.py:396] After eval at step 2259: RAM USED (GB) 143.419392
I0402 06:02:16.793648 139847977113344 logging_writer.py:48] [2259] global_step=2259, preemption_count=0, score=642.845290, test/loss=0.384459, test/num_examples=3581, test/ssim=0.573236, total_duration=960.480202, train/loss=0.372082, train/ssim=0.549334, validation/loss=0.383646, validation/num_examples=3554, validation/ssim=0.556425
I0402 06:02:16.889551 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2259.
I0402 06:02:16.890095 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 2259: RAM USED (GB) 143.420465152
I0402 06:03:18.037694 139847826142976 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.627448, loss=0.328791
I0402 06:03:18.041047 139915587815232 submission.py:139] 2500) loss = 0.329, grad_norm = 0.627
I0402 06:03:37.110604 139915587815232 submission_runner.py:373] Before eval at step 2574: RAM USED (GB) 143.431581696
I0402 06:03:37.110802 139915587815232 spec.py:298] Evaluating on the training split.
I0402 06:03:39.178892 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 06:03:41.346539 139915587815232 spec.py:326] Evaluating on the test split.
I0402 06:03:43.437241 139915587815232 submission_runner.py:382] Time since start: 1047.15s, 	Step: 2574, 	{'train/ssim': 0.7106575965881348, 'train/loss': 0.2869323321751186, 'validation/ssim': 0.6952814500386888, 'validation/loss': 0.3047509394674135, 'validation/num_examples': 3554, 'test/ssim': 0.7116994423694498, 'test/loss': 0.3062192250549777, 'test/num_examples': 3581}
I0402 06:03:43.437609 139915587815232 submission_runner.py:396] After eval at step 2574: RAM USED (GB) 143.418150912
I0402 06:03:43.445970 139847977113344 logging_writer.py:48] [2574] global_step=2574, preemption_count=0, score=715.803195, test/loss=0.306219, test/num_examples=3581, test/ssim=0.711699, total_duration=1047.152520, train/loss=0.286932, train/ssim=0.710658, validation/loss=0.304751, validation/num_examples=3554, validation/ssim=0.695281
I0402 06:03:43.540853 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2574.
I0402 06:03:43.541557 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 2574: RAM USED (GB) 143.418159104
I0402 06:04:18.181479 139915587815232 submission_runner.py:373] Before eval at step 2714: RAM USED (GB) 143.430393856
I0402 06:04:18.181673 139915587815232 spec.py:298] Evaluating on the training split.
I0402 06:04:20.192396 139915587815232 spec.py:310] Evaluating on the validation split.
I0402 06:04:22.287424 139915587815232 spec.py:326] Evaluating on the test split.
I0402 06:04:24.358440 139915587815232 submission_runner.py:382] Time since start: 1088.22s, 	Step: 2714, 	{'train/ssim': 0.7069330896650042, 'train/loss': 0.2925638130732945, 'validation/ssim': 0.6924699172587225, 'validation/loss': 0.30973153873716236, 'validation/num_examples': 3554, 'test/ssim': 0.7084216448355907, 'test/loss': 0.31133533809079167, 'test/num_examples': 3581}
I0402 06:04:24.358845 139915587815232 submission_runner.py:396] After eval at step 2714: RAM USED (GB) 143.432998912
I0402 06:04:24.366914 139847826142976 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=747.196290, test/loss=0.311335, test/num_examples=3581, test/ssim=0.708422, total_duration=1088.224530, train/loss=0.292564, train/ssim=0.706933, validation/loss=0.309732, validation/num_examples=3554, validation/ssim=0.692470
I0402 06:04:24.463909 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2714.
I0402 06:04:24.464469 139915587815232 submission_runner.py:416] After logging and checkpointing eval at step 2714: RAM USED (GB) 143.432454144
I0402 06:04:24.472210 139847977113344 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=747.196290
I0402 06:04:24.644062 139915587815232 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/fastmri_pytorch/trial_1/checkpoint_2714.
I0402 06:04:25.834321 139915587815232 submission_runner.py:550] Tuning trial 1/1
I0402 06:04:25.834547 139915587815232 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0402 06:04:25.839993 139915587815232 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ssim': 0.2415801456996373, 'train/loss': 0.8037490844726562, 'validation/ssim': 0.2333616000182453, 'validation/loss': 0.8096762395804024, 'validation/num_examples': 3554, 'test/ssim': 0.25698948838596936, 'test/loss': 0.8089340197177813, 'test/num_examples': 3581, 'score': 47.79495692253113, 'total_duration': 47.796865940093994, 'global_step': 1, 'preemption_count': 0}), (309, {'train/ssim': 0.7049869809831891, 'train/loss': 0.30833782468523296, 'validation/ssim': 0.6848600663425014, 'validation/loss': 0.3293337502637873, 'validation/num_examples': 3554, 'test/ssim': 0.7031311358995392, 'test/loss': 0.3307962979527367, 'test/num_examples': 3581, 'score': 123.79812717437744, 'total_duration': 350.4601936340332, 'global_step': 309, 'preemption_count': 0}), (535, {'train/ssim': 0.7054862976074219, 'train/loss': 0.30438266481672016, 'validation/ssim': 0.6896361961214477, 'validation/loss': 0.32253222876072735, 'validation/num_examples': 3554, 'test/ssim': 0.7057318028309132, 'test/loss': 0.3247646064406765, 'test/num_examples': 3581, 'score': 199.5099971294403, 'total_duration': 438.52757263183594, 'global_step': 535, 'preemption_count': 0}), (768, {'train/ssim': 0.7192912101745605, 'train/loss': 0.29730258669172016, 'validation/ssim': 0.6994507316799733, 'validation/loss': 0.31704346115292625, 'validation/num_examples': 3554, 'test/ssim': 0.7167340842947152, 'test/loss': 0.3189693515908615, 'test/num_examples': 3581, 'score': 275.2342712879181, 'total_duration': 526.1286878585815, 'global_step': 768, 'preemption_count': 0}), (1009, {'train/ssim': 0.7121045248849052, 'train/loss': 0.29122304916381836, 'validation/ssim': 0.6948699006621061, 'validation/loss': 0.31016029611221513, 'validation/num_examples': 3554, 'test/ssim': 0.7124002984501536, 'test/loss': 0.31161424881318067, 'test/num_examples': 3581, 'score': 350.98670530319214, 'total_duration': 613.5996406078339, 'global_step': 1009, 'preemption_count': 0}), (1323, {'train/ssim': 0.7158542360578265, 'train/loss': 0.2968353543962751, 'validation/ssim': 0.6974837990644345, 'validation/loss': 0.31432480347847497, 'validation/num_examples': 3554, 'test/ssim': 0.7136976321427325, 'test/loss': 0.31728552440397234, 'test/num_examples': 3581, 'score': 423.9572858810425, 'total_duration': 700.5746598243713, 'global_step': 1323, 'preemption_count': 0}), (1637, {'train/ssim': 0.6882866450718471, 'train/loss': 0.3352555547441755, 'validation/ssim': 0.6735762218846723, 'validation/loss': 0.35131908066834905, 'validation/num_examples': 3554, 'test/ssim': 0.6899931523361142, 'test/loss': 0.35434598264931233, 'test/num_examples': 3581, 'score': 496.9820182323456, 'total_duration': 787.299355506897, 'global_step': 1637, 'preemption_count': 0}), (1950, {'train/ssim': 0.6737407956804548, 'train/loss': 0.3695085048675537, 'validation/ssim': 0.6609158749384496, 'validation/loss': 0.383117984906971, 'validation/num_examples': 3554, 'test/ssim': 0.6768030818032672, 'test/loss': 0.3870068304833671, 'test/num_examples': 3581, 'score': 569.8637504577637, 'total_duration': 873.9053316116333, 'global_step': 1950, 'preemption_count': 0}), (2259, {'train/ssim': 0.5493340492248535, 'train/loss': 0.37208168847220285, 'validation/ssim': 0.5564254048035664, 'validation/loss': 0.3836455251235404, 'validation/num_examples': 3554, 'test/ssim': 0.5732363924837685, 'test/loss': 0.38445876184637673, 'test/num_examples': 3581, 'score': 642.845290184021, 'total_duration': 960.4802017211914, 'global_step': 2259, 'preemption_count': 0}), (2574, {'train/ssim': 0.7106575965881348, 'train/loss': 0.2869323321751186, 'validation/ssim': 0.6952814500386888, 'validation/loss': 0.3047509394674135, 'validation/num_examples': 3554, 'test/ssim': 0.7116994423694498, 'test/loss': 0.3062192250549777, 'test/num_examples': 3581, 'score': 715.8031947612762, 'total_duration': 1047.1525197029114, 'global_step': 2574, 'preemption_count': 0}), (2714, {'train/ssim': 0.7069330896650042, 'train/loss': 0.2925638130732945, 'validation/ssim': 0.6924699172587225, 'validation/loss': 0.30973153873716236, 'validation/num_examples': 3554, 'test/ssim': 0.7084216448355907, 'test/loss': 0.31133533809079167, 'test/num_examples': 3581, 'score': 747.1962895393372, 'total_duration': 1088.2245297431946, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0402 06:04:25.840118 139915587815232 submission_runner.py:553] Timing: 747.1962895393372
I0402 06:04:25.840161 139915587815232 submission_runner.py:554] ====================
I0402 06:04:25.840252 139915587815232 submission_runner.py:613] Final fastmri score: 747.1962895393372
