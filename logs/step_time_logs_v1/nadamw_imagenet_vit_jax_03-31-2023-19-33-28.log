I0331 19:33:48.653794 140253182756672 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/imagenet_vit_jax.
I0331 19:33:48.707097 140253182756672 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 19:33:49.642524 140253182756672 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0331 19:33:49.643242 140253182756672 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 19:33:49.648039 140253182756672 submission_runner.py:511] Using RNG seed 287064203
I0331 19:33:51.888334 140253182756672 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 19:33:51.888522 140253182756672 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1.
I0331 19:33:51.888711 140253182756672 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/hparams.json.
I0331 19:33:52.020433 140253182756672 submission_runner.py:230] Starting train once: RAM USED (GB) 4.208775168
I0331 19:33:52.020574 140253182756672 submission_runner.py:231] Initializing dataset.
I0331 19:33:52.031457 140253182756672 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:33:52.039113 140253182756672 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 19:33:52.039237 140253182756672 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 19:33:52.271306 140253182756672 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:33:58.833246 140253182756672 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.37704704
I0331 19:33:58.833454 140253182756672 submission_runner.py:240] Initializing model.
I0331 19:34:09.816331 140253182756672 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.386256896
I0331 19:34:09.816519 140253182756672 submission_runner.py:252] Initializing optimizer.
I0331 19:34:10.449657 140253182756672 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.386490368
I0331 19:34:10.449835 140253182756672 submission_runner.py:261] Initializing metrics bundle.
I0331 19:34:10.449884 140253182756672 submission_runner.py:276] Initializing checkpoint and logger.
I0331 19:34:10.450856 140253182756672 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0331 19:34:11.279902 140253182756672 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/meta_data_0.json.
I0331 19:34:11.280887 140253182756672 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/flags_0.json.
I0331 19:34:11.285296 140253182756672 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.385417216
I0331 19:34:11.285474 140253182756672 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.385417216
I0331 19:34:11.285539 140253182756672 submission_runner.py:313] Starting training loop.
I0331 19:34:14.439585 140253182756672 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 13.687689216
I0331 19:35:01.775479 140074624730880 logging_writer.py:48] [0] global_step=0, grad_norm=0.3254491686820984, loss=6.907756805419922
I0331 19:35:01.792709 140253182756672 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 46.54465024
I0331 19:35:01.793050 140253182756672 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 46.54465024
I0331 19:35:01.793147 140253182756672 spec.py:298] Evaluating on the training split.
I0331 19:35:01.800257 140253182756672 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:01.806877 140253182756672 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 19:35:01.807001 140253182756672 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 19:35:01.876766 140253182756672 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:20.379569 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 19:35:20.385501 140253182756672 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:20.394648 140253182756672 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 19:35:20.394909 140253182756672 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 19:35:20.446717 140253182756672 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 19:35:38.952782 140253182756672 spec.py:326] Evaluating on the test split.
I0331 19:35:38.960155 140253182756672 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 19:35:38.964990 140253182756672 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0331 19:35:38.997395 140253182756672 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 19:35:50.040166 140253182756672 submission_runner.py:382] Time since start: 50.51s, 	Step: 1, 	{'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000}
I0331 19:35:50.040688 140253182756672 submission_runner.py:396] After eval at step 1: RAM USED (GB) 105.994313728
I0331 19:35:50.050402 140015497623296 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=50.425355, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=50.507508, train/accuracy=0.000898, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0331 19:35:50.292223 140253182756672 checkpoints.py:356] Saving checkpoint at step: 1
I0331 19:35:50.944125 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_1
I0331 19:35:50.945285 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_1.
I0331 19:35:50.947836 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 106.206642176
I0331 19:35:50.951759 140253182756672 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 106.206642176
I0331 19:36:11.098496 140253182756672 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 105.604091904
I0331 19:36:49.854465 140072573695744 logging_writer.py:48] [100] global_step=100, grad_norm=0.3814498782157898, loss=6.899482727050781
I0331 19:37:29.231271 140072582088448 logging_writer.py:48] [200] global_step=200, grad_norm=0.4913908839225769, loss=6.846991539001465
I0331 19:38:08.634784 140072573695744 logging_writer.py:48] [300] global_step=300, grad_norm=0.5429286360740662, loss=6.789967060089111
I0331 19:38:48.471734 140072582088448 logging_writer.py:48] [400] global_step=400, grad_norm=1.0972850322723389, loss=6.718220233917236
I0331 19:39:28.598690 140072573695744 logging_writer.py:48] [500] global_step=500, grad_norm=0.8111585974693298, loss=6.630768775939941
I0331 19:40:08.660733 140072582088448 logging_writer.py:48] [600] global_step=600, grad_norm=0.8711333274841309, loss=6.699417591094971
I0331 19:40:48.668253 140072573695744 logging_writer.py:48] [700] global_step=700, grad_norm=2.105809450149536, loss=6.528368949890137
I0331 19:41:28.646238 140072582088448 logging_writer.py:48] [800] global_step=800, grad_norm=1.9858579635620117, loss=6.4591064453125
I0331 19:42:08.568366 140072573695744 logging_writer.py:48] [900] global_step=900, grad_norm=1.1799695491790771, loss=6.505389213562012
I0331 19:42:48.594441 140072582088448 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.163851022720337, loss=6.322788715362549
I0331 19:42:51.108839 140253182756672 submission_runner.py:373] Before eval at step 1008: RAM USED (GB) 79.816966144
I0331 19:42:51.109127 140253182756672 spec.py:298] Evaluating on the training split.
I0331 19:43:02.403128 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 19:43:08.878012 140253182756672 spec.py:326] Evaluating on the test split.
I0331 19:43:10.894873 140253182756672 submission_runner.py:382] Time since start: 519.82s, 	Step: 1008, 	{'train/accuracy': 0.029355468228459358, 'train/loss': 6.022684097290039, 'validation/accuracy': 0.027300000190734863, 'validation/loss': 6.048311233520508, 'validation/num_examples': 50000, 'test/accuracy': 0.022200001403689384, 'test/loss': 6.135765075683594, 'test/num_examples': 10000}
I0331 19:43:10.895787 140253182756672 submission_runner.py:396] After eval at step 1008: RAM USED (GB) 85.735952384
I0331 19:43:10.921680 140015757666048 logging_writer.py:48] [1008] global_step=1008, preemption_count=0, score=462.336784, test/accuracy=0.022200, test/loss=6.135765, test/num_examples=10000, total_duration=519.818274, train/accuracy=0.029355, train/loss=6.022684, validation/accuracy=0.027300, validation/loss=6.048311, validation/num_examples=50000
I0331 19:43:11.454497 140253182756672 checkpoints.py:356] Saving checkpoint at step: 1008
I0331 19:43:13.290287 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_1008
I0331 19:43:13.310522 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_1008.
I0331 19:43:13.313749 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 1008: RAM USED (GB) 95.878950912
I0331 19:43:50.017287 140015766058752 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4678453207015991, loss=6.240933895111084
I0331 19:44:29.495461 140077522999040 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2007778882980347, loss=6.670886993408203
I0331 19:45:09.039836 140015766058752 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.1712414026260376, loss=6.632608890533447
I0331 19:45:49.226255 140077522999040 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2748476266860962, loss=6.168931007385254
I0331 19:46:29.197266 140015766058752 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.266945481300354, loss=6.626297950744629
I0331 19:47:09.546097 140077522999040 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.4526658058166504, loss=5.996925354003906
I0331 19:47:49.676029 140015766058752 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.3665215969085693, loss=6.076621055603027
I0331 19:48:29.705717 140077522999040 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0257680416107178, loss=6.227319240570068
I0331 19:49:09.774040 140015766058752 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1218558549880981, loss=5.953948974609375
I0331 19:49:49.812192 140077522999040 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.13868248462677, loss=5.97858190536499
I0331 19:50:13.565112 140253182756672 submission_runner.py:373] Before eval at step 2061: RAM USED (GB) 87.004409856
I0331 19:50:13.565360 140253182756672 spec.py:298] Evaluating on the training split.
I0331 19:50:25.016871 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 19:50:31.848062 140253182756672 spec.py:326] Evaluating on the test split.
I0331 19:50:33.533504 140253182756672 submission_runner.py:382] Time since start: 962.28s, 	Step: 2061, 	{'train/accuracy': 0.07630859315395355, 'train/loss': 5.272983074188232, 'validation/accuracy': 0.07401999831199646, 'validation/loss': 5.304529666900635, 'validation/num_examples': 50000, 'test/accuracy': 0.05700000375509262, 'test/loss': 5.511024475097656, 'test/num_examples': 10000}
I0331 19:50:33.533923 140253182756672 submission_runner.py:396] After eval at step 2061: RAM USED (GB) 90.180755456
I0331 19:50:33.547301 140015766058752 logging_writer.py:48] [2061] global_step=2061, preemption_count=0, score=874.186071, test/accuracy=0.057000, test/loss=5.511024, test/num_examples=10000, total_duration=962.276794, train/accuracy=0.076309, train/loss=5.272983, validation/accuracy=0.074020, validation/loss=5.304530, validation/num_examples=50000
I0331 19:50:34.381600 140253182756672 checkpoints.py:356] Saving checkpoint at step: 2061
I0331 19:50:37.437043 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_2061
I0331 19:50:37.458668 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_2061.
I0331 19:50:37.462041 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 2061: RAM USED (GB) 101.917908992
I0331 19:50:53.297727 140077522999040 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.5796047449111938, loss=5.960019111633301
I0331 19:51:32.623056 140077489428224 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0425831079483032, loss=5.82613468170166
I0331 19:52:12.167828 140077522999040 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9991944432258606, loss=6.0954790115356445
I0331 19:52:51.931513 140077489428224 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.949552595615387, loss=6.476367950439453
I0331 19:53:32.173732 140077522999040 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2176262140274048, loss=5.789371490478516
I0331 19:54:12.247138 140077489428224 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.1383872032165527, loss=5.919849872589111
I0331 19:54:52.312897 140077522999040 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2764308452606201, loss=5.699790954589844
I0331 19:55:32.692796 140077489428224 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9883198738098145, loss=6.543582916259766
I0331 19:56:12.822640 140077522999040 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2443931102752686, loss=5.6593241691589355
I0331 19:56:52.961743 140077489428224 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4055535793304443, loss=5.648556709289551
I0331 19:57:33.222216 140077522999040 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.525195598602295, loss=5.514430999755859
I0331 19:57:37.737819 140253182756672 submission_runner.py:373] Before eval at step 3113: RAM USED (GB) 93.131083776
I0331 19:57:37.738069 140253182756672 spec.py:298] Evaluating on the training split.
I0331 19:57:49.303704 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 19:57:57.214109 140253182756672 spec.py:326] Evaluating on the test split.
I0331 19:57:58.867223 140253182756672 submission_runner.py:382] Time since start: 1406.45s, 	Step: 3113, 	{'train/accuracy': 0.1457226574420929, 'train/loss': 4.625796318054199, 'validation/accuracy': 0.13492000102996826, 'validation/loss': 4.70241641998291, 'validation/num_examples': 50000, 'test/accuracy': 0.10190000385046005, 'test/loss': 5.014127731323242, 'test/num_examples': 10000}
I0331 19:57:58.867727 140253182756672 submission_runner.py:396] After eval at step 3113: RAM USED (GB) 95.940186112
I0331 19:57:58.879480 140077489428224 logging_writer.py:48] [3113] global_step=3113, preemption_count=0, score=1286.274662, test/accuracy=0.101900, test/loss=5.014128, test/num_examples=10000, total_duration=1406.447771, train/accuracy=0.145723, train/loss=4.625796, validation/accuracy=0.134920, validation/loss=4.702416, validation/num_examples=50000
I0331 19:57:58.994640 140253182756672 checkpoints.py:356] Saving checkpoint at step: 3113
I0331 19:58:01.099360 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_3113
I0331 19:58:01.119955 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_3113.
I0331 19:58:01.123321 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 3113: RAM USED (GB) 105.041698816
I0331 19:58:36.767218 140077522999040 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.109620451927185, loss=5.4423017501831055
I0331 19:59:16.455268 140077195847424 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0496370792388916, loss=5.531949520111084
I0331 19:59:56.314334 140077522999040 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8589040040969849, loss=6.455373287200928
I0331 20:00:36.399793 140077195847424 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0043152570724487, loss=5.70281982421875
I0331 20:01:16.606322 140077522999040 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.0094197988510132, loss=5.9151291847229
I0331 20:01:56.645045 140077195847424 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.088820457458496, loss=6.385383129119873
I0331 20:02:36.870584 140077522999040 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0075323581695557, loss=5.9955291748046875
I0331 20:03:16.989554 140077195847424 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.3627668619155884, loss=5.223736763000488
I0331 20:03:57.020845 140077522999040 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.270508885383606, loss=5.402325630187988
I0331 20:04:37.482644 140077195847424 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.1273113489151, loss=5.269645690917969
I0331 20:05:01.172313 140253182756672 submission_runner.py:373] Before eval at step 4161: RAM USED (GB) 99.499241472
I0331 20:05:01.172645 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:05:13.832196 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:05:23.051905 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:05:24.686755 140253182756672 submission_runner.py:382] Time since start: 1849.88s, 	Step: 4161, 	{'train/accuracy': 0.2064257711172104, 'train/loss': 4.119557857513428, 'validation/accuracy': 0.19189999997615814, 'validation/loss': 4.223005294799805, 'validation/num_examples': 50000, 'test/accuracy': 0.14580000936985016, 'test/loss': 4.582226753234863, 'test/num_examples': 10000}
I0331 20:05:24.687342 140253182756672 submission_runner.py:396] After eval at step 4161: RAM USED (GB) 106.503168
I0331 20:05:24.699424 140077522999040 logging_writer.py:48] [4161] global_step=4161, preemption_count=0, score=1697.051493, test/accuracy=0.145800, test/loss=4.582227, test/num_examples=10000, total_duration=1849.882800, train/accuracy=0.206426, train/loss=4.119558, validation/accuracy=0.191900, validation/loss=4.223005, validation/num_examples=50000
I0331 20:05:24.845426 140253182756672 checkpoints.py:356] Saving checkpoint at step: 4161
I0331 20:05:26.137296 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_4161
I0331 20:05:26.153129 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_4161.
I0331 20:05:26.156133 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 4161: RAM USED (GB) 112.719499264
I0331 20:05:42.020383 140077195847424 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8071248531341553, loss=6.3602471351623535
I0331 20:06:21.539452 140077481035520 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.3465919494628906, loss=5.299516677856445
I0331 20:07:01.078908 140077195847424 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.1647647619247437, loss=5.132011890411377
I0331 20:07:41.426203 140077481035520 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.505371332168579, loss=5.142324924468994
I0331 20:08:21.881356 140077195847424 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.1320160627365112, loss=5.088352203369141
I0331 20:09:02.028133 140077481035520 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1975995302200317, loss=5.050877571105957
I0331 20:09:42.155375 140077195847424 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.1708720922470093, loss=5.562875270843506
I0331 20:10:22.414296 140077481035520 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8163954615592957, loss=5.928118705749512
I0331 20:11:02.706018 140077195847424 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.933194637298584, loss=5.8196868896484375
I0331 20:11:42.924695 140077481035520 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8731347322463989, loss=6.092194080352783
I0331 20:12:23.010048 140077195847424 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.4208800792694092, loss=4.910587787628174
I0331 20:12:26.355629 140253182756672 submission_runner.py:373] Before eval at step 5210: RAM USED (GB) 104.707403776
I0331 20:12:26.355923 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:12:40.941086 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:12:50.791313 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:12:52.439251 140253182756672 submission_runner.py:382] Time since start: 2295.07s, 	Step: 5210, 	{'train/accuracy': 0.2510351538658142, 'train/loss': 3.789574146270752, 'validation/accuracy': 0.23323999345302582, 'validation/loss': 3.8981120586395264, 'validation/num_examples': 50000, 'test/accuracy': 0.17900000512599945, 'test/loss': 4.3219780921936035, 'test/num_examples': 10000}
I0331 20:12:52.439734 140253182756672 submission_runner.py:396] After eval at step 5210: RAM USED (GB) 113.498247168
I0331 20:12:52.448179 140077481035520 logging_writer.py:48] [5210] global_step=5210, preemption_count=0, score=2108.523406, test/accuracy=0.179000, test/loss=4.321978, test/num_examples=10000, total_duration=2295.065243, train/accuracy=0.251035, train/loss=3.789574, validation/accuracy=0.233240, validation/loss=3.898112, validation/num_examples=50000
I0331 20:12:52.568991 140253182756672 checkpoints.py:356] Saving checkpoint at step: 5210
I0331 20:12:53.689742 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_5210
I0331 20:12:53.709243 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_5210.
I0331 20:12:53.712110 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 5210: RAM USED (GB) 118.875291648
I0331 20:13:29.602721 140077195847424 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.0798126459121704, loss=5.038783073425293
I0331 20:14:09.301795 140077464250112 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8654571771621704, loss=5.0639495849609375
I0331 20:14:49.392682 140077195847424 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.182478427886963, loss=4.816686630249023
I0331 20:15:30.461440 140077464250112 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.0418002605438232, loss=5.014434337615967
I0331 20:16:11.055828 140077195847424 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.0500601530075073, loss=5.054418087005615
I0331 20:16:51.038445 140077464250112 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.965868353843689, loss=6.3326544761657715
I0331 20:17:31.186942 140077195847424 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1443105936050415, loss=4.726393699645996
I0331 20:18:11.441006 140077464250112 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8405551910400391, loss=5.574859619140625
I0331 20:18:51.571284 140077195847424 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9444618821144104, loss=4.989703178405762
I0331 20:19:32.287109 140077464250112 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9536473155021667, loss=4.676646709442139
I0331 20:19:53.984865 140253182756672 submission_runner.py:373] Before eval at step 6256: RAM USED (GB) 111.243399168
I0331 20:19:53.985149 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:20:08.653834 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:20:17.675887 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:20:19.340063 140253182756672 submission_runner.py:382] Time since start: 2742.70s, 	Step: 6256, 	{'train/accuracy': 0.3115234375, 'train/loss': 3.348360300064087, 'validation/accuracy': 0.29071998596191406, 'validation/loss': 3.47202467918396, 'validation/num_examples': 50000, 'test/accuracy': 0.22220000624656677, 'test/loss': 3.9722795486450195, 'test/num_examples': 10000}
I0331 20:20:19.340624 140253182756672 submission_runner.py:396] After eval at step 6256: RAM USED (GB) 115.962802176
I0331 20:20:19.352837 140077195847424 logging_writer.py:48] [6256] global_step=6256, preemption_count=0, score=2518.411442, test/accuracy=0.222200, test/loss=3.972280, test/num_examples=10000, total_duration=2742.698610, train/accuracy=0.311523, train/loss=3.348360, validation/accuracy=0.290720, validation/loss=3.472025, validation/num_examples=50000
I0331 20:20:19.499245 140253182756672 checkpoints.py:356] Saving checkpoint at step: 6256
I0331 20:20:20.759730 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_6256
I0331 20:20:20.781614 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_6256.
I0331 20:20:20.785288 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 6256: RAM USED (GB) 122.072383488
I0331 20:20:38.696671 140077464250112 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0269079208374023, loss=4.70320987701416
I0331 20:21:18.499260 140077455857408 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7173159122467041, loss=6.0423760414123535
I0331 20:21:58.780075 140077464250112 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.2775118350982666, loss=5.314992904663086
I0331 20:22:39.136403 140077455857408 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.863473117351532, loss=5.798220157623291
I0331 20:23:19.870416 140077464250112 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.9060607552528381, loss=6.253770351409912
I0331 20:24:00.340471 140077455857408 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7423301339149475, loss=5.8711838722229
I0331 20:24:40.877221 140077464250112 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.064564824104309, loss=4.733978271484375
I0331 20:25:21.406509 140077455857408 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9835044741630554, loss=4.55197811126709
I0331 20:26:02.351970 140077464250112 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7140216827392578, loss=6.008481979370117
I0331 20:26:42.934343 140077455857408 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.0830923318862915, loss=4.747279167175293
I0331 20:27:20.932938 140253182756672 submission_runner.py:373] Before eval at step 7294: RAM USED (GB) 117.02310912
I0331 20:27:20.933207 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:27:35.045786 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:27:44.916510 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:27:46.566339 140253182756672 submission_runner.py:382] Time since start: 3189.64s, 	Step: 7294, 	{'train/accuracy': 0.3493945300579071, 'train/loss': 3.1636009216308594, 'validation/accuracy': 0.3254999816417694, 'validation/loss': 3.2964208126068115, 'validation/num_examples': 50000, 'test/accuracy': 0.24890001118183136, 'test/loss': 3.824521780014038, 'test/num_examples': 10000}
I0331 20:27:46.566919 140253182756672 submission_runner.py:396] After eval at step 7294: RAM USED (GB) 124.373147648
I0331 20:27:46.576879 140077464250112 logging_writer.py:48] [7294] global_step=7294, preemption_count=0, score=2927.576390, test/accuracy=0.248900, test/loss=3.824522, test/num_examples=10000, total_duration=3189.643360, train/accuracy=0.349395, train/loss=3.163601, validation/accuracy=0.325500, validation/loss=3.296421, validation/num_examples=50000
I0331 20:27:46.698387 140253182756672 checkpoints.py:356] Saving checkpoint at step: 7294
I0331 20:27:47.798532 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_7294
I0331 20:27:47.816389 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_7294.
I0331 20:27:47.820408 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 7294: RAM USED (GB) 129.531916288
I0331 20:27:50.637998 140077455857408 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.464187502861023, loss=4.4807233810424805
I0331 20:28:30.119781 140077439072000 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7067570686340332, loss=6.180114269256592
I0331 20:29:10.184597 140077455857408 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8917750120162964, loss=4.455150604248047
I0331 20:29:50.614295 140077439072000 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6729132533073425, loss=6.201704025268555
I0331 20:30:31.126039 140077455857408 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0736300945281982, loss=4.390307426452637
I0331 20:31:11.697717 140077439072000 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9602206945419312, loss=4.430630683898926
I0331 20:31:52.639997 140077455857408 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9570526480674744, loss=4.492491722106934
I0331 20:32:33.163110 140077439072000 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8382677435874939, loss=4.319368362426758
I0331 20:33:13.875038 140077455857408 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.1717529296875, loss=4.388103008270264
I0331 20:33:54.376399 140077439072000 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9454469680786133, loss=4.246899127960205
I0331 20:34:35.100984 140077455857408 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.1059463024139404, loss=4.4163737297058105
I0331 20:34:47.946531 140253182756672 submission_runner.py:373] Before eval at step 8333: RAM USED (GB) 123.162148864
I0331 20:34:47.946815 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:35:02.678674 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:35:12.690562 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:35:14.341296 140253182756672 submission_runner.py:382] Time since start: 3636.66s, 	Step: 8333, 	{'train/accuracy': 0.3866601586341858, 'train/loss': 2.98698091506958, 'validation/accuracy': 0.35569998621940613, 'validation/loss': 3.1359670162200928, 'validation/num_examples': 50000, 'test/accuracy': 0.2702000141143799, 'test/loss': 3.6870434284210205, 'test/num_examples': 10000}
I0331 20:35:14.341895 140253182756672 submission_runner.py:396] After eval at step 8333: RAM USED (GB) 130.211721216
I0331 20:35:14.355366 140077439072000 logging_writer.py:48] [8333] global_step=8333, preemption_count=0, score=3336.724368, test/accuracy=0.270200, test/loss=3.687043, test/num_examples=10000, total_duration=3636.657411, train/accuracy=0.386660, train/loss=2.986981, validation/accuracy=0.355700, validation/loss=3.135967, validation/num_examples=50000
I0331 20:35:14.503484 140253182756672 checkpoints.py:356] Saving checkpoint at step: 8333
I0331 20:35:15.591139 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_8333
I0331 20:35:15.613764 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_8333.
I0331 20:35:15.616332 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 8333: RAM USED (GB) 135.207432192
I0331 20:35:42.513563 140077455857408 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6618852019309998, loss=5.566256046295166
I0331 20:36:22.673279 140077430679296 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7773250937461853, loss=5.112958908081055
I0331 20:37:03.201177 140077455857408 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.3162527084350586, loss=5.529491424560547
I0331 20:37:43.878085 140077430679296 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8506205081939697, loss=4.716660499572754
I0331 20:38:24.492522 140077455857408 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7966163754463196, loss=5.199110984802246
I0331 20:39:04.947278 140077430679296 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8052336573600769, loss=4.275510787963867
I0331 20:39:45.459733 140077455857408 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9809402823448181, loss=4.3271164894104
I0331 20:40:26.359493 140077430679296 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.8595697283744812, loss=4.883411407470703
I0331 20:41:07.019609 140077455857408 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7450586557388306, loss=5.0379838943481445
I0331 20:41:47.607733 140077430679296 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7211872339248657, loss=5.189093589782715
I0331 20:42:15.659672 140253182756672 submission_runner.py:373] Before eval at step 9371: RAM USED (GB) 129.055985664
I0331 20:42:15.659982 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:42:30.228634 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:42:40.943369 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:42:42.586917 140253182756672 submission_runner.py:382] Time since start: 4084.37s, 	Step: 9371, 	{'train/accuracy': 0.44023436307907104, 'train/loss': 2.568216562271118, 'validation/accuracy': 0.3959999978542328, 'validation/loss': 2.8050405979156494, 'validation/num_examples': 50000, 'test/accuracy': 0.30730000138282776, 'test/loss': 3.391570806503296, 'test/num_examples': 10000}
I0331 20:42:42.587357 140253182756672 submission_runner.py:396] After eval at step 9371: RAM USED (GB) 137.442398208
I0331 20:42:42.598825 140077455857408 logging_writer.py:48] [9371] global_step=9371, preemption_count=0, score=3745.145547, test/accuracy=0.307300, test/loss=3.391571, test/num_examples=10000, total_duration=4084.372855, train/accuracy=0.440234, train/loss=2.568217, validation/accuracy=0.396000, validation/loss=2.805041, validation/num_examples=50000
I0331 20:42:42.773618 140253182756672 checkpoints.py:356] Saving checkpoint at step: 9371
I0331 20:42:44.067863 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_9371
I0331 20:42:44.089390 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_9371.
I0331 20:42:44.103657 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 9371: RAM USED (GB) 143.785762816
I0331 20:42:55.941910 140077430679296 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8708400726318359, loss=4.354799747467041
I0331 20:43:35.691437 140077422286592 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.799208402633667, loss=4.50724983215332
I0331 20:44:16.141768 140077430679296 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8223064541816711, loss=6.073251247406006
I0331 20:44:56.872499 140077422286592 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8892838358879089, loss=4.9443159103393555
I0331 20:45:37.425472 140077430679296 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7532644867897034, loss=5.982206344604492
I0331 20:46:18.126123 140077422286592 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6875123381614685, loss=5.4361186027526855
I0331 20:46:58.962727 140077430679296 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.015265941619873, loss=4.151461601257324
I0331 20:47:40.056124 140077422286592 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7742270231246948, loss=4.15952730178833
I0331 20:48:20.722850 140077430679296 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8888282775878906, loss=3.9895691871643066
I0331 20:49:01.428990 140077422286592 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.864316999912262, loss=4.097541809082031
I0331 20:49:41.953525 140077430679296 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6108832955360413, loss=5.651892185211182
I0331 20:49:44.488169 140253182756672 submission_runner.py:373] Before eval at step 10408: RAM USED (GB) 134.731845632
I0331 20:49:44.488442 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:49:59.433762 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:50:10.552416 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:50:12.203982 140253182756672 submission_runner.py:382] Time since start: 4533.20s, 	Step: 10408, 	{'train/accuracy': 0.44550779461860657, 'train/loss': 2.621795177459717, 'validation/accuracy': 0.4178199768066406, 'validation/loss': 2.7690317630767822, 'validation/num_examples': 50000, 'test/accuracy': 0.32350000739097595, 'test/loss': 3.3453187942504883, 'test/num_examples': 10000}
I0331 20:50:12.204545 140253182756672 submission_runner.py:396] After eval at step 10408: RAM USED (GB) 143.087816704
I0331 20:50:12.220579 140077422286592 logging_writer.py:48] [10408] global_step=10408, preemption_count=0, score=4153.292053, test/accuracy=0.323500, test/loss=3.345319, test/num_examples=10000, total_duration=4533.198835, train/accuracy=0.445508, train/loss=2.621795, validation/accuracy=0.417820, validation/loss=2.769032, validation/num_examples=50000
I0331 20:50:12.481413 140253182756672 checkpoints.py:356] Saving checkpoint at step: 10408
I0331 20:50:13.857407 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_10408
I0331 20:50:13.878684 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_10408.
I0331 20:50:13.893997 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 10408: RAM USED (GB) 149.957066752
I0331 20:50:50.600254 140077430679296 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.866077721118927, loss=3.9345550537109375
I0331 20:51:30.754472 140077070022400 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7580507397651672, loss=4.438155174255371
I0331 20:52:11.382730 140077430679296 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5803406834602356, loss=5.7678728103637695
I0331 20:52:51.901174 140077070022400 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8919957280158997, loss=4.054058074951172
I0331 20:53:32.501413 140077430679296 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8796958327293396, loss=4.057440757751465
I0331 20:54:13.221909 140077070022400 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8393480777740479, loss=3.976613759994507
I0331 20:54:54.198669 140077430679296 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9203316569328308, loss=3.9438440799713135
I0331 20:55:35.437695 140077070022400 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7814297676086426, loss=4.890727996826172
I0331 20:56:16.289228 140077430679296 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5925516486167908, loss=5.9542131423950195
I0331 20:56:56.943233 140077070022400 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.8236275911331177, loss=3.958845615386963
I0331 20:57:14.054813 140253182756672 submission_runner.py:373] Before eval at step 11444: RAM USED (GB) 140.321902592
I0331 20:57:14.055095 140253182756672 spec.py:298] Evaluating on the training split.
I0331 20:57:29.255934 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 20:57:40.152799 140253182756672 spec.py:326] Evaluating on the test split.
I0331 20:57:41.804343 140253182756672 submission_runner.py:382] Time since start: 4982.76s, 	Step: 11444, 	{'train/accuracy': 0.486640602350235, 'train/loss': 2.3957459926605225, 'validation/accuracy': 0.4520599842071533, 'validation/loss': 2.5678303241729736, 'validation/num_examples': 50000, 'test/accuracy': 0.35100001096725464, 'test/loss': 3.141294479370117, 'test/num_examples': 10000}
I0331 20:57:41.804923 140253182756672 submission_runner.py:396] After eval at step 11444: RAM USED (GB) 148.561514496
I0331 20:57:41.819020 140077430679296 logging_writer.py:48] [11444] global_step=11444, preemption_count=0, score=4560.956842, test/accuracy=0.351000, test/loss=3.141294, test/num_examples=10000, total_duration=4982.763622, train/accuracy=0.486641, train/loss=2.395746, validation/accuracy=0.452060, validation/loss=2.567830, validation/num_examples=50000
I0331 20:57:42.004347 140253182756672 checkpoints.py:356] Saving checkpoint at step: 11444
I0331 20:57:43.322777 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_11444
I0331 20:57:43.344753 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_11444.
I0331 20:57:43.356332 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 11444: RAM USED (GB) 155.058642944
I0331 20:58:05.833099 140077070022400 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8311291337013245, loss=3.90250301361084
I0331 20:58:46.127105 140075056756480 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.0135388374328613, loss=3.9011330604553223
I0331 20:59:26.860948 140077070022400 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6103693842887878, loss=5.276844024658203
I0331 21:00:07.394082 140075056756480 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.72218257188797, loss=3.84696888923645
I0331 21:00:48.053652 140077070022400 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7625238299369812, loss=3.8444206714630127
I0331 21:01:28.603975 140075056756480 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6205974817276001, loss=5.5222063064575195
I0331 21:02:08.886353 140077070022400 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.831366777420044, loss=3.945340633392334
I0331 21:02:49.361740 140075056756480 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7794138193130493, loss=5.021562099456787
I0331 21:03:30.278559 140077070022400 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9243263006210327, loss=4.361424446105957
I0331 21:04:11.133735 140075056756480 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6906057000160217, loss=5.81847620010376
I0331 21:04:43.529272 140253182756672 submission_runner.py:373] Before eval at step 12481: RAM USED (GB) 146.496045056
I0331 21:04:43.529582 140253182756672 spec.py:298] Evaluating on the training split.
I0331 21:04:59.008939 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 21:05:10.062834 140253182756672 spec.py:326] Evaluating on the test split.
I0331 21:05:11.718974 140253182756672 submission_runner.py:382] Time since start: 5432.24s, 	Step: 12481, 	{'train/accuracy': 0.5110155940055847, 'train/loss': 2.2773497104644775, 'validation/accuracy': 0.47147998213768005, 'validation/loss': 2.4619946479797363, 'validation/num_examples': 50000, 'test/accuracy': 0.3701000213623047, 'test/loss': 3.0588738918304443, 'test/num_examples': 10000}
I0331 21:05:11.719437 140253182756672 submission_runner.py:396] After eval at step 12481: RAM USED (GB) 154.123214848
I0331 21:05:11.729360 140077070022400 logging_writer.py:48] [12481] global_step=12481, preemption_count=0, score=4968.715453, test/accuracy=0.370100, test/loss=3.058874, test/num_examples=10000, total_duration=5432.235076, train/accuracy=0.511016, train/loss=2.277350, validation/accuracy=0.471480, validation/loss=2.461995, validation/num_examples=50000
I0331 21:05:11.953958 140253182756672 checkpoints.py:356] Saving checkpoint at step: 12481
I0331 21:05:13.257758 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_12481
I0331 21:05:13.278528 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_12481.
I0331 21:05:13.290998 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 12481: RAM USED (GB) 160.60506112
I0331 21:05:21.234783 140075056756480 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6265428066253662, loss=5.832057476043701
I0331 21:06:01.024995 140075048363776 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7743456959724426, loss=3.847170352935791
I0331 21:06:42.212304 140075056756480 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7996202111244202, loss=4.457913398742676
I0331 21:07:23.130609 140075048363776 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9878654479980469, loss=3.8117027282714844
I0331 21:08:04.324768 140075056756480 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8953967690467834, loss=3.8864216804504395
I0331 21:08:45.098376 140075048363776 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7197595834732056, loss=5.787848949432373
I0331 21:09:26.284807 140075056756480 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7817133069038391, loss=3.820141077041626
I0331 21:10:07.324724 140075048363776 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.8708329200744629, loss=3.7673590183258057
I0331 21:10:47.866698 140075056756480 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9400089979171753, loss=3.7003979682922363
I0331 21:11:28.382111 140075048363776 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6687105298042297, loss=3.7331418991088867
I0331 21:12:09.417289 140075056756480 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7576035261154175, loss=4.917890548706055
I0331 21:12:13.651012 140253182756672 submission_runner.py:373] Before eval at step 13512: RAM USED (GB) 152.735629312
I0331 21:12:13.651288 140253182756672 spec.py:298] Evaluating on the training split.
I0331 21:12:29.255545 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 21:12:39.999946 140253182756672 spec.py:326] Evaluating on the test split.
I0331 21:12:41.651316 140253182756672 submission_runner.py:382] Time since start: 5882.36s, 	Step: 13512, 	{'train/accuracy': 0.540332019329071, 'train/loss': 2.120326519012451, 'validation/accuracy': 0.49201998114585876, 'validation/loss': 2.3461105823516846, 'validation/num_examples': 50000, 'test/accuracy': 0.38670000433921814, 'test/loss': 2.9538872241973877, 'test/num_examples': 10000}
I0331 21:12:41.651862 140253182756672 submission_runner.py:396] After eval at step 13512: RAM USED (GB) 161.555165184
I0331 21:12:41.666454 140075048363776 logging_writer.py:48] [13512] global_step=13512, preemption_count=0, score=5375.138455, test/accuracy=0.386700, test/loss=2.953887, test/num_examples=10000, total_duration=5882.361854, train/accuracy=0.540332, train/loss=2.120327, validation/accuracy=0.492020, validation/loss=2.346111, validation/num_examples=50000
I0331 21:12:41.913501 140253182756672 checkpoints.py:356] Saving checkpoint at step: 13512
I0331 21:12:43.085733 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_13512
I0331 21:12:43.105998 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_13512.
I0331 21:12:43.120990 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 13512: RAM USED (GB) 167.742144512
I0331 21:13:18.224478 140075056756480 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.971467137336731, loss=3.659848690032959
I0331 21:13:58.932762 140074905753344 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7286131381988525, loss=4.7083740234375
I0331 21:14:39.920514 140075056756480 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.690627932548523, loss=4.227095603942871
I0331 21:15:20.856608 140074905753344 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7958110570907593, loss=3.6093862056732178
I0331 21:16:01.129841 140253182756672 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 158.423371776
I0331 21:16:01.130136 140253182756672 spec.py:298] Evaluating on the training split.
I0331 21:16:16.827013 140253182756672 spec.py:310] Evaluating on the validation split.
I0331 21:16:27.580558 140253182756672 spec.py:326] Evaluating on the test split.
I0331 21:16:29.226509 140253182756672 submission_runner.py:382] Time since start: 6109.84s, 	Step: 14000, 	{'train/accuracy': 0.5444140434265137, 'train/loss': 2.0895581245422363, 'validation/accuracy': 0.5029000043869019, 'validation/loss': 2.273826837539673, 'validation/num_examples': 50000, 'test/accuracy': 0.3996000289916992, 'test/loss': 2.8818907737731934, 'test/num_examples': 10000}
I0331 21:16:29.227156 140253182756672 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 166.27572736
I0331 21:16:29.239343 140075056756480 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5565.647086, test/accuracy=0.399600, test/loss=2.881891, test/num_examples=10000, total_duration=6109.840334, train/accuracy=0.544414, train/loss=2.089558, validation/accuracy=0.502900, validation/loss=2.273827, validation/num_examples=50000
I0331 21:16:29.475107 140253182756672 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 21:16:30.803822 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_14000
I0331 21:16:30.827325 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_14000.
I0331 21:16:30.830260 140253182756672 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 173.021769728
I0331 21:16:30.856706 140074905753344 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=5565.647086
I0331 21:16:31.076964 140253182756672 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 21:16:32.342175 140253182756672 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_14000
I0331 21:16:32.357857 140253182756672 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_vit_jax/trial_1/checkpoint_14000.
I0331 21:16:33.258450 140253182756672 submission_runner.py:550] Tuning trial 1/1
I0331 21:16:33.258666 140253182756672 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0331 21:16:33.262864 140253182756672 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 50.425354957580566, 'total_duration': 50.507508277893066, 'global_step': 1, 'preemption_count': 0}), (1008, {'train/accuracy': 0.029355468228459358, 'train/loss': 6.022684097290039, 'validation/accuracy': 0.027300000190734863, 'validation/loss': 6.048311233520508, 'validation/num_examples': 50000, 'test/accuracy': 0.022200001403689384, 'test/loss': 6.135765075683594, 'test/num_examples': 10000, 'score': 462.3367838859558, 'total_duration': 519.8182735443115, 'global_step': 1008, 'preemption_count': 0}), (2061, {'train/accuracy': 0.07630859315395355, 'train/loss': 5.272983074188232, 'validation/accuracy': 0.07401999831199646, 'validation/loss': 5.304529666900635, 'validation/num_examples': 50000, 'test/accuracy': 0.05700000375509262, 'test/loss': 5.511024475097656, 'test/num_examples': 10000, 'score': 874.1860706806183, 'total_duration': 962.2767941951752, 'global_step': 2061, 'preemption_count': 0}), (3113, {'train/accuracy': 0.1457226574420929, 'train/loss': 4.625796318054199, 'validation/accuracy': 0.13492000102996826, 'validation/loss': 4.70241641998291, 'validation/num_examples': 50000, 'test/accuracy': 0.10190000385046005, 'test/loss': 5.014127731323242, 'test/num_examples': 10000, 'score': 1286.2746620178223, 'total_duration': 1406.4477713108063, 'global_step': 3113, 'preemption_count': 0}), (4161, {'train/accuracy': 0.2064257711172104, 'train/loss': 4.119557857513428, 'validation/accuracy': 0.19189999997615814, 'validation/loss': 4.223005294799805, 'validation/num_examples': 50000, 'test/accuracy': 0.14580000936985016, 'test/loss': 4.582226753234863, 'test/num_examples': 10000, 'score': 1697.0514934062958, 'total_duration': 1849.8827998638153, 'global_step': 4161, 'preemption_count': 0}), (5210, {'train/accuracy': 0.2510351538658142, 'train/loss': 3.789574146270752, 'validation/accuracy': 0.23323999345302582, 'validation/loss': 3.8981120586395264, 'validation/num_examples': 50000, 'test/accuracy': 0.17900000512599945, 'test/loss': 4.3219780921936035, 'test/num_examples': 10000, 'score': 2108.523406267166, 'total_duration': 2295.0652425289154, 'global_step': 5210, 'preemption_count': 0}), (6256, {'train/accuracy': 0.3115234375, 'train/loss': 3.348360300064087, 'validation/accuracy': 0.29071998596191406, 'validation/loss': 3.47202467918396, 'validation/num_examples': 50000, 'test/accuracy': 0.22220000624656677, 'test/loss': 3.9722795486450195, 'test/num_examples': 10000, 'score': 2518.41144156456, 'total_duration': 2742.698610305786, 'global_step': 6256, 'preemption_count': 0}), (7294, {'train/accuracy': 0.3493945300579071, 'train/loss': 3.1636009216308594, 'validation/accuracy': 0.3254999816417694, 'validation/loss': 3.2964208126068115, 'validation/num_examples': 50000, 'test/accuracy': 0.24890001118183136, 'test/loss': 3.824521780014038, 'test/num_examples': 10000, 'score': 2927.5763895511627, 'total_duration': 3189.6433601379395, 'global_step': 7294, 'preemption_count': 0}), (8333, {'train/accuracy': 0.3866601586341858, 'train/loss': 2.98698091506958, 'validation/accuracy': 0.35569998621940613, 'validation/loss': 3.1359670162200928, 'validation/num_examples': 50000, 'test/accuracy': 0.2702000141143799, 'test/loss': 3.6870434284210205, 'test/num_examples': 10000, 'score': 3336.724367618561, 'total_duration': 3636.657410621643, 'global_step': 8333, 'preemption_count': 0}), (9371, {'train/accuracy': 0.44023436307907104, 'train/loss': 2.568216562271118, 'validation/accuracy': 0.3959999978542328, 'validation/loss': 2.8050405979156494, 'validation/num_examples': 50000, 'test/accuracy': 0.30730000138282776, 'test/loss': 3.391570806503296, 'test/num_examples': 10000, 'score': 3745.1455466747284, 'total_duration': 4084.3728551864624, 'global_step': 9371, 'preemption_count': 0}), (10408, {'train/accuracy': 0.44550779461860657, 'train/loss': 2.621795177459717, 'validation/accuracy': 0.4178199768066406, 'validation/loss': 2.7690317630767822, 'validation/num_examples': 50000, 'test/accuracy': 0.32350000739097595, 'test/loss': 3.3453187942504883, 'test/num_examples': 10000, 'score': 4153.292053461075, 'total_duration': 4533.198834657669, 'global_step': 10408, 'preemption_count': 0}), (11444, {'train/accuracy': 0.486640602350235, 'train/loss': 2.3957459926605225, 'validation/accuracy': 0.4520599842071533, 'validation/loss': 2.5678303241729736, 'validation/num_examples': 50000, 'test/accuracy': 0.35100001096725464, 'test/loss': 3.141294479370117, 'test/num_examples': 10000, 'score': 4560.956842422485, 'total_duration': 4982.76362156868, 'global_step': 11444, 'preemption_count': 0}), (12481, {'train/accuracy': 0.5110155940055847, 'train/loss': 2.2773497104644775, 'validation/accuracy': 0.47147998213768005, 'validation/loss': 2.4619946479797363, 'validation/num_examples': 50000, 'test/accuracy': 0.3701000213623047, 'test/loss': 3.0588738918304443, 'test/num_examples': 10000, 'score': 4968.715453147888, 'total_duration': 5432.235075712204, 'global_step': 12481, 'preemption_count': 0}), (13512, {'train/accuracy': 0.540332019329071, 'train/loss': 2.120326519012451, 'validation/accuracy': 0.49201998114585876, 'validation/loss': 2.3461105823516846, 'validation/num_examples': 50000, 'test/accuracy': 0.38670000433921814, 'test/loss': 2.9538872241973877, 'test/num_examples': 10000, 'score': 5375.138454914093, 'total_duration': 5882.361853837967, 'global_step': 13512, 'preemption_count': 0}), (14000, {'train/accuracy': 0.5444140434265137, 'train/loss': 2.0895581245422363, 'validation/accuracy': 0.5029000043869019, 'validation/loss': 2.273826837539673, 'validation/num_examples': 50000, 'test/accuracy': 0.3996000289916992, 'test/loss': 2.8818907737731934, 'test/num_examples': 10000, 'score': 5565.647086381912, 'total_duration': 6109.840334177017, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0331 21:16:33.262985 140253182756672 submission_runner.py:553] Timing: 5565.647086381912
I0331 21:16:33.263036 140253182756672 submission_runner.py:554] ====================
I0331 21:16:33.263145 140253182756672 submission_runner.py:613] Final imagenet_vit score: 5565.647086381912
