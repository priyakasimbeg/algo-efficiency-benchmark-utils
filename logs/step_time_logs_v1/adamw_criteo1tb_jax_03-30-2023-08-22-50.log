I0330 08:23:11.481725 140002941855552 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/criteo1tb_jax.
I0330 08:23:11.854239 140002941855552 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0330 08:23:15.579831 140002941855552 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0330 08:23:15.580603 140002941855552 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0330 08:23:15.586266 140002941855552 submission_runner.py:504] Using RNG seed 2274299138
I0330 08:23:18.710272 140002941855552 submission_runner.py:513] --- Tuning run 1/1 ---
I0330 08:23:18.710494 140002941855552 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/criteo1tb_jax/trial_1.
I0330 08:23:18.710697 140002941855552 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/hparams.json.
I0330 08:23:18.849578 140002941855552 submission_runner.py:230] Starting train once: RAM USED (GB) 4.70030336
I0330 08:23:18.849758 140002941855552 submission_runner.py:231] Initializing dataset.
I0330 08:23:18.849955 140002941855552 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.70030336
I0330 08:23:18.850027 140002941855552 submission_runner.py:240] Initializing model.
I0330 08:23:26.257287 140002941855552 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.39983104
I0330 08:23:26.257492 140002941855552 submission_runner.py:252] Initializing optimizer.
I0330 08:23:29.535812 140002941855552 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.406990848
I0330 08:23:29.536020 140002941855552 submission_runner.py:261] Initializing metrics bundle.
I0330 08:23:29.536071 140002941855552 submission_runner.py:275] Initializing checkpoint and logger.
I0330 08:23:29.541134 140002941855552 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_adamw/criteo1tb_jax/trial_1 with prefix checkpoint_
I0330 08:23:29.541456 140002941855552 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0330 08:23:29.541545 140002941855552 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0330 08:23:31.611651 140002941855552 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/meta_data_0.json.
I0330 08:23:31.612663 140002941855552 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/flags_0.json.
I0330 08:23:31.663472 140002941855552 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 8.428875776
I0330 08:23:31.663759 140002941855552 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.428875776
I0330 08:23:31.663858 140002941855552 submission_runner.py:312] Starting training loop.
I0330 08:26:08.381632 140002941855552 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 46.523633664
I0330 08:26:32.534264 139796856960768 logging_writer.py:48] [0] global_step=0, grad_norm=8.252314567565918, loss=0.6698683500289917
I0330 08:26:32.600536 140002941855552 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 54.428655616
I0330 08:26:32.600821 140002941855552 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 54.428655616
I0330 08:26:32.600905 140002941855552 spec.py:298] Evaluating on the training split.
I0330 08:36:33.664966 140002941855552 spec.py:310] Evaluating on the validation split.
I0330 08:41:07.463620 140002941855552 spec.py:326] Evaluating on the test split.
I0330 08:46:06.437684 140002941855552 submission_runner.py:380] Time since start: 180.94s, 	Step: 1, 	{'train/loss': 0.6697299219037436, 'validation/loss': 0.6751410337078652, 'validation/num_examples': 89000000, 'test/loss': 0.671065803381536, 'test/num_examples': 89274637}
I0330 08:46:06.438160 140002941855552 submission_runner.py:390] After eval at step 1: RAM USED (GB) 96.724795392
I0330 08:46:06.456006 139739109783296 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=180.728511, test/loss=0.671066, test/num_examples=89274637, total_duration=180.937040, train/loss=0.669730, validation/loss=0.675141, validation/num_examples=89000000
I0330 08:46:12.377984 140002941855552 checkpoints.py:356] Saving checkpoint at step: 1
I0330 08:46:46.719539 140002941855552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_1
I0330 08:46:47.167461 140002941855552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_1.
I0330 08:46:47.692077 140002941855552 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 96.863002624
I0330 08:46:47.697940 140002941855552 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 96.82223104
I0330 08:46:47.738331 140002941855552 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 96.821932032
I0330 08:48:15.925219 139739101390592 logging_writer.py:48] [100] global_step=100, grad_norm=0.13509178161621094, loss=0.13082613050937653
I0330 08:50:15.508373 139739067819776 logging_writer.py:48] [200] global_step=200, grad_norm=0.056516945362091064, loss=0.12834559381008148
I0330 08:52:30.767359 139739101390592 logging_writer.py:48] [300] global_step=300, grad_norm=0.09113796055316925, loss=0.12615711987018585
I0330 08:54:37.542457 139739067819776 logging_writer.py:48] [400] global_step=400, grad_norm=0.047264233231544495, loss=0.1254713088274002
I0330 08:55:48.475147 140002941855552 submission_runner.py:371] Before eval at step 458: RAM USED (GB) 104.713449472
I0330 08:55:48.475347 140002941855552 spec.py:298] Evaluating on the training split.
I0330 09:06:48.296357 140002941855552 spec.py:310] Evaluating on the validation split.
I0330 09:10:46.587692 140002941855552 spec.py:326] Evaluating on the test split.
I0330 09:14:39.855032 140002941855552 submission_runner.py:380] Time since start: 1936.81s, 	Step: 458, 	{'train/loss': 0.12618743469822136, 'validation/loss': 0.1269995168539326, 'validation/num_examples': 89000000, 'test/loss': 0.1294851638545447, 'test/num_examples': 89274637}
I0330 09:14:39.855732 140002941855552 submission_runner.py:390] After eval at step 458: RAM USED (GB) 111.083544576
I0330 09:14:39.869440 139739101390592 logging_writer.py:48] [458] global_step=458, preemption_count=0, score=719.443531, test/loss=0.129485, test/num_examples=89274637, total_duration=1936.811105, train/loss=0.126187, validation/loss=0.127000, validation/num_examples=89000000
I0330 09:14:46.676678 140002941855552 checkpoints.py:356] Saving checkpoint at step: 458
I0330 09:15:27.247576 140002941855552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_458
I0330 09:15:27.733516 140002941855552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_458.
I0330 09:15:28.209752 140002941855552 submission_runner.py:409] After logging and checkpointing eval at step 458: RAM USED (GB) 111.121969152
I0330 09:15:50.951333 139739067819776 logging_writer.py:48] [500] global_step=500, grad_norm=0.008418121375143528, loss=0.1248103529214859
I0330 09:18:13.066538 139795934205696 logging_writer.py:48] [600] global_step=600, grad_norm=0.046256229281425476, loss=0.125574991106987
I0330 09:20:15.783094 139739067819776 logging_writer.py:48] [700] global_step=700, grad_norm=0.04606495052576065, loss=0.12613314390182495
I0330 09:22:15.336148 140002941855552 submission_runner.py:371] Before eval at step 800: RAM USED (GB) 111.795523584
I0330 09:22:15.336342 140002941855552 spec.py:298] Evaluating on the training split.
I0330 09:33:14.697284 140002941855552 spec.py:310] Evaluating on the validation split.
I0330 09:37:33.530170 140002941855552 spec.py:326] Evaluating on the test split.
I0330 09:41:52.827565 140002941855552 submission_runner.py:380] Time since start: 3523.67s, 	Step: 800, 	{'train/loss': 0.12409956302600718, 'validation/loss': 0.12608826966292136, 'validation/num_examples': 89000000, 'test/loss': 0.12841211552616003, 'test/num_examples': 89274637}
I0330 09:41:52.828054 140002941855552 submission_runner.py:390] After eval at step 800: RAM USED (GB) 115.884187648
I0330 09:41:52.836025 139737071339264 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1124.236911, test/loss=0.128412, test/num_examples=89274637, total_duration=3523.672131, train/loss=0.124100, validation/loss=0.126088, validation/num_examples=89000000
I0330 09:41:58.585227 140002941855552 checkpoints.py:356] Saving checkpoint at step: 800
I0330 09:42:32.095861 140002941855552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_800
I0330 09:42:32.604516 140002941855552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_800.
I0330 09:42:33.046928 140002941855552 submission_runner.py:409] After logging and checkpointing eval at step 800: RAM USED (GB) 115.934277632
I0330 09:42:33.053537 139737062946560 logging_writer.py:48] [800] global_step=800, preemption_count=0, score=1124.236911
I0330 09:42:38.529786 140002941855552 checkpoints.py:356] Saving checkpoint at step: 800
I0330 09:43:18.774624 140002941855552 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_800
I0330 09:43:19.211512 140002941855552 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/criteo1tb_jax/trial_1/checkpoint_800.
I0330 09:45:11.610299 140002941855552 submission_runner.py:543] Tuning trial 1/1
I0330 09:45:11.611444 140002941855552 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0330 09:45:11.613236 140002941855552 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/loss': 0.6697299219037436, 'validation/loss': 0.6751410337078652, 'validation/num_examples': 89000000, 'test/loss': 0.671065803381536, 'test/num_examples': 89274637, 'score': 180.72851085662842, 'total_duration': 180.93703961372375, 'global_step': 1, 'preemption_count': 0}), (458, {'train/loss': 0.12618743469822136, 'validation/loss': 0.1269995168539326, 'validation/num_examples': 89000000, 'test/loss': 0.1294851638545447, 'test/num_examples': 89274637, 'score': 719.4435312747955, 'total_duration': 1936.8111045360565, 'global_step': 458, 'preemption_count': 0}), (800, {'train/loss': 0.12409956302600718, 'validation/loss': 0.12608826966292136, 'validation/num_examples': 89000000, 'test/loss': 0.12841211552616003, 'test/num_examples': 89274637, 'score': 1124.2369112968445, 'total_duration': 3523.6721312999725, 'global_step': 800, 'preemption_count': 0})], 'global_step': 800}
I0330 09:45:11.613394 140002941855552 submission_runner.py:546] Timing: 1124.2369112968445
I0330 09:45:11.613451 140002941855552 submission_runner.py:547] ====================
I0330 09:45:11.613569 140002941855552 submission_runner.py:606] Final criteo1tb score: 1124.2369112968445
