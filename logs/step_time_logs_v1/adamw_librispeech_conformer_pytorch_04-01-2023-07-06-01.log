WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 07:06:23.157446 139882764515136 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 07:06:23.157474 140170263009088 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 07:06:23.157498 140083126343488 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 07:06:23.158083 140500683978560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 07:06:23.158384 140334197622592 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 07:06:23.158490 140259223525184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 07:06:23.158874 140618134841152 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 07:06:23.158986 139631400933184 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 07:06:23.159204 140618134841152 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.159338 139631400933184 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.168195 139882764515136 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.168227 140170263009088 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.168248 140083126343488 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.168726 140500683978560 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.168980 140334197622592 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.169163 140259223525184 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 07:06:23.689652 140259223525184 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_adamw/librispeech_conformer_pytorch.
W0401 07:06:23.699943 140170263009088 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.700465 140334197622592 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.701432 140500683978560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.701557 140083126343488 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.701836 139631400933184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.702759 140618134841152 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.703662 139882764515136 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 07:06:23.724358 140259223525184 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 07:06:23.727639 140259223525184 submission_runner.py:504] Using RNG seed 3038326175
I0401 07:06:23.728639 140259223525184 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 07:06:23.728749 140259223525184 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1.
I0401 07:06:23.729010 140259223525184 logger_utils.py:84] Saving hparams to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/hparams.json.
I0401 07:06:23.730037 140259223525184 submission_runner.py:230] Starting train once: RAM USED (GB) 5.751545856
I0401 07:06:23.730144 140259223525184 submission_runner.py:231] Initializing dataset.
I0401 07:06:23.730242 140259223525184 input_pipeline.py:20] Loading split = train-clean-100
I0401 07:06:23.759269 140259223525184 input_pipeline.py:20] Loading split = train-clean-360
I0401 07:06:24.072212 140259223525184 input_pipeline.py:20] Loading split = train-other-500
I0401 07:06:24.486260 140259223525184 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.973864448
I0401 07:06:24.486435 140259223525184 submission_runner.py:240] Initializing model.
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
  warnings.warn('Lazy modules are a new feature under heavy development '
I0401 07:06:31.439962 140259223525184 submission_runner.py:251] After Initializing model: RAM USED (GB) 19.184316416
I0401 07:06:31.440230 140259223525184 submission_runner.py:252] Initializing optimizer.
I0401 07:06:31.441246 140259223525184 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 19.178090496
I0401 07:06:31.441364 140259223525184 submission_runner.py:261] Initializing metrics bundle.
I0401 07:06:31.441416 140259223525184 submission_runner.py:275] Initializing checkpoint and logger.
I0401 07:06:31.442757 140259223525184 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 07:06:31.442862 140259223525184 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 07:06:32.191660 140259223525184 submission_runner.py:296] Saving meta data to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/meta_data_0.json.
I0401 07:06:32.192575 140259223525184 submission_runner.py:299] Saving flags to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/flags_0.json.
I0401 07:06:32.196861 140259223525184 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 19.17908992
I0401 07:06:32.197815 140259223525184 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 19.17908992
I0401 07:06:32.197912 140259223525184 submission_runner.py:312] Starting training loop.
I0401 07:06:33.795442 140259223525184 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 24.823226368
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
/algorithmic-efficiency/algorithmic_efficiency/workloads/librispeech_conformer/librispeech_pytorch/preprocessor.py:488: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  spectrum = torch.abs(spectrum)
I0401 07:06:38.613372 140232880019200 logging_writer.py:48] [0] global_step=0, grad_norm=76.246582, loss=30.288311
I0401 07:06:38.626370 140259223525184 submission.py:119] 0) loss = 30.288, grad_norm = 76.247
I0401 07:06:38.627112 140259223525184 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 32.680513536
I0401 07:06:38.653270 140259223525184 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 32.682610688
I0401 07:06:38.653404 140259223525184 spec.py:298] Evaluating on the training split.
I0401 07:06:38.654138 140259223525184 input_pipeline.py:20] Loading split = train-clean-100
I0401 07:06:38.682983 140259223525184 input_pipeline.py:20] Loading split = train-clean-360
I0401 07:06:39.090875 140259223525184 input_pipeline.py:20] Loading split = train-other-500
I0401 07:06:52.330868 140259223525184 spec.py:310] Evaluating on the validation split.
I0401 07:06:52.332031 140259223525184 input_pipeline.py:20] Loading split = dev-clean
I0401 07:06:52.335444 140259223525184 input_pipeline.py:20] Loading split = dev-other
I0401 07:07:02.613523 140259223525184 spec.py:326] Evaluating on the test split.
I0401 07:07:02.617623 140259223525184 input_pipeline.py:20] Loading split = test-clean
I0401 07:07:08.029214 140259223525184 submission_runner.py:380] Time since start: 6.46s, 	Step: 1, 	{'train/ctc_loss': 29.12666825386028, 'train/wer': 1.8829994275901545, 'validation/ctc_loss': 28.493184272654208, 'validation/wer': 1.6299811712451118, 'validation/num_examples': 5348, 'test/ctc_loss': 28.64129621265888, 'test/wer': 1.6906343306318932, 'test/num_examples': 2472}
I0401 07:07:08.030052 140259223525184 submission_runner.py:390] After eval at step 1: RAM USED (GB) 46.113435648
I0401 07:07:08.049167 140218481067776 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=6.453652, test/ctc_loss=28.641296, test/num_examples=2472, test/wer=1.690634, total_duration=6.455502, train/ctc_loss=29.126668, train/wer=1.882999, validation/ctc_loss=28.493184, validation/num_examples=5348, validation/wer=1.629981
I0401 07:07:08.625266 140259223525184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_1.
I0401 07:07:08.625917 140259223525184 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 46.154354688
I0401 07:07:08.635380 140259223525184 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 46.156972032
I0401 07:07:08.674851 140259223525184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.674899 140334197622592 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.674912 139631400933184 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.674889 140500683978560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.674945 140170263009088 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.675043 140618134841152 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.675235 139882764515136 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:08.675844 140083126343488 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 07:07:09.786245 140218132854528 logging_writer.py:48] [1] global_step=1, grad_norm=68.677406, loss=29.824278
I0401 07:07:09.789546 140259223525184 submission.py:119] 1) loss = 29.824, grad_norm = 68.677
I0401 07:07:09.790355 140259223525184 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 46.384648192
I0401 07:07:10.627631 140218481067776 logging_writer.py:48] [2] global_step=2, grad_norm=72.449677, loss=30.242044
I0401 07:07:10.631828 140259223525184 submission.py:119] 2) loss = 30.242, grad_norm = 72.450
I0401 07:07:11.642143 140218132854528 logging_writer.py:48] [3] global_step=3, grad_norm=76.201485, loss=30.203350
I0401 07:07:11.645942 140259223525184 submission.py:119] 3) loss = 30.203, grad_norm = 76.201
I0401 07:07:12.452666 140218481067776 logging_writer.py:48] [4] global_step=4, grad_norm=76.312843, loss=29.670059
I0401 07:07:12.456212 140259223525184 submission.py:119] 4) loss = 29.670, grad_norm = 76.313
I0401 07:07:13.262341 140218132854528 logging_writer.py:48] [5] global_step=5, grad_norm=79.186089, loss=29.759689
I0401 07:07:13.265565 140259223525184 submission.py:119] 5) loss = 29.760, grad_norm = 79.186
I0401 07:07:14.070882 140218481067776 logging_writer.py:48] [6] global_step=6, grad_norm=80.597870, loss=29.673161
I0401 07:07:14.074212 140259223525184 submission.py:119] 6) loss = 29.673, grad_norm = 80.598
I0401 07:07:14.877638 140218132854528 logging_writer.py:48] [7] global_step=7, grad_norm=78.419685, loss=28.655682
I0401 07:07:14.881078 140259223525184 submission.py:119] 7) loss = 28.656, grad_norm = 78.420
I0401 07:07:15.686338 140218481067776 logging_writer.py:48] [8] global_step=8, grad_norm=86.918144, loss=28.445316
I0401 07:07:15.689612 140259223525184 submission.py:119] 8) loss = 28.445, grad_norm = 86.918
I0401 07:07:16.494973 140218132854528 logging_writer.py:48] [9] global_step=9, grad_norm=82.815460, loss=28.054052
I0401 07:07:16.498175 140259223525184 submission.py:119] 9) loss = 28.054, grad_norm = 82.815
I0401 07:07:17.302291 140218481067776 logging_writer.py:48] [10] global_step=10, grad_norm=86.380638, loss=27.551754
I0401 07:07:17.305674 140259223525184 submission.py:119] 10) loss = 27.552, grad_norm = 86.381
I0401 07:07:18.112637 140218132854528 logging_writer.py:48] [11] global_step=11, grad_norm=89.275612, loss=27.348799
I0401 07:07:18.115932 140259223525184 submission.py:119] 11) loss = 27.349, grad_norm = 89.276
I0401 07:07:18.924943 140218481067776 logging_writer.py:48] [12] global_step=12, grad_norm=90.553703, loss=26.833170
I0401 07:07:18.929029 140259223525184 submission.py:119] 12) loss = 26.833, grad_norm = 90.554
I0401 07:07:19.736738 140218132854528 logging_writer.py:48] [13] global_step=13, grad_norm=90.652969, loss=25.838646
I0401 07:07:19.740044 140259223525184 submission.py:119] 13) loss = 25.839, grad_norm = 90.653
I0401 07:07:20.541552 140218481067776 logging_writer.py:48] [14] global_step=14, grad_norm=92.868713, loss=25.107630
I0401 07:07:20.544649 140259223525184 submission.py:119] 14) loss = 25.108, grad_norm = 92.869
I0401 07:07:21.349173 140218132854528 logging_writer.py:48] [15] global_step=15, grad_norm=90.521355, loss=23.744526
I0401 07:07:21.352512 140259223525184 submission.py:119] 15) loss = 23.745, grad_norm = 90.521
I0401 07:07:22.157468 140218481067776 logging_writer.py:48] [16] global_step=16, grad_norm=91.667625, loss=23.191425
I0401 07:07:22.160623 140259223525184 submission.py:119] 16) loss = 23.191, grad_norm = 91.668
I0401 07:07:22.963586 140218132854528 logging_writer.py:48] [17] global_step=17, grad_norm=97.260071, loss=22.242147
I0401 07:07:22.967064 140259223525184 submission.py:119] 17) loss = 22.242, grad_norm = 97.260
I0401 07:07:23.772999 140218481067776 logging_writer.py:48] [18] global_step=18, grad_norm=94.367081, loss=21.146513
I0401 07:07:23.776450 140259223525184 submission.py:119] 18) loss = 21.147, grad_norm = 94.367
I0401 07:07:24.584658 140218132854528 logging_writer.py:48] [19] global_step=19, grad_norm=93.802940, loss=19.561172
I0401 07:07:24.587982 140259223525184 submission.py:119] 19) loss = 19.561, grad_norm = 93.803
I0401 07:07:25.394443 140218481067776 logging_writer.py:48] [20] global_step=20, grad_norm=91.739738, loss=18.092518
I0401 07:07:25.397996 140259223525184 submission.py:119] 20) loss = 18.093, grad_norm = 91.740
I0401 07:07:26.205549 140218132854528 logging_writer.py:48] [21] global_step=21, grad_norm=78.931534, loss=17.840225
I0401 07:07:26.209043 140259223525184 submission.py:119] 21) loss = 17.840, grad_norm = 78.932
I0401 07:07:27.012206 140218481067776 logging_writer.py:48] [22] global_step=22, grad_norm=73.583275, loss=16.807253
I0401 07:07:27.015428 140259223525184 submission.py:119] 22) loss = 16.807, grad_norm = 73.583
I0401 07:07:27.816425 140218132854528 logging_writer.py:48] [23] global_step=23, grad_norm=71.729950, loss=16.285158
I0401 07:07:27.820234 140259223525184 submission.py:119] 23) loss = 16.285, grad_norm = 71.730
I0401 07:07:28.624062 140218481067776 logging_writer.py:48] [24] global_step=24, grad_norm=66.552467, loss=15.203464
I0401 07:07:28.627509 140259223525184 submission.py:119] 24) loss = 15.203, grad_norm = 66.552
I0401 07:07:29.434456 140218132854528 logging_writer.py:48] [25] global_step=25, grad_norm=61.175537, loss=14.281148
I0401 07:07:29.437872 140259223525184 submission.py:119] 25) loss = 14.281, grad_norm = 61.176
I0401 07:07:30.244359 140218481067776 logging_writer.py:48] [26] global_step=26, grad_norm=51.251717, loss=13.094180
I0401 07:07:30.247694 140259223525184 submission.py:119] 26) loss = 13.094, grad_norm = 51.252
I0401 07:07:31.055121 140218132854528 logging_writer.py:48] [27] global_step=27, grad_norm=54.773342, loss=12.639783
I0401 07:07:31.058808 140259223525184 submission.py:119] 27) loss = 12.640, grad_norm = 54.773
I0401 07:07:31.863667 140218481067776 logging_writer.py:48] [28] global_step=28, grad_norm=60.546101, loss=11.940740
I0401 07:07:31.867421 140259223525184 submission.py:119] 28) loss = 11.941, grad_norm = 60.546
I0401 07:07:32.671888 140218132854528 logging_writer.py:48] [29] global_step=29, grad_norm=51.187744, loss=10.931932
I0401 07:07:32.675042 140259223525184 submission.py:119] 29) loss = 10.932, grad_norm = 51.188
I0401 07:07:33.484410 140218481067776 logging_writer.py:48] [30] global_step=30, grad_norm=76.795464, loss=10.041865
I0401 07:07:33.487870 140259223525184 submission.py:119] 30) loss = 10.042, grad_norm = 76.795
I0401 07:07:34.294300 140218132854528 logging_writer.py:48] [31] global_step=31, grad_norm=72.569954, loss=8.735620
I0401 07:07:34.297534 140259223525184 submission.py:119] 31) loss = 8.736, grad_norm = 72.570
I0401 07:07:35.102530 140218481067776 logging_writer.py:48] [32] global_step=32, grad_norm=37.861073, loss=7.746579
I0401 07:07:35.105819 140259223525184 submission.py:119] 32) loss = 7.747, grad_norm = 37.861
I0401 07:07:35.910902 140218132854528 logging_writer.py:48] [33] global_step=33, grad_norm=14.747834, loss=7.348778
I0401 07:07:35.914121 140259223525184 submission.py:119] 33) loss = 7.349, grad_norm = 14.748
I0401 07:07:36.721367 140218481067776 logging_writer.py:48] [34] global_step=34, grad_norm=7.109573, loss=7.245274
I0401 07:07:36.724707 140259223525184 submission.py:119] 34) loss = 7.245, grad_norm = 7.110
I0401 07:07:37.531006 140218132854528 logging_writer.py:48] [35] global_step=35, grad_norm=11.600173, loss=7.276341
I0401 07:07:37.534579 140259223525184 submission.py:119] 35) loss = 7.276, grad_norm = 11.600
I0401 07:07:38.343436 140218481067776 logging_writer.py:48] [36] global_step=36, grad_norm=14.931848, loss=7.375538
I0401 07:07:38.346892 140259223525184 submission.py:119] 36) loss = 7.376, grad_norm = 14.932
I0401 07:07:39.153645 140218132854528 logging_writer.py:48] [37] global_step=37, grad_norm=16.860258, loss=7.428696
I0401 07:07:39.156799 140259223525184 submission.py:119] 37) loss = 7.429, grad_norm = 16.860
I0401 07:07:39.963480 140218481067776 logging_writer.py:48] [38] global_step=38, grad_norm=17.277754, loss=7.451222
I0401 07:07:39.967026 140259223525184 submission.py:119] 38) loss = 7.451, grad_norm = 17.278
I0401 07:07:40.777470 140218132854528 logging_writer.py:48] [39] global_step=39, grad_norm=17.474953, loss=7.419722
I0401 07:07:40.780528 140259223525184 submission.py:119] 39) loss = 7.420, grad_norm = 17.475
I0401 07:07:41.588173 140218481067776 logging_writer.py:48] [40] global_step=40, grad_norm=16.850988, loss=7.372154
I0401 07:07:41.591627 140259223525184 submission.py:119] 40) loss = 7.372, grad_norm = 16.851
I0401 07:07:42.410938 140218132854528 logging_writer.py:48] [41] global_step=41, grad_norm=16.204437, loss=7.327782
I0401 07:07:42.414442 140259223525184 submission.py:119] 41) loss = 7.328, grad_norm = 16.204
I0401 07:07:43.218525 140218481067776 logging_writer.py:48] [42] global_step=42, grad_norm=14.669950, loss=7.252841
I0401 07:07:43.222030 140259223525184 submission.py:119] 42) loss = 7.253, grad_norm = 14.670
I0401 07:07:44.026026 140218132854528 logging_writer.py:48] [43] global_step=43, grad_norm=12.927164, loss=7.182703
I0401 07:07:44.029074 140259223525184 submission.py:119] 43) loss = 7.183, grad_norm = 12.927
I0401 07:07:44.837066 140218481067776 logging_writer.py:48] [44] global_step=44, grad_norm=10.881908, loss=7.121118
I0401 07:07:44.840332 140259223525184 submission.py:119] 44) loss = 7.121, grad_norm = 10.882
I0401 07:07:45.648082 140218132854528 logging_writer.py:48] [45] global_step=45, grad_norm=8.431897, loss=7.080780
I0401 07:07:45.652265 140259223525184 submission.py:119] 45) loss = 7.081, grad_norm = 8.432
I0401 07:07:46.458388 140218481067776 logging_writer.py:48] [46] global_step=46, grad_norm=6.189977, loss=7.025169
I0401 07:07:46.461659 140259223525184 submission.py:119] 46) loss = 7.025, grad_norm = 6.190
I0401 07:07:47.267435 140218132854528 logging_writer.py:48] [47] global_step=47, grad_norm=3.722684, loss=6.994543
I0401 07:07:47.271275 140259223525184 submission.py:119] 47) loss = 6.995, grad_norm = 3.723
I0401 07:07:48.079869 140218481067776 logging_writer.py:48] [48] global_step=48, grad_norm=3.589414, loss=6.969681
I0401 07:07:48.083189 140259223525184 submission.py:119] 48) loss = 6.970, grad_norm = 3.589
I0401 07:07:48.888625 140218132854528 logging_writer.py:48] [49] global_step=49, grad_norm=5.084329, loss=6.983222
I0401 07:07:48.893027 140259223525184 submission.py:119] 49) loss = 6.983, grad_norm = 5.084
I0401 07:07:49.700280 140218481067776 logging_writer.py:48] [50] global_step=50, grad_norm=5.412885, loss=6.986264
I0401 07:07:49.704077 140259223525184 submission.py:119] 50) loss = 6.986, grad_norm = 5.413
I0401 07:07:50.508534 140218132854528 logging_writer.py:48] [51] global_step=51, grad_norm=5.833771, loss=6.959280
I0401 07:07:50.512451 140259223525184 submission.py:119] 51) loss = 6.959, grad_norm = 5.834
I0401 07:07:51.317820 140218481067776 logging_writer.py:48] [52] global_step=52, grad_norm=5.439641, loss=6.962084
I0401 07:07:51.321231 140259223525184 submission.py:119] 52) loss = 6.962, grad_norm = 5.440
I0401 07:07:52.129534 140218132854528 logging_writer.py:48] [53] global_step=53, grad_norm=5.259616, loss=6.943029
I0401 07:07:52.133182 140259223525184 submission.py:119] 53) loss = 6.943, grad_norm = 5.260
I0401 07:07:52.940412 140218481067776 logging_writer.py:48] [54] global_step=54, grad_norm=5.191235, loss=6.926499
I0401 07:07:52.943858 140259223525184 submission.py:119] 54) loss = 6.926, grad_norm = 5.191
I0401 07:07:53.748608 140218132854528 logging_writer.py:48] [55] global_step=55, grad_norm=4.479683, loss=6.910811
I0401 07:07:53.752523 140259223525184 submission.py:119] 55) loss = 6.911, grad_norm = 4.480
I0401 07:07:54.558154 140218481067776 logging_writer.py:48] [56] global_step=56, grad_norm=3.076040, loss=6.885108
I0401 07:07:54.561898 140259223525184 submission.py:119] 56) loss = 6.885, grad_norm = 3.076
I0401 07:07:55.370959 140218132854528 logging_writer.py:48] [57] global_step=57, grad_norm=2.855416, loss=6.872594
I0401 07:07:55.374963 140259223525184 submission.py:119] 57) loss = 6.873, grad_norm = 2.855
I0401 07:07:56.182380 140218481067776 logging_writer.py:48] [58] global_step=58, grad_norm=2.762813, loss=6.867067
I0401 07:07:56.186484 140259223525184 submission.py:119] 58) loss = 6.867, grad_norm = 2.763
I0401 07:07:56.991830 140218132854528 logging_writer.py:48] [59] global_step=59, grad_norm=2.820497, loss=6.846720
I0401 07:07:56.995463 140259223525184 submission.py:119] 59) loss = 6.847, grad_norm = 2.820
I0401 07:07:57.801319 140218481067776 logging_writer.py:48] [60] global_step=60, grad_norm=2.828236, loss=6.840766
I0401 07:07:57.804769 140259223525184 submission.py:119] 60) loss = 6.841, grad_norm = 2.828
I0401 07:07:58.613823 140218132854528 logging_writer.py:48] [61] global_step=61, grad_norm=3.159128, loss=6.821032
I0401 07:07:58.617344 140259223525184 submission.py:119] 61) loss = 6.821, grad_norm = 3.159
I0401 07:07:59.423840 140218481067776 logging_writer.py:48] [62] global_step=62, grad_norm=2.753731, loss=6.814692
I0401 07:07:59.427795 140259223525184 submission.py:119] 62) loss = 6.815, grad_norm = 2.754
I0401 07:08:00.236399 140218132854528 logging_writer.py:48] [63] global_step=63, grad_norm=2.525202, loss=6.809916
I0401 07:08:00.240477 140259223525184 submission.py:119] 63) loss = 6.810, grad_norm = 2.525
I0401 07:08:01.044543 140218481067776 logging_writer.py:48] [64] global_step=64, grad_norm=2.536068, loss=6.783607
I0401 07:08:01.047903 140259223525184 submission.py:119] 64) loss = 6.784, grad_norm = 2.536
I0401 07:08:01.856636 140218132854528 logging_writer.py:48] [65] global_step=65, grad_norm=2.421127, loss=6.790206
I0401 07:08:01.860890 140259223525184 submission.py:119] 65) loss = 6.790, grad_norm = 2.421
I0401 07:08:02.670010 140218481067776 logging_writer.py:48] [66] global_step=66, grad_norm=2.441321, loss=6.757032
I0401 07:08:02.673746 140259223525184 submission.py:119] 66) loss = 6.757, grad_norm = 2.441
I0401 07:08:03.483136 140218132854528 logging_writer.py:48] [67] global_step=67, grad_norm=2.387185, loss=6.731732
I0401 07:08:03.487317 140259223525184 submission.py:119] 67) loss = 6.732, grad_norm = 2.387
I0401 07:08:04.293159 140218481067776 logging_writer.py:48] [68] global_step=68, grad_norm=2.446928, loss=6.737530
I0401 07:08:04.297301 140259223525184 submission.py:119] 68) loss = 6.738, grad_norm = 2.447
I0401 07:08:05.101032 140218132854528 logging_writer.py:48] [69] global_step=69, grad_norm=3.144050, loss=6.742021
I0401 07:08:05.104633 140259223525184 submission.py:119] 69) loss = 6.742, grad_norm = 3.144
I0401 07:08:05.907848 140218481067776 logging_writer.py:48] [70] global_step=70, grad_norm=2.784708, loss=6.716521
I0401 07:08:05.911365 140259223525184 submission.py:119] 70) loss = 6.717, grad_norm = 2.785
I0401 07:08:06.722618 140218132854528 logging_writer.py:48] [71] global_step=71, grad_norm=2.740164, loss=6.699525
I0401 07:08:06.726119 140259223525184 submission.py:119] 71) loss = 6.700, grad_norm = 2.740
I0401 07:08:07.534542 140218481067776 logging_writer.py:48] [72] global_step=72, grad_norm=2.315807, loss=6.675270
I0401 07:08:07.538466 140259223525184 submission.py:119] 72) loss = 6.675, grad_norm = 2.316
I0401 07:08:08.348902 140218132854528 logging_writer.py:48] [73] global_step=73, grad_norm=2.329946, loss=6.678802
I0401 07:08:08.352564 140259223525184 submission.py:119] 73) loss = 6.679, grad_norm = 2.330
I0401 07:08:09.160507 140218481067776 logging_writer.py:48] [74] global_step=74, grad_norm=2.142843, loss=6.657963
I0401 07:08:09.164301 140259223525184 submission.py:119] 74) loss = 6.658, grad_norm = 2.143
I0401 07:08:09.978700 140218132854528 logging_writer.py:48] [75] global_step=75, grad_norm=2.223099, loss=6.648314
I0401 07:08:09.982388 140259223525184 submission.py:119] 75) loss = 6.648, grad_norm = 2.223
I0401 07:08:10.788606 140218481067776 logging_writer.py:48] [76] global_step=76, grad_norm=2.168926, loss=6.628113
I0401 07:08:10.792615 140259223525184 submission.py:119] 76) loss = 6.628, grad_norm = 2.169
I0401 07:08:11.600467 140218132854528 logging_writer.py:48] [77] global_step=77, grad_norm=2.094877, loss=6.618925
I0401 07:08:11.603925 140259223525184 submission.py:119] 77) loss = 6.619, grad_norm = 2.095
I0401 07:08:12.407591 140218481067776 logging_writer.py:48] [78] global_step=78, grad_norm=2.071647, loss=6.619503
I0401 07:08:12.411977 140259223525184 submission.py:119] 78) loss = 6.620, grad_norm = 2.072
I0401 07:08:13.218406 140218132854528 logging_writer.py:48] [79] global_step=79, grad_norm=2.064622, loss=6.603185
I0401 07:08:13.221805 140259223525184 submission.py:119] 79) loss = 6.603, grad_norm = 2.065
I0401 07:08:14.025213 140218481067776 logging_writer.py:48] [80] global_step=80, grad_norm=2.308422, loss=6.584250
I0401 07:08:14.029029 140259223525184 submission.py:119] 80) loss = 6.584, grad_norm = 2.308
I0401 07:08:14.837038 140218132854528 logging_writer.py:48] [81] global_step=81, grad_norm=2.166877, loss=6.571208
I0401 07:08:14.841377 140259223525184 submission.py:119] 81) loss = 6.571, grad_norm = 2.167
I0401 07:08:15.647851 140218481067776 logging_writer.py:48] [82] global_step=82, grad_norm=2.171114, loss=6.574244
I0401 07:08:15.652107 140259223525184 submission.py:119] 82) loss = 6.574, grad_norm = 2.171
I0401 07:08:16.458634 140218132854528 logging_writer.py:48] [83] global_step=83, grad_norm=2.157695, loss=6.564386
I0401 07:08:16.462139 140259223525184 submission.py:119] 83) loss = 6.564, grad_norm = 2.158
I0401 07:08:17.272994 140218481067776 logging_writer.py:48] [84] global_step=84, grad_norm=1.926696, loss=6.539234
I0401 07:08:17.276816 140259223525184 submission.py:119] 84) loss = 6.539, grad_norm = 1.927
I0401 07:08:18.086250 140218132854528 logging_writer.py:48] [85] global_step=85, grad_norm=2.009580, loss=6.522287
I0401 07:08:18.089826 140259223525184 submission.py:119] 85) loss = 6.522, grad_norm = 2.010
I0401 07:08:18.891330 140218481067776 logging_writer.py:48] [86] global_step=86, grad_norm=1.870245, loss=6.497999
I0401 07:08:18.895089 140259223525184 submission.py:119] 86) loss = 6.498, grad_norm = 1.870
I0401 07:08:19.699936 140218132854528 logging_writer.py:48] [87] global_step=87, grad_norm=1.897139, loss=6.502980
I0401 07:08:19.704063 140259223525184 submission.py:119] 87) loss = 6.503, grad_norm = 1.897
I0401 07:08:20.508751 140218481067776 logging_writer.py:48] [88] global_step=88, grad_norm=1.874773, loss=6.498354
I0401 07:08:20.512408 140259223525184 submission.py:119] 88) loss = 6.498, grad_norm = 1.875
I0401 07:08:21.319103 140218132854528 logging_writer.py:48] [89] global_step=89, grad_norm=1.982685, loss=6.497025
I0401 07:08:21.322502 140259223525184 submission.py:119] 89) loss = 6.497, grad_norm = 1.983
I0401 07:08:22.131604 140218481067776 logging_writer.py:48] [90] global_step=90, grad_norm=1.739317, loss=6.462321
I0401 07:08:22.135525 140259223525184 submission.py:119] 90) loss = 6.462, grad_norm = 1.739
I0401 07:08:22.942109 140218132854528 logging_writer.py:48] [91] global_step=91, grad_norm=2.069690, loss=6.457397
I0401 07:08:22.945519 140259223525184 submission.py:119] 91) loss = 6.457, grad_norm = 2.070
I0401 07:08:23.752782 140218481067776 logging_writer.py:48] [92] global_step=92, grad_norm=1.736079, loss=6.435764
I0401 07:08:23.757079 140259223525184 submission.py:119] 92) loss = 6.436, grad_norm = 1.736
I0401 07:08:24.570902 140218132854528 logging_writer.py:48] [93] global_step=93, grad_norm=1.693835, loss=6.427721
I0401 07:08:24.574294 140259223525184 submission.py:119] 93) loss = 6.428, grad_norm = 1.694
I0401 07:08:25.383894 140218481067776 logging_writer.py:48] [94] global_step=94, grad_norm=1.616010, loss=6.423254
I0401 07:08:25.387500 140259223525184 submission.py:119] 94) loss = 6.423, grad_norm = 1.616
I0401 07:08:26.192842 140218132854528 logging_writer.py:48] [95] global_step=95, grad_norm=1.597961, loss=6.404271
I0401 07:08:26.196248 140259223525184 submission.py:119] 95) loss = 6.404, grad_norm = 1.598
I0401 07:08:27.003325 140218481067776 logging_writer.py:48] [96] global_step=96, grad_norm=1.637573, loss=6.414081
I0401 07:08:27.007215 140259223525184 submission.py:119] 96) loss = 6.414, grad_norm = 1.638
I0401 07:08:27.812396 140218132854528 logging_writer.py:48] [97] global_step=97, grad_norm=1.575194, loss=6.389312
I0401 07:08:27.815988 140259223525184 submission.py:119] 97) loss = 6.389, grad_norm = 1.575
I0401 07:08:28.624657 140218481067776 logging_writer.py:48] [98] global_step=98, grad_norm=1.656889, loss=6.375739
I0401 07:08:28.628519 140259223525184 submission.py:119] 98) loss = 6.376, grad_norm = 1.657
I0401 07:08:29.436818 140218132854528 logging_writer.py:48] [99] global_step=99, grad_norm=1.693954, loss=6.367466
I0401 07:08:29.440506 140259223525184 submission.py:119] 99) loss = 6.367, grad_norm = 1.694
I0401 07:08:30.250032 140218481067776 logging_writer.py:48] [100] global_step=100, grad_norm=1.469616, loss=6.359763
I0401 07:08:30.253587 140259223525184 submission.py:119] 100) loss = 6.360, grad_norm = 1.470
I0401 07:13:50.209827 140218132854528 logging_writer.py:48] [500] global_step=500, grad_norm=0.443735, loss=5.793715
I0401 07:13:50.217223 140259223525184 submission.py:119] 500) loss = 5.794, grad_norm = 0.444
I0401 07:20:29.869532 140218481067776 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.440912, loss=5.452381
I0401 07:20:29.873944 140259223525184 submission.py:119] 1000) loss = 5.452, grad_norm = 1.441
I0401 07:27:11.122592 140224709523200 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.140945, loss=3.555246
I0401 07:27:11.129845 140259223525184 submission.py:119] 1500) loss = 3.555, grad_norm = 1.141
I0401 07:33:50.229979 140224701130496 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.265708, loss=2.927658
I0401 07:33:50.234496 140259223525184 submission.py:119] 2000) loss = 2.928, grad_norm = 1.266
I0401 07:40:30.611068 140224709523200 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.039711, loss=2.637908
I0401 07:40:30.618133 140259223525184 submission.py:119] 2500) loss = 2.638, grad_norm = 1.040
I0401 07:47:09.227707 140224701130496 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.795957, loss=2.477403
I0401 07:47:09.232008 140259223525184 submission.py:119] 3000) loss = 2.477, grad_norm = 0.796
I0401 07:47:09.234339 140259223525184 submission_runner.py:371] Before eval at step 3001: RAM USED (GB) 41.19570432
I0401 07:47:09.234478 140259223525184 spec.py:298] Evaluating on the training split.
I0401 07:47:20.295663 140259223525184 spec.py:310] Evaluating on the validation split.
I0401 07:47:30.450448 140259223525184 spec.py:326] Evaluating on the test split.
I0401 07:47:35.869946 140259223525184 submission_runner.py:380] Time since start: 2437.04s, 	Step: 3001, 	{'train/ctc_loss': 2.87291170668662, 'train/wer': 0.6149916864284352, 'validation/ctc_loss': 3.0622598327305606, 'validation/wer': 0.6219379133877275, 'validation/num_examples': 5348, 'test/ctc_loss': 2.7245661010387767, 'test/wer': 0.5653321958848739, 'test/num_examples': 2472}
I0401 07:47:35.870738 140259223525184 submission_runner.py:390] After eval at step 3001: RAM USED (GB) 39.88926464
I0401 07:47:35.884980 140224701130496 logging_writer.py:48] [3001] global_step=3001, preemption_count=0, score=2398.328870, test/ctc_loss=2.724566, test/num_examples=2472, test/wer=0.565332, total_duration=2437.035739, train/ctc_loss=2.872912, train/wer=0.614992, validation/ctc_loss=3.062260, validation/num_examples=5348, validation/wer=0.621938
I0401 07:47:36.468781 140259223525184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_3001.
I0401 07:47:36.469497 140259223525184 submission_runner.py:409] After logging and checkpointing eval at step 3001: RAM USED (GB) 39.895609344
I0401 07:54:16.549188 140224701130496 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.805518, loss=2.264102
I0401 07:54:16.589610 140259223525184 submission.py:119] 3500) loss = 2.264, grad_norm = 0.806
I0401 08:00:55.017821 140224692737792 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.205483, loss=2.170305
I0401 08:00:55.025355 140259223525184 submission.py:119] 4000) loss = 2.170, grad_norm = 1.205
I0401 08:07:34.696045 140224701130496 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.854064, loss=1.989016
I0401 08:07:34.702440 140259223525184 submission.py:119] 4500) loss = 1.989, grad_norm = 0.854
I0401 08:14:12.525615 140224692737792 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.632266, loss=1.941271
I0401 08:14:12.657027 140259223525184 submission.py:119] 5000) loss = 1.941, grad_norm = 0.632
I0401 08:20:52.347879 140224701130496 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.756588, loss=1.933375
I0401 08:20:52.355169 140259223525184 submission.py:119] 5500) loss = 1.933, grad_norm = 0.757
I0401 08:27:30.179984 140224692737792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.586450, loss=1.865490
I0401 08:27:30.184968 140259223525184 submission.py:119] 6000) loss = 1.865, grad_norm = 0.586
I0401 08:27:36.548358 140259223525184 submission_runner.py:371] Before eval at step 6009: RAM USED (GB) 39.980105728
I0401 08:27:36.548576 140259223525184 spec.py:298] Evaluating on the training split.
I0401 08:27:47.715831 140259223525184 spec.py:310] Evaluating on the validation split.
I0401 08:27:58.101706 140259223525184 spec.py:326] Evaluating on the test split.
I0401 08:28:03.600252 140259223525184 submission_runner.py:380] Time since start: 4864.35s, 	Step: 6009, 	{'train/ctc_loss': 0.6771175749098701, 'train/wer': 0.2276882819527353, 'validation/ctc_loss': 0.9004466444080269, 'validation/wer': 0.2670303674021146, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6049392299503193, 'test/wer': 0.19864724879653892, 'test/num_examples': 2472}
I0401 08:28:03.601067 140259223525184 submission_runner.py:390] After eval at step 6009: RAM USED (GB) 39.89143552
I0401 08:28:03.615273 140224692737792 logging_writer.py:48] [6009] global_step=6009, preemption_count=0, score=4789.553028, test/ctc_loss=0.604939, test/num_examples=2472, test/wer=0.198647, total_duration=4864.347436, train/ctc_loss=0.677118, train/wer=0.227688, validation/ctc_loss=0.900447, validation/num_examples=5348, validation/wer=0.267030
I0401 08:28:04.210075 140259223525184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_6009.
I0401 08:28:04.210782 140259223525184 submission_runner.py:409] After logging and checkpointing eval at step 6009: RAM USED (GB) 39.905501184
I0401 08:34:37.384802 140224692737792 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.484197, loss=1.731355
I0401 08:34:37.391410 140259223525184 submission.py:119] 6500) loss = 1.731, grad_norm = 0.484
I0401 08:41:15.255120 140224684345088 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.482669, loss=1.735366
I0401 08:41:15.259658 140259223525184 submission.py:119] 7000) loss = 1.735, grad_norm = 0.483
I0401 08:47:54.857071 140224692737792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.513402, loss=1.726481
I0401 08:47:54.863354 140259223525184 submission.py:119] 7500) loss = 1.726, grad_norm = 0.513
I0401 08:54:32.649886 140224684345088 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.433736, loss=1.607160
I0401 08:54:32.654453 140259223525184 submission.py:119] 8000) loss = 1.607, grad_norm = 0.434
I0401 09:01:12.408104 140224692737792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.620389, loss=1.606225
I0401 09:01:12.414269 140259223525184 submission.py:119] 8500) loss = 1.606, grad_norm = 0.620
I0401 09:07:50.148757 140224684345088 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.416494, loss=1.609297
I0401 09:07:50.153527 140259223525184 submission.py:119] 9000) loss = 1.609, grad_norm = 0.416
I0401 09:08:04.480082 140259223525184 submission_runner.py:371] Before eval at step 9019: RAM USED (GB) 40.014393344
I0401 09:08:04.480290 140259223525184 spec.py:298] Evaluating on the training split.
I0401 09:08:15.722240 140259223525184 spec.py:310] Evaluating on the validation split.
I0401 09:08:25.721653 140259223525184 spec.py:326] Evaluating on the test split.
I0401 09:08:31.395629 140259223525184 submission_runner.py:380] Time since start: 7292.28s, 	Step: 9019, 	{'train/ctc_loss': 0.49325880254744575, 'train/wer': 0.16862648894703847, 'validation/ctc_loss': 0.7182059936646072, 'validation/wer': 0.21542026746487714, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4580312641138138, 'test/wer': 0.1537383462311864, 'test/num_examples': 2472}
I0401 09:08:31.396422 140259223525184 submission_runner.py:390] After eval at step 9019: RAM USED (GB) 39.814438912
I0401 09:08:31.413337 140224684345088 logging_writer.py:48] [9019] global_step=9019, preemption_count=0, score=7181.024857, test/ctc_loss=0.458031, test/num_examples=2472, test/wer=0.153738, total_duration=7292.277033, train/ctc_loss=0.493259, train/wer=0.168626, validation/ctc_loss=0.718206, validation/num_examples=5348, validation/wer=0.215420
I0401 09:08:32.003005 140259223525184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_9019.
I0401 09:08:32.003747 140259223525184 submission_runner.py:409] After logging and checkpointing eval at step 9019: RAM USED (GB) 39.819808768
I0401 09:14:57.376097 140224684345088 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.469490, loss=1.581884
I0401 09:14:57.383256 140259223525184 submission.py:119] 9500) loss = 1.582, grad_norm = 0.469
I0401 09:21:34.226324 140259223525184 submission_runner.py:371] Before eval at step 10000: RAM USED (GB) 39.871647744
I0401 09:21:34.226691 140259223525184 spec.py:298] Evaluating on the training split.
I0401 09:21:45.517954 140259223525184 spec.py:310] Evaluating on the validation split.
I0401 09:21:55.458494 140259223525184 spec.py:326] Evaluating on the test split.
I0401 09:22:01.017496 140259223525184 submission_runner.py:380] Time since start: 8102.03s, 	Step: 10000, 	{'train/ctc_loss': 0.46365813762669206, 'train/wer': 0.1618175375473601, 'validation/ctc_loss': 0.6864137532650191, 'validation/wer': 0.20636315357504947, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4445219248016001, 'test/wer': 0.14985883452156074, 'test/num_examples': 2472}
I0401 09:22:01.018275 140259223525184 submission_runner.py:390] After eval at step 10000: RAM USED (GB) 39.842975744
I0401 09:22:01.038677 140224684345088 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7960.368705, test/ctc_loss=0.444522, test/num_examples=2472, test/wer=0.149859, total_duration=8102.026434, train/ctc_loss=0.463658, train/wer=0.161818, validation/ctc_loss=0.686414, validation/num_examples=5348, validation/wer=0.206363
I0401 09:22:01.612857 140259223525184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0401 09:22:01.613584 140259223525184 submission_runner.py:409] After logging and checkpointing eval at step 10000: RAM USED (GB) 39.871721472
I0401 09:22:01.621724 140224675952384 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7960.368705
I0401 09:22:02.756773 140259223525184 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_adamw/librispeech_conformer_pytorch/trial_1/checkpoint_10000.
I0401 09:22:02.874710 140259223525184 submission_runner.py:543] Tuning trial 1/1
I0401 09:22:02.874914 140259223525184 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.22838767981804783, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0401 09:22:02.875335 140259223525184 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ctc_loss': 29.12666825386028, 'train/wer': 1.8829994275901545, 'validation/ctc_loss': 28.493184272654208, 'validation/wer': 1.6299811712451118, 'validation/num_examples': 5348, 'test/ctc_loss': 28.64129621265888, 'test/wer': 1.6906343306318932, 'test/num_examples': 2472, 'score': 6.4536521434783936, 'total_duration': 6.455501556396484, 'global_step': 1, 'preemption_count': 0}), (3001, {'train/ctc_loss': 2.87291170668662, 'train/wer': 0.6149916864284352, 'validation/ctc_loss': 3.0622598327305606, 'validation/wer': 0.6219379133877275, 'validation/num_examples': 5348, 'test/ctc_loss': 2.7245661010387767, 'test/wer': 0.5653321958848739, 'test/num_examples': 2472, 'score': 2398.328869819641, 'total_duration': 2437.0357387065887, 'global_step': 3001, 'preemption_count': 0}), (6009, {'train/ctc_loss': 0.6771175749098701, 'train/wer': 0.2276882819527353, 'validation/ctc_loss': 0.9004466444080269, 'validation/wer': 0.2670303674021146, 'validation/num_examples': 5348, 'test/ctc_loss': 0.6049392299503193, 'test/wer': 0.19864724879653892, 'test/num_examples': 2472, 'score': 4789.553027629852, 'total_duration': 4864.347435951233, 'global_step': 6009, 'preemption_count': 0}), (9019, {'train/ctc_loss': 0.49325880254744575, 'train/wer': 0.16862648894703847, 'validation/ctc_loss': 0.7182059936646072, 'validation/wer': 0.21542026746487714, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4580312641138138, 'test/wer': 0.1537383462311864, 'test/num_examples': 2472, 'score': 7181.024856567383, 'total_duration': 7292.277032613754, 'global_step': 9019, 'preemption_count': 0}), (10000, {'train/ctc_loss': 0.46365813762669206, 'train/wer': 0.1618175375473601, 'validation/ctc_loss': 0.6864137532650191, 'validation/wer': 0.20636315357504947, 'validation/num_examples': 5348, 'test/ctc_loss': 0.4445219248016001, 'test/wer': 0.14985883452156074, 'test/num_examples': 2472, 'score': 7960.368704557419, 'total_duration': 8102.026434421539, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 09:22:02.875414 140259223525184 submission_runner.py:546] Timing: 7960.368704557419
I0401 09:22:02.875481 140259223525184 submission_runner.py:547] ====================
I0401 09:22:02.875636 140259223525184 submission_runner.py:606] Final librispeech_conformer score: 7960.368704557419
