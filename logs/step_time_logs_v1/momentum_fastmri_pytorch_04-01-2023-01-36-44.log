WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
I0401 01:37:05.784594 140140857395008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
I0401 01:37:05.784645 140519487244096 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
I0401 01:37:05.784666 139998883948352 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 5
I0401 01:37:05.785367 140448190633792 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 6
I0401 01:37:05.785539 140352790218560 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 7
I0401 01:37:05.785707 140116678195008 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
I0401 01:37:05.785779 140281992439616 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
I0401 01:37:05.795640 139874520373056 distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
I0401 01:37:05.795943 139874520373056 distributed_c10d.py:353] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.796052 140448190633792 distributed_c10d.py:353] Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.796158 140352790218560 distributed_c10d.py:353] Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.796342 140116678195008 distributed_c10d.py:353] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.796382 140281992439616 distributed_c10d.py:353] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.805521 140140857395008 distributed_c10d.py:353] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.805565 140519487244096 distributed_c10d.py:353] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:05.805585 139998883948352 distributed_c10d.py:353] Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
I0401 01:37:06.332929 139874520373056 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_momentum/fastmri_pytorch.
W0401 01:37:06.455236 140519487244096 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.455563 140140857395008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.456687 140448190633792 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.457008 140281992439616 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.457090 140352790218560 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.457212 140116678195008 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.457260 139998883948352 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
W0401 01:37:06.459817 139874520373056 xla_bridge.py:352] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0401 01:37:06.463601 139874520373056 submission_runner.py:504] Using RNG seed 3082744278
I0401 01:37:06.464760 139874520373056 submission_runner.py:513] --- Tuning run 1/1 ---
I0401 01:37:06.464885 139874520373056 submission_runner.py:518] Creating tuning directory at /experiment_runs/timing_momentum/fastmri_pytorch/trial_1.
I0401 01:37:06.465114 139874520373056 logger_utils.py:84] Saving hparams to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/hparams.json.
I0401 01:37:06.466142 139874520373056 submission_runner.py:230] Starting train once: RAM USED (GB) 5.64348928
I0401 01:37:06.466246 139874520373056 submission_runner.py:231] Initializing dataset.
I0401 01:37:06.466429 139874520373056 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 5.64348928
I0401 01:37:06.466504 139874520373056 submission_runner.py:240] Initializing model.
I0401 01:37:10.765922 139874520373056 submission_runner.py:251] After Initializing model: RAM USED (GB) 15.29987072
I0401 01:37:10.766111 139874520373056 submission_runner.py:252] Initializing optimizer.
I0401 01:37:11.325884 139874520373056 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 15.30392576
I0401 01:37:11.326074 139874520373056 submission_runner.py:261] Initializing metrics bundle.
I0401 01:37:11.326126 139874520373056 submission_runner.py:275] Initializing checkpoint and logger.
I0401 01:37:11.329268 139874520373056 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 01:37:11.329438 139874520373056 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 01:37:11.909509 139874520373056 submission_runner.py:296] Saving meta data to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/meta_data_0.json.
I0401 01:37:11.910565 139874520373056 submission_runner.py:299] Saving flags to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/flags_0.json.
I0401 01:37:11.953309 139874520373056 submission_runner.py:304] After checkpoint and logger metrics bundle: RAM USED (GB) 15.353393152
I0401 01:37:11.954351 139874520373056 submission_runner.py:311] Before starting training loop and logger metrics bundle: RAM USED (GB) 15.353393152
I0401 01:37:11.954469 139874520373056 submission_runner.py:312] Starting training loop.
I0401 01:37:54.128080 139874520373056 submission_runner.py:333] After dataselection batch at step 0: RAM USED (GB) 23.855955968
I0401 01:37:58.167804 139832282048256 logging_writer.py:48] [0] global_step=0, grad_norm=4.382926, loss=0.735154
I0401 01:37:58.175550 139874520373056 submission.py:139] 0) loss = 0.735, grad_norm = 4.383
I0401 01:37:58.176130 139874520373056 submission_runner.py:350] After update parameters step 0: RAM USED (GB) 32.685215744
I0401 01:37:58.176762 139874520373056 submission_runner.py:371] Before eval at step 1: RAM USED (GB) 32.68573184
I0401 01:37:58.176883 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:39:33.857299 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:40:39.580513 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:41:43.363049 139874520373056 submission_runner.py:380] Time since start: 46.22s, 	Step: 1, 	{'train/ssim': 0.28941198757716585, 'train/loss': 0.7752715519496373, 'validation/ssim': 0.28286531928153136, 'validation/loss': 0.7852436295371412, 'validation/num_examples': 3554, 'test/ssim': 0.3056446321514591, 'test/loss': 0.781013086101124, 'test/num_examples': 3581}
I0401 01:41:43.363426 139874520373056 submission_runner.py:390] After eval at step 1: RAM USED (GB) 69.0241536
I0401 01:41:43.373522 139807720187648 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=46.220869, test/loss=0.781013, test/num_examples=3581, test/ssim=0.305645, total_duration=46.222795, train/loss=0.775272, train/ssim=0.289412, validation/loss=0.785244, validation/num_examples=3554, validation/ssim=0.282865
I0401 01:41:43.477301 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1.
I0401 01:41:43.477774 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 1: RAM USED (GB) 69.02439936
I0401 01:41:43.489924 139874520373056 submission_runner.py:333] After dataselection batch at step 1: RAM USED (GB) 69.034160128
I0401 01:41:43.492822 140140857395008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492827 140448190633792 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492832 140352790218560 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492876 139998883948352 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492969 139874520373056 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492920 140116678195008 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492934 140281992439616 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.492992 140519487244096 distributed.py:1027] Reducer buckets have been rebuilt in this iteration.
I0401 01:41:43.553691 139807711794944 logging_writer.py:48] [1] global_step=1, grad_norm=4.403135, loss=0.721077
I0401 01:41:43.560064 139874520373056 submission.py:139] 1) loss = 0.721, grad_norm = 4.403
I0401 01:41:43.560973 139874520373056 submission_runner.py:350] After update parameters step 1: RAM USED (GB) 69.040459776
I0401 01:41:43.631016 139807720187648 logging_writer.py:48] [2] global_step=2, grad_norm=4.342638, loss=0.733803
I0401 01:41:43.635687 139874520373056 submission.py:139] 2) loss = 0.734, grad_norm = 4.343
I0401 01:41:43.704690 139807711794944 logging_writer.py:48] [3] global_step=3, grad_norm=4.369522, loss=0.706104
I0401 01:41:43.707945 139874520373056 submission.py:139] 3) loss = 0.706, grad_norm = 4.370
I0401 01:41:43.771562 139807720187648 logging_writer.py:48] [4] global_step=4, grad_norm=3.126874, loss=0.814616
I0401 01:41:43.775790 139874520373056 submission.py:139] 4) loss = 0.815, grad_norm = 3.127
I0401 01:41:43.849903 139807711794944 logging_writer.py:48] [5] global_step=5, grad_norm=2.423303, loss=0.660851
I0401 01:41:43.855544 139874520373056 submission.py:139] 5) loss = 0.661, grad_norm = 2.423
I0401 01:41:43.928069 139807720187648 logging_writer.py:48] [6] global_step=6, grad_norm=1.606544, loss=0.621720
I0401 01:41:43.933602 139874520373056 submission.py:139] 6) loss = 0.622, grad_norm = 1.607
I0401 01:41:44.003024 139807711794944 logging_writer.py:48] [7] global_step=7, grad_norm=1.075767, loss=0.587041
I0401 01:41:44.006398 139874520373056 submission.py:139] 7) loss = 0.587, grad_norm = 1.076
I0401 01:41:44.076585 139807720187648 logging_writer.py:48] [8] global_step=8, grad_norm=0.934925, loss=0.532694
I0401 01:41:44.081421 139874520373056 submission.py:139] 8) loss = 0.533, grad_norm = 0.935
I0401 01:41:44.156343 139807711794944 logging_writer.py:48] [9] global_step=9, grad_norm=1.308508, loss=0.545984
I0401 01:41:44.160770 139874520373056 submission.py:139] 9) loss = 0.546, grad_norm = 1.309
I0401 01:41:44.235009 139807720187648 logging_writer.py:48] [10] global_step=10, grad_norm=1.644898, loss=0.598581
I0401 01:41:44.238604 139874520373056 submission.py:139] 10) loss = 0.599, grad_norm = 1.645
I0401 01:41:44.306970 139807711794944 logging_writer.py:48] [11] global_step=11, grad_norm=1.778620, loss=0.590355
I0401 01:41:44.311041 139874520373056 submission.py:139] 11) loss = 0.590, grad_norm = 1.779
I0401 01:41:44.383504 139807720187648 logging_writer.py:48] [12] global_step=12, grad_norm=1.956972, loss=0.550026
I0401 01:41:44.388766 139874520373056 submission.py:139] 12) loss = 0.550, grad_norm = 1.957
I0401 01:41:44.458309 139807711794944 logging_writer.py:48] [13] global_step=13, grad_norm=1.961994, loss=0.511211
I0401 01:41:44.463684 139874520373056 submission.py:139] 13) loss = 0.511, grad_norm = 1.962
I0401 01:41:44.549224 139807720187648 logging_writer.py:48] [14] global_step=14, grad_norm=1.616689, loss=0.507990
I0401 01:41:44.552763 139874520373056 submission.py:139] 14) loss = 0.508, grad_norm = 1.617
I0401 01:41:44.807410 139807711794944 logging_writer.py:48] [15] global_step=15, grad_norm=1.315030, loss=0.496384
I0401 01:41:44.811111 139874520373056 submission.py:139] 15) loss = 0.496, grad_norm = 1.315
I0401 01:41:45.057045 139807720187648 logging_writer.py:48] [16] global_step=16, grad_norm=0.945150, loss=0.492057
I0401 01:41:45.060602 139874520373056 submission.py:139] 16) loss = 0.492, grad_norm = 0.945
I0401 01:41:45.306448 139807711794944 logging_writer.py:48] [17] global_step=17, grad_norm=0.840982, loss=0.395627
I0401 01:41:45.311564 139874520373056 submission.py:139] 17) loss = 0.396, grad_norm = 0.841
I0401 01:41:45.611916 139807720187648 logging_writer.py:48] [18] global_step=18, grad_norm=1.117017, loss=0.388390
I0401 01:41:45.618296 139874520373056 submission.py:139] 18) loss = 0.388, grad_norm = 1.117
I0401 01:41:45.882310 139807711794944 logging_writer.py:48] [19] global_step=19, grad_norm=1.222072, loss=0.387082
I0401 01:41:45.888578 139874520373056 submission.py:139] 19) loss = 0.387, grad_norm = 1.222
I0401 01:41:46.116868 139807720187648 logging_writer.py:48] [20] global_step=20, grad_norm=1.271481, loss=0.430666
I0401 01:41:46.121423 139874520373056 submission.py:139] 20) loss = 0.431, grad_norm = 1.271
I0401 01:41:46.472497 139807711794944 logging_writer.py:48] [21] global_step=21, grad_norm=1.391651, loss=0.466370
I0401 01:41:46.477355 139874520373056 submission.py:139] 21) loss = 0.466, grad_norm = 1.392
I0401 01:41:46.724747 139807720187648 logging_writer.py:48] [22] global_step=22, grad_norm=1.254675, loss=0.453664
I0401 01:41:46.729674 139874520373056 submission.py:139] 22) loss = 0.454, grad_norm = 1.255
I0401 01:41:46.973183 139807711794944 logging_writer.py:48] [23] global_step=23, grad_norm=1.106575, loss=0.429986
I0401 01:41:46.978820 139874520373056 submission.py:139] 23) loss = 0.430, grad_norm = 1.107
I0401 01:41:47.284863 139807720187648 logging_writer.py:48] [24] global_step=24, grad_norm=0.643071, loss=0.410536
I0401 01:41:47.290377 139874520373056 submission.py:139] 24) loss = 0.411, grad_norm = 0.643
I0401 01:41:47.452145 139807711794944 logging_writer.py:48] [25] global_step=25, grad_norm=0.546053, loss=0.384880
I0401 01:41:47.455906 139874520373056 submission.py:139] 25) loss = 0.385, grad_norm = 0.546
I0401 01:41:47.737671 139807720187648 logging_writer.py:48] [26] global_step=26, grad_norm=0.645031, loss=0.352557
I0401 01:41:47.742603 139874520373056 submission.py:139] 26) loss = 0.353, grad_norm = 0.645
I0401 01:41:48.009850 139807711794944 logging_writer.py:48] [27] global_step=27, grad_norm=0.799948, loss=0.419978
I0401 01:41:48.013344 139874520373056 submission.py:139] 27) loss = 0.420, grad_norm = 0.800
I0401 01:41:48.289426 139807720187648 logging_writer.py:48] [28] global_step=28, grad_norm=0.922033, loss=0.311775
I0401 01:41:48.292659 139874520373056 submission.py:139] 28) loss = 0.312, grad_norm = 0.922
I0401 01:41:48.579591 139807711794944 logging_writer.py:48] [29] global_step=29, grad_norm=0.916964, loss=0.414760
I0401 01:41:48.583375 139874520373056 submission.py:139] 29) loss = 0.415, grad_norm = 0.917
I0401 01:41:48.798707 139807720187648 logging_writer.py:48] [30] global_step=30, grad_norm=0.725793, loss=0.388245
I0401 01:41:48.802488 139874520373056 submission.py:139] 30) loss = 0.388, grad_norm = 0.726
I0401 01:41:49.089650 139807711794944 logging_writer.py:48] [31] global_step=31, grad_norm=0.532465, loss=0.359259
I0401 01:41:49.093423 139874520373056 submission.py:139] 31) loss = 0.359, grad_norm = 0.532
I0401 01:41:49.324462 139807720187648 logging_writer.py:48] [32] global_step=32, grad_norm=0.280930, loss=0.401691
I0401 01:41:49.327755 139874520373056 submission.py:139] 32) loss = 0.402, grad_norm = 0.281
I0401 01:41:49.636609 139807711794944 logging_writer.py:48] [33] global_step=33, grad_norm=0.430029, loss=0.313292
I0401 01:41:49.639908 139874520373056 submission.py:139] 33) loss = 0.313, grad_norm = 0.430
I0401 01:41:49.865271 139807720187648 logging_writer.py:48] [34] global_step=34, grad_norm=0.532735, loss=0.344644
I0401 01:41:49.868529 139874520373056 submission.py:139] 34) loss = 0.345, grad_norm = 0.533
I0401 01:41:50.115081 139807711794944 logging_writer.py:48] [35] global_step=35, grad_norm=0.580606, loss=0.411636
I0401 01:41:50.120252 139874520373056 submission.py:139] 35) loss = 0.412, grad_norm = 0.581
I0401 01:41:50.394068 139807720187648 logging_writer.py:48] [36] global_step=36, grad_norm=0.452151, loss=0.446759
I0401 01:41:50.401112 139874520373056 submission.py:139] 36) loss = 0.447, grad_norm = 0.452
I0401 01:41:50.667335 139807711794944 logging_writer.py:48] [37] global_step=37, grad_norm=0.591512, loss=0.429266
I0401 01:41:50.671767 139874520373056 submission.py:139] 37) loss = 0.429, grad_norm = 0.592
I0401 01:41:50.879393 139807720187648 logging_writer.py:48] [38] global_step=38, grad_norm=0.633138, loss=0.291991
I0401 01:41:50.883614 139874520373056 submission.py:139] 38) loss = 0.292, grad_norm = 0.633
I0401 01:41:51.178489 139807711794944 logging_writer.py:48] [39] global_step=39, grad_norm=0.670364, loss=0.316212
I0401 01:41:51.184381 139874520373056 submission.py:139] 39) loss = 0.316, grad_norm = 0.670
I0401 01:41:51.420530 139807720187648 logging_writer.py:48] [40] global_step=40, grad_norm=0.196135, loss=0.353389
I0401 01:41:51.423803 139874520373056 submission.py:139] 40) loss = 0.353, grad_norm = 0.196
I0401 01:41:51.674745 139807711794944 logging_writer.py:48] [41] global_step=41, grad_norm=0.131273, loss=0.384306
I0401 01:41:51.678089 139874520373056 submission.py:139] 41) loss = 0.384, grad_norm = 0.131
I0401 01:41:51.986095 139807720187648 logging_writer.py:48] [42] global_step=42, grad_norm=0.235540, loss=0.361113
I0401 01:41:51.992460 139874520373056 submission.py:139] 42) loss = 0.361, grad_norm = 0.236
I0401 01:41:52.236561 139807711794944 logging_writer.py:48] [43] global_step=43, grad_norm=0.502712, loss=0.405199
I0401 01:41:52.242152 139874520373056 submission.py:139] 43) loss = 0.405, grad_norm = 0.503
I0401 01:41:52.507898 139807720187648 logging_writer.py:48] [44] global_step=44, grad_norm=0.508751, loss=0.405912
I0401 01:41:52.511348 139874520373056 submission.py:139] 44) loss = 0.406, grad_norm = 0.509
I0401 01:41:52.761674 139807711794944 logging_writer.py:48] [45] global_step=45, grad_norm=0.664683, loss=0.397311
I0401 01:41:52.765454 139874520373056 submission.py:139] 45) loss = 0.397, grad_norm = 0.665
I0401 01:41:53.024577 139807720187648 logging_writer.py:48] [46] global_step=46, grad_norm=0.453640, loss=0.401773
I0401 01:41:53.029231 139874520373056 submission.py:139] 46) loss = 0.402, grad_norm = 0.454
I0401 01:41:53.322460 139807711794944 logging_writer.py:48] [47] global_step=47, grad_norm=0.329755, loss=0.424041
I0401 01:41:53.325942 139874520373056 submission.py:139] 47) loss = 0.424, grad_norm = 0.330
I0401 01:41:53.565333 139807720187648 logging_writer.py:48] [48] global_step=48, grad_norm=0.101389, loss=0.414988
I0401 01:41:53.568483 139874520373056 submission.py:139] 48) loss = 0.415, grad_norm = 0.101
I0401 01:41:53.845522 139807711794944 logging_writer.py:48] [49] global_step=49, grad_norm=0.178788, loss=0.330905
I0401 01:41:53.848814 139874520373056 submission.py:139] 49) loss = 0.331, grad_norm = 0.179
I0401 01:41:54.105055 139807720187648 logging_writer.py:48] [50] global_step=50, grad_norm=0.267945, loss=0.353408
I0401 01:41:54.108154 139874520373056 submission.py:139] 50) loss = 0.353, grad_norm = 0.268
I0401 01:41:54.391415 139807711794944 logging_writer.py:48] [51] global_step=51, grad_norm=0.355145, loss=0.396549
I0401 01:41:54.396543 139874520373056 submission.py:139] 51) loss = 0.397, grad_norm = 0.355
I0401 01:41:54.695920 139807720187648 logging_writer.py:48] [52] global_step=52, grad_norm=0.598853, loss=0.273858
I0401 01:41:54.699454 139874520373056 submission.py:139] 52) loss = 0.274, grad_norm = 0.599
I0401 01:41:54.919089 139807711794944 logging_writer.py:48] [53] global_step=53, grad_norm=0.233128, loss=0.417635
I0401 01:41:54.924353 139874520373056 submission.py:139] 53) loss = 0.418, grad_norm = 0.233
I0401 01:41:55.159614 139807720187648 logging_writer.py:48] [54] global_step=54, grad_norm=0.263286, loss=0.354199
I0401 01:41:55.165391 139874520373056 submission.py:139] 54) loss = 0.354, grad_norm = 0.263
I0401 01:41:55.394656 139807711794944 logging_writer.py:48] [55] global_step=55, grad_norm=0.138596, loss=0.385086
I0401 01:41:55.399796 139874520373056 submission.py:139] 55) loss = 0.385, grad_norm = 0.139
I0401 01:41:55.688680 139807720187648 logging_writer.py:48] [56] global_step=56, grad_norm=0.267558, loss=0.435786
I0401 01:41:55.694037 139874520373056 submission.py:139] 56) loss = 0.436, grad_norm = 0.268
I0401 01:41:55.982147 139807711794944 logging_writer.py:48] [57] global_step=57, grad_norm=0.146810, loss=0.332556
I0401 01:41:55.985287 139874520373056 submission.py:139] 57) loss = 0.333, grad_norm = 0.147
I0401 01:41:56.244654 139807720187648 logging_writer.py:48] [58] global_step=58, grad_norm=0.109065, loss=0.449475
I0401 01:41:56.250737 139874520373056 submission.py:139] 58) loss = 0.449, grad_norm = 0.109
I0401 01:41:56.493563 139807711794944 logging_writer.py:48] [59] global_step=59, grad_norm=0.172979, loss=0.312024
I0401 01:41:56.499044 139874520373056 submission.py:139] 59) loss = 0.312, grad_norm = 0.173
I0401 01:41:56.779082 139807720187648 logging_writer.py:48] [60] global_step=60, grad_norm=0.328896, loss=0.361349
I0401 01:41:56.784548 139874520373056 submission.py:139] 60) loss = 0.361, grad_norm = 0.329
I0401 01:41:57.040434 139807711794944 logging_writer.py:48] [61] global_step=61, grad_norm=0.344626, loss=0.351787
I0401 01:41:57.046099 139874520373056 submission.py:139] 61) loss = 0.352, grad_norm = 0.345
I0401 01:41:57.302401 139807720187648 logging_writer.py:48] [62] global_step=62, grad_norm=0.247545, loss=0.285047
I0401 01:41:57.305904 139874520373056 submission.py:139] 62) loss = 0.285, grad_norm = 0.248
I0401 01:41:57.574832 139807711794944 logging_writer.py:48] [63] global_step=63, grad_norm=0.079317, loss=0.291993
I0401 01:41:57.578522 139874520373056 submission.py:139] 63) loss = 0.292, grad_norm = 0.079
I0401 01:41:57.859402 139807720187648 logging_writer.py:48] [64] global_step=64, grad_norm=0.131237, loss=0.350301
I0401 01:41:57.862780 139874520373056 submission.py:139] 64) loss = 0.350, grad_norm = 0.131
I0401 01:41:58.073315 139807711794944 logging_writer.py:48] [65] global_step=65, grad_norm=0.119203, loss=0.308530
I0401 01:41:58.077519 139874520373056 submission.py:139] 65) loss = 0.309, grad_norm = 0.119
I0401 01:41:58.386563 139807720187648 logging_writer.py:48] [66] global_step=66, grad_norm=0.234105, loss=0.336195
I0401 01:41:58.389737 139874520373056 submission.py:139] 66) loss = 0.336, grad_norm = 0.234
I0401 01:41:58.663323 139807711794944 logging_writer.py:48] [67] global_step=67, grad_norm=0.118655, loss=0.431085
I0401 01:41:58.666748 139874520373056 submission.py:139] 67) loss = 0.431, grad_norm = 0.119
I0401 01:41:58.906585 139807720187648 logging_writer.py:48] [68] global_step=68, grad_norm=0.110994, loss=0.389601
I0401 01:41:58.912175 139874520373056 submission.py:139] 68) loss = 0.390, grad_norm = 0.111
I0401 01:41:59.169024 139807711794944 logging_writer.py:48] [69] global_step=69, grad_norm=0.159419, loss=0.294822
I0401 01:41:59.175175 139874520373056 submission.py:139] 69) loss = 0.295, grad_norm = 0.159
I0401 01:41:59.429733 139807720187648 logging_writer.py:48] [70] global_step=70, grad_norm=0.069405, loss=0.357415
I0401 01:41:59.433446 139874520373056 submission.py:139] 70) loss = 0.357, grad_norm = 0.069
I0401 01:41:59.680061 139807711794944 logging_writer.py:48] [71] global_step=71, grad_norm=0.224488, loss=0.310311
I0401 01:41:59.683982 139874520373056 submission.py:139] 71) loss = 0.310, grad_norm = 0.224
I0401 01:41:59.943374 139807720187648 logging_writer.py:48] [72] global_step=72, grad_norm=0.150903, loss=0.303616
I0401 01:41:59.946688 139874520373056 submission.py:139] 72) loss = 0.304, grad_norm = 0.151
I0401 01:42:00.222040 139807711794944 logging_writer.py:48] [73] global_step=73, grad_norm=0.159057, loss=0.462482
I0401 01:42:00.225485 139874520373056 submission.py:139] 73) loss = 0.462, grad_norm = 0.159
I0401 01:42:00.486483 139807720187648 logging_writer.py:48] [74] global_step=74, grad_norm=0.096943, loss=0.298302
I0401 01:42:00.492070 139874520373056 submission.py:139] 74) loss = 0.298, grad_norm = 0.097
I0401 01:42:00.760391 139807711794944 logging_writer.py:48] [75] global_step=75, grad_norm=0.175938, loss=0.270962
I0401 01:42:00.764557 139874520373056 submission.py:139] 75) loss = 0.271, grad_norm = 0.176
I0401 01:42:01.025997 139807720187648 logging_writer.py:48] [76] global_step=76, grad_norm=0.187796, loss=0.418602
I0401 01:42:01.031332 139874520373056 submission.py:139] 76) loss = 0.419, grad_norm = 0.188
I0401 01:42:01.352320 139807711794944 logging_writer.py:48] [77] global_step=77, grad_norm=0.096720, loss=0.244931
I0401 01:42:01.357520 139874520373056 submission.py:139] 77) loss = 0.245, grad_norm = 0.097
I0401 01:42:01.572531 139807720187648 logging_writer.py:48] [78] global_step=78, grad_norm=0.178365, loss=0.257051
I0401 01:42:01.578670 139874520373056 submission.py:139] 78) loss = 0.257, grad_norm = 0.178
I0401 01:42:01.835224 139807711794944 logging_writer.py:48] [79] global_step=79, grad_norm=0.141168, loss=0.291099
I0401 01:42:01.839798 139874520373056 submission.py:139] 79) loss = 0.291, grad_norm = 0.141
I0401 01:42:02.065775 139807720187648 logging_writer.py:48] [80] global_step=80, grad_norm=0.147419, loss=0.256843
I0401 01:42:02.069411 139874520373056 submission.py:139] 80) loss = 0.257, grad_norm = 0.147
I0401 01:42:02.327488 139807711794944 logging_writer.py:48] [81] global_step=81, grad_norm=0.182747, loss=0.240460
I0401 01:42:02.330671 139874520373056 submission.py:139] 81) loss = 0.240, grad_norm = 0.183
I0401 01:42:02.601806 139807720187648 logging_writer.py:48] [82] global_step=82, grad_norm=0.177581, loss=0.319623
I0401 01:42:02.605025 139874520373056 submission.py:139] 82) loss = 0.320, grad_norm = 0.178
I0401 01:42:02.898270 139807711794944 logging_writer.py:48] [83] global_step=83, grad_norm=0.149908, loss=0.268090
I0401 01:42:02.903136 139874520373056 submission.py:139] 83) loss = 0.268, grad_norm = 0.150
I0401 01:42:03.210076 139807720187648 logging_writer.py:48] [84] global_step=84, grad_norm=0.201493, loss=0.298223
I0401 01:42:03.214794 139874520373056 submission.py:139] 84) loss = 0.298, grad_norm = 0.201
I0401 01:42:03.473601 139807711794944 logging_writer.py:48] [85] global_step=85, grad_norm=0.169744, loss=0.309476
I0401 01:42:03.477787 139874520373056 submission.py:139] 85) loss = 0.309, grad_norm = 0.170
I0401 01:42:03.718417 139807720187648 logging_writer.py:48] [86] global_step=86, grad_norm=0.097790, loss=0.320598
I0401 01:42:03.723861 139874520373056 submission.py:139] 86) loss = 0.321, grad_norm = 0.098
I0401 01:42:03.951529 139807711794944 logging_writer.py:48] [87] global_step=87, grad_norm=0.200584, loss=0.363915
I0401 01:42:03.955148 139874520373056 submission.py:139] 87) loss = 0.364, grad_norm = 0.201
I0401 01:42:04.241721 139807720187648 logging_writer.py:48] [88] global_step=88, grad_norm=0.332542, loss=0.238791
I0401 01:42:04.245467 139874520373056 submission.py:139] 88) loss = 0.239, grad_norm = 0.333
I0401 01:42:04.504040 139807711794944 logging_writer.py:48] [89] global_step=89, grad_norm=0.054072, loss=0.359329
I0401 01:42:04.507812 139874520373056 submission.py:139] 89) loss = 0.359, grad_norm = 0.054
I0401 01:42:04.794712 139807720187648 logging_writer.py:48] [90] global_step=90, grad_norm=0.207019, loss=0.274882
I0401 01:42:04.799258 139874520373056 submission.py:139] 90) loss = 0.275, grad_norm = 0.207
I0401 01:42:05.077264 139807711794944 logging_writer.py:48] [91] global_step=91, grad_norm=0.190383, loss=0.396889
I0401 01:42:05.082088 139874520373056 submission.py:139] 91) loss = 0.397, grad_norm = 0.190
I0401 01:42:05.300307 139807720187648 logging_writer.py:48] [92] global_step=92, grad_norm=0.114303, loss=0.295648
I0401 01:42:05.305182 139874520373056 submission.py:139] 92) loss = 0.296, grad_norm = 0.114
I0401 01:42:05.546294 139807711794944 logging_writer.py:48] [93] global_step=93, grad_norm=0.228029, loss=0.254281
I0401 01:42:05.551807 139874520373056 submission.py:139] 93) loss = 0.254, grad_norm = 0.228
I0401 01:42:05.800467 139807720187648 logging_writer.py:48] [94] global_step=94, grad_norm=0.130108, loss=0.390781
I0401 01:42:05.804015 139874520373056 submission.py:139] 94) loss = 0.391, grad_norm = 0.130
I0401 01:42:06.076264 139807711794944 logging_writer.py:48] [95] global_step=95, grad_norm=0.133544, loss=0.358688
I0401 01:42:06.079414 139874520373056 submission.py:139] 95) loss = 0.359, grad_norm = 0.134
I0401 01:42:06.325186 139807720187648 logging_writer.py:48] [96] global_step=96, grad_norm=0.148453, loss=0.228112
I0401 01:42:06.328481 139874520373056 submission.py:139] 96) loss = 0.228, grad_norm = 0.148
I0401 01:42:06.590392 139807711794944 logging_writer.py:48] [97] global_step=97, grad_norm=0.037161, loss=0.285247
I0401 01:42:06.595551 139874520373056 submission.py:139] 97) loss = 0.285, grad_norm = 0.037
I0401 01:42:06.862091 139807720187648 logging_writer.py:48] [98] global_step=98, grad_norm=0.168575, loss=0.276361
I0401 01:42:06.867976 139874520373056 submission.py:139] 98) loss = 0.276, grad_norm = 0.169
I0401 01:42:07.113505 139807711794944 logging_writer.py:48] [99] global_step=99, grad_norm=0.138394, loss=0.303886
I0401 01:42:07.119090 139874520373056 submission.py:139] 99) loss = 0.304, grad_norm = 0.138
I0401 01:42:07.385921 139807720187648 logging_writer.py:48] [100] global_step=100, grad_norm=0.076998, loss=0.288617
I0401 01:42:07.391216 139874520373056 submission.py:139] 100) loss = 0.289, grad_norm = 0.077
I0401 01:43:03.762892 139874520373056 submission_runner.py:371] Before eval at step 313: RAM USED (GB) 86.91830784
I0401 01:43:03.763142 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:43:05.867468 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:43:08.778382 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:43:10.961625 139874520373056 submission_runner.py:380] Time since start: 351.79s, 	Step: 313, 	{'train/ssim': 0.7096695218767438, 'train/loss': 0.30405466897147043, 'validation/ssim': 0.687234564047552, 'validation/loss': 0.3233647386747327, 'validation/num_examples': 3554, 'test/ssim': 0.7054653002609257, 'test/loss': 0.3250898772929175, 'test/num_examples': 3581}
I0401 01:43:10.962008 139874520373056 submission_runner.py:390] After eval at step 313: RAM USED (GB) 88.220557312
I0401 01:43:10.975921 139807711794944 logging_writer.py:48] [313] global_step=313, preemption_count=0, score=122.392916, test/loss=0.325090, test/num_examples=3581, test/ssim=0.705465, total_duration=351.790849, train/loss=0.304055, train/ssim=0.709670, validation/loss=0.323365, validation/num_examples=3554, validation/ssim=0.687235
I0401 01:43:11.089685 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_313.
I0401 01:43:11.090999 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 313: RAM USED (GB) 88.238870528
I0401 01:44:19.513025 139807720187648 logging_writer.py:48] [500] global_step=500, grad_norm=0.250566, loss=0.237744
I0401 01:44:19.517920 139874520373056 submission.py:139] 500) loss = 0.238, grad_norm = 0.251
I0401 01:44:31.359322 139874520373056 submission_runner.py:371] Before eval at step 530: RAM USED (GB) 104.747175936
I0401 01:44:31.359514 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:44:33.435609 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:44:36.126021 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:44:38.297611 139874520373056 submission_runner.py:380] Time since start: 439.39s, 	Step: 530, 	{'train/ssim': 0.7158823694501605, 'train/loss': 0.2934785229819162, 'validation/ssim': 0.6930908477903419, 'validation/loss': 0.31261667778603336, 'validation/num_examples': 3554, 'test/ssim': 0.7107540366037769, 'test/loss': 0.314408878346656, 'test/num_examples': 3581}
I0401 01:44:38.298102 139874520373056 submission_runner.py:390] After eval at step 530: RAM USED (GB) 106.105012224
I0401 01:44:38.306757 139807711794944 logging_writer.py:48] [530] global_step=530, preemption_count=0, score=198.412738, test/loss=0.314409, test/num_examples=3581, test/ssim=0.710754, total_duration=439.385703, train/loss=0.293479, train/ssim=0.715882, validation/loss=0.312617, validation/num_examples=3554, validation/ssim=0.693091
I0401 01:44:38.421201 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_530.
I0401 01:44:38.423591 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 530: RAM USED (GB) 106.125901824
I0401 01:45:58.629988 139874520373056 submission_runner.py:371] Before eval at step 762: RAM USED (GB) 123.67294464
I0401 01:45:58.630210 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:46:00.700356 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:46:03.180487 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:46:05.367671 139874520373056 submission_runner.py:380] Time since start: 526.66s, 	Step: 762, 	{'train/ssim': 0.7182157380240304, 'train/loss': 0.33097611154828754, 'validation/ssim': 0.6957946674301843, 'validation/loss': 0.34846890717325546, 'validation/num_examples': 3554, 'test/ssim': 0.7130157973506004, 'test/loss': 0.3506141626378805, 'test/num_examples': 3581}
I0401 01:46:05.368066 139874520373056 submission_runner.py:390] After eval at step 762: RAM USED (GB) 125.070557184
I0401 01:46:05.379152 139807720187648 logging_writer.py:48] [762] global_step=762, preemption_count=0, score=274.086579, test/loss=0.350614, test/num_examples=3581, test/ssim=0.713016, total_duration=526.657394, train/loss=0.330976, train/ssim=0.718216, validation/loss=0.348469, validation/num_examples=3554, validation/ssim=0.695795
I0401 01:46:05.491348 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_762.
I0401 01:46:05.491806 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 762: RAM USED (GB) 125.092892672
I0401 01:47:25.134160 139807711794944 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.277428, loss=0.218159
I0401 01:47:25.140390 139874520373056 submission.py:139] 1000) loss = 0.218, grad_norm = 0.277
I0401 01:47:25.695355 139874520373056 submission_runner.py:371] Before eval at step 1003: RAM USED (GB) 142.704685056
I0401 01:47:25.695636 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:47:27.766227 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:47:30.820240 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:47:33.076488 139874520373056 submission_runner.py:380] Time since start: 613.72s, 	Step: 1003, 	{'train/ssim': 0.726893697466169, 'train/loss': 0.28754687309265137, 'validation/ssim': 0.7042804432505627, 'validation/loss': 0.3063191342501407, 'validation/num_examples': 3554, 'test/ssim': 0.7214437280743856, 'test/loss': 0.3081017190336149, 'test/num_examples': 3581}
I0401 01:47:33.076845 139874520373056 submission_runner.py:390] After eval at step 1003: RAM USED (GB) 143.367098368
I0401 01:47:33.084596 139807720187648 logging_writer.py:48] [1003] global_step=1003, preemption_count=0, score=349.803982, test/loss=0.308102, test/num_examples=3581, test/ssim=0.721444, total_duration=613.722040, train/loss=0.287547, train/ssim=0.726894, validation/loss=0.306319, validation/num_examples=3554, validation/ssim=0.704280
I0401 01:47:33.184114 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1003.
I0401 01:47:33.184613 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 1003: RAM USED (GB) 143.376580608
I0401 01:48:53.416730 139874520373056 submission_runner.py:371] Before eval at step 1318: RAM USED (GB) 143.51177728
I0401 01:48:53.417015 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:48:55.482569 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:48:57.834767 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:48:59.915947 139874520373056 submission_runner.py:380] Time since start: 701.44s, 	Step: 1318, 	{'train/ssim': 0.7330144473484584, 'train/loss': 0.2807929686137608, 'validation/ssim': 0.7103441160048537, 'validation/loss': 0.30027843984111213, 'validation/num_examples': 3554, 'test/ssim': 0.72731960182648, 'test/loss': 0.3021007070192509, 'test/num_examples': 3581}
I0401 01:48:59.916311 139874520373056 submission_runner.py:390] After eval at step 1318: RAM USED (GB) 143.71182592
I0401 01:48:59.924594 139807711794944 logging_writer.py:48] [1318] global_step=1318, preemption_count=0, score=422.602110, test/loss=0.302101, test/num_examples=3581, test/ssim=0.727320, total_duration=701.441511, train/loss=0.280793, train/ssim=0.733014, validation/loss=0.300278, validation/num_examples=3554, validation/ssim=0.710344
I0401 01:49:00.022581 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1318.
I0401 01:49:00.023093 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 1318: RAM USED (GB) 143.712800768
I0401 01:49:45.681928 139807720187648 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.469945, loss=0.281905
I0401 01:49:45.685756 139874520373056 submission.py:139] 1500) loss = 0.282, grad_norm = 0.470
I0401 01:50:20.189291 139874520373056 submission_runner.py:371] Before eval at step 1631: RAM USED (GB) 143.537098752
I0401 01:50:20.189551 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:50:22.124501 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:50:24.347822 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:50:26.435899 139874520373056 submission_runner.py:380] Time since start: 788.21s, 	Step: 1631, 	{'train/ssim': 0.7310944284711566, 'train/loss': 0.2799022878919329, 'validation/ssim': 0.7080833765651379, 'validation/loss': 0.2991635607546075, 'validation/num_examples': 3554, 'test/ssim': 0.7253987925640534, 'test/loss': 0.30101516412576795, 'test/num_examples': 3581}
I0401 01:50:26.436268 139874520373056 submission_runner.py:390] After eval at step 1631: RAM USED (GB) 143.757791232
I0401 01:50:26.444190 139807711794944 logging_writer.py:48] [1631] global_step=1631, preemption_count=0, score=495.536668, test/loss=0.301015, test/num_examples=3581, test/ssim=0.725399, total_duration=788.211096, train/loss=0.279902, train/ssim=0.731094, validation/loss=0.299164, validation/num_examples=3554, validation/ssim=0.708083
I0401 01:50:26.542248 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1631.
I0401 01:50:26.542732 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 1631: RAM USED (GB) 143.757217792
I0401 01:51:46.729970 139874520373056 submission_runner.py:371] Before eval at step 1947: RAM USED (GB) 143.559585792
I0401 01:51:46.730252 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:51:48.638544 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:51:50.620554 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:51:52.544692 139874520373056 submission_runner.py:380] Time since start: 874.75s, 	Step: 1947, 	{'train/ssim': 0.7335963249206543, 'train/loss': 0.2778477157865252, 'validation/ssim': 0.7102435471036156, 'validation/loss': 0.29743269714801984, 'validation/num_examples': 3554, 'test/ssim': 0.7271057316392069, 'test/loss': 0.2993239056827702, 'test/num_examples': 3581}
I0401 01:51:52.545188 139874520373056 submission_runner.py:390] After eval at step 1947: RAM USED (GB) 143.852036096
I0401 01:51:52.554543 139807720187648 logging_writer.py:48] [1947] global_step=1947, preemption_count=0, score=568.290429, test/loss=0.299324, test/num_examples=3581, test/ssim=0.727106, total_duration=874.753357, train/loss=0.277848, train/ssim=0.733596, validation/loss=0.297433, validation/num_examples=3554, validation/ssim=0.710244
I0401 01:51:52.661906 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_1947.
I0401 01:51:52.662531 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 1947: RAM USED (GB) 143.851495424
I0401 01:52:04.506334 139807711794944 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.185100, loss=0.299132
I0401 01:52:04.510149 139874520373056 submission.py:139] 2000) loss = 0.299, grad_norm = 0.185
I0401 01:53:12.969318 139874520373056 submission_runner.py:371] Before eval at step 2259: RAM USED (GB) 143.783661568
I0401 01:53:12.969529 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:53:15.039277 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:53:17.270143 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:53:19.433078 139874520373056 submission_runner.py:380] Time since start: 961.00s, 	Step: 2259, 	{'train/ssim': 0.7268424715314593, 'train/loss': 0.28445957388196674, 'validation/ssim': 0.7036979130029544, 'validation/loss': 0.30456014020294037, 'validation/num_examples': 3554, 'test/ssim': 0.7203109728427813, 'test/loss': 0.30675141207501394, 'test/num_examples': 3581}
I0401 01:53:19.433429 139874520373056 submission_runner.py:390] After eval at step 2259: RAM USED (GB) 143.99766528
I0401 01:53:19.442449 139807720187648 logging_writer.py:48] [2259] global_step=2259, preemption_count=0, score=641.349070, test/loss=0.306751, test/num_examples=3581, test/ssim=0.720311, total_duration=960.995486, train/loss=0.284460, train/ssim=0.726842, validation/loss=0.304560, validation/num_examples=3554, validation/ssim=0.703698
I0401 01:53:19.539496 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2259.
I0401 01:53:19.539961 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 2259: RAM USED (GB) 143.99862784
I0401 01:54:20.321545 139807711794944 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.265623, loss=0.287089
I0401 01:54:20.324953 139874520373056 submission.py:139] 2500) loss = 0.287, grad_norm = 0.266
I0401 01:54:39.587156 139874520373056 submission_runner.py:371] Before eval at step 2575: RAM USED (GB) 143.777538048
I0401 01:54:39.587378 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:54:41.632912 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:54:43.754754 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:54:45.834593 139874520373056 submission_runner.py:380] Time since start: 1047.61s, 	Step: 2575, 	{'train/ssim': 0.7197796957833427, 'train/loss': 0.2799290248325893, 'validation/ssim': 0.7000733108821047, 'validation/loss': 0.29855551046136397, 'validation/num_examples': 3554, 'test/ssim': 0.7167749902916434, 'test/loss': 0.29999622983061647, 'test/num_examples': 3581}
I0401 01:54:45.835025 139874520373056 submission_runner.py:390] After eval at step 2575: RAM USED (GB) 143.997206528
I0401 01:54:45.842922 139807720187648 logging_writer.py:48] [2575] global_step=2575, preemption_count=0, score=714.015744, test/loss=0.299996, test/num_examples=3581, test/ssim=0.716775, total_duration=1047.611997, train/loss=0.279929, train/ssim=0.719780, validation/loss=0.298556, validation/num_examples=3554, validation/ssim=0.700073
I0401 01:54:45.939820 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2575.
I0401 01:54:45.940371 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 2575: RAM USED (GB) 143.996612608
I0401 01:55:19.930237 139874520373056 submission_runner.py:371] Before eval at step 2714: RAM USED (GB) 143.85692672
I0401 01:55:19.930480 139874520373056 spec.py:298] Evaluating on the training split.
I0401 01:55:21.973142 139874520373056 spec.py:310] Evaluating on the validation split.
I0401 01:55:24.126989 139874520373056 spec.py:326] Evaluating on the test split.
I0401 01:55:26.201994 139874520373056 submission_runner.py:380] Time since start: 1087.96s, 	Step: 2714, 	{'train/ssim': 0.7319682666233608, 'train/loss': 0.27626056330544607, 'validation/ssim': 0.710172310798572, 'validation/loss': 0.29529550495480444, 'validation/num_examples': 3554, 'test/ssim': 0.7271030727494066, 'test/loss': 0.2968845447326166, 'test/num_examples': 3581}
I0401 01:55:26.202344 139874520373056 submission_runner.py:390] After eval at step 2714: RAM USED (GB) 144.007057408
I0401 01:55:26.210499 139807711794944 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=744.790200, test/loss=0.296885, test/num_examples=3581, test/ssim=0.727103, total_duration=1087.956643, train/loss=0.276261, train/ssim=0.731968, validation/loss=0.295296, validation/num_examples=3554, validation/ssim=0.710172
I0401 01:55:26.308339 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2714.
I0401 01:55:26.308883 139874520373056 submission_runner.py:409] After logging and checkpointing eval at step 2714: RAM USED (GB) 144.006463488
I0401 01:55:26.316627 139807720187648 logging_writer.py:48] [2714] global_step=2714, preemption_count=0, score=744.790200
I0401 01:55:26.465732 139874520373056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_momentum/fastmri_pytorch/trial_1/checkpoint_2714.
I0401 01:55:27.509270 139874520373056 submission_runner.py:543] Tuning trial 1/1
I0401 01:55:27.509519 139874520373056 submission_runner.py:544] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 01:55:27.515249 139874520373056 submission_runner.py:545] Metrics: {'eval_results': [(1, {'train/ssim': 0.28941198757716585, 'train/loss': 0.7752715519496373, 'validation/ssim': 0.28286531928153136, 'validation/loss': 0.7852436295371412, 'validation/num_examples': 3554, 'test/ssim': 0.3056446321514591, 'test/loss': 0.781013086101124, 'test/num_examples': 3581, 'score': 46.220869064331055, 'total_duration': 46.222795248031616, 'global_step': 1, 'preemption_count': 0}), (313, {'train/ssim': 0.7096695218767438, 'train/loss': 0.30405466897147043, 'validation/ssim': 0.687234564047552, 'validation/loss': 0.3233647386747327, 'validation/num_examples': 3554, 'test/ssim': 0.7054653002609257, 'test/loss': 0.3250898772929175, 'test/num_examples': 3581, 'score': 122.39291644096375, 'total_duration': 351.79084873199463, 'global_step': 313, 'preemption_count': 0}), (530, {'train/ssim': 0.7158823694501605, 'train/loss': 0.2934785229819162, 'validation/ssim': 0.6930908477903419, 'validation/loss': 0.31261667778603336, 'validation/num_examples': 3554, 'test/ssim': 0.7107540366037769, 'test/loss': 0.314408878346656, 'test/num_examples': 3581, 'score': 198.4127380847931, 'total_duration': 439.385703086853, 'global_step': 530, 'preemption_count': 0}), (762, {'train/ssim': 0.7182157380240304, 'train/loss': 0.33097611154828754, 'validation/ssim': 0.6957946674301843, 'validation/loss': 0.34846890717325546, 'validation/num_examples': 3554, 'test/ssim': 0.7130157973506004, 'test/loss': 0.3506141626378805, 'test/num_examples': 3581, 'score': 274.08657908439636, 'total_duration': 526.6573941707611, 'global_step': 762, 'preemption_count': 0}), (1003, {'train/ssim': 0.726893697466169, 'train/loss': 0.28754687309265137, 'validation/ssim': 0.7042804432505627, 'validation/loss': 0.3063191342501407, 'validation/num_examples': 3554, 'test/ssim': 0.7214437280743856, 'test/loss': 0.3081017190336149, 'test/num_examples': 3581, 'score': 349.803982257843, 'total_duration': 613.7220396995544, 'global_step': 1003, 'preemption_count': 0}), (1318, {'train/ssim': 0.7330144473484584, 'train/loss': 0.2807929686137608, 'validation/ssim': 0.7103441160048537, 'validation/loss': 0.30027843984111213, 'validation/num_examples': 3554, 'test/ssim': 0.72731960182648, 'test/loss': 0.3021007070192509, 'test/num_examples': 3581, 'score': 422.60210967063904, 'total_duration': 701.4415106773376, 'global_step': 1318, 'preemption_count': 0}), (1631, {'train/ssim': 0.7310944284711566, 'train/loss': 0.2799022878919329, 'validation/ssim': 0.7080833765651379, 'validation/loss': 0.2991635607546075, 'validation/num_examples': 3554, 'test/ssim': 0.7253987925640534, 'test/loss': 0.30101516412576795, 'test/num_examples': 3581, 'score': 495.53666830062866, 'total_duration': 788.211095571518, 'global_step': 1631, 'preemption_count': 0}), (1947, {'train/ssim': 0.7335963249206543, 'train/loss': 0.2778477157865252, 'validation/ssim': 0.7102435471036156, 'validation/loss': 0.29743269714801984, 'validation/num_examples': 3554, 'test/ssim': 0.7271057316392069, 'test/loss': 0.2993239056827702, 'test/num_examples': 3581, 'score': 568.2904288768768, 'total_duration': 874.7533566951752, 'global_step': 1947, 'preemption_count': 0}), (2259, {'train/ssim': 0.7268424715314593, 'train/loss': 0.28445957388196674, 'validation/ssim': 0.7036979130029544, 'validation/loss': 0.30456014020294037, 'validation/num_examples': 3554, 'test/ssim': 0.7203109728427813, 'test/loss': 0.30675141207501394, 'test/num_examples': 3581, 'score': 641.3490703105927, 'total_duration': 960.9954855442047, 'global_step': 2259, 'preemption_count': 0}), (2575, {'train/ssim': 0.7197796957833427, 'train/loss': 0.2799290248325893, 'validation/ssim': 0.7000733108821047, 'validation/loss': 0.29855551046136397, 'validation/num_examples': 3554, 'test/ssim': 0.7167749902916434, 'test/loss': 0.29999622983061647, 'test/num_examples': 3581, 'score': 714.0157437324524, 'total_duration': 1047.6119973659515, 'global_step': 2575, 'preemption_count': 0}), (2714, {'train/ssim': 0.7319682666233608, 'train/loss': 0.27626056330544607, 'validation/ssim': 0.710172310798572, 'validation/loss': 0.29529550495480444, 'validation/num_examples': 3554, 'test/ssim': 0.7271030727494066, 'test/loss': 0.2968845447326166, 'test/num_examples': 3581, 'score': 744.7902002334595, 'total_duration': 1087.9566431045532, 'global_step': 2714, 'preemption_count': 0})], 'global_step': 2714}
I0401 01:55:27.515420 139874520373056 submission_runner.py:546] Timing: 744.7902002334595
I0401 01:55:27.515464 139874520373056 submission_runner.py:547] ====================
I0401 01:55:27.515550 139874520373056 submission_runner.py:606] Final fastmri score: 744.7902002334595
