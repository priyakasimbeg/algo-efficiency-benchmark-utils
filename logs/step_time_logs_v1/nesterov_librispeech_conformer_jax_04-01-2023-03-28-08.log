I0401 03:28:27.921316 140405181323072 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nesterov/librispeech_conformer_jax.
I0401 03:28:27.985468 140405181323072 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0401 03:28:28.850461 140405181323072 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0401 03:28:28.851105 140405181323072 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0401 03:28:28.856499 140405181323072 submission_runner.py:511] Using RNG seed 1183047135
I0401 03:28:31.167825 140405181323072 submission_runner.py:520] --- Tuning run 1/1 ---
I0401 03:28:31.168019 140405181323072 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1.
I0401 03:28:31.168186 140405181323072 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/hparams.json.
I0401 03:28:31.293925 140405181323072 submission_runner.py:230] Starting train once: RAM USED (GB) 4.475613184
I0401 03:28:31.294082 140405181323072 submission_runner.py:231] Initializing dataset.
I0401 03:28:31.294240 140405181323072 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.475613184
I0401 03:28:31.294295 140405181323072 submission_runner.py:240] Initializing model.
I0401 03:28:36.965477 140405181323072 submission_runner.py:251] After Initializing model: RAM USED (GB) 7.89467136
I0401 03:28:36.965662 140405181323072 submission_runner.py:252] Initializing optimizer.
I0401 03:28:37.694442 140405181323072 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 7.895437312
I0401 03:28:37.694605 140405181323072 submission_runner.py:261] Initializing metrics bundle.
I0401 03:28:37.694660 140405181323072 submission_runner.py:276] Initializing checkpoint and logger.
I0401 03:28:37.695589 140405181323072 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0401 03:28:37.695915 140405181323072 logger_utils.py:231] Unable to record workload.train_mean information. Continuing without it.
I0401 03:28:37.696000 140405181323072 logger_utils.py:231] Unable to record workload.train_stddev information. Continuing without it.
I0401 03:28:38.522325 140405181323072 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0401 03:28:38.523205 140405181323072 submission_runner.py:300] Saving flags to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/flags_0.json.
I0401 03:28:38.528242 140405181323072 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 7.891218432
I0401 03:28:38.528453 140405181323072 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 7.891218432
I0401 03:28:38.528516 140405181323072 submission_runner.py:313] Starting training loop.
I0401 03:28:38.724368 140405181323072 input_pipeline.py:20] Loading split = train-clean-100
I0401 03:28:38.755203 140405181323072 input_pipeline.py:20] Loading split = train-clean-360
I0401 03:28:39.082306 140405181323072 input_pipeline.py:20] Loading split = train-other-500
I0401 03:28:42.457866 140405181323072 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 8.99780608
2023-04-01 03:29:37.801597: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2023-04-01 03:29:37.894809: E external/org_tensorflow/tensorflow/compiler/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/interpreters/mlir.py:592: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0401 03:29:39.796349 140231101634304 logging_writer.py:48] [0] global_step=0, grad_norm=40.13011932373047, loss=31.43141746520996
I0401 03:29:39.815785 140405181323072 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 13.293101056
I0401 03:29:39.816096 140405181323072 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 13.293101056
I0401 03:29:39.816196 140405181323072 spec.py:298] Evaluating on the training split.
I0401 03:29:39.919809 140405181323072 input_pipeline.py:20] Loading split = train-clean-100
I0401 03:29:39.948106 140405181323072 input_pipeline.py:20] Loading split = train-clean-360
I0401 03:29:40.315107 140405181323072 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0401 03:30:28.538106 140405181323072 spec.py:310] Evaluating on the validation split.
I0401 03:30:28.607710 140405181323072 input_pipeline.py:20] Loading split = dev-clean
I0401 03:30:28.612487 140405181323072 input_pipeline.py:20] Loading split = dev-other
I0401 03:31:04.663783 140405181323072 spec.py:326] Evaluating on the test split.
I0401 03:31:04.730999 140405181323072 input_pipeline.py:20] Loading split = test-clean
I0401 03:31:31.402865 140405181323072 submission_runner.py:382] Time since start: 61.29s, 	Step: 1, 	{'train/ctc_loss': DeviceArray(31.630554, dtype=float32), 'train/wer': 0.9632571892634779, 'validation/ctc_loss': DeviceArray(31.10666, dtype=float32), 'validation/wer': 0.9135061602137985, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.095015, dtype=float32), 'test/wer': 0.9217191721000142, 'test/num_examples': 2472}
I0401 03:31:31.404293 140405181323072 submission_runner.py:396] After eval at step 1: RAM USED (GB) 20.413956096
I0401 03:31:31.420587 140227343537920 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=61.091688, test/ctc_loss=31.095014572143555, test/num_examples=2472, test/wer=0.921719, total_duration=61.287603, train/ctc_loss=31.63055419921875, train/wer=0.963257, validation/ctc_loss=31.106660842895508, validation/num_examples=5348, validation/wer=0.913506
I0401 03:31:31.678036 140405181323072 checkpoints.py:356] Saving checkpoint at step: 1
I0401 03:31:32.747992 140405181323072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_1
I0401 03:31:32.768661 140405181323072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_1.
I0401 03:31:32.786194 140405181323072 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 20.428054528
I0401 03:31:32.853074 140405181323072 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 20.428099584
I0401 03:31:48.250468 140405181323072 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 20.87667712
I0401 03:33:03.239411 140232404051712 logging_writer.py:48] [100] global_step=100, grad_norm=1.5956560373306274, loss=5.9467878341674805
I0401 03:34:19.024357 140232412444416 logging_writer.py:48] [200] global_step=200, grad_norm=4.583273410797119, loss=5.946632385253906
I0401 03:35:34.825858 140232404051712 logging_writer.py:48] [300] global_step=300, grad_norm=4.095672130584717, loss=5.938826084136963
I0401 03:36:50.569317 140232412444416 logging_writer.py:48] [400] global_step=400, grad_norm=2.9579226970672607, loss=5.886650085449219
I0401 03:38:06.317336 140232404051712 logging_writer.py:48] [500] global_step=500, grad_norm=2.4458534717559814, loss=5.885875701904297
I0401 03:39:22.009242 140232412444416 logging_writer.py:48] [600] global_step=600, grad_norm=1.6844418048858643, loss=5.853198528289795
I0401 03:40:37.782164 140232404051712 logging_writer.py:48] [700] global_step=700, grad_norm=0.41898059844970703, loss=5.803274631500244
I0401 03:41:53.541698 140232412444416 logging_writer.py:48] [800] global_step=800, grad_norm=1.794148325920105, loss=5.845669269561768
I0401 03:43:15.201073 140232404051712 logging_writer.py:48] [900] global_step=900, grad_norm=1.4751214981079102, loss=5.845806121826172
I0401 03:44:42.135811 140232412444416 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.138387680053711, loss=5.815783977508545
I0401 03:46:03.079813 140232563513088 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1773741245269775, loss=5.814537048339844
I0401 03:47:18.745041 140232555120384 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2288150787353516, loss=5.834874153137207
I0401 03:48:34.419698 140232563513088 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0695950984954834, loss=5.812839031219482
I0401 03:49:50.108328 140232555120384 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6945701837539673, loss=5.808387756347656
I0401 03:51:05.787382 140232563513088 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0578138828277588, loss=5.810356140136719
I0401 03:52:26.676150 140232555120384 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.630315899848938, loss=5.811580657958984
I0401 03:53:47.640239 140232563513088 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.20360127091407776, loss=5.7913665771484375
I0401 03:55:10.484208 140232555120384 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.2747634649276733, loss=5.8518476486206055
I0401 03:56:35.157035 140232563513088 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8763311505317688, loss=5.8162641525268555
I0401 03:57:55.891326 140232555120384 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3061698079109192, loss=5.793257713317871
I0401 03:59:17.192604 140232563513088 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.512762188911438, loss=5.791655540466309
I0401 04:00:32.997340 140232555120384 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.23562641441822052, loss=5.798037528991699
I0401 04:01:48.723515 140232563513088 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.09652528166770935, loss=5.814903736114502
I0401 04:03:04.473756 140232555120384 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.1601155400276184, loss=5.773623943328857
I0401 04:04:20.182081 140232563513088 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.22372998297214508, loss=5.806636333465576
I0401 04:05:35.892582 140232555120384 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6712557077407837, loss=5.82111120223999
I0401 04:06:55.044411 140232563513088 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.20812150835990906, loss=5.787108898162842
I0401 04:08:17.106719 140232555120384 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.31987854838371277, loss=5.7939887046813965
I0401 04:09:42.091933 140232563513088 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5080704092979431, loss=5.815879821777344
I0401 04:11:07.922749 140232555120384 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2933388352394104, loss=5.801982879638672
I0401 04:11:33.282806 140405181323072 submission_runner.py:373] Before eval at step 3031: RAM USED (GB) 22.320508928
I0401 04:11:33.283020 140405181323072 spec.py:298] Evaluating on the training split.
I0401 04:11:59.936247 140405181323072 spec.py:310] Evaluating on the validation split.
I0401 04:12:36.131182 140405181323072 spec.py:326] Evaluating on the test split.
I0401 04:12:54.595872 140405181323072 submission_runner.py:382] Time since start: 2574.75s, 	Step: 3031, 	{'train/ctc_loss': DeviceArray(5.9543867, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.9690914, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.949898, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 04:12:54.597469 140405181323072 submission_runner.py:396] After eval at step 3031: RAM USED (GB) 20.438310912
I0401 04:12:54.618056 140232563513088 logging_writer.py:48] [3031] global_step=3031, preemption_count=0, score=2455.475009, test/ctc_loss=5.949897766113281, test/num_examples=2472, test/wer=0.899580, total_duration=2574.752532, train/ctc_loss=5.9543867111206055, train/wer=0.944636, validation/ctc_loss=5.969091415405273, validation/num_examples=5348, validation/wer=0.895995
I0401 04:12:54.858998 140405181323072 checkpoints.py:356] Saving checkpoint at step: 3031
I0401 04:12:55.939064 140405181323072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_3031
I0401 04:12:55.959819 140405181323072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_3031.
I0401 04:12:55.972706 140405181323072 submission_runner.py:416] After logging and checkpointing eval at step 3031: RAM USED (GB) 20.465033216
I0401 04:13:52.822070 140232235833088 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2384745329618454, loss=5.787213325500488
I0401 04:15:08.859078 140232227440384 logging_writer.py:48] [3200] global_step=3200, grad_norm=177.15737915039062, loss=47.235939025878906
I0401 04:16:21.799482 140232235833088 logging_writer.py:48] [3300] global_step=3300, grad_norm=nan, loss=nan
I0401 04:17:34.574524 140232227440384 logging_writer.py:48] [3400] global_step=3400, grad_norm=nan, loss=nan
I0401 04:18:53.187572 140232235833088 logging_writer.py:48] [3500] global_step=3500, grad_norm=nan, loss=nan
I0401 04:20:16.917106 140232227440384 logging_writer.py:48] [3600] global_step=3600, grad_norm=nan, loss=nan
I0401 04:21:38.864697 140232235833088 logging_writer.py:48] [3700] global_step=3700, grad_norm=nan, loss=nan
I0401 04:23:00.880437 140232227440384 logging_writer.py:48] [3800] global_step=3800, grad_norm=nan, loss=nan
I0401 04:24:20.653682 140232235833088 logging_writer.py:48] [3900] global_step=3900, grad_norm=nan, loss=nan
I0401 04:25:43.762294 140232227440384 logging_writer.py:48] [4000] global_step=4000, grad_norm=nan, loss=nan
I0401 04:27:07.854336 140232235833088 logging_writer.py:48] [4100] global_step=4100, grad_norm=nan, loss=nan
I0401 04:28:25.801021 140232235833088 logging_writer.py:48] [4200] global_step=4200, grad_norm=nan, loss=nan
I0401 04:29:38.787035 140232227440384 logging_writer.py:48] [4300] global_step=4300, grad_norm=nan, loss=nan
I0401 04:30:51.736170 140232235833088 logging_writer.py:48] [4400] global_step=4400, grad_norm=nan, loss=nan
I0401 04:32:04.900768 140232227440384 logging_writer.py:48] [4500] global_step=4500, grad_norm=nan, loss=nan
I0401 04:33:26.635621 140232235833088 logging_writer.py:48] [4600] global_step=4600, grad_norm=nan, loss=nan
I0401 04:34:48.790828 140232227440384 logging_writer.py:48] [4700] global_step=4700, grad_norm=nan, loss=nan
I0401 04:36:07.692730 140232235833088 logging_writer.py:48] [4800] global_step=4800, grad_norm=nan, loss=nan
I0401 04:37:26.980857 140232227440384 logging_writer.py:48] [4900] global_step=4900, grad_norm=nan, loss=nan
I0401 04:38:46.952684 140232235833088 logging_writer.py:48] [5000] global_step=5000, grad_norm=nan, loss=nan
I0401 04:40:09.950144 140232227440384 logging_writer.py:48] [5100] global_step=5100, grad_norm=nan, loss=nan
I0401 04:41:30.062188 140232235833088 logging_writer.py:48] [5200] global_step=5200, grad_norm=nan, loss=nan
I0401 04:42:42.788745 140232227440384 logging_writer.py:48] [5300] global_step=5300, grad_norm=nan, loss=nan
I0401 04:43:55.518963 140232235833088 logging_writer.py:48] [5400] global_step=5400, grad_norm=nan, loss=nan
I0401 04:45:08.283373 140232227440384 logging_writer.py:48] [5500] global_step=5500, grad_norm=nan, loss=nan
I0401 04:46:24.795298 140232235833088 logging_writer.py:48] [5600] global_step=5600, grad_norm=nan, loss=nan
I0401 04:47:45.583133 140232227440384 logging_writer.py:48] [5700] global_step=5700, grad_norm=nan, loss=nan
I0401 04:49:09.336140 140232235833088 logging_writer.py:48] [5800] global_step=5800, grad_norm=nan, loss=nan
I0401 04:50:29.251965 140232227440384 logging_writer.py:48] [5900] global_step=5900, grad_norm=nan, loss=nan
I0401 04:51:51.222299 140232235833088 logging_writer.py:48] [6000] global_step=6000, grad_norm=nan, loss=nan
I0401 04:52:56.632135 140405181323072 submission_runner.py:373] Before eval at step 6079: RAM USED (GB) 21.77755136
I0401 04:52:56.632349 140405181323072 spec.py:298] Evaluating on the training split.
I0401 04:53:23.221670 140405181323072 spec.py:310] Evaluating on the validation split.
I0401 04:54:00.121210 140405181323072 spec.py:326] Evaluating on the test split.
I0401 04:54:18.640384 140405181323072 submission_runner.py:382] Time since start: 5058.10s, 	Step: 6079, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 04:54:18.641772 140405181323072 submission_runner.py:396] After eval at step 6079: RAM USED (GB) 21.041958912
I0401 04:54:18.660739 140231943993088 logging_writer.py:48] [6079] global_step=6079, preemption_count=0, score=4850.155949, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=5058.099041, train/ctc_loss=nan, train/wer=0.942722, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0401 04:54:18.888004 140405181323072 checkpoints.py:356] Saving checkpoint at step: 6079
I0401 04:54:19.862874 140405181323072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_6079
I0401 04:54:19.884844 140405181323072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_6079.
I0401 04:54:19.900730 140405181323072 submission_runner.py:416] After logging and checkpointing eval at step 6079: RAM USED (GB) 21.058113536
I0401 04:54:35.974132 140231935600384 logging_writer.py:48] [6100] global_step=6100, grad_norm=nan, loss=nan
I0401 04:55:52.849954 140231943993088 logging_writer.py:48] [6200] global_step=6200, grad_norm=nan, loss=nan
I0401 04:57:05.801683 140231935600384 logging_writer.py:48] [6300] global_step=6300, grad_norm=nan, loss=nan
I0401 04:58:18.875220 140231943993088 logging_writer.py:48] [6400] global_step=6400, grad_norm=nan, loss=nan
I0401 04:59:31.941377 140231935600384 logging_writer.py:48] [6500] global_step=6500, grad_norm=nan, loss=nan
I0401 05:00:44.919394 140231943993088 logging_writer.py:48] [6600] global_step=6600, grad_norm=nan, loss=nan
I0401 05:02:06.525856 140231935600384 logging_writer.py:48] [6700] global_step=6700, grad_norm=nan, loss=nan
I0401 05:03:32.234871 140231943993088 logging_writer.py:48] [6800] global_step=6800, grad_norm=nan, loss=nan
I0401 05:04:52.813574 140231935600384 logging_writer.py:48] [6900] global_step=6900, grad_norm=nan, loss=nan
I0401 05:06:11.125507 140231943993088 logging_writer.py:48] [7000] global_step=7000, grad_norm=nan, loss=nan
I0401 05:07:36.792683 140231935600384 logging_writer.py:48] [7100] global_step=7100, grad_norm=nan, loss=nan
I0401 05:08:59.680659 140231943993088 logging_writer.py:48] [7200] global_step=7200, grad_norm=nan, loss=nan
I0401 05:10:16.358731 140231943993088 logging_writer.py:48] [7300] global_step=7300, grad_norm=nan, loss=nan
I0401 05:11:29.310944 140231935600384 logging_writer.py:48] [7400] global_step=7400, grad_norm=nan, loss=nan
I0401 05:12:42.029170 140231943993088 logging_writer.py:48] [7500] global_step=7500, grad_norm=nan, loss=nan
I0401 05:13:57.256250 140231935600384 logging_writer.py:48] [7600] global_step=7600, grad_norm=nan, loss=nan
I0401 05:15:20.889611 140231943993088 logging_writer.py:48] [7700] global_step=7700, grad_norm=nan, loss=nan
I0401 05:16:40.884052 140231935600384 logging_writer.py:48] [7800] global_step=7800, grad_norm=nan, loss=nan
I0401 05:18:08.091006 140231943993088 logging_writer.py:48] [7900] global_step=7900, grad_norm=nan, loss=nan
I0401 05:19:28.910764 140231935600384 logging_writer.py:48] [8000] global_step=8000, grad_norm=nan, loss=nan
I0401 05:20:53.816604 140231943993088 logging_writer.py:48] [8100] global_step=8100, grad_norm=nan, loss=nan
I0401 05:22:15.050440 140231935600384 logging_writer.py:48] [8200] global_step=8200, grad_norm=nan, loss=nan
I0401 05:23:33.484501 140232563513088 logging_writer.py:48] [8300] global_step=8300, grad_norm=nan, loss=nan
I0401 05:24:46.209398 140232555120384 logging_writer.py:48] [8400] global_step=8400, grad_norm=nan, loss=nan
I0401 05:25:58.929378 140232563513088 logging_writer.py:48] [8500] global_step=8500, grad_norm=nan, loss=nan
I0401 05:27:11.606417 140232555120384 logging_writer.py:48] [8600] global_step=8600, grad_norm=nan, loss=nan
I0401 05:28:29.335590 140232563513088 logging_writer.py:48] [8700] global_step=8700, grad_norm=nan, loss=nan
I0401 05:29:48.049124 140232555120384 logging_writer.py:48] [8800] global_step=8800, grad_norm=nan, loss=nan
I0401 05:31:08.036333 140232563513088 logging_writer.py:48] [8900] global_step=8900, grad_norm=nan, loss=nan
I0401 05:32:28.134569 140232555120384 logging_writer.py:48] [9000] global_step=9000, grad_norm=nan, loss=nan
I0401 05:33:53.009445 140232563513088 logging_writer.py:48] [9100] global_step=9100, grad_norm=nan, loss=nan
I0401 05:34:20.000116 140405181323072 submission_runner.py:373] Before eval at step 9135: RAM USED (GB) 22.64307712
I0401 05:34:20.000332 140405181323072 spec.py:298] Evaluating on the training split.
I0401 05:34:46.879796 140405181323072 spec.py:310] Evaluating on the validation split.
I0401 05:35:23.518104 140405181323072 spec.py:326] Evaluating on the test split.
I0401 05:35:43.150985 140405181323072 submission_runner.py:382] Time since start: 7541.47s, 	Step: 9135, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 05:35:43.152370 140405181323072 submission_runner.py:396] After eval at step 9135: RAM USED (GB) 22.369755136
I0401 05:35:43.171036 140231979833088 logging_writer.py:48] [9135] global_step=9135, preemption_count=0, score=7244.334080, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=7541.468083, train/ctc_loss=nan, train/wer=0.943324, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0401 05:35:43.392050 140405181323072 checkpoints.py:356] Saving checkpoint at step: 9135
I0401 05:35:44.364851 140405181323072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_9135
I0401 05:35:44.386728 140405181323072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_9135.
I0401 05:35:44.401601 140405181323072 submission_runner.py:416] After logging and checkpointing eval at step 9135: RAM USED (GB) 22.381842432
I0401 05:36:32.349864 140231971440384 logging_writer.py:48] [9200] global_step=9200, grad_norm=nan, loss=nan
I0401 05:37:48.869242 140231979833088 logging_writer.py:48] [9300] global_step=9300, grad_norm=nan, loss=nan
I0401 05:39:01.564473 140231971440384 logging_writer.py:48] [9400] global_step=9400, grad_norm=nan, loss=nan
I0401 05:40:14.345085 140231979833088 logging_writer.py:48] [9500] global_step=9500, grad_norm=nan, loss=nan
I0401 05:41:27.579333 140231971440384 logging_writer.py:48] [9600] global_step=9600, grad_norm=nan, loss=nan
I0401 05:42:46.008308 140231979833088 logging_writer.py:48] [9700] global_step=9700, grad_norm=nan, loss=nan
I0401 05:44:08.102304 140231971440384 logging_writer.py:48] [9800] global_step=9800, grad_norm=nan, loss=nan
I0401 05:45:30.401198 140231979833088 logging_writer.py:48] [9900] global_step=9900, grad_norm=nan, loss=nan
I0401 05:46:49.682187 140405181323072 submission_runner.py:373] Before eval at step 10000: RAM USED (GB) 22.822981632
I0401 05:46:49.682442 140405181323072 spec.py:298] Evaluating on the training split.
I0401 05:47:16.267191 140405181323072 spec.py:310] Evaluating on the validation split.
I0401 05:47:54.219908 140405181323072 spec.py:326] Evaluating on the test split.
I0401 05:48:12.792678 140405181323072 submission_runner.py:382] Time since start: 8291.15s, 	Step: 10000, 	{'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472}
I0401 05:48:12.794410 140405181323072 submission_runner.py:396] After eval at step 10000: RAM USED (GB) 21.027586048
I0401 05:48:12.814222 140232133433088 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7907.945034, test/ctc_loss=nan, test/num_examples=2472, test/wer=0.899580, total_duration=8291.150936, train/ctc_loss=nan, train/wer=0.943700, validation/ctc_loss=nan, validation/num_examples=5348, validation/wer=0.895995
I0401 05:48:13.019569 140405181323072 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 05:48:14.050798 140405181323072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_10000
I0401 05:48:14.071994 140405181323072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0401 05:48:14.080327 140405181323072 submission_runner.py:416] After logging and checkpointing eval at step 10000: RAM USED (GB) 21.042573312
I0401 05:48:14.088389 140232125040384 logging_writer.py:48] [10000] global_step=10000, preemption_count=0, score=7907.945034
I0401 05:48:14.255192 140405181323072 checkpoints.py:356] Saving checkpoint at step: 10000
I0401 05:48:15.551541 140405181323072 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_10000
I0401 05:48:15.572789 140405181323072 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nesterov/librispeech_conformer_jax/trial_1/checkpoint_10000.
I0401 05:48:16.921316 140405181323072 submission_runner.py:550] Tuning trial 1/1
I0401 05:48:16.921562 140405181323072 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.4394877561366806, one_minus_beta1=0.07113602458522507, warmup_factor=0.05, weight_decay=9.611851572925426e-07, label_smoothing=0.2, dropout_rate=0.0, decay_steps_factor=0.9, end_factor=0.001)
I0401 05:48:16.926082 140405181323072 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/ctc_loss': DeviceArray(31.630554, dtype=float32), 'train/wer': 0.9632571892634779, 'validation/ctc_loss': DeviceArray(31.10666, dtype=float32), 'validation/wer': 0.9135061602137985, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(31.095015, dtype=float32), 'test/wer': 0.9217191721000142, 'test/num_examples': 2472, 'score': 61.09168815612793, 'total_duration': 61.28760313987732, 'global_step': 1, 'preemption_count': 0}), (3031, {'train/ctc_loss': DeviceArray(5.9543867, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': DeviceArray(5.9690914, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(5.949898, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2455.475009202957, 'total_duration': 2574.7525317668915, 'global_step': 3031, 'preemption_count': 0}), (6079, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4850.155949354172, 'total_duration': 5058.099040985107, 'global_step': 6079, 'preemption_count': 0}), (9135, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7244.334079742432, 'total_duration': 7541.468083381653, 'global_step': 9135, 'preemption_count': 0}), (10000, {'train/ctc_loss': DeviceArray(nan, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': DeviceArray(nan, dtype=float32), 'validation/wer': 0.8959951374349969, 'validation/num_examples': 5348, 'test/ctc_loss': DeviceArray(nan, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7907.945033550262, 'total_duration': 8291.150936126709, 'global_step': 10000, 'preemption_count': 0})], 'global_step': 10000}
I0401 05:48:16.926216 140405181323072 submission_runner.py:553] Timing: 7907.945033550262
I0401 05:48:16.926265 140405181323072 submission_runner.py:554] ====================
I0401 05:48:16.926608 140405181323072 submission_runner.py:613] Final librispeech_conformer score: 7907.945033550262
