I0331 18:08:28.447344 140711398131520 logger_utils.py:67] Creating experiment directory at /experiment_runs/timing_nadamw/imagenet_resnet_jax.
I0331 18:08:28.491748 140711398131520 xla_bridge.py:345] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0331 18:08:29.453102 140711398131520 xla_bridge.py:345] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0331 18:08:29.454488 140711398131520 xla_bridge.py:345] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0331 18:08:29.457840 140711398131520 submission_runner.py:511] Using RNG seed 3280663172
I0331 18:08:30.759424 140711398131520 submission_runner.py:520] --- Tuning run 1/1 ---
I0331 18:08:30.759605 140711398131520 submission_runner.py:525] Creating tuning directory at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1.
I0331 18:08:30.759810 140711398131520 logger_utils.py:84] Saving hparams to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/hparams.json.
I0331 18:08:30.879550 140711398131520 submission_runner.py:230] Starting train once: RAM USED (GB) 4.246663168
I0331 18:08:30.879700 140711398131520 submission_runner.py:231] Initializing dataset.
I0331 18:08:30.890114 140711398131520 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:08:30.897315 140711398131520 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 18:08:30.897435 140711398131520 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 18:08:31.108915 140711398131520 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:08:32.039808 140711398131520 submission_runner.py:239] After Initializing dataset: RAM USED (GB) 4.317990912
I0331 18:08:32.039996 140711398131520 submission_runner.py:240] Initializing model.
I0331 18:08:43.317801 140711398131520 submission_runner.py:251] After Initializing model: RAM USED (GB) 8.393621504
I0331 18:08:43.317994 140711398131520 submission_runner.py:252] Initializing optimizer.
I0331 18:08:44.367927 140711398131520 submission_runner.py:260] After Initializing metrics bundle: RAM USED (GB) 8.393637888
I0331 18:08:44.368105 140711398131520 submission_runner.py:261] Initializing metrics bundle.
I0331 18:08:44.368168 140711398131520 submission_runner.py:276] Initializing checkpoint and logger.
I0331 18:08:44.368979 140711398131520 checkpoints.py:466] Found no checkpoint files in /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0331 18:08:45.133131 140711398131520 submission_runner.py:297] Saving meta data to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0331 18:08:45.134047 140711398131520 submission_runner.py:300] Saving flags to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/flags_0.json.
I0331 18:08:45.136982 140711398131520 submission_runner.py:305] After checkpoint and logger metrics bundle: RAM USED (GB) 8.390823936
I0331 18:08:45.137183 140711398131520 submission_runner.py:312] Before starting training loop and logger metrics bundle: RAM USED (GB) 8.390823936
I0331 18:08:45.137249 140711398131520 submission_runner.py:313] Starting training loop.
I0331 18:08:48.732478 140711398131520 submission_runner.py:335] After dataselection batch at step 0: RAM USED (GB) 13.571252224
I0331 18:09:30.032675 140533443852032 logging_writer.py:48] [0] global_step=0, grad_norm=0.5981613993644714, loss=6.925225257873535
I0331 18:09:30.045045 140711398131520 submission_runner.py:352] After update parameters step 0: RAM USED (GB) 24.415686656
I0331 18:09:30.045259 140711398131520 submission_runner.py:373] Before eval at step 1: RAM USED (GB) 24.415686656
I0331 18:09:30.045361 140711398131520 spec.py:298] Evaluating on the training split.
I0331 18:09:30.529529 140711398131520 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:09:30.535657 140711398131520 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 18:09:30.535764 140711398131520 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 18:09:30.594542 140711398131520 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:09:42.237066 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 18:09:42.900877 140711398131520 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:09:42.914345 140711398131520 dataset_info.py:642] Field info.splits from disk and from code do not match. Keeping the one from code.
I0331 18:09:42.914713 140711398131520 dataset_info.py:642] Field info.supervised_keys from disk and from code do not match. Keeping the one from code.
I0331 18:09:42.973183 140711398131520 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0331 18:10:01.541917 140711398131520 spec.py:326] Evaluating on the test split.
I0331 18:10:01.954545 140711398131520 dataset_info.py:566] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 18:10:01.959411 140711398131520 dataset_builder.py:510] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0331 18:10:01.989882 140711398131520 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0331 18:10:10.666195 140711398131520 submission_runner.py:382] Time since start: 44.91s, 	Step: 1, 	{'train/accuracy': 0.0010363520123064518, 'train/loss': 6.912936210632324, 'validation/accuracy': 0.001180000021122396, 'validation/loss': 6.912210941314697, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912200927734375, 'test/num_examples': 10000}
I0331 18:10:10.666909 140711398131520 submission_runner.py:396] After eval at step 1: RAM USED (GB) 66.046578688
I0331 18:10:10.673711 140505249720064 logging_writer.py:48] [1] global_step=1, preemption_count=0, score=44.825652, test/accuracy=0.000900, test/loss=6.912201, test/num_examples=10000, total_duration=44.908066, train/accuracy=0.001036, train/loss=6.912936, validation/accuracy=0.001180, validation/loss=6.912211, validation/num_examples=50000
I0331 18:10:10.846271 140711398131520 checkpoints.py:356] Saving checkpoint at step: 1
I0331 18:10:11.495563 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1
I0331 18:10:11.496848 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1.
I0331 18:10:11.502057 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 1: RAM USED (GB) 66.138980352
I0331 18:10:11.506555 140711398131520 submission_runner.py:335] After dataselection batch at step 1: RAM USED (GB) 66.138980352
I0331 18:10:11.589056 140711398131520 submission_runner.py:352] After update parameters step 1: RAM USED (GB) 66.513747968
I0331 18:10:45.434818 140505258112768 logging_writer.py:48] [100] global_step=100, grad_norm=0.5900586843490601, loss=6.875059604644775
I0331 18:11:19.368608 140505409115904 logging_writer.py:48] [200] global_step=200, grad_norm=0.651543140411377, loss=6.7359819412231445
I0331 18:11:53.234508 140505258112768 logging_writer.py:48] [300] global_step=300, grad_norm=0.721403181552887, loss=6.599205017089844
I0331 18:12:27.126519 140505409115904 logging_writer.py:48] [400] global_step=400, grad_norm=0.8033119440078735, loss=6.468955993652344
I0331 18:13:00.978276 140505258112768 logging_writer.py:48] [500] global_step=500, grad_norm=1.0288845300674438, loss=6.2915425300598145
I0331 18:13:34.940124 140505409115904 logging_writer.py:48] [600] global_step=600, grad_norm=1.4655977487564087, loss=6.155323505401611
I0331 18:14:09.004472 140505258112768 logging_writer.py:48] [700] global_step=700, grad_norm=1.912541389465332, loss=5.97246789932251
I0331 18:14:42.869495 140505409115904 logging_writer.py:48] [800] global_step=800, grad_norm=2.520534038543701, loss=5.913153171539307
I0331 18:15:16.643472 140505258112768 logging_writer.py:48] [900] global_step=900, grad_norm=2.0952794551849365, loss=5.734501838684082
I0331 18:15:50.724878 140505409115904 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.198348522186279, loss=5.6524739265441895
I0331 18:16:24.572079 140505258112768 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.4155797958374023, loss=5.560657978057861
I0331 18:16:58.425937 140505409115904 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.515167236328125, loss=5.395650863647461
I0331 18:17:32.242460 140505258112768 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.871964931488037, loss=5.420298099517822
I0331 18:18:06.155036 140505409115904 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.4100263118743896, loss=5.358772277832031
I0331 18:18:39.890375 140505258112768 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.623029708862305, loss=5.3278045654296875
I0331 18:18:41.644756 140711398131520 submission_runner.py:373] Before eval at step 1507: RAM USED (GB) 68.023341056
I0331 18:18:41.644929 140711398131520 spec.py:298] Evaluating on the training split.
I0331 18:18:48.643452 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 18:18:56.333150 140711398131520 spec.py:326] Evaluating on the test split.
I0331 18:18:58.332644 140711398131520 submission_runner.py:382] Time since start: 596.51s, 	Step: 1507, 	{'train/accuracy': 0.13606105744838715, 'train/loss': 4.62773323059082, 'validation/accuracy': 0.11935999989509583, 'validation/loss': 4.732553482055664, 'validation/num_examples': 50000, 'test/accuracy': 0.08630000054836273, 'test/loss': 5.069921016693115, 'test/num_examples': 10000}
I0331 18:18:58.333282 140711398131520 submission_runner.py:396] After eval at step 1507: RAM USED (GB) 73.46921472
I0331 18:18:58.339895 140505425901312 logging_writer.py:48] [1507] global_step=1507, preemption_count=0, score=549.280461, test/accuracy=0.086300, test/loss=5.069921, test/num_examples=10000, total_duration=596.506100, train/accuracy=0.136061, train/loss=4.627733, validation/accuracy=0.119360, validation/loss=4.732553, validation/num_examples=50000
I0331 18:18:58.517952 140711398131520 checkpoints.py:356] Saving checkpoint at step: 1507
I0331 18:18:59.358558 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1507
I0331 18:18:59.359547 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_1507.
I0331 18:18:59.367621 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 1507: RAM USED (GB) 73.442586624
I0331 18:19:31.108957 140505643980544 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.8585457801818848, loss=5.271871089935303
I0331 18:20:04.894254 140533653526272 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.8042120933532715, loss=5.046506881713867
I0331 18:20:38.774415 140505643980544 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.474567890167236, loss=5.138438701629639
I0331 18:21:12.744973 140533653526272 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.9112374782562256, loss=4.8961405754089355
I0331 18:21:46.673266 140505643980544 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.0550055503845215, loss=4.923817157745361
I0331 18:22:20.479101 140533653526272 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.481067180633545, loss=4.809659957885742
I0331 18:22:54.505493 140505643980544 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.9528050422668457, loss=4.81401252746582
I0331 18:23:28.475500 140533653526272 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.011086940765381, loss=4.662022113800049
I0331 18:24:02.434591 140505643980544 logging_writer.py:48] [2400] global_step=2400, grad_norm=7.907822132110596, loss=4.5850911140441895
I0331 18:24:36.344789 140533653526272 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.7349038124084473, loss=4.583990097045898
I0331 18:25:10.277559 140505643980544 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.396884918212891, loss=4.478724479675293
I0331 18:25:44.131300 140533653526272 logging_writer.py:48] [2700] global_step=2700, grad_norm=7.197053909301758, loss=4.418561935424805
I0331 18:26:18.039276 140505643980544 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.842670440673828, loss=4.33062219619751
I0331 18:26:51.798223 140533653526272 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.222415447235107, loss=4.337398052215576
I0331 18:27:25.579541 140505643980544 logging_writer.py:48] [3000] global_step=3000, grad_norm=7.570230484008789, loss=4.430909633636475
I0331 18:27:29.433241 140711398131520 submission_runner.py:373] Before eval at step 3013: RAM USED (GB) 74.5452544
I0331 18:27:29.433414 140711398131520 spec.py:298] Evaluating on the training split.
I0331 18:27:36.298512 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 18:27:44.041918 140711398131520 spec.py:326] Evaluating on the test split.
I0331 18:27:46.239537 140711398131520 submission_runner.py:382] Time since start: 1124.29s, 	Step: 3013, 	{'train/accuracy': 0.30666056275367737, 'train/loss': 3.3929946422576904, 'validation/accuracy': 0.27487999200820923, 'validation/loss': 3.5563743114471436, 'validation/num_examples': 50000, 'test/accuracy': 0.2070000171661377, 'test/loss': 4.062821865081787, 'test/num_examples': 10000}
I0331 18:27:46.240221 140711398131520 submission_runner.py:396] After eval at step 3013: RAM USED (GB) 79.901618176
I0331 18:27:46.247126 140533653526272 logging_writer.py:48] [3013] global_step=3013, preemption_count=0, score=1053.759018, test/accuracy=0.207000, test/loss=4.062822, test/num_examples=10000, total_duration=1124.294576, train/accuracy=0.306661, train/loss=3.392995, validation/accuracy=0.274880, validation/loss=3.556374, validation/num_examples=50000
I0331 18:27:46.419999 140711398131520 checkpoints.py:356] Saving checkpoint at step: 3013
I0331 18:27:47.044699 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_3013
I0331 18:27:47.045713 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_3013.
I0331 18:27:47.050480 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 3013: RAM USED (GB) 80.008777728
I0331 18:28:16.876694 140505643980544 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5221102237701416, loss=4.290868759155273
I0331 18:28:50.636773 140533619955456 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.239590644836426, loss=4.1330647468566895
I0331 18:29:24.460167 140505643980544 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.5244176387786865, loss=4.265993595123291
I0331 18:29:58.240387 140533619955456 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.138241767883301, loss=4.143594264984131
I0331 18:30:32.205814 140505643980544 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.559714317321777, loss=4.126199722290039
I0331 18:31:05.974221 140533619955456 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.387162923812866, loss=4.005959987640381
I0331 18:31:39.793678 140505643980544 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.810621738433838, loss=3.9677798748016357
I0331 18:32:13.704725 140533619955456 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.900226593017578, loss=4.022477626800537
I0331 18:32:47.465280 140505643980544 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.29495906829834, loss=4.05037784576416
I0331 18:33:21.412585 140533619955456 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.109691619873047, loss=4.014479160308838
I0331 18:33:55.346023 140505643980544 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.5524189472198486, loss=3.8872718811035156
I0331 18:34:29.112992 140533619955456 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.043518543243408, loss=3.907935857772827
I0331 18:35:02.953217 140505643980544 logging_writer.py:48] [4300] global_step=4300, grad_norm=5.1081624031066895, loss=3.8033652305603027
I0331 18:35:36.822487 140533619955456 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.680510997772217, loss=3.784512758255005
I0331 18:36:10.841347 140505643980544 logging_writer.py:48] [4500] global_step=4500, grad_norm=7.294915676116943, loss=3.7048912048339844
I0331 18:36:17.293048 140711398131520 submission_runner.py:373] Before eval at step 4521: RAM USED (GB) 80.894599168
I0331 18:36:17.293220 140711398131520 spec.py:298] Evaluating on the training split.
I0331 18:36:24.010211 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 18:36:32.381422 140711398131520 spec.py:326] Evaluating on the test split.
I0331 18:36:34.550254 140711398131520 submission_runner.py:382] Time since start: 1652.15s, 	Step: 4521, 	{'train/accuracy': 0.42010122537612915, 'train/loss': 2.7191038131713867, 'validation/accuracy': 0.39139997959136963, 'validation/loss': 2.866050958633423, 'validation/num_examples': 50000, 'test/accuracy': 0.2955000102519989, 'test/loss': 3.4994893074035645, 'test/num_examples': 10000}
I0331 18:36:34.551089 140711398131520 submission_runner.py:396] After eval at step 4521: RAM USED (GB) 86.055010304
I0331 18:36:34.559389 140533619955456 logging_writer.py:48] [4521] global_step=4521, preemption_count=0, score=1558.368806, test/accuracy=0.295500, test/loss=3.499489, test/num_examples=10000, total_duration=1652.154364, train/accuracy=0.420101, train/loss=2.719104, validation/accuracy=0.391400, validation/loss=2.866051, validation/num_examples=50000
I0331 18:36:34.806364 140711398131520 checkpoints.py:356] Saving checkpoint at step: 4521
I0331 18:36:35.971016 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_4521
I0331 18:36:35.984953 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_4521.
I0331 18:36:35.993249 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 4521: RAM USED (GB) 86.143660032
I0331 18:37:03.082577 140505643980544 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.6221578121185303, loss=3.701829671859741
I0331 18:37:37.007246 140533611562752 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.0764000415802, loss=3.7539761066436768
I0331 18:38:10.765033 140505643980544 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.127333641052246, loss=3.695809841156006
I0331 18:38:44.635991 140533611562752 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.084299087524414, loss=3.6070735454559326
I0331 18:39:18.346568 140505643980544 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.555782794952393, loss=3.6311733722686768
I0331 18:39:52.144719 140533611562752 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.8698408603668213, loss=3.650017023086548
I0331 18:40:25.883206 140505643980544 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.9051334857940674, loss=3.572347402572632
I0331 18:40:59.743766 140533611562752 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.4728689193725586, loss=3.5052378177642822
I0331 18:41:33.611698 140505643980544 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.1138579845428467, loss=3.486346960067749
I0331 18:42:07.484044 140533611562752 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.314363956451416, loss=3.442807197570801
I0331 18:42:41.264815 140505643980544 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.524059295654297, loss=3.5223441123962402
I0331 18:43:15.082882 140533611562752 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.630998134613037, loss=3.5416955947875977
I0331 18:43:49.028488 140505643980544 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.373246669769287, loss=3.4662249088287354
I0331 18:44:22.900479 140533611562752 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.2659194469451904, loss=3.3792028427124023
I0331 18:44:56.664337 140505643980544 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.2272167205810547, loss=3.349194049835205
I0331 18:45:06.268808 140711398131520 submission_runner.py:373] Before eval at step 6030: RAM USED (GB) 86.840492032
I0331 18:45:06.268995 140711398131520 spec.py:298] Evaluating on the training split.
I0331 18:45:13.493068 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 18:45:22.407326 140711398131520 spec.py:326] Evaluating on the test split.
I0331 18:45:24.309836 140711398131520 submission_runner.py:382] Time since start: 2181.13s, 	Step: 6030, 	{'train/accuracy': 0.4949975907802582, 'train/loss': 2.3120062351226807, 'validation/accuracy': 0.46107998490333557, 'validation/loss': 2.484922170639038, 'validation/num_examples': 50000, 'test/accuracy': 0.35590001940727234, 'test/loss': 3.1410000324249268, 'test/num_examples': 10000}
I0331 18:45:24.310482 140711398131520 submission_runner.py:396] After eval at step 6030: RAM USED (GB) 92.048678912
I0331 18:45:24.317345 140533611562752 logging_writer.py:48] [6030] global_step=6030, preemption_count=0, score=2058.788753, test/accuracy=0.355900, test/loss=3.141000, test/num_examples=10000, total_duration=2181.130283, train/accuracy=0.494998, train/loss=2.312006, validation/accuracy=0.461080, validation/loss=2.484922, validation/num_examples=50000
I0331 18:45:24.485170 140711398131520 checkpoints.py:356] Saving checkpoint at step: 6030
I0331 18:45:25.329343 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_6030
I0331 18:45:25.342761 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_6030.
I0331 18:45:25.348370 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 6030: RAM USED (GB) 92.119261184
I0331 18:45:49.309248 140505643980544 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.4288673400878906, loss=3.4180352687835693
I0331 18:46:23.182261 140533548680960 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.3071117401123047, loss=3.2858002185821533
I0331 18:46:56.802161 140505643980544 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.5270819664001465, loss=3.3402674198150635
I0331 18:47:30.691822 140533548680960 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.776876211166382, loss=3.4077908992767334
I0331 18:48:04.597523 140505643980544 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.7799710035324097, loss=3.366682529449463
I0331 18:48:38.517666 140533548680960 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.7682690620422363, loss=3.3132834434509277
I0331 18:49:12.414954 140505643980544 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.650162696838379, loss=3.355962038040161
I0331 18:49:46.215051 140533548680960 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.3695132732391357, loss=3.3663411140441895
I0331 18:50:20.119785 140505643980544 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.175704002380371, loss=3.3888020515441895
I0331 18:50:53.950732 140533548680960 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.3829214572906494, loss=3.2005228996276855
I0331 18:51:27.674714 140505643980544 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.3959131240844727, loss=3.259925365447998
I0331 18:52:01.447683 140533548680960 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.5916478633880615, loss=3.262650966644287
I0331 18:52:35.236716 140505643980544 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.1757988929748535, loss=3.3329648971557617
I0331 18:53:09.290840 140533548680960 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.0656490325927734, loss=3.177755832672119
I0331 18:53:42.966914 140505643980544 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.619697093963623, loss=3.1359386444091797
I0331 18:53:55.624233 140711398131520 submission_runner.py:373] Before eval at step 7539: RAM USED (GB) 92.7599616
I0331 18:53:55.624417 140711398131520 spec.py:298] Evaluating on the training split.
I0331 18:54:02.866136 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 18:54:12.478156 140711398131520 spec.py:326] Evaluating on the test split.
I0331 18:54:14.641256 140711398131520 submission_runner.py:382] Time since start: 2710.49s, 	Step: 7539, 	{'train/accuracy': 0.5557836294174194, 'train/loss': 2.0007176399230957, 'validation/accuracy': 0.516319990158081, 'validation/loss': 2.1838903427124023, 'validation/num_examples': 50000, 'test/accuracy': 0.4001000225543976, 'test/loss': 2.861550807952881, 'test/num_examples': 10000}
I0331 18:54:14.641834 140711398131520 submission_runner.py:396] After eval at step 7539: RAM USED (GB) 97.808044032
I0331 18:54:14.649242 140533548680960 logging_writer.py:48] [7539] global_step=7539, preemption_count=0, score=2556.305523, test/accuracy=0.400100, test/loss=2.861551, test/num_examples=10000, total_duration=2710.485613, train/accuracy=0.555784, train/loss=2.000718, validation/accuracy=0.516320, validation/loss=2.183890, validation/num_examples=50000
I0331 18:54:14.816927 140711398131520 checkpoints.py:356] Saving checkpoint at step: 7539
I0331 18:54:15.523101 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_7539
I0331 18:54:15.524193 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_7539.
I0331 18:54:15.530169 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 7539: RAM USED (GB) 97.87611136
I0331 18:54:36.591978 140505643980544 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.6780707836151123, loss=3.214561939239502
I0331 18:55:10.499430 140534903445248 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.0667858123779297, loss=3.2112927436828613
I0331 18:55:44.269716 140505643980544 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.072035074234009, loss=3.2056496143341064
I0331 18:56:18.230460 140534903445248 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.2513482570648193, loss=3.223184823989868
I0331 18:56:52.027760 140505643980544 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.4956984519958496, loss=3.15035343170166
I0331 18:57:25.856426 140534903445248 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.797577381134033, loss=3.137099504470825
I0331 18:58:00.024818 140505643980544 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.5618221759796143, loss=3.1954526901245117
I0331 18:58:33.941457 140534903445248 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8676447868347168, loss=3.033334732055664
I0331 18:59:07.636626 140505643980544 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.424656629562378, loss=3.1850552558898926
I0331 18:59:41.547206 140534903445248 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.0083422660827637, loss=3.176077127456665
I0331 19:00:15.316495 140505643980544 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.5594756603240967, loss=3.161473512649536
I0331 19:00:49.207339 140534903445248 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.7138220071792603, loss=3.118452310562134
I0331 19:01:23.228581 140505643980544 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.1808512210845947, loss=3.074031352996826
I0331 19:01:56.967152 140534903445248 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.689659595489502, loss=3.1916394233703613
I0331 19:02:30.700756 140505643980544 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.6320818662643433, loss=3.059081554412842
I0331 19:02:45.682011 140711398131520 submission_runner.py:373] Before eval at step 9046: RAM USED (GB) 98.562469888
I0331 19:02:45.682214 140711398131520 spec.py:298] Evaluating on the training split.
I0331 19:02:52.956963 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 19:03:02.251296 140711398131520 spec.py:326] Evaluating on the test split.
I0331 19:03:04.240039 140711398131520 submission_runner.py:382] Time since start: 3240.54s, 	Step: 9046, 	{'train/accuracy': 0.5922353267669678, 'train/loss': 1.8043971061706543, 'validation/accuracy': 0.5458799600601196, 'validation/loss': 2.0229859352111816, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.729520320892334, 'test/num_examples': 10000}
I0331 19:03:04.240673 140711398131520 submission_runner.py:396] After eval at step 9046: RAM USED (GB) 103.843557376
I0331 19:03:04.248906 140534903445248 logging_writer.py:48] [9046] global_step=9046, preemption_count=0, score=3057.445300, test/accuracy=0.425100, test/loss=2.729520, test/num_examples=10000, total_duration=3240.543497, train/accuracy=0.592235, train/loss=1.804397, validation/accuracy=0.545880, validation/loss=2.022986, validation/num_examples=50000
I0331 19:03:04.424694 140711398131520 checkpoints.py:356] Saving checkpoint at step: 9046
I0331 19:03:05.371259 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_9046
I0331 19:03:05.385974 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_9046.
I0331 19:03:05.395463 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 9046: RAM USED (GB) 103.801171968
I0331 19:03:23.929121 140505643980544 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.6641871929168701, loss=3.1026315689086914
I0331 19:03:57.400327 140534660187904 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.9469481706619263, loss=3.0008163452148438
I0331 19:04:31.193505 140505643980544 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.8262913227081299, loss=3.0880584716796875
I0331 19:05:04.731122 140534660187904 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.6830002069473267, loss=3.083951950073242
I0331 19:05:38.574342 140505643980544 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.626575231552124, loss=2.9849016666412354
I0331 19:06:12.365854 140534660187904 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8842859268188477, loss=3.0305428504943848
I0331 19:06:45.938487 140505643980544 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.0074188709259033, loss=3.0605597496032715
I0331 19:07:19.644805 140534660187904 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.7823220491409302, loss=3.0109686851501465
I0331 19:07:53.337689 140505643980544 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6097592115402222, loss=2.959162950515747
I0331 19:08:26.955146 140534660187904 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4540475606918335, loss=3.084089756011963
I0331 19:09:00.607027 140505643980544 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.235710620880127, loss=2.966447353363037
I0331 19:09:34.275386 140534660187904 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5514603853225708, loss=3.069024085998535
I0331 19:10:07.852361 140505643980544 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.4094258546829224, loss=2.951287269592285
I0331 19:10:41.545181 140534660187904 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.2561370134353638, loss=2.865485668182373
I0331 19:11:15.212850 140505643980544 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.7965012788772583, loss=2.935246229171753
I0331 19:11:35.506151 140711398131520 submission_runner.py:373] Before eval at step 10562: RAM USED (GB) 104.532832256
I0331 19:11:35.506338 140711398131520 spec.py:298] Evaluating on the training split.
I0331 19:11:42.574794 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 19:11:51.777927 140711398131520 spec.py:326] Evaluating on the test split.
I0331 19:11:53.905053 140711398131520 submission_runner.py:382] Time since start: 3770.37s, 	Step: 10562, 	{'train/accuracy': 0.6567681431770325, 'train/loss': 1.4954708814620972, 'validation/accuracy': 0.5811799764633179, 'validation/loss': 1.849315881729126, 'validation/num_examples': 50000, 'test/accuracy': 0.45920002460479736, 'test/loss': 2.5371360778808594, 'test/num_examples': 10000}
I0331 19:11:53.905736 140711398131520 submission_runner.py:396] After eval at step 10562: RAM USED (GB) 109.728256
I0331 19:11:53.913016 140534660187904 logging_writer.py:48] [10562] global_step=10562, preemption_count=0, score=3557.704668, test/accuracy=0.459200, test/loss=2.537136, test/num_examples=10000, total_duration=3770.366421, train/accuracy=0.656768, train/loss=1.495471, validation/accuracy=0.581180, validation/loss=1.849316, validation/num_examples=50000
I0331 19:11:54.084247 140711398131520 checkpoints.py:356] Saving checkpoint at step: 10562
I0331 19:11:54.860942 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_10562
I0331 19:11:54.864640 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_10562.
I0331 19:11:54.869799 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 10562: RAM USED (GB) 109.794668544
I0331 19:12:07.962183 140505643980544 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.2127419710159302, loss=2.9528183937072754
I0331 19:12:41.827089 140534651795200 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.796022891998291, loss=2.9459586143493652
I0331 19:13:15.487817 140505643980544 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.38626229763031, loss=2.8934671878814697
I0331 19:13:49.176876 140534651795200 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5777829885482788, loss=2.8490147590637207
I0331 19:14:22.824784 140505643980544 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.735333800315857, loss=2.924452304840088
I0331 19:14:56.607598 140534651795200 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.398686408996582, loss=2.9976279735565186
I0331 19:15:30.327356 140505643980544 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.9318996667861938, loss=2.871899127960205
I0331 19:16:04.121290 140534651795200 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.6130146980285645, loss=2.9028172492980957
I0331 19:16:38.031328 140505643980544 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.834093451499939, loss=2.824829339981079
I0331 19:17:11.615641 140534651795200 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.1870981454849243, loss=2.8386175632476807
I0331 19:17:45.085571 140505643980544 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.1836788654327393, loss=2.831618070602417
I0331 19:18:18.784109 140534651795200 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.2517259120941162, loss=2.820107936859131
I0331 19:18:52.514543 140505643980544 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.1230791807174683, loss=2.735288381576538
I0331 19:19:26.073941 140534651795200 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.5202560424804688, loss=2.8622817993164062
I0331 19:19:59.865169 140505643980544 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.3744683265686035, loss=2.7227540016174316
I0331 19:20:24.923839 140711398131520 submission_runner.py:373] Before eval at step 12076: RAM USED (GB) 110.344359936
I0331 19:20:24.924062 140711398131520 spec.py:298] Evaluating on the training split.
I0331 19:20:31.972133 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 19:20:41.580805 140711398131520 spec.py:326] Evaluating on the test split.
I0331 19:20:43.520056 140711398131520 submission_runner.py:382] Time since start: 4299.78s, 	Step: 12076, 	{'train/accuracy': 0.6671715378761292, 'train/loss': 1.4440593719482422, 'validation/accuracy': 0.6018399596214294, 'validation/loss': 1.7481681108474731, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.4620704650878906, 'test/num_examples': 10000}
I0331 19:20:43.520739 140711398131520 submission_runner.py:396] After eval at step 12076: RAM USED (GB) 115.524079616
I0331 19:20:43.528251 140534651795200 logging_writer.py:48] [12076] global_step=12076, preemption_count=0, score=4062.070015, test/accuracy=0.470800, test/loss=2.462070, test/num_examples=10000, total_duration=4299.783326, train/accuracy=0.667172, train/loss=1.444059, validation/accuracy=0.601840, validation/loss=1.748168, validation/num_examples=50000
I0331 19:20:43.730705 140711398131520 checkpoints.py:356] Saving checkpoint at step: 12076
I0331 19:20:44.666865 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_12076
I0331 19:20:44.680470 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_12076.
I0331 19:20:44.686023 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 12076: RAM USED (GB) 115.626971136
I0331 19:20:53.188045 140505643980544 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1154725551605225, loss=2.7793333530426025
I0331 19:21:27.243916 140534643402496 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.2143293619155884, loss=2.7970049381256104
I0331 19:22:01.014324 140505643980544 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9434966444969177, loss=2.7149624824523926
I0331 19:22:34.935178 140534643402496 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.2029504776000977, loss=2.7309155464172363
I0331 19:23:08.773343 140505643980544 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.1412379741668701, loss=2.7904481887817383
I0331 19:23:42.575655 140534643402496 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0739569664001465, loss=2.683032989501953
I0331 19:24:16.504905 140505643980544 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.3312493562698364, loss=2.824674129486084
I0331 19:24:50.392109 140534643402496 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.2494584321975708, loss=2.751042366027832
I0331 19:25:24.279327 140505643980544 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.1642059087753296, loss=2.7614760398864746
I0331 19:25:58.208796 140534643402496 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.623216152191162, loss=2.8348488807678223
I0331 19:26:32.323011 140505643980544 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.2101085186004639, loss=2.704103708267212
I0331 19:27:06.191516 140534643402496 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.2234121561050415, loss=2.7650644779205322
I0331 19:27:40.043419 140505643980544 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0574347972869873, loss=2.7232818603515625
I0331 19:28:13.890067 140534643402496 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.4817126989364624, loss=2.8384804725646973
I0331 19:28:47.809371 140505643980544 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.229956865310669, loss=2.7542600631713867
I0331 19:29:14.713786 140711398131520 submission_runner.py:373] Before eval at step 13581: RAM USED (GB) 116.272058368
I0331 19:29:14.714017 140711398131520 spec.py:298] Evaluating on the training split.
I0331 19:29:21.844052 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 19:29:31.355411 140711398131520 spec.py:326] Evaluating on the test split.
I0331 19:29:33.488322 140711398131520 submission_runner.py:382] Time since start: 4829.57s, 	Step: 13581, 	{'train/accuracy': 0.6809231638908386, 'train/loss': 1.3985317945480347, 'validation/accuracy': 0.616379976272583, 'validation/loss': 1.7017579078674316, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.3998289108276367, 'test/num_examples': 10000}
I0331 19:29:33.488934 140711398131520 submission_runner.py:396] After eval at step 13581: RAM USED (GB) 121.505316864
I0331 19:29:33.497385 140534643402496 logging_writer.py:48] [13581] global_step=13581, preemption_count=0, score=4566.466235, test/accuracy=0.486100, test/loss=2.399829, test/num_examples=10000, total_duration=4829.574169, train/accuracy=0.680923, train/loss=1.398532, validation/accuracy=0.616380, validation/loss=1.701758, validation/num_examples=50000
I0331 19:29:33.735629 140711398131520 checkpoints.py:356] Saving checkpoint at step: 13581
I0331 19:29:34.697961 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_13581
I0331 19:29:34.711037 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_13581.
I0331 19:29:34.720226 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 13581: RAM USED (GB) 121.599827968
I0331 19:29:41.479570 140505643980544 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.088787317276001, loss=2.7211039066314697
I0331 19:30:15.320691 140534635009792 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.2781641483306885, loss=2.7141735553741455
I0331 19:30:49.054923 140505643980544 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.4227865934371948, loss=2.683396816253662
I0331 19:31:22.891286 140534635009792 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9269612431526184, loss=2.763850212097168
I0331 19:31:56.246943 140711398131520 submission_runner.py:373] Before eval at step 14000: RAM USED (GB) 122.105540608
I0331 19:31:56.247134 140711398131520 spec.py:298] Evaluating on the training split.
I0331 19:32:03.222976 140711398131520 spec.py:310] Evaluating on the validation split.
I0331 19:32:12.803255 140711398131520 spec.py:326] Evaluating on the test split.
I0331 19:32:14.874847 140711398131520 submission_runner.py:382] Time since start: 4991.11s, 	Step: 14000, 	{'train/accuracy': 0.6869021058082581, 'train/loss': 1.3595224618911743, 'validation/accuracy': 0.6282399892807007, 'validation/loss': 1.6294463872909546, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.3274717330932617, 'test/num_examples': 10000}
I0331 19:32:14.875510 140711398131520 submission_runner.py:396] After eval at step 14000: RAM USED (GB) 127.263809536
I0331 19:32:14.883386 140505643980544 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4706.361275, test/accuracy=0.500100, test/loss=2.327472, test/num_examples=10000, total_duration=4991.108306, train/accuracy=0.686902, train/loss=1.359522, validation/accuracy=0.628240, validation/loss=1.629446, validation/num_examples=50000
I0331 19:32:15.123738 140711398131520 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 19:32:16.065798 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_14000
I0331 19:32:16.079288 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0331 19:32:16.083982 140711398131520 submission_runner.py:416] After logging and checkpointing eval at step 14000: RAM USED (GB) 127.452233728
I0331 19:32:16.094249 140534635009792 logging_writer.py:48] [14000] global_step=14000, preemption_count=0, score=4706.361275
I0331 19:32:16.245269 140711398131520 checkpoints.py:356] Saving checkpoint at step: 14000
I0331 19:32:17.568848 140711398131520 checkpoints.py:317] Saved checkpoint at /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_14000
I0331 19:32:17.581685 140711398131520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/timing_nadamw/imagenet_resnet_jax/trial_1/checkpoint_14000.
I0331 19:32:17.738263 140711398131520 submission_runner.py:550] Tuning trial 1/1
I0331 19:32:17.738482 140711398131520 submission_runner.py:551] Hyperparameters: Hyperparameters(learning_rate=0.0019814680146414726, one_minus_beta1=0.045677535963609565, beta2=0.999, warmup_factor=0.05, weight_decay=0.010340635370188849, label_smoothing=0.1, dropout_rate=0.0)
I0331 19:32:17.741947 140711398131520 submission_runner.py:552] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010363520123064518, 'train/loss': 6.912936210632324, 'validation/accuracy': 0.001180000021122396, 'validation/loss': 6.912210941314697, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912200927734375, 'test/num_examples': 10000, 'score': 44.82565212249756, 'total_duration': 44.90806555747986, 'global_step': 1, 'preemption_count': 0}), (1507, {'train/accuracy': 0.13606105744838715, 'train/loss': 4.62773323059082, 'validation/accuracy': 0.11935999989509583, 'validation/loss': 4.732553482055664, 'validation/num_examples': 50000, 'test/accuracy': 0.08630000054836273, 'test/loss': 5.069921016693115, 'test/num_examples': 10000, 'score': 549.2804605960846, 'total_duration': 596.5060997009277, 'global_step': 1507, 'preemption_count': 0}), (3013, {'train/accuracy': 0.30666056275367737, 'train/loss': 3.3929946422576904, 'validation/accuracy': 0.27487999200820923, 'validation/loss': 3.5563743114471436, 'validation/num_examples': 50000, 'test/accuracy': 0.2070000171661377, 'test/loss': 4.062821865081787, 'test/num_examples': 10000, 'score': 1053.759017944336, 'total_duration': 1124.2945764064789, 'global_step': 3013, 'preemption_count': 0}), (4521, {'train/accuracy': 0.42010122537612915, 'train/loss': 2.7191038131713867, 'validation/accuracy': 0.39139997959136963, 'validation/loss': 2.866050958633423, 'validation/num_examples': 50000, 'test/accuracy': 0.2955000102519989, 'test/loss': 3.4994893074035645, 'test/num_examples': 10000, 'score': 1558.368805885315, 'total_duration': 1652.1543638706207, 'global_step': 4521, 'preemption_count': 0}), (6030, {'train/accuracy': 0.4949975907802582, 'train/loss': 2.3120062351226807, 'validation/accuracy': 0.46107998490333557, 'validation/loss': 2.484922170639038, 'validation/num_examples': 50000, 'test/accuracy': 0.35590001940727234, 'test/loss': 3.1410000324249268, 'test/num_examples': 10000, 'score': 2058.7887530326843, 'total_duration': 2181.1302828788757, 'global_step': 6030, 'preemption_count': 0}), (7539, {'train/accuracy': 0.5557836294174194, 'train/loss': 2.0007176399230957, 'validation/accuracy': 0.516319990158081, 'validation/loss': 2.1838903427124023, 'validation/num_examples': 50000, 'test/accuracy': 0.4001000225543976, 'test/loss': 2.861550807952881, 'test/num_examples': 10000, 'score': 2556.3055233955383, 'total_duration': 2710.485612630844, 'global_step': 7539, 'preemption_count': 0}), (9046, {'train/accuracy': 0.5922353267669678, 'train/loss': 1.8043971061706543, 'validation/accuracy': 0.5458799600601196, 'validation/loss': 2.0229859352111816, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.729520320892334, 'test/num_examples': 10000, 'score': 3057.445300102234, 'total_duration': 3240.54349732399, 'global_step': 9046, 'preemption_count': 0}), (10562, {'train/accuracy': 0.6567681431770325, 'train/loss': 1.4954708814620972, 'validation/accuracy': 0.5811799764633179, 'validation/loss': 1.849315881729126, 'validation/num_examples': 50000, 'test/accuracy': 0.45920002460479736, 'test/loss': 2.5371360778808594, 'test/num_examples': 10000, 'score': 3557.704668045044, 'total_duration': 3770.3664214611053, 'global_step': 10562, 'preemption_count': 0}), (12076, {'train/accuracy': 0.6671715378761292, 'train/loss': 1.4440593719482422, 'validation/accuracy': 0.6018399596214294, 'validation/loss': 1.7481681108474731, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.4620704650878906, 'test/num_examples': 10000, 'score': 4062.0700154304504, 'total_duration': 4299.783325910568, 'global_step': 12076, 'preemption_count': 0}), (13581, {'train/accuracy': 0.6809231638908386, 'train/loss': 1.3985317945480347, 'validation/accuracy': 0.616379976272583, 'validation/loss': 1.7017579078674316, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.3998289108276367, 'test/num_examples': 10000, 'score': 4566.466235399246, 'total_duration': 4829.574169397354, 'global_step': 13581, 'preemption_count': 0}), (14000, {'train/accuracy': 0.6869021058082581, 'train/loss': 1.3595224618911743, 'validation/accuracy': 0.6282399892807007, 'validation/loss': 1.6294463872909546, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.3274717330932617, 'test/num_examples': 10000, 'score': 4706.361274957657, 'total_duration': 4991.10830616951, 'global_step': 14000, 'preemption_count': 0})], 'global_step': 14000}
I0331 19:32:17.742055 140711398131520 submission_runner.py:553] Timing: 4706.361274957657
I0331 19:32:17.742145 140711398131520 submission_runner.py:554] ====================
I0331 19:32:17.747334 140711398131520 submission_runner.py:613] Final imagenet_resnet score: 4706.361274957657
